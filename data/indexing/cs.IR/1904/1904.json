[{"id": "1904.00110", "submitter": "Erion \\c{C}ano", "authors": "Erion \\c{C}ano and Ond\\v{r}ej Bojar", "title": "Keyphrase Generation: A Text Summarization Struggle", "comments": "7 pages, 3 tables. Published in proceedings of 2019 Annual Conference\n  of the North American Chapter of the Association for Computational\n  Linguistics. Identical to the previous version", "journal-ref": null, "doi": "10.18653/v1/N19-1070", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Authors' keyphrases assigned to scientific articles are essential for\nrecognizing content and topic aspects. Most of the proposed supervised and\nunsupervised methods for keyphrase generation are unable to produce terms that\nare valuable but do not appear in the text. In this paper, we explore the\npossibility of considering the keyphrase string as an abstractive summary of\nthe title and the abstract. First, we collect, process and release a large\ndataset of scientific paper metadata that contains 2.2 million records. Then we\nexperiment with popular text summarization neural architectures. Despite using\nadvanced deep learning models, large quantities of data and many days of\ncomputation, our systematic evaluation on four test datasets reveals that the\nexplored text summarization methods could not produce better keyphrases than\nthe simpler unsupervised methods, or the existing supervised ones.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 22:43:26 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 19:54:28 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["\u00c7ano", "Erion", ""], ["Bojar", "Ond\u0159ej", ""]]}, {"id": "1904.00132", "submitter": "Chenyang Huang", "authors": "Chenyang Huang, Amine Trabelsi, Osmar R. Za\\\"iane", "title": "ANA at SemEval-2019 Task 3: Contextual Emotion detection in\n  Conversations through hierarchical LSTMs and BERT", "comments": "Accepted at the SemEval-2019 International Workshop on Semantic\n  Evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the system submitted by ANA Team for the SemEval-2019\nTask 3: EmoContext. We propose a novel Hierarchical LSTMs for Contextual\nEmotion Detection (HRLCE) model. It classifies the emotion of an utterance\ngiven its conversational context. The results show that, in this task, our\nHRCLE outperforms the most recent state-of-the-art text classification\nframework: BERT. We combine the results generated by BERT and HRCLE to achieve\nan overall score of 0.7709 which ranked 5th on the final leader board of the\ncompetition among 165 Teams.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 01:51:24 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 20:43:22 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Huang", "Chenyang", ""], ["Trabelsi", "Amine", ""], ["Za\u00efane", "Osmar R.", ""]]}, {"id": "1904.00289", "submitter": "Casper Petersen", "authors": "Casper Petersen", "title": "On the Estimation and Use of Statistical Modelling in Information\n  Retrieval", "comments": "Phd thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several tasks in information retrieval (IR) rely on assumptions regarding the\ndistribution of some property (such as term frequency) in the data being\nprocessed. This thesis argues that such distributional assumptions can lead to\nincorrect conclusions and proposes a statistically principled method for\ndetermining the \"true\" distribution. This thesis further applies this method to\nderive a new family of ranking models that adapt their computations to the\nstatistics of the data being processed. Experimental evaluation shows results\non par or better than multiple strong baselines on several TREC collections.\nOverall, this thesis concludes that distributional assumptions can be replaced\nwith an effective, efficient and principled method for determining the \"true\"\ndistribution and that using the \"true\" distribution can lead to improved\nretrieval performance.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 21:14:09 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Petersen", "Casper", ""]]}, {"id": "1904.00542", "submitter": "Ramy Baly", "authors": "Ramy Baly (MIT Computer Science and Artificial Intelligence\n  Laboratory, MA, USA) and Georgi Karadzhov (SiteGround Hosting EOOD, Bulgaria)\n  and Abdelrhman Saleh (Harvard University, MA, USA) and James Glass (MIT\n  Computer Science and Artificial Intelligence Laboratory, MA, USA) and Preslav\n  Nakov (Qatar Computing Research Institute, HBKU, Qatar)", "title": "Multi-Task Ordinal Regression for Jointly Predicting the Trustworthiness\n  and the Leading Political Ideology of News Media", "comments": "Fact-checking, political ideology, news media, NAACL-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of fake news, bias, and propaganda, we study two important but\nrelatively under-explored problems: (i) trustworthiness estimation (on a\n3-point scale) and (ii) political ideology detection (left/right bias on a\n7-point scale) of entire news outlets, as opposed to evaluating individual\narticles. In particular, we propose a multi-task ordinal regression framework\nthat models the two problems jointly. This is motivated by the observation that\nhyper-partisanship is often linked to low trustworthiness, e.g., appealing to\nemotions rather than sticking to the facts, while center media tend to be\ngenerally more impartial and trustworthy. We further use several auxiliary\ntasks, modeling centrality, hyperpartisanship, as well as left-vs.-right bias\non a coarse-grained scale. The evaluation results show sizable performance\ngains by the joint models over models that target the problems in isolation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 02:54:54 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Baly", "Ramy", "", "MIT Computer Science and Artificial Intelligence\n  Laboratory, MA, USA"], ["Karadzhov", "Georgi", "", "SiteGround Hosting EOOD, Bulgaria"], ["Saleh", "Abdelrhman", "", "Harvard University, MA, USA"], ["Glass", "James", "", "MIT\n  Computer Science and Artificial Intelligence Laboratory, MA, USA"], ["Nakov", "Preslav", "", "Qatar Computing Research Institute, HBKU, Qatar"]]}, {"id": "1904.00672", "submitter": "Leyang Xue", "authors": "Leyang Xue, Peng Zhang, An Zeng", "title": "Enhancing the long-term performance of recommender system", "comments": "16 pages, 10 figures", "journal-ref": "Physica A. 531.2019.121731", "doi": "10.1016/j.physa.2019.121731", "report-no": "0378-4371", "categories": "physics.soc-ph cs.HC cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender system is a critically important tool in online commercial system\nand provide users with personalized recommendation on items. So far, numerous\nrecommendation algorithms have been made to further improve the recommendation\nperformance in a single-step recommendation, while the long-term recommendation\nperformance is neglected. In this paper, we proposed an approach called\nAdjustment of Recommendation List (ARL) to enhance the long-term recommendation\naccuracy. In order to observe the long-term accuracy, we developed an evolution\nmodel of network to simulate the interaction between the recommender system and\nuser's behaviour. The result shows that not only long-term recommendation\naccuracy can be enhanced significantly but the diversity of item in online\nsystem maintains healthy. Notably, an optimal parameter n* of ARL existed in\nlong-term recommendation, indicating that there is a trade-off between keeping\ndiversity of item and user's preference to maximize the long-term\nrecommendation accuracy. Finally, we confirmed that the optimal parameter n* is\nstable during evolving network, which reveals the robustness of ARL method.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 09:55:16 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Xue", "Leyang", ""], ["Zhang", "Peng", ""], ["Zeng", "An", ""]]}, {"id": "1904.00714", "submitter": "Evgeny Krivosheev", "authors": "Evgeny Krivosheev, Fabio Casati, Marcos Baez, Boualem Benatallah", "title": "Combining Crowd and Machines for Multi-predicate Item Screening", "comments": "Please cite the CSCW2018 version of this\n  paper:@article{krivosheev2018combining, title={Combining Crowd and Machines\n  for Multi-predicate Item Screening}, author={Krivosheev, Evgeny and Casati,\n  Fabio and Baez, Marcos and Benatallah, Boualem}, journal={Proceedings of the\n  ACM on Human-Computer Interaction}, volume={2}, number={CSCW}, pages={97},\n  year={2018}, publisher={ACM} }", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses how crowd and machine classifiers can be efficiently\ncombined to screen items that satisfy a set of predicates. We show that this is\na recurring problem in many domains, present machine-human (hybrid) algorithms\nthat screen items efficiently and estimate the gain over human-only or\nmachine-only screening in terms of performance and cost. We further show how,\ngiven a new classification problem and a set of classifiers of unknown accuracy\nfor the problem at hand, we can identify how to manage the cost-accuracy trade\noff by progressively determining if we should spend budget to obtain test data\n(to assess the accuracy of the given classifiers), or to train an ensemble of\nclassifiers, or whether we should leverage the existing machine classifiers\nwith the crowd, and in this case how to efficiently combine them based on their\nestimated characteristics to obtain the classification. We demonstrate that the\ntechniques we propose obtain significant cost/accuracy improvements with\nrespect to the leading classification algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 12:02:55 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Krivosheev", "Evgeny", ""], ["Casati", "Fabio", ""], ["Baez", "Marcos", ""], ["Benatallah", "Boualem", ""]]}, {"id": "1904.00720", "submitter": "Ziyu Yao", "authors": "Ziyu Yao, Jayavardhan Reddy Peddamail, Huan Sun", "title": "CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning", "comments": "10 pages, 2 figures. Accepted by The Web Conference (WWW) 2019", "journal-ref": null, "doi": "10.1145/3308558.3313632", "report-no": null, "categories": "cs.SE cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To accelerate software development, much research has been performed to help\npeople understand and reuse the huge amount of available code resources. Two\nimportant tasks have been widely studied: code retrieval, which aims to\nretrieve code snippets relevant to a given natural language query from a code\nbase, and code annotation, where the goal is to annotate a code snippet with a\nnatural language description. Despite their advancement in recent years, the\ntwo tasks are mostly explored separately. In this work, we investigate a novel\nperspective of Code annotation for Code retrieval (hence called `CoaCor'),\nwhere a code annotation model is trained to generate a natural language\nannotation that can represent the semantic meaning of a given code snippet and\ncan be leveraged by a code retrieval model to better distinguish relevant code\nsnippets from others. To this end, we propose an effective framework based on\nreinforcement learning, which explicitly encourages the code annotation model\nto generate annotations that can be used for the retrieval task. Through\nextensive experiments, we show that code annotations generated by our framework\nare much more detailed and more useful for code retrieval, and they can further\nimprove the performance of existing code retrieval models significantly.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 19:22:22 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Yao", "Ziyu", ""], ["Peddamail", "Jayavardhan Reddy", ""], ["Sun", "Huan", ""]]}, {"id": "1904.00762", "submitter": "Subba Reddy Oota", "authors": "Subba Reddy Oota, Adithya Avvaru, Mounika Marreddy, Radhika Mamidi", "title": "Affect in Tweets Using Experts Model", "comments": "10 pages, 6 figures, The 32nd Pacific Asia Conference on Language,\n  Information and Computation (PACLIC 32)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the intensity of emotion has gained significance as modern textual\ninputs in potential applications like social media, e-retail markets,\npsychology, advertisements etc., carry a lot of emotions, feelings, expressions\nalong with its meaning. However, the approaches of traditional sentiment\nanalysis primarily focuses on classifying the sentiment in general (positive or\nnegative) or at an aspect level(very positive, low negative, etc.) and cannot\nexploit the intensity information. Moreover, automatically identifying emotions\nlike anger, fear, joy, sadness, disgust etc., from text introduces challenging\nscenarios where single tweet may contain multiple emotions with different\nintensities and some emotions may even co-occur in some of the tweets. In this\npaper, we propose an architecture, Experts Model, inspired from the standard\nMixture of Experts (MoE) model. The key idea here is each expert learns\ndifferent sets of features from the feature vector which helps in better\nemotion detection from the tweet. We compared the results of our Experts Model\nwith both baseline results and top five performers of SemEval-2018 Task-1,\nAffect in Tweets (AIT). The experimental results show that our proposed\napproach deals with the emotion detection problem and stands at top-5 results.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 11:10:29 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Oota", "Subba Reddy", ""], ["Avvaru", "Adithya", ""], ["Marreddy", "Mounika", ""], ["Mamidi", "Radhika", ""]]}, {"id": "1904.01313", "submitter": "Yanxuan Li", "authors": "Yanxuan Li", "title": "Short Text Classification Improved by Feature Space Extension", "comments": "8 pages,2 figures and 7 tables.to be published in", "journal-ref": null, "doi": "10.1088/1757-899X/533/1/012046", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the explosive development of mobile Internet, short text has been\napplied extensively. The difference between classifying short text and long\ndocuments is that short text is of shortness and sparsity. Thus, it is\nchallenging to deal with short text classification owing to its less semantic\ninformation. In this paper, we propose a novel topic-based convolutional neural\nnetwork (TB-CNN) based on Latent Dirichlet Allocation (LDA) model and\nconvolutional neural network. Comparing to traditional CNN methods, TB-CNN\ngenerates topic words with LDA model to reduce the sparseness and combines the\nembedding vectors of topic words and input words to extend feature space of\nshort text. The validation results on IMDB movie review dataset show the\nimprovement and effectiveness of TB-CNN.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 10:00:58 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Li", "Yanxuan", ""]]}, {"id": "1904.01353", "submitter": "Oleksandra Panasiuk", "authors": "Oleksandra Panasiuk, Omar Holzknecht, Umutcan \\c{S}im\\c{s}ek, Elias\n  K\\\"arle and Dieter Fensel", "title": "Verification and Validation of Semantic Annotations", "comments": "Accepted for the A.P. Ershov Informatics Conference 2019(the PSI\n  Conference Series, 12th edition) proceeding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a framework to perform verification and validation\nof semantically annotated data. The annotations, extracted from websites, are\nverified against the schema.org vocabulary and Domain Specifications to ensure\nthe syntactic correctness and completeness of the annotations. The Domain\nSpecifications allow checking the compliance of annotations against\ncorresponding domain-specific constraints. The validation mechanism will detect\nerrors and inconsistencies between the content of the analyzed schema.org\nannotations and the content of the web pages where the annotations were found.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 11:50:55 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 11:16:54 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Panasiuk", "Oleksandra", ""], ["Holzknecht", "Omar", ""], ["\u015eim\u015fek", "Umutcan", ""], ["K\u00e4rle", "Elias", ""], ["Fensel", "Dieter", ""]]}, {"id": "1904.01718", "submitter": "Haozhen Zhao", "authors": "Rishi Chhatwal, Nathaniel Huber-Fliflet, Robert Keeling, Jianping\n  Zhang, Haozhen Zhao", "title": "Empirical Evaluations of Preprocessing Parameters' Impact on Predictive\n  Coding's Effectiveness", "comments": "2016 IEEE International Conference on Big Data (Big Data)", "journal-ref": null, "doi": "10.1109/BigData.2016.7840747", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive coding, once used in only a small fraction of legal and business\nmatters, is now widely deployed to quickly cull through increasingly vast\namounts of data and reduce the need for costly and inefficient human document\nreview. Previously, the sole front-end input used to create a predictive model\nwas the exemplar documents (training data) chosen by subject-matter experts.\nMany predictive coding tools require users to rely on static preprocessing\nparameters and a single machine learning algorithm to develop the predictive\nmodel. Little research has been published discussing the impact preprocessing\nparameters and learning algorithms have on the effectiveness of the technology.\nA deeper dive into the generation of a predictive model shows that the settings\nand algorithm can have a strong effect on the accuracy and efficacy of a\npredictive coding tool. Understanding how these input parameters affect the\noutput will empower legal teams with the information they need to implement\npredictive coding as efficiently and effectively as possible. This paper\noutlines different preprocessing parameters and algorithms as applied to\nmultiple real-world data sets to understand the influence of various\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 00:54:55 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Chhatwal", "Rishi", ""], ["Huber-Fliflet", "Nathaniel", ""], ["Keeling", "Robert", ""], ["Zhang", "Jianping", ""], ["Zhao", "Haozhen", ""]]}, {"id": "1904.01719", "submitter": "Haozhen Zhao", "authors": "Rishi Chhatwal, Nathaniel Huber-Fliflet, Robert Keeling, Jianping\n  Zhang, Haozhen Zhao", "title": "Empirical Evaluations of Active Learning Strategies in Legal Document\n  Review", "comments": "2017 IEEE International Conference on Big Data (Big Data)", "journal-ref": null, "doi": "10.1109/BigData.2017.8258076", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One type of machine learning, text classification, is now regularly applied\nin the legal matters involving voluminous document populations because it can\nreduce the time and expense associated with the review of those documents. One\nform of machine learning - Active Learning - has drawn attention from the legal\ncommunity because it offers the potential to make the machine learning process\neven more effective. Active Learning, applied to legal documents, is considered\na new technology in the legal domain and is continuously applied to all\ndocuments in a legal matter until an insignificant number of relevant documents\nare left for review. This implementation is slightly different than traditional\nimplementations of Active Learning where the process stops once achieving\nacceptable model performance. The purpose of this paper is twofold: (i) to\nquestion whether Active Learning actually is a superior learning methodology\nand (ii) to highlight the ways that Active Learning can be most effectively\napplied to real legal industry data. Unlike other studies, our experiments were\nperformed against large data sets taken from recent, real-world legal matters\ncovering a variety of areas. We conclude that, although these experiments show\nthe Active Learning strategy popularly used in legal document review can\nquickly identify informative training documents, it becomes less effective over\ntime. In particular, our findings suggest this most popular form of Active\nLearning in the legal arena, where the highest-scoring documents are selected\nas training examples, is in fact not the most efficient approach in most\ninstances. Ultimately, a different Active Learning strategy may be best suited\nto initiate the predictive modeling process but not to continue through the\nentire document review.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 00:56:14 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Chhatwal", "Rishi", ""], ["Huber-Fliflet", "Nathaniel", ""], ["Keeling", "Robert", ""], ["Zhang", "Jianping", ""], ["Zhao", "Haozhen", ""]]}, {"id": "1904.01721", "submitter": "Haozhen Zhao", "authors": "Rishi Chhatwal, Peter Gronvall, Nathaniel Huber-Fliflet, Robert\n  Keeling, Jianping Zhang, Haozhen Zhao", "title": "Explainable Text Classification in Legal Document Review A Case Study of\n  Explainable Predictive Coding", "comments": "2018 IEEE International Conference on Big Data", "journal-ref": null, "doi": "10.1109/BigData.2018.8622073", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's legal environment, lawsuits and regulatory investigations require\ncompanies to embark upon increasingly intensive data-focused engagements to\nidentify, collect and analyze large quantities of data. When documents are\nstaged for review the process can require companies to dedicate an\nextraordinary level of resources, both with respect to human resources, but\nalso with respect to the use of technology-based techniques to intelligently\nsift through data. For several years, attorneys have been using a variety of\ntools to conduct this exercise, and most recently, they are accepting the use\nof machine learning techniques like text classification to efficiently cull\nmassive volumes of data to identify responsive documents for use in these\nmatters. In recent years, a group of AI and Machine Learning researchers have\nbeen actively researching Explainable AI. In an explainable AI system, actions\nor decisions are human understandable. In typical legal `document review'\nscenarios, a document can be identified as responsive, as long as one or more\nof the text snippets in a document are deemed responsive. In these scenarios,\nif predictive coding can be used to locate these responsive snippets, then\nattorneys could easily evaluate the model's document classification decision.\nWhen deployed with defined and explainable results, predictive coding can\ndrastically enhance the overall quality and speed of the document review\nprocess by reducing the time it takes to review documents. The authors of this\npaper propose the concept of explainable predictive coding and simple\nexplainable predictive coding methods to locate responsive snippets within\nresponsive documents. We also report our preliminary experimental results using\nthe data from an actual legal matter that entailed this type of document\nreview.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 00:57:32 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Chhatwal", "Rishi", ""], ["Gronvall", "Peter", ""], ["Huber-Fliflet", "Nathaniel", ""], ["Keeling", "Robert", ""], ["Zhang", "Jianping", ""], ["Zhao", "Haozhen", ""]]}, {"id": "1904.01722", "submitter": "Haozhen Zhao", "authors": "Peter Gronvall, Nathaniel Huber-Fliflet, Jianping Zhang, Robert\n  Keeling, Robert Neary, Haozhen Zhao", "title": "An Empirical Study of the Application of Machine Learning and Keyword\n  Terms Methodologies to Privilege-Document Review Projects in Legal Matters", "comments": "2018 IEEE International Conference on Big Data (Big Data)", "journal-ref": null, "doi": "10.1109/BigData.2018.8621945", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protecting privileged communications and data from disclosure is paramount\nfor legal teams. Unrestricted legal advice, such as attorney-client\ncommunications or litigation strategy. are vital to the legal process and are\nexempt from disclosure in litigations or regulatory events. To protect this\ninformation from being disclosed, companies and outside counsel must review\nvast amounts of documents to determine those that contain privileged material.\nThis process is extremely costly and time consuming. As data volumes increase,\nlegal counsel employ methods to reduce the number of documents requiring review\nwhile balancing the need to ensure the protection of privileged information.\nKeyword searching is relied upon as a method to target privileged information\nand reduce document review populations. Keyword searches are effective at\ncasting a wide net but return over inclusive results -- most of which do not\ncontain privileged information -- and without detailed knowledge of the data,\nkeyword lists cannot be crafted to find all privilege material.\nOverly-inclusive keyword searching can also be problematic, because even while\nit drives up costs, it also can cast `too far of a net' and thus produce\nunreliable results.To overcome these weaknesses of keyword searching, legal\nteams are using a new method to target privileged information called predictive\nmodeling. Predictive modeling can successfully identify privileged material but\nlittle research has been published to confirm its effectiveness when compared\nto keyword searching. This paper summarizes a study of the effectiveness of\nkeyword searching and predictive modeling when applied to real-world data. With\nthis study, this group of collaborators wanted to examine and understand the\nbenefits and weaknesses of both approaches to legal teams with identifying\nprivilege material in document populations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 00:59:37 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Gronvall", "Peter", ""], ["Huber-Fliflet", "Nathaniel", ""], ["Zhang", "Jianping", ""], ["Keeling", "Robert", ""], ["Neary", "Robert", ""], ["Zhao", "Haozhen", ""]]}, {"id": "1904.01723", "submitter": "Haozhen Zhao", "authors": "Fusheng Wei, Han Qin, Shi Ye, Haozhen Zhao", "title": "Empirical Study of Deep Learning for Text Classification in Legal\n  Document Review", "comments": "2018 IEEE International Conference on Big Data (Big Data)", "journal-ref": null, "doi": "10.1109/BigData.2018.8622157", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive coding has been widely used in legal matters to find relevant or\nprivileged documents in large sets of electronically stored information. It\nsaves the time and cost significantly. Logistic Regression (LR) and Support\nVector Machines (SVM) are two popular machine learning algorithms used in\npredictive coding. Recently, deep learning received a lot of attentions in many\nindustries. This paper reports our preliminary studies in using deep learning\nin legal document review. Specifically, we conducted experiments to compare\ndeep learning results with results obtained using a SVM algorithm on the four\ndatasets of real legal matters. Our results showed that CNN performed better\nwith larger volume of training dataset and should be a fit method in the text\nclassification in legal industry.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 01:00:41 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Wei", "Fusheng", ""], ["Qin", "Han", ""], ["Ye", "Shi", ""], ["Zhao", "Haozhen", ""]]}, {"id": "1904.01725", "submitter": "Haozhen Zhao", "authors": "Han Qin, Kit Riehle, Haozhen Zhao", "title": "Using Google Analytics to Support Cybersecurity Forensics", "comments": "2017 IEEE International Conference on Big Data (Big Data)", "journal-ref": null, "doi": "10.1109/BigData.2017.8258385", "report-no": null, "categories": "cs.IR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web traffic is a valuable data source, typically used in the marketing space\nto track brand awareness and advertising effectiveness. However, web traffic is\nalso a rich source of information for cybersecurity monitoring efforts. To\nbetter understand the threat of malicious cyber actors, this study develops a\nmethodology to monitor and evaluate web activity using data archived from\nGoogle Analytics. Google Analytics collects and aggregates web traffic,\nincluding information about web visitors' location, date and time of visit,\nvisited webpages, and searched keywords. This study seeks to streamline\nanalysis of this data and uses rule-based anomaly detection and predictive\nmodeling to identify web traffic that deviates from normal patterns. Rather\nthan evaluating pieces of web traffic individually, the methodology seeks to\nemulate real user behavior by creating a new unit of analysis: the user\nsession. User sessions group individual pieces of traffic from the same\nlocation and date, which transforms the available information from single\npoint-in-time snapshots to dynamic sessions showing users' trajectory and\nintent. The result is faster and better insight into large volumes of noisy web\ntraffic.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 01:01:52 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Qin", "Han", ""], ["Riehle", "Kit", ""], ["Zhao", "Haozhen", ""]]}, {"id": "1904.01933", "submitter": "Jinze Bai", "authors": "Jinze Bai, Chang Zhou, Junshuai Song, Xiaoru Qu, Weiting An, Zhao Li,\n  Jun Gao", "title": "Personalized Bundle List Recommendation", "comments": "WWW2019, 11 pages", "journal-ref": null, "doi": "10.1145/3308558.3313568", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product bundling, offering a combination of items to customers, is one of the\nmarketing strategies commonly used in online e-commerce and offline retailers.\nA high-quality bundle generalizes frequent items of interest, and diversity\nacross bundles boosts the user-experience and eventually increases transaction\nvolume. In this paper, we formalize the personalized bundle list recommendation\nas a structured prediction problem and propose a bundle generation network\n(BGN), which decomposes the problem into quality/diversity parts by the\ndeterminantal point processes (DPPs). BGN uses a typical encoder-decoder\nframework with a proposed feature-aware softmax to alleviate the inadequate\nrepresentation of traditional softmax, and integrates the masked beam search\nand DPP selection to produce high-quality and diversified bundle list with an\nappropriate bundle size. We conduct extensive experiments on three public\ndatasets and one industrial dataset, including two generated from co-purchase\nrecords and the other two extracted from real-world online bundle services. BGN\nsignificantly outperforms the state-of-the-art methods in terms of quality,\ndiversity and response time over all datasets. In particular, BGN improves the\nprecision of the best competitors by 16\\% on average while maintaining the\nhighest diversity on four datasets, and yields a 3.85x improvement of response\ntime over the best competitors in the bundle list recommendation problem.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 11:51:43 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Bai", "Jinze", ""], ["Zhou", "Chang", ""], ["Song", "Junshuai", ""], ["Qu", "Xiaoru", ""], ["An", "Weiting", ""], ["Li", "Zhao", ""], ["Gao", "Jun", ""]]}, {"id": "1904.02020", "submitter": "Zita Marinho", "authors": "Afonso Mendes and Shashi Narayan and Sebasti\\~ao Miranda and Zita\n  Marinho and Andr\\'e F. T. Martins and Shay B. Cohen", "title": "Jointly Extracting and Compressing Documents with Summary State\n  Representations", "comments": null, "journal-ref": "NAACL 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new neural model for text summarization that first extracts\nsentences from a document and then compresses them. The proposed model offers a\nbalance that sidesteps the difficulties in abstractive methods while generating\nmore concise summaries than extractive methods. In addition, our model\ndynamically determines the length of the output summary based on the gold\nsummaries it observes during training and does not require length constraints\ntypical to extractive summarization. The model achieves state-of-the-art\nresults on the CNN/DailyMail and Newsroom datasets, improving over current\nextractive and abstractive methods. Human evaluations demonstrate that our\nmodel generates concise and informative summaries. We also make available a new\ndataset of oracle compressive summaries derived automatically from the\nCNN/DailyMail reference summaries.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 14:24:04 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 16:09:19 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Mendes", "Afonso", ""], ["Narayan", "Shashi", ""], ["Miranda", "Sebasti\u00e3o", ""], ["Marinho", "Zita", ""], ["Martins", "Andr\u00e9 F. T.", ""], ["Cohen", "Shay B.", ""]]}, {"id": "1904.02032", "submitter": "Justin Wood", "authors": "Justin Wood, Nicholas J. Matiasz, Alcino J. Silva, William Hsu, Alexej\n  Abyzov, Wei Wang", "title": "OpBerg: Discovering causal sentences using optimal alignments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The biological literature is rich with sentences that describe causal\nrelations. Methods that automatically extract such sentences can help\nbiologists to synthesize the literature and even discover latent relations that\nhad not been articulated explicitly. Current methods for extracting causal\nsentences are based on either machine learning or a predefined database of\ncausal terms. Machine learning approaches require a large set of labeled\ntraining data and can be susceptible to noise. Methods based on predefined\ndatabases are limited by the quality of their curation and are unable to\ncapture new concepts or mistakes in the input. We address these challenges by\nadapting and improving a method designed for a seemingly unrelated problem:\nfinding alignments between genomic sequences. This paper presents a novel and\noutperforming method for extracting causal relations from text by aligning the\npart-of-speech representations of an input set with that of known causal\nsentences. Our experiments show that when applied to the task of finding causal\nsentences in biological literature, our method improves on the accuracy of\nother methods in a computationally efficient manner.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 14:36:49 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Wood", "Justin", ""], ["Matiasz", "Nicholas J.", ""], ["Silva", "Alcino J.", ""], ["Hsu", "William", ""], ["Abyzov", "Alexej", ""], ["Wang", "Wei", ""]]}, {"id": "1904.02037", "submitter": "Zita Marinho", "authors": "Sebasti\\~ao Miranda and David Nogueira and Afonso Mendes and Andreas\n  Vlachos and Andrew Secker and Rebecca Garrett and Jeff Mitchel and Zita\n  Marinho", "title": "Automated Fact Checking in the News Room", "comments": null, "journal-ref": "WEBCONF 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fact checking is an essential task in journalism; its importance has been\nhighlighted due to recently increased concerns and efforts in combating\nmisinformation. In this paper, we present an automated fact-checking platform\nwhich given a claim, it retrieves relevant textual evidence from a document\ncollection, predicts whether each piece of evidence supports or refutes the\nclaim, and returns a final verdict. We describe the architecture of the system\nand the user interface, focusing on the choices made to improve its\nuser-friendliness and transparency. We conduct a user study of the\nfact-checking platform in a journalistic setting: we integrated it with a\ncollection of news articles and provide an evaluation of the platform using\nfeedback from journalists in their workflow. We found that the predictions of\nour platform were correct 58\\% of the time, and 59\\% of the returned evidence\nwas relevant.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 14:46:44 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Miranda", "Sebasti\u00e3o", ""], ["Nogueira", "David", ""], ["Mendes", "Afonso", ""], ["Vlachos", "Andreas", ""], ["Secker", "Andrew", ""], ["Garrett", "Rebecca", ""], ["Mitchel", "Jeff", ""], ["Marinho", "Zita", ""]]}, {"id": "1904.02064", "submitter": "Byoungwook Jang", "authors": "Byoungwook Jang, Alfred Hero", "title": "Minimum Volume Topic Modeling", "comments": "Accepted in AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new topic modeling procedure that takes advantage of the fact\nthat the Latent Dirichlet Allocation (LDA) log likelihood function is\nasymptotically equivalent to the logarithm of the volume of the topic simplex.\nThis allows topic modeling to be reformulated as finding the probability\nsimplex that minimizes its volume and encloses the documents that are\nrepresented as distributions over words. A convex relaxation of the minimum\nvolume topic model optimization is proposed, and it is shown that the relaxed\nproblem has the same global minimum as the original problem under the\nseparability assumption and the sufficiently scattered assumption introduced by\nArora et al. (2013) and Huang et al. (2016). A locally convergent alternating\ndirection method of multipliers (ADMM) approach is introduced for solving the\nrelaxed minimum volume problem. Numerical experiments illustrate the benefits\nof our approach in terms of computation time and topic recovery performance.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 15:34:20 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Jang", "Byoungwook", ""], ["Hero", "Alfred", ""]]}, {"id": "1904.02077", "submitter": "Pengcheng Lin", "authors": "Peng-Cheng Lin and Wan-Lei Zhao", "title": "Graph based Nearest Neighbor Search: Promises and Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.DS cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph based nearest neighbor search gets more and more popular on\nlarge-scale retrieval tasks. The attractiveness of this type of approaches lies\nin its superior performance over most of the known nearest neighbor search\napproaches as well as its genericness to various metrics. In this paper, the\nrole of two strategies, namely hierarchical structure and graph diversification\nthat are adopted as the key steps in the graph based approaches, is\ninvestigated. We find the hierarchical structure could not achieve \"much better\nlogarithmic complexity scaling\" as it was claimed in the original paper,\nparticularly on high dimensional cases. Moreover, we find that similar high\nsearch speed efficiency as the one with hierarchical structure could be\nachieved with the support of flat k-NN graph after graph diversification.\nFinally, we point out the difficulty, that is faced by most of the graph based\nsearch approaches, is directly linked to \"curse of dimensionality\".\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 16:12:55 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 09:51:07 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 09:01:31 GMT"}, {"version": "v4", "created": "Wed, 5 Jun 2019 14:23:49 GMT"}, {"version": "v5", "created": "Tue, 18 Jun 2019 09:07:06 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Lin", "Peng-Cheng", ""], ["Zhao", "Wan-Lei", ""]]}, {"id": "1904.02100", "submitter": "Stavros Shiaeles Dr", "authors": "Muhammad Ali, Stavros Shiaeles, Maria Papadaki, Bogdan Ghita", "title": "Agent-based Vs Agent-less Sandbox for Dynamic Behavioral Analysis", "comments": "2018 Global Information Infrastructure and Networking Symposium\n  (GIIS)", "journal-ref": null, "doi": "10.1109/GIIS.2018.8635598", "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malicious software is detected and classified by either static analysis or\ndynamic analysis. In static analysis, malware samples are reverse engineered\nand analyzed so that signatures of malware can be constructed. These techniques\ncan be easily thwarted through polymorphic, metamorphic malware, obfuscation\nand packing techniques, whereas in dynamic analysis malware samples are\nexecuted in a controlled environment using the sandboxing technique, in order\nto model the behavior of malware. In this paper, we have analyzed Petya,\nSpyeye, VolatileCedar, PAFISH etc. through Agent-based and Agentless dynamic\nsandbox systems in order to investigate and benchmark their efficiency in\nadvanced malware detection.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 18:31:21 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Ali", "Muhammad", ""], ["Shiaeles", "Stavros", ""], ["Papadaki", "Maria", ""], ["Ghita", "Bogdan", ""]]}, {"id": "1904.02449", "submitter": "Xie De", "authors": "Cheng Deng, Zhaojia Chen, Xianglong Liu, Xinbo Gao, Dacheng Tao", "title": "Triplet-Based Deep Hashing Network for Cross-Modal Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the benefits of its low storage requirements and high retrieval\nefficiency, hashing has recently received increasing attention. In\nparticular,cross-modal hashing has been widely and successfully used in\nmultimedia similarity search applications. However, almost all existing methods\nemploying cross-modal hashing cannot obtain powerful hash codes due to their\nignoring the relative similarity between heterogeneous data that contains\nricher semantic information, leading to unsatisfactory retrieval performance.\nIn this paper, we propose a triplet-based deep hashing (TDH) network for\ncross-modal retrieval. First, we utilize the triplet labels, which describes\nthe relative relationships among three instances as supervision in order to\ncapture more general semantic correlations between cross-modal instances. We\nthen establish a loss function from the inter-modal view and the intra-modal\nview to boost the discriminative abilities of the hash codes. Finally, graph\nregularization is introduced into our proposed TDH method to preserve the\noriginal semantic similarity between hash codes in Hamming space. Experimental\nresults show that our proposed method outperforms several state-of-the-art\napproaches on two popular cross-modal datasets.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 10:10:26 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Deng", "Cheng", ""], ["Chen", "Zhaojia", ""], ["Liu", "Xianglong", ""], ["Gao", "Xinbo", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.02496", "submitter": "Jonathan Mamou", "authors": "Jonathan Mamou, Oren Pereg, Moshe Wasserblat, Ido Dagan", "title": "Multi-Context Term Embeddings: the Use Case of Corpus-based Term Set\n  Expansion", "comments": "6 pages, RepEval 2019 (NAACL-HLT workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel algorithm that combines multi-context term\nembeddings using a neural classifier and we test this approach on the use case\nof corpus-based term set expansion. In addition, we present a novel and unique\ndataset for intrinsic evaluation of corpus-based term set expansion algorithms.\nWe show that, over this dataset, our algorithm provides up to 5 mean average\nprecision points over the best baseline.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 11:45:52 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 08:51:49 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Mamou", "Jonathan", ""], ["Pereg", "Oren", ""], ["Wasserblat", "Moshe", ""], ["Dagan", "Ido", ""]]}, {"id": "1904.03016", "submitter": "Sina Mohseni", "authors": "Sina Mohseni and Eric Ragan and Xia Hu", "title": "Open Issues in Combating Fake News: Interpretability as an Opportunity", "comments": "arXiv admin note: text overlap with arXiv:1811.12349", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combating fake news needs a variety of defense methods. Although rumor\ndetection and various linguistic analysis techniques are common methods to\ndetect false content in social media, there are other feasible mitigation\napproaches that could be explored in the machine learning community. In this\npaper, we present open issues and opportunities in fake news research that need\nfurther attention. We first review different stages of the news life cycle in\nsocial media and discuss core vulnerability issues for news feed algorithms in\npropagating fake news content with three examples. We then discuss how\ncomplexity and unclarity of the fake news problem limit the advancements in\nthis field. Lastly, we present research opportunities from interpretable\nmachine learning to mitigate fake news problems with 1) interpretable fake news\ndetection and 2) transparent news feed algorithms. We propose three dimensions\nof interpretability consisting of algorithmic interpretability, human\ninterpretability, and the inclusion of supporting evidence that can benefit\nfake news mitigation methods in different ways.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 06:23:25 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Mohseni", "Sina", ""], ["Ragan", "Eric", ""], ["Hu", "Xia", ""]]}, {"id": "1904.03172", "submitter": "Alexander Shvets", "authors": "Alexander Shvets", "title": "Improving Scientific Article Visibility by Neural Title Simplification", "comments": "Contribution to the Proceedings of the 8th International Workshop on\n  Bibliometric-enhanced Information Retrieval (BIR 2019) as part of the 41th\n  European Conference on Information Retrieval (ECIR 2019), Cologne, Germany,\n  April 14, 2019. CEUR Workshop Proceedings, CEUR-WS.org 2019. Keywords:\n  Scientific Text Summarization, Machine Translation, Recommender Systems,\n  Personalized Simplification", "journal-ref": "Proceedings of the 8th International Workshop on\n  Bibliometric-enhanced Information Retrieval (BIR) co-located with ECIR 2019,\n  Cologne, Germany (pp. 140-147)", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapidly growing amount of data that scientific content providers should\ndeliver to a user makes them create effective recommendation tools. A title of\nan article is often the only shown element to attract people's attention. We\noffer an approach to automatic generating titles with various levels of\ninformativeness to benefit from different categories of users. Statistics from\nResearchGate used to bias train datasets and specially designed post-processing\nstep applied to neural sequence-to-sequence models allow reaching the desired\nvariety of simplified titles to gain a trade-off between the attractiveness and\ntransparency of recommendation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 17:44:12 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Shvets", "Alexander", ""]]}, {"id": "1904.03223", "submitter": "Anshuman Suri", "authors": "Parag Agrawal and Anshuman Suri", "title": "NELEC at SemEval-2019 Task 3: Think Twice Before Going Deep", "comments": "International Workshop on Semantic Evaluation (SemEval), NAACL-HLT\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Machine Learning techniques yield close to human performance on\ntext-based classification tasks. However, the presence of multi-modal noise in\nchat data such as emoticons, slang, spelling mistakes, code-mixed data, etc.\nmakes existing deep-learning solutions perform poorly. The inability of\ndeep-learning systems to robustly capture these covariates puts a cap on their\nperformance. We propose NELEC: Neural and Lexical Combiner, a system which\nelegantly combines textual and deep-learning based methods for sentiment\nclassification. We evaluate our system as part of the third task of 'Contextual\nEmotion Detection in Text' as part of SemEval-2019. Our system performs\nsignificantly better than the baseline, as well as our deep-learning model\nbenchmarks. It achieved a micro-averaged F1 score of 0.7765, ranking 3rd on the\ntest-set leader-board. Our code is available at\nhttps://github.com/iamgroot42/nelec\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 18:31:06 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Agrawal", "Parag", ""], ["Suri", "Anshuman", ""]]}, {"id": "1904.03401", "submitter": "Rui Portocarrero Sarmento MSc", "authors": "Rui Portocarrero Sarmento", "title": "Idealize - A Notion of Idea Strength", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business Entrepreneurs frequently thrive on looking for ways to test business\nideas, without giving too much information. Recent techniques in startup\ndevelopment promote the use of surveys to measure the potential client's\ninterest. In this preliminary report, we describe the concept behind Idealize,\na Shiny R application to measure the local trend strength of a potential idea.\nAdditionally, the system might provide a relative distance to the capital city\nof the country. The tests were made for the United States of America, i.e.,\nmade available regarding native English language. This report shows some of the\ntests results with this system.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 09:44:30 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Sarmento", "Rui Portocarrero", ""]]}, {"id": "1904.03513", "submitter": "Ramy Baly", "authors": "Abdelrhman Saleh (1), Ramy Baly (2), Alberto Barr\\'on-Cede\\~no (3),\n  Giovanni Da San Martino (3), Mitra Mohtarami (2), Preslav Nakov (3) and James\n  Glass (2) ((1) Harvard University, MA, USA, (2) MIT Computer Science and\n  Artificial Intelligence Laboratory, MA, USA, (3) Qatar Computing Research\n  Institute, HBKU, Qatar)", "title": "Team QCRI-MIT at SemEval-2019 Task 4: Propaganda Analysis Meets\n  Hyperpartisan News Detection", "comments": "Hyperpartisanship, propaganda, news media, fake news, SemEval-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our submission to SemEval-2019 Task 4 on\nHyperpartisan News Detection. Our system relies on a variety of engineered\nfeatures originally used to detect propaganda. This is based on the assumption\nthat biased messages are propagandistic in the sense that they promote a\nparticular political cause or viewpoint. We trained a logistic regression model\nwith features ranging from simple bag-of-words to vocabulary richness and text\nreadability features. Our system achieved 72.9% accuracy on the test data that\nis annotated manually and 60.8% on the test data that is annotated with distant\nsupervision. Additional experiments showed that significant performance\nimprovements can be achieved with better feature pre-processing.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 19:04:29 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Saleh", "Abdelrhman", ""], ["Baly", "Ramy", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["Martino", "Giovanni Da San", ""], ["Mohtarami", "Mitra", ""], ["Nakov", "Preslav", ""], ["Glass", "James", ""]]}, {"id": "1904.03889", "submitter": "Ramtin Yazdanian", "authors": "Ramtin Yazdanian, Leila Zia, Jonathan Morgan, Bahodir Mansurov, Robert\n  West", "title": "Eliciting New Wikipedia Users' Interests via Automatically Mined\n  Questionnaires: For a Warm Welcome, Not a Cold Start", "comments": "Accepted at the 13th International AAAI Conference on Web and Social\n  Media (ICWSM-2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Every day, thousands of users sign up as new Wikipedia contributors. Once\njoined, these users have to decide which articles to contribute to, which users\nto seek out and learn from or collaborate with, etc. Any such task is a hard\nand potentially frustrating one given the sheer size of Wikipedia. Supporting\nnewcomers in their first steps by recommending articles they would enjoy\nediting or editors they would enjoy collaborating with is thus a promising\nroute toward converting them into long-term contributors. Standard recommender\nsystems, however, rely on users' histories of previous interactions with the\nplatform. As such, these systems cannot make high-quality recommendations to\nnewcomers without any previous interactions -- the so-called cold-start\nproblem. The present paper addresses the cold-start problem on Wikipedia by\ndeveloping a method for automatically building short questionnaires that, when\ncompleted by a newly registered Wikipedia user, can be used for a variety of\npurposes, including article recommendations that can help new editors get\nstarted. Our questionnaires are constructed based on the text of Wikipedia\narticles as well as the history of contributions by the already onboarded\nWikipedia editors. We assess the quality of our questionnaire-based\nrecommendations in an offline evaluation using historical data, as well as an\nonline evaluation with hundreds of real Wikipedia newcomers, concluding that\nour method provides cohesive, human-readable questions that perform well\nagainst several baselines. By addressing the cold-start problem, this work can\nhelp with the sustainable growth and maintenance of Wikipedia's diverse editor\ncommunity.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 08:40:55 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Yazdanian", "Ramtin", ""], ["Zia", "Leila", ""], ["Morgan", "Jonathan", ""], ["Mansurov", "Bahodir", ""], ["West", "Robert", ""]]}, {"id": "1904.03990", "submitter": "Bart Theeten", "authors": "Bart Theeten, Frederik Vandeputte, Tom Van Cutsem", "title": "Import2vec - Learning Embeddings for Software Libraries", "comments": "MSR19 Conference 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of developing suitable learning representations\n(embeddings) for library packages that capture semantic similarity among\nlibraries. Such representations are known to improve the performance of\ndownstream learning tasks (e.g. classification) or applications such as\ncontextual search and analogical reasoning.\n  We apply word embedding techniques from natural language processing (NLP) to\ntrain embeddings for library packages (\"library vectors\"). Library vectors\nrepresent libraries by similar context of use as determined by import\nstatements present in source code. Experimental results obtained from training\nsuch embeddings on three large open source software corpora reveals that\nlibrary vectors capture semantically meaningful relationships among software\nlibraries, such as the relationship between frameworks and their plug-ins and\nlibraries commonly used together within ecosystems such as big data\ninfrastructure projects (in Java), front-end and back-end web development\nframeworks (in JavaScript) and data science toolkits (in Python).\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 14:36:19 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Theeten", "Bart", ""], ["Vandeputte", "Frederik", ""], ["Van Cutsem", "Tom", ""]]}, {"id": "1904.04096", "submitter": "Fatma Nasoz", "authors": "Nishit Shrestha and Fatma Nasoz", "title": "Deep Learning Sentiment Analysis of Amazon.com Reviews and Ratings", "comments": "15 pages, 10 figures, 3 tables, journal article", "journal-ref": "International Journal on Soft Computing, Artificial Intelligence\n  and Applications (IJSCAI), Vol.8, No.1, February 2019", "doi": "10.5121/ijscai.2019.8101", "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our study employs sentiment analysis to evaluate the compatibility of\nAmazon.com reviews with their corresponding ratings. Sentiment analysis is the\ntask of identifying and classifying the sentiment expressed in a piece of text\nas being positive or negative. On e-commerce websites such as Amazon.com,\nconsumers can submit their reviews along with a specific polarity rating. In\nsome instances, there is a mismatch between the review and the rating. To\nidentify the reviews with mismatched ratings we performed sentiment analysis\nusing deep learning on Amazon.com product review data. Product reviews were\nconverted to vectors using paragraph vector, which then was used to train a\nrecurrent neural network with gated recurrent unit. Our model incorporated both\nsemantic relationship of review text and product information. We also developed\na web service application that predicts the rating score for a submitted review\nusing the trained model and if there is a mismatch between predicted rating\nscore and submitted rating score, it provides feedback to the reviewer.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 21:34:45 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Shrestha", "Nishit", ""], ["Nasoz", "Fatma", ""]]}, {"id": "1904.04206", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Elham Azimi, AmirAli Abdolrashidi", "title": "Deep-Sentiment: Sentiment Analysis Using Ensemble of CNN and Bi-LSTM\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularity of social networks, and e-commerce websites, sentiment\nanalysis has become a more active area of research in the past few years. On a\nhigh level, sentiment analysis tries to understand the public opinion about a\nspecific product or topic, or trends from reviews or tweets. Sentiment analysis\nplays an important role in better understanding customer/user opinion, and also\nextracting social/political trends. There has been a lot of previous works for\nsentiment analysis, some based on hand-engineering relevant textual features,\nand others based on different neural network architectures. In this work, we\npresent a model based on an ensemble of long-short-term-memory (LSTM), and\nconvolutional neural network (CNN), one to capture the temporal information of\nthe data, and the other one to extract the local structure thereof. Through\nexperimental results, we show that using this ensemble model we can outperform\nboth individual models. We are also able to achieve a very high accuracy rate\ncompared to the previous works.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:26:20 GMT"}], "update_date": "2019-04-14", "authors_parsed": [["Minaee", "Shervin", ""], ["Azimi", "Elham", ""], ["Abdolrashidi", "AmirAli", ""]]}, {"id": "1904.04272", "submitter": "Martin Engilberge", "authors": "Martin Engilberge, Louis Chevallier, Patrick P\\'erez, Matthieu Cord", "title": "SoDeep: a Sorting Deep net to learn ranking loss surrogates", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several tasks in machine learning are evaluated using non-differentiable\nmetrics such as mean average precision or Spearman correlation. However, their\nnon-differentiability prevents from using them as objective functions in a\nlearning framework. Surrogate and relaxation methods exist but tend to be\nspecific to a given metric.\n  In the present work, we introduce a new method to learn approximations of\nsuch non-differentiable objective functions. Our approach is based on a deep\narchitecture that approximates the sorting of arbitrary sets of scores. It is\ntrained virtually for free using synthetic data. This sorting deep (SoDeep) net\ncan then be combined in a plug-and-play manner with existing deep\narchitectures. We demonstrate the interest of our approach in three different\ntasks that require ranking: Cross-modal text-image retrieval, multi-label image\nclassification and visual memorability ranking. Our approach yields very\ncompetitive results on these three tasks, which validates the merit and the\nflexibility of SoDeep as a proxy for sorting operation in ranking-based losses.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:02:43 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Engilberge", "Martin", ""], ["Chevallier", "Louis", ""], ["P\u00e9rez", "Patrick", ""], ["Cord", "Matthieu", ""]]}, {"id": "1904.04381", "submitter": "Jiaxuan You", "authors": "Jiaxuan You, Yichen Wang, Aditya Pal, Pong Eksombatchai, Chuck\n  Rosenberg, Jure Leskovec", "title": "Hierarchical Temporal Convolutional Networks for Dynamic Recommender\n  Systems", "comments": "Accepted by the Web Conference 2019 (WWW 2019) as a full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems that can learn from cross-session data to dynamically\npredict the next item a user will choose are crucial for online platforms.\nHowever, existing approaches often use out-of-the-box sequence models which are\nlimited by speed and memory consumption, are often infeasible for production\nenvironments, and usually do not incorporate cross-session information, which\nis crucial for effective recommendations. Here we propose Hierarchical Temporal\nConvolutional Networks (HierTCN), a hierarchical deep learning architecture\nthat makes dynamic recommendations based on users' sequential multi-session\ninteractions with items. HierTCN is designed for web-scale systems with\nbillions of items and hundreds of millions of users. It consists of two levels\nof models: The high-level model uses Recurrent Neural Networks (RNN) to\naggregate users' evolving long-term interests across different sessions, while\nthe low-level model is implemented with Temporal Convolutional Networks (TCN),\nutilizing both the long-term interests and the short-term interactions within\nsessions to predict the next interaction. We conduct extensive experiments on a\npublic XING dataset and a large-scale Pinterest dataset that contains 6 million\nusers with 1.6 billion interactions. We show that HierTCN is 2.5x faster than\nRNN-based models and uses 90% less data memory compared to TCN-based models. We\nfurther develop an effective data caching scheme and a queue-based mini-batch\ngenerator, enabling our model to be trained within 24 hours on a single GPU.\nOur model consistently outperforms state-of-the-art dynamic recommendation\nmethods, with up to 18% improvement in recall and 10% in mean reciprocal rank.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 22:15:44 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 17:15:17 GMT"}], "update_date": "2019-04-14", "authors_parsed": [["You", "Jiaxuan", ""], ["Wang", "Yichen", ""], ["Pal", "Aditya", ""], ["Eksombatchai", "Pong", ""], ["Rosenberg", "Chuck", ""], ["Leskovec", "Jure", ""]]}, {"id": "1904.04447", "submitter": "Bin Liu", "authors": "Bin Liu, Ruiming Tang, Yingzhi Chen, Jinkai Yu, Huifeng Guo, Yuzhou\n  Zhang", "title": "Feature Generation by Convolutional Neural Network for Click-Through\n  Rate Prediction", "comments": null, "journal-ref": "TheWebConf 2019", "doi": "10.1145/3308558.3313497", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-Through Rate prediction is an important task in recommender systems,\nwhich aims to estimate the probability of a user to click on a given item.\nRecently, many deep models have been proposed to learn low-order and high-order\nfeature interactions from original features. However, since useful interactions\nare always sparse, it is difficult for DNN to learn them effectively under a\nlarge number of parameters. In real scenarios, artificial features are able to\nimprove the performance of deep models (such as Wide & Deep Learning), but\nfeature engineering is expensive and requires domain knowledge, making it\nimpractical in different scenarios. Therefore, it is necessary to augment\nfeature space automatically. In this paper, We propose a novel Feature\nGeneration by Convolutional Neural Network (FGCNN) model with two components:\nFeature Generation and Deep Classifier. Feature Generation leverages the\nstrength of CNN to generate local patterns and recombine them to generate new\nfeatures. Deep Classifier adopts the structure of IPNN to learn interactions\nfrom the augmented feature space. Experimental results on three large-scale\ndatasets show that FGCNN significantly outperforms nine state-of-the-art\nmodels. Moreover, when applying some state-of-the-art models as Deep\nClassifier, better performance is always achieved, showing the great\ncompatibility of our FGCNN model. This work explores a novel direction for CTR\npredictions: it is quite useful to reduce the learning difficulties of DNN by\nautomatically identifying important features.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 03:27:06 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Liu", "Bin", ""], ["Tang", "Ruiming", ""], ["Chen", "Yingzhi", ""], ["Yu", "Jinkai", ""], ["Guo", "Huifeng", ""], ["Zhang", "Yuzhou", ""]]}, {"id": "1904.04547", "submitter": "Anirban Santara", "authors": "Anirban Santara, Jayeeta Datta, Sourav Sarkar, Ankur Garg, Kirti\n  Padia, Pabitra Mitra", "title": "PUNCH: Positive UNlabelled Classification based information retrieval in\n  Hyperspectral images", "comments": "9 pages, under review at ACMMM-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images of land-cover captured by airborne or satellite-mounted\nsensors provide a rich source of information about the chemical composition of\nthe materials present in a given place. This makes hyperspectral imaging an\nimportant tool for earth sciences, land-cover studies, and military and\nstrategic applications. However, the scarcity of labeled training examples and\nspatial variability of spectral signature are two of the biggest challenges\nfaced by hyperspectral image classification. In order to address these issues,\nwe aim to develop a framework for material-agnostic information retrieval in\nhyperspectral images based on Positive-Unlabelled (PU) classification. Given a\nhyperspectral scene, the user labels some positive samples of a material he/she\nis looking for and our goal is to retrieve all the remaining instances of the\nquery material in the scene. Additionally, we require the system to work\nequally well for any material in any scene without the user having to disclose\nthe identity of the query material. This material-agnostic nature of the\nframework provides it with superior generalization abilities. We explore two\nalternative approaches to solve the hyperspectral image classification problem\nwithin this framework. The first approach is an adaptation of non-negative risk\nestimation based PU learning for hyperspectral data. The second approach is\nbased on one-versus-all positive-negative classification where the negative\nclass is approximately sampled using a novel spectral-spatial retrieval model.\nWe propose two annotator models - uniform and blob - that represent the\nlabelling patterns of a human annotator. We compare the performances of the\nproposed algorithms for each annotator model on three benchmark hyperspectral\nimage datasets - Indian Pines, Pavia University and Salinas.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 09:00:28 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Santara", "Anirban", ""], ["Datta", "Jayeeta", ""], ["Sarkar", "Sourav", ""], ["Garg", "Ankur", ""], ["Padia", "Kirti", ""], ["Mitra", "Pabitra", ""]]}, {"id": "1904.04794", "submitter": "Ushasi Chaudhuri", "authors": "Ushasi Chaudhuri, Biplab Banerjee, Avik Bhattacharya, and Mihai Datcu", "title": "CMIR-NET : A Deep Learning Based Model For Cross-Modal Retrieval In\n  Remote Sensing", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2020.02.006", "report-no": null, "categories": "eess.IV cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of cross-modal information retrieval in the domain of\nremote sensing. In particular, we are interested in two application scenarios:\ni) cross-modal retrieval between panchromatic (PAN) and multi-spectral imagery,\nand ii) multi-label image retrieval between very high resolution (VHR) images\nand speech based label annotations. Notice that these multi-modal retrieval\nscenarios are more challenging than the traditional uni-modal retrieval\napproaches given the inherent differences in distributions between the\nmodalities. However, with the growing availability of multi-source remote\nsensing data and the scarcity of enough semantic annotations, the task of\nmulti-modal retrieval has recently become extremely important. In this regard,\nwe propose a novel deep neural network based architecture which is considered\nto learn a discriminative shared feature space for all the input modalities,\nsuitable for semantically coherent information retrieval. Extensive experiments\nare carried out on the benchmark large-scale PAN - multi-spectral DSRSID\ndataset and the multi-label UC-Merced dataset. Together with the Merced\ndataset, we generate a corpus of speech signals corresponding to the labels.\nSuperior performance with respect to the current state-of-the-art is observed\nin all the cases.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 17:16:54 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 06:19:19 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Chaudhuri", "Ushasi", ""], ["Banerjee", "Biplab", ""], ["Bhattacharya", "Avik", ""], ["Datcu", "Mihai", ""]]}, {"id": "1904.04995", "submitter": "Xuewei Tang", "authors": "Shanshan Huang, Xiaojun Wan, Xuewei Tang", "title": "AMRec: An Intelligent System for Academic Method Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding new academic Methods for research problems is the key task in a\nresearcher's research career. It is usually very difficult for new researchers\nto find good Methods for their research problems since they lack of research\nexperiences. In order to help researchers carry out their researches in a more\nconvenient way, we describe a novel recommendation system called AMRec to\nrecommend new academic Methods for research problems in this paper. Our\nproposed system first extracts academic concepts (Tasks and Methods) and their\nrelations from academic literatures, and then leverages the regularized matrix\nfactorization Method for academic Method recommendation. Preliminary evaluation\nresults verify the effectiveness of our proposed system.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 03:49:37 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Huang", "Shanshan", ""], ["Wan", "Xiaojun", ""], ["Tang", "Xuewei", ""]]}, {"id": "1904.05033", "submitter": "Prakhar Gupta", "authors": "Prakhar Gupta, Matteo Pagliardini and Martin Jaggi", "title": "Better Word Embeddings by Disentangling Contextual n-Gram Information", "comments": "NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained word vectors are ubiquitous in Natural Language Processing\napplications. In this paper, we show how training word embeddings jointly with\nbigram and even trigram embeddings, results in improved unigram embeddings. We\nclaim that training word embeddings along with higher n-gram embeddings helps\nin the removal of the contextual information from the unigrams, resulting in\nbetter stand-alone word embeddings. We empirically show the validity of our\nhypothesis by outperforming other competing word representation models by a\nsignificant margin on a wide variety of tasks. We make our models publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 07:44:06 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Gupta", "Prakhar", ""], ["Pagliardini", "Matteo", ""], ["Jaggi", "Martin", ""]]}, {"id": "1904.05165", "submitter": "Stephen Bonner", "authors": "Stephen Bonner, Flavian Vasile", "title": "Causal Embeddings for Recommendation: An Extended Abstract", "comments": "Accepted to the International Joint Conferences on Artificial\n  Intelligence (IJCAI) Sister Conference Best Paper Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendations are commonly used to modify user's natural behavior, for\nexample, increasing product sales or the time spent on a website. This results\nin a gap between the ultimate business objective and the classical setup where\nrecommendations are optimized to be coherent with past user behavior. To bridge\nthis gap, we propose a new learning setup for recommendation that optimizes for\nthe Incremental Treatment Effect (ITE) of the policy. We show this is\nequivalent to learning to predict recommendation outcomes under a fully random\nrecommendation policy and propose a new domain adaptation algorithm that learns\nfrom logged data containing outcomes from a biased recommendation policy and\npredicts recommendation outcomes according to random exposure. We compare our\nmethod against state-of-the-art factorization methods, in addition to new\napproaches of causal recommendation and show significant improvements.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 13:13:57 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 18:40:55 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Bonner", "Stephen", ""], ["Vasile", "Flavian", ""]]}, {"id": "1904.05247", "submitter": "Linus W. Dietz", "authors": "Rinita Roy and Linus W. Dietz", "title": "A Model for Using Physiological Conditions for Proactive Tourist\n  Recommendations", "comments": null, "journal-ref": "ABIS '2019 Proceedings of the 23rd International Workshop on\n  Personalization and Recommendation on the Web and Beyond", "doi": "10.1145/3345002.3349289", "report-no": null, "categories": "cs.HC cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mobile proactive tourist recommender systems can support tourists by\nrecommending the best choice depending on different contexts related to herself\nand the environment. In this paper, we propose to utilize wearable sensors to\ngather health information about a tourist and use them for recommending tourist\nactivities. We discuss a range of wearable devices, sensors to infer\nphysiological conditions of the users, and exemplify the feasibility using a\npopular self-quantification mobile app. Our main contribution then comprises a\ndata model to derive relations between the parameters measured by the wearable\nsensors, such as heart rate, body temperature, blood pressure, and use them to\ninfer the physiological condition of a user. This model can then be used to\nderive classes of tourist activities that determine which items should be\nrecommended.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:37:28 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Roy", "Rinita", ""], ["Dietz", "Linus W.", ""]]}, {"id": "1904.05308", "submitter": "Davy Weissenbacher", "authors": "Davy Weissenbacher, Abeed Sarker, Ari Klein, Karen O'Connor, Arjun\n  Magge Ranganatha, Graciela Gonzalez-Hernandez", "title": "Deep Neural Networks Ensemble for Detecting Medication Mentions in\n  Tweets", "comments": "This is a pre-copy-editing, author-produced PDF of an article\n  accepted for publication in JAMIA following peer review. The definitive\n  publisher-authenticated version is \"D. Weissenbacher, A. Sarker, A. Klein, K.\n  O'Connor, A. Magge, G. Gonzalez-Hernandez, Deep neural networks ensemble for\n  detecting medication mentions in tweets, Journal of the American Medical\n  Informatics Association, ocz156, 2019\"", "journal-ref": "Journal of the American Medical Informatics Association, ocz156,\n  2019", "doi": "10.1093/jamia/ocz156", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: After years of research, Twitter posts are now recognized as an\nimportant source of patient-generated data, providing unique insights into\npopulation health. A fundamental step to incorporating Twitter data in\npharmacoepidemiological research is to automatically recognize medication\nmentions in tweets. Given that lexical searches for medication names may fail\ndue to misspellings or ambiguity with common words, we propose a more advanced\nmethod to recognize them. Methods: We present Kusuri, an Ensemble Learning\nclassifier, able to identify tweets mentioning drug products and dietary\nsupplements. Kusuri (\"medication\" in Japanese) is composed of two modules.\nFirst, four different classifiers (lexicon-based, spelling-variant-based,\npattern-based and one based on a weakly-trained neural network) are applied in\nparallel to discover tweets potentially containing medication names. Second, an\nensemble of deep neural networks encoding morphological, semantical and\nlong-range dependencies of important words in the tweets discovered is used to\nmake the final decision. Results: On a balanced (50-50) corpus of 15,005\ntweets, Kusuri demonstrated performances close to human annotators with 93.7%\nF1-score, the best score achieved thus far on this corpus. On a corpus made of\nall tweets posted by 113 Twitter users (98,959 tweets, with only 0.26%\nmentioning medications), Kusuri obtained 76.3% F1-score. There is not a prior\ndrug extraction system that compares running on such an extremely unbalanced\ndataset. Conclusion: The system identifies tweets mentioning drug names with\nperformance high enough to ensure its usefulness and ready to be integrated in\nlarger natural language processing systems.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:18:17 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 16:34:33 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Weissenbacher", "Davy", ""], ["Sarker", "Abeed", ""], ["Klein", "Ari", ""], ["O'Connor", "Karen", ""], ["Ranganatha", "Arjun Magge", ""], ["Gonzalez-Hernandez", "Graciela", ""]]}, {"id": "1904.05331", "submitter": "Nitish Nag", "authors": "Nitish Nag, Aditya Bharadwaj, Aditya Narendra Rao, Akash Kulhalli,\n  Kushal Samir Mehta, Nishant Bhattacharya, Pratul Ramkumar, Dinkar Sitaram,\n  Ramesh Jain", "title": "Flavour Enhanced Food Recommendation", "comments": "In Proceedings of 5th International Workshop on Multimedia Assisted\n  Dietary Management, Nice, France, October 21, 2019, MADiMa 2019, 6 pages", "journal-ref": null, "doi": "10.1145/3347448.3357169", "report-no": null, "categories": "cs.SI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mechanism to use the features of flavour to enhance the quality\nof food recommendations. An empirical method to determine the flavour of food\nis incorporated into a recommendation engine based on major gustatory nerves.\nSuch a system has advantages of suggesting food items that the user is more\nlikely to enjoy based upon matching with their flavour profile through use of\nthe taste biological domain knowledge. This preliminary intends to spark more\nrobust mechanisms by which flavour of food is taken into consideration as a\nmajor feature set into food recommendation systems. Our long term vision is to\nintegrate this with health factors to recommend healthy and tasty food to users\nto enhance quality of life.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 02:44:26 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 20:36:12 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Nag", "Nitish", ""], ["Bharadwaj", "Aditya", ""], ["Rao", "Aditya Narendra", ""], ["Kulhalli", "Akash", ""], ["Mehta", "Kushal Samir", ""], ["Bhattacharya", "Nishant", ""], ["Ramkumar", "Pratul", ""], ["Sitaram", "Dinkar", ""], ["Jain", "Ramesh", ""]]}, {"id": "1904.05374", "submitter": "Daniela Vianna", "authors": "Daniela Vianna, Varvara Kalokyri, Alexander Borgida, Thu D. Nguyen,\n  Amelie Marian", "title": "Searching Heterogeneous Personal Digital Traces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital traces of our lives are now constantly produced by various connected\ndevices, internet services and interactions. Our actions result in a multitude\nof heterogeneous data objects, or traces, kept in various locations in the\ncloud or on local devices. Users have very few tools to organize, understand,\nand search the digital traces they produce. We propose a simple but flexible\ndata model to aggregate, organize, and find personal information within a\ncollection of a user's personal digital traces. Our model uses as basic\ndimensions the six questions: what, when, where, who, why, and how. These\nnatural questions model universal aspects of a personal data collection and\nserve as unifying features of each personal data object, regardless of its\nsource. We propose indexing and search techniques to aid users in searching for\ntheir past information in their unified personal digital data sets using our\nmodel. Experiments performed over real user data from a variety of data sources\nsuch as Facebook, Dropbox, and Gmail show that our approach significantly\nimproves search accuracy when compared with traditional search tools.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 18:04:55 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Vianna", "Daniela", ""], ["Kalokyri", "Varvara", ""], ["Borgida", "Alexander", ""], ["Nguyen", "Thu D.", ""], ["Marian", "Amelie", ""]]}, {"id": "1904.05439", "submitter": "Marco Rovera", "authors": "Marco Rovera, Federico Nanni, Simone Paolo Ponzetto", "title": "Event-based Access to Historical Italian War Memoirs", "comments": "23 pages, 6 figures", "journal-ref": "J. Comput. Cult. Herit. 14, 1, Article 2 (February 2021)", "doi": "10.1145/3406210", "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The progressive digitization of historical archives provides new, often\ndomain specific, textual resources that report on facts and events which have\nhappened in the past; among these, memoirs are a very common type of primary\nsource. In this paper, we present an approach for extracting information from\nItalian historical war memoirs and turning it into structured knowledge. This\nis based on the semantic notions of events, participants and roles. We evaluate\nquantitatively each of the key-steps of our approach and provide a graph-based\nrepresentation of the extracted knowledge, which allows to move between a Close\nand a Distant Reading of the collection.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:30:36 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 13:19:25 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 14:42:18 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Rovera", "Marco", ""], ["Nanni", "Federico", ""], ["Ponzetto", "Simone Paolo", ""]]}, {"id": "1904.05440", "submitter": "Ashutosh Modi", "authors": "Yeyao Zhang, Eleftheria Tsipidi, Sasha Schriber, Mubbasir Kapadia,\n  Markus Gross, Ashutosh Modi", "title": "Generating Animations from Screenplays", "comments": "9+1+6 Pages, Accepted at StarSEM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.GR cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically generating animation from natural language text finds\napplication in a number of areas e.g. movie script writing, instructional\nvideos, and public safety. However, translating natural language text into\nanimation is a challenging task. Existing text-to-animation systems can handle\nonly very simple sentences, which limits their applications. In this paper, we\ndevelop a text-to-animation system which is capable of handling complex\nsentences. We achieve this by introducing a text simplification step into the\nprocess. Building on an existing animation generation system for screenwriting,\nwe create a robust NLP pipeline to extract information from screenplays and map\nthem to the system's knowledge base. We develop a set of linguistic\ntransformation rules that simplify complex sentences. Information extracted\nfrom the simplified sentences is used to generate a rough storyboard and video\ndepicting the text. Our sentence simplification module outperforms existing\nsystems in terms of BLEU and SARI metrics.We further evaluated our system via a\nuser study: 68 % participants believe that our system generates reasonable\nanimation from input screenplays.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 21:04:54 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Zhang", "Yeyao", ""], ["Tsipidi", "Eleftheria", ""], ["Schriber", "Sasha", ""], ["Kapadia", "Mubbasir", ""], ["Gross", "Markus", ""], ["Modi", "Ashutosh", ""]]}, {"id": "1904.05544", "submitter": "Zhuo Lei", "authors": "Zhuo Lei, Chao Zhang, Qian Zhang and Guoping Qiu", "title": "FrameRank: A Text Processing Approach to Video Summarization", "comments": "accepted by ICME 2019 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization has been extensively studied in the past decades.\nHowever, user-generated video summarization is much less explored since there\nlack large-scale video datasets within which human-generated video summaries\nare unambiguously defined and annotated. Toward this end, we propose a\nuser-generated video summarization dataset - UGSum52 - that consists of 52\nvideos (207 minutes). In constructing the dataset, because of the subjectivity\nof user-generated video summarization, we manually annotate 25 summaries for\neach video, which are in total 1300 summaries. To the best of our knowledge, it\nis currently the largest dataset for user-generated video summarization.\n  Based on this dataset, we present FrameRank, an unsupervised video\nsummarization method that employs a frame-to-frame level affinity graph to\nidentify coherent and informative frames to summarize a video. We use the\nKullback-Leibler(KL)-divergence-based graph to rank temporal segments according\nto the amount of semantic information contained in their frames. We illustrate\nthe effectiveness of our method by applying it to three datasets SumMe, TVSum\nand UGSum52 and show it achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 06:16:17 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 10:45:19 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Lei", "Zhuo", ""], ["Zhang", "Chao", ""], ["Zhang", "Qian", ""], ["Qiu", "Guoping", ""]]}, {"id": "1904.05737", "submitter": "Andrew Yates", "authors": "Siddhant Arora, Andrew Yates", "title": "Investigating Retrieval Method Selection with Axiomatic Features", "comments": "Algorithm Selection and Meta-Learning in Information Retrieval\n  (AMIR'19) workshop at ECIR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider algorithm selection in the context of ad-hoc information\nretrieval. Given a query and a pair of retrieval methods, we propose a\nmeta-learner that predicts how to combine the methods' relevance scores into an\noverall relevance score. Inspired by neural models' different properties with\nregard to IR axioms, these predictions are based on features that quantify\naxiom-related properties of the query and its top ranked documents. We conduct\nan evaluation on TREC Web Track data and find that the meta-learner often\nsignificantly improves over the individual methods. Finally, we conduct feature\nand query weight analyses to investigate the meta-learner's behavior.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 14:50:58 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Arora", "Siddhant", ""], ["Yates", "Andrew", ""]]}, {"id": "1904.05880", "submitter": "Idan Schwartz", "authors": "Idan Schwartz and Seunghak Yu and Tamir Hazan and Alexander Schwing", "title": "Factor Graph Attention", "comments": "Accepted to CVPR 2019; revised version includes bottom-up features", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialog is an effective way to exchange information, but subtle details and\nnuances are extremely important. While significant progress has paved a path to\naddress visual dialog with algorithms, details and nuances remain a challenge.\nAttention mechanisms have demonstrated compelling results to extract details in\nvisual question answering and also provide a convincing framework for visual\ndialog due to their interpretability and effectiveness. However, the many data\nutilities that accompany visual dialog challenge existing attention techniques.\nWe address this issue and develop a general attention mechanism for visual\ndialog which operates on any number of data utilities. To this end, we design a\nfactor graph based attention mechanism which combines any number of utility\nrepresentations. We illustrate the applicability of the proposed approach on\nthe challenging and recently introduced VisDial datasets, outperforming recent\nstate-of-the-art methods by 1.1% for VisDial0.9 and by 2% for VisDial1.0 on\nMRR. Our ensemble model improved the MRR score on VisDial1.0 by more than 6%.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 17:59:58 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 20:05:12 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2020 23:35:13 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Schwartz", "Idan", ""], ["Yu", "Seunghak", ""], ["Hazan", "Tamir", ""], ["Schwing", "Alexander", ""]]}, {"id": "1904.05985", "submitter": "Chu Wang", "authors": "Chu Wang, Lei Tang, Shujun Bian, Da Zhang, Zuohua Zhang, Yongning Wu", "title": "Reference Product Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a product of interest, we propose a search method to surface a set of\nreference products. The reference products can be used as candidates to support\ndownstream modeling tasks and business applications. The search method consists\nof product representation learning and fingerprint-type vector searching. The\nproduct catalog information is transformed into a high-quality embedding of low\ndimensions via a novel attention auto-encoder neural network, and the embedding\nis further coupled with a binary encoding vector for fast retrieval. We conduct\nextensive experiments to evaluate the proposed method, and compare it with peer\nservices to demonstrate its advantage in terms of search return rate and\nprecision.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 23:47:01 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Wang", "Chu", ""], ["Tang", "Lei", ""], ["Bian", "Shujun", ""], ["Zhang", "Da", ""], ["Zhang", "Zuohua", ""], ["Wu", "Yongning", ""]]}, {"id": "1904.06483", "submitter": "Daniel Pfeifer", "authors": "Daniel Pfeifer and Jochen L. Leidner", "title": "Topic Grouper: An Agglomerative Clustering Approach to Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Topic Grouper as a complementary approach in the field of\nprobabilistic topic modeling. Topic Grouper creates a disjunctive partitioning\nof the training vocabulary in a stepwise manner such that resulting partitions\nrepresent topics. It is governed by a simple generative model, where the\nlikelihood to generate the training documents via topics is optimized. The\nalgorithm starts with one-word topics and joins two topics at every step. It\ntherefore generates a solution for every desired number of topics ranging\nbetween the size of the training vocabulary and one. The process represents an\nagglomerative clustering that corresponds to a binary tree of topics. A\nresulting tree may act as a containment hierarchy, typically with more general\ntopics towards the root of tree and more specific topics towards the leaves.\nTopic Grouper is not governed by a background distribution such as the\nDirichlet and avoids hyper parameter optimizations.\n  We show that Topic Grouper has reasonable predictive power and also a\nreasonable theoretical and practical complexity. Topic Grouper can deal well\nwith stop words and function words and tends to push them into their own\ntopics. Also, it can handle topic distributions, where some topics are more\nfrequent than others. We present typical examples of computed topics from\nevaluation datasets, where topics appear conclusive and coherent. In this\ncontext, the fact that each word belongs to exactly one topic is not a major\nlimitation; in some scenarios this can even be a genuine advantage, e.g.~a\nrelated shopping basket analysis may aid in optimizing groupings of articles in\nsales catalogs.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 05:06:18 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Pfeifer", "Daniel", ""], ["Leidner", "Jochen L.", ""]]}, {"id": "1904.06611", "submitter": "John Collomosse", "authors": "John Collomosse, Tu Bui, Hailin Jin", "title": "LiveSketch: Query Perturbations for Guided Sketch-based Visual Search", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LiveSketch is a novel algorithm for searching large image collections using\nhand-sketched queries. LiveSketch tackles the inherent ambiguity of sketch\nsearch by creating visual suggestions that augment the query as it is drawn,\nmaking query specification an iterative rather than one-shot process that helps\ndisambiguate users' search intent. Our technical contributions are: a triplet\nconvnet architecture that incorporates an RNN based variational autoencoder to\nsearch for images using vector (stroke-based) queries; real-time clustering to\nidentify likely search intents (and so, targets within the search embedding);\nand the use of backpropagation from those targets to perturb the input stroke\nsequence, so suggesting alterations to the query in order to guide the search.\nWe show improvements in accuracy and time-to-task over contemporary baselines\nusing a 67M image corpus.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 00:33:15 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Collomosse", "John", ""], ["Bui", "Tu", ""], ["Jin", "Hailin", ""]]}, {"id": "1904.06652", "submitter": "Jimmy Lin", "authors": "Wei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin", "title": "Data Augmentation for BERT Fine-Tuning in Open-Domain Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a simple combination of passage retrieval using off-the-shelf IR\ntechniques and a BERT reader was found to be very effective for question\nanswering directly on Wikipedia, yielding a large improvement over the previous\nstate of the art on a standard benchmark dataset. In this paper, we present a\ndata augmentation technique using distant supervision that exploits positive as\nwell as negative examples. We apply a stage-wise approach to fine tuning BERT\non multiple datasets, starting with data that is \"furthest\" from the test data\nand ending with the \"closest\". Experimental results show large gains in\neffectiveness over previous approaches on English QA datasets, and we establish\nnew baselines on two recent Chinese QA datasets.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 08:17:06 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Yang", "Wei", ""], ["Xie", "Yuqing", ""], ["Tan", "Luchen", ""], ["Xiong", "Kun", ""], ["Li", "Ming", ""], ["Lin", "Jimmy", ""]]}, {"id": "1904.06672", "submitter": "Ahsaas Bajaj", "authors": "Ahsaas Bajaj, Shubham Krishna, Mukund Rungta, Hemant Tiwari and Vanraj\n  Vala", "title": "RelEmb: A relevance-based application embedding for Mobile App retrieval\n  and categorization", "comments": "13 Pages. Accepted at CICLing 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Retrieval Systems have revolutionized the organization and\nextraction of Information. In recent years, mobile applications (apps) have\nbecome primary tools of collecting and disseminating information. However,\nlimited research is available on how to retrieve and organize mobile apps on\nusers' devices. In this paper, authors propose a novel method to estimate\napp-embeddings which are then applied to tasks like app clustering,\nclassification, and retrieval. Usage of app-embedding for query expansion,\nnearest neighbor analysis enables unique and interesting use cases to enhance\nend-user experience with mobile apps.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 10:35:56 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Bajaj", "Ahsaas", ""], ["Krishna", "Shubham", ""], ["Rungta", "Mukund", ""], ["Tiwari", "Hemant", ""], ["Vala", "Vanraj", ""]]}, {"id": "1904.06690", "submitter": "Fei Sun", "authors": "Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng\n  Jiang", "title": "BERT4Rec: Sequential Recommendation with Bidirectional Encoder\n  Representations from Transformer", "comments": "To appear in CIKM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling users' dynamic and evolving preferences from their historical\nbehaviors is challenging and crucial for recommendation systems. Previous\nmethods employ sequential neural networks (e.g., Recurrent Neural Network) to\nencode users' historical interactions from left to right into hidden\nrepresentations for making recommendations. Although these methods achieve\nsatisfactory results, they often assume a rigidly ordered sequence which is not\nalways practical. We argue that such left-to-right unidirectional architectures\nrestrict the power of the historical sequence representations. For this\npurpose, we introduce a Bidirectional Encoder Representations from Transformers\nfor sequential Recommendation (BERT4Rec). However, jointly conditioning on both\nleft and right context in deep bidirectional model would make the training\nbecome trivial since each item can indirectly \"see the target item\". To address\nthis problem, we train the bidirectional model using the Cloze task, predicting\nthe masked items in the sequence by jointly conditioning on their left and\nright context. Comparing with predicting the next item at each position in a\nsequence, the Cloze task can produce more samples to train a more powerful\nbidirectional model. Extensive experiments on four benchmark datasets show that\nour model outperforms various state-of-the-art sequential models consistently.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 13:01:46 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 07:36:44 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Sun", "Fei", ""], ["Liu", "Jun", ""], ["Wu", "Jian", ""], ["Pei", "Changhua", ""], ["Lin", "Xiao", ""], ["Ou", "Wenwu", ""], ["Jiang", "Peng", ""]]}, {"id": "1904.06744", "submitter": "Won-Yong Shin", "authors": "Adeel Malik, Joongheon Kim, Kwang Soon Kim, Won-Yong Shin", "title": "A Personalized Preference Learning Framework for Caching in Mobile\n  Networks", "comments": "21 pages, 10 figures, 1 table, to appear in the IEEE Transactions on\n  Mobile Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IR cs.IT cs.LG cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper comprehensively studies a content-centric mobile network based on\na preference learning framework, where each mobile user is equipped with a\nfinite-size cache. We consider a practical scenario where each user requests a\ncontent file according to its own preferences, which is motivated by the\nexistence of heterogeneity in file preferences among different users. Under our\nmodel, we consider a single-hop-based device-to-device (D2D) content delivery\nprotocol and characterize the average hit ratio for the following two file\npreference cases: the personalized file preferences and the common file\npreferences. By assuming that the model parameters such as user activity\nlevels, user file preferences, and file popularity are unknown and thus need to\nbe inferred, we present a collaborative filtering (CF)-based approach to learn\nthese parameters. Then, we reformulate the hit ratio maximization problems into\na submodular function maximization and propose two computationally efficient\nalgorithms including a greedy approach to efficiently solve the cache\nallocation problems. We analyze the computational complexity of each algorithm.\nMoreover, we analyze the corresponding level of the approximation that our\ngreedy algorithm can achieve compared to the optimal solution. Using a\nreal-world dataset, we demonstrate that the proposed framework employing the\npersonalized file preferences brings substantial gains over its counterpart for\nvarious system parameters.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 05:54:53 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 16:22:14 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 05:43:21 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Malik", "Adeel", ""], ["Kim", "Joongheon", ""], ["Kim", "Kwang Soon", ""], ["Shin", "Won-Yong", ""]]}, {"id": "1904.06808", "submitter": "Bhaskar Mitra", "authors": "Corby Rosset, Bhaskar Mitra, Chenyan Xiong, Nick Craswell, Xia Song\n  and Saurabh Tiwary", "title": "An Axiomatic Approach to Regularizing Neural Ranking Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Axiomatic information retrieval (IR) seeks a set of principle properties\ndesirable in IR models. These properties when formally expressed provide\nguidance in the search for better relevance estimation functions. Neural\nranking models typically contain a large number of parameters. The training of\nthese models involve a search for appropriate parameter values based on large\nquantities of labeled examples. Intuitively, axioms that can guide the search\nfor better traditional IR models should also help in better parameter\nestimation for machine learning based rankers. This work explores the use of IR\naxioms to augment the direct supervision from labeled data for training neural\nranking models. We modify the documents in our dataset along the lines of\nwell-known axioms during training and add a regularization loss based on the\nagreement between the ranking model and the axioms on which version of the\ndocument---the original or the perturbed---should be preferred. Our experiments\nshow that the neural ranking model achieves faster convergence and better\ngeneralization with axiomatic regularization.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 02:19:48 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Rosset", "Corby", ""], ["Mitra", "Bhaskar", ""], ["Xiong", "Chenyan", ""], ["Craswell", "Nick", ""], ["Song", "Xia", ""], ["Tiwary", "Saurabh", ""]]}, {"id": "1904.06813", "submitter": "Changhua Pei", "authors": "Changhua Pei, Yi Zhang, Yongfeng Zhang, Fei Sun, Xiao Lin, Hanxiao\n  Sun, Jian Wu, Peng Jiang and Wenwu Ou", "title": "Personalized Re-ranking for Recommendation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking is a core task in recommender systems, which aims at providing an\nordered list of items to users. Typically, a ranking function is learned from\nthe labeled dataset to optimize the global performance, which produces a\nranking score for each individual item. However, it may be sub-optimal because\nthe scoring function applies to each item individually and does not explicitly\nconsider the mutual influence between items, as well as the differences of\nusers' preferences or intents. Therefore, we propose a personalized re-ranking\nmodel for recommender systems. The proposed re-ranking model can be easily\ndeployed as a follow-up modular after any ranking algorithm, by directly using\nthe existing ranking feature vectors. It directly optimizes the whole\nrecommendation list by employing a transformer structure to efficiently encode\nthe information of all items in the list. Specifically, the Transformer applies\na self-attention mechanism that directly models the global relationships\nbetween any pair of items in the whole list. We confirm that the performance\ncan be further improved by introducing pre-trained embedding to learn\npersonalized encoding functions for different users. Experimental results on\nboth offline benchmarks and real-world online e-commerce systems demonstrate\nthe significant improvements of the proposed re-ranking model.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 02:47:40 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 06:02:13 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2019 03:00:56 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Pei", "Changhua", ""], ["Zhang", "Yi", ""], ["Zhang", "Yongfeng", ""], ["Sun", "Fei", ""], ["Lin", "Xiao", ""], ["Sun", "Hanxiao", ""], ["Wu", "Jian", ""], ["Jiang", "Peng", ""], ["Ou", "Wenwu", ""]]}, {"id": "1904.06862", "submitter": "Elisa Claire Alem\\'an Carre\\'on", "authors": "Elisa Claire Alem\\'an Carre\\'on, Hirofumi Nonaka, Asahi Hentona and\n  Hirochika Yamashiro", "title": "Measuring the influence of mere exposure effect of TV commercial adverts\n  on purchase behavior based on machine learning prediction models", "comments": null, "journal-ref": "2019. Information Processing & Management, Vol. 56, No. 4. pp.\n  1339 - 1355. URL https://authors.elsevier.com/a/1YotZ15hYdcAq4", "doi": "10.1016/j.ipm.2019.03.007", "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since its introduction, television has been the main channel of investment\nfor advertisements in order to influence customers purchase behavior. Many have\nattributed the mere exposure effect as the source of influence in purchase\nintention and purchase decision; however, most of the studies of television\nadvertisement effects are not only outdated, but their sample size is\nquestionable and their environments do not reflect reality. With the advent of\nthe internet, social media and new information technologies, many recent\nstudies focus on the effects of online advertisement, meanwhile, the investment\nin television advertisement still has not declined. In response to this, we\napplied machine learning algorithms SVM and XGBoost, as well as Logistic\nRegression, to construct a number of prediction models based on at-home\nadvertisement exposure time and demographic data, examining the predictability\nof Actual Purchase and Purchase Intention behaviors of 3000 customers across 36\ndifferent products during the span of 3 months. If models based on exposure\ntime had unreliable predictability in contrast to models based on demographic\ndata, doubts would surface about the effectiveness of the hard investment in\ntelevision advertising. Based on our results, we found that models based on\nadvert exposure time were consistently low in their predictability in\ncomparison with models based on demographic data only, and with models based on\nboth demographic data and exposure time data. We also found that there was not\na statistically significant difference between these last two kinds of models.\nThis suggests that advert exposure time has little to no effect in the\nshort-term in increasing positive actual purchase behavior.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 05:50:56 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 07:30:59 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Carre\u00f3n", "Elisa Claire Alem\u00e1n", ""], ["Nonaka", "Hirofumi", ""], ["Hentona", "Asahi", ""], ["Yamashiro", "Hirochika", ""]]}, {"id": "1904.07094", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Andrew Yates, Arman Cohan, Nazli Goharian", "title": "CEDR: Contextualized Embeddings for Document Ranking", "comments": "Appeared in SIGIR 2019, 4 pages", "journal-ref": null, "doi": "10.1145/3331184.3331317", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although considerable attention has been given to neural ranking\narchitectures recently, far less attention has been paid to the term\nrepresentations that are used as input to these models. In this work, we\ninvestigate how two pretrained contextualized language models (ELMo and BERT)\ncan be utilized for ad-hoc document ranking. Through experiments on TREC\nbenchmarks, we find that several existing neural ranking architectures can\nbenefit from the additional context provided by contextualized language models.\nFurthermore, we propose a joint approach that incorporates BERT's\nclassification vector into existing neural models and show that it outperforms\nstate-of-the-art ad-hoc ranking baselines. We call this joint approach CEDR\n(Contextualized Embeddings for Document Ranking). We also address practical\nchallenges in using these models for ranking, including the maximum input\nlength imposed by BERT and runtime performance impacts of contextualized\nlanguage models.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 14:55:59 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 21:16:59 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2019 15:03:22 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["MacAvaney", "Sean", ""], ["Yates", "Andrew", ""], ["Cohan", "Arman", ""], ["Goharian", "Nazli", ""]]}, {"id": "1904.07307", "submitter": "Alexander W. Wong", "authors": "Alexander William Wong, Ken Wong, Abram Hindle", "title": "Tracing Forum Posts to MOOC Content using Topic Analysis", "comments": "6 pages, 4 figures, Course project for UofA CMPUT 660, Winter 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive Open Online Courses are educational programs that are open and\naccessible to a large number of people through the internet. To facilitate\nlearning, MOOC discussion forums exist where students and instructors\ncommunicate questions, answers, and thoughts related to the course.\n  The primary objective of this paper is to investigate tracing discussion\nforum posts back to course lecture videos and readings using topic analysis. We\nutilize both unsupervised and supervised variants of Latent Dirichlet\nAllocation (LDA) to extract topics from course material and classify forum\nposts. We validate our approach on posts bootstrapped from five Coursera\ncourses and determine that topic models can be used to map student discussion\nposts back to the underlying course lecture or reading. Labeled LDA outperforms\nunsupervised Hierarchical Dirichlet Process LDA and base LDA for our\ntraceability task. This research is useful as it provides an automated approach\nfor clustering student discussions by course material, enabling instructors to\nquickly evaluate student misunderstanding of content and clarify materials\naccordingly.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 19:49:06 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Wong", "Alexander William", ""], ["Wong", "Ken", ""], ["Hindle", "Abram", ""]]}, {"id": "1904.07428", "submitter": "Jiaming Qu", "authors": "Jiaming Qu and Yue Wang", "title": "A Medical Literature Search System for Identifying Effective Treatments\n  in Precision Medicine", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Precision Medicine Initiative states that treatments for a patient should\ntake into account not only the patient's disease, but his/her specific genetic\nvariation as well. The vast biomedical literature holds the potential for\nphysicians to identify effective treatment options for a cancer patient.\nHowever, the complexity and ambiguity of medical terms can result in vocabulary\nmismatch between the physician's query and the literature. The physician's\nsearch intent (finding treatments instead of other types of studies) is\ndifficult to explicitly formulate in a query. Therefore, simple ad hot\nretrieval approach will suffer from low recall and precision. In this paper, we\npropose a new retrieval system that helps physicians identify effective\ntreatments in precision medicine. Given a cancer patient with a specific\ndisease, genetic variation, and demographic information, the system aims to\nidentify biomedical publications that report effective treatments. We approach\nthis goal from two directions. First, we expand the original disease and gene\nterms using biomedical knowledge bases to improve recall of the initial\nretrieval. We then improve precision by promoting treatment-related\npublications to the top using a machine learning reranker trained on 2017 Text\nRetrieval Conference Precision Medicine (PM) track corpus. Batch evaluation\nresults on 2018 PM track corpus show that the proposed approach effectively\nimproves both recall and precision, achieving performance comparable to the top\nentries on the leaderboard of 2018 PM track.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 03:20:33 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 03:31:01 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Qu", "Jiaming", ""], ["Wang", "Yue", ""]]}, {"id": "1904.07467", "submitter": "Dominik K\\\"oppl", "authors": "Kazuya Tsuruta and Dominik K\\\"oppl and Shunsuke Kanda and Yuto\n  Nakashima and Shunsuke Inenaga and Hideo Bannai and Masayuki Takeda", "title": "c-trie++: A Dynamic Trie Tailored for Fast Prefix Searches", "comments": null, "journal-ref": "Full version of conference paper at DCC, pages 243-252, 2020", "doi": "10.1109/DCC47342.2020.00032", "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a dynamic set $K$ of $k$ strings of total length $n$ whose characters\nare drawn from an alphabet of size $\\sigma$, a keyword dictionary is a data\nstructure built on $K$ that provides locate, prefix search, and update\noperations on $K$. Under the assumption that $\\alpha = w / \\lg \\sigma$\ncharacters fit into a single machine word $w$, we propose a keyword dictionary\nthat represents $K$ in $n \\lg \\sigma + \\Theta(k \\lg n)$ bits of space,\nsupporting all operations in $O(m / \\alpha + \\lg \\alpha)$ expected time on an\ninput string of length $m$ in the word RAM model. This data structure is\nunderlined with an exhaustive practical evaluation, highlighting the practical\nusefulness of the proposed data structure, especially for prefix searches - one\nof the most elementary keyword dictionary operations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 05:20:47 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 07:37:22 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 07:43:59 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Tsuruta", "Kazuya", ""], ["K\u00f6ppl", "Dominik", ""], ["Kanda", "Shunsuke", ""], ["Nakashima", "Yuto", ""], ["Inenaga", "Shunsuke", ""], ["Bannai", "Hideo", ""], ["Takeda", "Masayuki", ""]]}, {"id": "1904.07531", "submitter": "Yifan Qiao", "authors": "Yifan Qiao, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu", "title": "Understanding the Behaviors of BERT in Ranking", "comments": "There is an error in Table 1 and we will update them to correct\n  results. Please refer to MS MARCO Leaderboard for the actually evaluation\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the performances and behaviors of BERT in ranking tasks.\nWe explore several different ways to leverage the pre-trained BERT and\nfine-tune it on two ranking tasks: MS MARCO passage reranking and TREC Web\nTrack ad hoc document ranking. Experimental results on MS MARCO demonstrate the\nstrong effectiveness of BERT in question-answering focused passage ranking\ntasks, as well as the fact that BERT is a strong interaction-based seq2seq\nmatching model. Experimental results on TREC show the gaps between the BERT\npre-trained on surrounding contexts and the needs of ad hoc document ranking.\nAnalyses illustrate how BERT allocates its attentions between query-document\ntokens in its Transformer layers, how it prefers semantic matches between\nparaphrase tokens, and how that differs with the soft match patterns learned by\na click-trained neural ranker.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 08:30:31 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 04:26:14 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 19:39:00 GMT"}, {"version": "v4", "created": "Fri, 26 Apr 2019 12:44:38 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Qiao", "Yifan", ""], ["Xiong", "Chenyan", ""], ["Liu", "Zhenghao", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "1904.07569", "submitter": "Jamal Al Qundus", "authors": "Jamal Al Qundus and Adrian Paschke", "title": "Investigating the Effect of Attributes on User Trust in Social Media", "comments": "5 pages, 1 figure", "journal-ref": "International Conference on Database and Expert Systems\n  Applications 2018", "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One main challenge in social media is to identify trustworthy information. If\nwe cannot recognize information as trustworthy, that information may become\nuseless or be lost. Opposite, we could consume wrong or fake information with\nmajor consequences. How does a user handle the information provided before\nconsuming it? Are the comments on a post, the author or votes essential for\ntaking such a decision? Are these attributes considered together and which\nattribute is more important? To answer these questions, we developed a trust\nmodel to support knowledge sharing of user content in social media. This trust\nmodel is based on the dimensions of stability, quality, and credibility. Each\ndimension contains metrics (user role, user IQ, votes, etc.) that are important\nto the user based on data analysis. We present in this paper, an evaluation of\nthe proposed trust model using conjoint analysis (CA) as an evaluation method.\nThe results obtained from 348 responses, validate the trust model. A trust\ndegree translator interprets the content as very trusted, trusted, untrusted,\nand very untrusted based on the calculated value of trust. Furthermore, the\nresults show different importance for each dimension: stability 24%,\ncredibility 35% and quality 41%.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 10:00:55 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Qundus", "Jamal Al", ""], ["Paschke", "Adrian", ""]]}, {"id": "1904.07619", "submitter": "Giulio Ermanno Pibiri", "authors": "Raffaele Perego, Giulio Ermanno Pibiri, Rossano Venturini", "title": "Compressed Indexes for Fast Search of Semantic Data", "comments": "Published in IEEE Transactions on Knowledge and Data Engineering\n  (TKDE), 14 January 2020", "journal-ref": null, "doi": "10.1109/TKDE.2020.2966609", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sheer increase in volume of RDF data demands efficient solutions for the\ntriple indexing problem, that is devising a compressed data structure to\ncompactly represent RDF triples by guaranteeing, at the same time, fast pattern\nmatching operations. This problem lies at the heart of delivering good\npractical performance for the resolution of complex SPARQL queries on large RDF\ndatasets. In this work, we propose a trie-based index layout to solve the\nproblem and introduce two novel techniques to reduce its space of\nrepresentation for improved effectiveness. The extensive experimental analysis\nconducted over a wide range of publicly available real-world datasets, reveals\nthat our best space/time trade-off configuration substantially outperforms\nexisting solutions at the state-of-the-art, by taking 30-60% less space and\nspeeding up query execution by a factor of 2-81x.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 12:27:24 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 07:42:26 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 09:28:13 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Perego", "Raffaele", ""], ["Pibiri", "Giulio Ermanno", ""], ["Venturini", "Rossano", ""]]}, {"id": "1904.07629", "submitter": "Zhaoning Li", "authors": "Zhaoning Li, Qi Li, Xiaotian Zou, Jiangtao Ren", "title": "Causality Extraction based on Self-Attentive BiLSTM-CRF with Transferred\n  Embeddings", "comments": "39 pages, 11 figures, 6 tables", "journal-ref": "Neurocomputing, Volume 423, 2021, Pages 207-219", "doi": "10.1016/j.neucom.2020.08.078", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causality extraction from natural language texts is a challenging open\nproblem in artificial intelligence. Existing methods utilize patterns,\nconstraints, and machine learning techniques to extract causality, heavily\ndepending on domain knowledge and requiring considerable human effort and time\nfor feature engineering. In this paper, we formulate causality extraction as a\nsequence labeling problem based on a novel causality tagging scheme. On this\nbasis, we propose a neural causality extractor with the BiLSTM-CRF model as the\nbackbone, named SCITE (Self-attentive BiLSTM-CRF wIth Transferred Embeddings),\nwhich can directly extract cause and effect without extracting candidate causal\npairs and identifying their relations separately. To address the problem of\ndata insufficiency, we transfer contextual string embeddings, also known as\nFlair embeddings, which are trained on a large corpus in our task. In addition,\nto improve the performance of causality extraction, we introduce a multihead\nself-attention mechanism into SCITE to learn the dependencies between causal\nwords. We evaluate our method on a public dataset, and experimental results\ndemonstrate that our method achieves significant and consistent improvement\ncompared to baselines.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 12:54:00 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2019 19:00:21 GMT"}, {"version": "v3", "created": "Mon, 29 Apr 2019 13:59:57 GMT"}, {"version": "v4", "created": "Tue, 29 Oct 2019 07:14:23 GMT"}, {"version": "v5", "created": "Wed, 17 Jun 2020 16:28:53 GMT"}, {"version": "v6", "created": "Sun, 8 Nov 2020 13:30:15 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Zhaoning", ""], ["Li", "Qi", ""], ["Zou", "Xiaotian", ""], ["Ren", "Jiangtao", ""]]}, {"id": "1904.07687", "submitter": "Andrei Damian I", "authors": "Andrei Damian, Laurentiu Piciu, Sergiu Turlea, Nicolae Tapus", "title": "Advanced Customer Activity Prediction based on Deep Hierarchic\n  Encoder-Decoders", "comments": "2019 22nd International Conference on Control Systems and Computer\n  Science (CSCS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product recommender systems and customer profiling techniques have always\nbeen a priority in online retail. Recent machine learning research advances and\nalso wide availability of massive parallel numerical computing has enabled\nvarious approaches and directions of recommender systems advancement. Worth to\nmention is the fact that in past years multiple traditional \"offline\" retail\nbusiness are gearing more and more towards employing inferential and even\npredictive analytics both to stock-related problems such as predictive\nreplenishment but also to enrich customer interaction experience. One of the\nmost important areas of recommender systems research and development is that of\nDeep Learning based models which employ representational learning to model\nconsumer behavioral patterns. Current state of the art in Deep Learning based\nrecommender systems uses multiple approaches ranging from already classical\nmethods such as the ones based on learning product representation vector, to\nrecurrent analysis of customer transactional time-series and up to generative\nmodels based on adversarial training. Each of these methods has multiple\nadvantages and inherent weaknesses such as inability of understanding the\nactual user-journey, ability to propose only single product recommendation or\ntop-k product recommendations without prediction of actual next-best-offer. In\nour work we will present a new and innovative architectural approach of\napplying state-of-the-art hierarchical multi-module encoder-decoder\narchitecture in order to solve several of current state-of-the-art recommender\nsystems issues. Our approach will also produce by-products such as product\nneed-based segmentation and customer behavioral segmentation - all in an\nend-to-end trainable approach. Finally, we will present a couple methods that\nsolve known retail & distribution pain-points based on the proposed\narchitecture.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 18:15:33 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 05:22:36 GMT"}, {"version": "v3", "created": "Thu, 16 May 2019 14:20:25 GMT"}, {"version": "v4", "created": "Fri, 21 Jun 2019 17:03:21 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Damian", "Andrei", ""], ["Piciu", "Laurentiu", ""], ["Turlea", "Sergiu", ""], ["Tapus", "Nicolae", ""]]}, {"id": "1904.07695", "submitter": "Jipeng Qiang", "authors": "Qiang Jipeng and Qian Zhenyu and Li Yun and Yuan Yunhao and Wu Xindong", "title": "Short Text Topic Modeling Techniques, Applications, and Performance: A\n  Survey", "comments": "arXiv admin note: text overlap with arXiv:1808.02215 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing short texts infers discriminative and coherent latent topics that\nis a critical and fundamental task since many real-world applications require\nsemantic understanding of short texts. Traditional long text topic modeling\nalgorithms (e.g., PLSA and LDA) based on word co-occurrences cannot solve this\nproblem very well since only very limited word co-occurrence information is\navailable in short texts. Therefore, short text topic modeling has already\nattracted much attention from the machine learning research community in recent\nyears, which aims at overcoming the problem of sparseness in short texts. In\nthis survey, we conduct a comprehensive review of various short text topic\nmodeling techniques proposed in the literature. We present three categories of\nmethods based on Dirichlet multinomial mixture, global word co-occurrences, and\nself-aggregation, with example of representative approaches in each category\nand analysis of their performance on various tasks. We develop the first\ncomprehensive open-source library, called STTM, for use in Java that integrates\nall surveyed algorithms within a unified interface, benchmark datasets, to\nfacilitate the expansion of new methods in this research field. Finally, we\nevaluate these state-of-the-art methods on many real-world datasets and compare\ntheir performance against one another and versus long text topic modeling\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 09:08:46 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Jipeng", "Qiang", ""], ["Zhenyu", "Qian", ""], ["Yun", "Li", ""], ["Yunhao", "Yuan", ""], ["Xindong", "Wu", ""]]}, {"id": "1904.07765", "submitter": "Oznur Alkan", "authors": "Oznur Alkan, Elizabeth M. Daly, Adi Botea", "title": "An Evaluation Framework for Interactive Recommender System", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional recommender systems present a relatively static list of\nrecommendations to a user where the feedback is typically limited to an\naccept/reject or a rating model. However, these simple modes of feedback may\nonly provide limited insights as to why a user likes or dislikes an item and\nwhat aspects of the item the user has considered. Interactive recommender\nsystems present an opportunity to engage the user in the process by allowing\nthem to interact with the recommendations, provide feedback and impact the\nresults in real-time. Evaluation of the impact of the user interaction\ntypically requires an extensive user study which is time consuming and gives\nresearchers limited opportunities to tune their solutions without having to\nconduct multiple rounds of user feedback. Additionally, user experience and\ndesign aspects can have a significant impact on the user feedback which may\nresult in not necessarily assessing the quality of some of the underlying\nalgorithmic decisions in the overall solution. As a result, we present an\nevaluation framework which aims to simulate the users interacting with the\nrecommender. We formulate metrics to evaluate the quality of the interactive\nrecommenders which are outputted by the framework once simulation is completed.\nWhile simulation along is not sufficient to evaluate a complete solution, the\nresults can be useful to help researchers tune their solution before moving to\nthe user study stage.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 15:36:48 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Alkan", "Oznur", ""], ["Daly", "Elizabeth M.", ""], ["Botea", "Adi", ""]]}, {"id": "1904.07965", "submitter": "Fabrizio Sebastiani", "authors": "Andrea Esuli, Alejandro Moreo, Fabrizio Sebastiani", "title": "Cross-Lingual Sentiment Quantification", "comments": "Identical to previous version, but for the abstract, which is now\n  identical to the one in the published version", "journal-ref": "Published in IEEE Intelligent Systems 35(3):106-114, 2020. The\n  present version is identical to the published one but for formatting", "doi": "10.1109/MIS.2020.2979203", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\emph{Sentiment Quantification} (i.e., the task of estimating the relative\nfrequency of sentiment-related classes -- such as \\textsf{Positive} and\n\\textsf{Negative} -- in a set of unlabelled documents) is an important topic in\nsentiment analysis, as the study of sentiment-related quantities and trends\nacross a population is often of higher interest than the analysis of individual\ninstances. In this work we propose a method for \\emph{Cross-Lingual Sentiment\nQuantification}, the task of performing sentiment quantification when training\ndocuments are available for a source language $\\mathcal{S}$ but not for the\ntarget language $\\mathcal{T}$ for which sentiment quantification needs to be\nperformed. Cross-lingual sentiment quantification (and cross-lingual\n\\emph{text} quantification in general) has never been discussed before in the\nliterature; we establish baseline results for the binary case by combining\nstate-of-the-art quantification methods with methods capable of generating\ncross-lingual vectorial representations of the source and target documents\ninvolved. We present experimental results obtained on publicly available\ndatasets for cross-lingual sentiment classification; the results show that the\npresented methods can perform cross-lingual sentiment quantification with a\nsurprising level of accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 20:32:02 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 13:50:58 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Esuli", "Andrea", ""], ["Moreo", "Alejandro", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1904.07982", "submitter": "Muhammad Mahbubur Rahman", "authors": "Muhammad Mahbubur Rahman, Sorami Hisamoto, Kevin Duh", "title": "Query Expansion for Cross-Language Question Re-Ranking", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community question-answering (CQA) platforms have become very popular forums\nfor asking and answering questions daily. While these forums are rich\nrepositories of community knowledge, they present challenges for finding\nrelevant answers and similar questions, due to the open-ended nature of\ninformal discussions. Further, if the platform allows questions and answers in\nmultiple languages, we are faced with the additional challenge of matching\ncross-lingual information. In this work, we focus on the cross-language\nquestion re-ranking shared task, which aims to find existing questions that may\nbe written in different languages. Our contribution is an exploration of query\nexpansion techniques for this problem. We investigate expansions based on Word\nEmbeddings, DBpedia concepts linking, and Hypernym, and show that they\noutperform existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 20:55:59 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Rahman", "Muhammad Mahbubur", ""], ["Hisamoto", "Sorami", ""], ["Duh", "Kevin", ""]]}, {"id": "1904.08010", "submitter": "Mathieu Roche", "authors": "Mathieu Roche", "title": "How to define co-occurrence in different domains of study?", "comments": "CICLING'2018 (International Conference on Computational Linguistics\n  and Intelligent Text Processing) - March 18 to 24, 2018 - Hanoi, Vietnam (not\n  published in CICLING proceedings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This position paper presents a comparative study of co-occurrences. Some\nsimilarities and differences in the definition exist depending on the research\ndomain (e.g. linguistics, NLP, computer science). This paper discusses these\npoints, and deals with the methodological aspects in order to identify\nco-occurrences in a multidisciplinary paradigm.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 23:16:56 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Roche", "Mathieu", ""]]}, {"id": "1904.08030", "submitter": "Chao Li", "authors": "Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Pipei Huang, Huan Zhao,\n  Guoliang Kang, Qiwei Chen, Wei Li, Dik Lun Lee", "title": "Multi-Interest Network with Dynamic Routing for Recommendation at Tmall", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial recommender systems usually consist of the matching stage and the\nranking stage, in order to handle the billion-scale of users and items. The\nmatching stage retrieves candidate items relevant to user interests, while the\nranking stage sorts candidate items by user interests. Thus, the most critical\nability is to model and represent user interests for either stage. Most of the\nexisting deep learning-based models represent one user as a single vector which\nis insufficient to capture the varying nature of user's interests. In this\npaper, we approach this problem from a different view, to represent one user\nwith multiple vectors encoding the different aspects of the user's interests.\nWe propose the Multi-Interest Network with Dynamic routing (MIND) for dealing\nwith user's diverse interests in the matching stage. Specifically, we design a\nmulti-interest extractor layer based on capsule routing mechanism, which is\napplicable for clustering historical behaviors and extracting diverse\ninterests. Furthermore, we develop a technique named label-aware attention to\nhelp learn a user representation with multiple vectors. Through extensive\nexperiments on several public benchmarks and one large-scale industrial dataset\nfrom Tmall, we demonstrate that MIND can achieve superior performance than\nstate-of-the-art methods for recommendation. Currently, MIND has been deployed\nfor handling major online traffic at the homepage on Mobile Tmall App.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 00:39:24 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Li", "Chao", ""], ["Liu", "Zhiyuan", ""], ["Wu", "Mengmeng", ""], ["Xu", "Yuchi", ""], ["Huang", "Pipei", ""], ["Zhao", "Huan", ""], ["Kang", "Guoliang", ""], ["Chen", "Qiwei", ""], ["Li", "Wei", ""], ["Lee", "Dik Lun", ""]]}, {"id": "1904.08042", "submitter": "Xin Wen", "authors": "Xin Wen, Zhizhong Han, Xinyu Yin, Yu-Shen Liu", "title": "Adversarial Cross-Modal Retrieval via Learning and Transferring\n  Single-Modal Similarities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval aims to retrieve relevant data across different\nmodalities (e.g., texts vs. images). The common strategy is to apply\nelement-wise constraints between manually labeled pair-wise items to guide the\ngenerators to learn the semantic relationships between the modalities, so that\nthe similar items can be projected close to each other in the common\nrepresentation subspace. However, such constraints often fail to preserve the\nsemantic structure between unpaired but semantically similar items (e.g. the\nunpaired items with the same class label are more similar than items with\ndifferent labels). To address the above problem, we propose a novel cross-modal\nsimilarity transferring (CMST) method to learn and preserve the semantic\nrelationships between unpaired items in an unsupervised way. The key idea is to\nlearn the quantitative similarities in single-modal representation subspace,\nand then transfer them to the common representation subspace to establish the\nsemantic relationships between unpaired items across modalities. Experiments\nshow that our method outperforms the state-of-the-art approaches both in the\nclass-based and pair-based retrieval tasks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 01:19:51 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Wen", "Xin", ""], ["Han", "Zhizhong", ""], ["Yin", "Xinyu", ""], ["Liu", "Yu-Shen", ""]]}, {"id": "1904.08049", "submitter": "Yanjun  Qi Dr.", "authors": "Jack Lanchantin, Arshdeep Sekhon and Yanjun Qi", "title": "Neural Message Passing for Multi-Label Classification", "comments": "19pages. We provide our code and datasets at\n  https://github.com/QData/LaMP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification (MLC) is the task of assigning a set of target\nlabels for a given sample. Modeling the combinatorial label interactions in MLC\nhas been a long-haul challenge. We propose Label Message Passing (LaMP) Neural\nNetworks to efficiently model the joint prediction of multiple labels. LaMP\ntreats labels as nodes on a label-interaction graph and computes the hidden\nrepresentation of each label node conditioned on the input using\nattention-based neural message passing. Attention enables LaMP to assign\ndifferent importance to neighbor nodes per label, learning how labels interact\n(implicitly). The proposed models are simple, accurate, interpretable,\nstructure-agnostic, and applicable for predicting dense labels since LaMP is\nincredibly parallelizable. We validate the benefits of LaMP on seven real-world\nMLC datasets, covering a broad spectrum of input/output types and outperforming\nthe state-of-the-art results. Notably, LaMP enables intuitive interpretation of\nhow classifying each label depends on the elements of a sample and at the same\ntime rely on its interaction with other labels. We provide our code and\ndatasets at https://github.com/QData/LaMP\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 01:58:17 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Lanchantin", "Jack", ""], ["Sekhon", "Arshdeep", ""], ["Qi", "Yanjun", ""]]}, {"id": "1904.08067", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Kiana Jafari Meimandi, Mojtaba Heidarysafa, Sanjana\n  Mendu, Laura E. Barnes, Donald E. Brown", "title": "Text Classification Algorithms: A Survey", "comments": null, "journal-ref": null, "doi": "10.3390/info10040150", "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, there has been an exponential growth in the number of\ncomplex documents and texts that require a deeper understanding of machine\nlearning methods to be able to accurately classify texts in many applications.\nMany machine learning approaches have achieved surpassing results in natural\nlanguage processing. The success of these learning algorithms relies on their\ncapacity to understand complex models and non-linear relationships within data.\nHowever, finding suitable structures, architectures, and techniques for text\nclassification is a challenge for researchers. In this paper, a brief overview\nof text classification algorithms is discussed. This overview covers different\ntext feature extractions, dimensionality reduction methods, existing algorithms\nand techniques, and evaluations methods. Finally, the limitations of each\ntechnique and their application in the real-world problem are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 03:29:05 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 01:20:53 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 18:28:33 GMT"}, {"version": "v4", "created": "Tue, 25 Jun 2019 22:51:18 GMT"}, {"version": "v5", "created": "Wed, 20 May 2020 16:27:00 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Kowsari", "Kamran", ""], ["Meimandi", "Kiana Jafari", ""], ["Heidarysafa", "Mojtaba", ""], ["Mendu", "Sanjana", ""], ["Barnes", "Laura E.", ""], ["Brown", "Donald E.", ""]]}, {"id": "1904.08100", "submitter": "Jiaju Qi", "authors": "Lei Lei, Jiaju Qi and Kan Zheng", "title": "Patent Analytics Based on Feature Vector Space Model: A Case of IoT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of approved patents worldwide increases rapidly each year, which\nrequires new patent analytics to efficiently mine the valuable information\nattached to these patents. Vector space model (VSM) represents documents as\nhigh-dimensional vectors, where each dimension corresponds to a unique term.\nWhile originally proposed for information retrieval systems, VSM has also seen\nwide applications in patent analytics, and used as a fundamental tool to map\npatent documents to structured data. However, VSM method suffers from several\nlimitations when applied to patent analysis tasks, such as loss of\nsentence-level semantics and curse-of-dimensionality problems. In order to\naddress the above limitations, we propose a patent analytics based on feature\nvector space model (FVSM), where the FVSM is constructed by mapping patent\ndocuments to feature vectors extracted by convolutional neural networks (CNN).\nThe applications of FVSM for three typical patent analysis tasks, i.e., patents\nsimilarity comparison, patent clustering, and patent map generation are\ndiscussed. A case study using patents related to Internet of Things (IoT)\ntechnology is illustrated to demonstrate the performance and effectiveness of\nFVSM. The proposed FVSM can be adopted by other patent analysis studies to\nreplace VSM, based on which various big data learning tasks can be performed.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 06:20:53 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Lei", "Lei", ""], ["Qi", "Jiaju", ""], ["Zheng", "Kan", ""]]}, {"id": "1904.08375", "submitter": "Rodrigo Nogueira", "authors": "Rodrigo Nogueira, Wei Yang, Jimmy Lin, Kyunghyun Cho", "title": "Document Expansion by Query Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One technique to improve the retrieval effectiveness of a search engine is to\nexpand documents with terms that are related or representative of the\ndocuments' content.From the perspective of a question answering system, this\nmight comprise questions the document can potentially answer. Following this\nobservation, we propose a simple method that predicts which queries will be\nissued for a given document and then expands it with those predictions with a\nvanilla sequence-to-sequence model, trained using datasets consisting of pairs\nof query and relevant documents. By combining our method with a\nhighly-effective re-ranking component, we achieve the state of the art in two\nretrieval tasks. In a latency-critical regime, retrieval results alone (without\nre-ranking) approach the effectiveness of more computationally expensive neural\nre-rankers but are much faster.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:20:14 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 00:40:54 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Nogueira", "Rodrigo", ""], ["Yang", "Wei", ""], ["Lin", "Jimmy", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1904.08418", "submitter": "Mohamed Hamroun", "authors": "Mohamed Hamroun, Sonia Lajmi", "title": "An efficient multi-language Video Search Engine to facilitate the HADJ\n  and the UMRA", "comments": "arXiv admin note: text overlap with arXiv:1308.3225", "journal-ref": "Conference: 17th Scientific Forum for the Research of Hajj, Umrah\n  and Madinah Visit At: Madinah, Saudi Arabia Volume: 17 , May 2017", "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos clips became the most important and prominent multimedia document to\nillustrate the rituals process of Hajj and Umrah. Therefore, it is necessary to\ndevelop a system to facilitate access to information related to the duties, the\npillars, the stages and the prayers. In this paper present a new project\naccomplishing a search engine in a large video database enabling any pilgrims\nto get the information that he care about as fast, accurate. This project is\nbased on two techniques: (a) the weighting method to determine the degree of\naffiliation of a video clip to a particular topic (b) organizing data using\nseveral layers.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 12:30:54 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Hamroun", "Mohamed", ""], ["Lajmi", "Sonia", ""]]}, {"id": "1904.08524", "submitter": "Pranav Ravindra Maneriker", "authors": "Nikhita Vedula, Nedim Lipka, Pranav Maneriker, Srinivasan\n  Parthasarathy", "title": "Towards Open Intent Discovery for Conversational Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and identifying user intent from text, both written and spoken,\nplays an important role in modelling and understand dialogs. Existing research\nfor intent discovery model it as a classification task with a predefined set of\nknown categories. To generailze beyond these preexisting classes, we define a\nnew task of \\textit{open intent discovery}. We investigate how intent can be\ngeneralized to those not seen during training. To this end, we propose a\ntwo-stage approach to this task - predicting whether an utterance contains an\nintent, and then tagging the intent in the input utterance. Our model consists\nof a bidirectional LSTM with a CRF on top to capture contextual semantics,\nsubject to some constraints. Self-attention is used to learn long distance\ndependencies. Further, we adapt an adversarial training approach to improve\nrobustness and perforamce across domains. We also present a dataset of 25k\nreal-life utterances that have been labelled via crowd sourcing. Our\nexperiments across different domains and real-world datasets show the\neffectiveness of our approach, with less than 100 annotated examples needed per\nunique domain to recognize diverse intents. The approach outperforms\nstate-of-the-art baselines by 5-15% F1 score points.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 22:40:01 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Vedula", "Nikhita", ""], ["Lipka", "Nedim", ""], ["Maneriker", "Pranav", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1904.08542", "submitter": "Vinay Verma Kumar", "authors": "Vinay Kumar Verma, Aakansha Mishra, Ashish Mishra and Piyush Rai", "title": "Generative Model for Zero-Shot Sketch-Based Image Retrieval", "comments": "Accepted at CVPR-Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic model for Sketch-Based Image Retrieval (SBIR)\nwhere, at retrieval time, we are given sketches from novel classes, that were\nnot present at training time. Existing SBIR methods, most of which rely on\nlearning class-wise correspondences between sketches and images, typically work\nwell only for previously seen sketch classes, and result in poor retrieval\nperformance on novel classes. To address this, we propose a generative model\nthat learns to generate images, conditioned on a given novel class sketch. This\nenables us to reduce the SBIR problem to a standard image-to-image search\nproblem. Our model is based on an inverse auto-regressive flow based\nvariational autoencoder, with a feedback mechanism to ensure robust image\ngeneration. We evaluate our model on two very challenging datasets, Sketchy,\nand TU Berlin, with novel train-test split. The proposed approach significantly\noutperforms various baselines on both the datasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 00:11:04 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Verma", "Vinay Kumar", ""], ["Mishra", "Aakansha", ""], ["Mishra", "Ashish", ""], ["Rai", "Piyush", ""]]}, {"id": "1904.08572", "submitter": "Di Jin", "authors": "Di Jin, Mark Heimann, Ryan Rossi, and Danai Koutra", "title": "node2bits: Compact Time- and Attribute-aware Node Representations for\n  User Stitching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identity stitching, the task of identifying and matching various online\nreferences (e.g., sessions over different devices and timespans) to the same\nuser in real-world web services, is crucial for personalization and\nrecommendations. However, traditional user stitching approaches, such as\ngrouping or blocking, require quadratic pairwise comparisons between a massive\nnumber of user activities, thus posing both computational and storage\nchallenges. Recent works, which are often application-specific, heuristically\nseek to reduce the amount of comparisons, but they suffer from low precision\nand recall. To solve the problem in an application-independent way, we take a\nheterogeneous network-based approach in which users (nodes) interact with\ncontent (e.g., sessions, websites), and may have attributes (e.g., location).\nWe propose node2bits, an efficient framework that represents multi-dimensional\nfeatures of node contexts with binary hashcodes. node2bits leverages\nfeature-based temporal walks to encapsulate short- and long-term interactions\nbetween nodes in heterogeneous web networks, and adopts SimHash to obtain\ncompact, binary representations and avoid the quadratic complexity for\nsimilarity search. Extensive experiments on large-scale real networks show that\nnode2bits outperforms traditional techniques and existing works that generate\nreal-valued embeddings by up to 5.16% in F1 score on user stitching, while\ntaking only up to 1.56% as much storage.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 02:32:47 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 16:55:06 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Jin", "Di", ""], ["Heimann", "Mark", ""], ["Rossi", "Ryan", ""], ["Koutra", "Danai", ""]]}, {"id": "1904.08587", "submitter": "Longqi Yang", "authors": "Longqi Yang, Chen Fang, Hailin Jin, Walter Chang, Deborah Estrin", "title": "Creative Procedural-Knowledge Extraction From Web Design Tutorials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex design tasks often require performing diverse actions in a specific\norder. To (semi-)autonomously accomplish these tasks, applications need to\nunderstand and learn a wide range of design procedures, i.e., Creative\nProcedural-Knowledge (CPK). Prior knowledge base construction and mining have\nnot typically addressed the creative fields, such as design and arts. In this\npaper, we formalize an ontology of CPK using five components: goal, workflow,\naction, command and usage; and extract components' values from online design\ntutorials. We scraped 19.6K tutorial-related webpages and built a web\napplication for professional designers to identify and summarize CPK\ncomponents. The annotated dataset consists of 819 unique commands, 47,491\nactions, and 2,022 workflows and goals. Based on this dataset, we propose a\ngeneral CPK extraction pipeline and demonstrate that existing text\nclassification and sequence-to-sequence models are limited in identifying,\npredicting and summarizing complex operations described in heterogeneous\nstyles. Through quantitative and qualitative error analysis, we discuss CPK\nextraction challenges that need to be addressed by future research.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 04:22:23 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Yang", "Longqi", ""], ["Fang", "Chen", ""], ["Jin", "Hailin", ""], ["Chang", "Walter", ""], ["Estrin", "Deborah", ""]]}, {"id": "1904.08623", "submitter": "Xie De", "authors": "Xianglong Liu, Lei Huang, Cheng Deng, Bo Lang, Dacheng Tao", "title": "Query-Adaptive Hash Code Ranking for Large-Scale Multi-View Visual\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash based nearest neighbor search has become attractive in many\napplications. However, the quantization in hashing usually degenerates the\ndiscriminative power when using Hamming distance ranking. Besides, for\nlarge-scale visual search, existing hashing methods cannot directly support the\nefficient search over the data with multiple sources, and while the literature\nhas shown that adaptively incorporating complementary information from diverse\nsources or views can significantly boost the search performance. To address the\nproblems, this paper proposes a novel and generic approach to building multiple\nhash tables with multiple views and generating fine-grained ranking results at\nbitwise and tablewise levels. For each hash table, a query-adaptive bitwise\nweighting is introduced to alleviate the quantization loss by simultaneously\nexploiting the quality of hash functions and their complement for nearest\nneighbor search. From the tablewise aspect, multiple hash tables are built for\ndifferent data views as a joint index, over which a query-specific rank fusion\nis proposed to rerank all results from the bitwise ranking by diffusing in a\ngraph. Comprehensive experiments on image search over three well-known\nbenchmarks show that the proposed method achieves up to 17.11% and 20.28%\nperformance gains on single and multiple table search over state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 07:49:26 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Liu", "Xianglong", ""], ["Huang", "Lei", ""], ["Deng", "Cheng", ""], ["Lang", "Bo", ""], ["Tao", "Dacheng", ""]]}, {"id": "1904.08689", "submitter": "Bj\\\"orn {\\TH}\\'or J\\'onsson", "authors": "Bj\\\"orn {\\TH}\\'or J\\'onsson, Omar Shahbaz Khan, Hanna Ragnarsd\\'ottir,\n  {\\TH}\\'orhildur {\\TH}orleiksd\\'ottir, Jan Zah\\'alka, Stevan Rudinac, Gylfi\n  {\\TH}\\'or Gu{\\dh}mundsson, Laurent Amsaleg, Marcel Worring", "title": "Exquisitor: Interactive Learning at Large", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing scale is a dominant trend in today's multimedia collections, which\nespecially impacts interactive applications. To facilitate interactive\nexploration of large multimedia collections, new approaches are needed that are\ncapable of learning on the fly new analytic categories based on the visual and\ntextual content. To facilitate general use on standard desktops, laptops, and\nmobile devices, they must furthermore work with limited computing resources. We\npresent Exquisitor, a highly scalable interactive learning approach, capable of\nintelligent exploration of the large-scale YFCC100M image collection with\nextremely efficient responses from the interactive classifier. Based on\nrelevance feedback from the user on previously suggested items, Exquisitor uses\nsemantic features, extracted from both visual and text attributes, to suggest\nrelevant media items to the user. Exquisitor builds upon the state of the art\nin large-scale data representation, compression and indexing, introducing a\ncluster-based retrieval mechanism that facilitates the efficient suggestions.\nWith Exquisitor, each interaction round over the full YFCC100M collection is\ncompleted in less than 0.3 seconds using a single CPU core. That is 4x less\ntime using 16x smaller computational resources than the most efficient\nstate-of-the-art method, with a positive impact on result quality. These\nresults open up many interesting research avenues, both for exploration of\nindustry-scale media collections and for media exploration on mobile devices.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 11:07:04 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 12:41:49 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2019 05:57:38 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["J\u00f3nsson", "Bj\u00f6rn \u00de\u00f3r", ""], ["Khan", "Omar Shahbaz", ""], ["Ragnarsd\u00f3ttir", "Hanna", ""], ["\u00deorleiksd\u00f3ttir", "\u00de\u00f3rhildur", ""], ["Zah\u00e1lka", "Jan", ""], ["Rudinac", "Stevan", ""], ["Gu\u00f0mundsson", "Gylfi \u00de\u00f3r", ""], ["Amsaleg", "Laurent", ""], ["Worring", "Marcel", ""]]}, {"id": "1904.08696", "submitter": "Jade Nardi", "authors": "Julien Lavauzelle and Jade Nardi", "title": "Weighted Lifted Codes: Local Correctabilities and Application to Robust\n  Private Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR math.AG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low degree Reed-Muller codes are known to satisfy local decoding properties\nwhich find applications in private information retrieval (PIR) protocols, for\ninstance. However, their practical instantiation encounters a first barrier due\nto their poor information rate in the low degree regime. This lead the\ncommunity to design codes with similar local properties but larger dimension,\nnamely the lifted Reed-Solomon codes.\n  However, a second practical barrier appears when one requires that the PIR\nprotocol resists collusions of servers. In this paper, we propose a solution to\nthis problem by considering \\emph{weighted} Reed-Muller codes. We prove that\nsuch codes allow us to build PIR protocols with optimal computation complexity\nand resisting to a small number of colluding servers.\n  In order to improve the dimension of the codes, we then introduce an analogue\nof the lifting process for weigthed degrees. With a careful analysis of their\ndegree sets, we notably show that the weighted lifting of Reed-Solomon codes\nproduces families of codes with remarkable asymptotic parameters.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 11:23:17 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Lavauzelle", "Julien", ""], ["Nardi", "Jade", ""]]}, {"id": "1904.08709", "submitter": "Simone Paolo Ponzetto", "authors": "Lydia Weiland, Ioana Hulpus, Simone Paolo Ponzetto, Wolfgang\n  Effelsberg, Laura Dietz", "title": "Knowledge-rich Image Gist Understanding Beyond Literal Meaning", "comments": null, "journal-ref": "Data & Knowledge Engineering, Volume 117, September 2018, Pages\n  114-132", "doi": "10.1016/j.datak.2018.07.006", "report-no": null, "categories": "cs.IR cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate the problem of understanding the message (gist) conveyed by\nimages and their captions as found, for instance, on websites or news articles.\nTo this end, we propose a methodology to capture the meaning of image-caption\npairs on the basis of large amounts of machine-readable knowledge that has\npreviously been shown to be highly effective for text understanding. Our method\nidentifies the connotation of objects beyond their denotation: where most\napproaches to image understanding focus on the denotation of objects, i.e.,\ntheir literal meaning, our work addresses the identification of connotations,\ni.e., iconic meanings of objects, to understand the message of images. We view\nimage understanding as the task of representing an image-caption pair on the\nbasis of a wide-coverage vocabulary of concepts such as the one provided by\nWikipedia, and cast gist detection as a concept-ranking problem with\nimage-caption pairs as queries. To enable a thorough investigation of the\nproblem of gist understanding, we produce a gold standard of over 300\nimage-caption pairs and over 8,000 gist annotations covering a wide variety of\ntopics at different levels of abstraction. We use this dataset to\nexperimentally benchmark the contribution of signals from heterogeneous\nsources, namely image and text. The best result with a Mean Average Precision\n(MAP) of 0.69 indicate that by combining both dimensions we are able to better\nunderstand the meaning of our image-caption pairs than when using language or\nvision information alone. We test the robustness of our gist detection approach\nwhen receiving automatically generated input, i.e., using automatically\ngenerated image tags or generated captions, and prove the feasibility of an\nend-to-end automated process.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 11:50:20 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Weiland", "Lydia", ""], ["Hulpus", "Ioana", ""], ["Ponzetto", "Simone Paolo", ""], ["Effelsberg", "Wolfgang", ""], ["Dietz", "Laura", ""]]}, {"id": "1904.08754", "submitter": "Fabio Giachelle", "authors": "Fabio Giachelle, Gianmaria Silvello", "title": "A Progressive Visual Analytics Tool for Incremental Experimental\n  Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a visual tool, AVIATOR, that integrates the progressive\nvisual analytics paradigm in the IR evaluation process. This tool serves to\nspeed-up and facilitate the performance assessment of retrieval models enabling\na result analysis through visual facilities. AVIATOR goes one step beyond the\ncommon \"compute wait visualize\" analytics paradigm, introducing a continuous\nevaluation mechanism that minimizes human and computational resource\nconsumption.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:16:53 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Giachelle", "Fabio", ""], ["Silvello", "Gianmaria", ""]]}, {"id": "1904.08770", "submitter": "Sandip Modha", "authors": "Sandip Modha and Prasenjit Majumder", "title": "An Empirical Evaluation of Text Representation Schemes on Multilingual\n  Social Web to Filter the Textual Aggression", "comments": "21 Page, 2 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper attempt to study the effectiveness of text representation schemes\non two tasks namely: User Aggression and Fact Detection from the social media\ncontents. In User Aggression detection, The aim is to identify the level of\naggression from the contents generated in the Social media and written in the\nEnglish, Devanagari Hindi and Romanized Hindi. Aggression levels are\ncategorized into three predefined classes namely: `Non-aggressive`, `Overtly\nAggressive`, and `Covertly Aggressive`. During the disaster-related incident,\nSocial media like, Twitter is flooded with millions of posts. In such emergency\nsituations, identification of factual posts is important for organizations\ninvolved in the relief operation. We anticipated this problem as a combination\nof classification and Ranking problem. This paper presents a comparison of\nvarious text representation scheme based on BoW techniques, distributed\nword/sentence representation, transfer learning on classifiers. Weighted $F_1$\nscore is used as a primary evaluation metric. Results show that text\nrepresentation using BoW performs better than word embedding on machine\nlearning classifiers. While pre-trained Word embedding techniques perform\nbetter on classifiers based on deep neural net. Recent transfer learning model\nlike ELMO, ULMFiT are fine-tuned for the Aggression classification task.\nHowever, results are not at par with pre-trained word embedding model. Overall,\nword embedding using fastText produce best weighted $F_1$-score than Word2Vec\nand Glove. Results are further improved using pre-trained vector model.\nStatistical significance tests are employed to ensure the significance of the\nclassification results. In the case of lexically different test Dataset, other\nthan training Dataset, deep neural models are more robust and perform\nsubstantially better than machine learning classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:10:52 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Modha", "Sandip", ""], ["Majumder", "Prasenjit", ""]]}, {"id": "1904.08842", "submitter": "Ruihan Yang", "authors": "Ruihan Yang, Tianyao Chen, Yiyi Zhang and Gus Xia", "title": "Inspecting and Interacting with Meaningful Music Representations using\n  VAE", "comments": "Accepted for poster at the International Conference on New Interfaces\n  for Musical Expression (NIME), June 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Autoencoders(VAEs) have already achieved great results on image\ngeneration and recently made promising progress on music generation. However,\nthe generation process is still quite difficult to control in the sense that\nthe learned latent representations lack meaningful music semantics. It would be\nmuch more useful if people can modify certain music features, such as rhythm\nand pitch contour, via latent representations to test different composition\nideas. In this paper, we propose a new method to inspect the pitch and rhythm\ninterpretations of the latent representations and we name it disentanglement by\naugmentation. Based on the interpretable representations, an intuitive\ngraphical user interface is designed for users to better direct the music\ncreation process by manipulating the pitch contours and rhythmic complexity.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 15:22:33 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Yang", "Ruihan", ""], ["Chen", "Tianyao", ""], ["Zhang", "Yiyi", ""], ["Xia", "Gus", ""]]}, {"id": "1904.08861", "submitter": "Jimmy Lin", "authors": "Jimmy Lin", "title": "The Simplest Thing That Can Possibly Work: Pseudo-Relevance Feedback\n  Using Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent commentary that has questioned today's pursuit of\never-more complex models and mathematical formalisms in applied machine\nlearning and whether meaningful empirical progress is actually being made, this\npaper tries to tackle the decades-old problem of pseudo-relevance feedback with\n\"the simplest thing that can possibly work\". I present a technique based on\ntraining a document relevance classifier for each information need using\npseudo-labels from an initial ranked list and then applying the classifier to\nrerank the retrieved documents. Experiments demonstrate significant\nimprovements across a number of newswire collections, with initial rankings\nsupplied by \"bag of words\" BM25 as well as from a well-tuned query expansion\nmodel. While this simple technique draws elements from several well-known\nthreads in the literature, to my knowledge this exact combination has not\npreviously been proposed and evaluated.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 16:06:31 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Lin", "Jimmy", ""]]}, {"id": "1904.09068", "submitter": "Liu Yang", "authors": "Liu Yang, Junjie Hu, Minghui Qiu, Chen Qu, Jianfeng Gao, W. Bruce\n  Croft, Xiaodong Liu, Yelong Shen, Jingjing Liu", "title": "A Hybrid Retrieval-Generation Neural Conversation Model", "comments": "Accepted as a Full Paper in CIKM 2019. 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent personal assistant systems that are able to have multi-turn\nconversations with human users are becoming increasingly popular. Most previous\nresearch has been focused on using either retrieval-based or generation-based\nmethods to develop such systems. Retrieval-based methods have the advantage of\nreturning fluent and informative responses with great diversity. However, the\nperformance of the methods is limited by the size of the response repository.\nOn the other hand, generation-based methods can produce highly coherent\nresponses on any topics. But the generated responses are often generic and not\ninformative due to the lack of grounding knowledge. In this paper, we propose a\nhybrid neural conversation model that combines the merits of both response\nretrieval and generation methods. Experimental results on Twitter and\nFoursquare data show that the proposed model outperforms both retrieval-based\nmethods and generation-based methods (including a recently proposed\nknowledge-grounded neural conversation model) under both automatic evaluation\nmetrics and human evaluation. We hope that the findings in this study provide\nnew insights on how to integrate text retrieval and text generation models for\nbuilding conversation systems.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 04:10:03 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 23:52:20 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Yang", "Liu", ""], ["Hu", "Junjie", ""], ["Qiu", "Minghui", ""], ["Qu", "Chen", ""], ["Gao", "Jianfeng", ""], ["Croft", "W. Bruce", ""], ["Liu", "Xiaodong", ""], ["Shen", "Yelong", ""], ["Liu", "Jingjing", ""]]}, {"id": "1904.09131", "submitter": "Antonin Delpeuch", "authors": "Antonin Delpeuch", "title": "OpenTapioca: Lightweight Entity Linking for Wikidata", "comments": "to appear in proceedings of the Wikidata Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a simple Named Entity Linking system that can be trained from\nWikidata only. This demonstrates the strengths and weaknesses of this data\nsource for this task and provides an easily reproducible baseline to compare\nother systems against. Our model is lightweight to train, to run and to keep\nsynchronous with Wikidata in real time.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 09:44:22 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 17:50:32 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Delpeuch", "Antonin", ""]]}, {"id": "1904.09171", "submitter": "Jimmy Lin", "authors": "Wei Yang, Kuang Lu, Peilin Yang, and Jimmy Lin", "title": "Critically Examining the \"Neural Hype\": Weak Baselines and the\n  Additivity of Effectiveness Gains from Neural Ranking Models", "comments": "Published in the Proceedings of the 42nd Annual International ACM\n  SIGIR Conference on Research and Development in Information Retrieval (SIGIR\n  2019)", "journal-ref": null, "doi": "10.1145/3331184.3331340", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is neural IR mostly hype? In a recent SIGIR Forum article, Lin expressed\nskepticism that neural ranking models were actually improving ad hoc retrieval\neffectiveness in limited data scenarios. He provided anecdotal evidence that\nauthors of neural IR papers demonstrate \"wins\" by comparing against weak\nbaselines. This paper provides a rigorous evaluation of those claims in two\nways: First, we conducted a meta-analysis of papers that have reported\nexperimental results on the TREC Robust04 test collection. We do not find\nevidence of an upward trend in effectiveness over time. In fact, the best\nreported results are from a decade ago and no recent neural approach comes\nclose. Second, we applied five recent neural models to rerank the strong\nbaselines that Lin used to make his arguments. A significant improvement was\nobserved for one of the models, demonstrating additivity in gains. While there\nappears to be merit to neural IR approaches, at least some of the gains\nreported in the literature appear illusory.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 12:44:00 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 01:33:07 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Yang", "Wei", ""], ["Lu", "Kuang", ""], ["Yang", "Peilin", ""], ["Lin", "Jimmy", ""]]}, {"id": "1904.09234", "submitter": "Adam Rambousek", "authors": "Adam Rambousek, Harry Parkin, Ales Horak", "title": "Software Tools for Big Data Resources in Family Names Dictionaries", "comments": "This is an Accepted Manuscript of an article published by Taylor &\n  Francis in Names on 09 Apr 2018, available online:\n  https://www.tandfonline.com/doi/full/10.1080/00277738.2018.1453276", "journal-ref": "Names, 66:4, 246-255 (2018)", "doi": "10.1080/00277738.2018.1453276", "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design and development of specific software tools\nused during the creation of Family Names in Britain and Ireland (FaNBI)\nresearch project, started by the University of the West of England in 2010 and\nfinished successfully in 2016. First, the overview of the project and\nmethodology is provided. Next section contains the description of dictionary\nmanagement tools and software tools to combine input data resources.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 08:25:40 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Rambousek", "Adam", ""], ["Parkin", "Harry", ""], ["Horak", "Ales", ""]]}, {"id": "1904.09557", "submitter": "Grace Lee", "authors": "Grace E. Lee and Aixin Sun", "title": "A Study on Agreement in PICO Span Annotations", "comments": "Accepted in SIGIR 2019 (Short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In evidence-based medicine, relevance of medical literature is determined by\npredefined relevance conditions. The conditions are defined based on PICO\nelements, namely, Patient, Intervention, Comparator, and Outcome. Hence, PICO\nannotations in medical literature are essential for automatic relevant document\nfiltering. However, defining boundaries of text spans for PICO elements is not\nstraightforward. In this paper, we study the agreement of PICO annotations made\nby multiple human annotators, including both experts and non-experts.\nAgreements are estimated by a standard span agreement (i.e., matching both\nlabels and boundaries of text spans), and two types of relaxed span agreement\n(i.e., matching labels without guaranteeing matching boundaries of spans).\nBased on the analysis, we report two observations: (i) Boundaries of PICO span\nannotations by individual human annotators are very diverse. (ii) Despite the\ndisagreement in span boundaries, general areas of the span annotations are\nbroadly agreed by annotators. Our results suggest that applying a standard\nagreement alone may undermine the agreement of PICO spans, and adopting both a\nstandard and a relaxed agreements is more suitable for PICO span evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 07:30:35 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Lee", "Grace E.", ""], ["Sun", "Aixin", ""]]}, {"id": "1904.09654", "submitter": "Maruthi Rohit Ayyagari", "authors": "Maruthi Rohit Ayyagari", "title": "Integrating Association Rules with Decision Trees in Object-Relational\n  Databases", "comments": "8 pages, 4 figures, 7 tables, journal", "journal-ref": "International Journal of Engineering Trends and Technology 67.3\n  (2019): 102-108", "doi": "10.14445/22312803/IJCTT-V67I3P120", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research has provided evidence that associative classification produces more\naccurate results compared to other classification models. The Classification\nBased on Association (CBA) is one of the famous Associative Classification\nalgorithms that generates accurate classifiers. However, current association\nclassification algorithms reside external to databases, which reduces the\nflexibility of enterprise analytics systems. This paper implements the CBA in\nOracle database using two variant models: hardcoding the CBA in Oracle Data\nMining (ODM) package and Integrating Oracle Apriori model with the Oracle\nDecision tree model. We compared the proposed model performance with Naive\nBayes, Support Vector Machine, Random Forests, and Decision Tree over 18\ndatasets from UCI. Results showed that our models outperformed the original CBA\nmodel with 1 percent and is competitive to chosen classification models over\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 20:04:58 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Ayyagari", "Maruthi Rohit", ""]]}, {"id": "1904.09671", "submitter": "Rami Al-Rfou", "authors": "Rami Al-Rfou and Dustin Zelle and Bryan Perozzi", "title": "DDGK: Learning Graph Representations for Deep Divergence Graph Kernels", "comments": "www '19", "journal-ref": "Proceedings of the 2019 World Wide Web Conference (WWW '19), May\n  13--17, 2019, San Francisco, CA, USA", "doi": "10.1145/3308558.3313668", "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can neural networks learn to compare graphs without feature engineering? In\nthis paper, we show that it is possible to learn representations for graph\nsimilarity with neither domain knowledge nor supervision (i.e.\\ feature\nengineering or labeled graphs). We propose Deep Divergence Graph Kernels, an\nunsupervised method for learning representations over graphs that encodes a\nrelaxed notion of graph isomorphism. Our method consists of three parts. First,\nwe learn an encoder for each anchor graph to capture its structure. Second, for\neach pair of graphs, we train a cross-graph attention network which uses the\nnode representations of an anchor graph to reconstruct another graph. This\napproach, which we call isomorphism attention, captures how well the\nrepresentations of one graph can encode another. We use the attention-augmented\nencoder's predictions to define a divergence score for each pair of graphs.\nFinally, we construct an embedding space for all graphs using these pair-wise\ndivergence scores.\n  Unlike previous work, much of which relies on 1) supervision, 2) domain\nspecific knowledge (e.g. a reliance on Weisfeiler-Lehman kernels), and 3) known\nnode alignment, our unsupervised method jointly learns node representations,\ngraph representations, and an attention-based alignment between graphs.\n  Our experimental results show that Deep Divergence Graph Kernels can learn an\nunsupervised alignment between graphs, and that the learned representations\nachieve competitive results when used as features on a number of challenging\ngraph classification tasks. Furthermore, we illustrate how the learned\nattention allows insight into the the alignment of sub-structures across\ngraphs.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 22:45:04 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Al-Rfou", "Rami", ""], ["Zelle", "Dustin", ""], ["Perozzi", "Bryan", ""]]}, {"id": "1904.10169", "submitter": "Vahid Ranjbar", "authors": "Shaghayegh Najari, Mostafa Salehi, Vahid Ranjbar, Mahdi Jalili", "title": "Link Prediction in Multiplex Networks based on Interlayer Similarity", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2019.04.214", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some networked systems can be better modelled by multilayer structure where\nthe individual nodes develop relationships in multiple layers. Multilayer\nnetworks with similar nodes across layers are also known as multiplex networks.\nThis manuscript proposes a novel framework for predicting forthcoming or\nmissing links in multiplex networks. The link prediction problem in multiplex\nnetworks is how to predict links in one of the layers, taking into account the\nstructural information of other layers. The proposed link prediction framework\nis based on interlayer similarity and proximity-based features extracted from\nthe layer for which the link prediction is considered. To this end, commonly\nused proximity-based features such as Adamic-Adar and Jaccard Coefficient are\nconsidered. These features that have been originally proposed to predict\nmissing links in monolayer networks, do not require learning, and thus are\nsimple to compute. The proposed method introduces a systematic approach to take\ninto account interlayer similarity for the link prediction purpose.\nExperimental results on both synthetic and real multiplex networks reveal the\neffectiveness of the proposed method and show its superior performance than\nstate-of-the-art algorithms proposed for the link prediction problem in\nmultiplex networks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 06:18:55 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 19:47:46 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Najari", "Shaghayegh", ""], ["Salehi", "Mostafa", ""], ["Ranjbar", "Vahid", ""], ["Jalili", "Mahdi", ""]]}, {"id": "1904.10273", "submitter": "Sainath Adapa", "authors": "Sainath Adapa", "title": "Sequential modeling of Sessions using Recurrent Neural Networks for Skip\n  Prediction", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems play an essential role in music streaming services,\nprominently in the form of personalized playlists. Exploring the user\ninteractions within these listening sessions can be beneficial to understanding\nthe user preferences in the context of a single session. In the 'Spotify\nSequential Skip Prediction Challenge', WSDM, and Spotify are challenging people\nto understand the way users sequentially interact with music. We describe our\nsolution approach in this paper and also state proposals for further\nimprovements to the model. The proposed model initially generates a fixed\nvector representation of the session, and this additional information is\nincorporated into an Encoder-Decoder style architecture. This method achieved\nthe seventh position in the competition, with a mean average accuracy of 0.604\non the test set. The solution code is available at\nhttps://github.com/sainathadapa/spotify-sequential-skip-prediction.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 12:26:24 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Adapa", "Sainath", ""]]}, {"id": "1904.10322", "submitter": "Peijie Sun", "authors": "Le Wu, Peijie Sun, Yanjie Fu, Richang Hong, Xiting Wang and Meng Wang", "title": "A Neural Influence Diffusion Model for Social Recommendation", "comments": "10 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1811.02815", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise user and item embedding learning is the key to building a successful\nrecommender system. Traditionally, Collaborative Filtering(CF) provides a way\nto learn user and item embeddings from the user-item interaction history.\nHowever, the performance is limited due to the sparseness of user behavior\ndata. With the emergence of online social networks, social recommender systems\nhave been proposed to utilize each user's local neighbors' preferences to\nalleviate the data sparsity for better user embedding modeling. We argue that,\nfor each user of a social platform, her potential embedding is influenced by\nher trusted users. As social influence recursively propagates and diffuses in\nthe social network, each user's interests change in the recursive process.\nNevertheless, the current social recommendation models simply developed static\nmodels by leveraging the local neighbors of each user without simulating the\nrecursive diffusion in the global social network, leading to suboptimal\nrecommendation performance. In this paper, we propose a deep influence\npropagation model to stimulate how users are influenced by the recursive social\ndiffusion process for social recommendation. For each user, the diffusion\nprocess starts with an initial embedding that fuses the related features and a\nfree user latent vector that captures the latent behavior preference. The key\nidea of our proposed model is that we design a layer-wise influence propagation\nstructure to model how users' latent embeddings evolve as the social diffusion\nprocess continues. We further show that our proposed model is general and could\nbe applied when the user~(item) attributes or the social network structure is\nnot available. Finally, extensive experimental results on two real-world\ndatasets clearly show the effectiveness of our proposed model, with more than\n13% performance improvements over the best baselines.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 16:52:18 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Wu", "Le", ""], ["Sun", "Peijie", ""], ["Fu", "Yanjie", ""], ["Hong", "Richang", ""], ["Wang", "Xiting", ""], ["Wang", "Meng", ""]]}, {"id": "1904.10367", "submitter": "Gabriel de Souza Pereira Moreira", "authors": "Gabriel de Souza Pereira Moreira, Dietmar Jannach, Adilson Marques da\n  Cunha", "title": "Contextual Hybrid Session-based News Recommendation with Recurrent\n  Neural Networks", "comments": "20 pgs. Published at IEEE Access, Volume 7, 2019.\n  https://ieeexplore.ieee.org/document/8908688", "journal-ref": "IEEE Access 7 (2019): 169185-169203", "doi": "10.1109/ACCESS.2019.2954957", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems help users deal with information overload by providing\ntailored item suggestions to them. The recommendation of news is often\nconsidered to be challenging, since the relevance of an article for a user can\ndepend on a variety of factors, including the user's short-term reading\ninterests, the reader's context, or the recency or popularity of an article.\nPrevious work has shown that the use of Recurrent Neural Networks is promising\nfor the next-in-session prediction task, but has certain limitations when only\nrecorded item click sequences are used as input. In this work, we present a\ncontextual hybrid, deep learning based approach for session-based news\nrecommendation that is able to leverage a variety of information types. We\nevaluated our approach on two public datasets, using a temporal evaluation\nprotocol that simulates the dynamics of a news portal in a realistic way. Our\nresults confirm the benefits of considering additional types of information,\nincluding article popularity and recency, in the proposed way, resulting in\nsignificantly higher recommendation accuracy and catalog coverage than other\nsession-based algorithms. Additional experiments show that the proposed\nparameterizable loss function used in our method also allows us to balance two\nusually conflicting quality factors, accuracy and novelty.\n  Keywords: Artificial Neural Networks, Context-Aware Recommender Systems,\nHybrid Recommender Systems, News Recommender Systems, Session-based\nRecommendation\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 21:47:43 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 16:56:09 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Moreira", "Gabriel de Souza Pereira", ""], ["Jannach", "Dietmar", ""], ["da Cunha", "Adilson Marques", ""]]}, {"id": "1904.10403", "submitter": "Kasra Safari", "authors": "Kasra Safari, Scott Sanner", "title": "Optimizing Search API Queries for Twitter Topic Classifiers Using a\n  Maximum Set Coverage Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter has grown to become an important platform to access immediate\ninformation about major events and dynamic topics. As one example, recent work\nhas shown that classifiers trained to detect topical content on Twitter can\ngeneralize well beyond the training data. Since access to Twitter data is\nhidden behind a limited search API, it is impossible (for most users) to apply\nthese classifiers directly to the Twitter unfiltered data streams (\"firehose\").\nRather, applications must first decide what content to retrieve through the\nsearch API before filtering that content with topical classifiers. Thus, it is\ncritically important to query the Twitter API relative to the intended topical\nclassifier in a way that minimizes the amount of negatively classified data\nretrieved. In this paper, we propose a sequence of query optimization methods\nthat generalize notions of the maximum coverage problem to find the subset of\nquery terms within the API limits that cover most of the topically relevant\ntweets without sacrificing precision. We evaluate the proposed methods on a\nlarge dataset of Twitter data collected during 2013 and 2014 labeled using\nmanually curated hashtags for eight topics. Among many insights, our analysis\nshows that the best of the proposed methods can significantly outperform the\nfirehose on precision and F1-score while achieving high recall within strict\nAPI limitations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 16:13:35 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 23:54:40 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Safari", "Kasra", ""], ["Sanner", "Scott", ""]]}, {"id": "1904.10503", "submitter": "Michael Sigamani", "authors": "Cihan Dogan, Aimore Dutra, Adam Gara, Alfredo Gemma, Lei Shi, Michael\n  Sigamani, Ella Walters", "title": "Fine-Grained Named Entity Recognition using ELMo and Wikidata", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained Named Entity Recognition is a task whereby we detect and\nclassify entity mentions to a large set of types. These types can span diverse\ndomains such as finance, healthcare, and politics. We observe that when the\ntype set spans several domains the accuracy of the entity detection becomes a\nlimitation for supervised learning models. The primary reason being the lack of\ndatasets where entity boundaries are properly annotated, whilst covering a\nlarge spectrum of entity types. Furthermore, many named entity systems suffer\nwhen considering the categorization of fine grained entity types. Our work\nattempts to address these issues, in part, by combining state-of-the-art deep\nlearning models (ELMo) with an expansive knowledge base (Wikidata). Using our\nframework, we cross-validate our model on the 112 fine-grained entity types\nbased on the hierarchy given from the Wiki(gold) dataset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 19:18:26 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Dogan", "Cihan", ""], ["Dutra", "Aimore", ""], ["Gara", "Adam", ""], ["Gemma", "Alfredo", ""], ["Shi", "Lei", ""], ["Sigamani", "Michael", ""], ["Walters", "Ella", ""]]}, {"id": "1904.10522", "submitter": "Hyunsu Cho", "authors": "Theodore Vasiloudis, Hyunsu Cho, Henrik Bostr\\\"om", "title": "Block-distributed Gradient Boosted Trees", "comments": "SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gradient Boosted Tree (GBT) algorithm is one of the most popular machine\nlearning algorithms used in production, for tasks that include Click-Through\nRate (CTR) prediction and learning-to-rank. To deal with the massive datasets\navailable today, many distributed GBT methods have been proposed. However, they\nall assume a row-distributed dataset, addressing scalability only with respect\nto the number of data points and not the number of features, and increasing\ncommunication cost for high-dimensional data. In order to allow for scalability\nacross both the data point and feature dimensions, and reduce communication\ncost, we propose block-distributed GBTs. We achieve communication efficiency by\nmaking full use of the data sparsity and adapting the Quickscorer algorithm to\nthe block-distributed setting. We evaluate our approach using datasets with\nmillions of features, and demonstrate that we are able to achieve multiple\norders of magnitude reduction in communication cost for sparse data, with no\nloss in accuracy, while providing a more scalable design. As a result, we are\nable to reduce the training time for high-dimensional data, and allow more\ncost-effective scale-out without the need for expensive network communication.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 20:10:36 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 19:32:35 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Vasiloudis", "Theodore", ""], ["Cho", "Hyunsu", ""], ["Bostr\u00f6m", "Henrik", ""]]}, {"id": "1904.10527", "submitter": "Guy Aridor", "authors": "Guy Aridor, Duarte Goncalves, Shan Sikdar", "title": "Deconstructing the Filter Bubble: User Decision-Making and Recommender\n  Systems", "comments": "preprint for ACM RecSys '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a model of user decision-making in the context of recommender\nsystems via numerical simulation. Our model provides an explanation for the\nfindings of Nguyen, et. al (2014), where, in environments where recommender\nsystems are typically deployed, users consume increasingly similar items over\ntime even without recommendation. We find that recommendation alleviates these\nnatural filter-bubble effects, but that it also leads to an increase in\nhomogeneity across users, resulting in a trade-off between homogenizing\nacross-user consumption and diversifying within-user consumption. Finally, we\ndiscuss how our model highlights the importance of collecting data on user\nbeliefs and their evolution over time both to design better recommendations and\nto further understand their impact.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 20:28:52 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 16:39:16 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 16:12:30 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Aridor", "Guy", ""], ["Goncalves", "Duarte", ""], ["Sikdar", "Shan", ""]]}, {"id": "1904.10743", "submitter": "Jiyu Chen", "authors": "Jiyu Chen, Karin Verspoor, Zenan Zhai", "title": "A bag-of-concepts model improves relation extraction in a narrow\n  knowledge domain with limited data", "comments": "To appear in Proceedings of the Student Research Workshop at the\n  North American Association for Computational Linguistics (NAACL) meeting 2019", "journal-ref": "In Proceedings of the Student Research Workshop at North American\n  Association for Computational Linguistics (NAACL) 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on a traditional relation extraction task in the context\nof limited annotated data and a narrow knowledge domain. We explore this task\nwith a clinical corpus consisting of 200 breast cancer follow-up treatment\nletters in which 16 distinct types of relations are annotated. We experiment\nwith an approach to extracting typed relations called window-bounded\nco-occurrence (WBC), which uses an adjustable context window around entity\nmentions of a relevant type, and compare its performance with a more typical\nintra-sentential co-occurrence baseline. We further introduce a new\nbag-of-concepts (BoC) approach to feature engineering based on the\nstate-of-the-art word embeddings and word synonyms. We demonstrate the\ncompetitiveness of BoC by comparing with methods of higher complexity, and\nexplore its effectiveness on this small dataset.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 11:06:54 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Chen", "Jiyu", ""], ["Verspoor", "Karin", ""], ["Zhai", "Zenan", ""]]}, {"id": "1904.10784", "submitter": "David Rohde", "authors": "David Rohde, Stephen Bonner", "title": "Latent Variable Session-Based Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session based recommendation provides an attractive alternative to the\ntraditional feature engineering approach to recommendation. Feature engineering\napproaches require hand tuned features of the users history to be created to\nproduce a context vector. In contrast a session based approach is able to\ndynamically model the users state as they act. We present a probabilistic\nframework for session based recommendation. A latent variable for the user\nstate is updated as the user views more items and we learn more about their\ninterests. The latent variable model is conceptually simple and elegant; yet\nrequires sophisticated computational technique to approximate the integral over\nthe latent variable. We provide computational solutions using both the\nre-parameterization trick and also using the Bouchard bound for the softmax\nfunction, we further explore employing a variational auto-encoder and a\nvariational Expectation-Maximization algorithm for tightening the variational\nbound. The model performs well against a number of baselines. The intuitive\nnature of the model allows an elegant formulation combining correlations\nbetween items and their popularity and that sheds light on other popular\nrecommendation methods. An attractive feature of the latent variable approach\nis that, as the user continues to act, the posterior on the user's state\ntightens reflecting the recommender system's increased knowledge about that\nuser.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 13:10:38 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 15:21:49 GMT"}, {"version": "v3", "created": "Tue, 17 Sep 2019 16:41:29 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Rohde", "David", ""], ["Bonner", "Stephen", ""]]}, {"id": "1904.10799", "submitter": "David Rohde", "authors": "Dmytro Mykhaylov, David Rohde, Flavian Vasile, Martin Bompaire,\n  Olivier Jeunen", "title": "Three Methods for Training on Bandit Feedback", "comments": "5 pages 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are three quite distinct ways to train a machine learning model on\nrecommender system logs. The first method is to model the reward prediction for\neach possible recommendation to the user, at the scoring time the best\nrecommendation is found by computing an argmax over the personalized\nrecommendations. This method obeys principles such as the conditionality\nprinciple and the likelihood principle. A second method is useful when the\nmodel does not fit reality and underfits. In this case, we can use the fact\nthat we know the distribution of historical recommendations (concentrated on\npreviously identified good actions with some exploration) to adjust the errors\nin the fit to be evenly distributed over all actions. Finally, the inverse\npropensity score can be used to produce an estimate of the decision rules\nexpected performance. The latter two methods violate the conditionality and\nlikelihood principle but are shown to have good performance in certain\nsettings. In this paper we review the literature around this fundamental, yet\noften overlooked choice and do some experiments using the RecoGym simulation\nenvironment.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 13:23:18 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 16:26:20 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Mykhaylov", "Dmytro", ""], ["Rohde", "David", ""], ["Vasile", "Flavian", ""], ["Bompaire", "Martin", ""], ["Jeunen", "Olivier", ""]]}, {"id": "1904.10876", "submitter": "Valerio Lorini", "authors": "V. Lorini (European Commission, Joint Research Centre (JRC), Ispra,\n  Italy, Universitat Pompeu Fabra, Barcelona, Spain), C. Castillo (Universitat\n  Pompeu Fabra, Barcelona, Spain), F. Dottori (European Commission, Joint\n  Research Centre (JRC), Ispra, Italy), M. Kalas (KAJO, Bytca, Slovakia), D.\n  Nappo (European Commission, Joint Research Centre (JRC), Ispra, Italy), P.\n  Salamon (European Commission, Joint Research Centre (JRC), Ispra, Italy)", "title": "Integrating Social Media into a Pan-European Flood Awareness System: A\n  Multilingual Approach", "comments": "accepted at ISCRAM2019 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes a prototype system that integrates social media analysis\ninto the European Flood Awareness System (EFAS). This integration allows the\ncollection of social media data to be automatically triggered by flood risk\nwarnings determined by a hydro-meteorological model. Then, we adopt a\nmulti-lingual approach to find flood-related messages by employing two\nstate-of-the-art methodologies: language-agnostic word embeddings and\nlanguage-aligned word embeddings. Both approaches can be used to bootstrap a\nclassifier of social media messages for a new language with little or no\nlabeled data. Finally, we describe a method for selecting relevant and\nrepresentative messages and displaying them back in the interface of EFAS.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 15:40:14 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Lorini", "V.", "", "European Commission, Joint Research Centre"], ["Castillo", "C.", "", "Universitat\n  Pompeu Fabra, Barcelona, Spain"], ["Dottori", "F.", "", "European Commission, Joint\n  Research Centre"], ["Kalas", "M.", "", "KAJO, Bytca, Slovakia"], ["Nappo", "D.", "", "European Commission, Joint Research Centre"], ["Salamon", "P.", "", "European Commission, Joint Research Centre"]]}, {"id": "1904.11171", "submitter": "Lei Zhu", "authors": "Li Wang, Lei Zhu, En Yu, Jiande Sun, Huaxiang Zhang", "title": "Fusion-supervised Deep Cross-modal Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep hashing has recently received attention in cross-modal retrieval for its\nimpressive advantages. However, existing hashing methods for cross-modal\nretrieval cannot fully capture the heterogeneous multi-modal correlation and\nexploit the semantic information. In this paper, we propose a novel\n\\emph{Fusion-supervised Deep Cross-modal Hashing} (FDCH) approach. Firstly,\nFDCH learns unified binary codes through a fusion hash network with paired\nsamples as input, which effectively enhances the modeling of the correlation of\nheterogeneous multi-modal data. Then, these high-quality unified hash codes\nfurther supervise the training of the modality-specific hash networks for\nencoding out-of-sample queries. Meanwhile, both pair-wise similarity\ninformation and classification information are embedded in the hash networks\nunder one stream framework, which simultaneously preserves cross-modal\nsimilarity and keeps semantic consistency. Experimental results on two\nbenchmark datasets demonstrate the state-of-the-art performance of FDCH.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 06:43:27 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 10:04:12 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Wang", "Li", ""], ["Zhu", "Lei", ""], ["Yu", "En", ""], ["Sun", "Jiande", ""], ["Zhang", "Huaxiang", ""]]}, {"id": "1904.11207", "submitter": "Lei Zhu", "authors": "Lei Zhu, Zi Huang, Zhihui Li, Liang Xie, Heng Tao Shen", "title": "Exploring Auxiliary Context: Discrete Semantic Transfer Hashing for\n  Scalable Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised hashing can desirably support scalable content-based image\nretrieval (SCBIR) for its appealing advantages of semantic label independence,\nmemory and search efficiency. However, the learned hash codes are embedded with\nlimited discriminative semantics due to the intrinsic limitation of image\nrepresentation. To address the problem, in this paper, we propose a novel\nhashing approach, dubbed as \\emph{Discrete Semantic Transfer Hashing} (DSTH).\nThe key idea is to \\emph{directly} augment the semantics of discrete image hash\ncodes by exploring auxiliary contextual modalities. To this end, a unified\nhashing framework is formulated to simultaneously preserve visual similarities\nof images and perform semantic transfer from contextual modalities. Further, to\nguarantee direct semantic transfer and avoid information loss, we explicitly\nimpose the discrete constraint, bit--uncorrelation constraint and bit-balance\nconstraint on hash codes. A novel and effective discrete optimization method\nbased on augmented Lagrangian multiplier is developed to iteratively solve the\noptimization problem. The whole learning process has linear computation\ncomplexity and desirable scalability. Experiments on three benchmark datasets\ndemonstrate the superiority of DSTH compared with several state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 08:37:20 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Zhu", "Lei", ""], ["Huang", "Zi", ""], ["Li", "Zhihui", ""], ["Xie", "Liang", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1904.11228", "submitter": "Lei Zhu", "authors": "Xiao Dong, Lei Zhu, Xuemeng Song, Jingjing Li, Zhiyong Cheng", "title": "Adaptive Collaborative Similarity Learning for Unsupervised Multi-view\n  Feature Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate the research problem of unsupervised multi-view\nfeature selection. Conventional solutions first simply combine multiple\npre-constructed view-specific similarity structures into a collaborative\nsimilarity structure, and then perform the subsequent feature selection. These\ntwo processes are separate and independent. The collaborative similarity\nstructure remains fixed during feature selection. Further, the simple\nundirected view combination may adversely reduce the reliability of the\nultimate similarity structure for feature selection, as the view-specific\nsimilarity structures generally involve noises and outlying entries. To\nalleviate these problems, we propose an adaptive collaborative similarity\nlearning (ACSL) for multi-view feature selection. We propose to dynamically\nlearn the collaborative similarity structure, and further integrate it with the\nultimate feature selection into a unified framework. Moreover, a reasonable\nrank constraint is devised to adaptively learn an ideal collaborative\nsimilarity structure with proper similarity combination weights and desirable\nneighbor assignment, both of which could positively facilitate the feature\nselection. An effective solution guaranteed with the proved convergence is\nderived to iteratively tackle the formulated optimization problem. Experiments\ndemonstrate the superiority of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 09:21:31 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Dong", "Xiao", ""], ["Zhu", "Lei", ""], ["Song", "Xuemeng", ""], ["Li", "Jingjing", ""], ["Cheng", "Zhiyong", ""]]}, {"id": "1904.11266", "submitter": "Lei Zhu", "authors": "Yudong Han, Lei Zhu, Zhiyong Cheng, Jingjing Li, Xiaobai Liu", "title": "Discrete Optimal Graph Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph based clustering is one of the major clustering methods. Most of it\nwork in three separate steps: similarity graph construction, clustering label\nrelaxing and label discretization with k-means. Such common practice has three\ndisadvantages: 1) the predefined similarity graph is often fixed and may not be\noptimal for the subsequent clustering. 2) the relaxing process of cluster\nlabels may cause significant information loss. 3) label discretization may\ndeviate from the real clustering result since k-means is sensitive to the\ninitialization of cluster centroids. To tackle these problems, in this paper,\nwe propose an effective discrete optimal graph clustering (DOGC) framework. A\nstructured similarity graph that is theoretically optimal for clustering\nperformance is adaptively learned with a guidance of reasonable rank\nconstraint. Besides, to avoid the information loss, we explicitly enforce a\ndiscrete transformation on the intermediate continuous label, which derives a\ntractable optimization problem with discrete solution. Further, to compensate\nthe unreliability of the learned labels and enhance the clustering accuracy, we\ndesign an adaptive robust module that learns prediction function for the unseen\ndata based on the learned discrete cluster labels. Finally, an iterative\noptimization strategy guaranteed with convergence is developed to directly\nsolve the clustering results. Extensive experiments conducted on both real and\nsynthetic datasets demonstrate the superiority of our proposed methods compared\nwith several state-of-the-art clustering approaches.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 11:31:29 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Han", "Yudong", ""], ["Zhu", "Lei", ""], ["Cheng", "Zhiyong", ""], ["Li", "Jingjing", ""], ["Liu", "Xiaobai", ""]]}, {"id": "1904.11388", "submitter": "Bharat Gaind", "authors": "Bharat Gaind, Nitish Varshney, Shubham Goel, Akash Mondal", "title": "Identifying short-term interests from mobile app adoption pattern", "comments": "Accepted and presented in the 20th International Conference on\n  Computational Linguistics and Intelligent Text Processing, France, 2019 and\n  soon to be published in the Computaci\\'on y Sistemas (Scopus-indexed) journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase in an average user's dependence on their mobile devices,\nthe reliance on collecting his browsing history from mobile browsers has also\nincreased. This browsing history is highly utilized in the advertising industry\nfor providing targeted ads in the purview of inferring his short-term interests\nand pushing relevant ads. However, the major limitation of such an extraction\nfrom mobile browsers is that they reset when the browser is closed or when the\ndevice is shut down/restarted; thus rendering existing methods to identify the\nuser's short-term interests on mobile devices users, ineffective. In this\npaper, we propose an alternative method to identify such short-term interests\nby analysing their mobile app adoption (installation/uninstallation) patterns\nover a period of time. Such a method can be highly effective in pinpointing the\nuser's ephemeral inclinations like buying/renting an apartment, buying/selling\na car or a sudden increased interest in shopping (possibly due to a recent\nsalary bonus, he received). Subsequently, these derived interests are also used\nfor targeted experiments. Our experiments result in up to 93.68% higher\nclick-through rate in comparison to the ads shown without any user-interest\nknowledge. Also, up to 51% higher revenue in the long term is expected as a\nresult of the application of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 14:44:04 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Gaind", "Bharat", ""], ["Varshney", "Nitish", ""], ["Goel", "Shubham", ""], ["Mondal", "Akash", ""]]}, {"id": "1904.11547", "submitter": "Feiyang Pan", "authors": "Feiyang Pan, Shuokai Li, Xiang Ao, Pingzhong Tang, Qing He", "title": "Warm Up Cold-start Advertisements: Improving CTR Predictions via\n  Learning to Learn ID Embeddings", "comments": "Accepted at SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) prediction has been one of the most central problems\nin computational advertising. Lately, embedding techniques that produce\nlow-dimensional representations of ad IDs drastically improve CTR prediction\naccuracies. However, such learning techniques are data demanding and work\npoorly on new ads with little logging data, which is known as the cold-start\nproblem.\n  In this paper, we aim to improve CTR predictions during both the cold-start\nphase and the warm-up phase when a new ad is added to the candidate pool. We\npropose Meta-Embedding, a meta-learning-based approach that learns to generate\ndesirable initial embeddings for new ad IDs. The proposed method trains an\nembedding generator for new ad IDs by making use of previously learned ads\nthrough gradient-based meta-learning. In other words, our method learns how to\nlearn better embeddings. When a new ad comes, the trained generator initializes\nthe embedding of its ID by feeding its contents and attributes. Next, the\ngenerated embedding can speed up the model fitting during the warm-up phase\nwhen a few labeled examples are available, compared to the existing\ninitialization methods.\n  Experimental results on three real-world datasets showed that Meta-Embedding\ncan significantly improve both the cold-start and warm-up performances for six\nexisting CTR prediction models, ranging from lightweight models such as\nFactorization Machines to complicated deep models such as PNN and DeepFM. All\nof the above apply to conversion rate (CVR) predictions as well.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 19:26:42 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Pan", "Feiyang", ""], ["Li", "Shuokai", ""], ["Ao", "Xiang", ""], ["Tang", "Pingzhong", ""], ["He", "Qing", ""]]}, {"id": "1904.11797", "submitter": "Elisa Claire Alem\\'an Carre\\'on", "authors": "Hiroki Horino, Hirofumi Nonaka, Elisa Claire Alem\\'an Carre\\'on and\n  Toru Hiraoka", "title": "Development of an Entropy-Based Feature Selection Method and Analysis of\n  Online Reviews on Real Estate", "comments": null, "journal-ref": "In proceedings of the 2017 IEEE International Conference on\n  Industrial Engineering & Engineering Management (2017 IEEE IEEM). pp. 2351 -\n  2355. Singapore, (2017, December, 12)", "doi": "10.1109/IEEM.2017.8290312", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, data posted about real estate on the Internet is currently\nincreasing. In this study, in order to analyze user needs for real estate, we\nfocus on \"Mansion Community\" which is a Japanese bulletin board system\n(hereinafter referred to as BBS) about Japanese real estate. In our study,\nextraction of keywords is performed based on the calculation of the entropy\nvalue of each word, and we used them as features in a machine learning\nclassifier to analyze 6 million posts at \"Mansion Community\". As a result, we\nachieved a 0.69 F-measure and found that the customers are particularly\nconcerned about the facility of apartment, access, and price of an apartment.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 15:48:10 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Horino", "Hiroki", ""], ["Nonaka", "Hirofumi", ""], ["Carre\u00f3n", "Elisa Claire Alem\u00e1n", ""], ["Hiraoka", "Toru", ""]]}, {"id": "1904.11798", "submitter": "Sara Morsy", "authors": "Sara Morsy and George Karypis", "title": "Will this Course Increase or Decrease Your GPA? Towards Grade-aware\n  Course Recommendation", "comments": "Under revision for Journal of Educational Data Mining (JEDM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to help undergraduate students towards successfully completing their\ndegrees, developing tools that can assist students during the course selection\nprocess is a significant task in the education domain. The optimal set of\ncourses for each student should include courses that help him/her graduate in a\ntimely fashion and for which he/she is well-prepared for so as to get a good\ngrade in. To this end, we propose two different grade-aware course\nrecommendation approaches to recommend to each student his/her optimal set of\ncourses. The first approach ranks the courses by using an objective function\nthat differentiates between courses that are expected to increase or decrease a\nstudent's GPA. The second approach combines the grades predicted by grade\nprediction methods with the rankings produced by course recommendation methods\nto improve the final course rankings. To obtain the course rankings in the\nfirst approach, we adapt two widely-used representation learning techniques to\nlearn the optimal temporal ordering between courses. Our experiments on a large\ndataset obtained from the University of Minnesota that includes students from\n23 different majors show that the grade-aware course recommendation methods can\ndo better on recommending more courses in which the students are expected to\nperform well and recommending fewer courses in which they are expected not to\nperform well in than grade-unaware course recommendation methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 21:27:42 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Morsy", "Sara", ""], ["Karypis", "George", ""]]}, {"id": "1904.11799", "submitter": "Mohit Sharma", "authors": "Mohit Sharma, Jiayu Zhou, Junling Hu, George Karypis", "title": "Feature-based factorized Bilinear Similarity Model for Cold-Start Top-n\n  Item Recommendation", "comments": "9 pages, Proceedings of the 2015 SIAM International Conference on\n  Data Mining", "journal-ref": null, "doi": "10.1137/1.9781611974010.22", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending new items to existing users has remained a challenging problem\ndue to absence of user's past preferences for these items. The user\npersonalized non-collaborative methods based on item features can be used to\naddress this item cold-start problem. These methods rely on similarities\nbetween the target item and user's previous preferred items. While computing\nsimilarities based on item features, these methods overlook the interactions\namong the features of the items and consider them independently. Modeling\ninteractions among features can be helpful as some features, when considered\ntogether, provide a stronger signal on the relevance of an item when compared\nto case where features are considered independently. To address this important\nissue, in this work we introduce the Feature-based factorized Bilinear\nSimilarity Model (FBSM), which learns factorized bilinear similarity model for\nTOP-n recommendation of new items, given the information about items preferred\nby users in past as well as the features of these items. We carry out extensive\nempirical evaluations on benchmark datasets, and we find that the proposed FBSM\napproach improves upon traditional non-collaborative methods in terms of\nrecommendation performance. Moreover, the proposed approach also learns\ninsightful interactions among item features from data, which lead to deep\nunderstanding on how these interactions contribute to personalized\nrecommendation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 05:10:48 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Sharma", "Mohit", ""], ["Zhou", "Jiayu", ""], ["Hu", "Junling", ""], ["Karypis", "George", ""]]}, {"id": "1904.11800", "submitter": "Mohit Sharma", "authors": "Mohit Sharma, and George Karypis", "title": "Adaptive Matrix Completion for the Users and the Items in Tail", "comments": "7 pages, 3 figures, ACM WWW'19", "journal-ref": null, "doi": "10.1145/3308558.3313736", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are widely used to recommend the most appealing items to\nusers. These recommendations can be generated by applying collaborative\nfiltering methods. The low-rank matrix completion method is the\nstate-of-the-art collaborative filtering method. In this work, we show that the\nskewed distribution of ratings in the user-item rating matrix of real-world\ndatasets affects the accuracy of matrix-completion-based approaches. Also, we\nshow that the number of ratings that an item or a user has positively\ncorrelates with the ability of low-rank matrix-completion-based approaches to\npredict the ratings for the item or the user accurately. Furthermore, we use\nthese insights to develop four matrix completion-based approaches, i.e.,\nFrequency Adaptive Rating Prediction (FARP), Truncated Matrix Factorization\n(TMF), Truncated Matrix Factorization with Dropout (TMF + Dropout) and Inverse\nFrequency Weighted Matrix Factorization (IFWMF), that outperforms traditional\nmatrix-completion-based approaches for the users and the items with few ratings\nin the user-item rating matrix.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 04:55:10 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2020 00:58:20 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Sharma", "Mohit", ""], ["Karypis", "George", ""]]}, {"id": "1904.11886", "submitter": "Adam G. Dunn", "authors": "Eliza Harrison, Paige Martin, Didi Surian, Adam G. Dunn", "title": "Recommending research articles to consumers of online vaccination\n  information", "comments": "12 pages, 5 figures, 2 tables", "journal-ref": "Quantitative Science Studies, 1(2):810-823 (2020)", "doi": "10.1162/qss_a_00030", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online health communications often provide biased interpretations of evidence\nand have unreliable links to the source research. We tested the feasibility of\na tool for matching webpages to their source evidence. From 207,538 eligible\nvaccination-related PubMed articles, we evaluated several approaches using\n3,573 unique links to webpages from Altmetric. We evaluated methods for ranking\nthe source articles for vaccine-related research described on webpages,\ncomparing simple baseline feature representation and dimensionality reduction\napproaches to those augmented with canonical correlation analysis (CCA).\nPerformance measures included the median rank of the correct source article;\nthe percentage of webpages for which the source article was correctly ranked\nfirst (recall@1); and the percentage ranked within the top 50 candidate\narticles (recall@50). While augmenting baseline methods using CCA generally\nimproved results, no CCA-based approach outperformed a baseline method, which\nranked the correct source article first for over one quarter of webpages and in\nthe top 50 for more than half. Tools to help people identify evidence-based\nsources for the content they access on vaccination-related webpages are\npotentially feasible and may support the prevention of bias and\nmisrepresentation of research in news and social media.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 15:12:05 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 12:21:58 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Harrison", "Eliza", ""], ["Martin", "Paige", ""], ["Surian", "Didi", ""], ["Dunn", "Adam G.", ""]]}, {"id": "1904.12039", "submitter": "Elisa Claire Alem\\'an Carre\\'on", "authors": "Elisa Claire Alem\\'an Carre\\'on, Tetsuro Ito, Hirofumi Nonaka, Minoru\n  Kumano, Toru Hiraoka and Masaharu Hirota", "title": "Causal relationship between eWOM topics and profit of rural tourism at\n  Japanese Roadside Stations \"MICHINOEKI\"", "comments": null, "journal-ref": "In Proceedings of the 10th International Conference on Management\n  of Emergent Digital EcoSystems (MEDES'18). pp. 212 - 218. Tokyo, Japan.\n  September 25-28, 2018", "doi": "10.1145/3281375.3281395", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Affected by urbanization, centralization and the decrease of overall\npopulation, Japan has been making efforts to revitalize the rural areas across\nthe country. One particular effort is to increase tourism to these rural areas\nvia regional branding, using local farm products as tourist attractions across\nJapan. Particularly, a program subsidized by the government called Michinoeki,\nwhich stands for 'roadside station', was created 20 years ago and it strives to\nprovide a safe and comfortable space for cultural interaction between road\ntravelers and the local community, as well as offering refreshment, and\nrelevant information to travelers. However, despite its importance in the\nrevitalization of the Japanese economy, studies with newer technologies and\nmethodologies are lacking. Using sales data from establishments in the Kyushu\narea of Japan, we used Support Vector to classify content from Twitter into\nrelevant topics and studied their causal relationship to the sales for each\nestablishment using LiNGAM, a linear non-gaussian acyclic model built for\ncausal structure analysis, to perform an improved market analysis considering\nmore than just correlation. Under the hypotheses stated by the LiNGAM model, we\ndiscovered a positive causal relationship between the number of tweets\nmentioning those establishments, specially mentioning deserts, a need for\nbetter access and traf^ic options, and a potentially untapped customer base in\nmotorcycle biker groups.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 07:11:26 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 05:05:43 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Carre\u00f3n", "Elisa Claire Alem\u00e1n", ""], ["Ito", "Tetsuro", ""], ["Nonaka", "Hirofumi", ""], ["Kumano", "Minoru", ""], ["Hiraoka", "Toru", ""], ["Hirota", "Masaharu", ""]]}, {"id": "1904.12040", "submitter": "Elisa Claire Alem\\'an Carre\\'on", "authors": "Asahi Hentona, Takeshi Sakumoto, Hugo Alberto Mendoza Espa\\~na,\n  Hirofumi Nonaka, Shotaro Kataoka, Toru Hiraoka, Kensei Nakai, Elisa Claire\n  Alem\\'an Carre\\'on and Masaharu Hirota", "title": "Community Detection and Growth Potential Prediction from Patent Citation\n  Networks", "comments": "arXiv admin note: text overlap with arXiv:1607.00653 by other authors", "journal-ref": "In Proceedings of the 10th International Conference on Management\n  of Emergent Digital EcoSystems (MEDES'18). pp. 204 - 211. Tokyo, Japan.\n  September 25-28, 2018", "doi": "10.1145/3281375.3281396", "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The scoring of patents is useful for technology management analysis.\nTherefore, a necessity of developing citation network clustering and prediction\nof future citations for practical patent scoring arises. In this paper, we\npropose a community detection method using the Node2vec. And in order to\nanalyze growth potential we compare three ''time series analysis methods'', the\nLong Short-Term Memory (LSTM), ARIMA model, and Hawkes Process. The results of\nour experiments, we could find common technical points from those clusters by\nNode2vec. Furthermore, we found that the prediction accuracy of the ARIMA model\nwas higher than that of other models.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 15:36:35 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Hentona", "Asahi", ""], ["Sakumoto", "Takeshi", ""], ["Espa\u00f1a", "Hugo Alberto Mendoza", ""], ["Nonaka", "Hirofumi", ""], ["Kataoka", "Shotaro", ""], ["Hiraoka", "Toru", ""], ["Nakai", "Kensei", ""], ["Carre\u00f3n", "Elisa Claire Alem\u00e1n", ""], ["Hirota", "Masaharu", ""]]}, {"id": "1904.12058", "submitter": "Muhan Zhang", "authors": "Muhan Zhang, Yixin Chen", "title": "Inductive Matrix Completion Based on Graph Neural Networks", "comments": "Accepted as a spotlight presentation at ICLR-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an inductive matrix completion model without using side\ninformation. By factorizing the (rating) matrix into the product of\nlow-dimensional latent embeddings of rows (users) and columns (items), a\nmajority of existing matrix completion methods are transductive, since the\nlearned embeddings cannot generalize to unseen rows/columns or to new matrices.\nTo make matrix completion inductive, most previous works use content (side\ninformation), such as user's age or movie's genre, to make predictions.\nHowever, high-quality content is not always available, and can be hard to\nextract. Under the extreme setting where not any side information is available\nother than the matrix to complete, can we still learn an inductive matrix\ncompletion model? In this paper, we propose an Inductive Graph-based Matrix\nCompletion (IGMC) model to address this problem. IGMC trains a graph neural\nnetwork (GNN) based purely on 1-hop subgraphs around (user, item) pairs\ngenerated from the rating matrix and maps these subgraphs to their\ncorresponding ratings. It achieves highly competitive performance with\nstate-of-the-art transductive baselines. In addition, IGMC is inductive -- it\ncan generalize to users/items unseen during the training (given that their\ninteractions exist), and can even transfer to new tasks. Our transfer learning\nexperiments show that a model trained out of the MovieLens dataset can be\ndirectly used to predict Douban movie ratings with surprisingly good\nperformance. Our work demonstrates that: 1) it is possible to train inductive\nmatrix completion models without using side information while achieving similar\nor better performances than state-of-the-art transductive methods; 2) local\ngraph patterns around a (user, item) pair are effective predictors of the\nrating this user gives to the item; and 3) Long-range dependencies might not be\nnecessary for modeling recommender systems.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 21:58:46 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 03:08:54 GMT"}, {"version": "v3", "created": "Sun, 16 Feb 2020 04:27:14 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Zhang", "Muhan", ""], ["Chen", "Yixin", ""]]}, {"id": "1904.12111", "submitter": "Jinkun Cao", "authors": "Jinkun Cao, Jinhao Zhu, Liwei Lin, Zhengui Xue, Ruhui Ma, Haibing Guan", "title": "A Novel Fuzzy Search Approach over Encrypted Data with Improved Accuracy\n  and Efficiency", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As cloud computing becomes prevalent in recent years, more and more\nenterprises and individuals outsource their data to cloud servers. To avoid\nprivacy leaks, outsourced data usually is encrypted before being sent to cloud\nservers, which disables traditional search schemes for plain text. To meet both\nend of security and searchability, search-supported encryption is proposed.\nHowever, many previous schemes suffer severe vulnerability when typos and\nsemantic diversity exist in query requests. To overcome such flaw, higher\nerror-tolerance is always expected for search-supported encryption design,\nsometimes defined as 'fuzzy search'. In this paper, we propose a new scheme of\nmulti-keyword fuzzy search over encrypted and outsourced data. Our approach\nintroduces a new mechanism to map a natural language expression into a\nword-vector space. Compared with previous approaches, our design shows higher\nrobustness when multiple kinds of typos are involved. Besides, our approach is\nenhanced with novel data structures to improve search efficiency. These two\ninnovations can work well for both accuracy and efficiency. Moreover, these\ndesigns will not hurt the fundamental security. Experiments on a real-world\ndataset demonstrate the effectiveness of our proposed approach, which\noutperforms currently popular approaches focusing on similar tasks.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 04:50:31 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 13:16:00 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Cao", "Jinkun", ""], ["Zhu", "Jinhao", ""], ["Lin", "Liwei", ""], ["Xue", "Zhengui", ""], ["Ma", "Ruhui", ""], ["Guan", "Haibing", ""]]}, {"id": "1904.12162", "submitter": "Hideaki Hata", "authors": "Rungroj Maipradit, Hideaki Hata, Kenichi Matsumoto", "title": "Sentiment Classification using N-gram IDF and Automated Machine Learning", "comments": "4 pages, IEEE Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sentiment classification method with a general machine learning\nframework. For feature representation, n-gram IDF is used to extract\nsoftware-engineering-related, dataset-specific, positive, neutral, and negative\nn-gram expressions. For classifiers, an automated machine learning tool is\nused. In the comparison using publicly available datasets, our method achieved\nthe highest F1 values in positive and negative sentences on all datasets.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 14:46:34 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 13:30:47 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Maipradit", "Rungroj", ""], ["Hata", "Hideaki", ""], ["Matsumoto", "Kenichi", ""]]}, {"id": "1904.12211", "submitter": "Mojtaba Nayyeri", "authors": "Mojtaba Nayyeri, Sahar Vahdati, Jens Lehmann, Hamed Shariat Yazdi", "title": "Soft Marginal TransE for Scholarly Knowledge Graph Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KGs), i.e. representation of information as a semantic\ngraph, provide a significant test bed for many tasks including question\nanswering, recommendation, and link prediction. Various amount of scholarly\nmetadata have been made vailable as knowledge graphs from the diversity of data\nproviders and agents. However, these high-quantities of data remain far from\nquality criteria in terms of completeness while growing at a rapid pace. Most\nof the attempts in completing such KGs are following traditional data\ndigitization, harvesting and collaborative curation approaches. Whereas,\nadvanced AI-related approaches such as embedding models - specifically designed\nfor such tasks - are usually evaluated for standard benchmarks such as Freebase\nand Wordnet. The tailored nature of such datasets prevents those approaches to\nshed the lights on more accurate discoveries. Application of such models on\ndomain-specific KGs takes advantage of enriched meta-data and provides accurate\nresults where the underlying domain can enormously benefit. In this work, the\nTransE embedding model is reconciled for a specific link prediction task on\nscholarly metadata. The results show a significant shift in the accuracy and\nperformance evaluation of the model on a dataset with scholarly metadata. The\nnewly proposed version of TransE obtains 99.9% for link prediction task while\noriginal TransE gets 95%. In terms of accuracy and Hit@10, TransE outperforms\nother embedding models such as ComplEx, TransH and TransR experimented over\nscholarly knowledge graphs\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 20:40:03 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Nayyeri", "Mojtaba", ""], ["Vahdati", "Sahar", ""], ["Lehmann", "Jens", ""], ["Yazdi", "Hamed Shariat", ""]]}, {"id": "1904.12320", "submitter": "Laurent Bou\\'e", "authors": "Laurent Bou\\'e", "title": "Real numbers, data science and chaos: How to fit any dataset with a\n  single parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.GL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how any dataset of any modality (time-series, images, sound...) can\nbe approximated by a well-behaved (continuous, differentiable...) scalar\nfunction with a single real-valued parameter. Building upon elementary concepts\nfrom chaos theory, we adopt a pedagogical approach demonstrating how to adjust\nthis parameter in order to achieve arbitrary precision fit to all samples of\nthe data. Targeting an audience of data scientists with a taste for the curious\nand unusual, the results presented here expand on previous similar observations\nregarding expressiveness power and generalization of machine learning models.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 13:29:49 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Bou\u00e9", "Laurent", ""]]}, {"id": "1904.12387", "submitter": "Fady Medhat", "authors": "Fady Medhat, Mahnaz Mohammadi, Sardar Jaf, Chris G. Willcocks, Toby P.\n  Breckon, Peter Matthews, Andrew Stephen McGough, Georgios Theodoropoulos and\n  Boguslaw Obara", "title": "TMIXT: A process flow for Transcribing MIXed handwritten and\n  machine-printed Text", "comments": "big data, unstructured data, Optical Character Recognition (OCR),\n  Handwritten Text Recognition (HTR), machine-printed text recognition, IAM\n  handwriting database, TMIXT", "journal-ref": "IEEE International Conference on Big Data (Big Data) 2018", "doi": "10.1109/BigData.2018.8622136", "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handling large corpuses of documents is of significant importance in many\nfields, no more so than in the areas of crime investigation and defence, where\nan organisation may be presented with a large volume of scanned documents which\nneed to be processed in a finite time. However, this problem is exacerbated\nboth by the volume, in terms of scanned documents and the complexity of the\npages, which need to be processed. Often containing many different elements,\nwhich each need to be processed and understood. Text recognition, which is a\nprimary task of this process, is usually dependent upon the type of text, being\neither handwritten or machine-printed. Accordingly, the recognition involves\nprior classification of the text category, before deciding on the recognition\nmethod to be applied. This poses a more challenging task if a document contains\nboth handwritten and machine-printed text. In this work, we present a generic\nprocess flow for text recognition in scanned documents containing mixed\nhandwritten and machine-printed text without the need to classify text in\nadvance. We realize the proposed process flow using several open-source image\nprocessing and text recognition packages1. The evaluation is performed using a\nspecially developed variant, presented in this work, of the IAM handwriting\ndatabase, where we achieve an average transcription accuracy of nearly 80% for\npages containing both printed and handwritten text.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 21:46:39 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Medhat", "Fady", ""], ["Mohammadi", "Mahnaz", ""], ["Jaf", "Sardar", ""], ["Willcocks", "Chris G.", ""], ["Breckon", "Toby P.", ""], ["Matthews", "Peter", ""], ["McGough", "Andrew Stephen", ""], ["Theodoropoulos", "Georgios", ""], ["Obara", "Boguslaw", ""]]}, {"id": "1904.12535", "submitter": "Ping Li", "authors": "Mingming Sun, Xu Li, Xin Wang, Miao Fan, Yue Feng, Ping Li", "title": "Logician: A Unified End-to-End Neural Approach for Open-Domain\n  Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of open information extraction (OIE)\nfor extracting entity and relation level intermediate structures from sentences\nin open-domain. We focus on four types of valuable intermediate structures\n(Relation, Attribute, Description, and Concept), and propose a unified\nknowledge expression form, SAOKE, to express them. We publicly release a data\nset which contains more than forty thousand sentences and the corresponding\nfacts in the SAOKE format labeled by crowd-sourcing. To our knowledge, this is\nthe largest publicly available human labeled data set for open information\nextraction tasks. Using this labeled SAOKE data set, we train an end-to-end\nneural model using the sequenceto-sequence paradigm, called Logician, to\ntransform sentences into facts. For each sentence, different to existing\nalgorithms which generally focus on extracting each single fact without\nconcerning other possible facts, Logician performs a global optimization over\nall possible involved facts, in which facts not only compete with each other to\nattract the attention of words, but also cooperate to share words. An\nexperimental study on various types of open domain relation extraction tasks\nreveals the consistent superiority of Logician to other states-of-the-art\nalgorithms. The experiments verify the reasonableness of SAOKE format, the\nvaluableness of SAOKE data set, the effectiveness of the proposed Logician\nmodel, and the feasibility of the methodology to apply end-to-end learning\nparadigm on supervised data sets for the challenging tasks of open information\nextraction.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 09:37:31 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Sun", "Mingming", ""], ["Li", "Xu", ""], ["Wang", "Xin", ""], ["Fan", "Miao", ""], ["Feng", "Yue", ""], ["Li", "Ping", ""]]}, {"id": "1904.12573", "submitter": "Leonid Keselman", "authors": "Leonid Keselman", "title": "Venue Analytics: A Simple Alternative to Citation-Based Metrics", "comments": "10 pages, Accepted to ACM/IEEE JCDL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a method for automatically organizing and evaluating the quality\nof different publishing venues in Computer Science. Since this method only\nrequires paper publication data as its input, we can demonstrate our method on\na large portion of the DBLP dataset, spanning 50 years, with millions of\nauthors and thousands of publishing venues. By formulating venue authorship as\na regression problem and targeting metrics of interest, we obtain venue scores\nfor every conference and journal in our dataset. The obtained scores can also\nprovide a per-year model of conference quality, showing how fields develop and\nchange over time. Additionally, these venue scores can be used to evaluate\nindividual academic authors and academic institutions. We show that using venue\nscores to evaluate both authors and institutions produces quantitative measures\nthat are comparable to approaches using citations or peer assessment. In\ncontrast to many other existing evaluation metrics, our use of large-scale,\nopenly available data enables this approach to be repeatable and transparent.\n  To help others build upon this work, all of our code and data is available at\nhttps://github.com/leonidk/venue_scores\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 12:01:58 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 16:03:52 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Keselman", "Leonid", ""]]}, {"id": "1904.12574", "submitter": "Da Xu", "authors": "Da Xu, Chuanwei Ruan, Jason Cho, Evren Korpeoglu, Sushant Kumar,\n  Kannan Achan", "title": "Knowledge-aware Complementary Product Representation Learning", "comments": null, "journal-ref": null, "doi": "10.1145/3336191.3371854", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning product representations that reflect complementary relationship\nplays a central role in e-commerce recommender system. In the absence of the\nproduct relationships graph, which existing methods rely on, there is a need to\ndetect the complementary relationships directly from noisy and sparse customer\npurchase activities. Furthermore, unlike simple relationships such as\nsimilarity, complementariness is asymmetric and non-transitive. Standard usage\nof representation learning emphasizes on only one set of embedding, which is\nproblematic for modelling such properties of complementariness. We propose\nusing knowledge-aware learning with dual product embedding to solve the above\nchallenges. We encode contextual knowledge into product representation by\nmulti-task learning, to alleviate the sparsity issue. By explicitly modelling\nwith user bias terms, we separate the noise of customer-specific preferences\nfrom the complementariness. Furthermore, we adopt the dual embedding framework\nto capture the intrinsic properties of complementariness and provide geometric\ninterpretation motivated by the classic separating hyperplane theory. Finally,\nwe propose a Bayesian network structure that unifies all the components, which\nalso concludes several popular models as special cases. The proposed method\ncompares favourably to state-of-art methods, in downstream classification and\nrecommendation tasks. We also develop an implementation that scales efficiently\nto a dataset with millions of items and customers.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 03:01:12 GMT"}, {"version": "v2", "created": "Sun, 16 Jun 2019 19:46:18 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2019 21:09:31 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Xu", "Da", ""], ["Ruan", "Chuanwei", ""], ["Cho", "Jason", ""], ["Korpeoglu", "Evren", ""], ["Kumar", "Sushant", ""], ["Achan", "Kannan", ""]]}, {"id": "1904.12575", "submitter": "Hongwei Wang", "authors": "Hongwei Wang, Miao Zhao, Xing Xie, Wenjie Li, Minyi Guo", "title": "Knowledge Graph Convolutional Networks for Recommender Systems", "comments": "Proceedings of the 2019 World Wide Web Conference", "journal-ref": null, "doi": "10.1145/3308558.3313417", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To alleviate sparsity and cold start problem of collaborative filtering based\nrecommender systems, researchers and engineers usually collect attributes of\nusers and items, and design delicate algorithms to exploit these additional\ninformation. In general, the attributes are not isolated but connected with\neach other, which forms a knowledge graph (KG). In this paper, we propose\nKnowledge Graph Convolutional Networks (KGCN), an end-to-end framework that\ncaptures inter-item relatedness effectively by mining their associated\nattributes on the KG. To automatically discover both high-order structure\ninformation and semantic information of the KG, we sample from the neighbors\nfor each entity in the KG as their receptive field, then combine neighborhood\ninformation with bias when calculating the representation of a given entity.\nThe receptive field can be extended to multiple hops away to model high-order\nproximity information and capture users' potential long-distance interests.\nMoreover, we implement the proposed KGCN in a minibatch fashion, which enables\nour model to operate on large datasets and KGs. We apply the proposed model to\nthree datasets about movie, book, and music recommendation, and experiment\nresults demonstrate that our approach outperforms strong recommender baselines.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 17:17:34 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Wang", "Hongwei", ""], ["Zhao", "Miao", ""], ["Xie", "Xing", ""], ["Li", "Wenjie", ""], ["Guo", "Minyi", ""]]}, {"id": "1904.12576", "submitter": "Armel Jacques Nzekon Nzeko'o", "authors": "Armel Jacques Nzekon Nzeko'o, Maurice Tchuente, Matthieu Latapy", "title": "Link Stream Graph for Temporal Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several researches on recommender systems are based on explicit rating data,\nbut in many real world e-commerce platforms, ratings are not always available,\nand in those situations, recommender systems have to deal with implicit data\nsuch as users' purchase history, browsing history and streaming history. In\nthis context, classical bipartite user-item graphs (BIP) are widely used to\ncompute top-N recommendations. However, these graphs have some limitations,\nparticularly in terms of taking temporal dynamic into account. This is not good\nbecause users' preference change over time. To overcome this limit, the\nSession-based Temporal Graph (STG) was proposed by Xiang et al. to combine\nlong- and short-term preferences in a graph-based recommender system. But in\nthe STG, time is divided into slices and therefore considered discontinuously.\nThis approach loses details of the real temporal dynamics of user actions. To\naddress this challenge, we propose the Link Stream Graph (LSG) which is an\nextension of link stream representation proposed by Latapy et al. and which\nallows to model interactions between users and items by considering time\ncontinuously. Experiments conducted on four real world implicit datasets for\ntemporal recommendation, with 3 evaluation metrics, show that LSG is the best\nin 9 out of 12 cases compared to BIP and STG which are the most used\nstate-of-the-art recommender graphs.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 14:04:44 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Nzeko'o", "Armel Jacques Nzekon", ""], ["Tchuente", "Maurice", ""], ["Latapy", "Matthieu", ""]]}, {"id": "1904.12577", "submitter": "Martin Hole\\v{c}ek", "authors": "Martin Hole\\v{c}ek, Anton\\'in Hoskovec, Petr Baudi\\v{s}, Pavel Klinger", "title": "Table understanding in structured documents", "comments": "Changed from previous version based on icdar2019 feedback to include\n  6 pages, 2 figures. Slightly changed paper name and abstract to be less\n  misleading. Corrected grammar and shortened content heavily, corrected\n  misleading information and readability. Currently in review for icdar2019-wml\n  subconference/workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract--- Table detection and extraction has been studied in the context of\ndocuments like reports, where tables are clearly outlined and stand out from\nthe document structure visually. We study this topic in a rather more\nchallenging domain of layout-heavy business documents, particularly invoices.\nInvoices present the novel challenges of tables being often without outlines -\neither in the form of borders or surrounding text flow - with ragged columns\nand widely varying data content. We will also show, that we can extract\nspecific information from structurally different tables or table-like\nstructures with one model. We present a comprehensive representation of a page\nusing graph over word boxes, positional embeddings, trainable textual features\nand rephrase the table detection as a text box labeling problem. We will work\non our newly presented dataset of pro forma invoices, invoices and debit note\ndocuments using this representation and propose multiple baselines to solve\nthis labeling problem. We then propose a novel neural network model that\nachieves strong, practical results on the presented dataset and analyze the\nmodel performance and effects of graph convolutions and self-attention in\ndetail.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 15:08:04 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 14:38:06 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Hole\u010dek", "Martin", ""], ["Hoskovec", "Anton\u00edn", ""], ["Baudi\u0161", "Petr", ""], ["Klinger", "Pavel", ""]]}, {"id": "1904.12578", "submitter": "Ronghui You", "authors": "Ronghui You, Zihan Zhang, Suyang Dai and Shanfeng Zhu", "title": "HAXMLNet: Hierarchical Attention Network for Extreme Multi-Label Text\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme multi-label text classification (XMTC) addresses the problem of\ntagging each text with the most relevant labels from an extreme-scale label\nset. Traditional methods use bag-of-words (BOW) representations without context\ninformation as their features. The state-ot-the-art deep learning-based method,\nAttentionXML, which uses a recurrent neural network (RNN) and the multi-label\nattention, can hardly deal with extreme-scale (hundreds of thousands labels)\nproblem. To address this, we propose our HAXMLNet, which uses an efficient and\neffective hierarchical structure with the multi-label attention. Experimental\nresults show that HAXMLNet reaches a competitive performance with other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 08:09:15 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["You", "Ronghui", ""], ["Zhang", "Zihan", ""], ["Dai", "Suyang", ""], ["Zhu", "Shanfeng", ""]]}, {"id": "1904.12579", "submitter": "Furao Shen", "authors": "Yi Yang, Baile Xu, Furao Shen, Jian Zhao", "title": "Operation-aware Neural Networks for User Response Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User response prediction makes a crucial contribution to the rapid\ndevelopment of online advertising system and recommendation system. The\nimportance of learning feature interactions has been emphasized by many works.\nMany deep models are proposed to automatically learn high-order feature\ninteractions. Since most features in advertising system and recommendation\nsystem are high-dimensional sparse features, deep models usually learn a\nlow-dimensional distributed representation for each feature in the bottom\nlayer. Besides traditional fully-connected architectures, some new operations,\nsuch as convolutional operations and product operations, are proposed to learn\nfeature interactions better. In these models, the representation is shared\namong different operations. However, the best representation for different\noperations may be different. In this paper, we propose a new neural model named\nOperation-aware Neural Networks (ONN) which learns different representations\nfor different operations. Our experimental results on two large-scale\nreal-world ad click/conversion datasets demonstrate that ONN consistently\noutperforms the state-of-the-art models in both offline-training environment\nand online-training environment.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:04:52 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Yang", "Yi", ""], ["Xu", "Baile", ""], ["Shen", "Furao", ""], ["Zhao", "Jian", ""]]}, {"id": "1904.12580", "submitter": "Mahidhar Dwarampudi", "authors": "Dwarampudi Mahidhar Reddy, Dr. N V Subba Reddy, Dr. N V Subba Reddy", "title": "Twitter Sentiment Analysis using Distributed Word and Sentence\n  Representation", "comments": "8 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important part of the information gathering and data analysis is to find\nout what people think about, either a product or an entity. Twitter is an\nopinion rich social networking site. The posts or tweets from this data can be\nused for mining people's opinions. The recent surge of activity in this area\ncan be attributed to the computational treatment of data, which made opinion\nextraction and sentiment analysis easier. This paper classifies tweets into\npositive and negative sentiments, but instead of using traditional methods or\npreprocessing text data here we use the distributed representations of words\nand sentences to classify the tweets. We use Long Short Term Memory (LSTM)\nNetworks, Convolutional Neural Networks (CNNs) and Artificial Neural Networks.\nThe first two are used on Distributed Representation of words while the latter\nis used on the distributed representation of sentences. This paper achieves\naccuracies as high as 81%. It also suggests the best and optimal ways for\ncreating distributed representations of words for sentiment analysis, out of\nthe available methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 17:46:54 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Reddy", "Dwarampudi Mahidhar", ""], ["Reddy", "Dr. N V Subba", ""], ["Reddy", "Dr. N V Subba", ""]]}, {"id": "1904.12583", "submitter": "Nazakat Ali", "authors": "Nazakat Ali, Jang-Eui Hong", "title": "Using Social Network Service to determine the Initial User Requirements\n  for Small Software Businesses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background/Objectives: Software engineering community has been studied\nextensively on large-sized software organizations and has provided suitable and\ninteresting solutions. However, small software companies that make a large part\nof the software industry have been overlooked. Methods/Statistical analysis:\nThe current requirement engineering practices are not suitable for small\nsoftware companies. We propose a social network-based requirement engineering\napproach that will complement the traditional requirement engineering\napproaches and will make it suitable for small software companies. Findings: We\nhave applied our SNS-based requirements determination approach to knowing about\nits validity. As a result, we concluded that 33.06 % of invited end-users\nparticipated in our approach and figured out 156 distinct user requirements. It\nhas been seen that it was not necessary for users to have requirements\nengineering knowledge to participate in our proposed SNS-based approach that\nmade maximum users to be involved during requirements elicitation process. By\ninvestigating the ideas and opinions communicated by users, we were able to\nfigure out a high number of user requirements. It was observed that maximum\nuser-requirements were determined within a short period of time (7days). Our\nexperience with SNS-based approach also says that end-users hardly know about\nnon-functional requirements and express it explicitly.\nImprovements/Applications: we believe that researchers will consider SNS other\nthan Facebook that would allow applying our SNS-based approach for requirements\nidentification. We have experienced our approach with Facebook but we do not\nknow how our approach would actually work with other SNSs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 04:44:31 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Ali", "Nazakat", ""], ["Hong", "Jang-Eui", ""]]}, {"id": "1904.12587", "submitter": "Thomas K\\\"ollmer", "authors": "Thomas K\\\"ollmer and Jens Hasselbach and Patrick Aichroth", "title": "Text Classification Components for Detecting Descriptions and Names of\n  CAD models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply text analysis approaches for a specialized search engine for 3D CAD\nmodels and associated products. The main goals are to distinguish between\nactual product descriptions and other text on a website, as well as to decide\nwhether a given text is or contains a product name.\n  For this we use paragraph vectors for text classification, a character-level\nlong short-term memory network (LSTM) for a single word classification and an\nLSTM tagger based on word embeddings for detecting product names within\nsentences. Despite the need to collect bigger datasets in our specific problem\ndomain, the first results are promising and partially fit for production use.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 15:41:26 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["K\u00f6llmer", "Thomas", ""], ["Hasselbach", "Jens", ""], ["Aichroth", "Patrick", ""]]}, {"id": "1904.12593", "submitter": "Rui Portocarrero Sarmento MSc", "authors": "Rui Portocarrero Sarmento", "title": "Density-based Community Detection/Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modularity-based algorithms used for community detection have been increasing\nin recent years. Modularity and its application have been generating\ncontroversy since some authors argue it is not a metric without disadvantages.\nIt has been shown that algorithms that use modularity to detect communities\nsuffer a resolution limit and, therefore, it is unable to identify small\ncommunities in some situations. In this work, we try to apply a density\noptimization of communities found by the label propagation algorithm and study\nwhat happens regarding modularity of optimized results. We introduce a metric\nwe call ADC (Average Density per Community); we use this metric to prove our\noptimization provides improvements to the community density obtained with\nbenchmark algorithms. Additionally, we provide evidence this optimization might\nnot alter modularity of resulting communities significantly. Additionally, by\nalso using the SSC (Strongly Connected Components) concept we developed a\ncommunity detection algorithm that we also compare with the label propagation\nalgorithm. These comparisons were executed with several test networks and with\ndifferent network sizes. The results of the optimization algorithm proved to be\ninteresting. Additionally, the results of the community detection algorithm\nturned out to be similar to the benchmark algorithm we used.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 21:01:21 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Sarmento", "Rui Portocarrero", ""]]}, {"id": "1904.12604", "submitter": "Jingxuan Yang", "authors": "Jingxuan Yang, Jun Xu, Jianzhuo Tong, Sheng Gao, Jun Guo, Jirong Wen", "title": "Pre-training of Context-aware Item Representation for Next Basket\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next basket recommendation, which aims to predict the next a few items that a\nuser most probably purchases given his historical transactions, plays a vital\nrole in market basket analysis. From the viewpoint of item, an item could be\npurchased by different users together with different items, for different\nreasons. Therefore, an ideal recommender system should represent an item\nconsidering its transaction contexts. Existing state-of-the-art deep learning\nmethods usually adopt the static item representations, which are invariant\namong all of the transactions and thus cannot achieve the full potentials of\ndeep learning. Inspired by the pre-trained representations of BERT in natural\nlanguage processing, we propose to conduct context-aware item representation\nfor next basket recommendation, called Item Encoder Representations from\nTransformers (IERT). In the offline phase, IERT pre-trains deep item\nrepresentations conditioning on their transaction contexts. In the online\nrecommendation phase, the pre-trained model is further fine-tuned with an\nadditional output layer. The output contextualized item embeddings are used to\ncapture users' sequential behaviors and general tastes to conduct\nrecommendation. Experimental results on the Ta-Feng data set show that IERT\noutperforms the state-of-the-art baseline methods, which demonstrated the\neffectiveness of IERT in next basket representation.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 14:57:57 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Yang", "Jingxuan", ""], ["Xu", "Jun", ""], ["Tong", "Jianzhuo", ""], ["Gao", "Sheng", ""], ["Guo", "Jun", ""], ["Wen", "Jirong", ""]]}, {"id": "1904.12605", "submitter": "Yangyang Wu", "authors": "Jinyin Chen, Yangyang Wu, Lu Fan, Xiang Lin, Haibin Zheng, Shanqing\n  Yu, Qi Xuan", "title": "N2VSCDNNR: A Local Recommender System Based on Node2vec and Rich\n  Information Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are becoming more and more important in our daily lives.\nHowever, traditional recommendation methods are challenged by data sparsity and\nefficiency, as the numbers of users, items, and interactions between the two in\nmany real-world applications increase fast. In this work, we propose a novel\nclustering recommender system based on node2vec technology and rich information\nnetwork, namely N2VSCDNNR, to solve these challenges. In particular, we use a\nbipartite network to construct the user-item network, and represent the\ninteractions among users (or items) by the corresponding one-mode projection\nnetwork. In order to alleviate the data sparsity problem, we enrich the network\nstructure according to user and item categories, and construct the one-mode\nprojection category network. Then, considering the data sparsity problem in the\nnetwork, we employ node2vec to capture the complex latent relationships among\nusers (or items) from the corresponding one-mode projection category network.\nMoreover, considering the dependency on parameter settings and information loss\nproblem in clustering methods, we use a novel spectral clustering method, which\nis based on dynamic nearest-neighbors (DNN) and a novel automatically\ndetermining cluster number (ADCN) method that determines the cluster centers\nbased on the normal distribution method, to cluster the users and items\nseparately. After clustering, we propose the two-phase personalized\nrecommendation to realize the personalized recommendation of items for each\nuser. A series of experiments validate the outstanding performance of our\nN2VSCDNNR over several advanced embedding and side information based\nrecommendation algorithms. Meanwhile, N2VSCDNNR seems to have lower time\ncomplexity than the baseline methods in online recommendations, indicating its\npotential to be widely applied in large-scale systems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 13:45:24 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Chen", "Jinyin", ""], ["Wu", "Yangyang", ""], ["Fan", "Lu", ""], ["Lin", "Xiang", ""], ["Zheng", "Haibin", ""], ["Yu", "Shanqing", ""], ["Xuan", "Qi", ""]]}, {"id": "1904.12606", "submitter": "Dongxu Zhang", "authors": "Dongxu Zhang and Subhabrata Mukherjee and Colin Lockard and Xin Luna\n  Dong and Andrew McCallum", "title": "OpenKI: Integrating Open Information Extraction and Knowledge Bases with\n  Relation Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider advancing web-scale knowledge extraction and\nalignment by integrating OpenIE extractions in the form of (subject, predicate,\nobject) triples with Knowledge Bases (KB). Traditional techniques from\nuniversal schema and from schema mapping fall in two extremes: either they\nperform instance-level inference relying on embedding for (subject, object)\npairs, thus cannot handle pairs absent in any existing triples; or they perform\npredicate-level mapping and completely ignore background evidence from\nindividual entities, thus cannot achieve satisfying quality. We propose OpenKI\nto handle sparsity of OpenIE extractions by performing instance-level\ninference: for each entity, we encode the rich information in its neighborhood\nin both KB and OpenIE extractions, and leverage this information in relation\ninference by exploring different methods of aggregation and attention. In order\nto handle unseen entities, our model is designed without creating\nentity-specific parameters. Extensive experiments show that this method not\nonly significantly improves state-of-the-art for conventional OpenIE\nextractions like ReVerb, but also boosts the performance on OpenIE from\nsemi-structured data, where new entity pairs are abundant and data are fairly\nsparse.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 14:05:38 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Zhang", "Dongxu", ""], ["Mukherjee", "Subhabrata", ""], ["Lockard", "Colin", ""], ["Dong", "Xin Luna", ""], ["McCallum", "Andrew", ""]]}, {"id": "1904.12607", "submitter": "Daniel Martens", "authors": "Daniel Martens and Walid Maalej", "title": "Towards Understanding and Detecting Fake Reviews in App Stores", "comments": null, "journal-ref": null, "doi": "10.1007/s10664-019-09706-9", "report-no": null, "categories": "cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  App stores include an increasing amount of user feedback in form of app\nratings and reviews. Research and recently also tool vendors have proposed\nanalytics and data mining solutions to leverage this feedback to developers and\nanalysts, e.g., for supporting release decisions. Research also showed that\npositive feedback improves apps' downloads and sales figures and thus their\nsuccess. As a side effect, a market for fake, incentivized app reviews emerged\nwith yet unclear consequences for developers, app users, and app store\noperators. This paper studies fake reviews, their providers, characteristics,\nand how well they can be automatically detected. We conducted disguised\nquestionnaires with 43 fake review providers and studied their review policies\nto understand their strategies and offers. By comparing 60,000 fake reviews\nwith 62 million reviews from the Apple App Store we found significant\ndifferences, e.g., between the corresponding apps, reviewers, rating\ndistribution, and frequency. This inspired the development of a simple\nclassifier to automatically detect fake reviews in app stores. On a labelled\nand imbalanced dataset including one-tenth of fake reviews, as reported in\nother domains, our classifier achieved a recall of 91% and an AUC/ROC value of\n98%. We discuss our findings and their impact on software engineering, app\nusers, and app store operators.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 18:01:04 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Martens", "Daniel", ""], ["Maalej", "Walid", ""]]}, {"id": "1904.12617", "submitter": "Danielle Braun", "authors": "Yujia Bao, Zhengyi Deng, Yan Wang, Heeyoon Kim, Victor Diego Armengol,\n  Francisco Acevedo, Nofal Ouardaoui, Cathy Wang, Giovanni Parmigiani, Regina\n  Barzilay, Danielle Braun, Kevin S Hughes", "title": "Using Machine Learning and Natural Language Processing to Review and\n  Classify the Medical Literature on Cancer Susceptibility Genes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PURPOSE: The medical literature relevant to germline genetics is growing\nexponentially. Clinicians need tools monitoring and prioritizing the literature\nto understand the clinical implications of the pathogenic genetic variants. We\ndeveloped and evaluated two machine learning models to classify abstracts as\nrelevant to the penetrance (risk of cancer for germline mutation carriers) or\nprevalence of germline genetic mutations. METHODS: We conducted literature\nsearches in PubMed and retrieved paper titles and abstracts to create an\nannotated dataset for training and evaluating the two machine learning\nclassification models. Our first model is a support vector machine (SVM) which\nlearns a linear decision rule based on the bag-of-ngrams representation of each\ntitle and abstract. Our second model is a convolutional neural network (CNN)\nwhich learns a complex nonlinear decision rule based on the raw title and\nabstract. We evaluated the performance of the two models on the classification\nof papers as relevant to penetrance or prevalence. RESULTS: For penetrance\nclassification, we annotated 3740 paper titles and abstracts and used 60% for\ntraining the model, 20% for tuning the model, and 20% for evaluating the model.\nThe SVM model achieves 89.53% accuracy (percentage of papers that were\ncorrectly classified) while the CNN model achieves 88.95 % accuracy. For\nprevalence classification, we annotated 3753 paper titles and abstracts. The\nSVM model achieves 89.14% accuracy while the CNN model achieves 89.13 %\naccuracy. CONCLUSION: Our models achieve high accuracy in classifying abstracts\nas relevant to penetrance or prevalence. By facilitating literature review,\nthis tool could help clinicians and researchers keep abreast of the burgeoning\nknowledge of gene-cancer associations and keep the knowledge bases for clinical\ndecision support tools up to date.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 17:20:21 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Bao", "Yujia", ""], ["Deng", "Zhengyi", ""], ["Wang", "Yan", ""], ["Kim", "Heeyoon", ""], ["Armengol", "Victor Diego", ""], ["Acevedo", "Francisco", ""], ["Ouardaoui", "Nofal", ""], ["Wang", "Cathy", ""], ["Parmigiani", "Giovanni", ""], ["Barzilay", "Regina", ""], ["Braun", "Danielle", ""], ["Hughes", "Kevin S", ""]]}, {"id": "1904.12620", "submitter": "Tao Li", "authors": "Tao Li and Lei Lin", "title": "AnonymousNet: Natural Face De-Identification with Measurable Privacy", "comments": "CVPR-19 Workshop on Computer Vision: Challenges and Opportunities for\n  Privacy and Security (CV-COPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With billions of personal images being generated from social media and\ncameras of all sorts on a daily basis, security and privacy are unprecedentedly\nchallenged. Although extensive attempts have been made, existing face image\nde-identification techniques are either insufficient in photo-reality or\nincapable of balancing privacy and usability qualitatively and quantitatively,\ni.e., they fail to answer counterfactual questions such as \"is it private\nnow?\", \"how private is it?\", and \"can it be more private?\" In this paper, we\npropose a novel framework called AnonymousNet, with an effort to address these\nissues systematically, balance usability, and enhance privacy in a natural and\nmeasurable manner. The framework encompasses four stages: facial attribute\nestimation, privacy-metric-oriented face obfuscation, directed natural image\nsynthesis, and adversarial perturbation. Not only do we achieve the\nstate-of-the-arts in terms of image quality and attribute prediction accuracy,\nwe are also the first to show that facial privacy is measurable, can be\nfactorized, and accordingly be manipulated in a photo-realistic fashion to\nfulfill different requirements and application scenarios. Experiments further\ndemonstrate the effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 07:57:32 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Li", "Tao", ""], ["Lin", "Lei", ""]]}, {"id": "1904.12623", "submitter": "Semhar Michael", "authors": "Damon Bayer and Semhar Michael", "title": "Exploring the Daschle Collection using Text Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A U.S. Senator from South Dakota donated documents that were accumulated\nduring his service as a house representative and senator to be housed at the\nBridges library at South Dakota State University. This project investigated the\nutility of quantitative statistical methods to explore some portions of this\nvast document collection. The available scanned documents and emails from\nconstituents are analyzed using natural language processing methods including\nthe Latent Dirichlet Allocation (LDA) model. This model identified major topics\nbeing discussed in a given collection of documents. Important events and\npopular issues from the Senator Daschles career are reflected in the changing\ntopics from the model. These quantitative statistical methods provide a summary\nof the massive amount of text without requiring significant human effort or\ntime and can be applied to similar collections.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 16:31:20 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Bayer", "Damon", ""], ["Michael", "Semhar", ""]]}, {"id": "1904.12624", "submitter": "Apostol Vassilev", "authors": "Apostol Vassilev", "title": "BowTie - A deep learning feedforward neural network for sentiment\n  analysis", "comments": "12 pages, 7 figures, 4 tables", "journal-ref": null, "doi": "10.1007/978-3-030-37599-7_30", "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to model and encode the semantics of human-written text and select the\ntype of neural network to process it are not settled issues in sentiment\nanalysis. Accuracy and transferability are critical issues in machine learning\nin general. These properties are closely related to the loss estimates for the\ntrained model. I present a computationally-efficient and accurate feedforward\nneural network for sentiment prediction capable of maintaining low losses. When\ncoupled with an effective semantics model of the text, it provides highly\naccurate models with low losses. Experimental results on representative\nbenchmark datasets and comparisons to other methods show the advantages of the\nnew approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:38:57 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Vassilev", "Apostol", ""]]}, {"id": "1904.12626", "submitter": "Francisco Bischoff", "authors": "Francisco Bischoff, Pedro Pereira Rodrigues", "title": "tsmp: An R Package for Time Series with Matrix Profile", "comments": null, "journal-ref": null, "doi": "10.32614/RJ-2020-021", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This article describes tsmp, an R package that implements the matrix profile\nconcept for time series. The tsmp package is a toolkit that allows all-pairs\nsimilarity joins, motif, discords and chains discovery, semantic segmentation,\netc. Here we describe how the tsmp package may be used by showing some of the\nuse-cases from the original articles and evaluate the algorithm speed in the R\nenvironment. This package can be downloaded at\nhttps://CRAN.R-project.org/package=tsmp.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 05:27:09 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Bischoff", "Francisco", ""], ["Rodrigues", "Pedro Pereira", ""]]}, {"id": "1904.12643", "submitter": "Mohit Sharma", "authors": "Mohit Sharma, F.Maxwell Harper, and George Karypis", "title": "Learning from Sets of Items in Recommender Systems", "comments": "27 pages, 17 figures, ACM TiiS (2019), DOI provided", "journal-ref": null, "doi": "10.1145/3326128", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing recommender systems use the ratings provided by users on\nindividual items. An additional source of preference information is to use the\nratings that users provide on sets of items. The advantages of using\npreferences on sets are two-fold. First, a rating provided on a set conveys\nsome preference information about each of the set's items, which allows us to\nacquire a user's preferences for more items that the number of ratings that the\nuser provided. Second, due to privacy concerns, users may not be willing to\nreveal their preferences on individual items explicitly but may be willing to\nprovide a single rating to a set of items, since it provides some level of\ninformation hiding. This paper investigates two questions related to using\nset-level ratings in recommender systems. First, how users' item-level ratings\nrelate to their set-level ratings. Second, how collaborative filtering-based\nmodels for item-level rating prediction can take advantage of such set-level\nratings. We have collected set-level ratings from active users of Movielens on\nsets of movies that they have rated in the past. Our analysis of these ratings\nshows that though the majority of the users provide the average of the ratings\non a set's constituent items as the rating on the set, there exists a\nsignificant number of users that tend to consistently either under- or\nover-rate the sets. We have developed collaborative filtering-based methods to\nexplicitly model these user behaviors that can be used to recommend items to\nusers. Experiments on real data and on synthetic data that resembles the under-\nor over-rating behavior in the real data, demonstrate that these models can\nrecover the overall characteristics of the underlying data and predict the\nuser's ratings on individual items.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 04:42:12 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Sharma", "Mohit", ""], ["Harper", "F. Maxwell", ""], ["Karypis", "George", ""]]}, {"id": "1904.12674", "submitter": "Kyungwoo Song", "authors": "Kyungwoo Song, Mingi Ji, Sungrae Park, Il-Chul Moon", "title": "Hierarchical Context enabled Recurrent Neural Network for Recommendation", "comments": null, "journal-ref": "AAAI 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long user history inevitably reflects the transitions of personal interests\nover time. The analyses on the user history require the robust sequential model\nto anticipate the transitions and the decays of user interests. The user\nhistory is often modeled by various RNN structures, but the RNN structures in\nthe recommendation system still suffer from the long-term dependency and the\ninterest drifts. To resolve these challenges, we suggest HCRNN with three\nhierarchical contexts of the global, the local, and the temporary interests.\nThis structure is designed to withhold the global long-term interest of users,\nto reflect the local sub-sequence interests, and to attend the temporary\ninterests of each transition. Besides, we propose a hierarchical context-based\ngate structure to incorporate our \\textit{interest drift assumption}. As we\nsuggest a new RNN structure, we support HCRNN with a complementary\n\\textit{bi-channel attention} structure to utilize hierarchical context. We\nexperimented the suggested structure on the sequential recommendation tasks\nwith CiteULike, MovieLens, and LastFM, and our model showed the best\nperformances in the sequential recommendations.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 09:07:55 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Song", "Kyungwoo", ""], ["Ji", "Mingi", ""], ["Park", "Sungrae", ""], ["Moon", "Il-Chul", ""]]}, {"id": "1904.12683", "submitter": "Sebastian Hofst\\\"atter", "authors": "Sebastian Hofst\\\"atter, Navid Rekabsaz, Carsten Eickhoff, Allan\n  Hanbury", "title": "On the Effect of Low-Frequency Terms on Neural-IR Models", "comments": "Accepted at SIGIR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-frequency terms are a recurring challenge for information retrieval\nmodels, especially neural IR frameworks struggle with adequately capturing\ninfrequently observed words. While these terms are often removed from neural\nmodels - mainly as a concession to efficiency demands - they traditionally play\nan important role in the performance of IR models. In this paper, we analyze\nthe effects of low-frequency terms on the performance and robustness of neural\nIR models. We conduct controlled experiments on three recent neural IR models,\ntrained on a large-scale passage retrieval collection. We evaluate the neural\nIR models with various vocabulary sizes for their respective word embeddings,\nconsidering different levels of constraints on the available GPU memory. We\nobserve that despite the significant benefits of using larger vocabularies, the\nperformance gap between the vocabularies can be, to a great extent, mitigated\nby extensive tuning of a related parameter: the number of documents to re-rank.\nWe further investigate the use of subword-token embedding models, and in\nparticular FastText, for neural IR models. Our experiments show that using\nFastText brings slight improvements to the overall performance of the neural IR\nmodels in comparison to models trained on the full vocabulary, while the\nimprovement becomes much more pronounced for queries containing low-frequency\nterms.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 13:07:44 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 08:12:24 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Hofst\u00e4tter", "Sebastian", ""], ["Rekabsaz", "Navid", ""], ["Eickhoff", "Carsten", ""], ["Hanbury", "Allan", ""]]}, {"id": "1904.12796", "submitter": "Xin Xin", "authors": "Xin Xin, Xiangnan He, Yongfeng Zhang, Yongdong Zhang, Joemon Jose", "title": "Relational Collaborative Filtering:Modeling Multiple Item Relations for\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing item-based collaborative filtering (ICF) methods leverage only the\nrelation of collaborative similarity. Nevertheless, there exist multiple\nrelations between items in real-world scenarios. Distinct from the\ncollaborative similarity that implies co-interact patterns from the user\nperspective, these relations reveal fine-grained knowledge on items from\ndifferent perspectives of meta-data, functionality, etc. However, how to\nincorporate multiple item relations is less explored in recommendation\nresearch. In this work, we propose Relational Collaborative Filtering (RCF), a\ngeneral framework to exploit multiple relations between items in recommender\nsystem. We find that both the relation type and the relation value are crucial\nin inferring user preference. To this end, we develop a two-level hierarchical\nattention mechanism to model user preference. The first-level attention\ndiscriminates which types of relations are more important, and the second-level\nattention considers the specific relation values to estimate the contribution\nof a historical item in recommending the target item. To make the item\nembeddings be reflective of the relational structure between items, we further\nformulate a task to preserve the item relations, and jointly train it with the\nrecommendation task of preference modeling. Empirical results on two real\ndatasets demonstrate the strong performance of RCF. Furthermore, we also\nconduct qualitative analyses to show the benefits of explanations brought by\nthe modeling of multiple item relations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 16:20:23 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 16:01:19 GMT"}, {"version": "v3", "created": "Sat, 11 May 2019 15:24:03 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Xin", "Xin", ""], ["He", "Xiangnan", ""], ["Zhang", "Yongfeng", ""], ["Zhang", "Yongdong", ""], ["Jose", "Joemon", ""]]}, {"id": "1904.12856", "submitter": "Utkarsh Porwal", "authors": "Utkarsh Porwal", "title": "Learning Image Information for eCommerce Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing similarity between a query and a document is fundamental in any\ninformation retrieval system. In search engines, computing query-document\nsimilarity is an essential step in both retrieval and ranking stages. In eBay\nsearch, document is an item and the query-item similarity can be computed by\ncomparing different facets of the query-item pair. Query text can be compared\nwith the text of the item title. Likewise, a category constraint applied on the\nquery can be compared with the listing category of the item. However, images\nare one signal that are usually present in the items but are not present in the\nquery. Images are one of the most intuitive signals used by users to determine\nthe relevance of the item given a query. Including this signal in estimating\nsimilarity between the query-item pair is likely to improve the relevance of\nthe search engine. We propose a novel way of deriving image information for\nqueries. We attempt to learn image information for queries from item images\ninstead of generating explicit image features or an image for queries. We use\ncanonical correlation analysis (CCA) to learn a new subspace where projecting\nthe original data will give us a new query and item representation. We\nhypothesize that this new query representation will also have image information\nabout the query. We estimate the query-item similarity using a vector space\nmodel and report the performance of the proposed method on eBay's search data.\nWe show 11.89\\% relevance improvement over the baseline using area under the\nreceiver operating characteristic curve (AUROC) as the evaluation metric. We\nalso show 3.1\\% relevance improvement over the baseline with area under the\nprecision recall curve (AUPRC) .\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 23:48:31 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Porwal", "Utkarsh", ""]]}, {"id": "1904.12986", "submitter": "Elisa Claire Alem\\'an Carre\\'on", "authors": "Kensei Nakai, Hirofumi Nonaka, Asahi Hentona, Yuki Kanai, Takeshi\n  Sakumoto, Shotaro Kataoka, Elisa Claire Alem\\'an Carre\\'on and Toru Hiraoka", "title": "Community Detection and Growth Potential Prediction Using the Stochastic\n  Block Model and the Long Short-term Memory from Patent Citation Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.12040", "journal-ref": "In Proceedings of the 2018 IEEE International Conference on\n  Industrial Engineering and Engineering Management (IEEM2018). pp. 1884 -\n  1888. Bangkok, Thailand. December 16-19, 2018", "doi": "10.1109/IEEM.2018.8607487", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scoring patent documents is very useful for technology management. However,\nconventional methods are based on static models and, thus, do not reflect the\ngrowth potential of the technology cluster of the patent. Because even if the\ncluster of a patent has no hope of growing, we recognize the patent is\nimportant if PageRank or other ranking score is high. Therefore, there arises a\nnecessity of developing citation network clustering and prediction of future\ncitations. In our research, clustering of patent citation networks by\nStochastic Block Model was done with the aim of enabling corporate managers and\ninvestors to evaluate the scale and life cycle of technology. As a result, we\nconfirmed nested SBM is appropriate for graph clustering of patent citation\nnetworks. Also, a high MAPE value was obtained and the direction accuracy\nachieved a value greater than 50% when predicting growth potential for each\ncluster by using LSTM.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 15:50:58 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Nakai", "Kensei", ""], ["Nonaka", "Hirofumi", ""], ["Hentona", "Asahi", ""], ["Kanai", "Yuki", ""], ["Sakumoto", "Takeshi", ""], ["Kataoka", "Shotaro", ""], ["Carre\u00f3n", "Elisa Claire Alem\u00e1n", ""], ["Hiraoka", "Toru", ""]]}, {"id": "1904.13022", "submitter": "Markus Luczak-Roesch", "authors": "Cameron Lai and Markus Luczak-Roesch", "title": "You can't see what you can't see: Experimental evidence for how much\n  relevant information may be missed due to Google's Web search personalisation", "comments": "paper submitted to the 11th Intl. Conf. on Social Informatics;\n  revision corrects error in interpretation of parameter Psi/p in RBO resulting\n  from discrepancy between the documentation of the implementation in R\n  (https://rdrr.io/bioc/gespeR/man/rbo.html) and the original definition\n  (https://dl.acm.org/citation.cfm?id=1852106) as per 20/05/2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The influence of Web search personalisation on professional knowledge work is\nan understudied area. Here we investigate how public sector officials\nself-assess their dependency on the Google Web search engine, whether they are\naware of the potential impact of algorithmic biases on their ability to\nretrieve all relevant information, and how much relevant information may\nactually be missed due to Web search personalisation. We find that the majority\nof participants in our experimental study are neither aware that there is a\npotential problem nor do they have a strategy to mitigate the risk of missing\nrelevant information when performing online searches. Most significantly, we\nprovide empirical evidence that up to 20% of relevant information may be missed\ndue to Web search personalisation. This work has significant implications for\nWeb research by public sector professionals, who should be provided with\ntraining about the potential algorithmic biases that may affect their judgments\nand decision making, as well as clear guidelines how to minimise the risk of\nmissing relevant information.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 02:45:32 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 10:08:45 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Lai", "Cameron", ""], ["Luczak-Roesch", "Markus", ""]]}, {"id": "1904.13033", "submitter": "Harald Steck", "authors": "Harald Steck", "title": "Collaborative Filtering via High-Dimensional Regression", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the SLIM approach obtained high ranking-accuracy in many experiments in\nthe literature, it is also known for its high computational cost of learning\nits parameters from data. For this reason, we focus in this paper on variants\nof high-dimensional regression problems that have closed-form solutions.\nMoreover, we motivate a re-scaling rather than a re-weighting approach for\ndealing with biases regarding item-popularities in the data. We also discuss\nproperties of the sparse solution, and outline a computationally efficient\napproximation. In experiments on three publicly available data sets, we\nobserved not only extremely reduced training times, but also significantly\nimproved ranking accuracy compared to SLIM. Surprisingly, various\nstate-of-the-art models, including deep non-linear autoencoders, were also\noutperformed on two of the three data sets in our experiments, in particular\nfor recommendations with highly personalized relevance.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 03:21:19 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Steck", "Harald", ""]]}, {"id": "1904.13213", "submitter": "Elisa Claire Alem\\'an Carre\\'on", "authors": "Yoshiki Horii, Hirofumi Nonaka, Elisa Claire Alem\\'an Carre\\'on,\n  Hiroki Horino and Toru Hiraoka", "title": "Topic Classification Method for Analyzing Effect of eWOM on Consumer\n  Game Sales", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.11797,\n  arXiv:1904.12039, 1904.13214", "journal-ref": "In proceedings of the Joint 10th International Conference on Soft\n  Computing and Intelligent Systems and 19th International Symposium on\n  Advanced Intelligent Systems in conjunction with Intelligent Systems Workshop\n  2018 (2018 SCIS-ISIS2018)", "doi": "10.6084/m9.figshare.8026778", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electronic word-of-mouth (eWOM) has become an important resource for the\nanalysis of marketing research. In this study, in order to analyze user needs\nfor consumer game software, we focus on tweet data. And we proposed topic\nextraction method using entropy-based feature selection based feature\nexpansion. We also applied it to the classification of the data extracted from\ntweet data by using SVM. As a result, we achieved a 0.63 F-measure.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 16:18:50 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Horii", "Yoshiki", ""], ["Nonaka", "Hirofumi", ""], ["Carre\u00f3n", "Elisa Claire Alem\u00e1n", ""], ["Horino", "Hiroki", ""], ["Hiraoka", "Toru", ""]]}, {"id": "1904.13214", "submitter": "Elisa Claire Alem\\'an Carre\\'on", "authors": "Elisa Claire Alem\\'an Carre\\'on, Hirofumi Nonaka and Toru Hiraoka", "title": "Analysis of Chinese Tourists in Japan by Text Mining of a Hotel Portal\n  Site", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.11797,\n  arXiv:1904.13213, arXiv:1904.12039", "journal-ref": "In proceedings of the 18th International Symposium on Advanced\n  Intelligent Systems (ISIS2017), pp. 191 - 198. Daegu, South Korea (2017,\n  October 12)", "doi": "10.6084/m9.figshare.7831853", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With an increasingly large number of Chinese tourists in Japan, the hotel\nindustry is in need of an affordable market research tool that does not rely on\nexpensive and time-consuming surveys or interviews. Because this problem is\nreal and relevant to the hotel industry in Japan, and otherwise completely\nunexplored in other studies, we have extracted a list of potential keywords\nfrom Chinese reviews of Japanese hotels in the hotel portal site Ctrip1 using a\nmathematical model to then use them in a sentiment analysis with a machine\nlearning classifier. While most studies that use information collected from the\ninternet use pre-existing data analysis tools, in our study, we designed the\nmathematical model to have the highest possible performing results in\nclassification, while also exploring on the potential business implications\nthese may have.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 15:33:28 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 04:59:25 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Carre\u00f3n", "Elisa Claire Alem\u00e1n", ""], ["Nonaka", "Hirofumi", ""], ["Hiraoka", "Toru", ""]]}, {"id": "1904.13219", "submitter": "Noreddine Gherabi", "authors": "Noreddine Gherabi, Bahaj Mohamed", "title": "A new algorithm for shape matching and pattern recognition using dynamic\n  programming", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.08501", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for shape recognition and retrieval based on dynamic\nprogramming. Our approach uses the dynamic programming algorithm to compute the\noptimal score and to find the optimal alignment between two strings. First,\neach contour of shape is represented by a set of points. After alignment and\nmatching between two shapes, the contours are transformed into a string of\nsymbols and numbers. Finally we find the best alignment of two complete strings\nand compute the optimal cost of similarity. In general, dynamic programming has\ntwo phases: the forward phase and the backward phase. In the forward phase, we\ncompute the optimal cost for each subproblem. In the backward phase, we\nreconstruct the solution that gives the optimal cost. Our algorithm is tested\nin a database that contains various shapes such as MPEG-7.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 11:27:30 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Gherabi", "Noreddine", ""], ["Mohamed", "Bahaj", ""]]}, {"id": "1904.13325", "submitter": "Chih-Yi Chiu", "authors": "Sarawut Markchit and Chih-Yi Chiu", "title": "Effective and Efficient Indexing in Cross-Modal Hashing-Based Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To overcome the barrier of storage and computation, the hashing technique has\nbeen widely used for nearest neighbor search in multimedia retrieval\napplications recently. Particularly, cross-modal retrieval that searches across\ndifferent modalities becomes an active but challenging problem. Although dozens\nof cross-modal hashing algorithms are proposed to yield compact binary codes,\nthe exhaustive search is impractical for the real-time purpose, and Hamming\ndistance computation suffers inaccurate results. In this paper, we propose a\nnovel search method that utilizes a probability-based index scheme over binary\nhash codes in cross-modal retrieval. The proposed hash code indexing scheme\nexploits a few binary bits of the hash code as the index code. We construct an\ninverted index table based on index codes and train a neural network to improve\nthe indexing accuracy and efficiency. Experiments are performed on two\nbenchmark datasets for retrieval across image and text modalities, where hash\ncodes are generated by three cross-modal hashing methods. Results show the\nproposed method effectively boost the performance on these hash methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 15:41:05 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 01:59:24 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Markchit", "Sarawut", ""], ["Chiu", "Chih-Yi", ""]]}, {"id": "1904.13355", "submitter": "Kai Shu", "authors": "Kai Shu, Xinyi Zhou, Suhang Wang, Reza Zafarani, and Huan Liu", "title": "The Role of User Profile for Fake News Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consuming news from social media is becoming increasingly popular. Social\nmedia appeals to users due to its fast dissemination of information, low cost,\nand easy access. However, social media also enables the widespread of fake\nnews. Because of the detrimental societal effects of fake news, detecting fake\nnews has attracted increasing attention. However, the detection performance\nonly using news contents is generally not satisfactory as fake news is written\nto mimic true news. Thus, there is a need for an in-depth understanding on the\nrelationship between user profiles on social media and fake news. In this\npaper, we study the challenging problem of understanding and exploiting user\nprofiles on social media for fake news detection. In an attempt to understand\nconnections between user profiles and fake news, first, we measure users'\nsharing behaviors on social media and group representative users who are more\nlikely to share fake and real news; then, we perform a comparative analysis of\nexplicit and implicit profile features between these user groups, which reveals\ntheir potential to help differentiate fake news from real news. To exploit user\nprofile features, we demonstrate the usefulness of these user profile features\nin a fake news classification task. We further validate the effectiveness of\nthese features through feature importance analysis. The findings of this work\nlay the foundation for deeper exploration of user profile features of social\nmedia and enhance the capabilities for fake news detection.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 16:35:28 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Shu", "Kai", ""], ["Zhou", "Xinyi", ""], ["Wang", "Suhang", ""], ["Zafarani", "Reza", ""], ["Liu", "Huan", ""]]}]