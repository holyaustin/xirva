[{"id": "1806.00358", "submitter": "Rajarshi Das", "authors": "Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish\n  Yuvraj, Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche,\n  Pavan Kapanipathi, Nicholas Mattei, Ryan Musa, Kartik Talamadupula, Michael\n  Witbrock", "title": "A Systematic Classification of Knowledge, Reasoning, and Context within\n  the ARC Dataset", "comments": "Presented at the Machine Reading for Question Answering (MRQA 2018)\n  Workshop at the 55th Annual Meeting of the Association for Computational\n  Linguistics (ACL 2018). 11 pages, 5 tables, 4 figures. Added missing\n  citations in the latest draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent work of Clark et al. introduces the AI2 Reasoning Challenge (ARC)\nand the associated ARC dataset that partitions open domain, complex science\nquestions into an Easy Set and a Challenge Set. That paper includes an analysis\nof 100 questions with respect to the types of knowledge and reasoning required\nto answer them; however, it does not include clear definitions of these types,\nnor does it offer information about the quality of the labels. We propose a\ncomprehensive set of definitions of knowledge and reasoning types necessary for\nanswering the questions in the ARC dataset. Using ten annotators and a\nsophisticated annotation interface, we analyze the distribution of labels\nacross the Challenge Set and statistics related to them. Additionally, we\ndemonstrate that although naive information retrieval methods return sentences\nthat are irrelevant to answering the query, sufficient supporting text is often\npresent in the (ARC) corpus. Evaluating with human-selected relevant sentences\nimproves the performance of a neural machine comprehension model by 42 points.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 14:06:45 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 20:59:32 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Boratko", "Michael", ""], ["Padigela", "Harshit", ""], ["Mikkilineni", "Divyendra", ""], ["Yuvraj", "Pritish", ""], ["Das", "Rajarshi", ""], ["McCallum", "Andrew", ""], ["Chang", "Maria", ""], ["Fokoue-Nkoutche", "Achille", ""], ["Kapanipathi", "Pavan", ""], ["Mattei", "Nicholas", ""], ["Musa", "Ryan", ""], ["Talamadupula", "Kartik", ""], ["Witbrock", "Michael", ""]]}, {"id": "1806.00723", "submitter": "Lei Chen", "authors": "Le Wu, Lei Chen, Richang Hong, Yanjie Fu, Xing Xie, Meng Wang", "title": "A Hierarchical Attention Model for Social Contextual Image\n  Recommendation", "comments": null, "journal-ref": "IEEE Transactions on Knowledge and Data Engineering( Volume: 32,\n  Issue: 10, Oct. 1 2020)", "doi": "10.1109/TKDE.2019.2913394", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image based social networks are among the most popular social networking\nservices in recent years. With tremendous images uploaded everyday,\nunderstanding users' preferences on user-generated images and making\nrecommendations have become an urgent need. In fact, many hybrid models have\nbeen proposed to fuse various kinds of side information~(e.g., image visual\nrepresentation, social network) and user-item historical behavior for enhancing\nrecommendation performance. However, due to the unique characteristics of the\nuser generated images in social image platforms, the previous studies failed to\ncapture the complex aspects that influence users' preferences in a unified\nframework. Moreover, most of these hybrid models relied on predefined weights\nin combining different kinds of information, which usually resulted in\nsub-optimal recommendation performance. To this end, in this paper, we develop\na hierarchical attention model for social contextual image recommendation. In\naddition to basic latent user interest modeling in the popular matrix\nfactorization based recommendation, we identify three key aspects (i.e., upload\nhistory, social influence, and owner admiration) that affect each user's latent\npreferences, where each aspect summarizes a contextual factor from the complex\nrelationships between users and images. After that, we design a hierarchical\nattention network that naturally mirrors the hierarchical relationship\n(elements in each aspects level, and the aspect level) of users' latent\ninterests with the identified key aspects. Specifically, by taking embeddings\nfrom state-of-the-art deep learning models that are tailored for each kind of\ndata, the hierarchical attention network could learn to attend differently to\nmore or less content. Finally, extensive experimental results on real-world\ndatasets clearly show the superiority of our proposed model.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 01:45:05 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 12:49:41 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 14:20:40 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Wu", "Le", ""], ["Chen", "Lei", ""], ["Hong", "Richang", ""], ["Fu", "Yanjie", ""], ["Xie", "Xing", ""], ["Wang", "Meng", ""]]}, {"id": "1806.00755", "submitter": "Mucahid Kutlu", "authors": "Mucahid Kutlu, Tyler McDonnell, Aashish Sheshadri, Tamer Elsayed,\n  Matthew Lease", "title": "Mix and Match: Collaborative Expert-Crowd Judging for Building Test\n  Collections Accurately and Affordably", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing offers an affordable and scalable means to collect relevance\njudgments for IR test collections. However, crowd assessors may show higher\nvariance in judgment quality than trusted assessors. In this paper, we\ninvestigate how to effectively utilize both groups of assessors in partnership.\nWe specifically investigate how agreement in judging is correlated with three\nfactors: relevance category, document rankings, and topical variance. Based on\nthis, we then propose two collaborative judging methods in which a portion of\nthe document-topic pairs are assessed by in-house judges while the rest are\nassessed by crowd-workers. Experiments conducted on two TREC collections show\nencouraging results when we distribute work intelligently between our two\ngroups of assessors.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 09:01:19 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 14:06:56 GMT"}, {"version": "v3", "created": "Sat, 9 Jun 2018 09:45:39 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Kutlu", "Mucahid", ""], ["McDonnell", "Tyler", ""], ["Sheshadri", "Aashish", ""], ["Elsayed", "Tamer", ""], ["Lease", "Matthew", ""]]}, {"id": "1806.00778", "submitter": "Yi Tay", "authors": "Yi Tay, Luu Anh Tuan, Siu Cheung Hui", "title": "Multi-Cast Attention Networks for Retrieval-based Question Answering and\n  Response Prediction", "comments": "Accepted to KDD 2018 (Paper titled only \"Multi-Cast Attention\n  Networks\" in KDD version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention is typically used to select informative sub-phrases that are used\nfor prediction. This paper investigates the novel use of attention as a form of\nfeature augmentation, i.e, casted attention. We propose Multi-Cast Attention\nNetworks (MCAN), a new attention mechanism and general model architecture for a\npotpourri of ranking tasks in the conversational modeling and question\nanswering domains. Our approach performs a series of soft attention operations,\neach time casting a scalar feature upon the inner word embeddings. The key idea\nis to provide a real-valued hint (feature) to a subsequent encoder layer and is\ntargeted at improving the representation learning process. There are several\nadvantages to this design, e.g., it allows an arbitrary number of attention\nmechanisms to be casted, allowing for multiple attention types (e.g.,\nco-attention, intra-attention) and attention variants (e.g., alignment-pooling,\nmax-pooling, mean-pooling) to be executed simultaneously. This not only\neliminates the costly need to tune the nature of the co-attention layer, but\nalso provides greater extents of explainability to practitioners. Via extensive\nexperiments on four well-known benchmark datasets, we show that MCAN achieves\nstate-of-the-art performance. On the Ubuntu Dialogue Corpus, MCAN outperforms\nexisting state-of-the-art models by $9\\%$. MCAN also achieves the best\nperforming score to date on the well-studied TrecQA dataset.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 12:22:28 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Tay", "Yi", ""], ["Tuan", "Luu Anh", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1806.00914", "submitter": "Manoj Reddy Dareddy", "authors": "Manoj Reddy Dareddy, Ariyam Das, Junghoo Cho, Carlo Zaniolo", "title": "How Much Are You Willing to Share? A \"Poker-Styled\" Selective Privacy\n  Preserving Framework for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most industrial recommender systems rely on the popular collaborative\nfiltering (CF) technique for providing personalized recommendations to its\nusers. However, the very nature of CF is adversarial to the idea of user\nprivacy, because users need to share their preferences with others in order to\nbe grouped with like-minded people and receive accurate recommendations. While\nprevious privacy preserving approaches have been successful inasmuch as they\nconcealed user preference information to some extent from a centralized\nrecommender system, they have also, nevertheless, incurred significant\ntrade-offs in terms of privacy, scalability, and accuracy. They are also\nvulnerable to privacy breaches by malicious actors. In light of these\nobservations, we propose a novel selective privacy preserving (SP2) paradigm\nthat allows users to custom define the scope and extent of their individual\nprivacies, by marking their personal ratings as either public (which can be\nshared) or private (which are never shared and stored only on the user device).\nOur SP2 framework works in two steps: (i) First, it builds an initial\nrecommendation model based on the sum of all public ratings that have been\nshared by users and (ii) then, this public model is fine-tuned on each user's\ndevice based on the user private ratings, thus eventually learning a more\naccurate model. Furthermore, in this work, we introduce three different\nalgorithms for implementing an end-to-end SP2 framework that can scale\neffectively from thousands to hundreds of millions of items. Our user survey\nshows that an overwhelming fraction of users are likely to rate much more items\nto improve the overall recommendations when they can control what ratings will\nbe publicly shared with others.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 01:25:06 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Dareddy", "Manoj Reddy", ""], ["Das", "Ariyam", ""], ["Cho", "Junghoo", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "1806.00955", "submitter": "Omer Ben-Porat", "authors": "Omer Ben-Porat and Moshe Tennenholtz", "title": "A Game-Theoretic Approach to Recommendation Systems with Strategic\n  Content Providers", "comments": "A short version of this paper appears in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a game-theoretic approach to the study of recommendation systems\nwith strategic content providers. Such systems should be fair and stable.\nShowing that traditional approaches fail to satisfy these requirements, we\npropose the Shapley mediator. We show that the Shapley mediator fulfills the\nfairness and stability requirements, runs in linear time, and is the only\neconomically efficient mechanism satisfying these properties.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 05:08:56 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 08:38:24 GMT"}, {"version": "v3", "created": "Thu, 18 Oct 2018 13:32:21 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Ben-Porat", "Omer", ""], ["Tennenholtz", "Moshe", ""]]}, {"id": "1806.01059", "submitter": "Preethi Lahoti", "authors": "Preethi Lahoti, Krishna P. Gummadi, Gerhard Weikum", "title": "iFair: Learning Individually Fair Data Representations for Algorithmic\n  Decision Making", "comments": "Accepted at ICDE 2019. Please cite the ICDE 2019 proceedings version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People are rated and ranked, towards algorithmic decision making in an\nincreasing number of applications, typically based on machine learning.\nResearch on how to incorporate fairness into such tasks has prevalently pursued\nthe paradigm of group fairness: giving adequate success rates to specifically\nprotected groups. In contrast, the alternative paradigm of individual fairness\nhas received relatively little attention, and this paper advances this less\nexplored direction. The paper introduces a method for probabilistically mapping\nuser records into a low-rank representation that reconciles individual fairness\nand the utility of classifiers and rankings in downstream applications. Our\nnotion of individual fairness requires that users who are similar in all\ntask-relevant attributes such as job qualification, and disregarding all\npotentially discriminating attributes such as gender, should have similar\noutcomes. We demonstrate the versatility of our method by applying it to\nclassification and learning-to-rank tasks on a variety of real-world datasets.\nOur experiments show substantial improvements over the best prior work for this\nsetting.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 11:42:08 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 16:29:17 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Lahoti", "Preethi", ""], ["Gummadi", "Krishna P.", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1806.01103", "submitter": "Raphael Polig", "authors": "Raphael Polig, Kubilay Atasu, Laura Chiticariu, Christoph Hagleitner,\n  H. Peter Hofstee, Frederick R. Reiss, Eva Sitaridi, Huaiyu Zhu", "title": "Giving Text Analytics a Boost", "comments": null, "journal-ref": "IEEE Micro ( Volume: 34, Issue: 4, July-Aug. 2014 ) p. 6-14", "doi": "10.1109/MM.2014.69", "report-no": null, "categories": "cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of textual data has reached a new scale and continues to grow at\nan unprecedented rate. IBM's SystemT software is a powerful text analytics\nsystem, which offers a query-based interface to reveal the valuable information\nthat lies within these mounds of data. However, traditional server\narchitectures are not capable of analyzing the so-called \"Big Data\" in an\nefficient way, despite the high memory bandwidth that is available. We show\nthat by using a streaming hardware accelerator implemented in reconfigurable\nlogic, the throughput rates of the SystemT's information extraction queries can\nbe improved by an order of magnitude. We present how such a system can be\ndeployed by extending SystemT's existing compilation flow and by using a\nmulti-threaded communication interface that can efficiently use the bandwidth\nof the accelerator.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 09:58:59 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Polig", "Raphael", ""], ["Atasu", "Kubilay", ""], ["Chiticariu", "Laura", ""], ["Hagleitner", "Christoph", ""], ["Hofstee", "H. Peter", ""], ["Reiss", "Frederick R.", ""], ["Sitaridi", "Eva", ""], ["Zhu", "Huaiyu", ""]]}, {"id": "1806.01139", "submitter": "Jerome Dockes", "authors": "J\\'er\\^ome Dock\\`es (PARIETAL), Demian Wassermann (PARIETAL), Russell\n  Poldrack, Fabian Suchanek, Bertrand Thirion (PARIETAL), Ga\\\"el Varoquaux\n  (PARIETAL)", "title": "Text to brain: predicting the spatial distribution of neuroimaging\n  observations from text reports", "comments": null, "journal-ref": "MICCAI 2018 - 21st International Conference on Medical Image\n  Computing and Computer Assisted Intervention, Sep 2018, Granada, Spain.\n  pp.1-18, 2018", "doi": null, "report-no": null, "categories": "stat.ME cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the digital nature of magnetic resonance imaging, the resulting\nobservations are most frequently reported and stored in text documents. There\nis a trove of information untapped in medical health records, case reports, and\nmedical publications. In this paper, we propose to mine brain medical\npublications to learn the spatial distribution associated with anatomical\nterms. The problem is formulated in terms of minimization of a risk on\ndistributions which leads to a least-deviation cost function. An efficient\nalgorithm in the dual then learns the mapping from documents to brain\nstructures. Empirical results using coordinates extracted from the\nbrain-imaging literature show that i) models must adapt to semantic variation\nin the terms used to describe a given anatomical structure, ii) voxel-wise\nparameterization leads to higher likelihood of locations reported in unseen\ndocuments, iii) least-deviation cost outperforms least-square. As a proof of\nconcept for our method, we use our model of spatial distributions to predict\nthe distribution of specific neurological conditions from text-only reports.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 14:16:43 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 06:37:41 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 10:48:51 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Dock\u00e8s", "J\u00e9r\u00f4me", "", "PARIETAL"], ["Wassermann", "Demian", "", "PARIETAL"], ["Poldrack", "Russell", "", "PARIETAL"], ["Suchanek", "Fabian", "", "PARIETAL"], ["Thirion", "Bertrand", "", "PARIETAL"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL"]]}, {"id": "1806.01180", "submitter": "Kyungyun Lee", "authors": "Kyungyun Lee, Keunwoo Choi, Juhan Nam", "title": "Revisiting Singing Voice Detection: a Quantitative Review and the Future\n  Outlook", "comments": "Accepted to the 19th International Society of Music Information\n  Retrieval (ISMIR) Conference, Paris, France, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the vocal component plays a crucial role in popular music, singing\nvoice detection has been an active research topic in music information\nretrieval. Although several proposed algorithms have shown high performances,\nwe argue that there still is a room to improve to build a more robust singing\nvoice detection system. In order to identify the area of improvement, we first\nperform an error analysis on three recent singing voice detection systems.\nBased on the analysis, we design novel methods to test the systems on multiple\nsets of internally curated and generated data to further examine the pitfalls,\nwhich are not clearly revealed with the current datasets. From the experiment\nresults, we also propose several directions towards building a more robust\nsinging voice detector.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:25:37 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Lee", "Kyungyun", ""], ["Choi", "Keunwoo", ""], ["Nam", "Juhan", ""]]}, {"id": "1806.01264", "submitter": "Subhabrata Mukherjee", "authors": "Guineng Zheng, Subhabrata Mukherjee, Xin Luna Dong, Feifei Li", "title": "OpenTag: Open Attribute Value Extraction from Product Profiles [Deep\n  Learning, Active Learning, Named Entity Recognition]", "comments": "Proceedings of the 24th ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining, London, UK, August 19-23, 2018", "journal-ref": null, "doi": "10.1145/3219819.3219839", "report-no": null, "categories": "cs.CL cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extraction of missing attribute values is to find values describing an\nattribute of interest from a free text input. Most past related work on\nextraction of missing attribute values work with a closed world assumption with\nthe possible set of values known beforehand, or use dictionaries of values and\nhand-crafted features. How can we discover new attribute values that we have\nnever seen before? Can we do this with limited human annotation or supervision?\nWe study this problem in the context of product catalogs that often have\nmissing values for many attributes of interest.\n  In this work, we leverage product profile information such as titles and\ndescriptions to discover missing values of product attributes. We develop a\nnovel deep tagging model OpenTag for this extraction problem with the following\ncontributions: (1) we formalize the problem as a sequence tagging task, and\npropose a joint model exploiting recurrent neural networks (specifically,\nbidirectional LSTM) to capture context and semantics, and Conditional Random\nFields (CRF) to enforce tagging consistency, (2) we develop a novel attention\nmechanism to provide interpretable explanation for our model's decisions, (3)\nwe propose a novel sampling strategy exploring active learning to reduce the\nburden of human annotation. OpenTag does not use any dictionary or hand-crafted\nfeatures as in prior works. Extensive experiments in real-life datasets in\ndifferent domains show that OpenTag with our active learning strategy discovers\nnew attribute values from as few as 150 annotated samples (reduction in 3.3x\namount of annotation effort) with a high F-score of 83%, outperforming\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 19:41:07 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 17:29:28 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Zheng", "Guineng", ""], ["Mukherjee", "Subhabrata", ""], ["Dong", "Xin Luna", ""], ["Li", "Feifei", ""]]}, {"id": "1806.01351", "submitter": "Khoi-Nguyen Tran", "authors": "Khoi-Nguyen Tran and Jey Han Lau and Danish Contractor and Utkarsh\n  Gupta and Bikram Sengupta and Christopher J. Butler and Mukesh Mohania", "title": "Document Chunking and Learning Objective Generation for Instruction\n  Design", "comments": "Proceedings of the 11th International Conference on Education Data\n  Mining (EDM 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instructional Systems Design is the practice of creating of instructional\nexperiences that make the acquisition of knowledge and skill more efficient,\neffective, and appealing. Specifically in designing courses, an hour of\ntraining material can require between 30 to 500 hours of effort in sourcing and\norganizing reference data for use in just the preparation of course material.\nIn this paper, we present the first system of its kind that helps reduce the\neffort associated with sourcing reference material and course creation. We\npresent algorithms for document chunking and automatic generation of learning\nobjectives from content, creating descriptive content metadata to improve\ncontent-discoverability. Unlike existing methods, the learning objectives\ngenerated by our system incorporate pedagogically motivated Bloom's verbs. We\ndemonstrate the usefulness of our methods using real world data from the\nbanking industry and through a live deployment at a large pharmaceutical\ncompany.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 06:47:28 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 02:18:26 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Tran", "Khoi-Nguyen", ""], ["Lau", "Jey Han", ""], ["Contractor", "Danish", ""], ["Gupta", "Utkarsh", ""], ["Sengupta", "Bikram", ""], ["Butler", "Christopher J.", ""], ["Mohania", "Mukesh", ""]]}, {"id": "1806.01665", "submitter": "Rong Gong", "authors": "Rong Gong and Xavier Serra", "title": "Singing voice phoneme segmentation by hierarchically inferring syllable\n  and phoneme onset positions", "comments": "Interspeech 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the singing voice phoneme segmentation problem in\nthe singing training scenario by using language-independent information --\nonset and prior coarse duration. We propose a two-step method. In the first\nstep, we jointly calculate the syllable and phoneme onset detection functions\n(ODFs) using a convolutional neural network (CNN). In the second step, the\nsyllable and phoneme boundaries and labels are inferred hierarchically by using\na duration-informed hidden Markov model (HMM). To achieve the inference, we\nincorporate the a priori duration model as the transition probabilities and the\nODFs as the emission probabilities into the HMM. The proposed method is\ndesigned in a language-independent way such that no phoneme class labels are\nused. For the model training and algorithm evaluation, we collect a new jingju\n(also known as Beijing or Peking opera) solo singing voice dataset and manually\nannotate the boundaries and labels at phrase, syllable and phoneme levels. The\ndataset is publicly available. The proposed method is compared with a baseline\nmethod based on hidden semi-Markov model (HSMM) forced alignment. The\nevaluation results show that the proposed method outperforms the baseline by a\nlarge margin regarding both segmentation and onset detection tasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 12:54:19 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Gong", "Rong", ""], ["Serra", "Xavier", ""]]}, {"id": "1806.01973", "submitter": "Rex Ying", "authors": "Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L.\n  Hamilton, Jure Leskovec", "title": "Graph Convolutional Neural Networks for Web-Scale Recommender Systems", "comments": "KDD 2018", "journal-ref": null, "doi": "10.1145/3219819.3219890", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in deep neural networks for graph-structured data have\nled to state-of-the-art performance on recommender system benchmarks. However,\nmaking these methods practical and scalable to web-scale recommendation tasks\nwith billions of items and hundreds of millions of users remains a challenge.\nHere we describe a large-scale deep recommendation engine that we developed and\ndeployed at Pinterest. We develop a data-efficient Graph Convolutional Network\n(GCN) algorithm PinSage, which combines efficient random walks and graph\nconvolutions to generate embeddings of nodes (i.e., items) that incorporate\nboth graph structure as well as node feature information. Compared to prior GCN\napproaches, we develop a novel method based on highly efficient random walks to\nstructure the convolutions and design a novel training strategy that relies on\nharder-and-harder training examples to improve robustness and convergence of\nthe model. We also develop an efficient MapReduce model inference algorithm to\ngenerate embeddings using a trained model. We deploy PinSage at Pinterest and\ntrain it on 7.5 billion examples on a graph with 3 billion nodes representing\npins and boards, and 18 billion edges. According to offline metrics, user\nstudies and A/B tests, PinSage generates higher-quality recommendations than\ncomparable deep learning and graph-based alternatives. To our knowledge, this\nis the largest application of deep graph embeddings to date and paves the way\nfor a new generation of web-scale recommender systems based on graph\nconvolutional architectures.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 01:26:33 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Ying", "Rex", ""], ["He", "Ruining", ""], ["Chen", "Kaifeng", ""], ["Eksombatchai", "Pong", ""], ["Hamilton", "William L.", ""], ["Leskovec", "Jure", ""]]}, {"id": "1806.02056", "submitter": "Farhan Khawar", "authors": "Farhan Khawar, Nevin L. Zhang", "title": "Learning Hierarchical Item Categories from Implicit Feedback Data for\n  Efficient Recommendations and Browsing", "comments": "Published in SIGIR 2019 Workshop on ExplainAble Recommendation and\n  Search (EARS'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching, browsing, and recommendations are common ways in which the \"choice\noverload\" faced by users in the online marketplace can be mitigated. In this\npaper we propose the use of hierarchical item categories, obtained from\nimplicit feedback data, to enable efficient browsing and recommendations. We\npresent a method of creating hierarchical item categories from implicit\nfeedback data only i.e., without any other information on the items like name,\ngenre etc. Categories created in this fashion are based on users'\nco-consumption of items. Thus, they can be more useful for users in finding\ninteresting and relevant items while they are browsing through the hierarchy.\nWe also show that this item hierarchy can be useful in making category based\nrecommendations, which makes the recommendations more explainable and increases\nthe diversity of the recommendations without compromising much on the accuracy.\nItem hierarchy can also be useful in the creation of an automatic item taxonomy\nskeleton by bypassing manual labeling and annotation. This can especially be\nuseful for small vendors. Our data-driven hierarchical categories are based on\nhierarchical latent tree analysis (HLTA) which has been previously used for\ntext analysis. We present a scaled up learning algorithm \\emph{HLTA-Forest} so\nthat HLTA can be applied to implicit feedback data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 08:25:58 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 07:20:20 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Khawar", "Farhan", ""], ["Zhang", "Nevin L.", ""]]}, {"id": "1806.02281", "submitter": "Rohan Ramanath", "authors": "Rohan Ramanath, Gungor Polatkan, Liqin Xu, Harold Lee, Bo Hu, Shan\n  Zhou", "title": "Deploying Deep Ranking Models for Search Verticals", "comments": "Published at the SysML Conference - 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an architecture executing a complex machine\nlearning model such as a neural network capturing semantic similarity between a\nquery and a document; and deploy to a real-world production system serving\n500M+users. We present the challenges that arise in a real-world system and how\nwe solve them. We demonstrate that our architecture provides competitive\nmodeling capability without any significant performance impact to the system in\nterms of latency. Our modular solution and insights can be used by other\nreal-world search systems to realize and productionize recent gains in neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 16:20:24 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Ramanath", "Rohan", ""], ["Polatkan", "Gungor", ""], ["Xu", "Liqin", ""], ["Lee", "Harold", ""], ["Hu", "Bo", ""], ["Zhou", "Shan", ""]]}, {"id": "1806.02557", "submitter": "Zhenpeng Chen", "authors": "Zhenpeng Chen and Sheng Shen and Ziniu Hu and Xuan Lu and Qiaozhu Mei\n  and Xuanzhe Liu", "title": "Emoji-Powered Representation Learning for Cross-Lingual Sentiment\n  Classification", "comments": "Accepted at The Web Conference 2019 (WWW 2019). Please include WWW in\n  any citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment classification typically relies on a large amount of labeled data.\nIn practice, the availability of labels is highly imbalanced among different\nlanguages, e.g., more English texts are labeled than texts in any other\nlanguages, which creates a considerable inequality in the quality of related\ninformation services received by users speaking different languages. To tackle\nthis problem, cross-lingual sentiment classification approaches aim to transfer\nknowledge learned from one language that has abundant labeled examples (i.e.,\nthe source language, usually English) to another language with fewer labels\n(i.e., the target language). The source and the target languages are usually\nbridged through off-the-shelf machine translation tools. Through such a\nchannel, cross-language sentiment patterns can be successfully learned from\nEnglish and transferred into the target languages. This approach, however,\noften fails to capture sentiment knowledge specific to the target language, and\nthus compromises the accuracy of the downstream classification task. In this\npaper, we employ emojis, which are widely available in many languages, as a new\nchannel to learn both the cross-language and the language-specific sentiment\npatterns. We propose a novel representation learning method that uses emoji\nprediction as an instrument to learn respective sentiment-aware representations\nfor each language. The learned representations are then integrated to\nfacilitate cross-lingual sentiment classification. The proposed method\ndemonstrates state-of-the-art performance on benchmark datasets, which is\nsustained even when sentiment labels are scarce.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 08:23:11 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 07:16:15 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Chen", "Zhenpeng", ""], ["Shen", "Sheng", ""], ["Hu", "Ziniu", ""], ["Lu", "Xuan", ""], ["Mei", "Qiaozhu", ""], ["Liu", "Xuanzhe", ""]]}, {"id": "1806.02743", "submitter": "Martin Toepfer", "authors": "Martin Toepfer and Christin Seifert", "title": "Content-Based Quality Estimation for Automatic Subject Indexing of Short\n  Texts under Precision and Recall Constraints", "comments": "authors' manuscript, paper submitted to TPDL-2018 conference, 12\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic annotations have to satisfy quality constraints to be useful for\ndigital libraries, which is particularly challenging on large and diverse\ndatasets. Confidence scores of multi-label classification methods typically\nrefer only to the relevance of particular subjects, disregarding indicators of\ninsufficient content representation at the document-level. Therefore, we\npropose a novel approach that detects documents rather than concepts where\nquality criteria are met. Our approach uses a deep, multi-layered regression\narchitecture, which comprises a variety of content-based indicators. We\nevaluated multiple configurations using text collections from law and\neconomics, where the available content is restricted to very short texts.\nNotably, we demonstrate that the proposed quality estimation technique can\ndetermine subsets of the previously unseen data where considerable gains in\ndocument-level recall can be achieved, while upholding precision at the same\ntime. Hence, the approach effectively performs a filtering that ensures high\ndata quality standards in operative information retrieval systems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 15:58:59 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Toepfer", "Martin", ""], ["Seifert", "Christin", ""]]}, {"id": "1806.02863", "submitter": "Rahul Gupta", "authors": "Rahul Gupta, Saurabh Sahu, Carol Espy-Wilson, Shrikanth Narayanan", "title": "Semi-supervised and Transfer learning approaches for low resource\n  sentiment classification", "comments": "5 pages, Accepted to International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment classification involves quantifying the affective reaction of a\nhuman to a document, media item or an event. Although researchers have\ninvestigated several methods to reliably infer sentiment from lexical, speech\nand body language cues, training a model with a small set of labeled datasets\nis still a challenge. For instance, in expanding sentiment analysis to new\nlanguages and cultures, it may not always be possible to obtain comprehensive\nlabeled datasets. In this paper, we investigate the application of\nsemi-supervised and transfer learning methods to improve performances on low\nresource sentiment classification tasks. We experiment with extracting dense\nfeature representations, pre-training and manifold regularization in enhancing\nthe performance of sentiment classification systems. Our goal is a coherent\nimplementation of these methods and we evaluate the gains achieved by these\nmethods in matched setting involving training and testing on a single corpus\nsetting as well as two cross corpora settings. In both the cases, our\nexperiments demonstrate that the proposed methods can significantly enhance the\nmodel performance against a purely supervised approach, particularly in cases\ninvolving a handful of training data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 18:59:26 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Gupta", "Rahul", ""], ["Sahu", "Saurabh", ""], ["Espy-Wilson", "Carol", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "1806.03144", "submitter": "Bernard Jacquemin", "authors": "Eric Kergosien (GERIICO), Amin Farvardin (UMR TETIS), Maguelonne\n  Teisseire (UMR TETIS), Marie-No\\\"elle Bessagnet (LIUPPA), Joachim Sch\\\"opfel\n  (GERIICO), St\\'ephane Chaudiron (GERIICO), Bernard Jacquemin (GERIICO), Annig\n  Le Parc-Lacayrelle (LIUPPA), Mathieu Roche (UMR TETIS), Christian Sallaberry\n  (LIUPPA), Jean-Philippe Tonneau (UMR TETIS)", "title": "Automatic Identification of Research Fields in Scientific Papers", "comments": null, "journal-ref": "Proceedings of the Eleventh International Conference on Language\n  Resources and Evaluation, pp.1902-1907, 2018, http://lrec2018.lrec-conf.org", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TERRE-ISTEX project aims to identify scientific research dealing with\nspecific geographical territories areas based on heterogeneous digital content\navailable in scientific papers. The project is divided into three main work\npackages: (1) identification of the periods and places of empirical studies,\nand which reflect the publications resulting from the analyzed text samples,\n(2) identification of the themes which appear in these documents, and (3)\ndevelopment of a web-based geographical information retrieval tool (GIR). The\nfirst two actions combine Natural Language Processing patterns with text mining\nmethods. The integration of the spatial, thematic and temporal dimensions in a\nGIR contributes to a better understanding of what kind of research has been\ncarried out, of its topics and its geographical and historical coverage.\nAnother originality of the TERRE-ISTEX project is the heterogeneous character\nof the corpus, including PhD theses and scientific articles from the ISTEX\ndigital libraries and the CIRAD research center.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 13:29:46 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Kergosien", "Eric", "", "GERIICO"], ["Farvardin", "Amin", "", "UMR TETIS"], ["Teisseire", "Maguelonne", "", "UMR TETIS"], ["Bessagnet", "Marie-No\u00eblle", "", "LIUPPA"], ["Sch\u00f6pfel", "Joachim", "", "GERIICO"], ["Chaudiron", "St\u00e9phane", "", "GERIICO"], ["Jacquemin", "Bernard", "", "GERIICO"], ["Parc-Lacayrelle", "Annig Le", "", "LIUPPA"], ["Roche", "Mathieu", "", "UMR TETIS"], ["Sallaberry", "Christian", "", "LIUPPA"], ["Tonneau", "Jean-Philippe", "", "UMR TETIS"]]}, {"id": "1806.03242", "submitter": "Mansaf Alam Dr", "authors": "Leena Khanna, Shailendra Narayan Singh, Mansaf Alam", "title": "Multidimensional Analysis of Psychological Factors affecting Students\n  Academic Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Academic performance of any individual is dependent upon numerous aspects\nregarding the day to day life of the individual under consideration. Academic\nperformance is measured in terms of the grade point average or GPA as it is\ncalled. Grade point average is dependent not only on the faculty but also on\nvarious psychological parameters including the study habits, social anxiety and\nallied. In this study, a detail analysis of numerous psychological factors\nimpacting the grade point was carried and based upon various psychological\nfactors the performance for the student in forth coming examination was\nforecasted.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 07:34:15 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Khanna", "Leena", ""], ["Singh", "Shailendra Narayan", ""], ["Alam", "Mansaf", ""]]}, {"id": "1806.03277", "submitter": "Yueming Sun", "authors": "Yueming Sun, Yi Zhang", "title": "Conversational Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A personalized conversational sales agent could have much commercial\npotential. E-commerce companies such as Amazon, eBay, JD, Alibaba etc. are\npiloting such kind of agents with their users. However, the research on this\ntopic is very limited and existing solutions are either based on single round\nadhoc search engine or traditional multi round dialog system. They usually only\nutilize user inputs in the current session, ignoring users' long term\npreferences. On the other hand, it is well known that sales conversion rate can\nbe greatly improved based on recommender systems, which learn user preferences\nbased on past purchasing behavior and optimize business oriented metrics such\nas conversion rate or expected revenue. In this work, we propose to integrate\nresearch in dialog systems and recommender systems into a novel and unified\ndeep reinforcement learning framework to build a personalized conversational\nrecommendation agent that optimizes a per session based utility function.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 17:15:28 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Sun", "Yueming", ""], ["Zhang", "Yi", ""]]}, {"id": "1806.03368", "submitter": "Habeeb Hooshmand", "authors": "Habeeb Hooshmand, Joseph Martinsen, Jonathan Arauco, Alishah\n  Dholasaniya, Bhavik Bhatt", "title": "An Exploration of H-1B Visa Applications in the United States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The H-1B visa program is a very important tool for US-based businesses and\neducational institutes to recruit foreign talent. While the ultimate decision\nto certify an application lies with the United States Department of Labor,\nthere are signals that can be used to determine whether an application is\nlikely to be certified or denied. In this paper we first perform a data-driven\nexploratory analysis. We then leverage the features to train several\nclassifiers and compare their performance. Finally, we discuss the implications\nof this work and future work that can be done in this area.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 22:46:08 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Hooshmand", "Habeeb", ""], ["Martinsen", "Joseph", ""], ["Arauco", "Jonathan", ""], ["Dholasaniya", "Alishah", ""], ["Bhatt", "Bhavik", ""]]}, {"id": "1806.03482", "submitter": "Won-Yong Shin", "authors": "Cong Tran, Won-Yong Shin, Sang-Il Choi", "title": "DIR-ST$^2$: Delineation of Imprecise Regions Using\n  Spatio--Temporal--Textual Information", "comments": "11 pages, 12 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An imprecise region is referred to as a geographical area without a\nclearly-defined boundary in the literature. Previous clustering-based\napproaches exploit spatial information to find such regions. However, the prior\nstudies suffer from the following two problems: the subjectivity in selecting\nclustering parameters and the inclusion of a large portion of the undesirable\nregion (i.e., a large number of noise points). To overcome these problems, we\npresent DIR-ST$^2$, a novel framework for delineating an imprecise region by\niteratively performing density-based clustering, namely DBSCAN, along with not\nonly spatio--textual information but also temporal information on social media.\nSpecifically, we aim at finding a proper radius of a circle used in the\niterative DBSCAN process by gradually reducing the radius for each iteration in\nwhich the temporal information acquired from all resulting clusters are\nleveraged. Then, we propose an efficient and automated algorithm delineating\nthe imprecise region via hierarchical clustering. Experiment results show that\nby virtue of the significant noise reduction in the region, our DIR-ST$^2$\nmethod outperforms the state-of-the-art approach employing one-class support\nvector machine in terms of the $\\mathcal{F}_1$ score from comparison with\nprecisely-defined regions regarded as a ground truth, and returns apparently\nbetter delineation of imprecise regions. The computational complexity of\nDIR-ST$^2$ is also analytically and numerically shown.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 14:49:01 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Tran", "Cong", ""], ["Shin", "Won-Yong", ""], ["Choi", "Sang-Il", ""]]}, {"id": "1806.03483", "submitter": "Chengyuan Zhang", "authors": "Chengyuan Zhang, Ruipeng Chen, Lei Zhu, Anfeng Liu, Yunwu Lin and Fang\n  Huang", "title": "Hierarchical Information Quadtree: Efficient Spatial Temporal Image\n  Search for Multimedia Stream", "comments": "Published at Multimedia Tools and Applications. arXiv admin note:\n  text overlap with arXiv:1805.02009", "journal-ref": null, "doi": "10.1007/s11042-018-6284-y", "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive amount of multimedia data that contain times- tamps and geographical\ninformation are being generated at an unprecedented scale in many emerging\napplications such as photo sharing web site and social networks applications.\nDue to their importance, a large body of work has focused on efficiently\ncomputing various spatial image queries. In this paper,we study the spatial\ntemporal image query which considers three important constraints during the\nsearch including time recency, spatial proximity and visual relevance. A novel\nindex structure, namely Hierarchical Information Quadtree(\\hiq), to efficiently\ninsert/delete spatial temporal images with high arrive rates. Base on \\hiq an\nefficient algorithm is developed to support spatial temporal image query. We\nshow via extensive experimentation with real spatial databases clearly\ndemonstrate the efficiency of our methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 14:53:07 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 07:53:15 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Zhang", "Chengyuan", ""], ["Chen", "Ruipeng", ""], ["Zhu", "Lei", ""], ["Liu", "Anfeng", ""], ["Lin", "Yunwu", ""], ["Huang", "Fang", ""]]}, {"id": "1806.03529", "submitter": "Mor Geva", "authors": "Mor Geva and Jonathan Berant", "title": "Learning to Search in Long Documents Using Document Structure", "comments": "COLING 2018 (camera ready version); v2: added acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reading comprehension models are based on recurrent neural networks that\nsequentially process the document tokens. As interest turns to answering more\ncomplex questions over longer documents, sequential reading of large portions\nof text becomes a substantial bottleneck. Inspired by how humans use document\nstructure, we propose a novel framework for reading comprehension. We represent\ndocuments as trees, and model an agent that learns to interleave quick\nnavigation through the document tree with more expensive answer extraction. To\nencourage exploration of the document tree, we propose a new algorithm, based\non Deep Q-Network (DQN), which strategically samples tree nodes at training\ntime. Empirically we find our algorithm improves question answering performance\ncompared to DQN and a strong information-retrieval (IR) baseline, and that\nensembling our model with the IR baseline results in further gains in\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 18:55:00 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 13:14:28 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Geva", "Mor", ""], ["Berant", "Jonathan", ""]]}, {"id": "1806.03555", "submitter": "Aman Agarwal", "authors": "Aman Agarwal, Ivan Zaitsev, Thorsten Joachims", "title": "Consistent Position Bias Estimation without Online Interventions for\n  Learning-to-Rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presentation bias is one of the key challenges when learning from implicit\nfeedback in search engines, as it confounds the relevance signal with\nuninformative signals due to position in the ranking, saliency, and other\npresentation factors. While it was recently shown how counterfactual\nlearning-to-rank (LTR) approaches \\cite{Joachims/etal/17a} can provably\novercome presentation bias if observation propensities are known, it remains to\nshow how to accurately estimate these propensities. In this paper, we propose\nthe first method for producing consistent propensity estimates without manual\nrelevance judgments, disruptive interventions, or restrictive relevance\nmodeling assumptions. We merely require that we have implicit feedback data\nfrom multiple different ranking functions. Furthermore, we argue that our\nestimation technique applies to an extended class of Contextual Position-Based\nPropensity Models, where propensities not only depend on position but also on\nobservable features of the query and document. Initial simulation studies\nconfirm that the approach is scalable, accurate, and robust.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 23:04:56 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Agarwal", "Aman", ""], ["Zaitsev", "Ivan", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1806.03568", "submitter": "Nan Wang", "authors": "Nan Wang, Hongning Wang, Yiling Jia, Yue Yin", "title": "Explainable Recommendation via Multi-Task Learning in Opinionated Text\n  Data", "comments": "10 pages, SIGIR 2018", "journal-ref": null, "doi": "10.1145/3209978.3210010", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining automatically generated recommendations allows users to make more\ninformed and accurate decisions about which results to utilize, and therefore\nimproves their satisfaction. In this work, we develop a multi-task learning\nsolution for explainable recommendation. Two companion learning tasks of user\npreference modeling for recommendation} and \\textit{opinionated content\nmodeling for explanation are integrated via a joint tensor factorization. As a\nresult, the algorithm predicts not only a user's preference over a list of\nitems, i.e., recommendation, but also how the user would appreciate a\nparticular item at the feature level, i.e., opinionated textual explanation.\nExtensive experiments on two large collections of Amazon and Yelp reviews\nconfirmed the effectiveness of our solution in both recommendation and\nexplanation tasks, compared with several existing recommendation algorithms.\nAnd our extensive user study clearly demonstrates the practical value of the\nexplainable recommendations generated by our algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 01:43:40 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Wang", "Nan", ""], ["Wang", "Hongning", ""], ["Jia", "Yiling", ""], ["Yin", "Yue", ""]]}, {"id": "1806.03576", "submitter": "Yu Zhan", "authors": "Yu Zhan, Wan-Lei Zhao", "title": "Instance Search via Instance Level Segmentation and Feature\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance search is an interesting task as well as a challenging issue due to\nthe lack of effective feature representation. In this paper, an instance level\nfeature representation built upon fully convolutional instance-aware\nsegmentation is proposed. The feature is ROI-pooled from the segmented instance\nregion. So that instances in various sizes and layouts are represented by deep\nfeatures in uniform length. This representation is further enhanced by the use\nof deformable ResNeXt blocks. Superior performance is observed in terms of its\ndistinctiveness and scalability on a challenging evaluation dataset built by\nourselves. In addition, the proposed enhancement on the network structure also\nshows superior performance on the instance segmentation task.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 02:39:52 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 03:14:51 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Zhan", "Yu", ""], ["Zhao", "Wan-Lei", ""]]}, {"id": "1806.03577", "submitter": "Weinan Zhang", "authors": "Weinan Zhang", "title": "Generative Adversarial Nets for Information Retrieval: Fundamentals and\n  Advances", "comments": "4 pages, SIGIR 2018 tutorial", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative adversarial nets (GANs) have been widely studied during the recent\ndevelopment of deep learning and unsupervised learning. With an adversarial\ntraining mechanism, GAN manages to train a generative model to fit the\nunderlying unknown real data distribution under the guidance of the\ndiscriminative model estimating whether a data instance is real or generated.\nSuch a framework is originally proposed for fitting continuous data\ndistribution such as images, thus it is not straightforward to be directly\napplied to information retrieval scenarios where the data is mostly discrete,\nsuch as IDs, text and graphs. In this tutorial, we focus on discussing the GAN\ntechniques and the variants on discrete data fitting in various information\nretrieval scenarios. (i) We introduce the fundamentals of GAN framework and its\ntheoretic properties; (ii) we carefully study the promising solutions to extend\nGAN onto discrete data generation; (iii) we introduce IRGAN, the fundamental\nGAN framework of fitting single ID data distribution and the direct application\non information retrieval; (iv) we further discuss the task of sequential\ndiscrete data generation tasks, e.g., text generation, and the corresponding\nGAN solutions; (v) we present the most recent work on graph/network data\nfitting with node embedding techniques by GANs. Meanwhile, we also introduce\nthe relevant open-source platforms such as IRGAN and Texygen to help audience\nconduct research experiments on GANs in information retrieval. Finally, we\nconclude this tutorial with a comprehensive summarization and a prospect of\nfurther research directions for GANs in information retrieval.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 03:28:10 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Zhang", "Weinan", ""]]}, {"id": "1806.03648", "submitter": "Ken Yano", "authors": "Ken Yano", "title": "Neural Disease Named Entity Extraction with Character-based BiLSTM+CRF\n  in Japanese Medical Text", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an 'end-to-end' character-based recurrent neural network that\nextracts disease named entities from a Japanese medical text and simultaneously\njudges its modality as either positive or negative; i.e., the mentioned disease\nor symptom is affirmed or negated. The motivation to adopt neural networks is\nto learn effective lexical and structural representation features for Entity\nRecognition and also for Positive/Negative classification from an annotated\ncorpora without explicitly providing any rule-based or manual feature sets. We\nconfirmed the superiority of our method over previous char-based CRF or SVM\nmethods in the results.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 12:34:00 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Yano", "Ken", ""]]}, {"id": "1806.03688", "submitter": "Michael Bommarito II", "authors": "Michael J Bommarito II and Daniel Martin Katz and Eric M Detterman", "title": "LexNLP: Natural language processing and information extraction for legal\n  and regulatory texts", "comments": "9 pages, 0 figures; see also\n  https://github.com/LexPredict/lexpredict-lexnlp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LexNLP is an open source Python package focused on natural language\nprocessing and machine learning for legal and regulatory text. The package\nincludes functionality to (i) segment documents, (ii) identify key text such as\ntitles and section headings, (iii) extract over eighteen types of structured\ninformation like distances and dates, (iv) extract named entities such as\ncompanies and geopolitical entities, (v) transform text into features for model\ntraining, and (vi) build unsupervised and supervised models such as word\nembedding or tagging models. LexNLP includes pre-trained models based on\nthousands of unit tests drawn from real documents available from the SEC EDGAR\ndatabase as well as various judicial and regulatory proceedings. LexNLP is\ndesigned for use in both academic research and industrial applications, and is\ndistributed at https://github.com/LexPredict/lexpredict-lexnlp.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 16:55:40 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Bommarito", "Michael J", "II"], ["Katz", "Daniel Martin", ""], ["Detterman", "Eric M", ""]]}, {"id": "1806.03733", "submitter": "Fei Mi", "authors": "Fei Mi, Boi Faltings", "title": "Context Tree for Adaptive Session-based Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been growing interests in recent years from both practical and\nresearch perspectives for session-based recommendation tasks as long-term user\nprofiles do not often exist in many real-life recommendation applications. In\nthis case, recommendations for user's immediate next actions need to be\ngenerated based on patterns in anonymous short sessions. An often overlooked\naspect is that new items with limited observations arrive continuously in many\ndomains (e.g. news and discussion forums). Therefore, recommendations need to\nbe adaptive to such frequent changes. In this paper, we benchmark a new\nnonparametric method called context tree (CT) against various state-of-the-art\nmethods on extensive datasets for session-based recommendation task. Apart from\nthe standard static evaluation protocol adopted by previous literatures, we\ninclude an adaptive configuration to mimic the situation when new items with\nlimited observations arrives continuously. Our results show that CT outperforms\ntwo best-performing approaches (recurrent neural network; heuristic-based\nnearest neighbor) in majority of the tested configurations and datasets. We\nanalyze reasons for this and demonstrate that it is because of the better\nadaptation to changes in the domain, as well as the remarkable capability to\nlearn static sequential patterns. Moreover, our running time analysis\nillustrates the efficiency of using CT as other nonparametric methods.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 22:10:29 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Mi", "Fei", ""], ["Faltings", "Boi", ""]]}, {"id": "1806.03790", "submitter": "Daniel Cohen", "authors": "Daniel Cohen, Scott M. Jordan, W. Bruce Croft", "title": "Distributed Evaluations: Ending Neural Point Metrics", "comments": "ACM SIGIR - LND4IR Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of neural models across the field of information retrieval,\nnumerous publications have incrementally pushed the envelope of performance for\na multitude of IR tasks. However, these networks often sample data in random\norder, are initialized randomly, and their success is determined by a single\nevaluation score. These issues are aggravated by neural models achieving\nincremental improvements from previous neural baselines, leading to multiple\nnear state of the art models that are difficult to reproduce and quickly become\ndeprecated. As neural methods are starting to be incorporated into low resource\nand noisy collections that further exacerbate this issue, we propose evaluating\nneural models both over multiple random seeds and a set of hyperparameters\nwithin $\\epsilon$ distance of the chosen configuration for a given metric.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 03:26:29 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Cohen", "Daniel", ""], ["Jordan", "Scott M.", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1806.04004", "submitter": "Nicolas Fiorini", "authors": "Nicolas Fiorini, Kathi Canese, Rostyslav Bryzgunov, Ievgeniia\n  Radetska, Asta Gindulyte, Martin Latterner, Vadim Miller, Maxim Osipov,\n  Michael Kholodov, Grisha Starchenko, Evgeny Kireev, Zhiyong Lu", "title": "PubMed Labs: An experimental platform for improving biomedical\n  literature search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PubMed is a freely accessible system for searching the biomedical literature,\nwith approximately 2.5 million users worldwide on an average workday. We have\nrecently developed PubMed Labs (www.pubmed.gov/labs), an experimental platform\nfor users to test new features/tools and provide feedback, which enables us to\nmake more informed decisions about potential changes to improve the search\nquality and overall usability of PubMed. In doing so, we hope to better meet\nour user needs in an era of information overload. Another novel aspect of\nPubMed Labs lies in its mobile-first and responsive layout, which offers better\nsupport for accessing PubMed on the increasingly popular use of mobile and\nsmall-screen devices. Currently, PubMed Labs only includes a core subset of\nPubMed functionalities, e.g. search, facets. We encourage users to test PubMed\nLabs and share their experience with us, based on which we expect to\ncontinuously improve PubMed Labs with more advanced features and better user\nexperience.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 14:10:01 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Fiorini", "Nicolas", ""], ["Canese", "Kathi", ""], ["Bryzgunov", "Rostyslav", ""], ["Radetska", "Ievgeniia", ""], ["Gindulyte", "Asta", ""], ["Latterner", "Martin", ""], ["Miller", "Vadim", ""], ["Osipov", "Maxim", ""], ["Kholodov", "Michael", ""], ["Starchenko", "Grisha", ""], ["Kireev", "Evgeny", ""], ["Lu", "Zhiyong", ""]]}, {"id": "1806.04277", "submitter": "J\\'er\\'emy Barbay", "authors": "J\\'er\\'emy Barbay, Andr\\'es Olivares", "title": "Indexed Dynamic Programming to boost Edit Distance and LCSS Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are efficient dynamic programming solutions to the computation of the\nEdit Distance from $S\\in[1..\\sigma]^n$ to $T\\in[1..\\sigma]^m$, for many natural\nsubsets of edit operations, typically in time within $O(nm)$ in the worst-case\nover strings of respective lengths $n$ and $m$ (which is likely to be optimal),\nand in time within $O(n{+}m)$ in some special cases (e.g. disjoint alphabets).\nWe describe how indexing the strings (in linear time), and using such an index\nto refine the recurrence formulas underlying the dynamic programs, yield faster\nalgorithms in a variety of models, on a continuum of classes of instances of\nintermediate difficulty between the worst and the best case, thus refining the\nanalysis beyond the worst case analysis. As a side result, we describe similar\nproperties for the computation of the Longest Common Sub Sequence $LCSS(S,T)$\nbetween $S$ and $T$, since it is a particular case of Edit Distance, and we\ndiscuss the application of similar algorithmic and analysis techniques for\nother dynamic programming solutions. More formally, we propose a parameterized\nanalysis of the computational complexity of the Edit Distance for various set\nof operators and of the Longest Common Sub Sequence in function of the area of\nthe dynamic program matrix relevant to the computation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 00:22:04 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Barbay", "J\u00e9r\u00e9my", ""], ["Olivares", "Andr\u00e9s", ""]]}, {"id": "1806.04411", "submitter": "Sheikh Muhammad Sarwar", "authors": "John Foley, Sheikh Muhammad Sarwar, James Allan", "title": "Named Entity Recognition with Extremely Limited Data", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional information retrieval treats named entity recognition as a\npre-indexing corpus annotation task, allowing entity tags to be indexed and\nused during search. Named entity taggers themselves are typically trained on\nthousands or tens of thousands of examples labeled by humans.\n  However, there is a long tail of named entities classes, and for these cases,\nlabeled data may be impossible to find or justify financially. We propose\nexploring named entity recognition as a search task, where the named entity\nclass of interest is a query, and entities of that class are the relevant\n\"documents\". What should that query look like? Can we even perform NER-style\nlabeling with tens of labels? This study presents an exploration of CRF-based\nNER models with handcrafted features and of how we might transform them into\nsearch queries.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 09:33:23 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 17:12:27 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Foley", "John", ""], ["Sarwar", "Sheikh Muhammad", ""], ["Allan", "James", ""]]}, {"id": "1806.04425", "submitter": "Gregory Goren", "authors": "Gregory Goren, Oren Kurland, Moshe Tennenholtz, Fiana Raiber", "title": "Ranking Robustness Under Adversarial Document Manipulations", "comments": null, "journal-ref": null, "doi": "10.1145/3209978.3210012", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many queries in the Web retrieval setting there is an on-going ranking\ncompetition: authors manipulate their documents so as to promote them in\nrankings. Such competitions can have unwarranted effects not only in terms of\nretrieval effectiveness, but also in terms of ranking robustness. A case in\npoint, rankings can (rapidly) change due to small indiscernible perturbations\nof documents. While there has been a recent growing interest in analyzing the\nrobustness of classifiers to adversarial manipulations, there has not yet been\na study of the robustness of relevance-ranking functions. We address this\nchallenge by formally analyzing different definitions and aspects of the\nrobustness of learning-to-rank-based ranking functions. For example, we\nformally show that increased regularization of linear ranking functions\nincreases ranking robustness. This finding leads us to conjecture that\ndecreased variance of any ranking function results in increased robustness. We\npropose several measures for quantifying ranking robustness and use them to\nanalyze ranking competitions between documents' authors. The empirical findings\nsupport our formal analysis and conjecture for both RankSVM and LambdaMART.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 10:03:42 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 12:26:00 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Goren", "Gregory", ""], ["Kurland", "Oren", ""], ["Tennenholtz", "Moshe", ""], ["Raiber", "Fiana", ""]]}, {"id": "1806.04511", "submitter": "Aysu Can", "authors": "Ethem F. Can, Aysu Ezen-Can, Fazli Can", "title": "Multilingual Sentiment Analysis: An RNN-Based Framework for Limited Data", "comments": "ACM SIGIR 2018 Workshop on Learning from Limited or Noisy Data\n  (LND4IR'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is a widely studied NLP task where the goal is to\ndetermine opinions, emotions, and evaluations of users towards a product, an\nentity or a service that they are reviewing. One of the biggest challenges for\nsentiment analysis is that it is highly language dependent. Word embeddings,\nsentiment lexicons, and even annotated data are language specific. Further,\noptimizing models for each language is very time consuming and labor intensive\nespecially for recurrent neural network models. From a resource perspective, it\nis very challenging to collect data for different languages.\n  In this paper, we look for an answer to the following research question: can\na sentiment analysis model trained on a language be reused for sentiment\nanalysis in other languages, Russian, Spanish, Turkish, and Dutch, where the\ndata is more limited? Our goal is to build a single model in the language with\nthe largest dataset available for the task, and reuse it for languages that\nhave limited resources. For this purpose, we train a sentiment analysis model\nusing recurrent neural networks with reviews in English. We then translate\nreviews in other languages and reuse this model to evaluate the sentiments.\nExperimental results show that our robust approach of single model trained on\nEnglish reviews statistically significantly outperforms the baselines in\nseveral different languages.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 14:01:31 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Can", "Ethem F.", ""], ["Ezen-Can", "Aysu", ""], ["Can", "Fazli", ""]]}, {"id": "1806.04735", "submitter": "Hussein Suleman", "authors": "Hussein Suleman", "title": "Information Retrieval in African Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing Information Retrieval (IR) tools and techniques in African\nlanguages suffers from the dual problems of a lack of algorithms and very small\ntest data collections. This affects the creation of practical IR systems and\nlimits the ability to apply IR to address human and socio-economic problems,\nwhich is an urgent need in poor countries. This position paper presents an\noverview of recent and current work conducted at the University of Cape Town in\nthis area. While many problems have been investigated at an early stage,\nlimited dataset sizes for local African languages still persists as a\nsignificant limitation and stumbling block.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 19:36:52 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Suleman", "Hussein", ""]]}, {"id": "1806.04815", "submitter": "Hamed Zamani", "authors": "Hamed Zamani, W. Bruce Croft", "title": "Towards Theoretical Understanding of Weak Supervision for Information\n  Retrieval", "comments": "A position paper accepted to the 2018 ACM SIGIR Workshop on Learning\n  from Limited or Noisy Data for Information Retrieval (LND4IR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network approaches have recently shown to be effective in several\ninformation retrieval (IR) tasks. However, neural approaches often require\nlarge volumes of training data to perform effectively, which is not always\navailable. To mitigate the shortage of labeled data, training neural IR models\nwith weak supervision has been recently proposed and received considerable\nattention in the literature. In weak supervision, an existing model\nautomatically generates labels for a large set of unlabeled data, and a machine\nlearning model is further trained on the generated \"weak\" data. Surprisingly,\nit has been shown in prior art that the trained neural model can outperform the\nweak labeler by a significant margin. Although these obtained improvements have\nbeen intuitively justified in previous work, the literature still lacks\ntheoretical justification for the observed empirical findings. In this position\npaper, we propose to theoretically study weak supervision, in particular for IR\ntasks, e.g., learning to rank. We briefly review a set of our recent\ntheoretical findings that shed light on learning from weakly supervised data,\nand provide guidelines on how train learning to rank models with weak\nsupervision.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 01:45:11 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Zamani", "Hamed", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1806.05004", "submitter": "John Foley", "authors": "John Foley", "title": "Explainable Agreement through Simulation for Tasks with Subjective\n  Labels", "comments": "2-page position paper at LND4IR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of information retrieval often works with limited and noisy data in\nan attempt to classify documents into subjective categories, e.g., relevance,\nsentiment and controversy. We typically quantify a notion of agreement to\nunderstand the difficulty of the labeling task, but when we present final\nresults, we do so using measures that are unaware of agreement or the inherent\nsubjectivity of the task. We propose using user simulation to understand the\neffect size of this noisy agreement data. By simulating truth and predictions,\nwe can understand the maximum scores a dataset can support: for if a classifier\nis doing better than a reasonable model of a human, we cannot conclude that it\nis actually better, but that it may be learning noise present in the dataset.\nWe present a brief case study on controversy detection that concludes that a\ncommonly-used dataset has been exhausted: in order to advance the\nstate-of-the-art, more data must be gathered at the current level of label\nagreement in order to distinguish between techniques with confidence.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 13:04:01 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Foley", "John", ""]]}, {"id": "1806.05180", "submitter": "Andreas Hanselowski Dr.", "authors": "Andreas Hanselowski, Avinesh PVS, Benjamin Schiller, Felix Caspelherr,\n  Debanjan Chaudhuri, Christian M. Meyer and Iryna Gurevych", "title": "A Retrospective Analysis of the Fake News Challenge Stance Detection\n  Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2017 Fake News Challenge Stage 1 (FNC-1) shared task addressed a stance\nclassification task as a crucial first step towards detecting fake news. To\ndate, there is no in-depth analysis paper to critically discuss FNC-1's\nexperimental setup, reproduce the results, and draw conclusions for\nnext-generation stance classification methods. In this paper, we provide such\nan in-depth analysis for the three top-performing systems. We first find that\nFNC-1's proposed evaluation metric favors the majority class, which can be\neasily classified, and thus overestimates the true discriminative power of the\nmethods. Therefore, we propose a new F1-based metric yielding a changed system\nranking. Next, we compare the features and architectures used, which leads to a\nnovel feature-rich stacked LSTM model that performs on par with the best\nsystems, but is superior in predicting minority classes. To understand the\nmethods' ability to generalize, we derive a new dataset and perform both\nin-domain and cross-domain experiments. Our qualitative and quantitative study\nhelps interpreting the original FNC-1 scores and understand which features help\nimproving performance and why. Our new dataset and all source code used during\nthe reproduction study are publicly available for future research.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 15:38:09 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Hanselowski", "Andreas", ""], ["PVS", "Avinesh", ""], ["Schiller", "Benjamin", ""], ["Caspelherr", "Felix", ""], ["Chaudhuri", "Debanjan", ""], ["Meyer", "Christian M.", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1806.05259", "submitter": "Philipp Mayr", "authors": "Ameni Kacem, Philipp Mayr", "title": "Analysis of Search Stratagem Utilisation", "comments": "20 pages, 3 figures, accepted paper in Scientometrics", "journal-ref": null, "doi": "10.1007/s11192-018-2821-8", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Interactive IR, researchers consider the user behaviour towards systems\nand search tasks in order to adapt search results and to improve the search\nexperience of users. Analysing the users' past interactions with the system is\none typical approach. In this paper, we analyse the user behaviour in retrieval\nsessions towards Marcia Bates' search stratagems such as Footnote Chasing,\nCitation Searching, Keyword Searching, Author Searching and Journal Run in a\nreal-life academic search engine. In fact, search stratagems represent\nhigh-level search behaviour as the users go beyond simple execution of queries\nand investigate more of the system functionalities. We performed analyses of\nthese five search stratagems using two datasets extracted from the social\nsciences search engine sowiport. A specific focus was the detection of the\nsearch phase and frequency of the usage of these stratagems. In addition, we\nexplored the impact of these stratagems on the whole search process\nperformance. We addressed mainly the usage patterns' observation of the\nstratagems, their impact on the conduct of retrieval sessions and explore\nwhether they are used similarly in both datasets. From the observation and\nmetrics proposed, we can conclude that the utilisation of search stratagems in\nreal retrieval sessions leads to an improvement of the precision in terms of\npositive interactions. However, the difference is that Footnote Chasing,\nCitation Searching and Journal Run appear mostly at the end of a session while\nKeyword and Author Searching appear typically at the beginning. Thus, we can\nconclude from the log analysis that the improvement of search functionalities\nincluding personalisation and/or recommendation could be achieved by\nconsidering references, citations, and journals in the ranking process.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 20:29:52 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Kacem", "Ameni", ""], ["Mayr", "Philipp", ""]]}, {"id": "1806.05359", "submitter": "Omer Ben-Porat", "authors": "Omer Ben-Porat, Itay Rosenberg and Moshe Tennenholtz", "title": "Convergence of Learning Dynamics in Information Retrieval Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a game-theoretic model of information retrieval with strategic\nauthors. We examine two different utility schemes: authors who aim at\nmaximizing exposure and authors who want to maximize active selection of their\ncontent (i.e. the number of clicks). We introduce the study of author learning\ndynamics in such contexts. We prove that under the probability ranking\nprinciple (PRP), which forms the basis of the current state of the art ranking\nmethods, any better-response learning dynamics converges to a pure Nash\nequilibrium. We also show that other ranking methods induce a strategic\nenvironment under which such a convergence may not occur.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 04:17:08 GMT"}, {"version": "v2", "created": "Sun, 22 Jul 2018 07:17:29 GMT"}, {"version": "v3", "created": "Wed, 20 Feb 2019 12:12:15 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Ben-Porat", "Omer", ""], ["Rosenberg", "Itay", ""], ["Tennenholtz", "Moshe", ""]]}, {"id": "1806.05480", "submitter": "Ciprian-Octavian Truica", "authors": "Ciprian-Octavian Truic\\u{a} and Julien Velcin and Alexandru Boicea", "title": "Automatic Language Identification for Romance Languages using Stop Words\n  and Diacritics", "comments": null, "journal-ref": null, "doi": "10.1109/SYNASC.2015.45", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic language identification is a natural language processing problem\nthat tries to determine the natural language of a given content. In this paper\nwe present a statistical method for automatic language identification of\nwritten text using dictionaries containing stop words and diacritics. We\npropose different approaches that combine the two dictionaries to accurately\ndetermine the language of textual corpora. This method was chosen because stop\nwords and diacritics are very specific to a language, although some languages\nhave some similar words and special characters they are not all common. The\nlanguages taken into account were romance languages because they are very\nsimilar and usually it is hard to distinguish between them from a computational\npoint of view. We have tested our method using a Twitter corpus and a news\narticle corpus. Both corpora consists of UTF-8 encoded text, so the diacritics\ncould be taken into account, in the case that the text has no diacritics only\nthe stop words are used to determine the language of the text. The experimental\nresults show that the proposed method has an accuracy of over 90% for small\ntexts and over 99.8% for\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 11:38:24 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Truic\u0103", "Ciprian-Octavian", ""], ["Velcin", "Julien", ""], ["Boicea", "Alexandru", ""]]}, {"id": "1806.05520", "submitter": "Chengsheng Mao", "authors": "Chengsheng Mao, Yuan Zhao, Mengxin Sun, Yuan Luo", "title": "Are My EHRs Private Enough? -Event-level Privacy Protection", "comments": "accepted by TCBB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy is a major concern in sharing human subject data to researchers for\nsecondary analyses. A simple binary consent (opt-in or not) may significantly\nreduce the amount of sharable data, since many patients might only be concerned\nabout a few sensitive medical conditions rather than the entire medical\nrecords. We propose event-level privacy protection, and develop a feature\nablation method to protect event-level privacy in electronic medical records.\nUsing a list of 13 sensitive diagnoses, we evaluate the feasibility and the\nefficacy of the proposed method. As feature ablation progresses, the\nidentifiability of a sensitive medical condition decreases with varying speeds\non different diseases. We find that these sensitive diagnoses can be divided\ninto 3 categories: (1) 5 diseases have fast declining identifiability (AUC\nbelow 0.6 with less than 400 features excluded); (2) 7 diseases with\nprogressively declining identifiability (AUC below 0.7 with between 200 and 700\nfeatures excluded); and (3) 1 disease with slowly declining identifiability\n(AUC above 0.7 with 1000 features excluded). The fact that the majority (12 out\nof 13) of the sensitive diseases fall into the first two categories suggests\nthe potential of the proposed feature ablation method as a solution for\nevent-level record privacy protection.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 18:48:14 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Mao", "Chengsheng", ""], ["Zhao", "Yuan", ""], ["Sun", "Mengxin", ""], ["Luo", "Yuan", ""]]}, {"id": "1806.05522", "submitter": "Won-Yong Shin", "authors": "Minh D. Nguyen, Won-Yong Shin", "title": "Improved Density-Based Spatio--Textual Clustering on Social Media", "comments": "14 pages, 10 figures, 6 tables, Submitted for publication to the IEEE\n  Transactions on Knowledge and Data Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DBSCAN may not be sufficient when the input data type is heterogeneous in\nterms of textual description. When we aim to discover clusters of geo-tagged\nrecords relevant to a particular point-of-interest (POI) on social media,\nexamining only one type of input data (e.g., the tweets relevant to a POI) may\ndraw an incomplete picture of clusters due to noisy regions. To overcome this\nproblem, we introduce DBSTexC, a newly defined density-based clustering\nalgorithm using spatio--textual information. We first characterize POI-relevant\nand POI-irrelevant tweets as the texts that include and do not include a POI\nname or its semantically coherent variations, respectively. By leveraging the\nproportion of POI-relevant and POI-irrelevant tweets, the proposed algorithm\ndemonstrates much higher clustering performance than the DBSCAN case in terms\nof $\\mathcal{F}_1$ score and its variants. While DBSTexC performs exactly as\nDBSCAN with the textually homogeneous inputs, it far outperforms DBSCAN with\nthe textually heterogeneous inputs. Furthermore, to further improve the\nclustering quality by fully capturing the geographic distribution of tweets, we\npresent fuzzy DBSTexC (F-DBSTexC), an extension of DBSTexC, which incorporates\nthe notion of fuzzy clustering into the DBSTexC. We then demonstrate the\nrobustness of F-DBSTexC via intensive experiments. The computational complexity\nof our algorithms is also analytically and numerically shown.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 13:11:52 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Nguyen", "Minh D.", ""], ["Shin", "Won-Yong", ""]]}, {"id": "1806.05736", "submitter": "Mohammad Aliannejadi", "authors": "Mohammad Aliannejadi and Fabio Crestani", "title": "Personalized Context-Aware Point of Interest Recommendation", "comments": "To appear at ACM Transactions on Information Systems (TOIS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized recommendation of Points of Interest (POIs) plays a key role in\nsatisfying users on Location-Based Social Networks (LBSNs). In this paper, we\npropose a probabilistic model to find the mapping between user-annotated tags\nand locations' taste keywords. Furthermore, we introduce a dataset on\nlocations' contextual appropriateness and demonstrate its usefulness in\npredicting the contextual relevance of locations. We investigate four\napproaches to use our proposed mapping for addressing the data sparsity\nproblem: one model to reduce the dimensionality of location taste keywords and\nthree models to predict user tags for a new location. Moreover, we present\ndifferent scores calculated from multiple LBSNs and show how we incorporate new\ninformation from the mapping into a POI recommendation approach. Then, the\ncomputed scores are integrated using learning to rank techniques. The\nexperiments on two TREC datasets show the effectiveness of our approach,\nbeating state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 20:36:39 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Aliannejadi", "Mohammad", ""], ["Crestani", "Fabio", ""]]}, {"id": "1806.05789", "submitter": "Usman Roshan", "authors": "Yunzhe Xue and Usman Roshan", "title": "Image classification and retrieval with random depthwise signed\n  convolutional neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a random convolutional neural network to generate a feature space\nin which we study image classification and retrieval performance. Put briefly\nwe apply random convolutional blocks followed by global average pooling to\ngenerate a new feature, and we repeat this k times to produce a k-dimensional\nfeature space. This can be interpreted as partitioning the space of image\npatches with random hyperplanes which we formalize as a random depthwise\nconvolutional neural network. In the network's final layer we perform image\nclassification and retrieval with the linear support vector machine and\nk-nearest neighbor classifiers and study other empirical properties. We show\nthat the ratio of image pixel distribution similarity across classes to within\nclasses is higher in our network's final layer compared to the input space.\nWhen we apply the linear support vector machine for image classification we see\nthat the accuracy is higher than if we were to train just the final layer of\nVGG16, ResNet18, and DenseNet40 with random weights. In the same setting we\ncompare it to an unsupervised feature learning method and find our accuracy to\nbe comparable on CIFAR10 but higher on CIFAR100 and STL10. We see that the\naccuracy is not far behind that of trained networks, particularly in the top-k\nsetting. For example the top-2 accuracy of our network is near 90% on both\nCIFAR10 and a 10-class mini ImageNet, and 85% on STL10. We find that k-nearest\nneighbor gives a comparable precision on the Corel Princeton Image Similarity\nBenchmark than if we were to use the final layer of trained networks. As with\nother networks we find that our network fails to a black box attack even though\nwe lack a gradient and use the sign activation. We highlight sensitivity of our\nnetwork to background as a potential pitfall and an advantage. Overall our work\npushes the boundary of what can be achieved with random weights.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 02:26:11 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 22:12:31 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 21:20:48 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Xue", "Yunzhe", ""], ["Roshan", "Usman", ""]]}, {"id": "1806.06192", "submitter": "Zachary Kaden", "authors": "Hima Varsha Dureddy, Zachary Kaden", "title": "Handling Cold-Start Collaborative Filtering with Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in recommender systems is handling new users, whom are also\ncalled $\\textit{cold-start}$ users. In this paper, we propose a novel approach\nfor learning an optimal series of questions with which to interview cold-start\nusers for movie recommender systems. We propose learning interview questions\nusing Deep Q Networks to create user profiles to make better recommendations to\ncold-start users. While our proposed system is trained using a movie\nrecommender system, our Deep Q Network model should generalize across various\ntypes of recommender systems.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 05:58:00 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Dureddy", "Hima Varsha", ""], ["Kaden", "Zachary", ""]]}, {"id": "1806.06208", "submitter": "Sauradip Nag", "authors": "Sauradip Nag, Pallab Kumar Ganguly, Sumit Roy, Sourab Jha, Krishna\n  Bose, Abhishek Jha, Kousik Dasgupta", "title": "Offline Extraction of Indic Regional Language from Natural Scene Image\n  using Text Segmentation and Deep Convolutional Sequence", "comments": "Accepted in Second International Conference on Computational\n  Intelligence, Communications, and Business Analytics (CICBA-2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regional language extraction from a natural scene image is always a\nchallenging proposition due to its dependence on the text information extracted\nfrom Image. Text Extraction on the other hand varies on different lighting\ncondition, arbitrary orientation, inadequate text information, heavy background\ninfluence over text and change of text appearance. This paper presents a novel\nunified method for tackling the above challenges. The proposed work uses an\nimage correction and segmentation technique on the existing Text Detection\nPipeline an Efficient and Accurate Scene Text Detector (EAST). EAST uses\nstandard PVAnet architecture to select features and non maximal suppression to\ndetect text from image. Text recognition is done using combined architecture of\nMaxOut convolution neural network (CNN) and Bidirectional long short term\nmemory (LSTM) network. After recognizing text using the Deep Learning based\napproach, the native Languages are translated to English and tokenized using\nstandard Text Tokenizers. The tokens that very likely represent a location is\nused to find the Global Positioning System (GPS) coordinates of the location\nand subsequently the regional languages spoken in that location is extracted.\nThe proposed method is tested on a self generated dataset collected from\nGovernment of India dataset and experimented on Standard Dataset to evaluate\nthe performance of the proposed technique. Comparative study with a few\nstate-of-the-art methods on text detection, recognition and extraction of\nregional language from images shows that the proposed method outperforms the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 08:31:06 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 20:10:03 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Nag", "Sauradip", ""], ["Ganguly", "Pallab Kumar", ""], ["Roy", "Sumit", ""], ["Jha", "Sourab", ""], ["Bose", "Krishna", ""], ["Jha", "Abhishek", ""], ["Dasgupta", "Kousik", ""]]}, {"id": "1806.06407", "submitter": "Sarit Chakraborty", "authors": "Bijoyan Das, Sarit Chakraborty", "title": "An Improved Text Sentiment Classification Model Using TF-IDF and Next\n  Word Negation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of Text sentiment analysis, the demand for automatic\nclassification of electronic documents has increased by leaps and bound. The\nparadigm of text classification or text mining has been the subject of many\nresearch works in recent time. In this paper we propose a technique for text\nsentiment classification using term frequency- inverse document frequency\n(TF-IDF) along with Next Word Negation (NWN). We have also compared the\nperformances of binary bag of words model, TF-IDF model and TF-IDF with next\nword negation (TF-IDF-NWN) model for text classification. Our proposed model is\nthen applied on three different text mining algorithms and we found the Linear\nSupport vector machine (LSVM) is the most appropriate to work with our proposed\nmodel. The achieved results show significant increase in accuracy compared to\nearlier methods.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 16:25:57 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Das", "Bijoyan", ""], ["Chakraborty", "Sarit", ""]]}, {"id": "1806.06423", "submitter": "C. H. Huck Yang", "authors": "C.-H. Huck Yang, Jia-Hong Huang, Fangyu Liu, Fang-Yi Chiu, Mengya Gao,\n  Weifeng Lyu, I-Hung Lin M.D., Jesper Tegner", "title": "A Novel Hybrid Machine Learning Model for Auto-Classification of Retinal\n  Diseases", "comments": "Accepted at the Joint ICML and IJCAI Workshop on Computational\n  Biology (ICML-IJCAI WCB) to be held in Stockholm SWEDEN, 2018. Referring to\n  https://sites.google.com/view/wcb2018/accepted-papers?authuser=0", "journal-ref": "ICML-IJCAI Workshop 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic clinical diagnosis of retinal diseases has emerged as a promising\napproach to facilitate discovery in areas with limited access to specialists.\nWe propose a novel visual-assisted diagnosis hybrid model based on the support\nvector machine (SVM) and deep neural networks (DNNs). The model incorporates\ncomplementary strengths of DNNs and SVM. Furthermore, we present a new clinical\nretina label collection for ophthalmology incorporating 32 retina diseases\nclasses. Using EyeNet, our model achieves 89.73% diagnosis accuracy and the\nmodel performance is comparable to the professional ophthalmologists.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 18:22:55 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Yang", "C. -H. Huck", ""], ["Huang", "Jia-Hong", ""], ["Liu", "Fangyu", ""], ["Chiu", "Fang-Yi", ""], ["Gao", "Mengya", ""], ["Lyu", "Weifeng", ""], ["D.", "I-Hung Lin M.", ""], ["Tegner", "Jesper", ""]]}, {"id": "1806.06446", "submitter": "Yi Tay", "authors": "Yi Tay, Shuai Zhang, Luu Anh Tuan, Siu Cheung Hui", "title": "Self-Attentive Neural Collaborative Filtering", "comments": "We discovered a bug in our tensorflow implementation that involved\n  accidental mixing of vectors across batches, rendering the main claim of the\n  paper incorrect. We are withdrawing this paper until we find out why", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn as we discovered a bug in our tensorflow\nimplementation that involved accidental mixing of vectors across batches. This\nlead to different inference results given different batch sizes which is\ncompletely strange. The performance scores still remain the same but we\nconcluded that it was not the self-attention that contributed to the\nperformance. We are withdrawing the paper because this renders the main claim\nof the paper false. Thanks to Guan Xinyu from NUS for discovering this issue in\nour previously open source code.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 20:58:12 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 12:04:56 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Tay", "Yi", ""], ["Zhang", "Shuai", ""], ["Tuan", "Luu Anh", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1806.06535", "submitter": "Massimo Quadrana", "authors": "Massimo Quadrana, Marta Reznakova, Tao Ye, Erik Schmidt and Hossein\n  Vahabi", "title": "Modeling Musical Taste Evolution with Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the music of the moment can often be a challenging problem, even for\nwell-versed music listeners. Musical tastes are constantly in flux, and the\nproblem of developing computational models for musical taste dynamics presents\na rich and nebulous problem space. A variety of factors all play some role in\ndetermining preferences (e.g., popularity, musicological, social, geographical,\ngenerational), and these factors vary across different listeners and contexts.\nIn this paper, we leverage a massive dataset on internet radio station creation\nfrom a large music streaming company in order to develop computational models\nof listener taste evolution. We delve deep into the complexities of this\ndomain, identifying some of the unique challenges that it presents, and develop\na model utilizing recurrent neural networks. We apply our model to the problem\nof next station prediction and show that it not only outperforms several\nbaselines, but excels at long tail music personalization, particularly by\nlearning the long-term dependency structure of listener music preference\nevolution.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 07:51:10 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Quadrana", "Massimo", ""], ["Reznakova", "Marta", ""], ["Ye", "Tao", ""], ["Schmidt", "Erik", ""], ["Vahabi", "Hossein", ""]]}, {"id": "1806.06583", "submitter": "Yin Zheng", "authors": "Xuefei Ning, Yin Zheng, Zhuxi Jiang, Yu Wang, Huazhong Yang, Junzhou\n  Huang", "title": "Nonparametric Topic Modeling with Neural Inference", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on combining nonparametric topic models with Auto-Encoding\nVariational Bayes (AEVB). Specifically, we first propose iTM-VAE, where the\ntopics are treated as trainable parameters and the document-specific topic\nproportions are obtained by a stick-breaking construction. The inference of\niTM-VAE is modeled by neural networks such that it can be computed in a simple\nfeed-forward manner. We also describe how to introduce a hyper-prior into\niTM-VAE so as to model the uncertainty of the prior parameter. Actually, the\nhyper-prior technique is quite general and we show that it can be applied to\nother AEVB based models to alleviate the {\\it collapse-to-prior} problem\nelegantly. Moreover, we also propose HiTM-VAE, where the document-specific\ntopic distributions are generated in a hierarchical manner. HiTM-VAE is even\nmore flexible and can generate topic distributions with better variability.\nExperimental results on 20News and Reuters RCV1-V2 datasets show that the\nproposed models outperform the state-of-the-art baselines significantly. The\nadvantages of the hyper-prior technique and the hierarchical model construction\nare also confirmed by experiments.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 10:22:18 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Ning", "Xuefei", ""], ["Zheng", "Yin", ""], ["Jiang", "Zhuxi", ""], ["Wang", "Yu", ""], ["Yang", "Huazhong", ""], ["Huang", "Junzhou", ""]]}, {"id": "1806.06671", "submitter": "Haifeng Zhu", "authors": "Pengpeng Zhao, Haifeng Zhu, Yanchi Liu, Zhixu Li, Jiajie Xu, Victor S.\n  Sheng", "title": "Where to Go Next: A Spatio-temporal LSTM model for Next POI\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next Point-of-Interest (POI) recommendation is of great value for both\nlocation-based service providers and users. Recently Recurrent Neural Networks\n(RNNs) have been proved to be effective on sequential recommendation tasks.\nHowever, existing RNN solutions rarely consider the spatio-temporal intervals\nbetween neighbor check-ins, which are essential for modeling user check-in\nbehaviors in next POI recommendation. In this paper, we propose a new variant\nof LSTM, named STLSTM, which implements time gates and distance gates into LSTM\nto capture the spatio-temporal relation between successive check-ins.\nSpecifically, one-time gate and one distance gate are designed to control\nshort-term interest update, and another time gate and distance gate are\ndesigned to control long-term interest update. Furthermore, to reduce the\nnumber of parameters and improve efficiency, we further integrate coupled input\nand forget gates with our proposed model. Finally, we evaluate the proposed\nmodel using four real-world datasets from various location-based social\nnetworks. Our experimental results show that our model significantly\noutperforms the state-of-the-art approaches for next POI recommendation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 13:43:51 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Zhao", "Pengpeng", ""], ["Zhu", "Haifeng", ""], ["Liu", "Yanchi", ""], ["Li", "Zhixu", ""], ["Xu", "Jiajie", ""], ["Sheng", "Victor S.", ""]]}, {"id": "1806.06676", "submitter": "Richard Vogl", "authors": "Richard Vogl and Gerhard Widmer and Peter Knees", "title": "Towards multi-instrument drum transcription", "comments": "Published in Proceedings of the 21th International Conference on\n  Digital Audio Effects (DAFx18), 4 - 8 September, 2018, Aveiro, Portugal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic drum transcription, a subtask of the more general automatic music\ntranscription, deals with extracting drum instrument note onsets from an audio\nsource. Recently, progress in transcription performance has been made using\nnon-negative matrix factorization as well as deep learning methods. However,\nthese works primarily focus on transcribing three drum instruments only: snare\ndrum, bass drum, and hi-hat. Yet, for many applications, the ability to\ntranscribe more drum instruments which make up standard drum kits used in\nwestern popular music would be desirable. In this work, convolutional and\nconvolutional recurrent neural networks are trained to transcribe a wider range\nof drum instruments. First, the shortcomings of publicly available datasets in\nthis context are discussed. To overcome these limitations, a larger synthetic\ndataset is introduced. Then, methods to train models using the new dataset\nfocusing on generalization to real world data are investigated. Finally, the\ntrained models are evaluated on publicly available datasets and results are\ndiscussed. The contributions of this work comprise: (i.) a large-scale\nsynthetic dataset for drum transcription, (ii.) first steps towards an\nautomatic drum transcription system that supports a larger range of instruments\nby evaluating and discussing training setups and the impact of datasets in this\ncontext, and (iii.) a publicly available set of trained models for drum\ntranscription. Additional materials are available at\nhttp://ifs.tuwien.ac.at/~vogl/dafx2018\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 13:45:48 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 11:56:38 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Vogl", "Richard", ""], ["Widmer", "Gerhard", ""], ["Knees", "Peter", ""]]}, {"id": "1806.06773", "submitter": "Rong Gong", "authors": "Rong Gong and Xavier Serra", "title": "Towards an efficient deep learning model for musical onset detection", "comments": "Paper rejected by the 19th International Society for Music\n  Information Retrieval Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient and reproducible deep learning model\nfor musical onset detection (MOD). We first review the state-of-the-art deep\nlearning models for MOD, and identify their shortcomings and challenges: (i)\nthe lack of hyper-parameter tuning details, (ii) the non-availability of code\nfor training models on other datasets, and (iii) ignoring the network\ncapability when comparing different architectures. Taking the above issues into\naccount, we experiment with seven deep learning architectures. The most\nefficient one achieves equivalent performance to our implementation of the\nstate-of-the-art architecture. However, it has only 28.3% of the total number\nof trainable parameters compared to the state-of-the-art. Our experiments are\nconducted using two different datasets: one mainly consists of instrumental\nmusic excerpts, and another developed by ourselves includes only solo singing\nvoice excerpts. Further, inter-dataset transfer learning experiments are\nconducted. The results show that the model pre-trained on one dataset fails to\ndetect onsets on another dataset, which denotes the importance of providing the\nimplementation code to enable re-training the model for a different dataset.\nDatasets, code and a Jupyter notebook running on Google Colab are publicly\navailable to make this research understandable and easy to reproduce.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:30:35 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 10:12:23 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Gong", "Rong", ""], ["Serra", "Xavier", ""]]}, {"id": "1806.06870", "submitter": "Shawn Jones", "authors": "Shawn M. Jones, Michele C. Weigle, Michael L. Nelson", "title": "The Off-Topic Memento Toolkit", "comments": "10 pages, 14 figures, to appear in the proceedings of the 15th\n  International Conference on Digital Preservation (iPres 2018)", "journal-ref": null, "doi": "10.17605/OSF.IO/UBW87", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web archive collections are created with a particular purpose in mind. A\ncurator selects seeds, or original resources, which are then captured by an\narchiving system and stored as archived web pages, or mementos. The systems\nthat build web archive collections are often configured to revisit the same\noriginal resource multiple times. This is incredibly useful for understanding\nan unfolding news story or the evolution of an organization. Unfortunately,\nover time, some of these original resources can go off-topic and no longer suit\nthe purpose for which the collection was originally created. They can go\noff-topic due to web site redesigns, changes in domain ownership, financial\nissues, hacking, technical problems, or because their content has moved on from\nthe original topic. Even though they are off-topic, the archiving system will\nstill capture them, thus it becomes imperative to anyone performing research on\nthese collections to identify these off-topic mementos. Hence, we present the\nOff-Topic Memento Toolkit, which allows users to detect off-topic mementos\nwithin web archive collections. The mementos identified by this toolkit can\nthen be separately removed from a collection or merely excluded from downstream\nanalysis. The following similarity measures are available: byte count, word\ncount, cosine similarity, Jaccard distance, S{\\o}rensen-Dice distance, Simhash\nusing raw text content, Simhash using term frequency, and Latent Semantic\nIndexing via the gensim library. We document the implementation of each of\nthese similarity measures. We possess a gold standard dataset generated by\nmanual analysis, which contains both off-topic and on-topic mementos. Using\nthis gold standard dataset, we establish a default threshold corresponding to\nthe best F1 score for each measure. We also provide an overview of potential\nfuture directions that the toolkit may take.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 18:05:12 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 17:52:02 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Jones", "Shawn M.", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1806.06946", "submitter": "Alexey Potapov", "authors": "Alexey Potapov, Innokentii Zhdanov, Oleg Scherbakov, Nikolai\n  Skorobogatko, Hugo Latapie, Enzo Fenoglio", "title": "Semantic Image Retrieval by Uniting Deep Neural Networks and Cognitive\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image and video retrieval by their semantic content has been an important and\nchallenging task for years, because it ultimately requires bridging the\nsymbolic/subsymbolic gap. Recent successes in deep learning enabled detection\nof objects belonging to many classes greatly outperforming traditional computer\nvision techniques. However, deep learning solutions capable of executing\nretrieval queries are still not available. We propose a hybrid solution\nconsisting of a deep neural network for object detection and a cognitive\narchitecture for query execution. Specifically, we use YOLOv2 and OpenCog.\nQueries allowing the retrieval of video frames containing objects of specified\nclasses and specified spatial arrangement are implemented.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 14:53:53 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Potapov", "Alexey", ""], ["Zhdanov", "Innokentii", ""], ["Scherbakov", "Oleg", ""], ["Skorobogatko", "Nikolai", ""], ["Latapie", "Hugo", ""], ["Fenoglio", "Enzo", ""]]}, {"id": "1806.06998", "submitter": "Leif Hanlen", "authors": "Leif W. Hanlen and Richard Nock and Hanna Suominen and Neil Bacon", "title": "Private Text Classification", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidential text corpora exist in many forms, but do not allow arbitrary\nsharing. We explore how to use such private corpora using privacy preserving\ntext analytics. We construct typical text processing applications using\nappropriate privacy preservation techniques (including homomorphic encryption,\nRademacher operators and secure computation). We set out the preliminary\nmaterials from Rademacher operators for binary classifiers, and then construct\nbasic text processing approaches to match those binary classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 01:24:32 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Hanlen", "Leif W.", ""], ["Nock", "Richard", ""], ["Suominen", "Hanna", ""], ["Bacon", "Neil", ""]]}, {"id": "1806.07296", "submitter": "Eliot Brenner", "authors": "Eliot Brenner, Jun Zhao, Aliasgar Kutiyanawala, Zheng Yan", "title": "End-to-End Neural Ranking for eCommerce Product Search: an application\n  of task models and textual embeddings", "comments": "Accepted to appear at the SIGIR 2018 workshop on eCommerce", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of retrieving and ranking items in an eCommerce\ncatalog, often called SKUs, in order of relevance to a user-issued query. The\ninput data for the ranking are the texts of the queries and textual fields of\nthe SKUs indexed in the catalog. We review the ways in which this problem both\nresembles and differs from the problems of IR in the context of web search. The\ndifferences between the product-search problem and the IR problem of web search\nnecessitate a different approach in terms of both models and datasets. We first\nreview the recent state-of-the-art models for web search IR, distinguishing\nbetween two distinct types of model which we call the distributed type and the\nlocal-interaction type. The different types of relevance models developed for\nIR have complementary advantages and disadvantages when applied to eCommerce\nproduct search. Further, we explain why the conventional methods for dataset\nconstruction employed in the IR literature fail to produce data which suffices\nfor training or evaluation of models for eCommerce product search. We explain\nhow our own approach, applying task modeling techniques to the click-through\nlogs of an eCommerce site, enables the construction of a large-scale dataset\nfor training and robust benchmarking of relevance models. Our experiments\nconsist of applying several of the models from the IR literature to our own\ndataset. Empirically, we have established that, when applied to our dataset,\ncertain models of local-interaction type reduce ranking errors by one-third\ncompared to the baseline tf-idf. Applied to our dataset, the distributed models\nfail to outperform the baseline. As a basis for a deployed system, the\ndistributed models have several advantages, computationally, over the\nlocal-interaction models. This motivates an ongoing program of work, which we\noutline at the conclusion of the paper.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 14:57:08 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Brenner", "Eliot", ""], ["Zhao", "Jun", ""], ["Kutiyanawala", "Aliasgar", ""], ["Yan", "Zheng", ""]]}, {"id": "1806.07309", "submitter": "Christian Otto", "authors": "Justyna Medrek, Christian Otto, Ralph Ewerth", "title": "Recommending Scientific Videos based on Metadata Enrichment using Linked\n  Open Data", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of available videos in the Web has significantly increased not\nonly for entertainment etc., but also to convey educational or scientific\ninformation in an effective way. There are several web portals that offer\naccess to the latter kind of video material. One of them is the TIB AV-Portal\nof the Leibniz Information Centre for Science and Technology (TIB), which hosts\nscientific and educational video content. In contrast to other video portals,\nautomatic audiovisual analysis (visual concept classification, optical\ncharacter recognition, speech recognition) is utilized to enhance metadata\ninformation and semantic search. In this paper, we propose to further exploit\nand enrich this automatically generated information by linking it to the\nIntegrated Authority File (GND) of the German National Library. This\ninformation is used to derive a measure to compare the similarity of two videos\nwhich serves as a basis for recommending semantically similar videos. A user\nstudy demonstrates the feasibility of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 15:34:45 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 08:23:08 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Medrek", "Justyna", ""], ["Otto", "Christian", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1806.07516", "submitter": "Nguyen Vo", "authors": "Nguyen Vo, Kyumin Lee", "title": "The Rise of Guardians: Fact-checking URL Recommendation to Combat Fake\n  News", "comments": "SIGIR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large body of research work and efforts have been focused on detecting fake\nnews and building online fact-check systems in order to debunk fake news as\nsoon as possible. Despite the existence of these systems, fake news is still\nwildly shared by online users. It indicates that these systems may not be fully\nutilized. After detecting fake news, what is the next step to stop people from\nsharing it? How can we improve the utilization of these fact-check systems? To\nfill this gap, in this paper, we (i) collect and analyze online users called\nguardians, who correct misinformation and fake news in online discussions by\nreferring fact-checking URLs; and (ii) propose a novel fact-checking URL\nrecommendation model to encourage the guardians to engage more in fact-checking\nactivities. We found that the guardians usually took less than one day to reply\nto claims in online conversations and took another day to spread verified\ninformation to hundreds of millions of followers. Our proposed recommendation\nmodel outperformed four state-of-the-art models by 11%~33%. Our source code and\ndataset are available at https://github.com/nguyenvo09/CombatingFakeNews.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 01:26:21 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 22:04:46 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Vo", "Nguyen", ""], ["Lee", "Kyumin", ""]]}, {"id": "1806.07573", "submitter": "Heri Ramampiaro", "authors": "{\\O}ystein Repp and Heri Ramampiaro", "title": "Extracting News Events from Microblogs", "comments": null, "journal-ref": "In Journal of Statistics and Management Systems, 21(4), pp.\n  695-723. Taylor & Francis", "doi": "10.1080/09720510.2018.1486273", "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter stream has become a large source of information for many people, but\nthe magnitude of tweets and the noisy nature of its content have made\nharvesting the knowledge from Twitter a challenging task for researchers for a\nlong time. Aiming at overcoming some of the main challenges of extracting the\nhidden information from tweet streams, this work proposes a new approach for\nreal-time detection of news events from the Twitter stream. We divide our\napproach into three steps. The first step is to use a neural network or deep\nlearning to detect news-relevant tweets from the stream. The second step is to\napply a novel streaming data clustering algorithm to the detected news tweets\nto form news events. The third and final step is to rank the detected events\nbased on the size of the event clusters and growth speed of the tweet\nfrequencies. We evaluate the proposed system on a large, publicly available\ncorpus of annotated news events from Twitter. As part of the evaluation, we\ncompare our approach with a related state-of-the-art solution. Overall, our\nexperiments and user-based evaluation show that our approach on detecting\ncurrent (real) news events delivers a state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 06:54:17 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Repp", "\u00d8ystein", ""], ["Ramampiaro", "Heri", ""]]}, {"id": "1806.07603", "submitter": "Anett Hoppe", "authors": "Anett Hoppe and Jascha Hagen and Helge Holzmann and G\\\"unter Kniesel\n  and Ralph Ewerth", "title": "An Analytics Tool for Exploring Scientific Software and Related\n  Publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific software is one of the key elements for reproducible research.\nHowever, classic publications and related scientific software are typically not\n(sufficiently) linked, and it lacks tools to jointly explore these artefacts.\nIn this paper, we report on our work on developing an analytics tool for\njointly exploring software and publications. The presented prototype, a concept\nfor automatic code discovery, and two use cases demonstrate the feasibility and\nusefulness of the proposal.\n  Submitted to TPDL 2018 as Demonstration Paper.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 08:17:19 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Hoppe", "Anett", ""], ["Hagen", "Jascha", ""], ["Holzmann", "Helge", ""], ["Kniesel", "G\u00fcnter", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1806.07640", "submitter": "Konstantin Avrachenkov", "authors": "Konstantin Avrachenkov, Arun Kadavankandy, Nelly Litvak", "title": "Mean Field Analysis of Personalized PageRank with Implications for Local\n  Graph Clustering", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": "10.1007/s10955-018-2099-5", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse a mean-field model of Personalized PageRank on the Erdos-Renyi\nrandom graph containing a denser planted Erdos-Renyi subgraph. We investigate\nthe regimes where the values of Personalized PageRank concentrate around the\nmean-field value. We also study the optimization of the damping factor, the\nonly parameter in Personalized PageRank. Our theoretical results help to\nunderstand the applicability of Personalized PageRank and its limitations for\nlocal graph clustering.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 09:47:42 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Avrachenkov", "Konstantin", ""], ["Kadavankandy", "Arun", ""], ["Litvak", "Nelly", ""]]}, {"id": "1806.07678", "submitter": "Zhipeng Wu", "authors": "Zhipeng Wu, Hui Tian, Xuzhen Zhu, Shuo Wang", "title": "Optimization Matrix Factorization Recommendation Algorithm Based on\n  Rating Centrality", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-93803-5_11", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization (MF) is extensively used to mine the user preference\nfrom explicit ratings in recommender systems. However, the reliability of\nexplicit ratings is not always consistent, because many factors may affect the\nuser's final evaluation on an item, including commercial advertising and a\nfriend's recommendation. Therefore, mining the reliable ratings of user is\ncritical to further improve the performance of the recommender system. In this\nwork, we analyze the deviation degree of each rating in overall rating\ndistribution of user and item, and propose the notion of user-based rating\ncentrality and item-based rating centrality, respectively. Moreover, based on\nthe rating centrality, we measure the reliability of each user rating and\nprovide an optimized matrix factorization recommendation algorithm.\nExperimental results on two popular recommendation datasets reveal that our\nmethod gets better performance compared with other matrix factorization\nrecommendation algorithms, especially on sparse datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 11:56:04 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Wu", "Zhipeng", ""], ["Tian", "Hui", ""], ["Zhu", "Xuzhen", ""], ["Wang", "Shuo", ""]]}, {"id": "1806.07713", "submitter": "Amin Omidvar", "authors": "Amin Omidvar, Hui Jiang, Aijun An", "title": "Using Neural Network for Identifying Clickbaits in Online News Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online news media sometimes use misleading headlines to lure users to open\nthe news article. These catchy headlines that attract users but disappointed\nthem at the end, are called Clickbaits. Because of the importance of automatic\nclickbait detection in online medias, lots of machine learning methods were\nproposed and employed to find the clickbait headlines. In this research, a\nmodel using deep learning methods is proposed to find the clickbaits in\nClickbait Challenge 2017's dataset. The proposed model gained the first rank in\nthe Clickbait Challenge 2017 in terms of Mean Squared Error. Also, data\nanalytics and visualization techniques are employed to explore and discover the\nprovided dataset to get more insight from the data.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 13:21:53 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Omidvar", "Amin", ""], ["Jiang", "Hui", ""], ["An", "Aijun", ""]]}, {"id": "1806.07715", "submitter": "Weijia Lu", "authors": "Weijia Lu and Jie Shuai and Shuyan Gu and Joel Xue", "title": "Method to Annotate Arrhythmias by Deep Network", "comments": null, "journal-ref": null, "doi": "10.1109/Cybermatics_2018.2018.00307", "report-no": null, "categories": "eess.SP cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study targets to automatically annotate on arrhythmia by deep network.\nThe investigated types include sinus rhythm, asystole (Asys), supraventricular\ntachycardia (Tachy), ventricular flutter or fibrillation (VF/VFL), ventricular\ntachycardia (VT). Methods: 13s limb lead ECG chunks from MIT malignant\nventricular arrhythmia database (VFDB) and MIT normal sinus rhythm database\nwere partitioned into subsets for 5-fold cross validation. These signals were\nresampled to 200Hz, filtered to remove baseline wandering, projected to 2D gray\nspectrum and then fed into a deep network with brand-new structure. In this\nnetwork, a feature vector for a single time point was retrieved by residual\nlayers, from which latent representation was extracted by variational\nautoencoder (VAE). These front portions were trained to meet a certain\nthreshold in loss function, then fixed while training procedure switched to\nremaining bidirectional recurrent neural network (RNN), the very portions to\npredict an arrhythmia category. Attention windows were polynomial lumped on RNN\noutputs for learning from details to outlines. And over sampling was employed\nfor imbalanced data. The trained model was wrapped into docker image for\ndeployment in edge or cloud. Conclusion: Promising sensitivities were achieved\nin four arrhythmias and good precision rates in two ventricular arrhythmias\nwere also observed. Moreover, it was proven that latent representation by VAE,\ncan significantly boost the speed of convergence and accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 00:15:06 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Lu", "Weijia", ""], ["Shuai", "Jie", ""], ["Gu", "Shuyan", ""], ["Xue", "Joel", ""]]}, {"id": "1806.07727", "submitter": "Chakkrit Tantithamthavorn", "authors": "Chakkrit Tantithamthavorn, Surafel Lemma Abebe, Ahmed E. Hassan,\n  Akinori Ihara, Kenichi Matsumoto", "title": "The Impact of IR-based Classifier Configuration on the Performance and\n  the Effort of Method-Level Bug Localization", "comments": "Accepted at Journal of Information and Software Technology (IST)", "journal-ref": null, "doi": "10.1016/j.infsof.2018.06.001", "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: IR-based bug localization is a classifier that assists developers in\nlocating buggy source code entities (e.g., files and methods) based on the\ncontent of a bug report. Such IR-based classifiers have various parameters that\ncan be configured differently (e.g., the choice of entity representation).\nObjective: In this paper, we investigate the impact of the choice of the\nIR-based classifier configuration on the top-k performance and the required\neffort to examine source code entities before locating a bug at the method\nlevel. Method: We execute a large space of classifier configuration, 3,172 in\ntotal, on 5,266 bug reports of two software systems, i.e., Eclipse and Mozilla.\nResults: We find that (1) the choice of classifier configuration impacts the\ntop-k performance from 0.44% to 36% and the required effort from 4,395 to\n50,000 LOC; (2) classifier configurations with similar top-k performance might\nrequire different efforts; (3) VSM achieves both the best top-k performance and\nthe least required effort for method-level bug localization; (4) the likelihood\nof randomly picking a configuration that performs within 20% of the best top-k\nclassifier configuration is on average 5.4% and that of the least effort is on\naverage 1%; (5) configurations related to the entity representation of the\nanalyzed data have the most impact on both the top-k performance and the\nrequired effort; and (6) the most efficient classifier configuration obtained\nat the method-level can also be used at the file-level (and vice versa).\nConclusion: Our results lead us to conclude that configuration has a large\nimpact on both the top-k performance and the required effort for method-level\nbug localization, suggesting that the IR-based configuration settings should be\ncarefully selected and the required effort metric should be included in future\nbug localization studies.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 13:43:33 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Tantithamthavorn", "Chakkrit", ""], ["Abebe", "Surafel Lemma", ""], ["Hassan", "Ahmed E.", ""], ["Ihara", "Akinori", ""], ["Matsumoto", "Kenichi", ""]]}, {"id": "1806.07942", "submitter": "Myungha Jang", "authors": "Myungha Jang and James Allan", "title": "Explaining Controversy on Social Media via Stance Summarization", "comments": "4 pages, accepted to SIGIR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an era in which new controversies rapidly emerge and evolve on social\nmedia, navigating social media platforms to learn about a new controversy can\nbe an overwhelming task. In this light, there has been significant work that\nstudies how to identify and measure controversy online. However, we currently\nlack a tool for effectively understanding controversy in social media. For\nexample, users have to manually examine postings to find the arguments of\nconflicting stances that make up the controversy.\n  In this paper, we study methods to generate a stance-aware summary that\nexplains a given controversy by collecting arguments of two conflicting\nstances. We focus on Twitter and treat stance summarization as a ranking\nproblem of finding the top k tweets that best summarize the two conflicting\nstances of a controversial topic. We formalize the characteristics of a good\nstance summary and propose a ranking model accordingly. We first evaluate our\nmethods on five controversial topics on Twitter. Our user evaluation shows that\nour methods consistently outperform other baseline techniques in generating a\nsummary that explains the given controversy.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 19:49:57 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2018 20:58:43 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Jang", "Myungha", ""], ["Allan", "James", ""]]}, {"id": "1806.08130", "submitter": "Chao Liu", "authors": "Chao Liu, Zhenzhen Zheng, Jinkang Jia", "title": "Behavior-based evaluation of session satisfaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, web search becomes more and more popular all over the world. Many\nresearchers and developers have done lots of studies on behaviors of search\nusers. In practice, the full understanding of these behaviors can not only help\nto evaluate the usefulness of newly-developed ranking algorithms and other\nchanges of search engine, but also to guide the growth direction of search\nengine. As far as we know, most of past work are mainly focused on single\nsearch evaluation, which do promote the rapid development of search engine in\nearly stage. However,these page-level behaviors are so limited that can no\nlonger give explicit feedbacks on minor changes of the search engine. We think\nthat it will be more accurate and sensitive when more information on search\nsession are provided. In this paper, a session level evaluation method is\nproposed. The session-level features are retrieved and carefully analyzed. Some\nlinear and non-linear features which can reflect the final degree of\nsatisfaction are chosen and adopted in evaluation models. A two-layer hybrid\nevaluation model with different granularity, which can achieve good precision\nand recall, is designed and trained. Lots of real experiments are evaluated by\nthe model, the result shows it achieved a higher accuracy performance than\ntraditional page-level evaluation metrics. Furthermore, for practical\napplication, it is important to interpret the reason of each session's\nsatisfaction judgement. In all, a session-level evaluation model with improved\nperformance and well capability on interpretation is proposed and applied in\nreal practice in search engine companies.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 09:22:25 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 02:13:15 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Liu", "Chao", ""], ["Zheng", "Zhenzhen", ""], ["Jia", "Jinkang", ""]]}, {"id": "1806.08202", "submitter": "Hussein AL-Natsheh", "authors": "Hussein T. Al-Natsheh, Lucie Martinet, Fabrice Muhlenbach, Fabien\n  Rico, Djamel A. Zighed", "title": "Metadata Enrichment of Multi-Disciplinary Digital Library: A\n  Semantic-based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the scientific digital libraries, some papers from different research\ncommunities can be described by community-dependent keywords even if they share\na semantically similar topic. Articles that are not tagged with enough keyword\nvariations are poorly indexed in any information retrieval system which limits\npotentially fruitful exchanges between scientific disciplines. In this paper,\nwe introduce a novel experimentally designed pipeline for multi-label\nsemantic-based tagging developed for open-access metadata digital libraries.\nThe approach starts by learning from a standard scientific categorization and a\nsample of topic tagged articles to find semantically relevant articles and\nenrich its metadata accordingly. Our proposed pipeline aims to enable\nresearchers reaching articles from various disciplines that tend to use\ndifferent terminologies. It allows retrieving semantically relevant articles\ngiven a limited known variation of search terms. In addition to achieving an\naccuracy that is higher than an expanded query based method using a topic\nsynonym set extracted from a semantic network, our experiments also show a\nhigher computational scalability versus other comparable techniques. We created\na new benchmark extracted from the open-access metadata of a scientific digital\nlibrary and published it along with the experiment code to allow further\nresearch in the topic.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 12:35:23 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Al-Natsheh", "Hussein T.", ""], ["Martinet", "Lucie", ""], ["Muhlenbach", "Fabrice", ""], ["Rico", "Fabien", ""], ["Zighed", "Djamel A.", ""]]}, {"id": "1806.08211", "submitter": "Pranjul Yadav", "authors": "Marcelo Tallis, Pranjul Yadav", "title": "Reacting to Variations in Product Demand: An Application for Conversion\n  Rate (CR) Prediction in Sponsored Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online internet advertising, machine learning models are widely used to\ncompute the likelihood of a user engaging with product related advertisements.\nHowever, the performance of traditional machine learning models is often\nimpacted due to variations in user and advertiser behavior. For example, search\nengine traffic for florists usually tends to peak around Valentine's day,\nMother's day, etc. To overcome, this challenge, in this manuscript we propose\nthree models which are able to incorporate the effects arising due to\nvariations in product demand. The proposed models are a combination of product\ndemand features, specialized data sampling methodologies and ensemble\ntechniques. We demonstrate the performance of our proposed models on datasets\nobtained from a real-world setting. Our results show that the proposed models\nmore accurately predict the outcome of users interactions with product related\nadvertisements while simultaneously being robust to fluctuations in user and\nadvertiser behaviors.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 23:15:00 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Tallis", "Marcelo", ""], ["Yadav", "Pranjul", ""]]}, {"id": "1806.08245", "submitter": "Ching Tarn", "authors": "Ching Tarn, Yinan Zhang, Ye Feng", "title": "Reductive Clustering: An Efficient Linear-time Graph-based Divisive\n  Cluster Analysis Approach", "comments": "http://res.ctarn.io/reductive-clustering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient linear-time graph-based divisive cluster analysis\napproach called Reductive Clustering. The approach tries to reveal the\nhierarchical structural information through reducing the graph into a more\nconcise one repeatedly. With the reductions, the original graph can be divided\ninto subgraphs recursively, and a lite informative dendrogram is constructed\nbased on the divisions. The reduction consists of three steps: selection,\nconnection, and partition. First a subset of vertices of the graph are selected\nas representatives to build a concise graph. The representatives are\nre-connected to maintain a consistent structure with the previous graph. If\npossible, the concise graph is divided into subgraphs, and each subgraph is\nfurther reduced recursively until the termination condition is met. We discuss\nthe approach, along with several selection and connection methods, in detail\nboth theoretically and experimentally in this paper. Our implementations run in\nlinear time and achieve outstanding performance on various types of datasets.\nExperimental results show that they outperform state-of-the-art clustering\nalgorithms with significantly less computing resource requirements.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 13:44:17 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 06:40:56 GMT"}, {"version": "v3", "created": "Fri, 25 Sep 2020 12:20:22 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Tarn", "Ching", ""], ["Zhang", "Yinan", ""], ["Feng", "Ye", ""]]}, {"id": "1806.08246", "submitter": "Eric M\\\"uller-Budack", "authors": "Eric M\\\"uller-Budack, Kader Pustu-Iren, Sebastian Diering, Ralph\n  Ewerth", "title": "Finding Person Relations in Image Data of the Internet Archive", "comments": null, "journal-ref": "In: M\\'endez E., Crestani F., Ribeiro C., David G., Lopes J. (eds)\n  Digital Libraries for Open Knowledge. TPDL 2018. Lecture Notes in Computer\n  Science, vol 11057. Springer, Cham", "doi": "10.1007/978-3-030-00066-0_20", "report-no": null, "categories": "cs.DL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multimedia content in the World Wide Web is rapidly growing and contains\nvaluable information for many applications in different domains. For this\nreason, the Internet Archive initiative has been gathering billions of\ntime-versioned web pages since the mid-nineties. However, the huge amount of\ndata is rarely labeled with appropriate metadata and automatic approaches are\nrequired to enable semantic search. Normally, the textual content of the\nInternet Archive is used to extract entities and their possible relations\nacross domains such as politics and entertainment, whereas image and video\ncontent is usually neglected. In this paper, we introduce a system for person\nrecognition in image content of web news stored in the Internet Archive. Thus,\nthe system complements entity recognition in text and allows researchers and\nanalysts to track media coverage and relations of persons more precisely. Based\non a deep learning face recognition approach, we suggest a system that\nautomatically detects persons of interest and gathers sample material, which is\nsubsequently used to identify them in the image data of the Internet Archive.\nWe evaluate the performance of the face recognition system on an appropriate\nstandard benchmark dataset and demonstrate the feasibility of the approach with\ntwo use cases.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 13:48:21 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 13:04:14 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["M\u00fcller-Budack", "Eric", ""], ["Pustu-Iren", "Kader", ""], ["Diering", "Sebastian", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1806.08694", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani and Jaap Kamps", "title": "Learning to Rank from Samples of Variable Quality", "comments": "Presented at The First International SIGIR2016 Workshop on Learning\n  From Limited Or Noisy Data For Information Retrieval. arXiv admin note:\n  substantial text overlap with arXiv:1711.02799", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks requires many training samples, but in\npractice, training labels are expensive to obtain and may be of varying\nquality, as some may be from trusted expert labelers while others might be from\nheuristics or other sources of weak supervision such as crowd-sourcing. This\ncreates a fundamental quality-versus quantity trade-off in the learning\nprocess. Do we learn from the small amount of high-quality data or the\npotentially large amount of weakly-labeled data? We argue that if the learner\ncould somehow know and take the label-quality into account when learning the\ndata representation, we could get the best of both worlds. To this end, we\nintroduce \"fidelity-weighted learning\" (FWL), a semi-supervised student-teacher\napproach for training deep neural networks using weakly-labeled data. FWL\nmodulates the parameter updates to a student network (trained on the task we\ncare about) on a per-sample basis according to the posterior confidence of its\nlabel-quality estimated by a teacher (who has access to the high-quality\nlabels). Both student and teacher are learned from the data. We evaluate FWL on\ndocument ranking where we outperform state-of-the-art alternative\nsemi-supervised methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 09:40:20 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Kamps", "Jaap", ""]]}, {"id": "1806.08977", "submitter": "Yujie Lin", "authors": "Yujie Lin, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Jun Ma, and Maarten\n  de Rijke", "title": "Explainable Outfit Recommendation with Joint Outfit Matching and Comment\n  Generation", "comments": null, "journal-ref": null, "doi": "10.1109/TKDE.2019.2906190", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most previous work on outfit recommendation focuses on designing visual\nfeatures to enhance recommendations. Existing work neglects user comments of\nfashion items, which have been proved to be effective in generating\nexplanations along with better recommendation results. We propose a novel\nneural network framework, neural outfit recommendation (NOR), that\nsimultaneously provides outfit recommendations and generates abstractive\ncomments. NOR consists of two parts: outfit matching and comment generation.\nFor outfit matching, we propose a convolutional neural network with a mutual\nattention mechanism to extract visual features. The visual features are then\ndecoded into a rating score for the matching prediction. For abstractive\ncomment generation, we propose a gated recurrent neural network with a\ncross-modality attention mechanism to transform visual features into a concise\nsentence. The two parts are jointly trained based on a multi-task learning\nframework in an end-to-end back-propagation paradigm. Extensive experiments\nconducted on an existing dataset and a collected real-world dataset show NOR\nachieves significant improvements over state-of-the-art baselines for outfit\nrecommendation. Meanwhile, our generated comments achieve impressive ROUGE and\nBLEU scores in comparison to human-written comments. The generated comments can\nbe regarded as explanations for the recommendation results. We release the\ndataset and code to facilitate future research.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 14:40:30 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 03:00:23 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 03:12:09 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Lin", "Yujie", ""], ["Ren", "Pengjie", ""], ["Chen", "Zhumin", ""], ["Ren", "Zhaochun", ""], ["Ma", "Jun", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1806.09082", "submitter": "Grant Atkins", "authors": "Grant C. Atkins, Alexander Nwala, Michele C. Weigle, Michael L. Nelson", "title": "Measuring News Similarity Across Ten U.S. News Sites", "comments": "This is an extended version of the paper to appear in the proceedings\n  of the 15th International Conference on Digital Preservation (iPres 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  News websites make editorial decisions about what stories to include on their\nwebsite homepages and what stories to emphasize (e.g., large font size for main\nstory). The emphasized stories on a news website are often highly similar to\nmany other news websites (e.g, a terrorist event story). The selective emphasis\nof a top news story and the similarity of news across different news\norganizations are well-known phenomena but not well-measured. We provide a\nmethod for identifying the top news story for a select set of U.S.-based news\nwebsites and then quantify the similarity across them. To achieve this, we\nfirst developed a headline and link extractor that parses select websites, and\nthen examined ten United States based news website homepages during a three\nmonth period, November 2016 to January 2017. Using archived copies, retrieved\nfrom the Internet Archive (IA), we discuss the methods and difficulties for\nparsing these websites, and how events such as a presidential election can lead\nnews websites to alter their document representation just for these events. We\nuse our parser to extract k = 1, 3, 10 maximum number of stories for each news\nsite. Second, we used the cosine similarity measure to calculate news\nsimilarity at 8PM Eastern Time for each day in the three months. The similarity\nscores show a buildup (0.335) before Election Day, with a declining value\n(0.328) on Election Day, and an increase (0.354) after Election Day. Our method\nshows that we can effectively identity top stories and quantify news\nsimilarity.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 04:44:36 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 00:01:57 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Atkins", "Grant C.", ""], ["Nwala", "Alexander", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1806.09279", "submitter": "Amritpal Kaur", "authors": "Amritpal Kaur and Harkiran Kaur", "title": "Framework for Opinion Mining Approach to Augment Education System\n  Performance", "comments": "5 pages, 2 figures", "journal-ref": "http://ijitce.co.uk/vol8n6.aspx June 2018 Issue Vol.8 No.6", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The extensive expansion growth of social networking sites allows the people\nto share their views and experiences freely with their peers on internet. Due\nto this, huge amount of data is generated on everyday basis which can be used\nfor the opinion mining to extract the views of people in a particular field.\nOpinion mining finds its applications in many areas such as Tourism, Politics,\neducation and entertainment, etc. It has not been extensively implemented in\narea of education system. This paper discusses the malpractices in the present\nexamination system. In the present scenario, Opinion mining is vastly used for\ndecision making. The authors of this paper have designed a framework by\napplying Na\\\"ive Bayes approach to the education dataset. The various phases of\nNa\\\"ive Bayes approach include three steps: conversion of data into frequency\ntable, making classes of dataset and apply the Na\\\"ive Bayes algorithm equation\nto calculate the probabilities of classes. Finally the highest probability\nclass is the outcome of this prediction. These predictions are used to make\nimprovements in the education system and help to provide better education.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 04:17:44 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Kaur", "Amritpal", ""], ["Kaur", "Harkiran", ""]]}, {"id": "1806.09317", "submitter": "Massimo Melucci", "authors": "Massimo Melucci", "title": "Evaluation of Information Retrieval Systems Using Structural Equation\n  Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of the experimental data collected by testing systems\nacross input datasets and model parameters is of strategic importance for\nsystem design and implementation. In particular, finding relationships between\nvariables and detecting the latent variables affecting retrieval performance\ncan provide designers, engineers and experimenters with useful if not necessary\ninformation about how a system is performing. This paper discusses the use of\nStructural Equation Modelling (SEM) in providing an in-depth explanation of\nevaluation results and an explanation of failures and successes of a system; in\nparticular, we focus on the case of Information Retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 08:05:26 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Melucci", "Massimo", ""]]}, {"id": "1806.09447", "submitter": "Giulio Ermanno Pibiri", "authors": "Giulio Ermanno Pibiri, Rossano Venturini", "title": "Handling Massive N-Gram Datasets Efficiently", "comments": "Published in ACM Transactions on Information Systems (TOIS), February\n  2019, Article No: 25", "journal-ref": null, "doi": "10.1145/3302913", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the two fundamental problems concerning the handling of\nlarge n-gram language models: indexing, that is compressing the n-gram strings\nand associated satellite data without compromising their retrieval speed; and\nestimation, that is computing the probability distribution of the strings from\na large textual source. Regarding the problem of indexing, we describe\ncompressed, exact and lossless data structures that achieve, at the same time,\nhigh space reductions and no time degradation with respect to state-of-the-art\nsolutions and related software packages. In particular, we present a compressed\ntrie data structure in which each word following a context of fixed length k,\ni.e., its preceding k words, is encoded as an integer whose value is\nproportional to the number of words that follow such context. Since the number\nof words following a given context is typically very small in natural\nlanguages, we lower the space of representation to compression levels that were\nnever achieved before. Despite the significant savings in space, our technique\nintroduces a negligible penalty at query time. Regarding the problem of\nestimation, we present a novel algorithm for estimating modified Kneser-Ney\nlanguage models, that have emerged as the de-facto choice for language modeling\nin both academia and industry, thanks to their relatively low perplexity\nperformance. Estimating such models from large textual sources poses the\nchallenge of devising algorithms that make a parsimonious use of the disk. The\nstate-of-the-art algorithm uses three sorting steps in external memory: we show\nan improved construction that requires only one sorting step thanks to\nexploiting the properties of the extracted n-gram strings. With an extensive\nexperimental analysis performed on billions of n-grams, we show an average\nimprovement of 4.5X on the total running time of the state-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 13:23:12 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 09:20:53 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Pibiri", "Giulio Ermanno", ""], ["Venturini", "Rossano", ""]]}, {"id": "1806.09511", "submitter": "Jose Marcelino", "authors": "Jos\\'e Marcelino, Jo\\~ao Faria, Lu\\'is Ba\\'ia, Ricardo Gamelas Sousa", "title": "A Hierarchical Deep Learning Natural Language Parser for Fashion", "comments": "In Proceedings of KDD 2018 (KDD Workshop on AI for Fashion)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a hierarchical deep learning natural language parser for\nfashion. Our proposal intends not only to recognize fashion-domain entities but\nalso to expose syntactic and morphologic insights. We leverage the usage of an\narchitecture of specialist models, each one for a different task (from parsing\nto entity recognition). Such architecture renders a hierarchical model able to\ncapture the nuances of the fashion language. The natural language parser is\nable to deal with textual ambiguities which are left unresolved by our\ncurrently existing solution. Our empirical results establish a robust baseline,\nwhich justifies the use of hierarchical architectures of deep learning models\nwhile opening new research avenues to explore.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 14:57:52 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Marcelino", "Jos\u00e9", ""], ["Faria", "Jo\u00e3o", ""], ["Ba\u00eda", "Lu\u00eds", ""], ["Sousa", "Ricardo Gamelas", ""]]}, {"id": "1806.09736", "submitter": "Amir Karami", "authors": "Amir Karami and Noelle M. Pendergraft", "title": "Computational Analysis of Insurance Complaints: GEICO Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online environment has provided a great opportunity for insurance\npolicyholders to share their complaints with respect to different services.\nThese complaints can reveal valuable information for insurance companies who\nseek to improve their services; however, analyzing a huge number of online\ncomplaints is a complicated task for human and must involve computational\nmethods to create an efficient process. This research proposes a computational\napproach to characterize the major topics of a large number of online\ncomplaints. Our approach is based on using the topic modeling approach to\ndisclose the latent semantic of complaints. The proposed approach deployed on\nthousands of GEICO negative reviews. Analyzing 1,371 GEICO complaints indicates\nthat there are 30 major complains in four categories: (1) customer service, (2)\ninsurance coverage, paperwork, policy, and reports, (3) legal issues, and (4)\ncosts, estimates, and payments. This research approach can be used in other\napplications to explore a large number of reviews.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 00:12:14 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Karami", "Amir", ""], ["Pendergraft", "Noelle M.", ""]]}, {"id": "1806.09793", "submitter": "Khuong Vo An", "authors": "Khanh Dang, Khuong Vo and Josef K\\\"ung", "title": "A NoSQL Data-based Personalized Recommendation System for C2C e-Commerce", "comments": "Accepted to DEXA 2017", "journal-ref": null, "doi": "10.1007/978-3-319-64471-4_25", "report-no": null, "categories": "cs.IR cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the considerable development of customer-to-customer (C2C) e-commerce in\nthe recent years, there is a big demand for an effective recommendation system\nthat suggests suitable websites for users to sell their items with some\nspecified needs. Nonetheless, e-commerce recommendation systems are mostly\ndesigned for business-to-customer (B2C) websites, where the systems offer the\nconsumers the products that they might like to buy. Almost none of the related\nresearch works focus on choosing selling sites for target items. In this paper,\nwe introduce an approach that recommends the selling websites based upon the\nitem's description, category, and desired selling price. This approach employs\nNoSQL data-based machine learning techniques for building and training topic\nmodels and classification models. The trained models can then be used to rank\nthe websites dynamically with respect to the user needs. The experimental\nresults with real-world datasets from Vietnam C2C websites will demonstrate the\neffectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 05:02:30 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Dang", "Khanh", ""], ["Vo", "Khuong", ""], ["K\u00fcng", "Josef", ""]]}, {"id": "1806.09827", "submitter": "Sim\\'on Roca-Sotelo", "authors": "Sim\\'on Roca-Sotelo and Jer\\'onimo Arenas-Garc\\'ia", "title": "Unveiling the semantic structure of text documents using paragraph-aware\n  Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classic Topic Models are built under the Bag Of Words assumption, in which\nword position is ignored for simplicity. Besides, symmetric priors are\ntypically used in most applications. In order to easily learn topics with\ndifferent properties among the same corpus, we propose a new line of work in\nwhich the paragraph structure is exploited. Our proposal is based on the\nfollowing assumption: in many text document corpora there are formal\nconstraints shared across all the collection, e.g. sections. When this\nassumption is satisfied, some paragraphs may be related to general concepts\nshared by all documents in the corpus, while others would contain the genuine\ndescription of documents. Assuming each paragraph can be semantically more\ngeneral, specific, or hybrid, we look for ways to measure this, transferring\nthis distinction to topics and being able to learn what we call specific and\ngeneral topics. Experiments show that this is a proper methodology to highlight\ncertain paragraphs in structured documents at the same time we learn\ninteresting and more diverse topics.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 07:50:37 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Roca-Sotelo", "Sim\u00f3n", ""], ["Arenas-Garc\u00eda", "Jer\u00f3nimo", ""]]}, {"id": "1806.10037", "submitter": "Ayush Singhal", "authors": "Ayush Singhal, Rakesh Pant, Pradeep Sinha", "title": "AlertMix: A Big Data platform for multi-source streaming data", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for stream processing is increasing at an unprecedented rate. Big\ndata is no longer limited to processing of big volumes of data. In most\nreal-world scenarios, the need for processing stream data as it comes can only\nmeet the business needs. It is required for trading, fraud detection, system\nmonitoring, product maintenance and of course social media data such as Twitter\nand YouTube videos. In such cases, a \"too late architecture\" that focuses on\nbatch processing cannot realize the use cases. In this article, we present an\nend to end Big data platform called AlertMix for processing multi-source\nstreaming data. Its architecture and how various Big data technologies are\nutilized are explained in this work. We present the performance of our platform\non real live streaming data which is currently handled by the platform.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 22:53:42 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Singhal", "Ayush", ""], ["Pant", "Rakesh", ""], ["Sinha", "Pradeep", ""]]}, {"id": "1806.10540", "submitter": "Jinseok Kim", "authors": "Jinseok Kim", "title": "Evaluating author name disambiguation for digital libraries: A case of\n  DBLP", "comments": "Scientometrics (2018)", "journal-ref": null, "doi": "10.1007/s11192-018-2824-5", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Author name ambiguity in a digital library may affect the findings of\nresearch that mines authorship data of the library. This study evaluates author\nname disambiguation in DBLP, a widely used but insufficiently evaluated digital\nlibrary for its disambiguation performance. In doing so, this study takes a\ntriangulation approach that author name disambiguation for a digital library\ncan be better evaluated when its performance is assessed on multiple labeled\ndatasets with comparison to baselines. Tested on three types of labeled data\ncontaining 5,000 ~ 700K disambiguated names and 6M pairs of disambiguated\nnames, DBLP is shown to assign author names quite accurately to distinct\nauthors, resulting in pairwise precision, recall, and F1 measures around 0.90\nor above overall. DBLP's author name disambiguation performs well even on large\nambiguous name blocks but deficiently on distinguishing authors with the same\nnames. When compared to other disambiguation algorithms, DBLP's disambiguation\nperformance is quite competitive, possibly due to its hybrid disambiguation\napproach combining algorithmic disambiguation and manual error correction. A\ndiscussion follows on strengths and weaknesses of labeled datasets used in this\nstudy for future efforts to evaluate author name disambiguation on a digital\nlibrary scale.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 15:49:27 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 13:45:32 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Kim", "Jinseok", ""]]}, {"id": "1806.10674", "submitter": "Jinseok Kim", "authors": "Jinseok Kim", "title": "Author-Based Analysis of Conference versus Journal Publication in\n  Computer Science", "comments": null, "journal-ref": "Kim, J. (2018). Author-Based Analysis of Conference versus Journal\n  Publication in Computer Science. Journal of the Association for Information\n  Science and Technology", "doi": "10.1002/asi.24079", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conference publications in computer science (CS) have attracted scholarly\nattention due to their unique status as a main research outlet unlike other\nscience fields where journals are dominantly used for communicating research\nfindings. One frequent research question has been how different conference and\njournal publications are, considering a paper as a unit of analysis. This study\ntakes an author-based approach to analyze publishing patterns of 517,763\nscholars who have ever published both in CS conferences and journals for the\nlast 57 years, as recorded in DBLP. The analysis shows that the majority of CS\nscholars tend to make their scholarly debut, publish more papers, and\ncollaborate with more coauthors in conferences than in journals. Importantly,\nconference papers seem to serve as a distinct channel of scholarly\ncommunication, not a mere preceding step to journal publications: coauthors and\ntitle words of authors across conferences and journals tend not to overlap\nmuch. This study corroborates findings of previous studies on this topic from a\ndistinctive perspective and suggests that conference authorship in CS calls for\nmore special attention from scholars and administrators outside CS who have\nfocused on journal publications to mine authorship data and evaluate scholarly\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 20:06:17 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Kim", "Jinseok", ""]]}, {"id": "1806.10773", "submitter": "Yang Yang", "authors": "Yang Yang and Marius Pesavento and Symeon Chatzinotas and Bj\\\"orn\n  Ottersten", "title": "Successive Convex Approximation Algorithms for Sparse Signal Estimation\n  with Nonconvex Regularizations", "comments": "submitted to IEEE Journal of Selected Topics in Signal Processing,\n  special issue in Robust Subspace Learning", "journal-ref": null, "doi": "10.1109/JSTSP.2018.2877584", "report-no": null, "categories": "cs.LG cs.DC cs.IR math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a successive convex approximation framework for\nsparse optimization where the nonsmooth regularization function in the\nobjective function is nonconvex and it can be written as the difference of two\nconvex functions. The proposed framework is based on a nontrivial combination\nof the majorization-minimization framework and the successive convex\napproximation framework proposed in literature for a convex regularization\nfunction. The proposed framework has several attractive features, namely, i)\nflexibility, as different choices of the approximate function lead to different\ntype of algorithms; ii) fast convergence, as the problem structure can be\nbetter exploited by a proper choice of the approximate function and the\nstepsize is calculated by the line search; iii) low complexity, as the\napproximate function is convex and the line search scheme is carried out over a\ndifferentiable function; iv) guaranteed convergence to a stationary point. We\ndemonstrate these features by two example applications in subspace learning,\nnamely, the network anomaly detection problem and the sparse subspace\nclustering problem. Customizing the proposed framework by adopting the\nbest-response type approximation, we obtain soft-thresholding with exact line\nsearch algorithms for which all elements of the unknown parameter are updated\nin parallel according to closed-form expressions. The attractive features of\nthe proposed algorithms are illustrated numerically.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 05:21:16 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Yang", "Yang", ""], ["Pesavento", "Marius", ""], ["Chatzinotas", "Symeon", ""], ["Ottersten", "Bj\u00f6rn", ""]]}, {"id": "1806.10813", "submitter": "Robin Brochier", "authors": "Robin Brochier, Adrien Guille (ERIC), Benjamin Rothan, Julien Velcin\n  (ERIC)", "title": "Impact of the Query Set on the Evaluation of Expert Finding Systems", "comments": null, "journal-ref": "BIRNDL 2018 (SIGIR 2018), Jul 2018, Ann Arbor, Michigan, USA,\n  France", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expertise is a loosely defined concept that is hard to formalize. Much\nresearch has focused on designing efficient algorithms for expert finding in\nlarge databases in various application domains. The evaluation of such\nrecommender systems lies most of the time on human-annotated sets of experts\nassociated with topics. The protocol of evaluation consists in using the\nnamings or short descriptions of these topics as raw queries in order to rank\nthe available set of candidates. Several measures taken from the field of\ninformation retrieval are then applied to rate the rankings of candidates\nagainst the ground truth set of experts. In this paper, we apply this\ntopic-query evaluation methodology with the AMiner data and explore a new\ndocument-query methodology to evaluate experts retrieval from a set of queries\nsampled directly from the experts documents. Specifically, we describe two\ndatasets extracted from AMiner, three baseline algorithms from the literature\nbased on several document representations and provide experiment results to\nshow that using a wide range of more realistic queries provides different\nevaluation results to the usual topic-queries.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 08:09:52 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Brochier", "Robin", "", "ERIC"], ["Guille", "Adrien", "", "ERIC"], ["Rothan", "Benjamin", "", "ERIC"], ["Velcin", "Julien", "", "ERIC"]]}, {"id": "1806.10869", "submitter": "Yan Xiao", "authors": "Yan Xiao, Jiafeng Guo, Yixing Fan, Yanyan Lan, Jun Xu, and Xueqi Cheng", "title": "Beyond Precision: A Study on Recall of Initial Retrieval with Neural\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vocabulary mismatch is a central problem in information retrieval (IR), i.e.,\nthe relevant documents may not contain the same (symbolic) terms of the query.\nRecently, neural representations have shown great success in capturing semantic\nrelatedness, leading to new possibilities to alleviate the vocabulary mismatch\nproblem in IR. However, most existing efforts in this direction have been\ndevoted to the re-ranking stage. That is to leverage neural representations to\nhelp re-rank a set of candidate documents, which are typically obtained from an\ninitial retrieval stage based on some symbolic index and search scheme (e.g.,\nBM25 over the inverted index). This naturally raises a question: if the\nrelevant documents have not been found in the initial retrieval stage due to\nvocabulary mismatch, there would be no chance to re-rank them to the top\npositions later. Therefore, in this paper, we study the problem how to employ\nneural representations to improve the recall of relevant documents in the\ninitial retrieval stage. Specifically, to meet the efficiency requirement of\nthe initial stage, we introduce a neural index for the neural representations\nof documents, and propose two hybrid search schemes based on both neural and\nsymbolic indices, namely the parallel search scheme and the sequential search\nscheme. Our experiments show that both hybrid index and search schemes can\nimprove the recall of the initial retrieval stage with small overhead.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 10:41:24 GMT"}, {"version": "v2", "created": "Sun, 22 Jul 2018 13:18:17 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Xiao", "Yan", ""], ["Guo", "Jiafeng", ""], ["Fan", "Yixing", ""], ["Lan", "Yanyan", ""], ["Xu", "Jun", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1806.11189", "submitter": "Veera Raghavendra Chikka", "authors": "Veera Raghavendra Chikka, Kamalakar Karlapalem", "title": "A hybrid deep learning approach for medical relation extraction", "comments": "4 pages, 4 tables, 1 figure, 2018 KDD workshop on Machine Learning\n  for Medicine and Healthcare", "journal-ref": "MLMH 2018 2018 KDD workshop on Machine Learning for Medicine and\n  Healthcare. London, United Kingdom. August 20, 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Mining relationships between treatment(s) and medical problem(s) is vital in\nthe biomedical domain. This helps in various applications, such as decision\nsupport system, safety surveillance, and new treatment discovery. We propose a\ndeep learning approach that utilizes both word level and sentence-level\nrepresentations to extract the relationships between treatment and problem.\nWhile deep learning techniques demand a large amount of data for training, we\nmake use of a rule-based system particularly for relationship classes with\nfewer samples. Our final relations are derived by jointly combining the results\nfrom deep learning and rule-based models. Our system achieved a promising\nperformance on the relationship classes of I2b2 2010 relation extraction task.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 06:38:01 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Chikka", "Veera Raghavendra", ""], ["Karlapalem", "Kamalakar", ""]]}, {"id": "1806.11226", "submitter": "Kamelia Aryafar", "authors": "Murium Iqbal, Adair Kovac, Kamelia Aryafar", "title": "A Multimodal Recommender System for Large-scale Assortment Generation in\n  E-commerce", "comments": "SIGIR eComm Accepted Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-commerce platforms surface interesting products largely through product\nrecommendations that capture users' styles and aesthetic preferences. Curating\nrecommendations as a complete complementary set, or assortment, is critical for\na successful e-commerce experience, especially for product categories such as\nfurniture, where items are selected together with the overall theme, style or\nambiance of a space in mind. In this paper, we propose two visually-aware\nrecommender systems that can automatically curate an assortment of living room\nfurniture around a couple of pre-selected seed pieces for the room. The first\nsystem aims to maximize the visual-based style compatibility of the entire\nselection by making use of transfer learning and topic modeling. The second\nsystem extends the first by incorporating text data and applying polylingual\ntopic modeling to infer style over both modalities. We review the production\npipeline for surfacing these visually-aware recommender systems and compare\nthem through offline validations and large-scale online A/B tests on Overstock.\nOur experimental results show that complimentary style is best discovered over\nproduct sets when both visual and textual data are incorporated.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 23:11:54 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Iqbal", "Murium", ""], ["Kovac", "Adair", ""], ["Aryafar", "Kamelia", ""]]}, {"id": "1806.11330", "submitter": "Jaspreet Singh", "authors": "Jaspreet Singh and Avishek Anand", "title": "Posthoc Interpretability of Learning to Rank Models using Secondary\n  Training Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive models are omnipresent in automated and assisted decision making\nscenarios. But for the most part they are used as black boxes which output a\nprediction without understanding partially or even completely how different\nfeatures influence the model prediction avoiding algorithmic transparency.\nRankings are ordering over items encoding implicit comparisons typically\nlearned using a family of features using learning-to-rank models. In this paper\nwe focus on how best we can understand the decisions made by a ranker in a\npost-hoc model agnostic manner. We operate on the notion of interpretability\nbased on explainability of rankings over an interpretable feature space.\nFurthermore we train a tree based model (inherently interpretable) using labels\nfrom the ranker, called secondary training data to provide explanations.\nConsequently, we attempt to study how well does a subset of features,\npotentially interpretable, explain the full model under different training\nsizes and algorithms. We do experiments on the learning to rank datasets with\n30k queries and report results that serve show in certain settings we can learn\na faithful interpretable ranker.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 10:03:48 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Singh", "Jaspreet", ""], ["Anand", "Avishek", ""]]}, {"id": "1806.11371", "submitter": "Sreekanth Vempati", "authors": "Pankaj Agarwal, Sreekanth Vempati and Sumit Borar", "title": "Personalizing Similar Product Recommendations in Fashion E-commerce", "comments": "AI for fashion, The third international workshop on fashion and KDD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In fashion e-commerce platforms, product discovery is one of the key\ncomponents of a good user experience. There are numerous ways using which\npeople find the products they desire. Similar product recommendations is one of\nthe popular modes using which users find products that resonate with their\nintent. Generally these recommendations are not personalized to a specific\nuser. Traditionally, collaborative filtering based approaches have been popular\nin the literature for recommending non-personalized products given a query\nproduct. Also, there has been focus on personalizing the product listing for a\ngiven user. In this paper, we marry these approaches so that users will be\nrecommended with personalized similar products. Our experimental results on a\nlarge fashion e-commerce platform (Myntra) show that we can improve the key\nmetrics by applying personalization on similar product recommendations.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 12:05:02 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Agarwal", "Pankaj", ""], ["Vempati", "Sreekanth", ""], ["Borar", "Sumit", ""]]}, {"id": "1806.11423", "submitter": "Shreya Singh", "authors": "Shreya Singh, G Mohammed Abdulla, Sumit Borar, Sagar Arora", "title": "Footwear Size Recommendation System", "comments": "7 pages, 5 figures, 5 tables, AI meets Fashion workshop, KDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While shopping for fashion products, customers usually prefer to try-out\nproducts to examine fit, material, overall look and feel. Due to lack of try\nout options during online shopping, it becomes pivotal to provide customers\nwith as much of this information as possible to enhance their shopping\nexperience. Also it becomes essential to provide same experience for new\ncustomers. Our work here focuses on providing a production ready size\nrecommendation system for shoes and address the challenge of providing\nrecommendation for users with no previous purchases on the platform. In our\nwork, we present a probabilistic approach based on user co-purchase data\nfacilitated by generating a brand-brand relationship graph. Specifically we\naddress two challenges that are commonly faced while implementing such\nsolution. 1. Sparse signals for less popular or new products in the system 2.\nExtending the solution for new users. Further we compare and contrast this\napproach with our previous work and show significant improvement both in\nrecommendation precision and coverage.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 14:22:49 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Singh", "Shreya", ""], ["Abdulla", "G Mohammed", ""], ["Borar", "Sumit", ""], ["Arora", "Sagar", ""]]}, {"id": "1806.11424", "submitter": "Aniket Jain", "authors": "Aniket Jain, Yadunath Gupta, Pawan Kumar Singh, Aruna Rajan", "title": "Understanding Fashionability: What drives sales of a style?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use customer demand data for fashion articles on Myntra, and derive a\nfashionability or style quotient, which represents customer demand for the\nstylistic content of a fashion article, decoupled with its commercials (price,\noffers, etc.). We demonstrate learning for assortment planning in fashion that\nwould aim to keep a healthy mix of breadth and depth across various styles, and\nwe show the relationship between a customer's perception of a style vs a\nmerchandiser's catalogue of styles. We also backtest our method to calculate\nprediction errors in our style quotient and customer demand, and discuss\nvarious implications and findings.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 17:09:58 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Jain", "Aniket", ""], ["Gupta", "Yadunath", ""], ["Singh", "Pawan Kumar", ""], ["Rajan", "Aruna", ""]]}, {"id": "1806.11479", "submitter": "Bo Xiao", "authors": "Bo Xiao, Nicholas Monath, Shankar Ananthakrishnan, Abishek Ravi", "title": "Play Duration based User-Entity Affinity Modeling in Spoken Dialog\n  System", "comments": "Interspeech 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia streaming services over spoken dialog systems have become\nubiquitous. User-entity affinity modeling is critical for the system to\nunderstand and disambiguate user intents and personalize user experiences.\nHowever, fully voice-based interaction demands quantification of novel\nbehavioral cues to determine user affinities. In this work, we propose using\nplay duration cues to learn a matrix factorization based collaborative\nfiltering model. We first binarize play durations to obtain implicit positive\nand negative affinity labels. The Bayesian Personalized Ranking objective and\nlearning algorithm are employed in our low-rank matrix factorization approach.\nTo cope with uncertainties in the implicit affinity labels, we propose to apply\na weighting function that emphasizes the importance of high confidence samples.\nBased on a large-scale database of Alexa music service records, we evaluate the\naffinity models by computing Spearman correlation between play durations and\npredicted affinities. Comparing different data utilizations and weighting\nfunctions, we find that employing both positive and negative affinity samples\nwith a convex weighting function yields the best performance. Further analysis\ndemonstrates the model's effectiveness on individual entity level and provides\ninsights on the temporal dynamics of observed affinities.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 15:39:43 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Xiao", "Bo", ""], ["Monath", "Nicholas", ""], ["Ananthakrishnan", "Shankar", ""], ["Ravi", "Abishek", ""]]}]