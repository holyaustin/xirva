[{"id": "1406.0146", "submitter": "Darko Hric", "authors": "Darko Hric, Richard K. Darst, Santo Fortunato", "title": "Community detection in networks: Structural communities versus ground\n  truth", "comments": "21 pages, 19 figures", "journal-ref": "Phys. Rev. E 90, 062805 (2014)", "doi": "10.1103/PhysRevE.90.062805", "report-no": null, "categories": "physics.soc-ph cs.IR cs.SI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms to find communities in networks rely just on structural\ninformation and search for cohesive subsets of nodes. On the other hand, most\nscholars implicitly or explicitly assume that structural communities represent\ngroups of nodes with similar (non-topological) properties or functions. This\nhypothesis could not be verified, so far, because of the lack of network\ndatasets with information on the classification of the nodes. We show that\ntraditional community detection methods fail to find the metadata groups in\nmany large networks. Our results show that there is a marked separation between\nstructural communities and metadata groups, in line with recent findings. That\nmeans that either our current modeling of community structure has to be\nsubstantially modified, or that metadata groups may not be recoverable from\ntopology alone.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jun 2014 09:06:16 GMT"}, {"version": "v2", "created": "Thu, 11 Dec 2014 18:08:15 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Hric", "Darko", ""], ["Darst", "Richard K.", ""], ["Fortunato", "Santo", ""]]}, {"id": "1406.0296", "submitter": "Ovidiu-Andrei Schipor", "authors": "Felicia Florentina Giza, Cristina Elena Turcu, Ovidiu Andrei Schipor", "title": "Using Mobile Agents for Information Retrival in B2B Systems", "comments": "6 pages, 2 figures, in Romanian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an architecture of an information retrieval system that\nuse the advantages offered by mobile agents to collect information from\ndifferent sources and bring the result to the calling user. Mobile agent\ntechnology will be used for determine the traceability of a product and also\nfor searching information about a specific entity.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 09:00:01 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Giza", "Felicia Florentina", ""], ["Turcu", "Cristina Elena", ""], ["Schipor", "Ovidiu Andrei", ""]]}, {"id": "1406.1065", "submitter": "Wolfgang Orthuber", "authors": "Wolfgang Orthuber", "title": "Uniform definition of comparable and searchable information on the web", "comments": "36 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basically information means selection within a domain (value or definition\nset) of possibilities. For objectifiable, comparable and precise information\nthe domain should be the same for all. Therefore the global (online) definition\nof the domain is proposed here. It is advantageous to define an ordered domain,\nbecause this allows using numbers for addressing the elements and because\nnature is ordered in many respects. The original data can be ordered in\nmultiple independent ways. We can define a domain with multiple independent\nnumeric dimensions to reflect this. Because we want to search information in\nthe domain, for quantification of similarity we define a distance function or\nmetric. Therefore we propose \"Domain Spaces\" (DSs) which are online defined\nnestable metric spaces. Their elements are called \"Domain Vectors\" (DVs) and\nhave the simple form:\n  URL (of common DS definition) plus sequence of numbers\n  At this the sequence must be given so that the mapping of numbers to the DS\ndimensions is clear. By help of appropriate software DVs can be represented\ne.g. as words and numbers. Compared to words, however, DVs have (as original\ninformation) important objectifiable advantages (clear definition, objectivity,\ninformation content, range, resolution, efficiency, searchability). Using DSs\nusers can define which information they make searchable and how it is\nsearchable. DSs can be also used to make quantitative (numeric) data as uniform\nDVs interoperable, comparable and searchable. The approach is demonstrated in\nan online database with search engine (http://NumericSearch.com). The search\nprocedure is called \"Numeric Search\". It consists of two systematic steps: 1.\nSelection of the appropriate DS e.g. by conventional word based search within\nthe DS definitions. 2. Range and/or similarity search of DVs in the selected\nDS.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jun 2014 17:39:07 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 09:42:44 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Orthuber", "Wolfgang", ""]]}, {"id": "1406.1143", "submitter": "Jimmy Lin", "authors": "Sarah Weissman, Samet Ayhan, Joshua Bradley, and Jimmy Lin", "title": "Identifying Duplicate and Contradictory Information in Wikipedia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our study identifies sentences in Wikipedia articles that are either\nidentical or highly similar by applying techniques for near-duplicate detection\nof web pages. This is accomplished with a MapReduce implementation of minhash\nto identify clusters of sentences with high Jaccard similarity. We show that\nthese clusters can be categorized into six different types, two of which are\nparticularly interesting: identical sentences quantify the extent to which\ncontent in Wikipedia is copied and pasted, and near-duplicate sentences that\nstate contradictory facts point to quality issues in Wikipedia.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 18:59:00 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Weissman", "Sarah", ""], ["Ayhan", "Samet", ""], ["Bradley", "Joshua", ""], ["Lin", "Jimmy", ""]]}, {"id": "1406.1580", "submitter": "Vijay Bhaskar Semwal", "authors": "Vishwanath Bijalwan, Pinki Kumari, Jordan Pascual and Vijay Bhaskar\n  Semwal", "title": "Machine learning approach for text and document mining", "comments": "arXiv admin note: text overlap with arXiv:1003.1795, arXiv:1212.2065\n  by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text Categorization (TC), also known as Text Classification, is the task of\nautomatically classifying a set of text documents into different categories\nfrom a predefined set. If a document belongs to exactly one of the categories,\nit is a single-label classification task; otherwise, it is a multi-label\nclassification task. TC uses several tools from Information Retrieval (IR) and\nMachine Learning (ML) and has received much attention in the last years from\nboth researchers in the academia and industry developers. In this paper, we\nfirst categorize the documents using KNN based machine learning approach and\nthen return the most relevant documents.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 04:37:19 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["Bijalwan", "Vishwanath", ""], ["Kumari", "Pinki", ""], ["Pascual", "Jordan", ""], ["Semwal", "Vijay Bhaskar", ""]]}, {"id": "1406.1583", "submitter": "Monika Rani", "authors": "Satendra kumar, Mamta kathuria, Alok Kumar Gupta and Monika Rani", "title": "Fuzzy clustering of web documents using equivalence relations and fuzzy\n  hierarchical clustering", "comments": "5 pages, Software Engineering (CONSEG), 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional clustering algorithms have difficulties in handling the\nchallenges posed by the collection of natural data which is often vague and\nuncertain. Fuzzy clustering methods have the potential to manage such\nsituations efficiently. Fuzzy clustering method is offered to construct\nclusters with uncertain boundaries and allows that one object belongs to one or\nmore clusters with some membership degree. In this paper, an algorithm and\nexperimental results are presented for fuzzy clustering of web documents using\nequivalence relations and fuzzy hierarchical clustering.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jun 2014 05:12:38 GMT"}], "update_date": "2014-06-09", "authors_parsed": [["kumar", "Satendra", ""], ["kathuria", "Mamta", ""], ["Gupta", "Alok Kumar", ""], ["Rani", "Monika", ""]]}, {"id": "1406.1855", "submitter": "Ramya Perumalsamy", "authors": "P. Ramya, S. Sasirekha", "title": "Text Mining System for Non-Expert Miners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service oriented architecture integrated with text mining allows services to\nextract information in a well defined manner. In this paper, it is proposed to\ndesign a knowledge extracting system for the Ocean Information Data System.\nDeployed ARGO floating sensors of INCOIS (Indian National Council for Ocean\nInformation Systems) organization reflects the characteristics of ocean. This\nis forwarded to the OIDS (Ocean Information Data System). For the data received\nfrom OIDS, pre-processing techniques are applied. Pre-processing involves the\nheader retrieval and data separation. Header information is used to identify\nthe region of sensor, whereas data is used in the analysis process of Ocean\nInformation System. Analyzed data is segmented based on the region, by the\nheader value. Mining technique and composition principle is applied on the\nsegments for further analysis. Index Terms-- Service oriented architecture;\nText Mining; ARGO floating sensor; INCOIS; OIDS; Pre-processing.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 03:04:02 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Ramya", "P.", ""], ["Sasirekha", "S.", ""]]}, {"id": "1406.1875", "submitter": "Xi  zheng", "authors": "Xi Zheng, Akanksha Bansal, Matthew Lease", "title": "Bullseye: Structured Passage Retrieval and Document Highlighting for\n  Scholarly Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Bullseye system for scholarly search. Given a collection of\nresearch papers, Bullseye: 1) identifies relevant passages using any\non-the-shelf algorithm; 2) automatically detects document structure and\nrestricts retrieved passages to user-specifed sections; and 3) highlights those\npassages for each PDF document retrieved. We evaluate Bullseye with regard to\nthree aspects: system effectiveness, user effectiveness, and user effort. In a\nsystem-blind evaluation, users were asked to compare passage retrieval using\nBullseye vs. a baseline which ignores document structure, in regard to four\ntypes of graded assessments. Results show modest improvement in system\neffectiveness while both user effectiveness and user effort show substantial\nimprovement. Users also report very strong demand for passage highlighting in\nscholarly search across both systems considered.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 09:01:44 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Zheng", "Xi", ""], ["Bansal", "Akanksha", ""], ["Lease", "Matthew", ""]]}, {"id": "1406.1969", "submitter": "Adeyinka K Akanbi MR", "authors": "Adeyinka K. Akanbi, Olusanya Y. Agunbiade, Sadiq Kuti, Olumuyiwa J.\n  Dehinbo", "title": "A Semantic Enhanced Model for effective Spatial Information Retrieval", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of information on the web is geographically referenced. Discovering and\nretrieving this geographic information to satisfy various users needs across\nboth open and distributed Spatial Data Infrastructures (SDI) poses eminent\nresearch challenges. However, this is mostly caused by semantic heterogeneity\nin users query and lack of semantic referencing of the Geographic Information\n(GI) metadata. To addressing these challenges, this paper discusses ontology\nbased semantic enhanced model, which explicitly represents GI metadata, and\nprovides linked RDF instances of each entity. The system focuses on semantic\nsearch, ontology, and efficient spatial information retrieval. In particular,\nan integrated model that uses specific domain information extraction to improve\nthe searching and retrieval of ranked spatial search results.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 10:52:22 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Akanbi", "Adeyinka K.", ""], ["Agunbiade", "Olusanya Y.", ""], ["Kuti", "Sadiq", ""], ["Dehinbo", "Olumuyiwa J.", ""]]}, {"id": "1406.2015", "submitter": "Kalyan Veeramachaneni", "authors": "Kalyan Veeramachaneni, Sherif Halawa, Franck Dernoncourt, Una-May\n  O'Reilly, Colin Taylor, Chuong Do", "title": "MOOCdb: Developing Standards and Systems to Support MOOC Data Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a shared data model for enabling data science in Massive Open\nOnline Courses (MOOCs). The model captures students interactions with the\nonline platform. The data model is platform agnostic and is based on some basic\ncore actions that students take on an online learning platform. Students\nusually interact with the platform in four different modes: Observing,\nSubmitting, Collaborating and giving feedback. In observing mode students are\nsimply browsing the online platform, watching videos, reading material, reading\nbook or watching forums. In submitting mode, students submit information to the\nplatform. This includes submissions towards quizzes, homeworks, or any\nassessment modules. In collaborating mode students interact with other students\nor instructors on forums, collaboratively editing wiki or chatting on google\nhangout or other hangout venues. With this basic definitions of activities, and\na data model to store events pertaining to these activities, we then create a\ncommon terminology to map Coursera and edX data into this shared data model.\nThis shared data model called MOOCdb becomes the foundation for a number of\ncollaborative frameworks that enable progress in data science without the need\nto share the data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 19:19:45 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Veeramachaneni", "Kalyan", ""], ["Halawa", "Sherif", ""], ["Dernoncourt", "Franck", ""], ["O'Reilly", "Una-May", ""], ["Taylor", "Colin", ""], ["Do", "Chuong", ""]]}, {"id": "1406.2022", "submitter": "Rahul Tejwani", "authors": "Rahul Tejwani (University at Buffalo)", "title": "Two-dimensional Sentiment Analysis of text", "comments": "sentiment analysis, two-dimensional", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment Analysis aims to get the underlying viewpoint of the text, which\ncould be anything that holds a subjective opinion, such as an online review,\nMovie rating, Comments on Blog posts etc. This paper presents a novel approach\nthat classify text in two-dimensional Emotional space, based on the sentiments\nof the author. The approach uses existing lexical resources to extract feature\nset, which is trained using Supervised Learning techniques.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jun 2014 20:05:36 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Tejwani", "Rahul", "", "University at Buffalo"]]}, {"id": "1406.2049", "submitter": "Xue Li", "authors": "Xue Li, Yu-Jin Zhang, Bin Shen, Bao-Di Liu", "title": "Image Tag Completion by Low-rank Factorization with Dual Reconstruction\n  Structure Preserved", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel tag completion algorithm is proposed in this paper, which is designed\nwith the following features: 1) Low-rank and error s-parsity: the incomplete\ninitial tagging matrix D is decomposed into the complete tagging matrix A and a\nsparse error matrix E. However, instead of minimizing its nuclear norm, A is\nfurther factor-ized into a basis matrix U and a sparse coefficient matrix V,\ni.e. D=UV+E. This low-rank formulation encapsulating sparse coding enables our\nalgorithm to recover latent structures from noisy initial data and avoid\nperforming too much denoising; 2) Local reconstruction structure consistency:\nto steer the completion of D, the local linear reconstruction structures in\nfeature space and tag space are obtained and preserved by U and V respectively.\nSuch a scheme could alleviate the negative effect of distances measured by\nlow-level features and incomplete tags. Thus, we can seek a balance between\nexploiting as much information and not being mislead to suboptimal performance.\nExperiments conducted on Corel5k dataset and the newly issued Flickr30Concepts\ndataset demonstrate the effectiveness and efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 01:22:43 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Li", "Xue", ""], ["Zhang", "Yu-Jin", ""], ["Shen", "Bin", ""], ["Liu", "Bao-Di", ""]]}, {"id": "1406.2235", "submitter": "Michael Smith", "authors": "Michael R. Smith, Tony Martinez, Michael Gashler", "title": "A Hybrid Latent Variable Neural Network Model for Item Recommendation", "comments": "10 pages, 3 tables. arXiv admin note: text overlap with\n  arXiv:1312.5394", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is used to recommend items to a user without\nrequiring a knowledge of the item itself and tends to outperform other\ntechniques. However, collaborative filtering suffers from the cold-start\nproblem, which occurs when an item has not yet been rated or a user has not\nrated any items. Incorporating additional information, such as item or user\ndescriptions, into collaborative filtering can address the cold-start problem.\nIn this paper, we present a neural network model with latent input variables\n(latent neural network or LNN) as a hybrid collaborative filtering technique\nthat addresses the cold-start problem. LNN outperforms a broad selection of\ncontent-based filters (which make recommendations based on item descriptions)\nand other hybrid approaches while maintaining the accuracy of state-of-the-art\ncollaborative filtering techniques.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 16:21:11 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Smith", "Michael R.", ""], ["Martinez", "Tony", ""], ["Gashler", "Michael", ""]]}, {"id": "1406.2431", "submitter": "Oren Anava", "authors": "Oren Anava, Shahar Golan, Nadav Golbandi, Zohar Karnin, Ronny Lempel,\n  Oleg Rokhlenko, Oren Somekh", "title": "Budget-Constrained Item Cold-Start Handling in Collaborative Filtering\n  Recommenders via Optimal Design", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that collaborative filtering (CF) based recommender systems\nprovide better modeling of users and items associated with considerable rating\nhistory. The lack of historical ratings results in the user and the item\ncold-start problems. The latter is the main focus of this work. Most of the\ncurrent literature addresses this problem by integrating content-based\nrecommendation techniques to model the new item. However, in many cases such\ncontent is not available, and the question arises is whether this problem can\nbe mitigated using CF techniques only. We formalize this problem as an\noptimization problem: given a new item, a pool of available users, and a budget\nconstraint, select which users to assign with the task of rating the new item\nin order to minimize the prediction error of our model. We show that the\nobjective function is monotone-supermodular, and propose efficient optimal\ndesign based algorithms that attain an approximation to its optimum. Our\nfindings are verified by an empirical study using the Netflix dataset, where\nthe proposed algorithms outperform several baselines for the problem at hand.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 06:17:23 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 21:10:43 GMT"}, {"version": "v3", "created": "Tue, 20 Sep 2016 09:51:02 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Anava", "Oren", ""], ["Golan", "Shahar", ""], ["Golbandi", "Nadav", ""], ["Karnin", "Zohar", ""], ["Lempel", "Ronny", ""], ["Rokhlenko", "Oleg", ""], ["Somekh", "Oren", ""]]}, {"id": "1406.2538", "submitter": "Guntis Barzdins", "authors": "Guntis Barzdins", "title": "FrameNet CNL: a Knowledge Representation and Information Extraction\n  Language", "comments": "CNL-2014 camera-ready version. The final publication is available at\n  link.springer.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a FrameNet-based information extraction and knowledge\nrepresentation framework, called FrameNet-CNL. The framework is used on natural\nlanguage documents and represents the extracted knowledge in a tailor-made\nFrame-ontology from which unambiguous FrameNet-CNL paraphrase text can be\ngenerated automatically in multiple languages. This approach brings together\nthe fields of information extraction and CNL, because a source text can be\nconsidered belonging to FrameNet-CNL, if information extraction parser produces\nthe correct knowledge representation as a result. We describe a\nstate-of-the-art information extraction parser used by a national news agency\nand speculate that FrameNet-CNL eventually could shape the natural language\nsubset used for writing the newswire articles.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 13:16:36 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Barzdins", "Guntis", ""]]}, {"id": "1406.2580", "submitter": "L.T. Handoko", "authors": "D. H. Apriyanti, A.A. Arymurthy, L.T. Handoko", "title": "Identification of Orchid Species Using Content-Based Flower Image\n  Retrieval", "comments": "Proceeding of International Conference on Computer, Control,\n  Informatics and its Applications 2013, pp. 53-57", "journal-ref": null, "doi": "10.1109/IC3INA.2013.6819148", "report-no": "KRPURWODADILIPI-13044", "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we developed the system for recognizing the orchid species by\nusing the images of flower. We used MSRM (Maximal Similarity based on Region\nMerging) method for segmenting the flower object from the background and\nextracting the shape feature such as the distance from the edge to the centroid\npoint of the flower, aspect ratio, roundness, moment invariant, fractal\ndimension and also extract color feature. We used HSV color feature with\nignoring the V value. To retrieve the image, we used Support Vector Machine\n(SVM) method. Orchid is a unique flower. It has a part of flower called lip\n(labellum) that distinguishes it from other flowers even from other types of\norchids. Thus, in this paper, we proposed to do feature extraction not only on\nflower region but also on lip (labellum) region. The result shows that our\nproposed method can increase the accuracy value of content based flower image\nretrieval for orchid species up to $\\pm$ 14%. The most dominant feature is\nCentroid Contour Distance, Moment Invariant and HSV Color. The system accuracy\nis 85,33% in validation phase and 79,33% in testing phase.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jun 2014 15:11:27 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Apriyanti", "D. H.", ""], ["Arymurthy", "A. A.", ""], ["Handoko", "L. T.", ""]]}, {"id": "1406.2746", "submitter": "Ekin Oguz", "authors": "Mishari Almishari, Mohamed Ali Kaafar, Gene Tsudik, Ekin Oguz", "title": "Are 140 Characters Enough? A Large-Scale Linkability Study of Tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microblogging is a very popular Internet activity that informs and entertains\ngreat multitudes of people world-wide via quickly and scalably disseminated\nterse messages containing all kinds of newsworthy utterances. Even though\nmicroblogging is neither designed nor meant to emphasize privacy, numerous\ncontributors hide behind pseudonyms and compartmentalize their different\nincarnations via multiple accounts within the same, or across multiple,\nsite(s). Prior work has shown that stylometric analysis is a very powerful tool\ncapable of linking product or service reviews and blogs that are produced by\nthe same author when the number of authors is large. In this paper, we explore\nlinkability of tweets. Our results, based on a very large corpus of tweets,\nclearly demonstrate that, at least for relatively active tweeters, linkability\nof tweets by the same author is easily attained even when the number of\ntweeters is large. We also show that our linkability results hold for a set of\nactual Twitter users who tweet from multiple accounts. This has some obvious\nprivacy implications, both positive and negative.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 00:13:59 GMT"}, {"version": "v2", "created": "Tue, 9 Sep 2014 03:32:41 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Almishari", "Mishari", ""], ["Kaafar", "Mohamed Ali", ""], ["Tsudik", "Gene", ""], ["Oguz", "Ekin", ""]]}, {"id": "1406.2880", "submitter": "Ulf Sch\\\"oneberg", "authors": "Ulf Sch\\\"oneberg and Wolfram Sperber", "title": "POS Tagging and its Applications for Mathematics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content analysis of scientific publications is a nontrivial task, but a\nuseful and important one for scientific information services. In the Gutenberg\nera it was a domain of human experts; in the digital age many machine-based\nmethods, e.g., graph analysis tools and machine-learning techniques, have been\ndeveloped for it. Natural Language Processing (NLP) is a powerful\nmachine-learning approach to semiautomatic speech and language processing,\nwhich is also applicable to mathematics. The well established methods of NLP\nhave to be adjusted for the special needs of mathematics, in particular for\nhandling mathematical formulae. We demonstrate a mathematics-aware part of\nspeech tagger and give a short overview about our adaptation of NLP methods for\nmathematical publications. We show the use of the tools developed for key\nphrase extraction and classification in the database zbMATH.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 12:25:26 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Sch\u00f6neberg", "Ulf", ""], ["Sperber", "Wolfram", ""]]}, {"id": "1406.3170", "submitter": "Simon Gog", "authors": "Simon Gog and Matthias Petri", "title": "Compact Indexes for Flexible Top-k Retrieval", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We engineer a self-index based retrieval system capable of rank-safe\nevaluation of top-k queries. The framework generalizes the GREEDY approach of\nCulpepper et al. (ESA 2010) to handle multi-term queries, including over\nphrases. We propose two techniques which significantly reduce the ranking time\nfor a wide range of popular Information Retrieval (IR) relevance measures, such\nas TFxIDF and BM25. First, we reorder elements in the document array according\nto document weight. Second, we introduce the repetition array, which\ngeneralizes Sadakane's (JDA 2007) document frequency structure to document\nsubsets. Combining document and repetition array, we achieve attractive\nfunctionality-space trade-offs. We provide an extensive evaluation of our\nsystem on terabyte-sized IR collections.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 09:47:09 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Gog", "Simon", ""], ["Petri", "Matthias", ""]]}, {"id": "1406.3188", "submitter": "Elisabeth Lex", "authors": "Elisabeth Lex and Inayat Khan and Horst Bischof and Michael Granitzer", "title": "Assessing the Quality of Web Content", "comments": "4 pages, ECML/PKDD 2010 Discovery Challenge Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our approach towards the ECML/PKDD Discovery Challenge\n2010. The challenge consists of three tasks: (1) a Web genre and facet\nclassification task for English hosts, (2) an English quality task, and (3) a\nmultilingual quality task (German and French). In our approach, we create an\nensemble of three classifiers to predict unseen Web hosts whereas each\nclassifier is trained on a different feature set. Our final NDCG on the whole\ntest set is 0:575 for Task 1, 0:852 for Task 2, and 0:81 (French) and 0:77\n(German) for Task 3, which ranks second place in the ECML/PKDD Discovery\nChallenge 2010.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 10:40:15 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Lex", "Elisabeth", ""], ["Khan", "Inayat", ""], ["Bischof", "Horst", ""], ["Granitzer", "Michael", ""]]}, {"id": "1406.3216", "submitter": "Giuseppe Cascavilla", "authors": "Andrea Burattin, Giuseppe Cascavilla, Mauro Conti", "title": "SocialSpy: Browsing (Supposedly) Hidden Information in Online Social\n  Networks", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Social Networks are becoming the most important \"places\" where people\nshare information about their lives. With the increasing concern that users\nhave about privacy, most social networks offer ways to control the privacy of\nthe user. Unfortunately, we believe that current privacy settings are not as\neffective as users might think.\n  In this paper, we highlight this problem focusing on one of the most popular\nsocial networks, Facebook. In particular, we show how easy it is to retrieve\ninformation that a user might have set as (and hence thought as) \"private\". As\na case study, we focus on retrieving the list of friends for users that did set\nthis information as \"hidden\" (to non-friends). We propose four different\nstrategies to achieve this goal, and we evaluate them. The results of our\nthorough experiments show the feasibility of our strategies as well as their\neffectiveness: our approach is able to retrieve a significant percentage of the\nnames of the \"hidden\" friends: i.e., some 25% on average, and more than 70% for\nsome users.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 12:34:52 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2014 15:35:29 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Burattin", "Andrea", ""], ["Cascavilla", "Giuseppe", ""], ["Conti", "Mauro", ""]]}, {"id": "1406.3277", "submitter": "Hadi Fanaee-T", "authors": "Hadi Fanaee-T and Mehran Yazdi", "title": "A Semantic VSM-Based Recommender System", "comments": null, "journal-ref": "International Journal of Computer Theory and Engineering vol. 5,\n  no. 2, pp. 331-336, 2013", "doi": "10.7763/IJCTE.2013.V5.704", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online forums enable users to discuss together around various topics. One of\nthe serious problems of these environments is high volume of discussions and\nthus information overload problem. Unfortunately without considering the users\ninterests, traditional Information Retrieval (IR) techniques are not able to\nsolve the problem. Therefore, employment of a Recommender System (RS) that\ncould suggest favorite's topics of users according to their tastes could\nincreases the dynamism of forum and prevent the users from duplicate posts. In\naddition, consideration of semantics can be useful for increasing the\nperformance of IR based RS. Our goal is study of impact of ontology and data\nmining techniques on improving of content-based RS. For this purpose, at first,\nthree type of ontologies will be constructed from the domain corpus with\nutilization of text mining, Natural Language Processing (NLP) and Wordnet and\nthen they will be used as an input in two kind of RS: one, fully ontology-based\nand one with enriching the user profile vector with ontology in vector space\nmodel (VSM) (proposed method). Afterward the results will be compared with the\nsimple VSM based RS. Given results show that the proposed RS presents the\nhighest performance.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 16:02:16 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Fanaee-T", "Hadi", ""], ["Yazdi", "Mehran", ""]]}, {"id": "1406.3287", "submitter": "Matthew Mayo", "authors": "Matthew Mayo", "title": "A Clustering Analysis of Tweet Length and its Relation to Sentiment", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis of Twitter data is performed. The researcher has made the\nfollowing contributions via this paper: (1) an innovative method for deriving\nsentiment score dictionaries using an existing sentiment dictionary as seed\nwords is explored, and (2) an analysis of clustered tweet sentiment scores\nbased on tweet length is performed.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 17:01:10 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 14:07:35 GMT"}, {"version": "v3", "created": "Wed, 18 Feb 2015 19:44:24 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Mayo", "Matthew", ""]]}, {"id": "1406.3714", "submitter": "Richa Sharma", "authors": "Richa Sharma, Shweta Nigam and Rekha Jain", "title": "Mining of product reviews at aspect level", "comments": null, "journal-ref": "International Journal in Foundations of Computer Science &\n  Technology (IJFCST), Vol.4, No.3, May 2014", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Todays world is a world of Internet, almost all work can be done with the\nhelp of it, from simple mobile phone recharge to biggest business deals can be\ndone with the help of this technology. People spent their most of the times on\nsurfing on the Web it becomes a new source of entertainment, education,\ncommunication, shopping etc. Users not only use these websites but also give\ntheir feedback and suggestions that will be useful for other users. In this way\na large amount of reviews of users are collected on the Web that needs to be\nexplored, analyse and organized for better decision making. Opinion Mining or\nSentiment Analysis is a Natural Language Processing and Information Extraction\ntask that identifies the users views or opinions explained in the form of\npositive, negative or neutral comments and quotes underlying the text. Aspect\nbased opinion mining is one of the level of Opinion mining that determines the\naspect of the given reviews and classify the review for each feature. In this\npaper an aspect based opinion mining system is proposed to classify the reviews\nas positive, negative and neutral for each feature. Negation is also handled in\nthe proposed system. Experimental results using reviews of products show the\neffectiveness of the system.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jun 2014 10:39:57 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Sharma", "Richa", ""], ["Nigam", "Shweta", ""], ["Jain", "Rekha", ""]]}, {"id": "1406.3870", "submitter": "Iaakov Exman", "authors": "Iaakov Exman and Alex Krepch", "title": "An Anti_Turing Test: Reduced Variables for Social Network Friends'\n  Recommendations", "comments": "11 pages, 4 figures, Extended version of paper originally published\n  in the SKY International Workshop on Software Knowledge, September 2013", "journal-ref": null, "doi": "10.5220/0004641600550061", "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A routine activity of social networks servers is to recommend candidate\nfriends that one may know and stimulate addition of these people to one's\ncontacts. An intriguing issue is how these recommendation lists are composed.\nThis work investigates the main variables involved in the recommendation\nactivity, in order to reproduce these lists including its time dependent\ncharacteristics. We propose relevant algorithms. Besides conventional\napproaches, such as friend_of_a_friend, two techniques of importance have not\nbeen emphasized in previous works: randomization and direct use of\ninterestingness criteria. An automatic software tool to implement these\ntechniques is proposed. Its architecture and implementation are discussed.\nAfter a preliminary analysis of actual data collected from social networks, the\ntool is used to simulate social network friends' recommendations.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jun 2014 23:58:17 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Exman", "Iaakov", ""], ["Krepch", "Alex", ""]]}, {"id": "1406.3882", "submitter": "Yui Noma", "authors": "Yui Noma, Makiko Konoshima", "title": "Eclipse Hashing: Alexandrov Compactification and Hashing with\n  Hyperspheres for Fast Similarity Search", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The similarity searches that use high-dimensional feature vectors consisting\nof a vast amount of data have a wide range of application. One way of\nconducting a fast similarity search is to transform the feature vectors into\nbinary vectors and perform the similarity search by using the Hamming distance.\nSuch a transformation is a hashing method, and the choice of hashing function\nis important. Hashing methods using hyperplanes or hyperspheres are proposed.\nOne study reported here is inspired by Spherical LSH, and we use hypersperes to\nhash the feature vectors. Our method, called Eclipse-hashing, performs a\ncompactification of R^n by using the inverse stereographic projection, which is\na kind of Alexandrov compactification. By using Eclipse-hashing, one can obtain\nthe hypersphere-hash function without explicitly using hyperspheres. Hence, the\nnumber of nonlinear operations is reduced and the processing time of hashing\nbecomes shorter. Furthermore, we also show that as a result of improving the\napproximation accuracy, Eclipse-hashing is more accurate than\nhyperplane-hashing.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jun 2014 01:38:50 GMT"}], "update_date": "2014-06-17", "authors_parsed": [["Noma", "Yui", ""], ["Konoshima", "Makiko", ""]]}, {"id": "1406.4737", "submitter": "Sanjay Chakraborty", "authors": "Sanjay Chakraborty and N.K. Nagwani", "title": "Performance Evaluation of Incremental K-means Clustering Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The incremental K-means clustering algorithm has already been proposed and\nanalysed in paper [Chakraborty and Nagwani, 2011]. It is a very innovative\napproach which is applicable in periodically incremental environment and\ndealing with a bulk of updates. In this paper the performance evaluation is\ndone for this incremental K-means clustering algorithm using air pollution\ndatabase. This paper also describes the comparison on the performance\nevaluations between existing K-means clustering and incremental K-means\nclustering using that particular database. It also evaluates that the\nparticular point of change in the database upto which incremental K-means\nclustering performs much better than the existing K-means clustering. That\nparticular point of change in the database is known as \"Threshold value\" or \"%\ndelta change in the database\". This paper also defines the basic methodology\nfor the incremental K-means clustering algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 14:34:18 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Chakraborty", "Sanjay", ""], ["Nagwani", "N. K.", ""]]}, {"id": "1406.4751", "submitter": "Sanjay Chakraborty", "authors": "Sanjay Chakraborty, N.K.Nagwani and Lopamudra Dey", "title": "Performance Comparison of Incremental K-means and Incremental DBSCAN\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental K-means and DBSCAN are two very important and popular clustering\ntechniques for today's large dynamic databases (Data warehouses, WWW and so on)\nwhere data are changed at random fashion. The performance of the incremental\nK-means and the incremental DBSCAN are different with each other based on their\ntime analysis characteristics. Both algorithms are efficient compare to their\nexisting algorithms with respect to time, cost and effort. In this paper, the\nperformance evaluation of incremental DBSCAN clustering algorithm is\nimplemented and most importantly it is compared with the performance of\nincremental K-means clustering algorithm and it also explains the\ncharacteristics of these two algorithms based on the changes of the data in the\ndatabase. This paper also explains some logical differences between these two\nmost popular clustering algorithms. This paper uses an air pollution database\nas original database on which the experiment is performed.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 15:00:53 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Chakraborty", "Sanjay", ""], ["Nagwani", "N. K.", ""], ["Dey", "Lopamudra", ""]]}, {"id": "1406.4754", "submitter": "Sanjay Chakraborty", "authors": "Sanjay Chakraborty and N.K.Nagwani", "title": "Analysis and Study of Incremental DBSCAN Clustering Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the incremental behaviours of Density based clustering.\nIt specially focuses on the Density Based Spatial Clustering of Applications\nwith Noise (DBSCAN) algorithm and its incremental approach.DBSCAN relies on a\ndensity based notion of clusters.It discovers clusters of arbitrary shapes in\nspatial databases with noise.In incremental approach, the DBSCAN algorithm is\napplied to a dynamic database where the data may be frequently updated. After\ninsertions or deletions to the dynamic database, the clustering discovered by\nDBSCAN has to be updated. And we measure the new cluster by directly compute\nthe new data entering into the existing clusters instead of rerunning the\nalgorithm.It finally discovers new updated clusters and outliers as well.Thus\nit describes at what percent of delta change in the original database the\nactual and incremental DBSCAN algorithms behave like same.DBSCAN is widely used\nin those situations where large multidimensional databases are maintained such\nas Data Warehouse.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 15:03:29 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Chakraborty", "Sanjay", ""], ["Nagwani", "N. K.", ""]]}, {"id": "1406.4784", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "Improved Densification of One Permutation Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing work on densification of one permutation hashing reduces the\nquery processing cost of the $(K,L)$-parameterized Locality Sensitive Hashing\n(LSH) algorithm with minwise hashing, from $O(dKL)$ to merely $O(d + KL)$,\nwhere $d$ is the number of nonzeros of the data vector, $K$ is the number of\nhashes in each hash table, and $L$ is the number of hash tables. While that is\na substantial improvement, our analysis reveals that the existing densification\nscheme is sub-optimal. In particular, there is no enough randomness in that\nprocedure, which affects its accuracy on very sparse datasets.\n  In this paper, we provide a new densification procedure which is provably\nbetter than the existing scheme. This improvement is more significant for very\nsparse datasets which are common over the web. The improved technique has the\nsame cost of $O(d + KL)$ for query processing, thereby making it strictly\npreferable over the existing procedure. Experimental evaluations on public\ndatasets, in the task of hashing based near neighbor search, support our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 16:16:22 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1406.4803", "submitter": "Deepshree Vadeyar", "authors": "Deepshree A. Vadeyar, Yogish H.K", "title": "Reorganization of Links to Improve User Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Website can be easily design but to efficient user navigation is not a easy\ntask since user behavior is keep changing and developer view is quite different\nfrom what user wants, so to improve navigation one way is reorganization of\nwebsite structure. For reorganization here proposed strategy is farthest first\ntraversal clustering algorithm perform clustering on two numeric parameters and\nfor finding frequent traversal path of user Apriori algorithm is used. Our aim\nis to perform reorganization with fewer changes in website structure.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 17:25:06 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Vadeyar", "Deepshree A.", ""], ["K", "Yogish H.", ""]]}, {"id": "1406.4877", "submitter": "David Martins de Matos", "authors": "Francisco Raposo, Ricardo Ribeiro, David Martins de Matos", "title": "On the Application of Generic Summarization Algorithms to Music", "comments": "12 pages, 1 table; Submitted to IEEE Signal Processing Letters", "journal-ref": "IEEE Signal Processing Letters, IEEE, vol. 22, n. 1, January 2015", "doi": "10.1109/LSP.2014.2347582", "report-no": null, "categories": "cs.IR cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several generic summarization algorithms were developed in the past and\nsuccessfully applied in fields such as text and speech summarization. In this\npaper, we review and apply these algorithms to music. To evaluate this\nsummarization's performance, we adopt an extrinsic approach: we compare a Fado\nGenre Classifier's performance using truncated contiguous clips against the\nsummaries extracted with those algorithms on 2 different datasets. We show that\nMaximal Marginal Relevance (MMR), LexRank and Latent Semantic Analysis (LSA)\nall improve classification performance in both datasets used for testing.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 20:10:22 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Raposo", "Francisco", ""], ["Ribeiro", "Ricardo", ""], ["de Matos", "David Martins", ""]]}, {"id": "1406.5162", "submitter": "Baichuan Zhang", "authors": "Baichuan Zhang, Tanay Kumar Saha and Mohammad Al Hasan", "title": "Name Disambiguation from link data in a collaboration graph using\n  temporal and topological features", "comments": "The short version of this paper has been accepted to ASONAM 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a social community, multiple persons may share the same name, phone number\nor some other identifying attributes. This, along with other phenomena, such as\nname abbreviation, name misspelling, and human error leads to erroneous\naggregation of records of multiple persons under a single reference. Such\nmistakes affect the performance of document retrieval, web search, database\nintegration, and more importantly, improper attribution of credit (or blame).\nThe task of entity disambiguation partitions the records belonging to multiple\npersons with the objective that each decomposed partition is composed of\nrecords of a unique person. Existing solutions to this task use either\nbiographical attributes, or auxiliary features that are collected from external\nsources, such as Wikipedia. However, for many scenarios, such auxiliary\nfeatures are not available, or they are costly to obtain. Besides, the attempt\nof collecting biographical or external data sustains the risk of privacy\nviolation. In this work, we propose a method for solving entity disambiguation\ntask from link information obtained from a collaboration network. Our method is\nnon-intrusive of privacy as it uses only the time-stamped graph topology of an\nanonymized network. Experimental results on two real-life academic\ncollaboration networks show that the proposed method has satisfactory\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jun 2014 19:22:33 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 16:07:38 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2016 19:39:13 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Zhang", "Baichuan", ""], ["Saha", "Tanay Kumar", ""], ["Hasan", "Mohammad Al", ""]]}, {"id": "1406.5616", "submitter": "S. K. Sahay", "authors": "R.K. Roul and S.K. Sahay", "title": "An Effective Approach for Web Document Classification using the Concept\n  of Association Analysis of Data Mining", "comments": "9 Pages", "journal-ref": "IJCSET, 2012, Vol. 3, No. 10, p. 483", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential growth of the web increased the importance of web document\nclassification and data mining. To get the exact information, in the form of\nknowing what classes a web document belongs to, is expensive. Automatic\nclassification of web document is of great use to search engines which provides\nthis information at a low cost. In this paper, we propose an approach for\nclassifying the web document using the frequent item word sets generated by the\nFrequent Pattern (FP) Growth which is an association analysis technique of data\nmining. These set of associated words act as feature set. The final\nclassification obtained after Na\\\"ive Bayes classifier used on the feature set.\nFor the experimental work, we use Gensim package, as it is simple and robust.\nResults show that our approach can be effectively classifying the web document.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 14:33:09 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Roul", "R. K.", ""], ["Sahay", "S. K.", ""]]}, {"id": "1406.5617", "submitter": "S. K. Sahay", "authors": "R.K. Roul, O. R. Devanand and S.K. Sahay", "title": "Web Document Clustering and Ranking using Tf-Idf based Apriori Approach", "comments": "5 Pages", "journal-ref": "IJCA Proceedings on ICACEA, No. 2, p. 34 (2014)", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic web has increased exponentially over the past few years with more\nthan thousands of documents related to a subject available to the user now.\nMost of the web documents are unstructured and not in an organized manner and\nhence user facing more difficult to find relevant documents. A more useful and\nefficient mechanism is combining clustering with ranking, where clustering can\ngroup the similar documents in one place and ranking can be applied to each\ncluster for viewing the top documents at the beginning.. Besides the particular\nclustering algorithm, the different term weighting functions applied to the\nselected features to represent web document is a main aspect in clustering\ntask. Keeping this approach in mind, here we proposed a new mechanism called\nTf-Idf based Apriori for clustering the web documents. We then rank the\ndocuments in each cluster using Tf-Idf and similarity factor of documents based\non the user query. This approach will helps the user to get all his relevant\ndocuments in one place and can restrict his search to some top documents of his\nchoice. For experimental purpose, we have taken the Classic3 and Classic4\ndatasets of Cornell University having more than 10,000 documents and use gensim\ntoolkit to carry out our work. We have compared our approach with traditional\napriori algorithm and found that our approach is giving better results for\nhigher minimum support. Our ranking mechanism is also giving a good F-measure\nof 78%.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 14:38:21 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Roul", "R. K.", ""], ["Devanand", "O. R.", ""], ["Sahay", "S. K.", ""]]}, {"id": "1406.5690", "submitter": "Sonali Gupta", "authors": "Sonali Gupta, Komal kumar Bhatia, Pikakshi Manchanda", "title": "WebParF: A Web partitioning framework for Parallel Crawlers", "comments": "8pages, 7 figures, ISSN : 0975-3397 Vol.5 no.8, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever proliferating size and scale of the WWW [1] efficient ways of\nexploring content are of increasing importance. How can we efficiently retrieve\ninformation from it through crawling? And in this era of tera and multi-core\nprocessors, we ought to think of multi-threaded processes as a serving\nsolution. So, even better how can we improve the crawling performance by using\nparallel crawlers that work independently? The paper devotes to the fundamental\ndevelopment in the field of parallel crawlers [4] highlighting the advantages\nand challenges arising from its design. The paper also focuses on the aspect of\nURL distribution among the various parallel crawling processes or threads and\nordering the URLs within each distributed set of URLs. How to distribute URLs\nfrom the URL frontier to the various concurrently executing crawling process\nthreads is an orthogonal problem. The paper provides a solution to the problem\nby designing a framework WebParF that partitions the URL frontier into a\nseveral URL queues while considering the various design issues.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 09:33:21 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Gupta", "Sonali", ""], ["Bhatia", "Komal kumar", ""], ["Manchanda", "Pikakshi", ""]]}, {"id": "1406.5751", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Vijay Gadepally, Pete Michaleas, Nabil Schear, Mayank\n  Varia, Arkady Yerukhimovich, Robert K. Cunningham (MIT)", "title": "Computing on Masked Data: a High Performance Method for Improving Big\n  Data Veracity", "comments": "to appear in IEEE High Performance Extreme Computing 2014\n  (ieee-hpec.org)", "journal-ref": null, "doi": "10.1109/HPEC.2014.7040946", "report-no": null, "categories": "cs.CR astro-ph.IM cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing gap between data and users calls for innovative tools that\naddress the challenges faced by big data volume, velocity and variety. Along\nwith these standard three V's of big data, an emerging fourth \"V\" is veracity,\nwhich addresses the confidentiality, integrity, and availability of the data.\nTraditional cryptographic techniques that ensure the veracity of data can have\noverheads that are too large to apply to big data. This work introduces a new\ntechnique called Computing on Masked Data (CMD), which improves data veracity\nby allowing computations to be performed directly on masked data and ensuring\nthat only authorized recipients can unmask the data. Using the sparse linear\nalgebra of associative arrays, CMD can be performed with significantly less\noverhead than other approaches while still supporting a wide range of linear\nalgebraic operations on the masked data. Databases with strong support of\nsparse operations, such as SciDB or Apache Accumulo, are ideally suited to this\ntechnique. Examples are shown for the application of CMD to a complex DNA\nmatching algorithm and to database operations over social media data.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 19:06:04 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Kepner", "Jeremy", "", "MIT"], ["Gadepally", "Vijay", "", "MIT"], ["Michaleas", "Pete", "", "MIT"], ["Schear", "Nabil", "", "MIT"], ["Varia", "Mayank", "", "MIT"], ["Yerukhimovich", "Arkady", "", "MIT"], ["Cunningham", "Robert K.", "", "MIT"]]}, {"id": "1406.5824", "submitter": "Serena Yeung", "authors": "Serena Yeung, Alireza Fathi, and Li Fei-Fei", "title": "VideoSET: Video Summary Evaluation through Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present VideoSET, a method for Video Summary Evaluation\nthrough Text that can evaluate how well a video summary is able to retain the\nsemantic information contained in its original video. We observe that semantics\nis most easily expressed in words, and develop a text-based approach for the\nevaluation. Given a video summary, a text representation of the video summary\nis first generated, and an NLP-based metric is then used to measure its\nsemantic distance to ground-truth text summaries written by humans. We show\nthat our technique has higher agreement with human judgment than pixel-based\ndistance metrics. We also release text annotations and ground-truth text\nsummaries for a number of publicly available video datasets, for use by the\ncomputer vision community.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 07:56:23 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Yeung", "Serena", ""], ["Fathi", "Alireza", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1406.5946", "submitter": "Jan Broekaert", "authors": "Lorena P\\'erez-Garc\\'ia, Jan Broekaert and Nicole Note", "title": "Is a `Wirikuta empowerment' of the Huichol measurable on the Internet?", "comments": "14 pages, 1 figure, 4 graphs (submitted to journal)", "journal-ref": "Internet Research, 26, 5, 1269 - 1290 (2016)", "doi": "10.1108/IntR-07-2014-0185", "report-no": null, "categories": "cs.SI cs.CY cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current social and activist movements find the opportunity in social media to\neffectively impact on the agenda of governing bodies and create `global'\nperceptions -- it is often claimed. Content related to the social and activist\nmovements is online, to be accessed, supported or disputed and distributed from\nvirtually anywhere at any time, in the public sphere of the Internet. This\nactivity allows the enlargement of social movements and would increase the\nempowerment in the concerned communities. The aim of this explorative study is\nto assess whether the temporal evolution of the Normalised Web Distance (NWD)\n--as defined by Cilibrasi & Vit\\'anyi (2007)-- between identifying terms\nconcerning this activism could be used to measure the progress or decline of\nsocial empowerment through the Internet. The NWD relies on the page count\nnumber of single and joint queries, which in our study have been registered\nusing a freely available web browser (e.g. Google Search) providing a time\nsearch window for temporal query results. To explore this meta-data technique,\nwe introduce the case of a perceived Wirikuta online movement, which originated\nin Mexico with the aim to protect the Huichols' sacred land and water resources\nfrom open mining projects for silver ore. We conducted a small scale Internet\nstudy relating the key terms `Wirikuta', `Huichol', and `Wixarika' and their\nco-occurrence with seven positive qualifiers (e.g. `sacred land'), five\nnegative qualifiers (e.g. `violence') and one neutral qualifier (`table') over\ntime, annually from 1994 till 2013. We confirm close semantic clustering over\ntime of traditional indigeneous identity terms of the Huichol, and observe a\nslight convergence of key terms to `mines' and less pronounced to `sacred land'\nand a divergence with respect to `ancestors' indicating a complex image of a\ntendency of empowerment.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 15:32:14 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["P\u00e9rez-Garc\u00eda", "Lorena", ""], ["Broekaert", "Jan", ""], ["Note", "Nicole", ""]]}, {"id": "1406.6126", "submitter": "Ross Moore", "authors": "Ross Moore", "title": "PDF/A-3u as an archival format for Accessible mathematics", "comments": "This is a post-print version of original in volume: S.M. Watt et al.\n  (Eds.): CICM 2014, LNAI 8543, pp.184-199, 2014; available at\n  http://link.springer.com/search?query=LNAI+8543, along with supplementary\n  PDF. This version, with supplement as attachment, is enriched to validate as\n  PDF/A-3u modulo an error in white-space handling in the pdfTeX version used\n  to generate it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Including LaTeX source of mathematical expressions, within the PDF document\nof a text-book or research paper, has definite benefits regarding\n`Accessibility' considerations. Here we describe three ways in which this can\nbe done, fully compatibly with international standards ISO 32000, ISO 19005-3,\nand the forthcoming ISO 32000-2 (PDF 2.0). Two methods use embedded files, also\nknown as `attachments', holding information in either LaTeX or MathML formats,\nbut use different PDF structures to relate these attachments to regions of the\ndocument window. One uses structure, so is applicable to a fully `Tagged PDF'\ncontext, while the other uses /AF tagging of the relevant content. The third\nmethod requires no tagging at all, instead including the source coding as the\n/ActualText replacement of a so-called `fake space'. Information provided this\nway is extracted via simple Select/Copy/Paste actions, and is available to\nexisting screen-reading software and assistive technologies.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 02:48:42 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Moore", "Ross", ""]]}, {"id": "1406.6312", "submitter": "Ahmed El-Kishky", "authors": "Ahmed El-Kishky, Yanglei Song, Chi Wang, Clare Voss, Jiawei Han", "title": "Scalable Topical Phrase Mining from Text Corpora", "comments": null, "journal-ref": "Proceedings of the VLDB Endowment, Vol. 8(3), pp. 305 - 316, 2014", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most topic modeling algorithms model text corpora with unigrams, human\ninterpretation often relies on inherent grouping of terms into phrases. As\nsuch, we consider the problem of discovering topical phrases of mixed lengths.\nExisting work either performs post processing to the inference results of\nunigram-based topic models, or utilizes complex n-gram-discovery topic models.\nThese methods generally produce low-quality topical phrases or suffer from poor\nscalability on even moderately-sized datasets. We propose a different approach\nthat is both computationally efficient and effective. Our solution combines a\nnovel phrase mining framework to segment a document into single and multi-word\nphrases, and a new topic model that operates on the induced document partition.\nOur approach discovers high quality topical phrases with negligible extra cost\nto the bag-of-words topic model in a variety of datasets including research\npublication titles, abstracts, reviews, and news articles.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jun 2014 17:10:29 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 00:18:06 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["El-Kishky", "Ahmed", ""], ["Song", "Yanglei", ""], ["Wang", "Chi", ""], ["Voss", "Clare", ""], ["Han", "Jiawei", ""]]}, {"id": "1406.6314", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Richard Nock", "title": "Further heuristics for $k$-means: The merge-and-split heuristic and the\n  $(k,l)$-means", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the optimal $k$-means clustering is NP-hard in general and many\nheuristics have been designed for minimizing monotonically the $k$-means\nobjective. We first show how to extend Lloyd's batched relocation heuristic and\nHartigan's single-point relocation heuristic to take into account empty-cluster\nand single-point cluster events, respectively. Those events tend to\nincreasingly occur when $k$ or $d$ increases, or when performing several\nrestarts. First, we show that those special events are a blessing because they\nallow to partially re-seed some cluster centers while further minimizing the\n$k$-means objective function. Second, we describe a novel heuristic,\nmerge-and-split $k$-means, that consists in merging two clusters and splitting\nthis merged cluster again with two new centers provided it improves the\n$k$-means objective. This novel heuristic can improve Hartigan's $k$-means when\nit has converged to a local minimum. We show empirically that this\nmerge-and-split $k$-means improves over the Hartigan's heuristic which is the\n{\\em de facto} method of choice. Finally, we propose the $(k,l)$-means\nobjective that generalizes the $k$-means objective by associating the data\npoints to their $l$ closest cluster centers, and show how to either directly\nconvert or iteratively relax the $(k,l)$-means into a $k$-means in order to\nreach better local minima.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 02:34:34 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Nielsen", "Frank", ""], ["Nock", "Richard", ""]]}, {"id": "1406.6449", "submitter": "Kezun Zhang", "authors": "Kezun Zhang and Yanghua Xiao and Hanghang Tong and Haixun Wang and Wei\n  Wang", "title": "The Links Have It: Infobox Generation by Summarization over Linked\n  Entities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Online encyclopedia such as Wikipedia has become one of the best sources of\nknowledge. Much effort has been devoted to expanding and enriching the\nstructured data by automatic information extraction from unstructured text in\nWikipedia. Although remarkable progresses have been made, their effectiveness\nand efficiency is still limited as they try to tackle an extremely difficult\nnatural language understanding problems and heavily relies on supervised\nlearning approaches which require large amount effort to label the training\ndata. In this paper, instead of performing information extraction over\nunstructured natural language text directly, we focus on a rich set of\nsemi-structured data in Wikipedia articles: linked entities. The idea of this\npaper is the following: If we can summarize the relationship between the entity\nand its linked entities, we immediately harvest some of the most important\ninformation about the entity. To this end, we propose a novel rank aggregation\napproach to remove noise, an effective clustering and labeling algorithm to\nextract knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jun 2014 03:33:17 GMT"}], "update_date": "2014-06-26", "authors_parsed": [["Zhang", "Kezun", ""], ["Xiao", "Yanghua", ""], ["Tong", "Hanghang", ""], ["Wang", "Haixun", ""], ["Wang", "Wei", ""]]}, {"id": "1406.6840", "submitter": "Hardik Joshi Mr.", "authors": "Hardik Joshi", "title": "From Citation count to Argumentation count: a new metric to indicate the\n  usefulness of an article", "comments": "Technical Conference cum Workshop on Digital Library Using DSpace\n  hosted by Gujarat National Law University on 21-23 March, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation count is a quantifiable measure to indicate the number of times an\narticle is cited by other articles. It is believed that if an article is cited\noften then it must be an important or influential article; however, there is no\nguarantee that the most cited articles are good in quality. In this paper, the\nauthor suggests argumentation count, a new metric for citation analysis. The\nproposed metric, argumentation count is a triplet of quantities for each\nconcept of an article that helps in providing a quantifiable measure about the\nusefulness of an article.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 11:01:11 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Joshi", "Hardik", ""]]}, {"id": "1406.7093", "submitter": "Yinglong Ma", "authors": "Yinglong Ma and Moyi Shi", "title": "Using multi-categorization semantic analysis and personalization for\n  semantic search", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": "SCCE-002", "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic search technology has received more attention in the last years.\nCompared with the keyword based search, semantic search is used to excavate the\nlatent semantics information and help users find the information items that\nthey want indeed. In this paper, we present a novel approach for semantic\nsearch which combines Multi-Categorization Semantic Analysis with\npersonalization technology. The MCSA approach can classify documents into\nmultiple categories, which is distinct from the existing approaches of\nclassifying documents into a single category. Then, the search history and\npersonal information for users are significantly considered in analysing and\nmatching the original search result by Term Vector DataBase. A series of\npersonalization algorithms are proposed to match personal information and\nsearch history. At last, the related experiments are made to validate the\neffectiveness and efficiency of our method. The experimental results show that\nour method based on MCSA and personalization outperforms some existing methods\nwith the higher search accuracy and the lower extra time cost.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jun 2014 07:43:38 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Ma", "Yinglong", ""], ["Shi", "Moyi", ""]]}, {"id": "1406.7727", "submitter": "Emanuel Laci\\'c", "authors": "Emanuel Lacic, Dominik Kowald, Paul Seitlinger, Christoph Trattner,\n  Denis Parra", "title": "Recommending Items in Social Tagging Systems Using Tag and Time\n  Information", "comments": "6 pages, 2 tables, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a novel item recommendation approach that aims at\nimproving Collaborative Filtering (CF) in social tagging systems using the\ninformation about tags and time. Our algorithm follows a two-step approach,\nwhere in the first step a potentially interesting candidate item-set is found\nusing user-based CF and in the second step this candidate item-set is ranked\nusing item-based CF. Within this ranking step we integrate the information of\ntag usage and time using the Base-Level Learning (BLL) equation coming from\nhuman memory theory that is used to determine the reuse-probability of words\nand tags using a power-law forgetting function.\n  As the results of our extensive evaluation conducted on data-sets gathered\nfrom three social tagging systems (BibSonomy, CiteULike and MovieLens) show,\nthe usage of tag-based and time information via the BLL equation also helps to\nimprove the ranking and recommendation process of items and thus, can be used\nto realize an effective item recommender that outperforms two alternative\nalgorithms which also exploit time and tag-based information.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 13:29:33 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Lacic", "Emanuel", ""], ["Kowald", "Dominik", ""], ["Seitlinger", "Paul", ""], ["Trattner", "Christoph", ""], ["Parra", "Denis", ""]]}, {"id": "1406.7749", "submitter": "Walter Lasecki", "authors": "Richard Absalom, Marcus Luczak-Rosch, Dap Hartmann, and Aske Plaat", "title": "Crowd-Sourcing Fuzzy and Faceted Classification for Concept Search", "comments": null, "journal-ref": null, "doi": null, "report-no": "ci-2014/82", "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for concepts in science and technology is often a difficult task.\nTo facilitate concept search, different types of human-generated metadata have\nbeen created to define the content of scientific and technical disclosures.\nClassification schemes such as the International Patent Classification (IPC)\nand MEDLINE's MeSH are structured and controlled, but require trained experts\nand central management to restrict ambiguity (Mork, 2013). While unstructured\ntags of folksonomies can be processed to produce a degree of structure\n(Kalendar, 2010; Karampinas, 2012; Sarasua, 2012; Bragg, 2013) the freedom\nenjoyed by the crowd typically results in less precision (Stock 2007).\n  Existing classification schemes suffer from inflexibility and ambiguity.\nSince humans understand language, inference, implication, abstraction and hence\nconcepts better than computers, we propose to harness the collective wisdom of\nthe crowd. To do so, we propose a novel classification scheme that is\nsufficiently intuitive for the crowd to use, yet powerful enough to facilitate\nsearch by analogy, and flexible enough to deal with ambiguity. The system will\nenhance existing classification information. Linking up with the semantic web\nand computer intelligence, a Citizen Science effort (Good, 2013) would support\ninnovation by improving the quality of granted patents, reducing duplicitous\nresearch, and stimulating problem-oriented solution design.\n  A prototype of our design is in preparation. A crowd-sourced fuzzy and\nfaceted classification scheme will allow for better concept search and improved\naccess to prior art in science and technology.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jun 2014 14:18:29 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Absalom", "Richard", ""], ["Luczak-Rosch", "Marcus", ""], ["Hartmann", "Dap", ""], ["Plaat", "Aske", ""]]}]