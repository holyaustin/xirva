[{"id": "1804.00069", "submitter": "Edward Raff", "authors": "Edward Raff, Jared Sylvester, Charles Nicholas", "title": "Engineering a Simplified 0-Bit Consistent Weighted Sampling", "comments": null, "journal-ref": "In Proceedings of the 27th ACM International Conference on\n  Information and Knowledge Management. (2018) 1203-1212", "doi": "10.1145/3269206.3271690", "report-no": null, "categories": "stat.ML cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Min-Hashing approach to sketching has become an important tool in data\nanalysis, information retrial, and classification. To apply it to real-valued\ndatasets, the ICWS algorithm has become a seminal approach that is widely used,\nand provides state-of-the-art performance for this problem space. However, ICWS\nsuffers a computational burden as the sketch size K increases. We develop a new\nSimplified approach to the ICWS algorithm, that enables us to obtain over 20x\nspeedups compared to the standard algorithm. The veracity of our approach is\ndemonstrated empirically on multiple datasets and scenarios, showing that our\nnew Simplified CWS obtains the same quality of results while being an order of\nmagnitude faster.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 22:12:44 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 07:45:44 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Raff", "Edward", ""], ["Sylvester", "Jared", ""], ["Nicholas", "Charles", ""]]}, {"id": "1804.00566", "submitter": "Mohammed AL Zamil Prof.", "authors": "Maher Abdullah and Mohammed GH. I. Al Zamil", "title": "The Effectiveness of Classification on Information Retrieval System\n  (Case Study)", "comments": "the paper consists of 16 pages. It presents an idea that is expected\n  to be expanded in the near future", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large amount of unstructured designed information is difficult to deal with.\nObtaining specific information is a hard mission and takes a lot of time.\nInformation Retrieval System (IR) is a way to solve this kind of problem. IR is\na good mechanism but does not give the perfect solution. Other techniques have\nbeen added to IR to develop the result. One of the techniques is text\nclassification. Text classification task is to assign a document to one or more\ncategory. It could be done manually or algorithmically. Text classification\nenhances the output of this process by reducing the results. This study proved\nthat text classification has a positive influence on Information Retrieval\nSystems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 14:42:42 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Abdullah", "Maher", ""], ["Zamil", "Mohammed GH. I. Al", ""]]}, {"id": "1804.00982", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, John Glover, Afshin Mehrabani, Parsa Ghaffari", "title": "360{\\deg} Stance Detection", "comments": "Proceedings of NAACL-HLT 2018: System Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of fake news and filter bubbles makes it increasingly\ndifficult to form an unbiased, balanced opinion towards a topic. To ameliorate\nthis, we propose 360{\\deg} Stance Detection, a tool that aggregates news with\nmultiple perspectives on a topic. It presents them on a spectrum ranging from\nsupport to opposition, enabling the user to base their opinion on multiple\npieces of diverse evidence.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 14:17:09 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Ruder", "Sebastian", ""], ["Glover", "John", ""], ["Mehrabani", "Afshin", ""], ["Ghaffari", "Parsa", ""]]}, {"id": "1804.01614", "submitter": "Jianbin Qin", "authors": "Jianbin Qin and Chuan Xiao", "title": "Pigeonring: A Principle for Faster Thresholded Similarity Search", "comments": "17 pages, 38 figures. Accepted and published in VLDB 2019. Please\n  cite the VLDB paper: @article{DBLP:journals/pvldb/QinX18, author = {Jianbin\n  Qin and Chuan Xiao}, title = {Pigeonring: {A} Principle for Faster\n  Thresholded Similarity Search}, journal = {{PVLDB}}, year = {2018}, }", "journal-ref": "Proceedings of the VLDB Endowment, vol12, 2018, 1, 28-42", "doi": "10.14778/3275536.3275539", "report-no": null, "categories": "cs.DB cs.DS cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pigeonhole principle states that if $n$ items are contained in $m$ boxes,\nthen at least one box has no more than $n / m$ items. It is utilized to solve\nmany data management problems, especially for thresholded similarity searches.\nDespite many pigeonhole principle-based solutions proposed in the last few\ndecades, the condition stated by the principle is weak. It only constrains the\nnumber of items in a single box. By organizing the boxes in a ring, we propose\na new principle, called the pigeonring principle, which constrains the number\nof items in multiple boxes and yields stronger conditions. To utilize the new\nprinciple, we focus on problems defined in the form of identifying data objects\nwhose similarities or distances to the query is constrained by a threshold.\nMany solutions to these problems utilize the pigeonhole principle to find\ncandidates that satisfy a filtering condition. By the new principle, stronger\nfiltering conditions can be established. We show that the pigeonhole principle\nis a special case of the new principle. This suggests that all the pigeonhole\nprinciple-based solutions are possible to be accelerated by the new principle.\nA universal filtering framework is introduced to encompass the solutions to\nthese problems based on the new principle. Besides, we discuss how to quickly\nfind candidates specified by the new principle. The implementation requires\nonly minor modifications on top of existing pigeonhole principle-based\nalgorithms. Experimental results on real datasets demonstrate the applicability\nof the new principle as well as the superior performance of the algorithms\nbased on the new principle.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 22:01:43 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 21:42:29 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 03:10:24 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Qin", "Jianbin", ""], ["Xiao", "Chuan", ""]]}, {"id": "1804.01963", "submitter": "Emmanuel Dufourq Mr", "authors": "Emmanuel Dufourq, Bruce A. Bassett", "title": "Automated Classification of Text Sentiment", "comments": "In \"2017 Annual Conference of the South African Institute of Computer\n  Scientists and Information\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The ability to identify sentiment in text, referred to as sentiment analysis,\nis one which is natural to adult humans. This task is, however, not one which a\ncomputer can perform by default. Identifying sentiments in an automated,\nalgorithmic manner will be a useful capability for business and research in\ntheir search to understand what consumers think about their products or\nservices and to understand human sociology. Here we propose two new Genetic\nAlgorithms (GAs) for the task of automated text sentiment analysis. The GAs\nlearn whether words occurring in a text corpus are either sentiment or\namplifier words, and their corresponding magnitude. Sentiment words, such as\n'horrible', add linearly to the final sentiment. Amplifier words in contrast,\nwhich are typically adjectives/adverbs like 'very', multiply the sentiment of\nthe following word. This increases, decreases or negates the sentiment of the\nfollowing word. The sentiment of the full text is then the sum of these terms.\nThis approach grows both a sentiment and amplifier dictionary which can be\nreused for other purposes and fed into other machine learning algorithms. We\nreport the results of multiple experiments conducted on large Amazon data sets.\nThe results reveal that our proposed approach was able to outperform several\npublic and/or commercial sentiment analysis algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 17:21:48 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Dufourq", "Emmanuel", ""], ["Bassett", "Bruce A.", ""]]}, {"id": "1804.02158", "submitter": "S Uskudarli", "authors": "A. Y{\\i}ld{\\i}r{\\i}m, S. Uskudarli", "title": "Microblog Topic Identification using Linked Open Data", "comments": null, "journal-ref": "Plos one, 15(8), p.e0236863 (2020)", "doi": "10.1371/journal.pone.0236863", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extensive use of social media for sharing and obtaining information has\nresulted in the development of topic detection models to facilitate the\ncomprehension of the overwhelming amount of short and distributed posts.\nProbabilistic topic models, such as Latent Dirichlet Allocation, and matrix\nfactorization based approaches such as Latent Semantic Analysis and\nNon-negative Matrix Factorization represent topics as sets of terms that are\nuseful for many automated processes. However, the determination of what a topic\nis about is left as a further task. Alternatively, techniques that produce\nsummaries are human comprehensible, but less suitable for automated processing.\nThis work proposes an approach that utilizes Linked Open Data (LOD) resources\nto extract semantically represented topics from collections of microposts. The\nproposed approach utilizes entity linking to identify the elements of topics\nfrom microposts. The elements are related through co-occurrence graphs, which\nare processed to yield topics. The topics are represented using an ontology\nthat is introduced for this purpose. A prototype of the approach is used to\nidentify topics from 11 datasets consisting of more than one million posts\ncollected from Twitter during various events, such as the 2016 US election\ndebates and the death of Carrie Fisher. The characteristics of the approach\nwith more than 5 thousand generated topics are described in detail. The\npotentials of semantic topics in revealing information, that is not otherwise\neasily observable, is demonstrated with semantic queries of various\ncomplexities. A human evaluation of topics from 36 randomly selected intervals\nresulted in a precision of 81.0% and F1 score of 93.3%. Furthermore, they are\ncompared with topics generated from the same datasets from an approach that\nproduces human readable topics from microblog post collections.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 07:44:13 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 14:59:33 GMT"}, {"version": "v3", "created": "Sun, 27 Jan 2019 19:55:20 GMT"}, {"version": "v4", "created": "Mon, 21 Sep 2020 15:10:19 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Y\u0131ld\u0131r\u0131m", "A.", ""], ["Uskudarli", "S.", ""]]}, {"id": "1804.02245", "submitter": "Christian Torrero", "authors": "Christian Torrero, Carlo Caprini and Daniele Miorandi", "title": "A Wikipedia-based approach to profiling activities on social media", "comments": "8 pages, 5 figures - Work presented at the WikiWorkshop2018 during\n  the Web Conference 2018 hosted in Lyon (France) from April 23 till April 27", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online user profiling is a very active research field, catalyzing great\ninterest by both scientists and practitioners. In this paper, in particular, we\nlook at approaches able to mine social media activities of users to create a\nrich user profile. We look at the case in which the profiling is meant to\ncharacterize the user's interests along a set of predefined dimensions (that we\nrefer to as categories). A conventional way to do so is to use semantic\nanalysis techniques to (i) extract relevant entities from the online\nconversations of users (ii) mapping said entities to the predefined categories\nof interest. While entity extraction is a well-understood topic, the mapping\npart lacks a reference standardized approach. In this paper we propose using\ngraph navigation techniques on the Wikipedia tree to achieve such a mapping. A\nprototypical implementation is presented and some preliminary results are\nreported.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 13:06:13 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 08:59:23 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 16:34:40 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Torrero", "Christian", ""], ["Caprini", "Carlo", ""], ["Miorandi", "Daniele", ""]]}, {"id": "1804.02525", "submitter": "Dario Pavllo", "authors": "Dario Pavllo, Tiziano Piccardi, Robert West", "title": "Quootstrap: Scalable Unsupervised Extraction of Quotation-Speaker Pairs\n  from Large News Corpora via Bootstrapping", "comments": "Accepted at the 12th International Conference on Web and Social Media\n  (ICWSM), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Quootstrap, a method for extracting quotations, as well as the\nnames of the speakers who uttered them, from large news corpora. Whereas prior\nwork has addressed this problem primarily with supervised machine learning, our\napproach follows a fully unsupervised bootstrapping paradigm. It leverages the\nredundancy present in large news corpora, more precisely, the fact that the\nsame quotation often appears across multiple news articles in slightly\ndifferent contexts. Starting from a few seed patterns, such as [\"Q\", said S.],\nour method extracts a set of quotation-speaker pairs (Q, S), which are in turn\nused for discovering new patterns expressing the same quotations; the process\nis then repeated with the larger pattern set. Our algorithm is highly scalable,\nwhich we demonstrate by running it on the large ICWSM 2011 Spinn3r corpus.\nValidating our results against a crowdsourced ground truth, we obtain 90%\nprecision at 40% recall using a single seed pattern, with significantly higher\nrecall values for more frequently reported (and thus likely more interesting)\nquotations. Finally, we showcase the usefulness of our algorithm's output for\ncomputational social science by analyzing the sentiment expressed in our\nextracted quotations.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 07:50:50 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Pavllo", "Dario", ""], ["Piccardi", "Tiziano", ""], ["West", "Robert", ""]]}, {"id": "1804.02734", "submitter": "Keyang Xu", "authors": "Keyang Xu, Kyle Yingkai Gao, Jamie Callan", "title": "A Structure-Oriented Unsupervised Crawling Strategy for Social Media\n  Sites", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing techniques for efficiently crawling social media sites rely on URL\npatterns, query logs, and human supervision. This paper describes SOUrCe, a\nstructure-oriented unsupervised crawler that uses page structures to learn how\nto crawl a social media site efficiently. SOUrCe consists of two stages. During\nits unsupervised learning phase, SOUrCe constructs a sitemap that clusters\npages based on their structural similarity and generates a navigation table\nthat describes how the different types of pages in the site are linked\ntogether. During its harvesting phase, it uses the navigation table and a\ncrawling policy to guide the choice of which links to crawl next. Experiments\nshow that this architecture supports different styles of crawling efficiently,\nand does a better job of staying focused on user-created contents than baseline\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 18:25:48 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Xu", "Keyang", ""], ["Gao", "Kyle Yingkai", ""], ["Callan", "Jamie", ""]]}, {"id": "1804.02956", "submitter": "Jane Hayes", "authors": "Clinton Woodson, Jane Huffman Hayes, Sarah Griffioen", "title": "Towards Reproducible Research: Automatic Classification of Empirical\n  Requirements Engineering Papers", "comments": "This work was supported in part by NSF grant CCF-1511117; 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research must be reproducible in order to make an impact on science and to\ncontribute to the body of knowledge in our field. Yet studies have shown that\n70% of research from academic labs cannot be reproduced. In software\nengineering, and more specifically requirements engineering (RE), reproducible\nresearch is rare, with datasets not always available or methods not fully\ndescribed. This lack of reproducible research hinders progress, with\nresearchers having to replicate an experiment from scratch. A researcher\nstarting out in RE has to sift through conference papers, finding ones that are\nempirical, then must look through the data available from the empirical paper\n(if any) to make a preliminary determination if the paper can be reproduced.\nThis paper addresses two parts of that problem, identifying RE papers and\nidentifying empirical papers within the RE papers. Recent RE and empirical\nconference papers were used to learn features and to build an automatic\nclassifier to identify RE and empirical papers. We introduce the Empirical\nRequirements Research Classifier (ERRC) method, which uses natural language\nprocessing and machine learning to perform supervised classification of\nconference papers. We compare our method to a baseline keyword-based approach.\nTo evaluate our approach, we examine sets of papers from the IEEE Requirements\nEngineering conference and the IEEE International Symposium on Software Testing\nand Analysis. We found that the ERRC method performed better than the baseline\nmethod in all but a few cases.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 13:09:16 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Woodson", "Clinton", ""], ["Hayes", "Jane Huffman", ""], ["Griffioen", "Sarah", ""]]}, {"id": "1804.03032", "submitter": "Wan-Lei Zhao", "authors": "Wan-Lei Zhao, Hui Wang, Chong-Wah Ngo", "title": "Approximate k-NN Graph Construction: a Generic Online Approach", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor search and k-nearest neighbor graph construction are two\nfundamental issues arise from many disciplines such as multimedia information\nretrieval, data-mining and machine learning. They become more and more imminent\ngiven the big data emerge in various fields in recent years. In this paper, a\nsimple but effective solution both for approximate k-nearest neighbor search\nand approximate k-nearest neighbor graph construction is presented. These two\nissues are addressed jointly in our solution. On the one hand, the approximate\nk-nearest neighbor graph construction is treated as a search task. Each sample\nalong with its k-nearest neighbors are joined into the k-nearest neighbor graph\nby performing the nearest neighbor search sequentially on the graph under\nconstruction. On the other hand, the built k-nearest neighbor graph is used to\nsupport k-nearest neighbor search. Since the graph is built online, the dynamic\nupdate on the graph, which is not possible from most of the existing solutions,\nis supported. This solution is feasible for various distance measures. Its\neffectiveness both as k-nearest neighbor construction and k-nearest neighbor\nsearch approaches is verified across different types of data in different\nscales, various dimensions and under different metrics.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 14:49:19 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 12:25:05 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 11:21:12 GMT"}, {"version": "v4", "created": "Wed, 21 Aug 2019 00:17:09 GMT"}, {"version": "v5", "created": "Thu, 17 Sep 2020 06:08:09 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Zhao", "Wan-Lei", ""], ["Wang", "Hui", ""], ["Ngo", "Chong-Wah", ""]]}, {"id": "1804.03137", "submitter": "Takumi Ichimura", "authors": "Shin Kamada, Takumi Ichimura, Takanobu Watanabe", "title": "Recommendation System of Grants-in-Aid for Researchers by using JSPS\n  Keyword", "comments": "6 pages, 3 figures, Proc. of IEEE 8th International Workshop on\n  Computational Intelligence and Applications (IWCIA2015)", "journal-ref": null, "doi": "10.1109/IWCIA.2015.7449479", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An acquisition of a research grant is important for the researchers to\nconduct a research. The university will build up the organization and reinforce\nthe acquirement of external funds. The researcher becomes aware of grant\ninformation and should investigate what kinds of grant it is. Therefore, the\nstaff at the support center for the Industry-Academia collaboration will\nclassify the grant into some categories according to the research fields.\nHowever, the task is difficult to realize the matching of the research fields,\nbecause the expert knowledge is required to completely classify them. We have\ndeveloped recommendation system of Grant-in-Aid system for researchers by using\nJSPS (Japan Society for the Promotion of Science) keywords. The characteristic\nkeywords are extracted from web sites and then the association rules between\nresearchers and grants are determined in the IF-THEN rule format. This paper\ndiscusses the experimental results by using the developed system.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 05:34:04 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Kamada", "Shin", ""], ["Ichimura", "Takumi", ""], ["Watanabe", "Takanobu", ""]]}, {"id": "1804.03396", "submitter": "Lin Qiu", "authors": "Lin Qiu, Hao Zhou, Yanru Qu, Weinan Zhang, Suoheng Li, Shu Rong,\n  Dongyu Ru, Lihua Qian, Kewei Tu and Yong Yu", "title": "QA4IE: A Question Answering based Framework for Information Extraction", "comments": "Accepted by 17th International Semantic Web Conference (ISWC'18)", "journal-ref": null, "doi": "10.1007/978-3-030-00671-6_12", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Extraction (IE) refers to automatically extracting structured\nrelation tuples from unstructured texts. Common IE solutions, including\nRelation Extraction (RE) and open IE systems, can hardly handle cross-sentence\ntuples, and are severely restricted by limited relation types as well as\ninformal relation specifications (e.g., free-text based relation tuples). In\norder to overcome these weaknesses, we propose a novel IE framework named\nQA4IE, which leverages the flexible question answering (QA) approaches to\nproduce high quality relation triples across sentences. Based on the framework,\nwe develop a large IE benchmark with high quality human evaluation. This\nbenchmark contains 293K documents, 2M golden relation triples, and 636 relation\ntypes. We compare our system with some IE baselines on our benchmark and the\nresults show that our system achieves great improvements.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 08:31:03 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 12:38:21 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Qiu", "Lin", ""], ["Zhou", "Hao", ""], ["Qu", "Yanru", ""], ["Zhang", "Weinan", ""], ["Li", "Suoheng", ""], ["Rong", "Shu", ""], ["Ru", "Dongyu", ""], ["Qian", "Lihua", ""], ["Tu", "Kewei", ""], ["Yu", "Yong", ""]]}, {"id": "1804.03540", "submitter": "Arkaitz Zubiaga", "authors": "Arkaitz Zubiaga", "title": "Mining Social Media for Newsgathering: A Review", "comments": "Accepted for publication in Online Social Networks and Media", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media is becoming an increasingly important data source for learning\nabout breaking news and for following the latest developments of ongoing news.\nThis is in part possible thanks to the existence of mobile devices, which\nallows anyone with access to the Internet to post updates from anywhere,\nleading in turn to a growing presence of citizen journalism. Consequently,\nsocial media has become a go-to resource for journalists during the process of\nnewsgathering. Use of social media for newsgathering is however challenging,\nand suitable tools are needed in order to facilitate access to useful\ninformation for reporting. In this paper, we provide an overview of research in\ndata mining and natural language processing for mining social media for\nnewsgathering. We discuss five different areas that researchers have worked on\nto mitigate the challenges inherent to social media newsgathering: news\ndiscovery, curation of news, validation and verification of content,\nnewsgathering dashboards, and other tasks. We outline the progress made so far\nin the field, summarise the current challenges as well as discuss future\ndirections in the use of computational journalism to assist with social media\nnewsgathering. This review is relevant to computer scientists researching news\nin social media as well as for interdisciplinary researchers interested in the\nintersection of computer science and journalism.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 13:54:05 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 14:22:53 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Zubiaga", "Arkaitz", ""]]}, {"id": "1804.03580", "submitter": "Marco Ponza", "authors": "Marco Ponza, Paolo Ferragina, Francesco Piccinno", "title": "SWAT: A System for Detecting Salient Wikipedia Entities in Texts", "comments": null, "journal-ref": "Computational Intelligence, Wiley-Blackwell Publishing (2019)", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of entity salience by proposing the design and\nimplementation of SWAT, a system that identifies the salient Wikipedia entities\noccurring in an input document. SWAT consists of several modules that are able\nto detect and classify on-the-fly Wikipedia entities as salient or not, based\non a large number of syntactic, semantic and latent features properly extracted\nvia a supervised process which has been trained over millions of examples drawn\nfrom the New York Times corpus. The validation process is performed through a\nlarge experimental assessment, eventually showing that SWAT improves known\nsolutions over all publicly available datasets. We release SWAT via an API that\nwe describe and comment in the paper in order to ease its use in other\nsoftware.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 15:05:49 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 12:25:36 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Ponza", "Marco", ""], ["Ferragina", "Paolo", ""], ["Piccinno", "Francesco", ""]]}, {"id": "1804.03608", "submitter": "Tanmay Gupta", "authors": "Tanmay Gupta, Dustin Schwenk, Ali Farhadi, Derek Hoiem, and Aniruddha\n  Kembhavi", "title": "Imagine This! Scripts to Compositions to Videos", "comments": "Supplementary material included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagining a scene described in natural language with realistic layout and\nappearance of entities is the ultimate test of spatial, visual, and semantic\nworld knowledge. Towards this goal, we present the Composition, Retrieval, and\nFusion Network (CRAFT), a model capable of learning this knowledge from\nvideo-caption data and applying it while generating videos from novel captions.\nCRAFT explicitly predicts a temporal-layout of mentioned entities (characters\nand objects), retrieves spatio-temporal entity segments from a video database\nand fuses them to generate scene videos. Our contributions include sequential\ntraining of components of CRAFT while jointly modeling layout and appearances,\nand losses that encourage learning compositional representations for retrieval.\nWe evaluate CRAFT on semantic fidelity to caption, composition consistency, and\nvisual quality. CRAFT outperforms direct pixel generation approaches and\ngeneralizes well to unseen captions and to unseen video databases with no text\nannotations. We demonstrate CRAFT on FLINTSTONES, a new richly annotated\nvideo-caption dataset with over 25000 videos. For a glimpse of videos generated\nby CRAFT, see https://youtu.be/688Vv86n0z8.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 15:59:45 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Gupta", "Tanmay", ""], ["Schwenk", "Dustin", ""], ["Farhadi", "Ali", ""], ["Hoiem", "Derek", ""], ["Kembhavi", "Aniruddha", ""]]}, {"id": "1804.03713", "submitter": "Philipp Mayr", "authors": "Philipp Mayr, Ingo Frommholz, Guillaume Cabanac", "title": "Report on the 7th International Workshop on Bibliometric-enhanced\n  Information Retrieval (BIR 2018)", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bibliometric-enhanced Information Retrieval (BIR) workshop series has\nstarted at ECIR in 2014 and serves as the annual gathering of IR researchers\nwho address various information-related tasks on scientific corpora and\nbibliometrics. We welcome contributions elaborating on dedicated IR systems, as\nwell as studies revealing original characteristics on how scientific knowledge\nis created, communicated, and used. This report presents all accepted papers at\nthe 7th BIR workshop at ECIR 2018 in Grenoble, France.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 20:47:22 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Mayr", "Philipp", ""], ["Frommholz", "Ingo", ""], ["Cabanac", "Guillaume", ""]]}, {"id": "1804.03979", "submitter": "Silvia Biasotti", "authors": "Silvia Biasotti and Elia Moscoso Thompson and Michela Spagnuolo", "title": "Experimental similarity assessment for a collection of fragmented\n  artifacts", "comments": "Eurographics Workshop on 3D Object Retrieval 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Visual Heritage domain, search engines are expected to support\narchaeologists and curators to address cross-correlation and searching across\nmultiple collections. Archaeological excavations return artifacts that often\nare damaged with parts that are fragmented in more pieces or totally missing.\nThe notion of similarity among fragments cannot simply base on the geometric\nshape but style, material, color, decorations, etc. are all important factors\nthat concur to this concept. In this work, we discuss to which extent the\nexisting techniques for 3D similarity matching are able to approach fragment\nsimilarity, what is missing and what is necessary to be further developed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 13:42:55 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Biasotti", "Silvia", ""], ["Thompson", "Elia Moscoso", ""], ["Spagnuolo", "Michela", ""]]}, {"id": "1804.04191", "submitter": "Tao Ding", "authors": "Shimei Pan, Tao Ding", "title": "Automatically Infer Human Traits and Behavior from Social Media Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the complexity of human minds and their behavioral flexibility, it\nrequires sophisticated data analysis to sift through a large amount of human\nbehavioral evidence to model human minds and to predict human behavior. People\ncurrently spend a significant amount of time on social media such as Twitter\nand Facebook. Thus many aspects of their lives and behaviors have been\ndigitally captured and continuously archived on these platforms. This makes\nsocial media a great source of large, rich and diverse human behavioral\nevidence. In this paper, we survey the recent work on applying machine learning\nto infer human traits and behavior from social media data. We will also point\nout several future research directions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 19:59:44 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Pan", "Shimei", ""], ["Ding", "Tao", ""]]}, {"id": "1804.04205", "submitter": "Ziyi Zhao", "authors": "Ziyi Zhao, Krittaphat Pugdeethosapol, Sheng Lin, Zhe Li, Caiwen Ding,\n  Yanzhi Wang, Qinru Qiu", "title": "Learning Topics using Semantic Locality", "comments": "International Conference of Pattern Recognition (ICPR) in 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topic modeling discovers the latent topic probability of the given text\ndocuments. To generate the more meaningful topic that better represents the\ngiven document, we proposed a new feature extraction technique which can be\nused in the data preprocessing stage. The method consists of three steps.\nFirst, it generates the word/word-pair from every single document. Second, it\napplies a two-way TF-IDF algorithm to word/word-pair for semantic filtering.\nThird, it uses the K-means algorithm to merge the word pairs that have the\nsimilar semantic meaning.\n  Experiments are carried out on the Open Movie Database (OMDb), Reuters\nDataset and 20NewsGroup Dataset. The mean Average Precision score is used as\nthe evaluation metric. Comparing our results with other state-of-the-art topic\nmodels, such as Latent Dirichlet allocation and traditional Restricted\nBoltzmann Machines. Our proposed data preprocessing can improve the generated\ntopic accuracy by up to 12.99\\%.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 20:23:23 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Zhao", "Ziyi", ""], ["Pugdeethosapol", "Krittaphat", ""], ["Lin", "Sheng", ""], ["Li", "Zhe", ""], ["Ding", "Caiwen", ""], ["Wang", "Yanzhi", ""], ["Qiu", "Qinru", ""]]}, {"id": "1804.04212", "submitter": "Hugo Caselles-Dupr\\'e", "authors": "Hugo Caselles-Dupr\\'e, Florian Lesaint, Jimena Royo-Letelier", "title": "Word2Vec applied to Recommendation: Hyperparameters Matter", "comments": "This paper is published on the 12th ACM Conference on Recommender\n  Systems, Vancouver, Canada, 2nd-7th October 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skip-gram with negative sampling, a popular variant of Word2vec originally\ndesigned and tuned to create word embeddings for Natural Language Processing,\nhas been used to create item embeddings with successful applications in\nrecommendation. While these fields do not share the same type of data, neither\nevaluate on the same tasks, recommendation applications tend to use the same\nalready tuned hyperparameters values, even if optimal hyperparameters values\nare often known to be data and task dependent. We thus investigate the marginal\nimportance of each hyperparameter in a recommendation setting through large\nhyperparameter grid searches on various datasets. Results reveal that\noptimizing neglected hyperparameters, namely negative sampling distribution,\nnumber of epochs, subsampling parameter and window-size, significantly improves\nperformance on a recommendation task, and can increase it by an order of\nmagnitude. Importantly, we find that optimal hyperparameters configurations for\nNatural Language Processing tasks and Recommendation tasks are noticeably\ndifferent.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 20:37:35 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 19:30:18 GMT"}, {"version": "v3", "created": "Wed, 29 Aug 2018 15:16:08 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Caselles-Dupr\u00e9", "Hugo", ""], ["Lesaint", "Florian", ""], ["Royo-Letelier", "Jimena", ""]]}, {"id": "1804.04264", "submitter": "Phu Mon Htut", "authors": "Phu Mon Htut, Samuel R. Bowman, Kyunghyun Cho", "title": "Training a Ranking Function for Open-Domain Question Answering", "comments": "To appear at NAACL-SRW 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there have been amazing advances in deep learning methods\nfor machine reading. In machine reading, the machine reader has to extract the\nanswer from the given ground truth paragraph. Recently, the state-of-the-art\nmachine reading models achieve human level performance in SQuAD which is a\nreading comprehension-style question answering (QA) task. The success of\nmachine reading has inspired researchers to combine information retrieval with\nmachine reading to tackle open-domain QA. However, these systems perform poorly\ncompared to reading comprehension-style QA because it is difficult to retrieve\nthe pieces of paragraphs that contain the answer to the question. In this\nstudy, we propose two neural network rankers that assign scores to different\npassages based on their likelihood of containing the answer to a given\nquestion. Additionally, we analyze the relative importance of semantic\nsimilarity and word level relevance matching in open-domain QA.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 00:25:45 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Htut", "Phu Mon", ""], ["Bowman", "Samuel R.", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1804.04266", "submitter": "Dai Quoc Nguyen", "authors": "Dai Quoc Nguyen, Thanh Vu, Tu Dinh Nguyen and Dinh Phung", "title": "A Capsule Network-based Embedding Model for Search Personalization", "comments": "A updated version is available at: arXiv:1808.04122", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search personalization aims to tailor search results to each specific user\nbased on the user's personal interests and preferences (i.e., the user\nprofile). Recent research approaches to search personalization by modelling the\npotential 3-way relationship between the submitted query, the user and the\nsearch results (i.e., documents). That relationship is then used to personalize\nthe search results to that user. In this paper, we introduce a novel embedding\nmodel based on capsule network, which recently is a breakthrough in deep\nlearning, to model the 3-way relationships for search personalization. In the\nmodel, each user (submitted query or returned document) is embedded by a vector\nin the same vector space. The 3-way relationship is described as a triple of\n(query, user, document) which is then modeled as a 3-column matrix containing\nthe three embedding vectors. After that, the 3-column matrix is fed into a deep\nlearning architecture to re-rank the search results returned by a basis ranker.\nExperimental results on query logs from a commercial web search engine show\nthat our model achieves better performances than the basis ranker as well as\nstrong search personalization baselines.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 00:36:53 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 12:05:45 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Nguyen", "Dai Quoc", ""], ["Vu", "Thanh", ""], ["Nguyen", "Tu Dinh", ""], ["Phung", "Dinh", ""]]}, {"id": "1804.04327", "submitter": "Lucas Vinh Tran", "authors": "Lucas Vinh Tran, Tuan-Anh Nguyen Pham, Yi Tay, Yiding Liu, Gao Cong,\n  Xiaoli Li", "title": "Interact and Decide: Medley of Sub-Attention Networks for Effective\n  Group Recommendation", "comments": "Accepted at SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Medley of Sub-Attention Networks (MoSAN), a new novel\nneural architecture for the group recommendation task. Group-level\nrecommendation is known to be a challenging task, in which intricate group\ndynamics have to be considered. As such, this is to be contrasted with the\nstandard recommendation problem where recommendations are personalized with\nrespect to a single user. Our proposed approach hinges upon the key intuition\nthat the decision making process (in groups) is generally dynamic, i.e., a\nuser's decision is highly dependent on the other group members. All in all, our\nkey motivation manifests in a form of an attentive neural model that captures\nfine-grained interactions between group members. In our MoSAN model, each\nsub-attention module is representative of a single member, which models a\nuser's preference with respect to all other group members. Subsequently, a\nMedley of Sub-Attention modules is then used to collectively make the group's\nfinal decision. Overall, our proposed model is both expressive and effective.\nVia a series of extensive experiments, we show that MoSAN not only achieves\nstate-of-the-art performance but also improves standard baselines by a\nconsiderable margin.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 05:54:13 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 09:45:35 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 03:15:39 GMT"}, {"version": "v4", "created": "Sat, 23 Nov 2019 05:12:52 GMT"}, {"version": "v5", "created": "Thu, 28 Nov 2019 10:08:23 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Tran", "Lucas Vinh", ""], ["Pham", "Tuan-Anh Nguyen", ""], ["Tay", "Yi", ""], ["Liu", "Yiding", ""], ["Cong", "Gao", ""], ["Li", "Xiaoli", ""]]}, {"id": "1804.04343", "submitter": "Amit Saha", "authors": "Ramdoot Pydipaty and Amit Saha", "title": "On Using Non-Volatile Memory in Apache Lucene", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apache Lucene is a widely popular information retrieval library used to\nprovide search functionality in an extremely wide variety of applications.\nNaturally, it has to efficiently index and search large number of documents.\nWith non-volatile memory in DIMM form factor (NVDIMM), software now has access\nto durable, byte-addressable memory with write latency within an order of\nmagnitude of DRAM write latency.\n  In this preliminary article, we present the first reported work on the impact\nof using NVDIMM on the performance of committing, searching, and near-real time\nsearching in Apache Lucene. We show modest improvements by using NVM but, our\nempirical study suggests that bigger impact requires redesigning Lucene to\naccess NVM as byte-addressable memory using loads and stores, instead of\naccessing NVM via the file system.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 06:39:28 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Pydipaty", "Ramdoot", ""], ["Saha", "Amit", ""]]}, {"id": "1804.04410", "submitter": "Bhaskar Mitra", "authors": "Corby Rosset, Damien Jose, Gargi Ghosh, Bhaskar Mitra and Saurabh\n  Tiwary", "title": "Optimizing Query Evaluations using Reinforcement Learning for Web Search", "comments": "ACM SIGIR 2018 short paper (pre-print)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In web search, typically a candidate generation step selects a small set of\ndocuments---from collections containing as many as billions of web pages---that\nare subsequently ranked and pruned before being presented to the user. In Bing,\nthe candidate generation involves scanning the index using statically designed\nmatch plans that prescribe sequences of different match criteria and stopping\nconditions. In this work, we pose match planning as a reinforcement learning\ntask and observe up to 20% reduction in index blocks accessed, with small or no\ndegradation in the quality of the candidate sets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 10:22:28 GMT"}, {"version": "v2", "created": "Sat, 18 Aug 2018 13:05:12 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Rosset", "Corby", ""], ["Jose", "Damien", ""], ["Ghosh", "Gargi", ""], ["Mitra", "Bhaskar", ""], ["Tiwary", "Saurabh", ""]]}, {"id": "1804.04475", "submitter": "Arnab Bhattacharya", "authors": "Mitodru Niyogi, Kripabandhu Ghosh, Arnab Bhattacharya", "title": "Learning Multilingual Embeddings for Cross-Lingual Information Retrieval\n  in the Presence of Topically Aligned Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual information retrieval is a challenging task in the absence of\naligned parallel corpora. In this paper, we address this problem by considering\ntopically aligned corpora designed for evaluating an IR setup. To emphasize, we\nneither use any sentence-aligned corpora or document-aligned corpora, nor do we\nuse any language specific resources such as dictionary, thesaurus, or grammar\nrules. Instead, we use an embedding into a common space and learn word\ncorrespondences directly from there. We test our proposed approach for\nbilingual IR on standard FIRE datasets for Bangla, Hindi and English. The\nproposed method is superior to the state-of-the-art method not only for IR\nevaluation measures but also in terms of time requirements. We extend our\nmethod successfully to the trilingual setting.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 12:46:08 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Niyogi", "Mitodru", ""], ["Ghosh", "Kripabandhu", ""], ["Bhattacharya", "Arnab", ""]]}, {"id": "1804.04635", "submitter": "Colin Lockard", "authors": "Colin Lockard, Xin Luna Dong, Arash Einolghozati, Prashant Shiralkar", "title": "CERES: Distantly Supervised Relation Extraction from the Semi-Structured\n  Web", "comments": "Expanded version of paper under review for VLDB", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The web contains countless semi-structured websites, which can be a rich\nsource of information for populating knowledge bases. Existing methods for\nextracting relations from the DOM trees of semi-structured webpages can achieve\nhigh precision and recall only when manual annotations for each website are\navailable. Although there have been efforts to learn extractors from\nautomatically-generated labels, these methods are not sufficiently robust to\nsucceed in settings with complex schemas and information-rich websites.\n  In this paper we present a new method for automatic extraction from\nsemi-structured websites based on distant supervision. We automatically\ngenerate training labels by aligning an existing knowledge base with a web page\nand leveraging the unique structural characteristics of semi-structured\nwebsites. We then train a classifier based on the potentially noisy and\nincomplete labels to predict new relation instances. Our method can compete\nwith annotation-based techniques in the literature in terms of extraction\nquality. A large-scale experiment on over 400,000 pages from dozens of\nmulti-lingual long-tail websites harvested 1.25 million facts at a precision of\n90%.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 17:19:36 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Lockard", "Colin", ""], ["Dong", "Xin Luna", ""], ["Einolghozati", "Arash", ""], ["Shiralkar", "Prashant", ""]]}, {"id": "1804.04760", "submitter": "Joobin Gharibshah", "authors": "Joobin Gharibshah, Evangelos E. Papalexakis, and Michalis Faloutsos", "title": "RIPEx: Extracting malicious IP addresses from security forums using\n  cross-forum learning", "comments": "12 pages, Accepted in n 22nd Pacific-Asia Conference on Knowledge\n  Discovery and Data Mining (PAKDD), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Is it possible to extract malicious IP addresses reported in security forums\nin an automatic way? This is the question at the heart of our work. We focus on\nsecurity forums, where security professionals and hackers share knowledge and\ninformation, and often report misbehaving IP addresses. So far, there have only\nbeen a few efforts to extract information from such security forums. We propose\nRIPEx, a systematic approach to identify and label IP addresses in security\nforums by utilizing a cross-forum learning method. In more detail, the\nchallenge is twofold: (a) identifying IP addresses from other numerical\nentities, such as software version numbers, and (b) classifying the IP address\nas benign or malicious. We propose an integrated solution that tackles both\nthese problems. A novelty of our approach is that it does not require training\ndata for each new forum. Our approach does knowledge transfer across forums: we\nuse a classifier from our source forums to identify seed information for\ntraining a classifier on the target forum. We evaluate our method using data\ncollected from five security forums with a total of 31K users and 542K posts.\nFirst, RIPEx can distinguish IP address from other numeric expressions with 95%\nprecision and above 93% recall on average. Second, RIPEx identifies malicious\nIP addresses with an average precision of 88% and over 78% recall, using our\ncross-forum learning. Our work is a first step towards harnessing the wealth of\nuseful information that can be found in security forums.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 01:08:42 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Gharibshah", "Joobin", ""], ["Papalexakis", "Evangelos E.", ""], ["Faloutsos", "Michalis", ""]]}, {"id": "1804.04918", "submitter": "Chaochao Chen", "authors": "Chaochao Chen, Ziqi Liu, Peilin Zhao, Longfei Li, Jun Zhou, Xiaolong\n  Li", "title": "Distributed Collaborative Hashing and Its Applications in Ant Financial", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering, especially latent factor model, has been popularly\nused in personalized recommendation. Latent factor model aims to learn user and\nitem latent factors from user-item historic behaviors. To apply it into real\nbig data scenarios, efficiency becomes the first concern, including offline\nmodel training efficiency and online recommendation efficiency. In this paper,\nwe propose a Distributed Collaborative Hashing (DCH) model which can\nsignificantly improve both efficiencies. Specifically, we first propose a\ndistributed learning framework, following the state-of-the-art parameter server\nparadigm, to learn the offline collaborative model. Our model can be learnt\nefficiently by distributedly computing subgradients in minibatches on workers\nand updating model parameters on servers asynchronously. We then adopt hashing\ntechnique to speedup the online recommendation procedure. Recommendation can be\nquickly made through exploiting lookup hash tables. We conduct thorough\nexperiments on two real large-scale datasets. The experimental results\ndemonstrate that, comparing with the classic and state-of-the-art (distributed)\nlatent factor models, DCH has comparable performance in terms of recommendation\naccuracy but has both fast convergence speed in offline model training\nprocedure and realtime efficiency in online recommendation procedure.\nFurthermore, the encouraging performance of DCH is also shown for several\nreal-world applications in Ant Financial.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 12:37:51 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 03:21:25 GMT"}, {"version": "v3", "created": "Thu, 31 May 2018 03:52:47 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Chen", "Chaochao", ""], ["Liu", "Ziqi", ""], ["Zhao", "Peilin", ""], ["Li", "Longfei", ""], ["Zhou", "Jun", ""], ["Li", "Xiaolong", ""]]}, {"id": "1804.04946", "submitter": "Takumi Ichimura", "authors": "Takumi Ichimura, Issei Tachibana", "title": "Affective Recommendation System for Tourists by Using Emotion Generating\n  Calculations", "comments": "6 pages, 10 figures. arXiv admin note: substantial text overlap with\n  arXiv:1804.02657 and arXiv:1804.03994", "journal-ref": "Proc. of IEEE 7th International Workshop on Computational\n  Intelligence and Applications (IWCIA2014)", "doi": "10.1109/IWCIA.2014.6987727", "report-no": null, "categories": "cs.HC cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emotion orientated intelligent interface consists of Emotion Generating\nCalculations (EGC) and Mental State Transition Network (MSTN). We have\ndeveloped the Android EGC application software which the agent works to\nevaluate the feelings in the conversation. In this paper, we develop the\ntourist information system which can estimate the user's feelings at the\nsightseeing spot. The system can recommend the sightseeing spot and the local\nfood corresponded to the user's feeling. The system calculates the\nrecommendation list by the estimate function which consists of Google search\nresults, the important degree of a term at the sightseeing website, and the the\naroused emotion by EGC. In order to show the effectiveness, this paper\ndescribes the experimental results for some situations during Hiroshima\nsightseeing.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 04:55:27 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Ichimura", "Takumi", ""], ["Tachibana", "Issei", ""]]}, {"id": "1804.04950", "submitter": "Huifeng Guo", "authors": "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He, and\n  Zhenhua Dong", "title": "DeepFM: An End-to-End Wide & Deep Learning Framework for CTR Prediction", "comments": "14 pages. arXiv admin note: text overlap with arXiv:1703.04247", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning sophisticated feature interactions behind user behaviors is critical\nin maximizing CTR for recommender systems. Despite great progress, existing\nmethods have a strong bias towards low- or high-order interactions, or rely on\nexpertise feature engineering. In this paper, we show that it is possible to\nderive an end-to-end learning model that emphasizes both low- and high-order\nfeature interactions. The proposed framework, DeepFM, combines the power of\nfactorization machines for recommendation and deep learning for feature\nlearning in a new neural network architecture. Compared to the latest Wide &\nDeep model from Google, DeepFM has a shared raw feature input to both its\n\"wide\" and \"deep\" components, with no need of feature engineering besides raw\nfeatures. DeepFM, as a general learning framework, can incorporate various\nnetwork architectures in its deep component. In this paper, we study two\ninstances of DeepFM where its \"deep\" component is DNN and PNN respectively, for\nwhich we denote as DeepFM-D and DeepFM-P. Comprehensive experiments are\nconducted to demonstrate the effectiveness of DeepFM-D and DeepFM-P over the\nexisting models for CTR prediction, on both benchmark data and commercial data.\nWe conduct online A/B test in Huawei App Market, which reveals that DeepFM-D\nleads to more than 10% improvement of click-through rate in the production\nenvironment, compared to a well-engineered LR model. We also covered related\npractice in deploying our framework in Huawei App Market.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 01:12:13 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 13:39:20 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Guo", "Huifeng", ""], ["Tang", "Ruiming", ""], ["Ye", "Yunming", ""], ["Li", "Zhenguo", ""], ["He", "Xiuqiang", ""], ["Dong", "Zhenhua", ""]]}, {"id": "1804.04956", "submitter": "Moritz Schubotz", "authors": "Moritz Schubotz, Andre Greiner-Petter, Philipp Scharpf, Norman\n  Meuschke, Howard Cohl, Bela Gipp", "title": "Improving the Representation and Conversion of Mathematical Formulae by\n  Considering their Textual Context", "comments": "10 pages, 4 figures", "journal-ref": "Proceedings of the ACM/IEEE-CS Joint Conference on Digital\n  Libraries (JCDL), Jun. 2018, Fort Worth, USA", "doi": "10.1145/3197026.3197058", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical formulae represent complex semantic information in a concise\nform. Especially in Science, Technology, Engineering, and Mathematics,\nmathematical formulae are crucial to communicate information, e.g., in\nscientific papers, and to perform computations using computer algebra systems.\nEnabling computers to access the information encoded in mathematical formulae\nrequires machine-readable formats that can represent both the presentation and\ncontent, i.e., the semantics, of formulae. Exchanging such information between\nsystems additionally requires conversion methods for mathematical\nrepresentation formats. We analyze how the semantic enrichment of formulae\nimproves the format conversion process and show that considering the textual\ncontext of formulae reduces the error rate of such conversions. Our main\ncontributions are: (1) providing an openly available benchmark dataset for the\nmathematical format conversion task consisting of a newly created test\ncollection, an extensive, manually curated gold standard and task-specific\nevaluation metrics; (2) performing a quantitative evaluation of\nstate-of-the-art tools for mathematical format conversions; (3) presenting a\nnew approach that considers the textual context of formulae to reduce the error\nrate for mathematical format conversions. Our benchmark dataset facilitates\nfuture research on mathematical format conversions as well as research on many\nproblems in mathematical information retrieval. Because we annotated and linked\nall components of formulae, e.g., identifiers, operators and other entities, to\nWikidata entries, the gold standard can, for instance, be used to train methods\nfor formula concept discovery and recognition. Such methods can then be applied\nto improve mathematical information retrieval systems, e.g., for semantic\nformula search, recommendation of mathematical content, or detection of\nmathematical plagiarism.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 14:08:16 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Schubotz", "Moritz", ""], ["Greiner-Petter", "Andre", ""], ["Scharpf", "Philipp", ""], ["Meuschke", "Norman", ""], ["Cohl", "Howard", ""], ["Gipp", "Bela", ""]]}, {"id": "1804.05090", "submitter": "Shuai Zheng", "authors": "Shuai Zheng, Chris Ding, Feiping Nie", "title": "Regularized Singular Value Decomposition and Application to Recommender\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singular value decomposition (SVD) is the mathematical basis of principal\ncomponent analysis (PCA). Together, SVD and PCA are one of the most widely used\nmathematical formalism/decomposition in machine learning, data mining, pattern\nrecognition, artificial intelligence, computer vision, signal processing, etc.\nIn recent applications, regularization becomes an increasing trend. In this\npaper, we present a regularized SVD (RSVD), present an efficient computational\nalgorithm, and provide several theoretical analysis. We show that although RSVD\nis non-convex, it has a closed-form global optimal solution. Finally, we apply\nRSVD to the application of recommender system and experimental result show that\nRSVD outperforms SVD significantly.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 18:54:30 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Zheng", "Shuai", ""], ["Ding", "Chris", ""], ["Nie", "Feiping", ""]]}, {"id": "1804.05118", "submitter": "Chensheng Wu", "authors": "Chensheng Wu, Robert Lee, Christopher C. Davis", "title": "Object Detection and Geometric Profiling through Dirty Water Media Using\n  Asymmetry Properties of Backscattered Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.IR physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scattering of light observed through the turbid underwater channel is\noften regarded as the leading challenge when designing underwater\nelectro-optical imaging systems. There have been many approaches to address the\neffects of scattering such as using pulsed laser sources to reject scattered\nlight temporally, or using intensity modulated waveforms and matched filters to\nremove the scattered light spectrally. In this paper, a new method is proposed\nwhich primarily uses the backscattering asymmetry property for object detection\nand geometric profiling. In our approach, two parallel and identical continuous\nwave (CW) laser beams with narrow beam widths (~2mm) are used as active\nillumination sources. The two beams also have controllable spacing and aiming\nangle, as well as initial phase difference for convenience of scanning and\nprofiling a target. Through theory and experimental results, it will be shown\nthat when an object leans or tilts towards one of the beam's central\ntrajectory, the asymmetry in the backscattered signals can be used to indicate\nthe location or slope of the target's surface, respectively. By varying the\nspacing or aiming angle of the two beams, a number of surface samples can be\ncollected to reconstruct the object's shape geometrically. The resolution and\nrange limit of our approach are also measured and reported in this work. In\napplication, our proposed method provides an economic solution to perform\nimaging through turbid underwater environments. Additionally, the idea can be\ncombined with the pulsed or modulated laser signals for enhanced imaging\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 21:10:04 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Wu", "Chensheng", ""], ["Lee", "Robert", ""], ["Davis", "Christopher C.", ""]]}, {"id": "1804.05482", "submitter": "Ignacio Ramirez", "authors": "Ignacio Ramirez", "title": "Binary Matrix Factorization via Dictionary Learning", "comments": "submitted for review to IEEE JSTSP on April 15th, 2018", "journal-ref": null, "doi": "10.1109/JSTSP.2018.2875674", "report-no": null, "categories": "stat.ML cs.CV cs.IR cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Matrix factorization is a key tool in data analysis; its applications include\nrecommender systems, correlation analysis, signal processing, among others.\nBinary matrices are a particular case which has received significant attention\nfor over thirty years, especially within the field of data mining. Dictionary\nlearning refers to a family of methods for learning overcomplete basis (also\ncalled frames) in order to efficiently encode samples of a given type; this\narea, now also about twenty years old, was mostly developed within the signal\nprocessing field. In this work we propose two binary matrix factorization\nmethods based on a binary adaptation of the dictionary learning paradigm to\nbinary matrices. The proposed algorithms focus on speed and scalability; they\nwork with binary factors combined with bit-wise operations and a few auxiliary\ninteger ones. Furthermore, the methods are readily applicable to online binary\nmatrix factorization. Another important issue in matrix factorization is the\nchoice of rank for the factors; we address this model selection problem with an\nefficient method based on the Minimum Description Length principle. Our\npreliminary results show that the proposed methods are effective at producing\ninterpretable factorizations of various data types of different nature.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 02:36:24 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 01:13:05 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Ramirez", "Ignacio", ""]]}, {"id": "1804.05514", "submitter": "Mayank Singh", "authors": "Mayank Singh, Pradeep Dogga, Sohan Patro, Dhiraj Barnwal, Ritam Dutt,\n  Rajarshi Haldar, Pawan Goyal and Animesh Mukherjee", "title": "CL Scholar: The ACL Anthology Knowledge Graph Miner", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CL Scholar, the ACL Anthology knowledge graph miner to facilitate\nhigh-quality search and exploration of current research progress in the\ncomputational linguistics community. In contrast to previous works,\nperiodically crawling, indexing and processing of new incoming articles is\ncompletely automated in the current system. CL Scholar utilizes both textual\nand network information for knowledge graph construction. As an additional\nnovel initiative, CL Scholar supports more than 1200 scholarly natural language\nqueries along with standard keyword-based search on constructed knowledge\ngraph. It answers binary, statistical and list based natural language queries.\nThe current system is deployed at http://cnerg.iitkgp.ac.in/aclakg. We also\nprovide REST API support along with bulk download facility. Our code and data\nare available at https://github.com/CLScholar.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 06:15:06 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Singh", "Mayank", ""], ["Dogga", "Pradeep", ""], ["Patro", "Sohan", ""], ["Barnwal", "Dhiraj", ""], ["Dutt", "Ritam", ""], ["Haldar", "Rajarshi", ""], ["Goyal", "Pawan", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "1804.05669", "submitter": "Takumi Ichimura", "authors": "Takumi Ichimura, Shin Kamada", "title": "A Clonal Selection Algorithm with Levenshtein Distance based Image\n  Similarity in Multidimensional Subjective Tourist Information and Discovery\n  of Cryptic Spots by Interactive GHSOM", "comments": "6 pages, 9 figures, Proc. of 2013 IEEE International Conference on\n  Systems, Man, and Cybernetics (IEEE SMC 2013). arXiv admin note: substantial\n  text overlap with arXiv:1804.03993, arXiv:1804.02628, arXiv:1804.02816", "journal-ref": null, "doi": "10.1109/SMC.2013.357", "report-no": null, "categories": "cs.IR cs.CV cs.SI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile Phone based Participatory Sensing (MPPS) system involves a community\nof users sending personal information and participating in autonomous sensing\nthrough their mobile phones. Sensed data can also be obtained from external\nsensing devices that can communicate wirelessly to the phone. Our developed\ntourist subjective data collection system with Android smartphone can determine\nthe filtering rules to provide the important information of sightseeing spot.\nThe rules are automatically generated by Interactive Growing Hierarchical SOM.\nHowever, the filtering rules related to photograph were not generated, because\nthe extraction of the specified characteristics from images cannot be realized.\nWe propose the effective method of the Levenshtein distance to deduce the\nspatial proximity of image viewpoints and thus determine the specified pattern\nin which images should be processed. To verify the proposed method, some\nexperiments to classify the subjective data with images are executed by\nInteractive GHSOM and Clonal Selection Algorithm with Immunological Memory\nCells in this paper.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 08:18:16 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Ichimura", "Takumi", ""], ["Kamada", "Shin", ""]]}, {"id": "1804.05936", "submitter": "Qingyao Ai", "authors": "Qingyao Ai, Keping Bi, Jiafeng Guo, W. Bruce Croft", "title": "Learning a Deep Listwise Context Model for Ranking Refinement", "comments": null, "journal-ref": null, "doi": "10.1145/3209978.3209985", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to rank has been intensively studied and widely applied in\ninformation retrieval. Typically, a global ranking function is learned from a\nset of labeled data, which can achieve good performance on average but may be\nsuboptimal for individual queries by ignoring the fact that relevant documents\nfor different queries may have different distributions in the feature space.\nInspired by the idea of pseudo relevance feedback where top ranked documents,\nwhich we refer as the \\textit{local ranking context}, can provide important\ninformation about the query's characteristics, we propose to use the inherent\nfeature distributions of the top results to learn a Deep Listwise Context Model\nthat helps us fine tune the initial ranked list. Specifically, we employ a\nrecurrent neural network to sequentially encode the top results using their\nfeature vectors, learn a local context model and use it to re-rank the top\nresults. There are three merits with our model: (1) Our model can capture the\nlocal ranking context based on the complex interactions between top results\nusing a deep neural network; (2) Our model can be built upon existing\nlearning-to-rank methods by directly using their extracted feature vectors; (3)\nOur model is trained with an attention-based loss function, which is more\neffective and efficient than many existing listwise methods. Experimental\nresults show that the proposed model can significantly improve the\nstate-of-the-art learning to rank methods on benchmark retrieval corpora.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 20:57:31 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 18:06:21 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Ai", "Qingyao", ""], ["Bi", "Keping", ""], ["Guo", "Jiafeng", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1804.05938", "submitter": "Qingyao Ai", "authors": "Qingyao Ai, Keping Bi, Cheng Luo, Jiafeng Guo, W. Bruce Croft", "title": "Unbiased Learning to Rank with Unbiased Propensity Estimation", "comments": null, "journal-ref": null, "doi": "10.1145/3209978.3209986", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to rank with biased click data is a well-known challenge. A variety\nof methods has been explored to debias click data for learning to rank such as\nclick models, result interleaving and, more recently, the unbiased\nlearning-to-rank framework based on inverse propensity weighting. Despite their\ndifferences, most existing studies separate the estimation of click bias\n(namely the \\textit{propensity model}) from the learning of ranking algorithms.\nTo estimate click propensities, they either conduct online result\nrandomization, which can negatively affect the user experience, or offline\nparameter estimation, which has special requirements for click data and is\noptimized for objectives (e.g. click likelihood) that are not directly related\nto the ranking performance of the system. In this work, we address those\nproblems by unifying the learning of propensity models and ranking models. We\nfind that the problem of estimating a propensity model from click data is a\ndual problem of unbiased learning to rank. Based on this observation, we\npropose a Dual Learning Algorithm (DLA) that jointly learns an unbiased ranker\nand an \\textit{unbiased propensity model}. DLA is an automatic unbiased\nlearning-to-rank framework as it directly learns unbiased ranking models from\nbiased click data without any preprocessing. It can adapt to the change of bias\ndistributions and is applicable to online learning. Our empirical experiments\nwith synthetic and real-world data show that the models trained with DLA\nsignificantly outperformed the unbiased learning-to-rank algorithms based on\nresult randomization and the models trained with relevance signals extracted by\nclick models.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 21:03:07 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 18:09:08 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Ai", "Qingyao", ""], ["Bi", "Keping", ""], ["Luo", "Cheng", ""], ["Guo", "Jiafeng", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1804.05942", "submitter": "Justin Sybrandt", "authors": "Justin Sybrandt, Angelo Carrabba, Alexander Herzog, Ilya Safro", "title": "Are Abstracts Enough for Hypothesis Generation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential for automatic hypothesis generation (HG) systems to improve\nresearch productivity keeps pace with the growing set of publicly available\nscientific information. But as data becomes easier to acquire, we must\nunderstand the effect different textual data sources have on our resulting\nhypotheses. Are abstracts enough for HG, or does it need full-text papers? How\nmany papers does an HG system need to make valuable predictions? How sensitive\nis a general-purpose HG system to hyperparameter values or input quality? What\neffect does corpus size and document length have on HG results? To answer these\nquestions we train multiple versions of knowledge network-based HG system,\nMoliere, on varying corpora in order to compare challenges and trade offs in\nterms of result quality and computational requirements. Moliere generalizes\nmain principles of similar knowledge network-based HG systems and reinforces\nthem with topic modeling components. The corpora include the abstract and\nfull-text versions of PubMed Central, as well as iterative halves of MEDLINE,\nwhich allows us to compare the effect document length and count has on the\nresults. We find that, quantitatively, corpora with a higher median document\nlength result in marginally higher quality results, yet require substantially\nlonger to process. However, qualitatively, full-length papers introduce a\nsignificant number of intruder terms to the resulting topics, which decreases\nhuman interpretability. Additionally, we find that the effect of document\nlength is greater than that of document count, even if both sets contain only\npaper abstracts. Reproducibility: Our code and data are available at\ngithub.com/jsybran/moliere, and bit.ly/2GxghpM respectively.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 17:08:45 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 17:36:53 GMT"}, {"version": "v3", "created": "Sat, 20 Oct 2018 19:57:05 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Sybrandt", "Justin", ""], ["Carrabba", "Angelo", ""], ["Herzog", "Alexander", ""], ["Safro", "Ilya", ""]]}, {"id": "1804.05995", "submitter": "Michele Catasta", "authors": "Tiziano Piccardi, Michele Catasta, Leila Zia, Robert West", "title": "Structuring Wikipedia Articles with Section Recommendations", "comments": "SIGIR '18 camera-ready", "journal-ref": null, "doi": "10.1145/3209978.3209984", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sections are the building blocks of Wikipedia articles. They enhance\nreadability and can be used as a structured entry point for creating and\nexpanding articles. Structuring a new or already existing Wikipedia article\nwith sections is a hard task for humans, especially for newcomers or less\nexperienced editors, as it requires significant knowledge about how a\nwell-written article looks for each possible topic. Inspired by this need, the\npresent paper defines the problem of section recommendation for Wikipedia\narticles and proposes several approaches for tackling it. Our systems can help\neditors by recommending what sections to add to already existing or newly\ncreated Wikipedia articles. Our basic paradigm is to generate recommendations\nby sourcing sections from articles that are similar to the input article. We\nexplore several ways of defining similarity for this purpose (based on topic\nmodeling, collaborative filtering, and Wikipedia's category system). We use\nboth automatic and human evaluation approaches for assessing the performance of\nour recommendation system, concluding that the category-based approach works\nbest, achieving precision@10 of about 80% in the human evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 00:47:41 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 00:30:43 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Piccardi", "Tiziano", ""], ["Catasta", "Michele", ""], ["Zia", "Leila", ""], ["West", "Robert", ""]]}, {"id": "1804.06201", "submitter": "Herbert Hu", "authors": "Guangneng Hu, Yu Zhang, Qiang Yang", "title": "LCMR: Local and Centralized Memories for Collaborative Filtering with\n  Unstructured Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) is the key technique for recommender systems.\nPure CF approaches exploit the user-item interaction data (e.g., clicks, likes,\nand views) only and suffer from the sparsity issue. Items are usually\nassociated with content information such as unstructured text (e.g., abstracts\nof articles and reviews of products). CF can be extended to leverage text. In\nthis paper, we develop a unified neural framework to exploit interaction data\nand content information seamlessly. The proposed framework, called LCMR, is\nbased on memory networks and consists of local and centralized memories for\nexploiting content information and interaction data, respectively. By modeling\ncontent information as local memories, LCMR attentively learns what to exploit\nwith the guidance of user-item interaction. On real-world datasets, LCMR shows\nbetter performance by comparing with various baselines in terms of the hit\nratio and NDCG metrics. We further conduct analyses to understand how local and\ncentralized memories work for the proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 12:32:23 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 16:23:00 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Hu", "Guangneng", ""], ["Zhang", "Yu", ""], ["Yang", "Qiang", ""]]}, {"id": "1804.06426", "submitter": "Philipp Mayr", "authors": "Zeljko Carevic, Sascha Sch\\\"uller, Philipp Mayr, Norbert Fuhr", "title": "Contextualised Browsing in a Digital Library's Living Lab", "comments": "10 pages, 2 figures, paper accepted at JCDL 2018", "journal-ref": null, "doi": "10.1145/3197026.3197054", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextualisation has proven to be effective in tailoring \\linebreak search\nresults towards the users' information need. While this is true for a basic\nquery search, the usage of contextual session information during exploratory\nsearch especially on the level of browsing has so far been underexposed in\nresearch. In this paper, we present two approaches that contextualise browsing\non the level of structured metadata in a Digital Library (DL), (1) one variant\nbases on document similarity and (2) one variant utilises implicit session\ninformation, such as queries and different document metadata encountered during\nthe session of a users. We evaluate our approaches in a living lab environment\nusing a DL in the social sciences and compare our contextualisation approaches\nagainst a non-contextualised approach. For a period of more than three months\nwe analysed 47,444 unique retrieval sessions that contain search activities on\nthe level of browsing. Our results show that a contextualisation of browsing\nsignificantly outperforms our baseline in terms of the position of the first\nclicked item in the result set. The mean rank of the first clicked document\n(measured as mean first relevant - MFR) was 4.52 using a non-contextualised\nranking compared to 3.04 when re-ranking the result lists based on similarity\nto the previously viewed document. Furthermore, we observed that both\ncontextual approaches show a noticeably higher click-through rate. A\ncontextualisation based on document similarity leads to almost twice as many\ndocument views compared to the non-contextualised ranking.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 18:30:29 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Carevic", "Zeljko", ""], ["Sch\u00fcller", "Sascha", ""], ["Mayr", "Philipp", ""], ["Fuhr", "Norbert", ""]]}, {"id": "1804.06769", "submitter": "Herbert Hu", "authors": "Guangneng Hu, Yu Zhang, Qiang Yang", "title": "CoNet: Collaborative Cross Networks for Cross-Domain Recommendation", "comments": "Deep transfer learning for recommender systems", "journal-ref": "CIKM 2018", "doi": "10.1145/3269206.3271684", "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cross-domain recommendation technique is an effective way of alleviating\nthe data sparse issue in recommender systems by leveraging the knowledge from\nrelevant domains. Transfer learning is a class of algorithms underlying these\ntechniques. In this paper, we propose a novel transfer learning approach for\ncross-domain recommendation by using neural networks as the base model. In\ncontrast to the matrix factorization based cross-domain techniques, our method\nis deep transfer learning, which can learn complex user-item interaction\nrelationships. We assume that hidden layers in two base networks are connected\nby cross mappings, leading to the collaborative cross networks (CoNet). CoNet\nenables dual knowledge transfer across domains by introducing cross connections\nfrom one base network to another and vice versa. CoNet is achieved in\nmulti-layer feedforward networks by adding dual connections and joint loss\nfunctions, which can be trained efficiently by back-propagation. The proposed\nmodel is thoroughly evaluated on two large real-world datasets. It outperforms\nbaselines by relative improvements of 7.84\\% in NDCG. We demonstrate the\nnecessity of adaptively selecting representations to transfer. Our model can\nreduce tens of thousands training examples comparing with non-transfer methods\nand still has the competitive performance with them.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 14:48:21 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 17:12:11 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 14:00:05 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Hu", "Guangneng", ""], ["Zhang", "Yu", ""], ["Yang", "Qiang", ""]]}, {"id": "1804.06786", "submitter": "Jack Hessel", "authors": "Jack Hessel, David Mimno, Lillian Lee", "title": "Quantifying the visual concreteness of words and topics in multimodal\n  datasets", "comments": "NAACL HLT 2018, 14 pages, 6 figures, data available at\n  http://www.cs.cornell.edu/~jhessel/concreteness/concreteness.html", "journal-ref": "2018 North American Chapter of the Association for Computational\n  Linguistics: Human Language Technologies (NAACL HLT)", "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal machine learning algorithms aim to learn visual-textual\ncorrespondences. Previous work suggests that concepts with concrete visual\nmanifestations may be easier to learn than concepts with abstract ones. We give\nan algorithm for automatically computing the visual concreteness of words and\ntopics within multimodal datasets. We apply the approach in four settings,\nranging from image captions to images/text scraped from historical books. In\naddition to enabling explorations of concepts in multimodal datasets, our\nconcreteness scores predict the capacity of machine learning algorithms to\nlearn textual/visual relationships. We find that 1) concrete concepts are\nindeed easier to learn; 2) the large number of algorithms we consider have\nsimilar failure cases; 3) the precise positive relationship between\nconcreteness and performance varies between datasets. We conclude with\nrecommendations for using concreteness scores to facilitate future multimodal\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 15:23:04 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 19:15:45 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Hessel", "Jack", ""], ["Mimno", "David", ""], ["Lee", "Lillian", ""]]}, {"id": "1804.06905", "submitter": "Diyah Puspitaningrum", "authors": "Diyah Puspitaningrum, I.S.W.B. Prasetya, P.A. Wicaksono", "title": "Highly Relevant Routing Recommendation Systems for Handling Few Data\n  Using MDL Principle and Embedded Relevance Boosting Factors", "comments": "ACM SIGIR 2018 Workshop on Learning from Limited or Noisy Data for\n  Information Retrieval (LND4IR'18), July 12, 2018, Ann Arbor, Michigan, USA, 8\n  pages, 9 figures", "journal-ref": null, "doi": null, "report-no": "Rep2.13.6.2018", "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A route recommendation system can provide better recommendation if it also\ntakes collected user reviews into account, e.g. places that generally get\npositive reviews may be preferred. However, to classify sentiment, many\nclassification algorithms existing today suffer in handling small data items\nsuch as short written reviews. In this paper we propose a model for a strongly\nrelevant route recommendation system that is based on an MDL-based (Minimum\nDescription Length) sentiment classification and show that such a system is\ncapable of handling small data items (short user reviews). Another highlight of\nthe model is the inclusion of a set of boosting factors in the relevance\ncalculation to improve the relevance in any recommendation system that\nimplements the model.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 20:13:30 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 03:08:43 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Puspitaningrum", "Diyah", ""], ["Prasetya", "I. S. W. B.", ""], ["Wicaksono", "P. A.", ""]]}, {"id": "1804.07000", "submitter": "Marcel Trotzek", "authors": "Marcel Trotzek, Sven Koitka, Christoph M. Friedrich", "title": "Utilizing Neural Networks and Linguistic Metadata for Early Detection of\n  Depression Indications in Text Sequences", "comments": "This work has been submitted to the IEEE and has been accepted for\n  future publication in IEEE Transactions on Knowledge and Data Engineering.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible. 14 pages, 3 figures, 7 tables", "journal-ref": null, "doi": "10.1109/TKDE.2018.2885515", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depression is ranked as the largest contributor to global disability and is\nalso a major reason for suicide. Still, many individuals suffering from forms\nof depression are not treated for various reasons. Previous studies have shown\nthat depression also has an effect on language usage and that many depressed\nindividuals use social media platforms or the internet in general to get\ninformation or discuss their problems. This paper addresses the early detection\nof depression using machine learning models based on messages on a social\nplatform. In particular, a convolutional neural network based on different word\nembeddings is evaluated and compared to a classification based on user-level\nlinguistic metadata. An ensemble of both approaches is shown to achieve\nstate-of-the-art results in a current early detection task. Furthermore, the\ncurrently popular ERDE score as metric for early detection systems is examined\nin detail and its drawbacks in the context of shared tasks are illustrated. A\nslightly modified metric is proposed and compared to the original score.\nFinally, a new word embedding was trained on a large corpus of the same domain\nas the described task and is evaluated as well.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 05:25:51 GMT"}, {"version": "v2", "created": "Sun, 25 Nov 2018 12:44:50 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 08:15:23 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Trotzek", "Marcel", ""], ["Koitka", "Sven", ""], ["Friedrich", "Christoph M.", ""]]}, {"id": "1804.07152", "submitter": "Liu Weiyi", "authors": "Weiyi Liu, Zhining Liu, Toyotaro Suzumura, Guangmin Hu", "title": "Scalable attribute-aware network embedding with locality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adding attributes for nodes to network embedding helps to improve the ability\nof the learned joint representation to depict features from topology and\nattributes simultaneously. Recent research on the joint embedding has exhibited\na promising performance on a variety of tasks by jointly embedding the two\nspaces. However, due to the indispensable requirement of globality based\ninformation, present approaches contain a flaw of in-scalability. Here we\npropose \\emph{SANE}, a scalable attribute-aware network embedding algorithm\nwith locality, to learn the joint representation from topology and attributes.\nBy enforcing the alignment of a local linear relationship between each node and\nits K-nearest neighbors in topology and attribute space, the joint embedding\nrepresentations are more informative comparing with a single representation\nfrom topology or attributes alone. And we argue that the locality in\n\\emph{SANE} is the key to learning the joint representation at scale. By using\nseveral real-world networks from diverse domains, We demonstrate the efficacy\nof \\emph{SANE} in performance and scalability aspect. Overall, for performance\non label classification, SANE successfully reaches up to the highest F1-score\non most datasets, and even closer to the baseline method that needs label\ninformation as extra inputs, compared with other state-of-the-art joint\nrepresentation algorithms. What's more, \\emph{SANE} has an up to 71.4\\%\nperformance gain compared with the single topology-based algorithm. For\nscalability, we have demonstrated the linearly time complexity of \\emph{SANE}.\nIn addition, we intuitively observe that when the network size scales to\n100,000 nodes, the \"learning joint embedding\" step of \\emph{SANE} only takes\n$\\approx10$ seconds.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 23:59:50 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 00:40:08 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Liu", "Weiyi", ""], ["Liu", "Zhining", ""], ["Suzumura", "Toyotaro", ""], ["Hu", "Guangmin", ""]]}, {"id": "1804.07447", "submitter": "Christopher George", "authors": "Christopher A. George, Onur Ozdemir, Connie Fournelle, and Kendra E.\n  Moore", "title": "The Role-Relevance Model for Enhanced Semantic Targeting in Unstructured\n  Text", "comments": "10 pages, 3 figures, 6 tables, presented at SPIE Defense + Commercial\n  Sensing: Next Generation Analyst (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized search provides a potentially powerful tool, however, it is\nlimited due to the large number of roles that a person has: parent, employee,\nconsumer, etc. We present the role-relevance algorithm: a search technique that\nfavors search results relevant to the user's current role. The role-relevance\nalgorithm uses three factors to score documents: (1) the number of keywords\neach document contains; (2) each document's geographic relevance to the user's\nrole (if applicable); and (3) each document's topical relevance to the user's\nrole (if applicable). Topical relevance is assessed using a novel extension to\nLatent Dirichlet Allocation (LDA) that allows standard LDA to score document\nrelevance to user-defined topics. Overall results on a pre-labeled corpus show\nan average improvement in search precision of approximately 20% compared to\nkeyword search alone.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 04:25:18 GMT"}, {"version": "v2", "created": "Sun, 29 Apr 2018 19:25:55 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["George", "Christopher A.", ""], ["Ozdemir", "Onur", ""], ["Fournelle", "Connie", ""], ["Moore", "Kendra E.", ""]]}, {"id": "1804.07525", "submitter": "Jerome Darmont", "authors": "Ciprian-Octavian Truica (UPB), J\\'er\\^ome Darmont (ERIC), Alexandru\n  Boicea (UPB), Florin Radulescu (UPB)", "title": "Benchmarking Top-K Keyword and Top-K Document Processing with\n  T${}^2$K${}^2$ and T${}^2$K${}^2$D${}^2$", "comments": null, "journal-ref": "Future Generation Computer Systems, Elsevier, 2018, 85, pp.60-75.\n  https://www.sciencedirect.com/science/article/pii/S0167739X17323580", "doi": "10.1016/j.future.2018.02.037", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-k keyword and top-k document extraction are very popular text analysis\ntechniques. Top-k keywords and documents are often computed on-the-fly, but\nthey exploit weighted vocabularies that are costly to build. To compare\ncompeting weighting schemes and database implementations, benchmarking is\ncustomary. To the best of our knowledge, no benchmark currently addresses these\nproblems. Hence, in this paper, we present T${}^2$K${}^2$, a top-k keywords and\ndocuments benchmark, and its decision support-oriented evolution\nT${}^2$K${}^2$D${}^2$. Both benchmarks feature a real tweet dataset and queries\nwith various complexities and selectivities. They help evaluate weighting\nschemes and database implementations in terms of computing performance. To\nillustrate our bench-marks' relevance and genericity, we successfully ran\nperformance tests on the TF-IDF and Okapi BM25 weighting schemes, on one hand,\nand on different relational (Oracle, PostgreSQL) and document-oriented\n(MongoDB) database implementations, on the other hand.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 10:01:43 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Truica", "Ciprian-Octavian", "", "UPB"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"], ["Boicea", "Alexandru", "", "UPB"], ["Radulescu", "Florin", "", "UPB"]]}, {"id": "1804.07583", "submitter": "Besnik Fetahu", "authors": "Besnik Fetahu", "title": "Approaches for Enriching and Improving Textual Knowledge Bases", "comments": "PhD thesis, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifiability is one of the core editing principles in Wikipedia, where\neditors are encouraged to provide citations for the added statements.\nStatements can be any arbitrary piece of text, ranging from a sentence up to a\nparagraph. However, in many cases, citations are either outdated, missing, or\nlink to non-existing references (e.g. dead URL, moved content etc.). In total,\n20\\% of the cases such citations refer to news articles and represent the\nsecond most cited source. Even in cases where citations are provided, there are\nno explicit indicators for the span of a citation for a given piece of text. In\naddition to issues related with the verifiability principle, many Wikipedia\nentity pages are incomplete, with relevant information that is already\navailable in online news sources missing. Even for the already existing\ncitations, there is often a delay between the news publication time and the\nreference time.\n  In this thesis, we address the aforementioned issues and propose automated\napproaches that enforce the verifiability principle in Wikipedia, and suggest\nrelevant and missing news references for further enriching Wikipedia entity\npages.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 12:49:54 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 07:49:13 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Fetahu", "Besnik", ""]]}, {"id": "1804.07686", "submitter": "Saehan Jo", "authors": "Saehan Jo, Immanuel Trummer, Weicheng Yu, Daniel Liu, Xuezhi Wang,\n  Cong Yu, Niyati Mehta", "title": "Verifying Text Summaries of Relational Data Sets", "comments": "18 pages, 13 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel natural language query interface, the AggChecker, aimed at\ntext summaries of relational data sets. The tool focuses on natural language\nclaims that translate into an SQL query and a claimed query result. Similar in\nspirit to a spell checker, the AggChecker marks up text passages that seem to\nbe inconsistent with the actual data. At the heart of the system is a\nprobabilistic model that reasons about the input document in a holistic\nfashion. Based on claim keywords and the document structure, it maps each text\nclaim to a probability distribution over associated query translations. By\nefficiently executing tens to hundreds of thousands of candidate translations\nfor a typical input document, the system maps text claims to correctness\nprobabilities. This process becomes practical via a specialized processing\nbackend, avoiding redundant work via query merging and result caching.\nVerification is an interactive process in which users are shown tentative\nresults, enabling them to take corrective actions if necessary.\n  Our system was tested on a set of 53 public articles containing 392 claims.\nOur test cases include articles from major newspapers, summaries of survey\nresults, and Wikipedia articles. Our tool revealed erroneous claims in roughly\na third of test cases. A detailed user study shows that users using our tool\nare in average six times faster at checking text summaries, compared to generic\nSQL interfaces. In fully automated verification, our tool achieves\nsignificantly higher recall and precision than baselines from the areas of\nnatural language query interfaces and fact-checking.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 15:46:05 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 18:27:55 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Jo", "Saehan", ""], ["Trummer", "Immanuel", ""], ["Yu", "Weicheng", ""], ["Liu", "Daniel", ""], ["Wang", "Xuezhi", ""], ["Yu", "Cong", ""], ["Mehta", "Niyati", ""]]}, {"id": "1804.07748", "submitter": "Polyvios Pratikakis", "authors": "Polyvios Pratikakis", "title": "twAwler: A lightweight twitter crawler", "comments": "8 pages, 7 figures, about to submit for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents twAwler, a lightweight twitter crawler that targets\nlanguage-specific communities of users. twAwler takes advantage of multiple\nendpoints of the twitter API to explore user relations and quickly recognize\nusers belonging to the targetted set. It performs a complete crawl for all\nusers, discovering many standard user relations, including the retweet graph,\nmention graph, reply graph, quote graph, follow graph, etc. twAwler respects\nall twitter policies and rate limits, while able to monitor large communities\nof active users.\n  twAwler was used between August 2016 and March 2018 to generate an extensive\ndataset of close to all Greek-speaking twitter accounts (about 330 thousand)\nand their tweets and relations. In total, the crawler has gathered 750 million\ntweets of which 424 million are in Greek; 750 million follow relations;\ninformation about 300 thousand lists, their members (119 million member\nrelations) and subscribers (27 thousand subscription relations); 705 thousand\ntrending topics; information on 52 million users in total of which 292 thousand\nhave been since suspended, 141 thousand have deleted their account, and 3.5\nmillion are protected and cannot be crawled. twAwler mines the collected tweets\nfor the retweet, quote, reply, and mention graphs, which, in addition to the\nfollow relation crawled, offer vast opportunities for analysis and further\nresearch.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 17:44:36 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Pratikakis", "Polyvios", ""]]}, {"id": "1804.07814", "submitter": "Yanshan Wang", "authors": "Yanshan Wang, Sunghwan Sohn, Sijia Liu, Feichen Shen, Liwei Wang,\n  Elizabeth J. Atkinson, Shreyasee Amin, Hongfang Liu", "title": "A Deep Representation Empowered Distant Supervision Paradigm for\n  Clinical Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To automatically create large labeled training datasets and reduce\nthe efforts of feature engineering for training accurate machine learning\nmodels for clinical information extraction. Materials and Methods: We propose a\ndistant supervision paradigm empowered by deep representation for extracting\ninformation from clinical text. In this paradigm, the rule-based NLP algorithms\nare utilized to generate weak labels and create large training datasets\nautomatically. Additionally, we use pre-trained word embeddings as deep\nrepresentation to eliminate the need of task-specific feature engineering for\nmachine learning. We evaluated the effectiveness of the proposed paradigm on\ntwo clinical information extraction tasks: smoking status extraction and\nproximal femur (hip) fracture extraction. We tested three prevalent machine\nlearning models, namely, Convolutional Neural Networks (CNN), Support Vector\nMachine (SVM), and Random Forrest (RF). Results: The results indicate that CNN\nis the best fit to the proposed distant supervision paradigm. It outperforms\nthe rule-based NLP algorithms given large datasets by capturing additional\nextraction patterns. We also verified the advantage of word embedding feature\nrepresentation in the paradigm over term frequency-inverse document frequency\n(tf-idf) and topic modeling representations. Discussion: In the clinical\ndomain, the limited amount of labeled data is always a bottleneck for applying\nmachine learning. Additionally, the performance of machine learning approaches\nhighly depends on task-specific feature engineering. The proposed paradigm\ncould alleviate those problems by leveraging rule-based NLP algorithms to\nautomatically assign weak labels and eliminating the need of task-specific\nfeature engineering using word embedding feature representation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 20:18:46 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Wang", "Yanshan", ""], ["Sohn", "Sunghwan", ""], ["Liu", "Sijia", ""], ["Shen", "Feichen", ""], ["Wang", "Liwei", ""], ["Atkinson", "Elizabeth J.", ""], ["Amin", "Shreyasee", ""], ["Liu", "Hongfang", ""]]}, {"id": "1804.07931", "submitter": "Liqin Zhao", "authors": "Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu,\n  Kun Gai", "title": "Entire Space Multi-Task Model: An Effective Approach for Estimating\n  Post-Click Conversion Rate", "comments": "accept by SIGIR-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating post-click conversion rate (CVR) accurately is crucial for ranking\nsystems in industrial applications such as recommendation and advertising.\nConventional CVR modeling applies popular deep learning methods and achieves\nstate-of-the-art performance. However it encounters several task-specific\nproblems in practice, making CVR modeling challenging. For example,\nconventional CVR models are trained with samples of clicked impressions while\nutilized to make inference on the entire space with samples of all impressions.\nThis causes a sample selection bias problem. Besides, there exists an extreme\ndata sparsity problem, making the model fitting rather difficult. In this\npaper, we model CVR in a brand-new perspective by making good use of sequential\npattern of user actions, i.e., impression -> click -> conversion. The proposed\nEntire Space Multi-task Model (ESMM) can eliminate the two problems\nsimultaneously by i) modeling CVR directly over the entire space, ii) employing\na feature representation transfer learning strategy. Experiments on dataset\ngathered from Taobao's recommender system demonstrate that ESMM significantly\noutperforms competitive methods. We also release a sampling version of this\ndataset to enable future research. To the best of our knowledge, this is the\nfirst public dataset which contains samples with sequential dependence of click\nand conversion labels for CVR modeling.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 09:59:29 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 12:54:14 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Ma", "Xiao", ""], ["Zhao", "Liqin", ""], ["Huang", "Guan", ""], ["Wang", "Zhi", ""], ["Hu", "Zelin", ""], ["Zhu", "Xiaoqiang", ""], ["Gai", "Kun", ""]]}, {"id": "1804.07958", "submitter": "Sha Yuan", "authors": "Sha Yuan, Yu Zhang, Jie Tang, Juan Bautista Cabot\\`a", "title": "Expert Finding in Community Question Answering: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development recently of Community Question Answering (CQA)\nsatisfies users quest for professional and personal knowledge about anything.\nIn CQA, one central issue is to find users with expertise and willingness to\nanswer the given questions. Expert finding in CQA often exhibits very different\nchallenges compared to traditional methods. Sparse data and new features\nviolate fundamental assumptions of traditional recommendation systems. This\npaper focuses on reviewing and categorizing the current progress on expert\nfinding in CQA. We classify all the existing solutions into four different\ncategories: matrix factorization based models (MF-based models), gradient\nboosting tree based models (GBT-based models), deep learning based models\n(DL-based models) and ranking based models (R-based models). We find that\nMF-based models outperform other categories of models in the field of expert\nfinding in CQA. Moreover, we use innovative diagrams to clarify several\nimportant concepts of ensemble learning, and find that ensemble models with\nseveral specific single models can further boosting the performance. Further,\nwe compare the performance of different models on different types of matching\ntasks, including text vs. text, graph vs. text, audio vs. text and video vs.\ntext. The results can help the model selection of expert finding in practice.\nFinally, we explore some potential future issues in expert finding research in\nCQA.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 12:37:45 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Yuan", "Sha", ""], ["Zhang", "Yu", ""], ["Tang", "Jie", ""], ["Cabot\u00e0", "Juan Bautista", ""]]}, {"id": "1804.08057", "submitter": "Md Faisal Mahbub Chowdhury", "authors": "Md Faisal Mahbub Chowdhury and Vijil Chenthamarakshan and Rishav\n  Chakravarti and Alfio M. Gliozzo", "title": "A Study on Passage Re-ranking in Embedding based Unsupervised Semantic\n  Search", "comments": "Fixed latex compiling issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art approaches for (embedding based) unsupervised semantic\nsearch exploits either compositional similarity (of a query and a passage) or\npair-wise word (or term) similarity (from the query and the passage). By\ndesign, word based approaches do not incorporate similarity in the larger\ncontext (query/passage), while compositional similarity based approaches are\nusually unable to take advantage of the most important cues in the context. In\nthis paper we propose a new compositional similarity based approach, called\nvariable centroid vector (VCVB), that tries to address both of these\nlimitations. We also presents results using a different type of compositional\nsimilarity based approach by exploiting universal sentence embedding. We\nprovide empirical evaluation on two different benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 02:20:32 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 17:14:06 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 15:49:13 GMT"}, {"version": "v4", "created": "Wed, 13 Mar 2019 17:06:22 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Chowdhury", "Md Faisal Mahbub", ""], ["Chenthamarakshan", "Vijil", ""], ["Chakravarti", "Rishav", ""], ["Gliozzo", "Alfio M.", ""]]}, {"id": "1804.08234", "submitter": "Muhmmad Al-Khiza'ay", "authors": "Muhmmad Al-Khiza'ay, Noora Alallaq, Qusay Alanoz, Adil Al-Azzawi,\n  N.Maheswari", "title": "PeRView: A Framework for Personalized Review Selection Using\n  Micro-Reviews", "comments": "27 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the contemporary era, social media has its influence on people in making\ndecisions. The proliferation of online reviews with diversified and verbose\ncontent often causes problems inaccurate decision making. Since online reviews\nhave an impact on people of all walks of life while taking decisions, choosing\nappropriate reviews based on the podsolization consisting is very important\nsince it relies on using such micro-reviews consistency to evaluate the review\nset section. Micro-reviews are very concise and directly talk about product or\nservice instead of having unnecessary verbose content. Thus, micro-reviews can\nhelp in choosing reviews based on their personalized consistency that is\nrelated to directly or indirectly to the main profile of the reviews.\nPersonalized reviews selection that is highly relevant with high personalized\ncoverage in terms of matching with micro-reviews is the main problem that is\nconsidered in this paper. Furthermore, personalization with user preferences\nwhile making review selection is also considered based on the personalized\nusers' profile. Towards this end, we proposed a framework known as PeRView for\npersonalized review selection using micro-reviews based on the proposed\nevaluation metric approach which considering two main factors (personalized\nmatching score and subset size). Personalized Review Selection Algorithm (PRSA)\nis proposed which makes use of multiple similarity measures merged to have\nhighly efficient personalized reviews matching function for selection. The\nexperimental results based on using reviews dataset which is collected from\nYELP.COM while micro-reviews dataset is obtained from Foursqure.COM. show that\nthe personalized reviews selection is a very empirical case of study.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 03:07:45 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Al-Khiza'ay", "Muhmmad", ""], ["Alallaq", "Noora", ""], ["Alanoz", "Qusay", ""], ["Al-Azzawi", "Adil", ""], ["Maheswari", "N.", ""]]}, {"id": "1804.08275", "submitter": "Ting Yao", "authors": "Zhaofan Qiu and Yingwei Pan and Ting Yao and Tao Mei", "title": "Deep Semantic Hashing with Generative Adversarial Networks", "comments": "SIGIR 2017 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has been a widely-adopted technique for nearest neighbor search in\nlarge-scale image retrieval tasks. Recent research has shown that leveraging\nsupervised information can lead to high quality hashing. However, the cost of\nannotating data is often an obstacle when applying supervised hashing to a new\ndomain. Moreover, the results can suffer from the robustness problem as the\ndata at training and test stage could come from similar but different\ndistributions. This paper studies the exploration of generating synthetic data\nthrough semi-supervised generative adversarial networks (GANs), which leverages\nlargely unlabeled and limited labeled training data to produce highly\ncompelling data with intrinsic invariance and global coherence, for better\nunderstanding statistical structures of natural data. We demonstrate that the\nabove two limitations can be well mitigated by applying the synthetic data for\nhashing. Specifically, a novel deep semantic hashing with GANs (DSH-GANs) is\npresented, which mainly consists of four components: a deep convolution neural\nnetworks (CNN) for learning image representations, an adversary stream to\ndistinguish synthetic images from real ones, a hash stream for encoding image\nrepresentations to hash codes and a classification stream. The whole\narchitecture is trained end-to-end by jointly optimizing three losses, i.e.,\nadversarial loss to correct label of synthetic or real for each sample, triplet\nranking loss to preserve the relative similarity ordering in the input\nreal-synthetic triplets and classification loss to classify each sample\naccurately. Extensive experiments conducted on both CIFAR-10 and NUS-WIDE image\nbenchmarks validate the capability of exploiting synthetic images for hashing.\nOur framework also achieves superior results when compared to state-of-the-art\ndeep hash models.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 08:19:55 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Qiu", "Zhaofan", ""], ["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Mei", "Tao", ""]]}, {"id": "1804.08759", "submitter": "Chen Qu", "authors": "Chen Qu, Liu Yang, W. Bruce Croft, Johanne R. Trippas, Yongfeng Zhang\n  and Minghui Qiu", "title": "Analyzing and Characterizing User Intent in Information-seeking\n  Conversations", "comments": "Accepted by SIGIR 2018 as a short paper", "journal-ref": null, "doi": "10.1145/3209978.3210124", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and characterizing how people interact in information-seeking\nconversations is crucial in developing conversational search systems. In this\npaper, we introduce a new dataset designed for this purpose and use it to\nanalyze information-seeking conversations by user intent distribution,\nco-occurrence, and flow patterns. The MSDialog dataset is a labeled dialog\ndataset of question answering (QA) interactions between information seekers and\nproviders from an online forum on Microsoft products. The dataset contains more\nthan 2,000 multi-turn QA dialogs with 10,000 utterances that are annotated with\nuser intent on the utterance level. Annotations were done using crowdsourcing.\nWith MSDialog, we find some highly recurring patterns in user intent during an\ninformation-seeking process. They could be useful for designing conversational\nsearch systems. We will make our dataset freely available to encourage\nexploration of information-seeking conversation models.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 22:07:28 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Qu", "Chen", ""], ["Yang", "Liu", ""], ["Croft", "W. Bruce", ""], ["Trippas", "Johanne R.", ""], ["Zhang", "Yongfeng", ""], ["Qiu", "Minghui", ""]]}, {"id": "1804.08806", "submitter": "Charilaos Kanatsoulis", "authors": "Charilaos I. Kanatsoulis, Xiao Fu, Nicholas D. Sidiropoulos, Mingyi\n  Hong", "title": "Structured SUMCOR Multiview Canonical Correlation Analysis for\n  Large-Scale Data", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2018.2878544", "report-no": null, "categories": "cs.LG cs.IR eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sum-of-correlations (SUMCOR) formulation of generalized canonical\ncorrelation analysis (GCCA) seeks highly correlated low-dimensional\nrepresentations of different views via maximizing pairwise latent similarity of\nthe views. SUMCOR is considered arguably the most natural extension of\nclassical two-view CCA to the multiview case, and thus has numerous\napplications in signal processing and data analytics. Recent work has proposed\neffective algorithms for handling the SUMCOR problem at very large scale.\nHowever, the existing scalable algorithms cannot incorporate structural\nregularization and prior information -- which are critical for good performance\nin real-world applications. In this work, we propose a new computational\nframework for large-scale SUMCOR GCCA that can easily incorporate a suite of\nstructural regularizers which are frequently used in data analytics. The\nupdates of the proposed algorithm are lightweight and the memory complexity is\nalso low. In addition, the proposed algorithm can be readily implemented in a\nparallel fashion. We show that the proposed algorithm converges to a\nKarush-Kuhn-Tucker (KKT) point of the regularized SUMCOR problem. Judiciously\ndesigned simulations and real-data experiments are employed to demonstrate the\neffectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 01:47:55 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Kanatsoulis", "Charilaos I.", ""], ["Fu", "Xiao", ""], ["Sidiropoulos", "Nicholas D.", ""], ["Hong", "Mingyi", ""]]}, {"id": "1804.08847", "submitter": "Elvis Saravia", "authors": "Elvis Saravia, Hsien-Chi Toby Liu, Yi-Shin Chen", "title": "DeepEmo: Learning and Enriching Pattern-Based Emotion Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a graph-based mechanism to extract rich-emotion bearing patterns,\nwhich fosters a deeper analysis of online emotional expressions, from a corpus.\nThe patterns are then enriched with word embeddings and evaluated through\nseveral emotion recognition tasks. Moreover, we conduct analysis on the\nemotion-oriented patterns to demonstrate its applicability and to explore its\nproperties. Our experimental results demonstrate that the proposed techniques\noutperform most state-of-the-art emotion recognition techniques.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 06:00:28 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Saravia", "Elvis", ""], ["Liu", "Hsien-Chi Toby", ""], ["Chen", "Yi-Shin", ""]]}, {"id": "1804.08891", "submitter": "Nikolaos Polatidis Dr", "authors": "Nikolaos Polatidis, Christos K. Georgiadis", "title": "A multi-level collaborative filtering method that improves\n  recommendations", "comments": null, "journal-ref": "Polatidis, Nikolaos, and Christos K. Georgiadis. A multi-level\n  collaborative filtering method that improves recommendations. Expert Systems\n  with Applications 48 (2016): 100-110", "doi": "10.1016/j.eswa.2015.11.023", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is one of the most used approaches for providing\nrecommendations in various online environments. Even though collaborative\nrecommendation methods have been widely utilized due to their simplicity and\nease of use, accuracy is still an issue. In this paper we propose a multi-level\nrecommendation method with its main purpose being to assist users in decision\nmaking by providing recommendations of better quality. The proposed method can\nbe applied in different online domains that use collaborative recommender\nsystems, thus improving the overall user experience. The efficiency of the\nproposed method is shown by providing an extensive experimental evaluation\nusing five real datasets and with comparisons to alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 08:20:45 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Polatidis", "Nikolaos", ""], ["Georgiadis", "Christos K.", ""]]}, {"id": "1804.08944", "submitter": "Moritz Einfalt", "authors": "Rainer Lienhart, Moritz Einfalt, Dan Zecha", "title": "Mining Automatically Estimated Poses from Video Recordings of Top\n  Athletes", "comments": "Under review for the International Journal of Computer Science in\n  Sport", "journal-ref": null, "doi": "10.2478/ijcss-2018-0005", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose detection systems based on state-of-the-art DNNs are on the go to\nbe extended, adapted and re-trained to fit the application domain of specific\nsports. Therefore, plenty of noisy pose data will soon be available from videos\nrecorded at a regular and frequent basis. This work is among the first to\ndevelop mining algorithms that can mine the expected abundance of noisy and\nannotation-free pose data from video recordings in individual sports. Using\nswimming as an example of a sport with dominant cyclic motion, we show how to\ndetermine unsupervised time-continuous cycle speeds and temporally striking\nposes as well as measure unsupervised cycle stability over time. Additionally,\nwe use long jump as an example of a sport with a rigid phase-based motion to\npresent a technique to automatically partition the temporally estimated pose\nsequences into their respective phases. This enables the extraction of\nperformance relevant, pose-based metrics currently used by national\nprofessional sports associations. Experimental results prove the effectiveness\nof our mining algorithms, which can also be applied to other cycle-based or\nphase-based types of sport.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 10:30:12 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 12:43:27 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Lienhart", "Rainer", ""], ["Einfalt", "Moritz", ""], ["Zecha", "Dan", ""]]}, {"id": "1804.09066", "submitter": "Mohammadreza Zolfaghari", "authors": "Mohammadreza Zolfaghari, Kamaljeet Singh, Thomas Brox", "title": "ECO: Efficient Convolutional Network for Online Video Understanding", "comments": "Submitted to ECCV 2018. 17 pages, 7 figures, Supplementary Material,\n  https://github.com/mzolfaghari/ECO-efficient-video-understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state of the art in video understanding suffers from two problems: (1)\nThe major part of reasoning is performed locally in the video, therefore, it\nmisses important relationships within actions that span several seconds. (2)\nWhile there are local methods with fast per-frame processing, the processing of\nthe whole video is not efficient and hampers fast video retrieval or online\nclassification of long-term activities. In this paper, we introduce a network\narchitecture that takes long-term content into account and enables fast\nper-video processing at the same time. The architecture is based on merging\nlong-term content already in the network rather than in a post-hoc fusion.\nTogether with a sampling strategy, which exploits that neighboring frames are\nlargely redundant, this yields high-quality action classification and video\ncaptioning at up to 230 videos per second, where each video can consist of a\nfew hundred frames. The approach achieves competitive performance across all\ndatasets while being 10x to 80x faster than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 14:30:56 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 09:46:08 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Zolfaghari", "Mohammadreza", ""], ["Singh", "Kamaljeet", ""], ["Brox", "Thomas", ""]]}, {"id": "1804.09539", "submitter": "Yuxin Peng", "authors": "Jinwei Qi, Yuxin Peng and Yuxin Yuan", "title": "Cross-media Multi-level Alignment with Relation Attention Network", "comments": "7 pages, accepted by International Joint Conference on Artificial\n  Intelligence (IJCAI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of multimedia data, such as image and text, it is a\nhighly challenging problem to effectively correlate and retrieve the data of\ndifferent media types. Naturally, when correlating an image with textual\ndescription, people focus on not only the alignment between discriminative\nimage regions and key words, but also the relations lying in the visual and\ntextual context. Relation understanding is essential for cross-media\ncorrelation learning, which is ignored by prior cross-media retrieval works. To\naddress the above issue, we propose Cross-media Relation Attention Network\n(CRAN) with multi-level alignment. First, we propose visual-language relation\nattention model to explore both fine-grained patches and their relations of\ndifferent media types. We aim to not only exploit cross-media fine-grained\nlocal information, but also capture the intrinsic relation information, which\ncan provide complementary hints for correlation learning. Second, we propose\ncross-media multi-level alignment to explore global, local and relation\nalignments across different media types, which can mutually boost to learn more\nprecise cross-media correlation. We conduct experiments on 2 cross-media\ndatasets, and compare with 10 state-of-the-art methods to verify the\neffectiveness of proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 13:22:38 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Qi", "Jinwei", ""], ["Peng", "Yuxin", ""], ["Yuan", "Yuxin", ""]]}, {"id": "1804.09661", "submitter": "Aaron Jaech", "authors": "Aaron Jaech and Mari Ostendorf", "title": "Personalized Language Model for Query Auto-Completion", "comments": "ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query auto-completion is a search engine feature whereby the system suggests\ncompleted queries as the user types. Recently, the use of a recurrent neural\nnetwork language model was suggested as a method of generating query\ncompletions. We show how an adaptable language model can be used to generate\npersonalized completions and how the model can use online updating to make\npredictions for users not seen during training. The personalized predictions\nare significantly better than a baseline that uses no user information.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 16:26:39 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Jaech", "Aaron", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1804.09943", "submitter": "Tobias Strau{\\ss}", "authors": "Tobias Strau{\\ss} and Max Weidemann and Johannes Michael and Gundram\n  Leifert and Tobias Gr\\\"uning and Roger Labahn", "title": "System Description of CITlab's Recognition & Retrieval Engine for\n  ICDAR2017 Competition on Information Extraction in Historical Handwritten\n  Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a recognition and retrieval system for the ICDAR2017 Competition\non Information Extraction in Historical Handwritten Records which successfully\ninfers person names and other data from marriage records. The system extracts\ninformation from the line images with a high accuracy and outperforms the\nbaseline. The optical model is based on Neural Networks. To infer the desired\ninformation, regular expressions are used to describe the set of feasible words\nsequences.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 08:52:19 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Strau\u00df", "Tobias", ""], ["Weidemann", "Max", ""], ["Michael", "Johannes", ""], ["Leifert", "Gundram", ""], ["Gr\u00fcning", "Tobias", ""], ["Labahn", "Roger", ""]]}, {"id": "1804.09996", "submitter": "Matthijs Douze", "authors": "Matthijs Douze and Alexandre Sablayrolles and Herv\\'e J\\'egou", "title": "Link and code: Fast indexing with graphs and compact regression codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search approaches based on graph walks have recently attained\noutstanding speed-accuracy trade-offs, taking aside the memory requirements. In\nthis paper, we revisit these approaches by considering, additionally, the\nmemory constraint required to index billions of images on a single server. This\nleads us to propose a method based both on graph traversal and compact\nrepresentations. We encode the indexed vectors using quantization and exploit\nthe graph structure to refine the similarity estimation.\n  In essence, our method takes the best of these two worlds: the search\nstrategy is based on nested graphs, thereby providing high precision with a\nrelatively small set of comparisons. At the same time it offers a significant\nmemory compression. As a result, our approach outperforms the state of the art\non operating points considering 64-128 bytes per vector, as demonstrated by our\nresults on two billion-scale public benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 11:24:42 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 10:01:51 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Douze", "Matthijs", ""], ["Sablayrolles", "Alexandre", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "1804.10276", "submitter": "Nikolaos Polatidis Dr", "authors": "Nikolaos Polatidis, Elias Pimenidis, Michalis Pavlidis, Spyridon\n  Papastergiou, Haralambos Mouratidis", "title": "From product recommendation to cyber-attack prediction: Generating\n  attack graphs and predicting future attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern information society depends on reliable functionality of information\nsystems infrastructure, while at the same time the number of cyber-attacks has\nbeen increasing over the years and damages have been caused. Furthermore,\ngraphs can be used to show paths than can be exploited by attackers to intrude\ninto systems and gain unauthorized access through vulnerability exploitation.\nThis paper presents a method that builds attack graphs using data supplied from\nthe maritime supply chain infrastructure. The method delivers all possible\npaths that can be exploited to gain access. Then, a recommendation system is\nutilized to make predictions about future attack steps within the network. We\nshow that recommender systems can be used in cyber defense by predicting\nattacks. The goal of this paper is to identify attack paths and show how a\nrecommendation method can be used to classify future cyber-attacks in terms of\nrisk management. The proposed method has been experimentally evaluated and\nvalidated, with the results showing that it is both practical and effective.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 20:49:40 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Polatidis", "Nikolaos", ""], ["Pimenidis", "Elias", ""], ["Pavlidis", "Michalis", ""], ["Papastergiou", "Spyridon", ""], ["Mouratidis", "Haralambos", ""]]}, {"id": "1804.10795", "submitter": "Menghan Wang", "authors": "Menghan Wang, Xiaolin Zheng, Kun Zhang", "title": "User-Sensitive Recommendation Ensemble with Clustered Multi-Task\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers recommendation algorithm ensembles in a user-sensitive\nmanner. Recently researchers have proposed various effective recommendation\nalgorithms, which utilized different aspects of the data and different\ntechniques. However, the \"user skewed prediction\" problem may exist for almost\nall recommendation algorithms -- algorithms with best average predictive\naccuracy may cover up that the algorithms may perform poorly for some part of\nusers, which will lead to biased services in real scenarios. In this paper, we\npropose a user-sensitive ensemble method named \"UREC\" to address this issue. We\nfirst cluster users based on the recommendation predictions, then we use\nmulti-task learning to learn the user-sensitive ensemble function for the\nusers. In addition, to alleviate the negative effects of new user problem to\nclustering users, we propose an approximate approach based on a spectral\nrelaxation. Experiments on real-world datasets demonstrate the superiority of\nour methods.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 12:35:27 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Wang", "Menghan", ""], ["Zheng", "Xiaolin", ""], ["Zhang", "Kun", ""]]}, {"id": "1804.10862", "submitter": "Travis Ebesu", "authors": "Travis Ebesu, Bin Shen, Yi Fang", "title": "Collaborative Memory Network for Recommendation Systems", "comments": "Published as ACM SIGIR 2018 Full Paper", "journal-ref": null, "doi": "10.1145/3209978.3209991", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems play a vital role to keep users engaged with\npersonalized content in modern online platforms. Deep learning has\nrevolutionized many research fields and there is a recent surge of interest in\napplying it to collaborative filtering (CF). However, existing methods compose\ndeep learning architectures with the latent factor model ignoring a major class\nof CF models, neighborhood or memory-based approaches. We propose Collaborative\nMemory Networks (CMN), a deep architecture to unify the two classes of CF\nmodels capitalizing on the strengths of the global structure of latent factor\nmodel and local neighborhood-based structure in a nonlinear fashion. Motivated\nby the success of Memory Networks, we fuse a memory component and neural\nattention mechanism as the neighborhood component. The associative addressing\nscheme with the user and item memories in the memory module encodes complex\nuser-item relations coupled with the neural attention mechanism to learn a\nuser-item specific neighborhood. Finally, the output module jointly exploits\nthe neighborhood with the user and item memories to produce the ranking score.\nStacking multiple memory modules together yield deeper architectures capturing\nincreasingly complex user-item relations. Furthermore, we show strong\nconnections between CMN components, memory networks and the three classes of CF\nmodels. Comprehensive experimental results demonstrate the effectiveness of CMN\non three public datasets outperforming competitive baselines. Qualitative\nvisualization of the attention weights provide insight into the model's\nrecommendation process and suggest the presence of higher order interactions.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 03:17:36 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 00:02:12 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Ebesu", "Travis", ""], ["Shen", "Bin", ""], ["Fang", "Yi", ""]]}, {"id": "1804.10877", "submitter": "Jiaming Shen", "authors": "Jiaming Shen, Jinfeng Xiao, Xinwei He, Jingbo Shang, Saurabh Sinha,\n  Jiawei Han", "title": "Entity Set Search of Scientific Literature: An Unsupervised Ranking\n  Approach", "comments": "SIGIR 2018 Full Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literature search is critical for any scientific research. Different from Web\nor general domain search, a large portion of queries in scientific literature\nsearch are entity-set queries, that is, multiple entities of possibly different\ntypes. Entity-set queries reflect user's need for finding documents that\ncontain multiple entities and reveal inter-entity relationships and thus pose\nnon-trivial challenges to existing search algorithms that model each entity\nseparately. However, entity-set queries are usually sparse (i.e., not so\nrepetitive), which makes ineffective many supervised ranking models that rely\nheavily on associated click history. To address these challenges, we introduce\nSetRank, an unsupervised ranking framework that models inter-entity\nrelationships and captures entity type information. Furthermore, we develop a\nnovel unsupervised model selection algorithm, based on the technique of\nweighted rank aggregation, to automatically choose the parameter settings in\nSetRank without resorting to a labeled validation set. We evaluate our proposed\nunsupervised approach using datasets from TREC Genomics Tracks and Semantic\nScholar's query log. The experiments demonstrate that SetRank significantly\noutperforms the baseline unsupervised models, especially on entity-set queries,\nand our model selection algorithm effectively chooses suitable parameter\nsettings.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 05:45:51 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Shen", "Jiaming", ""], ["Xiao", "Jinfeng", ""], ["He", "Xinwei", ""], ["Shang", "Jingbo", ""], ["Sinha", "Saurabh", ""], ["Han", "Jiawei", ""]]}, {"id": "1804.10911", "submitter": "Yadi Lao", "authors": "Yadi Lao, Jun Xu, Yanyan Lan, Jiafeng Guo, Sheng Gao, Xueqi Cheng", "title": "A Tree Search Algorithm for Sequence Labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel reinforcement learning based model for\nsequence tagging, referred to as MM-Tag. Inspired by the success and\nmethodology of the AlphaGo Zero, MM-Tag formalizes the problem of sequence\ntagging with a Monte Carlo tree search (MCTS) enhanced Markov decision process\n(MDP) model, in which the time steps correspond to the positions of words in a\nsentence from left to right, and each action corresponds to assign a tag to a\nword. Two long short-term memory networks (LSTM) are used to summarize the past\ntag assignments and words in the sentence. Based on the outputs of LSTMs, the\npolicy for guiding the tag assignment and the value for predicting the whole\ntagging accuracy of the whole sentence are produced. The policy and value are\nthen strengthened with MCTS, which takes the produced raw policy and value as\ninputs, simulates and evaluates the possible tag assignments at the subsequent\npositions, and outputs a better search policy for assigning tags. A\nreinforcement learning algorithm is proposed to train the model parameters. Our\nwork is the first to apply the MCTS enhanced MDP model to the sequence tagging\ntask. We show that MM-Tag can accurately predict the tags thanks to the\nexploratory decision making mechanism introduced by MCTS. Experimental results\nshow based on a chunking benchmark showed that MM-Tag outperformed the\nstate-of-the-art sequence tagging baselines including CRF and CRF with LSTM.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 11:57:15 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 01:13:09 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Lao", "Yadi", ""], ["Xu", "Jun", ""], ["Lan", "Yanyan", ""], ["Guo", "Jiafeng", ""], ["Gao", "Sheng", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1804.10949", "submitter": "Giulio Ermanno Pibiri", "authors": "Giulio Ermanno Pibiri and Rossano Venturini", "title": "On Optimally Partitioning Variable-Byte Codes", "comments": "Published in IEEE Transactions on Knowledge and Data Engineering\n  (TKDE), 15 April 2019", "journal-ref": null, "doi": "10.1109/TKDE.2019.2911288", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquitous Variable-Byte encoding is one of the fastest compressed\nrepresentation for integer sequences. However, its compression ratio is usually\nnot competitive with other more sophisticated encoders, especially when the\nintegers to be compressed are small that is the typical case for inverted\nindexes. This paper shows that the compression ratio of Variable-Byte can be\nimproved by 2x by adopting a partitioned representation of the inverted lists.\nThis makes Variable-Byte surprisingly competitive in space with the best\nbit-aligned encoders, hence disproving the folklore belief that Variable-Byte\nis space-inefficient for inverted index compression. Despite the significant\nspace savings, we show that our optimization almost comes for free, given that:\nwe introduce an optimal partitioning algorithm that does not affect indexing\ntime because of its linear-time complexity; we show that the query processing\nspeed of Variable-Byte is preserved, with an extensive experimental analysis\nand comparison with several other state-of-the-art encoders.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 14:57:53 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 09:09:28 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Pibiri", "Giulio Ermanno", ""], ["Venturini", "Rossano", ""]]}, {"id": "1804.11131", "submitter": "Suzan Verberne", "authors": "Suzan Verberne, Arjen P. de Vries, Wessel Kraaij", "title": "Author-topic profiles for academic search", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implemented and evaluated a two-stage retrieval method for personalized\nacademic search in which the initial search results are re-ranked using an\nauthor-topic profile. In academic search tasks, the user's own data can help\noptimizing the ranking of search results to match the searcher's specific\nindividual needs. The author-topic profile consists of topic-specific terms,\nstored in a graph. We re-rank the top-1000 retrieved documents using ten\nfeatures that represent the similarity between the document and the\nauthor-topic graph. We found that the re-ranking gives a small but significant\nimprovement over the reproduced best method from the literature. Storing the\nprofile as a graph has a number of advantages: it is flexible with respect to\nnode and relation types; it is a visualization of knowledge that is\ninterpretable by the user, and it offers the possibility to view relational\ncharacteristics of individual nodes.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 11:43:12 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Verberne", "Suzan", ""], ["de Vries", "Arjen P.", ""], ["Kraaij", "Wessel", ""]]}, {"id": "1804.11146", "submitter": "Micael Carvalho", "authors": "Micael Carvalho, R\\'emi Cad\\`ene, David Picard, Laure Soulier, Nicolas\n  Thome, Matthieu Cord", "title": "Cross-Modal Retrieval in the Cooking Context: Learning Semantic\n  Text-Image Embeddings", "comments": "accepted at the 41st International ACM SIGIR Conference on Research\n  and Development in Information Retrieval, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing powerful tools that support cooking activities has rapidly gained\npopularity due to the massive amounts of available data, as well as recent\nadvances in machine learning that are capable of analyzing them. In this paper,\nwe propose a cross-modal retrieval model aligning visual and textual data (like\npictures of dishes and their recipes) in a shared representation space. We\ndescribe an effective learning scheme, capable of tackling large-scale\nproblems, and validate it on the Recipe1M dataset containing nearly 1 million\npicture-recipe pairs. We show the effectiveness of our approach regarding\nprevious state-of-the-art models and present qualitative results over\ncomputational cooking use cases.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 12:14:32 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Carvalho", "Micael", ""], ["Cad\u00e8ne", "R\u00e9mi", ""], ["Picard", "David", ""], ["Soulier", "Laure", ""], ["Thome", "Nicolas", ""], ["Cord", "Matthieu", ""]]}, {"id": "1804.11149", "submitter": "Sheikh Shams Azam", "authors": "Sheikh Shams Azam, Manoj Raju, Venkatesh Pagidimarri, Vamsi\n  Kasivajjala", "title": "Q-Map: Clinical Concept Mining from Clinical Documents", "comments": "6 pages, 1 figure", "journal-ref": "International Journal of Computer and Information Engineering,\n  12(9), 2018, 691 - 696", "doi": "10.5281/zenodo.1474513", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, there has been a steep rise in the data-driven analysis\nin major areas of medicine, such as clinical decision support system, survival\nanalysis, patient similarity analysis, image analytics etc. Most of the data in\nthe field are well-structured and available in numerical or categorical formats\nwhich can be used for experiments directly. But on the opposite end of the\nspectrum, there exists a wide expanse of data that is intractable for direct\nanalysis owing to its unstructured nature which can be found in the form of\ndischarge summaries, clinical notes, procedural notes which are in human\nwritten narrative format and neither have any relational model nor any standard\ngrammatical structure. An important step in the utilization of these texts for\nsuch studies is to transform and process the data to retrieve structured\ninformation from the haystack of irrelevant data using information retrieval\nand data mining techniques. To address this problem, the authors present Q-Map\nin this paper, which is a simple yet robust system that can sift through\nmassive datasets with unregulated formats to retrieve structured information\naggressively and efficiently. It is backed by an effective mining technique\nwhich is based on a string matching algorithm that is indexed on curated\nknowledge sources, that is both fast and configurable. The authors also briefly\nexamine its comparative performance with MetaMap, one of the most reputed tools\nfor medical concepts retrieval and present the advantages the former displays\nover the latter.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 12:19:03 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 12:24:39 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Azam", "Sheikh Shams", ""], ["Raju", "Manoj", ""], ["Pagidimarri", "Venkatesh", ""], ["Kasivajjala", "Vamsi", ""]]}, {"id": "1804.11177", "submitter": "Qianqian Xu", "authors": "Qianqian Xu, Jiechao Xiong, Xiaochun Cao, Qingming Huang, and Yuan Yao", "title": "From Social to Individuals: a Parsimonious Path of Multi-level Models\n  for Crowdsourced Preference Aggregation", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence as a regular paper. arXiv admin note: substantial text overlap\n  with arXiv:1607.03401", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In crowdsourced preference aggregation, it is often assumed that all the\nannotators are subject to a common preference or social utility function which\ngenerates their comparison behaviors in experiments. However, in reality\nannotators are subject to variations due to multi-criteria, abnormal, or a\nmixture of such behaviors. In this paper, we propose a parsimonious\nmixed-effects model, which takes into account both the fixed effect that the\nmajority of annotators follows a common linear utility model, and the random\neffect that some annotators might deviate from the common significantly and\nexhibit strongly personalized preferences. The key algorithm in this paper\nestablishes a dynamic path from the social utility to individual variations,\nwith different levels of sparsity on personalization. The algorithm is based on\nthe Linearized Bregman Iterations, which leads to easy parallel implementations\nto meet the need of large-scale data analysis. In this unified framework, three\nkinds of random utility models are presented, including the basic linear model\nwith L2 loss, Bradley-Terry model, and Thurstone-Mosteller model. The validity\nof these multi-level models are supported by experiments with both simulated\nand real-world datasets, which shows that the parsimonious multi-level models\nexhibit improvements in both interpretability and predictive precision compared\nwith traditional HodgeRank.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 03:56:22 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Xu", "Qianqian", ""], ["Xiong", "Jiechao", ""], ["Cao", "Xiaochun", ""], ["Huang", "Qingming", ""], ["Yao", "Yuan", ""]]}, {"id": "1804.11192", "submitter": "Yongfeng Zhang", "authors": "Yongfeng Zhang and Xu Chen", "title": "Explainable Recommendation: A Survey and New Perspectives", "comments": "101 pages, published in Foundations and Trends in Information\n  Retrieval, 14(1), pp.1-101 (2020)", "journal-ref": "Foundations and Trends in Information Retrieval, 14(1), pp.1-101\n  (2020)", "doi": "10.1561/1500000066", "report-no": null, "categories": "cs.IR cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable recommendation attempts to develop models that generate not only\nhigh-quality recommendations but also intuitive explanations. The explanations\nmay either be post-hoc or directly come from an explainable model (also called\ninterpretable or transparent model in some contexts). Explainable\nrecommendation tries to address the problem of why: by providing explanations\nto users or system designers, it helps humans to understand why certain items\nare recommended by the algorithm, where the human can either be users or system\ndesigners. Explainable recommendation helps to improve the transparency,\npersuasiveness, effectiveness, trustworthiness, and satisfaction of\nrecommendation systems. It also facilitates system designers for better system\ndebugging. In recent years, a large number of explainable recommendation\napproaches -- especially model-based methods -- have been proposed and applied\nin real-world systems.\n  In this survey, we provide a comprehensive review for the explainable\nrecommendation research. We first highlight the position of explainable\nrecommendation in recommender system research by categorizing recommendation\nproblems into the 5W, i.e., what, when, who, where, and why. We then conduct a\ncomprehensive survey of explainable recommendation on three perspectives: 1) We\nprovide a chronological research timeline of explainable recommendation. 2) We\nprovide a two-dimensional taxonomy to classify existing explainable\nrecommendation research. 3) We summarize how explainable recommendation applies\nto different recommendation tasks. We also devote a chapter to discuss the\nexplanation perspectives in broader IR and AI/ML research. We end the survey by\ndiscussing potential future directions to promote the explainable\nrecommendation research area and beyond.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 13:49:44 GMT"}, {"version": "v10", "created": "Sun, 13 Sep 2020 03:17:09 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 22:17:18 GMT"}, {"version": "v3", "created": "Sun, 13 May 2018 05:45:50 GMT"}, {"version": "v4", "created": "Tue, 4 Sep 2018 02:22:46 GMT"}, {"version": "v5", "created": "Wed, 24 Jul 2019 17:10:28 GMT"}, {"version": "v6", "created": "Mon, 12 Aug 2019 22:27:04 GMT"}, {"version": "v7", "created": "Thu, 15 Aug 2019 15:28:16 GMT"}, {"version": "v8", "created": "Wed, 1 Jan 2020 17:04:33 GMT"}, {"version": "v9", "created": "Fri, 20 Mar 2020 17:06:15 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zhang", "Yongfeng", ""], ["Chen", "Xu", ""]]}, {"id": "1804.11243", "submitter": "Shih-Feng Yang", "authors": "Shih-Feng Yang and Julia Taylor Rayz", "title": "An Event Detection Approach Based On Twitter Hashtags", "comments": "The 18th International Conference on Computational Linguistics and\n  Intelligent Text Processing, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter is one of the most popular microblogging services in the world. The\ngreat amount of information within Twitter makes it an important information\nchannel for people to learn and share news. Twitter hashtag is an popular\nfeature that can be viewed as human-labeled information which people use to\nidentify the topic of a tweet. Many researchers have proposed event-detection\napproaches that can monitor Twitter data and determine whether special events,\nsuch as accidents, extreme weather, earthquakes, or crimes take place. Although\nmany approaches use hashtags as one of their features, few of them explicitly\nfocus on the effectiveness of using hashtags on event detection. In this study,\nwe proposed an event detection approach that utilizes hashtags in tweets. We\nadopted the feature extraction used in STREAMCUBE and applied a clustering\nK-means approach to it. The experiments demonstrated that the K-means approach\nperformed better than STREAMCUBE in the clustering results. A discussion on\noptimal K values for the K-means approach is also provided.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 19:57:29 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Yang", "Shih-Feng", ""], ["Rayz", "Julia Taylor", ""]]}, {"id": "1804.11335", "submitter": "Xixi Li", "authors": "Xixi Li and Jiahao Xing and Haihui Wang and Lingfang Zheng and Suling\n  Jia and Qiang Wang", "title": "A Hybrid Recommendation Method Based on Feature for Offline Book\n  Personalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation system has been widely used in different areas. Collaborative\nfiltering focuses on rating, ignoring the features of items itself. In order to\neffectively evaluate customers preferences on books, taking into consideration\nof the characteristics of offline book retail, we use LDA model to calculate\ncustomers preference on book topics and use word2vec to calculate customers\npreference on book types. When forecasting rating on books, we take two factors\ninto consideration: similarity of customers and correlation between customers\nand books. The experiment shows that our hybrid recommendation method based on\nfeatures performances better than single recommendation method in offline book\nretail data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 02:47:08 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Li", "Xixi", ""], ["Xing", "Jiahao", ""], ["Wang", "Haihui", ""], ["Zheng", "Lingfang", ""], ["Jia", "Suling", ""], ["Wang", "Qiang", ""]]}]