[{"id": "2103.00154", "submitter": "Dinuka De Zoysa", "authors": "B.D.M. De Zoysa, Y.A.M.M.A. Ali, M.D.I. Maduranga, Indika Perera,\n  Saliya Ekanayake, Anil Vullikanti", "title": "Parallel Algorithms for Densest Subgraph Discovery Using Shared Memory\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of finding dense components of a graph is a widely explored area\nin data analysis, with diverse applications in fields and branches of study\nincluding community mining, spam detection, computer security and\nbioinformatics. This research project explores previously available algorithms\nin order to study them and identify potential modifications that could result\nin an improved version with considerable performance and efficiency leap.\nFurthermore, efforts were also steered towards devising a novel algorithm for\nthe problem of densest subgraph discovery. This paper presents an improved\nimplementation of a widely used densest subgraph discovery algorithm and a\nnovel parallel algorithm which produces better results than a 2-approximation.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 08:07:39 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["De Zoysa", "B. D. M.", ""], ["Ali", "Y. A. M. M. A.", ""], ["Maduranga", "M. D. I.", ""], ["Perera", "Indika", ""], ["Ekanayake", "Saliya", ""], ["Vullikanti", "Anil", ""]]}, {"id": "2103.00199", "submitter": "Abdul Hameed Azeemi", "authors": "Abdul Hameed Azeemi, Adeel Waheed", "title": "COVID-19 Tweets Analysis through Transformer Language Models", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the public sentiment and perception in a healthcare crisis is\nessential for developing appropriate crisis management techniques. While some\nstudies have used Twitter data for predictive modelling during COVID-19,\nfine-grained sentiment analysis of the opinion of people on social media during\nthis pandemic has not yet been done. In this study, we perform an in-depth,\nfine-grained sentiment analysis of tweets in COVID-19. For this purpose, we\nperform supervised training of four transformer language models on the\ndownstream task of multi-label classification of tweets into seven tone\nclasses: [confident, anger, fear, joy, sadness, analytical, tentative]. We\nachieve a LRAP (Label Ranking Average Precision) score of 0.9267 through\nRoBERTa. This trained transformer model is able to correctly predict, with high\naccuracy, the tone of a tweet. We then leverage this model for predicting tones\nfor 200,000 tweets on COVID-19. We then perform a country-wise analysis of the\ntone of tweets, and extract useful indicators of the psychological condition\nabout the people in this pandemic.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 12:06:33 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Azeemi", "Abdul Hameed", ""], ["Waheed", "Adeel", ""]]}, {"id": "2103.00287", "submitter": "Wenying Ji", "authors": "Yitong Li, Duoduo Liao, Jundong Li, Wenying Ji", "title": "Automated Generation of Interorganizational Disaster Response Networks\n  through Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a disaster occurs, maintaining and restoring community lifelines\nsubsequently require collective efforts from various stakeholders. Aiming at\nreducing the efforts associated with generating Stakeholder Collaboration\nNetworks (SCNs), this paper proposes a systematic approach to reliable\ninformation extraction for stakeholder collaboration and automated network\ngeneration. Specifically, stakeholders and their interactions are extracted\nfrom texts through Named Entity Recognition (NER), one of the techniques in\nnatural language processing. Once extracted, the collaboration information is\ntransformed into structured datasets to generate the SCNs automatically. A case\nstudy of stakeholder collaboration during Hurricane Harvey was investigated and\nit had demonstrated the feasibility and applicability of the proposed method.\nHence, the proposed approach was proved to significantly reduce practitioners'\ninterpretation and data collection workloads. In the end, discussions and\nfuture work are provided.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 18:20:51 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Li", "Yitong", ""], ["Liao", "Duoduo", ""], ["Li", "Jundong", ""], ["Ji", "Wenying", ""]]}, {"id": "2103.00368", "submitter": "Yiling Jia", "authors": "Yiling Jia, Huazheng Wang, Stephen Guo, Hongning Wang", "title": "PairRank: Online Pairwise Learning to Rank by Divide-and-Conquer", "comments": "The Web Conference 2021", "journal-ref": null, "doi": "10.1145/3442381.3449972", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Learning to Rank (OL2R) eliminates the need of explicit relevance\nannotation by directly optimizing the rankers from their interactions with\nusers. However, the required exploration drives it away from successful\npractices in offline learning to rank, which limits OL2R's empirical\nperformance and practical applicability. In this work, we propose to estimate a\npairwise learning to rank model online. In each round, candidate documents are\npartitioned and ranked according to the model's confidence on the estimated\npairwise rank order, and exploration is only performed on the uncertain pairs\nof documents, i.e., \\emph{divide-and-conquer}. Regret directly defined on the\nnumber of mis-ordered pairs is proven, which connects the online solution's\ntheoretical convergence with its expected ranking performance. Comparisons\nagainst an extensive list of OL2R baselines on two public learning to rank\nbenchmark datasets demonstrate the effectiveness of the proposed solution.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 01:16:55 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 05:42:50 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 22:07:14 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Jia", "Yiling", ""], ["Wang", "Huazheng", ""], ["Guo", "Stephen", ""], ["Wang", "Hongning", ""]]}, {"id": "2103.00370", "submitter": "Mark Hamilton", "authors": "Mark Hamilton, Scott Lundberg, Lei Zhang, Stephanie Fu, William T.\n  Freeman", "title": "Model-Agnostic Explainability for Visual Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  What makes two images similar? We propose new approaches to generate\nmodel-agnostic explanations for image similarity, search, and retrieval. In\nparticular, we extend Class Activation Maps (CAMs), Additive Shapley\nExplanations (SHAP), and Locally Interpretable Model-Agnostic Explanations\n(LIME) to the domain of image retrieval and search. These approaches enable\nblack and grey-box model introspection and can help diagnose errors and\nunderstand the rationale behind a model's similarity judgments. Furthermore, we\nextend these approaches to extract a full pairwise correspondence between the\nquery and retrieved image pixels, an approach we call \"joint interpretations\".\nFormally, we show joint search interpretations arise from projecting Harsanyi\ndividends, and that this approach generalizes Shapley Values and The\nShapley-Taylor indices. We introduce a fast kernel-based method for estimating\nShapley-Taylor indices and empirically show that these game-theoretic measures\nyield more consistent explanations for image similarity architectures.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 01:24:15 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hamilton", "Mark", ""], ["Lundberg", "Scott", ""], ["Zhang", "Lei", ""], ["Fu", "Stephanie", ""], ["Freeman", "William T.", ""]]}, {"id": "2103.00380", "submitter": "Abheesht Sharma", "authors": "Abheesht Sharma and Harshit Pandey", "title": "LRG at TREC 2020: Document Ranking with XLNet-Based Models", "comments": "Published at TREC 2020", "journal-ref": "In Proceedings of the Twenty-Ninth Text REtrieval Conference (TREC\n  2020)", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing a good information retrieval system in popular mediums of\nentertainment is a quickly growing area of investigation for companies and\nresearchers alike. We delve into the domain of information retrieval for\npodcasts. In Spotify's Podcast Challenge, we are given a user's query with a\ndescription to find the most relevant short segment from the given dataset\nhaving all the podcasts. Previous techniques that include solely classical\nInformation Retrieval (IR) techniques, perform poorly when descriptive queries\nare presented. On the other hand, models which exclusively rely on large neural\nnetworks tend to perform better. The downside to this technique is that a\nconsiderable amount of time and computing power are required to infer the\nresult. We experiment with two hybrid models which first filter out the best\npodcasts based on user's query with a classical IR technique, and then perform\nre-ranking on the shortlisted documents based on the detailed description using\na transformer-based model.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 03:04:29 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 13:49:17 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Sharma", "Abheesht", ""], ["Pandey", "Harshit", ""]]}, {"id": "2103.00436", "submitter": "Jin Chen", "authors": "Jin Chen, Ju Xu, Gangwei Jiang, Tiezheng Ge, Zhiqiang Zhang, Defu\n  Lian, Kai Zheng", "title": "Automated Creative Optimization for E-Commerce Advertising", "comments": "10 pages, 5 figures, the Web Conference 2021", "journal-ref": null, "doi": "10.1145/3442381.3449909", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advertising creatives are ubiquitous in E-commerce advertisements and\naesthetic creatives may improve the click-through rate (CTR) of the products.\nNowadays smart advertisement platforms provide the function of compositing\ncreatives based on source materials provided by advertisers. Since a great\nnumber of creatives can be generated, it is difficult to accurately predict\ntheir CTR given a limited amount of feedback. Factorization machine (FM), which\nmodels inner product interaction between features, can be applied for the CTR\nprediction of creatives. However, interactions between creative elements may be\nmore complex than the inner product, and the FM-estimated CTR may be of high\nvariance due to limited feedback. To address these two issues, we propose an\nAutomated Creative Optimization (AutoCO) framework to model complex interaction\nbetween creative elements and to balance between exploration and exploitation.\nSpecifically, motivated by AutoML, we propose one-shot search algorithms for\nsearching effective interaction functions between elements. We then develop\nstochastic variational inference to estimate the posterior distribution of\nparameters based on the reparameterization trick, and apply Thompson Sampling\nfor efficiently exploring potentially better creatives. We evaluate the\nproposed method with both a synthetic dataset and two public datasets. The\nexperimental results show our method can outperform competing baselines with\nrespect to cumulative regret. The online A/B test shows our method leads to a 7\nincrease in CTR compared to the baseline.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 09:39:24 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chen", "Jin", ""], ["Xu", "Ju", ""], ["Jiang", "Gangwei", ""], ["Ge", "Tiezheng", ""], ["Zhang", "Zhiqiang", ""], ["Lian", "Defu", ""], ["Zheng", "Kai", ""]]}, {"id": "2103.00442", "submitter": "Fei Sun", "authors": "Xu Xie, Fei Sun, Xiaoyong Yang, Zhao Yang, Jinyang Gao, Wenwu Ou, and\n  Bin Cui", "title": "Explore User Neighborhood for Real-time E-commerce Recommendation", "comments": "To appear in ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recommender systems play a vital role in modern online services, such as\nAmazon and Taobao. Traditional personalized methods, which focus on user-item\n(UI) relations, have been widely applied in industrial settings, owing to their\nefficiency and effectiveness. Despite their success, we argue that these\napproaches ignore local information hidden in similar users. To tackle this\nproblem, user-based methods exploit similar user relations to make\nrecommendations in a local perspective. Nevertheless, traditional user-based\nmethods, like userKNN and matrix factorization, are intractable to be deployed\nin the real-time applications since such transductive models have to be\nrecomputed or retrained with any new interaction. To overcome this challenge,\nwe propose a framework called self-complementary collaborative filtering~(SCCF)\nwhich can make recommendations with both global and local information in real\ntime. On the one hand, it utilizes UI relations and user neighborhood to\ncapture both global and local information. On the other hand, it can identify\nsimilar users for each user in real time by inferring user representations on\nthe fly with an inductive model. The proposed framework can be seamlessly\nincorporated into existing inductive UI approach and benefit from user\nneighborhood with little additional computation. It is also the first attempt\nto apply user-based methods in real-time settings. The effectiveness and\nefficiency of SCCF are demonstrated through extensive offline experiments on\nfour public datasets, as well as a large scale online A/B test in Taobao.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 09:56:09 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xie", "Xu", ""], ["Sun", "Fei", ""], ["Yang", "Xiaoyong", ""], ["Yang", "Zhao", ""], ["Gao", "Jinyang", ""], ["Ou", "Wenwu", ""], ["Cui", "Bin", ""]]}, {"id": "2103.00498", "submitter": "He Zhao", "authors": "He Zhao, Dinh Phung, Viet Huynh, Yuan Jin, Lan Du, Wray Buntine", "title": "Topic Modelling Meets Deep Neural Networks: A Survey", "comments": "A review on Neural Topic Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Topic modelling has been a successful technique for text analysis for almost\ntwenty years. When topic modelling met deep neural networks, there emerged a\nnew and increasingly popular research area, neural topic models, with over a\nhundred models developed and a wide range of applications in neural language\nunderstanding such as text generation, summarisation and language models. There\nis a need to summarise research developments and discuss open problems and\nfuture directions. In this paper, we provide a focused yet comprehensive\noverview of neural topic models for interested researchers in the AI community,\nso as to facilitate them to navigate and innovate in this fast-growing research\narea. To the best of our knowledge, ours is the first review focusing on this\nspecific topic.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 12:59:28 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhao", "He", ""], ["Phung", "Dinh", ""], ["Huynh", "Viet", ""], ["Jin", "Yuan", ""], ["Du", "Lan", ""], ["Buntine", "Wray", ""]]}, {"id": "2103.00532", "submitter": "Muhammad Mudassar Qureshi", "authors": "Muhammad Mudassar Qureshi, Muhammad Shoaib, Kalsoom", "title": "An Efficient Indexing and Searching Technique for Information Retrieval\n  for Urdu Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing techniques are used to improve retrieval of data in response to\ncertain search condition. Inverted files are mostly used for creating indexes.\nThis paper proposes indexing technique for Urdu language. Language processing\nstep in Index creation is different for a particular language. We discuss index\ncreation steps specifically for Urdu language. We explore morphological rules\nfor Urdu language and implement these rules to create Urdu stemmer. We\nimplement our proposed technique with different implementations and compare\nresults. We suggest that indexes should be created without stop words and also\nindex file should be an order index file.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 15:09:49 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Qureshi", "Muhammad Mudassar", ""], ["Shoaib", "Muhammad", ""], ["Kalsoom", "", ""]]}, {"id": "2103.00686", "submitter": "Divya Mahajan", "authors": "Muhammad Adnan, Yassaman Ebrahimzadeh Maboud, Divya Mahajan, Prashant\n  J. Nair", "title": "High-Performance Training by Exploiting Hot-Embeddings in Recommendation\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recommendation models are commonly used learning models that suggest relevant\nitems to a user for e-commerce and online advertisement-based applications.\nCurrent recommendation models include deep-learning-based (DLRM) and time-based\nsequence (TBSM) models. These models use massive embedding tables to store a\nnumerical representation of item's and user's categorical variables\n(memory-bound) while also using neural networks to generate outputs\n(compute-bound). Due to these conflicting compute and memory requirements, the\ntraining process for recommendation models is divided across CPU and GPU for\nembedding and neural network executions, respectively. Such a training process\nnaively assigns the same level of importance to each embedding entry. This\npaper observes that some training inputs and their accesses into the embedding\ntables are heavily skewed with certain entries being accessed up to 10000x\nmore. This paper tries to leverage skewed embedded table accesses to\nefficiently use the GPU resources during training. To this end, this paper\nproposes a Frequently Accessed Embeddings (FAE) framework that exposes a\ndynamic knob to the software based on the GPU memory capacity and the input\npopularity index. This framework efficiently estimates and varies the size of\nthe hot portions of the embedding tables within GPUs and reallocates the rest\nof the embeddings on the CPU. Overall, our framework speeds-up the training of\nthe recommendation models on Kaggle, Terabyte, and Alibaba datasets by 2.34x as\ncompared to a baseline that uses Intel-Xeon CPUs and Nvidia Tesla-V100 GPUs,\nwhile maintaining accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 01:43:26 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 19:16:36 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Adnan", "Muhammad", ""], ["Maboud", "Yassaman Ebrahimzadeh", ""], ["Mahajan", "Divya", ""], ["Nair", "Prashant J.", ""]]}, {"id": "2103.00800", "submitter": "Han Zhang", "authors": "Yiming Qiu, Kang Zhang, Han Zhang, Songlin Wang, Sulong Xu, Yun Xiao,\n  Bo Long, Wen-Yun Yang", "title": "Query Rewriting via Cycle-Consistent Translation for E-Commerce Search", "comments": "12 pages, 9 figures; accepted by ICDE2021", "journal-ref": null, "doi": "10.1109/ICDE51399.2021.00276", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Nowadays e-commerce search has become an integral part of many people's\nshopping routines. One critical challenge in today's e-commerce search is the\nsemantic matching problem where the relevant items may not contain the exact\nterms in the user query. In this paper, we propose a novel deep neural network\nbased approach to query rewriting, in order to tackle this problem.\nSpecifically, we formulate query rewriting into a cyclic machine translation\nproblem to leverage abundant click log data. Then we introduce a novel cyclic\nconsistent training algorithm in conjunction with state-of-the-art machine\ntranslation models to achieve the optimal performance in terms of query\nrewriting accuracy. In order to make it practical in industrial scenarios, we\noptimize the syntax tree construction to reduce computational cost and online\nserving latency. Offline experiments show that the proposed method is able to\nrewrite hard user queries into more standard queries that are more appropriate\nfor the inverted index to retrieve. Comparing with human curated rule-based\nmethod, the proposed model significantly improves query rewriting diversity\nwhile maintaining good relevancy. Online A/B experiments show that it improves\ncore e-commerce business metrics significantly. Since the summer of 2020, the\nproposed model has been launched into our search engine production, serving\nhundreds of millions of users.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:47:12 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 08:25:19 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Qiu", "Yiming", ""], ["Zhang", "Kang", ""], ["Zhang", "Han", ""], ["Wang", "Songlin", ""], ["Xu", "Sulong", ""], ["Xiao", "Yun", ""], ["Long", "Bo", ""], ["Yang", "Wen-Yun", ""]]}, {"id": "2103.00917", "submitter": "Hung Du", "authors": "Hung Du and Yong-Bin Kang", "title": "An open-source framework for ExpFinder integrating $N$-gram Vector Space\n  Model and $\\mu$CO-HITS", "comments": "9 pages, 4 figures, \"Submitted to Software Impacts\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MS cs.SE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Finding experts drives successful collaborations and high-quality product\ndevelopment in academic and research domains. To contribute to the expert\nfinding research community, we have developed ExpFinder which is a novel\nensemble model for expert finding by integrating an $N$-gram vector space model\n($n$VSM) and a graph-based model ($\\mu$CO-HITS). This paper provides\ndescriptions of ExpFinder's architecture, key components, functionalities, and\nillustrative examples. ExpFinder is an effective and competitive model for\nexpert finding, significantly outperforming a number of expert finding models.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 11:14:01 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 22:30:50 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Du", "Hung", ""], ["Kang", "Yong-Bin", ""]]}, {"id": "2103.00956", "submitter": "Yixing Fan", "authors": "Yixing Fan, Jiafeng Guo, Xinyu Ma, Ruqing Zhang, Yanyan Lan, and Xueqi\n  Cheng", "title": "A Linguistic Study on Relevance Modeling in Information Retrieval", "comments": null, "journal-ref": null, "doi": "10.1145/3442381.3450009", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relevance plays a central role in information retrieval (IR), which has\nreceived extensive studies starting from the 20th century. The definition and\nthe modeling of relevance has always been critical challenges in both\ninformation science and computer science research areas. Along with the debate\nand exploration on relevance, IR has already become a core task in many\nreal-world applications, such as Web search engines, question answering\nsystems, conversational bots, and so on. While relevance acts as a unified\nconcept in all these retrieval tasks, the inherent definitions are quite\ndifferent due to the heterogeneity of these tasks. This raises a question to\nus: Do these different forms of relevance really lead to different modeling\nfocuses? To answer this question, in this work, we conduct an empirical study\non relevance modeling in three representative IR tasks, i.e., document\nretrieval, answer retrieval, and response retrieval. Specifically, we attempt\nto study the following two questions: 1) Does relevance modeling in these tasks\nreally show differences in terms of natural language understanding (NLU)? We\nemploy 16 linguistic tasks to probe a unified retrieval model over these three\nretrieval tasks to answer this question. 2) If there do exist differences, how\ncan we leverage the findings to enhance the relevance modeling? We proposed\nthree intervention methods to investigate how to leverage different modeling\nfocuses of relevance to improve these IR tasks. We believe the way we study the\nproblem as well as our findings would be beneficial to the IR community.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 12:32:04 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Fan", "Yixing", ""], ["Guo", "Jiafeng", ""], ["Ma", "Xinyu", ""], ["Zhang", "Ruqing", ""], ["Lan", "Yanyan", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2103.01432", "submitter": "Yong-Bin Kang", "authors": "Yong-Bin Kang and Timos Sellis", "title": "TopicTracker: A Platform for Topic Trajectory Identification and\n  Visualisation", "comments": "Submitted to SoftwareX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic trajectory information provides crucial insight into the dynamics of\ntopics and their evolutionary relationships over a given time. Also, this\ninformation can help to improve our understanding on how new topics have\nemerged or formed through a sequential or interrelated events of emergence,\nmodification and integration of prior topics. Nevertheless, the implementation\nof the existing methods for topic trajectory identification is rarely available\nas usable software. In this paper, we present TopicTracker, a platform for\ntopic trajectory identification and visualisation. The key of Topic Tracker is\nthat it can represent the three facets of information together, given two kinds\nof input: a time-stamped topic profile consisting of the set of the underlying\ntopics over time, and the evolution strength matrix among them: evolutionary\npathways of dynamic topics, evolution states of the topics, and topic\nimportance. TopicTracker is a publicly available software implemented using the\nR software.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 02:58:48 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Kang", "Yong-Bin", ""], ["Sellis", "Timos", ""]]}, {"id": "2103.01453", "submitter": "Jin Chen", "authors": "Jin Chen, Tiezheng Ge, Gangwei Jiang, Zhiqiang Zhang, Defu Lian, Kai\n  Zheng", "title": "Efficient Optimal Selection for Composited Advertising Creatives with\n  Tree Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ad creatives are one of the prominent mediums for online e-commerce\nadvertisements. Ad creatives with enjoyable visual appearance may increase the\nclick-through rate (CTR) of products. Ad creatives are typically handcrafted by\nadvertisers and then delivered to the advertising platforms for advertisement.\nIn recent years, advertising platforms are capable of instantly compositing ad\ncreatives with arbitrarily designated elements of each ingredient, so\nadvertisers are only required to provide basic materials. While facilitating\nthe advertisers, a great number of potential ad creatives can be composited,\nmaking it difficult to accurately estimate CTR for them given limited real-time\nfeedback. To this end, we propose an Adaptive and Efficient ad creative\nSelection (AES) framework based on a tree structure. The tree structure on\ncompositing ingredients enables dynamic programming for efficient ad creative\nselection on the basis of CTR. Due to limited feedback, the CTR estimator is\nusually of high variance. Exploration techniques based on Thompson sampling are\nwidely used for reducing variances of the CTR estimator, alleviating feedback\nsparsity. Based on the tree structure, Thompson sampling is adapted with\ndynamic programming, leading to efficient exploration for potential ad\ncreatives with the largest CTR. We finally evaluate the proposed algorithm on\nthe synthetic dataset and the real-world dataset. The results show that our\napproach can outperform competing baselines in terms of convergence rate and\noverall CTR.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 03:39:41 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Chen", "Jin", ""], ["Ge", "Tiezheng", ""], ["Jiang", "Gangwei", ""], ["Zhang", "Zhiqiang", ""], ["Lian", "Defu", ""], ["Zheng", "Kai", ""]]}, {"id": "2103.01472", "submitter": "Kwan Hui Lim Dr", "authors": "Jolin Shaynn-Ly Kwan, Kwan Hui Lim", "title": "TweetCOVID: A System for Analyzing Public Sentiments and Discussions\n  about COVID-19 via Twitter Activities", "comments": "Accepted to the 26th International Conference on Intelligent User\n  Interfaces (IUI'21), Demo Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has created widespread health and economical impacts,\naffecting millions around the world. To better understand these impacts, we\npresent the TweetCOVID system that offers the capability to understand the\npublic reactions to the COVID-19 pandemic in terms of their sentiments,\nemotions, topics of interest and controversial discussions, over a range of\ntime periods and locations, using public tweets. We also present three example\nuse cases that illustrates the usefulness of our proposed TweetCOVID system.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 05:00:41 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Kwan", "Jolin Shaynn-Ly", ""], ["Lim", "Kwan Hui", ""]]}, {"id": "2103.01474", "submitter": "Dong Li", "authors": "Ruoming Jin and Dong Li and Benjamin Mudrak and Jing Gao and Zhi Liu", "title": "On Estimating Recommendation Evaluation Metrics under Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Since the recent study (Krichene and Rendle 2020) done by Krichene and Rendle\non the sampling-based top-k evaluation metric for recommendation, there has\nbeen a lot of debates on the validity of using sampling to evaluate\nrecommendation algorithms. Though their work and the recent work (Li et\nal.2020) have proposed some basic approaches for mapping the sampling-based\nmetrics to their global counterparts which rank the entire set of items, there\nis still a lack of understanding and consensus on how sampling should be used\nfor recommendation evaluation. The proposed approaches either are rather\nuninformative (linking sampling to metric evaluation) or can only work on\nsimple metrics, such as Recall/Precision (Krichene and Rendle 2020; Li et al.\n2020). In this paper, we introduce a new research problem on learning the\nempirical rank distribution, and a new approach based on the estimated rank\ndistribution, to estimate the top-k metrics. Since this question is closely\nrelated to the underlying mechanism of sampling for recommendation, tackling it\ncan help better understand the power of sampling and can help resolve the\nquestions of if and how should we use sampling for evaluating recommendation.\nWe introduce two approaches based on MLE (MaximalLikelihood Estimation) and its\nweighted variants, and ME(Maximal Entropy) principals to recover the empirical\nrank distribution, and then utilize them for metrics estimation. The\nexperimental results show the advantages of using the new approaches for\nevaluating recommendation algorithms based on top-k metrics.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 05:08:21 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 06:04:29 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Jin", "Ruoming", ""], ["Li", "Dong", ""], ["Mudrak", "Benjamin", ""], ["Gao", "Jing", ""], ["Liu", "Zhi", ""]]}, {"id": "2103.01576", "submitter": "Jan Philipp Portisch", "authors": "Jan Portisch and Michael Hladik and Heiko Paulheim", "title": "FinMatcher at FinSim-2: Hypernym Detection in the Financial Services\n  Domain using Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the FinMatcher system and its results for the FinSim 2021\nshared task which is co-located with the Workshop on Financial Technology on\nthe Web (FinWeb) in conjunction with The Web Conference. The FinSim-2 shared\ntask consists of a set of concept labels from the financial services domain.\nThe goal is to find the most relevant top-level concept from a given set of\nconcepts. The FinMatcher system exploits three publicly available knowledge\ngraphs, namely WordNet, Wikidata, and WebIsALOD. The graphs are used to\ngenerate explicit features as well as latent features which are fed into a\nneural classifier to predict the closest hypernym.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 08:56:28 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Portisch", "Jan", ""], ["Hladik", "Michael", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2103.01696", "submitter": "Feng Zhu", "authors": "Feng Zhu, Yan Wang, Chaochao Chen, Jun Zhou, Longfei Li, Guanfeng Liu", "title": "Cross-Domain Recommendation: Challenges, Progress, and Prospects", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To address the long-standing data sparsity problem in recommender systems\n(RSs), cross-domain recommendation (CDR) has been proposed to leverage the\nrelatively richer information from a richer domain to improve the\nrecommendation performance in a sparser domain. Although CDR has been\nextensively studied in recent years, there is a lack of a systematic review of\nthe existing CDR approaches. To fill this gap, in this paper, we provide a\ncomprehensive review of existing CDR approaches, including challenges, research\nprogress, and future directions. Specifically, we first summarize existing CDR\napproaches into four types, including single-target CDR, multi-domain\nrecommendation, dual-target CDR, and multi-target CDR. We then present the\ndefinitions and challenges of these CDR approaches. Next, we propose a\nfull-view categorization and new taxonomies on these approaches and report\ntheir research progress in detail. In the end, we share several promising\nresearch directions in CDR.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 12:58:08 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Zhu", "Feng", ""], ["Wang", "Yan", ""], ["Chen", "Chaochao", ""], ["Zhou", "Jun", ""], ["Li", "Longfei", ""], ["Liu", "Guanfeng", ""]]}, {"id": "2103.01913", "submitter": "Krishna Srinivasan", "authors": "Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky,\n  Marc Najork", "title": "WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The milestone improvements brought about by deep representation learning and\npre-training techniques have led to large performance gains across downstream\nNLP, IR and Vision tasks. Multimodal modeling techniques aim to leverage large\nhigh-quality visio-linguistic datasets for learning complementary information\n(across image and text modalities). In this paper, we introduce the\nWikipedia-based Image Text (WIT) Dataset\n(https://github.com/google-research-datasets/wit) to better facilitate\nmultimodal, multilingual learning. WIT is composed of a curated set of 37.6\nmillion entity rich image-text examples with 11.5 million unique images across\n108 Wikipedia languages. Its size enables WIT to be used as a pretraining\ndataset for multimodal models, as we show when applied to downstream tasks such\nas image-text retrieval. WIT has four main and unique advantages. First, WIT is\nthe largest multimodal dataset by the number of image-text examples by 3x (at\nthe time of writing). Second, WIT is massively multilingual (first of its kind)\nwith coverage over 100+ languages (each of which has at least 12K examples) and\nprovides cross-lingual texts for many images. Third, WIT represents a more\ndiverse set of concepts and real world entities relative to what previous\ndatasets cover. Lastly, WIT provides a very challenging real-world test set, as\nwe empirically illustrate using an image-text retrieval task as an example.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:13:54 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 16:41:01 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Srinivasan", "Krishna", ""], ["Raman", "Karthik", ""], ["Chen", "Jiecao", ""], ["Bendersky", "Michael", ""], ["Najork", "Marc", ""]]}, {"id": "2103.01986", "submitter": "Vijay Gadepally", "authors": "El Kindi Rezig, Michael Cafarella, Vijay Gadepally", "title": "Technical Report on Data Integration and Preparation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AI application developers typically begin with a dataset of interest and a\nvision of the end analytic or insight they wish to gain from the data at hand.\nAlthough these are two very important components of an AI workflow, one often\nspends the first few weeks (sometimes months) in the phase we refer to as data\nconditioning. This step typically includes tasks such as figuring out how to\nprepare data for analytics, dealing with inconsistencies in the dataset, and\ndetermining which algorithm (or set of algorithms) will be best suited for the\napplication. Larger, faster, and messier datasets such as those from Internet\nof Things sensors, medical devices or autonomous vehicles only amplify these\nissues. These challenges, often referred to as the three Vs (volume, velocity,\nvariety) of Big Data, require low-level tools for data management, preparation\nand integration. In most applications, data can come from structured and/or\nunstructured sources and often includes inconsistencies, formatting\ndifferences, and a lack of ground-truth labels.\n  In this report, we highlight a number of tools that can be used to simplify\ndata integration and preparation steps. Specifically, we focus on data\nintegration tools and techniques, a deep dive into an exemplar data integration\ntool, and a deep-dive in the evolving field of knowledge graphs. Finally, we\nprovide readers with a list of practical steps and considerations that they can\nuse to simplify the data integration challenge. The goal of this report is to\nprovide readers with a view of state-of-the-art as well as practical tips that\ncan be used by data creators that make data integration more seamless.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 19:12:06 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Rezig", "El Kindi", ""], ["Cafarella", "Michael", ""], ["Gadepally", "Vijay", ""]]}, {"id": "2103.02259", "submitter": "Xun Yang", "authors": "Xun Yang, Yunli Wang, Cheng Chen, Qing Tan, Chuan Yu, Jian Xu,\n  Xiaoqiang Zhu", "title": "Computation Resource Allocation Solution in Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.IR cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recommender systems rely heavily on increasing computation resources to\nimprove their business goal. By deploying computation-intensive models and\nalgorithms, these systems are able to inference user interests and exhibit\ncertain ads or commodities from the candidate set to maximize their business\ngoals. However, such systems are facing two challenges in achieving their\ngoals. On the one hand, facing massive online requests, computation-intensive\nmodels and algorithms are pushing their computation resources to the limit. On\nthe other hand, the response time of these systems is strictly limited to a\nshort period, e.g. 300 milliseconds in our real system, which is also being\nexhausted by the increasingly complex models and algorithms.\n  In this paper, we propose the computation resource allocation solution (CRAS)\nthat maximizes the business goal with limited computation resources and\nresponse time. We comprehensively illustrate the problem and formulate such a\nproblem as an optimization problem with multiple constraints, which could be\nbroken down into independent sub-problems. To solve the sub-problems, we\npropose the revenue function to facilitate the theoretical analysis, and obtain\nthe optimal computation resource allocation strategy. To address the\napplicability issues, we devise the feedback control system to help our\nstrategy constantly adapt to the changing online environment. The effectiveness\nof our method is verified by extensive experiments based on the real dataset\nfrom Taobao.com. We also deploy our method in the display advertising system of\nAlibaba. The online results show that our computation resource allocation\nsolution achieves significant business goal improvement without any increment\nof computation cost, which demonstrates the efficacy of our method in real\nindustrial practice.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 08:41:43 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Yang", "Xun", ""], ["Wang", "Yunli", ""], ["Chen", "Cheng", ""], ["Tan", "Qing", ""], ["Yu", "Chuan", ""], ["Xu", "Jian", ""], ["Zhu", "Xiaoqiang", ""]]}, {"id": "2103.02280", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Andrew Yates, Sergey Feldman, Doug Downey, Arman\n  Cohan, Nazli Goharian", "title": "Simplified Data Wrangling with ir_datasets", "comments": "SIGIR 2021 Resource", "journal-ref": null, "doi": "10.1145/3404835.3463254", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing the data for Information Retrieval (IR) experiments can be\nchallenging. Dataset documentation is scattered across the Internet and once\none obtains a copy of the data, there are numerous different data formats to\nwork with. Even basic formats can have subtle dataset-specific nuances that\nneed to be considered for proper use. To help mitigate these challenges, we\nintroduce a new robust and lightweight tool (ir_datasets) for acquiring,\nmanaging, and performing typical operations over datasets used in IR. We\nprimarily focus on textual datasets used for ad-hoc search. This tool provides\nboth a Python and command line interface to numerous IR datasets and\nbenchmarks. To our knowledge, this is the most extensive tool of its kind.\nIntegrations with popular IR indexing and experimentation toolkits demonstrate\nthe tool's utility. We also provide documentation of these datasets through the\nir_datasets catalog: https://ir-datasets.com/. The catalog acts as a hub for\ninformation on datasets used in IR, providing core information about what data\neach benchmark provides as well as links to more detailed information. We\nwelcome community contributions and intend to continue to maintain and grow\nthis tool.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 09:38:36 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 13:13:47 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["MacAvaney", "Sean", ""], ["Yates", "Andrew", ""], ["Feldman", "Sergey", ""], ["Downey", "Doug", ""], ["Cohan", "Arman", ""], ["Goharian", "Nazli", ""]]}, {"id": "2103.02462", "submitter": "Lucas Chaves Lima", "authors": "Lucas Chaves Lima, Dustin Brandon Wright, Isabelle Augenstein, Maria\n  Maistro", "title": "University of Copenhagen Participation in TREC Health Misinformation\n  Track 2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we describe our participation in the TREC Health\nMisinformation Track 2020. We submitted $11$ runs to the Total Recall Task and\n13 runs to the Ad Hoc task. Our approach consists of 3 steps: (1) we create an\ninitial run with BM25 and RM3; (2) we estimate credibility and misinformation\nscores for the documents in the initial run; (3) we merge the relevance,\ncredibility and misinformation scores to re-rank documents in the initial run.\nTo estimate credibility scores, we implement a classifier which exploits\nfeatures based on the content and the popularity of a document. To compute the\nmisinformation score, we apply a stance detection approach with a pretrained\nTransformer language model. Finally, we use different approaches to merge\nscores: weighted average, the distance among score vectors and rank fusion.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 15:13:25 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Lima", "Lucas Chaves", ""], ["Wright", "Dustin Brandon", ""], ["Augenstein", "Isabelle", ""], ["Maistro", "Maria", ""]]}, {"id": "2103.02464", "submitter": "Kwan Hui Lim Dr", "authors": "Ngai Lam Ho, Kwan Hui Lim", "title": "User Preferential Tour Recommendation Based on POI-Embedding Methods", "comments": "Accepted to the 26th International Conference on Intelligent User\n  Interfaces (IUI'21), Poster Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tour itinerary planning and recommendation are challenging tasks for tourists\nin unfamiliar countries. Many tour recommenders only consider broad POI\ncategories and do not align well with users' preferences and other locational\nconstraints. We propose an algorithm to recommend personalized tours using\nPOI-embedding methods, which provides a finer representation of POI types. Our\nrecommendation algorithm will generate a sequence of POIs that optimizes time\nand locational constraints, as well as user's preferences based on past\ntrajectories from similar tourists. Our tour recommendation algorithm is\nmodelled as a word embedding model in natural language processing, coupled with\nan iterative algorithm for generating itineraries that satisfies time\nconstraints. Using a Flickr dataset of 4 cities, preliminary experimental\nresults show that our algorithm is able to recommend a relevant and accurate\nitinerary, based on measures of recall, precision and F1-scores.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 15:18:23 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Ho", "Ngai Lam", ""], ["Lim", "Kwan Hui", ""]]}, {"id": "2103.02537", "submitter": "Chen Qu", "authors": "Chen Qu, Liu Yang, Cen Chen, W. Bruce Croft, Kalpesh Krishna and Mohit\n  Iyyer", "title": "Weakly-Supervised Open-Retrieval Conversational Question Answering", "comments": "Accepted to ECIR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on Question Answering (QA) and Conversational QA (ConvQA)\nemphasize the role of retrieval: a system first retrieves evidence from a large\ncollection and then extracts answers. This open-retrieval ConvQA setting\ntypically assumes that each question is answerable by a single span of text\nwithin a particular passage (a span answer). The supervision signal is thus\nderived from whether or not the system can recover an exact match of this\nground-truth answer span from the retrieved passages. This method is referred\nto as span-match weak supervision. However, information-seeking conversations\nare challenging for this span-match method since long answers, especially\nfreeform answers, are not necessarily strict spans of any passage. Therefore,\nwe introduce a learned weak supervision approach that can identify a\nparaphrased span of the known answer in a passage. Our experiments on QuAC and\nCoQA datasets show that the span-match weak supervisor can only handle\nconversations with span answers, and has less satisfactory results for freeform\nanswers generated by people. Our method is more flexible as it can handle both\nspan answers and freeform answers. Moreover, our method can be more powerful\nwhen combined with the span-match method which shows it is complementary to the\nspan-match method. We also conduct in-depth analyses to show more insights on\nopen-retrieval ConvQA under a weak supervision setting.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 17:23:30 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Qu", "Chen", ""], ["Yang", "Liu", ""], ["Chen", "Cen", ""], ["Croft", "W. Bruce", ""], ["Krishna", "Kalpesh", ""], ["Iyyer", "Mohit", ""]]}, {"id": "2103.02590", "submitter": "Vito Walter Anelli Dr.", "authors": "Vito Walter Anelli, Alejandro Bellog\\'in, Antonio Ferrara, Daniele\n  Malitesta, Felice Antonio Merra, Claudio Pomo, Francesco Maria Donini,\n  Tommaso Di Noia", "title": "Elliot: a Comprehensive and Rigorous Framework for Reproducible\n  Recommender Systems Evaluation", "comments": "Published at SIGIR 2021, 10 pages, 1 figure", "journal-ref": "Proceedings of the 44th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval 2021", "doi": "10.1145/3404835.3463245", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender Systems have shown to be an effective way to alleviate the\nover-choice problem and provide accurate and tailored recommendations. However,\nthe impressive number of proposed recommendation algorithms, splitting\nstrategies, evaluation protocols, metrics, and tasks, has made rigorous\nexperimental evaluation particularly challenging. Puzzled and frustrated by the\ncontinuous recreation of appropriate evaluation benchmarks, experimental\npipelines, hyperparameter optimization, and evaluation procedures, we have\ndeveloped an exhaustive framework to address such needs. Elliot is a\ncomprehensive recommendation framework that aims to run and reproduce an entire\nexperimental pipeline by processing a simple configuration file. The framework\nloads, filters, and splits the data considering a vast set of strategies (13\nsplitting methods and 8 filtering approaches, from temporal training-test\nsplitting to nested K-folds Cross-Validation). Elliot optimizes hyperparameters\n(51 strategies) for several recommendation algorithms (50), selects the best\nmodels, compares them with the baselines providing intra-model statistics,\ncomputes metrics (36) spanning from accuracy to beyond-accuracy, bias, and\nfairness, and conducts statistical analysis (Wilcoxon and Paired t-test). The\naim is to provide the researchers with a tool to ease (and make them\nreproducible) all the experimental evaluation phases, from data reading to\nresults collection. Elliot is available on GitHub\n(https://github.com/sisinflab/elliot).\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:42:55 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 18:21:20 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Anelli", "Vito Walter", ""], ["Bellog\u00edn", "Alejandro", ""], ["Ferrara", "Antonio", ""], ["Malitesta", "Daniele", ""], ["Merra", "Felice Antonio", ""], ["Pomo", "Claudio", ""], ["Donini", "Francesco Maria", ""], ["Di Noia", "Tommaso", ""]]}, {"id": "2103.02735", "submitter": "Lequn Luke Wang", "authors": "Lequn Wang, Yiwei Bai, Wen Sun, Thorsten Joachims", "title": "Fairness of Exposure in Stochastic Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandit algorithms have become widely used for recommendation in\nonline systems (e.g. marketplaces, music streaming, news), where they now wield\nsubstantial influence on which items get exposed to the users. This raises\nquestions of fairness to the items -- and to the sellers, artists, and writers\nthat benefit from this exposure. We argue that the conventional bandit\nformulation can lead to an undesirable and unfair winner-takes-all allocation\nof exposure. To remedy this problem, we propose a new bandit objective that\nguarantees merit-based fairness of exposure to the items while optimizing\nutility to the users. We formulate fairness regret and reward regret in this\nsetting, and present algorithms for both stochastic multi-armed bandits and\nstochastic linear bandits. We prove that the algorithms achieve sub-linear\nfairness regret and reward regret. Beyond the theoretical analysis, we also\nprovide empirical evidence that these algorithms can fairly allocate exposure\nto different arms effectively.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 22:50:29 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Wang", "Lequn", ""], ["Bai", "Yiwei", ""], ["Sun", "Wen", ""], ["Joachims", "Thorsten", ""]]}, {"id": "2103.02866", "submitter": "Shalini Pandey", "authors": "Shalini Pandey, George Karypis and Jaideep Srivasatava", "title": "IACN: Influence-aware and Attention-based Co-evolutionary Network for\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recommending relevant items to users is a crucial task on online communities\nsuch as Reddit and Twitter. For recommendation system, representation learning\npresents a powerful technique that learns embeddings to represent user\nbehaviors and capture item properties. However, learning embeddings on online\ncommunities is a challenging task because the user interest keep evolving. This\nevolution can be captured from 1) interaction between user and item, 2)\ninfluence from other users in the community. The existing dynamic embedding\nmodels only consider either of the factors to update user embeddings. However,\nat a given time, user interest evolves due to a combination of the two factors.\nTo this end, we propose Influence-aware and Attention-based Co-evolutionary\nNetwork (IACN). Essentially, IACN consists of two key components: interaction\nmodeling and influence modeling layer. The interaction modeling layer is\nresponsible for updating the embedding of a user and an item when the user\ninteracts with the item. The influence modeling layer captures the temporal\nexcitation caused by interactions of other users. To integrate the signals\nobtained from the two layers, we design a novel fusion layer that effectively\ncombines interaction-based and influence-based embeddings to predict final user\nembedding. Our model outperforms the existing state-of-the-art models from\nvarious domains.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 07:08:20 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Pandey", "Shalini", ""], ["Karypis", "George", ""], ["Srivasatava", "Jaideep", ""]]}, {"id": "2103.02900", "submitter": "Isayas Wakgari", "authors": "Isayas Wakgari Kelbessa", "title": "The effects of having lists of synonyms on the performance of Afaan\n  Oromo Text Retrieval system", "comments": "13 pages, 3 figures,3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Obtaining relevant information from a collection of informational resources\nin Afaan Oromo is very important for Afaan Oromo speakers, developing a system\nthat help users of Afaan Oromo is mandatory. That is why this study is\nenvisioned to make possible retrieval of Afaan Oromo text documents by applying\ntechniques of modern information retrieval system. In the developed Afaan Oromo\nprototype, Probabilistic approach was used as an information retrieval models\nand precision and recall measurement were used as the performance measurement\nor evaluation technique. Apache Solr was also used as an environmental\nprogramming language to achieve the evaluation goal. Afaan Oromo text retrieval\nis evaluated using 158 documents and 13 arbitrarily selected queries that can\ndetermine the effectiveness of retrieval using the precision-recall. The\naverage result obtained by our evaluation before the addition of synonymy was\n72.91% precision and 86.8% recall respectively. After the addition of synonymy,\nthe value was changed to 71.39% average precision and 90.5% average recall. The\nF-measure for the evaluation before synonymy addition was 79.25% and after\naddition changed to 79.82%. The addition of synonymy improves the system\nperformance by 0.57%. The study therefore, experimentally proves that the\naddition of the thesaurus system can improve the system performance.\nSpellchecking, pagination, hit highlighting and autosuggestion is also possible\nin the developed prototype for Afaan Oromo.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 09:04:36 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Kelbessa", "Isayas Wakgari", ""]]}, {"id": "2103.03223", "submitter": "Tobias Schumacher", "authors": "Tobias Schumacher, Markus Strohmaier, Florian Lemmerich", "title": "A Comparative Evaluation of Quantification Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantification represents the problem of predicting class distributions in a\ngiven target set. It also represents a growing research field in supervised\nmachine learning, for which a large variety of different algorithms has been\nproposed in recent years. However, a comprehensive empirical comparison of\nquantification methods that supports algorithm selection is not available yet.\nIn this work, we close this research gap by conducting a thorough empirical\nperformance comparison of 24 different quantification methods. To consider a\nbroad range of different scenarios for binary as well as multiclass\nquantification settings, we carried out almost 3 million experimental runs on\n40 data sets. We observe that no single algorithm generally outperforms all\ncompetitors, but identify a group of methods including the Median Sweep and the\nDyS framework that perform significantly better in binary settings. For the\nmulticlass setting, we observe that a different, broad group of algorithms\nyields good performance, including the Generalized Probabilistic Adjusted\nCount, the readme method, the energy distance minimization method, the EM\nalgorithm for quantification, and Friedman's method. More generally, we find\nthat the performance on multiclass quantification is inferior to the results\nobtained in the binary setting. Our results can guide practitioners who intend\nto apply quantification algorithms and help researchers to identify\nopportunities for future research.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:51:06 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Schumacher", "Tobias", ""], ["Strohmaier", "Markus", ""], ["Lemmerich", "Florian", ""]]}, {"id": "2103.03335", "submitter": "Leonid Boytsov", "authors": "Iurii Mokrii, Leonid Boytsov, Pavel Braslavski", "title": "A Systematic Evaluation of Transfer Learning and Pseudo-labeling with\n  BERT-based Ranking Models", "comments": null, "journal-ref": "SIGIR 2021 (44th International ACM SIGIR Conference on Research\n  and Development in Information Retrieval)", "doi": "10.1145/3404835.3463093", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to high annotation costs making the best use of existing human-created\ntraining data is an important research direction. We, therefore, carry out a\nsystematic evaluation of transferability of BERT-based neural ranking models\nacross five English datasets. Previous studies focused primarily on zero-shot\nand few-shot transfer from a large dataset to a dataset with a small number of\nqueries. In contrast, each of our collections has a substantial number of\nqueries, which enables a full-shot evaluation mode and improves reliability of\nour results. Furthermore, since source datasets licences often prohibit\ncommercial use, we compare transfer learning to training on pseudo-labels\ngenerated by a BM25 scorer. We find that training on pseudo-labels -- possibly\nwith subsequent fine-tuning using a modest number of annotated queries -- can\nproduce a competitive or better model compared to transfer learning. Yet, it is\nnecessary to improve the stability and/or effectiveness of the few-shot\ntraining, which, sometimes, can degrade performance of a pretrained model.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 21:08:06 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 16:34:14 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 03:18:49 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Mokrii", "Iurii", ""], ["Boytsov", "Leonid", ""], ["Braslavski", "Pavel", ""]]}, {"id": "2103.03578", "submitter": "Xiaoguang Li", "authors": "Chang Liu, Xiaoguang Li, Guohao Cai, Zhenhua Dong, Hong Zhu, Lifeng\n  Shang", "title": "Non-invasive Self-attention for Side Information Fusion in Sequential\n  Recommendation", "comments": "Accepted at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential recommender systems aim to model users' evolving interests from\ntheir historical behaviors, and hence make customized time-relevant\nrecommendations. Compared with traditional models, deep learning approaches\nsuch as CNN and RNN have achieved remarkable advancements in recommendation\ntasks. Recently, the BERT framework also emerges as a promising method,\nbenefited from its self-attention mechanism in processing sequential data.\nHowever, one limitation of the original BERT framework is that it only\nconsiders one input source of the natural language tokens. It is still an open\nquestion to leverage various types of information under the BERT framework.\nNonetheless, it is intuitively appealing to utilize other side information,\nsuch as item category or tag, for more comprehensive depictions and better\nrecommendations. In our pilot experiments, we found naive approaches, which\ndirectly fuse types of side information into the item embeddings, usually bring\nvery little or even negative effects. Therefore, in this paper, we propose the\nNOninVasive self-attention mechanism (NOVA) to leverage side information\neffectively under the BERT framework. NOVA makes use of side information to\ngenerate better attention distribution, rather than directly altering the item\nembedding, which may cause information overwhelming. We validate the NOVA-BERT\nmodel on both public and commercial datasets, and our method can stably\noutperform the state-of-the-art models with negligible computational overheads.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 10:28:49 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Liu", "Chang", ""], ["Li", "Xiaoguang", ""], ["Cai", "Guohao", ""], ["Dong", "Zhenhua", ""], ["Zhu", "Hong", ""], ["Shang", "Lifeng", ""]]}, {"id": "2103.03587", "submitter": "Paula Gomez Duran", "authors": "Paula G\\'omez Duran, Alexandros Karatzoglou, Jordi Vitri\\`a, Xin Xin,\n  Ioannis Arapakis", "title": "Graph Convolutional Embeddings for Recommender Systems", "comments": "10 pages, 4 figures, SIGIR July 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern recommender systems (RS) work by processing a number of signals that\ncan be inferred from large sets of user-item interaction data. The main signal\nto analyze stems from the raw matrix that represents interactions. However, we\ncan increase the performance of RS by considering other kinds of signals like\nthe context of interactions, which could be, for example, the time or date of\nthe interaction, the user location, or sequential data corresponding to the\nhistorical interactions of the user with the system. These complex,\ncontext-based interaction signals are characterized by a rich relational\nstructure that can be represented by a multi-partite graph. Graph Convolutional\nNetworks (GCNs) have been used successfully in collaborative filtering with\nsimple user-item interaction data. In this work, we generalize the use of GCNs\nfor N-partite graphs by considering N multiple context dimensions and propose a\nsimple way for their seamless integration in modern deep learning RS\narchitectures. More specifically, we define a graph convolutional embedding\nlayer for N-partite graphs that processes user-item-context interactions, and\nconstructs node embeddings by leveraging their relational structure.\nExperiments on several datasets from recommender systems to drug re-purposing\nshow the benefits of the introduced GCN embedding layer by measuring the\nperformance of different context-enriched tasks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 10:46:16 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Duran", "Paula G\u00f3mez", ""], ["Karatzoglou", "Alexandros", ""], ["Vitri\u00e0", "Jordi", ""], ["Xin", "Xin", ""], ["Arapakis", "Ioannis", ""]]}, {"id": "2103.03756", "submitter": "Andrew Tristan", "authors": "Andrew Tristan, Vinicius Woloszyn and Ben Kaden", "title": "BOPI: A Programming Interface For Reuse Of Research Data Available On\n  DSpace Repositories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A recent study showed that more than 70% of researchers fail to reproduce\ntheir peers's experiments and more than half fail to reproduce their own\nexperiments. Obviously, from a perspective of scientific quality this is a more\nthan unsatisfying numbers. One approach to mitigate this flaw lies in the\ntransparent provision of relevant research data to increase the base of\navailable material to evaluate and possibly reconduct experiments. However,\nsuch data needs to be presented and accessed in a findable and purposefully\nusable way. In this work, we report the development of a programming interface\nto enhance findability and accessibility of research data (available in DSpace\nsystems) and hence reproducibility of scientific experiments with data. This\ninterface allows researchers to (i) find research data in multiples languages\ntrough automatic translation of metadata; (ii) display a preview of data\nwithout download it beforehand; (iii) provide a detailed statistics of the data\nwith interactive graphs for quality assessment; (iv) automatic download of data\ndirectly from Python-based experiments. Usability tests revealed that this\ninterface improves the effectiveness, efficiency and satisfaction during the\nreuse of research data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 15:32:51 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 09:02:12 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Tristan", "Andrew", ""], ["Woloszyn", "Vinicius", ""], ["Kaden", "Ben", ""]]}, {"id": "2103.04083", "submitter": "Muhao Chen", "authors": "Changping Meng, Muhao Chen, Jie Mao, Jennifer Neville", "title": "ReadNet: A Hierarchical Transformer Framework for Web Article\n  Readability Analysis", "comments": "ECIR 2020", "journal-ref": null, "doi": "10.1007/978-3-030-45439-5_3", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing the readability of articles has been an important sociolinguistic\ntask. Addressing this task is necessary to the automatic recommendation of\nappropriate articles to readers with different comprehension abilities, and it\nfurther benefits education systems, web information systems, and digital\nlibraries. Current methods for assessing readability employ empirical measures\nor statistical learning techniques that are limited by their ability to\ncharacterize complex patterns such as article structures and semantic meanings\nof sentences. In this paper, we propose a new and comprehensive framework which\nuses a hierarchical self-attention model to analyze document readability. In\nthis model, measurements of sentence-level difficulty are captured along with\nthe semantic meanings of each sentence. Additionally, the sentence-level\nfeatures are incorporated to characterize the overall readability of an article\nwith consideration of article structures. We evaluate our proposed approach on\nthree widely-used benchmark datasets against several strong baseline\napproaches. Experimental results show that our proposed method achieves the\nstate-of-the-art performance on estimating the readability for various web\narticles and literature.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 09:42:23 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Meng", "Changping", ""], ["Chen", "Muhao", ""], ["Mao", "Jie", ""], ["Neville", "Jennifer", ""]]}, {"id": "2103.04156", "submitter": "Eleni Partalidou", "authors": "Eleni Partalidou, Despina Christou and Grigorios Tsoumakas", "title": "Improving Zero-Shot Entity Retrieval through Effective Dense\n  Representations", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity Linking (EL) seeks to align entity mentions in text to entries in a\nknowledge-base and is usually comprised of two phases: candidate generation and\ncandidate ranking. While most methods focus on the latter, it is the candidate\ngeneration phase that sets an upper bound to both time and accuracy performance\nof the overall EL system. This work's contribution is a significant improvement\nin candidate generation which thus raises the performance threshold for EL, by\ngenerating candidates that include the gold entity in the least candidate set\n(top-K). We propose a simple approach that efficiently embeds mention-entity\npairs in dense space through a BERT-based bi-encoder. Specifically, we extend\n(Wu et al., 2020) by introducing a new pooling function and incorporating\nentity type side-information. We achieve a new state-of-the-art 84.28% accuracy\non top-50 candidates on the Zeshel dataset, compared to the previous 82.06% on\nthe top-64 of (Wu et al., 2020). We report the results from extensive\nexperimentation using our proposed model on both seen and unseen entity\ndatasets. Our results suggest that our method could be a useful complement to\nexisting EL approaches.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 17:00:09 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Partalidou", "Eleni", ""], ["Christou", "Despina", ""], ["Tsoumakas", "Grigorios", ""]]}, {"id": "2103.04390", "submitter": "Abdul Hameed Azeemi", "authors": "Abdul Hameed Azeemi, Muhammad Hamza Sohail, Talha Zubair, Muaz\n  Maqbool, Irfan Younas, Omair Shafiq", "title": "RevDet: Robust and Memory Efficient Event Detection and Tracking in\n  Large News Feeds", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the ever-growing volume of online news feeds, event-based organization\nof news articles has many practical applications including better information\nnavigation and the ability to view and analyze events as they develop.\nAutomatically tracking the evolution of events in large news corpora still\nremains a challenging task, and the existing techniques for Event Detection and\nTracking do not place a particular focus on tracking events in very large and\nconstantly updating news feeds. Here, we propose a new method for robust and\nefficient event detection and tracking, which we call RevDet algorithm. RevDet\nadopts an iterative clustering approach for tracking events. Even though many\nevents continue to develop for many days or even months, RevDet is able to\ndetect and track those events while utilizing only a constant amount of space\non main memory. We also devise a redundancy removal strategy which effectively\neliminates duplicate news articles and substantially reduces the size of data.\nWe construct a large, comprehensive new ground truth dataset specifically for\nevent detection and tracking approaches by augmenting two existing datasets:\nw2e and GDELT. We implement RevDet algorithm and evaluate its performance on\nthe ground truth event chains. We discover that our algorithm is able to\naccurately recover event chains in the ground-truth dataset. We also compare\nthe memory efficiency of our algorithm with the standard single pass clustering\napproach, and demonstrate the appropriateness of our algorithm for event\ndetection and tracking task in large news feeds.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 16:20:44 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Azeemi", "Abdul Hameed", ""], ["Sohail", "Muhammad Hamza", ""], ["Zubair", "Talha", ""], ["Maqbool", "Muaz", ""], ["Younas", "Irfan", ""], ["Shafiq", "Omair", ""]]}, {"id": "2103.04437", "submitter": "Alex Brandsen", "authors": "Alex Brandsen, Suzan Verberne, Karsten Lambers, Milco Wansleeben", "title": "Usability Evaluation for Online Professional Search in the Dutch\n  Archaeology Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents AGNES, the first information retrieval system for\narchaeological grey literature, allowing full-text search of these long\narchaeological documents.\n  This search system has a web interface that allows archaeology professionals\nand scholars to search through a collection of over 60,000 Dutch excavation\nreports, totalling 361 million words. We conducted a user study for the\nevaluation of AGNES's search interface, with a small but diverse user group.\nThe evaluation was done by screen capturing and a think aloud protocol,\ncombined with a user interface feedback questionnaire. The evaluation covered\nboth controlled use (completion of a pre-defined task) as well as free use\n(completion of a freely chosen task). The free use allows us to study the\ninformation needs of archaeologists, as well as their interactions with the\nsearch system. We conclude that: (1) the information needs of archaeologists\nare typically recall-oriented, often requiring a list of items as answer; (2)\nthe users prefer the use of free-text queries over metadata filters, confirming\nthe value of a free-text search system; (3) the compilation of a diverse user\ngroup contributed to the collection of diverse issues as feedback for improving\nthe system. We are currently refining AGNES's user interface and improving its\nprecision for archaeological entities, so that AGNES will help archaeologists\nto answer their research questions more effectively and efficiently, leading to\na more coherent narrative of the past.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 19:48:35 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Brandsen", "Alex", ""], ["Verberne", "Suzan", ""], ["Lambers", "Karsten", ""], ["Wansleeben", "Milco", ""]]}, {"id": "2103.04668", "submitter": "Rion Brattig Correia", "authors": "Tiago Simas and Rion Brattig Correia and Luis M. Rocha", "title": "The distance backbone of complex networks", "comments": "To appear in the Journal of Complex Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.IR q-bio.QM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Redundancy needs more precise characterization as it is a major factor in the\nevolution and robustness of networks of multivariate interactions. We\ninvestigate the complexity of such interactions by inferring a connection\ntransitivity that includes all possible measures of path length for weighted\ngraphs. The result, without breaking the graph into smaller components, is a\ndistance backbone subgraph sufficient to compute all shortest paths. This is\nimportant for understanding the dynamics of spread and communication phenomena\nin real-world networks. The general methodology we formally derive yields a\nprincipled graph reduction technique and provides a finer characterization of\nthe triangular geometry of all edges -- those that contribute to shortest paths\nand those that do not but are involved in other network phenomena. We\ndemonstrate that the distance backbone is very small in large networks across\ndomains ranging from air traffic to the human brain connectome, revealing that\nnetwork robustness to attacks and failures seems to stem from surprisingly vast\namounts of redundancy.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 11:08:59 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 16:24:33 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Simas", "Tiago", ""], ["Correia", "Rion Brattig", ""], ["Rocha", "Luis M.", ""]]}, {"id": "2103.04808", "submitter": "Alberto Parravicini", "authors": "Alberto Parravicini, Luca Giuseppe Cellamare, Marco Siracusa, Marco\n  Domenico Santambrogio", "title": "Scaling up HBM Efficiency of Top-K SpMV for Approximate Embedding\n  Similarity on FPGAs", "comments": "To appear in Proceedings of the 58th Design Automation Conference\n  (DAC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Top-K SpMV is a key component of similarity-search on sparse embeddings. This\nsparse workload does not perform well on general-purpose NUMA systems that\nemploy traditional caching strategies. Instead, modern FPGA accelerator cards\nhave a few tricks up their sleeve. We introduce a Top-K SpMV FPGA design that\nleverages reduced precision and a novel packet-wise CSR matrix compression,\nenabling custom data layouts and delivering bandwidth efficiency often\nunreachable even in architectures with higher peak bandwidth. With HBM-based\nboards, we are 100x faster than a multi-threaded CPU implementation and 2x\nfaster than a GPU with 20% higher bandwidth, with 14.2x higher\npower-efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:09:40 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Parravicini", "Alberto", ""], ["Cellamare", "Luca Giuseppe", ""], ["Siracusa", "Marco", ""], ["Santambrogio", "Marco Domenico", ""]]}, {"id": "2103.04831", "submitter": "Yinqiong Cai", "authors": "Yinqiong Cai, Yixing Fan, Jiafeng Guo, Fei Sun, Ruqing Zhang, Xueqi\n  Cheng", "title": "Semantic Models for the First-stage Retrieval: A Comprehensive Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multi-stage ranking pipelines have been a practical solution in modern search\nsystems, where the first-stage retrieval is to return a subset of candidate\ndocuments, and the latter stages attempt to re-rank those candidates. Unlike\nthe re-ranking stages going through quick technique shifts during the past\ndecades, the first-stage retrieval has long been dominated by classical\nterm-based models. Unfortunately, these models suffer from the vocabulary\nmismatch problem, which may block the re-ranking stages from relevant documents\nat the very beginning. Therefore, it has been a long-term desire to build\nsemantic models for the first-stage retrieval that can achieve high recall\nefficiently. Recently, we have witnessed an explosive growth of research\ninterests on the first-stage semantic retrieval models. We believe it is the\nright time to survey the current status, learn from existing methods, and gain\nsome insights for future development. In this paper, we describe the current\nlandscape of semantic retrieval models from three major paradigms, paying\nspecial attention to recent neural-based methods. We review the benchmark\ndatasets, optimization methods and evaluation metrics, and summarize the\nstate-of-the-art models. We also discuss the unresolved challenges and suggest\npotentially promising directions for future work.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:36:09 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 05:58:03 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Cai", "Yinqiong", ""], ["Fan", "Yixing", ""], ["Guo", "Jiafeng", ""], ["Sun", "Fei", ""], ["Zhang", "Ruqing", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2103.05248", "submitter": "Mo Zhou", "authors": "Mo Zhou, Le Wang, Zhenxing Niu, Qilin Zhang, Yinghui Xu, Nanning\n  Zheng, Gang Hua", "title": "Practical Relative Order Attack in Deep Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies unveil the vulnerabilities of deep ranking models, where an\nimperceptible perturbation can trigger dramatic changes in the ranking result.\nWhile previous attempts focus on manipulating absolute ranks of certain\ncandidates, the possibility of adjusting their relative order remains\nunder-explored. In this paper, we formulate a new adversarial attack against\ndeep ranking systems, i.e., the Order Attack, which covertly alters the\nrelative order among a selected set of candidates according to an\nattacker-specified permutation, with limited interference to other unrelated\ncandidates. Specifically, it is formulated as a triplet-style loss imposing an\ninequality chain reflecting the specified permutation. However, direct\noptimization of such white-box objective is infeasible in a real-world attack\nscenario due to various black-box limitations. To cope with them, we propose a\nShort-range Ranking Correlation metric as a surrogate objective for black-box\nOrder Attack to approximate the white-box method. The Order Attack is evaluated\non the Fashion-MNIST and Stanford-Online-Products datasets under both white-box\nand black-box threat models. The black-box attack is also successfully\nimplemented on a major e-commerce platform. Comprehensive experimental\nevaluations demonstrate the effectiveness of the proposed methods, revealing a\nnew type of ranking model vulnerability.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 06:41:18 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 01:16:13 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 12:18:44 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhou", "Mo", ""], ["Wang", "Le", ""], ["Niu", "Zhenxing", ""], ["Zhang", "Qilin", ""], ["Xu", "Yinghui", ""], ["Zheng", "Nanning", ""], ["Hua", "Gang", ""]]}, {"id": "2103.05256", "submitter": "Shahrzad Naseri", "authors": "Shahrzad Naseri, Jeffrey Dalton, Andrew Yates, James Allan", "title": "CEQE: Contextualized Embeddings for Query Expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work we leverage recent advances in context-sensitive language models\nto improve the task of query expansion. Contextualized word representation\nmodels, such as ELMo and BERT, are rapidly replacing static embedding models.\nWe propose a new model, Contextualized Embeddings for Query Expansion (CEQE),\nthat utilizes query-focused contextualized embedding vectors. We study the\nbehavior of contextual representations generated for query expansion in ad-hoc\ndocument retrieval. We conduct our experiments on probabilistic retrieval\nmodels as well as in combination with neural ranking models. We evaluate CEQE\non two standard TREC collections: Robust and Deep Learning. We find that CEQE\noutperforms static embedding-based expansion methods on multiple collections\n(by up to 18% on Robust and 31% on Deep Learning on average precision) and also\nimproves over proven probabilistic pseudo-relevance feedback (PRF) models. We\nfurther find that multiple passes of expansion and reranking result in\ncontinued gains in effectiveness with CEQE-based approaches outperforming other\napproaches. The final model incorporating neural and CEQE-based expansion score\nachieves gains of up to 5% in P@20 and 2% in AP on Robust over the\nstate-of-the-art transformer-based re-ranking model, Birch.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 07:00:48 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Naseri", "Shahrzad", ""], ["Dalton", "Jeffrey", ""], ["Yates", "Andrew", ""], ["Allan", "James", ""]]}, {"id": "2103.05457", "submitter": "Jayaprakash Akula", "authors": "Jayaprakash A, Abhishek, Rishabh Dabral, Ganesh Ramakrishnan, Preethi\n  Jyothi", "title": "Rudder: A Cross Lingual Video and Text Retrieval Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video retrieval using natural language queries requires learning semantically\nmeaningful joint embeddings between the text and the audio-visual input. Often,\nsuch joint embeddings are learnt using pairwise (or triplet) contrastive loss\nobjectives which cannot give enough attention to 'difficult-to-retrieve'\nsamples during training. This problem is especially pronounced in data-scarce\nsettings where the data is relatively small (10% of the large scale MSR-VTT) to\ncover the rather complex audio-visual embedding space. In this context, we\nintroduce Rudder - a multilingual video-text retrieval dataset that includes\naudio and textual captions in Marathi, Hindi, Tamil, Kannada, Malayalam and\nTelugu. Furthermore, we propose to compensate for data scarcity by using domain\nknowledge to augment supervision. To this end, in addition to the conventional\nthree samples of a triplet (anchor, positive, and negative), we introduce a\nfourth term - a partial - to define a differential margin based partialorder\nloss. The partials are heuristically sampled such that they semantically lie in\nthe overlap zone between the positives and the negatives, thereby resulting in\nbroader embedding coverage. Our proposals consistently outperform the\nconventional max-margin and triplet losses and improve the state-of-the-art on\nMSR-VTT and DiDeMO datasets. We report benchmark results on Rudder while also\nobserving significant gains using the proposed partial order loss, especially\nwhen the language specific retrieval models are jointly trained by availing the\ncross-lingual alignment across the language-specific datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 14:50:01 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["A", "Jayaprakash", ""], ["Abhishek", "", ""], ["Dabral", "Rishabh", ""], ["Ramakrishnan", "Ganesh", ""], ["Jyothi", "Preethi", ""]]}, {"id": "2103.05563", "submitter": "Ahmet Orun", "authors": "Ahmet Orun", "title": "Low-level cognitive skill transfer between two individuals' minds via\n  computer game-based framework", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The novel technique introduced here aims to accomplish the first stage of\ntransferring low-level cognitive skills between two individuals (e.g. from\nexpert to learner) to ease the consecutive higher level declarative learning\nprocess for the target \"learner\" individual in a game environment. Such\nlow-level cognitive skill is associated with the procedural knowledge and\nestablished at low-level of mind which can be unveiled and transferred by only\na novel technique (rather than by a traditional educational environment ) like\na highly interactive computer game domain in which a user exposes his/her\nunconscious mind behaviors via the game-hero non-deliberately during the game\nsessions. The cognitive data exposed by the game-hero would be recorded, and\nthen be modelled by the artificial intelligence technique like Bayesian\nnetworks for an early stage of cognitive skill transfer and the cognitive\nstimuli are also generated to be used as game agents to train the learner.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 01:52:16 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Orun", "Ahmet", ""]]}, {"id": "2103.05673", "submitter": "Tomas Sousa Pereira", "authors": "Tomas Sousa-Pereira, Tiago Cunha, Carlos Soares", "title": "u-cf2vec: Representation Learning for Personalized Algorithm Selection\n  in Recommender Systems", "comments": null, "journal-ref": "2020 International Conference on Data Mining Workshops (ICDMW)", "doi": "10.1109/ICDMW51313.2020.00034", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collaborative Filtering (CF) has become the standard approach to solve\nrecommendation systems (RS) problems. Collaborative Filtering algorithms try to\nmake predictions about interests of a user by collecting the personal interests\nfrom multiple users. There are multiple CF algorithms, each one of them with\nits own biases. It is the Machine Learning practitioner that has to choose the\nbest algorithm for each task beforehand. In Recommender Systems, different\nalgorithms have different performance for different users within the same\ndataset. Meta Learning (MtL) has been used to choose the best algorithm for a\ngiven problem. Meta Learning is usually applied to select algorithms for a\nwhole dataset. Adapting it to select the to the algorithm for a single user in\na RS involves several challenges. The most important is the design of the\nmetafeatures which, in typical meta learning, characterize datasets while here,\nthey must characterize a single user. This work presents a new meta-learning\nbased framework named $\\mu$-cf2vec to select the best algorithm for each user.\nWe propose using Representation Learning techniques to extract the\nmetafeatures. Representation Learning tries to extract representations that can\nbe reused in other learning tasks. In this work we also implement the framework\nusing different RL techniques to evaluate which one can be more useful to solve\nthis task. In the meta level, the meta learning model will use the metafeatures\nto extract knowledge that will be used to predict the best algorithm for each\nuser. We evaluated an implementation of this framework using MovieLens 20M\ndataset. Our implementation achieved consistent gains in the meta level,\nhowever, in the base level we only achieved marginal gains.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:20:25 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Sousa-Pereira", "Tomas", ""], ["Cunha", "Tiago", ""], ["Soares", "Carlos", ""]]}, {"id": "2103.05923", "submitter": "XinZhou Dong", "authors": "Xinzhou Dong, Beihong Jin, Wei Zhuo, Beibei Li, Taofeng Xue", "title": "Improving Sequential Recommendation with Attribute-augmented Graph\n  Neural Networks", "comments": null, "journal-ref": "The 25th Pacific-Asia Conference on Knowledge Discovery and Data\n  Mining (PAKDD-2021), May 11-14, 2021, Delhi, India", "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many practical recommender systems provide item recommendation for different\nusers only via mining user-item interactions but totally ignoring the rich\nattribute information of items that users interact with. In this paper, we\npropose an attribute-augmented graph neural network model named Murzim. Murzim\ntakes as input the graphs constructed from the user-item interaction sequences\nand corresponding item attribute sequences. By combining the GNNs with node\naggregation and an attention network, Murzim can capture user preference\npatterns, generate embeddings for user-item interaction sequences, and then\ngenerate recommendations through next-item prediction. We conduct extensive\nexperiments on multiple datasets. Experimental results show that Murzim\noutperforms several state-of-the-art methods in terms of recall and MRR, which\nillustrates that Murzim can make use of item attribute information to produce\nbetter recommendations. At present, Murzim has been deployed in MX Player, one\nof India's largest streaming platforms, and is recommending videos for tens of\nthousands of users.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 08:29:49 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Dong", "Xinzhou", ""], ["Jin", "Beihong", ""], ["Zhuo", "Wei", ""], ["Li", "Beibei", ""], ["Xue", "Taofeng", ""]]}, {"id": "2103.06105", "submitter": "Zi-Yuan Hu", "authors": "Zi-Yuan Hu, Jin Huang, Zhi-Hong Deng, Chang-Dong Wang, Ling Huang,\n  Jian-Huang Lai and Philip S. Yu", "title": "BCFNet: A Balanced Collaborative Filtering Network with Attention\n  Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Filtering (CF) based recommendation methods have been widely\nstudied, which can be generally categorized into two types, i.e.,\nrepresentation learning-based CF methods and matching function learning-based\nCF methods. Representation learning tries to learn a common low dimensional\nspace for the representations of users and items. In this case, a user and item\nmatch better if they have higher similarity in that common space. Matching\nfunction learning tries to directly learn the complex matching function that\nmaps user-item pairs to matching scores. Although both methods are well\ndeveloped, they suffer from two fundamental flaws, i.e., the representation\nlearning resorts to applying a dot product which has limited expressiveness on\nthe latent features of users and items, while the matching function learning\nhas weakness in capturing low-rank relations. To overcome such flaws, we\npropose a novel recommendation model named Balanced Collaborative Filtering\nNetwork (BCFNet), which has the strengths of the two types of methods. In\naddition, an attention mechanism is designed to better capture the hidden\ninformation within implicit feedback and strengthen the learning ability of the\nneural network. Furthermore, a balance module is designed to alleviate the\nover-fitting issue in DNNs. Extensive experiments on eight real-world datasets\ndemonstrate the effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 14:59:23 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 11:30:34 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Hu", "Zi-Yuan", ""], ["Huang", "Jin", ""], ["Deng", "Zhi-Hong", ""], ["Wang", "Chang-Dong", ""], ["Huang", "Ling", ""], ["Lai", "Jian-Huang", ""], ["Yu", "Philip S.", ""]]}, {"id": "2103.06109", "submitter": "Deng-Cheng Yan", "authors": "Dengcheng Yan, Tianyi Tang, Wenxin Xie, Yiwen Zhang, Qiang He", "title": "Session-based Social and Dependency-aware Software Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the increase of complexity of modern software, social collaborative\ncoding and reuse of open source software packages become more and more popular,\nwhich thus greatly enhances the development efficiency and software quality.\nHowever, the explosive growth of open source software packages exposes\ndevelopers to the challenge of information overload. While this can be\naddressed by conventional recommender systems, they usually do not consider\nparticular constraints of social coding such as social influence among\ndevelopers and dependency relations among software packages. In this paper, we\naim to model the dynamic interests of developers with both social influence and\ndependency constraints, and propose the Session-based Social and\nDependency-aware software Recommendation (SSDRec) model. This model integrates\nrecurrent neural network (RNN) and graph attention network (GAT) into a unified\nframework. A RNN is employed to model the short-term dynamic interests of\ndevelopers in each session and two GATs are utilized to capture social\ninfluence from friends and dependency constraints from dependent software\npackages, respectively. Extensive experiments are conducted on real-world\ndatasets and the results demonstrate that our model significantly outperforms\nthe competitive baselines.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 15:03:48 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 09:01:45 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Yan", "Dengcheng", ""], ["Tang", "Tianyi", ""], ["Xie", "Wenxin", ""], ["Zhang", "Yiwen", ""], ["He", "Qiang", ""]]}, {"id": "2103.06124", "submitter": "Aditya Desai", "authors": "Aditya Desai, Yanzhou Pan, Kuangyuan Sun, Li Chou, Anshumali\n  Shrivastava", "title": "Semantically Constrained Memory Allocation (SCMA) for Embedding in\n  Efficient Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based models are utilized to achieve state-of-the-art\nperformance for recommendation systems. A key challenge for these models is to\nwork with millions of categorical classes or tokens. The standard approach is\nto learn end-to-end, dense latent representations or embeddings for each token.\nThe resulting embeddings require large amounts of memory that blow up with the\nnumber of tokens. Training and inference with these models create storage, and\nmemory bandwidth bottlenecks leading to significant computing and energy\nconsumption when deployed in practice. To this end, we present the problem of\n\\textit{Memory Allocation} under budget for embeddings and propose a novel\nformulation of memory shared embedding, where memory is shared in proportion to\nthe overlap in semantic information. Our formulation admits a practical and\nefficient randomized solution with Locality sensitive hashing based Memory\nAllocation (LMA). We demonstrate a significant reduction in the memory\nfootprint while maintaining performance. In particular, our LMA embeddings\nachieve the same performance compared to standard embeddings with a 16$\\times$\nreduction in memory footprint. Moreover, LMA achieves an average improvement of\nover 0.003 AUC across different memory regimes than standard DLRM models on\nCriteo and Avazu datasets\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:55:49 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Desai", "Aditya", ""], ["Pan", "Yanzhou", ""], ["Sun", "Kuangyuan", ""], ["Chou", "Li", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "2103.06125", "submitter": "Lucas N. Ferreira", "authors": "Lucas N. Ferreira, Jim Whitehead", "title": "Learning to Generate Music With Sentiment", "comments": "International Society for Music Information Retrieval (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Learning models have shown very promising results in automatically\ncomposing polyphonic music pieces. However, it is very hard to control such\nmodels in order to guide the compositions towards a desired goal. We are\ninterested in controlling a model to automatically generate music with a given\nsentiment. This paper presents a generative Deep Learning model that can be\ndirected to compose music with a given sentiment. Besides music generation, the\nsame model can be used for sentiment analysis of symbolic music. We evaluate\nthe accuracy of the model in classifying sentiment of symbolic music using a\nnew dataset of video game soundtracks. Results show that our model is able to\nobtain good prediction accuracy. A user study shows that human subjects agreed\nthat the generated music has the intended sentiment, however negative pieces\ncan be ambiguous.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 03:16:52 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Ferreira", "Lucas N.", ""], ["Whitehead", "Jim", ""]]}, {"id": "2103.06130", "submitter": "Jumanah Alshehri", "authors": "Jumanah Alshehri, Marija Stanojevic, Eduard Dragut, Zoran Obradovic", "title": "Stay on Topic, Please: Aligning User Comments to the Content of a News\n  Article", "comments": "Accepted as a full paper at the 43rd European Conference on\n  Information Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social scientists have shown that up to 50% if the content posted to a news\narticle have no relation to its journalistic content. In this study we propose\na classification algorithm to categorize user comments posted to a new article\nbase don their alignment to its content. The alignment seek to match user\ncomments to an article based on similarity off content, entities in discussion,\nand topic. We proposed a BERTAC, BAERT-based approach that learn jointly\narticle-comment embeddings and infers the relevance class of comments. We\nintroduce an ordinal classification loss that penalizes the difference between\nthe predicted and true label. We conduct a thorough study to show influence of\nthe proposed loss on the learning process. The results on five representative\nnews outlets show that our approach can learn the comment class with up to 36%\naverage accuracy improvement compering to the baselines, and up to 25%\ncompering to the BA-BC model. BA-BC is out approach that consists of two models\naimed to capture dis-jointly the formal language of news articles and the\ninformal language of comments. We also conduct a user study to evaluate human\nlabeling performance to understand the difficulty of the classification task.\nThe user agreement on comment-article alignment is \"moderate\" per\nKrippendorff's alpha score, which suggests that the classification task is\ndifficult.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:29:00 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Alshehri", "Jumanah", ""], ["Stanojevic", "Marija", ""], ["Dragut", "Eduard", ""], ["Obradovic", "Zoran", ""]]}, {"id": "2103.06137", "submitter": "Xixun Lin", "authors": "Xixun Lin, Jia Wu, Chuan Zhou, Shirui Pan, Yanan Cao, Bin Wang", "title": "Task-adaptive Neural Process for User Cold-Start Recommendation", "comments": "Accepted by WWW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  User cold-start recommendation is a long-standing challenge for recommender\nsystems due to the fact that only a few interactions of cold-start users can be\nexploited. Recent studies seek to address this challenge from the perspective\nof meta learning, and most of them follow a manner of parameter initialization,\nwhere the model parameters can be learned by a few steps of gradient updates.\nWhile these gradient-based meta-learning models achieve promising performances\nto some extent, a fundamental problem of them is how to adapt the global\nknowledge learned from previous tasks for the recommendations of cold-start\nusers more effectively. In this paper, we develop a novel meta-learning\nrecommender called task-adaptive neural process (TaNP). TaNP is a new member of\nthe neural process family, where making recommendations for each user is\nassociated with a corresponding stochastic process. TaNP directly maps the\nobserved interactions of each user to a predictive distribution, sidestepping\nsome training issues in gradient-based meta-learning models. More importantly,\nto balance the trade-off between model capacity and adaptation reliability, we\nintroduce a novel task-adaptive mechanism. It enables our model to learn the\nrelevance of different tasks and customize the global knowledge to the\ntask-related decoder parameters for estimating user preferences. We validate\nTaNP on multiple benchmark datasets in different experimental settings.\nEmpirical results demonstrate that TaNP yields consistent improvements over\nseveral state-of-the-art meta-learning recommenders.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 10:05:03 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Lin", "Xixun", ""], ["Wu", "Jia", ""], ["Zhou", "Chuan", ""], ["Pan", "Shirui", ""], ["Cao", "Yanan", ""], ["Wang", "Bin", ""]]}, {"id": "2103.06138", "submitter": "Marlesson Santana", "authors": "Marlesson R. O. Santana, Anderson Soares", "title": "Hybrid Model with Time Modeling for Sequential Recommender Systems", "comments": "5 pages, 2 figures, WSDM Workshop on Web Tourism 2021", "journal-ref": "ACM WSDM Workshop on Web Tourism (WSDM Webtour'21), March 12,\n  2021, Jerusalem, Israel", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning based methods have been used successfully in recommender system\nproblems. Approaches using recurrent neural networks, transformers, and\nattention mechanisms are useful to model users' long- and short-term\npreferences in sequential interactions. To explore different session-based\nrecommendation solutions, Booking.com recently organized the WSDM WebTour 2021\nChallenge, which aims to benchmark models to recommend the final city in a\ntrip. This study presents our approach to this challenge. We conducted several\nexperiments to test different state-of-the-art deep learning architectures for\nrecommender systems. Further, we proposed some changes to Neural Attentive\nRecommendation Machine (NARM), adapted its architecture for the challenge\nobjective, and implemented training approaches that can be used in any\nsession-based model to improve accuracy. Our experimental result shows that the\nimproved NARM outperforms all other state-of-the-art benchmark methods.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 19:28:22 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Santana", "Marlesson R. O.", ""], ["Soares", "Anderson", ""]]}, {"id": "2103.06192", "submitter": "Tom Lotze", "authors": "Tom Lotze, Stefan Klut, Mohammad Aliannejadi, Evangelos Kanoulas", "title": "Ranking Clarifying Questions Based on Predicted User Engagement", "comments": "Appeared in MICROS Workshop, co-located with ECIR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve online search results, clarification questions can be used to\nelucidate the information need of the user. This research aims to predict the\nuser engagement with the clarification pane as an indicator of relevance based\non the lexical information: query, question, and answers. Subsequently, the\npredicted user engagement can be used as a feature to rank the clarification\npanes. Regression and classification are applied for predicting user engagement\nand compared to naive heuristic baselines (e.g. mean) on the new MIMICS dataset\n[20]. An ablation study is carried out using a RankNet model to determine\nwhether the predicted user engagement improves clarification pane ranking\nperformance. The prediction models were able to improve significantly upon the\nnaive baselines, and the predicted user engagement feature significantly\nimproved the RankNet results in terms of NDCG and MRR. This research\ndemonstrates the potential for ranking clarification panes based on lexical\ninformation only and can serve as a first neural baseline for future research\nto improve on. The code is available online.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:11:59 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 16:23:05 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 13:31:10 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Lotze", "Tom", ""], ["Klut", "Stefan", ""], ["Aliannejadi", "Mohammad", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "2103.06364", "submitter": "Himan Abdollahpouri", "authors": "Himan Abdollahpouri, Masoud Mansoury, Robin Burke, Bamshad Mobasher,\n  Edward Malthouse", "title": "User-centered Evaluation of Popularity Bias in Recommender Systems", "comments": "Proceedings of the 29th ACM Conference on User Modeling, Adaptation\n  and Personalization (UMAP '21), June 21--25, 2021, Utrecht, Netherlands.\n  arXiv admin note: text overlap with arXiv:2007.12230", "journal-ref": null, "doi": "10.1145/3450613.3456821", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommendation and ranking systems are known to suffer from popularity bias;\nthe tendency of the algorithm to favor a few popular items while\nunder-representing the majority of other items. Prior research has examined\nvarious approaches for mitigating popularity bias and enhancing the\nrecommendation of long-tail, less popular, items. The effectiveness of these\napproaches is often assessed using different metrics to evaluate the extent to\nwhich over-concentration on popular items is reduced. However, not much\nattention has been given to the user-centered evaluation of this bias; how\ndifferent users with different levels of interest towards popular items are\naffected by such algorithms. In this paper, we show the limitations of the\nexisting metrics to evaluate popularity bias mitigation when we want to assess\nthese algorithms from the users' perspective and we propose a new metric that\ncan address these limitations. In addition, we present an effective approach\nthat mitigates popularity bias from the user-centered point of view. Finally,\nwe investigate several state-of-the-art approaches proposed in recent years to\nmitigate popularity bias and evaluate their performances using the existing\nmetrics and also from the users' perspective. Our experimental results using\ntwo publicly-available datasets show that existing popularity bias mitigation\ntechniques ignore the users' tolerance towards popular items. Our proposed\nuser-centered method can tackle popularity bias effectively for different users\nwhile also improving the existing metrics.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:12:51 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Abdollahpouri", "Himan", ""], ["Mansoury", "Masoud", ""], ["Burke", "Robin", ""], ["Mobasher", "Bamshad", ""], ["Malthouse", "Edward", ""]]}, {"id": "2103.06443", "submitter": "Sourav Garg", "authors": "Sourav Garg, Tobias Fischer and Michael Milford", "title": "Where is your place, Visual Place Recognition?", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Place Recognition (VPR) is often characterized as being able to\nrecognize the same place despite significant changes in appearance and\nviewpoint. VPR is a key component of Spatial Artificial Intelligence, enabling\nrobotic platforms and intelligent augmentation platforms such as augmented\nreality devices to perceive and understand the physical world. In this paper,\nwe observe that there are three \"drivers\" that impose requirements on spatially\nintelligent agents and thus VPR systems: 1) the particular agent including its\nsensors and computational resources, 2) the operating environment of this\nagent, and 3) the specific task that the artificial agent carries out. In this\npaper, we characterize and survey key works in the VPR area considering those\ndrivers, including their place representation and place matching choices. We\nalso provide a new definition of VPR based on the visual overlap -- akin to\nspatial view cells in the brain -- that enables us to find similarities and\ndifferences to other research areas in the robotics and computer vision fields.\nWe identify numerous open challenges and suggest areas that require more\nin-depth attention in future works.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 04:11:04 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Garg", "Sourav", ""], ["Fischer", "Tobias", ""], ["Milford", "Michael", ""]]}, {"id": "2103.06499", "submitter": "Yifan Qiao", "authors": "Yingrui Yang, Yifan Qiao, Jinjin Shao, Mayuresh Anand, Xifeng Yan, Tao\n  Yang", "title": "Composite Re-Ranking for Efficient Document Search with BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although considerable efforts have been devoted to transformer-based ranking\nmodels for document search, the relevance-efficiency tradeoff remains a\ncritical problem for ad-hoc ranking. To overcome this challenge, this paper\npresents BECR (BERT-based Composite Re-Ranking), a composite re-ranking scheme\nthat combines deep contextual token interactions and traditional lexical\nterm-matching features. In particular, BECR exploits a token encoding mechanism\nto decompose the query representations into pre-computable uni-grams and\nskip-n-grams. By applying token encoding on top of a dual-encoder architecture,\nBECR separates the attentions between a query and a document while capturing\nthe contextual semantics of a query. In contrast to previous approaches, this\nframework does not perform expensive BERT computations during online inference.\nThus, it is significantly faster, yet still able to achieve high\ncompetitiveness in ad-hoc ranking relevance. Finally, an in-depth comparison\nbetween BECR and other start-of-the-art neural ranking baselines is described\nusing the TREC datasets, thereby further demonstrating the enhanced relevance\nand efficiency of BECR.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 06:52:29 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 04:54:15 GMT"}, {"version": "v3", "created": "Sat, 17 Apr 2021 04:51:03 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Yang", "Yingrui", ""], ["Qiao", "Yifan", ""], ["Shao", "Jinjin", ""], ["Anand", "Mayuresh", ""], ["Yan", "Xifeng", ""], ["Yang", "Tao", ""]]}, {"id": "2103.06518", "submitter": "Hergys Rexha", "authors": "Hergys Rexha, Sebastien Lafond", "title": "Data Collection and Utilization Framework for Edge AI Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As data being produced by IoT applications continues to explode, there is a\ngrowing need to bring computing power closer to the source of the data to meet\nthe response time, power dissipation and cost goals of performance-critical\napplications in various domains like the Industrial Internet of Things (IIoT),\nAutomated Driving, Medical Imaging or Surveillance among others. This paper\nproposes a data collection and utilization framework that allows runtime\nplatform and application data to be sent to an edge and cloud system via data\ncollection agents running close to the platform. Agents are connected to a\ncloud system able to train AI models to improve overall energy efficiency of an\nAI application executed on an edge platform. In the implementation part, we\nshow the benefits of FPGA-based platform for the task of object detection.\nFurthermore, we show that it is feasible to collect relevant data from an FPGA\nplatform, transmit the data to a cloud system for processing and receiving\nfeedback actions to execute an edge AI application energy efficiently. As\nfuture work, we foresee the possibility to train, deploy and continuously\nimprove a base model able to efficiently adapt the execution of edge\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 08:07:29 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 07:30:52 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Rexha", "Hergys", ""], ["Lafond", "Sebastien", ""]]}, {"id": "2103.06523", "submitter": "Jaekeol Choi", "authors": "Jaekeol Choi, Euna Jung, Jangwon Suh, Wonjong Rhee", "title": "Improving Bi-encoder Document Ranking Models with Two Rankers and\n  Multi-teacher Distillation", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BERT-based Neural Ranking Models (NRMs) can be classified according to how\nthe query and document are encoded through BERT's self-attention layers -\nbi-encoder versus cross-encoder. Bi-encoder models are highly efficient because\nall the documents can be pre-processed before the query time, but their\nperformance is inferior compared to cross-encoder models. Both models utilize a\nranker that receives BERT representations as the input and generates a\nrelevance score as the output. In this work, we propose a method where\nmulti-teacher distillation is applied to a cross-encoder NRM and a bi-encoder\nNRM to produce a bi-encoder NRM with two rankers. The resulting student\nbi-encoder achieves an improved performance by simultaneously learning from a\ncross-encoder teacher and a bi-encoder teacher and also by combining relevance\nscores from the two rankers. We call this method TRMD (Two Rankers and\nMulti-teacher Distillation). In the experiments, TwinBERT and ColBERT are\nconsidered as baseline bi-encoders. When monoBERT is used as the cross-encoder\nteacher, together with either TwinBERT or ColBERT as the bi-encoder teacher,\nTRMD produces a student bi-encoder that performs better than the corresponding\nbaseline bi-encoder. For P@20, the maximum improvement was 11.4%, and the\naverage improvement was 6.8%. As an additional experiment, we considered\nproducing cross-encoder students with TRMD, and found that it could also\nimprove the cross-encoders.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 08:22:05 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Choi", "Jaekeol", ""], ["Jung", "Euna", ""], ["Suh", "Jangwon", ""], ["Rhee", "Wonjong", ""]]}, {"id": "2103.06560", "submitter": "Deng-Cheng Yan", "authors": "Dengcheng Yan, Wenxin Xie, Yiwen Zhang", "title": "Heterogeneous Information Network-based Interest Composition with Graph\n  Neural Network for Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Heterogeneous information network (HIN) is widely applied to recommendation\nsystems due to its capability of modeling various auxiliary information with\nmeta-path. However, existing HIN-based recommendation models usually fuse the\ninformation from various meta-paths by simple weighted sum or concatenation,\nwhich limits the improvement of performance because it lacks the capability of\ninterest compositions among meta-paths. In this article, we propose a HIN-based\nInterest Composition model for Recommendation (HicRec). Specially, the\nrepresentations of users and items are learnt with graph neural network on both\ngraph structure and features in each meta-path, and a parameter sharing\nmechanism is utilized here to ensure the representations of users and items are\nin the same latent space. Then, users' interest on each item from each pair of\nrelated meta-paths is calculated by a combination of the representations of\nusers and items. The composed interests of users are obtained by a composition\nof them from both intra- and inter-meta-paths for recommendation. Extensive\nexperiments are conducted on three real-world datasets and the results\ndemonstrate the outperformance of our proposed HicRec against the baselines.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 09:39:39 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 08:54:23 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Yan", "Dengcheng", ""], ["Xie", "Wenxin", ""], ["Zhang", "Yiwen", ""]]}, {"id": "2103.06561", "submitter": "Zhiwu Lu", "authors": "Yuqi Huo, Manli Zhang, Guangzhen Liu, Haoyu Lu, Yizhao Gao, Guoxing\n  Yang, Jingyuan Wen, Heng Zhang, Baogui Xu, Weihao Zheng, Zongzheng Xi,\n  Yueqian Yang, Anwen Hu, Jinming Zhao, Ruichen Li, Yida Zhao, Liang Zhang,\n  Yuqing Song, Xin Hong, Wanqing Cui, Danyang Hou, Yingyan Li, Junyi Li, Peiyu\n  Liu, Zheng Gong, Chuhao Jin, Yuchong Sun, Shizhe Chen, Zhiwu Lu, Zhicheng\n  Dou, Qin Jin, Yanyan Lan, Wayne Xin Zhao, Ruihua Song, and Ji-Rong Wen", "title": "WenLan: Bridging Vision and Language by Large-Scale Multi-Modal\n  Pre-Training", "comments": "This paper is the outcome of the Chinese multi-modal pre-training\n  project called 'WenLan'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-modal pre-training models have been intensively explored to bridge\nvision and language in recent years. However, most of them explicitly model the\ncross-modal interaction between image-text pairs, by assuming that there exists\nstrong semantic correlation between the text and image modalities. Since this\nstrong assumption is often invalid in real-world scenarios, we choose to\nimplicitly model the cross-modal correlation for large-scale multi-modal\npre-training, which is the focus of the Chinese project `WenLan' led by our\nteam. Specifically, with the weak correlation assumption over image-text pairs,\nwe propose a two-tower pre-training model called BriVL within the cross-modal\ncontrastive learning framework. Unlike OpenAI CLIP that adopts a simple\ncontrastive learning method, we devise a more advanced algorithm by adapting\nthe latest method MoCo into the cross-modal scenario. By building a large\nqueue-based dictionary, our BriVL can incorporate more negative samples in\nlimited GPU resources. We further construct a large Chinese multi-source\nimage-text dataset called RUC-CAS-WenLan for pre-training our BriVL model.\nExtensive experiments demonstrate that the pre-trained BriVL model outperforms\nboth UNITER and OpenAI CLIP on various downstream tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 09:39:49 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 07:52:50 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 14:01:03 GMT"}, {"version": "v4", "created": "Wed, 17 Mar 2021 12:17:02 GMT"}, {"version": "v5", "created": "Fri, 19 Mar 2021 23:30:38 GMT"}, {"version": "v6", "created": "Thu, 8 Jul 2021 13:56:05 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Huo", "Yuqi", ""], ["Zhang", "Manli", ""], ["Liu", "Guangzhen", ""], ["Lu", "Haoyu", ""], ["Gao", "Yizhao", ""], ["Yang", "Guoxing", ""], ["Wen", "Jingyuan", ""], ["Zhang", "Heng", ""], ["Xu", "Baogui", ""], ["Zheng", "Weihao", ""], ["Xi", "Zongzheng", ""], ["Yang", "Yueqian", ""], ["Hu", "Anwen", ""], ["Zhao", "Jinming", ""], ["Li", "Ruichen", ""], ["Zhao", "Yida", ""], ["Zhang", "Liang", ""], ["Song", "Yuqing", ""], ["Hong", "Xin", ""], ["Cui", "Wanqing", ""], ["Hou", "Danyang", ""], ["Li", "Yingyan", ""], ["Li", "Junyi", ""], ["Liu", "Peiyu", ""], ["Gong", "Zheng", ""], ["Jin", "Chuhao", ""], ["Sun", "Yuchong", ""], ["Chen", "Shizhe", ""], ["Lu", "Zhiwu", ""], ["Dou", "Zhicheng", ""], ["Jin", "Qin", ""], ["Lan", "Yanyan", ""], ["Zhao", "Wayne Xin", ""], ["Song", "Ruihua", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2103.06909", "submitter": "Himan Abdollahpouri", "authors": "Himan Abdollahpouri, Edward Malthouse, Joseph Konstan, Bamshad\n  Mobasher, Jeremy Gilbert", "title": "Toward the Next Generation of News Recommender Systems", "comments": "WWW '21 Companion, April 19-23, 2021, Ljubljana, Slovenia", "journal-ref": null, "doi": "10.1145/3442442.3452327", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a vision and research agenda for the next generation of\nnews recommender systems (RS), called the table d'hote approach. A table d'hote\n(translates as host's table) meal is a sequence of courses that create a\nbalanced and enjoyable dining experience for a guest. Likewise, we believe news\nRS should strive to create a similar experience for the users by satisfying the\nnews-diet needs of a user. While extant news RS considers criteria such as\ndiversity and serendipity, and RS bundles have been studied for other contexts\nsuch as tourism, table d'hote goes further by ensuring the recommended articles\nsatisfy a diverse set of user needs in the right proportions and in a specific\norder. In table d'hote, available articles need to be stratified based on the\ndifferent ways that news can create value for the reader, building from\ntheories and empirical research in journalism and user engagement. Using\ntheories and empirical research from communication on the uses and\ngratifications (U&G) consumers derive from media, we define two main strata in\na table d'hote news RS, each with its own substrata: 1) surveillance, which\nconsists of information the user needs to know, and 2) serendipity, which are\nthe articles offering unexpected surprises. The diversity of the articles\naccording to the defined strata and the order of the articles within the list\nof recommendations are also two important aspects of the table d'hote in order\nto give the users the most effective reading experience. We propose our vision,\nlink it to the existing concepts in the RS literature, and identify challenges\nfor future research.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 19:07:06 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Abdollahpouri", "Himan", ""], ["Malthouse", "Edward", ""], ["Konstan", "Joseph", ""], ["Mobasher", "Bamshad", ""], ["Gilbert", "Jeremy", ""]]}, {"id": "2103.06942", "submitter": "Kartik Vempala", "authors": "Kartik Vempala (Bloomberg LP)", "title": "Imagined-Trailing-Whitespace-Agnostic Levenshtein Distance For Plaintext\n  Table Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The standard algorithm for Levenshtein distance, treats trailing whitespace\nthe same as any other letter or symbol. However, when humans compare 2 strings,\nwe implicitly assume that both strings are padded by infinite trailing\nwhitespace. This informs our expectations for what the costs for insertion,\ndeletion and replacement, should be. This violation of our expectations results\nin non-intuitive edit distance values. To account for this specific human\nintuition, a naive approach which considers \"all possible\" substrings of\ntrailing whitespace would yield an $O(n^3)$ algorithm. In this work, we provide\nan efficient $O(n^2)$ algorithm to compute the same. Keywords: Imagined\nInfinite Trailing Whitespace, Human Friendly, Intuitive Edit Distance, Table\nDetection, Table Alignment\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 20:39:40 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Vempala", "Kartik", "", "Bloomberg LP"]]}, {"id": "2103.07052", "submitter": "Yifan Zhang", "authors": "Yifan Zhang, Dainis Boumber, Marjan Hosseinia, Fan Yang, Arjun\n  Mukherjee", "title": "Improving Authorship Verification using Linguistic Divergence", "comments": "Published in ROMCIR 2021. Workshop held as part of ECIR 2021. March\n  28 - April 1, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an unsupervised solution to the Authorship Verification task that\nutilizes pre-trained deep language models to compute a new metric called\nDV-Distance. The proposed metric is a measure of the difference between the two\nauthors comparing against pre-trained language models. Our design addresses the\nproblem of non-comparability in authorship verification, frequently encountered\nin small or cross-domain corpora. To the best of our knowledge, this paper is\nthe first one to introduce a method designed with non-comparability in mind\nfrom the ground up, rather than indirectly. It is also one of the first to use\nDeep Language Models in this setting. The approach is intuitive, and it is easy\nto understand and interpret through visualization. Experiments on four datasets\nshow our methods matching or surpassing current state-of-the-art and strong\nbaselines in most tasks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 03:01:17 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Zhang", "Yifan", ""], ["Boumber", "Dainis", ""], ["Hosseinia", "Marjan", ""], ["Yang", "Fan", ""], ["Mukherjee", "Arjun", ""]]}, {"id": "2103.07575", "submitter": "Yusen Lin", "authors": "Yusen Lin", "title": "A Review on Semi-Supervised Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relation extraction (RE) plays an important role in extracting knowledge from\nunstructured text but requires a large amount of labeled corpus. To reduce the\nexpensive annotation efforts, semisupervised learning aims to leverage both\nlabeled and unlabeled data. In this paper, we review and compare three typical\nmethods in semi-supervised RE with deep learning or meta-learning:\nself-ensembling, which forces consistent under perturbations but may confront\ninsufficient supervision; self-training, which iteratively generates pseudo\nlabels and retrain itself with the enlarged labeled set; dual learning, which\nleverages a primal task and a dual task to give mutual feedback. Mean-teacher\n(Tarvainen and Valpola, 2017), LST (Li et al., 2019), and DualRE (Lin et al.,\n2019) are elaborated as the representatives to alleviate the weakness of these\nthree methods, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 23:43:23 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Lin", "Yusen", ""]]}, {"id": "2103.07597", "submitter": "Amirali Salehi-Abari", "authors": "Sarina Sajadi Ghaemmaghami and Amirali Salehi-Abari", "title": "DeepGroup: Representation Learning for Group Recommendation with\n  Implicit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Group recommender systems facilitate group decision making for a set of\nindividuals (e.g., a group of friends, a team, a corporation, etc.). Many of\nthese systems, however, either assume that (i) user preferences can be elicited\n(or inferred) and then aggregated into group preferences or (ii) group\npreferences are partially observed/elicited. We focus on making recommendations\nfor a new group of users whose preferences are unknown, but we are given the\ndecisions/choices of other groups. By formulating this problem as group\nrecommendation from group implicit feedback, we focus on two of its practical\ninstances: group decision prediction and reverse social choice. Given a set of\ngroups and their observed decisions, group decision prediction intends to\npredict the decision of a new group of users, whereas reverse social choice\naims to infer the preferences of those users involved in observed group\ndecisions. These two problems are of interest to not only group recommendation,\nbut also to personal privacy when the users intend to conceal their personal\npreferences but have participated in group decisions. To tackle these two\nproblems, we propose and study DeepGroup -- a deep learning approach for group\nrecommendation with group implicit data. We empirically assess the predictive\npower of DeepGroup on various real-world datasets, group conditions (e.g.,\nhomophily or heterophily), and group decision (or voting) rules. Our extensive\nexperiments not only demonstrate the efficacy of DeepGroup, but also shed light\non the privacy-leakage concerns of some decision making processes.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 02:05:26 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Ghaemmaghami", "Sarina Sajadi", ""], ["Salehi-Abari", "Amirali", ""]]}, {"id": "2103.07768", "submitter": "Robin Swezey", "authors": "Robin Swezey, Bruno Charron", "title": "Large-scale Recommendation for Portfolio Optimization", "comments": null, "journal-ref": "In Proceedings of the 12th ACM Conference on Recommender Systems\n  (RecSys 2018). Association for Computing Machinery, New York, NY, USA,\n  382-386", "doi": "10.1145/3240323.3240386", "report-no": null, "categories": "cs.AI cs.CE cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual investors are now massively using online brokers to trade stocks\nwith convenient interfaces and low fees, albeit losing the advice and\npersonalization traditionally provided by full-service brokers. We frame the\nproblem faced by online brokers of replicating this level of service in a\nlow-cost and automated manner for a very large number of users. Because of the\ncare required in recommending financial products, we focus on a risk-management\napproach tailored to each user's portfolio and risk profile. We show that our\nhybrid approach, based on Modern Portfolio Theory and Collaborative Filtering,\nprovides a sound and effective solution. The method is applicable to stocks as\nwell as other financial assets, and can be easily combined with various\nfinancial forecasting models. We validate our proposal by comparing it with\nseveral baselines in a domain expert-based study.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 18:22:48 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Swezey", "Robin", ""], ["Charron", "Bruno", ""]]}, {"id": "2103.07769", "submitter": "Preslav Nakov", "authors": "Preslav Nakov, David Corney, Maram Hasanain, Firoj Alam, Tamer\n  Elsayed, Alberto Barr\\'on-Cede\\~no, Paolo Papotti, Shaden Shaar, Giovanni Da\n  San Martino", "title": "Automated Fact-Checking for Assisting Human Fact-Checkers", "comments": "fact-checking, fact-checkers, check-worthiness, detecting previously\n  fact-checked claims, evidence retrieval", "journal-ref": "IJCAI-2021", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CR cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reporting and the analysis of current events around the globe has\nexpanded from professional, editor-lead journalism all the way to citizen\njournalism. Nowadays, politicians and other key players enjoy direct access to\ntheir audiences through social media, bypassing the filters of official cables\nor traditional media. However, the multiple advantages of free speech and\ndirect communication are dimmed by the misuse of media to spread inaccurate or\nmisleading claims. These phenomena have led to the modern incarnation of the\nfact-checker -- a professional whose main aim is to examine claims using\navailable evidence and to assess their veracity. As in other text forensics\ntasks, the amount of information available makes the work of the fact-checker\nmore difficult. With this in mind, starting from the perspective of the\nprofessional fact-checker, we survey the available intelligent technologies\nthat can support the human expert in the different steps of her fact-checking\nendeavor. These include identifying claims worth fact-checking, detecting\nrelevant previously fact-checked claims, retrieving relevant evidence to\nfact-check a claim, and actually verifying a claim. In each case, we pay\nattention to the challenges in future work and the potential impact on\nreal-world fact-checking.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 18:29:14 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 12:27:05 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Nakov", "Preslav", ""], ["Corney", "David", ""], ["Hasanain", "Maram", ""], ["Alam", "Firoj", ""], ["Elsayed", "Tamer", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["Papotti", "Paolo", ""], ["Shaar", "Shaden", ""], ["Martino", "Giovanni Da San", ""]]}, {"id": "2103.07779", "submitter": "Robin Swezey", "authors": "Robin Swezey, Young-joo Chung", "title": "Recommending Short-lived Dynamic Packages for Golf Booking Services", "comments": null, "journal-ref": "In Proceedings of the 24th ACM International on Conference on\n  Information and Knowledge Management (CIKM 2015). Association for Computing\n  Machinery, New York, NY, USA, 1779-1782", "doi": "10.1145/2806416.2806608", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach to recommending short-lived dynamic packages for\ngolf booking services. Two challenges are addressed in this work. The first is\nthe short life of the items, which puts the system in a state of a permanent\ncold start. The second is the uninformative nature of the package attributes,\nwhich makes clustering or figuring latent packages challenging. Although such\nsettings are fairly pervasive, they have not been studied in traditional\nrecommendation research, and there is thus a call for original approaches for\nrecommender systems. In this paper, we introduce a hybrid method that leverages\nuser analysis and its relation to the packages, as well as package pricing and\nenvironmental analysis, and traditional collaborative filtering. The proposed\napproach achieved appreciable improvement in precision compared with baselines.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 19:48:04 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Swezey", "Robin", ""], ["Chung", "Young-joo", ""]]}, {"id": "2103.07849", "submitter": "Ziwei Zhu", "authors": "Ziwei Zhu, Jianling Wang, James Caverlee", "title": "Fairness-aware Personalized Ranking Recommendation via Adversarial\n  Learning", "comments": "11 pages, 11 figures, an extended version of a conference published\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation algorithms typically build models based on historical\nuser-item interactions (e.g., clicks, likes, or ratings) to provide a\npersonalized ranked list of items. These interactions are often distributed\nunevenly over different groups of items due to varying user preferences.\nHowever, we show that recommendation algorithms can inherit or even amplify\nthis imbalanced distribution, leading to unfair recommendations to item groups.\nConcretely, we formalize the concepts of ranking-based statistical parity and\nequal opportunity as two measures of fairness in personalized ranking\nrecommendation for item groups. Then, we empirically show that one of the most\nwidely adopted algorithms -- Bayesian Personalized Ranking -- produces unfair\nrecommendations, which motivates our effort to propose the novel fairness-aware\npersonalized ranking model. The debiased model is able to improve the two\nproposed fairness metrics while preserving recommendation performance.\nExperiments on three public datasets show strong fairness improvement of the\nproposed model versus state-of-the-art alternatives.\n  This is paper is an extended and reorganized version of our SIGIR\n2020~\\cite{zhu2020measuring} paper. In this paper, we re-frame the studied\nproblem as `item recommendation fairness' in personalized ranking\nrecommendation systems, and provide more details about the training process of\nthe proposed model and details of experiment setup.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 05:58:15 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zhu", "Ziwei", ""], ["Wang", "Jianling", ""], ["Caverlee", "James", ""]]}, {"id": "2103.07901", "submitter": "Navid Rekabsaz", "authors": "Navid Rekabsaz, Oleg Lesota, Markus Schedl, Jon Brassey, Carsten\n  Eickhoff", "title": "TripClick: The Log Files of a Large Health Web Search Engine", "comments": "Accepted at SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3463242", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click logs are valuable resources for a variety of information retrieval (IR)\ntasks. This includes query understanding/analysis, as well as learning\neffective IR models particularly when the models require large amounts of\ntraining data. We release a large-scale domain-specific dataset of click logs,\nobtained from user interactions of the Trip Database health web search engine.\nOur click log dataset comprises approximately 5.2 million user interactions\ncollected between 2013 and 2020. We use this dataset to create a standard IR\nevaluation benchmark -- TripClick -- with around 700,000 unique free-text\nqueries and 1.3 million pairs of query-document relevance signals, whose\nrelevance is estimated by two click-through models. As such, the collection is\none of the few datasets offering the necessary data richness and scale to train\nneural IR models with a large amount of parameters, and notably the first in\nthe health domain. Using TripClick, we conduct experiments to evaluate a\nvariety of IR models, showing the benefits of exploiting this data to train\nneural architectures. In particular, the evaluation results show that the best\nperforming neural IR model significantly improves the performance by a large\nmargin relative to classical IR models, especially for more frequent queries.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 11:56:08 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 08:43:45 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Rekabsaz", "Navid", ""], ["Lesota", "Oleg", ""], ["Schedl", "Markus", ""], ["Brassey", "Jon", ""], ["Eickhoff", "Carsten", ""]]}, {"id": "2103.08057", "submitter": "ChihWei Hsu", "authors": "Martin Mladenov, Chih-Wei Hsu, Vihan Jain, Eugene Ie, Christopher\n  Colby, Nicolas Mayoraz, Hubert Pham, Dustin Tran, Ivan Vendrov, Craig\n  Boutilier", "title": "RecSim NG: Toward Principled Uncertainty Modeling for Recommender\n  Ecosystems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of recommender systems that optimize multi-turn interaction\nwith users, and model the interactions of different agents (e.g., users,\ncontent providers, vendors) in the recommender ecosystem have drawn increasing\nattention in recent years. Developing and training models and algorithms for\nsuch recommenders can be especially difficult using static datasets, which\noften fail to offer the types of counterfactual predictions needed to evaluate\npolicies over extended horizons. To address this, we develop RecSim NG, a\nprobabilistic platform for the simulation of multi-agent recommender systems.\nRecSim NG is a scalable, modular, differentiable simulator implemented in\nEdward2 and TensorFlow. It offers: a powerful, general probabilistic\nprogramming language for agent-behavior specification; tools for probabilistic\ninference and latent-variable model learning, backed by automatic\ndifferentiation and tracing; and a TensorFlow-based runtime for running\nsimulations on accelerated hardware. We describe RecSim NG and illustrate how\nit can be used to create transparent, configurable, end-to-end models of a\nrecommender ecosystem, complemented by a small set of simple use cases that\ndemonstrate how RecSim NG can help both researchers and practitioners easily\ndevelop and train novel algorithms for recommender systems.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 22:37:42 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Mladenov", "Martin", ""], ["Hsu", "Chih-Wei", ""], ["Jain", "Vihan", ""], ["Ie", "Eugene", ""], ["Colby", "Christopher", ""], ["Mayoraz", "Nicolas", ""], ["Pham", "Hubert", ""], ["Tran", "Dustin", ""], ["Vendrov", "Ivan", ""], ["Boutilier", "Craig", ""]]}, {"id": "2103.08115", "submitter": "Muhao Chen", "authors": "Junheng Hao, Muhao Chen, Wenchao Yu, Yizhou Sun, Wei Wang", "title": "Universal Representation Learning of Knowledge Bases by Jointly\n  Embedding Instances and Ontological Concepts", "comments": "KDD-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large-scale knowledge bases simultaneously represent two views of\nknowledge graphs (KGs): an ontology view for abstract and commonsense concepts,\nand an instance view for specific entities that are instantiated from\nontological concepts. Existing KG embedding models, however, merely focus on\nrepresenting one of the two views alone. In this paper, we propose a novel\ntwo-view KG embedding model, JOIE, with the goal to produce better knowledge\nembedding and enable new applications that rely on multi-view knowledge. JOIE\nemploys both cross-view and intra-view modeling that learn on multiple facets\nof the knowledge base. The cross-view association model is learned to bridge\nthe embeddings of ontological concepts and their corresponding instance-view\nentities. The intra-view models are trained to capture the structured knowledge\nof instance and ontology views in separate embedding spaces, with a\nhierarchy-aware encoding technique enabled for ontologies with hierarchies. We\nexplore multiple representation techniques for the two model components and\ninvestigate with nine variants of JOIE. Our model is trained on large-scale\nknowledge bases that consist of massive instances and their corresponding\nontological concepts connected via a (small) set of cross-view links.\nExperimental results on public datasets show that the best variant of JOIE\nsignificantly outperforms previous models on instance-view triple prediction\ntask as well as ontology population on ontologyview KG. In addition, our model\nsuccessfully extends the use of KG embeddings to entity typing with promising\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 03:24:37 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Hao", "Junheng", ""], ["Chen", "Muhao", ""], ["Yu", "Wenchao", ""], ["Sun", "Yizhou", ""], ["Wang", "Wei", ""]]}, {"id": "2103.08458", "submitter": "Shaina Raza Ms", "authors": "Shaina Raza, Chen Ding", "title": "Deep Dynamic Neural Network to trade-off between Accuracy and Diversity\n  in a News Recommender System", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The news recommender systems are marked by a few unique challenges specific\nto the news domain. These challenges emerge from rapidly evolving readers'\ninterests over dynamically generated news items that continuously change over\ntime. News reading is also driven by a blend of a reader's long-term and\nshort-term interests. In addition, diversity is required in a news recommender\nsystem, not only to keep the reader engaged in the reading process but to get\nthem exposed to different views and opinions. In this paper, we propose a deep\nneural network that jointly learns informative news and readers' interests into\na unified framework. We learn the news representation (features) from the\nheadlines, snippets (body) and taxonomy (category, subcategory) of news. We\nlearn a reader's long-term interests from the reader's click history,\nshort-term interests from the recent clicks via LSTMSs and the diversified\nreader's interests through the attention mechanism. We also apply different\nlevels of attention to our model. We conduct extensive experiments on two news\ndatasets to demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:30:25 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 00:30:07 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Raza", "Shaina", ""], ["Ding", "Chen", ""]]}, {"id": "2103.08541", "submitter": "Tal Schuster", "authors": "Tal Schuster, Adam Fisch, Regina Barzilay", "title": "Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical fact verification models use retrieved written evidence to verify\nclaims. Evidence sources, however, often change over time as more information\nis gathered and revised. In order to adapt, models must be sensitive to subtle\ndifferences in supporting evidence. We present VitaminC, a benchmark infused\nwith challenging cases that require fact verification models to discern and\nadjust to slight factual changes. We collect over 100,000 Wikipedia revisions\nthat modify an underlying fact, and leverage these revisions, together with\nadditional synthetically constructed ones, to create a total of over 400,000\nclaim-evidence pairs. Unlike previous resources, the examples in VitaminC are\ncontrastive, i.e., they contain evidence pairs that are nearly identical in\nlanguage and content, with the exception that one supports a given claim while\nthe other does not. We show that training using this design increases\nrobustness -- improving accuracy by 10% on adversarial fact verification and 6%\non adversarial natural language inference (NLI). Moreover, the structure of\nVitaminC leads us to define additional tasks for fact-checking resources:\ntagging relevant words in the evidence for verifying the claim, identifying\nfactual revisions, and providing automatic edits via factually consistent text\ngeneration.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 17:05:13 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Schuster", "Tal", ""], ["Fisch", "Adam", ""], ["Barzilay", "Regina", ""]]}, {"id": "2103.08733", "submitter": "Nikolaos Kondylidis", "authors": "Nikolaos Kondylidis, Jie Zou and Evangelos Kanoulas", "title": "Category Aware Explainable Conversational Recommendation", "comments": "Workshop on Mixed-Initiative ConveRsatiOnal Systems (MICROS) @ECIR,\n  2021", "journal-ref": "Workshop on Mixed-Initiative ConveRsatiOnal Systems (MICROS)\n  @ECIR, 2021", "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most conversational recommendation approaches are either not explainable, or\nthey require external user's knowledge for explaining or their explanations\ncannot be applied in real time due to computational limitations. In this work,\nwe present a real time category based conversational recommendation approach,\nwhich can provide concise explanations without prior user knowledge being\nrequired. We first perform an explainable user model in the form of preferences\nover the items' categories, and then use the category preferences to recommend\nitems. The user model is performed by applying a BERT-based neural architecture\non the conversation. Then, we translate the user model into item recommendation\nscores using a Feed Forward Network. User preferences during the conversation\nin our approach are represented by category vectors which are directly\ninterpretable. The experimental results on the real conversational\nrecommendation dataset ReDial demonstrate comparable performance to the\nstate-of-the-art, while our approach is explainable. We also show the potential\npower of our framework by involving an oracle setting of category preference\nprediction.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 21:45:13 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kondylidis", "Nikolaos", ""], ["Zou", "Jie", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "2103.08786", "submitter": "Jessie J. Smith", "authors": "Nasim Sonboli and Jessie J. Smith, Florencia Cabral Berenfus, Robin\n  Burke, Casey Fiesler", "title": "Fairness and Transparency in Recommendation: The Users' Perspective", "comments": null, "journal-ref": null, "doi": "10.1145/3450613.3456835", "report-no": null, "categories": "cs.IR cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Though recommender systems are defined by personalization, recent work has\nshown the importance of additional, beyond-accuracy objectives, such as\nfairness. Because users often expect their recommendations to be purely\npersonalized, these new algorithmic objectives must be communicated\ntransparently in a fairness-aware recommender system. While explanation has a\nlong history in recommender systems research, there has been little work that\nattempts to explain systems that use a fairness objective. Even though the\nprevious work in other branches of AI has explored the use of explanations as a\ntool to increase fairness, this work has not been focused on recommendation.\nHere, we consider user perspectives of fairness-aware recommender systems and\ntechniques for enhancing their transparency. We describe the results of an\nexploratory interview study that investigates user perceptions of fairness,\nrecommender systems, and fairness-aware objectives. We propose three features\n-- informed by the needs of our participants -- that could improve user\nunderstanding of and trust in fairness-aware recommender systems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 00:42:09 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Sonboli", "Nasim", ""], ["Smith", "Jessie J.", ""], ["Berenfus", "Florencia Cabral", ""], ["Burke", "Robin", ""], ["Fiesler", "Casey", ""]]}, {"id": "2103.08819", "submitter": "Yanping Wang", "authors": "Bangchao Wang (1 and 2), Ziyang Weng (1), Yanping Wang (3) ((1) School\n  of Mathematics and Computer Science, Wuhan Textile University, Wuhan, China,\n  (2) School of Computer Science, Wuhan University, Wuhan, China, (3) School of\n  Information Management, Wuhan University, Wuhan, China)", "title": "A Novel Paper Recommendation Method Empowered by Knowledge Graph: for\n  Research Beginners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for papers from different academic databases is the most commonly\nused method by research beginners to obtain cross-domain technical solutions.\nHowever, it is usually inefficient and sometimes even useless because\ntraditional search methods neither consider knowledge heterogeneity in\ndifferent domains nor build the bottom layer of search, including but not\nlimited to the characteristic description text of target solutions and\nsolutions to be excluded. To alleviate this problem, a novel paper\nrecommendation method is proposed herein by introducing \"master-slave\" domain\nknowledge graphs, which not only help users express their requirements more\naccurately but also helps the recommendation system better express knowledge.\nSpecifically, it is not restricted by the cold start problem and is a\nchallenge-oriented method. To identify the rationality and usefulness of the\nproposed method, we selected two cross-domains and three different academic\ndatabases for verification. The experimental results demonstrate the\nfeasibility of obtaining new technical papers in the cross-domain scenario by\nresearch beginners using the proposed method. Further, a new research paradigm\nfor research beginners in the early stages is proposed herein.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 03:06:06 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Wang", "Bangchao", "", "1 and 2"], ["Weng", "Ziyang", ""], ["Wang", "Yanping", ""]]}, {"id": "2103.08971", "submitter": "Tsing Zhang", "authors": "Jianqing Zhang (1), Dongjing Wang (1), Dongjin Yu (1) ((1) School of\n  Computer Science and Technology, Hangzhou Dianzi University, China)", "title": "TLSAN: Time-aware Long- and Short-term Attention Network for Next-item\n  Recommendation", "comments": null, "journal-ref": "Neurocomputing, Volume 441, 21 June 2021, Pages 179-191", "doi": "10.1016/j.neucom.2021.02.015", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, deep neural networks are widely applied in recommender systems for\ntheir effectiveness in capturing/modeling users' preferences. Especially, the\nattention mechanism in deep learning enables recommender systems to incorporate\nvarious features in an adaptive way. Specifically, as for the next item\nrecommendation task, we have the following three observations: 1) users'\nsequential behavior records aggregate at time positions (\"time-aggregation\"),\n2) users have personalized taste that is related to the \"time-aggregation\"\nphenomenon (\"personalized time-aggregation\"), and 3) users' short-term\ninterests play an important role in the next item prediction/recommendation. In\nthis paper, we propose a new Time-aware Long- and Short-term Attention Network\n(TLSAN) to address those observations mentioned above. Specifically, TLSAN\nconsists of two main components. Firstly, TLSAN models \"personalized\ntime-aggregation\" and learn user-specific temporal taste via trainable\npersonalized time position embeddings with category-aware correlations in\nlong-term behaviors. Secondly, long- and short-term feature-wise attention\nlayers are proposed to effectively capture users' long- and short-term\npreferences for accurate recommendation. Especially, the attention mechanism\nenables TLSAN to utilize users' preferences in an adaptive way, and its usage\nin long- and short-term layers enhances TLSAN's ability of dealing with sparse\ninteraction data. Extensive experiments are conducted on Amazon datasets from\ndifferent fields (also with different size), and the results show that TLSAN\noutperforms state-of-the-art baselines in both capturing users' preferences and\nperforming time-sensitive next-item recommendation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 10:51:57 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zhang", "Jianqing", ""], ["Wang", "Dongjing", ""], ["Yu", "Dongjin", ""]]}, {"id": "2103.08976", "submitter": "BaiRan Fu", "authors": "Bairan Fu and Wenming Zhang and Guangneng Hu and Xinyu Dai and Shujian\n  Huang and Jiajun Chen", "title": "Dual Side Deep Context-aware Modulation for Social Recommendation", "comments": "Accepted by WWW2021 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social recommendation is effective in improving the recommendation\nperformance by leveraging social relations from online social networking\nplatforms. Social relations among users provide friends' information for\nmodeling users' interest in candidate items and help items expose to potential\nconsumers (i.e., item attraction). However, there are two issues haven't been\nwell-studied: Firstly, for the user interests, existing methods typically\naggregate friends' information contextualized on the candidate item only, and\nthis shallow context-aware aggregation makes them suffer from the limited\nfriends' information. Secondly, for the item attraction, if the item's past\nconsumers are the friends of or have a similar consumption habit to the\ntargeted user, the item may be more attractive to the targeted user, but most\nexisting methods neglect the relation enhanced context-aware item attraction.\nTo address the above issues, we proposed DICER (Dual Side Deep Context-aware\nModulation for SocialRecommendation). Specifically, we first proposed a novel\ngraph neural network to model the social relation and collaborative relation,\nand on top of high-order relations, a dual side deep context-aware modulation\nis introduced to capture the friends' information and item attraction.\nEmpirical results on two real-world datasets show the effectiveness of the\nproposed model and further experiments are conducted to help understand how the\ndual context-aware modulation works.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 11:08:30 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Fu", "Bairan", ""], ["Zhang", "Wenming", ""], ["Hu", "Guangneng", ""], ["Dai", "Xinyu", ""], ["Huang", "Shujian", ""], ["Chen", "Jiajun", ""]]}, {"id": "2103.09306", "submitter": "Qingyao Ai", "authors": "Qingyao Ai, Brendan O Connor, W. Bruce Croft", "title": "A Neural Passage Model for Ad-hoc Document Retrieval", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-76941-7_41", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional statistical retrieval models often treat each document as a\nwhole. In many cases, however, a document is relevant to a query only because a\nsmall part of it contain the targeted information. In this work, we propose a\nneural passage model (NPM) that uses passage-level information to improve the\nperformance of ad-hoc retrieval. Instead of using a single window to extract\npassages, our model automatically learns to weight passages with different\ngranularities in the training process. We show that the passage-based document\nranking paradigm from previous studies can be directly derived from our neural\nframework. Also, our experiments on a TREC collection showed that the NPM can\nsignificantly outperform the existing passage-based retrieval models.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 20:28:28 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Ai", "Qingyao", ""], ["Connor", "Brendan O", ""], ["Croft", "W. Bruce", ""]]}, {"id": "2103.09442", "submitter": "Ming Zhang", "authors": "Ming Zhang, Hong Yan", "title": "Improved Deep Classwise Hashing With Centers Similarity Learning for\n  Image Retrieval", "comments": "Accepted at ICPR'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep supervised hashing for image retrieval has attracted researchers'\nattention due to its high efficiency and superior retrieval performance. Most\nexisting deep supervised hashing works, which are based on pairwise/triplet\nlabels, suffer from the expensive computational cost and insufficient\nutilization of the semantics information. Recently, deep classwise hashing\nintroduced a classwise loss supervised by class labels information\nalternatively; however, we find it still has its drawback. In this paper, we\npropose an improved deep classwise hashing, which enables hashing learning and\nclass centers learning simultaneously. Specifically, we design a two-step\nstrategy on center similarity learning. It interacts with the classwise loss to\nattract the class center to concentrate on the intra-class samples while\npushing other class centers as far as possible. The centers similarity learning\ncontributes to generating more compact and discriminative hashing codes. We\nconduct experiments on three benchmark datasets. It shows that the proposed\nmethod effectively surpasses the original method and outperforms\nstate-of-the-art baselines under various commonly-used evaluation metrics for\nimage retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 05:01:13 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Zhang", "Ming", ""], ["Yan", "Hong", ""]]}, {"id": "2103.09907", "submitter": "Tao Zhou", "authors": "Yan-Li Lee, Tao Zhou", "title": "Collaborative Filtering Approach to Link Prediction", "comments": "19 pages, 3 tables, 1 figure", "journal-ref": null, "doi": "10.1016/j.physa.2021.126107", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Link prediction is a fundamental challenge in network science. Among various\nmethods, local similarity indices are widely used for their high\ncost-performance. However, the performance is less robust: for some networks\nlocal indices are highly competitive to state-of-the-art algorithms while for\nsome other networks they are very poor. Inspired by techniques developed for\nrecommender systems, we propose an enhancement framework for local indices\nbased on collaborative filtering (CF). Considering the delicate but important\ndifference between personalized recommendation and link prediction, we further\npropose an improved framework named as self-included collaborative filtering\n(SCF). The SCF framework significantly improved the accuracy and robustness of\nwell-known local indices. The combination of SCF framework and a simple local\nindex can produce an index with competitive performance and much lower\ncomplexity compared with elaborately-designed state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 07:40:57 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Lee", "Yan-Li", ""], ["Zhou", "Tao", ""]]}, {"id": "2103.09944", "submitter": "Gaurav Gupta", "authors": "Gaurav Gupta, Tharun Medini, Anshumali Shrivastava, Alexander J Smola", "title": "IRLI: Iterative Re-partitioning for Learning to Index", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural models have transformed the fundamental information retrieval problem\nof mapping a query to a giant set of items. However, the need for efficient and\nlow latency inference forces the community to reconsider efficient approximate\nnear-neighbor search in the item space. To this end, learning to index is\ngaining much interest in recent times. Methods have to trade between obtaining\nhigh accuracy while maintaining load balance and scalability in distributed\nsettings. We propose a novel approach called IRLI (pronounced `early'), which\niteratively partitions the items by learning the relevant buckets directly from\nthe query-item relevance data. Furthermore, IRLI employs a superior\npower-of-$k$-choices based load balancing strategy. We mathematically show that\nIRLI retrieves the correct item with high probability under very natural\nassumptions and provides superior load balancing. IRLI surpasses the best\nbaseline's precision on multi-label classification while being $5x$ faster on\ninference. For near-neighbor search tasks, the same method outperforms the\nstate-of-the-art Learned Hashing approach NeuralLSH by requiring only ~\n{1/6}^th of the candidates for the same recall. IRLI is both data and model\nparallel, making it ideal for distributed GPU implementation. We demonstrate\nthis advantage by indexing 100 million dense vectors and surpassing the popular\nFAISS library by >10% on recall.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 23:13:25 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Gupta", "Gaurav", ""], ["Medini", "Tharun", ""], ["Shrivastava", "Anshumali", ""], ["Smola", "Alexander J", ""]]}, {"id": "2103.10474", "submitter": "Abiola Osho", "authors": "Onifade Olufade, Arise Abiola, Ogboo Chisom", "title": "Dynamic Model for Query-Document Expansion towards Improving Retrieval\n  Relevance", "comments": null, "journal-ref": "In Proceedings of the 6th International Conference on Mobile\n  eServices, Ogbomosho, Oyo State, Nigeria. October 28-30, 2015. pp 62-75", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Getting relevant information from search engines has been the heart of\nresearch works in information retrieval. Query expansion is a retrieval\ntechnique that has been studied and proved to yield positive results in\nrelevance. Users are required to express their queries as a shortlist of words,\nsentences, or questions. With this short format, a huge amount of information\nis lost in the process of translating the information need from the actual\nquery size since the user cannot convey all his thoughts in a few words. This\nmostly leads to poor query representation which contributes to undesired\nretrieval effectiveness. This loss of information has made the study of query\nexpansion technique a strong area of study. This research work focuses on two\nmethods of retrieval for both tweet-length queries and sentence-length queries.\nTwo algorithms have been proposed and the implementation is expected to produce\na better relevance retrieval model than most state-the-art relevance models.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 18:45:52 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Olufade", "Onifade", ""], ["Abiola", "Arise", ""], ["Chisom", "Ogboo", ""]]}, {"id": "2103.10631", "submitter": "Eric Schwenker", "authors": "Eric Schwenker, Weixin Jiang, Trevor Spreadbury, Nicola Ferrier,\n  Oliver Cossairt, Maria K. Y. Chan", "title": "EXSCLAIM! -- An automated pipeline for the construction of labeled\n  materials imaging datasets from literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cond-mat.mtrl-sci", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to recent improvements in image resolution and acquisition speed,\nmaterials microscopy is experiencing an explosion of published imaging data.\nThe standard publication format, while sufficient for traditional data\ningestion scenarios where a select number of images can be critically examined\nand curated manually, is not conducive to large-scale data aggregation or\nanalysis, hindering data sharing and reuse. Most images in publications are\npresented as components of a larger figure with their explicit context buried\nin the main body or caption text, so even if aggregated, collections of images\nwith weak or no digitized contextual labels have limited value. To solve the\nproblem of curating labeled microscopy data from literature, this work\nintroduces the EXSCLAIM! Python toolkit for the automatic EXtraction,\nSeparation, and Caption-based natural Language Annotation of IMages from\nscientific literature. We highlight the methodology behind the construction of\nEXSCLAIM! and demonstrate its ability to extract and label open-source\nscientific images at high volume.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 04:48:12 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Schwenker", "Eric", ""], ["Jiang", "Weixin", ""], ["Spreadbury", "Trevor", ""], ["Ferrier", "Nicola", ""], ["Cossairt", "Oliver", ""], ["Chan", "Maria K. Y.", ""]]}, {"id": "2103.10683", "submitter": "Qifan Chen", "authors": "Qifan Chen, Yang Lu, Simon Poon", "title": "Discovering Redundant Activities in Event Logs for the Simplification of\n  Process Models", "comments": "There exist technical errors in the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining acts as a valuable tool to analyse the behaviour of an\norganisation by offering techniques to discover, monitor and enhance real\nprocesses. The key to process mining is to discovery understandable process\nmodels. However, real-life logs can be complex with redundant activities, which\nshare similar behaviour but have different syntax. We show that the existence\nof such redundant activities heavily affects the quality of discovered process\nmodels. Existing approaches filter activities by frequency, which cannot solve\nproblems caused by redundant activities. In this paper, we propose first to\ndiscover redundant activities in the log level and, then, use the discovery\nresults to simplify event logs. Two publicly available data sets are used to\nevaluate the usability of our approach in real-life processes. Our approach can\nbe adopted as a preprocessing step before applying any discovery algorithms to\nproduce simplify models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 08:24:16 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 00:22:19 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 06:16:03 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Chen", "Qifan", ""], ["Lu", "Yang", ""], ["Poon", "Simon", ""]]}, {"id": "2103.10693", "submitter": "Zhe Xie", "authors": "Zhe Xie, Chengxuan Liu, Yichi Zhang, Hongtao Lu, Dong Wang and Yue\n  Ding", "title": "Adversarial and Contrastive Variational Autoencoder for Sequential\n  Recommendation", "comments": "11 pages, WWW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential recommendation as an emerging topic has attracted increasing\nattention due to its important practical significance. Models based on deep\nlearning and attention mechanism have achieved good performance in sequential\nrecommendation. Recently, the generative models based on Variational\nAutoencoder (VAE) have shown the unique advantage in collaborative filtering.\nIn particular, the sequential VAE model as a recurrent version of VAE can\neffectively capture temporal dependencies among items in user sequence and\nperform sequential recommendation. However, VAE-based models suffer from a\ncommon limitation that the representational ability of the obtained approximate\nposterior distribution is limited, resulting in lower quality of generated\nsamples. This is especially true for generating sequences. To solve the above\nproblem, in this work, we propose a novel method called Adversarial and\nContrastive Variational Autoencoder (ACVAE) for sequential recommendation.\nSpecifically, we first introduce the adversarial training for sequence\ngeneration under the Adversarial Variational Bayes (AVB) framework, which\nenables our model to generate high-quality latent variables. Then, we employ\nthe contrastive loss. The latent variables will be able to learn more\npersonalized and salient characteristics by minimizing the contrastive loss.\nBesides, when encoding the sequence, we apply a recurrent and convolutional\nstructure to capture global and local relationships in the sequence. Finally,\nwe conduct extensive experiments on four real-world datasets. The experimental\nresults show that our proposed ACVAE model outperforms other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 09:01:14 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Xie", "Zhe", ""], ["Liu", "Chengxuan", ""], ["Zhang", "Yichi", ""], ["Lu", "Hongtao", ""], ["Wang", "Dong", ""], ["Ding", "Yue", ""]]}, {"id": "2103.10742", "submitter": "Yang Lu", "authors": "Yang Lu, Qifan Chen and Simon Poon", "title": "Detecting and Understanding Branching Frequency Changes in Process\n  Models", "comments": null, "journal-ref": "Enterprise, Business-Process and Information Systems Modeling.\n  BPMDS 2021, EMMSAD 2021. Lecture Notes in Business Information Processing,\n  vol 421. Springer, Cham", "doi": "10.1007/978-3-030-79186-5_3", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business processes are continuously evolving in order to adapt to changes due\nto various factors. One type of process changes are branching frequency\nchanges, which are related to changes in frequencies between different options\nwhen there is an exclusive choice. Existing methods either cannot detect such\nchanges or cannot provide accurate and comprehensive results. In this paper, we\npropose a method which takes both event logs and process models as input and\ngenerates a choice sequence for each exclusive choice in the process model. The\nmethod then identifies change points based on the choice sequences. We evaluate\nour method on a real-life event log. Results show that our method can identify\nbranching frequency changes in process models and provide comprehensive results\nto users.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 11:26:25 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 10:06:04 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 02:12:15 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lu", "Yang", ""], ["Chen", "Qifan", ""], ["Poon", "Simon", ""]]}, {"id": "2103.11095", "submitter": "Wei Zhang", "authors": "Wei Zhang, Xin Lai, and Jianyong Wang", "title": "Social Link Inference via Multi-View Matching Network from\n  Spatio-Temporal Trajectories", "comments": "12 pages, Published in IEEE TNNLS (Key source code added)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate the problem of social link inference in a\ntarget Location-aware Social Network (LSN), which aims at predicting the\nunobserved links between users within the network. This problem is critical for\ndownstream applications including network completion and friend recommendation.\nIn addition to the network structures commonly used in general link prediction,\nthe studies tailored for social link inference in an LSN leverage user\ntrajectories from the spatial aspect. However, the temporal factor lying in\nuser trajectories is largely overlooked by most of the prior studies, limiting\nthe capabilities of capturing the temporal relevance between users. Moreover,\neffective user matching by fusing different views, i.e., social, spatial, and\ntemporal factors, remains unresolved, which hinders the potential improvement\nof link inference. To this end, this paper devises a novel multi-view matching\nnetwork (MVMN) by regarding each of the three factors as one view of any target\nuser pair. MVMN enjoys the flexibility and completeness of modeling each factor\nby developing its suitable matching module: 1) location matching module, 2)\ntime-series matching module, and 3) relation matching module. Each module\nlearns a view-specific representation for matching, and MVMN fuses them for\nfinal link inference. Extensive experiments on two real-world datasets\ndemonstrate the superiority of our approach against several competitive\nbaselines for link prediction and sequence matching, validating the\ncontribution of its key components.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 04:41:43 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhang", "Wei", ""], ["Lai", "Xin", ""], ["Wang", "Jianyong", ""]]}, {"id": "2103.11271", "submitter": "Vuong M. Ngo", "authors": "Vuong M. Ngo and Sven Helmer and Nhien-An Le-Khac and M-Tahar Kechadi", "title": "Structural Textile Pattern Recognition and Processing Based on\n  Hypergraphs", "comments": "38 pages, 23 figures", "journal-ref": "Information Retrieval Journal, Springer, 2021", "doi": "10.1007/s10791-020-09384-y", "report-no": null, "categories": "cs.IR cs.CC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The humanities, like many other areas of society, are currently undergoing\nmajor changes in the wake of digital transformation. However, in order to make\ncollection of digitised material in this area easily accessible, we often still\nlack adequate search functionality. For instance, digital archives for textiles\noffer keyword search, which is fairly well understood, and arrange their\ncontent following a certain taxonomy, but search functionality at the level of\nthread structure is still missing. To facilitate the clustering and search, we\nintroduce an approach for recognising similar weaving patterns based on their\nstructures for textile archives. We first represent textile structures using\nhypergraphs and extract multisets of k-neighbourhoods describing weaving\npatterns from these graphs. Then, the resulting multisets are clustered using\nvarious distance measures and various clustering algorithms (K-Means for\nsimplicity and hierarchical agglomerative algorithms for precision). We\nevaluate the different variants of our approach experimentally, showing that\nthis can be implemented efficiently (meaning it has linear complexity), and\ndemonstrate its quality to query and cluster datasets containing large textile\nsamples. As, to the est of our knowledge, this is the first practical approach\nfor explicitly modelling complex and irregular weaving patterns usable for\nretrieval, we aim at establishing a solid baseline.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 00:44:40 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ngo", "Vuong M.", ""], ["Helmer", "Sven", ""], ["Le-Khac", "Nhien-An", ""], ["Kechadi", "M-Tahar", ""]]}, {"id": "2103.11297", "submitter": "Ryan Rossi", "authors": "Camille Harris, Ryan A. Rossi, Sana Malik, Jane Hoffswell, Fan Du, Tak\n  Yeon Lee, Eunyee Koh, Handong Zhao", "title": "Insight-centric Visualization Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization recommendation systems simplify exploratory data analysis (EDA)\nand make understanding data more accessible to users of all skill levels by\nautomatically generating visualizations for users to explore. However, most\nexisting visualization recommendation systems focus on ranking all\nvisualizations into a single list or set of groups based on particular\nattributes or encodings. This global ranking makes it difficult and\ntime-consuming for users to find the most interesting or relevant insights. To\naddress these limitations, we introduce a novel class of visualization\nrecommendation systems that automatically rank and recommend both groups of\nrelated insights as well as the most important insights within each group. Our\nproposed approach combines results from many different learning-based methods\nto discover insights automatically. A key advantage is that this approach\ngeneralizes to a wide variety of attribute types such as categorical,\nnumerical, and temporal, as well as complex non-trivial combinations of these\ndifferent attribute types. To evaluate the effectiveness of our approach, we\nimplemented a new insight-centric visualization recommendation system,\nSpotLight, which generates and ranks annotated visualizations to explain each\ninsight. We conducted a user study with 12 participants and two datasets which\nshowed that users are able to quickly understand and find relevant insights in\nunfamiliar data.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 03:30:22 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Harris", "Camille", ""], ["Rossi", "Ryan A.", ""], ["Malik", "Sana", ""], ["Hoffswell", "Jane", ""], ["Du", "Fan", ""], ["Lee", "Tak Yeon", ""], ["Koh", "Eunyee", ""], ["Zhao", "Handong", ""]]}, {"id": "2103.11804", "submitter": "Valeria Mazzeo", "authors": "V. Mazzeo, A. Rapisarda and G. Giuffrida", "title": "Detection of fake news on CoViD-19 on Web Search Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In early January 2020, after China reported the first cases of the new\ncoronavirus (SARS-CoV-2) in the city of Wuhan, unreliable and not fully\naccurate information has started spreading faster than the virus itself.\nAlongside this pandemic, people have experienced a parallel infodemic, i.e., an\noverabundance of information, some of which misleading or even harmful, that\nhas widely spread around the globe. Although Social Media are increasingly\nbeing used as information source, Web Search Engines, like Google or Yahoo!,\nstill represent a powerful and trustworthy resource for finding information on\nthe Web. This is due to their capability to capture the largest amount of\ninformation, helping users quickly identify the most relevant, useful, although\nnot always the most reliable, results for their search queries. This study aims\nto detect potential misleading and fake contents by capturing and analysing\ntextual information, which flow through Search Engines. By using a real-world\ndataset associated with recent CoViD-19 pandemic, we first apply re-sampling\ntechniques for class imbalance, then we use existing Machine Learning\nalgorithms for classification of not reliable news. By extracting lexical and\nhost-based features of associated Uniform Resource Locators (URLs) for news\narticles, we show that the proposed methods, so common in phishing and\nmalicious URLs detection, can improve the efficiency and performance of\nclassifiers. Based on these findings, we suggest that the use of both textual\nand URLs features can improve the effectiveness of fake news detection methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:07:26 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 20:42:13 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mazzeo", "V.", ""], ["Rapisarda", "A.", ""], ["Giuffrida", "G.", ""]]}, {"id": "2103.11859", "submitter": "Andrew A. Krizhanovsky", "authors": "Andrew Krizhanovsky, Natalia Krizhanovsky and Irina Novak", "title": "Part of speech and gramset tagging algorithms for unknown words based on\n  morphological dictionaries of the Veps and Karelian languages", "comments": "17 pages, 4 tables, 7 figures, published in the conference proceeding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This research devoted to the low-resource Veps and Karelian languages.\nAlgorithms for assigning part of speech tags to words and grammatical\nproperties to words are presented in the article. These algorithms use our\nmorphological dictionaries, where the lemma, part of speech and a set of\ngrammatical features (gramset) are known for each word form. The algorithms are\nbased on the analogy hypothesis that words with the same suffixes are likely to\nhave the same inflectional models, the same part of speech and gramset. The\naccuracy of these algorithms were evaluated and compared. 313 thousand Vepsian\nand 66 thousand Karelian words were used to verify the accuracy of these\nalgorithms. The special functions were designed to assess the quality of\nresults of the developed algorithms. 92.4% of Vepsian words and 86.8% of\nKarelian words were assigned a correct part of speech by the developed\nalgorithm. 95.3% of Vepsian words and 90.7% of Karelian words were assigned a\ncorrect gramset by our algorithm. Morphological and semantic tagging of texts,\nwhich are closely related and inseparable in our corpus processes, are\ndescribed in the paper.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:58:46 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Krizhanovsky", "Andrew", ""], ["Krizhanovsky", "Natalia", ""], ["Novak", "Irina", ""]]}, {"id": "2103.11951", "submitter": "Endri Kacupaj", "authors": "Aynur Guluzade, Endri Kacupaj, Maria Maleshkova", "title": "Demographic Aware Probabilistic Medical Knowledge Graph Embeddings of\n  Electronic Medical Records", "comments": "Artificial Intelligence in Medicine 2021 (AIME 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical knowledge graphs (KGs) constructed from Electronic Medical Records\n(EMR) contain abundant information about patients and medical entities. The\nutilization of KG embedding models on these data has proven to be efficient for\ndifferent medical tasks. However, existing models do not properly incorporate\npatient demographics and most of them ignore the probabilistic features of the\nmedical KG. In this paper, we propose DARLING (Demographic Aware pRobabiListic\nmedIcal kNowledge embeddinG), a demographic-aware medical KG embedding\nframework that explicitly incorporates demographics in the medical entities\nspace by associating patient demographics with a corresponding hyperplane. Our\nframework leverages the probabilistic features within the medical entities for\nlearning their representations through demographic guidance. We evaluate\nDARLING through link prediction for treatments and medicines, on a medical KG\nconstructed from EMR data, and illustrate its superior performance compared to\nexisting KG embedding models.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:45:05 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 22:57:44 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Guluzade", "Aynur", ""], ["Kacupaj", "Endri", ""], ["Maleshkova", "Maria", ""]]}, {"id": "2103.12286", "submitter": "Ryan Dailey", "authors": "Ryan Dailey, Aniesh Chawla, Andrew Liu, Sripath Mishra, Ling Zhang,\n  Josh Majors, Yung-Hsiang Lu, George K. Thiruvathukal", "title": "Automated Discovery of Real-Time Network Camera Data From Heterogeneous\n  Web Pages", "comments": "This paper has been accepted by ACM Transactions on Internet\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reduction in the cost of Network Cameras along with a rise in connectivity\nenables entities all around the world to deploy vast arrays of camera networks.\nNetwork cameras offer real-time visual data that can be used for studying\ntraffic patterns, emergency response, security, and other applications.\nAlthough many sources of Network Camera data are available, collecting the data\nremains difficult due to variations in programming interface and website\nstructures. Previous solutions rely on manually parsing the target website,\ntaking many hours to complete. We create a general and automated solution for\naggregating Network Camera data spread across thousands of uniquely structured\nweb pages. We analyze heterogeneous web page structures and identify common\ncharacteristics among 73 sample Network Camera websites (each website has\nmultiple web pages). These characteristics are then used to build an automated\ncamera discovery module that crawls and aggregates Network Camera data. Our\nsystem successfully extracts 57,364 Network Cameras from 237,257 unique web\npages.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 03:45:56 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Dailey", "Ryan", ""], ["Chawla", "Aniesh", ""], ["Liu", "Andrew", ""], ["Mishra", "Sripath", ""], ["Zhang", "Ling", ""], ["Majors", "Josh", ""], ["Lu", "Yung-Hsiang", ""], ["Thiruvathukal", "George K.", ""]]}, {"id": "2103.12404", "submitter": "Junmei Hao", "authors": "Junmei Hao, Jingcheng Shi, Qing Da, Anxiang Zeng, Yujie Dun, Xueming\n  Qian, Qianying Lin", "title": "Diversity Regularized Interests Modeling for Recommender Systems", "comments": "7pages,4figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of E-commerce and the increase in the quantity of\nitems, users are presented with more items hence their interests broaden. It is\nincreasingly difficult to model user intentions with traditional methods, which\nmodel the user's preference for an item by combining a single user vector and\nan item vector. Recently, some methods are proposed to generate multiple user\ninterest vectors and achieve better performance compared to traditional\nmethods. However, empirical studies demonstrate that vectors generated from\nthese multi-interests methods are sometimes homogeneous, which may lead to\nsub-optimal performance. In this paper, we propose a novel method of Diversity\nRegularized Interests Modeling (DRIM) for Recommender Systems. We apply a\ncapsule network in a multi-interest extractor to generate multiple user\ninterest vectors. Each interest of the user should have a certain degree of\ndistinction, thus we introduce three strategies as the diversity regularized\nseparator to separate multiple user interest vectors. Experimental results on\npublic and industrial data sets demonstrate the ability of the model to capture\ndifferent interests of a user and the superior performance of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 09:10:37 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Hao", "Junmei", ""], ["Shi", "Jingcheng", ""], ["Da", "Qing", ""], ["Zeng", "Anxiang", ""], ["Dun", "Yujie", ""], ["Qian", "Xueming", ""], ["Lin", "Qianying", ""]]}, {"id": "2103.12420", "submitter": "Emrah Inan", "authors": "Emrah Inan, Paul Thompson, Tim Yates, Sophia Ananiadou", "title": "HSEarch: semantic search system for workplace accident reports", "comments": "Accepted to appear in ECIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Semantic search engines, which integrate the output of text mining (TM)\nmethods, can significantly increase the ease and efficiency of finding relevant\ndocuments and locating important information within them. We present a novel\nsearch engine for the construction industry, HSEarch\n(http://www.nactem.ac.uk/hse/), which uses TM methods to provide\nsemantically-enhanced, faceted search over a repository of workplace accident\nreports. Compared to previous TM-driven search engines for the construction\nindustry, HSEarch provides a more interactive means for users to explore the\ncontents of the repository, to review documents more systematically and to\nlocate relevant knowledge within them.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 09:59:27 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Inan", "Emrah", ""], ["Thompson", "Paul", ""], ["Yates", "Tim", ""], ["Ananiadou", "Sophia", ""]]}, {"id": "2103.12440", "submitter": "Florian Boudin", "authors": "Florian Boudin and Ygor Gallina", "title": "Redefining Absent Keyphrases and their Effect on Retrieval Effectiveness", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural keyphrase generation models have recently attracted much interest due\nto their ability to output absent keyphrases, that is, keyphrases that do not\nappear in the source text. In this paper, we discuss the usefulness of absent\nkeyphrases from an Information Retrieval (IR) perspective, and show that the\ncommonly drawn distinction between present and absent keyphrases is not made\nexplicit enough. We introduce a finer-grained categorization scheme that sheds\nmore light on the impact of absent keyphrases on scientific document retrieval.\nUnder this scheme, we find that only a fraction (around 20%) of the words that\nmake up keyphrases actually serves as document expansion, but that this small\nfraction of words is behind much of the gains observed in retrieval\neffectiveness. We also discuss how the proposed scheme can offer a new angle to\nevaluate the output of neural keyphrase generation models.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 10:42:18 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 14:10:40 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Boudin", "Florian", ""], ["Gallina", "Ygor", ""]]}, {"id": "2103.12475", "submitter": "Aleksandr Petrov", "authors": "Aleksandr Petrov, Yuriy Makarov", "title": "Attention-based neural re-ranking approach for next city in trip\n  recommendations", "comments": "The paper was accepted on ACM WSDM WebTour 2021 Workshop on Web\n  Tourism, https://web.ec.tuwien.ac.at/webtour21/?page_id=27", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes an approach to solving the next destination city\nrecommendation problem for a travel reservation system. We propose a two stages\napproach: a heuristic approach for candidates selection and an attention neural\nnetwork model for candidates re-ranking. Our method was inspired by listwise\nlearning-to-rank methods and recent developments in natural language processing\nand the transformer architecture in particular. We used this approach to solve\nthe Booking.com recommendations challenge Our team achieved 5th place on the\nchallenge using this method, with 0.555 accuracy@4 value on the closed part of\nthe dataset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 11:56:40 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Petrov", "Aleksandr", ""], ["Makarov", "Yuriy", ""]]}, {"id": "2103.12537", "submitter": "Shaina Raza Ms", "authors": "Shaina Raza", "title": "A News Recommender System Considering Temporal Dynamics and Diversity", "comments": "A doctoral symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a news recommender system, a reader's preferences change over time. Some\npreferences drift quite abruptly (short-term preferences), while others change\nover a longer period of time (long-term preferences). Although the existing\nnews recommender systems consider the reader's full history, they often ignore\nthe dynamics in the reader's behavior. Thus, they cannot meet the demand of the\nnews readers for their time-varying preferences. In addition, the\nstate-of-the-art news recommendation models are often focused on providing\naccurate predictions, which can work well in traditional recommendation\nscenarios. However, in a news recommender system, diversity is essential, not\nonly to keep news readers engaged, but also to play a key role in a democratic\nsociety. In this PhD dissertation, our goal is to build a news recommender\nsystem to address these two challenges. Our system should be able to: (i)\naccommodate the dynamics in reader behavior; and (ii) consider both accuracy\nand diversity in the design of the recommendation model. Our news recommender\nsystem can also work for unprofiled, anonymous and short-term readers, by\nleveraging the rich side information of the news items and by including the\nimplicit feedback in our model. We evaluate our model with multiple evaluation\nmeasures (both accuracy and diversity-oriented metrics) to demonstrate the\neffectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 13:45:34 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Raza", "Shaina", ""]]}, {"id": "2103.12874", "submitter": "Gabriel Tavares", "authors": "Sylvio Barbon Jr, Paolo Ceravolo, Ernesto Damiani, Gabriel Marques\n  Tavares", "title": "Using Meta-learning to Recommend Process Discovery Methods", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Process discovery methods have obtained remarkable achievements in Process\nMining, delivering comprehensible process models to enhance management\ncapabilities. However, selecting the suitable method for a specific event log\nhighly relies on human expertise, hindering its broad application. Solutions\nbased on Meta-learning (MtL) have been promising for creating systems with\nreduced human assistance. This paper presents a MtL solution for recommending\nprocess discovery methods that maximize model quality according to\ncomplementary dimensions. Thanks to our MtL pipeline, it was possible to\nrecommend a discovery method with 92% of accuracy using light-weight features\nthat describe the event log. Our experimental analysis also provided\nsignificant insights on the importance of log features in generating\nrecommendations, paving the way to a deeper understanding of the discovery\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 22:40:21 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Barbon", "Sylvio", "Jr"], ["Ceravolo", "Paolo", ""], ["Damiani", "Ernesto", ""], ["Tavares", "Gabriel Marques", ""]]}, {"id": "2103.12876", "submitter": "Chen Zhao", "authors": "Chen Zhao, Chenyan Xiong, Xin Qian and Jordan Boyd-Graber", "title": "Complex Factoid Question Answering with a Free-Text Knowledge Graph", "comments": "WWW2020", "journal-ref": null, "doi": "10.1145/3366423.3380197", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce DELFT, a factoid question answering system which combines the\nnuance and depth of knowledge graph question answering approaches with the\nbroader coverage of free-text. DELFT builds a free-text knowledge graph from\nWikipedia, with entities as nodes and sentences in which entities co-occur as\nedges. For each question, DELFT finds the subgraph linking question entity\nnodes to candidates using text sentences as edges, creating a dense and high\ncoverage semantic graph. A novel graph neural network reasons over the\nfree-text graph-combining evidence on the nodes via information along edge\nsentences-to select a final answer. Experiments on three question answering\ndatasets show DELFT can answer entity-rich questions better than machine\nreading based models, bert-based answer ranking and memory networks. DELFT's\nadvantage comes from both the high coverage of its free-text knowledge\ngraph-more than double that of dbpedia relations-and the novel graph neural\nnetwork which reasons on the rich but noisy free-text evidence.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 22:53:09 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zhao", "Chen", ""], ["Xiong", "Chenyan", ""], ["Qian", "Xin", ""], ["Boyd-Graber", "Jordan", ""]]}, {"id": "2103.12906", "submitter": "Sheshera Mysore", "authors": "Sheshera Mysore, Tim O'Gorman, Andrew McCallum, Hamed Zamani", "title": "CSFCube -- A Test Collection of Computer Science Research Articles for\n  Faceted Query by Example", "comments": "Submitted for single-blind review at the CIKM 2021 Resource Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Query by Example is a well-known information retrieval task in which a\ndocument is chosen by the user as the search query and the goal is to retrieve\nrelevant documents from a large collection. However, a document often covers\nmultiple aspects of a topic. To address this scenario we introduce the task of\nfaceted Query by Example in which users can also specify a finer grained aspect\nin addition to the input query document. We focus on the application of this\ntask in scientific literature search. We envision models which are able to\nretrieve scientific papers analogous to a query scientific paper along\nspecifically chosen rhetorical structure elements as one solution to this\nproblem. In this work, the rhetorical structure elements, which we refer to as\nfacets, indicate backgrounds, methods, or results of a scientific paper. We\nintroduce and describe an expert annotated test collection to evaluate models\ntrained to perform this task. Our test collection consists of a diverse set of\n50 query documents, drawn from computational linguistics and machine learning\nvenues. We carefully followed the annotation guideline used by TREC for depth-k\npooling (k = 100 or 250) and the resulting data collection consists of graded\nrelevance scores with high annotation agreement. The data is freely available\nfor research purposes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 01:02:12 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 17:35:01 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Mysore", "Sheshera", ""], ["O'Gorman", "Tim", ""], ["McCallum", "Andrew", ""], ["Zamani", "Hamed", ""]]}, {"id": "2103.12982", "submitter": "Wen-Yun Yang", "authors": "Rui Li, Yunjiang Jiang, Wenyun Yang, Guoyu Tang, Songlin Wang, Chaoyi\n  Ma, Wei He, Xi Xiong, Yun Xiao, Eric Yihong Zhao", "title": "From Semantic Retrieval to Pairwise Ranking: Applying Deep Learning in\n  E-commerce Search", "comments": "Accepted in SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce deep learning models to the two most important stages in product\nsearch at JD.com, one of the largest e-commerce platforms in the world.\nSpecifically, we outline the design of a deep learning system that retrieves\nsemantically relevant items to a query within milliseconds, and a pairwise deep\nre-ranking system, which learns subtle user preferences. Compared to\ntraditional search systems, the proposed approaches are better at semantic\nretrieval and personalized ranking, achieving significant improvements.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 04:37:32 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Li", "Rui", ""], ["Jiang", "Yunjiang", ""], ["Yang", "Wenyun", ""], ["Tang", "Guoyu", ""], ["Wang", "Songlin", ""], ["Ma", "Chaoyi", ""], ["He", "Wei", ""], ["Xiong", "Xi", ""], ["Xiao", "Yun", ""], ["Zhao", "Eric Yihong", ""]]}, {"id": "2103.13235", "submitter": "Elias Iosif", "authors": "Elias Iosif and Klitos Christodoulou and Andreas Vlachos", "title": "Web Mining for Estimating Regulatory Blockchain Readiness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regulatory framework of cryptocurrencies (and, in general, blockchain\ntokens) is of paramount importance. This framework drives nearly all key\ndecisions in the respective business areas. In this work, a computational model\nis proposed for quantitatively estimating the regulatory stance of countries\nwith respect to cryptocurrencies. This is conducted via web mining utilizing\nweb search engines. The proposed model is experimentally validated. In\naddition, unsupervised learning (clustering) is applied for better analyzing\nthe automatically derived estimations. Overall, very good performance is\nachieved by the proposed algorithmic approach.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 14:54:01 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Iosif", "Elias", ""], ["Christodoulou", "Klitos", ""], ["Vlachos", "Andreas", ""]]}, {"id": "2103.13506", "submitter": "Lei Guo", "authors": "Lei Guo, Hongzhi Yin, Tong Chen, Xiangliang Zhang, Kai Zheng", "title": "Hierarchical Hyperedge Embedding-based Representation Learning for Group\n  Recommendation", "comments": null, "journal-ref": "ACM Transactions on Information Systems. 2021", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study group recommendation in a particular scenario, namely\nOccasional Group Recommendation (OGR). Most existing works have addressed OGR\nby aggregating group members' personal preferences to learn the group\nrepresentation. However, the representation learning for a group is most\ncomplex beyond the fusion of group member representation, as the personal\npreferences and group preferences may be in different spaces. In addition, the\nlearned user representation is not accurate due to the sparsity of users'\ninteraction data. Moreover, the group similarity in terms of common group\nmembers has been overlooked, which however has the great potential to improve\nthe group representation learning. In this work, we focus on addressing the\nabove challenges in group representation learning task, and devise a\nhierarchical hyperedge embedding-based group recommender, namely HyperGroup.\nSpecifically, we propose to leverage the user-user interactions to alleviate\nthe sparsity issue of user-item interactions, and design a GNN-based\nrepresentation learning network to enhance the learning of individuals'\npreferences from their friends' preferences, which provides a solid foundation\nfor learning groups' preferences. To exploit the group similarity to learn a\nmore accurate group representation from highly limited group-item interactions,\nwe connect all groups as a network of overlapping sets, and treat the task of\ngroup preference learning as embedding hyperedges in a hypergraph, where an\ninductive hyperedge embedding method is proposed. To further enhance the\ngroup-level preference modeling, we develop a joint training strategy to learn\nboth user-item and group-item interactions in the same process. We conduct\nextensive experiments on two real-world datasets and the experimental results\ndemonstrate the superiority of our proposed HyperGroup in comparison to the\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 22:26:27 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Guo", "Lei", ""], ["Yin", "Hongzhi", ""], ["Chen", "Tong", ""], ["Zhang", "Xiangliang", ""], ["Zheng", "Kai", ""]]}, {"id": "2103.13526", "submitter": "Francesco Osborne", "authors": "Thiviyan Thanapalasingam, Francesco Osborne, Aliaksandr Birukou and\n  Enrico Motta", "title": "Ontology-Based Recommendation of Editorial Products", "comments": "In: The Semantic Web - ISWC 2018. Lecture Notes in Computer Science,\n  vol 11137. Springer, Cham", "journal-ref": null, "doi": "10.1007/978-3-030-00668-6_21", "report-no": null, "categories": "cs.DL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Major academic publishers need to be able to analyse their vast catalogue of\nproducts and select the best items to be marketed in scientific venues. This is\na complex exercise that requires characterising with a high precision the\ntopics of thousands of books and matching them with the interests of the\nrelevant communities. In Springer Nature, this task has been traditionally\nhandled manually by publishing editors. However, the rapid growth in the number\nof scientific publications and the dynamic nature of the Computer Science\nlandscape has made this solution increasingly inefficient. We have addressed\nthis issue by creating Smart Book Recommender (SBR), an ontology-based\nrecommender system developed by The Open University (OU) in collaboration with\nSpringer Nature, which supports their Computer Science editorial team in\nselecting the products to market at specific venues. SBR recommends books,\njournals, and conference proceedings relevant to a conference by taking\nadvantage of a semantically enhanced representation of about 27K editorial\nproducts. This is based on the Computer Science Ontology, a very large-scale,\nautomatically generated taxonomy of research areas. SBR also allows users to\ninvestigate why a certain publication was suggested by the system. It does so\nby means of an interactive graph view that displays the topic taxonomy of the\nrecommended editorial product and compares it with the topic-centric\ncharacterization of the input conference. An evaluation carried out with seven\nSpringer Nature editors and seven OU researchers has confirmed the\neffectiveness of the solution.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 23:23:53 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Thanapalasingam", "Thiviyan", ""], ["Osborne", "Francesco", ""], ["Birukou", "Aliaksandr", ""], ["Motta", "Enrico", ""]]}, {"id": "2103.13527", "submitter": "Francesco Osborne", "authors": "Angelo A. Salatino, Francesco Osborne, Aliaksandr Birukou and Enrico\n  Motta", "title": "Improving Editorial Workflow and Metadata Quality at Springer Nature", "comments": "In: The Semantic Web - ISWC 2019. Lecture Notes in Computer Science,\n  vol 11779. Springer, Cham", "journal-ref": null, "doi": "10.1007/978-3-030-30796-7_31", "report-no": null, "categories": "cs.DL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying the research topics that best describe the scope of a scientific\npublication is a crucial task for editors, in particular because the quality of\nthese annotations determine how effectively users are able to discover the\nright content in online libraries. For this reason, Springer Nature, the\nworld's largest academic book publisher, has traditionally entrusted this task\nto their most expert editors. These editors manually analyse all new books,\npossibly including hundreds of chapters, and produce a list of the most\nrelevant topics. Hence, this process has traditionally been very expensive,\ntime-consuming, and confined to a few senior editors. For these reasons, back\nin 2016 we developed Smart Topic Miner (STM), an ontology-driven application\nthat assists the Springer Nature editorial team in annotating the volumes of\nall books covering conference proceedings in Computer Science. Since then STM\nhas been regularly used by editors in Germany, China, Brazil, India, and Japan,\nfor a total of about 800 volumes per year. Over the past three years the\ninitial prototype has iteratively evolved in response to feedback from the\nusers and evolving requirements. In this paper we present the most recent\nversion of the tool and describe the evolution of the system over the years,\nthe key lessons learnt, and the impact on the Springer Nature workflow. In\nparticular, our solution has drastically reduced the time needed to annotate\nproceedings and significantly improved their discoverability, resulting in 9.3\nmillion additional downloads. We also present a user study involving 9 editors,\nwhich yielded excellent results in term of usability, and report an evaluation\nof the new topic classifier used by STM, which outperforms previous versions in\nrecall and F-measure.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 23:23:59 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Salatino", "Angelo A.", ""], ["Osborne", "Francesco", ""], ["Birukou", "Aliaksandr", ""], ["Motta", "Enrico", ""]]}, {"id": "2103.13864", "submitter": "Febin Sebastian Elayanithottathil", "authors": "Febin Sebastian Elayanithottathil and Janis Keuper", "title": "A Retail Product Categorisation Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Most eCommerce applications, like web-shops have millions of products. In\nthis context, the identification of similar products is a common sub-task,\nwhich can be utilized in the implementation of recommendation systems, product\nsearch engines and internal supply logistics. Providing this data set, our goal\nis to boost the evaluation of machine learning methods for the prediction of\nthe category of the retail products from tuples of images and descriptions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 14:23:48 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 09:31:56 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Elayanithottathil", "Febin Sebastian", ""], ["Keuper", "Janis", ""]]}, {"id": "2103.14000", "submitter": "Ke Yang", "authors": "Meike Zehlike, Ke Yang, Julia Stoyanovich", "title": "Fairness in Ranking: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, there has been much work on incorporating fairness\nrequirements into algorithmic rankers, with contributions coming from the data\nmanagement, algorithms, information retrieval, and recommender systems\ncommunities. In this survey we give a systematic overview of this work,\noffering a broad perspective that connects formalizations and algorithmic\napproaches across subfields. An important contribution of our work is in\ndeveloping a common narrative around the value frameworks that motivate\nspecific fairness-enhancing interventions in ranking. This allows us to unify\nthe presentation of mitigation objectives and of algorithmic techniques to help\nmeet those objectives or identify trade-offs.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:38:20 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 23:48:52 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zehlike", "Meike", ""], ["Yang", "Ke", ""], ["Stoyanovich", "Julia", ""]]}, {"id": "2103.14371", "submitter": "Ramya C", "authors": "Dr. Ramya C, Dr. Shreedhara K S", "title": "A PSO Strategy of Finding Relevant Web Documents using a New Similarity\n  Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the world of the Internet and World Wide Web, which offers a tremendous\namount of information, an increasing emphasis is being given to searching\nservices and functionality. Currently, a majority of web portals offer their\nsearching utilities, be it better or worse. These can search for the content\nwithin the sites, mainly text the textual content of documents. In this paper a\nnovel similarity measure called SMDR (Similarity Measure for Documents\nRetrieval) is proposed to help retrieve more similar documents from the\nrepository thus contributing considerably to the effectiveness of Web\nInformation Retrieval (WIR) process. Bio-inspired PSO methodology is used with\nthe intent to reduce the response time of the system and optimizes WIR process,\nhence contributes to the efficiency of the system. This paper also demonstrates\na comparative study of the proposed system with the existing method in terms of\naccuracy, sensitivity, F-measure and specificity. Finally, extensive\nexperiments are conducted on CACM collections. Better precision-recall rates\nare achieved than the existing system. Experimental results demonstrate the\neffectiveness and efficiency of the proposed system.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 10:19:38 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["C", "Dr. Ramya", ""], ["S", "Dr. Shreedhara K", ""]]}, {"id": "2103.14455", "submitter": "Casper Hansen", "authors": "Christian Hansen, Casper Hansen, Jakob Grue Simonsen, Christina Lioma", "title": "Projected Hamming Dissimilarity for Bit-Level Importance Coding in\n  Collaborative Filtering", "comments": "Proceedings of the 2021 World Wide Web Conference, published under\n  Creative Commons CC-BY 4.0 License", "journal-ref": null, "doi": "10.1145/3442381.3450011", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When reasoning about tasks that involve large amounts of data, a common\napproach is to represent data items as objects in the Hamming space where\noperations can be done efficiently and effectively. Object similarity can then\nbe computed by learning binary representations (hash codes) of the objects and\ncomputing their Hamming distance. While this is highly efficient, each bit\ndimension is equally weighted, which means that potentially discriminative\ninformation of the data is lost. A more expressive alternative is to use\nreal-valued vector representations and compute their inner product; this allows\nvarying the weight of each dimension but is many magnitudes slower. To fix\nthis, we derive a new way of measuring the dissimilarity between two objects in\nthe Hamming space with binary weighting of each dimension (i.e., disabling\nbits): we consider a field-agnostic dissimilarity that projects the vector of\none object onto the vector of the other. When working in the Hamming space,\nthis results in a novel projected Hamming dissimilarity, which by choice of\nprojection, effectively allows a binary importance weighting of the hash code\nof one object through the hash code of the other. We propose a variational\nhashing model for learning hash codes optimized for this projected Hamming\ndissimilarity, and experimentally evaluate it in collaborative filtering\nexperiments. The resultant hash codes lead to effectiveness gains of up to +7%\nin NDCG and +14% in MRR compared to state-of-the-art hashing-based\ncollaborative filtering baselines, while requiring no additional storage and no\ncomputational overhead compared to using the Hamming distance.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 13:22:31 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Hansen", "Christian", ""], ["Hansen", "Casper", ""], ["Simonsen", "Jakob Grue", ""], ["Lioma", "Christina", ""]]}, {"id": "2103.14460", "submitter": "Casper Hansen", "authors": "Christian Hansen, Casper Hansen, Jakob Grue Simonsen, Stephen Alstrup,\n  Christina Lioma", "title": "Unsupervised Multi-Index Semantic Hashing", "comments": "Proceedings of the 2021 World Wide Web Conference, published under\n  Creative Commons CC-BY 4.0 License", "journal-ref": null, "doi": "10.1145/3442381.3450014", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic hashing represents documents as compact binary vectors (hash codes)\nand allows both efficient and effective similarity search in large-scale\ninformation retrieval. The state of the art has primarily focused on learning\nhash codes that improve similarity search effectiveness, while assuming a\nbrute-force linear scan strategy for searching over all the hash codes, even\nthough much faster alternatives exist. One such alternative is multi-index\nhashing, an approach that constructs a smaller candidate set to search over,\nwhich depending on the distribution of the hash codes can lead to sub-linear\nsearch time. In this work, we propose Multi-Index Semantic Hashing (MISH), an\nunsupervised hashing model that learns hash codes that are both effective and\nhighly efficient by being optimized for multi-index hashing. We derive novel\ntraining objectives, which enable to learn hash codes that reduce the candidate\nsets produced by multi-index hashing, while being end-to-end trainable. In\nfact, our proposed training objectives are model agnostic, i.e., not tied to\nhow the hash codes are generated specifically in MISH, and are straight-forward\nto include in existing and future semantic hashing models. We experimentally\ncompare MISH to state-of-the-art semantic hashing baselines in the task of\ndocument similarity search. We find that even though multi-index hashing also\nimproves the efficiency of the baselines compared to a linear scan, they are\nstill upwards of 33% slower than MISH, while MISH is still able to obtain\nstate-of-the-art effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 13:33:48 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Hansen", "Christian", ""], ["Hansen", "Casper", ""], ["Simonsen", "Jakob Grue", ""], ["Alstrup", "Stephen", ""], ["Lioma", "Christina", ""]]}, {"id": "2103.14748", "submitter": "Alejandro Bellogin", "authors": "Miriam Fern\\'andez and Alejandro Bellog\\'in and Iv\\'an Cantador", "title": "Analysing the Effect of Recommendation Algorithms on the Amplification\n  of Misinformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation algorithms have been pointed out as one of the major culprits\nof misinformation spreading in the digital sphere. However, it is still unclear\nhow these algorithms really propagate misinformation, e.g., it has not been\nshown which particular recommendation approaches are more prone to suggest\nmisinforming items, or which internal parameters of the algorithms could be\ninfluencing more on their misinformation propagation capacity.\n  Motivated by this fact, in this paper we present an analysis of the effect of\nsome of the most popular recommendation algorithms on the spread of\nmisinformation in Twitter. A set of guidelines on how to adapt these algorithms\nis provided based on such analysis and a comprehensive review of the research\nliterature. A dataset is also generated and released to the scientific\ncommunity to stimulate discussions on the future design and development of\nrecommendation algorithms to counter misinformation. The dataset includes\neditorially labelled news items and claims regarding their misinformation\nnature.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 21:53:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Fern\u00e1ndez", "Miriam", ""], ["Bellog\u00edn", "Alejandro", ""], ["Cantador", "Iv\u00e1n", ""]]}, {"id": "2103.14866", "submitter": "Yanchao Tan", "authors": "Yanchao Tan, Carl Yang, Xiangyu Wei, Yun Ma, Xiaolin Zheng", "title": "Multi-Facet Recommender Networks with Spherical Optimization", "comments": "Accept by ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit feedback is widely explored by modern recommender systems. Since the\nfeedback is often sparse and imbalanced, it poses great challenges to the\nlearning of complex interactions among users and items. Metric learning has\nbeen proposed to capture user-item interactions from implicit feedback, but\nexisting methods only represent users and items in a single metric space,\nignoring the fact that users can have multiple preferences and items can have\nmultiple properties, which leads to potential conflicts limiting their\nperformance in recommendation. To capture the multiple facets of user\npreferences and item properties while resolving their potential conflicts, we\npropose the novel framework of Multi-fAcet Recommender networks with Spherical\noptimization (MARS). By designing a cross-facet similarity measurement, we\nproject users and items into multiple metric spaces for fine-grained\nrepresentation learning, and compare them only in the proper spaces.\nFurthermore, we devise a spherical optimization strategy to enhance the\neffectiveness and robustness of the multi-facet recommendation framework.\nExtensive experiments on six real-world benchmark datasets show drastic\nperformance gains brought by MARS, which constantly achieves up to 40\\%\nimprovements over the state-of-the-art baselines regarding both HR and nDCG\nmetrics.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 09:53:44 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tan", "Yanchao", ""], ["Yang", "Carl", ""], ["Wei", "Xiangyu", ""], ["Ma", "Yun", ""], ["Zheng", "Xiaolin", ""]]}, {"id": "2103.14934", "submitter": "Zhuoren Jiang", "authors": "Zhuoren Jiang, Xiaozhong Liu, Liangcai Gao, Zhi Tang", "title": "Community-based Cyberreading for Information Understanding", "comments": null, "journal-ref": "SIGIR 2016", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although the content in scientific publications is increasingly challenging,\nit is necessary to investigate another important problem, that of scientific\ninformation understanding. For this proposed problem, we investigate novel\nmethods to assist scholars (readers) to better understand scientific\npublications by enabling physical and virtual collaboration. For physical\ncollaboration, an algorithm will group readers together based on their profiles\nand reading behavior, and will enable the cyberreading collaboration within a\nonline reading group. For virtual collaboration, instead of pushing readers to\ncommunicate with others, we cluster readers based on their estimated\ninformation needs. For each cluster, a learning to rank model will be generated\nto recommend readers' communitized resources (i.e., videos, slides, and wikis)\nto help them understand the target publication.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 16:00:05 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Jiang", "Zhuoren", ""], ["Liu", "Xiaozhong", ""], ["Gao", "Liangcai", ""], ["Tang", "Zhi", ""]]}, {"id": "2103.15386", "submitter": "Hui Wang", "authors": "Hui Wang, Wan-Lei Zhao and Xiangxiang Zeng", "title": "Large-Scale Approximate k-NN Graph Construction on GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  k-nearest neighbor graph is a key data structure in many disciplines such as\nmanifold learning, machine learning and information retrieval, etc. NN-Descent\nwas proposed as an effective solution for the graph construction problem.\nHowever, it cannot be directly transplanted to GPU due to the intensive memory\naccesses required in the approach. In this paper, NN-Descent has been\nredesigned to adapt to the GPU architecture. In particular, the number of\nmemory accesses has been reduced significantly. The redesign fully exploits the\nparallelism of the GPU hardware. In the meantime, the genericness as well as\nthe simplicity of NN-Descent are well-preserved. In addition, a simple but\neffective k-NN graph merge approach is presented. It allows two graphs to be\nmerged efficiently on GPUs. More importantly, it makes the construction of\nhigh-quality k-NN graphs for out-of-GPU-memory datasets tractable. The results\nshow that our approach is 100-250x faster than single-thread NN-Descent and is\n2.5-5x faster than existing GPU-based approaches.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 07:24:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wang", "Hui", ""], ["Zhao", "Wan-Lei", ""], ["Zeng", "Xiangxiang", ""]]}, {"id": "2103.15454", "submitter": "ByungSoo Ko", "authors": "Geonmo Gu, Byungsoo Ko, Han-Gyu Kim", "title": "Proxy Synthesis: Learning with Synthetic Classes for Deep Metric\n  Learning", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main purposes of deep metric learning is to construct an embedding\nspace that has well-generalized embeddings on both seen (training) classes and\nunseen (test) classes. Most existing works have tried to achieve this using\ndifferent types of metric objectives and hard sample mining strategies with\ngiven training data. However, learning with only the training data can be\noverfitted to the seen classes, leading to the lack of generalization\ncapability on unseen classes. To address this problem, we propose a simple\nregularizer called Proxy Synthesis that exploits synthetic classes for stronger\ngeneralization in deep metric learning. The proposed method generates synthetic\nembeddings and proxies that work as synthetic classes, and they mimic unseen\nclasses when computing proxy-based losses. Proxy Synthesis derives an embedding\nspace considering class relations and smooth decision boundaries for robustness\non unseen classes. Our method is applicable to any proxy-based losses,\nincluding softmax and its variants. Extensive experiments on four famous\nbenchmarks in image retrieval tasks demonstrate that Proxy Synthesis\nsignificantly boosts the performance of proxy-based losses and achieves\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:39:07 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gu", "Geonmo", ""], ["Ko", "Byungsoo", ""], ["Kim", "Han-Gyu", ""]]}, {"id": "2103.15514", "submitter": "Haomei Duan", "authors": "Haomei Duan and Jinghua Zhu", "title": "Context-aware short-term interest first model for session-based\n  recommendation", "comments": "15 pages,5 figures,8th International Conference on Computer Science\n  and Information Technology (CoSIT 2021), March 27 ~ 28, 2021, Sydney,\n  Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the case that user profiles are not available, the recommendation based on\nanonymous session is particularly important, which aims to predict the items\nthat the user may click at the next moment based on the user's access sequence\nover a while. In recent years, with the development of recurrent neural\nnetwork, attention mechanism, and graph neural network, the performance of\nsession-based recommendation has been greatly improved. However, the previous\nmethods did not comprehensively consider the context dependencies and\nshort-term interest first of the session. Therefore, we propose a context-aware\nshort-term interest first model (CASIF).The aim of this paper is improve the\naccuracy of recommendations by combining context and short-term interest. In\nCASIF, we dynamically construct a graph structure for session sequences and\ncapture rich context dependencies via graph neural network (GNN), latent\nfeature vectors are captured as inputs of the next step. Then we build the\nshort-term interest first module, which can to capture the user's general\ninterest from the session in the context of long-term memory, at the same time\nget the user's current interest from the item of the last click. In the end,\nthe short-term and long-term interest are combined as the final interest and\nmultiplied by the candidate vector to obtain the recommendation probability.\nFinally, a large number of experiments on two real-world datasets demonstrate\nthe effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 11:36:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Duan", "Haomei", ""], ["Zhu", "Jinghua", ""]]}, {"id": "2103.15581", "submitter": "Vishwani Gupta", "authors": "Vishwani Gupta and Katharina Beckh and Sven Giesselbach and Dennis\n  Wegener and Tim Wirtz", "title": "Supporting verification of news articles with automated search for\n  semantically similar articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fake information poses one of the major threats for society in the 21st\ncentury. Identifying misinformation has become a key challenge due to the\namount of fake news that is published daily. Yet, no approach is established\nthat addresses the dynamics and versatility of fake news editorials. Instead of\nclassifying content, we propose an evidence retrieval approach to handle fake\nnews. The learning task is formulated as an unsupervised machine learning\nproblem. For validation purpose, we provide the user with a set of news\narticles from reliable news sources supporting the hypothesis of the news\narticle in query and the final decision is left to the user. Technically we\npropose a two-step process: (i) Aggregation-step: With information extracted\nfrom the given text we query for similar content from reliable news sources.\n(ii) Refining-step: We narrow the supporting evidence down by measuring the\nsemantic distance of the text with the collection from step (i). The distance\nis calculated based on Word2Vec and the Word Mover's Distance. In our\nexperiments, only content that is below a certain distance threshold is\nconsidered as supporting evidence. We find that our approach is agnostic to\nconcept drifts, i.e. the machine learning task is independent of the hypotheses\nin a text. This makes it highly adaptable in times where fake news is as\ndiverse as classical news is. Our pipeline offers the possibility for further\nanalysis in the future, such as investigating bias and differences in news\nreporting.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:56:59 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gupta", "Vishwani", ""], ["Beckh", "Katharina", ""], ["Giesselbach", "Sven", ""], ["Wegener", "Dennis", ""], ["Wirtz", "Tim", ""]]}, {"id": "2103.15950", "submitter": "Kalpa Gunaratna", "authors": "Kalpa Gunaratna, Yu Wang, Hongxia Jin", "title": "Entity Context Graph: Learning Entity Representations\n  fromSemi-Structured Textual Sources on the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge is captured in the form of entities and their relationships and\nstored in knowledge graphs. Knowledge graphs enhance the capabilities of\napplications in many different areas including Web search, recommendation, and\nnatural language understanding. This is mainly because, entities enable\nmachines to understand things that go beyond simple tokens. Many modern\nalgorithms use learned entity embeddings from these structured representations.\nHowever, building a knowledge graph takes time and effort, hence very costly\nand nontrivial. On the other hand, many Web sources describe entities in some\nstructured format and therefore, finding ways to get them into useful entity\nknowledge is advantageous. We propose an approach that processes entity centric\ntextual knowledge sources to learn entity embeddings and in turn avoids the\nneed for a traditional knowledge graph. We first extract triples into the new\nrepresentation format that does not use traditional complex triple extraction\nmethods defined by pre-determined relationship labels. Then we learn entity\nembeddings through this new type of triples. We show that the embeddings\nlearned from our approach are: (i) high quality and comparable to a known\nknowledge graph-based embeddings and can be used to improve them further, (ii)\nbetter than a contextual language model-based entity embeddings, and (iii) easy\nto compute and versatile in domain-specific applications where a knowledge\ngraph is not readily available\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 20:52:14 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Gunaratna", "Kalpa", ""], ["Wang", "Yu", ""], ["Jin", "Hongxia", ""]]}, {"id": "2103.15953", "submitter": "Jussi Karlgren", "authors": "Rosie Jones, Ben Carterette, Ann Clifton, Maria Eskevich, Gareth J. F.\n  Jones, Jussi Karlgren, Aasish Pappu, Sravana Reddy, Yongze Yu", "title": "TREC 2020 Podcasts Track Overview", "comments": null, "journal-ref": "The Proceedings of the Twenty-Ninth Text REtrieval Conference\n  Proceedings (TREC 2020)", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Podcast Track is new at the Text Retrieval Conference (TREC) in 2020. The\npodcast track was designed to encourage research into podcasts in the\ninformation retrieval and NLP research communities. The track consisted of two\nshared tasks: segment retrieval and summarization, both based on a dataset of\nover 100,000 podcast episodes (metadata, audio, and automatic transcripts)\nwhich was released concurrently with the track. The track generated\nconsiderable interest, attracted hundreds of new registrations to TREC and\nfifteen teams, mostly disjoint between search and summarization, made final\nsubmissions for assessment. Deep learning was the dominant experimental\napproach for both search experiments and summarization. This paper gives an\noverview of the tasks and the results of the participants' experiments. The\ntrack will return to TREC 2021 with the same two tasks, incorporating slight\nmodifications in response to participant feedback.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 20:58:10 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Jones", "Rosie", ""], ["Carterette", "Ben", ""], ["Clifton", "Ann", ""], ["Eskevich", "Maria", ""], ["Jones", "Gareth J. F.", ""], ["Karlgren", "Jussi", ""], ["Pappu", "Aasish", ""], ["Reddy", "Sravana", ""], ["Yu", "Yongze", ""]]}, {"id": "2103.16061", "submitter": "Qifan Chen", "authors": "Qifan Chen, Yang Lu, Charmaine Tam, Simon Poon", "title": "A Novel Approach to Detect Redundant Activity Labels For More\n  Representative Event Logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The insights revealed from process mining heavily rely on the quality of\nevent logs. Activities extracted from healthcare information systems with the\nfree-text nature may lead to inconsistent labels. Such inconsistency would then\nlead to redundancy of activity labels, which refer to labels that have\ndifferent syntax but share the same behaviours. The identifications of these\nlabels from data-driven process discovery are difficult and rely heavily on\nresource-intensive human review. Existing work achieves low accuracy either\nredundant activity labels are in low occurrence frequency or the existence of\nnumerical data values as attributes in event logs. However, these phenomena are\ncommonly observed in healthcare information systems. In this paper, we propose\nan approach to detect redundant activity labels using control-flow relations\nand numerical data values from event logs. Natural Language Processing is also\nintegrated into our method to assess semantic similarity between labels, which\nprovides users with additional insights. We have evaluated our approach through\nsynthetic logs generated from the real-life Sepsis log and a case study using\nthe MIMIC-III data set. The results demonstrate that our approach can\nsuccessfully detect redundant activity labels. This approach can add value to\nthe preprocessing step to generate more representative event logs for process\nmining tasks in the healthcare domain.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 04:18:39 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 03:43:29 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Chen", "Qifan", ""], ["Lu", "Yang", ""], ["Tam", "Charmaine", ""], ["Poon", "Simon", ""]]}, {"id": "2103.16103", "submitter": "Minjin Choi", "authors": "Minjin Choi, Yoonki Jeong, Joonseok Lee, and Jongwuk Lee", "title": "Local Collaborative Autoencoders", "comments": "In Proceedings of the Fourteenth ACM Inter-national Conference on Web\n  Search and Data Mining. 9 pages", "journal-ref": null, "doi": "10.1145/3437963.3441808", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Top-N recommendation is a challenging problem because complex and sparse\nuser-item interactions should be adequately addressed to achieve high-quality\nrecommendation results. The local latent factor approach has been successfully\nused with multiple local models to capture diverse user preferences with\ndifferent sub-communities. However, previous studies have not fully explored\nthe potential of local models, and failed to identify many small and coherent\nsub-communities. In this paper, we present Local Collaborative Autoencoders\n(LOCA), a generalized local latent factor framework. Specifically, LOCA adopts\ndifferent neighborhood ranges at the training and inference stages. Besides,\nLOCA uses a novel sub-community discovery method, maximizing the coverage of a\nunion of local models and employing a large number of diverse local models. By\nadopting autoencoders as the base model, LOCA captures latent non-linear\npatterns representing meaningful user-item interactions within sub-communities.\nOur experimental results demonstrate that LOCA is scalable and outperforms\nstate-of-the-art models on several public benchmarks, by 2.99~4.70% in Recall\nand 1.02~7.95% in NDCG, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 06:26:39 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Choi", "Minjin", ""], ["Jeong", "Yoonki", ""], ["Lee", "Joonseok", ""], ["Lee", "Jongwuk", ""]]}, {"id": "2103.16104", "submitter": "Minjin Choi", "authors": "Minjin Choi, jinhong Kim, Joonseok Lee, Hyunjung Shim and Jongwuk Lee", "title": "Session-aware Linear Item-Item Models for Session-based Recommendation", "comments": "In Proceedings of the Web Conference 2021. 12 pages", "journal-ref": null, "doi": "10.1145/3442381.3450005", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommendation aims at predicting the next item given a\nsequence of previous items consumed in the session, e.g., on e-commerce or\nmultimedia streaming services. Specifically, session data exhibits some unique\ncharacteristics, i.e., session consistency and sequential dependency over items\nwithin the session, repeated item consumption, and session timeliness. In this\npaper, we propose simple-yet-effective linear models for considering the\nholistic aspects of the sessions. The comprehensive nature of our models helps\nimprove the quality of session-based recommendation. More importantly, it\nprovides a generalized framework for reflecting different perspectives of\nsession data. Furthermore, since our models can be solved by closed-form\nsolutions, they are highly scalable. Experimental results demonstrate that the\nproposed linear models show competitive or state-of-the-art performance in\nvarious metrics on several real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 06:28:40 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 06:49:07 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Choi", "Minjin", ""], ["Kim", "jinhong", ""], ["Lee", "Joonseok", ""], ["Shim", "Hyunjung", ""], ["Lee", "Jongwuk", ""]]}, {"id": "2103.16624", "submitter": "Ikechukwu Onyenwe", "authors": "D.C. Asogwa, S.O. Anigbogu, I.E. Onyenwe, F.A. Sani", "title": "Text Classification Using Hybrid Machine Learning Algorithms on Big Data", "comments": "8 pages, 2 figures, 8 tables, Journal", "journal-ref": "International Journal of Trend in Research and Development, Volume\n  6(5), ISSN: 2394-9333, 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there are unprecedented data growth originating from different\nonline platforms which contribute to big data in terms of volume, velocity,\nvariety and veracity (4Vs). Given this nature of big data which is\nunstructured, performing analytics to extract meaningful information is\ncurrently a great challenge to big data analytics. Collecting and analyzing\nunstructured textual data allows decision makers to study the escalation of\ncomments/posts on our social media platforms. Hence, there is need for\nautomatic big data analysis to overcome the noise and the non-reliability of\nthese unstructured dataset from the digital media platforms. However, current\nmachine learning algorithms used are performance driven focusing on the\nclassification/prediction accuracy based on known properties learned from the\ntraining samples. With the learning task in a large dataset, most machine\nlearning models are known to require high computational cost which eventually\nleads to computational complexity. In this work, two supervised machine\nlearning algorithms are combined with text mining techniques to produce a\nhybrid model which consists of Na\\\"ive Bayes and support vector machines (SVM).\nThis is to increase the efficiency and accuracy of the results obtained and\nalso to reduce the computational cost and complexity. The system also provides\nan open platform where a group of persons with a common interest can share\ntheir comments/messages and these comments classified automatically as legal or\nillegal. This improves the quality of conversation among users. The hybrid\nmodel was developed using WEKA tools and Java programming language. The result\nshows that the hybrid model gave 96.76% accuracy as against the 61.45% and\n69.21% of the Na\\\"ive Bayes and SVM models respectively.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 19:02:48 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Asogwa", "D. C.", ""], ["Anigbogu", "S. O.", ""], ["Onyenwe", "I. E.", ""], ["Sani", "F. A.", ""]]}, {"id": "2103.16654", "submitter": "Mohammadreza Fani Sani", "authors": "Mansoureh Yari Eili, Jalal Rezaeenour, Mohammadreza Fani Sani", "title": "A Systematic Literature Review on Process-Aware Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Considering processes of a business in a recommender system is highly\nadvantageous. Although most studies in the business process analysis domain are\nof descriptive and predictive nature, the feasibility of constructing a\nprocess-aware recommender system is assessed in a few works. One reason can be\nthe lack of knowledge on process mining potential for recommendation problems.\nTherefore, this paper aims to identify and analyze the published studies on\nprocess-aware recommender system techniques in business process management and\nprocess mining domain. A systematic review was conducted on 33 academic\narticles published between 2008 and 2020 according to several aspects. In this\nregard, we provide a state-of-the-art review with critical details and\nresearchers with a better perception of which path to pursue in this field.\nMoreover, based on a knowledge base and holistic perspective, we discuss some\nresearch gaps and open challenges in this field.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 19:56:29 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 08:21:48 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Eili", "Mansoureh Yari", ""], ["Rezaeenour", "Jalal", ""], ["Sani", "Mohammadreza Fani", ""]]}, {"id": "2103.16669", "submitter": "Koustav Rudra", "authors": "Koustav Rudra and Zeon Trevor Fernando and Avishek Anand", "title": "An In-depth Analysis of Passage-Level Label Transfer for Contextual\n  Document Ranking", "comments": "Paper is about the performance analysis of contextual ranking\n  strategies in an ad-hoc document retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently introduced pre-trained contextualized autoregressive models like\nBERT have shown improvements in document retrieval tasks. One of the major\nlimitations of the current approaches can be attributed to the manner they deal\nwith variable-size document lengths using a fixed input BERT model. Common\napproaches either truncate or split longer documents into small\nsentences/passages and subsequently label them - using the original document\nlabel or from another externally trained model. In this paper, we conduct a\ndetailed study of the design decisions about splitting and label transfer on\nretrieval effectiveness and efficiency. We find that direct transfer of\nrelevance labels from documents to passages introduces label noise that\nstrongly affects retrieval effectiveness for large training datasets. We also\nfind that query processing times are adversely affected by fine-grained\nsplitting schemes. As a remedy, we propose a careful passage level labelling\nscheme using weak supervision that delivers improved performance (3-14% in\nterms of nDCG score) over most of the recently proposed models for ad-hoc\nretrieval while maintaining manageable computational complexity on four diverse\ndocument retrieval datasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 20:28:02 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Rudra", "Koustav", ""], ["Fernando", "Zeon Trevor", ""], ["Anand", "Avishek", ""]]}, {"id": "2103.16940", "submitter": "ByungSoo Ko", "authors": "Byungsoo Ko, Geonmo Gu, Han-Gyu Kim", "title": "Learning with Memory-based Virtual Classes for Deep Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The core of deep metric learning (DML) involves learning visual similarities\nin high-dimensional embedding space. One of the main challenges is to\ngeneralize from seen classes of training data to unseen classes of test data.\nRecent works have focused on exploiting past embeddings to increase the number\nof instances for the seen classes. Such methods achieve performance improvement\nvia augmentation, while the strong focus on seen classes still remains. This\ncan be undesirable for DML, where training and test data exhibit entirely\ndifferent classes. In this work, we present a novel training strategy for DML\ncalled MemVir. Unlike previous works, MemVir memorizes both embedding features\nand class weights to utilize them as additional virtual classes. The\nexploitation of virtual classes not only utilizes augmented information for\ntraining but also alleviates a strong focus on seen classes for better\ngeneralization. Moreover, we embed the idea of curriculum learning by slowly\nadding virtual classes for a gradual increase in learning difficulty, which\nimproves the learning stability as well as the final performance. MemVir can be\neasily applied to many existing loss functions without any modification.\nExtensive experimental results on famous benchmarks demonstrate the superiority\nof MemVir over state-of-the-art competitors. Code of MemVir will be publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 09:44:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Ko", "Byungsoo", ""], ["Gu", "Geonmo", ""], ["Kim", "Han-Gyu", ""]]}]