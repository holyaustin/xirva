[{"id": "1310.0250", "submitter": "Patrick O. Glauner", "authors": "Patrick O. Glauner, Jan Iwaszkiewicz, Jean-Yves Le Meur and Tibor\n  Simko", "title": "Use of Solr and Xapian in the Invenio document repository software", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invenio is a free comprehensive web-based document repository and digital\nlibrary software suite originally developed at CERN. It can serve a variety of\nuse cases from an institutional repository or digital library to a web journal.\nIn order to fully use full-text documents for efficient search and ranking,\nSolr was integrated into Invenio through a generic bridge. Solr indexes\nextracted full-texts and most relevant metadata. Consequently, Invenio takes\nadvantage of Solr's efficient search and word similarity ranking capabilities.\nIn this paper, we first give an overview of Invenio, its capabilities and\nfeatures. We then present our open source Solr integration as well as\nscalability challenges that arose for an Invenio-based multi-million record\nrepository: the CERN Document Server. We also compare our Solr adapter to an\nalternative Xapian adapter using the same generic bridge. Both integrations are\ndistributed with the Invenio package and ready to be used by the institutions\nusing or adopting Invenio.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 11:37:06 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Glauner", "Patrick O.", ""], ["Iwaszkiewicz", "Jan", ""], ["Meur", "Jean-Yves Le", ""], ["Simko", "Tibor", ""]]}, {"id": "1310.0894", "submitter": "Richard Chow", "authors": "Richard Chow, Hongxia Jin, Bart Knijnenburg, Gokay Saldamli", "title": "Differential Data Analysis for Recommender Systems", "comments": "Extended version of RecSys 2013 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present techniques to characterize which data is important to a\nrecommender system and which is not. Important data is data that contributes\nmost to the accuracy of the recommendation algorithm, while less important data\ncontributes less to the accuracy or even decreases it. Characterizing the\nimportance of data has two potential direct benefits: (1) increased privacy and\n(2) reduced data management costs, including storage. For privacy, we enable\nincreased recommendation accuracy for comparable privacy levels using existing\ndata obfuscation techniques. For storage, our results indicate that we can\nachieve large reductions in recommendation data and yet maintain recommendation\naccuracy.\n  Our main technique is called differential data analysis. The name is inspired\nby other sorts of differential analysis, such as differential power analysis\nand differential cryptanalysis, where insight comes through analysis of\nslightly differing inputs. In differential data analysis we chunk the data and\ncompare results in the presence or absence of each chunk. We present results\napplying differential data analysis to two datasets and three different kinds\nof attributes. The first attribute is called user hardship. This is a novel\nattribute, particularly relevant to location datasets, that indicates how\nburdensome a data point was to achieve. The second and third attributes are\nmore standard: timestamp and user rating. For user rating, we confirm previous\nwork concerning the increased importance to the recommender of data\ncorresponding to high and low user ratings.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 04:47:47 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Chow", "Richard", ""], ["Jin", "Hongxia", ""], ["Knijnenburg", "Bart", ""], ["Saldamli", "Gokay", ""]]}, {"id": "1310.1498", "submitter": "Nikolas Landia", "authors": "Nikolas Landia, Stephan Doerfel, Robert J\\\"aschke, Sarabjot Singh\n  Anand, Andreas Hotho and Nathan Griffiths", "title": "Deeper Into the Folksonomy Graph: FolkRank Adaptations and Extensions\n  for Improved Tag Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The information contained in social tagging systems is often modelled as a\ngraph of connections between users, items and tags. Recommendation algorithms\nsuch as FolkRank, have the potential to leverage complex relationships in the\ndata, corresponding to multiple hops in the graph. We present an in-depth\nanalysis and evaluation of graph models for social tagging data and propose\nnovel adaptations and extensions of FolkRank to improve tag recommendations. We\nhighlight implicit assumptions made by the widely used folksonomy model, and\npropose an alternative and more accurate graph-representation of the data. Our\nextensions of FolkRank address the new item problem by incorporating content\ndata into the algorithm, and significantly improve prediction results on\nunpruned datasets. Our adaptations address issues in the iterative weight\nspreading calculation that potentially hinder FolkRank's ability to leverage\nthe deep graph as an information source. Moreover, we evaluate the benefit of\nconsidering each deeper level of the graph, and present important insights\nregarding the characteristics of social tagging data in general. Our results\nsuggest that the base assumption made by conventional weight propagation\nmethods, that closeness in the graph always implies a positive relationship,\ndoes not hold for the social tagging domain.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2013 17:27:42 GMT"}], "update_date": "2013-10-08", "authors_parsed": [["Landia", "Nikolas", ""], ["Doerfel", "Stephan", ""], ["J\u00e4schke", "Robert", ""], ["Anand", "Sarabjot Singh", ""], ["Hotho", "Andreas", ""], ["Griffiths", "Nathan", ""]]}, {"id": "1310.2125", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta and Sohan Seth and Samuel Kaski", "title": "Retrieval of Experiments with Sequential Dirichlet Process Mixtures in\n  Model Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of retrieving relevant experiments given a query\nexperiment, motivated by the public databases of datasets in molecular biology\nand other experimental sciences, and the need of scientists to relate to\nearlier work on the level of actual measurement data. Since experiments are\ninherently noisy and databases ever accumulating, we argue that a retrieval\nengine should possess two particular characteristics. First, it should compare\nmodels learnt from the experiments rather than the raw measurements themselves:\nthis allows incorporating experiment-specific prior knowledge to suppress noise\neffects and focus on what is important. Second, it should be updated\nsequentially from newly published experiments, without explicitly storing\neither the measurements or the models, which is critical for saving storage\nspace and protecting data privacy: this promotes life long learning. We\nformulate the retrieval as a ``supermodelling'' problem, of sequentially\nlearning a model of the set of posterior distributions, represented as sets of\nMCMC samples, and suggest the use of Particle-Learning-based sequential\nDirichlet process mixture (DPM) for this purpose. The relevance measure for\nretrieval is derived from the supermodel through the mixture representation. We\ndemonstrate the performance of the proposed retrieval method on simulated data\nand molecular biological experiments.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 13:10:26 GMT"}, {"version": "v2", "created": "Thu, 6 Mar 2014 22:04:33 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Seth", "Sohan", ""], ["Kaski", "Samuel", ""]]}, {"id": "1310.2127", "submitter": "Shanmugapriyaa S", "authors": "S. Shanmugapriyaa, K. S. Kuppusamy, G. Aghila", "title": "BloSEn: Blog Search Engine Based On Post Concept Clustering", "comments": "12 pages", "journal-ref": "International Journal of Ambient Systems and Applications (IJASA)\n  Vol.1, No.3, September 2013", "doi": "10.5121/ijasa.2013.1302", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on building a blog search engine which doesn't focus only\non keyword search but includes extended search capabilities. It also\nincorporates the blog-post concept clustering which is based on the category\nextracted from the blog post semantic content analysis. The proposed approach\nis titled as \"BloSen (Blog Search Engine)\". It involves in extracting the posts\nfrom blogs and parsing them to extract the blog elements and store them as\nfields in a document format. Inverted index is being built on the fields of the\ndocuments. Search is induced on the index and requested query is processed\nbased on the documents so far made from blog posts. It currently focuses on\nBlogger and Wordpress hosted blogs since both these hosting services are the\nmost popular ones in the blogosphere. The proposed BloSen model is experimented\nwith a prototype implementation and the results of the experiments with the\nuser's relevance cumulative metric value of 95.44% confirms the efficiency of\nthe proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 13:16:30 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Shanmugapriyaa", "S.", ""], ["Kuppusamy", "K. S.", ""], ["Aghila", "G.", ""]]}, {"id": "1310.2375", "submitter": "Dhanamma Jagli", "authors": "Dhanamma Jagli, Sangeeta Oswal", "title": "Web Usage Mining: Pattern Discovery and Forecasting", "comments": null, "journal-ref": "IFRSA International Journal of Data Warehousing & Mining |Vol\n  2|issue4|November 2012", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web usage mining: automatic discovery of patterns in clickstreams and\nassociated data collected or generated as a result of user interactions with\none or more Web sites. This paper describes web usage mining for our college\nlog files to analyze the behavioral patterns and profiles of users interacting\nwith a Web site. The discovered patterns are represented as clusters that are\nfrequently accessed by groups of visitors with common interests. In this paper,\nthe visitors and hits were forecasted to predict the further access statistics.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 07:19:40 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Jagli", "Dhanamma", ""], ["Oswal", "Sangeeta", ""]]}, {"id": "1310.2409", "submitter": "Ning Chen", "authors": "Ning Chen, Jun Zhu, Fei Xia, Bo Zhang", "title": "Discriminative Relational Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and engineering fields involve analyzing network data. For\ndocument networks, relational topic models (RTMs) provide a probabilistic\ngenerative process to describe both the link structure and document contents,\nand they have shown promise on predicting network structures and discovering\nlatent topic representations. However, existing RTMs have limitations in both\nthe restricted model expressiveness and incapability of dealing with imbalanced\nnetwork data. To expand the scope and improve the inference accuracy of RTMs,\nthis paper presents three extensions: 1) unlike the common link likelihood with\na diagonal weight matrix that allows the-same-topic interactions only, we\ngeneralize it to use a full weight matrix that captures all pairwise topic\ninteractions and is applicable to asymmetric networks; 2) instead of doing\nstandard Bayesian inference, we perform regularized Bayesian inference\n(RegBayes) with a regularization parameter to deal with the imbalanced link\nstructure issue in common real networks and improve the discriminative ability\nof learned latent representations; and 3) instead of doing variational\napproximation with strict mean-field assumptions, we present collapsed Gibbs\nsampling algorithms for the generalized relational topic models by exploring\ndata augmentation without making restricting assumptions. Under the generic\nRegBayes framework, we carefully investigate two popular discriminative loss\nfunctions, namely, the logistic log-loss and the max-margin hinge loss.\nExperimental results on several real network datasets demonstrate the\nsignificance of these extensions on improving the prediction performance, and\nthe time efficiency can be dramatically improved with a simple fast\napproximation method.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 09:32:56 GMT"}], "update_date": "2013-10-10", "authors_parsed": [["Chen", "Ning", ""], ["Zhu", "Jun", ""], ["Xia", "Fei", ""], ["Zhang", "Bo", ""]]}, {"id": "1310.3333", "submitter": "Sriramkumar Balasubramanian", "authors": "Sriramkumar Balasubramanian and Raghuram Reddy Nagireddy", "title": "Visualizing Bags of Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation of this work is two-fold - a) to compare between two different\nmodes of visualizing data that exists in a bag of vectors format b) to propose\na theoretical model that supports a new mode of visualizing data. Visualizing\nhigh dimensional data can be achieved using Minimum Volume Embedding, but the\ndata has to exist in a format suitable for computing similarities while\npreserving local distances. This paper compares the visualization between two\nmethods of representing data and also proposes a new method providing sample\nvisualizations for that method.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2013 03:48:38 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Balasubramanian", "Sriramkumar", ""], ["Nagireddy", "Raghuram Reddy", ""]]}, {"id": "1310.3808", "submitter": "Philipp Mayr", "authors": "Howard D. White, Philipp Mayr", "title": "Pennants for Descriptors", "comments": "3 pages, 1 figure, paper presented at the NKOS workshop at TPDL 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new technique (called pennants) for displaying the descriptors\nrelated to a descriptor across literatures, rather in a thesaurus. It has\ndefinite implications for online searching and browsing. Pennants, named for\nthe flag they resemble, are a form of algorithmic prediction. Their cognitive\nbase is in relevance theory (RT) from linguistic pragmatics (Sperber & Wilson\n1995).\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 19:49:34 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["White", "Howard D.", ""], ["Mayr", "Philipp", ""]]}, {"id": "1310.4136", "submitter": "Thiago S. F. X. Teixeira", "authors": "Thiago S. F. X. Teixeira, George Teodoro, Eduardo Valle, Joel H. Saltz", "title": "Scalable Locality-Sensitive Hashing for Similarity Search in\n  High-Dimensional, Large-Scale Multimedia Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search is critical for many database applications, including the\nincreasingly popular online services for Content-Based Multimedia Retrieval\n(CBMR). These services, which include image search engines, must handle an\noverwhelming volume of data, while keeping low response times. Thus,\nscalability is imperative for similarity search in Web-scale applications, but\nmost existing methods are sequential and target shared-memory machines. Here we\naddress these issues with a distributed, efficient, and scalable index based on\nLocality-Sensitive Hashing (LSH). LSH is one of the most efficient and popular\ntechniques for similarity search, but its poor referential locality properties\nhas made its implementation a challenging problem. Our solution is based on a\nwidely asynchronous dataflow parallelization with a number of optimizations\nthat include a hierarchical parallelization to decouple indexing and data\nstorage, locality-aware data partition strategies to reduce message passing,\nand multi-probing to limit memory usage. The proposed parallelization attained\nan efficiency of 90% in a distributed system with about 800 CPU cores. In\nparticular, the original locality-aware data partition reduced the number of\nmessages exchanged in 30%. Our parallel LSH was evaluated using the largest\npublic dataset for similarity search (to the best of our knowledge) with $10^9$\n128-d SIFT descriptors extracted from Web images. This is two orders of\nmagnitude larger than datasets that previous LSH parallelizations could handle.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 18:21:39 GMT"}], "update_date": "2013-10-16", "authors_parsed": [["Teixeira", "Thiago S. F. X.", ""], ["Teodoro", "George", ""], ["Valle", "Eduardo", ""], ["Saltz", "Joel H.", ""]]}, {"id": "1310.4366", "submitter": "Dmitry Ignatov", "authors": "Elena Nenova and Dmitry I. Ignatov and Andrey V. Konstantinov", "title": "An FCA-based Boolean Matrix Factorisation for Collaborative Filtering", "comments": "http://ceur-ws.org/Vol-977/paper8.pdf", "journal-ref": "In: C. Carpineto, A. Napoli, S.O. Kuznetsov (eds), FCA Meets IR\n  2013, Vol. 977, CEUR Workshop Proceeding, 2013. P. 57-73", "doi": null, "report-no": null, "categories": "cs.IR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for Collaborative Filtering which is based on\nBoolean Matrix Factorisation (BMF) and Formal Concept Analysis. In a series of\nexperiments on real data (Movielens dataset) we compare the approach with the\nSVD- and NMF-based algorithms in terms of Mean Average Error (MAE). One of the\nexperimental consequences is that it is enough to have a binary-scaled rating\ndata to obtain almost the same quality in terms of MAE by BMF than for the\nSVD-based algorithm in case of non-scaled data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 13:17:37 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Nenova", "Elena", ""], ["Ignatov", "Dmitry I.", ""], ["Konstantinov", "Andrey V.", ""]]}, {"id": "1310.4774", "submitter": "Avinash Bhute", "authors": "Avinash N Bhute, B.B. Meshram", "title": "IntelligentWeb Agent for Search Engines", "comments": "5 pages. International Conference on Trends and Advances in\n  Computation and Engineering TRACE -2010. arXiv admin note: text overlap with\n  arXiv:1205.2891 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we review studies of the growth of the Internet and\ntechnologies that are useful for information search and retrieval on the Web.\nSearch engines are retrieve the efficient information. We collected data on the\nInternet from several different sources, e.g., current as well as projected\nnumber of users, hosts, and Web sites. The trends cited by the sources are\nconsistent and point to exponential growth in the past and in the coming\ndecade. Hence it is not surprising that about 85% of Internet users surveyed\nclaim using search engines and search services to find specific information and\nusers are not satisfied with the performance of the current generation of\nsearch engines; the slow retrieval speed, communication delays, and poor\nquality of retrieved results. Web agents, programs acting autonomously on some\ntask, are already present in the form of spiders, crawler, and robots. Agents\noffer substantial benefits and hazards, and because of this, their development\nmust involve attention to technical details. This paper illustrates the\ndifferent types of agents,crawlers, robots,etc for mining the contents of web\nin a methodical, automated manner, also discusses the use of crawler to gather\nspecific types of information from Web pages, such as harvesting e-mail\naddresses\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2013 17:07:38 GMT"}], "update_date": "2013-10-18", "authors_parsed": [["Bhute", "Avinash N", ""], ["Meshram", "B. B.", ""]]}, {"id": "1310.4954", "submitter": "Miguel A. Martinez-Prieto", "authors": "Sandra \\'Alvarez-Garc\\'ia and Nieves R. Brisaboa and Javier D.\n  Fern\\'andez and Miguel A. Mart\\'inez-Prieto and Gonzalo Navarro", "title": "Compressed Vertical Partitioning for Full-In-Memory RDF Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web of Data has been gaining momentum and this leads to increasingly\npublish more semi-structured datasets following the RDF model, based on atomic\ntriple units of subject, predicate, and object. Although it is a simple model,\ncompression methods become necessary because datasets are increasingly larger\nand various scalability issues arise around their organization and storage.\nThis requirement is more restrictive in RDF stores because efficient SPARQL\nresolution on the compressed RDF datasets is also required.\n  This article introduces a novel RDF indexing technique (called k2-triples)\nsupporting efficient SPARQL resolution in compressed space. k2-triples, uses\nthe predicate to vertically partition the dataset into disjoint subsets of\npairs (subject, object), one per predicate. These subsets are represented as\nbinary matrices in which 1-bits mean that the corresponding triple exists in\nthe dataset. This model results in very sparse matrices, which are efficiently\ncompressed using k2-trees. We enhance this model with two compact indexes\nlisting the predicates related to each different subject and object, in order\nto address the specific weaknesses of vertically partitioned representations.\nThe resulting technique not only achieves by far the most compressed\nrepresentations, but also the best overall performance for RDF retrieval in our\nexperiments. Our approach uses up to 10 times less space than a state of the\nart baseline, and outperforms its performance by several order of magnitude on\nthe most basic query patterns. In addition, we optimize traditional join\nalgorithms on k2-triples and define a novel one leveraging its specific\nfeatures. Our experimental results show that our technique overcomes\ntraditional vertical partitioning for join resolution, reporting the best\nnumbers for joins in which the non-joined nodes are provided, and being\ncompetitive in the majority of the cases.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 08:58:01 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2013 09:00:47 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["\u00c1lvarez-Garc\u00eda", "Sandra", ""], ["Brisaboa", "Nieves R.", ""], ["Fern\u00e1ndez", "Javier D.", ""], ["Mart\u00ednez-Prieto", "Miguel A.", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "1310.5042", "submitter": "Peter Turney", "authors": "Peter D. Turney", "title": "Distributional semantics beyond words: Supervised learning of analogy\n  and paraphrase", "comments": null, "journal-ref": "Transactions of the Association for Computational Linguistics\n  (TACL), (2013), 1, 353-366", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been several efforts to extend distributional semantics beyond\nindividual words, to measure the similarity of word pairs, phrases, and\nsentences (briefly, tuples; ordered sets of words, contiguous or\nnoncontiguous). One way to extend beyond words is to compare two tuples using a\nfunction that combines pairwise similarities between the component words in the\ntuples. A strength of this approach is that it works with both relational\nsimilarity (analogy) and compositional similarity (paraphrase). However, past\nwork required hand-coding the combination function for different tasks. The\nmain contribution of this paper is that combination functions are generated by\nsupervised learning. We achieve state-of-the-art results in measuring\nrelational similarity between word pairs (SAT analogies and SemEval~2012 Task\n2) and measuring compositional similarity between noun-modifier phrases and\nunigrams (multiple-choice paraphrase questions).\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 14:50:39 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Turney", "Peter D.", ""]]}, {"id": "1310.5142", "submitter": "Hyun Joon Jung", "authors": "Hyun Joon Jung and Matthew Lease", "title": "Crowdsourced Task Routing via Matrix Factorization", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We describe methods to predict a crowd worker's accuracy on new tasks based\non his accuracy on past tasks. Such prediction provides a foundation for\nidentifying the best workers to route work to in order to maximize accuracy on\nthe new task. Our key insight is to model similarity of past tasks to the\ntarget task such that past task accuracies can be optimally integrated to\npredict target task accuracy. We describe two matrix factorization (MF)\napproaches from collaborative filtering which not only exploit such task\nsimilarity, but are known to be robust to sparse data. Experiments on synthetic\nand real-world datasets provide feasibility assessment and comparative\nevaluation of MF approaches vs. two baseline methods. Across a range of data\nscales and task similarity conditions, we evaluate: 1) prediction error over\nall workers; and 2) how well each method predicts the best workers to use for\neach task. Results show the benefit of task routing over random assignment, the\nstrength of probabilistic MF over baseline methods, and the robustness of\nmethods under different conditions.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 14:37:24 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Jung", "Hyun Joon", ""], ["Lease", "Matthew", ""]]}, {"id": "1310.5597", "submitter": "Francisco  Couto", "authors": "Francisco M Couto", "title": "CIDS country rankings: comparing documents and citations of USA, UK and\n  China top researchers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report presents a bibliometric analysis of the top 30 cited\nresearchers from USA, UK and China. The analysis is based on Google Scholar\ndata using CIDS. The researchers were identified using their email suffix: edu,\nuk and cn. This na\\\"{i}ve approach was able to produce rankings consistent with\nthe SCImago country rankings using mininal resources in a fully automated way.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 15:22:44 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2013 13:30:44 GMT"}], "update_date": "2013-10-28", "authors_parsed": [["Couto", "Francisco M", ""]]}, {"id": "1310.5698", "submitter": "Joan Guisado-G\\'amez", "authors": "Joan Guisado-G\\'amez, David Dominguez-Sal and Josep-LLuis Larriba-Pey", "title": "Massive Query Expansion by Exploiting Graph Knowledge Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword based search engines have problems with term ambiguity and vocabulary\nmismatch. In this paper, we propose a query expansion technique that enriches\nqueries expressed as keywords and short natural language descriptions. We\npresent a new massive query expansion strategy that enriches queries using a\nknowledge base by identifying the query concepts, and adding relevant synonyms\nand semantically related terms. We propose two approaches: (i) lexical\nexpansion that locates the relevant concepts in the knowledge base; and, (ii)\ntopological expansion that analyzes the network of relations among the\nconcepts, and suggests semantically related terms by path and community\nanalysis of the knowledge graph. We perform our expansions by using two\nversions of the Wikipedia as knowledge base, concluding that the combination of\nboth lexical and topological expansion provides improvements of the system's\nprecision up to more than 27%.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 14:27:45 GMT"}], "update_date": "2013-10-23", "authors_parsed": [["Guisado-G\u00e1mez", "Joan", ""], ["Dominguez-Sal", "David", ""], ["Larriba-Pey", "Josep-LLuis", ""]]}, {"id": "1310.5777", "submitter": "Xianwen Wang", "authors": "Xianwen Wang, Lian Peng, Chunbo Zhang, Shenmeng Xu, Zhi Wang, Chuanli\n  Wang and Xianbing Wang", "title": "Exploring Scientists' Working Timetable: A Global Survey", "comments": null, "journal-ref": "Journal of Informetrics. 2013, 7(3):665-675", "doi": "10.1016/j.joi.2013.04.003", "report-no": null, "categories": "cs.DL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our previous study (Wang et al., 2012), we analyzed scientists' working\ntimetable of 3 countries, using realtime downloading data of scientific\nliteratures. In this paper, we make a through analysis about global scientists'\nworking habits. Top 30 countries/territories from Europe, Asia, Australia,\nNorth America, Latin America and Africa are selected as representatives and\nanalyzed in detail. Regional differences for scientists' working habits exists\nin different countries. Besides different working cultures, social factors\ncould affect scientists' research activities and working patterns.\nNevertheless, a common conclusion is that scientists today are often working\novertime. Although scientists may feel engaged and fulfilled about their hard\nworking, working too much still warns us to reconsider the work - life balance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 02:00:01 GMT"}], "update_date": "2013-10-23", "authors_parsed": [["Wang", "Xianwen", ""], ["Peng", "Lian", ""], ["Zhang", "Chunbo", ""], ["Xu", "Shenmeng", ""], ["Wang", "Zhi", ""], ["Wang", "Chuanli", ""], ["Wang", "Xianbing", ""]]}, {"id": "1310.5963", "submitter": "Foruzan Kiamarzpour", "authors": "Foruzan Kiamarzpour, Rouhollah Dianat, Mohammad bahrani, Mehdi\n  Sadeghzadeh", "title": "Improving the methods of email classification based on words ontology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet has dramatically changed the relationship among people and their\nrelationships with others people and made the valuable information available\nfor the users. Email is the service, which the Internet provides today for its\nown users; this service has attracted most of the users' attention due to the\nlow cost. Along with the numerous benefits of Email, one of the weaknesses of\nthis service is that the number of received emails is continually being\nenhanced, thus the ways are needed to automatically filter these disturbing\nletters. Most of these filters utilize a combination of several techniques such\nas the Black or white List, using the keywords and so on in order to identify\nthe spam more accurately In this paper, we introduce a new method to classify\nthe spam. We are seeking to increase the accuracy of Email classification by\ncombining the output of several decision trees and the concept of ontology.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2013 15:35:46 GMT"}], "update_date": "2013-10-24", "authors_parsed": [["Kiamarzpour", "Foruzan", ""], ["Dianat", "Rouhollah", ""], ["bahrani", "Mohammad", ""], ["Sadeghzadeh", "Mehdi", ""]]}, {"id": "1310.6110", "submitter": "Keisuke Hara", "authors": "Keisuke Hara, Tomihisa Kamada", "title": "A two-step model and the algorithm for recalling in recommender systems", "comments": "6 pages, No figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a user finds an interesting recommendation in a recommender system, the\nuser may want to recall related items recommended in the past to reconsider or\nto enjoy them again. If the system can pick up such \"recalled\" items at each\nuser's request, it must deepen the user experience.\n  We propose a model and the algorithm for such personalized \"recalling\" in\nconventional recommender systems, which is an application of neural networks\nfor associative memory. In our model, the \"recalled\" items can reflect each\nuser's personality beyond naive similarities between items.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 04:41:18 GMT"}], "update_date": "2013-10-24", "authors_parsed": [["Hara", "Keisuke", ""], ["Kamada", "Tomihisa", ""]]}, {"id": "1310.6555", "submitter": "Stian Soiland-Reyes", "authors": "Paolo Ciccarese, Stian Soiland-Reyes, Tim Clark", "title": "Web Annotation as a First Class Object", "comments": "This is authors' Accepted version as of 2013-08-09, reformatted as\n  HTML with Linked Data. For the final, published version, see IEEE Explore:\n  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6682930", "journal-ref": "IEEE Internet Computing 2013; 17(6) 71-75", "doi": "10.1109/MIC.2013.123", "report-no": "uk-ac-man-scw:211608", "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scholars have made handwritten notes and comments in books and manuscripts\nfor centuries. Today's blogs and news sites typically invite users to express\ntheir opinions on the published content; URLs allow web resources to be shared\nwith accompanying annotations and comments using third-party services like\nTwitter or Facebook. These contributions have until recently been constrained\nwithin specific services, making them second-class citizens of the Web.\n  Web Annotations are now emerging as fully independent Linked Data in their\nown right, no longer restricted to plain textual comments in application silos.\nAnnotations can now range from bookmarks and comments, to fine-grained\nannotations of a selection of, for example, a section of a frame within a video\nstream. Technologies and standards now exist to create, publish, syndicate,\nmash-up and consume, finely targeted, semantically rich digital annotations on\npractically any content, as first-class Web citizens. This development is being\ndriven by the need for collaboration and annotation reuse amongst domain\nresearchers, computer scientists, scientific publishers, and scholarly content\ndatabases.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2013 10:40:14 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2014 13:54:11 GMT"}, {"version": "v3", "created": "Wed, 20 Mar 2019 02:02:00 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Ciccarese", "Paolo", ""], ["Soiland-Reyes", "Stian", ""], ["Clark", "Tim", ""]]}, {"id": "1310.6637", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "P YesuRaju, P KiranSree", "title": "A language independent web data extraction using vision based page\n  segmentation algorithm", "comments": "arXiv admin note: text overlap with arXiv:1201.0385 by other authors\n  without attribution", "journal-ref": "IJRET APR 2013,Volume: 2 Issue: 4,635 - 639,ISSN: 2319 - 1163", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web usage mining is a process of extracting useful information from server\nlogs i.e. users history. Web usage mining is a process of finding out what\nusers are looking for on the internet. Some users might be looking at only\ntextual data, where as some others might be interested in multimedia data. One\nwould retrieve the data by copying it and pasting it to the relevant document.\nBut this is tedious and time consuming as well as difficult when the data to be\nretrieved is plenty. Extracting structured data from a web page is challenging\nproblem due to complicated structured pages. Earlier they were used web page\nprogramming language dependent; the main problem is to analyze the html source\ncode. In earlier they were considered the scripts such as java scripts and\ncascade styles in the html files. When it makes different for existing\nsolutions to infer the regularity of the structure of the Web Pages only by\nanalyzing the tag structures. To overcome this problem we are using a new\nalgorithm called VIPS algorithm i.e. independent language. This approach\nprimary utilizes the visual features on the webpage to implement web data\nextraction.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2013 15:01:13 GMT"}], "update_date": "2013-10-25", "authors_parsed": [["YesuRaju", "P", ""], ["KiranSree", "P", ""]]}, {"id": "1310.7428", "submitter": "Dmitry Bugaychenko", "authors": "Dmitry Bugaychenko, Alexandr Dzuba", "title": "Musical recommendations and personalization in a social network", "comments": "This is a full version of a 4 pages article published at ACM RecSys\n  2013", "journal-ref": null, "doi": "10.1145/2507157.2507192", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a set of algorithms used for music recommendations and\npersonalization in a general purpose social network www.ok.ru, the second\nlargest social network in the CIS visited by more then 40 millions users per\nday. In addition to classical recommendation features like \"recommend a\nsequence\" and \"find similar items\" the paper describes novel algorithms for\nconstruction of context aware recommendations, personalization of the service,\nhandling of the cold-start problem, and more. All algorithms described in the\npaper are working on-line and are able to detect and address changes in the\nuser's behavior and needs in the real time.\n  The core component of the algorithms is a taste graph containing information\nabout different entities (users, tracks, artists, etc.) and relations between\nthem (for example, user A likes song B with certainty X, track B created by\nartist C, artist C is similar to artist D with certainty Y and so on). Using\nthe graph it is possible to select tracks a user would most probably like, to\narrange them in a way that they match each other well, to estimate which items\nfrom a fixed list are most relevant for the user, and more.\n  In addition, the paper describes the approach used to estimate algorithms\nefficiency and analyze the impact of different recommendation related features\non the users' behavior and overall activity at the service.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2013 14:30:52 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Bugaychenko", "Dmitry", ""], ["Dzuba", "Alexandr", ""]]}, {"id": "1310.7441", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis, Da Kuang and Haesun Park", "title": "Hierarchical Clustering of Hyperspectral Images using Rank-Two\n  Nonnegative Matrix Factorization", "comments": "29 pages, 19 figures. New experiment on Terrain data set. Accepted in\n  IEEE Trans. Geosci. Remote Sens", "journal-ref": "IEEE Trans. on Geoscience and Remote Sensing 53 (4), pp.\n  2066-2078, 2015", "doi": "10.1109/TGRS.2014.2352857", "report-no": null, "categories": "cs.CV cs.IR math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design a hierarchical clustering algorithm for\nhigh-resolution hyperspectral images. At the core of the algorithm, a new\nrank-two nonnegative matrix factorizations (NMF) algorithm is used to split the\nclusters, which is motivated by convex geometry concepts. The method starts\nwith a single cluster containing all pixels, and, at each step, (i) selects a\ncluster in such a way that the error at the next step is minimized, and (ii)\nsplits the selected cluster into two disjoint clusters using rank-two NMF in\nsuch a way that the clusters are well balanced and stable. The proposed method\ncan also be used as an endmember extraction algorithm in the presence of pure\npixels. The effectiveness of this approach is illustrated on several synthetic\nand real-world hyperspectral images, and shown to outperform standard\nclustering techniques such as k-means, spherical k-means and standard NMF.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2013 09:54:59 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2013 08:53:00 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2014 10:04:55 GMT"}, {"version": "v4", "created": "Tue, 19 Aug 2014 12:49:54 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Gillis", "Nicolas", ""], ["Kuang", "Da", ""], ["Park", "Haesun", ""]]}, {"id": "1310.7829", "submitter": "Minyar Sassi", "authors": "Ines Benali Sougui, Minyar Sassi Hidri, Amel Grissa-Touzi", "title": "About Summarization in Large Fuzzy Databases", "comments": null, "journal-ref": "The 5th International Conference on Advances in Databases,\n  Knowledge, and Data Applications (DBKDA), pp. 87-94, 2013", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moved by the need increased for modeling of the fuzzy data, the success of\nthe systems of exact generation of summary of data, we propose in this paper, a\nnew approach of generation of summary from fuzzy data called Fuzzy-SaintEtiQ.\nThis approach is an extension of the SaintEtiQ model to support the fuzzy data.\nIt presents the following optimizations such as 1) the minimization of the\nexpert risk; 2) the construction of a more detailed and more precise summaries\nhierarchy, and 3) the co-operation with the user by giving him fuzzy summaries\nin different hierarchical levels\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2013 15:17:39 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Sougui", "Ines Benali", ""], ["Hidri", "Minyar Sassi", ""], ["Grissa-Touzi", "Amel", ""]]}, {"id": "1310.7957", "submitter": "Ahmed Abbasi", "authors": "Zhu Zhang, Daniel Zeng, Ahmed Abbasi and Jing Peng", "title": "A Random Walk Model for Item Recommendation in Folksonomies", "comments": null, "journal-ref": "Zhang, Z., Zeng, D., Abbasi, A., and Peng, J. \"A Random Walk Model\n  for Item Recommendation in Folksonomies,\" In Proceedings of the 21st Annual\n  Workshop on Information Technologies and Systems, Shanghai, China, December\n  3-4, 2011", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social tagging, as a novel approach to information organization and\ndiscovery, has been widely adopted in many Web2.0 applications. The tags\nprovide a new type of information that can be exploited by recommender systems.\nNevertheless, the sparsity of ternary <user, tag, item> interaction data limits\nthe performance of tag-based collaborative filtering. This paper proposes a\nrandom-walk-based algorithm to deal with the sparsity problem in social tagging\ndata, which captures the potential transitive associations between users and\nitems through their interaction with tags. In particular, two smoothing\nstrategies are presented from both the user-centric and item-centric\nperspectives. Experiments on real-world data sets empirically demonstrate the\nefficacy of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 15:19:54 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Zhang", "Zhu", ""], ["Zeng", "Daniel", ""], ["Abbasi", "Ahmed", ""], ["Peng", "Jing", ""]]}, {"id": "1310.7994", "submitter": "Weicong Ding", "authors": "Weicong Ding, Prakash Ishwar, Mohammad H. Rohban, Venkatesh Saligrama", "title": "Necessary and Sufficient Conditions for Novel Word Detection in\n  Separable Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simplicial condition and other stronger conditions that imply it have\nrecently played a central role in developing polynomial time algorithms with\nprovable asymptotic consistency and sample complexity guarantees for topic\nestimation in separable topic models. Of these algorithms, those that rely\nsolely on the simplicial condition are impractical while the practical ones\nneed stronger conditions. In this paper, we demonstrate, for the first time,\nthat the simplicial condition is a fundamental, algorithm-independent,\ninformation-theoretic necessary condition for consistent separable topic\nestimation. Furthermore, under solely the simplicial condition, we present a\npractical quadratic-complexity algorithm based on random projections which\nconsistently detects all novel words of all topics using only up to\nsecond-order empirical word moments. This algorithm is amenable to distributed\nimplementation making it attractive for 'big-data' scenarios involving a\nnetwork of large distributed databases.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 01:19:26 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Ding", "Weicong", ""], ["Ishwar", "Prakash", ""], ["Rohban", "Mohammad H.", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1310.8226", "submitter": "Philipp Mayr", "authors": "Philipp Mayr, Andrea Scharnhorst, Birger Larsen, Philipp Schaer, Peter\n  Mutschke", "title": "Bibliometric-enhanced Information Retrieval", "comments": "6 pages, accepted workshop proposal for ECIR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bibliometric techniques are not yet widely used to enhance retrieval\nprocesses in digital libraries, although they offer value-added effects for\nusers. In this workshop we will explore how statistical modelling of\nscholarship, such as Bradfordizing or network analysis of coauthorship network,\ncan improve retrieval services for specific communities, as well as for large,\ncross-domain collections. This workshop aims to raise awareness of the missing\nlink between information retrieval (IR) and bibliometrics/scientometrics and to\ncreate a common ground for the incorporation of bibliometric-enhanced services\ninto retrieval at the digital library interface.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 16:38:03 GMT"}], "update_date": "2014-01-19", "authors_parsed": [["Mayr", "Philipp", ""], ["Scharnhorst", "Andrea", ""], ["Larsen", "Birger", ""], ["Schaer", "Philipp", ""], ["Mutschke", "Peter", ""]]}]