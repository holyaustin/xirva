[{"id": "1202.1112", "submitter": "Chi Ho Yeung", "authors": "Linyuan L\\\"u, Matus Medo, Chi Ho Yeung, Yi-Cheng Zhang, Zi-Ke Zhang,\n  Tao Zhou", "title": "Recommender Systems", "comments": "97 pages, 20 figures (To appear in Physics Reports)", "journal-ref": "Physics Reports Vol. 519 (1), P. 1-49 (2012)", "doi": "10.1016/j.physrep.2012.02.006", "report-no": null, "categories": "physics.soc-ph cond-mat.stat-mech cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing rapid expansion of the Internet greatly increases the necessity\nof effective recommender systems for filtering the abundant information.\nExtensive research for recommender systems is conducted by a broad range of\ncommunities including social and computer scientists, physicists, and\ninterdisciplinary researchers. Despite substantial theoretical and practical\nachievements, unification and comparison of different approaches are lacking,\nwhich impedes further advances. In this article, we review recent developments\nin recommender systems and discuss the major challenges. We compare and\nevaluate available algorithms and examine their roles in the future\ndevelopments. In addition to algorithms, physical aspects are described to\nillustrate macroscopic behavior of recommender systems. Potential impacts and\nfuture directions are discussed. We emphasize that recommendation has a great\nscientific depth and combines diverse research fields which makes it of\ninterests for physicists as well as interdisciplinary researchers.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2012 12:15:45 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["L\u00fc", "Linyuan", ""], ["Medo", "Matus", ""], ["Yeung", "Chi Ho", ""], ["Zhang", "Yi-Cheng", ""], ["Zhang", "Zi-Ke", ""], ["Zhou", "Tao", ""]]}, {"id": "1202.1656", "submitter": "Holger Kienle", "authors": "Holger M. Kienle", "title": "Open Data: Reverse Engineering and Maintenance Perspective", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Open data is an emerging paradigm to share large and diverse datasets --\nprimarily from governmental agencies, but also from other organizations -- with\nthe goal to enable the exploitation of the data for societal, academic, and\ncommercial gains. There are now already many datasets available with diverse\ncharacteristics in terms of size, encoding and structure. These datasets are\noften created and maintained in an ad-hoc manner. Thus, open data poses many\nchallenges and there is a need for effective tools and techniques to manage and\nmaintain it. In this paper we argue that software maintenance and reverse\nengineering have an opportunity to contribute to open data and to shape its\nfuture development. From the perspective of reverse engineering research, open\ndata is a new artifact that serves as input for reverse engineering techniques\nand processes. Specific challenges of open data are document scraping, image\nprocessing, and structure/schema recognition. From the perspective of\nmaintenance research, maintenance has to accommodate changes of open data\nsources by third-party providers, traceability of data transformation\npipelines, and quality assurance of data and transformations. We believe that\nthe increasing importance of open data and the research challenges that it\nbrings with it may possibly lead to the emergence of new research streams for\nreverse engineering as well as for maintenance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2012 11:08:37 GMT"}], "update_date": "2012-02-09", "authors_parsed": [["Kienle", "Holger M.", ""]]}, {"id": "1202.1837", "submitter": "Mehdi Naghavi", "authors": "Mehdi Naghavi and Mohsen Sharifi", "title": "A Proposed Architecture for Continuous Web Monitoring Through Online\n  Crawling of Blogs", "comments": "10 pages, 2 figures", "journal-ref": "International Journal of UbiComp (IJU), Vol.3, No.1, January 2012", "doi": "10.5121/iju.2012.3102", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Getting informed of what is registered in the Web space on time, can greatly\nhelp the psychologists, marketers and political analysts to familiarize,\nanalyse, make decision and act correctly based on the society`s different\nneeds. The great volume of information in the Web space hinders us to\ncontinuously online investigate the whole space of the Web. Focusing on the\nconsidered blogs limits our working domain and makes the online crawling in the\nWeb space possible. In this article, an architecture is offered which\ncontinuously online crawls the related blogs, using focused crawler, and\ninvestigates and analyses the obtained data. The online fetching is done based\non the latest announcements of the ping server machines. A weighted graph is\nformed based on targeting the important key phrases, so that a focused crawler\ncan do the fetching of the complete texts of the related Web pages, based on\nthe weighted graph.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2012 21:34:13 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Naghavi", "Mehdi", ""], ["Sharifi", "Mohsen", ""]]}, {"id": "1202.1841", "submitter": "Ferihane Kboubi", "authors": "F\\'erihane Kboubi, Anja Habacha Chaibi and Mohamed BenAhmed", "title": "Semantic Visualization and Navigation in Textual Corpus", "comments": "11 pages, 6 figures", "journal-ref": "International Journal of Information Sciences and Techniques\n  (IJIST) Vol.2, No.1, January 2012", "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.GR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives a survey of related work on the information visualization\ndomain and study the real integration of the cartography paradigms in actual\ninformation search systems. Based on this study, we propose a semantic\nvisualization and navigation approach which offer to users three search modes:\nprecise search, connotative search and thematic search. The objective is to\npropose to the users of an information search system, new interaction paradigms\nwhich support the semantic aspect of the considered information space and guide\nusers in their searches by assisting them to locate their interest center and\nto improve serendipity.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2012 21:38:11 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Kboubi", "F\u00e9rihane", ""], ["Chaibi", "Anja Habacha", ""], ["BenAhmed", "Mohamed", ""]]}, {"id": "1202.1881", "submitter": "K.S.Kuppusamy", "authors": "K.S.Kuppusamy and G.Aghila", "title": "A personalized web page content filtering model based on segmentation", "comments": "11 Pages, 6 Figures", "journal-ref": "International Journal of Information Sciences and Techniques\n  (IJIST) Vol.2, No.1, January 2012", "doi": "10.5121/ijist.2012.2104", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the view of massive content explosion in World Wide Web through diverse\nsources, it has become mandatory to have content filtering tools. The filtering\nof contents of the web pages holds greater significance in cases of access by\nminor-age people. The traditional web page blocking systems goes by the Boolean\nmethodology of either displaying the full page or blocking it completely. With\nthe increased dynamism in the web pages, it has become a common phenomenon that\ndifferent portions of the web page holds different types of content at\ndifferent time instances. This paper proposes a model to block the contents at\na fine-grained level i.e. instead of completely blocking the page it would be\nefficient to block only those segments which holds the contents to be blocked.\nThe advantages of this method over the traditional methods are fine-graining\nlevel of blocking and automatic identification of portions of the page to be\nblocked. The experiments conducted on the proposed model indicate 88% of\naccuracy in filtering out the segments.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 03:49:03 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Kuppusamy", "K. S.", ""], ["Aghila", "G.", ""]]}, {"id": "1202.2187", "submitter": "K.S.Kuppusamy", "authors": "K. S. Kuppusamy and G. Aghila", "title": "Museum: Multidimensional web page segment evaluation model", "comments": "ISSN 2151-9617", "journal-ref": "Journal of Computing Volume 3, Issue 3 (2011) 24-27", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evaluation of a web page with respect to a query is a vital task in the\nweb information retrieval domain. This paper proposes the evaluation of a web\npage as a bottom-up process from the segment level to the page level. A model\nfor evaluating the relevancy is proposed incorporating six different\ndimensions. An algorithm for evaluating the segments of a web page, using the\nabove mentioned six dimensions is proposed. The benefits of fine-granining the\nevaluation process to the segment level instead of the page level are explored.\nThe proposed model can be incorporated for various tasks like web page\npersonalization, result re-ranking, mobile device page rendering etc.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2012 04:49:58 GMT"}], "update_date": "2012-02-13", "authors_parsed": [["Kuppusamy", "K. S.", ""], ["Aghila", "G.", ""]]}, {"id": "1202.2368", "submitter": "Afzal  Godil", "authors": "Sarah Tang and Afzal Godil", "title": "An evaluation of local shape descriptors for 3D shape retrieval", "comments": "IS&T/SPIE Electronic Imaging 2012, Proceedings Vol. 8290\n  Three-Dimensional Image Processing (3DIP) and Applications II, Atilla M.\n  Baskurt; Robert Sitnik, Editors, 82900N Dates: Tuesday-Thursday 24 - 26\n  January 2012, Paper 8290-22", "journal-ref": null, "doi": "10.1117/12.912153", "report-no": "Paper 8290-22, Proceedings Vol. 8290", "categories": "cs.CV cs.CG cs.DL cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the usage of 3D models increases, so does the importance of developing\naccurate 3D shape retrieval algorithms. A common approach is to calculate a\nshape descriptor for each object, which can then be compared to determine two\nobjects' similarity. However, these descriptors are often evaluated\nindependently and on different datasets, making them difficult to compare.\nUsing the SHREC 2011 Shape Retrieval Contest of Non-rigid 3D Watertight Meshes\ndataset, we systematically evaluate a collection of local shape descriptors. We\napply each descriptor to the bag-of-words paradigm and assess the effects of\nvarying the dictionary's size and the number of sample points. In addition,\nseveral salient point detection methods are used to choose sample points; these\nmethods are compared to each other and to random selection. Finally,\ninformation from two local descriptors is combined in two ways and changes in\nperformance are investigated. This paper presents results of these experiment\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2012 21:02:39 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Tang", "Sarah", ""], ["Godil", "Afzal", ""]]}, {"id": "1202.2393", "submitter": "Dohy Hong", "authors": "Dohy Hong", "title": "Statistical reliability and path diversity based PageRank algorithm\n  improvements", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present new improvement ideas of the original PageRank\nalgorithm. The first idea is to introduce an evaluation of the statistical\nreliability of the ranking score of each node based on the local graph property\nand the second one is to introduce the notion of the path diversity. The path\ndiversity can be exploited to dynamically modify the increment value of each\nnode in the random surfer model or to dynamically adapt the damping factor. We\nillustrate the impact of such modifications through examples and simple\nsimulations.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2012 00:12:58 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Hong", "Dohy", ""]]}, {"id": "1202.2614", "submitter": "K.S.Kuppusamy", "authors": "K. S. Kuppusamy and G. Aghila", "title": "Semantic snippet construction for search engine results based on segment\n  evaluation", "comments": null, "journal-ref": "International Journal of Information Technology and Knowledge\n  Management, July-December 2011, Volume 4, No. 2, pp. 581-583", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The result listing from search engines includes a link and a snippet from the\nweb page for each result item. The snippet in the result listing plays a vital\nrole in assisting the user to click on it. This paper proposes a novel approach\nto construct the snippets based on a semantic evaluation of the segments in the\npage. The target segment(s) is/are identified by applying a model to evaluate\nsegments present in the page and selecting the segments with top scores. The\nproposed model makes the user judgment to click on a result item easier since\nthe snippet is constructed semantically after a critical evaluation based on\nmultiple factors. A prototype implementation of the proposed model confirms the\nempirical validation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 03:57:12 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Kuppusamy", "K. S.", ""], ["Aghila", "G.", ""]]}, {"id": "1202.2615", "submitter": "K.S.Kuppusamy", "authors": "K. S. Kuppusamy and G. Aghila", "title": "Live-marker: A personalized web page content marking tool", "comments": null, "journal-ref": "International Journal of Information Technology and Knowledge\n  Management July-December 2011, Volume 4, No. 2, pp. 485-488", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tremendous amount of increase in the quantity of information resources\navailable on the web has made the total time that the user spends on a single\npage very minimal. Users revisiting the same page would be able to fetch the\nrequired information much faster if the information that they consumed during\nthe previous visit(s) gets presented to them with a special style. This paper\nproposes a model which empowers the users to mark the content interesting to\nthem, so that it can be identified easily during successive visits. In addition\nto the explicit marking by the users, the model facilitates implicit marking\nbased on the user preferences. The prototype implementation based on proposed\nmodel validates the model's efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 04:02:40 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Kuppusamy", "K. S.", ""], ["Aghila", "G.", ""]]}, {"id": "1202.2617", "submitter": "K.S.Kuppusamy", "authors": "K. S. Kuppusamy and G. Aghila", "title": "Segmentation Based Approach to Dynamic Page Construction from Search\n  Engine Results", "comments": "9 Pages, 7 Figures; ISSN : 0975-3397; International Journal on\n  Computer Science and Engineering (IJCSE), Vol. 3 No. 3 Mar 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The results rendered by the search engines are mostly a linear snippet list.\nWith the prolific increase in the dynamism of web pages there is a need for\nenhanced result lists from search engines in order to cope-up with the\nexpectations of the users. This paper proposes a model for dynamic construction\nof a resultant page from various results fetched by the search engine, based on\nthe web page segmentation approach. With the incorporation of personalization\nthrough user profile during the candidate segment selection, the enriched\nresultant page is constructed. The benefits of this approach include instant,\none-shot navigation to relevant portions from various result items, in contrast\nto a linear page-by-page visit approach. The experiments conducted on the\nprototype model with various levels of users, quantifies the improvements in\nterms of amount of relevant information fetched.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 04:13:56 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Kuppusamy", "K. S.", ""], ["Aghila", "G.", ""]]}, {"id": "1202.2619", "submitter": "K.S.Kuppusamy", "authors": "K. S. Kuppusamy and G. Aghila", "title": "We.I.Pe: Web Identification of People using e-mail ID", "comments": "7 Pages, 4 Figures; ISSN : 0975-3397", "journal-ref": "International Journal on Computer Science and Engineering (IJCSE),\n  Vol. 3 No. 6 June 2011", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the phenomenal growth of content in the World Wide Web, the diversity of\nuser supplied queries have become vivid. Searching for people on the web has\nbecome an important type of search activity in the web search engines. This\npaper proposes a model named \"We.I.Pe\" to identify people on the World Wide Web\nusing e-mail Id as the primary input. The approach followed in this research\nwork provides the collected information, based on the user supplied e-mail id,\nin an easier to navigate manner. The grouping of collected information based on\nvarious sources makes the result visualization process more effective. The\nproposed model is validated by a prototype implementation. Experiments\nconducted on the prototype implementation provide encouraging results\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 04:17:30 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Kuppusamy", "K. S.", ""], ["Aghila", "G.", ""]]}, {"id": "1202.2622", "submitter": "K.S.Kuppusamy", "authors": "K. S. Kuppusamy and G. Aghila", "title": "A Model for Web Page Usage Mining Based on Segmentation", "comments": null, "journal-ref": "International Journal of Computer Science and Information\n  Technologies, Vol. 2, No 2 , 2011, 1144-1148", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The web page usage mining plays a vital role in enriching the page's content\nand structure based on the feedbacks received from the user's interactions with\nthe page. This paper proposes a model for micro-managing the tracking\nactivities by fine-tuning the mining from the page level to the segment level.\nThe proposed model enables the web-master to identify the segments which\nreceives more focus from users comparing with others. The segment level\nanalytics of user actions provides an important metric to analyse the factors\nwhich facilitate the increase in traffic for the page. The empirical validation\nof the model is performed through prototype implementation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 04:21:15 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Kuppusamy", "K. S.", ""], ["Aghila", "G.", ""]]}, {"id": "1202.2709", "submitter": "Qian-Ming Zhang", "authors": "Qian-Ming Zhang, Linyuan L\\\"u, Wen-Qiang Wang, Yu-Xiao Zhu, and Tao\n  Zhou", "title": "Potential Theory for Directed Networks", "comments": "8 pages, 6 figures", "journal-ref": "PLoS ONE, volume 8, number 2, pages e55437, year 2013", "doi": "10.1371/journal.pone.0055437", "report-no": null, "categories": "physics.data-an cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncovering factors underlying the network formation is a long-standing\nchallenge for data mining and network analysis. In particular, the microscopic\norganizing principles of directed networks are less understood than those of\nundirected networks. This article proposes a hypothesis named potential theory,\nwhich assumes that every directed link corresponds to a decrease of a unit\npotential and subgraphs with definable potential values for all nodes are\npreferred. Combining the potential theory with the clustering and homophily\nmechanisms, it is deduced that the Bi-fan structure consisting of 4 nodes and 4\ndirected links is the most favored local structure in directed networks. Our\nhypothesis receives strongly positive supports from extensive experiments on 15\ndirected networks drawn from disparate fields, as indicated by the most\naccurate and robust performance of Bi-fan predictor within the link prediction\nframework. In summary, our main contribution is twofold: (i) We propose a new\nmechanism for the local organization of directed networks; (ii) We design the\ncorresponding link prediction algorithm, which can not only testify our\nhypothesis, but also find out direct applications in missing link prediction\nand friendship recommendation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 12:35:34 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2013 00:56:56 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Zhang", "Qian-Ming", ""], ["L\u00fc", "Linyuan", ""], ["Wang", "Wen-Qiang", ""], ["Zhu", "Yu-Xiao", ""], ["Zhou", "Tao", ""]]}, {"id": "1202.2794", "submitter": "Vinay Vaishampayan", "authors": "Vinay Anant Vaishampayan", "title": "Query Matrices for Retrieving Binary Vectors Based on the Hamming\n  Distance Oracle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hamming oracle returns the Hamming distance between an unknown binary\n$n$-vector $x$ and a binary query $n$-vector y. The objective is to determine\n$x$ uniquely using a sequence of $m$ queries. What are the minimum number of\nqueries required in the worst case? We consider the query ratio $m/n$ to be our\nfigure of merit and derive upper bounds on the query ratio by explicitly\nconstructing $(m,n)$ query matrices. We show that our recursive and algebraic\nconstruction results in query ratios arbitrarily close to zero. Our\nconstruction is based on codes of constant weight. A decoding algorithm for\nrecovering the unknown binary vector is also described.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 17:13:56 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Vaishampayan", "Vinay Anant", ""]]}, {"id": "1202.2880", "submitter": "William Webber", "authors": "William Webber", "title": "Approximate Recall Confidence Intervals", "comments": "To appear in ACM Transactions on Information Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recall, the proportion of relevant documents retrieved, is an important\nmeasure of effectiveness in information retrieval, particularly in the legal,\npatent, and medical domains. Where document sets are too large for exhaustive\nrelevance assessment, recall can be estimated by assessing a random sample of\ndocuments; but an indication of the reliability of this estimate is also\nrequired. In this article, we examine several methods for estimating two-tailed\nrecall confidence intervals. We find that the normal approximation in current\nuse provides poor coverage in many circumstances, even when adjusted to correct\nits inappropriate symmetry. Analytic and Bayesian methods based on the ratio of\nbinomials are generally more accurate, but are inaccurate on small populations.\nThe method we recommend derives beta-binomial posteriors on retrieved and\nunretrieved yield, with fixed hyperparameters, and a Monte Carlo estimate of\nthe posterior distribution of recall. We demonstrate that this method gives\nmean coverage at or near the nominal level, across several scenarios, while\nbeing balanced and stable. We offer advice on sampling design, including the\nallocation of assessments to the retrieved and unretrieved segments, and\ncompare the proposed beta-binomial with the officially reported normal\nintervals for recent TREC Legal Track iterations.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 21:54:50 GMT"}, {"version": "v2", "created": "Thu, 17 May 2012 16:41:07 GMT"}, {"version": "v3", "created": "Wed, 29 Aug 2012 16:33:08 GMT"}, {"version": "v4", "created": "Tue, 23 Oct 2012 20:56:16 GMT"}], "update_date": "2012-10-25", "authors_parsed": [["Webber", "William", ""]]}, {"id": "1202.2892", "submitter": "Dmitry Ignatov", "authors": "Dmitry I. Ignatov, Jonas Poelmans, Vasily Zaharchuk", "title": "Recommender System Based on Algorithm of Bicluster Analysis RecBi", "comments": null, "journal-ref": "CEUR Workshop proceedings Vol-757, CDUD'11 - Concept Discovery in\n  Unstructured Data, pp. 122-126, 2011", "doi": null, "report-no": null, "categories": "cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose two new algorithms based on biclustering analysis,\nwhich can be used at the basis of a recommender system for educational\norientation of Russian School graduates. The first algorithm was designed to\nhelp students make a choice between different university faculties when some of\ntheir preferences are known. The second algorithm was developed for the special\nsituation when nothing is known about their preferences. The final version of\nthis recommender system will be used by Higher School of Economics.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 23:10:08 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Ignatov", "Dmitry I.", ""], ["Poelmans", "Jonas", ""], ["Zaharchuk", "Vasily", ""]]}, {"id": "1202.2895", "submitter": "Dmitry Ignatov", "authors": "Jonas Poelmans, Paul Elzinga, Alexey Neznanov, Stijn Viaene, Sergei O.\n  Kuznetsov, Dmitry Ignatov, Guido Dedene", "title": "Concept Relation Discovery and Innovation Enabling Technology (CORDIET)", "comments": null, "journal-ref": "In CEUR Workshop proceedings Vol-757, CDUD'11 - Concept Discovery\n  in Unstructured Data, pp. 53-62, 2011", "doi": null, "report-no": null, "categories": "cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept Relation Discovery and Innovation Enabling Technology (CORDIET), is a\ntoolbox for gaining new knowledge from unstructured text data. At the core of\nCORDIET is the C-K theory which captures the essential elements of innovation.\nThe tool uses Formal Concept Analysis (FCA), Emergent Self Organizing Maps\n(ESOM) and Hidden Markov Models (HMM) as main artifacts in the analysis\nprocess. The user can define temporal, text mining and compound attributes. The\ntext mining attributes are used to analyze the unstructured text in documents,\nthe temporal attributes use these document's timestamps for analysis. The\ncompound attributes are XML rules based on text mining and temporal attributes.\nThe user can cluster objects with object-cluster rules and can chop the data in\npieces with segmentation rules. The artifacts are optimized for efficient data\nanalysis; object labels in the FCA lattice and ESOM map contain an URL on which\nthe user can click to open the selected document.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2012 23:19:51 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Poelmans", "Jonas", ""], ["Elzinga", "Paul", ""], ["Neznanov", "Alexey", ""], ["Viaene", "Stijn", ""], ["Kuznetsov", "Sergei O.", ""], ["Ignatov", "Dmitry", ""], ["Dedene", "Guido", ""]]}, {"id": "1202.2903", "submitter": "Tao Zhou", "authors": "Linyuan Lu, Zi-Ke Zhang, Tao Zhou", "title": "Scaling Laws in Human Language", "comments": "6 pages, 4 figures", "journal-ref": "Scientific Reports 3 (2013) 1082", "doi": "10.1038/srep01082", "report-no": null, "categories": "physics.data-an cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zipf's law on word frequency is observed in English, French, Spanish,\nItalian, and so on, yet it does not hold for Chinese, Japanese or Korean\ncharacters. A model for writing process is proposed to explain the above\ndifference, which takes into account the effects of finite vocabulary size.\nExperiments, simulations and analytical solution agree well with each other.\nThe results show that the frequency distribution follows a power law with\nexponent being equal to 1, at which the corresponding Zipf's exponent diverges.\nActually, the distribution obeys exponential form in the Zipf's plot. Deviating\nfrom the Heaps' law, the number of distinct words grows with the text length in\nthree stages: It grows linearly in the beginning, then turns to a logarithmical\nform, and eventually saturates. This work refines previous understanding about\nZipf's law and Heaps' law in language systems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 01:06:35 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Lu", "Linyuan", ""], ["Zhang", "Zi-Ke", ""], ["Zhou", "Tao", ""]]}, {"id": "1202.3185", "submitter": "Xin Shuai", "authors": "Xin Shuai and Xiaozhong Liu and Johan Bollen", "title": "Improving News Ranking by Community Tweets", "comments": "6 pages, 3 figures, 4 tables", "journal-ref": "workshop on mining social network dynamics @www2012", "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users frequently express their information needs by means of short and\ngeneral queries that are difficult for ranking algorithms to interpret\ncorrectly. However, users' social contexts can offer important additional\ninformation about their information needs which can be leveraged by ranking\nalgorithms to provide augmented, personalized results. Existing methods mostly\nrely on users' individual behavioral data such as clickstream and log data, but\nas a result suffer from data sparsity and privacy issues. Here, we propose a\nCommunity Tweets Voting Model (CTVM) to re-rank Google and Yahoo news search\nresults on the basis of open, large-scale Twitter community data. Experimental\nresults show that CTVM outperforms baseline rankings from Google and Yahoo for\ncertain online communities. We propose an application scenario of CTVM and\nprovide an agenda for further research.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 01:35:26 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2012 21:37:42 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Shuai", "Xin", ""], ["Liu", "Xiaozhong", ""], ["Bollen", "Johan", ""]]}, {"id": "1202.3451", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh and Pedro Contreras", "title": "The Future of Search and Discovery in Big Data Analytics: Ultrametric\n  Information Spaces", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider observation data, comprised of n observation vectors with values on\na set of attributes. This gives us n points in attribute space. Having data\nstructured as a tree, implied by having our observations embedded in an\nultrametric topology, offers great advantage for proximity searching. If we\nhave preprocessed data through such an embedding, then an observation's nearest\nneighbor is found in constant computational time, i.e. O(1) time. A further\npowerful approach is discussed in this work: the inducing of a hierarchy, and\nhence a tree, in linear computational time, i.e. O(n) time for n observations.\nIt is with such a basis for proximity search and best match that we can address\nthe burgeoning problems of processing very large, and possibly also very high\ndimensional, data sets.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2012 21:33:44 GMT"}], "update_date": "2012-02-17", "authors_parsed": [["Murtagh", "Fionn", ""], ["Contreras", "Pedro", ""]]}, {"id": "1202.3492", "submitter": "Mikhail Simkin", "authors": "M.V. Simkin and V.P. Roychowdhury", "title": "Why does attention to web articles fall with time?", "comments": "To appear in JASIST", "journal-ref": "Journal of the Association for Information Science and Technology,\n  66(9):1847-1856, 2015", "doi": "10.1002/asi.23289", "report-no": null, "categories": "cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze access statistics of a hundred and fifty blog entries and news\narticles, for periods of up to three years. Access rate falls as an inverse\npower of time passed since publication. The power law holds for periods of up\nto thousand days. The exponents are different for different blogs and are\ndistributed between 0.6 and 3.2. We argue that the decay of attention to a web\narticle is caused by the link to it first dropping down the list of links on\nthe website's front page, and then disappearing from the front page and its\nsubsequent movement further into background. The other proposed explanations\nthat use a decaying with time novelty factor, or some intricate theory of human\ndynamics cannot explain all of the experimental observations.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2012 00:31:32 GMT"}, {"version": "v2", "created": "Sun, 21 Dec 2014 00:53:03 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Simkin", "M. V.", ""], ["Roychowdhury", "V. P.", ""]]}, {"id": "1202.3706", "submitter": "Laurent Charlin", "authors": "Laurent Charlin, Richard S. Zemel, Craig Boutilier", "title": "A Framework for Optimizing Paper Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-86-95", "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the heart of many scientific conferences is the problem of matching\nsubmitted papers to suitable reviewers. Arriving at a good assignment is a\nmajor and important challenge for any conference organizer. In this paper we\npropose a framework to optimize paper-to-reviewer assignments. Our framework\nuses suitability scores to measure pairwise affinity between papers and\nreviewers. We show how learning can be used to infer suitability scores from a\nsmall set of provided scores, thereby reducing the burden on reviewers and\norganizers. We frame the assignment problem as an integer program and propose\nseveral variations for the paper-to-reviewer matching domain. We also explore\nhow learning and matching interact. Experiments on two conference data sets\nexamine the performance of several learning methods as well as the\neffectiveness of the matching formulations.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Charlin", "Laurent", ""], ["Zemel", "Richard S.", ""], ["Boutilier", "Craig", ""]]}, {"id": "1202.3752", "submitter": "Nebojsa Jojic", "authors": "Nebojsa Jojic, Alessandro Perina", "title": "Multidimensional counting grids: Inferring word order from disordered\n  bags of words", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-547-556", "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of bags of words typically assume topic mixing so that the words in a\nsingle bag come from a limited number of topics. We show here that many sets of\nbag of words exhibit a very different pattern of variation than the patterns\nthat are efficiently captured by topic mixing. In many cases, from one bag of\nwords to the next, the words disappear and new ones appear as if the theme\nslowly and smoothly shifted across documents (providing that the documents are\nsomehow ordered). Examples of latent structure that describe such ordering are\neasily imagined. For example, the advancement of the date of the news stories\nis reflected in a smooth change over the theme of the day as certain evolving\nnews stories fall out of favor and new events create new stories. Overlaps\namong the stories of consecutive days can be modeled by using windows over\nlinearly arranged tight distributions over words. We show here that such\nstrategy can be extended to multiple dimensions and cases where the ordering of\ndata is not readily obvious. We demonstrate that this way of modeling\ncovariation in word occurrences outperforms standard topic models in\nclassification and prediction tasks in applications in biology, text modeling\nand computer vision.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Jojic", "Nebojsa", ""], ["Perina", "Alessandro", ""]]}, {"id": "1202.4063", "submitter": "Rafi Muhammad", "authors": "Sundus Hassan, Muhammad Rafi and Muhammad Shahid Shaikh", "title": "Comparing SVM and Naive Bayes classifiers for text categorization with\n  Wikitology as knowledge enrichment", "comments": "5 pages", "journal-ref": "Multitopic Conference (INMIC), 2011 IEEE 14th International", "doi": "10.1109/INMIC.2011.6151495", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The activity of labeling of documents according to their content is known as\ntext categorization. Many experiments have been carried out to enhance text\ncategorization by adding background knowledge to the document using knowledge\nrepositories like Word Net, Open Project Directory (OPD), Wikipedia and\nWikitology. In our previous work, we have carried out intensive experiments by\nextracting knowledge from Wikitology and evaluating the experiment on Support\nVector Machine with 10- fold cross-validations. The results clearly indicate\nWikitology is far better than other knowledge bases. In this paper we are\ncomparing Support Vector Machine (SVM) and Na\\\"ive Bayes (NB) classifiers under\ntext enrichment through Wikitology. We validated results with 10-fold cross\nvalidation and shown that NB gives an improvement of +28.78%, on the other hand\nSVM gives an improvement of +6.36% when compared with baseline results. Na\\\"ive\nBayes classifier is better choice when external enriching is used through any\nexternal knowledge base.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2012 09:23:02 GMT"}], "update_date": "2012-02-21", "authors_parsed": [["Hassan", "Sundus", ""], ["Rafi", "Muhammad", ""], ["Shaikh", "Muhammad Shahid", ""]]}, {"id": "1202.4815", "submitter": "Saurabh  Pal", "authors": "Surjeet Kumar Yadav, Brijesh Bharadwaj and Saurabh Pal", "title": "Data Mining Applications: A comparative Study for Predicting Student's\n  performance", "comments": "7 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1201.3417 and arXiv:1201.3418", "journal-ref": "International Journal of Innovative Technology and Creative\n  Engineering, Vol.1 No.12 (2011) 13-19", "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Discovery and Data Mining (KDD) is a multidisciplinary area\nfocusing upon methodologies for extracting useful knowledge from data and there\nare several useful KDD tools to extracting the knowledge. This knowledge can be\nused to increase the quality of education. But educational institution does not\nuse any knowledge discovery process approach on these data. Data mining can be\nused for decision making in educational system. A decision tree classifier is\none of the most widely used supervised learning methods used for data\nexploration based on divide & conquer technique. This paper discusses use of\ndecision trees in educational data mining. Decision tree algorithms are applied\non students' past performance data to generate the model and this model can be\nused to predict the students' performance. It helps earlier in identifying the\ndropouts and students who need special attention and allow the teacher to\nprovide appropriate advising/counseling.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 04:15:54 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2012 15:52:32 GMT"}], "update_date": "2012-02-24", "authors_parsed": [["Yadav", "Surjeet Kumar", ""], ["Bharadwaj", "Brijesh", ""], ["Pal", "Saurabh", ""]]}, {"id": "1202.5469", "submitter": "Arkaitz Zubiaga", "authors": "Arkaitz Zubiaga", "title": "Enhancing Navigation on Wikipedia with Social Tags", "comments": "Wikimania 2009, 5th International Conference of the Wikimedia\n  Community", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Social tagging has become an interesting approach to improve search and\nnavigation over the actual Web, since it aggregates the tags added by different\nusers to the same resource in a collaborative way. This way, it results in a\nlist of weighted tags describing its resource. Combined to a classical\ntaxonomic classification system such as that by Wikipedia, social tags can\nenhance document navigation and search. On the one hand, social tags suggest\nalternative navigation ways, including pivot-browsing, popularity-driven\nnavigation, and filtering. On the other hand, it provides new metadata,\nsometimes uncovered by documents' content, that can substantially improve\ndocument search. In this work, the inclusion of an interface to add\nuser-defined tags describing Wikipedia articles is proposed, as a way to\nimprove article navigation and retrieval. As a result, a prototype on applying\ntags over Wikipedia is proposed in order to evaluate its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2012 19:31:43 GMT"}], "update_date": "2012-02-27", "authors_parsed": [["Zubiaga", "Arkaitz", ""]]}, {"id": "1202.5477", "submitter": "Arkaitz Zubiaga", "authors": "Arkaitz Zubiaga and Raquel Mart\\'inez and V\\'ictor Fresno", "title": "Analyzing Tag Distributions in Folksonomies for Resource Classification", "comments": null, "journal-ref": "KSEM 2011, 5th International Conference on Knowledge Science,\n  Engineering and Management", "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Recent research has shown the usefulness of social tags as a data source to\nfeed resource classification. Little is known about the effect of settings on\nfolksonomies created on social tagging systems. In this work, we consider the\nsettings of social tagging systems to further understand tag distributions in\nfolksonomies. We analyze in depth the tag distributions on three large-scale\nsocial tagging datasets, and analyze the effect on a resource classification\ntask. To this end, we study the appropriateness of applying weighting schemes\nbased on the well-known TF-IDF for resource classification. We show the great\nimportance of settings as to altering tag distributions. Among those settings,\ntag suggestions produce very different folksonomies, which condition the\nsuccess of the employed weighting schemes. Our findings and analyses are\nrelevant for researchers studying tag-based resource classification, user\nbehavior in social networks, the structure of folksonomies and tag\ndistributions, as well as for developers of social tagging systems in search of\nan appropriate setting.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2012 18:36:06 GMT"}], "update_date": "2012-02-27", "authors_parsed": [["Zubiaga", "Arkaitz", ""], ["Mart\u00ednez", "Raquel", ""], ["Fresno", "V\u00edctor", ""]]}, {"id": "1202.5820", "submitter": "Zi-Ke Zhang Mr.", "authors": "Zi-Ke Zhang, Tao Zhou, Yi-Cheng Zhang", "title": "Tag-Aware Recommender Systems: A State-of-the-art Survey", "comments": "19 pages, 3 figures", "journal-ref": "Journal of Computer Science and Technology 26 (2011) 767", "doi": "10.1007/s11390-011-0176-1", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, Social Tagging Systems have attracted increasing\nattention from both physical and computer science communities. Besides the\nunderlying structure and dynamics of tagging systems, many efforts have been\naddressed to unify tagging information to reveal user behaviors and\npreferences, extract the latent semantic relations among items, make\nrecommendations, and so on. Specifically, this article summarizes recent\nprogress about tag-aware recommender systems, emphasizing on the contributions\nfrom three mainstream perspectives and approaches: network-based methods,\ntensor-based methods, and the topic-based methods. Finally, we outline some\nother tag-related works and future challenges of tag-aware recommendation\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 03:37:14 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Zhang", "Zi-Ke", ""], ["Zhou", "Tao", ""], ["Zhang", "Yi-Cheng", ""]]}, {"id": "1202.6101", "submitter": "Parikshit Ram", "authors": "Parikshit Ram and Alexander G. Gray", "title": "Maximum Inner-Product Search using Tree Data-structures", "comments": "Under submission in KDD 2012", "journal-ref": null, "doi": "10.1145/2339530.2339677", "report-no": null, "categories": "cs.CG cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of {\\em efficiently} finding the best match for a query in a\ngiven set with respect to the Euclidean distance or the cosine similarity has\nbeen extensively studied in literature. However, a closely related problem of\nefficiently finding the best match with respect to the inner product has never\nbeen explored in the general setting to the best of our knowledge. In this\npaper we consider this general problem and contrast it with the existing\nbest-match algorithms. First, we propose a general branch-and-bound algorithm\nusing a tree data structure. Subsequently, we present a dual-tree algorithm for\nthe case where there are multiple queries. Finally we present a new data\nstructure for increasing the efficiency of the dual-tree algorithm. These\nbranch-and-bound algorithms involve novel bounds suited for the purpose of\nbest-matching with inner products. We evaluate our proposed algorithms on a\nvariety of data sets from various applications, and exhibit up to five orders\nof magnitude improvement in query time over the naive search technique.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 01:32:01 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Ram", "Parikshit", ""], ["Gray", "Alexander G.", ""]]}, {"id": "1202.6158", "submitter": "Dohy Hong", "authors": "Dohy Hong", "title": "Optimized on-line computation of PageRank algorithm", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.IR math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present new ideas to accelerate the computation of the\neigenvector of the transition matrix associated to the PageRank algorithm. New\nideas are based on the decomposition of the matrix-vector product that can be\nseen as a fluid diffusion model, associated to new algebraic equations. We show\nthrough experiments on synthetic data and on real data-sets how much this\napproach can improve the computation efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 09:56:43 GMT"}], "update_date": "2012-02-29", "authors_parsed": [["Hong", "Dohy", ""]]}, {"id": "1202.6685", "submitter": "Massimiliano Dal Mas", "authors": "Massimiliano Dal Mas", "title": "Faceted Semantic Search for Personalized Social Search", "comments": "12 pages, 1 figures; 1 table; for details see:\n  http://www.maxdalmas.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actual social networks (like Facebook, Twitter, Linkedin, ...) need to deal\nwith vagueness on ontological indeterminacy. In this paper is analyzed the\nprototyping of a faceted semantic search for personalized social search using\nthe \"joint meaning\" in a community environment. User researches in a\n\"collaborative\" environment defined by folksonomies can be supported by the\nmost common features on the faceted semantic search. A solution for the\ncontext-aware personalized search is based on \"joint meaning\" understood as a\njoint construal of the creators of the contents and the user of the contents\nusing the faced taxonomy with the Semantic Web. A proof-of concept prototype\nshows how the proposed methodological approach can also be applied to existing\npresentation components, built with different languages and/or component\ntechnologies.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 20:59:55 GMT"}], "update_date": "2012-03-01", "authors_parsed": [["Mas", "Massimiliano Dal", ""]]}]