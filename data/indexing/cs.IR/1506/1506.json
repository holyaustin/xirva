[{"id": "1506.00092", "submitter": "Athanasios N. Nikolakopoulos", "authors": "Athanasios N. Nikolakopoulos and John D. Garofalakis", "title": "Random Surfing Without Teleportation", "comments": "13 pages. Published in the Volume: \"Algorithms, Probability, Networks\n  and Games, Springer-Verlag, 2015\". (The updated version corrects small\n  typos/errors)", "journal-ref": null, "doi": "10.1007/978-3-319-24024-4_19", "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the standard Random Surfer Model, the teleportation matrix is necessary to\nensure that the final PageRank vector is well-defined. The introduction of this\nmatrix, however, results in serious problems and imposes fundamental\nlimitations to the quality of the ranking vectors. In this work, building on\nthe recently proposed NCDawareRank framework, we exploit the decomposition of\nthe underlying space into blocks, and we derive easy to check necessary and\nsufficient conditions for random surfing without teleportation.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 08:29:02 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2016 12:59:16 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Nikolakopoulos", "Athanasios N.", ""], ["Garofalakis", "John D.", ""]]}, {"id": "1506.00717", "submitter": "J. Shane Culpepper", "authors": "Charles L. A. Clarke, J. Shane Culpepper, and Alistair Moffat", "title": "Assessing Efficiency-Effectiveness Tradeoffs in Multi-Stage Retrieval\n  Systems Without Using Relevance Judgments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale retrieval systems are often implemented as a cascading sequence\nof phases -- a first filtering step, in which a large set of candidate\ndocuments are extracted using a simple technique such as Boolean matching\nand/or static document scores; and then one or more ranking steps, in which the\npool of documents retrieved by the filter is scored more precisely using dozens\nor perhaps hundreds of different features. The documents returned to the user\nare then taken from the head of the final ranked list. Here we examine methods\nfor measuring the quality of filtering and preliminary ranking stages, and show\nhow to use these measurements to tune the overall performance of the system.\nStandard top-weighted metrics used for overall system evaluation are not\nappropriate for assessing filtering stages, since the output is a set of\ndocuments, rather than an ordered sequence of documents. Instead, we use an\napproach in which a quality score is computed based on the discrepancy between\nfiltered and full evaluation. Unlike previous approaches, our methods do not\nrequire relevance judgments, and thus can be used with virtually any query set.\nWe show that this quality score directly correlates with actual differences in\nmeasured effectiveness when relevance judgments are available. Since the\nquality score does not require relevance judgments, it can be used to identify\nqueries that perform particularly poorly for a given filter. Using these\nmethods, we explore a wide range of filtering options using thousands of\nqueries, categorize the relative merits of the different approaches, and\nidentify useful parameter combinations.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 01:08:37 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Clarke", "Charles L. A.", ""], ["Culpepper", "J. Shane", ""], ["Moffat", "Alistair", ""]]}, {"id": "1506.00765", "submitter": "Rongrong Ji Rongrong Ji", "authors": "Zheng Cai, Donglin Cao, Rongrong Ji", "title": "Video (GIF) Sentiment Analysis using Large-Scale Mid-Level Ontology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  With faster connection speed, Internet users are now making social network a\nhuge reservoir of texts, images and video clips (GIF). Sentiment analysis for\nsuch online platform can be used to predict political elections, evaluates\neconomic indicators and so on. However, GIF sentiment analysis is quite\nchallenging, not only because it hinges on spatio-temporal visual\ncontentabstraction, but also for the relationship between such abstraction and\nfinal sentiment remains unknown.In this paper, we dedicated to find out such\nrelationship.We proposed a SentiPairSequence basedspatiotemporal visual\nsentiment ontology, which forms the midlevel representations for GIFsentiment.\nThe establishment process of SentiPair contains two steps. First, we construct\nthe Synset Forest to define the semantic tree structure of visual sentiment\nlabel elements. Then, through theSynset Forest, we organically select and\ncombine sentiment label elements to form a mid-level visual sentiment\nrepresentation. Our experiments indicate that SentiPair outperforms other\ncompeting mid-level attributes. Using SentiPair, our analysis frameworkcan\nachieve satisfying prediction accuracy (72.6%). We also opened ourdataset\n(GSO-2015) to the research community. GSO-2015 contains more than 6,000\nmanually annotated GIFs out of more than 40,000 candidates. Each is labeled\nwith both sentiment and SentiPair Sequence.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 06:31:57 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Cai", "Zheng", ""], ["Cao", "Donglin", ""], ["Ji", "Rongrong", ""]]}, {"id": "1506.00904", "submitter": "Melanie JI Muller PhD", "authors": "Julia Kiseleva and Melanie J.I. M\\\"uller and Lucas Bernardi and Chad\n  Davis and Ivan Kovacek and Mats Stafseng Einarsen and Jaap Kamps and\n  Alexander Tuzhilin and Djoerd Hiemstra", "title": "Where to Go on Your Next Trip? Optimizing Travel Destinations Based on\n  User Preferences", "comments": "6 pages, 2 figures in SIGIR 2015, SIRIP Symposium on IR in Practice", "journal-ref": null, "doi": "10.1145/2766462.2776777", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation based on user preferences is a common task for e-commerce\nwebsites. New recommendation algorithms are often evaluated by offline\ncomparison to baseline algorithms such as recommending random or the most\npopular items. Here, we investigate how these algorithms themselves perform and\ncompare to the operational production system in large scale online experiments\nin a real-world application. Specifically, we focus on recommending travel\ndestinations at Booking.com, a major online travel site, to users searching for\ntheir preferred vacation activities. To build ranking models we use\nmulti-criteria rating data provided by previous users after their stay at a\ndestination. We implement three methods and compare them to the current\nbaseline in Booking.com: random, most popular, and Naive Bayes. Our general\nconclusion is that, in an online A/B test with live users, our Naive-Bayes\nbased ranker increased user engagement significantly over the current online\nsystem.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 14:40:37 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Kiseleva", "Julia", ""], ["M\u00fcller", "Melanie J. I.", ""], ["Bernardi", "Lucas", ""], ["Davis", "Chad", ""], ["Kovacek", "Ivan", ""], ["Einarsen", "Mats Stafseng", ""], ["Kamps", "Jaap", ""], ["Tuzhilin", "Alexander", ""], ["Hiemstra", "Djoerd", ""]]}, {"id": "1506.01165", "submitter": "Thanh The Van", "authors": "Thanh Manh Le, Thanh The Van", "title": "Image Retrieval System Base on EMD Similarity Measure and S-Tree", "comments": "14 pages, 3 figures, Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper approaches the binary signature for each image based on the\npercentage of the pixels in each color images, at the same time the paper\nbuilds a similar measure between images based on EMD (Earth Mover's Distance).\nBesides, the paper proceeded to create the S-tree based on the similar measure\nEMD to store the image's binary signatures to quickly query image signature\ndata. From there, the paper build an image retrieval algorithm and CBIR\n(Content-Based Image Retrieval) based on a similar measure EMD and S-tree.\nBased on this theory, the paper proceeded to build application and experimental\nassessment of the process of querying image on the database system which have\nover 10,000 images.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 08:50:13 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Le", "Thanh Manh", ""], ["Van", "Thanh The", ""]]}, {"id": "1506.01273", "submitter": "David Martins de Matos", "authors": "Marta Apar\\'icio, Paulo Figueiredo, Francisco Raposo, David Martins de\n  Matos, Ricardo Ribeiro, Lu\\'is Marujo", "title": "Summarization of Films and Documentaries Based on Subtitles and Scripts", "comments": "7 pages, 9 tables, 4 figures, submitted to Pattern Recognition\n  Letters (Elsevier)", "journal-ref": "Pattern Recognition Letters, Volume 73, 1 April 2016, Pages 7-12", "doi": "10.1016/j.patrec.2015.12.016", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assess the performance of generic text summarization algorithms applied to\nfilms and documentaries, using the well-known behavior of summarization of news\narticles as reference. We use three datasets: (i) news articles, (ii) film\nscripts and subtitles, and (iii) documentary subtitles. Standard ROUGE metrics\nare used for comparing generated summaries against news abstracts, plot\nsummaries, and synopses. We show that the best performing algorithms are LSA,\nfor news articles and documentaries, and LexRank and Support Sets, for films.\nDespite the different nature of films and documentaries, their relative\nbehavior is in accordance with that obtained for news articles.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 15:07:14 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 12:41:55 GMT"}, {"version": "v3", "created": "Wed, 9 Mar 2016 16:50:43 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Apar\u00edcio", "Marta", ""], ["Figueiredo", "Paulo", ""], ["Raposo", "Francisco", ""], ["de Matos", "David Martins", ""], ["Ribeiro", "Ricardo", ""], ["Marujo", "Lu\u00eds", ""]]}, {"id": "1506.01709", "submitter": "H\\'ector P. Mart\\'inez", "authors": "Vincent E. Farrugia, H\\'ector P. Mart\\'inez, Georgios N. Yannakakis", "title": "The Preference Learning Toolbox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preference learning (PL) is a core area of machine learning that handles\ndatasets with ordinal relations. As the number of generated data of ordinal\nnature is increasing, the importance and role of the PL field becomes central\nwithin machine learning research and practice. This paper introduces an open\nsource, scalable, efficient and accessible preference learning toolbox that\nsupports the key phases of the data training process incorporating various\npopular data preprocessing, feature selection and preference learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 19:58:56 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Farrugia", "Vincent E.", ""], ["Mart\u00ednez", "H\u00e9ctor P.", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "1506.01743", "submitter": "Nuno Moniz", "authors": "Nuno Moniz, Lu\\'is Torgo and Magdalini Eirinaki", "title": "Socially Driven News Recommendation", "comments": "17 pages, 2 figures, submitted to the ACM Transactions on Intelligent\n  Systems and Technology (ACM TIST), Special Issue on Social Media Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The participatory Web has enabled the ubiquitous and pervasive access of\ninformation, accompanied by an increase of speed and reach in information\nsharing. Data dissemination services such as news aggregators are expected to\nprovide up-to-date, real-time information to the end users. News aggregators\nare in essence recommendation systems that filter and rank news stories in\norder to select the few that will appear on the users front screen at any time.\nOne of the main challenges in such systems is to address the recency and\nlatency problems, that is, to identify as soon as possible how important a news\nstory is. In this work we propose an integrated framework that aims at\npredicting the importance of news items upon their publication with a focus on\nrecent and highly popular news, employing resampling strategies, and at\ntranslating the result into concrete news rankings. We perform an extensive\nexperimental evaluation using real-life datasets of the proposed framework as\nboth a stand-alone system and when applied to news recommendations from Google\nNews. Additionally, we propose and evaluate a combinatorial solution to the\naugmentation of official media recommendations with social information. Results\nshow that the proposed approach complements and enhances the news rankings\ngenerated by state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 22:32:40 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 12:45:08 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Moniz", "Nuno", ""], ["Torgo", "Lu\u00eds", ""], ["Eirinaki", "Magdalini", ""]]}, {"id": "1506.02190", "submitter": "Lei Tang", "authors": "Lei Tang", "title": "Thresholding for Top-k Recommendation with Temporal Dynamics", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on top-k recommendation in domains where underlying data\ndistribution shifts overtime. We propose to learn a time-dependent bias for\neach item over whatever existing recommendation engine. Such a bias learning\nprocess alleviates data sparsity in constructing the engine, and at the same\ntime captures recent trend shift observed in data. We present an alternating\noptimization framework to resolve the bias learning problem, and develop\nmethods to handle a variety of commonly used recommendation evaluation\ncriteria, as well as large number of items and users in practice. The proposed\nalgorithm is examined, both offline and online, using real world data sets\ncollected from the largest retailer worldwide. Empirical results demonstrate\nthat the bias learning can almost always boost recommendation performance. We\nencourage other practitioners to adopt it as a standard component in\nrecommender systems where temporal dynamics is a norm.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 20:13:28 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 05:37:07 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Tang", "Lei", ""]]}, {"id": "1506.02602", "submitter": "Soumen Roy", "authors": "Soumya Jyoti Banerjee, Mohammad Azharuddin, Debanjan Sen, Smruti\n  Savale, Himadri Datta, Anjan Kr Dasgupta and Soumen Roy", "title": "Using complex networks towards information retrieval and diagnostics in\n  multidimensional imaging", "comments": "Replaced by published version. Detailed Supplementary Information on\n  journal website", "journal-ref": "Scientific Reports, 5: 17271 (2015)", "doi": "10.1038/srep17271", "report-no": null, "categories": "cs.IR cond-mat.stat-mech physics.soc-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fresh and broad yet simple approach towards information\nretrieval in general and diagnostics in particular by applying the theory of\ncomplex networks on multidimensional, dynamic images. We demonstrate a\nsuccessful use of our method with the time series generated from high content\nthermal imaging videos of patients suffering from the aqueous deficient dry eye\n(ADDE) disease. Remarkably, network analyses of thermal imaging time series of\ncontact lens users and patients upon whom Laser-Assisted in situ Keratomileusis\n(Lasik) surgery has been conducted, exhibit pronounced similarity with results\nobtained from ADDE patients. We also propose a general framework for the\ntransformation of multidimensional images to networks for futuristic biometry.\nOur approach is general and scalable to other fluctuation-based devices where\nnetwork parameters derived from fluctuations, act as effective discriminators\nand diagnostic markers.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 18:14:23 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 01:41:43 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Banerjee", "Soumya Jyoti", ""], ["Azharuddin", "Mohammad", ""], ["Sen", "Debanjan", ""], ["Savale", "Smruti", ""], ["Datta", "Himadri", ""], ["Dasgupta", "Anjan Kr", ""], ["Roy", "Soumen", ""]]}, {"id": "1506.02816", "submitter": "George Gkotsis", "authors": "George Gkotsis, Maria Liakata, Carlos Pedrinaci, John Domingue", "title": "Leveraging Textual Features for Best Answer Prediction in\n  Community-based Question Answering", "comments": "1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of determining the best answer in\nCommunity-based Question Answering (CQA) websites by focussing on the content.\nIn particular, we present a system, ACQUA [http://acqua.kmi.open.ac.uk], that\ncan be installed onto the majority of browsers as a plugin. The service offers\na seamless and accurate prediction of the answer to be accepted. Previous\nresearch on this topic relies on the exploitation of community feedback on the\nanswers, which involves rating of either users (e.g., reputation) or answers\n(e.g. scores manually assigned to answers). We propose a new technique that\nleverages the content/textual features of answers in a novel way. Our approach\ndelivers better results than related linguistics-based solutions and manages to\nmatch rating-based approaches. More specifically, the gain in performance is\nachieved by rendering the values of these features into a discretised form. We\nalso show how our technique manages to deliver equally good results in\nreal-time settings, as opposed to having to rely on information not always\nreadily available, such as user ratings and answer scores. We ran an evaluation\non 21 StackExchange websites covering around 4 million questions and more than\n8 million answers. We obtain 84% average precision and 70% recall, which shows\nthat our technique is robust, effective, and widely applicable.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 08:09:34 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 10:07:48 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Gkotsis", "George", ""], ["Liakata", "Maria", ""], ["Pedrinaci", "Carlos", ""], ["Domingue", "John", ""]]}, {"id": "1506.03214", "submitter": "Vincent Lemaire", "authors": "Patrick Luciano, Ismail Rebai, Vincent Lemaire", "title": "Increasing loyalty using predictive modeling in Business-to-Business\n  Telecommunication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer Relationship Management (CRM) is a key element of modern marketing\nstrategies. One of the most practical way to build useful knowledge on\ncustomers in a CRM system to produce scores to forecast churn behavior,\npropensity to subscribe to a new service... In AMEA zone (Asia, Middle East and\nAfrica zone), the context of fierce competition may represent a higher\npercentage, and particularly in B2B market. But by contrast, to our knowledge,\nno scientific papers were dedicated and published to detail the way to improve\nloyalty in B2B Telco market. If we can assume at low segments similarities\nbetween B2B and B2C, some research is required in order to model B2B user\nbehavior versus B2C behavior. This problematic stands actual as \"Bring Your own\nDevice\" (BYOD) becomes more and more trendy. Answering business requirements,\nour team applied some B2C predictive tools with adapting them to B2B in an AMEA\ncountry Orange affiliate.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 08:41:41 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Luciano", "Patrick", ""], ["Rebai", "Ismail", ""], ["Lemaire", "Vincent", ""]]}, {"id": "1506.03775", "submitter": "Prakhar Biyani", "authors": "Prakhar Biyani, Cornelia Caragea, Narayan Bhamidipati", "title": "Entity-Specific Sentiment Classification of Yahoo News Comments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment classification is widely used for product reviews and in online\nsocial media such as forums, Twitter, and blogs. However, the problem of\nclassifying the sentiment of user comments on news sites has not been addressed\nyet. News sites cover a wide range of domains including politics, sports,\ntechnology, and entertainment, in contrast to other online social sites such as\nforums and review sites, which are specific to a particular domain. A user\nassociated with a news site is likely to post comments on diverse topics (e.g.,\npolitics, smartphones, and sports) or diverse entities (e.g., Obama, iPhone, or\nGoogle). Classifying the sentiment of users tied to various entities may help\nobtain a holistic view of their personality, which could be useful in\napplications such as online advertising, content personalization, and political\ncampaign planning. In this paper, we formulate the problem of entity-specific\nsentiment classification of comments posted on news articles in Yahoo News and\npropose novel features that are specific to news comments. Experimental results\nshow that our models outperform state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 18:53:56 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Biyani", "Prakhar", ""], ["Caragea", "Cornelia", ""], ["Bhamidipati", "Narayan", ""]]}, {"id": "1506.04094", "submitter": "Saeedeh Shekarpour", "authors": "Christoph Lange, Saeedeh Shekarpour, Soren Auer", "title": "The WDAqua ITN: Answering Questions using Web Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WDAqua is a Marie Curie Innovative Training Network (ITN) and is funded under\nEU grant number 642795 and runs from January 2015 to December 2018. WDAqua aims\nat advancing the state of the art by intertwining training, research and\ninnovation efforts, centered around one service: data-driven question\nanswering. Question answering is immediately useful to a wide audience of end\nusers, and we will demonstrate this in settings including e-commerce, public\nsector information, publishing and smart cities. Question answering also covers\nweb science and data science broadly, leading to transferrable research results\nand to transferrable skills of the researchers who have finished our training\nprogramme. To ensure that our research improves question answering overall,\nevery individual research project connects at least two of these steps.\nIntersectional secondments (within a consortium covering academia, research\ninstitutes and industrial research as well as network-wide workshops, R and D\nchallenges and innovation projects further balance ground-breaking research and\nthe needs of society and industry. Training-wise these offers equip early stage\nresearchers with the expertise and transferable technical and non-technical\nskills that will allow them to pursue a successful career as an academic,\ndecision maker, practitioner or entrepreneur.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 10:56:33 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Lange", "Christoph", ""], ["Shekarpour", "Saeedeh", ""], ["Auer", "Soren", ""]]}, {"id": "1506.04135", "submitter": "Fabrice Rossi", "authors": "Arnaud De Myttenaere (SAMM, Viadeo), Boris Golden (Viadeo),\n  B\\'en\\'edicte Le Grand (CRI), Fabrice Rossi (SAMM)", "title": "Reducing offline evaluation bias of collaborative filtering algorithms", "comments": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN), Apr 2015, Bruges, Belgium.\n  pp.137-142, 2015, Proceedings of the 23-th European Symposium on Artificial\n  Neural Networks, Computational Intelligence and Machine Learning (ESANN 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems have been integrated into the majority of large online\nsystems to filter and rank information according to user profiles. It thus\ninfluences the way users interact with the system and, as a consequence, bias\nthe evaluation of the performance of a recommendation algorithm computed using\nhistorical data (via offline evaluation). This paper presents a new application\nof a weighted offline evaluation to reduce this bias for collaborative\nfiltering algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 19:57:27 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["De Myttenaere", "Arnaud", "", "SAMM, Viadeo"], ["Golden", "Boris", "", "Viadeo"], ["Grand", "B\u00e9n\u00e9dicte Le", "", "CRI"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1506.04322", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Jennifer Neville, Ryan A. Rossi, Nick Duffield, and\n  Theodore L. Willke", "title": "Graphlet Decomposition: Framework, Algorithms, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From social science to biology, numerous applications often rely on graphlets\nfor intuitive and meaningful characterization of networks at both the global\nmacro-level as well as the local micro-level. While graphlets have witnessed a\ntremendous success and impact in a variety of domains, there has yet to be a\nfast and efficient approach for computing the frequencies of these subgraph\npatterns. However, existing methods are not scalable to large networks with\nmillions of nodes and edges, which impedes the application of graphlets to new\nproblems that require large-scale network analysis. To address these problems,\nwe propose a fast, efficient, and parallel algorithm for counting graphlets of\nsize k={3,4}-nodes that take only a fraction of the time to compute when\ncompared with the current methods used. The proposed graphlet counting\nalgorithms leverages a number of proven combinatorial arguments for different\ngraphlets. For each edge, we count a few graphlets, and with these counts along\nwith the combinatorial arguments, we obtain the exact counts of others in\nconstant time. On a large collection of 300+ networks from a variety of\ndomains, our graphlet counting strategies are on average 460x faster than\ncurrent methods. This brings new opportunities to investigate the use of\ngraphlets on much larger networks and newer applications as we show in the\nexperiments. To the best of our knowledge, this paper provides the largest\ngraphlet computations to date as well as the largest systematic investigation\non over 300+ networks from a variety of domains.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 21:32:12 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 23:23:31 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Neville", "Jennifer", ""], ["Rossi", "Ryan A.", ""], ["Duffield", "Nick", ""], ["Willke", "Theodore L.", ""]]}, {"id": "1506.04584", "submitter": "Lin Xu", "authors": "Zhihai Yang, Lin Xu, Zhongmin Cai", "title": "Re-scale AdaBoost for Attack Detection in Collaborative Filtering\n  Recommender Systems", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering recommender systems (CFRSs) are the key components of\nsuccessful e-commerce systems. Actually, CFRSs are highly vulnerable to attacks\nsince its openness. However, since attack size is far smaller than that of\ngenuine users, conventional supervised learning based detection methods could\nbe too \"dull\" to handle such imbalanced classification. In this paper, we\nimprove detection performance from following two aspects. First, we extract\nwell-designed features from user profiles based on the statistical properties\nof the diverse attack models, making hard classification task becomes easier to\nperform. Then, refer to the general idea of re-scale Boosting (RBoosting) and\nAdaBoost, we apply a variant of AdaBoost, called the re-scale AdaBoost\n(RAdaBoost) as our detection method based on extracted features. RAdaBoost is\ncomparable to the optimal Boosting-type algorithm and can effectively improve\nthe performance in some hard scenarios. Finally, a series of experiments on the\nMovieLens-100K data set are conducted to demonstrate the outperformance of\nRAdaBoost comparing with some classical techniques such as SVM, kNN and\nAdaBoost.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 13:07:52 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Yang", "Zhihai", ""], ["Xu", "Lin", ""], ["Cai", "Zhongmin", ""]]}, {"id": "1506.04757", "submitter": "Julian McAuley", "authors": "Julian McAuley, Christopher Targett, Qinfeng Shi, Anton van den Hengel", "title": "Image-based Recommendations on Styles and Substitutes", "comments": "11 pages, 10 figures, SIGIR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans inevitably develop a sense of the relationships between objects, some\nof which are based on their appearance. Some pairs of objects might be seen as\nbeing alternatives to each other (such as two pairs of jeans), while others may\nbe seen as being complementary (such as a pair of jeans and a matching shirt).\nThis information guides many of the choices that people make, from buying\nclothes to their interactions with each other. We seek here to model this human\nsense of the relationships between objects based on their appearance. Our\napproach is not based on fine-grained modeling of user annotations but rather\non capturing the largest dataset possible and developing a scalable method for\nuncovering human notions of the visual relationships within. We cast this as a\nnetwork inference problem defined on graphs of related images, and provide a\nlarge-scale dataset for the training and evaluation of the same. The system we\ndevelop is capable of recommending which clothes and accessories will go well\ntogether (and which will not), amongst a host of other applications.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 20:01:49 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["McAuley", "Julian", ""], ["Targett", "Christopher", ""], ["Shi", "Qinfeng", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1506.05247", "submitter": "Zhihai Yang", "authors": "Zhihai Yang", "title": "Defending Grey Attacks by Exploiting Wavelet Analysis in Collaborative\n  Filtering Recommender Systems", "comments": "16 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Shilling\" attacks or \"profile injection\" attacks have always major\nchallenges in collaborative filtering recommender systems (CFRSs). Many efforts\nhave been devoted to improve collaborative filtering techniques which can\neliminate the \"shilling\" attacks. However, most of them focused on detecting\npush attack or nuke attack which is rated with the highest score or lowest\nscore on the target items. Few pay attention to grey attack when a target item\nis rated with a lower or higher score than the average score, which shows a\nmore hidden rating behavior than push or nuke attack. In this paper, we present\na novel detection method to make recommender systems resistant to such attacks.\nTo characterize grey ratings, we exploit rating deviation of item to\ndiscriminate between grey attack profiles and genuine profiles. In addition, we\nalso employ novelty and popularity of item to construct rating series. Since it\nis difficult to discriminate between the rating series of attacker and genuine\nusers, we incorporate into discrete wavelet transform (DWT) to amplify these\ndifferences based on the rating series of rating deviation, novelty and\npopularity, respectively. Finally, we respectively extract features from rating\nseries of rating deviation-based, novelty-based and popularity-based by using\namplitude domain analysis method and combine all clustered results as our\ndetection results. We conduct a list of experiments on both the Book-Crossing\nand HetRec-2011 datasets in diverse attack models. Experimental results were\nincluded to validate the effectiveness of our approach in comparison with the\nbenchmarked methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 08:54:04 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 07:30:47 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Yang", "Zhihai", ""]]}, {"id": "1506.05402", "submitter": "Philipp Mayr", "authors": "Iana Atanassova, Marc Bertin, Philipp Mayr", "title": "Editorial for the First Workshop on Mining Scientific Papers:\n  Computational Linguistics and Bibliometrics", "comments": "4 pages, Workshop on Mining Scientific Papers: Computational\n  Linguistics and Bibliometrics at ISSI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The workshop \"Mining Scientific Papers: Computational Linguistics and\nBibliometrics\" (CLBib 2015), co-located with the 15th International Society of\nScientometrics and Informetrics Conference (ISSI 2015), brought together\nresearchers in Bibliometrics and Computational Linguistics in order to study\nthe ways Bibliometrics can benefit from large-scale text analytics and sense\nmining of scientific papers, thus exploring the interdisciplinarity of\nBibliometrics and Natural Language Processing (NLP). The goals of the workshop\nwere to answer questions like: How can we enhance author network analysis and\nBibliometrics using data obtained by text analytics? What insights can NLP\nprovide on the structure of scientific writing, on citation networks, and on\nin-text citation analysis? This workshop is the first step to foster the\nreflection on the interdisciplinarity and the benefits that the two disciplines\nBibliometrics and Natural Language Processing can drive from it.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 18:03:25 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Atanassova", "Iana", ""], ["Bertin", "Marc", ""], ["Mayr", "Philipp", ""]]}, {"id": "1506.05514", "submitter": "Ubai Sandouk", "authors": "Ubai Sandouk, Ke Chen", "title": "Learning Contextualized Semantics from Co-occurring Terms via a Siamese\n  Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": "2015-06-18", "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest challenges in Multimedia information retrieval and\nunderstanding is to bridge the semantic gap by properly modeling concept\nsemantics in context. The presence of out of vocabulary (OOV) concepts\nexacerbates this difficulty. To address the semantic gap issues, we formulate a\nproblem on learning contextualized semantics from descriptive terms and propose\na novel Siamese architecture to model the contextualized semantics from\ndescriptive terms. By means of pattern aggregation and probabilistic topic\nmodels, our Siamese architecture captures contextualized semantics from the\nco-occurring descriptive terms via unsupervised learning, which leads to a\nconcept embedding space of the terms in context. Furthermore, the co-occurring\nOOV concepts can be easily represented in the learnt concept embedding space.\nThe main properties of the concept embedding space are demonstrated via\nvisualization. Using various settings in semantic priming, we have carried out\na thorough evaluation by comparing our approach to a number of state-of-the-art\nmethods on six annotation corpora in different domains, i.e., MagTag5K, CAL500\nand Million Song Dataset in the music domain as well as Corel5K, LabelMe and\nSUNDatabase in the image domain. Experimental results on semantic priming\nsuggest that our approach outperforms those state-of-the-art methods\nconsiderably in various aspects.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 23:03:43 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Sandouk", "Ubai", ""], ["Chen", "Ke", ""]]}, {"id": "1506.05628", "submitter": "Martin Tomko PhD.", "authors": "Yongli Ren, Martin Tomko, Flora Salim, Kevin Ong, Mark Sanderson", "title": "Analyzing Web Behavior in Indoor Retail Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze 18 million rows of Wi-Fi access logs collected over a one year\nperiod from over 120,000 anonymized users at an inner-city shopping mall. The\nanonymized dataset gathered from an opt-in system provides users' approximate\nphysical location, as well as Web browsing and some search history. Such data\nprovides a unique opportunity to analyze the interaction between people's\nbehavior in physical retail spaces and their Web behavior, serving as a proxy\nto their information needs. We find: (1) the use of Wi-Fi network maps the\nopening hours of the mall; (2) there is a weekly periodicity in users' visits\nto the mall; (3) around 60% of registered Wi-Fi users actively browse the Web\nand around 10% of them use Wi-Fi for accessing Web search engines; (4) people\nare likely to spend a relatively constant amount of time browsing the Web while\ntheir visiting duration may vary; (5) people tend to visit similar mall\nlocations and Web content during their repeated visits to the mall; (6) the\nphysical spatial context has a small but significant influence on the Web\ncontent that indoor users browse; (7) accompanying users tend to access\nresources from the same Web domains.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 11:21:59 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Ren", "Yongli", ""], ["Tomko", "Martin", ""], ["Salim", "Flora", ""], ["Ong", "Kevin", ""], ["Sanderson", "Mark", ""]]}, {"id": "1506.05672", "submitter": "Philipp Schaer", "authors": "Nadine Dulisch, Andreas Oskar Kempf, Philipp Schaer", "title": "Query Expansion for Survey Question Retrieval in the Social Sciences", "comments": "to appear in Proceedings of 19th International Conference on Theory\n  and Practice of Digital Libraries 2015 (TPDL 2015)", "journal-ref": null, "doi": "10.1007/978-3-319-24592-8_3", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the importance of research data and the need to archive and\nto share it in the scientific community have increased enormously. This\nintroduces a whole new set of challenges for digital libraries. In the social\nsciences typical research data sets consist of surveys and questionnaires. In\nthis paper we focus on the use case of social science survey question reuse and\non mechanisms to support users in the query formulation for data sets. We\ndescribe and evaluate thesaurus- and co-occurrence-based approaches for query\nexpansion to improve retrieval quality in digital libraries and research data\narchives. The challenge here is to translate the information need and the\nunderlying sociological phenomena into proper queries. As we can show retrieval\nquality can be improved by adding related terms to the queries. In a direct\ncomparison automatically expanded queries using extracted co-occurring terms\ncan provide better results than queries manually reformulated by a domain\nexpert and better results than a keyword-based BM25 baseline.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 13:35:28 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Dulisch", "Nadine", ""], ["Kempf", "Andreas Oskar", ""], ["Schaer", "Philipp", ""]]}, {"id": "1506.05752", "submitter": "Zhihai Yang", "authors": "Zhihai Yang", "title": "Detecting Abnormal Profiles in Collaborative Filtering Recommender\n  Systems", "comments": "13 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1506.04584, arXiv:1506.05247", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalization collaborative filtering recommender systems (CFRSs) are the\ncrucial components of popular e-commerce services. In practice, CFRSs are also\nparticularly vulnerable to \"shilling\" attacks or \"profile injection\" attacks\ndue to their openness. The attackers can carefully inject chosen attack\nprofiles into CFRSs in order to bias the recommendation results to their\nbenefits. To reduce this risk, various detection techniques have been proposed\nto detect such attacks, which use diverse features extracted from user\nprofiles. However, relying on limited features to improve the detection\nperformance is difficult seemingly, since the existing features can not fully\ncharacterize the attack profiles and genuine profiles. In this paper, we\npropose a novel detection method to make recommender systems resistant to the\n\"shilling\" attacks or \"profile injection\" attacks. The existing features can be\nbriefly summarized as two aspects including rating behavior based and item\ndistribution based. We firstly formulate the problem as finding a mapping model\nbetween rating behavior and item distribution by exploiting the least-squares\napproximate solution. Based on the trained model, we design a detector by\nemploying a regressor to detect such attacks. Extensive experiments on both the\nMovieLens-100K and MovieLens-ml-latest-small datasets examine the effectiveness\nof our proposed detection method. Experimental results were included to\nvalidate the outperformance of our approach in comparison with benchmarked\nmethod including KNN.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 17:26:14 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 08:05:17 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2015 07:25:05 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Yang", "Zhihai", ""]]}, {"id": "1506.05865", "submitter": "Baotian Hu", "authors": "Baotian Hu, Qingcai Chen, Fangze Zhu", "title": "LCSTS: A Large Scale Chinese Short Text Summarization Dataset", "comments": "Recently, we received feedbacks from Yuya Taguchi from NAIST in Japan\n  and Qian Chen from USTC of China, that the results in the EMNLP2015 version\n  seem to be underrated. So we carefully checked our results and find out that\n  we made a mistake while using the standard ROUGE. Then we re-evaluate all\n  methods in the paper and get corrected results listed in Table 2 of this\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic text summarization is widely regarded as the highly difficult\nproblem, partially because of the lack of large text summarization data set.\nDue to the great challenge of constructing the large scale summaries for full\ntext, in this paper, we introduce a large corpus of Chinese short text\nsummarization dataset constructed from the Chinese microblogging website Sina\nWeibo, which is released to the public\n{http://icrc.hitsz.edu.cn/Article/show/139.html}. This corpus consists of over\n2 million real Chinese short texts with short summaries given by the author of\neach text. We also manually tagged the relevance of 10,666 short summaries with\ntheir corresponding short texts. Based on the corpus, we introduce recurrent\nneural network for the summary generation and achieve promising results, which\nnot only shows the usefulness of the proposed corpus for short text\nsummarization research, but also provides a baseline for further research on\nthis topic.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 02:40:42 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2015 14:33:39 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2015 02:43:38 GMT"}, {"version": "v4", "created": "Fri, 19 Feb 2016 16:35:35 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Hu", "Baotian", ""], ["Chen", "Qingcai", ""], ["Zhu", "Fangze", ""]]}, {"id": "1506.06033", "submitter": "Milivoj Simeonovski", "authors": "Milivoj Simeonovski, Fabian Bendun, Muhammad Rizwan Asghar, Michael\n  Backes, Ninja Marnau, Peter Druschel", "title": "Oblivion: Mitigating Privacy Leaks by Controlling the Discoverability of\n  Online Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines are the prevalently used tools to collect information about\nindividuals on the Internet. Search results typically comprise a variety of\nsources that contain personal information -- either intentionally released by\nthe person herself, or unintentionally leaked or published by third parties,\noften with detrimental effects on the individual's privacy. To grant\nindividuals the ability to regain control over their disseminated personal\ninformation, the European Court of Justice recently ruled that EU citizens have\na right to be forgotten in the sense that indexing systems, must offer them\ntechnical means to request removal of links from search results that point to\nsources violating their data protection rights. As of now, these technical\nmeans consist of a web form that requires a user to manually identify all\nrelevant links upfront and to insert them into the web form, followed by a\nmanual evaluation by employees of the indexing system to assess if the request\nis eligible and lawful.\n  We propose a universal framework Oblivion to support the automation of the\nright to be forgotten in a scalable, provable and privacy-preserving manner.\nFirst, Oblivion enables a user to automatically find and tag her disseminated\npersonal information using natural language processing and image recognition\ntechniques and file a request in a privacy-preserving manner. Second, Oblivion\nprovides indexing systems with an automated and provable eligibility mechanism,\nasserting that the author of a request is indeed affected by an online\nresource. The automated ligibility proof ensures censorship-resistance so that\nonly legitimately affected individuals can request the removal of corresponding\nlinks from search results. We have conducted comprehensive evaluations, showing\nthat Oblivion is capable of handling 278 removal requests per second, and is\nhence suitable for large-scale deployment.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 14:45:31 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Simeonovski", "Milivoj", ""], ["Bendun", "Fabian", ""], ["Asghar", "Muhammad Rizwan", ""], ["Backes", "Michael", ""], ["Marnau", "Ninja", ""], ["Druschel", "Peter", ""]]}, {"id": "1506.06418", "submitter": "Raphael Hoffmann", "authors": "Raphael Hoffmann, Luke Zettlemoyer, Daniel S. Weld", "title": "Extreme Extraction: Only One Hour per Relation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Extraction (IE) aims to automatically generate a large knowledge\nbase from natural language text, but progress remains slow. Supervised learning\nrequires copious human annotation, while unsupervised and weakly supervised\napproaches do not deliver competitive accuracy. As a result, most fielded\napplications of IE, as well as the leading TAC-KBP systems, rely on significant\namounts of manual engineering. Even \"Extreme\" methods, such as those reported\nin Freedman et al. 2011, require about 10 hours of expert labor per relation.\n  This paper shows how to reduce that effort by an order of magnitude. We\npresent a novel system, InstaRead, that streamlines authoring with an ensemble\nof methods: 1) encoding extraction rules in an expressive and compositional\nrepresentation, 2) guiding the user to promising rules based on corpus\nstatistics and mined resources, and 3) introducing a new interactive\ndevelopment cycle that provides immediate feedback --- even on large datasets.\nExperiments show that experts can create quality extractors in under an hour\nand even NLP novices can author good extractors. These extractors equal or\noutperform ones obtained by comparably supervised and state-of-the-art\ndistantly supervised approaches.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2015 22:04:39 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Hoffmann", "Raphael", ""], ["Zettlemoyer", "Luke", ""], ["Weld", "Daniel S.", ""]]}, {"id": "1506.06490", "submitter": "Baotian Hu", "authors": "Xiaoqiang Zhou, Baotian Hu, Qingcai Chen, Buzhou Tang, Xiaolong Wang", "title": "Answer Sequence Learning with Neural Networks for Answer Selection in\n  Community Question Answering", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the answer selection problem in community question answering\n(CQA) is regarded as an answer sequence labeling task, and a novel approach is\nproposed based on the recurrent architecture for this problem. Our approach\napplies convolution neural networks (CNNs) to learning the joint representation\nof question-answer pair firstly, and then uses the joint representation as\ninput of the long short-term memory (LSTM) to learn the answer sequence of a\nquestion for labeling the matching quality of each answer. Experiments\nconducted on the SemEval 2015 CQA dataset shows the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 07:26:51 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Zhou", "Xiaoqiang", ""], ["Hu", "Baotian", ""], ["Chen", "Qingcai", ""], ["Tang", "Buzhou", ""], ["Wang", "Xiaolong", ""]]}, {"id": "1506.06628", "submitter": "Yunchao Wei", "authors": "Yunchao Wei, Yao Zhao, Zhenfeng Zhu, Shikui Wei, Yanhui Xiao, Jiashi\n  Feng and Shuicheng Yan", "title": "Modality-dependent Cross-media Retrieval", "comments": "in ACM Transactions on Intelligent Systems and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the cross-media retrieval between images and\ntext, i.e., using image to search text (I2T) and using text to search images\n(T2I). Existing cross-media retrieval methods usually learn one couple of\nprojections, by which the original features of images and text can be projected\ninto a common latent space to measure the content similarity. However, using\nthe same projections for the two different retrieval tasks (I2T and T2I) may\nlead to a tradeoff between their respective performances, rather than their\nbest performances. Different from previous works, we propose a\nmodality-dependent cross-media retrieval (MDCR) model, where two couples of\nprojections are learned for different cross-media retrieval tasks instead of\none couple of projections. Specifically, by jointly optimizing the correlation\nbetween images and text and the linear regression from one modal space (image\nor text) to the semantic space, two couples of mappings are learned to project\nimages and text from their original feature spaces into two common latent\nsubspaces (one for I2T and the other for T2I). Extensive experiments show the\nsuperiority of the proposed MDCR compared with other methods. In particular,\nbased the 4,096 dimensional convolutional neural network (CNN) visual feature\nand 100 dimensional LDA textual feature, the mAP of the proposed method\nachieves 41.5\\%, which is a new state-of-the-art performance on the Wikipedia\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 14:33:39 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2015 01:34:01 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Wei", "Yunchao", ""], ["Zhao", "Yao", ""], ["Zhu", "Zhenfeng", ""], ["Wei", "Shikui", ""], ["Xiao", "Yanhui", ""], ["Feng", "Jiashi", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1506.07116", "submitter": "Carlo A. Trugenberger", "authors": "Carlo A. Trugenberger", "title": "Scientific Discovery by Machine Intelligence: A New Avenue for Drug\n  Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of big data is unstructured and of this majority the largest\nchunk is text. While data mining techniques are well developed and standardized\nfor structured, numerical data, the realm of unstructured data is still largely\nunexplored. The general focus lies on information extraction, which attempts to\nretrieve known information from text. The Holy Grail, however is knowledge\ndiscovery, where machines are expected to unearth entirely new facts and\nrelations that were not previously known by any human expert. Indeed,\nunderstanding the meaning of text is often considered as one of the main\ncharacteristics of human intelligence. The ultimate goal of semantic artificial\nintelligence is to devise software that can understand the meaning of free\ntext, at least in the practical sense of providing new, actionable information\ncondensed out of a body of documents. As a stepping stone on the road to this\nvision I will introduce a totally new approach to drug research, namely that of\nidentifying relevant information by employing a self-organizing semantic engine\nto text mine large repositories of biomedical research papers, a technique\npioneered by Merck with the InfoCodex software. I will describe the methodology\nand a first successful experiment for the discovery of new biomarkers and\nphenotypes for diabetes and obesity on the basis of PubMed abstracts, public\nclinical trials and Merck internal documents. The reported approach shows much\npromise and has potential to impact fundamentally pharmaceutical research as a\nway to shorten time-to-market of novel drugs, and for early recognition of dead\nends.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 18:04:14 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Trugenberger", "Carlo A.", ""]]}, {"id": "1506.07477", "submitter": "Jiatao Gu", "authors": "Jiatao Gu and Victor O.K. Li", "title": "Efficient Learning for Undirected Topic Models", "comments": "Accepted by ACL-IJCNLP 2015 short paper. 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replicated Softmax model, a well-known undirected topic model, is powerful in\nextracting semantic representations of documents. Traditional learning\nstrategies such as Contrastive Divergence are very inefficient. This paper\nprovides a novel estimator to speed up the learning based on Noise Contrastive\nEstimate, extended for documents of variant lengths and weighted inputs.\nExperiments on two benchmarks show that the new estimator achieves great\nlearning efficiency and high accuracy on document retrieval and classification.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 17:27:28 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Gu", "Jiatao", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1506.08316", "submitter": "Jianshu Chao", "authors": "Jianshu Chao and Eckehard Steinbach", "title": "Keypoint Encoding for Improved Feature Extraction from Compressed Video\n  at Low Bitrates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many mobile visual analysis applications, compressed video is transmitted\nover a communication network and analyzed by a server. Typical processing steps\nperformed at the server include keypoint detection, descriptor calculation, and\nfeature matching. Video compression has been shown to have an adverse effect on\nfeature-matching performance. The negative impact of compression can be reduced\nby using the keypoints extracted from the uncompressed video to calculate\ndescriptors from the compressed video. Based on this observation, we propose to\nprovide these keypoints to the server as side information and to extract only\nthe descriptors from the compressed video. First, we introduce four different\nframe types for keypoint encoding to address different types of changes in\nvideo content. These frame types represent a new scene, the same scene, a\nslowly changing scene, or a rapidly moving scene and are determined by\ncomparing features between successive video frames. Then, we propose Intra,\nSkip and Inter modes of encoding the keypoints for different frame types. For\nexample, keypoints for new scenes are encoded using the Intra mode, and\nkeypoints for unchanged scenes are skipped. As a result, the bitrate of the\nside information related to keypoint encoding is significantly reduced.\nFinally, we present pairwise matching and image retrieval experiments conducted\nto evaluate the performance of the proposed approach using the Stanford mobile\naugmented reality dataset and 720p format videos. The results show that the\nproposed approach offers significantly improved feature matching and image\nretrieval performance at a given bitrate.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 17:33:34 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 16:43:42 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Chao", "Jianshu", ""], ["Steinbach", "Eckehard", ""]]}, {"id": "1506.08454", "submitter": "Vijil Chenthamarakshan", "authors": "Vijil Chenthamarakshan, Prasad M Desphande, Raghu Krishnapuram,\n  Ramakrishna Varadarajan, Knut Stolze", "title": "WYSIWYE: An Algebra for Expressing Spatial and Textual Rules for Visual\n  Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual layout of a webpage can provide valuable clues for certain types\nof Information Extraction (IE) tasks. In traditional rule based IE frameworks,\nthese layout cues are mapped to rules that operate on the HTML source of the\nwebpages. In contrast, we have developed a framework in which the rules can be\nspecified directly at the layout level. This has many advantages, since the\nhigher level of abstraction leads to simpler extraction rules that are largely\nindependent of the source code of the page, and, therefore, more robust. It can\nalso enable specification of new types of rules that are not otherwise\npossible. To the best of our knowledge, there is no general framework that\nallows declarative specification of information extraction rules based on\nspatial layout. Our framework is complementary to traditional text based rules\nframework and allows a seamless combination of spatial layout based rules with\ntraditional text based rules. We describe the algebra that enables such a\nsystem and its efficient implementation using standard relational and text\nindexing features of a relational database. We demonstrate the simplicity and\nefficiency of this system for a task involving the extraction of software\nsystem requirements from software product pages.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 21:17:26 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 19:49:41 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Chenthamarakshan", "Vijil", ""], ["Desphande", "Prasad M", ""], ["Krishnapuram", "Raghu", ""], ["Varadarajan", "Ramakrishna", ""], ["Stolze", "Knut", ""]]}, {"id": "1506.08789", "submitter": "Najla Al-Saati", "authors": "Najla Al-Saati and Raghda Abdul-Jaleel", "title": "Requirement Tracing using Term Extraction", "comments": null, "journal-ref": "IJCSIS Vol. 13, No. 5, 2015", "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Requirements traceability is an essential step in ensuring the quality of\nsoftware during the early stages of its development life cycle. Requirements\ntracing usually consists of document parsing, candidate link generation and\nevaluation and traceability analysis. This paper demonstrates the applicability\nof Statistical Term Extraction metrics to generate candidate links. It is\napplied and validated using two data sets and four types of filters two for\neach data set, 0.2 and 0.25 for MODIS, 0 and 0.05 for CM1. This method\ngenerates requirements traceability matrices between textual requirements\nartifacts (such as high-level requirements traced to low-level requirements).\nThe proposed method includes ten word frequency metrics divided into three main\ngroups for calculating the frequency of terms. The results show that the\nproposed method gives better result when compared with the traditional TF-IDF\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 19:21:04 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Al-Saati", "Najla", ""], ["Abdul-Jaleel", "Raghda", ""]]}, {"id": "1506.08839", "submitter": "Julian McAuley", "authors": "Julian McAuley and Rahul Pandey and Jure Leskovec", "title": "Inferring Networks of Substitutable and Complementary Products", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a modern recommender system, it is important to understand how products\nrelate to each other. For example, while a user is looking for mobile phones,\nit might make sense to recommend other phones, but once they buy a phone, we\nmight instead want to recommend batteries, cases, or chargers. These two types\nof recommendations are referred to as substitutes and complements: substitutes\nare products that can be purchased instead of each other, while complements are\nproducts that can be purchased in addition to each other.\n  Here we develop a method to infer networks of substitutable and complementary\nproducts. We formulate this as a supervised link prediction task, where we\nlearn the semantics of substitutes and complements from data associated with\nproducts. The primary source of data we use is the text of product reviews,\nthough our method also makes use of features such as ratings, specifications,\nprices, and brands. Methodologically, we build topic models that are trained to\nautomatically discover topics from text that are successful at predicting and\nexplaining such relationships. Experimentally, we evaluate our system on the\nAmazon product catalog, a large dataset consisting of 9 million products, 237\nmillion links, and 144 million reviews.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 20:06:28 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["McAuley", "Julian", ""], ["Pandey", "Rahul", ""], ["Leskovec", "Jure", ""]]}, {"id": "1506.08891", "submitter": "Miao Fan", "authors": "Miao Fan, Doo Soon Kim", "title": "Detecting Table Region in PDF Documents Using Distant Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superior to state-of-the-art approaches which compete in table recognition\nwith 67 annotated government reports in PDF format released by {\\it ICDAR 2013\nTable Competition}, this paper contributes a novel paradigm leveraging\nlarge-scale unlabeled PDF documents to open-domain table detection. We\nintegrate the paradigm into our latest developed system ({\\it PdfExtra}) to\ndetect the region of tables by means of 9,466 academic articles from the entire\nrepository of {\\it ACL Anthology}, where almost all papers are archived by PDF\nformat without annotation for tables. The paradigm first designs heuristics to\nautomatically construct weakly labeled data. It then feeds diverse evidences,\nsuch as layouts of documents and linguistic features, which are extracted by\n{\\it Apache PDFBox} and processed by {\\it Stanford NLP} toolkit, into different\ncanonical classifiers. We finally use these classifiers, i.e. {\\it Naive\nBayes}, {\\it Logistic Regression} and {\\it Support Vector Machine}, to\ncollaboratively vote on the region of tables. Experimental results show that\n{\\it PdfExtra} achieves a great leap forward, compared with the\nstate-of-the-art approach. Moreover, we discuss the factors of different\nfeatures, learning models and even domains of documents that may impact the\nperformance. Extensive evaluations demonstrate that our paradigm is compatible\nenough to leverage various features and learning models for open-domain table\nregion detection within PDF files.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 22:54:17 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 17:13:17 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2015 01:27:26 GMT"}, {"version": "v4", "created": "Thu, 9 Jul 2015 17:06:10 GMT"}, {"version": "v5", "created": "Mon, 27 Jul 2015 23:00:40 GMT"}, {"version": "v6", "created": "Tue, 22 Sep 2015 17:53:05 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Fan", "Miao", ""], ["Kim", "Doo Soon", ""]]}, {"id": "1506.08966", "submitter": "Rafi Muhammad", "authors": "Bilal Hayat Butt, Muhammad Rafi, Arsal Jamal, Raja Sami Ur Rehman,\n  Syed Muhammad Zubair Alam and Muhammad Bilal Alam", "title": "Classification of Research Citations (CRC)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research is a continuous phenomenon. It is recursive in nature. Every\nresearch is based on some earlier research outcome. A general approach in\nreviewing the literature for a problem is to categorize earlier work for the\nsame problem as positive and negative citations. In this paper, we propose a\nnovel automated technique, which classifies whether an earlier work is cited as\nsentiment positive or sentiment negative. Our approach first extracted the\nportion of the cited text from citing paper. Using a sentiment lexicon we\nclassify the citation as positive or negative by picking a window of at most\nfive (5) sentences around the cited place (corpus). We have used Na\\\"ive-Bayes\nClassifier for sentiment analysis. The algorithm is evaluated on a manually\nannotated and class labelled collection of 150 research papers from the domain\nof computer science. Our preliminary results show an accuracy of 80%. We assert\nthat our approach can be generalized to classification of scientific research\npapers in different disciplines.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 07:16:39 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Butt", "Bilal Hayat", ""], ["Rafi", "Muhammad", ""], ["Jamal", "Arsal", ""], ["Rehman", "Raja Sami Ur", ""], ["Alam", "Syed Muhammad Zubair", ""], ["Alam", "Muhammad Bilal", ""]]}]