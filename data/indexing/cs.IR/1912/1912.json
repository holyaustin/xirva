[{"id": "1912.00127", "submitter": "Chowdhury Rahman", "authors": "Md. Hasibur Rahman, Chowdhury Rafeed Rahman, Ruhul Amin, Md. Habibur\n  Rahman Sifat and Afra Anika", "title": "A Hybrid Approach Towards Two Stage Bengali Question Classification\n  Utilizing Smart Data Balancing Technique", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-52856-0_36", "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question classification (QC) is the primary step of the Question Answering\n(QA) system. Question Classification (QC) system classifies the questions in\nparticular classes so that Question Answering (QA) System can provide correct\nanswers for the questions. Our system categorizes the factoid type questions\nasked in natural language after extracting features of the questions. We\npresent a two stage QC system for Bengali. It utilizes one dimensional\nconvolutional neural network for classifying questions into coarse classes in\nthe first stage. Word2vec representation of existing words of the question\ncorpus have been constructed and used for assisting 1D CNN. A smart data\nbalancing technique has been employed for giving data hungry convolutional\nneural network the advantage of a greater number of effective samples to learn\nfrom. For each coarse class, a separate Stochastic Gradient Descent (SGD) based\nclassifier has been used in order to differentiate among the finer classes\nwithin that coarse class. TF-IDF representation of each word has been used as\nfeature for the SGD classifiers implemented as part of second stage\nclassification. Experiments show the effectiveness of our proposed method for\nBengali question classification.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 04:00:31 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 02:15:32 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 03:53:55 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Rahman", "Md. Hasibur", ""], ["Rahman", "Chowdhury Rafeed", ""], ["Amin", "Ruhul", ""], ["Sifat", "Md. Habibur Rahman", ""], ["Anika", "Afra", ""]]}, {"id": "1912.00180", "submitter": "Anton Kolonin Dr.", "authors": "Anton Kolonin", "title": "Latent Semantic Search and Information Extraction Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation, concept, design and implementation of latent semantic search\nfor search engines have limited semantic search, entity extraction and property\nattribution features, have insufficient accuracy and response time of latent\nsearch, may impose privacy concerns and the search results are unavailable in\noffline mode for robotic search operations. The alternative suggestion involves\nautonomous search engine with adaptive storage consumption, configurable search\nscope and latent search response time with built-in options for entity\nextraction and property attribution available as open source platform for\nmobile, desktop and server solutions. The suggested architecture attempts to\nimplement artificial general intelligence (AGI) principles as long as\nautonomous behaviour constrained by limited resources is concerned, and it is\napplied for specific task of enabling Web search for artificial agents\nimplementing the AGI.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 10:32:52 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Kolonin", "Anton", ""]]}, {"id": "1912.00290", "submitter": "Yue Zhao", "authors": "Yue Zhao and Maciej K. Hryniewicki", "title": "XGBOD: Improving Supervised Outlier Detection with Unsupervised\n  Representation Learning", "comments": "Proceedings of the 2018 International Joint Conference on Neural\n  Networks (IJCNN)", "journal-ref": null, "doi": "10.1109/IJCNN.2018.8489605", "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new semi-supervised ensemble algorithm called XGBOD (Extreme Gradient\nBoosting Outlier Detection) is proposed, described and demonstrated for the\nenhanced detection of outliers from normal observations in various practical\ndatasets. The proposed framework combines the strengths of both supervised and\nunsupervised machine learning methods by creating a hybrid approach that\nexploits each of their individual performance capabilities in outlier\ndetection. XGBOD uses multiple unsupervised outlier mining algorithms to\nextract useful representations from the underlying data that augment the\npredictive capabilities of an embedded supervised classifier on an improved\nfeature space. The novel approach is shown to provide superior performance in\ncomparison to competing individual detectors, the full ensemble and two\nexisting representation learning based algorithms across seven outlier\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 00:09:10 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhao", "Yue", ""], ["Hryniewicki", "Maciej K.", ""]]}, {"id": "1912.00423", "submitter": "Cody Bumgardner", "authors": "Daniel Cotter and V. K. Cody Bumgardner", "title": "Semantic Enrichment of Streaming Healthcare Data", "comments": "11 pages, 9 figures, adapted from masters project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, the healthcare industry has made significant advances in\nthe digitization of patient information. However, a lack of interoperability\namong healthcare systems still imposes a high cost to patients, hospitals, and\ninsurers. Currently, most systems pass messages using idiosyncratic messaging\nstandards that require specialized knowledge to interpret. This increases the\ncost of systems integration and often puts more advanced uses of data out of\nreach. In this project, we demonstrate how two open standards, FHIR and RDF,\ncan be combined both to integrate data from disparate sources in real-time and\nmake that data queryable and susceptible to automated inference. To validate\nthe effectiveness of the semantic engine, we perform simulations of real-time\ndata feeds and demonstrate how they can be combined and used by client-side\napplications with no knowledge of the underlying sources.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 15:06:44 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Cotter", "Daniel", ""], ["Bumgardner", "V. K. Cody", ""]]}, {"id": "1912.00465", "submitter": "Lin Gong", "authors": "Lin Gong, Lu Lin, Weihao Song, Hongning Wang", "title": "JNET: Learning User Representations via Joint Network Embedding and\n  Topic Embedding", "comments": null, "journal-ref": null, "doi": "10.1145/3336191.3371770", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User representation learning is vital to capture diverse user preferences,\nwhile it is also challenging as user intents are latent and scattered among\ncomplex and different modalities of user-generated data, thus, not directly\nmeasurable. Inspired by the concept of user schema in social psychology, we\ntake a new perspective to perform user representation learning by constructing\na shared latent space to capture the dependency among different modalities of\nuser-generated data. Both users and topics are embedded to the same space to\nencode users' social connections and text content, to facilitate joint modeling\nof different modalities, via a probabilistic generative framework. We evaluated\nthe proposed solution on large collections of Yelp reviews and StackOverflow\ndiscussion posts, with their associated network structures. The proposed model\noutperformed several state-of-the-art topic modeling based user models with\nbetter predictive power in unseen documents, and state-of-the-art network\nembedding based user models with improved link prediction quality in unseen\nnodes. The learnt user representations are also proved to be useful in content\nrecommendation, e.g., expert finding in StackOverflow.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 18:21:42 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Gong", "Lin", ""], ["Lin", "Lu", ""], ["Song", "Weihao", ""], ["Wang", "Hongning", ""]]}, {"id": "1912.00508", "submitter": "Chang Li", "authors": "Chang Li, Haoyun Feng and Maarten de Rijke", "title": "Cascading Hybrid Bandits: Online Learning to Rank for Relevance and\n  Diversity", "comments": null, "journal-ref": null, "doi": "10.1145/3383313.3412245", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relevance ranking and result diversification are two core areas in modern\nrecommender systems. Relevance ranking aims at building a ranked list sorted in\ndecreasing order of item relevance, while result diversification focuses on\ngenerating a ranked list of items that covers a broad range of topics. In this\npaper, we study an online learning setting that aims to recommend a ranked list\nwith $K$ items that maximizes the ranking utility, i.e., a list whose items are\nrelevant and whose topics are diverse. We formulate it as the cascade hybrid\nbandits (CHB) problem. CHB assumes the cascading user behavior, where a user\nbrowses the displayed list from top to bottom, clicks the first attractive\nitem, and stops browsing the rest. We propose a hybrid contextual bandit\napproach, called CascadeHybrid, for solving this problem. CascadeHybrid models\nitem relevance and topical diversity using two independent functions and\nsimultaneously learns those functions from user click feedback. We conduct\nexperiments to evaluate CascadeHybrid on two real-world recommendation\ndatasets: MovieLens and Yahoo music datasets. Our experimental results show\nthat CascadeHybrid outperforms the baselines. In addition, we prove theoretical\nguarantees on the $n$-step performance demonstrating the soundness of\nCascadeHybrid.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 22:03:18 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 16:57:55 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 06:46:20 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Li", "Chang", ""], ["Feng", "Haoyun", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1912.00532", "submitter": "Marion Maisonobe", "authors": "Marion Maisonobe (GC UMR 8504 CNRS)", "title": "The future of urban models in the Big Data and AI era: a bibliometric\n  analysis (2000-2019)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article questions the effects on urban research dynamics of the Big Data\nand AI turn in urban management. To identify these effects, we use two\ncomplementary materials: bibliometric data and interviews. We consider two\nareas in urban research: one, covering the academic research dealing with\ntransportation systems and the other, with water systems. First, we measure the\nevolution of AI and Big Data keywords in these two areas. Second, we measure\nthe evolution of the share of publications published in computer science\njournals about urban traffic and water quality. To guide these bibliometric\nanalyses, we rely on the content of interviews conducted with academics and\nhigher education officials in Paris and Edinburgh at the beginning of 2018.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 14:47:37 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Maisonobe", "Marion", "", "GC UMR 8504 CNRS"]]}, {"id": "1912.00600", "submitter": "Wenjie Sun", "authors": "Meng Qiao, Zheng Shan, Fudong Liu, Wenjie Sun", "title": "A Fast Matrix-Completion-Based Approach for Recommendation Systems", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion is widely used in machine learning, engineering control,\nimage processing, and recommendation systems. Currently, a popular algorithm\nfor matrix completion is Singular Value Threshold (SVT). In this algorithm, the\nsingular value threshold should be set first. However, in a recommendation\nsystem, the dimension of the preference matrix keeps changing. Therefore, it is\ndifficult to directly apply SVT. In addition, what the users of a\nrecommendation system need is a sequence of personalized recommended results\nrather than the estimation of their scores. According to the above ideas, this\npaper proposes a novel approach named probability completion model~(PCM). By\nreducing the data dimension, the transitivity of the similar matrix, and\nsingular value decomposition, this approach quickly obtains a completion matrix\nwith the same probability distribution as the original matrix. The approach\ngreatly reduces the computation time based on the accuracy of the sacrifice\npart, and can quickly obtain a low-rank similarity matrix with data trend\napproximation properties. The experimental results show that PCM can quickly\ngenerate a complementary matrix with similar data trends as the original\nmatrix. The LCS score and efficiency of PCM are both higher than SVT.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 07:10:24 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 00:27:33 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Qiao", "Meng", ""], ["Shan", "Zheng", ""], ["Liu", "Fudong", ""], ["Sun", "Wenjie", ""]]}, {"id": "1912.00651", "submitter": "Malte Bonart", "authors": "Malte Bonart and Anastasiia Samokhina and Gernot Heisenberg and\n  Philipp Schaer", "title": "An Investigation of Biases in Web Search Engine Query Suggestions", "comments": "Preprint, Online Information Review, accepted for publication on\n  27-Sep-2019. This work is licensed under a Creative Commons\n  Attribution-NonCommercial 4.0 International License. Any reuse is allowed in\n  accordance with the terms outlined by the licence. For commercial purposes,\n  permission should be sought by contacting permissions@emeraldinsight.com", "journal-ref": null, "doi": "10.1108/OIR-11-2018-0341", "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Survey-based studies suggest that search engines are trusted more than social\nmedia or even traditional news, although cases of false information or\ndefamation are known. In this study, we analyze query suggestion features of\nthree search engines to see if these features introduce some bias into the\nquery and search process that might compromise this trust. We test our approach\non person-related search suggestions by querying the names of politicians from\nthe German Bundestag before the German federal election of 2017.\n  This study introduces a framework to systematically examine and automatically\nanalyze the varieties in different query suggestions for person names offered\nby major search engines. To test our framework, we collected data from the\nGoogle, Bing, and DuckDuckGo query suggestion APIs over a period of four months\nfor 629 different names of German politicians. The suggestions were clustered\nand statistically analyzed with regards to different biases, like gender,\nparty, or age and with regards to the stability of the suggestions over time.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 09:40:33 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Bonart", "Malte", ""], ["Samokhina", "Anastasiia", ""], ["Heisenberg", "Gernot", ""], ["Schaer", "Philipp", ""]]}, {"id": "1912.00667", "submitter": "Akansha Bhardwaj", "authors": "Akansha Bhardwaj, Jie Yang, Philippe Cudr\\'e-Mauroux", "title": "A Human-AI Loop Approach for Joint Keyword Discovery and Expectation\n  Estimation in Micropost Event Detection", "comments": "Accepted at AAAI, 2020", "journal-ref": "AAAI, 2020", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microblogging platforms such as Twitter are increasingly being used in event\ndetection. Existing approaches mainly use machine learning models and rely on\nevent-related keywords to collect the data for model training. These approaches\nmake strong assumptions on the distribution of the relevant micro-posts\ncontaining the keyword -- referred to as the expectation of the distribution --\nand use it as a posterior regularization parameter during model training. Such\napproaches are, however, limited as they fail to reliably estimate the\ninformativeness of a keyword and its expectation for model training. This paper\nintroduces a Human-AI loop approach to jointly discover informative keywords\nfor model training while estimating their expectation. Our approach iteratively\nleverages the crowd to estimate both keyword specific expectation and the\ndisagreement between the crowd and the model in order to discover new keywords\nthat are most beneficial for model training. These keywords and their\nexpectation not only improve the resulting performance but also make the model\ntraining process more transparent. We empirically demonstrate the merits of our\napproach, both in terms of accuracy and interpretability, on multiple\nreal-world datasets and show that our approach improves the state of the art by\n24.3%.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 10:18:27 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Bhardwaj", "Akansha", ""], ["Yang", "Jie", ""], ["Cudr\u00e9-Mauroux", "Philippe", ""]]}, {"id": "1912.00706", "submitter": "Roman Feldbauer", "authors": "Roman Feldbauer, Thomas Rattei and Arthur Flexer", "title": "scikit-hubness: Hubness Reduction and Approximate Neighbor Search", "comments": null, "journal-ref": null, "doi": "10.21105/joss.01957", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces scikit-hubness, a Python package for efficient nearest\nneighbor search in high-dimensional spaces. Hubness is an aspect of the curse\nof dimensionality, and is known to impair various learning tasks, including\nclassification, clustering, and visualization. scikit-hubness provides\nalgorithms for hubness analysis (\"Is my data affected by hubness?\"), hubness\nreduction (\"How can we improve neighbor retrieval in high dimensions?\"), and\napproximate neighbor search (\"Does it work for large data sets?\"). It is\nintegrated into the scikit-learn environment, enabling rapid adoption by\nPython-based machine learning researchers and practitioners. Users will find\nall functionality of the scikit-learn neighbors package, plus additional\nsupport for transparent hubness reduction and approximate nearest neighbor\nsearch. scikit-hubness is developed using several quality assessment tools and\nprinciples, such as PEP8 compliance, unit tests with high code coverage,\ncontinuous integration on all major platforms (Linux, MacOS, Windows), and\nadditional checks by LGTM. The source code is available at\nhttps://github.com/VarIr/scikit-hubness under the BSD 3-clause license. Install\nfrom the Python package index with $ pip install scikit-hubness.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 12:04:32 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Feldbauer", "Roman", ""], ["Rattei", "Thomas", ""], ["Flexer", "Arthur", ""]]}, {"id": "1912.00730", "submitter": "Preslav Nakov", "authors": "Preslav Nakov, Doris Hoogeveen, Llu\\'is M\\`arquez, Alessandro\n  Moschitti, Hamdy Mubarak, Timothy Baldwin, Karin Verspoor", "title": "SemEval-2017 Task 3: Community Question Answering", "comments": "community question answering, question-question similarity,\n  question-comment similarity, answer reranking, Multi-domain Question\n  Duplicate Detection, StackExchange, English, Arabic", "journal-ref": "SemEval-2017", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe SemEval-2017 Task 3 on Community Question Answering. This year,\nwe reran the four subtasks from SemEval-2016:(A) Question-Comment\nSimilarity,(B) Question-Question Similarity,(C) Question-External Comment\nSimilarity, and (D) Rerank the correct answers for a new question in Arabic,\nproviding all the data from 2015 and 2016 for training, and fresh data for\ntesting. Additionally, we added a new subtask E in order to enable\nexperimentation with Multi-domain Question Duplicate Detection in a\nlarger-scale scenario, using StackExchange subforums. A total of 23 teams\nparticipated in the task, and submitted a total of 85 runs (36 primary and 49\ncontrastive) for subtasks A-D. Unfortunately, no teams participated in subtask\nE. A variety of approaches and features were used by the participating systems\nto address the different subtasks. The best systems achieved an official score\n(MAP) of 88.43, 47.22, 15.46, and 61.16 in subtasks A, B, C, and D,\nrespectively. These scores are better than the baselines, especially for\nsubtasks A-C.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 12:57:52 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Nakov", "Preslav", ""], ["Hoogeveen", "Doris", ""], ["M\u00e0rquez", "Llu\u00eds", ""], ["Moschitti", "Alessandro", ""], ["Mubarak", "Hamdy", ""], ["Baldwin", "Timothy", ""], ["Verspoor", "Karin", ""]]}, {"id": "1912.00741", "submitter": "Preslav Nakov", "authors": "Sara Rosenthal, Noura Farra, Preslav Nakov", "title": "SemEval-2017 Task 4: Sentiment Analysis in Twitter", "comments": "sentiment analysis, Twitter, classification, quantification, ranking,\n  English, Arabic", "journal-ref": null, "doi": null, "report-no": "SemEval-2017", "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the fifth year of the Sentiment Analysis in Twitter\ntask. SemEval-2017 Task 4 continues with a rerun of the subtasks of\nSemEval-2016 Task 4, which include identifying the overall sentiment of the\ntweet, sentiment towards a topic with classification on a two-point and on a\nfive-point ordinal scale, and quantification of the distribution of sentiment\ntowards a topic across a number of tweets: again on a two-point and on a\nfive-point ordinal scale. Compared to 2016, we made two changes: (i) we\nintroduced a new language, Arabic, for all subtasks, and (ii)~we made available\ninformation from the profiles of the Twitter users who posted the target\ntweets. The task continues to be very popular, with a total of 48 teams\nparticipating this year.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 13:04:35 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Rosenthal", "Sara", ""], ["Farra", "Noura", ""], ["Nakov", "Preslav", ""]]}, {"id": "1912.00753", "submitter": "Zhiwen Tang", "authors": "Zhiwen Tang, Grace Hui Yang", "title": "Corpus-Level End-to-End Exploration for Interactive Systems", "comments": "Accepted into AAAI 2020", "journal-ref": null, "doi": "10.1609/aaai.v34i03.5635", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core interest in building Artificial Intelligence (AI) agents is to let\nthem interact with and assist humans. One example is Dynamic Search (DS), which\nmodels the process that a human works with a search engine agent to accomplish\na complex and goal-oriented task. Early DS agents using Reinforcement Learning\n(RL) have only achieved limited success for (1) their lack of direct control\nover which documents to return and (2) the difficulty to recover from wrong\nsearch trajectories. In this paper, we present a novel corpus-level end-to-end\nexploration (CE3) method to address these issues. In our method, an entire text\ncorpus is compressed into a global low-dimensional representation, which\nenables the agent to gain access to the full state and action spaces, including\nthe under-explored areas. We also propose a new form of retrieval function,\nwhose linear approximation allows end-to-end manipulation of documents.\nExperiments on the Text REtrieval Conference (TREC) Dynamic Domain (DD) Track\nshow that CE3 outperforms the state-of-the-art DS systems.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 00:38:56 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 00:30:59 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Tang", "Zhiwen", ""], ["Yang", "Grace Hui", ""]]}, {"id": "1912.00778", "submitter": "Itay Lieder", "authors": "Itay Lieder, Meirav Segal, Eran Avidan, Asaf Cohen, Tom Hope", "title": "Learning a faceted customer segmentation for discovering new business\n  opportunities at Intel", "comments": "3 pages, 4 figures, Published in proceedings of IEEE BigData 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For sales and marketing organizations within large enterprises, identifying\nand understanding new markets, customers and partners is a key challenge.\nIntel's Sales and Marketing Group (SMG) faces similar challenges while growing\nin new markets and domains and evolving its existing business. In today's\ncomplex technological and commercial landscape, there is need for intelligent\nautomation supporting a fine-grained understanding of businesses in order to\nhelp SMG sift through millions of companies across many geographies and\nlanguages and identify relevant directions. We present a system developed in\nour company that mines millions of public business web pages, and extracts a\nfaceted customer representation. We focus on two key customer aspects that are\nessential for finding relevant opportunities: industry segments (ranging from\nbroad verticals such as healthcare, to more specific fields such as 'video\nanalytics') and functional roles (e.g., 'manufacturer' or 'retail'). To address\nthe challenge of labeled data collection, we enrich our data with external\ninformation gleaned from Wikipedia, and develop a semi-supervised multi-label,\nmulti-lingual deep learning model that parses customer website texts and\nclassifies them into their respective facets. Our system scans and indexes\ncompanies as part of a large-scale knowledge graph that currently holds tens of\nmillions of connected entities with thousands being fetched, enriched and\nconnected to the graph by the hour in real time, and also supports knowledge\nand insight discovery. In experiments conducted in our company, we are able to\nsignificantly boost the performance of sales personnel in the task of\ndiscovering new customers and commercial partnership opportunities.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 15:48:26 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Lieder", "Itay", ""], ["Segal", "Meirav", ""], ["Avidan", "Eran", ""], ["Cohen", "Asaf", ""], ["Hope", "Tom", ""]]}, {"id": "1912.00898", "submitter": "Pietro Ghezzi", "authors": "Pietro Ghezzi, Peter G Bannister, Gonzalo Casino, Alessia Catalani,\n  Michel Goldman, Jessica Morley, Marie Neunez, Andreu Prados, Mariarosaria\n  Taddeo, Tania Vanzolini and Luciano Floridi", "title": "Online information of vaccines: information quality is an ethical\n  responsibility of search engines", "comments": "18 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The fact that internet companies may record our personal data and track our\nonline behavior for commercial or political purpose has emphasized aspects\nrelated to online privacy. This has also led to the development of search\nengines that promise no tracking and privacy. Search engines also have a major\nrole in spreading low-quality health information such as that of anti-vaccine\nwebsites. This study investigates the relationship between search engines'\napproach to privacy and the scientific quality of the information they return.\nWe analyzed the first 30 webpages returned searching 'vaccines autism' in\nEnglish, Spanish, Italian and French. The results show that alternative search\nengines (Duckduckgo, Ecosia, Qwant, Swisscows and Mojeek) may return more\nanti-vaccine pages (10 to 53 percent) than Google.com (zero). Some localized\nversions of Google, however, returned more anti-vaccine webpages (up to 10\npercent) than Google.com. Our study suggests that designing a search engine\nthat is privacy savvy and avoids issues with filter bubbles that can result\nfrom user tracking is necessary but insufficient; instead, mechanisms should be\ndeveloped to test search engines from the perspective of information quality\n(particularly for health-related webpages), before they can be deemed\ntrustworthy providers of public health information.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 16:24:40 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ghezzi", "Pietro", ""], ["Bannister", "Peter G", ""], ["Casino", "Gonzalo", ""], ["Catalani", "Alessia", ""], ["Goldman", "Michel", ""], ["Morley", "Jessica", ""], ["Neunez", "Marie", ""], ["Prados", "Andreu", ""], ["Taddeo", "Mariarosaria", ""], ["Vanzolini", "Tania", ""], ["Floridi", "Luciano", ""]]}, {"id": "1912.01059", "submitter": "Patrick Wieschollek", "authors": "Fabian Groh, Lukas Ruppert, Patrick Wieschollek, Hendrik P.A. Lensch", "title": "GGNN: Graph-based GPU Nearest Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate nearest neighbor (ANN) search in high dimensions is an integral\npart of several computer vision systems and gains importance in deep learning\nwith explicit memory representations. Since PQT and FAISS started to leverage\nthe massive parallelism offered by GPUs, GPU-based implementations are a\ncrucial resource for today's state-of-the-art ANN methods. While most of these\nmethods allow for faster queries, less emphasis is devoted to accelerate the\nconstruction of the underlying index structures. In this paper, we propose a\nnovel search structure based on nearest neighbor graphs and information\npropagation on graphs. Our method is designed to take advantage of GPU\narchitectures to accelerate the hierarchical building of the index structure\nand for performing the query. Empirical evaluation shows that GGNN\nsignificantly surpasses the state-of-the-art GPU- and CPU-based systems in\nterms of build-time, accuracy and search speed.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 19:46:13 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 08:15:19 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 15:49:47 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Groh", "Fabian", ""], ["Ruppert", "Lukas", ""], ["Wieschollek", "Patrick", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "1912.01070", "submitter": "Trapit Bansal", "authors": "Trapit Bansal, Pat Verga, Neha Choudhary, Andrew McCallum", "title": "Simultaneously Linking Entities and Extracting Relations from Biomedical\n  Text Without Mention-level Supervision", "comments": "Accepted in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the meaning of text often involves reasoning about entities and\ntheir relationships. This requires identifying textual mentions of entities,\nlinking them to a canonical concept, and discerning their relationships. These\ntasks are nearly always viewed as separate components within a pipeline, each\nrequiring a distinct model and training data. While relation extraction can\noften be trained with readily available weak or distant supervision, entity\nlinkers typically require expensive mention-level supervision -- which is not\navailable in many domains. Instead, we propose a model which is trained to\nsimultaneously produce entity linking and relation decisions while requiring no\nmention-level annotations. This approach avoids cascading errors that arise\nfrom pipelined methods and more accurately predicts entity relationships from\ntext. We show that our model outperforms a state-of-the art entity linking and\nrelation extraction pipeline on two biomedical datasets and can drastically\nimprove the overall recall of the system.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 20:37:18 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Bansal", "Trapit", ""], ["Verga", "Pat", ""], ["Choudhary", "Neha", ""], ["McCallum", "Andrew", ""]]}, {"id": "1912.01079", "submitter": "Sven Buechel", "authors": "Jo\\~ao Sedoc, Sven Buechel, Yehonathan Nachmany, Anneke Buffone, and\n  Lyle Ungar", "title": "Learning Word Ratings for Empathy and Distress from Document-Level User\n  Responses", "comments": "LREC 2020 camera-ready copy", "journal-ref": "Proceedings of The 12th Language Resources and Evaluation\n  Conference (LREC 2020). Pages 1657-1666", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the excellent performance of black box approaches to modeling\nsentiment and emotion, lexica (sets of informative words and associated\nweights) that characterize different emotions are indispensable to the NLP\ncommunity because they allow for interpretable and robust predictions. Emotion\nanalysis of text is increasing in popularity in NLP; however, manually creating\nlexica for psychological constructs such as empathy has proven difficult. This\npaper automatically creates empathy word ratings from document-level ratings.\nThe underlying problem of learning word ratings from higher-level supervision\nhas to date only been addressed in an ad hoc fashion and has not used deep\nlearning methods. We systematically compare a number of approaches to learning\nword ratings from higher-level supervision against a Mixed-Level Feed Forward\nNetwork (MLFFN), which we find performs best, and use the MLFFN to create the\nfirst-ever empathy lexicon. We then use Signed Spectral Clustering to gain\ninsights into the resulting words. The empathy and distress lexica are publicly\navailable at: http://www.wwbp.org/lexica.html.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 21:19:22 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 09:47:27 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Sedoc", "Jo\u00e3o", ""], ["Buechel", "Sven", ""], ["Nachmany", "Yehonathan", ""], ["Buffone", "Anneke", ""], ["Ungar", "Lyle", ""]]}, {"id": "1912.01111", "submitter": "Jayanta Mandi", "authors": "Dipankar Chakrabarti, Neelam Patodia, Udayan Bhattacharya, Indranil\n  Mitra, Satyaki Roy, Jayanta Mandi, Nandini Roy, Prasun Nandy", "title": "Use of Artificial Intelligence to Analyse Risk in Legal Documents for a\n  Better Decision Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Assessing risk for voluminous legal documents such as request for proposal;\ncontracts is tedious and error prone. We have developed \"risk-o-meter\", a\nframework, based on machine learning and natural language processing to review\nand assess risks of any legal document. Our framework uses Paragraph Vector, an\nunsupervised model to generate vector representation of text. This enables the\nframework to learn contextual relations of legal terms and generate sensible\ncontext aware embedding. The framework then feeds the vector space into a\nsupervised classification algorithm to predict whether a paragraph belongs to a\nper-defined risk category or not. The framework thus extracts risk prone\nparagraphs. This technique efficiently overcomes the limitations of\nkeyword-based search. We have achieved an accuracy of 91% for the risk category\nhaving the largest training dataset. This framework will help organizations\noptimize effort to identify risk from large document base with minimal human\nintervention and thus will help to have risk mitigated sustainable growth. Its\nmachine learning capability makes it scalable to uncover relevant information\nfrom any type of document apart from legal documents, provided the library is\nper-populated and rich.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 16:07:02 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Chakrabarti", "Dipankar", ""], ["Patodia", "Neelam", ""], ["Bhattacharya", "Udayan", ""], ["Mitra", "Indranil", ""], ["Roy", "Satyaki", ""], ["Mandi", "Jayanta", ""], ["Roy", "Nandini", ""], ["Nandy", "Prasun", ""]]}, {"id": "1912.01113", "submitter": "Preslav Nakov", "authors": "Preslav Nakov", "title": "Using the Web as an Implicit Training Set: Application to Noun Compound\n  Syntax and Semantics", "comments": "noun compounds, paraphrasing verbs, semantic interpretation, syntax,\n  multi-word expressions, MWEs, noun compound interpretation, noun compound\n  bracketing, prepositional phrase attachment, noun phrase coordination,\n  machine translation", "journal-ref": "PhD Thesis, University of California at Berkeley, 2007", "doi": null, "report-no": "Technical Report No. UCB/EECS-2007-173", "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important characteristic of English written text is the abundance of noun\ncompounds - sequences of nouns acting as a single noun, e.g., colon cancer\ntumor suppressor protein. While eventually mastered by domain experts, their\ninterpretation poses a major challenge for automated analysis. Understanding\nnoun compounds' syntax and semantics is important for many natural language\napplications, including question answering, machine translation, information\nretrieval, and information extraction. I address the problem of noun compounds\nsyntax by means of novel, highly accurate unsupervised and lightly supervised\nalgorithms using the Web as a corpus and search engines as interfaces to that\ncorpus. Traditionally the Web has been viewed as a source of page hit counts,\nused as an estimate for n-gram word frequencies. I extend this approach by\nintroducing novel surface features and paraphrases, which yield\nstate-of-the-art results for the task of noun compound bracketing. I also show\nhow these kinds of features can be applied to other structural ambiguity\nproblems, like prepositional phrase attachment and noun phrase coordination. I\naddress noun compound semantics by automatically generating paraphrasing verbs\nand prepositions that make explicit the hidden semantic relations between the\nnouns in a noun compound. I also demonstrate how these paraphrasing verbs can\nbe used to solve various relational similarity problems, and how paraphrasing\nnoun compounds can improve machine translation.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 21:33:31 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Nakov", "Preslav", ""]]}, {"id": "1912.01385", "submitter": "Sebastian Hofst\\\"atter", "authors": "Sebastian Hofst\\\"atter, Markus Zlabinger, Allan Hanbury", "title": "TU Wien @ TREC Deep Learning '19 -- Simple Contextualization for\n  Re-ranking", "comments": "Presented at TREC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of neural network models puts multiple objectives in conflict with\neach other: Ideally we would like to create a neural model that is effective,\nefficient, and interpretable at the same time. However, in most instances we\nhave to choose which property is most important to us. We used the opportunity\nof the TREC 2019 Deep Learning track to evaluate the effectiveness of a\nbalanced neural re-ranking approach. We submitted results of the TK\n(Transformer-Kernel) model: a neural re-ranking model for ad-hoc search using\nan efficient contextualization mechanism. TK employs a very small number of\nlightweight Transformer layers to contextualize query and document word\nembeddings. To score individual term interactions, we use a document-length\nenhanced kernel-pooling, which enables users to gain insight into the model.\nOur best result for the passage ranking task is: 0.420 MAP, 0.671 nDCG, 0.598\nP@10 (TUW19-p3 full). Our best result for the document ranking task is: 0.271\nMAP, 0.465 nDCG, 0.730 P@10 (TUW19-d3 re-ranking).\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 14:19:20 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Hofst\u00e4tter", "Sebastian", ""], ["Zlabinger", "Markus", ""], ["Hanbury", "Allan", ""]]}, {"id": "1912.01592", "submitter": "Sahan Bulathwela", "authors": "Sahan Bulathwela, Maria Perez-Ortiz, Emine Yilmaz and John\n  Shawe-Taylor", "title": "Towards an Integrative Educational Recommender for Lifelong Learners", "comments": "In Proceedings of AAAI Conference on Artificial Intelligence 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most ambitious use cases of computer-assisted learning is to build\na recommendation system for lifelong learning. Most recommender algorithms\nexploit similarities between content and users, overseeing the necessity to\nleverage sensible learning trajectories for the learner. Lifelong learning thus\npresents unique challenges, requiring scalable and transparent models that can\naccount for learner knowledge and content novelty simultaneously, while also\nretaining accurate learners representations for long periods of time. We\nattempt to build a novel educational recommender, that relies on an integrative\napproach combining multiple drivers of learners engagement. Our first step\ntowards this goal is TrueLearn, which models content novelty and background\nknowledge of learners and achieves promising performance while retaining a\nhuman interpretable learner model.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 18:40:44 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Bulathwela", "Sahan", ""], ["Perez-Ortiz", "Maria", ""], ["Yilmaz", "Emine", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1912.01799", "submitter": "Mengting Wan", "authors": "Mengting Wan, Jianmo Ni, Rishabh Misra, Julian McAuley", "title": "Addressing Marketing Bias in Product Recommendations", "comments": "9 pages; WSDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern collaborative filtering algorithms seek to provide personalized\nproduct recommendations by uncovering patterns in consumer-product\ninteractions. However, these interactions can be biased by how the product is\nmarketed, for example due to the selection of a particular human model in a\nproduct image. These correlations may result in the underrepresentation of\nparticular niche markets in the interaction data; for example, a female user\nwho would potentially like motorcycle products may be less likely to interact\nwith them if they are promoted using stereotypically 'male' images.\n  In this paper, we first investigate this correlation between users'\ninteraction feedback and products' marketing images on two real-world\ne-commerce datasets. We further examine the response of several standard\ncollaborative filtering algorithms to the distribution of consumer-product\nmarket segments in the input interaction data, revealing that marketing\nstrategy can be a source of bias for modern recommender systems. In order to\nprotect recommendation performance on underrepresented market segments, we\ndevelop a framework to address this potential marketing bias. Quantitative\nresults demonstrate that the proposed approach significantly improves the\nrecommendation fairness across different market segments, with a negligible\nloss (or better) recommendation accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 05:05:53 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Wan", "Mengting", ""], ["Ni", "Jianmo", ""], ["Misra", "Rishabh", ""], ["McAuley", "Julian", ""]]}, {"id": "1912.01831", "submitter": "Preslav Nakov", "authors": "Evgeni Stefchov, Galia Angelova, Preslav Nakov", "title": "Towards Constructing a Corpus for Studying the Effects of Treatments and\n  Substances Reported in PubMed Abstracts", "comments": "medical relation extraction, rationale extraction, effects and\n  treatments, bioNLP", "journal-ref": "AIMSA-2016: The 17th International Conference on Artificial\n  Intelligence: Methodology, Systems, Applications", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the construction of an annotated corpus of PubMed abstracts\nreporting about positive, negative or neutral effects of treatments or\nsubstances. Our ultimate goal is to annotate one sentence (rationale) for each\nabstract and to use this resource as a training set for text classification of\neffects discussed in PubMed abstracts. Currently, the corpus consists of 750\nabstracts. We describe the automatic processing that supports the corpus\nconstruction, the manual annotation activities and some features of the medical\nlanguage in the abstracts selected for the annotated corpus. It turns out that\nrecognizing the terminology and the abbreviations is key for determining the\nrationale sentence. The corpus will be applied to improve our classifier, which\ncurrently has accuracy of 78.80% achieved with normalization of the abstract\nterms based on UMLS concepts from specific semantic groups and an SVM with a\nlinear kernel. Finally, we discuss some other possible applications of this\ncorpus.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 07:22:32 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Stefchov", "Evgeni", ""], ["Angelova", "Galia", ""], ["Nakov", "Preslav", ""]]}, {"id": "1912.01901", "submitter": "Jibril Frej", "authors": "Jibril Frej and Didier Schwab and Jean-Pierre Chevallet", "title": "WIKIR: A Python toolkit for building a large-scale Wikipedia-based\n  English Information Retrieval Dataset", "comments": "Accepted at LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Over the past years, deep learning methods allowed for new state-of-the-art\nresults in ad-hoc information retrieval. However such methods usually require\nlarge amounts of annotated data to be effective. Since most standard ad-hoc\ninformation retrieval datasets publicly available for academic research (e.g.\nRobust04, ClueWeb09) have at most 250 annotated queries, the recent deep\nlearning models for information retrieval perform poorly on these datasets.\nThese models (e.g. DUET, Conv-KNRM) are trained and evaluated on data collected\nfrom commercial search engines not publicly available for academic research\nwhich is a problem for reproducibility and the advancement of research. In this\npaper, we propose WIKIR: an open-source toolkit to automatically build\nlarge-scale English information retrieval datasets based on Wikipedia. WIKIR is\npublicly available on GitHub. We also provide wikIR78k and wikIRS78k: two\nlarge-scale publicly available datasets that both contain 78,628 queries and\n3,060,191 (query, relevant documents) pairs.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 11:25:47 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 10:27:00 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 16:11:14 GMT"}, {"version": "v4", "created": "Tue, 17 Mar 2020 09:25:34 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Frej", "Jibril", ""], ["Schwab", "Didier", ""], ["Chevallet", "Jean-Pierre", ""]]}, {"id": "1912.01972", "submitter": "Preslav Nakov", "authors": "Preslav Nakov, Llu\\'is M\\`arquez, Alessandro Moschitti, Walid Magdy,\n  Hamdy Mubarak, Abed Alhakim Freihat, James Glass, Bilal Randeree", "title": "SemEval-2016 Task 3: Community Question Answering", "comments": "community question answering, question-question similarity,\n  question-comment similarity, answer reranking, English, Arabic. arXiv admin\n  note: substantial text overlap with arXiv:1912.00730", "journal-ref": "SemEval-2016", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the SemEval--2016 Task 3 on Community Question\nAnswering, which we offered in English and Arabic. For English, we had three\nsubtasks: Question--Comment Similarity (subtask A), Question--Question\nSimilarity (B), and Question--External Comment Similarity (C). For Arabic, we\nhad another subtask: Rerank the correct answers for a new question (D).\nEighteen teams participated in the task, submitting a total of 95 runs (38\nprimary and 57 contrastive) for the four subtasks. A variety of approaches and\nfeatures were used by the participating systems to address the different\nsubtasks, which are summarized in this paper. The best systems achieved an\nofficial score (MAP) of 79.19, 76.70, 55.41, and 45.83 in subtasks A, B, C, and\nD, respectively. These scores are significantly better than those for the\nbaselines that we provided. For subtask A, the best system improved over the\n2015 winner by 3 points absolute in terms of Accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 06:30:34 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Nakov", "Preslav", ""], ["M\u00e0rquez", "Llu\u00eds", ""], ["Moschitti", "Alessandro", ""], ["Magdy", "Walid", ""], ["Mubarak", "Hamdy", ""], ["Freihat", "Abed Alhakim", ""], ["Glass", "James", ""], ["Randeree", "Bilal", ""]]}, {"id": "1912.01973", "submitter": "Preslav Nakov", "authors": "Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio Sebastiani,\n  Veselin Stoyanov", "title": "SemEval-2016 Task 4: Sentiment Analysis in Twitter", "comments": "Sentiment analysis, sentiment towards a topic, quantification,\n  microblog sentiment analysis; Twitter opinion mining. arXiv admin note: text\n  overlap with arXiv:1912.00741", "journal-ref": "SemEval-2016", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the fourth year of the ``Sentiment Analysis in Twitter\nTask''. SemEval-2016 Task 4 comprises five subtasks, three of which represent a\nsignificant departure from previous editions. The first two subtasks are reruns\nfrom prior years and ask to predict the overall sentiment, and the sentiment\ntowards a topic in a tweet. The three new subtasks focus on two variants of the\nbasic ``sentiment classification in Twitter'' task. The first variant adopts a\nfive-point scale, which confers an ordinal character to the classification\ntask. The second variant focuses on the correct estimation of the prevalence of\neach class of interest, a task which has been called quantification in the\nsupervised learning literature. The task continues to be very popular,\nattracting a total of 43 teams.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 06:46:20 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Nakov", "Preslav", ""], ["Ritter", "Alan", ""], ["Rosenthal", "Sara", ""], ["Sebastiani", "Fabrizio", ""], ["Stoyanov", "Veselin", ""]]}, {"id": "1912.02077", "submitter": "Rezarta Islamaj", "authors": "Rezarta Islamaj, Lana Yeganova, Won Kim, Natalie Xie, W. John Wilbur,\n  Zhiyong Lu", "title": "PDC -- a probabilistic distributional clustering algorithm: a case study\n  on suicide articles in PubMed", "comments": "AMIA Informatics Summit 2020, 18 pages, Algorithm in the Appendix, 3\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to organize a large collection in a manner that facilitates human\ncomprehension is crucial given the ever-increasing volumes of information. In\nthis work, we present PDC (probabilistic distributional clustering), a novel\nalgorithm that, given a document collection, computes disjoint term sets\nrepresenting topics in the collection. The algorithm relies on probabilities of\nword co-occurrences to partition the set of terms appearing in the collection\nof documents into disjoint groups of related terms. In this work, we also\npresent an environment to visualize the computed topics in the term space and\nretrieve the most related PubMed articles for each group of terms. We\nillustrate the algorithm by applying it to PubMed documents on the topic of\nsuicide. Suicide is a major public health problem identified as the tenth\nleading cause of death in the US. In this application, our goal is to provide a\nglobal view of the mental health literature pertaining to the subject of\nsuicide, and through this, to help create a rich environment of multifaceted\ndata to guide health care researchers in their endeavor to better understand\nthe breadth, depth and scope of the problem. We demonstrate the usefulness of\nthe proposed algorithm by providing a web portal that allows mental health\nresearchers to peruse the suicide-related literature in PubMed.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 16:07:25 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Islamaj", "Rezarta", ""], ["Yeganova", "Lana", ""], ["Kim", "Won", ""], ["Xie", "Natalie", ""], ["Wilbur", "W. John", ""], ["Lu", "Zhiyong", ""]]}, {"id": "1912.02263", "submitter": "Steffen Rendle", "authors": "Steffen Rendle", "title": "Evaluation Metrics for Item Recommendation under Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of item recommendation requires ranking a large catalogue of items\ngiven a context. Item recommendation algorithms are evaluated using ranking\nmetrics that depend on the positions of relevant items. To speed up the\ncomputation of metrics, recent work often uses sampled metrics where only a\nsmaller set of random items and the relevant items are ranked. This paper\ninvestigates sampled metrics in more detail and shows that sampled metrics are\ninconsistent with their exact version. Sampled metrics do not persist relative\nstatements, e.g., 'algorithm A is better than B', not even in expectation.\nMoreover the smaller the sampling size, the less difference between metrics,\nand for very small sampling size, all metrics collapse to the AUC metric.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 21:48:42 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Rendle", "Steffen", ""]]}, {"id": "1912.02346", "submitter": "Grace Hui Yang", "authors": "Grace Hui Yang", "title": "Information Retrieval and Its Sister Disciplines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a summary graph to show the relationships between\nInformation Retrieval (IR) and other related disciplines. The figure tells the\nkey differences between them and the conditions under which one would\ntransition into another.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 02:03:30 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Yang", "Grace Hui", ""]]}, {"id": "1912.02387", "submitter": "Preslav Nakov", "authors": "Sara Rosenthal, Saif M Mohammad, Preslav Nakov, Alan Ritter, Svetlana\n  Kiritchenko, Veselin Stoyanov", "title": "SemEval-2015 Task 10: Sentiment Analysis in Twitter", "comments": "Sentiment analysis, sentiment towards a topic, quantification,\n  microblog sentiment analysis; Twitter opinion mining", "journal-ref": "SemEval-2015", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe the 2015 iteration of the SemEval shared task on\nSentiment Analysis in Twitter. This was the most popular sentiment analysis\nshared task to date with more than 40 teams participating in each of the last\nthree years. This year's shared task competition consisted of five sentiment\nprediction subtasks. Two were reruns from previous years: (A) sentiment\nexpressed by a phrase in the context of a tweet, and (B) overall sentiment of a\ntweet. We further included three new subtasks asking to predict (C) the\nsentiment towards a topic in a single tweet, (D) the overall sentiment towards\na topic in a set of tweets, and (E) the degree of prior polarity of a phrase.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 05:08:36 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Rosenthal", "Sara", ""], ["Mohammad", "Saif M", ""], ["Nakov", "Preslav", ""], ["Ritter", "Alan", ""], ["Kiritchenko", "Svetlana", ""], ["Stoyanov", "Veselin", ""]]}, {"id": "1912.02484", "submitter": "Oana Balalau", "authors": "Oana Balalau, Carlos Castillo, Mauro Sozio", "title": "EviDense: a Graph-based Method for Finding Unique High-impact Events\n  with Succinct Keyword-based Descriptions", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite the significant efforts made by the research community in recent\nyears, automatically acquiring valuable information about high impact-events\nfrom social media remains challenging. We present EviDense, a graph-based\napproach for finding high-impact events (such as disaster events) in social\nmedia. One of the challenges we address in our work is to provide for each\nevent a succinct keyword-based description, containing the most relevant\ninformation about it, such as what happened, the location, as well as its\ntimeframe. We evaluate our approach on a large collection of tweets posted over\na period of 19 months, using a crowdsourcing platform. Our evaluation shows\nthat our method outperforms state-of-the-art approaches for the same problem,\nin terms of having higher precision, lower number of duplicates, and presenting\na keyword-based description that is succinct and informative. We further\nimprove the results of our algorithm by incorporating news from mainstream\nmedia. A preliminary version of this work was presented as a 4-pages short\npaper at ICWSM 2018.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 10:33:41 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Balalau", "Oana", ""], ["Castillo", "Carlos", ""], ["Sozio", "Mauro", ""]]}, {"id": "1912.02852", "submitter": "Felipe Gonz\\'alez", "authors": "Felipe Gonz\\'alez, Andrea Figueroa, Claudia L\\'opez, Cecilia Arag\\'on", "title": "Information Privacy Opinions on Twitter: A Cross-Language Study", "comments": "Proceeding CSCW '19: Conference Companion Publication of the 2019 on\n  Computer Supported Cooperative Work and Social Computing", "journal-ref": null, "doi": "10.1145/3311957.3359501", "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cambridge Analytica scandal triggered a conversation on Twitter about\ndata practices and their implications. Our research proposes to leverage this\nconversation to extend the understanding of how information privacy is framed\nby users worldwide. We collected tweets about the scandal written in Spanish\nand English between April and July 2018. We created a word embedding to create\na reduced multi-dimensional representation of the tweets in each language. For\neach embedding, we conducted open coding to characterize the semantic contexts\nof key concepts: \"information\", \"privacy\", \"company\" and \"users\" (and their\nSpanish translations). Through a comparative analysis, we found a broader\nemphasis on privacy-related words associated with companies in English. We also\nidentified more terms related to data collection in English and fewer\nassociated with security mechanisms, control, and risks. Our findings hint at\nthe potential of cross-language comparisons of text to extend the understanding\nof worldwide differences in information privacy perspectives.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 19:45:30 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Gonz\u00e1lez", "Felipe", ""], ["Figueroa", "Andrea", ""], ["L\u00f3pez", "Claudia", ""], ["Arag\u00f3n", "Cecilia", ""]]}, {"id": "1912.02866", "submitter": "Tuomo Hiippala", "authors": "Tuomo Hiippala", "title": "Classifying Diagrams and Their Parts using Graph Neural Networks: A\n  Comparison of Crowd-Sourced and Expert Annotations", "comments": "9 pages; submitted to LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article compares two multimodal resources that consist of diagrams which\ndescribe topics in elementary school natural sciences. Both resources contain\nthe same diagrams and represent their structure using graphs, but differ in\nterms of their annotation schema and how the annotations have been created -\ndepending on the resource in question - either by crowd-sourced workers or\ntrained experts. This article reports on two experiments that evaluate how\neffectively crowd-sourced and expert-annotated graphs can represent the\nmultimodal structure of diagrams for representation learning using various\ngraph neural networks. The results show that the identity of diagram elements\ncan be learned from their layout features, while the expert annotations provide\nbetter representations of diagram types.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 20:34:53 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Hiippala", "Tuomo", ""]]}, {"id": "1912.02962", "submitter": "Shuqi Xu", "authors": "Shuqi Xu, Qianming Zhang, Linyuan Lv, Manuel Sebastian Mariani", "title": "Recommending investors for new startups by integrating network diffusion\n  and investors' domain preference", "comments": null, "journal-ref": "Information Sciences 515C (2020) 103-115", "doi": "10.1016/j.ins.2019.11.045", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, many startups have sprung up, which create a huge\ndemand for financial support from venture investors. However, due to the\ninformation asymmetry between investors and companies, the financing process is\nusually challenging and time-consuming, especially for the startups that have\nnot yet obtained any investment. Because of this, effective data-driven\ntechniques to automatically match startups with potentially relevant investors\nwould be highly desirable. Here, we analyze 34,469 valid investment events\ncollected from www.itjuzi.com and consider the cold-start problem of\nrecommending investors for new startups. We address this problem by\nconstructing different tripartite network representations of the data where\nnodes represent investors, companies, and companies' domains. First, we find\nthat investors have strong domain preferences when investing, which motivates\nus to introduce virtual links between investors and investment domains in the\ntripartite network construction. Our analysis of the recommendation performance\nof diffusion-based algorithms applied to various network representations\nindicates that prospective investors for new startups are effectively revealed\nby integrating network diffusion processes with investors' domain preference.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 03:18:24 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 11:47:08 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Xu", "Shuqi", ""], ["Zhang", "Qianming", ""], ["Lv", "Linyuan", ""], ["Mariani", "Manuel Sebastian", ""]]}, {"id": "1912.02990", "submitter": "Preslav Nakov", "authors": "Sara Rosenthal, Preslav Nakov, Alan Ritter, Veselin Stoyanov", "title": "SemEval-2014 Task 9: Sentiment Analysis in Twitter", "comments": "Sentiment analysis, microblog sentiment analysis, Twitter opinion\n  mining, sarcasm, LiveJournal, SMS", "journal-ref": "SemEval-2014", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the Sentiment Analysis in Twitter task, ran as part of\nSemEval-2014. It is a continuation of the last year's task that ran\nsuccessfully as part of SemEval-2013. As in 2013, this was the most popular\nSemEval task; a total of 46 teams contributed 27 submissions for subtask A (21\nteams) and 50 submissions for subtask B (44 teams). This year, we introduced\nthree new test sets: (i) regular tweets, (ii) sarcastic tweets, and (iii)\nLiveJournal sentences. We further tested on (iv) 2013 tweets, and (v) 2013 SMS\nmessages. The highest F1-score on (i) was achieved by NRC-Canada at 86.63 for\nsubtask A and by TeamX at 70.96 for subtask B.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 06:23:19 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Rosenthal", "Sara", ""], ["Nakov", "Preslav", ""], ["Ritter", "Alan", ""], ["Stoyanov", "Veselin", ""]]}, {"id": "1912.02998", "submitter": "Preslav Nakov", "authors": "Francisco Guzm\\'an, Llu\\'is M\\`arquez, Preslav Nakov", "title": "Machine Translation Evaluation Meets Community Question Answering", "comments": "community question answering, machine translation evaluation,\n  pairwise ranking, learning to rank", "journal-ref": "Annual meeting of the Association for Computational Linguistics\n  (ACL-2016)", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the applicability of machine translation evaluation (MTE) methods\nto a very different problem: answer ranking in community Question Answering. In\nparticular, we adopt a pairwise neural network (NN) architecture, which\nincorporates MTE features, as well as rich syntactic and semantic embeddings,\nand which efficiently models complex non-linear interactions. The evaluation\nresults show state-of-the-art performance, with sizeable contribution from both\nthe MTE features and from the pairwise NN architecture.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 06:35:21 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Guzm\u00e1n", "Francisco", ""], ["M\u00e0rquez", "Llu\u00eds", ""], ["Nakov", "Preslav", ""]]}, {"id": "1912.03041", "submitter": "Wenya Wang", "authors": "Wenya Wang and Sinno Jialin Pan", "title": "Integrating Deep Learning with Logic Fusion for Information Extraction", "comments": "Accepted in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information extraction (IE) aims to produce structured information from an\ninput text, e.g., Named Entity Recognition and Relation Extraction. Various\nattempts have been proposed for IE via feature engineering or deep learning.\nHowever, most of them fail to associate the complex relationships inherent in\nthe task itself, which has proven to be especially crucial. For example, the\nrelation between 2 entities is highly dependent on their entity types. These\ndependencies can be regarded as complex constraints that can be efficiently\nexpressed as logical rules. To combine such logic reasoning capabilities with\nlearning capabilities of deep neural networks, we propose to integrate logical\nknowledge in the form of first-order logic into a deep learning system, which\ncan be trained jointly in an end-to-end manner. The integrated framework is\nable to enhance neural outputs with knowledge regularization via logic rules,\nand at the same time update the weights of logic rules to comply with the\ncharacteristics of the training data. We demonstrate the effectiveness and\ngeneralization of the proposed model on multiple IE tasks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 09:38:23 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Wang", "Wenya", ""], ["Pan", "Sinno Jialin", ""]]}, {"id": "1912.03048", "submitter": "Adrien Guille", "authors": "Jean Dupuy and Adrien Guille and Julien Jacques", "title": "Document Network Embedding: Coping for Missing Content and Missing Links", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Searching through networks of documents is an important task. A promising\npath to improve the performance of information retrieval systems in this\ncontext is to leverage dense node and content representations learned with\nembedding techniques. However, these techniques cannot learn representations\nfor documents that are either isolated or whose content is missing. To tackle\nthis issue, assuming that the topology of the network and the content of the\ndocuments correlate, we propose to estimate the missing node representations\nfrom the available content representations, and conversely. Inspired by recent\nadvances in machine translation, we detail in this paper how to learn a linear\ntransformation from a set of aligned content and node representations. The\nprojection matrix is efficiently calculated in terms of the singular value\ndecomposition. The usefulness of the proposed method is highlighted by the\nimproved ability to predict the neighborhood of nodes whose links are\nunobserved based on the projected content representations, and to retrieve\nsimilar documents when content is missing, based on the projected node\nrepresentations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 10:09:20 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Dupuy", "Jean", ""], ["Guille", "Adrien", ""], ["Jacques", "Julien", ""]]}, {"id": "1912.03135", "submitter": "Preslav Nakov", "authors": "Francisco Guzman, Shafiq Joty, Lluis Marquez, Preslav Nakov", "title": "Pairwise Neural Machine Translation Evaluation", "comments": "machine translation evaluation, machine translation, pairwise\n  ranking, learning to rank. arXiv admin note: substantial text overlap with\n  arXiv:1710.02095", "journal-ref": "Conference of the Association for Computational Linguistics\n  (ACL'2015)", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for machine translation evaluation using neural\nnetworks in a pairwise setting, where the goal is to select the better\ntranslation from a pair of hypotheses, given the reference translation. In this\nframework, lexical, syntactic and semantic information from the reference and\nthe two hypotheses is compacted into relatively small distributed vector\nrepresentations, and fed into a multi-layer neural network that models the\ninteraction between each of the hypotheses and the reference, as well as\nbetween the two hypotheses. These compact representations are in turn based on\nword and sentence embeddings, which are learned using neural networks. The\nframework is flexible, allows for efficient learning and classification, and\nyields correlation with humans that rivals the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 05:17:05 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Guzman", "Francisco", ""], ["Joty", "Shafiq", ""], ["Marquez", "Lluis", ""], ["Nakov", "Preslav", ""]]}, {"id": "1912.03184", "submitter": "Roman Klinger", "authors": "Laura Bostan and Evgeny Kim and Roman Klinger", "title": "GoodNewsEveryone: A Corpus of News Headlines Annotated with Emotions,\n  Semantic Roles, and Reader Perception", "comments": "Accepted at LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most research on emotion analysis from text focuses on the task of emotion\nclassification or emotion intensity regression. Fewer works address emotions as\na phenomenon to be tackled with structured learning, which can be explained by\nthe lack of relevant datasets. We fill this gap by releasing a dataset of 5000\nEnglish news headlines annotated via crowdsourcing with their associated\nemotions, the corresponding emotion experiencers and textual cues, related\nemotion causes and targets, as well as the reader's perception of the emotion\nof the headline. This annotation task is comparably challenging, given the\nlarge number of classes and roles to be identified. We therefore propose a\nmultiphase annotation procedure in which we first find relevant instances with\nemotional content and then annotate the more fine-grained aspects. Finally, we\ndevelop a baseline for the task of automatic prediction of semantic role\nstructures and discuss the results. The corpus we release enables further\nresearch on emotion classification, emotion intensity prediction, emotion cause\ndetection, and supports further qualitative studies.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 15:30:58 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 10:02:19 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 13:32:42 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Bostan", "Laura", ""], ["Kim", "Evgeny", ""], ["Klinger", "Roman", ""]]}, {"id": "1912.03216", "submitter": "Daouda Diouf Dr", "authors": "Daouda Diouf and Djibril Seck", "title": "Modeling the Chlorophyll-a from Sea Surface Reflectance in West Africa\n  by Deep Learning Methods: A Comparison of Multiple Algorithms", "comments": "8 pages, 4 figures, 1 table", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA) Vol.10, No.6, November 2019", "doi": "10.5121/ijaia.2019.10603", "report-no": null, "categories": "cs.NE cs.IR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning provide successful applications in many fields. Recently,\nmachines learning are involved for oceans remote sensing applications. In this\nstudy, we use and compare about eight (8) deep learning estimators for\nretrieval of a mainly pigment of phytoplankton. Depending on the water case and\nthe multiple instruments simultaneouslyobserving the earth on a variety of\nplatforms, several algorithm are used to estimate the chlolophyll-a from marine\nreflectance. By using a long-term multi-sensor time-series of satellite\nocean-colour data, as MODIS, SeaWifs, VIIRS, MERIS, etc, we make a unique deep\nnetwork model able to establish a relationship between sea surface reflectance\nand chlorophyll-a from any measurement satellite sensor over West Africa. These\ndata fusion take into account the bias between case water and instruments. We\nconstruct several chlorophyll-a concentration prediction deep learning based\nmodels, compare them and therefore use the best for our study. Results obtained\nfor accuracy training and test are quite good. The mean absolute error are very\nlow and vary between 0,07 to 0,13 mg/m3.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 16:36:18 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Diouf", "Daouda", ""], ["Seck", "Djibril", ""]]}, {"id": "1912.03585", "submitter": "Souvick Ghosh", "authors": "Souvick Ghosh and Satanu Ghosh", "title": "Exploring the Ideal Depth of Neural Network when Predicting Question\n  Deletion on Community Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Community Question Answering (CQA) has emerged as a popular\nplatform for knowledge curation and archival. An interesting aspect of question\nanswering is that it combines aspects from natural language processing,\ninformation retrieval, and machine learning. In this paper, we have explored\nhow the depth of the neural network influences the accuracy of prediction of\ndeleted questions in question-answering forums. We have used different shallow\nand deep models for prediction and analyzed the relationships between number of\nhidden layers, accuracy, and computational time. The results suggest that while\ndeep networks perform better than shallow networks in modeling complex\nnon-linear functions, increasing the depth may not always produce desired\nresults. We observe that the performance of the deep neural network suffers\nsignificantly due to vanishing gradients when large number of hidden layers are\npresent. Constantly increasing the depth of the model increases accuracy\ninitially, after which the accuracy plateaus, and finally drops. Adding each\nlayer is also expensive in terms of the time required to train the model. This\nresearch is situated in the domain of neural information retrieval and\ncontributes towards building a theory on how deep neural networks can be\nefficiently and accurately used for predicting question deletion. We predict\ndeleted questions with more than 90\\% accuracy using two to ten hidden layers,\nwith less accurate results for shallower and deeper architectures.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 01:06:16 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Ghosh", "Souvick", ""], ["Ghosh", "Satanu", ""]]}, {"id": "1912.03590", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Houwen Peng, Jianlong Fu, Jiebo Luo", "title": "Learning 2D Temporal Adjacent Networks for Moment Localization with\n  Natural Language", "comments": "This paper is accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of retrieving a specific moment from an untrimmed\nvideo by a query sentence. This is a challenging problem because a target\nmoment may take place in relations to other temporal moments in the untrimmed\nvideo. Existing methods cannot tackle this challenge well since they consider\ntemporal moments individually and neglect the temporal dependencies. In this\npaper, we model the temporal relations between video moments by a\ntwo-dimensional map, where one dimension indicates the starting time of a\nmoment and the other indicates the end time. This 2D temporal map can cover\ndiverse video moments with different lengths, while representing their adjacent\nrelations. Based on the 2D map, we propose a Temporal Adjacent Network\n(2D-TAN), a single-shot framework for moment localization. It is capable of\nencoding the adjacent temporal relation, while learning discriminative features\nfor matching video moments with referring expressions. We evaluate the proposed\n2D-TAN on three challenging benchmarks, i.e., Charades-STA, ActivityNet\nCaptions, and TACoS, where our 2D-TAN outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 01:34:39 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 13:44:56 GMT"}, {"version": "v3", "created": "Sat, 26 Dec 2020 15:27:54 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhang", "Songyang", ""], ["Peng", "Houwen", ""], ["Fu", "Jianlong", ""], ["Luo", "Jiebo", ""]]}, {"id": "1912.03720", "submitter": "Wei Zhang", "authors": "Wei Zhang, Chao Dong, Jianhua Yin, Jianyong Wang", "title": "Attentive Representation Learning with Adversarial Training for Short\n  Text Clustering", "comments": "14pages, to appear in IEEE TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short text clustering has far-reaching effects on semantic analysis, showing\nits importance for multiple applications such as corpus summarization and\ninformation retrieval. However, it inevitably encounters the severe sparsity of\nshort text representations, making the previous clustering approaches still far\nfrom satisfactory. In this paper, we present a novel attentive representation\nlearning model for shot text clustering, wherein cluster-level attention is\nproposed to capture the correlations between text representations and cluster\nrepresentations. Relying on this, the representation learning and clustering\nfor short texts are seamlessly integrated into a unified model. To further\nensure robust model training for short texts, we apply adversarial training to\nthe unsupervised clustering setting, by injecting perturbations into the\ncluster representations. The model parameters and perturbations are optimized\nalternately through a minimax game. Extensive experiments on four real-world\nshort text datasets demonstrate the superiority of the proposed model over\nseveral strong competitors, verifying that robust adversarial training yields\nsubstantial performance gains.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 17:22:07 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 01:18:30 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Zhang", "Wei", ""], ["Dong", "Chao", ""], ["Yin", "Jianhua", ""], ["Wang", "Jianyong", ""]]}, {"id": "1912.03769", "submitter": "Arunkumar Bagavathi", "authors": "Arunkumar Bagavathi, Siddharth Krishnan, Sanjay Subrahmanyan, and S.\n  L. Narasimhan", "title": "ragamAI: A Network Based Recommender System to Arrange a Indian\n  Classical Music Concert", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  South Indian classical music (Carnatic music) is best consumed through live\nconcerts. A carnatic recital requires meticulous planning accounting for\nseveral parameters like the performers' repertoire, composition variety,\nmusical versatility, thematic structure, the recital's arrangement, etc. to\nensure that the audience have a comprehensive listening experience. In this\nwork, we present ragamAI a novel machine learning framework that utilizes the\ntonic nuances and musical structures in the carnatic music to generate a\nconcert recital that melodically captures the entire range in an octave.\nUtilizing the underlying idea of playlist and session-based recommender models,\nthe proposed model studies the mathematical structure present in past concerts\nand recommends relevant items for the playlist/concert. ragamAI ensembles\nrecommendations given by multiple models to learn user idea and past preference\nof sequences in concerts to extract recommendations. Our experiments on a vast\ncollection of concert show that our model performs 25%-50% better than baseline\nmodels. ragamAI's applications are two-fold. 1) it will assist musicians to\ncustomize their performance with the necessary variety required to sustain the\ninterest of the audience for the entirety of the concert 2) it will generate\ncarefully curated lists of south Indian classical music so that the listener\ncan discover the wide range of melody that the musical system can offer.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 21:39:06 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Bagavathi", "Arunkumar", ""], ["Krishnan", "Siddharth", ""], ["Subrahmanyan", "Sanjay", ""], ["Narasimhan", "S. L.", ""]]}, {"id": "1912.03804", "submitter": "Mojtaba Heidarysafa", "authors": "Mojtaba Heidarysafa, Kamran Kowsari, Tolu Odukoya, Philip Potter,\n  Laura E. Barnes, and Donald E. Brown", "title": "Women in ISIS Propaganda: A Natural Language Processing Analysis of\n  Topics and Emotions in a Comparison with Mainstream Religious Group", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online propaganda is central to the recruitment strategies of extremist\ngroups and in recent years these efforts have increasingly extended to women.\nTo investigate ISIS' approach to targeting women in their online propaganda and\nuncover implications for counterterrorism, we rely on text mining and natural\nlanguage processing (NLP). Specifically, we extract articles published in Dabiq\nand Rumiyah (ISIS's online English language publications) to identify prominent\ntopics. To identify similarities or differences between these texts and those\nproduced by non-violent religious groups, we extend the analysis to articles\nfrom a Catholic forum dedicated to women. We also perform an emotional analysis\nof both of these resources to better understand the emotional components of\npropaganda. We rely on Depechemood (a lexical-base emotion analysis method) to\ndetect emotions most likely to be evoked in readers of these materials. The\nfindings indicate that the emotional appeal of ISIS and Catholic materials are\nsimilar\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 01:11:59 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Heidarysafa", "Mojtaba", ""], ["Kowsari", "Kamran", ""], ["Odukoya", "Tolu", ""], ["Potter", "Philip", ""], ["Barnes", "Laura E.", ""], ["Brown", "Donald E.", ""]]}, {"id": "1912.04003", "submitter": "Aviad Elyashar", "authors": "Aviad Elyashar and Rami Puzis and Michael Fire", "title": "It Runs in the Family: Searching for Synonyms Using Digitized Family\n  Trees", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for a person's name is a common online activity. However, Web\nsearch engines provide few accurate results to queries containing names. In\ncontrast to a general word which has only one correct spelling, there are\nseveral legitimate spellings of a given name. Today, most techniques used to\nsuggest synonyms in online search are based on pattern matching and phonetic\nencoding, however they often perform poorly. As a result, there is a need for\nan effective tool for improved synonym suggestion. In this paper, we propose a\nrevolutionary approach for tackling the problem of synonym suggestion. Our\nnovel algorithm, GRAFT, utilizes historical data collected from genealogy\nwebsites, along with network algorithms. GRAFT is a general algorithm that\nsuggests synonyms using a graph based on names derived from digitized ancestral\nfamily trees. Synonyms are extracted from this graph, which is constructed\nusing generic ordering functions that outperform other algorithms that suggest\nsynonyms based on a single dimension, a factor that limits their performance.\nWe evaluated GRAFT's performance on three ground truth datasets of forenames\nand surnames, including a large-scale online genealogy dataset with over 16\nmillion profiles and more than 700,000 unique forenames and 500,000 surnames.\nWe compared GRAFT's performance at suggesting synonyms to 10 other algorithms,\nincluding phonetic encoding, string similarity algorithms, and machine and deep\nlearning algorithms. The results show GRAFT's superiority with respect to both\nforenames and surnames and demonstrate its use as a tool to improve synonym\nsuggestion.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 12:43:05 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 19:40:47 GMT"}, {"version": "v3", "created": "Sun, 12 Jul 2020 13:52:50 GMT"}, {"version": "v4", "created": "Fri, 29 Jan 2021 21:24:23 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Elyashar", "Aviad", ""], ["Puzis", "Rami", ""], ["Fire", "Michael", ""]]}, {"id": "1912.04099", "submitter": "Qiaosheng Zhang", "authors": "Qiaosheng Zhang, Vincent Y. F. Tan, and Changho Suh", "title": "Community Detection and Matrix Completion with Social and Item\n  Similarity Graphs", "comments": "To appear in the IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a binary rating matrix as well as\nclusters of users and items based on a partially observed matrix together with\nside-information in the form of social and item similarity graphs. These two\ngraphs are both generated according to the celebrated stochastic block model\n(SBM). We develop lower and upper bounds on sample complexity that match for\nvarious scenarios. Our information-theoretic results quantify the benefits of\nthe availability of the social and item similarity graphs. Further analysis\nreveals that under certain scenarios, the social and item similarity graphs\nproduce an interesting synergistic effect. This means that observing two graphs\nis strictly better than observing just one in terms of reducing the sample\ncomplexity.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 15:27:56 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 06:16:39 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Zhang", "Qiaosheng", ""], ["Tan", "Vincent Y. F.", ""], ["Suh", "Changho", ""]]}, {"id": "1912.04106", "submitter": "Stavros Vologiannidis", "authors": "Polychronis Charitidis, Stavros Doropoulos, Stavros Vologiannidis,\n  Ioannis Papastergiou, Sophia Karakeva", "title": "Towards countering hate speech against journalists on social media", "comments": null, "journal-ref": null, "doi": "10.1016/j.osnem.2020.100071", "report-no": null, "categories": "cs.IR cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The damaging effects of hate speech on social media are evident during the\nlast few years, and several organizations, researchers and social media\nplatforms tried to harness them in various ways. Despite these efforts, social\nmedia users are still affected by hate speech. The problem is even more\napparent to social groups that promote public discourse, such as journalists.\nIn this work, we focus on countering hate speech that is targeted to\njournalistic social media accounts. To accomplish this, a group of journalists\nassembled a definition of hate speech, taking into account the journalistic\npoint of view and the types of hate speech that are usually targeted against\njournalists. We then compile a large pool of tweets referring to\njournalism-related accounts in multiple languages. In order to annotate the\npool of unlabeled tweets according to the definition, we follow a concise\nannotation strategy that involves active learning annotation stages. The\noutcome of this paper is a novel, publicly available collection of Twitter\ndatasets in five different languages. Additionally, we experiment with\nstate-of-the-art deep learning architectures for hate speech detection and use\nour annotated datasets to train and evaluate them. Finally, we propose an\nensemble detection model that outperforms all individual models.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 07:51:23 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 19:55:34 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Charitidis", "Polychronis", ""], ["Doropoulos", "Stavros", ""], ["Vologiannidis", "Stavros", ""], ["Papastergiou", "Ioannis", ""], ["Karakeva", "Sophia", ""]]}, {"id": "1912.04107", "submitter": "Radu Tudor Ionescu", "authors": "S\\'ebastien D\\'ejean, Radu Tudor Ionescu, Josiane Mothe, Md Zia Ullah", "title": "Forward and Backward Feature Selection for Query Performance Prediction", "comments": "Accepted at SAC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of query performance prediction (QPP) is to automatically estimate\nthe effectiveness of a search result for any given query, without relevance\njudgements. Post-retrieval features have been shown to be more effective for\nthis task while being more expensive to compute than pre-retrieval features.\nCombining multiple post-retrieval features is even more effective, but\nstate-of-the-art QPP methods are impossible to interpret because of the\nblack-box nature of the employed machine learning models. However,\ninterpretation is useful for understanding the predictive model and providing\nmore answers about its behavior. Moreover, combining many post-retrieval\nfeatures is not applicable to real-world cases, since the query running time is\nof utter importance. In this paper, we investigate a new framework for feature\nselection in which the trained model explains well the prediction. We introduce\na step-wise (forward and backward) model selection approach where different\nsubsets of query features are used to fit different models from which the\nsystem selects the best one. We evaluate our approach on four TREC collections\nusing standard QPP features. We also develop two QPP features to address the\nissue of query-drift in the query feedback setting. We found that: (1) our\nmodel based on a limited number of selected features is as good as more complex\nmodels for QPP and better than non-selective models; (2) our model is more\nefficient than complex models during inference time since it requires fewer\nfeatures; (3) the predictive model is readable and understandable; and (4) one\nof our new QPP features is consistently selected across different collections,\nproving its usefulness.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 17:19:16 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["D\u00e9jean", "S\u00e9bastien", ""], ["Ionescu", "Radu Tudor", ""], ["Mothe", "Josiane", ""], ["Ullah", "Md Zia", ""]]}, {"id": "1912.04108", "submitter": "Liang Zhao", "authors": "Liang Zhao, Yang Wang, Daxiang Dong, Hao Tian", "title": "Learning to Recommend via Meta Parameter Partition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose to solve an important problem in recommendation --\nuser cold start, based on meta leaning method. Previous meta learning\napproaches finetune all parameters for each new user, which is both computing\nand storage expensive. In contrast, we divide model parameters into fixed and\nadaptive parts and develop a two-stage meta learning algorithm to learn them\nseparately. The fixed part, capturing user invariant features, is shared by all\nusers and is learned during offline meta learning stage. The adaptive part,\ncapturing user specific features, is learned during online meta learning stage.\nBy decoupling user invariant parameters from user dependent parameters, the\nproposed approach is more efficient and storage cheaper than previous methods.\nIt also has potential to deal with catastrophic forgetting while continually\nadapting for streaming coming users.\n  Experiments on production data demonstrates that the proposed method\nconverges faster and to a better performance than baseline methods.\nMeta-training without online meta model finetuning increases the AUC from\n72.24% to 74.72% (2.48% absolute improvement). Online meta training achieves a\nfurther gain of 2.46\\% absolute improvement comparing with offline meta\ntraining.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 05:58:31 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Zhao", "Liang", ""], ["Wang", "Yang", ""], ["Dong", "Daxiang", ""], ["Tian", "Hao", ""]]}, {"id": "1912.04109", "submitter": "Yangjun Xu", "authors": "Liang Chen and Yangjun Xu and Fenfang Xie and Min Huang and Zibin\n  Zheng", "title": "Data Poisoning Attacks on Neighborhood-based Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, collaborative filtering recommender systems have been widely\ndeployed in many commercial companies to make profit. Neighbourhood-based\ncollaborative filtering is common and effective. To date, despite its\neffectiveness, there has been little effort to explore their robustness and the\nimpact of data poisoning attacks on their performance. Can the\nneighbourhood-based recommender systems be easily fooled? To this end, we shed\nlight on the robustness of neighbourhood-based recommender systems and propose\na novel data poisoning attack framework encoding the purpose of attack and\nconstraint against them. We firstly illustrate how to calculate the optimal\ndata poisoning attack, namely UNAttack. We inject a few well-designed fake\nusers into the recommender systems such that target items will be recommended\nto as many normal users as possible. Extensive experiments are conducted on\nthree real-world datasets to validate the effectiveness and the transferability\nof our proposed method. Besides, some interesting phenomenons can be found. For\nexample, 1) neighbourhood-based recommender systems with Euclidean\nDistance-based similarity have strong robustness. 2) the fake users can be\ntransferred to attack the state-of-the-art collaborative filtering recommender\nsystems such as Neural Collaborative Filtering and Bayesian Personalized\nRanking Matrix Factorization.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 15:34:58 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Chen", "Liang", ""], ["Xu", "Yangjun", ""], ["Xie", "Fenfang", ""], ["Huang", "Min", ""], ["Zheng", "Zibin", ""]]}, {"id": "1912.04115", "submitter": "Shaurya Rohatgi", "authors": "Shaurya Rohatgi, Wei Zhong, Richard Zanibbi, Jian Wu, C. Lee Giles", "title": "Query Auto Completion for Math Formula Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query Auto Completion (QAC) is among the most appealing features of a web\nsearch engine. It helps users formulate queries quickly with less effort.\nAlthough there has been much effort in this area for text, to the best of our\nknowledge there is few work on mathematical formula auto completion. In this\npaper, we implement 5 existing QAC methods on mathematical formula and evaluate\nthem on the NTCIR-12 MathIR task dataset. We report the efficiency of retrieved\nresults using Mean Reciprocal Rank (MRR) and Mean Average Precision(MAP). Our\nstudy indicates that the Finite State Transducer outperforms other QAC models\nwith a MRR score of $0.642$.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 15:19:27 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Rohatgi", "Shaurya", ""], ["Zhong", "Wei", ""], ["Zanibbi", "Richard", ""], ["Wu", "Jian", ""], ["Giles", "C. Lee", ""]]}, {"id": "1912.04471", "submitter": "Bhaskar Mitra", "authors": "Bhaskar Mitra and Nick Craswell", "title": "Duet at TREC 2019 Deep Learning Track", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report discusses three submissions based on the Duet architecture to the\nDeep Learning track at TREC 2019. For the document retrieval task, we adapt the\nDuet model to ingest a \"multiple field\" view of documents---we refer to the new\narchitecture as Duet with Multiple Fields (DuetMF). A second submission\ncombines the DuetMF model with other neural and traditional relevance\nestimators in a learning-to-rank framework and achieves improved performance\nover the DuetMF baseline. For the passage retrieval task, we submit a single\nrun based on an ensemble of eight Duet models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 03:23:05 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Mitra", "Bhaskar", ""], ["Craswell", "Nick", ""]]}, {"id": "1912.04616", "submitter": "Matthias Samwald", "authors": "Anna Breit, Simon Ott, Asan Agibetov, Matthias Samwald", "title": "OpenBioLink: A benchmarking framework for large-scale biomedical link\n  prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  SUMMARY: Recently, novel machine-learning algorithms have shown potential for\npredicting undiscovered links in biomedical knowledge networks. However,\ndedicated benchmarks for measuring algorithmic progress have not yet emerged.\nWith OpenBioLink, we introduce a large-scale, high-quality and highly\nchallenging biomedical link prediction benchmark to transparently and\nreproducibly evaluate such algorithms. Furthermore, we present preliminary\nbaseline evaluation results. AVAILABILITY AND IMPLEMENTATION: Source code, data\nand supplementary files are openly available at\nhttps://github.com/OpenBioLink/OpenBioLink CONTACT: matthias.samwald ((at))\nmeduniwien.ac.at\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 10:26:13 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 14:53:06 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Breit", "Anna", ""], ["Ott", "Simon", ""], ["Agibetov", "Asan", ""], ["Samwald", "Matthias", ""]]}, {"id": "1912.04639", "submitter": "Gustavo Penha", "authors": "Gustavo Penha, Alexandru Balan and Claudia Hauff", "title": "Introducing MANtIS: a novel Multi-Domain Information Seeking Dialogues\n  Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search is an approach to information retrieval (IR), where\nusers engage in a dialogue with an agent in order to satisfy their information\nneeds. Previous conceptual work described properties and actions a good agent\nshould exhibit. Unlike them, we present a novel conceptual model defined in\nterms of conversational goals, which enables us to reason about current\nresearch practices in conversational search. Based on the literature, we elicit\nhow existing tasks and test collections from the fields of IR, natural language\nprocessing (NLP) and dialogue systems (DS) fit into this model. We describe a\nset of characteristics that an ideal conversational search dataset should have.\nLastly, we introduce MANtIS (the code and dataset are available at\nhttps://guzpenha.github.io/MANtIS/), a large-scale dataset containing\nmulti-domain and grounded information seeking dialogues that fulfill all of our\ndataset desiderata. We provide baseline results for the conversation response\nranking and user intent prediction tasks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 10:59:47 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Penha", "Gustavo", ""], ["Balan", "Alexandru", ""], ["Hauff", "Claudia", ""]]}, {"id": "1912.04696", "submitter": "Dominik Kowald PhD", "authors": "Dominik Kowald and Markus Schedl and Elisabeth Lex", "title": "The Unfairness of Popularity Bias in Music Recommendation: A\n  Reproducibility Study", "comments": "ECIR 2020 reproducibility track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research has shown that recommender systems are typically biased towards\npopular items, which leads to less popular items being underrepresented in\nrecommendations. The recent work of Abdollahpouri et al. in the context of\nmovie recommendations has shown that this popularity bias leads to unfair\ntreatment of both long-tail items as well as users with little interest in\npopular items. In this paper, we reproduce the analyses of Abdollahpouri et al.\nin the context of music recommendation. Specifically, we investigate three user\ngroups from the LastFM music platform that are categorized based on how much\ntheir listening preferences deviate from the most popular music among all\nLastFM users in the dataset: (i) low-mainstream users, (ii) medium-mainstream\nusers, and (iii) high-mainstream users. In line with Abdollahpouri et al., we\nfind that state-of-the-art recommendation algorithms favor popular items also\nin the music domain. However, their proposed Group Average Popularity metric\nyields different results for LastFM than for the movie domain, presumably due\nto the larger number of available items (i.e., music artists) in the LastFM\ndataset we use. Finally, we compare the accuracy results of the recommendation\nalgorithms for the three user groups and find that the low-mainstreaminess\ngroup significantly receives the worst recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 14:13:50 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 09:31:58 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Kowald", "Dominik", ""], ["Schedl", "Markus", ""], ["Lex", "Elisabeth", ""]]}, {"id": "1912.04713", "submitter": "Sebastian Hofst\\\"atter", "authors": "Sebastian Hofst\\\"atter, Markus Zlabinger, Allan Hanbury", "title": "Neural-IR-Explorer: A Content-Focused Tool to Explore Neural Re-Ranking\n  Results", "comments": "Accepted at ECIR 2020 (demo paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we look beyond metrics-based evaluation of Information\nRetrieval systems, to explore the reasons behind ranking results. We present\nthe content-focused Neural-IR-Explorer, which empowers users to browse through\nretrieval results and inspect the inner workings and fine-grained results of\nneural re-ranking models. The explorer includes a categorized overview of the\navailable queries, as well as an individual query result view with various\noptions to highlight semantic connections between query-document pairs. The\nNeural-IR-Explorer is available at: https://neural-ir-explorer.ec.tuwien.ac.at/\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 14:41:02 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Hofst\u00e4tter", "Sebastian", ""], ["Zlabinger", "Markus", ""], ["Hanbury", "Allan", ""]]}, {"id": "1912.04754", "submitter": "Angshul Majumdar Dr.", "authors": "Aanchal Mongia, Neha Jhamb, Emilie Chouzenoux and Angshul Majumdar", "title": "Deep Latent Factor Model for Collaborative Filtering", "comments": "This is an initial draft of the accepted paper at Elsevier Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent factor models have been used widely in collaborative filtering based\nrecommender systems. In recent years, deep learning has been successful in\nsolving a wide variety of machine learning problems. Motivated by the success\nof deep learning, we propose a deeper version of latent factor model.\nExperiments on benchmark datasets shows that our proposed technique\nsignificantly outperforms all state-of-the-art collaborative filtering\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 15:16:06 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Mongia", "Aanchal", ""], ["Jhamb", "Neha", ""], ["Chouzenoux", "Emilie", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1912.04832", "submitter": "Lukas Pfannschmidt", "authors": "Lukas Pfannschmidt, Jonathan Jakob, Fabian Hinder, Michael Biehl,\n  Peter Tino, Barbara Hammer", "title": "Feature Relevance Determination for Ordinal Regression in the Context of\n  Feature Redundancies and Privileged Information", "comments": "Preprint accepted at Neurocomputing", "journal-ref": null, "doi": "10.1016/j.neucom.2019.12.133", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in machine learning technologies have led to increasingly powerful\nmodels in particular in the context of big data. Yet, many application\nscenarios demand for robustly interpretable models rather than optimum model\naccuracy; as an example, this is the case if potential biomarkers or causal\nfactors should be discovered based on a set of given measurements. In this\ncontribution, we focus on feature selection paradigms, which enable us to\nuncover relevant factors of a given regularity based on a sparse model. We\nfocus on the important specific setting of linear ordinal regression, i.e.\\\ndata have to be ranked into one of a finite number of ordered categories by a\nlinear projection. Unlike previous work, we consider the case that features are\npotentially redundant, such that no unique minimum set of relevant features\nexists. We aim for an identification of all strongly and all weakly relevant\nfeatures as well as their type of relevance (strong or weak); we achieve this\ngoal by determining feature relevance bounds, which correspond to the minimum\nand maximum feature relevance, respectively, if searched over all equivalent\nmodels. In addition, we discuss how this setting enables us to substitute some\nof the features, e.g.\\ due to their semantics, and how to extend the framework\nof feature relevance intervals to the setting of privileged information, i.e.\\\npotentially relevant information is available for training purposes only, but\ncannot be used for the prediction itself.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 17:20:18 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Pfannschmidt", "Lukas", ""], ["Jakob", "Jonathan", ""], ["Hinder", "Fabian", ""], ["Biehl", "Michael", ""], ["Tino", "Peter", ""], ["Hammer", "Barbara", ""]]}, {"id": "1912.04961", "submitter": "Sai Prabhakar Pandi Selvaraj", "authors": "Sai P. Selvaraj, Sandeep Konam", "title": "Medication Regimen Extraction From Medical Conversations", "comments": "Proceedings of International Workshop on Health Intelligence\n  (W3PHIAI) of the 34th AAAI Conference on Artificial Intelligence, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting relevant information from medical conversations and providing it\nto doctors and patients might help in addressing doctor burnout and patient\nforgetfulness. In this paper, we focus on extracting the Medication Regimen\n(dosage and frequency for medications) discussed in a medical conversation. We\nframe the problem as a Question Answering (QA) task and perform comparative\nanalysis over: a QA approach, a new combined QA and Information Extraction\napproach, and other baselines. We use a small corpus of 6,692 annotated\ndoctor-patient conversations for the task. Clinical conversation corpora are\ncostly to create, difficult to handle (because of data privacy concerns), and\nthus scarce. We address this data scarcity challenge through data augmentation\nmethods, using publicly available embeddings and pretrain part of the network\non a related task (summarization) to improve the model's performance. Compared\nto the baseline, our best-performing models improve the dosage and frequency\nextractions' ROUGE-1 F1 scores from 54.28 and 37.13 to 89.57 and 45.94,\nrespectively. Using our best-performing model, we present the first fully\nautomated system that can extract Medication Regimen tags from spontaneous\ndoctor-patient conversations with about $\\approx$71% accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 20:18:39 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 04:03:31 GMT"}, {"version": "v3", "created": "Sun, 11 Oct 2020 18:04:27 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Selvaraj", "Sai P.", ""], ["Konam", "Sandeep", ""]]}, {"id": "1912.05006", "submitter": "Zhenyu Weng", "authors": "Zhenyu Weng, Yuesheng Zhu", "title": "Efficient Querying from Weighted Binary Codes", "comments": "13 pages, accepted by AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary codes are widely used to represent the data due to their small storage\nand efficient computation. However, there exists an ambiguity problem that lots\nof binary codes share the same Hamming distance to a query. To alleviate the\nambiguity problem, weighted binary codes assign different weights to each bit\nof binary codes and compare the binary codes by the weighted Hamming distance.\nTill now, performing the querying from the weighted binary codes efficiently is\nstill an open issue. In this paper, we propose a new method to rank the\nweighted binary codes and return the nearest weighted binary codes of the query\nefficiently. In our method, based on the multi-index hash tables, two\nalgorithms, the table bucket finding algorithm and the table merging algorithm,\nare proposed to select the nearest weighted binary codes of the query in a\nnon-exhaustive and accurate way. The proposed algorithms are justified by\nproving their theoretic properties. The experiments on three large-scale\ndatasets validate both the search efficiency and the search accuracy of our\nmethod. Especially for the number of weighted binary codes up to one billion,\nour method shows a great improvement of more than 1000 times faster than the\nlinear scan.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 06:48:29 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 02:41:13 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Weng", "Zhenyu", ""], ["Zhu", "Yuesheng", ""]]}, {"id": "1912.05171", "submitter": "Masaki Oguni", "authors": "Masaki Oguni, Yohei Seki, Yu Hirate", "title": "Character 3-gram Mover's Distance: An Effective Method for Detecting\n  Near-duplicate Japanese-language Recipes", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In user-generated recipe websites, users post their-original recipes. Some\nrecipes, however, are very similar in major components such as the cooking\ninstructions to other recipes. We refer to such recipes as \"near-duplicate\nrecipes\". In this study, we propose a method that extends the \"Word Mover's\nDistance\", which calculates distances between texts based on word embedding, to\ncharacter 3-gram embedding. Using a corpus of over 1.21 million recipes, we\nlearned the word embedding and the character 3-gram embedding by using a\nSkip-Gram model with negative sampling and fastText to extract candidate pairs\nof near-duplicate recipes. We then annotated these candidates and evaluated the\nproposed method against a comparison method. Our results demonstrated that\nnear-duplicate recipes that were not detected by the comparison method were\nsuccessfully detected by the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 08:27:57 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 11:44:40 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Oguni", "Masaki", ""], ["Seki", "Yohei", ""], ["Hirate", "Yu", ""]]}, {"id": "1912.05662", "submitter": "Roger Immich", "authors": "Diego O. Rodrigues, Frances A. Santos, Geraldo P. Rocha Filho, Ademar\n  T. Akabane, Raquel Cabral, Roger Immich, Wellington L. Junior, Felipe D.\n  Cunha, Daniel L. Guidoni, Thiago H. Silva, Denis Ros\\'ario, Eduardo\n  Cerqueira, Antonio A. F. Loureiro, Leandro A. Villas", "title": "Computa\\c{c}\\~ao Urbana da Teoria \\`a Pr\\'atica: Fundamentos,\n  Aplica\\c{c}\\~oes e Desafios", "comments": "in Portuguese. Simp\\'osio Brasileiro de Redes de Computadores e\n  Sistemas Distribu\\'idos (SBRC) 2019 - Minicursos", "journal-ref": "Simposio Brasileiro de Redes de Computadores e Sistemas\n  Distribuidos (SBRC), 2019", "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.DC cs.HC cs.IR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing of cities has resulted in innumerable technical and managerial\nchallenges for public administrators such as energy consumption, pollution,\nurban mobility and even supervision of private and public spaces in an\nappropriate way. Urban Computing emerges as a promising paradigm to solve such\nchallenges, through the extraction of knowledge, from a large amount of\nheterogeneous data existing in urban space. Moreover, Urban Computing\ncorrelates urban sensing, data management, and analysis to provide services\nthat have the potential to improve the quality of life of the citizens of large\nurban centers. Consider this context, this chapter aims to present the\nfundamentals of Urban Computing and the steps necessary to develop an\napplication in this area. To achieve this goal, the following questions will be\ninvestigated, namely: (i) What are the main research problems of Urban\nComputing?; (ii) What are the technological challenges for the implementation\nof services in Urban Computing?; (iii) What are the main methodologies used for\nthe development of services in Urban Computing?; and (iv) What are the\nrepresentative applications in this field?\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:01:58 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Rodrigues", "Diego O.", ""], ["Santos", "Frances A.", ""], ["Filho", "Geraldo P. Rocha", ""], ["Akabane", "Ademar T.", ""], ["Cabral", "Raquel", ""], ["Immich", "Roger", ""], ["Junior", "Wellington L.", ""], ["Cunha", "Felipe D.", ""], ["Guidoni", "Daniel L.", ""], ["Silva", "Thiago H.", ""], ["Ros\u00e1rio", "Denis", ""], ["Cerqueira", "Eduardo", ""], ["Loureiro", "Antonio A. F.", ""], ["Villas", "Leandro A.", ""]]}, {"id": "1912.05891", "submitter": "Liang Pang", "authors": "Liang Pang, Jun Xu, Qingyao Ai, Yanyan Lan, Xueqi Cheng, Jirong Wen", "title": "SetRank: Learning a Permutation-Invariant Ranking Model for Information\n  Retrieval", "comments": "Accepted at SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In learning-to-rank for information retrieval, a ranking model is\nautomatically learned from the data and then utilized to rank the sets of\nretrieved documents. Therefore, an ideal ranking model would be a mapping from\na document set to a permutation on the set, and should satisfy two critical\nrequirements: (1)~it should have the ability to model cross-document\ninteractions so as to capture local context information in a query; (2)~it\nshould be permutation-invariant, which means that any permutation of the\ninputted documents would not change the output ranking. Previous studies on\nlearning-to-rank either design uni-variate scoring functions that score each\ndocument separately, and thus failed to model the cross-document interactions;\nor construct multivariate scoring functions that score documents sequentially,\nwhich inevitably sacrifice the permutation invariance requirement. In this\npaper, we propose a neural learning-to-rank model called SetRank which directly\nlearns a permutation-invariant ranking model defined on document sets of any\nsize. SetRank employs a stack of (induced) multi-head self attention blocks as\nits key component for learning the embeddings for all of the retrieved\ndocuments jointly. The self-attention mechanism not only helps SetRank to\ncapture the local context information from cross-document interactions, but\nalso to learn permutation-equivariant representations for the inputted\ndocuments, which therefore achieving a permutation-invariant ranking model.\nExperimental results on three large scale benchmarks showed that the SetRank\nsignificantly outperformed the baselines include the traditional\nlearning-to-rank models and state-of-the-art Neural IR models.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 12:03:24 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 03:33:16 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Pang", "Liang", ""], ["Xu", "Jun", ""], ["Ai", "Qingyao", ""], ["Lan", "Yanyan", ""], ["Cheng", "Xueqi", ""], ["Wen", "Jirong", ""]]}, {"id": "1912.05946", "submitter": "Ahmed Baruwa", "authors": "Ahmed Baruwa, Mojeed Abisiga, Ibrahim Gbadegesin, Afeez Fakunle", "title": "Leveraging End-to-End Speech Recognition with Neural Architecture Search", "comments": null, "journal-ref": "IJSER, vol 10, Issue 11, 2019, pp 1113-1119", "doi": null, "report-no": null, "categories": "eess.AS cs.IR cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks (DNNs) have been demonstrated to outperform many\ntraditional machine learning algorithms in Automatic Speech Recognition (ASR).\nIn this paper, we show that a large improvement in the accuracy of deep speech\nmodels can be achieved with effective Neural Architecture Optimization at a\nvery low computational cost. Phone recognition tests with the popular\nLibriSpeech and TIMIT benchmarks proved this fact by displaying the ability to\ndiscover and train novel candidate models within a few hours (less than a day)\nmany times faster than the attention-based seq2seq models. Our method achieves\ntest error of 7% Word Error Rate (WER) on the LibriSpeech corpus and 13% Phone\nError Rate (PER) on the TIMIT corpus, on par with state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 08:15:58 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Baruwa", "Ahmed", ""], ["Abisiga", "Mojeed", ""], ["Gbadegesin", "Ibrahim", ""], ["Fakunle", "Afeez", ""]]}, {"id": "1912.06262", "submitter": "Yue Zhao", "authors": "Yue Zhao and John Handley", "title": "Extracting clinical concepts from user queries", "comments": "8 pages, 4 figures. Added references. Corrected wording and typos,\n  results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical concept extraction often begins with clinical Named Entity\nRecognition (NER). Often trained on annotated clinical notes, clinical NER\nmodels tend to struggle with tagging clinical entities in user queries because\nof the structural differences between clinical notes and user queries. User\nqueries, unlike clinical notes, are often ungrammatical and incoherent. In many\ncases, user queries are compounded of multiple clinical entities, without comma\nor conjunction words separating them. By using as dataset a mixture of\nannotated clinical notes and synthesized user queries, we adapt a clinical NER\nmodel based on the BiLSTM-CRF architecture for tagging clinical entities in\nuser queries. Our contribution are the following: 1) We found that when trained\non a mixture of synthesized user queries and clinical notes, the NER model\nperforms better on both user queries and clinical notes. 2) We provide an\nend-to-end and easy-to-implement framework for clinical concept extraction from\nuser queries.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 23:18:16 GMT"}, {"version": "v2", "created": "Mon, 23 Dec 2019 19:10:03 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Zhao", "Yue", ""], ["Handley", "John", ""]]}, {"id": "1912.06806", "submitter": "Preslav Nakov", "authors": "Preslav Nakov, Zornitsa Kozareva, Alan Ritter, Sara Rosenthal, Veselin\n  Stoyanov, Theresa Wilson", "title": "SemEval-2013 Task 2: Sentiment Analysis in Twitter", "comments": "Sentiment analysis, microblog sentiment analysis, Twitter opinion\n  mining, SMS", "journal-ref": "SemEval-2013", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, sentiment analysis in social media has attracted a lot of\nresearch interest and has been used for a number of applications.\nUnfortunately, research has been hindered by the lack of suitable datasets,\ncomplicating the comparison between approaches. To address this issue, we have\nproposed SemEval-2013 Task 2: Sentiment Analysis in Twitter, which included two\nsubtasks: A, an expression-level subtask, and B, a message-level subtask. We\nused crowdsourcing on Amazon Mechanical Turk to label a large Twitter training\ndataset along with additional test sets of Twitter and SMS messages for both\nsubtasks. All datasets used in the evaluation are released to the research\ncommunity. The task attracted significant interest and a total of 149\nsubmissions from 44 teams. The best-performing team achieved an F1 of 88.9% and\n69% for subtasks A and B, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 08:44:18 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Nakov", "Preslav", ""], ["Kozareva", "Zornitsa", ""], ["Ritter", "Alan", ""], ["Rosenthal", "Sara", ""], ["Stoyanov", "Veselin", ""], ["Wilson", "Theresa", ""]]}, {"id": "1912.06810", "submitter": "Preslav Nakov", "authors": "Alberto Barr\\'on-Cede\\~no, Giovanni Da San Martino, Israa Jaradat,\n  Preslav Nakov", "title": "Proppy: A System to Unmask Propaganda in Online News", "comments": "propaganda, disinformation, fake news", "journal-ref": "Thirty-Third AAAI Conference on Artificial Intelligence\n  (AAAI-2019)", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present proppy, the first publicly available real-world, real-time\npropaganda detection system for online news, which aims at raising awareness,\nthus potentially limiting the impact of propaganda and helping fight\ndisinformation. The system constantly monitors a number of news sources,\ndeduplicates and clusters the news into events, and organizes the articles\nabout an event on the basis of the likelihood that they contain propagandistic\ncontent. The system is trained on known propaganda sources using a variety of\nstylistic features. The evaluation results on a standard dataset show\nstate-of-the-art results for propaganda detection.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 08:58:01 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["Martino", "Giovanni Da San", ""], ["Jaradat", "Israa", ""], ["Nakov", "Preslav", ""]]}, {"id": "1912.06859", "submitter": "Svitlana Vakulenko", "authors": "Svitlana Vakulenko", "title": "Knowledge-based Conversational Search", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conversational interfaces that allow for intuitive and comprehensive access\nto digitally stored information remain an ambitious goal. In this thesis, we\nlay foundations for designing conversational search systems by analyzing the\nrequirements and proposing concrete solutions for automating some of the basic\ncomponents and tasks that such systems should support. We describe several\ninterdependent studies that were conducted to analyse the design requirements\nfor more advanced conversational search systems able to support complex\nhuman-like dialogue interactions and provide access to vast knowledge\nrepositories. In the first two research chapters, we focus on analyzing the\nstructures common to information-seeking dialogues by capturing recurrent\npatterns in terms of both domain-independent functional relations between\nutterances as well as domain-specific implicit semantic relations from shared\nbackground knowledge.\n  Our results show that question answering is one of the key components\nrequired for efficient information access but it is not the only type of\ndialogue interactions that a conversational search system should support. In\nthe third research chapter, we propose a novel approach for complex question\nanswering from a knowledge graph that surpasses the current state-of-the-art\nresults in terms of both efficacy and efficiency. In the last research chapter,\nwe turn our attention towards an alternative interaction mode, which we termed\nconversational browsing, in which, unlike question answering, the\nconversational system plays a more pro-active role in the course of a dialogue\ninteraction. We show that this approach helps users to discover relevant items\nthat are difficult to retrieve using only question answering due to the\nvocabulary mismatch problem.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 14:59:38 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Vakulenko", "Svitlana", ""]]}, {"id": "1912.06933", "submitter": "Christine Bauer", "authors": "Christine Bauer and Markus Schedl", "title": "Global and country-specific mainstreaminess measures: Definitions,\n  analysis, and usage for improving personalized music recommendation systems", "comments": "36 pages, 4 figures, 10 tables, PLOS ONE 14(6), paper e0217389", "journal-ref": "PLOS ONE 2019, 14(6), Art no. e0217389", "doi": "10.1371/journal.pone.0217389", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Popularity-based approaches are widely adopted in music recommendation\nsystems, both in industry and research. However, as the popularity distribution\nof music items typically is a long-tail distribution, popularity-based\napproaches to music recommendation fall short in satisfying listeners that have\nspecialized music. The contribution of this article is three-fold. We provide\nseveral quantitative measures describing the proximity of a user's music\npreference to the music mainstream. We define the measures at two levels:\nrelating a listener's music preferences to the global music preferences of all\nusers, or relating them to music preferences of the user's country. Moreover,\nwe adopt a distribution-based and a rank-based approach as means to decrease\nbias towards the head of the long-tail distribution. We analyze differences\nbetween countries in terms of their level of mainstreaminess, uncover both\npositive and negative outliers (substantially higher and lower country-specific\npopularity, respectively, compared to the global mainstream), and investigate\ndifferences between countries in terms of listening preferences related to\npopular music artists. We use the standardized LFM-1b dataset, from which we\nanalyze about 8 million listening events shared by about 53,000 users (from 47\ncountries) of the music streaming platform Last.fm. We show that there are\nsubstantial country-specific differences in listeners' music consumption\nbehavior with respect to the most popular artists listened to. We conduct\nrating prediction experiments in which we tailor recommendations to a user's\nlevel of preference for the music mainstream using the proposed 6\nmainstreaminess measures. Results suggest that, in terms of rating prediction\naccuracy, each of the presented mainstreaminess definitions has its merits.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 21:31:08 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Bauer", "Christine", ""], ["Schedl", "Markus", ""]]}, {"id": "1912.07090", "submitter": "Dimitrios Katsaros", "authors": "Dimitrios Katsaros", "title": "Hunting for supernovae articles in the universe of scientometrics", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note records an unusual situation with some Google Scholar's\nprofiles that imply the existence of \"supernovae\" articles, i.e., articles\nwhose impact -- in terms of number of citations -- in a single year gets\n(almost) an order of magnitude higher than the previous year and immediate\ndrops (and remains steady) to a very low level after the next year. We analyse\nthe issue and resolve the situation providing an answer whether there exist\nsupernovae articles.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 18:54:36 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Katsaros", "Dimitrios", ""]]}, {"id": "1912.07101", "submitter": "Omid Jafari", "authors": "Omid Jafari, Parth Nagarkar, Jonathan Monta\\~no", "title": "Efficient Bitmap-based Indexing and Retrieval of Similarity Search Image\n  Queries", "comments": "Submitted to 2020 IEEE Southwest Symposium on Image Analysis and\n  Interpretation (SSIAI 2020)", "journal-ref": "2020 IEEE Southwest Symposium on Image Analysis and Interpretation\n  (SSIAI), Albuquerque, NM, USA, 2020, pp. 58-61", "doi": "10.1109/SSIAI49293.2020.9094616", "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding similar images is a necessary operation in many multimedia\napplications. Images are often represented and stored as a set of\nhigh-dimensional features, which are extracted using localized feature\nextraction algorithms. Locality Sensitive Hashing is one of the most popular\napproximate processing techniques for finding similar points in\nhigh-dimensional spaces. Locality Sensitive Hashing (LSH) and its variants are\ndesigned to find similar points, but they are not designed to find objects\n(such as images, which are made up of a collection of points) efficiently. In\nthis paper, we propose an index structure, Bitmap-Image LSH (bImageLSH), for\nefficient processing of high-dimensional images. Using a real dataset, we\nexperimentally show the performance benefit of our novel design while keeping\nthe accuracy of the image results high.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 20:09:16 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Jafari", "Omid", ""], ["Nagarkar", "Parth", ""], ["Monta\u00f1o", "Jonathan", ""]]}, {"id": "1912.07274", "submitter": "Ke Sun", "authors": "Ke Sun and Tieyun Qian", "title": "Seq2seq Translation Model for Sequential Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The context information such as product category plays a critical role in\nsequential recommendation. Recent years have witnessed a growing interest in\ncontext-aware sequential recommender systems. Existing studies often treat the\ncontexts as auxiliary feature vectors without considering the sequential\ndependency in contexts. However, such a dependency provides valuable clues to\npredict the user's future behavior. For example, a user might buy electronic\naccessories after he/she buy an electronic product.\n  In this paper, we propose a novel seq2seq translation architecture to\nhighlight the importance of sequential dependency in contexts for sequential\nrecommendation. Specifically, we first construct a collateral context sequence\nin addition to the main interaction sequence. We then generalize recent\nadvancements in translation model from sequences of words in two languages to\nsequences of items and contexts in recommender systems. Taking the category\ninformation as an item's context, we develop a basic coupled and an extended\ntripled seq2seq translation models to encode the category-item and\nitem-category-item relations between the item and context sequences. We conduct\nextensive experiments on three real world datasets. The results demonstrate the\nsuperior performance of the proposed model compared with the state-of-the-art\nbaselines.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 10:04:01 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 11:40:50 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Sun", "Ke", ""], ["Qian", "Tieyun", ""]]}, {"id": "1912.07419", "submitter": "Elaheh Momeni", "authors": "Patrick Kiss and Elaheh Momeni", "title": "Optimized Tracking of Topic Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic evolution modeling has been researched for a long time and has gained\nconsiderable interest. A state-of-the-art method has been recently using word\nmodeling algorithms in combination with community detection mechanisms to\nachieve better results in a more effective way. We analyse results of this\napproach and discuss the two major challenges that this approach still faces.\nAlthough the topics that have resulted from the recent algorithm are good in\ngeneral, they are very noisy due to many topics that are very unimportant\nbecause of their size, words, or ambiguity. Additionally, the number of words\ndefining each topic is too large, making it difficult to analyse them in their\nunsorted state. In this paper, we propose approaches to tackle these challenges\nby adding topic filtering and network analysis metrics to define the importance\nof a topic. We test different combinations of these metrics to see which\ncombination yields the best results. Furthermore, we add word filtering and\nranking to each topic to identify the words with the highest novelty\nautomatically. We evaluate our enhancement methods in two ways: human\nqualitative evaluation and automatic quantitative evaluation. Moreover, we\ncreated two case studies to test the quality of the clusters and words. In the\nquantitative evaluation, we use the pairwise mutual information score to test\nthe coherency of topics. The quantitative evaluation also includes an analysis\nof execution times for each part of the program. The results of the\nexperimental evaluations show that the two evaluation methods agree on the\npositive feasibility of the algorithm. We then show possible extensions in the\nform of usability and future improvements to the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 14:43:11 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Kiss", "Patrick", ""], ["Momeni", "Elaheh", ""]]}, {"id": "1912.07745", "submitter": "Janis Dalins", "authors": "Janis Dalins, Campbell Wilson, Douglas Boudry", "title": "PDQ & TMK + PDQF -- A Test Drive of Facebook's Perceptual Hashing\n  Algorithms", "comments": "Submitted to Journal of Digital Investigation 08 SEP 2019. Under\n  review as at 13 December 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and reliable automated detection of modified image and multimedia\nfiles has long been a challenge for law enforcement, compounded by the harm\ncaused by repeated exposure to psychologically harmful materials. In August\n2019 Facebook open-sourced their PDQ and TMK + PDQF algorithms for image and\nvideo similarity measurement, respectively. In this report, we review the\nalgorithms' performance on detecting commonly encountered transformations on\nreal-world case data, sourced from contemporary investigations. We also provide\na reference implementation to demonstrate the potential application and\nintegration of such algorithms within existing law enforcement systems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 23:00:09 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Dalins", "Janis", ""], ["Wilson", "Campbell", ""], ["Boudry", "Douglas", ""]]}, {"id": "1912.07747", "submitter": "William Hsu", "authors": "Huichen Yang, Carlos A. Aguirre, Maria F. De La Torre, Derek\n  Christensen, Luis Bobadilla, Emily Davich, Jordan Roth, Lei Luo, Yihong\n  Theis, Alice Lam, T. Yong-Jin Han, David Buttler, William H. Hsu", "title": "Pipelines for Procedural Information Extraction from Scientific\n  Literature: Towards Recipes using Machine Learning and Data Science", "comments": "15th International Conference on Document Analysis and Recognition\n  Workshops (ICDARW 2019)", "journal-ref": null, "doi": "10.1109/ICDARW.2019.10037", "report-no": "2019-1", "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a machine learning and data science pipeline for\nstructured information extraction from documents, implemented as a suite of\nopen-source tools and extensions to existing tools. It centers around a\nmethodology for extracting procedural information in the form of recipes,\nstepwise procedures for creating an artifact (in this case synthesizing a\nnanomaterial), from published scientific literature. From our overall goal of\nproducing recipes from free text, we derive the technical objectives of a\nsystem consisting of pipeline stages: document acquisition and filtering,\npayload extraction, recipe step extraction as a relationship extraction task,\nrecipe assembly, and presentation through an information retrieval interface\nwith question answering (QA) functionality. This system meets computational\ninformation and knowledge management (CIKM) requirements of metadata-driven\npayload extraction, named entity extraction, and relationship extraction from\ntext. Functional contributions described in this paper include semi-supervised\nmachine learning methods for PDF filtering and payload extraction tasks,\nfollowed by structured extraction and data transformation tasks beginning with\nsection extraction, recipe steps as information tuples, and finally assembled\nrecipes. Measurable objective criteria for extraction quality include precision\nand recall of recipe steps, ordering constraints, and QA accuracy, precision,\nand recall. Results, key novel contributions, and significant open problems\nderived from this work center around the attribution of these holistic quality\nmeasures to specific machine learning and inference stages of the pipeline,\neach with their performance measures. The desired recipes contain identified\npreconditions, material inputs, and operations, and constitute the overall\noutput generated by our computational information and knowledge management\n(CIKM) system.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 23:04:03 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Yang", "Huichen", ""], ["Aguirre", "Carlos A.", ""], ["De La Torre", "Maria F.", ""], ["Christensen", "Derek", ""], ["Bobadilla", "Luis", ""], ["Davich", "Emily", ""], ["Roth", "Jordan", ""], ["Luo", "Lei", ""], ["Theis", "Yihong", ""], ["Lam", "Alice", ""], ["Han", "T. Yong-Jin", ""], ["Buttler", "David", ""], ["Hsu", "William H.", ""]]}, {"id": "1912.07911", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Jing Chen, Haonan Sun, Keyang Xu", "title": "A Heterogeneous Graphical Model to Understand User-Level Sentiments in\n  Social Media", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social Media has seen a tremendous growth in the last decade and is\ncontinuing to grow at a rapid pace. With such adoption, it is increasingly\nbecoming a rich source of data for opinion mining and sentiment analysis. The\ndetection and analysis of sentiment in social media is thus a valuable topic\nand attracts a lot of research efforts. Most of the earlier efforts focus on\nsupervised learning approaches to solve this problem, which require expensive\nhuman annotations and therefore limits their practical use. In our work, we\npropose a semi-supervised approach to predict user-level sentiments for\nspecific topics. We define and utilize a heterogeneous graph built from the\nsocial networks of the users with the knowledge that connected users in social\nnetworks typically share similar sentiments. Compared with the previous works,\nwe have several novelties: (1) we incorporate the influences/authoritativeness\nof the users into the model, 2) we include comment-based and like-based\nuser-user links to the graph, 3) we superimpose multiple heterogeneous graphs\ninto one thereby allowing multiple types of links to exist between two users.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 10:29:26 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Chen", "Jing", ""], ["Sun", "Haonan", ""], ["Xu", "Keyang", ""]]}, {"id": "1912.07915", "submitter": "Fengshi Jing", "authors": "Fengshi Jing and Qingpeng Zhang", "title": "Knowledge-Enhanced Attentive Learning for Answer Selection in Community\n  Question Answering Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the community question answering (CQA) system, the answer selection task\naims to identify the best answer for a specific question, and thus is playing a\nkey role in enhancing the service quality through recommending appropriate\nanswers for new questions. Recent advances in CQA answer selection focus on\nenhancing the performance by incorporating the community information,\nparticularly the expertise (previous answers) and authority (position in the\nsocial network) of an answerer. However, existing approaches for incorporating\nsuch information are limited in (a) only considering either the expertise or\nthe authority, but not both; (b) ignoring the domain knowledge to differentiate\ntopics of previous answers; and (c) simply using the authority information to\nadjust the similarity score, instead of fully utilizing it in the process of\nmeasuring the similarity between segments of the question and the answer. We\npropose the Knowledge-enhanced Attentive Answer Selection (KAAS) model, which\nenhances the performance through (a) considering both the expertise and the\nauthority of the answerer; (b) utilizing the human-labeled tags, the taxonomy\nof the tags, and the votes as the domain knowledge to infer the expertise of\nthe answer; (c) using matrix decomposition of the social network (formed by\nfollowing-relationship) to infer the authority of the answerer and\nincorporating such information in the process of evaluating the similarity\nbetween segments. Besides, for vertical community, we incorporate an external\nknowledge graph to capture more professional information for vertical CQA\nsystems. Then we adopt the attention mechanism to integrate the analysis of the\ntext of questions and answers and the aforementioned community information.\nExperiments with both vertical and general CQA sites demonstrate the superior\nperformance of the proposed KAAS model.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 10:33:17 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Jing", "Fengshi", ""], ["Zhang", "Qingpeng", ""]]}, {"id": "1912.08084", "submitter": "Preslav Nakov", "authors": "Pepa Gencheva, Ivan Koychev, Llu\\'is M\\`arquez, Alberto\n  Barr\\'on-Cede\\~no, Preslav Nakov", "title": "A Context-Aware Approach for Detecting Check-Worthy Claims in Political\n  Debates", "comments": "Check-worthiness; Fact-Checking; Veracity; Neural Networks. arXiv\n  admin note: substantial text overlap with arXiv:1908.01328", "journal-ref": "RANLP-2017", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of investigative journalism, we address the problem of\nautomatically identifying which claims in a given document are most worthy and\nshould be prioritized for fact-checking. Despite its importance, this is a\nrelatively understudied problem. Thus, we create a new dataset of political\ndebates, containing statements that have been fact-checked by nine reputable\nsources, and we train machine learning models to predict which claims should be\nprioritized for fact-checking, i.e., we model the problem as a ranking task.\nUnlike previous work, which has looked primarily at sentences in isolation, in\nthis paper we focus on a rich input representation modeling the context:\nrelationship between the target statement and the larger context of the debate,\ninteraction between the opponents, and reaction by the moderator and by the\npublic. Our experiments show state-of-the-art results, outperforming a strong\nrivaling system by a margin, while also confirming the importance of the\ncontextual information.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 10:29:13 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Gencheva", "Pepa", ""], ["Koychev", "Ivan", ""], ["M\u00e0rquez", "Llu\u00eds", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["Nakov", "Preslav", ""]]}, {"id": "1912.08140", "submitter": "Yashaswi Verma", "authors": "Yashaswi Verma", "title": "An Embarrassingly Simple Baseline for eXtreme Multi-label Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of eXtreme Multi-label Learning (XML) is to design and learn a model\nthat can automatically annotate a given data point with the most relevant\nsubset of labels from an extremely large label set. Recently, many techniques\nhave been proposed for XML that achieve reasonable performance on benchmark\ndatasets. Motivated by the complexities of these methods and their subsequent\ntraining requirements, in this paper we propose a simple baseline technique for\nthis task. Precisely, we present a global feature embedding technique for XML\nthat can easily scale to very large datasets containing millions of data points\nin very high-dimensional feature space, irrespective of number of samples and\nlabels. Next we show how an ensemble of such global embeddings can be used to\nachieve further boost in prediction accuracies with only linear increase in\ntraining and prediction time. During testing, we assign the labels using a\nweighted k-nearest neighbour classifier in the embedding space. Experiments\nreveal that though conceptually simple, this technique achieves quite\ncompetitive results, and has training time of less than one minute using a\nsingle CPU core with 15.6 GB RAM even for large-scale datasets such as\nAmazon-3M.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 17:11:17 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Verma", "Yashaswi", ""]]}, {"id": "1912.08422", "submitter": "Yuan Zhang", "authors": "Yuan Zhang, Xiaoran Xu, Hanning Zhou, Yan Zhang", "title": "Distilling Structured Knowledge into Embeddings for Explainable and\n  Accurate Recommendation", "comments": "Accepted by WSDM'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the embedding-based recommendation models (e.g., matrix\nfactorization and deep models) have been prevalent in both academia and\nindustry due to their effectiveness and flexibility. However, they also have\nsuch intrinsic limitations as lacking explainability and suffering from data\nsparsity. In this paper, we propose an end-to-end joint learning framework to\nget around these limitations without introducing any extra overhead by\ndistilling structured knowledge from a differentiable path-based recommendation\nmodel. Through extensive experiments, we show that our proposed framework can\nachieve state-of-the-art recommendation performance and meanwhile provide\ninterpretable recommendation reasons.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 07:43:52 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Zhang", "Yuan", ""], ["Xu", "Xiaoran", ""], ["Zhou", "Hanning", ""], ["Zhang", "Yan", ""]]}, {"id": "1912.08441", "submitter": "Fanchao Qi", "authors": "Lei Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Qun Liu, Maosong Sun", "title": "Multi-channel Reverse Dictionary Model", "comments": "Accepted by AAAI Conference on Artificial Intelligence 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reverse dictionary takes the description of a target word as input and\noutputs the target word together with other words that match the description.\nExisting reverse dictionary methods cannot deal with highly variable input\nqueries and low-frequency target words successfully. Inspired by the\ndescription-to-word inference process of humans, we propose the multi-channel\nreverse dictionary model, which can mitigate the two problems simultaneously.\nOur model comprises a sentence encoder and multiple predictors. The predictors\nare expected to identify different characteristics of the target word from the\ninput query. We evaluate our model on English and Chinese datasets including\nboth dictionary definitions and human-written descriptions. Experimental\nresults show that our model achieves the state-of-the-art performance, and even\noutperforms the most popular commercial reverse dictionary system on the\nhuman-written description dataset. We also conduct quantitative analyses and a\ncase study to demonstrate the effectiveness and robustness of our model. All\nthe code and data of this work can be obtained on\nhttps://github.com/thunlp/MultiRD.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 08:13:43 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 02:12:09 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Zhang", "Lei", ""], ["Qi", "Fanchao", ""], ["Liu", "Zhiyuan", ""], ["Wang", "Yasheng", ""], ["Liu", "Qun", ""], ["Sun", "Maosong", ""]]}, {"id": "1912.08555", "submitter": "Gustavo Penha", "authors": "Gustavo Penha and Claudia Hauff", "title": "Curriculum Learning Strategies for IR: An Empirical Study on\n  Conversation Response Ranking", "comments": "Accepted for publication in the 42nd European Conference on\n  Information Retrieval (ECIR'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural ranking models are traditionally trained on a series of random\nbatches, sampled uniformly from the entire training set. Curriculum learning\nhas recently been shown to improve neural models' effectiveness by sampling\nbatches non-uniformly, going from easy to difficult instances during training.\nIn the context of neural Information Retrieval (IR) curriculum learning has not\nbeen explored yet, and so it remains unclear (1) how to measure the difficulty\nof training instances and (2) how to transition from easy to difficult\ninstances during training. To address both challenges and determine whether\ncurriculum learning is beneficial for neural ranking models, we need\nlarge-scale datasets and a retrieval task that allows us to conduct a wide\nrange of experiments. For this purpose, we resort to the task of conversation\nresponse ranking: ranking responses given the conversation history. In order to\ndeal with challenge (1), we explore scoring functions to measure the difficulty\nof conversations based on different input spaces. To address challenge (2) we\nevaluate different pacing functions, which determine the velocity in which we\ngo from easy to difficult instances. We find that, overall, by just\nintelligently sorting the training data (i.e., by performing curriculum\nlearning) we can improve the retrieval effectiveness by up to 2%.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 12:13:30 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Penha", "Gustavo", ""], ["Hauff", "Claudia", ""]]}, {"id": "1912.08633", "submitter": "Anastasios Nentidis", "authors": "Anastasios Nentidis, Konstantinos Bougiatiotis, Anastasia Krithara,\n  Georgios Paliouras", "title": "iASiS Open Data Graph: Automated Semantic Integration of\n  Disease-Specific Knowledge", "comments": "6 pages, 2 figures, accepted in IEEE 33rd International Symposium on\n  Computer Based Medical Systems (CBMS2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biomedical research, unified access to up-to-date domain-specific\nknowledge is crucial, as such knowledge is continuously accumulated in\nscientific literature and structured resources. Identifying and extracting\nspecific information is a challenging task and computational analysis of\nknowledge bases can be valuable in this direction. However, for\ndisease-specific analyses researchers often need to compile their own datasets,\nintegrating knowledge from different resources, or reuse existing datasets,\nthat can be out-of-date. In this study, we propose a framework to automatically\nretrieve and integrate disease-specific knowledge into an up-to-date semantic\ngraph, the iASiS Open Data Graph. This disease-specific semantic graph provides\naccess to knowledge relevant to specific concepts and their individual aspects,\nin the form of concept relations and attributes. The proposed approach is\nimplemented as an open-source framework and applied to three diseases (Lung\nCancer, Dementia, and Duchenne Muscular Dystrophy). Exemplary queries are\npresented, investigating the potential of this automatically generated semantic\ngraph as a basis for retrieval and analysis of disease-specific knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 14:33:05 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 10:47:56 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Nentidis", "Anastasios", ""], ["Bougiatiotis", "Konstantinos", ""], ["Krithara", "Anastasia", ""], ["Paliouras", "Georgios", ""]]}, {"id": "1912.08694", "submitter": "Andrew Collins Mr", "authors": "Andrew Collins, Joeran Beel", "title": "Meta-Learned Per-Instance Algorithm Selection in Scholarly Recommender\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of recommender system algorithms varies in different\nreal-world scenarios. It is difficult to choose a best algorithm for a scenario\ndue to the quantity of algorithms available, and because of their varying\nperformances. Furthermore, it is not possible to choose one single algorithm\nthat will work optimally for all recommendation requests. We apply\nmeta-learning to this problem of algorithm selection for scholarly article\nrecommendation. We train a random forest, gradient boosting machine, and\ngeneralized linear model, to predict a best-algorithm from a pool of content\nsimilarity-based algorithms. We evaluate our approach on an offline dataset for\nscholarly article recommendation and attempt to predict the best algorithm\nper-instance. The best meta-learning model achieved an average increase in F1\nof 88% when compared to the average F1 of all base-algorithms (F1; 0.0708 vs\n0.0376) and was significantly able to correctly select each base-algorithm\n(Paired t-test; p < 0.1). The meta-learner had a 3% higher F1 when compared to\nthe single-best base-algorithm (F1; 0.0739 vs 0.0717). We further perform an\nonline evaluation of our approach, conducting an A/B test through our\nrecommender-as-a-service platform Mr. DLib. We deliver 148K recommendations to\nusers between January and March 2019. User engagement was significantly\nincreased for recommendations generated using our meta-learning approach when\ncompared to a random selection of algorithm (Click-through rate (CTR); 0.51%\nvs. 0.44%, Chi-Squared test; p < 0.1), however our approach did not produce a\nhigher CTR than the best algorithm alone (CTR; MoreLikeThis (Title): 0.58%).\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 16:13:27 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Collins", "Andrew", ""], ["Beel", "Joeran", ""]]}, {"id": "1912.08756", "submitter": "Jing Li", "authors": "Soroosh Khoram, Stephen J Wright, Jing Li", "title": "Interleaved Composite Quantization for High-Dimensional Similarity\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search retrieves the nearest neighbors of a query vector from a\ndataset of high-dimensional vectors. As the size of the dataset grows, the cost\nof performing the distance computations needed to implement a query can become\nprohibitive. A method often used to reduce this computational cost is\nquantization of the vector space and location-based encoding of the dataset\nvectors. These encodings can be used during query processing to find\napproximate nearest neighbors of the query point quickly. Search speed can be\nimproved by using shorter codes, but shorter codes have higher quantization\nerror, leading to degraded precision. In this work, we propose the Interleaved\nComposite Quantization (ICQ) which achieves fast similarity search without\nusing shorter codes. In ICQ, a small subset of the code is used to approximate\nthe distances, with complete codes being used only when necessary. Our method\neffectively reduces both code length and quantization error. Furthermore, ICQ\nis compatible with several recently proposed techniques for reducing\nquantization error and can be used in conjunction with these other techniques\nto improve results. We confirm these claims and show strong empirical\nperformance of ICQ using several synthetic and real-word datasets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 17:40:56 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 01:41:50 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Khoram", "Soroosh", ""], ["Wright", "Stephen J", ""], ["Li", "Jing", ""]]}, {"id": "1912.08868", "submitter": "Rashid Mehdiyev Dr", "authors": "Rashid Mehdiyev, Jean Nava, Karan Sodhi, Saurav Acharya, Annie Ibrahim\n  Rana", "title": "Topic subject creation using unsupervised learning for topic modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CY cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the use of Non-Negative Matrix Factorization (NMF) and Latent\nDirichlet Allocation (LDA) algorithms to perform topic mining and labelling\napplied to retail customer communications in attempt to characterize the\nsubject of customers inquiries. In this paper we compare both algorithms in the\ntopic mining performance and propose methods to assign topic subject labels in\nan automated way.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 20:11:03 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Mehdiyev", "Rashid", ""], ["Nava", "Jean", ""], ["Sodhi", "Karan", ""], ["Acharya", "Saurav", ""], ["Rana", "Annie Ibrahim", ""]]}, {"id": "1912.08904", "submitter": "Hamed Zamani", "authors": "Hamed Zamani, Nick Craswell", "title": "Macaw: An Extensible Conversational Information Seeking Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational information seeking (CIS) has been recognized as a major\nemerging research area in information retrieval. Such research will require\ndata and tools, to allow the implementation and study of conversational\nsystems. This paper introduces Macaw, an open-source framework with a modular\narchitecture for CIS research. Macaw supports multi-turn, multi-modal, and\nmixed-initiative interactions, and enables research for tasks such as document\nretrieval, question answering, recommendation, and structured data exploration.\nIt has a modular design to encourage the study of new CIS algorithms, which can\nbe evaluated in batch mode. It can also integrate with a user interface, which\nallows user studies and data collection in an interactive mode, where the back\nend can be fully algorithmic or a wizard of oz setup. Macaw is distributed\nunder the MIT License.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 21:51:22 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Zamani", "Hamed", ""], ["Craswell", "Nick", ""]]}, {"id": "1912.08928", "submitter": "Angelo Salatino", "authors": "Angelo Antonio Salatino", "title": "Early Detection of Research Trends", "comments": "This dissertation is submitted for the Degree of Doctor of Philosophy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to rapidly recognise new research trends is strategic for many\nstakeholders, including universities, institutional funding bodies, academic\npublishers and companies. The literature presents several approaches to\nidentifying the emergence of new research topics, which rely on the assumption\nthat the topic is already exhibiting a certain degree of popularity and\nconsistently referred to by a community of researchers. However, detecting the\nemergence of a new research area at an embryonic stage, i.e., before the topic\nhas been consistently labelled by a community of researchers and associated\nwith a number of publications, is still an open challenge. In this\ndissertation, we begin to address this challenge by performing a study of the\ndynamics preceding the creation of new topics. This study indicates that the\nemergence of a new topic is anticipated by a significant increase in the pace\nof collaboration between relevant research areas, which can be seen as the\n'ancestors' of the new topic. Based on this understanding, we developed Augur,\na novel approach to effectively detecting the emergence of new research topics.\nAugur analyses the diachronic relationships between research areas and is able\nto detect clusters of topics that exhibit dynamics correlated with the\nemergence of new research topics. Here we also present the Advanced Clique\nPercolation Method (ACPM), a new community detection algorithm developed\nspecifically for supporting this task. Augur was evaluated on a gold standard\nof 1,408 debutant topics in the 2000-2011 timeframe and outperformed four\nalternative approaches in terms of both precision and recall.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 20:20:20 GMT"}], "update_date": "2019-12-21", "authors_parsed": [["Salatino", "Angelo Antonio", ""]]}, {"id": "1912.08932", "submitter": "Angelo Loula", "authors": "Rafael Glauber and Angelo Loula", "title": "Collaborative Filtering vs. Content-Based Filtering: differences and\n  similarities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation Systems (SR) suggest items exploring user preferences, helping\nthem with the information overload problem. Two approaches to SR have received\nmore prominence, Collaborative Filtering, and Content-Based Filtering.\nMoreover, even though studies are indicating their advantages and\ndisadvantages, few results empirically prove their characteristics,\nsimilarities, and differences. In this work, an experimental methodology is\nproposed to perform comparisons between recommendation algorithms for different\napproaches going beyond the \"precision of the predictions\". For the\nexperiments, three algorithms of recommendation were tested: a baseline for\nCollaborative Filtration and two algorithms for Content-based Filtering that\nwere developed for this evaluation. The experiments demonstrate the behavior of\nthese systems in different data sets, its main characteristics and especially\nthe complementary aspect of the two main approaches.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 22:28:39 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Glauber", "Rafael", ""], ["Loula", "Angelo", ""]]}, {"id": "1912.08934", "submitter": "Mohammad Reza Zarei", "authors": "Mohammad Reza Zarei, Mohammad R. Moosavi", "title": "An Adaptive Similarity Measure to Tune Trust Influence in Memory-Based\n  Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the recommender systems is to provide relevant and potentially\ninteresting information to each user. This is fulfilled by utilizing the\nalready recorded tendencies of similar users or detecting items similar to\ninterested items of the user. Challenges such as data sparsity and cold start\nproblem are addressed in recent studies. Utilizing social information not only\nenhances the prediction accuracy but also tackles the data sparseness\nchallenges. In this paper, we investigate the impact of using direct and\nindirect trust information in a memory-based collaborative filtering\nrecommender system. An adaptive similarity measure is proposed and the\ncontribution of social information is tuned using two learning schemes, greedy\nand gradient-based optimization. The results of the proposed method are\ncompared with state-of-the-art memory-based and model-based CF approaches on\ntwo real-world datasets, Epinions and FilmTrust. The experiments show that our\nmethod is quite effective in designing an accurate and comprehensive\nrecommender system.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 22:47:47 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Zarei", "Mohammad Reza", ""], ["Moosavi", "Mohammad R.", ""]]}, {"id": "1912.08976", "submitter": "Dong Zhang", "authors": "Dong Zhang, Shu Zhao, Zhen Duan, Jie Chen, Yangping Zhang, Jie Tang", "title": "A multi-label classification method using a hierarchical and transparent\n  representation for paper-reviewer recommendation", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paper-reviewer recommendation task is of significant academic importance for\nconference chairs and journal editors. How to effectively and accurately\nrecommend reviewers for the submitted papers is a meaningful and still tough\ntask. In this paper, we propose a Multi-Label Classification method using a\nhierarchical and transparent Representation named Hiepar-MLC. Further, we\npropose a simple multi-label-based reviewer assignment MLBRA strategy to select\nthe appropriate reviewers. It is interesting that we also explore the\npaper-reviewer recommendation in the coarse-grained granularity.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 01:31:28 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Zhang", "Dong", ""], ["Zhao", "Shu", ""], ["Duan", "Zhen", ""], ["Chen", "Jie", ""], ["Zhang", "Yangping", ""], ["Tang", "Jie", ""]]}, {"id": "1912.09140", "submitter": "Eyal Shulman", "authors": "Eyal Shulman, Lior Wolf", "title": "Meta Decision Trees for Explainable Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of building explainable recommendation systems that are\nbased on a per-user decision tree, with decision rules that are based on single\nattribute values. We build the trees by applying learned regression functions\nto obtain the decision rules as well as the values at the leaf nodes. The\nregression functions receive as input the embedding of the user's training set,\nas well as the embedding of the samples that arrive at the current node. The\nembedding and the regressors are learned end-to-end with a loss that encourages\nthe decision rules to be sparse. By applying our method, we obtain a\ncollaborative filtering solution that provides a direct explanation to every\nrating it provides. With regards to accuracy, it is competitive with other\nalgorithms. However, as expected, explainability comes at a cost and the\naccuracy is typically slightly lower than the state of the art result reported\nin the literature.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 11:45:01 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Shulman", "Eyal", ""], ["Wolf", "Lior", ""]]}, {"id": "1912.09322", "submitter": "Sergio Gast\\'on Burdisso", "authors": "Sergio G. Burdisso, Marcelo Errecalde, Manuel Montes-y-G\\'omez", "title": "PySS3: A Python package implementing a novel text classifier with\n  visualization tools for Explainable AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recently introduced text classifier, called SS3, has obtained\nstate-of-the-art performance on the CLEF's eRisk tasks. SS3 was created to deal\nwith risk detection over text streams and, therefore, not only supports\nincremental training and classification but also can visually explain its\nrationale. However, little attention has been paid to the potential use of SS3\nas a general classifier. We believe this could be due to the unavailability of\nan open-source implementation of SS3. In this work, we introduce PySS3, a\npackage that implements SS3 and also comes with visualization tools that allow\nresearchers to deploy robust, explainable, and trusty machine learning models\nfor text classification.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 16:01:41 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 00:31:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Burdisso", "Sergio G.", ""], ["Errecalde", "Marcelo", ""], ["Montes-y-G\u00f3mez", "Manuel", ""]]}, {"id": "1912.09499", "submitter": "Haozhen Zhao", "authors": "Robert Keeling, Rishi Chhatwal, Nathaniel Huber-Fliflet, Jianping\n  Zhang, Fusheng Wei, Haozhen Zhao, Shi Ye, Han Qin", "title": "Empirical Comparisons of CNN with Other Learning Algorithms for Text\n  Classification in Legal Document Review", "comments": "2019 IEEE International Conference on Big Data (Big Data)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research has shown that Convolutional Neural Networks (CNN) can be\neffectively applied to text classification as part of a predictive coding\nprotocol. That said, most research to date has been conducted on data sets with\nshort documents that do not reflect the variety of documents in real world\ndocument reviews. Using data from four actual reviews with documents of varying\nlengths, we compared CNN with other popular machine learning algorithms for\ntext classification, including Logistic Regression, Support Vector Machine, and\nRandom Forest. For each data set, classification models were trained with\ndifferent training sample sizes using different learning algorithms. These\nmodels were then evaluated using a large randomly sampled test set of\ndocuments, and the results were compared using precision and recall curves. Our\nstudy demonstrates that CNN performed well, but that there was no single\nalgorithm that performed the best across the combination of data sets and\ntraining sample sizes. These results will help advance research into the legal\nprofession's use of machine learning algorithms that maximize performance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 19:06:47 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Keeling", "Robert", ""], ["Chhatwal", "Rishi", ""], ["Huber-Fliflet", "Nathaniel", ""], ["Zhang", "Jianping", ""], ["Wei", "Fusheng", ""], ["Zhao", "Haozhen", ""], ["Ye", "Shi", ""], ["Qin", "Han", ""]]}, {"id": "1912.09501", "submitter": "Haozhen Zhao", "authors": "Christian J. Mahoney, Jianping Zhang, Nathaniel Huber-Fliflet, Peter\n  Gronvall, Haozhen Zhao", "title": "A Framework for Explainable Text Classification in Legal Document Review", "comments": "2019 IEEE International Conference on Big Data (Big Data). arXiv\n  admin note: text overlap with arXiv:1904.01721", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Companies regularly spend millions of dollars producing electronically-stored\ndocuments in legal matters. Recently, parties on both sides of the 'legal\naisle' are accepting the use of machine learning techniques like text\nclassification to cull massive volumes of data and to identify responsive\ndocuments for use in these matters. While text classification is regularly used\nto reduce the discovery costs in legal matters, it also faces a peculiar\nperception challenge: amongst lawyers, this technology is sometimes looked upon\nas a \"black box\", little information provided for attorneys to understand why\ndocuments are classified as responsive. In recent years, a group of AI and ML\nresearchers have been actively researching Explainable AI, in which actions or\ndecisions are human understandable. In legal document review scenarios, a\ndocument can be identified as responsive, if one or more of its text snippets\nare deemed responsive. In these scenarios, if text classification can be used\nto locate these snippets, then attorneys could easily evaluate the model's\nclassification decision. When deployed with defined and explainable results,\ntext classification can drastically enhance overall quality and speed of the\nreview process by reducing the review time. Moreover, explainable predictive\ncoding provides lawyers with greater confidence in the results of that\nsupervised learning task. This paper describes a framework for explainable text\nclassification as a valuable tool in legal services: for enhancing the quality\nand efficiency of legal document review and for assisting in locating\nresponsive snippets within responsive documents. This framework has been\nimplemented in our legal analytics product, which has been used in hundreds of\nlegal matters. We also report our experimental results using the data from an\nactual legal matter that used this type of document review.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 19:07:23 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Mahoney", "Christian J.", ""], ["Zhang", "Jianping", ""], ["Huber-Fliflet", "Nathaniel", ""], ["Gronvall", "Peter", ""], ["Zhao", "Haozhen", ""]]}, {"id": "1912.09519", "submitter": "Chetan Bansal", "authors": "Nikitha Rao, Chetan Bansal, Thomas Zimmermann, Ahmed Hassan Awadallah,\n  Nachiappan Nagappan", "title": "Analyzing Web Search Behavior for Software Engineering Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web search plays an integral role in software engineering (SE) to help with\nvarious tasks such as finding documentation, debugging, installation, etc. In\nthis work, we present the first large-scale analysis of web search behavior for\nSE tasks using the search query logs from Bing, a commercial web search engine.\nFirst, we use distant supervision techniques to build a machine learning\nclassifier to extract the SE search queries with an F1 score of 93%. We then\nperform an analysis on one million search sessions to understand how software\nengineering related queries and sessions differ from other queries and\nsessions. Subsequently, we propose a taxonomy of intents to identify the\nvarious contexts in which web search is used in software engineering. Lastly,\nwe analyze millions of SE queries to understand the distribution, search\nmetrics and trends across these SE search intents. Our analysis shows that SE\nrelated queries form a significant portion of the overall web search traffic.\nAdditionally, we found that there are six major intent categories for which web\nsearch is used in software engineering. The techniques and insights can not\nonly help improve existing tools but can also inspire the development of new\ntools that aid in finding information for SE related tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 19:46:26 GMT"}, {"version": "v2", "created": "Sat, 30 May 2020 22:56:29 GMT"}, {"version": "v3", "created": "Sat, 29 Aug 2020 18:17:21 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Rao", "Nikitha", ""], ["Bansal", "Chetan", ""], ["Zimmermann", "Thomas", ""], ["Awadallah", "Ahmed Hassan", ""], ["Nagappan", "Nachiappan", ""]]}, {"id": "1912.09526", "submitter": "Jeremy Ash", "authors": "Jeremy R. Ash and Jacqueline M. Hughes-Oliver", "title": "Confidence Bands and Hypothesis Test Methods for Recall and Precision\n  Curves at Extremely Small Fractions with Applications to Drug Discovery", "comments": "41 pages, 7 figures, 13 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR cs.LG q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In virtual screening for drug discovery, recall curves are used to assess the\nperformance of ranking algorithms, in which recall is a function of the\nfraction of data prioritized for experimental testing. Unfortunately,\nresearchers almost never consider the uncertainty in the estimation of the\nrecall curve when benchmarking algorithms. We confirm that a recently developed\nprocedure for estimating pointwise confidence intervals for recall curves --\nand closely related variants, such as precision curves -- can be applied to a\nvariety of simulated data sets representative of those typically encountered in\nvirtual screening. Since it is more desirable in benchmarks to present the\nuncertainty of performance over a range of testing fractions, we extend the\npointwise confidence interval procedure to allow for the estimation of\nconfidence bands for these curves. We also present hypothesis test methods to\ndetermine significant differences between the curves for competing algorithms.\nWe show these methods have high power to detect significant differences at a\nrange of small fractions typically tested, while maintaining control of type I\nerror rate. These methods enable statistically rigorous comparisons of virtual\nscreening algorithms using a metric that quantifies the aspect of performance\nthat is of primary interest.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 20:08:03 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Ash", "Jeremy R.", ""], ["Hughes-Oliver", "Jacqueline M.", ""]]}, {"id": "1912.09593", "submitter": "Wei Huang", "authors": "Wei Huang, Richard Yi Da Xu", "title": "Gaussian Process Latent Variable Model Factorization for Context-aware\n  Recommender Systems", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-aware recommender systems (CARS) have gained increasing attention due\nto their ability to utilize contextual information. Compared to traditional\nrecommender systems, CARS are, in general, able to generate more accurate\nrecommendations. Latent factors approach accounts for a large proportion of\nCARS. Recently, a non-linear Gaussian Process (GP) based factorization method\nwas proven to outperform the state-of-the-art methods in CARS. Despite its\neffectiveness, GP model-based methods can suffer from over-fitting and may not\nbe able to determine the impact of each context automatically. In order to\naddress such shortcomings, we propose a Gaussian Process Latent Variable Model\nFactorization (GPLVMF) method, where we apply an appropriate prior to the\noriginal GP model. Our work is primarily inspired by the Gaussian Process\nLatent Variable Model (GPLVM), which was a non-linear dimensionality reduction\nmethod. As a result, we improve the performance on the real datasets\nsignificantly as well as capturing the importance of each context. In addition\nto the general advantages, our method provides two main contributions regarding\nrecommender system settings: (1) addressing the influence of bias by setting a\nnon-zero mean function, and (2) utilizing real-valued contexts by fixing the\nlatent space with real values.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 23:57:55 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Huang", "Wei", ""], ["Da Xu", "Richard Yi", ""]]}, {"id": "1912.09910", "submitter": "Bhaskar Mitra", "authors": "Laura Dietz, Bhaskar Mitra, Jeremy Pickens, Hana Anber, Sandeep Avula,\n  Asia Biega, Adrian Boteanu, Shubham Chatterjee, Jeff Dalton, Shiri\n  Dori-Hacohen, John Foley, Henry Feild, Ben Gamari, Rosie Jones, Pallika\n  Kanani, Sumanta Kashyapi, Widad Machmouchi, Matthew Mitsui, Steve Nole,\n  Alexandre Tachard Passos, Jordan Ramsdell, Adam Roegiest, David Smith and\n  Alessandro Sordoni", "title": "Report on the First HIPstIR Workshop on the Future of Information\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vision of HIPstIR is that early stage information retrieval (IR)\nresearchers get together to develop a future for non-mainstream ideas and\nresearch agendas in IR. The first iteration of this vision materialized in the\nform of a three day workshop in Portsmouth, New Hampshire attended by 24\nresearchers across academia and industry. Attendees pre-submitted one or more\ntopics that they want to pitch at the meeting. Then over the three days during\nthe workshop, we self-organized into groups and worked on six specific\nproposals of common interest. In this report, we present an overview of the\nworkshop and brief summaries of the six proposals that resulted from the\nworkshop.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 16:18:07 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Dietz", "Laura", ""], ["Mitra", "Bhaskar", ""], ["Pickens", "Jeremy", ""], ["Anber", "Hana", ""], ["Avula", "Sandeep", ""], ["Biega", "Asia", ""], ["Boteanu", "Adrian", ""], ["Chatterjee", "Shubham", ""], ["Dalton", "Jeff", ""], ["Dori-Hacohen", "Shiri", ""], ["Foley", "John", ""], ["Feild", "Henry", ""], ["Gamari", "Ben", ""], ["Jones", "Rosie", ""], ["Kanani", "Pallika", ""], ["Kashyapi", "Sumanta", ""], ["Machmouchi", "Widad", ""], ["Mitsui", "Matthew", ""], ["Nole", "Steve", ""], ["Passos", "Alexandre Tachard", ""], ["Ramsdell", "Jordan", ""], ["Roegiest", "Adam", ""], ["Smith", "David", ""], ["Sordoni", "Alessandro", ""]]}, {"id": "1912.10011", "submitter": "Cl\\'ement Rebuffel", "authors": "Cl\\'ement Rebuffel and Laure Soulier and Geoffrey Scoutheeten and\n  Patrick Gallinari", "title": "A Hierarchical Model for Data-to-Text Generation", "comments": "Accepted at the 42nd European Conference on IR Research, ECIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcribing structured data into natural language descriptions has emerged\nas a challenging task, referred to as \"data-to-text\". These structures\ngenerally regroup multiple elements, as well as their attributes. Most attempts\nrely on translation encoder-decoder methods which linearize elements into a\nsequence. This however loses most of the structure contained in the data. In\nthis work, we propose to overpass this limitation with a hierarchical model\nthat encodes the data-structure at the element-level and the structure level.\nEvaluations on RotoWire show the effectiveness of our model w.r.t. qualitative\nand quantitative metrics.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 18:41:32 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Rebuffel", "Cl\u00e9ment", ""], ["Soulier", "Laure", ""], ["Scoutheeten", "Geoffrey", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1912.10068", "submitter": "Sarah Dean", "authors": "Sarah Dean, Sarah Rich, Benjamin Recht", "title": "Recommendations and User Agency: The Reachability of\n  Collaboratively-Filtered Information", "comments": "appeared at FAccT '20", "journal-ref": null, "doi": "10.1145/3351095.3372866", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems often rely on models which are trained to maximize\naccuracy in predicting user preferences. When the systems are deployed, these\nmodels determine the availability of content and information to different\nusers. The gap between these objectives gives rise to a potential for\nunintended consequences, contributing to phenomena such as filter bubbles and\npolarization. In this work, we consider directly the information availability\nproblem through the lens of user recourse. Using ideas of reachability, we\npropose a computationally efficient audit for top-$N$ linear recommender\nmodels. Furthermore, we describe the relationship between model complexity and\nthe effort necessary for users to exert control over their recommendations. We\nuse this insight to provide a novel perspective on the user cold-start problem.\nFinally, we demonstrate these concepts with an empirical investigation of a\nstate-of-the-art model trained on a widely used movie ratings dataset.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 19:23:05 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 01:23:50 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Dean", "Sarah", ""], ["Rich", "Sarah", ""], ["Recht", "Benjamin", ""]]}, {"id": "1912.10162", "submitter": "Stavros Vologiannidis", "authors": "Eleni Partalidou, Eleftherios Spyromitros-Xioufis, Stavros Doropoulos,\n  Stavros Vologiannidis, Konstantinos I. Diamantaras", "title": "Design and implementation of an open source Greek POS Tagger and Entity\n  Recognizer using spaCy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes a machine learning approach to part-of-speech tagging and\nnamed entity recognition for Greek, focusing on the extraction of morphological\nfeatures and classification of tokens into a small set of classes for named\nentities. The architecture model that was used is introduced. The greek version\nof the spaCy platform was added into the source code, a feature that did not\nexist before our contribution, and was used for building the models.\nAdditionally, a part of speech tagger was trained that can detect the\nmorphology of the tokens and performs higher than the state-of-the-art results\nwhen classifying only the part of speech. For named entity recognition using\nspaCy, a model that extends the standard ENAMEX type (organization, location,\nperson) was built. Certain experiments that were conducted indicate the need\nfor flexibility in out-of-vocabulary words and there is an effort for resolving\nthis issue. Finally, the evaluation results are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 13:29:27 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Partalidou", "Eleni", ""], ["Spyromitros-Xioufis", "Eleftherios", ""], ["Doropoulos", "Stavros", ""], ["Vologiannidis", "Stavros", ""], ["Diamantaras", "Konstantinos I.", ""]]}, {"id": "1912.10170", "submitter": "Joeran Beel", "authors": "Dominika Tkaczyk, Andrew Collins, Joeran Beel", "title": "Na\\\"iveRole: Author-Contribution Extraction and Parsing from Biomedical\n  Manuscripts", "comments": "arXiv admin note: substantial text overlap with arXiv:1802.01174", "journal-ref": "27th AIAI Irish Conference on Artificial Intelligence and\n  Cognitive Science, 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information about the contributions of individual authors to scientific\npublications is important for assessing authors' achievements. Some biomedical\npublications have a short section that describes authors' roles and\ncontributions. It is usually written in natural language and hence author\ncontributions cannot be trivially extracted in a machine-readable format. In\nthis paper, we present 1) A statistical analysis of roles in author\ncontributions sections, and 2) Na\\\"iveRole, a novel approach to extract\nstructured authors' roles from author contribution sections. For the first\npart, we used co-clustering techniques, as well as Open Information Extraction,\nto semi-automatically discover the popular roles within a corpus of 2,000\ncontributions sections from PubMed Central. The discovered roles were used to\nautomatically build a training set for Na\\\"iveRole, our role extractor\napproach, based on Na\\\"ive Bayes. Na\\\"iveRole extracts roles with a\nmicro-averaged precision of 0.68, recall of 0.48 and F1 of 0.57. It is, to the\nbest of our knowledge, the first attempt to automatically extract author roles\nfrom research papers. This paper is an extended version of a previous poster\npublished at JCDL 2018.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 14:37:06 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Tkaczyk", "Dominika", ""], ["Collins", "Andrew", ""], ["Beel", "Joeran", ""]]}, {"id": "1912.10204", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Carolyn Penstein Rose", "title": "A Machine Learning Framework for Authorship Identification From Texts", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship identification is a process in which the author of a text is\nidentified. Most known literary texts can easily be attributed to a certain\nauthor because they are, for example, signed. Yet sometimes we find unfinished\npieces of work or a whole bunch of manuscripts with a wide variety of possible\nauthors. In order to assess the importance of such a manuscript, it is vital to\nknow who wrote it. In this work, we aim to develop a machine learning framework\nto effectively determine authorship. We formulate the task as a single-label\nmulti-class text categorization problem and propose a supervised machine\nlearning framework incorporating stylometric features. This task is highly\ninterdisciplinary in that it takes advantage of machine learning, information\nretrieval, and natural language processing. We present an approach and a model\nwhich learns the differences in writing style between $50$ different authors\nand is able to predict the author of a new text with high accuracy. The\naccuracy is seen to increase significantly after introducing certain linguistic\nstylometric features along with text features.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 05:47:58 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Rose", "Carolyn Penstein", ""]]}, {"id": "1912.10554", "submitter": "Mohammad Aliannejadi", "authors": "Mohammad Aliannejadi, Manajit Chakraborty, Esteban Andr\\'es R\\'issola,\n  Fabio Crestani", "title": "Harnessing Evolution of Multi-Turn Conversations for Effective Answer\n  Retrieval", "comments": "To appear in ACM CHIIR 2020, Vancouver, BC, Canada", "journal-ref": null, "doi": "10.1145/3343413.3377968", "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the improvements in speech recognition and voice generation technologies\nover the last years, a lot of companies have sought to develop conversation\nunderstanding systems that run on mobile phones or smart home devices through\nnatural language interfaces. Conversational assistants, such as Google\nAssistant and Microsoft Cortana, can help users to complete various types of\ntasks. This requires an accurate understanding of the user's information need\nas the conversation evolves into multiple turns. Finding relevant context in a\nconversation's history is challenging because of the complexity of natural\nlanguage and the evolution of a user's information need. In this work, we\npresent an extensive analysis of language, relevance, dependency of user\nutterances in a multi-turn information-seeking conversation. To this aim, we\nhave annotated relevant utterances in the conversations released by the TREC\nCaST 2019 track. The annotation labels determine which of the previous\nutterances in a conversation can be used to improve the current one.\nFurthermore, we propose a neural utterance relevance model based on BERT\nfine-tuning, outperforming competitive baselines. We study and compare the\nperformance of multiple retrieval models, utilizing different strategies to\nincorporate the user's context. The experimental results on both classification\nand retrieval tasks show that our proposed approach can effectively identify\nand incorporate the conversation context. We show that processing the current\nutterance using the predicted relevant utterance leads to a 38% relative\nimprovement in terms of nDCG@20. Finally, to foster research in this area, we\nhave released the dataset of the annotations.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 22:39:04 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 15:41:03 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Aliannejadi", "Mohammad", ""], ["Chakraborty", "Manajit", ""], ["R\u00edssola", "Esteban Andr\u00e9s", ""], ["Crestani", "Fabio", ""]]}, {"id": "1912.10816", "submitter": "Rene Haberland", "authors": "Ren\\'e Haberland", "title": "Narrowing Down XML Template Expansion and Schema Validation", "comments": "46 pages, 12 figures, 2 appendices", "journal-ref": "Master Thesis, 2007", "doi": null, "report-no": null, "categories": "cs.LO cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines how much template instantiation can narrow down schema\nvalidation for XML-documents. First, instantiation and validation are\nformalised. Properties towards their practical meaning are probed, an\nimplementation is developed. Requirements for an unification are elaborated and\na comparison is taken out. The semantics are formulated in terms of\ndenotational semantics as well as rule-based referring to the data models\nchosen. Formalisation makes it clearer instantiation is adequately represented.\nBoth semantics show, that the rules set for both, instantiation and validation,\ncannot totally be unified. However, reuse of simplified code also simplifies\nunification. Implementation allows unification of both processes on\ndocument-level. The validity of all implementations is guaranteed by a\ncomprehensive test suite. Analysis shows the minimal XML template language has\ngot regular grammar properties, except macros. An explanation was given, why\nfilters and arrows are not best, especially towards a unified language to be\nvariable and extensive. Recommendations for future language design are\nprovided. Instantiation shows a universal gap in applications, for instance, as\nseen by XSLT. Lack of expressibility of arbitrary functions in a schema is one\nsuch example, expressibility of the command language is another basic\nrestriction. Useful unification constraints are found out to be handy, such as\ntyping each slot. In order to obtain most flexibility out of command languages\nadaptations are required. An alternative to introducing constraints is the\neffective construction of special NFAs. Comparison criteria are introduced\nregarding mainly syntax and semantics. Comparisons is done accordingly. Despite\nits huge syntax definitions XSD was found weaker than RelaxNG or XML template\nlanguage. As template language the latter is considered universal.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 18:28:57 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 19:26:25 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Haberland", "Ren\u00e9", ""]]}, {"id": "1912.10817", "submitter": "Rene Haberland", "authors": "Ren\\'e Haberland", "title": "Using Prolog for Transforming XML Documents", "comments": "49 pages, 54 figures, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.IR cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Proponents of the programming language Prolog share the opinion Prolog is\nmore appropriate for transforming XML-documents as other well-established\ntechniques and languages like XSLT. In order to clarify this position this work\nproposes a tuProlog-styled interpreter for parsing XML-documents into\nProlog-internal lists and vice versa for serialising lists into XML-documents.\nBased on this implementation a comparison between XSLT and Prolog follows.\nFirst, criteria are researched, such as considered language features of XSLT,\nusability and expressibility. These criteria are validated. Second, it is\nassessed when Prolog distinguishes between input and output parameters towards\nreversible transformation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 18:24:17 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 10:40:40 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Haberland", "Ren\u00e9", ""]]}, {"id": "1912.10822", "submitter": "Jithin James", "authors": "Jithin James", "title": "DeepHashing using TripletLoss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing is one of the most efficient techniques for approximate nearest\nneighbour search for large scale image retrieval. Most of the techniques are\nbased on hand-engineered features and do not give optimal results all the time.\nDeep Convolutional Neural Networks have proven to generate very effective\nrepresentation of images that are used for various computer vision tasks and\ninspired by this there have been several Deep Hashing models like Wang et al.\n(2016) have been proposed. These models train on the triplet loss function\nwhich can be used to train models with superior representation capabilities.\nTaking the latest advancements in training using the triplet loss I propose new\ntechniques that help the Deep Hash-ing models train more faster and\nefficiently. Experiment result1show that using the more efficient techniques\nfor training on the triplet loss, we have obtained a 5%percent improvement in\nour model compared to the original work of Wang et al.(2016). Using a larger\nmodel and more training data we can drastically improve the performance using\nthe techniques we propose\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 18:17:48 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["James", "Jithin", ""]]}, {"id": "1912.10846", "submitter": "Qingyu Chen", "authors": "Qingyu Chen, Kyubum Lee, Shankai Yan, Sun Kim, Chih-Hsuan Wei, and\n  Zhiyong Lu", "title": "BioConceptVec: creating and evaluating literature-based biomedical\n  concept embeddings on a large scale", "comments": "33 pages, 6 figures, 7 tables, accepted by PLOS Computational Biology", "journal-ref": null, "doi": "10.1371/journal.pcbi.1007617", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the semantics of related biological concepts, such as genes and\nmutations, is of significant importance to many research tasks in computational\nbiology such as protein-protein interaction detection, gene-drug association\nprediction, and biomedical literature-based discovery. Here, we propose to\nleverage state-of-the-art text mining tools and machine learning models to\nlearn the semantics via vector representations (aka. embeddings) of over\n400,000 biological concepts mentioned in the entire PubMed abstracts. Our\nlearned embeddings, namely BioConceptVec, can capture related concepts based on\ntheir surrounding contextual information in the literature, which is beyond\nexact term match or co-occurrence-based methods. BioConceptVec has been\nthoroughly evaluated in multiple bioinformatics tasks consisting of over 25\nmillion instances from nine different biological datasets. The evaluation\nresults demonstrate that BioConceptVec has better performance than existing\nmethods in all tasks. Finally, BioConceptVec is made freely available to the\nresearch community and general public via\nhttps://github.com/ncbi-nlp/BioConceptVec.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 14:46:46 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Chen", "Qingyu", ""], ["Lee", "Kyubum", ""], ["Yan", "Shankai", ""], ["Kim", "Sun", ""], ["Wei", "Chih-Hsuan", ""], ["Lu", "Zhiyong", ""]]}, {"id": "1912.11160", "submitter": "Ilya Shenbin", "authors": "Ilya Shenbin, Anton Alekseev, Elena Tutubalina, Valentin Malykh,\n  Sergey I. Nikolenko", "title": "RecVAE: a New Variational Autoencoder for Top-N Recommendations with\n  Implicit Feedback", "comments": "In The Thirteenth ACM International Conference on Web Search and Data\n  Mining (WSDM '20), February 3-7, 2020, Houston, TX, USA. ACM, New York, NY,\n  USA, 9 pages", "journal-ref": null, "doi": "10.1145/3336191.3371831", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown the advantages of using autoencoders based on deep\nneural networks for collaborative filtering. In particular, the recently\nproposed Mult-VAE model, which used the multinomial likelihood variational\nautoencoders, has shown excellent results for top-N recommendations. In this\nwork, we propose the Recommender VAE (RecVAE) model that originates from our\nresearch on regularization techniques for variational autoencoders. RecVAE\nintroduces several novel ideas to improve Mult-VAE, including a novel composite\nprior distribution for the latent codes, a new approach to setting the $\\beta$\nhyperparameter for the $\\beta$-VAE framework, and a new approach to training\nbased on alternating updates. In experimental evaluation, we show that RecVAE\nsignificantly outperforms previously proposed autoencoder-based models,\nincluding Mult-VAE and RaCT, across classical collaborative filtering datasets,\nand present a detailed ablation study to assess our new developments. Code and\nmodels are available at https://github.com/ilya-shenbin/RecVAE.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 01:07:08 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Shenbin", "Ilya", ""], ["Alekseev", "Anton", ""], ["Tutubalina", "Elena", ""], ["Malykh", "Valentin", ""], ["Nikolenko", "Sergey I.", ""]]}, {"id": "1912.11211", "submitter": "Anna Zaitsev", "authors": "Mark Ledwich, Anna Zaitsev", "title": "Algorithmic Extremism: Examining YouTube's Rabbit Hole of Radicalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The role that YouTube and its behind-the-scenes recommendation algorithm\nplays in encouraging online radicalization has been suggested by both\njournalists and academics alike. This study directly quantifies these claims by\nexamining the role that YouTube's algorithm plays in suggesting radicalized\ncontent. After categorizing nearly 800 political channels, we were able to\ndifferentiate between political schemas in order to analyze the algorithm\ntraffic flows out and between each group. After conducting a detailed analysis\nof recommendations received by each channel type, we refute the popular\nradicalization claims. To the contrary, these data suggest that YouTube's\nrecommendation algorithm actively discourages viewers from visiting\nradicalizing or extremist content. Instead, the algorithm is shown to favor\nmainstream media and cable news content over independent YouTube channels with\nslant towards left-leaning or politically neutral channels. Our study thus\nsuggests that YouTube's recommendation algorithm fails to promote inflammatory\nor radicalized content, as previously claimed by several outlets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 05:09:01 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Ledwich", "Mark", ""], ["Zaitsev", "Anna", ""]]}, {"id": "1912.11564", "submitter": "Christine Bauer", "authors": "Markus Schedl and Christine Bauer", "title": "Online Music Listening Culture of Kids and Adolescents: Listening\n  Analysis and Music Recommendation Tailored to the Young", "comments": "4 pages, 1 figure, 1 table, KidRec 2017", "journal-ref": "1st International Workshop on Children and Recommender Systems\n  (KidRec 2017), co-located with 11th ACM Conference on Recommender Systems\n  (RecSys 2017), Como, Italy, 27 August", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we analyze a large dataset of user-generated music listening\nevents from Last.fm, focusing on users aged 6 to 18 years. Our contribution is\ntwo-fold. First, we study the music genre preferences of this young user group\nand analyze these preferences for homogeneity within more fine-grained age\ngroups and with respect to gender and countries. Second, we investigate the\nperformance of a collaborative filtering recommender when tailoring music\nrecommendations to different age groups. We find that doing so improves\nperformance for all user groups up to 18 years, but decreases performance for\nadult users aged 19 years and older.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 23:31:39 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Schedl", "Markus", ""], ["Bauer", "Christine", ""]]}, {"id": "1912.11612", "submitter": "Md Ataur Rahman", "authors": "Rabeya Sadia, Md Ataur Rahman, Md Hanif Seddiqui", "title": "N-gram Statistical Stemmer for Bangla Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stemming is a process that can be utilized to trim inflected words to stem or\nroot form. It is useful for enhancing the retrieval effectiveness, especially\nfor text search in order to solve the mismatch problems. Previous research on\nBangla stemming mostly relied on eliminating multiple suffixes from a solitary\nword through a recursive rule based procedure to recover progressively\napplicable relative root. Our proposed system has enhanced the aforementioned\nexploration by actualizing one of the stemming algorithms called N-gram\nstemming. By utilizing an affiliation measure called dice coefficient, related\nsets of words are clustered depending on their character structure. The\nsmallest word in one cluster may be considered as the stem. We additionally\nanalyzed Affinity Propagation clustering algorithms with coefficient similarity\nas well as with median similarity. Our result indicates N-gram stemming\ntechniques to be effective in general which gave us around 87% accurate\nclusters.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 07:31:44 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Sadia", "Rabeya", ""], ["Rahman", "Md Ataur", ""], ["Seddiqui", "Md Hanif", ""]]}, {"id": "1912.11688", "submitter": "Abhishek Singh", "authors": "Abhishek Kumar Singh, Manish Gupta, Vasudeva Varma", "title": "Unity in Diversity: Learning Distributed Heterogeneous Sentence\n  Representation for Extractive Summarization", "comments": "Accepted in AAAI Conference on Artificial Intelligence, 2018.\n  Retrieved from\n  https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16977", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated multi-document extractive text summarization is a widely studied\nresearch problem in the field of natural language understanding. Such\nextractive mechanisms compute in some form the worthiness of a sentence to be\nincluded into the summary. While the conventional approaches rely on human\ncrafted document-independent features to generate a summary, we develop a\ndata-driven novel summary system called HNet, which exploits the various\nsemantic and compositional aspects latent in a sentence to capture document\nindependent features. The network learns sentence representation in a way that,\nsalient sentences are closer in the vector space than non-salient sentences.\nThis semantic and compositional feature vector is then concatenated with the\ndocument-dependent features for sentence ranking. Experiments on the DUC\nbenchmark datasets (DUC-2001, DUC-2002 and DUC-2004) indicate that our model\nshows significant performance gain of around 1.5-2 points in terms of ROUGE\nscore compared with the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 16:25:29 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Singh", "Abhishek Kumar", ""], ["Gupta", "Manish", ""], ["Varma", "Vasudeva", ""]]}, {"id": "1912.11701", "submitter": "Abhishek Singh", "authors": "Abhishek Kumar Singh, Manish Gupta, Vasudeva Varma", "title": "Hybrid MemNet for Extractive Summarization", "comments": "Accepted in CIKM '17 Proceedings of the 2017 ACM on Conference on\n  Information and Knowledge Management", "journal-ref": "In Proceedings of the 2017 ACM on Conference on Information and\n  Knowledge Management (CIKM '17). ACM, New York, NY, USA, pages 2303-2306", "doi": "10.1145/3132847.3133127", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extractive text summarization has been an extensive research problem in the\nfield of natural language understanding. While the conventional approaches rely\nmostly on manually compiled features to generate the summary, few attempts have\nbeen made in developing data-driven systems for extractive summarization. To\nthis end, we present a fully data-driven end-to-end deep network which we call\nas Hybrid MemNet for single document summarization task. The network learns the\ncontinuous unified representation of a document before generating its summary.\nIt jointly captures local and global sentential information along with the\nnotion of summary worthy sentences. Experimental results on two different\ncorpora confirm that our model shows significant performance gains compared\nwith the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 17:48:09 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Singh", "Abhishek Kumar", ""], ["Gupta", "Manish", ""], ["Varma", "Vasudeva", ""]]}, {"id": "1912.11720", "submitter": "Qing Ping", "authors": "Qing Ping, Chaomei Chen", "title": "Convolutional Quantum-Like Language Model with Mutual-Attention for\n  Product Rating Prediction", "comments": "Accepted at MAISON workshop at ICTIR 19'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are designed to help mitigate information overload users\nexperience during online shopping. Recent work explores neural language models\nto learn user and item representations from user reviews and combines such\nrepresentations with rating information. Most existing convolutional-based\nneural models take pooling immediately after convolution and loses the\ninteraction information between the latent dimension of convolutional feature\nvectors along the way. Moreover, these models usually take all feature vectors\nat higher levels as equal and do not take into consideration that some features\nare more relevant to this specific user-item context. To bridge these gaps,\nthis paper proposes a convolutional quantum-like language model with\nmutual-attention for rating prediction (ConQAR). By introducing a quantum-like\ndensity matrix layer, interactions between latent dimensions of convolutional\nfeature vectors are well captured. With the attention weights learned from the\nmutual-attention layer, final representations of a user and an item absorb\ninformation from both itself and its counterparts for making rating prediction.\nExperiments on two large datasets show that our model outperforms multiple\nstate-of-the-art CNN-based models. We also perform an ablation test to analyze\nthe independent effects of the two components of our model. Moreover, we\nconduct a case study and present visualizations of the quantum probabilistic\ndistributions in one user and one item review document to show that the learned\ndistributions capture meaningful information about this user and item, and can\nbe potentially used as textual profiling of the user and item.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 22:01:59 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ping", "Qing", ""], ["Chen", "Chaomei", ""]]}, {"id": "1912.11730", "submitter": "Chen Ma", "authors": "Chen Ma, Liheng Ma, Yingxue Zhang, Jianing Sun, Xue Liu and Mark\n  Coates", "title": "Memory Augmented Graph Neural Networks for Sequential Recommendation", "comments": "Accepted by the 34th AAAI Conference on Artificial Intelligence (AAAI\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chronological order of user-item interactions can reveal time-evolving\nand sequential user behaviors in many recommender systems. The items that users\nwill interact with may depend on the items accessed in the past. However, the\nsubstantial increase of users and items makes sequential recommender systems\nstill face non-trivial challenges: (1) the hardness of modeling the short-term\nuser interests; (2) the difficulty of capturing the long-term user interests;\n(3) the effective modeling of item co-occurrence patterns. To tackle these\nchallenges, we propose a memory augmented graph neural network (MA-GNN) to\ncapture both the long- and short-term user interests. Specifically, we apply a\ngraph neural network to model the item contextual information within a\nshort-term period and utilize a shared memory network to capture the long-range\ndependencies between items. In addition to the modeling of user interests, we\nemploy a bilinear function to capture the co-occurrence patterns of related\nitems. We extensively evaluate our model on five real-world datasets, comparing\nwith several state-of-the-art methods and using a variety of performance\nmetrics. The experimental results demonstrate the effectiveness of our model\nfor the task of Top-K sequential recommendation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 00:15:42 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Ma", "Chen", ""], ["Ma", "Liheng", ""], ["Zhang", "Yingxue", ""], ["Sun", "Jianing", ""], ["Liu", "Xue", ""], ["Coates", "Mark", ""]]}, {"id": "1912.12135", "submitter": "Duhwan Mun", "authors": "Hyungki Kim and Duhwan Mun", "title": "Deep-learning-based classification and retrieval of components of a\n  process plant from segmented point clouds", "comments": "31 pages, 7305 words, 16 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology to recognize the type of component represented by a point cloud is\nrequired in the reconstruction process of an as-built model of a process plant\nbased on laser scanning. The reconstruction process of a process plant through\nlaser scanning is divided into point cloud registration, point cloud\nsegmentation, and component type recognition and placement. Loss of shape data\nor imbalance of point cloud density problems generally occur in the point cloud\ndata collected from large-scale facilities. In this study, we experimented with\nthe possibility of applying object recognition technology based on 3D deep\nlearning networks, which have been showing high performance recently, and\nanalyzed the results. For training data, we used a segmented point cloud\nrepository about components that we constructed by scanning a process plant.\nFor networks, we selected the multi-view convolutional neural network (MVCNN),\nwhich is a view-based method, and PointNet, which is designed to allow the\ndirect input of point cloud data. In the case of the MVCNN, we also performed\nan experiment on the generation method for two types of multi-view images that\ncan complement the shape occlusion of the segmented point cloud. In this\nexperiment, the MVCNN showed the highest retrieval accuracy of approximately\n87%, whereas PointNet showed the highest retrieval mean average precision of\napproximately 84%. Furthermore, both networks showed high recognition\nperformance for the segmented point cloud of plant components when there was\nsufficient training data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 01:34:28 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Kim", "Hyungki", ""], ["Mun", "Duhwan", ""]]}, {"id": "1912.12282", "submitter": "Surya Kallumadi", "authors": "Jon Degenhardt, Surya Kallumadi, Utkarsh Porwal, and Andrew Trotman", "title": "Report on the SIGIR 2019 Workshop on eCommerce (ECOM19)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SIGIR 2019 Workshop on eCommerce (ECOM19), was a full day workshop that\ntook place on Thursday, July 25, 2019 in Paris, France. The purpose of the\nworkshop was to serve as a platform for publication and discussion of\nInformation Retrieval and NLP research and their applications in the domain of\neCommerce. The workshop program was designed to bring together practitioners\nand researchers from academia and industry to discuss the challenges and\napproaches to product search and recommendation in the eCommerce domain. A\nsecond goal was to run a data challenge on real-world eCommerce data. The\nworkshop drew contributions from both industry as well as academia, in total\nthe workshop received 38 submissions, and accepted 24 (63%). There were two\nkeynotes by invited speakers, a poster session where all the accepted\nsubmissions were presented, a panel discussion, and three short talks by\ninvited speakers.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 18:46:52 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Degenhardt", "Jon", ""], ["Kallumadi", "Surya", ""], ["Porwal", "Utkarsh", ""], ["Trotman", "Andrew", ""]]}, {"id": "1912.12333", "submitter": "Benyou Wang", "authors": "Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang,\n  Jakob Grue Simonsen", "title": "Encoding word order in complex embeddings", "comments": "15 pages, 3 figures, ICLR 2020 spotlight paper. A typo on Ablation\n  Table was revised thanks to Jingquan Zeng from SCUT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential word order is important when processing text. Currently, neural\nnetworks (NNs) address this by modeling word position using position\nembeddings. The problem is that position embeddings capture the position of\nindividual words, but not the ordered relationship (e.g., adjacency or\nprecedence) between individual word positions. We present a novel and\nprincipled solution for modeling both the global absolute positions of words\nand their order relationships. Our solution generalizes word embeddings,\npreviously defined as independent vectors, to continuous word functions over a\nvariable (position). The benefit of continuous functions over variable\npositions is that word representations shift smoothly with increasing\npositions. Hence, word representations in different positions can correlate\nwith each other in a continuous function. The general solution of these\nfunctions is extended to complex-valued domain due to richer representations.\nWe extend CNN, RNN and Transformer NNs to complex-valued versions to\nincorporate our complex embedding (we make all code available). Experiments on\ntext classification, machine translation and language modeling show gains over\nboth classical word embeddings and position-enriched word embeddings. To our\nknowledge, this is the first work in NLP to link imaginary numbers in\ncomplex-valued representations to concrete meanings (i.e., word order).\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 20:59:38 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 04:37:07 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Wang", "Benyou", ""], ["Zhao", "Donghao", ""], ["Lioma", "Christina", ""], ["Li", "Qiuchi", ""], ["Zhang", "Peng", ""], ["Simonsen", "Jakob Grue", ""]]}, {"id": "1912.12398", "submitter": "Liang Yile", "authors": "Tieyun Qian, Yile Liang, Qing Li", "title": "Solving Cold Start Problem in Recommendation with Attribute Graph Neural\n  Networks", "comments": "Accepted by TKDE 2020, https://ieeexplore.ieee.org/document/9261110", "journal-ref": null, "doi": "10.1109/TKDE.2020.3038234", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion is a classic problem underlying recommender systems. It is\ntraditionally tackled with matrix factorization. Recently, deep learning based\nmethods, especially graph neural networks, have made impressive progress on\nthis problem. Despite their effectiveness, existing methods focus on modeling\nthe user-item interaction graph. The inherent drawback of such methods is that\ntheir performance is bound to the density of the interactions, which is however\nusually of high sparsity. More importantly, for a cold start user/item that\ndoes not have any interactions, such methods are unable to learn the preference\nembedding of the user/item since there is no link to this user/item in the\ngraph. In this work, we develop a novel framework Attribute Graph Neural\nNetworks (AGNN) by exploiting the attribute graph rather than the commonly used\ninteraction graph. This leads to the capability of learning embeddings for cold\nstart users/items. Our AGNN can produce the preference embedding for a cold\nuser/item by learning on the distribution of attributes with an extended\nvariational auto-encoder structure. Moreover, we propose a new graph neural\nnetwork variant, i.e., gated-GNN, to effectively aggregate various attributes\nof different modalities in a neighborhood. Empirical results on two real-world\ndatasets demonstrate that our model yields significant improvements for cold\nstart recommendations and outperforms or matches state-of-the-arts performance\nin the warm start scenario.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 04:07:55 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 14:01:00 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 06:36:08 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Qian", "Tieyun", ""], ["Liang", "Yile", ""], ["Li", "Qing", ""]]}, {"id": "1912.12481", "submitter": "Prakhar Gupta", "authors": "Ali Sabet, Prakhar Gupta, Jean-Baptiste Cordonnier, Robert West,\n  Martin Jaggi", "title": "Robust Cross-lingual Embeddings from Parallel Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in cross-lingual word embeddings have primarily relied on\nmapping-based methods, which project pretrained word embeddings from different\nlanguages into a shared space through a linear transformation. However, these\napproaches assume word embedding spaces are isomorphic between different\nlanguages, which has been shown not to hold in practice (S{\\o}gaard et al.,\n2018), and fundamentally limits their performance. This motivates investigating\njoint learning methods which can overcome this impediment, by simultaneously\nlearning embeddings across languages via a cross-lingual term in the training\nobjective. We propose a bilingual extension of the CBOW method which leverages\nsentence-aligned corpora to obtain robust cross-lingual word and sentence\nrepresentations. Our approach significantly improves cross-lingual sentence\nretrieval performance over all other approaches while maintaining parity with\nthe current state-of-the-art methods on word-translation. It also achieves\nparity with a deep RNN method on a zero-shot cross-lingual document\nclassification task, requiring far fewer computational resources for training\nand inference. As an additional advantage, our bilingual method leads to a much\nmore pronounced improvement in the the quality of monolingual word vectors\ncompared to other competing methods.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 16:18:33 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 17:02:33 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Sabet", "Ali", ""], ["Gupta", "Prakhar", ""], ["Cordonnier", "Jean-Baptiste", ""], ["West", "Robert", ""], ["Jaggi", "Martin", ""]]}, {"id": "1912.13023", "submitter": "Yun He", "authors": "Yun He, Jianling Wang, Wei Niu and James Caverlee", "title": "A Hierarchical Self-Attentive Model for Recommending User-Generated Item\n  Lists", "comments": "Accepted by CIKM 2019", "journal-ref": "CIKM 2019", "doi": "10.1145/3357384.3358030", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User-generated item lists are a popular feature of many different platforms.\nExamples include lists of books on Goodreads, playlists on Spotify and YouTube,\ncollections of images on Pinterest, and lists of answers on question-answer\nsites like Zhihu. Recommending item lists is critical for increasing user\nengagement and connecting users to new items, but many approaches are designed\nfor the item-based recommendation, without careful consideration of the complex\nrelationships between items and lists. Hence, in this paper, we propose a novel\nuser-generated list recommendation model called AttList. Two unique features of\nAttList are careful modeling of (i) hierarchical user preference, which\naggregates items to characterize the list that they belong to, and then\naggregates these lists to estimate the user preference, naturally fitting into\nthe hierarchical structure of item lists; and (ii) item and list consistency,\nthrough a novel self-attentive aggregation layer designed for capturing the\nconsistency of neighboring items and lists to better model user preference.\nThrough experiments over three real-world datasets reflecting different kinds\nof user-generated item lists, we find that AttList results in significant\nimprovements in NDCG, Precision@k, and Recall@k versus a suite of\nstate-of-the-art baselines. Furthermore, all code and data are available at\nhttps://github.com/heyunh2015/AttList.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 17:34:47 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["He", "Yun", ""], ["Wang", "Jianling", ""], ["Niu", "Wei", ""], ["Caverlee", "James", ""]]}, {"id": "1912.13031", "submitter": "Yun He", "authors": "Yun He, Yin Zhang, Weiwen Liu and James Caverlee", "title": "Consistency-Aware Recommendation for User-Generated ItemList\n  Continuation", "comments": "accepted by WSDM 2020", "journal-ref": "WSDM 2020", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User-generated item lists are popular on many platforms. Examples include\nvideo-based playlists on YouTube, image-based lists (or\"boards\") on Pinterest,\nbook-based lists on Goodreads, and answer-based lists on question-answer forums\nlike Zhihu. As users create these lists, a common challenge is in identifying\nwhat items to curate next. Some lists are organized around particular genres or\ntopics, while others are seemingly incoherent, reflecting individual\npreferences for what items belong together. Furthermore, this heterogeneity in\nitem consistency may vary from platform to platform, and from sub-community to\nsub-community. Hence, this paper proposes a generalizable approach for\nuser-generated item list continuation. Complementary to methods that exploit\nspecific content patterns (e.g., as in song-based playlists that rely on audio\nfeatures), the proposed approach models the consistency of item lists based on\nhuman curation patterns, and so can be deployed across a wide range of varying\nitem types (e.g., videos, images, books). A key contribution is in\nintelligently combining two preference models via a novel consistency-aware\ngating network - a general user preference model that captures a user's overall\ninterests, and a current preference priority model that captures a user's\ncurrent (as of the most recent item) interests. In this way, the proposed\nconsistency-aware recommender can dynamically adapt as user preferences evolve.\nEvaluation over four datasets(of songs, books, and answers) confirms these\nobservations and demonstrates the effectiveness of the proposed model versus\nstate-of-the-art alternatives. Further, all code and data are available at\nhttps://github.com/heyunh2015/ListContinuation_WSDM2020.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 18:00:42 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["He", "Yun", ""], ["Zhang", "Yin", ""], ["Liu", "Weiwen", ""], ["Caverlee", "James", ""]]}, {"id": "1912.13046", "submitter": "Edward Raff", "authors": "Edward Raff, Charles Nicholas, Mark McLean", "title": "A New Burrows Wheeler Transform Markov Distance", "comments": "To appear in: The Thirty-Fourth AAAI Conference on Artificial\n  Intelligence (AAAI-20), AICS-2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work inspired by compression algorithms has described how the Burrows\nWheeler Transform can be used to create a distance measure for bioinformatics\nproblems. We describe issues with this approach that were not widely known, and\nintroduce our new Burrows Wheeler Markov Distance (BWMD) as an alternative. The\nBWMD avoids the shortcomings of earlier efforts, and allows us to tackle\nproblems in variable length DNA sequence clustering. BWMD is also more\nadaptable to other domains, which we demonstrate on malware classification\ntasks. Unlike other compression-based distance metrics known to us, BWMD works\nby embedding sequences into a fixed-length feature vector. This allows us to\nprovide significantly improved clustering performance on larger malware\ncorpora, a weakness of prior methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 18:33:32 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Raff", "Edward", ""], ["Nicholas", "Charles", ""], ["McLean", "Mark", ""]]}, {"id": "1912.13072", "submitter": "Chiyu Zhang", "authors": "Muhammad Abdul-Mageed, Chiyu Zhang, Azadeh Hashemi, El Moatez Billah\n  Nagoudi", "title": "AraNet: A Deep Learning Toolkit for Arabic Social Media", "comments": "Accepted by The 4th Workshop on Open-Source Arabic Corpora and\n  Processing Tools (OSACT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe AraNet, a collection of deep learning Arabic social media\nprocessing tools. Namely, we exploit an extensive host of publicly available\nand novel social media datasets to train bidirectional encoders from\ntransformer models (BERT) to predict age, dialect, gender, emotion, irony, and\nsentiment. AraNet delivers state-of-the-art performance on a number of the\ncited tasks and competitively on others. In addition, AraNet has the advantage\nof being exclusively based on a deep learning framework and hence feature\nengineering free. To the best of our knowledge, AraNet is the first to performs\npredictions across such a wide range of tasks for Arabic NLP and thus meets a\ncritical needs. We publicly release AraNet to accelerate research and\nfacilitate comparisons across the different tasks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 20:05:37 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 18:31:26 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Abdul-Mageed", "Muhammad", ""], ["Zhang", "Chiyu", ""], ["Hashemi", "Azadeh", ""], ["Nagoudi", "El Moatez Billah", ""]]}, {"id": "1912.13080", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Luca Soldaini, Nazli Goharian", "title": "Teaching a New Dog Old Tricks: Resurrecting Multilingual Retrieval Using\n  Zero-shot Learning", "comments": "ECIR 2020 (short)", "journal-ref": null, "doi": "10.1007/978-3-030-45442-5_31", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While billions of non-English speaking users rely on search engines every\nday, the problem of ad-hoc information retrieval is rarely studied for\nnon-English languages. This is primarily due to a lack of data set that are\nsuitable to train ranking algorithms. In this paper, we tackle the lack of data\nby leveraging pre-trained multilingual language models to transfer a retrieval\nsystem trained on English collections to non-English queries and documents. Our\nmodel is evaluated in a zero-shot setting, meaning that we use them to predict\nrelevance scores for query-document pairs in languages never seen during\ntraining. Our results show that the proposed approach can significantly\noutperform unsupervised retrieval techniques for Arabic, Chinese Mandarin, and\nSpanish. We also show that augmenting the English training collection with some\nexamples from the target language can sometimes improve performance.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 20:46:38 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["MacAvaney", "Sean", ""], ["Soldaini", "Luca", ""], ["Goharian", "Nazli", ""]]}, {"id": "1912.13141", "submitter": "Thuan Nguyen", "authors": "Thuan Nguyen and Thinh Nguyen", "title": "Minimizing Impurity Partition Under Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set partitioning is a key component of many algorithms in machine learning,\nsignal processing, and communications. In general, the problem of finding a\npartition that minimizes a given impurity (loss function) is NP-hard. As such,\nthere exists a wealth of literature on approximate algorithms and theoretical\nanalyses of the partitioning problem under different settings. In this paper,\nwe formulate and solve a variant of the partition problem called the minimum\nimpurity partition under constraint (MIPUC). MIPUC finds an optimal partition\nthat minimizes a given loss function under a given concave constraint. MIPUC\ngeneralizes the recently proposed deterministic information bottleneck problem\nwhich finds an optimal partition that maximizes the mutual information between\nthe input and partition output while minimizing the partition output entropy.\nOur proposed algorithm is developed based on a novel optimality condition,\nwhich allows us to find a locally optimal solution efficiently. Moreover, we\nshow that the optimal partition produces a hard partition that is equivalent to\nthe cuts by hyperplanes in the probability space of the posterior probability\nthat finally yields a polynomial time complexity algorithm to find the globally\noptimal partition. Both theoretical and numerical results are provided to\nvalidate the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 01:55:41 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Nguyen", "Thuan", ""], ["Nguyen", "Thinh", ""]]}, {"id": "1912.13377", "submitter": "Alexey Drutsa", "authors": "Alexey Drutsa", "title": "Finite-State Extreme Effect Variable", "comments": "15 pages; 2 figures; 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize to the finite-state case the notion of the extreme effect\nvariable $Y$ that accumulates all the effect of a variant variable $V$ observed\nin changes of another variable $X$. We conduct theoretical analysis and turn\nthe problem of finding of an effect variable into a problem of a simultaneous\ndecomposition of a set of distributions. The states of the extreme effect\nvariable, on the one hand, are minimally affected by the variant variable $V$\nand, on the other hand, are extremely different with respect to the observable\nvariable $X$. We apply our technique to online evaluation of a web search\nengine through A/B testing and show its utility.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 21:40:26 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Drutsa", "Alexey", ""]]}, {"id": "1912.13455", "submitter": "Christoph Treude", "authors": "Sarah Nadi, Christoph Treude", "title": "Essential Sentences for Navigating Stack Overflow Answers", "comments": "to appear as full paper at SANER 2020, the 27th IEEE International\n  Conference on Software Analysis, Evolution and Reengineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stack Overflow (SO) has become an essential resource for software\ndevelopment. Despite its success and prevalence, navigating SO remains a\nchallenge. Ideally, SO users could benefit from highlighted navigational cues\nthat help them decide if an answer is relevant to their task and context. Such\nnavigational cues could be in the form of essential sentences that help the\nsearcher decide whether they want to read the answer or skip over it. In this\npaper, we compare four potential approaches for identifying essential\nsentences. We adopt two existing approaches and develop two new approaches\nbased on the idea that contextual information in a sentence (e.g., \"if using\nwindows\") could help identify essential sentences. We compare the four\ntechniques using a survey of 43 participants. Our participants indicate that it\nis not always easy to figure out what the best solution for their specific\nproblem is, given the options, and that they would indeed like to easily spot\ncontextual information that may narrow down the search. Our quantitative\ncomparison of the techniques shows that there is no single technique sufficient\nfor identifying essential sentences that can serve as navigational cues, while\nour qualitative analysis shows that participants valued explanations and\nspecific conditions, and did not value filler sentences or speculations. Our\nwork sheds light on the importance of navigational cues, and our findings can\nbe used to guide future research to find the best combination of techniques to\nidentify such cues.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 17:52:05 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Nadi", "Sarah", ""], ["Treude", "Christoph", ""]]}]