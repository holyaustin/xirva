[{"id": "1608.00104", "submitter": "Chenguang Wang", "authors": "Chenguang Wang, Yangqiu Song, Dan Roth, Ming Zhang, Jiawei Han", "title": "World Knowledge as Indirect Supervision for Document Clustering", "comments": "33 pages, 53 figures, ACM TKDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key obstacles in making learning protocols realistic in\napplications is the need to supervise them, a costly process that often\nrequires hiring domain experts. We consider the framework to use the world\nknowledge as indirect supervision. World knowledge is general-purpose\nknowledge, which is not designed for any specific domain. Then the key\nchallenges are how to adapt the world knowledge to domains and how to represent\nit for learning. In this paper, we provide an example of using world knowledge\nfor domain dependent document clustering. We provide three ways to specify the\nworld knowledge to domains by resolving the ambiguity of the entities and their\ntypes, and represent the data with world knowledge as a heterogeneous\ninformation network. Then we propose a clustering algorithm that can cluster\nmultiple types and incorporate the sub-type information as constraints. In the\nexperiments, we use two existing knowledge bases as our sources of world\nknowledge. One is Freebase, which is collaboratively collected knowledge about\nentities and their organizations. The other is YAGO2, a knowledge base\nautomatically extracted from Wikipedia and maps knowledge to the linguistic\nknowledge base, WordNet. Experimental results on two text benchmark datasets\n(20newsgroups and RCV1) show that incorporating world knowledge as indirect\nsupervision can significantly outperform the state-of-the-art clustering\nalgorithms as well as clustering algorithms enhanced with world knowledge\nfeatures.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 11:53:04 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Wang", "Chenguang", ""], ["Song", "Yangqiu", ""], ["Roth", "Dan", ""], ["Zhang", "Ming", ""], ["Han", "Jiawei", ""]]}, {"id": "1608.00134", "submitter": "Georgios Drakopoulos II", "authors": "Georgios Drakopoulos, Vasileios Megalooikonomou", "title": "A Graph Framework for Multimodal Medical Information Processing", "comments": "We need to correct certain errors both in the software description as\n  well as in the algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal medical information processing is currently the epicenter of\nintense interdisciplinary research, as proper data fusion may lead to more\naccurate diagnoses. Moreover, multimodality may disambiguate cases of\nco-morbidity. This paper presents a framework for retrieving, analyzing, and\nstoring medical information as a multilayer graph, an abstract format suitable\nfor data fusion and further processing. At the same time, this paper addresses\nthe need for reliable medical information through co-author graph ranking. A\nuse case pertaining to frailty based on Python and Neo4j serves as an\nillustration of the proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 15:42:34 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 15:22:14 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Drakopoulos", "Georgios", ""], ["Megalooikonomou", "Vasileios", ""]]}, {"id": "1608.00147", "submitter": "Joan Figuerola Hurtado", "authors": "Joan Figuerola Hurtado", "title": "Attention Span For Personalisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A click on an item is arguably the most widely used feature in recommender\nsystems. However, a click is one out of 174 events a browser can trigger. This\npaper presents a framework to effectively collect and store data from event\nstreams. A set of mining methods is provided to extract user engagement\nfeatures such as: attention span, scrolling depth and visible impressions. In\nthis work, we present an experiment where recommendations based on attention\nspan drove 340% higher click-through-rate than clicks.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 16:53:05 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Hurtado", "Joan Figuerola", ""]]}, {"id": "1608.00163", "submitter": "Santo Fortunato Prof.", "authors": "Santo Fortunato, Darko Hric", "title": "Community detection in networks: A user guide", "comments": "43 pages, 29 figures, 2 tables, 202 references. Final version\n  published in Physics Reports", "journal-ref": "Physics Reports 659, 1-44 (2016)", "doi": "10.1016/j.physrep.2016.09.002", "report-no": null, "categories": "physics.soc-ph cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Community detection in networks is one of the most popular topics of modern\nnetwork science. Communities, or clusters, are usually groups of vertices\nhaving higher probability of being connected to each other than to members of\nother groups, though other patterns are possible. Identifying communities is an\nill-defined problem. There are no universal protocols on the fundamental\ningredients, like the definition of community itself, nor on other crucial\nissues, like the validation of algorithms and the comparison of their\nperformances. This has generated a number of confusions and misconceptions,\nwhich undermine the progress in the field. We offer a guided tour through the\nmain aspects of the problem. We also point out strengths and weaknesses of\npopular methods, and give directions to their use.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 20:56:22 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 15:52:12 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Fortunato", "Santo", ""], ["Hric", "Darko", ""]]}, {"id": "1608.00276", "submitter": "Jeroen Vuurens", "authors": "Jeroen B. P. Vuurens, Martha Larson, Arjen P. de Vries", "title": "Exploring Deep Space: Learning Personalized Ranking in a Semantic Space", "comments": "6 pages, RecSys 2016 RSDL workshop", "journal-ref": null, "doi": "10.1145/2988450.2988457", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems leverage both content and user interactions to generate\nrecommendations that fit users' preferences. The recent surge of interest in\ndeep learning presents new opportunities for exploiting these two sources of\ninformation. To recommend items we propose to first learn a user-independent\nhigh-dimensional semantic space in which items are positioned according to\ntheir substitutability, and then learn a user-specific transformation function\nto transform this space into a ranking according to the user's past\npreferences. An advantage of the proposed architecture is that it can be used\nto effectively recommend items using either content that describes the items or\nuser-item ratings. We show that this approach significantly outperforms\nstate-of-the-art recommender systems on the MovieLens 1M dataset.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 22:38:46 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 18:43:04 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Vuurens", "Jeroen B. P.", ""], ["Larson", "Martha", ""], ["de Vries", "Arjen P.", ""]]}, {"id": "1608.00329", "submitter": "Sujatha Das Gollapalli", "authors": "Sujatha Das Gollapalli and Xiao-li Li", "title": "Keyphrase Extraction using Sequential Labeling", "comments": "10 pages including 2 pages of references, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrases efficiently summarize a document's content and are used in various\ndocument processing and retrieval tasks. Several unsupervised techniques and\nclassifiers exist for extracting keyphrases from text documents. Most of these\nmethods operate at a phrase-level and rely on part-of-speech (POS) filters for\ncandidate phrase generation. In addition, they do not directly handle\nkeyphrases of varying lengths. We overcome these modeling shortcomings by\naddressing keyphrase extraction as a sequential labeling task in this paper. We\nexplore a basic set of features commonly used in NLP tasks as well as\npredictions from various unsupervised methods to train our taggers. In addition\nto a more natural modeling for the keyphrase extraction problem, we show that\ntagging models yield significant performance benefits over existing\nstate-of-the-art extraction methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 06:00:22 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 03:07:46 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Gollapalli", "Sujatha Das", ""], ["Li", "Xiao-li", ""]]}, {"id": "1608.00758", "submitter": "Christina Lioma Assoc. Prof", "authors": "Christina Lioma and Fabien Tarissan and Jakob Grue Simonsen and Casper\n  Petersen and Birger Larsen", "title": "Exploiting the Bipartite Structure of Entity Grids for Document\n  Coherence and Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document coherence describes how much sense text makes in terms of its\nlogical organisation and discourse flow. Even though coherence is a relatively\ndifficult notion to quantify precisely, it can be approximated automatically.\nThis type of coherence modelling is not only interesting in itself, but also\nuseful for a number of other text processing tasks, including Information\nRetrieval (IR), where adjusting the ranking of documents according to both\ntheir relevance and their coherence has been shown to increase retrieval\neffectiveness [34,37].\n  The state of the art in unsupervised coherence modelling represents documents\nas bipartite graphs of sentences and discourse entities, and then projects\nthese bipartite graphs into one-mode undirected graphs. However, one-mode\nprojections may incur significant loss of the information present in the\noriginal bipartite structure. To address this we present three novel graph\nmetrics that compute document coherence on the original bipartite graph of\nsentences and entities. Evaluation on standard settings shows that: (i) one of\nour coherence metrics beats the state of the art in terms of coherence\naccuracy; and (ii) all three of our coherence metrics improve retrieval\neffectiveness because, as closer analysis reveals, they capture aspects of\ndocument quality that go undetected by both keyword-based standard ranking and\nby spam filtering. This work contributes document coherence metrics that are\ntheoretically principled, parameter-free, and useful to IR.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 10:24:11 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Lioma", "Christina", ""], ["Tarissan", "Fabien", ""], ["Simonsen", "Jakob Grue", ""], ["Petersen", "Casper", ""], ["Larsen", "Birger", ""]]}, {"id": "1608.00788", "submitter": "Christina Lioma Assoc. Prof", "authors": "Brian Brost and Ingemar J. Cox and Yevgeny Seldin and Christina Lioma", "title": "An Improved Multileaving Algorithm for Online Ranker Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online ranker evaluation is a key challenge in information retrieval. An\nimportant task in the online evaluation of rankers is using implicit user\nfeedback for inferring preferences between rankers. Interleaving methods have\nbeen found to be efficient and sensitive, i.e. they can quickly detect even\nsmall differences in quality. It has recently been shown that multileaving\nmethods exhibit similar sensitivity but can be more efficient than interleaving\nmethods. This paper presents empirical results demonstrating that existing\nmultileaving methods either do not scale well with the number of rankers, or,\nmore problematically, can produce results which substantially differ from\nevaluation measures like NDCG. The latter problem is caused by the fact that\nthey do not correctly account for the similarities that can occur between\nrankers being multileaved. We propose a new multileaving method for handling\nthis problem and demonstrate that it substantially outperforms existing\nmethods, in some cases reducing errors by as much as 50%.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 12:29:36 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Brost", "Brian", ""], ["Cox", "Ingemar J.", ""], ["Seldin", "Yevgeny", ""], ["Lioma", "Christina", ""]]}, {"id": "1608.01068", "submitter": "Xiao-Bo Jin", "authors": "Xiao-Bo Jin and Guang-Gang Geng and Kaizhu Huang and Zhi-Wei Yan", "title": "Ranking Entity Based on Both of Word Frequency and Word Sematic Features", "comments": "The paper decribes the apporoaches that help us to achieve the first\n  place in Baidu Cup 2016 NLP Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity search is a new application meeting either precise or vague\nrequirements from the search engines users. Baidu Cup 2016 Challenge just\nprovided such a chance to tackle the problem of the entity search. We achieved\nthe first place with the average MAP scores on 4 tasks including movie, tvShow,\ncelebrity and restaurant. In this paper, we propose a series of similarity\nfeatures based on both of the word frequency features and the word semantic\nfeatures and describe our ranking architecture and experiment details.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 03:49:54 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Jin", "Xiao-Bo", ""], ["Geng", "Guang-Gang", ""], ["Huang", "Kaizhu", ""], ["Yan", "Zhi-Wei", ""]]}, {"id": "1608.01247", "submitter": "Sai Keshav Kolluru", "authors": "S.K Kolluru and Prasenjit Mukherjee", "title": "Query Clustering using Segment Specific Context Embeddings", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel query clustering approach to capture the broad\ninterest areas of users querying search engines. We make use of recent advances\nin NLP - word2vec and extend it to get query2vec, vector representations of\nqueries, based on query contexts, obtained from the top search results for the\nquery and use a highly scalable Divide & Merge clustering algorithm on top of\nthe query vectors, to get the clusters. We have tried this approach on a\nvariety of segments, including Retail, Travel, Health, Phones and found the\nclusters to be effective in discovering user's interest areas which have high\nmonetization potential.\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 16:33:32 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 04:59:48 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Kolluru", "S. K", ""], ["Mukherjee", "Prasenjit", ""]]}, {"id": "1608.01573", "submitter": "Edel Garcia", "authors": "Edel Garcia", "title": "Local Term Weight Models from Power Transformations: Development of\n  BM25IR: A Best Match Model based on Inverse Regression", "comments": "16 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we show how power transformations can be used as a common\nframework for the derivation of local term weights. We found that under some\nparametric conditions, BM25 and inverse regression produce equivalent results.\nAs a special case of inverse regression, we show that the largest increment in\nterm weight occurs when a term is mentioned for the second time. A model based\non inverse regression (BM25IR) is presented. Simulations suggest that BM25IR\nworks fairly well for different BM25 parametric conditions and document\nlengths.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 15:14:04 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Garcia", "Edel", ""]]}, {"id": "1608.01972", "submitter": "Sun Kim", "authors": "Sun Kim, Nicolas Fiorini, W. John Wilbur and Zhiyong Lu", "title": "Bridging the Gap: Incorporating a Semantic Similarity Measure for\n  Effectively Mapping PubMed Queries to Documents", "comments": "10 pages, 1 figure, 3 tables", "journal-ref": null, "doi": "10.1016/j.jbi.2017.09.014", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main approach of traditional information retrieval (IR) is to examine how\nmany words from a query appear in a document. A drawback of this approach,\nhowever, is that it may fail to detect relevant documents where no or only few\nwords from a query are found. The semantic analysis methods such as LSA (latent\nsemantic analysis) and LDA (latent Dirichlet allocation) have been proposed to\naddress the issue, but their performance is not superior compared to common IR\napproaches. Here we present a query-document similarity measure motivated by\nthe Word Mover's Distance. Unlike other similarity measures, the proposed\nmethod relies on neural word embeddings to compute the distance between words.\nThis process helps identify related words when no direct matches are found\nbetween a query and a document. Our method is efficient and straightforward to\nimplement. The experimental results on TREC Genomics data show that our\napproach outperforms the BM25 ranking function by an average of 12% in mean\naverage precision. Furthermore, for a real-world dataset collected from the\nPubMed search logs, we combine the semantic measure with BM25 using a learning\nto rank method, which leads to improved ranking scores by up to 25%. This\nexperiment demonstrates that the proposed approach and BM25 nicely complement\neach other and together produce superior performance.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 18:53:42 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 19:33:57 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Kim", "Sun", ""], ["Fiorini", "Nicolas", ""], ["Wilbur", "W. John", ""], ["Lu", "Zhiyong", ""]]}, {"id": "1608.02021", "submitter": "Yefeng Ruan", "authors": "Yefeng Ruan and Tzu-Chun Lin", "title": "An Integrated Recommender Algorithm for Rating Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender system is currently widely used in many e-commerce systems, such\nas Amazon, eBay, and so on. It aims to help users to find items which they may\nbe interested in. In literature, neighborhood-based collaborative filtering and\nmatrix factorization are two common methods used in recommender systems. In\nthis paper, we combine these two methods with personalized weights on them.\nRather than using fixed weights for these two methods, we assume each user has\nher/his own preference over them. Our results shows that our algorithm\noutperforms neighborhood-based collaborative filtering algorithm, matrix\nfactorization algorithm and their combination with fixed weights.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2016 20:57:37 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Ruan", "Yefeng", ""], ["Lin", "Tzu-Chun", ""]]}, {"id": "1608.02761", "submitter": "Semih Yumusak", "authors": "Semih Yumusak, Erdogan Dogdu, Halife Kodaz, Andreas Kamilaris", "title": "SpEnD: Linked Data SPARQL Endpoints Discovery Using Search Engines", "comments": "This paper has been withdrawn by the author due to a crucial\n  illustration error in Figure 2, critical numerical errors in Table 5 and\n  Table 8", "journal-ref": null, "doi": "10.1587/transinf.2016DAP0025", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a novel metacrawling method is proposed for discovering and\nmonitoring linked data sources on the Web. We implemented the method in a\nprototype system, named SPARQL Endpoints Discovery (SpEnD). SpEnD starts with a\n\"search keyword\" discovery process for finding relevant keywords for the linked\ndata domain and specifically SPARQL endpoints. Then, these search keywords are\nutilized to find linked data sources via popular search engines (Google, Bing,\nYahoo, Yandex). By using this method, most of the currently listed SPARQL\nendpoints in existing endpoint repositories, as well as a significant number of\nnew SPARQL endpoints, have been discovered. Finally, we have developed a new\nSPARQL endpoint crawler (SpEC) for crawling and link analysis.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 11:05:20 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 23:32:06 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Yumusak", "Semih", ""], ["Dogdu", "Erdogan", ""], ["Kodaz", "Halife", ""], ["Kamilaris", "Andreas", ""]]}, {"id": "1608.02904", "submitter": "Jeniya Tabassum", "authors": "Jeniya Tabassum, Alan Ritter, Wei Xu", "title": "TweeTime: A Minimally Supervised Method for Recognizing and Normalizing\n  Time Expressions in Twitter", "comments": "EMNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe TweeTIME, a temporal tagger for recognizing and normalizing time\nexpressions in Twitter. Most previous work in social media analysis has to rely\non temporal resolvers that are designed for well-edited text, and therefore\nsuffer from the reduced performance due to domain mismatch. We present a\nminimally supervised method that learns from large quantities of unlabeled data\nand requires no hand-engineered rules or hand-annotated training corpora.\nTweeTIME achieves 0.68 F1 score on the end-to-end task of resolving date\nexpressions, outperforming a broad range of state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 18:29:04 GMT"}, {"version": "v2", "created": "Sat, 24 Sep 2016 03:54:13 GMT"}, {"version": "v3", "created": "Sat, 1 Oct 2016 19:24:23 GMT"}, {"version": "v4", "created": "Sun, 15 Nov 2020 09:34:25 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Tabassum", "Jeniya", ""], ["Ritter", "Alan", ""], ["Xu", "Wei", ""]]}, {"id": "1608.03544", "submitter": "Claudio Gentile", "authors": "Claudio Gentile, Shuai Li, Purushottam Kar, Alexandros Karatzoglou,\n  Evans Etrue, Giovanni Zappella", "title": "On Context-Dependent Clustering of Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a novel cluster-of-bandit algorithm CAB for collaborative\nrecommendation tasks that implements the underlying feedback sharing mechanism\nby estimating the neighborhood of users in a context-dependent manner. CAB\nmakes sharp departures from the state of the art by incorporating collaborative\neffects into inference as well as learning processes in a manner that\nseamlessly interleaving explore-exploit tradeoffs and collaborative steps. We\nprove regret bounds under various assumptions on the data, which exhibit a\ncrisp dependence on the expected number of clusters over the users, a natural\nmeasure of the statistical difficulty of the learning task. Experiments on\nproduction and real-world datasets show that CAB offers significantly increased\nprediction performance against a representative pool of state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2016 14:13:28 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 17:16:22 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Gentile", "Claudio", ""], ["Li", "Shuai", ""], ["Kar", "Purushottam", ""], ["Karatzoglou", "Alexandros", ""], ["Etrue", "Evans", ""], ["Zappella", "Giovanni", ""]]}, {"id": "1608.03580", "submitter": "Erik Waingarten", "authors": "Alexandr Andoni and Thijs Laarhoven and Ilya Razenshteyn and Erik\n  Waingarten", "title": "Optimal Hashing-based Time-Space Trade-offs for Approximate Near\n  Neighbors", "comments": "62 pages, 5 figures; a merger of arXiv:1511.07527 [cs.DS] and\n  arXiv:1605.02701 [cs.DS], which subsumes both of the preprints. New version\n  contains more elaborated proofs and fixed some typos", "journal-ref": null, "doi": "10.1137/1.9781611974782.4", "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  [See the paper for the full abstract.]\n  We show tight upper and lower bounds for time-space trade-offs for the\n$c$-Approximate Near Neighbor Search problem. For the $d$-dimensional Euclidean\nspace and $n$-point datasets, we develop a data structure with space $n^{1 +\n\\rho_u + o(1)} + O(dn)$ and query time $n^{\\rho_q + o(1)} + d n^{o(1)}$ for\nevery $\\rho_u, \\rho_q \\geq 0$ such that: \\begin{equation} c^2 \\sqrt{\\rho_q} +\n(c^2 - 1) \\sqrt{\\rho_u} = \\sqrt{2c^2 - 1}. \\end{equation}\n  This is the first data structure that achieves sublinear query time and\nnear-linear space for every approximation factor $c > 1$, improving upon\n[Kapralov, PODS 2015]. The data structure is a culmination of a long line of\nwork on the problem for all space regimes; it builds on Spherical\nLocality-Sensitive Filtering [Becker, Ducas, Gama, Laarhoven, SODA 2016] and\ndata-dependent hashing [Andoni, Indyk, Nguyen, Razenshteyn, SODA 2014] [Andoni,\nRazenshteyn, STOC 2015].\n  Our matching lower bounds are of two types: conditional and unconditional.\nFirst, we prove tightness of the whole above trade-off in a restricted model of\ncomputation, which captures all known hashing-based approaches. We then show\nunconditional cell-probe lower bounds for one and two probes that match the\nabove trade-off for $\\rho_q = 0$, improving upon the best known lower bounds\nfrom [Panigrahy, Talwar, Wieder, FOCS 2010]. In particular, this is the first\nspace lower bound (for any static data structure) for two probes which is not\npolynomially smaller than the one-probe bound. To show the result for two\nprobes, we establish and exploit a connection to locally-decodable codes.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 19:50:00 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 16:57:47 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Andoni", "Alexandr", ""], ["Laarhoven", "Thijs", ""], ["Razenshteyn", "Ilya", ""], ["Waingarten", "Erik", ""]]}, {"id": "1608.03811", "submitter": "Joani Mitro", "authors": "Joani Mitro", "title": "Content-based image retrieval tutorial", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper functions as a tutorial for individuals interested to enter the\nfield of information retrieval but wouldn't know where to begin from. It\ndescribes two fundamental yet efficient image retrieval techniques, the first\nbeing k - nearest neighbors (knn) and the second support vector machines(svm).\nThe goal is to provide the reader with both the theoretical and practical\naspects in order to acquire a better understanding. Along with this tutorial we\nhave also developed the equivalent software1 using the MATLAB environment in\norder to illustrate the techniques, so that the reader can have a hands-on\nexperience.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 14:40:46 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Mitro", "Joani", ""]]}, {"id": "1608.03905", "submitter": "Prodromos Malakasiotis", "authors": "Georgios-Ioannis Brokos, Prodromos Malakasiotis and Ion\n  Androutsopoulos", "title": "Using Centroids of Word Embeddings and Word Mover's Distance for\n  Biomedical Document Retrieval in Question Answering", "comments": "5 pages, 4 images, presented at BioNLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a document retrieval method for question answering that represents\ndocuments and questions as weighted centroids of word embeddings and reranks\nthe retrieved documents with a relaxation of Word Mover's Distance. Using\nbiomedical questions and documents from BIOASQ, we show that our method is\ncompetitive with PUBMED. With a top-k approximation, our method is fast, and\neasily portable to other domains and languages.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 20:33:54 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Brokos", "Georgios-Ioannis", ""], ["Malakasiotis", "Prodromos", ""], ["Androutsopoulos", "Ion", ""]]}, {"id": "1608.04037", "submitter": "Davi Frossard", "authors": "Davi E. N. Frossard, Igor O. Nunes, Renato A. Krohling", "title": "An approach to dealing with missing values in heterogeneous data using\n  k-nearest neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques such as clusterization, neural networks and decision making\nusually rely on algorithms that are not well suited to deal with missing\nvalues. However, real world data frequently contains such cases. The simplest\nsolution is to either substitute them by a best guess value or completely\ndisregard the missing values. Unfortunately, both approaches can lead to biased\nresults. In this paper, we propose a technique for dealing with missing values\nin heterogeneous data using imputation based on the k-nearest neighbors\nalgorithm. It can handle real (which we refer to as crisp henceforward),\ninterval and fuzzy data. The effectiveness of the algorithm is tested on\nseveral datasets and the numerical results are promising.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 23:45:21 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Frossard", "Davi E. N.", ""], ["Nunes", "Igor O.", ""], ["Krohling", "Renato A.", ""]]}, {"id": "1608.04089", "submitter": "Kerry Zhang", "authors": "Kerry Zhang, Jussi Karlgren, Cheng Zhang, Jens Lagergren", "title": "Viewpoint and Topic Modeling of Current Events", "comments": "16 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are multiple sides to every story, and while statistical topic models\nhave been highly successful at topically summarizing the stories in corpora of\ntext documents, they do not explicitly address the issue of learning the\ndifferent sides, the viewpoints, expressed in the documents. In this paper, we\nshow how these viewpoints can be learned completely unsupervised and\nrepresented in a human interpretable form. We use a novel approach of applying\nCorrLDA2 for this purpose, which learns topic-viewpoint relations that can be\nused to form groups of topics, where each group represents a viewpoint. A\ncorpus of documents about the Israeli-Palestinian conflict is then used to\ndemonstrate how a Palestinian and an Israeli viewpoint can be learned. By\nleveraging the magnitudes and signs of the feature weights of a linear SVM, we\nintroduce a principled method to evaluate associations between topics and\nviewpoints. With this, we demonstrate, both quantitatively and qualitatively,\nthat the learned topic groups are contextually coherent, and form consistently\ncorrect topic-viewpoint associations.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2016 11:36:52 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Zhang", "Kerry", ""], ["Karlgren", "Jussi", ""], ["Zhang", "Cheng", ""], ["Lagergren", "Jens", ""]]}, {"id": "1608.04185", "submitter": "Minh-Tien Nguyen", "authors": "Minh-Tien Nguyen, Viet-Anh Phan, Truong-Son Nguyen, and Minh-Le Nguyen", "title": "Learning to Rank Questions for Community Question Answering with Ranking\n  SVM", "comments": "Nine pages, three figures, ECML/PKDD 2016 Discovery Challenge:\n  Learning to Re-Rank Questions for Community Question Answering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our method to retrieve relevant queries given a new\nquestion in the context of Discovery Challenge: Learning to Re-Ranking\nQuestions for Community Question Answering competition. In order to do that, a\nset of learning to rank methods was investigated to select an appropriate\nmethod. The selected method was optimized on training data by using a search\nstrategy. After optimizing, the method was applied to development and test set.\nResults from the competition indicate that the performance of our method\noutperforms almost participants and show that Ranking SVM is efficient for\nretrieving relevant queries in community question answering.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 06:16:18 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 13:24:48 GMT"}, {"version": "v3", "created": "Thu, 8 Sep 2016 00:37:21 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Nguyen", "Minh-Tien", ""], ["Phan", "Viet-Anh", ""], ["Nguyen", "Truong-Son", ""], ["Nguyen", "Minh-Le", ""]]}, {"id": "1608.04468", "submitter": "Tobias Schnabel", "authors": "Thorsten Joachims, Adith Swaminathan, Tobias Schnabel", "title": "Unbiased Learning-to-Rank with Biased Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit feedback (e.g., clicks, dwell times, etc.) is an abundant source of\ndata in human-interactive systems. While implicit feedback has many advantages\n(e.g., it is inexpensive to collect, user centric, and timely), its inherent\nbiases are a key obstacle to its effective use. For example, position bias in\nsearch rankings strongly influences how many clicks a result receives, so that\ndirectly using click data as a training signal in Learning-to-Rank (LTR)\nmethods yields sub-optimal results. To overcome this bias problem, we present a\ncounterfactual inference framework that provides the theoretical basis for\nunbiased LTR via Empirical Risk Minimization despite biased data. Using this\nframework, we derive a Propensity-Weighted Ranking SVM for discriminative\nlearning from implicit feedback, where click models take the role of the\npropensity estimator. In contrast to most conventional approaches to de-bias\nthe data using click models, this allows training of ranking functions even in\nsettings where queries do not repeat. Beyond the theoretical support, we show\nempirically that the proposed learning method is highly effective in dealing\nwith biases, that it is robust to noise and propensity model misspecification,\nand that it scales efficiently. We also demonstrate the real-world\napplicability of our approach on an operational search engine, where it\nsubstantially improves retrieval performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 02:56:24 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Joachims", "Thorsten", ""], ["Swaminathan", "Adith", ""], ["Schnabel", "Tobias", ""]]}, {"id": "1608.04670", "submitter": "Ajinkya More", "authors": "Ajinkya More", "title": "Attribute Extraction from Product Titles in eCommerce", "comments": "Accepted at the Workshop on Enterprise Intelligence, KDD 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a named entity extraction system for detecting attributes\nin product titles of eCommerce retailers like Walmart. The absence of syntactic\nstructure in such short pieces of text makes extracting attribute values a\nchallenging problem. We find that combining sequence labeling algorithms such\nas Conditional Random Fields and Structured Perceptron with a curated\nnormalization scheme produces an effective system for the task of extracting\nproduct attribute values from titles. To keep the discussion concrete, we will\nillustrate the mechanics of the system from the point of view of a particular\nattribute - brand. We also discuss the importance of an attribute extraction\nsystem in the context of retail websites with large product catalogs, compare\nour approach to other potential approaches to this problem and end the paper\nwith a discussion of the performance of our system for extracting attributes.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 03:34:13 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["More", "Ajinkya", ""]]}, {"id": "1608.04872", "submitter": "Bernhard C. Geiger", "authors": "Bernhard C. Geiger, Rana Ali Amjad", "title": "Hard Clusters Maximize Mutual Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate mutual information as a cost function for\nclustering, and show in which cases hard, i.e., deterministic, clusters are\noptimal. Using convexity properties of mutual information, we show that certain\nformulations of the information bottleneck problem are solved by hard clusters.\nSimilarly, hard clusters are optimal for the information-theoretic\nco-clustering problem that deals with simultaneous clustering of two dependent\ndata sets. If both data sets have to be clustered using the same cluster\nassignment, hard clusters are not optimal in general. We point at interesting\nand practically relevant special cases of this so-called pairwise clustering\nproblem, for which we can either prove or have evidence that hard clusters are\noptimal. Our results thus show that one can relax the otherwise combinatorial\nhard clustering problem to a real-valued optimization problem with the same\nglobal optimum.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 06:38:35 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Geiger", "Bernhard C.", ""], ["Amjad", "Rana Ali", ""]]}, {"id": "1608.05346", "submitter": "Zaiqiao Meng", "authors": "Zaiqiao Meng and Hong Shen", "title": "Diversified Top-k Similarity Search in Large Attributed Networks", "comments": "9 pages, 4 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large network and a query node, finding its top-k similar nodes is a\nprimitive operation in many graph-based applications. Recently enhancing search\nresults with diversification have received much attention. In this paper, we\nexplore an novel problem of searching for top-k diversified similar nodes in\nattributed networks, with the motivation that modeling diversification in an\nattributed network should consider both the emergence of network links and the\nattribute features of nodes such as user profile information. We formulate this\npractical problem as two optimization problems: the Attributed Coverage\nDiversification (ACD) problem and the r-Dissimilar Attributed Coverage\nDiversification (r-DACD) problem. Based on the submodularity and the\nmonotonicity of ACD, we propose an efficient greedy algorithm achieving a tight\napproximation guarantee of 1-1/e. Unlike the expension based methods only\nconsidering nodes' neighborhood, ACD generalize the definition of\ndiversification to nodes' own features. To capture diversification in\ntopological structure of networks, the r-DACD problem introduce a dissimilarity\nconstraint. We refer to this problem as the Dissimilarity Constrained\nNon-monotone Submodular Maximization (DCNSM) problem. We prove that there is no\nconstant-factor approximation for DCNSM, and also present an efficient greedy\nalgorithms achieving $1/\\rho$ approximation, where $\\rho\\le\\Delta$, $\\Delta$ is\nthe maximum degree of its dissimilarity based graph. To the best of our\nknowledge, it is the first approximation algorithm for the Submodular\nMaximization problem with a distance constraint. The experimental results on\nreal-world attributed network datasets demonstrate the effectiveness of our\nmethods, and confirm that adding dissimilarity constraint can significantly\nenhance the performance of diversification.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 17:45:45 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Meng", "Zaiqiao", ""], ["Shen", "Hong", ""]]}, {"id": "1608.05380", "submitter": "Amira Ghenai Amira Ghenai", "authors": "Amira Ghenai, Moustafa M.Ghanem", "title": "Exploring Trust-Aware Neighbourhood in Trust-based Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Recommender Systems (RS) do not consider any personal user\ninformation beyond rating history. Such information, on the other hand, is\nwidely available on social networking sites (Facebook, Twitter). As a result,\nsocial networks have recently been used in recommendation systems. In this\npaper, we propose an efficient method for incorporating social signals into the\nrecommendation process by building a trust network which supplements the users'\nrating profiles. We first show the effect of different cold-start users types\non the Collaborative Filtering (CF) technique in several real-world datasets.\nLater, we propose a \"Trust-Aware Neighbourhood\" algorithm which addresses a\nperformance issue of the former by limiting the trusted neighbourhood. We show\nthe doubling of the rating coverage compared to the traditional CF technique,\nand a significant improvement in the accuracy for some datasets. Focusing\nspecifically on cold-start users, we propose a \"Hybrid Trust-Aware\nNeighbourhood\" algorithm which expands the neighbourhood by considering both\ntrust and rating history of the users. We show a near complete coverage with a\nrich trust network dataset-- Flixster. We conclude by discussing the potential\nimplementation of this algorithm in a budget-constrained cloud environment.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 19:21:08 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Ghenai", "Amira", ""], ["Ghanem", "Moustafa M.", ""]]}, {"id": "1608.05517", "submitter": "Xianwen Wang", "authors": "Xianwen Wang, Zhichao Fang", "title": "Detecting and Tracking The Real-time Hot Topics: A Study on\n  Computational Neuroscience", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, following the idea of our previous paper (Wang, et al.,\n2013a), we improve the method to detect and track hot topics in a specific\nfield by using the real-time article usage data. With the \"usage count\" data\nprovided by Web of Science, we take the field of computational neuroscience as\nan example to make analysis. About 10 thousand articles in the field of\nComputational Neuroscience are queried in Web of Science, when the records,\nincluding the usage count data of each paper, have been harvested and updated\nweekly from October 19, 2015 to March 21, 2016. The hot topics are defined by\nthe most frequently used keywords aggregated from the articles. The analysis\nreveals that hot topics in Computational Neuroscience are related to the key\ntechnologies, like \"fmri\", \"eeg\", \"erp\", etc. Furthermore, using the weekly\nupdated data, we track the dynamical changes of the topics. The characteristic\nof immediacy of usage data makes it possible to track the \"heat\" of hot topics\ntimely and dynamically.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 07:29:34 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Wang", "Xianwen", ""], ["Fang", "Zhichao", ""]]}, {"id": "1608.06054", "submitter": "Qin Liu", "authors": "Qin Liu, Zhenguo Li, John C.S. Lui, Jiefeng Cheng", "title": "PowerWalk: Scalable Personalized PageRank via Random Walks with\n  Vertex-Centric Decomposition", "comments": "technical report of our full paper in CIKM 2016", "journal-ref": null, "doi": "10.1145/2983323.2983713", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most methods for Personalized PageRank (PPR) precompute and store all\naccurate PPR vectors, and at query time, return the ones of interest directly.\nHowever, the storage and computation of all accurate PPR vectors can be\nprohibitive for large graphs, especially in caching them in memory for\nreal-time online querying. In this paper, we propose a distributed framework\nthat strikes a better balance between offline indexing and online querying. The\noffline indexing attains a fingerprint of the PPR vector of each vertex by\nperforming billions of \"short\" random walks in parallel across a cluster of\nmachines. We prove that our indexing method has an exponential convergence,\nachieving the same precision with previous methods using a much smaller number\nof random walks. At query time, the new PPR vector is composed by a linear\ncombination of related fingerprints, in a highly efficient vertex-centric\ndecomposition manner. Interestingly, the resulting PPR vector is much more\naccurate than its offline counterpart because it actually uses more random\nwalks in its estimation. More importantly, we show that such decomposition for\na batch of queries can be very efficiently processed using a shared\ndecomposition. Our implementation, PowerWalk, takes advantage of advanced\ndistributed graph engines and it outperforms the state-of-the-art algorithms by\norders of magnitude. Particularly, it responses to tens of thousands of queries\non graphs with billions of edges in just a few seconds.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 05:31:58 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Liu", "Qin", ""], ["Li", "Zhenguo", ""], ["Lui", "John C. S.", ""], ["Cheng", "Jiefeng", ""]]}, {"id": "1608.06253", "submitter": "Christina Lioma Assoc. Prof", "authors": "Brian Brost and Yevgeny Seldin and Ingemar J. Cox and Christina Lioma", "title": "Multi-Dueling Bandits and Their Application to Online Ranker Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New ranking algorithms are continually being developed and refined,\nnecessitating the development of efficient methods for evaluating these\nrankers. Online ranker evaluation focuses on the challenge of efficiently\ndetermining, from implicit user feedback, which ranker out of a finite set of\nrankers is the best. Online ranker evaluation can be modeled by dueling ban-\ndits, a mathematical model for online learning under limited feedback from\npairwise comparisons. Comparisons of pairs of rankers is performed by\ninterleaving their result sets and examining which documents users click on.\nThe dueling bandits model addresses the key issue of which pair of rankers to\ncompare at each iteration, thereby providing a solution to the\nexploration-exploitation trade-off. Recently, methods for simultaneously\ncomparing more than two rankers have been developed. However, the question of\nwhich rankers to compare at each iteration was left open. We address this\nquestion by proposing a generalization of the dueling bandits model that uses\nsimultaneous comparisons of an unrestricted number of rankers. We evaluate our\nalgorithm on synthetic data and several standard large-scale online ranker\nevaluation datasets. Our experimental results show that the algorithm yields\norders of magnitude improvement in performance compared to stateof- the-art\ndueling bandit algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 18:20:18 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Brost", "Brian", ""], ["Seldin", "Yevgeny", ""], ["Cox", "Ingemar J.", ""], ["Lioma", "Christina", ""]]}, {"id": "1608.06298", "submitter": "Jonathan Gemmell Jonathan Gemmell", "authors": "Greg Zanotti, Miller Horvath, Lucas Nunes Barbosa, Venkata Trinadh\n  Kumar Gupta Immedisetty, Jonathan Gemmell", "title": "Infusing Collaborative Recommenders with Distributed Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems assist users in navigating complex information spaces and\nfocus their attention on the content most relevant to their needs. Often these\nsystems rely on user activity or descriptions of the content. Social annotation\nsystems, in which users collaboratively assign tags to items, provide another\nmeans to capture information about users and items. Each of these data sources\nprovides unique benefits, capturing different relationships.\n  In this paper, we propose leveraging multiple sources of data: ratings data\nas users report their affinity toward an item, tagging data as users assign\nannotations to items, and item data collected from an online database. Taken\ntogether, these datasets provide the opportunity to learn rich distributed\nrepresentations by exploiting recent advances in neural network architectures.\nWe first produce representations that subjectively capture interesting\nrelationships among the data. We then empirically evaluate the utility of the\nrepresentations to predict a user's rating on an item and show that it\noutperforms more traditional representations. Finally, we demonstrate that\ntraditional representations can be combined with representations trained\nthrough a neural network to achieve even better results.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 20:05:01 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Zanotti", "Greg", ""], ["Horvath", "Miller", ""], ["Barbosa", "Lucas Nunes", ""], ["Immedisetty", "Venkata Trinadh Kumar Gupta", ""], ["Gemmell", "Jonathan", ""]]}, {"id": "1608.06651", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Maarten de Rijke, Marcel Worring", "title": "Unsupervised, Efficient and Semantic Expertise Retrieval", "comments": "WWW2016, Proceedings of the 25th International Conference on World\n  Wide Web. 2016", "journal-ref": null, "doi": "10.1145/2872427.2882974", "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an unsupervised discriminative model for the task of retrieving\nexperts in online document collections. We exclusively employ textual evidence\nand avoid explicit feature engineering by learning distributed word\nrepresentations in an unsupervised way. We compare our model to\nstate-of-the-art unsupervised statistical vector space and probabilistic\ngenerative approaches. Our proposed log-linear model achieves the retrieval\nperformance levels of state-of-the-art document-centric methods with the low\ninference cost of so-called profile-centric approaches. It yields a\nstatistically significant improved ranking over vector space and generative\nmodels in most cases, matching the performance of supervised methods on various\nbenchmarks. That is, by using solely text we can do as well as methods that\nwork with external evidence and/or relevance feedback. A contrastive analysis\nof rankings produced by discriminative and generative approaches shows that\nthey have complementary strengths due to the ability of the unsupervised\ndiscriminative model to perform semantic matching.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 20:55:09 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 04:57:54 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Van Gysel", "Christophe", ""], ["de Rijke", "Maarten", ""], ["Worring", "Marcel", ""]]}, {"id": "1608.06656", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Evangelos Kanoulas, Maarten de Rijke", "title": "Lexical Query Modeling in Session Search", "comments": "ICTIR2016, Proceedings of the 2nd ACM International Conference on the\n  Theory of Information Retrieval. 2016", "journal-ref": null, "doi": "10.1145/2970398.2970422", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexical query modeling has been the leading paradigm for session search. In\nthis paper, we analyze TREC session query logs and compare the performance of\ndifferent lexical matching approaches for session search. Naive methods based\non term frequency weighing perform on par with specialized session models. In\naddition, we investigate the viability of lexical query models in the setting\nof session search. We give important insights into the potential and\nlimitations of lexical query modeling for session search and propose future\ndirections for the field of session search.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 21:07:50 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Van Gysel", "Christophe", ""], ["Kanoulas", "Evangelos", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1608.06664", "submitter": "Shih-Chieh Su", "authors": "Shih-Chieh Su, Joseph Vaughn and Jean-Laurent Huynh", "title": "Topic Grids for Homogeneous Data Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the topic grids to detect anomaly and analyze the behavior based\non the access log content. Content-based behavioral risk is quantified in the\nhigh dimensional space where the topics are generated from the log. The topics\nare being projected homogeneously into a space that is perception- and\ninteraction-friendly to the human experts.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 22:44:42 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Su", "Shih-Chieh", ""], ["Vaughn", "Joseph", ""], ["Huynh", "Jean-Laurent", ""]]}, {"id": "1608.06876", "submitter": "Giacomo Berardi", "authors": "Ugo Scaiella, Giacomo Berardi, Giuliano Mega, Roberto Santoro", "title": "Sedano: A News Stream Processor for Business", "comments": "2 pages, 1 figure. SIGIR '16 July 17-21, 2016, Pisa, Italy", "journal-ref": "Proceedings of the 39th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR 2016). pp 525-526.\n  Pisa, IT, 2016", "doi": "10.1145/2911451.2926730", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Sedano, a system for processing and indexing a continuous stream\nof business-related news. Sedano defines pipelines whose stages analyze and\nenrich news items (e.g., newspaper articles and press releases). News data\ncoming from several content sources are stored, processed and then indexed in\norder to be consumed by Atoka, our business intelligence product. Atoka users\ncan retrieve news about specific companies, filtering according to various\nfacets. Sedano features both an entity-linking phase, which finds mentions of\ncompanies in news, and a classification phase, which classifies news according\nto a set of business events. Its flexible architecture allows Sedano to be\ndeployed on commodity machines while being scalable and fault-tolerant.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 15:52:19 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Scaiella", "Ugo", ""], ["Berardi", "Giacomo", ""], ["Mega", "Giuliano", ""], ["Santoro", "Roberto", ""]]}, {"id": "1608.07051", "submitter": "Dawei Chen", "authors": "Dawei Chen, Cheng Soon Ong, Lexing Xie", "title": "Learning Points and Routes to Recommend Trajectories", "comments": null, "journal-ref": null, "doi": "10.1145/2983323.2983672", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of recommending tours to travellers is an important and broadly\nstudied area. Suggested solutions include various approaches of\npoints-of-interest (POI) recommendation and route planning. We consider the\ntask of recommending a sequence of POIs, that simultaneously uses information\nabout POIs and routes. Our approach unifies the treatment of various sources of\ninformation by representing them as features in machine learning algorithms,\nenabling us to learn from past behaviour. Information about POIs are used to\nlearn a POI ranking model that accounts for the start and end points of tours.\nData about previous trajectories are used for learning transition patterns\nbetween POIs that enable us to recommend probable routes. In addition, a\nprobabilistic model is proposed to combine the results of POI ranking and the\nPOI to POI transitions. We propose a new F$_1$ score on pairs of POIs that\ncapture the order of visits. Empirical results show that our approach improves\non recent methods, and demonstrate that combining points and routes enables\nbetter trajectory recommendations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 08:39:47 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Chen", "Dawei", ""], ["Ong", "Cheng Soon", ""], ["Xie", "Lexing", ""]]}, {"id": "1608.07094", "submitter": "Mahamad Suhil", "authors": "D S Guru, Mahamad Suhil", "title": "A Novel Term_Class Relevance Measure for Text Categorization", "comments": "12 pages, 6 figures, 2 tables", "journal-ref": "Procedia Computer Science, vol.45, pp.13-22, 2015", "doi": "10.1016/j.procs.2015.03.074", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new measure called Term_Class relevance to\ncompute the relevancy of a term in classifying a document into a particular\nclass. The proposed measure estimates the degree of relevance of a given term,\nin placing an unlabeled document to be a member of a known class, as a product\nof Class_Term weight and Class_Term density; where the Class_Term weight is the\nratio of the number of documents of the class containing the term to the total\nnumber of documents containing the term and the Class_Term density is the\nrelative density of occurrence of the term in the class to the total occurrence\nof the term in the entire population. Unlike the other existing term weighting\nschemes such as TF-IDF and its variants, the proposed relevance measure takes\ninto account the degree of relative participation of the term across all\ndocuments of the class to the entire population. To demonstrate the\nsignificance of the proposed measure experimentation has been conducted on the\n20 Newsgroups dataset. Further, the superiority of the novel measure is brought\nout through a comparative analysis.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 11:46:06 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 12:51:50 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Guru", "D S", ""], ["Suhil", "Mahamad", ""]]}, {"id": "1608.07102", "submitter": "Qiang Liu", "authors": "Qiang Liu, Shu Wu, Liang Wang", "title": "Multi-behavioral Sequential Prediction with Recurrent Log-bilinear Model", "comments": "IEEE Transactions on Knowledge and Data Engineering (TKDE), to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of Internet applications, sequential prediction in\ncollaborative filtering has become an emerging and crucial task. Given the\nbehavioral history of a specific user, predicting his or her next choice plays\na key role in improving various online services. Meanwhile, there are more and\nmore scenarios with multiple types of behaviors, while existing works mainly\nstudy sequences with a single type of behavior. As a widely used approach,\nMarkov chain based models are based on a strong independence assumption. As two\nclassical neural network methods for modeling sequences, recurrent neural\nnetworks cannot well model short-term contexts, and the log-bilinear model is\nnot suitable for long-term contexts. In this paper, we propose a Recurrent\nLog-BiLinear (RLBL) model. It can model multiple types of behaviors in\nhistorical sequences with behavior-specific transition matrices. RLBL applies a\nrecurrent structure for modeling long-term contexts. It models several items in\neach hidden layer and employs position-specific transition matrices for\nmodeling short-term contexts. Moreover, considering continuous time difference\nin behavioral history is a key factor for dynamic prediction, we further extend\nRLBL and replace position-specific transition matrices with time-specific\ntransition matrices, and accordingly propose a Time-Aware Recurrent\nLog-BiLinear (TA-RLBL) model. Experimental results show that the proposed RLBL\nmodel and TA-RLBL model yield significant improvements over the competitive\ncompared methods on three datasets, i.e., Movielens-1M dataset, Global\nTerrorism Database and Tmall dataset with different numbers of behavior types.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 12:01:18 GMT"}, {"version": "v2", "created": "Mon, 10 Oct 2016 09:08:40 GMT"}, {"version": "v3", "created": "Thu, 8 Dec 2016 06:58:18 GMT"}, {"version": "v4", "created": "Fri, 27 Jan 2017 09:53:14 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Liu", "Qiang", ""], ["Wu", "Shu", ""], ["Wang", "Liang", ""]]}, {"id": "1608.07253", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Maarten de Rijke, Evangelos Kanoulas", "title": "Learning Latent Vector Spaces for Product Search", "comments": "CIKM2016, Proceedings of the 25th ACM International Conference on\n  Information and Knowledge Management. 2016", "journal-ref": null, "doi": "10.1145/2983323.2983702", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel latent vector space model that jointly learns the latent\nrepresentations of words, e-commerce products and a mapping between the two\nwithout the need for explicit annotations. The power of the model lies in its\nability to directly model the discriminative relation between products and a\nparticular word. We compare our method to existing latent vector space models\n(LSI, LDA and word2vec) and evaluate it as a feature in a learning to rank\nsetting. Our latent vector space model achieves its enhanced performance as it\nlearns better product representations. Furthermore, the mapping from words to\nproducts and the representations of words benefit directly from the errors\npropagated back from the product representations during parameter estimation.\nWe provide an in-depth analysis of the performance of our model and analyze the\nstructure of the learned representations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 18:57:50 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Van Gysel", "Christophe", ""], ["de Rijke", "Maarten", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "1608.07400", "submitter": "Robin Devooght", "authors": "Robin Devooght and Hugues Bersini", "title": "Collaborative Filtering with Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that collaborative filtering can be viewed as a sequence prediction\nproblem, and that given this interpretation, recurrent neural networks offer\nvery competitive approach. In particular we study how the long short-term\nmemory (LSTM) can be applied to collaborative filtering, and how it compares to\nstandard nearest neighbors and matrix factorization methods on movie\nrecommendation. We show that the LSTM is competitive in all aspects, and\nlargely outperforms other methods in terms of item coverage and short term\npredictions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 09:20:21 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 07:41:44 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Devooght", "Robin", ""], ["Bersini", "Hugues", ""]]}, {"id": "1608.07793", "submitter": "Zhongqi Lu", "authors": "Zhongqi Lu, Qiang Yang", "title": "Partially Observable Markov Decision Process for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report the \"Recurrent Deterioration\" (RD) phenomenon observed in online\nrecommender systems. The RD phenomenon is reflected by the trend of performance\ndegradation when the recommendation model is always trained based on users'\nfeedbacks of the previous recommendations. There are several reasons for the\nrecommender systems to encounter the RD phenomenon, including the lack of\nnegative training data and the evolution of users' interests, etc. Motivated to\ntackle the problems causing the RD phenomenon, we propose the POMDP-Rec\nframework, which is a neural-optimized Partially Observable Markov Decision\nProcess algorithm for recommender systems. We show that the POMDP-Rec framework\neffectively uses the accumulated historical data from real-world recommender\nsystems and automatically achieves comparable results with those models\nfine-tuned exhaustively by domain exports on public datasets.\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2016 09:42:52 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 15:41:02 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Lu", "Zhongqi", ""], ["Yang", "Qiang", ""]]}, {"id": "1608.07852", "submitter": "Chao-Lin Liu", "authors": "Chao-Lin Liu", "title": "Quantitative Analyses of Chinese Poetry of Tang and Song Dynasties:\n  Using Changing Colors and Innovative Terms as Examples", "comments": "2016 International Conference on Digital Humanities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tang (618-907 AD) and Song (960-1279) dynasties are two very important\nperiods in the development of Chinese literary. The most influential forms of\nthe poetry in Tang and Song were Shi and Ci, respectively. Tang Shi and Song Ci\nestablished crucial foundations of the Chinese literature, and their influences\nin both literary works and daily lives of the Chinese communities last until\ntoday.\n  We can analyze and compare the Complete Tang Shi and the Complete Song Ci\nfrom various viewpoints. In this presentation, we report our findings about the\ndifferences in their vocabularies. Interesting new words that started to appear\nin Song Ci and continue to be used in modern Chinese were identified. Colors\nare an important ingredient of the imagery in poetry, and we discuss the most\nfrequent color words that appeared in Tang Shi and Song Ci.\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2016 20:31:37 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Liu", "Chao-Lin", ""]]}, {"id": "1608.07952", "submitter": "Alex Olieman", "authors": "Alex Olieman, Jaap Kamps, Gleb Satyukov, Emil de Valk", "title": "Topical Generalization for Presentation of User Profiles", "comments": "(to be) presented at DIR'16, November 25, 2016, Delft, The\n  Netherlands", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained user profile generation approaches have made it increasingly\nfeasible to display on a profile page in which topics a user has expertise or\ninterest. Earlier work on topical user profiling has been directed at enhancing\nsearch and personalization functionality, but making such profiles useful for\nhuman consumption presents new challenges. With this work, we have taken a\nfirst step toward a semantic layout mode for topical user profiles. We have\ndeveloped a topical generalization approach which finds coherent groups of\ntopics and adds labels to them, based on their association with broader topics\nin the Wikipedia category graph. A nested layout mode, employing topical\ngeneralization, is compared with a simpler flat layout mode in our user study.\nThe results indicate that users favor the nested structure over flat profiles,\nbut tend to overlook the specific topics on the lower level. We propose a third\nlayout mode to address this issue.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 08:47:12 GMT"}, {"version": "v2", "created": "Sat, 15 Oct 2016 05:24:51 GMT"}, {"version": "v3", "created": "Sun, 20 Nov 2016 01:52:00 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Olieman", "Alex", ""], ["Kamps", "Jaap", ""], ["Satyukov", "Gleb", ""], ["de Valk", "Emil", ""]]}, {"id": "1608.08139", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Cristian Reyes, Eva Mohedano, Kevin McGuinness, Noel E. O'Connor and\n  Xavier Giro-i-Nieto", "title": "Where is my Phone ? Personal Object Retrieval from Egocentric Images", "comments": "Lifelogging Tools and Applications Workshop (LTA'16) at ACM\n  Multimedia 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a retrieval pipeline and evaluation scheme for the problem\nof finding the last appearance of personal objects in a large dataset of images\ncaptured from a wearable camera. Each personal object is modelled by a small\nset of images that define a query for a visual search engine.The retrieved\nresults are reranked considering the temporal timestamps of the images to\nincrease the relevance of the later detections. Finally, a temporal\ninterleaving of the results is introduced for robustness against false\ndetections. The Mean Reciprocal Rank is proposed as a metric to evaluate this\nproblem. This application could help into developing personal assistants\ncapable of helping users when they do not remember where they left their\npersonal belongings.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 16:41:52 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 23:13:09 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Reyes", "Cristian", ""], ["Mohedano", "Eva", ""], ["McGuinness", "Kevin", ""], ["O'Connor", "Noel E.", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "1608.08176", "submitter": "Amritanshu Agrawal", "authors": "Amritanshu Agrawal, Wei Fu, Tim Menzies", "title": "What is Wrong with Topic Modeling? (and How to Fix it Using Search-based\n  Software Engineering)", "comments": "15 pages + 2 page references. Accepted to IST", "journal-ref": "Information and Software Technology Journal, 2018", "doi": "10.1016/j.infsof.2018.02.005", "report-no": null, "categories": "cs.SE cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Topic modeling finds human-readable structures in unstructured\ntextual data. A widely used topic modeler is Latent Dirichlet allocation. When\nrun on different datasets, LDA suffers from \"order effects\" i.e. different\ntopics are generated if the order of training data is shuffled. Such order\neffects introduce a systematic error for any study. This error can relate to\nmisleading results;specifically, inaccurate topic descriptions and a reduction\nin the efficacy of text mining classification results. Objective: To provide a\nmethod in which distributions generated by LDA are more stable and can be used\nfor further analysis. Method: We use LDADE, a search-based software engineering\ntool that tunes LDA's parameters using DE (Differential Evolution). LDADE is\nevaluated on data from a programmer information exchange site (Stackoverflow),\ntitle and abstract text of thousands ofSoftware Engineering (SE) papers, and\nsoftware defect reports from NASA. Results were collected across different\nimplementations of LDA (Python+Scikit-Learn, Scala+Spark); across different\nplatforms (Linux, Macintosh) and for different kinds of LDAs (VEM,or using\nGibbs sampling). Results were scored via topic stability and text mining\nclassification accuracy. Results: In all treatments: (i) standard LDA exhibits\nvery large topic instability; (ii) LDADE's tunings dramatically reduce cluster\ninstability; (iii) LDADE also leads to improved performances for supervised as\nwell as unsupervised learning. Conclusion: Due to topic instability, using\nstandard LDA with its \"off-the-shelf\" settings should now be depreciated. Also,\nin future, we should require SE papers that use LDA to test and (if needed)\nmitigate LDA topic instability. Finally, LDADE is a candidate technology for\neffectively and efficiently reducing that instability.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 18:45:00 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 01:19:06 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 04:49:42 GMT"}, {"version": "v4", "created": "Tue, 20 Feb 2018 17:26:51 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Agrawal", "Amritanshu", ""], ["Fu", "Wei", ""], ["Menzies", "Tim", ""]]}, {"id": "1608.08180", "submitter": "Philipp Mayr", "authors": "Judit Bar-Ilan, Rob Koopman, Shenghui Wang, Andrea Scharnhorst, Marcus\n  John, Philipp Mayr, Dietmar Wolfram", "title": "Bibliometrics and Information Retrieval: Creating Knowledge through\n  Research Synergies", "comments": "4 pages, accepted at the 2016 Annual Meeting of the Association for\n  Information Science and Technology (ASIST 2016) in Copenhagen", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This panel brings together experts in bibliometrics and information retrieval\nto discuss how each of these two important areas of information science can\nhelp to inform the research of the other. There is a growing body of literature\nthat capitalizes on the synergies created by combining methodological\napproaches of each to solve research problems and practical issues related to\nhow information is created, stored, organized, retrieved and used. The session\nwill begin with an overview of the common threads that exist between IR and\nmetrics, followed by a summary of findings from the BIR workshops and examples\nof research projects that combine aspects of each area to benefit IR or metrics\nresearch areas, including search results ranking, semantic indexing and\nvisualization. The panel will conclude with an engaging discussion with the\naudience to identify future areas of research and collaboration.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 19:06:57 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Bar-Ilan", "Judit", ""], ["Koopman", "Rob", ""], ["Wang", "Shenghui", ""], ["Scharnhorst", "Andrea", ""], ["John", "Marcus", ""], ["Mayr", "Philipp", ""], ["Wolfram", "Dietmar", ""]]}, {"id": "1608.08182", "submitter": "Bo Li", "authors": "Bo Li, Yining Wang, Aarti Singh, Yevgeniy Vorobeychik", "title": "Data Poisoning Attacks on Factorization-Based Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation and collaborative filtering systems are important in modern\ninformation and e-commerce applications. As these systems are becoming\nincreasingly popular in the industry, their outputs could affect business\ndecision making, introducing incentives for an adversarial party to compromise\nthe availability or integrity of such systems. We introduce a data poisoning\nattack on collaborative filtering systems. We demonstrate how a powerful\nattacker with full knowledge of the learner can generate malicious data so as\nto maximize his/her malicious objectives, while at the same time mimicking\nnormal user behavior to avoid being detected. While the complete knowledge\nassumption seems extreme, it enables a robust assessment of the vulnerability\nof collaborative filtering schemes to highly motivated attacks. We present\nefficient solutions for two popular factorization-based collaborative filtering\nalgorithms: the \\emph{alternative minimization} formulation and the\n\\emph{nuclear norm minimization} method. Finally, we test the effectiveness of\nour proposed algorithms on real-world data and discuss potential defensive\nstrategies.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 19:09:27 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 22:26:13 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Li", "Bo", ""], ["Wang", "Yining", ""], ["Singh", "Aarti", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "1608.08414", "submitter": "Manuel Sebastian Mariani", "authors": "Manuel Sebastian Mariani, Matus Medo, Yi-Cheng Zhang", "title": "Identification of milestone papers through time-balanced network\n  centrality", "comments": "Main text (pp. 1-12) + Appendices (pp. 13-18)", "journal-ref": "Journal of Informetrics 10, 1207-1223 (2016)", "doi": "10.1016/j.joi.2016.10.005", "report-no": null, "categories": "physics.soc-ph cs.DL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citations between scientific papers and related bibliometric indices, such as\nthe $h$-index for authors and the impact factor for journals, are being\nincreasingly used - often in controversial ways - as quantitative tools for\nresearch evaluation. Yet, a fundamental research question remains still open:\nto which extent do quantitative metrics capture the significance of scientific\nworks? We analyze the network of citations among the $449,935$ papers published\nby the American Physical Society (APS) journals between 1893 and 2009, and\nfocus on the comparison of metrics built on the citation count with\nnetwork-based metrics. We contrast five article-level metrics with respect to\nthe rankings that they assign to a set of fundamental papers, called Milestone\nLetters, carefully selected by the APS editors for \"making long-lived\ncontributions to physics, either by announcing significant discoveries, or by\ninitiating new areas of research\". A new metric, which combines PageRank\ncentrality with the explicit requirement that paper score is not biased by\npaper age, is the best-performing metric overall in identifying the Milestone\nLetters. The lack of time bias in the new metric makes it also possible to use\nit to compare papers of different age on the same scale. We find that\nnetwork-based metrics identify the Milestone Letters better than metrics based\non the citation count, which suggests that the structure of the citation\nnetwork contains information that can be used to improve the ranking of\nscientific publications. The methods and results presented here are relevant\nfor all evolving systems where network centrality metrics are applied, for\nexample the World Wide Web and online social networks. An interactive Web\nplatform where it is possible to view the ranking of the APS papers by rescaled\nPageRank is available at the address \\url{http://www.sciencenow.info}.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 12:11:54 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 07:40:44 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2016 15:57:54 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Mariani", "Manuel Sebastian", ""], ["Medo", "Matus", ""], ["Zhang", "Yi-Cheng", ""]]}, {"id": "1608.08574", "submitter": "Babatunde Olabenjo", "authors": "Babatunde Olabenjo", "title": "Applying Naive Bayes Classification to Google Play Apps Categorization", "comments": "Experiment Results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are over one million apps on Google Play Store and over half a million\npublishers. Having such a huge number of apps and developers can pose a\nchallenge to app users and new publishers on the store. Discovering apps can be\nchallenging if apps are not correctly published in the right category, and, in\nturn, reduce earnings for app developers. Additionally, with over 41 categories\non Google Play Store, deciding on the right category to publish an app can be\nchallenging for developers due to the number of categories they have to choose\nfrom. Machine Learning has been very useful, especially in classification\nproblems such sentiment analysis, document classification and spam detection.\nThese strategies can also be applied to app categorization on Google Play Store\nto suggest appropriate categories for app publishers using details from their\napplication.\n  In this project, we built two variations of the Naive Bayes classifier using\nopen metadata from top developer apps on Google Play Store in other to classify\nnew apps on the store. These classifiers are then evaluated using various\nevaluation methods and their results compared against each other. The results\nshow that the Naive Bayes algorithm performs well for our classification\nproblem and can potentially automate app categorization for Android app\npublishers on Google Play Store\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 17:46:55 GMT"}], "update_date": "2016-09-03", "authors_parsed": [["Olabenjo", "Babatunde", ""]]}, {"id": "1608.08646", "submitter": "Veronika Strnadov\\'a-Neeley", "authors": "Veronika Strnadova-Neeley, Aydin Buluc, John R. Gilbert, Leonid\n  Oliker, Weimin Ouyang", "title": "LiRa: A New Likelihood-Based Similarity Score for Collaborative\n  Filtering", "comments": "- added acknowledgments - fixed typos (results unchanged) 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender system data presents unique challenges to the data mining,\nmachine learning, and algorithms communities. The high missing data rate, in\ncombination with the large scale and high dimensionality that is typical of\nrecommender systems data, requires new tools and methods for efficient data\nanalysis. Here, we address the challenge of evaluating similarity between two\nusers in a recommender system, where for each user only a small set of ratings\nis available. We present a new similarity score, that we call LiRa, based on a\nstatistical model of user similarity, for large-scale, discrete valued data\nwith many missing values. We show that this score, based on a ratio of\nlikelihoods, is more effective at identifying similar users than traditional\nsimilarity scores in user-based collaborative filtering, such as the Pearson\ncorrelation coefficient. We argue that our approach has significant potential\nto improve both accuracy and scalability in collaborative filtering.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 20:21:31 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 20:57:05 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Strnadova-Neeley", "Veronika", ""], ["Buluc", "Aydin", ""], ["Gilbert", "John R.", ""], ["Oliker", "Leonid", ""], ["Ouyang", "Weimin", ""]]}, {"id": "1608.08940", "submitter": "Luis Argerich", "authors": "Luis Argerich, Joaqu\\'in Torr\\'e Zaffaroni, Mat\\'ias J Cano", "title": "Hash2Vec, Feature Hashing for Word Embeddings", "comments": "ASAI 2016, 45JAIIO", "journal-ref": "45 JAIIO - ASAI 2016 - ISSN: 2451-7585 - Pages 33-40", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we propose the application of feature hashing to create word\nembeddings for natural language processing. Feature hashing has been used\nsuccessfully to create document vectors in related tasks like document\nclassification. In this work we show that feature hashing can be applied to\nobtain word embeddings in linear time with the size of the data. The results\nshow that this algorithm, that does not need training, is able to capture the\nsemantic meaning of words. We compare the results against GloVe showing that\nthey are similar. As far as we know this is the first application of feature\nhashing to the word embeddings problem and the results indicate this is a\nscalable technique with practical results for NLP applications.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 17:01:09 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Argerich", "Luis", ""], ["Zaffaroni", "Joaqu\u00edn Torr\u00e9", ""], ["Cano", "Mat\u00edas J", ""]]}, {"id": "1608.09002", "submitter": "Prantik Bhattacharyya", "authors": "Nemanja Spasojevic, Prantik Bhattacharyya, Adithya Rao", "title": "Mining Half a Billion Topical Experts Across Multiple Social Networks", "comments": "20 pages, 9 figures, 6 tables", "journal-ref": null, "doi": "10.1007/s13278-016-0356-7", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining topical experts on social media is a problem that has gained\nsignificant attention due to its wide-ranging applications. Here we present the\nfirst study that combines data from four major social networks -- Twitter,\nFacebook, Google+ and LinkedIn, along with the Wikipedia graph and internet\nwebpage text and metadata, to rank topical experts across the global population\nof users. We perform an in-depth analysis of 37 features derived from various\ndata sources such as message text, user lists, webpages, social graphs and\nwikipedia. This large-scale study includes more than 12 billion messages over a\n90-day sliding window and 58 billion social graph edges. Comparison reveals\nthat features derived from Twitter Lists, Wikipedia, internet webpages and\nTwitter Followers are especially good indicators of expertise. We train an\nexpertise ranking model using these features on a large ground truth dataset\ncontaining almost 90,000 labels. This model is applied within a production\nsystem that ranks over 650 million experts in more than 9,000 topical domains\non a daily basis. We provide results and examples on the effectiveness of our\nexpert ranking system, along with empirical validation. Finally, we make the\ntopical expertise data available through open REST APIs for wider use.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 19:14:03 GMT"}], "update_date": "2016-09-01", "authors_parsed": [["Spasojevic", "Nemanja", ""], ["Bhattacharyya", "Prantik", ""], ["Rao", "Adithya", ""]]}]