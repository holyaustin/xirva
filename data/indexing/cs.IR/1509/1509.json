[{"id": "1509.00190", "submitter": "Alex Stolz", "authors": "Alex Stolz and Martin Hepp", "title": "GR2RSS: Publishing Linked Open Commerce Data as RSS and Atom Feeds", "comments": "Technical report, 5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": "TR-2014-1", "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of Linked Open Data (LOD) content in Web pages is a\nchallenging and sometimes tedious task for Web developers. At the same moment,\nmost software packages for blogs, content management systems (CMS), and shop\napplications support the consumption of feed formats, namely RSS and Atom. In\nthis technical report, we demonstrate an on-line tool that fetches e-commerce\ndata from a SPARQL endpoint and syndicates obtained results as RSS or Atom\nfeeds. Our approach combines (1) the popularity and broad tooling support of\nexisting feed formats, (2) the precision of queries against structured data\nbuilt upon common Web vocabularies like schema.org, GoodRelations, FOAF, VCard,\nand WGS 84, and (3) the ease of integrating content from a large number of Web\nsites and other data sources in RDF in general.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 09:25:07 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Stolz", "Alex", ""], ["Hepp", "Martin", ""]]}, {"id": "1509.00594", "submitter": "Jian Gao", "authors": "Jian Gao and Tao Zhou", "title": "Evaluating user reputation in online rating systems via an iterative\n  group-based ranking method", "comments": "12 pages, 9 figures, 2 tables", "journal-ref": "Physica A: Statistical Mechanics and its Applications. 473 (2017):\n  546-560", "doi": "10.1016/j.physa.2017.01.055", "report-no": null, "categories": "cs.IR cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reputation is a valuable asset in online social lives and it has drawn\nincreased attention. How to evaluate user reputation in online rating systems\nis especially significant due to the existence of spamming attacks. To address\nthis issue, so far, a variety of methods have been proposed, including\nnetwork-based methods, quality-based methods and group-based ranking method. In\nthis paper, we propose an iterative group-based ranking (IGR) method by\nintroducing an iterative reputation-allocation process into the original\ngroup-based ranking (GR) method. More specifically, users with higher\nreputation have higher weights in dominating the corresponding group sizes. The\nreputation of users and the corresponding group sizes are iteratively updated\nuntil they become stable. Results on two real data sets suggest that the\nproposed IGR method has better performance and its robustness is considerably\nimproved comparing with the original GR method. Our work highlights the\npositive role of users' grouping behavior towards a better reputation\nevaluation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 07:58:13 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Gao", "Jian", ""], ["Zhou", "Tao", ""]]}, {"id": "1509.00690", "submitter": "Zahid Ansari", "authors": "Zahid Ansari, M.F.Azeem, A. Vinaya Babu and Waseem Ahmed", "title": "A Fuzzy Approach for Feature Evaluation and Dimensionality Reduction to\n  Improve the Quality of Web Usage Mining Results", "comments": null, "journal-ref": "International Journal on Advanced Science Engineering and\n  Information Technology, pp. 67-73 Vol. 2 No. 6, 2012. (ISSN: 2088-5334,\n  INSIGHT Publishers, Indonesia)", "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web Usage Mining is the application of data mining techniques to web usage\nlog repositories in order to discover the usage patterns that can be used to\nanalyze the users navigational behavior. During the preprocessing stage, raw\nweb log data is transformed into a set of user profiles. Each user profile\ncaptures a set of URLs representing a user session. Clustering can be applied\nto this sessionized data in order to capture similar interests and trends among\nusers navigational patterns. Since the sessionized data may contain thousands\nof user sessions and each user session may consist of hundreds of URL accesses,\ndimensionality reduction is achieved by eliminating the low support URLs. Very\nsmall sessions are also removed in order to filter out the noise from the data.\nBut direct elimination of low support URLs and small sized sessions may results\nin loss of a significant amount of information especially when the count of low\nsupport URLs and small sessions is large. We propose a fuzzy solution to deal\nwith this problem by assigning weights to URLs and user sessions based on a\nfuzzy membership function. After assigning the weights we apply a Fuzzy c-Mean\nClustering algorithm to discover the clusters of user profiles. In this paper,\nwe describe our fuzzy set theoretic approach to perform feature selection (or\ndimensionality reduction) and session weight assignment. Finally we compare our\nsoft computing based approach of dimensionality reduction with the traditional\napproach of direct elimination of small sessions and low support count URLs.\nOur results show that fuzzy feature evaluation and dimensionality reduction\nresults in better performance and validity indices for the discovered clusters.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 09:56:02 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Ansari", "Zahid", ""], ["Azeem", "M. F.", ""], ["Babu", "A. Vinaya", ""], ["Ahmed", "Waseem", ""]]}, {"id": "1509.00692", "submitter": "Zahid Ansari", "authors": "Zahid Ansari, Waseem Ahmed, M.F. Azeem and A.Vinaya Babu", "title": "Discovery of Web Usage Profiles Using Various Clustering Techniques", "comments": "arXiv admin note: substantial text overlap with arXiv:1507.03340", "journal-ref": "International Journal of Computer Information Systems, pp. 18-27\n  Vol. 1, No. 3, July 2011. (ISSN 2229-5208, Silicon Valley Publishers, United\n  Kingdom)", "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive growth of World Wide Web (WWW) has necessitated the development\nof Web personalization systems in order to understand the user preferences to\ndynamically serve customized content to individual users. To reveal information\nabout user preferences from Web usage data, Web Usage Mining (WUM) techniques\nare extensively being applied to the Web log data. Clustering techniques are\nwidely used in WUM to capture similar interests and trends among users\naccessing a Web site. Clustering aims to divide a data set into groups or\nclusters where inter-cluster similarities are minimized while the intra cluster\nsimilarities are maximized. This paper reviews four of the popularly used\nclustering techniques: k-Means, k-Medoids, Leader and DBSCAN. These techniques\nare implemented and tested against the Web user navigational data. Performance\nand validity results of each technique are presented and compared.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 09:31:37 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Ansari", "Zahid", ""], ["Ahmed", "Waseem", ""], ["Azeem", "M. F.", ""], ["Babu", "A. Vinaya", ""]]}, {"id": "1509.00693", "submitter": "Zahid Ansari", "authors": "Zahid Ansari, Mohammad Fazle Azeem, A. Vinaya Babu and Waseem Ahmed", "title": "A Fuzzy Clustering Based Approach for Mining Usage Profiles from Web Log\n  Data", "comments": null, "journal-ref": "International Journal of Computer Science and Information\n  Security, pp. 70-79 Vol. 9, No. 6, June 2011. (ISSN 1947-5500, IJCSIS\n  Publications, United State)", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web continues to grow at an amazing rate in both the size and\ncomplexity of Web sites and is well on its way to being the main reservoir of\ninformation and data. Due to this increase in growth and complexity of WWW, web\nsite publishers are facing increasing difficulty in attracting and retaining\nusers. To design popular and attractive websites publishers must understand\ntheir users needs. Therefore analyzing users behaviour is an important part of\nweb page design. Web Usage Mining (WUM) is the application of data mining\ntechniques to web usage log repositories in order to discover the usage\npatterns that can be used to analyze the users navigational behavior. WUM\ncontains three main steps: preprocessing, knowledge extraction and results\nanalysis. The goal of the preprocessing stage in Web usage mining is to\ntransform the raw web log data into a set of user profiles. Each such profile\ncaptures a sequence or a set of URLs representing a user session.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 09:13:23 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Ansari", "Zahid", ""], ["Azeem", "Mohammad Fazle", ""], ["Babu", "A. Vinaya", ""], ["Ahmed", "Waseem", ""]]}, {"id": "1509.01208", "submitter": "Da Kuang", "authors": "Da Kuang, Barry Drake, Haesun Park", "title": "Fast Clustering and Topic Modeling Based on Rank-2 Nonnegative Matrix\n  Factorization", "comments": "This paper has been withdrawn by the author to clarify the authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of unsupervised clustering and topic modeling is well\nrecognized with ever-increasing volumes of text data. In this paper, we propose\na fast method for hierarchical clustering and topic modeling called HierNMF2.\nOur method is based on fast Rank-2 nonnegative matrix factorization (NMF) that\nperforms binary clustering and an efficient node splitting rule. Further\nutilizing the final leaf nodes generated in HierNMF2 and the idea of\nnonnegative least squares fitting, we propose a new clustering/topic modeling\nmethod called FlatNMF2 that recovers a flat clustering/topic modeling result in\na very simple yet significantly more effective way than any other existing\nmethods. We implement highly optimized open source software in C++ for both\nHierNMF2 and FlatNMF2 for hierarchical and partitional clustering/topic\nmodeling of document data sets.\n  Substantial experimental tests are presented that illustrate significant\nimprovements both in computational time as well as quality of solutions. We\ncompare our methods to other clustering methods including K-means, standard\nNMF, and CLUTO, and also topic modeling methods including latent Dirichlet\nallocation (LDA) and recently proposed algorithms for NMF with separability\nconstraints. Overall, we present efficient tools for analyzing large-scale data\nsets, and techniques that can be generalized to many other data analytics\nproblem domains.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 18:55:28 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 04:29:04 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2015 18:06:13 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Kuang", "Da", ""], ["Drake", "Barry", ""], ["Park", "Haesun", ""]]}, {"id": "1509.01288", "submitter": "Max Zimmermann", "authors": "Max Zimmermann, Eirini Ntoutsi, Myra Spiliopoulou", "title": "Incremental Active Opinion Learning Over a Stream of Opinionated\n  Documents", "comments": "10 pages, 14 figures, conference: WISDOM (KDD'15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications that learn from opinionated documents, like tweets or product\nreviews, face two challenges. First, the opinionated documents constitute an\nevolving stream, where both the author's attitude and the vocabulary itself may\nchange. Second, labels of documents are scarce and labels of words are\nunreliable, because the sentiment of a word depends on the (unknown) context in\nthe author's mind. Most of the research on mining over opinionated streams\nfocuses on the first aspect of the problem, whereas for the second a continuous\nsupply of labels from the stream is assumed. Such an assumption though is\nutopian as the stream is infinite and the labeling cost is prohibitive. To this\nend, we investigate the potential of active stream learning algorithms that ask\nfor labels on demand. Our proposed ACOSTREAM 1 approach works with limited\nlabels: it uses an initial seed of labeled documents, occasionally requests\nadditional labels for documents from the human expert and incrementally adapts\nto the underlying stream while exploiting the available labeled documents. In\nits core, ACOSTREAM consists of a MNB classifier coupled with \"sampling\"\nstrategies for requesting class labels for new unlabeled documents. In the\nexperiments, we evaluate the classifier performance over time by varying: (a)\nthe class distribution of the opinionated stream, while assuming that the set\nof the words in the vocabulary is fixed but their polarities may change with\nthe class distribution; and (b) the number of unknown words arriving at each\nmoment, while the class polarity may also change. Our results show that active\nlearning on a stream of opinionated documents, delivers good performance while\nrequiring a small selection of labels\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 22:11:10 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Zimmermann", "Max", ""], ["Ntoutsi", "Eirini", ""], ["Spiliopoulou", "Myra", ""]]}, {"id": "1509.01476", "submitter": "Manuel Sebastian Mariani", "authors": "Manuel Sebastian Mariani, Matus Medo, Yi-Cheng Zhang", "title": "Ranking nodes in growing networks: When PageRank fails", "comments": "Article + Supplementary Information", "journal-ref": "Scientific Reports 5, 16181 (2015)", "doi": "10.1038/srep16181", "report-no": null, "categories": "physics.soc-ph cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PageRank is arguably the most popular ranking algorithm which is being\napplied in real systems ranging from information to biological and\ninfrastructure networks. Despite its outstanding popularity and broad use in\ndifferent areas of science, the relation between the algorithm's efficacy and\nproperties of the network on which it acts has not yet been fully understood.\nWe study here PageRank's performance on a network model supported by real data,\nand show that realistic temporal effects make PageRank fail in individuating\nthe most valuable nodes for a broad range of model parameters. Results on real\ndata are in qualitative agreement with our model-based findings. This failure\nof PageRank reveals that the static approach to information filtering is\ninappropriate for a broad class of growing systems, and suggest that\ntime-dependent algorithms that are based on the temporal linking patterns of\nthese systems are needed to better rank the nodes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 12:09:25 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Mariani", "Manuel Sebastian", ""], ["Medo", "Matus", ""], ["Zhang", "Yi-Cheng", ""]]}, {"id": "1509.01649", "submitter": "Valerii Garnaga", "authors": "Valerii Garnaga", "title": "Using of Neuro-Indexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The article describes a new data structure called neuro-index. It is an\nalternative to well-known file indexes. The neuro-index is fundamentally\ndifferent because it stores weight coefficients in neural network. It is not a\nreference type like \"keyword-position in a file\".\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2015 00:52:43 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Garnaga", "Valerii", ""]]}, {"id": "1509.01771", "submitter": "Gibran Fuentes-Pineda", "authors": "Gibran Fuentes-Pineda and Ivan Vladimir Meza-Ruiz", "title": "Sampled Weighted Min-Hashing for Large-Scale Topic Mining", "comments": "10 pages, Proceedings of the Mexican Conference on Pattern\n  Recognition 2015", "journal-ref": null, "doi": "10.1007/978-3-319-19264-2_20", "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Sampled Weighted Min-Hashing (SWMH), a randomized approach to\nautomatically mine topics from large-scale corpora. SWMH generates multiple\nrandom partitions of the corpus vocabulary based on term co-occurrence and\nagglomerates highly overlapping inter-partition cells to produce the mined\ntopics. While other approaches define a topic as a probabilistic distribution\nover a vocabulary, SWMH topics are ordered subsets of such vocabulary.\nInterestingly, the topics mined by SWMH underlie themes from the corpus at\ndifferent levels of granularity. We extensively evaluate the meaningfulness of\nthe mined topics both qualitatively and quantitatively on the NIPS (1.7 K\ndocuments), 20 Newsgroups (20 K), Reuters (800 K) and Wikipedia (4 M) corpora.\nAdditionally, we compare the quality of SWMH with Online LDA topics for\ndocument representation in classification.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 06:09:28 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2015 03:14:12 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Fuentes-Pineda", "Gibran", ""], ["Meza-Ruiz", "Ivan Vladimir", ""]]}, {"id": "1509.01865", "submitter": "Alex Olieman", "authors": "Alex Olieman, Jaap Kamps, Maarten Marx, and Arjan Nusselder", "title": "A Hybrid Approach to Domain-Specific Entity Linking", "comments": "SEM'15", "journal-ref": "Proc. Posters and Demos track of 11th Int. Conf. on Semantic\n  Systems (2015) 55-58", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current state-of-the-art Entity Linking (EL) systems are geared towards\ncorpora that are as heterogeneous as the Web, and therefore perform\nsub-optimally on domain-specific corpora. A key open problem is how to\nconstruct effective EL systems for specific domains, as knowledge of the local\ncontext should in principle increase, rather than decrease, effectiveness. In\nthis paper we propose the hybrid use of simple specialist linkers in\ncombination with an existing generalist system to address this problem. Our\nmain findings are the following. First, we construct a new reusable benchmark\nfor EL on a corpus of domain-specific conversations. Second, we test the\nperformance of a range of approaches under the same conditions, and show that\nspecialist linkers obtain high precision in isolation, and high recall when\ncombined with generalist linkers. Hence, we can effectively exploit local\ncontext and get the best of both worlds.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 23:16:45 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Olieman", "Alex", ""], ["Kamps", "Jaap", ""], ["Marx", "Maarten", ""], ["Nusselder", "Arjan", ""]]}, {"id": "1509.02010", "submitter": "Alex Olieman", "authors": "Alex Olieman, Jaap Kamps, and Rosa Merino Claros", "title": "LocLinkVis: A Geographic Information Retrieval-Based System for\n  Large-Scale Exploratory Search", "comments": "SEM'15", "journal-ref": "Proc. Posters and Demos Track of 11th Int. Conf. on Semantic\n  Systems (2015) 30-33", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present LocLinkVis (Locate-Link-Visualize); a system which\nsupports exploratory information access to a document collection based on\ngeo-referencing and visualization. It uses a gazetteer which contains\nrepresentations of places ranging from countries to buildings, and that is used\nto recognize toponyms, disambiguate them into places, and to visualize the\nresulting spatial footprints.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 12:36:19 GMT"}, {"version": "v2", "created": "Sat, 26 Sep 2015 15:16:54 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Olieman", "Alex", ""], ["Kamps", "Jaap", ""], ["Claros", "Rosa Merino", ""]]}, {"id": "1509.02207", "submitter": "Fredrik Nyg{\\aa}rd Carlsen", "authors": "Fredrik Nyg{\\aa}rd Carlsen", "title": "Personalized Search", "comments": null, "journal-ref": null, "doi": null, "report-no": "CERN-IT-Note-2015-007", "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the volume of electronically available information grows, relevant items\nbecome harder to find. This work presents an approach to personalizing search\nresults in scientific publication databases. This work focuses on re-ranking\nsearch results from existing search engines like Solr or ElasticSearch. This\nwork also includes the development of Obelix, a new recommendation system used\nto re-rank search results. The project was proposed and performed at CERN,\nusing the scientific publications available on the CERN Document Server (CDS).\nThis work experiments with re-ranking using offline and online evaluation of\nusers and documents in CDS. The experiments conclude that the personalized\nsearch result outperform both latest first and word similarity in terms of\nclick position in the search result for global search in CDS.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 22:20:47 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Carlsen", "Fredrik Nyg\u00e5rd", ""]]}, {"id": "1509.02218", "submitter": "Mitsuo Yoshida", "authors": "Mitsuo Yoshida, Yuki Arase, Takaaki Tsunoda, Mikio Yamamoto", "title": "Wikipedia Page View Reflects Web Search Trend", "comments": "2 pages, 4 figures, The 2015 ACM Web Science Conference (WebSci15)", "journal-ref": null, "doi": "10.1145/2786451.2786495", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The frequency of a web search keyword generally reflects the degree of public\ninterest in a particular subject matter. Search logs are therefore useful\nresources for trend analysis. However, access to search logs is typically\nrestricted to search engine providers. In this paper, we investigate whether\nsearch frequency can be estimated from a different resource such as Wikipedia\npage views of open data. We found frequently searched keywords to have\nremarkably high correlations with Wikipedia page views. This suggests that\nWikipedia page views can be an effective tool for determining popular global\nweb search trends.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 22:59:28 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Yoshida", "Mitsuo", ""], ["Arase", "Yuki", ""], ["Tsunoda", "Takaaki", ""], ["Yamamoto", "Mikio", ""]]}, {"id": "1509.02437", "submitter": "Rishabh Soni", "authors": "Rishabh Soni, K. James Mathai", "title": "Improved Twitter Sentiment Prediction through Cluster-then-Predict Model", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade humans have experienced exponential growth in the use of\nonline resources, in particular social media and microblogging websites such as\nFacebook, Twitter, YouTube and also mobile applications such as WhatsApp, Line,\netc. Many companies have identified these resources as a rich mine of marketing\nknowledge. This knowledge provides valuable feedback which allows them to\nfurther develop the next generation of their product. In this paper, sentiment\nanalysis of a product is performed by extracting tweets about that product and\nclassifying the tweets showing it as positive and negative sentiment. The\nauthors propose a hybrid approach which combines unsupervised learning in the\nform of K-means clustering to cluster the tweets and then performing supervised\nlearning methods such as Decision Trees and Support Vector Machines for\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 16:36:04 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Soni", "Rishabh", ""], ["Mathai", "K. James", ""]]}, {"id": "1509.02897", "submitter": "Ilya Razenshteyn", "authors": "Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn,\n  Ludwig Schmidt", "title": "Practical and Optimal LSH for Angular Distance", "comments": "22 pages, an extended abstract is to appear in the proceedings of the\n  29th Annual Conference on Neural Information Processing Systems (NIPS 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show the existence of a Locality-Sensitive Hashing (LSH) family for the\nangular distance that yields an approximate Near Neighbor Search algorithm with\nthe asymptotically optimal running time exponent. Unlike earlier algorithms\nwith this property (e.g., Spherical LSH [Andoni, Indyk, Nguyen, Razenshteyn\n2014], [Andoni, Razenshteyn 2015]), our algorithm is also practical, improving\nupon the well-studied hyperplane LSH [Charikar, 2002] in practice. We also\nintroduce a multiprobe version of this algorithm, and conduct experimental\nevaluation on real and synthetic data sets.\n  We complement the above positive results with a fine-grained lower bound for\nthe quality of any LSH family for angular distance. Our lower bound implies\nthat the above LSH family exhibits a trade-off between evaluation time and\nquality that is close to optimal for a natural class of LSH functions.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 19:24:33 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Andoni", "Alexandr", ""], ["Indyk", "Piotr", ""], ["Laarhoven", "Thijs", ""], ["Razenshteyn", "Ilya", ""], ["Schmidt", "Ludwig", ""]]}, {"id": "1509.03844", "submitter": "Yiannis Andreopoulos", "authors": "Alhabib Abbas, Nikos Deligiannis and Yiannis Andreopoulos", "title": "Vectors of Locally Aggregated Centers for Compact Video Representation", "comments": "Proc. IEEE International Conference on Multimedia and Expo, ICME\n  2015, Torino, Italy", "journal-ref": null, "doi": "10.1109/ICME.2015.7177501", "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel vector aggregation technique for compact video\nrepresentation, with application in accurate similarity detection within large\nvideo datasets. The current state-of-the-art in visual search is formed by the\nvector of locally aggregated descriptors (VLAD) of Jegou et. al. VLAD generates\ncompact video representations based on scale-invariant feature transform (SIFT)\nvectors (extracted per frame) and local feature centers computed over a\ntraining set. With the aim to increase robustness to visual distortions, we\npropose a new approach that operates at a coarser level in the feature\nrepresentation. We create vectors of locally aggregated centers (VLAC) by first\nclustering SIFT features to obtain local feature centers (LFCs) and then\nencoding the latter with respect to given centers of local feature centers\n(CLFCs), extracted from a training set. The sum-of-differences between the LFCs\nand the CLFCs are aggregated to generate an extremely-compact video description\nused for accurate video segment similarity detection. Experimentation using a\nvideo dataset, comprising more than 1000 minutes of content from the Open Video\nProject, shows that VLAC obtains substantial gains in terms of mean Average\nPrecision (mAP) against VLAD and the hyper-pooling method of Douze et. al.,\nunder the same compaction factor and the same set of distortions.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2015 13:06:36 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Abbas", "Alhabib", ""], ["Deligiannis", "Nikos", ""], ["Andreopoulos", "Yiannis", ""]]}, {"id": "1509.04219", "submitter": "Afroze Ibrahim Baqapuri", "authors": "Afroze Ibrahim Baqapuri", "title": "Twitter Sentiment Analysis", "comments": "Bachelors Thesis Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project addresses the problem of sentiment analysis in twitter; that is\nclassifying tweets according to the sentiment expressed in them: positive,\nnegative or neutral. Twitter is an online micro-blogging and social-networking\nplatform which allows users to write short status updates of maximum length 140\ncharacters. It is a rapidly expanding service with over 200 million registered\nusers - out of which 100 million are active users and half of them log on\ntwitter on a daily basis - generating nearly 250 million tweets per day. Due to\nthis large amount of usage we hope to achieve a reflection of public sentiment\nby analysing the sentiments expressed in the tweets. Analysing the public\nsentiment is important for many applications such as firms trying to find out\nthe response of their products in the market, predicting political elections\nand predicting socioeconomic phenomena like stock exchange. The aim of this\nproject is to develop a functional classifier for accurate and automatic\nsentiment classification of an unknown tweet stream.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 17:39:37 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Baqapuri", "Afroze Ibrahim", ""]]}, {"id": "1509.04524", "submitter": "Fran\\c{c}ois Renaville", "authors": "Fran\\c{c}ois Renaville", "title": "Open Access and Discovery Tools: How do Primo Libraries Manage Green\n  Open Access Collections?", "comments": "24 pages, 8 figures, 1 appendix", "journal-ref": "Varnum, Ken (ed.). (2016). Exploring Discovery: The Front Door to\n  Your Library's Licensed and Digitized Content\". ALA Editions. 233-256 pp", "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scholarly Open Access repositories contain lots of treasures including rare\nor otherwise unpublished materials and articles that scholars self-archive,\noften as part of their institution's mandate. But it can be hard to discover\nthis material unless users know exactly where to look. Since the very\nbeginning, libraries have played a major role in supporting the OA movement.\nNext to all services they can provide to support the deposit of research output\nin the repositories, they can make Open Access materials widely discoverable by\ntheir patrons through general search engines (Google, Bing...), specialized\nsearch engines (like Google Scholar) and library discovery tools, thus\nexpanding their collection to include materials that they would not necessarily\npay for. In this paper, we intend to focus on two aspects regarding Open Access\nand Primo discovery tool. In early 2013, Ex Libris Group started to add\ninstitutional repositories to Primo Central Index (PCI), their mega-aggregation\nof hundreds of millions of scholarly e-resources. After 2 years, it may be\ninteresting to take stock of the current situation of PCI regarding Open Access\ninstitutional repositories. On basis of a survey to carry out among the Primo\ncommunity, the paper also shows how libraries using Primo discovery tool\nintegrate Green Open Access contents in their catalog. Two major ways are\npossible for them: Firstly, they can directly harvest, index and manage any\nrepository in their Primo and display those free contents next to the more\ntraditional library collections; Secondly, if they are PCI subscribers, they\ncan quickly and easily activate any, if not all, of the Open Access\nrepositories contained PCI, making thus the contents of those directly\ndiscoverable to their end users.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 12:44:09 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2016 13:22:29 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Renaville", "Fran\u00e7ois", ""]]}, {"id": "1509.04525", "submitter": "Mazen Alsarem", "authors": "Mazen Alsarem (LIRIS), Pierre-Edouard Portier (LIRIS), Sylvie\n  Calabretto (LIRIS), Harald Kosch (FMI)", "title": "Ranking Entities in the Age of Two Webs, an Application to Semantic\n  Snippets", "comments": null, "journal-ref": "Extended Semantic Web Conference ESWC2015, May 2015, Portoroz,\n  Slovenia. 9088, pp.541-555, 2015, The Semantic Web. Latest Advances and New\n  Domains", "doi": "10.1007/978-3-319-18818-8_33", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advances of the Linked Open Data (LOD) initiative are giving rise to a\nmore structured Web of data. Indeed, a few datasets act as hubs (e.g., DBpedia)\nconnecting many other datasets. They also made possible new Web services for\nentity detection inside plain text (e.g., DBpedia Spotlight), thus allowing for\nnew applications that can benefit from a combination of the Web of documents\nand the Web of data. To ease the emergence of these new applications, we\npropose a query-biased algorithm (LDRANK) for the ranking of web of data\nresources with associated textual data. Our algorithm combines link analysis\nwith dimensionality reduction. We use crowdsourcing for building a publicly\navailable and reusable dataset for the evaluation of query-biased ranking of\nWeb of data resources detected in Web pages. We show that, on this dataset,\nLDRANK outperforms the state of the art. Finally, we use this algorithm for the\nconstruction of semantic snippets of which we evaluate the usefulness with a\ncrowdsourcing-based approach.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 12:45:41 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Alsarem", "Mazen", "", "LIRIS"], ["Portier", "Pierre-Edouard", "", "LIRIS"], ["Calabretto", "Sylvie", "", "LIRIS"], ["Kosch", "Harald", "", "FMI"]]}, {"id": "1509.04581", "submitter": "Zhen Liu", "authors": "Zhen Liu", "title": "Kernelized Deep Convolutional Neural Network for Describing Complex\n  Images", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the impressive capability to capture visual content, deep convolutional\nneural networks (CNN) have demon- strated promising performance in various\nvision-based ap- plications, such as classification, recognition, and objec- t\ndetection. However, due to the intrinsic structure design of CNN, for images\nwith complex content, it achieves lim- ited capability on invariance to\ntranslation, rotation, and re-sizing changes, which is strongly emphasized in\nthe s- cenario of content-based image retrieval. In this paper, to address this\nproblem, we proposed a new kernelized deep convolutional neural network. We\nfirst discuss our motiva- tion by an experimental study to demonstrate the\nsensitivi- ty of the global CNN feature to the basic geometric trans-\nformations. Then, we propose to represent visual content with approximate\ninvariance to the above geometric trans- formations from a kernelized\nperspective. We extract CNN features on the detected object-like patches and\naggregate these patch-level CNN features to form a vectorial repre- sentation\nwith the Fisher vector model. The effectiveness of our proposed algorithm is\ndemonstrated on image search application with three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 14:35:11 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Liu", "Zhen", ""]]}, {"id": "1509.04640", "submitter": "Laurent Charlin", "authors": "Laurent Charlin and Rajesh Ranganath and James McInerney and David M.\n  Blei", "title": "Dynamic Poisson Factorization", "comments": "RecSys 2015", "journal-ref": null, "doi": "10.1145/2792838.2800174", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for recommender systems use latent factors to explain the preferences\nand behaviors of users with respect to a set of items (e.g., movies, books,\nacademic papers). Typically, the latent factors are assumed to be static and,\ngiven these factors, the observed preferences and behaviors of users are\nassumed to be generated without order. These assumptions limit the explorative\nand predictive capabilities of such models, since users' interests and item\npopularity may evolve over time. To address this, we propose dPF, a dynamic\nmatrix factorization model based on the recent Poisson factorization model for\nrecommendations. dPF models the time evolving latent factors with a Kalman\nfilter and the actions with Poisson distributions. We derive a scalable\nvariational inference algorithm to infer the latent factors. Finally, we\ndemonstrate dPF on 10 years of user click data from arXiv.org, one of the\nlargest repository of scientific papers and a formidable source of information\nabout the behavior of scientists. Empirically we show performance improvement\nover both static and, more recently proposed, dynamic recommendation models. We\nalso provide a thorough exploration of the inferred posteriors over the latent\nvariables.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 16:57:15 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Charlin", "Laurent", ""], ["Ranganath", "Rajesh", ""], ["McInerney", "James", ""], ["Blei", "David M.", ""]]}, {"id": "1509.04811", "submitter": "Tadele Damessie T", "authors": "Tadele Tedla", "title": "amLite: Amharic Transliteration Using Key Map Dictionary", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  amLite is a framework developed to map ASCII transliterated Amharic texts\nback to the original Amharic letter texts. The aim of such a framework is to\nmake existing Amharic linguistic data consistent and interoperable among\nresearchers. For achieving the objective, a key map dictionary is constructed\nusing the possible ASCII combinations actively in use for transliterating\nAmharic letters; and a mapping of the combinations to the corresponding Amharic\nletters is done. The mapping is then used to replace the Amharic linguistic\ntext back to form the original Amharic letters text. The framework indicated\n97.7, 99.7 and 98.4 percentage accuracy on converting the three sample random\ntest data. It is; however, possible to improve the accuracy of the framework by\nadding an exception to the implementation of the algorithm, or by preprocessing\nthe input text prior to conversion. This paper outlined the rationales behind\nthe need for developing the framework and the processes undertaken in the\ndevelopment.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 05:00:59 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Tedla", "Tadele", ""]]}, {"id": "1509.04956", "submitter": "J.  Miguel Diaz-Banez", "authors": "Francisco G\\'omez, Joaqu\\'in Mora, Emilia G\\'omez, Jos\\'e Miguel\n  D\\'iaz-B\\'a\\~nez", "title": "Melodic Contour and Mid-Level Global Features Applied to the Analysis of\n  Flamenco Cantes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the topic of melodic characterization and similarity in\na specific musical repertoire: a cappella flamenco singing, more specifically\nin debla and martinete styles. We propose the combination of manual and\nautomatic description. First, we use a state-of-the-art automatic transcription\nmethod to account for general melodic similarity from music recordings. Second,\nwe define a specific set of representative mid-level melodic features, which\nare manually labeled by flamenco experts. Both approaches are then contrasted\nand combined into a global similarity measure. This similarity measure is\nassessed by inspecting the clusters obtained through phylogenetic algorithms\nalgorithms and by relating similarity to categorization in terms of style.\nFinally, we discuss the advantage of combining automatic and expert annotations\nas well as the need to include repertoire-specific descriptions for meaningful\nmelodic characterization in traditional music collections.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 15:56:22 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["G\u00f3mez", "Francisco", ""], ["Mora", "Joaqu\u00edn", ""], ["G\u00f3mez", "Emilia", ""], ["D\u00edaz-B\u00e1\u00f1ez", "Jos\u00e9 Miguel", ""]]}, {"id": "1509.05567", "submitter": "Dipasree Pal", "authors": "Dipasree Pal, Mandar Mitra and Samar Bhattacharya", "title": "Exploring Query Categorisation for Query Expansion: A Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vocabulary mismatch problem is one of the important challenges facing\ntraditional keyword-based Information Retrieval Systems. The aim of query\nexpansion (QE) is to reduce this query-document mismatch by adding related or\nsynonymous words or phrases to the query.\n  Several existing query expansion algorithms have proved their merit, but they\nare not uniformly beneficial for all kinds of queries. Our long-term goal is to\nformulate methods for applying QE techniques tailored to individual queries,\nrather than applying the same general QE method to all queries. As an initial\nstep, we have proposed a taxonomy of query classes (from a QE perspective) in\nthis report. We have discussed the properties of each query class with\nexamples. We have also discussed some QE strategies that might be effective for\neach query category.\n  In future work, we intend to test the proposed techniques using standard\ndatasets, and to explore automatic query categorisation methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 10:04:09 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Pal", "Dipasree", ""], ["Mitra", "Mandar", ""], ["Bhattacharya", "Samar", ""]]}, {"id": "1509.05671", "submitter": "Yuncheng Li", "authors": "Yuncheng Li, Yang Cong, Tao Mei, Jiebo Luo", "title": "User-Curated Image Collections: Modeling and Recommendation", "comments": "in IEEE BigData 2015", "journal-ref": null, "doi": "10.1109/BigData.2015.7363803", "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art image retrieval and recommendation systems\npredominantly focus on individual images. In contrast, socially curated image\ncollections, condensing distinctive yet coherent images into one set, are\nlargely overlooked by the research communities. In this paper, we aim to design\na novel recommendation system that can provide users with image collections\nrelevant to individual personal preferences and interests. To this end, two key\nissues need to be addressed, i.e., image collection modeling and similarity\nmeasurement. For image collection modeling, we consider each image collection\nas a whole in a group sparse reconstruction framework and extract concise\ncollection descriptors given the pretrained dictionaries. We then consider\nimage collection recommendation as a dynamic similarity measurement problem in\nresponse to user's clicked image set, and employ a metric learner to measure\nthe similarity between the image collection and the clicked image set. As there\nis no previous work directly comparable to this study, we implement several\ncompetitive baselines and related methods for comparison. The evaluations on a\nlarge scale Pinterest data set have validated the effectiveness of our proposed\nmethods for modeling and recommending image collections.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 15:45:41 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Li", "Yuncheng", ""], ["Cong", "Yang", ""], ["Mei", "Tao", ""], ["Luo", "Jiebo", ""]]}, {"id": "1509.06041", "submitter": "Quanzeng You", "authors": "Quanzeng You, Jiebo Luo, Hailin Jin, Jianchao Yang", "title": "Robust Image Sentiment Analysis Using Progressively Trained and Domain\n  Transferred Deep Networks", "comments": "9 pages, 5 figures, AAAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis of online user generated content is important for many\nsocial media analytics tasks. Researchers have largely relied on textual\nsentiment analysis to develop systems to predict political elections, measure\neconomic indicators, and so on. Recently, social media users are increasingly\nusing images and videos to express their opinions and share their experiences.\nSentiment analysis of such large scale visual content can help better extract\nuser sentiments toward events or topics, such as those in image tweets, so that\nprediction of sentiment from visual content is complementary to textual\nsentiment analysis. Motivated by the needs in leveraging large scale yet noisy\ntraining data to solve the extremely challenging problem of image sentiment\nanalysis, we employ Convolutional Neural Networks (CNN). We first design a\nsuitable CNN architecture for image sentiment analysis. We obtain half a\nmillion training samples by using a baseline sentiment algorithm to label\nFlickr images. To make use of such noisy machine labeled data, we employ a\nprogressive strategy to fine-tune the deep network. Furthermore, we improve the\nperformance on Twitter images by inducing domain transfer with a small number\nof manually labeled Twitter images. We have conducted extensive experiments on\nmanually labeled Twitter images. The results show that the proposed CNN can\nachieve better performance in image sentiment analysis than competing\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 18:36:01 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["You", "Quanzeng", ""], ["Luo", "Jiebo", ""], ["Jin", "Hailin", ""], ["Yang", "Jianchao", ""]]}, {"id": "1509.06553", "submitter": "Vidyadhar Rao", "authors": "Vidyadhar Rao, Prateek Jain, C.V Jawahar", "title": "Diverse Yet Efficient Retrieval using Hash Functions", "comments": "10 pages", "journal-ref": null, "doi": "10.1145/2911996.2911998", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical retrieval systems have three requirements: a) Accurate retrieval\ni.e., the method should have high precision, b) Diverse retrieval, i.e., the\nobtained set of points should be diverse, c) Retrieval time should be small.\nHowever, most of the existing methods address only one or two of the above\nmentioned requirements. In this work, we present a method based on randomized\nlocality sensitive hashing which tries to address all of the above requirements\nsimultaneously. While earlier hashing approaches considered approximate\nretrieval to be acceptable only for the sake of efficiency, we argue that one\ncan further exploit approximate retrieval to provide impressive trade-offs\nbetween accuracy and diversity. We extend our method to the problem of\nmulti-label prediction, where the goal is to output a diverse and accurate set\nof labels for a given document in real-time. Moreover, we introduce a new\nnotion to simultaneously evaluate a method's performance for both the precision\nand diversity measures. Finally, we present empirical results on several\ndifferent retrieval tasks and show that our method retrieves diverse and\naccurate images/labels while ensuring $100x$-speed-up over the existing diverse\nretrieval approaches.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 11:38:20 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2015 17:05:25 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Rao", "Vidyadhar", ""], ["Jain", "Prateek", ""], ["Jawahar", "C. V", ""]]}, {"id": "1509.06847", "submitter": "Manvi", "authors": "Manvi, Komal Kumar Bhatia, Ashutosh Dixit", "title": "Design and Implementation of Domain based Semantic Hidden Web Crawler", "comments": "12 pages,10 figures", "journal-ref": "IJIACS 2015 Volume 4 Special Issue ICRDESM-15 Paper id: 9D2N6Y", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web is a wide term which mainly consists of surface web and hidden web. One\ncan easily access the surface web using traditional web crawlers, but they are\nnot able to crawl the hidden portion of the web. These traditional crawlers\nretrieve contents from web pages, which are linked by hyperlinks ignoring the\ninformation hidden behind form pages, which cannot be extracted using simple\nhyperlink structure. Thus, they ignore large amount of data hidden behind\nsearch forms. This paper emphasizes on the extraction of hidden data behind\nhtml search forms. The proposed technique makes use of semantic mapping to fill\nthe html search form using domain specific database. Using semantics to fill\nvarious fields of a form leads to more accurate and qualitative data\nextraction.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 05:26:44 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Manvi", "", ""], ["Bhatia", "Komal Kumar", ""], ["Dixit", "Ashutosh", ""]]}, {"id": "1509.07344", "submitter": "Md Abul Hasnat", "authors": "Md. Abul Hasnat, Julien Velcin, St\\'ephane Bonnevay and Julien Jacques", "title": "Opinion mining from twitter data using evolutionary multinomial mixture\n  models", "comments": "Submitted to the Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image of an entity can be defined as a structured and dynamic representation\nwhich can be extracted from the opinions of a group of users or population.\nAutomatic extraction of such an image has certain importance in political\nscience and sociology related studies, e.g., when an extended inquiry from\nlarge-scale data is required. We study the images of two politically\nsignificant entities of France. These images are constructed by analyzing the\nopinions collected from a well known social media called Twitter. Our goal is\nto build a system which can be used to automatically extract the image of\nentities over time.\n  In this paper, we propose a novel evolutionary clustering method based on the\nparametric link among Multinomial mixture models. First we propose the\nformulation of a generalized model that establishes parametric links among the\nMultinomial distributions. Afterward, we follow a model-based clustering\napproach to explore different parametric sub-models and select the best model.\nFor the experiments, first we use synthetic temporal data. Next, we apply the\nmethod to analyze the annotated social media data. Results show that the\nproposed method is better than the state-of-the-art based on the common\nevaluation metrics. Additionally, our method can provide interpretation about\nthe temporal evolution of the clusters.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 12:40:12 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Hasnat", "Md. Abul", ""], ["Velcin", "Julien", ""], ["Bonnevay", "St\u00e9phane", ""], ["Jacques", "Julien", ""]]}, {"id": "1509.07845", "submitter": "Xintong Han", "authors": "Bharat Singh, Xintong Han, Zhe Wu, Vlad I. Morariu and Larry S. Davis", "title": "Selecting Relevant Web Trained Concepts for Automated Event Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex event retrieval is a challenging research problem, especially when no\ntraining videos are available. An alternative to collecting training videos is\nto train a large semantic concept bank a priori. Given a text description of an\nevent, event retrieval is performed by selecting concepts linguistically\nrelated to the event description and fusing the concept responses on unseen\nvideos. However, defining an exhaustive concept lexicon and pre-training it\nrequires vast computational resources. Therefore, recent approaches automate\nconcept discovery and training by leveraging large amounts of weakly annotated\nweb data. Compact visually salient concepts are automatically obtained by the\nuse of concept pairs or, more generally, n-grams. However, not all visually\nsalient n-grams are necessarily useful for an event query--some combinations of\nconcepts may be visually compact but irrelevant--and this drastically affects\nperformance. We propose an event retrieval algorithm that constructs pairs of\nautomatically discovered concepts and then prunes those concepts that are\nunlikely to be helpful for retrieval. Pruning depends both on the query and on\nthe specific video instance being evaluated. Our approach also addresses\ncalibration and domain adaptation issues that arise when applying concept\ndetectors to unseen videos. We demonstrate large improvements over other vision\nbased systems on the TRECVID MED 13 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 19:27:54 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Singh", "Bharat", ""], ["Han", "Xintong", ""], ["Wu", "Zhe", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1509.08396", "submitter": "Jai Manral", "authors": "Jai Manral and Mohammed Alamgir Hossain", "title": "An Innovative Approach for online Meta Search Engine Optimization", "comments": "The 6th Conference on Software, Knowledge, Information Management and\n  Applications, Chengdu, China, September 9-11 2012, #57", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to identify efficient techniques used in Web\nSearch Engine Optimization (SEO). Understanding SEO factors which can influence\npage ranking in search engine is significant for webmasters who wish to attract\nlarge number of users to their website. Different from previous relevant\nresearch, in this study we developed an intelligent Meta search engine which\naggregates results from various search engines and ranks them based on several\nimportant SEO parameters. The research tries to establish that using more SEO\nparameters in ranking algorithms helps in retrieving better search results thus\nincreasing user satisfaction. Initial results generated from Meta search engine\noutperformed existing search engines in terms of better retrieved search\nresults with high precision.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 17:08:28 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Manral", "Jai", ""], ["Hossain", "Mohammed Alamgir", ""]]}, {"id": "1509.08717", "submitter": "Nourh\\`ene Alaya", "authors": "Nourh\\`ene Alaya and Sadok Ben Yahia and Myriam Lamolle", "title": "Towards Unveiling the Ontology Key Features Altering Reasoner\n  Performances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning with ontologies is one of the core fields of research in\nDescription Logics. A variety of efficient reasoner with highly optimized\nalgorithms have been developed to allow inference tasks on expressive ontology\nlanguages such as OWL(DL). However, reasoner reported computing times have\nexceeded and sometimes fall behind the expected theoretical values. From an\nempirical perspective, it is not yet well understood, which particular aspects\nin the ontology are reasoner performance degrading factors. In this paper, we\nconducted an investigation about state of art works that attempted to portray\npotential correlation between reasoner empirical behaviour and particular\nontological features. These works were analysed and then broken down into\ncategories. Further, we proposed a set of ontology features covering a broad\nrange of structural and syntactic ontology characteristics. We claim that these\nfeatures are good indicators of the ontology hardness level against reasoning\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 12:31:03 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Alaya", "Nourh\u00e8ne", ""], ["Yahia", "Sadok Ben", ""], ["Lamolle", "Myriam", ""]]}, {"id": "1509.08881", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Building Subject-aligned Comparable Corpora and Mining it for Truly\n  Parallel Sentence Pairs", "comments": null, "journal-ref": "Procedia Technology, 18, Elsevier, p.126-132, 2014", "doi": "10.1016/j.protcy.2014.11.024", "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel sentences are a relatively scarce but extremely useful resource for\nmany applications including cross-lingual retrieval and statistical machine\ntranslation. This research explores our methodology for mining such data from\npreviously obtained comparable corpora. The task is highly practical since\nnon-parallel multilingual data exist in far greater quantities than parallel\ncorpora, but parallel sentences are a much more useful resource. Here we\npropose a web crawling method for building subject-aligned comparable corpora\nfrom Wikipedia articles. We also introduce a method for extracting truly\nparallel sentences that are filtered out from noisy or just comparable sentence\npairs. We describe our implementation of a specialized tool for this task as\nwell as training and adaption of a machine translation system that supplies our\nfilter with additional information about the similarity of comparable sentence\npairs.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 18:35:49 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.08909", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "Polish -English Statistical Machine Translation of Medical Texts", "comments": "New Research in Multimedia and Internet Systems, Springer. 09/2014,\n  ISSN: 1867-5662. arXiv admin note: text overlap with arXiv:1509.08874", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This new research explores the effects of various training methods on a\nPolish to English Statistical Machine Translation system for medical texts.\nVarious elements of the EMEA parallel text corpora from the OPUS project were\nused as the basis for training of phrase tables and language models and for\ndevelopment, tuning and testing of the translation system. The BLEU, NIST,\nMETEOR, RIBES and TER metrics have been used to evaluate the effects of various\nsystem and data preparations on translation results. Our experiments included\nsystems that used POS tagging, factored phrase models, hierarchical models,\nsyntactic taggers, and many different alignment methods. We also conducted a\ndeep analysis of Polish data as preparatory work for automatic data correction\nsuch as true casing and punctuation normalization phase.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 19:57:24 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.09093", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k, Krzysztof Marasek", "title": "A Sentence Meaning Based Alignment Method for Parallel Text Corpora\n  Preparation", "comments": "corpora filtration, text alignement, corpora improvement. arXiv admin\n  note: text overlap with arXiv:1509.08881", "journal-ref": "Advances in Intelligent Systems and Computing volume 275,\n  p.107-114, Publisher: Springer, ISSN 2194-5357, ISBN 978-3-319-05950-1, 2014", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text alignment is crucial to the accuracy of Machine Translation (MT)\nsystems, some NLP tools or any other text processing tasks requiring bilingual\ndata. This research proposes a language independent sentence alignment approach\nbased on Polish (not position-sensitive language) to English experiments. This\nalignment approach was developed on the TED Talks corpus, but can be used for\nany text domain or language pair. The proposed approach implements various\nheuristics for sentence recognition. Some of them value synonyms and semantic\ntext structure analysis as a part of additional information. Minimization of\ndata loss was ensured. The solution is compared to other sentence alignment\nimplementations. Also an improvement in MT system score with text processed\nwith described tool is shown.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 09:29:51 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1509.09130", "submitter": "Claire Vernade", "authors": "Claire Vernade (LTCI), Olivier Capp\\'e (LTCI)", "title": "Learning From Missing Data Using Selection Bias in Movie Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending items to users is a challenging task due to the large amount of\nmissing information. In many cases, the data solely consist of ratings or tags\nvoluntarily contributed by each user on a very limited subset of the available\nitems, so that most of the data of potential interest is actually missing.\nCurrent approaches to recommendation usually assume that the unobserved data is\nmissing at random. In this contribution, we provide statistical evidence that\nexisting movie recommendation datasets reveal a significant positive\nassociation between the rating of items and the propensity to select these\nitems. We propose a computationally efficient variational approach that makes\nit possible to exploit this selection bias so as to improve the estimation of\nratings from small populations of users. Results obtained with this approach\napplied to neighborhood-based collaborative filtering illustrate its potential\nfor improving the reliability of the recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 11:40:21 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Vernade", "Claire", "", "LTCI"], ["Capp\u00e9", "Olivier", "", "LTCI"]]}]