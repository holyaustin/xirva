[{"id": "1603.00145", "submitter": "Shifeng Liu", "authors": "Shifeng Liu, Zheng Hu, Sujit Dey and Xin Ke", "title": "On Tie Strength Augmented Social Correlation for Inferring Preference of\n  Mobile Telco Users", "comments": "This paper has been modified and the writing may make reader confused", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For mobile telecom operators, it is critical to build preference profiles of\ntheir customers and connected users, which can help operators make better\nmarketing strategies, and provide more personalized services. With the\ndeployment of deep packet inspection (DPI) in telecom networks, it is possible\nfor the telco operators to obtain user online preference. However, DPI has its\nlimitations and user preference derived only from DPI faces sparsity and cold\nstart problems. To better infer the user preference, social correlation in\ntelco users network derived from Call Detailed Records (CDRs) with regard to\nonline preference is investigated. Though widely verified in several online\nsocial networks, social correlation between online preference of users in\nmobile telco networks, where the CDRs derived relationship are of less social\nproperties and user mobile internet surfing activities are not visible to\nneighbourhood, has not been explored at a large scale. Based on a real world\ntelecom dataset including CDRs and preference of more than $550K$ users for\nseveral months, we verified that correlation does exist between online\npreference in such \\textit{ambiguous} social network. Furthermore, we found\nthat the stronger ties that users build, the more similarity between their\npreference may have. After defining the preference inferring task as a Top-$K$\nrecommendation problem, we incorporated Matrix Factorization Collaborative\nFiltering model with social correlation and tie strength based on call patterns\nto generate Top-$K$ preferred categories for users. The proposed Tie Strength\nAugmented Social Recommendation (TSASoRec) model takes data sparsity and cold\nstart user problems into account, considering both the recorded and missing\nrecorded category entries. The experiment on real dataset shows the proposed\nmodel can better infer user preference, especially for cold start users.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 05:20:47 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 22:06:05 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Liu", "Shifeng", ""], ["Hu", "Zheng", ""], ["Dey", "Sujit", ""], ["Ke", "Xin", ""]]}, {"id": "1603.00260", "submitter": "Dhruv Gupta", "authors": "Dhruv Gupta", "title": "Event Search and Analytics: Detecting Events in Semantically Annotated\n  Corpora for Search and Analytics", "comments": "Extended research report of an extended abstract published at WSDM\n  2016 Doctoral Consortium. in WSDM 2016 Proceedings of the Ninth ACM\n  International Conference on Web Search and Data Mining", "journal-ref": null, "doi": "10.1145/2835776.2855083", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, I present the questions that I seek to answer in my PhD\nresearch. I posit to analyze natural language text with the help of semantic\nannotations and mine important events for navigating large text corpora.\nSemantic annotations such as named entities, geographic locations, and temporal\nexpressions can help us mine events from the given corpora. These events thus\nprovide us with useful means to discover the locked knowledge in them. I pose\nthree problems that can help unlock this knowledge vault in semantically\nannotated text corpora: i. identifying important events; ii. semantic search;\nand iii. event analytics.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 13:14:33 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Gupta", "Dhruv", ""]]}, {"id": "1603.00806", "submitter": "Florian Strub", "authors": "Florian Strub (SEQUEL, CRIStAL), Jeremie Mary (CRIStAL, SEQUEL),\n  Romaric Gaudel (LIFL)", "title": "Hybrid Collaborative Filtering with Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Filtering aims at exploiting the feedback of users to provide\npersonalised recommendations. Such algorithms look for latent variables in a\nlarge sparse matrix of ratings. They can be enhanced by adding side information\nto tackle the well-known cold start problem. While Neu-ral Networks have\ntremendous success in image and speech recognition, they have received less\nattention in Collaborative Filtering. This is all the more surprising that\nNeural Networks are able to discover latent variables in large and\nheterogeneous datasets. In this paper, we introduce a Collaborative Filtering\nNeural network architecture aka CFN which computes a non-linear Matrix\nFactorization from sparse rating inputs and side information. We show\nexperimentally on the MovieLens and Douban dataset that CFN outper-forms the\nstate of the art and benefits from side information. We provide an\nimplementation of the algorithm as a reusable plugin for Torch, a popular\nNeural Network framework.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 17:48:25 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2016 19:18:09 GMT"}, {"version": "v3", "created": "Tue, 19 Jul 2016 08:10:08 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Strub", "Florian", "", "SEQUEL, CRIStAL"], ["Mary", "Jeremie", "", "CRIStAL, SEQUEL"], ["Gaudel", "Romaric", "", "LIFL"]]}, {"id": "1603.01336", "submitter": "Sabir Ribas", "authors": "Sabir Ribas, Alberto Ueda, Rodrygo L. T. Santos, Berthier\n  Ribeiro-Neto, Nivio Ziviani", "title": "Simplified Relative Citation Ratio for Static Paper Ranking: UFMG/LATIN\n  at WSDM Cup 2016", "comments": "WSDM Cup. The 9th ACM International Conference on Web Search and Data\n  Mining San Francisco, California, USA. February 22-25, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static rankings of papers play a key role in the academic search setting.\nMany features are commonly used in the literature to produce such rankings,\nsome examples are citation-based metrics, distinct applications of PageRank,\namong others. More recently, learning to rank techniques have been successfully\napplied to combine sets of features producing effective results. In this work,\nwe propose the metric S-RCR, which is a simplified version of a metric called\nRelative Citation Ratio --- both based on the idea of a co-citation network.\nWhen compared to the classical version, our simplification S-RCR leads to\nimproved efficiency with a reasonable effectiveness. We use S-RCR to rank over\n120 million papers in the Microsoft Academic Graph dataset. By using this\nsingle feature, which has no parameters and does not need to be tuned, our team\nwas able to reach the 3rd position in the first phase of the WSDM Cup 2016.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 03:00:46 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Ribas", "Sabir", ""], ["Ueda", "Alberto", ""], ["Santos", "Rodrygo L. T.", ""], ["Ribeiro-Neto", "Berthier", ""], ["Ziviani", "Nivio", ""]]}, {"id": "1603.01774", "submitter": "Behnam Ghavimi", "authors": "Behnam Ghavimi (1,2), Philipp Mayr (1), Sahar Vahdati (2) and\n  Christoph Lange (2,3) ((1) GESIS Leibniz Institute for the Social Sciences,\n  (2) University of Bonn, (3) Fraunhofer IAIS)", "title": "Identifying and Improving Dataset References in Social Sciences Full\n  Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific full text papers are usually stored in separate places than their\nunderlying research datasets. Authors typically make references to datasets by\nmentioning them for example by using their titles and the year of publication.\nHowever, in most cases explicit links that would provide readers with direct\naccess to referenced datasets are missing. Manually detecting references to\ndatasets in papers is time consuming and requires an expert in the domain of\nthe paper. In order to make explicit all links to datasets in papers that have\nbeen published already, we suggest and evaluate a semi-automatic approach for\nfinding references to datasets in social sciences papers. Our approach does not\nneed a corpus of papers (no cold start problem) and it performs well on a small\ntest corpus (gold standard). Our approach achieved an F-measure of 0.84 for\nidentifying references in full texts and an F-measure of 0.83 for finding\ncorrect matches of detected references in the da|ra dataset registry.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 01:09:08 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 12:36:27 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Ghavimi", "Behnam", ""], ["Mayr", "Philipp", ""], ["Vahdati", "Sahar", ""], ["Lange", "Christoph", ""]]}, {"id": "1603.01833", "submitter": "Giuliano Lancioni", "authors": "Giuliano Lancioni, Valeria Pettinari, Laura Garofalo, Marta\n  Campanelli, Ivana Pepe, Simona Olivieri, Ilaria Cicola", "title": "Semi-Automatic Data Annotation, POS Tagging and Mildly Context-Sensitive\n  Disambiguation: the eXtended Revised AraMorph (XRAM)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An extended, revised form of Tim Buckwalter's Arabic lexical and\nmorphological resource AraMorph, eXtended Revised AraMorph (henceforth XRAM),\nis presented which addresses a number of weaknesses and inconsistencies of the\noriginal model by allowing a wider coverage of real-world Classical and\ncontemporary (both formal and informal) Arabic texts. Building upon previous\nresearch, XRAM enhancements include (i) flag-selectable usage markers, (ii)\nprobabilistic mildly context-sensitive POS tagging, filtering, disambiguation\nand ranking of alternative morphological analyses, (iii) semi-automatic\nincrement of lexical coverage through extraction of lexical and morphological\ninformation from existing lexical resources. Testing of XRAM through a\nfront-end Python module showed a remarkable success level.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 14:12:30 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Lancioni", "Giuliano", ""], ["Pettinari", "Valeria", ""], ["Garofalo", "Laura", ""], ["Campanelli", "Marta", ""], ["Pepe", "Ivana", ""], ["Olivieri", "Simona", ""], ["Cicola", "Ilaria", ""]]}, {"id": "1603.01870", "submitter": "Sougata Chaudhuri", "authors": "Sougata Chaudhuri and Georgios Theocharous and Mohammad Ghavamzadeh", "title": "Personalized Advertisement Recommendation: A Ranking Approach to Address\n  the Ubiquitous Click Sparsity Problem", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of personalized advertisement recommendation (PAR),\nwhich consist of a user visiting a system (website) and the system displaying\none of $K$ ads to the user. The system uses an internal ad recommendation\npolicy to map the user's profile (context) to one of the ads. The user either\nclicks or ignores the ad and correspondingly, the system updates its\nrecommendation policy. PAR problem is usually tackled by scalable\n\\emph{contextual bandit} algorithms, where the policies are generally based on\nclassifiers. A practical problem in PAR is extreme click sparsity, due to very\nfew users actually clicking on ads. We systematically study the drawback of\nusing contextual bandit algorithms based on classifier-based policies, in face\nof extreme click sparsity. We then suggest an alternate policy, based on\nrankers, learnt by optimizing the Area Under the Curve (AUC) ranking loss,\nwhich can significantly alleviate the problem of click sparsity. We conduct\nextensive experiments on public datasets, as well as three industry proprietary\ndatasets, to illustrate the improvement in click-through-rate (CTR) obtained by\nusing the ranker-based policy over classifier-based policies.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 20:26:41 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Chaudhuri", "Sougata", ""], ["Theocharous", "Georgios", ""], ["Ghavamzadeh", "Mohammad", ""]]}, {"id": "1603.01987", "submitter": "Vittoria Cozza", "authors": "Vittoria Cozza and Marinella Petrocchi and Angelo Spognardi", "title": "A matter of words: NLP for quality evaluation of Wikipedia medical\n  articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Automatic quality evaluation of Web information is a task with many fields of\napplications and of great relevance, especially in critical domains like the\nmedical one. We move from the intuition that the quality of content of medical\nWeb documents is affected by features related with the specific domain. First,\nthe usage of a specific vocabulary (Domain Informativeness); then, the adoption\nof specific codes (like those used in the infoboxes of Wikipedia articles) and\nthe type of document (e.g., historical and technical ones). In this paper, we\npropose to leverage specific domain features to improve the results of the\nevaluation of Wikipedia medical articles. In particular, we evaluate the\narticles adopting an \"actionable\" model, whose features are related to the\ncontent of the articles, so that the model can also directly suggest strategies\nfor improving a given article quality. We rely on Natural Language Processing\n(NLP) and dictionaries-based techniques in order to extract the bio-medical\nconcepts in a text. We prove the effectiveness of our approach by classifying\nthe medical articles of the Wikipedia Medicine Portal, which have been\npreviously manually labeled by the Wiki Project team. The results of our\nexperiments confirm that, by considering domain-oriented features, it is\npossible to obtain sensible improvements with respect to existing solutions,\nmainly for those articles that other approaches have less correctly classified.\nOther than being interesting by their own, the results call for further\nresearch in the area of domain specific features suitable for Web data quality\nassessment.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 09:54:11 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Cozza", "Vittoria", ""], ["Petrocchi", "Marinella", ""], ["Spognardi", "Angelo", ""]]}, {"id": "1603.02609", "submitter": "Antti Kangasr\\\"a\\\"asi\\\"o", "authors": "Antti Kangasr\\\"a\\\"asi\\\"o, Yi Chen, Dorota G{\\l}owacka, Samuel Kaski", "title": "Interactive Modeling of Concept Drift and Errors in Relevance Feedback", "comments": null, "journal-ref": "24th Conference on User Modeling, Adaptation and Personalization,\n  UMAP'16, 2016", "doi": "10.1145/2930238.2930243", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users giving relevance feedback in exploratory search are often uncertain\nabout the correctness of their feedback, which may result in noisy or even\nerroneous feedback. Additionally, the search intent of the user may be volatile\nas the user is constantly learning and reformulating her search hypotheses\nduring the search. This may lead to a noticeable concept drift in the feedback.\nWe formulate a Bayesian regression model for predicting the accuracy of each\nindividual user feedback and thus find outliers in the feedback data set.\nAdditionally, we introduce a timeline interface that visualizes the feedback\nhistory to the user and gives her suggestions on which past feedback is likely\nin need of adjustment. This interface also allows the user to adjust the\nfeedback accuracy inferences made by the model. Simulation experiments\ndemonstrate that the performance of the new user model outperforms a simpler\nbaseline and that the performance approaches that of an oracle, given a small\namount of additional user interaction. A user study shows that the proposed\nmodelling technique, combined with the timeline interface, makes it easier for\nthe users to notice and correct mistakes in their feedback, and to discover new\nitems.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 18:06:32 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Kangasr\u00e4\u00e4si\u00f6", "Antti", ""], ["Chen", "Yi", ""], ["G\u0142owacka", "Dorota", ""], ["Kaski", "Samuel", ""]]}, {"id": "1603.03014", "submitter": "Christina Kraus", "authors": "Christina Kraus", "title": "Plagiarism Detection - State-of-the-art systems (2016) and evaluation\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Plagiarism detection systems comprise various approaches that aim to create a\nfair environment for academic publications and appropriately acknowledge the\nauthors' works. While the need for a reliable and performant plagiarism\ndetection system increases with an increasing amount of publications, current\nsystems still have shortcomings. Particularly intelligent research plagiarism\ndetection still leaves room for improvement. An important factor for progress\nin research is a suitable evaluation framework. In this paper, we give an\noverview on the evaluation of plagiarism detection. We then use a taxonomy\nprovided in former research, to classify recent approaches of plagiarism\ndetection. Based on this, we asses the current research situation in the field\nof plagiarism detection and derive further research questions and approaches to\nbe tackled in the future.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 18:31:13 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Kraus", "Christina", ""]]}, {"id": "1603.03281", "submitter": "UshaRani Yelipe", "authors": "Yelipe UshaRani, P. Sammulal", "title": "An Innovative Imputation and Classification Approach for Accurate\n  Disease Prediction", "comments": "Special Issue of Journal IJCSIS indexed in Web of Science and Thomson\n  Reuters ISI. https://sites.google.com/site/ijcsis/vol-14-s1-feb-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imputation of missing attribute values in medical datasets for extracting\nhidden knowledge from medical datasets is an interesting research topic of\ninterest which is very challenging. One cannot eliminate missing values in\nmedical records. The reason may be because some tests may not been conducted as\nthey are cost effective, values missed when conducting clinical trials, values\nmay not have been recorded to name some of the reasons. Data mining researchers\nhave been proposing various approaches to find and impute missing values to\nincrease classification accuracies so that disease may be predicted accurately.\nIn this paper, we propose a novel imputation approach for imputation of missing\nvalues and performing classification after fixing missing values. The approach\nis based on clustering concept and aims at dimensionality reduction of the\nrecords. The case study discussed shows that missing values can be fixed and\nimputed efficiently by achieving dimensionality reduction. The importance of\nproposed approach for classification is visible in the case study which assigns\nsingle class label in contrary to multi-label assignment if dimensionality\nreduction is not performed.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 14:31:33 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["UshaRani", "Yelipe", ""], ["Sammulal", "P.", ""]]}, {"id": "1603.04259", "submitter": "Oren Barkan", "authors": "Oren Barkan and Noam Koenigstein", "title": "Item2Vec: Neural Item Embedding for Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Collaborative Filtering (CF) algorithms are item-based in the sense that\nthey analyze item-item relations in order to produce item similarities.\nRecently, several works in the field of Natural Language Processing (NLP)\nsuggested to learn a latent representation of words using neural embedding\nalgorithms. Among them, the Skip-gram with Negative Sampling (SGNS), also known\nas word2vec, was shown to provide state-of-the-art results on various\nlinguistics tasks. In this paper, we show that item-based CF can be cast in the\nsame framework of neural word embedding. Inspired by SGNS, we describe a method\nwe name item2vec for item-based CF that produces embedding for items in a\nlatent space. The method is capable of inferring item-item relations even when\nuser information is not available. We present experimental results that\ndemonstrate the effectiveness of the item2vec method and show it is competitive\nwith SVD.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 13:37:03 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2016 13:45:53 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2017 20:37:53 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Barkan", "Oren", ""], ["Koenigstein", "Noam", ""]]}, {"id": "1603.04466", "submitter": "L. Elisa Celis", "authors": "L. Elisa Celis, Peter M. Krafft, Nathan Kobe", "title": "Sequential Voting Promotes Collective Discovery in Social Recommendation\n  Systems", "comments": "To be published in the 10th International AAAI Conference on Web and\n  Social Media (ICWSM) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.HC cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One goal of online social recommendation systems is to harness the wisdom of\ncrowds in order to identify high quality content. Yet the sequential voting\nmechanisms that are commonly used by these systems are at odds with existing\ntheoretical and empirical literature on optimal aggregation. This literature\nsuggests that sequential voting will promote herding---the tendency for\nindividuals to copy the decisions of others around them---and hence lead to\nsuboptimal content recommendation. Is there a problem with our practice, or a\nproblem with our theory? Previous attempts at answering this question have been\nlimited by a lack of objective measurements of content quality. Quality is\ntypically defined endogenously as the popularity of content in absence of\nsocial influence. The flaw of this metric is its presupposition that the\npreferences of the crowd are aligned with underlying quality. Domains in which\ncontent quality can be defined exogenously and measured objectively are thus\nneeded in order to better assess the design choices of social recommendation\nsystems. In this work, we look to the domain of education, where content\nquality can be measured via how well students are able to learn from the\nmaterial presented to them. Through a behavioral experiment involving a\nsimulated massive open online course (MOOC) run on Amazon Mechanical Turk, we\nshow that sequential voting systems can surface better content than systems\nthat elicit independent votes.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 20:48:43 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Celis", "L. Elisa", ""], ["Krafft", "Peter M.", ""], ["Kobe", "Nathan", ""]]}, {"id": "1603.04482", "submitter": "Abhinav Mishra", "authors": "Abhinav Mishra", "title": "An approach towards debiasing user ratings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing importance of e-commerce, many websites have emerged where\nusers can express their opinions about products, such as movies, books, songs,\netc. Such interactions can be modeled as bipartite graphs where the weight of\nthe directed edge from a user to a product denotes a rating that the user\nimparts to the product. These graphs are used for recommendation systems and\ndiscovering most reliable (trusted) products. For these applications, it is\nimportant to capture the bias of a user when she is rating a product. Users\nhave inherent bias---many users always impart high ratings while many others\nalways rate poorly. It is necessary to know the bias of a reviewer while\nreading the review of a product. It is equally important to compensate for this\nbias while assigning a ranking for an object. In this paper, we propose an\nalgorithm to capture the bias of a user and then subdue it to compute the true\nrating a product deserves. Experiments show the efficiency and effectiveness of\nour system in capturing the bias of users and then computing the true ratings\nof a product.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 21:24:10 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Mishra", "Abhinav", ""]]}, {"id": "1603.04522", "submitter": "Yong Liu Stephen", "authors": "Yong Liu, Peilin Zhao, Xin Liu, Min Wu, Xiao-Li Li", "title": "Learning Optimal Social Dependency for Recommendation", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social recommender systems exploit users' social relationships to improve the\nrecommendation accuracy. Intuitively, a user tends to trust different subsets\nof her social friends, regarding with different scenarios. Therefore, the main\nchallenge of social recommendation is to exploit the optimal social dependency\nbetween users for a specific recommendation task. In this paper, we propose a\nnovel recommendation method, named probabilistic relational matrix\nfactorization (PRMF), which aims to learn the optimal social dependency between\nusers to improve the recommendation accuracy, with or without users' social\nrelationships. Specifically, in PRMF, the latent features of users are assumed\nto follow a matrix variate normal (MVN) distribution. The positive and negative\ndependency between users are modeled by the row precision matrix of the MVN\ndistribution. Moreover, we have also proposed an efficient alternating\nalgorithm to solve the optimization problem of PRMF. The experimental results\non real datasets demonstrate that the proposed PRMF method outperforms\nstate-of-the-art social recommendation approaches, in terms of root mean square\nerror (RMSE) and mean absolute error (MAE).\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 01:41:11 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Liu", "Yong", ""], ["Zhao", "Peilin", ""], ["Liu", "Xin", ""], ["Wu", "Min", ""], ["Li", "Xiao-Li", ""]]}, {"id": "1603.04531", "submitter": "Haewoon Kwak", "authors": "Haewoon Kwak and Jisun An", "title": "Revealing the Hidden Patterns of News Photos: Analysis of Millions of\n  News Photos Using GDELT and Deep Learning-based Vision APIs", "comments": "Presented in the first workshop on NEws and publiC Opinion (NECO'16,\n  www.neco.io, colocated with ICWSM'16), Cologne, Germany, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we analyze more than two million news photos published in\nJanuary 2016. We demonstrate i) which objects appear the most in news photos;\nii) what the sentiments of news photos are; iii) whether the sentiment of news\nphotos is aligned with the tone of the text; iv) how gender is treated; and v)\nhow differently political candidates are portrayed. To our best knowledge, this\nis the first large-scale study of news photo contents using deep learning-based\nvision APIs.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 02:23:53 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 04:44:12 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Kwak", "Haewoon", ""], ["An", "Jisun", ""]]}, {"id": "1603.04595", "submitter": "Olivier Mor\\`ere", "authors": "Olivier Mor\\`ere, Jie Lin, Antoine Veillard, Vijay Chandrasekhar,\n  Tomaso Poggio", "title": "Nested Invariance Pooling and RBM Hashing for Image Instance Retrieval", "comments": "Image Instance Retrieval, CNN, Invariant Representation, Hashing,\n  Unsupervised Learning, Regularization. arXiv admin note: text overlap with\n  arXiv:1601.02093", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is the computation of very compact binary hashes for\nimage instance retrieval. Our approach has two novel contributions. The first\none is Nested Invariance Pooling (NIP), a method inspired from i-theory, a\nmathematical theory for computing group invariant transformations with\nfeed-forward neural networks. NIP is able to produce compact and\nwell-performing descriptors with visual representations extracted from\nconvolutional neural networks. We specifically incorporate scale, translation\nand rotation invariances but the scheme can be extended to any arbitrary sets\nof transformations. We also show that using moments of increasing order\nthroughout nesting is important. The NIP descriptors are then hashed to the\ntarget code size (32-256 bits) with a Restricted Boltzmann Machine with a novel\nbatch-level regularization scheme specifically designed for the purpose of\nhashing (RBMH). A thorough empirical evaluation with state-of-the-art shows\nthat the results obtained both with the NIP descriptors and the NIP+RBMH hashes\nare consistently outstanding across a wide range of datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 08:56:33 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 14:11:18 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Mor\u00e8re", "Olivier", ""], ["Lin", "Jie", ""], ["Veillard", "Antoine", ""], ["Chandrasekhar", "Vijay", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1603.04953", "submitter": "Matthieu Vergne", "authors": "Matthieu Vergne", "title": "Mitigation Procedures to Rank Experts through Information Retrieval\n  Measures", "comments": "Technical report, 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In order to find experts, different approaches build rankings of people,\nassuming that they are ranked by level of expertise, and use typical\nInformation Retrieval (IR) measures to evaluate their effectiveness. However,\nwe figured out that expert rankings (i) tend to be partially ordered, (ii)\nincomplete, and (iii) consequently provide more an order rather than absolute\nranks, which is not what usual IR measures exploit. To improve this state of\nthe art, we propose to revise the formalism used in IR to design proper\nmeasures for comparing expert rankings. In this report, we investigate a first\nstep by providing mitigation procedures for the three issues, and we analyse IR\nmeasures with the help of these procedures to identify interesting revisions\nand remaining limitations. From this analysis, we see that most of the measures\ncan be exploited for this more generic context because of our mitigation\nprocedures. Moreover, measures based on precision and recall, usually unable to\nconsider the order of the ranked items, are of first interest if we represent a\nranking as a set of ordered pairs. Cumulative measures, on the other hand, are\nspecifically designed for considering the order but suffer from a higher\ncomplexity, motivating the use of precision/recall measures with the right\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 03:58:38 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Vergne", "Matthieu", ""]]}, {"id": "1603.05215", "submitter": "Kejun Huang", "authors": "Kejun Huang, Yonina C. Eldar, Nicholas D. Sidiropoulos", "title": "Phase Retrieval from 1D Fourier Measurements: Convexity, Uniqueness, and\n  Algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2601291", "report-no": null, "categories": "math.OC cs.IR cs.IT math.IT math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers phase retrieval from the magnitude of 1D over-sampled\nFourier measurements, a classical problem that has challenged researchers in\nvarious fields of science and engineering. We show that an optimal vector in a\nleast-squares sense can be found by solving a convex problem, thus establishing\na hidden convexity in Fourier phase retrieval. We also show that the standard\nsemidefinite relaxation approach yields the optimal cost function value (albeit\nnot necessarily an optimal solution) in this case. A method is then derived to\nretrieve an optimal minimum phase solution in polynomial time. Using these\nresults, a new measuring technique is proposed which guarantees uniqueness of\nthe solution, along with an efficient algorithm that can solve large-scale\nFourier phase retrieval problems with uniqueness and optimality guarantees.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 18:47:21 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 17:03:54 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Huang", "Kejun", ""], ["Eldar", "Yonina C.", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "1603.05414", "submitter": "Honghai Yu", "authors": "Honghai Yu, Pierre Moulin, Hong Wei Ng, Xiaoli Li", "title": "Variable-Length Hashing", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing has emerged as a popular technique for large-scale similarity search.\nMost learning-based hashing methods generate compact yet correlated hash codes.\nHowever, this redundancy is storage-inefficient. Hence we propose a lossless\nvariable-length hashing (VLH) method that is both storage- and\nsearch-efficient. Storage efficiency is achieved by converting the fixed-length\nhash code into a variable-length code. Search efficiency is obtained by using a\nmultiple hash table structure. With VLH, we are able to deliberately add\nredundancy into hash codes to improve retrieval performance with little\nsacrifice in storage efficiency or search complexity. In particular, we propose\na block K-means hashing (B-KMH) method to obtain significantly improved\nretrieval performance with no increase in storage and marginal increase in\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 10:19:50 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Yu", "Honghai", ""], ["Moulin", "Pierre", ""], ["Ng", "Hong Wei", ""], ["Li", "Xiaoli", ""]]}, {"id": "1603.05572", "submitter": "Hong Liu", "authors": "Hong Liu, Rongrong Ji, Yongjian Wu, Gang Hua", "title": "Supervised Matrix Factorization for Cross-Modality Hashing", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Matrix factorization has been recently utilized for the task of multi-modal\nhashing for cross-modality visual search, where basis functions are learned to\nmap data from different modalities to the same Hamming embedding. In this\npaper, we propose a novel cross-modality hashing algorithm termed Supervised\nMatrix Factorization Hashing (SMFH) which tackles the multi-modal hashing\nproblem with a collective non-matrix factorization across the different\nmodalities. In particular, SMFH employs a well-designed binary code learning\nalgorithm to preserve the similarities among multi-modal original features\nthrough a graph regularization. At the same time, semantic labels, when\navailable, are incorporated into the learning procedure. We conjecture that all\nthese would facilitate to preserve the most relevant information during the\nbinary quantization process, and hence improve the retrieval accuracy. We\ndemonstrate the superior performance of SMFH on three cross-modality visual\nsearch benchmarks, i.e., the PASCAL-Sentence, Wiki, and NUS-WIDE, with\nquantitative comparison to various state-of-the-art methods\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 16:51:44 GMT"}, {"version": "v2", "created": "Sat, 26 Mar 2016 07:00:40 GMT"}, {"version": "v3", "created": "Tue, 29 Mar 2016 05:14:35 GMT"}, {"version": "v4", "created": "Wed, 30 Mar 2016 06:22:36 GMT"}, {"version": "v5", "created": "Sat, 16 Apr 2016 08:51:51 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Liu", "Hong", ""], ["Ji", "Rongrong", ""], ["Wu", "Yongjian", ""], ["Hua", "Gang", ""]]}, {"id": "1603.05670", "submitter": "Samuel R\\\"onnqvist", "authors": "Samuel R\\\"onnqvist, Peter Sarlin", "title": "Bank distress in the news: Describing events through deep learning", "comments": "Forthcoming in Neurocomputing. arXiv admin note: substantial text\n  overlap with arXiv:1507.07870 [in version 1]", "journal-ref": "Neurocomputing, 264, 2017", "doi": "10.1016/j.neucom.2016.12.11", "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.NE q-fin.CP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While many models are purposed for detecting the occurrence of significant\nevents in financial systems, the task of providing qualitative detail on the\ndevelopments is not usually as well automated. We present a deep learning\napproach for detecting relevant discussion in text and extracting natural\nlanguage descriptions of events. Supervised by only a small set of event\ninformation, comprising entity names and dates, the model is leveraged by\nunsupervised learning of semantic vector representations on extensive text\ndata. We demonstrate applicability to the study of financial risk based on news\n(6.6M articles), particularly bank distress and government interventions (243\nevents), where indices can signal the level of bank-stress-related reporting at\nthe entity level, or aggregated at national or European level, while being\ncoupled with explanations. Thus, we exemplify how text, as timely, widely\navailable and descriptive data, can serve as a useful complementary source of\ninformation for financial and systemic risk analytics.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 20:06:27 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2016 23:24:49 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""], ["Sarlin", "Peter", ""]]}, {"id": "1603.05782", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang and Alex Yong-Sang Chia", "title": "Unsupervised Cross-Media Hashing with Structure Preservation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen the exponential growth of heterogeneous multimedia\ndata. The need for effective and accurate data retrieval from heterogeneous\ndata sources has attracted much research interest in cross-media retrieval.\nHere, given a query of any media type, cross-media retrieval seeks to find\nrelevant results of different media types from heterogeneous data sources. To\nfacilitate large-scale cross-media retrieval, we propose a novel unsupervised\ncross-media hashing method. Our method incorporates local affinity and distance\nrepulsion constraints into a matrix factorization framework. Correspondingly,\nthe proposed method learns hash functions that generates unified hash codes\nfrom different media types, while ensuring intrinsic geometric structure of the\ndata distribution is preserved. These hash codes empower the similarity between\ndata of different media types to be evaluated directly. Experimental results on\ntwo large-scale multimedia datasets demonstrate the effectiveness of the\nproposed method, where we outperform the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 07:10:35 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Wang", "Xiangyu", ""], ["Chia", "Alex Yong-Sang", ""]]}, {"id": "1603.06038", "submitter": "Evgeny Frolov", "authors": "Evgeny Frolov and Ivan Oseledets", "title": "Tensor Methods and Recommender Systems", "comments": "Submitted to WIREs Data Mining and Knowledge Discovery. 41 page, 3\n  figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A substantial progress in development of new and efficient tensor\nfactorization techniques has led to an extensive research of their\napplicability in recommender systems field. Tensor-based recommender models\npush the boundaries of traditional collaborative filtering techniques by taking\ninto account a multifaceted nature of real environments, which allows to\nproduce more accurate, situational (e.g. context-aware, criteria-driven)\nrecommendations. Despite the promising results, tensor-based methods are poorly\ncovered in existing recommender systems surveys. This survey aims to complement\nprevious works and provide a comprehensive overview on the subject. To the best\nof our knowledge, this is the first attempt to consolidate studies from various\napplication domains in an easily readable, digestible format, which helps to\nget a notion of the current state of the field. We also provide a high level\ndiscussion of the future perspectives and directions for further improvement of\ntensor-based recommendation systems.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 03:38:47 GMT"}, {"version": "v2", "created": "Sun, 18 Feb 2018 14:44:44 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Frolov", "Evgeny", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1603.06494", "submitter": "Lisa Posch", "authors": "Lisa Posch", "title": "Enriching Ontologies with Encyclopedic Background Knowledge for Document\n  Indexing", "comments": null, "journal-ref": "13th International Semantic Web Conference, Riva del Garda, Italy,\n  October 19-23, 2014. Proceedings, Part II", "doi": "10.1007/978-3-319-11915-1_36", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly increasing number of scientific documents available publicly on\nthe Internet creates the challenge of efficiently organizing and indexing these\ndocuments. Due to the time consuming and tedious nature of manual\nclassification and indexing, there is a need for better methods to automate\nthis process. This thesis proposes an approach which leverages encyclopedic\nbackground knowledge for enriching domain-specific ontologies with textual and\nstructural information about the semantic vicinity of the ontologies' concepts.\nThe proposed approach aims to exploit this information for improving both\nontology-based methods for classifying and indexing documents and methods based\non supervised machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 16:53:32 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Posch", "Lisa", ""]]}, {"id": "1603.06679", "submitter": "Wenya Wang", "authors": "Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier and Xiaokui Xiao", "title": "Recursive Neural Conditional Random Fields for Aspect-based Sentiment\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In aspect-based sentiment analysis, extracting aspect terms along with the\nopinions being expressed from user-generated content is one of the most\nimportant subtasks. Previous studies have shown that exploiting connections\nbetween aspect and opinion terms is promising for this task. In this paper, we\npropose a novel joint model that integrates recursive neural networks and\nconditional random fields into a unified framework for explicit aspect and\nopinion terms co-extraction. The proposed model learns high-level\ndiscriminative features and double propagate information between aspect and\nopinion terms, simultaneously. Moreover, it is flexible to incorporate\nhand-crafted features into the proposed model to further boost its information\nextraction performance. Experimental results on the SemEval Challenge 2014\ndataset show the superiority of our proposed model over several baseline\nmethods as well as the winning systems of the challenge.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 05:59:00 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 06:24:06 GMT"}, {"version": "v3", "created": "Mon, 19 Sep 2016 14:00:43 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Wang", "Wenya", ""], ["Pan", "Sinno Jialin", ""], ["Dahlmeier", "Daniel", ""], ["Xiao", "Xiaokui", ""]]}, {"id": "1603.07016", "submitter": "Chifumi Nishioka", "authors": "Chifumi Nishioka and Ansgar Scherp", "title": "Profiling vs. Time vs. Content: What does Matter for Top-k Publication\n  Recommendation based on Twitter Profiles? - An Extended Technical Report", "comments": null, "journal-ref": null, "doi": "10.1145/2910896.2910898", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So far it is unclear how different factors of a scientific publication\nrecommender system based on users' tweets have an influence on the\nrecommendation performance. We examine three different factors, namely\nprofiling method, temporal decay, and richness of content. Regarding profiling,\nwe compare CF-IDF that replaces terms in TF-IDF by semantic concepts, HCF-IDF\nas novel hierarchical variant of CF-IDF, and topic modeling. As temporal decay\nfunctions, we apply sliding window and exponential decay. In terms of the\nrichness of content, we compare recommendations using both full-texts and\ntitles of publications and using only titles. Overall, the three factors make\ntwelve recommendation strategies. We have conducted an online experiment with\n123 participants and compared the strategies in a within-group design. The best\nrecommendations are achieved by the strategy combining CF-IDF, sliding window,\nand with full-texts. However, the strategies using the novel HCF-IDF profiling\nmethod achieve similar results with just using the titles of the publications.\nTherefore, HCF-IDF can make recommendations when only short and sparse data is\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 22:43:50 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 08:54:09 GMT"}, {"version": "v3", "created": "Mon, 18 Apr 2016 07:43:44 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Nishioka", "Chifumi", ""], ["Scherp", "Ansgar", ""]]}, {"id": "1603.07150", "submitter": "Martyn Harris", "authors": "Martyn Harris, Mark Levene, Dell Zhang, Dan Levene", "title": "The Anatomy of a Search and Mining System for Digital Archives", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Samtla (Search And Mining Tools with Linguistic Analysis) is a digital\nhumanities system designed in collaboration with historians and linguists to\nassist them with their research work in quantifying the content of any textual\ncorpora through approximate phrase search and document comparison. The\nretrieval engine uses a character-based n-gram language model rather than the\nconventional word-based one so as to achieve great flexibility in language\nagnostic query processing.\n  The index is implemented as a space-optimised character-based suffix tree\nwith an accompanying database of document content and metadata. A number of\ntext mining tools are integrated into the system to allow researchers to\ndiscover textual patterns, perform comparative analysis, and find out what is\ncurrently popular in the research community.\n  Herein we describe the system architecture, user interface, models and\nalgorithms, and data storage of the Samtla system. We also present several case\nstudies of its usage in practice together with an evaluation of the systems'\nranking performance through crowdsourcing.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 12:02:12 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Harris", "Martyn", ""], ["Levene", "Mark", ""], ["Zhang", "Dell", ""], ["Levene", "Dan", ""]]}, {"id": "1603.07313", "submitter": "Jes\\'us Tramullas", "authors": "Piedad Garrido, Jesus Tramullas, Manuel Coll", "title": "CONDITOR1: Topic Maps and DITA labelling tool for textual documents with\n  historical information", "comments": null, "journal-ref": "Journal of Digital Information, 10, 4, 2009", "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Conditor is a software tool which works with textual documents containing\nhistorical information. The purpose of this work two-fold: firstly to show the\nvalidity of the developed engine to correctly identify and label the entities\nof the universe of discourse with a labelled-combined XTM-DITA model. Secondly\nto explain the improvements achieved in the information retrieval process\nthanks to the use of a object-oriented database (JPOX) as well as its\nintegration into the Lucene-type database search process to not only accomplish\nmore accurate searches, but to also help the future development of a\nrecommender system. We finish with a brief demo in a 3D-graph of the results of\nthe aforementioned search.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 19:26:20 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Garrido", "Piedad", ""], ["Tramullas", "Jesus", ""], ["Coll", "Manuel", ""]]}, {"id": "1603.07534", "submitter": "Eiko Yoneki", "authors": "Juan M. Tirado, Ovidiu Serban, Qiang Guo, Eiko Yoneki", "title": "Web Data Knowledge Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A constantly growing amount of information is available through the web.\nUnfortunately, extracting useful content from this massive amount of data still\nremains an open issue. The lack of standard data models and structures forces\ndevelopers to create adhoc solutions from the scratch. The figure of the expert\nis still needed in many situations where developers do not have the correct\nbackground knowledge. This forces developers to spend time acquiring the needed\nbackground from the expert. In other directions, there are promising solutions\nemploying machine learning techniques. However, increasing accuracy requires an\nincrease in system complexity that cannot be endured in many projects. In this\nwork, we approach the web knowledge extraction problem using an expertcentric\nmethodology. This methodology defines a set of configurable, extendible and\nindependent components that permit the reutilisation of large pieces of code\namong projects. Our methodology differs from similar solutions in its\nexpert-driven design. This design, makes it possible for subject-matter expert\nto drive the knowledge extraction for a given set of documents. Additionally,\nwe propose the utilization of machine assisted solutions that guide the expert\nduring this process. To demonstrate the capabilities of our methodology, we\npresent a real use case scenario in which public procurement data is extracted\nfrom the web-based repositories of several public institutions across Europe.\nWe provide insightful details about the challenges we had to deal with in this\nuse case and additional discussions about how to apply our methodology.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 11:43:34 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Tirado", "Juan M.", ""], ["Serban", "Ovidiu", ""], ["Guo", "Qiang", ""], ["Yoneki", "Eiko", ""]]}, {"id": "1603.07624", "submitter": "Diego Klabjan", "authors": "Eun Hee Ko, Diego Klabjan", "title": "Semantic Properties of Customer Sentiment in Tweets", "comments": "The 28th IEEE International Conference on Advanced Information\n  Networking and Applications. Victoria, Canada, 2014", "journal-ref": null, "doi": "10.1109/WAINA.2014.151", "report-no": null, "categories": "cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of people are using online social networking services\n(SNSs), and a significant amount of information related to experiences in\nconsumption is shared in this new media form. Text mining is an emerging\ntechnique for mining useful information from the web. We aim at discovering in\nparticular tweets semantic patterns in consumers' discussions on social media.\nSpecifically, the purposes of this study are twofold: 1) finding similarity and\ndissimilarity between two sets of textual documents that include consumers'\nsentiment polarities, two forms of positive vs. negative opinions and 2)\ndriving actual content from the textual data that has a semantic trend. The\nconsidered tweets include consumers opinions on US retail companies (e.g.,\nAmazon, Walmart). Cosine similarity and K-means clustering methods are used to\nachieve the former goal, and Latent Dirichlet Allocation (LDA), a popular topic\nmodeling algorithm, is used for the latter purpose. This is the first study\nwhich discover semantic properties of textual data in consumption context\nbeyond sentiment analysis. In addition to major findings, we apply LDA (Latent\nDirichlet Allocations) to the same data and drew latent topics that represent\nconsumers' positive opinions and negative opinions on social media.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 15:22:52 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Ko", "Eun Hee", ""], ["Klabjan", "Diego", ""]]}, {"id": "1603.07646", "submitter": "Saurabh Kataria", "authors": "Saurabh Kataria", "title": "Recursive Neural Language Architecture for Tag Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning distributed representations for tags from\ntheir associated content for the task of tag recommendation. Considering\ntagging information is usually very sparse, effective learning from content and\ntag association is very crucial and challenging task. Recently, various neural\nrepresentation learning models such as WSABIE and its variants show promising\nperformance, mainly due to compact feature representations learned in a\nsemantic space. However, their capacity is limited by a linear compositional\napproach for representing tags as sum of equal parts and hurt their\nperformance. In this work, we propose a neural feedback relevance model for\nlearning tag representations with weighted feature representations. Our\nexperiments on two widely used datasets show significant improvement for\nquality of recommendations over various baselines.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 16:39:37 GMT"}], "update_date": "2016-03-25", "authors_parsed": [["Kataria", "Saurabh", ""]]}, {"id": "1603.07849", "submitter": "Eric Makita", "authors": "Eric Makita, Artem Lenskiy", "title": "A multinomial probabilistic model for movie genre predictions", "comments": "5 pages, 4 figures, 8th International Conference on Machine Learning\n  and Computing, Hong Kong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a movie genre-prediction based on multinomial probability\nmodel. To the best of our knowledge, this problem has not been addressed yet in\nthe field of recommender system. The prediction of a movie genre has many\npractical applications including complementing the items categories given by\nexperts and providing a surprise effect in the recommendations given to a user.\nWe employ mulitnomial event model to estimate a likelihood of a movie given\ngenre and the Bayes rule to evaluate the posterior probability of a genre given\na movie. Experiments with the MovieLens dataset validate our approach. We\nachieved 70% prediction rate using only 15% of the whole set for training.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 08:49:39 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Makita", "Eric", ""], ["Lenskiy", "Artem", ""]]}, {"id": "1603.08632", "submitter": "EPTCS", "authors": "Rui Couto (HASLab/INESCT TEC and Dept. of Informatics/University of\n  Minho), Ant\\'onio Nestor Ribeiro (HASLab/INESCT TEC and Dept. of\n  Informatics/University of Minho), Jos\\'e Creissac Campos (HASLab/INESCT TEC\n  and Dept. of Informatics/University of Minho)", "title": "Validating an Approach to Formalize Use Cases with Ontologies", "comments": "In Proceedings FESCA 2016, arXiv:1603.08371", "journal-ref": "EPTCS 205, 2016, pp. 1-15", "doi": "10.4204/EPTCS.205.1", "report-no": null, "categories": "cs.SE cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Use case driven development methodologies put use cases at the center of the\nsoftware development process. However, in order to support automated\ndevelopment and analysis, use cases need to be appropriately formalized. This\nwill also help guarantee consistency between requirements specifications and\nthe developed solutions. Formal methods tend to suffer from take up issues, as\nthey are usually hard to accept by industry. In this context, it is relevant\nnot only to produce languages and approaches to support formalization, but also\nto perform their validation. In previous works we have developed an approach to\nformalize use cases resorting to ontologies. In this paper we present the\nvalidation of one such approach. Through a three stage study, we evaluate the\nacceptance of the language and supporting tool. The first stage focusses on the\nacceptance of the process and language, the second on the support the tool\nprovides to the process, and finally the third one on the tool's usability\naspects. Results show test subjects found the approach feasible and useful and\nthe tool easy to use.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 04:33:54 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Couto", "Rui", "", "HASLab/INESCT TEC and Dept. of Informatics/University of\n  Minho"], ["Ribeiro", "Ant\u00f3nio Nestor", "", "HASLab/INESCT TEC and Dept. of\n  Informatics/University of Minho"], ["Campos", "Jos\u00e9 Creissac", "", "HASLab/INESCT TEC\n  and Dept. of Informatics/University of Minho"]]}, {"id": "1603.08675", "submitter": "Anupam Prakash", "authors": "Iordanis Kerenidis and Anupam Prakash", "title": "Quantum Recommendation Systems", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DS cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A recommendation system uses the past purchases or ratings of $n$ products by\na group of $m$ users, in order to provide personalized recommendations to\nindividual users. The information is modeled as an $m \\times n$ preference\nmatrix which is assumed to have a good rank-$k$ approximation, for a small\nconstant $k$.\n  In this work, we present a quantum algorithm for recommendation systems that\nhas running time $O(\\text{poly}(k)\\text{polylog}(mn))$. All known classical\nalgorithms for recommendation systems that work through reconstructing an\napproximation of the preference matrix run in time polynomial in the matrix\ndimension. Our algorithm provides good recommendations by sampling efficiently\nfrom an approximation of the preference matrix, without reconstructing the\nentire matrix. For this, we design an efficient quantum procedure to project a\ngiven vector onto the row space of a given matrix. This is the first algorithm\nfor recommendation systems that runs in time polylogarithmic in the dimensions\nof the matrix and provides an example of a quantum machine learning algorithm\nfor a real world application.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 08:25:22 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 03:41:23 GMT"}, {"version": "v3", "created": "Thu, 22 Sep 2016 09:32:16 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Kerenidis", "Iordanis", ""], ["Prakash", "Anupam", ""]]}, {"id": "1603.09320", "submitter": "Yury Malkov A", "authors": "Yu. A. Malkov, D. A. Yashunin", "title": "Efficient and robust approximate nearest neighbor search using\n  Hierarchical Navigable Small World graphs", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for the approximate K-nearest neighbor search based\non navigable small world graphs with controllable hierarchy (Hierarchical NSW,\nHNSW). The proposed solution is fully graph-based, without any need for\nadditional search structures, which are typically used at the coarse search\nstage of the most proximity graph techniques. Hierarchical NSW incrementally\nbuilds a multi-layer structure consisting from hierarchical set of proximity\ngraphs (layers) for nested subsets of the stored elements. The maximum layer in\nwhich an element is present is selected randomly with an exponentially decaying\nprobability distribution. This allows producing graphs similar to the\npreviously studied Navigable Small World (NSW) structures while additionally\nhaving the links separated by their characteristic distance scales. Starting\nsearch from the upper layer together with utilizing the scale separation boosts\nthe performance compared to NSW and allows a logarithmic complexity scaling.\nAdditional employment of a heuristic for selecting proximity graph neighbors\nsignificantly increases performance at high recall and in case of highly\nclustered data. Performance evaluation has demonstrated that the proposed\ngeneral metric space search index is able to strongly outperform previous\nopensource state-of-the-art vector-only approaches. Similarity of the algorithm\nto the skip list structure allows straightforward balanced distributed\nimplementation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 19:29:44 GMT"}, {"version": "v2", "created": "Sat, 21 May 2016 07:27:25 GMT"}, {"version": "v3", "created": "Sun, 30 Jul 2017 12:07:54 GMT"}, {"version": "v4", "created": "Tue, 14 Aug 2018 19:29:07 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Malkov", "Yu. A.", ""], ["Yashunin", "D. A.", ""]]}, {"id": "1603.09434", "submitter": "Ibrahim AlShourbaji H", "authors": "Ibrahim AlShourbaji, Samaher Al-Janabi and Ahmed Patel", "title": "Document Selection in a Distributed Search Engine Architecture", "comments": "8 pages, 6 figures in Middle-East Journal of Scientific Research,\n  IDOSI Publications, 2015", "journal-ref": null, "doi": "10.5829/idosi.mejsr.2015.23.07.22398", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Search Engine Architecture (DSEA) hosts numerous independent\ntopic-specific search engines and selects a subset of the databases to search\nwithin the architecture. The objective of this approach is to reduce the amount\nof space needed to perform a search by querying only a subset of the total data\navailable. In order to manipulate data across many databases, it is most\nefficient to identify a smaller subset of databases that would be most likely\nto return the data of specific interest that can then be examined in greater\ndetail. The selection index has been most commonly used as a method for\nchoosing the most applicable databases as it captures broad information about\neach database and its indexed documents. Employing this type of database allows\nthe researcher to find information more quickly, not only with less cost, but\nit also minimizes the potential for biases. This paper investigates the\neffectiveness of different databases selected within the framework and scope of\nthe distributed search engine architecture. The purpose of the study is to\nimprove the quality of distributed information retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 01:19:21 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["AlShourbaji", "Ibrahim", ""], ["Al-Janabi", "Samaher", ""], ["Patel", "Ahmed", ""]]}, {"id": "1603.09473", "submitter": "Ruining He", "authors": "Ruining He and Charles Packer and Julian McAuley", "title": "Learning Compatibility Across Categories for Heterogeneous Item\n  Recommendation", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying relationships between items is a key task of an online\nrecommender system, in order to help users discover items that are functionally\ncomplementary or visually compatible. In domains like clothing recommendation,\nthis task is particularly challenging since a successful system should be\ncapable of handling a large corpus of items, a huge amount of relationships\namong them, as well as the high-dimensional and semantically complicated\nfeatures involved. Furthermore, the human notion of \"compatibility\" to capture\ngoes beyond mere similarity: For two items to be compatible---whether jeans and\na t-shirt, or a laptop and a charger---they should be similar in some ways, but\nsystematically different in others.\n  In this paper we propose a novel method, Monomer, to learn complicated and\nheterogeneous relationships between items in product recommendation settings.\nRecently, scalable methods have been developed that address this task by\nlearning similarity metrics on top of the content of the products involved.\nHere our method relaxes the metricity assumption inherent in previous work and\nmodels multiple localized notions of 'relatedness,' so as to uncover ways in\nwhich related items should be systematically similar, and systematically\ndifferent. Quantitatively, we show that our system achieves state-of-the-art\nperformance on large-scale compatibility prediction tasks, especially in cases\nwhere there is substantial heterogeneity between related items. Qualitatively,\nwe demonstrate that richer notions of compatibility can be learned that go\nbeyond similarity, and that our model can make effective recommendations of\nheterogeneous content.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 07:22:30 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 07:25:36 GMT"}, {"version": "v3", "created": "Thu, 29 Sep 2016 00:43:21 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["He", "Ruining", ""], ["Packer", "Charles", ""], ["McAuley", "Julian", ""]]}, {"id": "1603.09522", "submitter": "Dorota Glowacka", "authors": "Dorota Glowacka, Yee Whye Teh, John Shawe-Taylor", "title": "Image Retrieval with a Bayesian Model of Relevance Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A content-based image retrieval system based on multinomial relevance\nfeedback is proposed. The system relies on an interactive search paradigm where\nat each round a user is presented with k images and selects the one closest to\ntheir ideal target. Two approaches, one based on the Dirichlet distribution and\none based the Beta distribution, are used to model the problem motivating an\nalgorithm that trades exploration and exploitation in presenting the images in\neach round. Experimental results show that the new approach compares favourably\nwith previous work.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 10:50:50 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Glowacka", "Dorota", ""], ["Teh", "Yee Whye", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1603.09687", "submitter": "Claudio Gennaro", "authors": "Claudio Gennaro", "title": "Large Scale Deep Convolutional Neural Network Features Search with\n  Lucene", "comments": "This paper has been withdrawn by the author due to many errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an approach to index Deep Convolutional Neural\nNetwork Features to support efficient content-based retrieval on large image\ndatabases. To this aim, we have converted the these features into a textual\nform, to index them into an inverted index by means of Lucene. In this way, we\nwere able to set up a robust retrieval system that combines full-text search\nwith content-based image retrieval capabilities. We evaluated different\nstrategies of textual representation in order to optimize the index occupation\nand the query response time. In order to show that our approach is able to\nhandle large datasets, we have developed a web-based prototype that provides an\ninterface for combined textual and visual searching into a dataset of about 100\nmillion of images.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 17:11:43 GMT"}, {"version": "v2", "created": "Fri, 1 Apr 2016 09:43:48 GMT"}, {"version": "v3", "created": "Thu, 5 May 2016 15:02:51 GMT"}, {"version": "v4", "created": "Wed, 20 Jul 2016 09:29:57 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Gennaro", "Claudio", ""]]}]