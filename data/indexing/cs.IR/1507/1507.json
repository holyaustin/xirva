[{"id": "1507.00043", "submitter": "Athanasios N. Nikolakopoulos", "authors": "Athanasios N. Nikolakopoulos and John D. Garofalakis", "title": "Top-N recommendations in the presence of sparsity: An NCD-based approach", "comments": "To appear in the Web Intelligence Journal as a regular paper", "journal-ref": null, "doi": "10.3233/WEB-150324", "report-no": null, "categories": "cs.IR cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making recommendations in the presence of sparsity is known to present one of\nthe most challenging problems faced by collaborative filtering methods. In this\nwork we tackle this problem by exploiting the innately hierarchical structure\nof the item space following an approach inspired by the theory of\nDecomposability. We view the itemspace as a Nearly Decomposable system and we\ndefine blocks of closely related elements and corresponding indirect proximity\ncomponents. We study the theoretical properties of the decomposition and we\nderive sufficient conditions that guarantee full item space coverage even in\ncold-start recommendation scenarios. A comprehensive set of experiments on the\nMovieLens and the Yahoo!R2Music datasets, using several widely applied\nperformance metrics, support our model's theoretically predicted properties and\nverify that NCDREC outperforms several state-of-the-art algorithms, in terms of\nrecommendation accuracy, diversity and sparseness insensitivity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 21:34:53 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 13:55:35 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Nikolakopoulos", "Athanasios N.", ""], ["Garofalakis", "John D.", ""]]}, {"id": "1507.00209", "submitter": "Hai Zhuge Mr", "authors": "Hai Zhuge", "title": "Dimensionality on Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summarization is one of the key features of human intelligence. It plays an\nimportant role in understanding and representation. With rapid and continual\nexpansion of texts, pictures and videos in cyberspace, automatic summarization\nbecomes more and more desirable. Text summarization has been studied for over\nhalf century, but it is still hard to automatically generate a satisfied\nsummary. Traditional methods process texts empirically and neglect the\nfundamental characteristics and principles of language use and understanding.\nThis paper summarizes previous text summarization approaches in a\nmulti-dimensional classification space, introduces a multi-dimensional\nmethodology for research and development, unveils the basic characteristics and\nprinciples of language use and understanding, investigates some fundamental\nmechanisms of summarization, studies the dimensions and forms of\nrepresentations, and proposes a multi-dimensional evaluation mechanisms.\nInvestigation extends to the incorporation of pictures into summary and to the\nsummarization of videos, graphs and pictures, and then reaches a general\nsummarization framework.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2015 12:39:50 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Zhuge", "Hai", ""]]}, {"id": "1507.00333", "submitter": "Jie Yang", "authors": "Yuan Lu and Jie Yang", "title": "Notes on Low-rank Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix factorization (MF) is an important technique in data science.\nThe key idea of MF is that there exists latent structures in the data, by\nuncovering which we could obtain a compressed representation of the data. By\nfactorizing an original matrix to low-rank matrices, MF provides a unified\nmethod for dimension reduction, clustering, and matrix completion. In this\narticle we review several important variants of MF, including: Basic MF,\nNon-negative MF, Orthogonal non-negative MF. As can be told from their names,\nnon-negative MF and orthogonal non-negative MF are variants of basic MF with\nnon-negativity and/or orthogonality constraints. Such constraints are useful in\nspecific senarios. In the first part of this article, we introduce, for each of\nthese models, the application scenarios, the distinctive properties, and the\noptimizing method. By properly adapting MF, we can go beyond the problem of\nclustering and matrix completion. In the second part of this article, we will\nextend MF to sparse matrix compeletion, enhance matrix compeletion using\nvarious regularization methods, and make use of MF for (semi-)supervised\nlearning by introducing latent space reinforcement and transformation. We will\nsee that MF is not only a useful model but also as a flexible framework that is\napplicable for various prediction problems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 20:47:34 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 20:44:46 GMT"}, {"version": "v3", "created": "Fri, 6 May 2016 10:35:36 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Lu", "Yuan", ""], ["Yang", "Jie", ""]]}, {"id": "1507.00955", "submitter": "Olga Kolchyna", "authors": "Olga Kolchyna, Tharsis T. P. Souza, Philip Treleaven, Tomaso Aste", "title": "Twitter Sentiment Analysis: Lexicon Method, Machine Learning Method and\n  Their Combination", "comments": "32 pages, 5 figures", "journal-ref": "Handbook of Sentiment Analysis in Finance. Mitra, G. and Yu, X.\n  (Eds.). (2016). ISBN 1910571571", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper covers the two approaches for sentiment analysis: i) lexicon based\nmethod; ii) machine learning method. We describe several techniques to\nimplement these approaches and discuss how they can be adopted for sentiment\nclassification of Twitter messages. We present a comparative study of different\nlexicon combinations and show that enhancing sentiment lexicons with emoticons,\nabbreviations and social-media slang expressions increases the accuracy of\nlexicon-based classification for Twitter. We discuss the importance of feature\ngeneration and feature selection processes for machine learning sentiment\nclassification. To quantify the performance of the main sentiment analysis\nmethods over Twitter we run these algorithms on a benchmark Twitter dataset\nfrom the SemEval-2013 competition, task 2-B. The results show that machine\nlearning method based on SVM and Naive Bayes classifiers outperforms the\nlexicon method. We present a new ensemble method that uses a lexicon based\nsentiment score as input feature for the machine learning approach. The\ncombined method proved to produce more precise classifications. We also show\nthat employing a cost-sensitive classifier for highly unbalanced datasets\nyields an improvement of sentiment classification performance up to 7%.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2015 15:46:55 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 17:24:18 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2015 11:44:33 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Kolchyna", "Olga", ""], ["Souza", "Tharsis T. P.", ""], ["Treleaven", "Philip", ""], ["Aste", "Tomaso", ""]]}, {"id": "1507.01168", "submitter": "Ashish Sureka", "authors": "Ashish Sureka", "title": "Kernel Based Sequential Data Anomaly Detection in Business Process Event\n  Logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business Process Management Systems (BPMS) log events and traces of\nactivities during the execution of a process. Anomalies are defined as\ndeviation or departure from the normal or common order. Anomaly detection in\nbusiness process logs has several applications such as fraud detection and\nunderstanding the causes of process errors. In this paper, we present a novel\napproach for anomaly detection in business process logs. We model the event\nlogs as a sequential data and apply kernel based anomaly detection techniques\nto identify outliers and discordant observations. Our technique is unsupervised\n(does not require a pre-annotated training dataset), employs kNN (k-nearest\nneighbor) kernel based technique and normalized longest common subsequence\n(LCS) similarity measure. We conduct experiments on a recent, large and\nreal-world incident management data of an enterprise and demonstrate that our\napproach is effective.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 05:33:22 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Sureka", "Ashish", ""]]}, {"id": "1507.01338", "submitter": "Bowen Yan", "authors": "Bowen Yan and Jianxi Luo", "title": "Filtering Patent Maps for Visualization of Diversification Paths of\n  Inventors and Organizations", "comments": "2 tables, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the information science literature, recent studies have used patent\ndatabases and patent classification information to construct network maps of\npatent technology classes. In such a patent technology map, almost all pairs of\ntechnology classes are connected, whereas most of the connections between them\nare extremely weak. This observation suggests the possibility of filtering the\npatent network map by removing weak links. However, removing links may reduce\nthe explanatory power of the network on inventor or organization\ndiversification. The network links may explain the patent portfolio\ndiversification paths of inventors and inventing organizations. We measure the\ndiversification explanatory power of the patent network map, and present a\nmethod to objectively choose an optimal trade-off between explanatory power and\nremoving weak links. We show that this method can remove a degree of\narbitrariness compared with previous filtering methods based on arbitrary\nthresholds, and also identify previous filtering methods that created filters\noutside the optimal trade-off. The filtered map aims to aid in network\nvisualization analyses of the technological diversification of inventors,\norganizations and other innovation agents, and potential foresight analysis.\nSuch applications to a prolific inventor (Leonard Forbes) and company (Google)\nare demonstrated.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 07:36:47 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 20:21:14 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Yan", "Bowen", ""], ["Luo", "Jianxi", ""]]}, {"id": "1507.01443", "submitter": "Erik Ferragut", "authors": "Erik M. Ferragut, Jason Laska", "title": "Nonparametric Bayesian Modeling for Automated Database Schema Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of merging databases arises in many government and commercial\napplications. Schema matching, a common first step, identifies equivalent\nfields between databases. We introduce a schema matching framework that builds\nnonparametric Bayesian models for each field and compares them by computing the\nprobability that a single model could have generated both fields. Our\nexperiments show that our method is more accurate and faster than the existing\ninstance-based matching algorithms in part because of the use of nonparametric\nBayesian models.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2015 13:26:02 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Ferragut", "Erik M.", ""], ["Laska", "Jason", ""]]}, {"id": "1507.02002", "submitter": "Giancarlo Crocetti", "authors": "Giancarlo Crocetti", "title": "Topical Discovery of Web Content", "comments": "83 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes the theory and the implementation of a new software tool,\nthe \"Web Topical Discovery System\" (WTDS), which provides an approach to the\nautomatic discovery and selection of new web pages relevant to specific\nanalytical needs. We will see how it is possible to specify the research\ncontext with search keywords related to the area of interest and consider the\nimportant problem of removing extraneous data from a web page containing an\narticle in order to reduce, to a minimum, false positives represented by a\nmatch on a keyword that is showing up on the latest news box of the same page.\nThe removal of duplicates, the analysis of richness of information contained in\nthe article and lexical diversity are all taken into consideration in order to\nprovide the optimum set of recommendations to the end user or system.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 01:06:55 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Crocetti", "Giancarlo", ""]]}, {"id": "1507.02020", "submitter": "Thierry Poibeau", "authors": "Thierry Poibeau (LaTTICe), Pablo Ruiz (LaTTICe)", "title": "Generating Navigable Semantic Maps from Social Sciences Corpora", "comments": "in Digital Humanities 2015, Jun 2015, Sydney, Australia. Actes de la\n  Conf{\\'e}rence Digital Humanities 2015. arXiv admin note: text overlap with\n  arXiv:1406.4211", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now commonplace to observe that we are facing a deluge of online\ninformation. Researchers have of course long acknowledged the potential value\nof this information since digital traces make it possible to directly observe,\ndescribe and analyze social facts, and above all the co-evolution of ideas and\ncommunities over time. However, most online information is expressed through\ntext, which means it is not directly usable by machines, since computers\nrequire structured, organized and typed information in order to be able to\nmanipulate it. Our goal is thus twofold: 1. Provide new natural language\nprocessing techniques aiming at automatically extracting relevant information\nfrom texts, especially in the context of social sciences, and connect these\npieces of information so as to obtain relevant socio-semantic networks; 2.\nProvide new ways of exploring these socio-semantic networks, thanks to tools\nallowing one to dynamically navigate these networks, de-construct and\nre-construct them interactively, from different points of view following the\nneeds expressed by domain experts.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 04:27:48 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Poibeau", "Thierry", "", "LaTTICe"], ["Ruiz", "Pablo", "", "LaTTICe"]]}, {"id": "1507.02021", "submitter": "Thierry Poibeau", "authors": "Fr\\'ed\\'erique M\\'elanie-Becquet (LaTTICe), Johan Ferguth (LaTTICe),\n  Katherine Gruel, Thierry Poibeau (LaTTICe)", "title": "Archaeology in the Digital Age: From Paper to Databases", "comments": "Digital Humanities 2015, Jun 2015, Sydney, Australia. 2015,\n  Proceedings of the conference \"Digital Humanities 2015\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research units in archaeology often manage large and precious archives\ncontaining various documents, including reports on fieldwork, scholarly studies\nand reference books. These archives are of course invaluable, recording decades\nof work, but are generally hard to consult and access. In this context,\ndigitizing full text documents is not enough: information must be formalized,\nstructured and easy to access thanks to friendly user interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 04:28:18 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["M\u00e9lanie-Becquet", "Fr\u00e9d\u00e9rique", "", "LaTTICe"], ["Ferguth", "Johan", "", "LaTTICe"], ["Gruel", "Katherine", "", "LaTTICe"], ["Poibeau", "Thierry", "", "LaTTICe"]]}, {"id": "1507.02140", "submitter": "Xiaojun Wan", "authors": "Yue Hu and Xiaojun Wan", "title": "Mining and Analyzing the Future Works in Scientific Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future works in scientific articles are valuable for researchers and they can\nguide researchers to new research directions or ideas. In this paper, we mine\nthe future works in scientific articles in order to 1) provide an insight for\nfuture work analysis and 2) facilitate researchers to search and browse future\nworks in a research area. First, we study the problem of future work extraction\nand propose a regular expression based method to address the problem. Second,\nwe define four different categories for the future works by observing the data\nand investigate the multi-class future work classification problem. Third, we\napply the extraction method and the classification model to a paper dataset in\nthe computer science field and conduct a further analysis of the future works.\nFinally, we design a prototype system to search and demonstrate the future\nworks mined from the scientific papers. Our evaluation results show that our\nextraction method can get high precision and recall values and our\nclassification model can also get good results and it outperforms several\nbaseline models. Further analysis of the future work sentences also indicates\ninteresting results.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 13:14:38 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Hu", "Yue", ""], ["Wan", "Xiaojun", ""]]}, {"id": "1507.02221", "submitter": "Alessandro Sordoni", "authors": "Alessandro Sordoni, Yoshua Bengio, Hossein Vahabi, Christina Lioma,\n  Jakob G. Simonsen and Jian-Yun Nie", "title": "A Hierarchical Recurrent Encoder-Decoder For Generative Context-Aware\n  Query Suggestion", "comments": "To appear in Conference of Information Knowledge and Management\n  (CIKM) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users may strive to formulate an adequate textual query for their information\nneed. Search engines assist the users by presenting query suggestions. To\npreserve the original search intent, suggestions should be context-aware and\naccount for the previous queries issued by the user. Achieving context\nawareness is challenging due to data sparsity. We present a probabilistic\nsuggestion model that is able to account for sequences of previous queries of\narbitrary lengths. Our novel hierarchical recurrent encoder-decoder\narchitecture allows the model to be sensitive to the order of queries in the\ncontext while avoiding data sparsity. Additionally, our model can suggest for\nrare, or long-tail, queries. The produced suggestions are synthetic and are\nsampled one word at a time, using computationally cheap decoding techniques.\nThis is in contrast to current synthetic suggestion models relying upon machine\nlearning pipelines and hand-engineered feature sets. Results show that it\noutperforms existing context-aware approaches in a next query prediction\nsetting. In addition to query suggestion, our model is general enough to be\nused in a variety of other applications.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2015 17:06:50 GMT"}], "update_date": "2015-07-09", "authors_parsed": [["Sordoni", "Alessandro", ""], ["Bengio", "Yoshua", ""], ["Vahabi", "Hossein", ""], ["Lioma", "Christina", ""], ["Simonsen", "Jakob G.", ""], ["Nie", "Jian-Yun", ""]]}, {"id": "1507.02447", "submitter": "Santosh Tirunagari", "authors": "Santosh Tirunagari", "title": "Data Mining of Causal Relations from Text: Analysing Maritime Accident\n  Investigation Reports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text mining is a process of extracting information of interest from text.\nSuch a method includes techniques from various areas such as Information\nRetrieval (IR), Natural Language Processing (NLP), and Information Extraction\n(IE). In this study, text mining methods are applied to extract causal\nrelations from maritime accident investigation reports collected from the\nMarine Accident Investigation Branch (MAIB). These causal relations provide\ninformation on various mechanisms behind accidents, including human and\norganizational factors relating to the accident. The objective of this study is\nto facilitate the analysis of the maritime accident investigation reports, by\nmeans of extracting contributory causes with more feasibility. A careful\ninvestigation of contributory causes from the reports provide opportunity to\nimprove safety in future.\n  Two methods have been employed in this study to extract the causal relations.\nThey are 1) Pattern classification method and 2) Connectives method. The\nearlier one uses naive Bayes and Support Vector Machines (SVM) as classifiers.\nThe latter simply searches for the words connecting cause and effect in\nsentences.\n  The causal patterns extracted using these two methods are compared to the\nmanual (human expert) extraction. The pattern classification method showed a\nfair and sensible performance with F-measure(average) = 65% when compared to\nconnectives method with F-measure(average) = 58%. This study is an evidence,\nthat text mining methods could be employed in extracting causal relations from\nmarine accident investigation reports.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 10:20:52 GMT"}], "update_date": "2015-07-10", "authors_parsed": [["Tirunagari", "Santosh", ""]]}, {"id": "1507.02743", "submitter": "Prateek Jain", "authors": "Kush Bhatia and Himanshu Jain and Purushottam Kar and Prateek Jain and\n  Manik Varma", "title": "Locally Non-linear Embeddings for Extreme Multi-label Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective in extreme multi-label learning is to train a classifier that\ncan automatically tag a novel data point with the most relevant subset of\nlabels from an extremely large label set. Embedding based approaches make\ntraining and prediction tractable by assuming that the training label matrix is\nlow-rank and hence the effective number of labels can be reduced by projecting\nthe high dimensional label vectors onto a low dimensional linear subspace.\nStill, leading embedding approaches have been unable to deliver high prediction\naccuracies or scale to large problems as the low rank assumption is violated in\nmost real world applications.\n  This paper develops the X-One classifier to address both limitations. The\nmain technical contribution in X-One is a formulation for learning a small\nensemble of local distance preserving embeddings which can accurately predict\ninfrequently occurring (tail) labels. This allows X-One to break free of the\ntraditional low-rank assumption and boost classification accuracy by learning\nembeddings which preserve pairwise distances between only the nearest label\nvectors.\n  We conducted extensive experiments on several real-world as well as benchmark\ndata sets and compared our method against state-of-the-art methods for extreme\nmulti-label classification. Experiments reveal that X-One can make\nsignificantly more accurate predictions then the state-of-the-art methods\nincluding both embeddings (by as much as 35%) as well as trees (by as much as\n6%). X-One can also scale efficiently to data sets with a million labels which\nare beyond the pale of leading embedding methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2015 23:29:10 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Bhatia", "Kush", ""], ["Jain", "Himanshu", ""], ["Kar", "Purushottam", ""], ["Jain", "Prateek", ""], ["Varma", "Manik", ""]]}, {"id": "1507.02907", "submitter": "Luis Marujo", "authors": "Lu\\'is Marujo, Ricardo Ribeiro, David Martins de Matos, Jo\\~ao P.\n  Neto, Anatole Gershman, Jaime Carbonell", "title": "Extending a Single-Document Summarizer to Multi-Document: a Hierarchical\n  Approach", "comments": "6 pages, Please cite: Proceedings of *SEM: the 4th Joint Conference\n  on Lexical and Computational Semantics (bibtex:\n  http://aclweb.org/anthology/S/S15/S15-1020.bib)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing amount of online content motivated the development of\nmulti-document summarization methods. In this work, we explore straightforward\napproaches to extend single-document summarization methods to multi-document\nsummarization. The proposed methods are based on the hierarchical combination\nof single-document summaries, and achieves state of the art results.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 13:59:00 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Marujo", "Lu\u00eds", ""], ["Ribeiro", "Ricardo", ""], ["de Matos", "David Martins", ""], ["Neto", "Jo\u00e3o P.", ""], ["Gershman", "Anatole", ""], ["Carbonell", "Jaime", ""]]}, {"id": "1507.02987", "submitter": "Felipe Fernandes Albrecht", "authors": "Felipe Albrecht", "title": "Genoogle: an indexed and parallelized search engine for similar DNA\n  sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.IR q-bio.GN", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The search for similar genetic sequences is one of the main bioinformatics\ntasks. The genetic sequences data banks are growing exponentially and the\nsearching techniques that use linear time are not capable to do the search in\nthe required time anymore. Another problem is that the clock speed of the\nmodern processors are not growing as it did before, instead, the processing\ncapacity is growing with the addiction of more processing cores and the\ntechniques which does not use parallel computing does not have benefits from\nthese extra cores. This work aims to use data indexing techniques to reduce the\nsearching process computation cost united with the parallelization of the\nsearching techniques to use the computational capacity of the multi core\nprocessors. To verify the viability of using these two techniques\nsimultaneously, a software which uses parallelization techniques with inverted\nindexes was developed.\n  Experiments were executed to analyze the performance gain when parallelism is\nutilized, the search time gain, and also the quality of the results when it\ncompared with others searching tools. The results of these experiments were\npromising, the parallelism gain overcame the expected speedup, the searching\ntime was 20 times faster than the parallelized NCBI BLAST, and the searching\nresults showed a good quality when compared with this tool.\n  The software source code is available at\nhttps://github.com/felipealbrecht/Genoogle .\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2015 18:50:31 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Albrecht", "Felipe", ""]]}, {"id": "1507.03067", "submitter": "Takeaki Uno", "authors": "Takeaki Uno, Hiroki Maegawa, Takanobu Nakahara, Yukinobu Hamuro, Ryo\n  Yoshinaka, Makoto Tatsuta", "title": "Micro-Clustering: Finding Small Clusters in Large Diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of un-supervised soft-clustering called\nmicro-clustering. The aim of the problem is to enumerate all groups composed of\nrecords strongly related to each other, while standard clustering methods\nseparate records at sparse parts. The problem formulation of micro-clustering\nis non-trivial. Clique mining in a similarity graph is a typical approach, but\nit results in a huge number of cliques that are of many similar cliques. We\npropose a new concept data polishing. The cause of huge solutions can be\nconsidered that the groups are not clear in the data, that is, the boundaries\nof the groups are not clear, because of noise, uncertainty, lie, lack, etc.\nData polishing clarifies the groups by perturbating the data. Specifically,\ndense subgraphs that would correspond to clusters are replaced by cliques. The\nclusters are clarified as maximal cliques, thus the number of maximal cliques\nwill be drastically reduced. We also propose an efficient algorithm applicable\neven for large scale data. Computational experiments showed the efficiency of\nour algorithm, i.e., the number of solutions is small, (e.g., 1,000), the\nmembers of each group are deeply related, and the computation time is short.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 06:21:19 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 12:41:49 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Uno", "Takeaki", ""], ["Maegawa", "Hiroki", ""], ["Nakahara", "Takanobu", ""], ["Hamuro", "Yukinobu", ""], ["Yoshinaka", "Ryo", ""], ["Tatsuta", "Makoto", ""]]}, {"id": "1507.03077", "submitter": "Adel Rahimi", "authors": "Adel Rahimi", "title": "A new hybrid stemming algorithm for Persian", "comments": "8 pages, 5 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Stemming has been an influential part in Information retrieval and search\nengines. There have been tremendous endeavours in making stemmer that are both\nefficient and accurate. Stemmers can have three method in stemming, Dictionary\nbased stemmer, statistical-based stemmers, and rule-based stemmers. This paper\naims at building a hybrid stemmer that uses both Dictionary based method and\nrule-based method for stemming. This ultimately helps the efficacy and\naccurateness of the stemmer.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2015 08:54:45 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 11:57:40 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Rahimi", "Adel", ""]]}, {"id": "1507.03650", "submitter": "Neil Shah", "authors": "Neil Shah, Yang Song", "title": "S-index: Towards Better Metrics for Quantifying Research Impact", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing growth in the volume of scientific literature available today\nprecludes researchers from efficiently discerning the relevant from irrelevant\ncontent. Researchers are constantly interested in impactful papers, authors and\nvenues in their respective fields. Moreover, they are interested in the\nso-called recent \"rising stars\" of these contexts which may lead to attractive\ndirections for future work, collaborations or impactful publication venues. In\nthis work, we address the problem of quantifying research impact in each of\nthese contexts, in order to better direct attention of researchers and\nstreamline the processes of comparison, ranking and evaluation of contribution.\nSpecifically, we begin by outlining intuitive underlying assumptions that\nimpact quantification methods should obey and evaluate when current\nstate-of-the-art methods fail to satisfy these properties. To this end, we\nintroduce the s-index metric which quantifies research impact through influence\npropagation over a heterogeneous citation network. s-index is tailored from\nthese intuitive assumptions and offers a number of desirable qualities\nincluding robustness, natural temporality and straightforward extensibility\nfrom the paper impact to broader author and venue impact contexts. We evaluate\nits effectiveness on the publicly available Microsoft Academic Search citation\ngraph with over 119 million papers and 1 billion citation edges with 103\nmillion and 21 thousand associated authors and venues respectively.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2015 23:07:13 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Shah", "Neil", ""], ["Song", "Yang", ""]]}, {"id": "1507.03928", "submitter": "Fernando Diaz", "authors": "Fernando Diaz", "title": "Pseudo-Query Reformulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic query reformulation refers to rewriting a user's original query in\norder to improve the ranking of retrieval results compared to the original\nquery. We present a general framework for automatic query reformulation based\non discrete optimization. Our approach, referred to as pseudo-query\nreformulation, treats automatic query reformulation as a search problem over\nthe graph of unweighted queries linked by minimal transformations (e.g. term\nadditions, deletions). This framework allows us to test existing performance\nprediction methods as heuristics for the graph search process. We demonstrate\nthe effectiveness of the approach on several publicly available datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2015 17:06:51 GMT"}], "update_date": "2015-07-15", "authors_parsed": [["Diaz", "Fernando", ""]]}, {"id": "1507.04113", "submitter": "Francesco Caltagirone", "authors": "Maria Chiara Angelini and Francesco Caltagirone and Florent Krzakala\n  and Lenka Zdeborov\\'a", "title": "Spectral Detection on Sparse Hypergraphs", "comments": "8 pages, 5 figures", "journal-ref": "2015 53rd Annual Allerton Conference on Communication, Control,\n  and Computing, pages 66 - 73, IEEE", "doi": "10.1109/ALLERTON.2015.7446987", "report-no": null, "categories": "cs.SI cond-mat.stat-mech cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of the assignment of nodes into communities from a\nset of hyperedges, where every hyperedge is a noisy observation of the\ncommunity assignment of the adjacent nodes. We focus in particular on the\nsparse regime where the number of edges is of the same order as the number of\nvertices. We propose a spectral method based on a generalization of the\nnon-backtracking Hashimoto matrix into hypergraphs. We analyze its performance\non a planted generative model and compare it with other spectral methods and\nwith Bayesian belief propagation (which was conjectured to be asymptotically\noptimal for this model). We conclude that the proposed spectral method detects\ncommunities whenever belief propagation does, while having the important\nadvantages to be simpler, entirely nonparametric, and to be able to learn the\nrule according to which the hyperedges were generated without prior\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 08:14:12 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Angelini", "Maria Chiara", ""], ["Caltagirone", "Francesco", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1507.04412", "submitter": "Xiwei Liu", "authors": "Xiwei Liu", "title": "Bridge the gap between network-based inference method and global ranking\n  method in personal recommendation", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the relationship between the network-based inference\nmethod and global ranking method in personal recommendation. By some\ntheoretical analysis, we prove that the recommendation result under the global\nranking method is the limit of applying network-based inference method with\ninfinity times.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2015 23:13:48 GMT"}], "update_date": "2015-07-17", "authors_parsed": [["Liu", "Xiwei", ""]]}, {"id": "1507.04798", "submitter": "Samuel R\\\"onnqvist", "authors": "Samuel R\\\"onnqvist", "title": "Exploratory topic modeling with distributional semantics", "comments": "Conference: The Fourteenth International Symposium on Intelligent\n  Data Analysis (IDA 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  As we continue to collect and store textual data in a multitude of domains,\nwe are regularly confronted with material whose largely unknown thematic\nstructure we want to uncover. With unsupervised, exploratory analysis, no prior\nknowledge about the content is required and highly open-ended tasks can be\nsupported. In the past few years, probabilistic topic modeling has emerged as a\npopular approach to this problem. Nevertheless, the representation of the\nlatent topics as aggregations of semi-coherent terms limits their\ninterpretability and level of detail.\n  This paper presents an alternative approach to topic modeling that maps\ntopics as a network for exploration, based on distributional semantics using\nlearned word vectors. From the granular level of terms and their semantic\nsimilarity relations global topic structures emerge as clustered regions and\ngradients of concepts. Moreover, the paper discusses the visual interactive\nrepresentation of the topic map, which plays an important role in supporting\nits exploration.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2015 23:11:45 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""]]}, {"id": "1507.04921", "submitter": "Chi Ho Yeung", "authors": "Chi Ho Yeung", "title": "Do recommender systems benefit users?", "comments": "15 pages, 6 figures", "journal-ref": "J. Stat. Mech. 043401 (2016)", "doi": "10.1088/1742-5468/2016/04/043401", "report-no": null, "categories": "cs.CY cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are present in many web applications to guide our\nchoices. They increase sales and benefit sellers, but whether they benefit\ncustomers by providing relevant products is questionable. Here we introduce a\nmodel to examine the benefit of recommender systems for users, and found that\nrecommendations from the system can be equivalent to random draws if one relies\ntoo strongly on the system. Nevertheless, with sufficient information about\nuser preferences, recommendations become accurate and an abrupt transition to\nthis accurate regime is observed for some algorithms. On the other hand, we\nfound that a high accuracy evaluated by common accuracy metrics does not\nnecessarily correspond to a high real accuracy nor a benefit for users, which\nserves as an alarm for operators and researchers of recommender systems. We\ntested our model with a real dataset and observed similar behaviors. Finally, a\nrecommendation approach with improved accuracy is suggested. These results\nimply that recommender systems can benefit users, but relying too strongly on\nthe system may render the system ineffective.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2015 14:56:11 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Yeung", "Chi Ho", ""]]}, {"id": "1507.05150", "submitter": "Amandianeze Nwana", "authors": "Amandianeze O. Nwana and Tshuan Chen", "title": "Towards Understanding User Preferences from User Tagging Behavior for\n  Personalization", "comments": "6 pages", "journal-ref": null, "doi": "10.1109/ISM.2015.79", "report-no": null, "categories": "cs.MM cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalizing image tags is a relatively new and growing area of research,\nand in order to advance this research community, we must review and challenge\nthe de-facto standard of defining tag importance. We believe that for greater\nprogress to be made, we must go beyond tags that merely describe objects that\nare visually represented in the image, towards more user-centric and subjective\nnotions such as emotion, sentiment, and preferences.\n  We focus on the notion of user preferences and show that the order that users\nlist tags on images is correlated to the order of preference over the tags that\nthey provided for the image. While this observation is not completely\nsurprising, to our knowledge, we are the first to explore this aspect of user\ntagging behavior systematically and report empirical results to support this\nobservation. We argue that this observation can be exploited to help advance\nthe image tagging (and related) communities.\n  Our contributions include: 1.) conducting a user study demonstrating this\nobservation, 2.) collecting a dataset with user tag preferences explicitly\ncollected.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 05:55:37 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 19:56:36 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Nwana", "Amandianeze O.", ""], ["Chen", "Tshuan", ""]]}, {"id": "1507.05214", "submitter": "Antonia Korba", "authors": "Antonia Korba", "title": "On the Application of Link Analysis Algorithms for Ranking Bipartite\n  Graphs", "comments": "Thesis in Greek for the University of Patras Engineering Diploma\n  (Master in Engineering) from the Department of Computer Engineering and\n  Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently bipartite graphs have been widely used to represent the relationship\ntwo sets of items for information retrieval applications. The Web offers a wide\nrange of data which can be represented by bipartite graphs, such us movies and\nreviewers in recomender systems, queries and URLs in search engines, users and\nposts in social networks. The size and the dynamic nature of such graphs\ngenerate the need for more efficient ranking methods.\n  In this thesis, at first we present the fundamental mathematical backround\nthat we use subsequently and we describe the basic principles of the\nPerron-Frobebius theory for non negative matrices as well as the the basic\nprinciples of the Markov chain theory. Then, we propose a novel algorithm named\nBipartiteRank, which is suitable to rank scenarios, that can be represented as\na bipartite graph. This algorithm is based on the random surfer model and\ninherits the basic mathematical characteristics of PageRank. What makes it\ndifferent, is the fact that it introduces an alternative type of teleportation,\nbased on the block structure of the bipartite graph in order to achieve more\nefficient ranking. Finally, we support this opinion with mathematical arguments\nand then we confirm it experimentally through a series of tests on real data.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2015 18:36:08 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Korba", "Antonia", ""]]}, {"id": "1507.05371", "submitter": "Luis F Voloch", "authors": "Guy Bresler, Devavrat Shah, and Luis F. Voloch", "title": "Regret Guarantees for Item-Item Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is much empirical evidence that item-item collaborative filtering works\nwell in practice. Motivated to understand this, we provide a framework to\ndesign and analyze various recommendation algorithms. The setup amounts to\nonline binary matrix completion, where at each time a random user requests a\nrecommendation and the algorithm chooses an entry to reveal in the user's row.\nThe goal is to minimize regret, or equivalently to maximize the number of +1\nentries revealed at any time. We analyze an item-item collaborative filtering\nalgorithm that can achieve fundamentally better performance compared to\nuser-user collaborative filtering. The algorithm achieves good \"cold-start\"\nperformance (appropriately defined) by quickly making good recommendations to\nnew users about whom there is little information.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 02:45:01 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 15:28:40 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Bresler", "Guy", ""], ["Shah", "Devavrat", ""], ["Voloch", "Luis F.", ""]]}, {"id": "1507.05497", "submitter": "Dmitry Ignatov", "authors": "Dmitry I. Ignatov and Denis Kornilov", "title": "RAPS: A Recommender Algorithm Based on Pattern Structures", "comments": "The paper presented at FCA4AI 2015 in conjunction with IJCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for recommender systems with numeric ratings which\nis based on Pattern Structures (RAPS). As the input the algorithm takes rating\nmatrix, e.g., such that it contains movies rated by users. For a target user,\nthe algorithm returns a rated list of items (movies) based on its previous\nratings and ratings of other users. We compare the results of the proposed\nalgorithm in terms of precision and recall measures with Slope One, one of the\nstate-of-the-art item-based algorithms, on Movie Lens dataset and RAPS\ndemonstrates the best or comparable quality.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2015 13:58:30 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Ignatov", "Dmitry I.", ""], ["Kornilov", "Denis", ""]]}, {"id": "1507.05929", "submitter": "Yaniv Plan", "authors": "Roger Donaldson, Arijit Gupta, Yaniv Plan, and Thomas Reimer", "title": "Random mappings designed for commercial search engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a practical random mapping that takes any set of documents\nrepresented as vectors in Euclidean space and then maps them to a sparse subset\nof the Hamming cube while retaining ordering of inter-vector inner products.\nOnce represented in the sparse space, it is natural to index documents using\ncommercial text-based search engines which are specialized to take advantage of\nthis sparse and discrete structure for large-scale document retrieval. We give\na theoretical analysis of the mapping scheme, characterizing exact asymptotic\nbehavior and also giving non-asymptotic bounds which we verify through\nnumerical simulations. We balance the theoretical treatment with several\npractical considerations; these allow substantial speed up of the method. We\nfurther illustrate the use of this method on search over two real data sets: a\ncorpus of images represented by their color histograms, and a corpus of daily\nstock market index values.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 18:05:54 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Donaldson", "Roger", ""], ["Gupta", "Arijit", ""], ["Plan", "Yaniv", ""], ["Reimer", "Thomas", ""]]}, {"id": "1507.05999", "submitter": "Peter Lofgren", "authors": "Peter Lofgren, Siddhartha Banerjee, Ashish Goel", "title": "Personalized PageRank Estimation and Search: A Bidirectional Approach", "comments": "WSDM 2016", "journal-ref": null, "doi": "10.1145/2835776.2835823", "report-no": null, "categories": "cs.DS cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new algorithms for Personalized PageRank estimation and\nPersonalized PageRank search. First, for the problem of estimating Personalized\nPageRank (PPR) from a source distribution to a target node, we present a new\nbidirectional estimator with simple yet strong guarantees on correctness and\nperformance, and 3x to 8x speedup over existing estimators in experiments on a\ndiverse set of networks. Moreover, it has a clean algebraic structure which\nenables it to be used as a primitive for the Personalized PageRank Search\nproblem: Given a network like Facebook, a query like \"people named John\", and a\nsearching user, return the top nodes in the network ranked by PPR from the\nperspective of the searching user. Previous solutions either score all nodes or\nscore candidate nodes one at a time, which is prohibitively slow for large\ncandidate sets. We develop a new algorithm based on our bidirectional PPR\nestimator which identifies the most relevant results by sampling candidates\nbased on their PPR; this is the first solution to PPR search that can find the\nbest results without iterating through the set of all candidate results.\nFinally, by combining PPR sampling with sequential PPR estimation and Monte\nCarlo, we develop practical algorithms for PPR search, and we show via\nexperiments that our algorithms are efficient on networks with billions of\nedges.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2015 21:50:08 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2015 22:54:15 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2015 02:07:04 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Lofgren", "Peter", ""], ["Banerjee", "Siddhartha", ""], ["Goel", "Ashish", ""]]}, {"id": "1507.06235", "submitter": "Richard Zanibbi", "authors": "Richard Zanibbi, Kenny Davila, Andrew Kane and Frank Tompa", "title": "The Tangent Search Engine: Improved Similarity Metrics and Scalability\n  for Math Formula Search", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever-increasing quantity and variety of data worldwide, the Web has\nbecome a rich repository of mathematical formulae. This necessitates the\ncreation of robust and scalable systems for Mathematical Information Retrieval,\nwhere users search for mathematical information using individual formulae\n(query-by-expression) or a combination of keywords and formulae. Often, the\npages that best satisfy users' information needs contain expressions that only\napproximately match the query formulae. For users trying to locate or re-find a\nspecific expression, browse for similar formulae, or who are mathematical\nnon-experts, the similarity of formulae depends more on the relative positions\nof symbols than on deep mathematical semantics.\n  We propose the Maximum Subtree Similarity (MSS) metric for\nquery-by-expression that produces intuitive rankings of formulae based on their\nappearance, as represented by the types and relative positions of symbols.\nBecause it is too expensive to apply the metric against all formulae in large\ncollections, we first retrieve expressions using an inverted index over tuples\nthat encode relationships between pairs of symbols, ranking hits using the Dice\ncoefficient. The top-k formulae are then re-ranked using MSS. Our approach\nobtains state-of-the-art performance on the NTCIR-11 Wikipedia formula\nretrieval benchmark and is efficient in terms of both index space and overall\nretrieval time. Retrieval systems for other graphical forms, including chemical\ndiagrams, flowcharts, figures, and tables, may also benefit from adopting our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2015 16:02:00 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Zanibbi", "Richard", ""], ["Davila", "Kenny", ""], ["Kane", "Andrew", ""], ["Tompa", "Frank", ""]]}, {"id": "1507.06452", "submitter": "Amin Mantrach", "authors": "Robin Devooght and Nicolas Kourtellis and Amin Mantrach", "title": "Dynamic Matrix Factorization with Priors on Unknown Values", "comments": "in the Proceedings of 21st ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced and effective collaborative filtering methods based on explicit\nfeedback assume that unknown ratings do not follow the same model as the\nobserved ones (\\emph{not missing at random}). In this work, we build on this\nassumption, and introduce a novel dynamic matrix factorization framework that\nallows to set an explicit prior on unknown values. When new ratings, users, or\nitems enter the system, we can update the factorization in time independent of\nthe size of data (number of users, items and ratings). Hence, we can quickly\nrecommend items even to very recent users. We test our methods on three large\ndatasets, including two very sparse ones, in static and dynamic conditions. In\neach case, we outrank state-of-the-art matrix factorization methods that do not\nuse a prior on unknown ratings.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 11:39:58 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Devooght", "Robin", ""], ["Kourtellis", "Nicolas", ""], ["Mantrach", "Amin", ""]]}, {"id": "1507.06593", "submitter": "Ashwinkumar Ganesan", "authors": "Ashwinkumar Ganesan, Kiante Brantley, Shimei Pan, Jian Chen", "title": "LDAExplore: Visualizing Topic Models Generated Using Latent Dirichlet\n  Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LDAExplore, a tool to visualize topic distributions in a given\ndocument corpus that are generated using Topic Modeling methods. Latent\nDirichlet Allocation (LDA) is one of the basic methods that is predominantly\nused to generate topics. One of the problems with methods like LDA is that\nusers who apply them may not understand the topics that are generated. Also,\nusers may find it difficult to search correlated topics and correlated\ndocuments. LDAExplore, tries to alleviate these problems by visualizing topic\nand word distributions generated from the document corpus and allowing the user\nto interact with them. The system is designed for users, who have minimal\nknowledge of LDA or Topic Modelling methods. To evaluate our design, we run a\npilot study which uses the abstracts of 322 Information Visualization papers,\nwhere every abstract is considered a document. The topics generated are then\nexplored by users. The results show that users are able to find correlated\ndocuments and group them based on topics that are similar.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 18:15:03 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Ganesan", "Ashwinkumar", ""], ["Brantley", "Kiante", ""], ["Pan", "Shimei", ""], ["Chen", "Jian", ""]]}, {"id": "1507.06667", "submitter": "Elham Khabiri", "authors": "Fenno F. Heath III, Richard Hull, Elham Khabiri, Matthew Riemer, Noi\n  Sukaviriya, and Roman Vaculin", "title": "Alexandria: Extensible Framework for Rapid Exploration of Social Media", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Alexandria system under development at IBM Research provides an\nextensible framework and platform for supporting a variety of big-data\nanalytics and visualizations. The system is currently focused on enabling rapid\nexploration of text-based social media data. The system provides tools to help\nwith constructing \"domain models\" (i.e., families of keywords and extractors to\nenable focus on tweets and other social media documents relevant to a project),\nto rapidly extract and segment the relevant social media and its authors, to\napply further analytics (such as finding trends and anomalous terms), and\nvisualizing the results. The system architecture is centered around a variety\nof REST-based service APIs to enable flexible orchestration of the system\ncapabilities; these are especially useful to support knowledge-worker driven\niterative exploration of social phenomena. The architecture also enables rapid\nintegration of Alexandria capabilities with other social media analytics\nsystem, as has been demonstrated through an integration with IBM Research's\nSystemG. This paper describes a prototypical usage scenario for Alexandria,\nalong with the architecture and key underlying analytics.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2015 20:10:44 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Heath", "Fenno F.", "III"], ["Hull", "Richard", ""], ["Khabiri", "Elham", ""], ["Riemer", "Matthew", ""], ["Sukaviriya", "Noi", ""], ["Vaculin", "Roman", ""]]}, {"id": "1507.06829", "submitter": "Lisa Posch", "authors": "Lisa Posch, Arnim Bleier, Philipp Schaer, Markus Strohmaier", "title": "The Polylingual Labeled Topic Model", "comments": "Accepted for publication at KI 2015 (38th edition of the German\n  Conference on Artificial Intelligence)", "journal-ref": null, "doi": "10.1007/978-3-319-24489-1_26", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the Polylingual Labeled Topic Model, a model which\ncombines the characteristics of the existing Polylingual Topic Model and\nLabeled LDA. The model accounts for multiple languages with separate topic\ndistributions for each language while restricting the permitted topics of a\ndocument to a set of predefined labels. We explore the properties of the model\nin a two-language setting on a dataset from the social science domain. Our\nexperiments show that our model outperforms LDA and Labeled LDA in terms of\ntheir held-out perplexity and that it produces semantically coherent topics\nwhich are well interpretable by human subjects.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2015 13:01:20 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Posch", "Lisa", ""], ["Bleier", "Arnim", ""], ["Schaer", "Philipp", ""], ["Strohmaier", "Markus", ""]]}, {"id": "1507.07382", "submitter": "Maxim Borisyak", "authors": "Maxim Borisyak, Roman Zykov, Artem Noskov", "title": "Application of Kullback-Leibler divergence for short-term user interest\n  detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical approaches in recommender systems such as collaborative filtering\nare concentrated mainly on static user preference extraction. This approach\nworks well as an example for music recommendations when a user behavior tends\nto be stable over long period of time, however the most common situation in\ne-commerce is different which requires reactive algorithms based on a\nshort-term user activity analysis. This paper introduces a small mathematical\nframework for short-term user interest detection formulated in terms of item\nproperties and its application for recommender systems enhancing. The framework\nis based on the fundamental concept of information theory --- Kullback-Leibler\ndivergence.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2015 12:05:58 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Borisyak", "Maxim", ""], ["Zykov", "Roman", ""], ["Noskov", "Artem", ""]]}, {"id": "1507.08107", "submitter": "Paul Lagr\\'ee", "authors": "Paul Lagr\\'ee and Bogdan Cautis and Hossein Vahabi", "title": "A Network-Aware Approach for Searching As-You-Type in Social Media\n  (Extended Version)", "comments": "11 pages, To appear in Conference of Information Knowledge and\n  Management (CIKM) 2015, Extended Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a novel approach for as-you-type top-$k$ keyword\nsearch over social media. We adopt a natural \"network-aware\" interpretation for\ninformation relevance, by which information produced by users who are closer to\nthe seeker is considered more relevant. In practice, this query model poses new\nchallenges for effectiveness and efficiency in online search, even when a\ncomplete query is given as input in one keystroke. This is mainly because it\nrequires a joint exploration of the social space and classic IR indexes such as\ninverted lists. We describe a memory-efficient and incremental prefix-based\nretrieval algorithm, which also exhibits an anytime behavior, allowing to\noutput the most likely answer within any chosen running-time limit. We evaluate\nit through extensive experiments for several applications and search scenarios,\nincluding searching for posts in micro-blogging (Twitter and Tumblr), as well\nas searching for businesses based on reviews in Yelp. They show that our\nsolution is effective in answering real-time as-you-type searches over social\nmedia.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 11:48:43 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Lagr\u00e9e", "Paul", ""], ["Cautis", "Bogdan", ""], ["Vahabi", "Hossein", ""]]}, {"id": "1507.08120", "submitter": "Daniel Lamprecht", "authors": "Daniel Lamprecht, Markus Strohmaier, Denis Helic", "title": "Improving Reachability and Navigability in Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate recommender systems from a network perspective\nand investigate recommendation networks, where nodes are items (e.g., movies)\nand edges are constructed from top-N recommendations (e.g., related movies). In\nparticular, we focus on evaluating the reachability and navigability of\nrecommendation networks and investigate the following questions: (i) How well\ndo recommendation networks support navigation and exploratory search? (ii) What\nis the influence of parameters, in particular different recommendation\nalgorithms and the number of recommendations shown, on reachability and\nnavigability? and (iii) How can reachability and navigability be improved in\nthese networks? We tackle these questions by first evaluating the reachability\nof recommendation networks by investigating their structural properties.\nSecond, we evaluate navigability by simulating three different models of\ninformation seeking scenarios. We find that with standard algorithms,\nrecommender systems are not well suited to navigation and exploration and\npropose methods to modify recommendations to improve this. Our work extends\nfrom one-click-based evaluations of recommender systems towards multi-click\nanalysis (i.e., sequences of dependent clicks) and presents a general,\ncomprehensive approach to evaluating navigability of arbitrary recommendation\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 12:44:49 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Lamprecht", "Daniel", ""], ["Strohmaier", "Markus", ""], ["Helic", "Denis", ""]]}, {"id": "1507.08198", "submitter": "Christina Lioma Assoc. Prof", "authors": "Christina Lioma and Jakob Grue Simonsen and Birger Larsen and Niels\n  Dalum Hansen", "title": "Non-Compositional Term Dependence for Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling term dependence in IR aims to identify co-occurring terms that are\ntoo heavily dependent on each other to be treated as a bag of words, and to\nadapt the indexing and ranking accordingly. Dependent terms are predominantly\nidentified using lexical frequency statistics, assuming that (a) if terms\nco-occur often enough in some corpus, they are semantically dependent; (b) the\nmore often they co-occur, the more semantically dependent they are. This\nassumption is not always correct: the frequency of co-occurring terms can be\nseparate from the strength of their semantic dependence. E.g. \"red tape\" might\nbe overall less frequent than \"tape measure\" in some corpus, but this does not\nmean that \"red\"+\"tape\" are less dependent than \"tape\"+\"measure\". This is\nespecially the case for non-compositional phrases, i.e. phrases whose meaning\ncannot be composed from the individual meanings of their terms (such as the\nphrase \"red tape\" meaning bureaucracy). Motivated by this lack of distinction\nbetween the frequency and strength of term dependence in IR, we present a\nprincipled approach for handling term dependence in queries, using both lexical\nfrequency and semantic evidence. We focus on non-compositional phrases,\nextending a recent unsupervised model for their detection [21] to IR. Our\napproach, integrated into ranking using Markov Random Fields [31], yields\neffectiveness gains over competitive TREC baselines, showing that there is\nstill room for improvement in the very well-studied area of term dependence in\nIR.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 16:08:48 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Lioma", "Christina", ""], ["Simonsen", "Jakob Grue", ""], ["Larsen", "Birger", ""], ["Hansen", "Niels Dalum", ""]]}, {"id": "1507.08234", "submitter": "Christina Lioma Assoc. Prof", "authors": "Casper Petersen and Christina Lioma and Jakob Grue Simonsen and Birger\n  Larsen", "title": "Entropy and Graph Based Modelling of Document Coherence using Discourse\n  Entities: An Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two novel models of document coherence and their application to\ninformation retrieval (IR). Both models approximate document coherence using\ndiscourse entities, e.g. the subject or object of a sentence. Our first model\nviews text as a Markov process generating sequences of discourse entities\n(entity n-grams); we use the entropy of these entity n-grams to approximate the\nrate at which new information appears in text, reasoning that as more new words\nappear, the topic increasingly drifts and text coherence decreases. Our second\nmodel extends the work of Guinaudeau & Strube [28] that represents text as a\ngraph of discourse entities, linked by different relations, such as their\ndistance or adjacency in text. We use several graph topology metrics to\napproximate different aspects of the discourse flow that can indicate\ncoherence, such as the average clustering or betweenness of discourse entities\nin text. Experiments with several instantiations of these models show that: (i)\nour models perform on a par with two other well-known models of text coherence\neven without any parameter tuning, and (ii) reranking retrieval results\naccording to their coherence scores gives notable performance gains, confirming\na relation between document coherence and relevance. This work contributes two\nnovel models of document coherence, the application of which to IR complements\nrecent work in the integration of document cohesiveness or comprehensibility to\nranking [5, 56].\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2015 17:40:19 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Petersen", "Casper", ""], ["Lioma", "Christina", ""], ["Simonsen", "Jakob Grue", ""], ["Larsen", "Birger", ""]]}, {"id": "1507.08396", "submitter": "Shuangyin Li", "authors": "Shuangyin Li, Jiefei Li, Guan Huang, Ruiyang Tan, and Rong Pan", "title": "Tag-Weighted Topic Model For Large-scale Semi-Structured Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, there have been massive Semi-Structured Documents (SSDs) during the\nevolution of the Internet. These SSDs contain both unstructured features (e.g.,\nplain text) and metadata (e.g., tags). Most previous works focused on modeling\nthe unstructured text, and recently, some other methods have been proposed to\nmodel the unstructured text with specific tags. To build a general model for\nSSDs remains an important problem in terms of both model fitness and\nefficiency. We propose a novel method to model the SSDs by a so-called\nTag-Weighted Topic Model (TWTM). TWTM is a framework that leverages both the\ntags and words information, not only to learn the document-topic and topic-word\ndistributions, but also to infer the tag-topic distributions for text mining\ntasks. We present an efficient variational inference method with an EM\nalgorithm for estimating the model parameters. Meanwhile, we propose three\nlarge-scale solutions for our model under the MapReduce distributed computing\nplatform for modeling large-scale SSDs. The experimental results show the\neffectiveness, efficiency and the robustness by comparing our model with the\nstate-of-the-art methods in document modeling, tags prediction and text\nclassification. We also show the performance of the three distributed solutions\nin terms of time and accuracy on document modeling.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 06:44:37 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Li", "Shuangyin", ""], ["Li", "Jiefei", ""], ["Huang", "Guan", ""], ["Tan", "Ruiyang", ""], ["Pan", "Rong", ""]]}, {"id": "1507.08439", "submitter": "Maciej Kula", "authors": "Maciej Kula (Lyst.com)", "title": "Metadata Embeddings for User and Item Cold-start Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present a hybrid matrix factorisation model representing users and items as\nlinear combinations of their content features' latent factors. The model\noutperforms both collaborative and content-based models in cold-start or sparse\ninteraction data scenarios (using both user and item metadata), and performs at\nleast as well as a pure collaborative matrix factorisation model where\ninteraction data is abundant. Additionally, feature embeddings produced by the\nmodel encode semantic information in a way reminiscent of word embedding\napproaches, making them useful for a range of related tasks such as tag\nrecommendations.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 10:08:14 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Kula", "Maciej", "", "Lyst.com"]]}, {"id": "1507.08586", "submitter": "Yanshan Wang", "authors": "Yanshan Wang, In-Chan Choi, Hongfang Liu", "title": "Generalized Ensemble Model for Document Ranking in Information Retrieval", "comments": null, "journal-ref": null, "doi": "10.2298/CSIS160229042W", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalized ensemble model (gEnM) for document ranking is proposed in this\npaper. The gEnM linearly combines basis document retrieval models and tries to\nretrieve relevant documents at high positions. In order to obtain the optimal\nlinear combination of multiple document retrieval models or rankers, an\noptimization program is formulated by directly maximizing the mean average\nprecision. Both supervised and unsupervised learning algorithms are presented\nto solve this program. For the supervised scheme, two approaches are considered\nbased on the data setting, namely batch and online setting. In the batch\nsetting, we propose a revised Newton's algorithm, gEnM.BAT, by approximating\nthe derivative and Hessian matrix. In the online setting, we advocate a\nstochastic gradient descent (SGD) based algorithm---gEnM.ON. As for the\nunsupervised scheme, an unsupervised ensemble model (UnsEnM) by iteratively\nco-learning from each constituent ranker is presented. Experimental study on\nbenchmark data sets verifies the effectiveness of the proposed algorithms.\nTherefore, with appropriate algorithms, the gEnM is a viable option in diverse\npractical information retrieval applications.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2015 17:09:28 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 15:34:53 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2017 20:54:43 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Wang", "Yanshan", ""], ["Choi", "In-Chan", ""], ["Liu", "Hongfang", ""]]}]