[{"id": "1206.0104", "submitter": "Dian Pratiwi", "authors": "Dian Pratiwi", "title": "The Use of Self Organizing Map Method and Feature Selection in Image\n  Database Classification System", "comments": "5 pages, 5 figures, 2 tables. citation in IJCSI volume 9 issue 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a technique in classifying the images into a number of\nclasses or clusters desired by means of Self Organizing Map (SOM) Artificial\nNeural Network method. A number of 250 color images to be classified as\npreviously done some processing, such as RGB to grayscale color conversion,\ncolor histogram, feature vector selection, and then classifying by the SOM\nFeature vector selection in this paper will use two methods, namely by PCA\n(Principal Component Analysis) and LSA (Latent Semantic Analysis) in which each\nof these methods would have taken the characteristic vector of 50, 100, and 150\nfrom 256 initial feature vector into the process of color histogram. Then the\nselection will be processed into the SOM network to be classified into five\nclasses using a learning rate of 0.5 and calculated accuracy. Classification of\nsome of the test results showed that the highest percentage of accuracy\nobtained when using PCA and the selection of 100 feature vector that is equal\nto 88%, compared to when using LSA selection that only 74%. Thus it can be\nconcluded that the method fits the PCA feature selection methods are applied in\nconjunction with SOM and has an accuracy rate better than the LSA feature\nselection methods. Keywords: Color Histogram, Feature Selection, LSA, PCA, SOM.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2012 07:16:18 GMT"}], "update_date": "2012-06-04", "authors_parsed": [["Pratiwi", "Dian", ""]]}, {"id": "1206.0335", "submitter": "Nima Hatami", "authors": "Nima Hatami, Camelia Chira and Giuliano Armano", "title": "A Route Confidence Evaluation Method for Reliable Hierarchical Text\n  Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Text Categorization (HTC) is becoming increasingly important\nwith the rapidly growing amount of text data available in the World Wide Web.\nAmong the different strategies proposed to cope with HTC, the Local Classifier\nper Node (LCN) approach attains good performance by mirroring the underlying\nclass hierarchy while enforcing a top-down strategy in the testing step.\nHowever, the problem of embedding hierarchical information (parent-child\nrelationship) to improve the performance of HTC systems still remains open. A\nconfidence evaluation method for a selected route in the hierarchy is proposed\nto evaluate the reliability of the final candidate labels in an HTC system. In\norder to take into account the information embedded in the hierarchy, weight\nfactors are used to take into account the importance of each level. An\nacceptance/rejection strategy in the top-down decision making process is\nproposed, which improves the overall categorization accuracy by rejecting a few\npercentage of samples, i.e., those with low reliability score. Experimental\nresults on the Reuters benchmark dataset (RCV1- v2) confirm the effectiveness\nof the proposed method, compared to other state-of-the art HTC methods.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2012 01:37:22 GMT"}], "update_date": "2012-06-05", "authors_parsed": [["Hatami", "Nima", ""], ["Chira", "Camelia", ""], ["Armano", "Giuliano", ""]]}, {"id": "1206.0905", "submitter": "Omri Mohamed Nazih", "authors": "Radhouane Boughamoura, Mohamed Nazih Omri and Habib Youssef", "title": "A Fuzzy Approach for Pertinent Information Extraction from Web Resources", "comments": "International Journal of Computational Science - 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in machine learning for information extraction has focused on two\ndistinct sub-problems: the conventional problem of filling template slots from\nnatural language text, and the problem of wrapper induction, learning simple\nextraction procedures (\"wrappers\") for highly structured text such as Web\npages. For suitable regular domains, existing wrapper induction algorithms can\nefficiently learn wrappers that are simple and highly accurate, but the\nregularity bias of these algorithms makes them unsuitable for most conventional\ninformation extraction tasks. This paper describes a new approach for wrapping\nsemistructured Web pages. The wrapper is capable of learning how to extract\nrelevant information from Web resources on the basis of user supplied examples.\nIt is based on inductive learning techniques as well as fuzzy logic rules.\nExperimental results show that our approach achieves noticeably better\nprecision and recall coefficient performance measures than SoftMealy, which is\none of the most recently reported wrappers capable of wrapping semi-structured\nWeb pages with missing attributes, multiple attributes, variant attribute\npermutations, exceptions, and typos.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2012 12:36:09 GMT"}], "update_date": "2012-06-06", "authors_parsed": [["Boughamoura", "Radhouane", ""], ["Omri", "Mohamed Nazih", ""], ["Youssef", "Habib", ""]]}, {"id": "1206.0925", "submitter": "Omri Mohamed Nazih", "authors": "Mohamed Nazih Omri", "title": "Possibilistic Pertinence Feedback and Semantic Networks for Goal's\n  Extraction", "comments": null, "journal-ref": "Asian Journal of Information Technology (4):258-265 - 2004", "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pertinence Feedback is a technique that enables a user to interactively\nexpress his information requirement by modifying his original query formulation\nwith further information. This information is provided by explicitly confirming\nthe pertinent of some indicating objects and/or goals extracted by the system.\nObviously the user cannot mark objects and/or goals as pertinent until some are\nextracted, so the first search has to be initiated by a query and the initial\nquery specification has to be good enough to pick out some pertinent objects\nand/or goals from the Semantic Network. In this paper we present a short survey\nof fuzzy and Semantic approaches to Knowledge Extraction. The goal of such\napproaches is to define flexible Knowledge Extraction Systems able to deal with\nthe inherent vagueness and uncertainty of the Extraction process. It has long\nbeen recognised that interactivity improves the effectiveness of Knowledge\nExtraction systems. Novice user's queries are the most natural and interactive\nmedium of communication and recent progress in recognition is making it\npossible to build systems that interact with the user. However, given the\ntypical novice user's queries submitted to Knowledge Extraction Systems, it is\neasy to imagine that the effects of goal recognition errors in novice user's\nqueries must be severely destructive on the system's effectiveness. The\nexperimental work reported in this paper shows that the use of possibility\ntheory in classical Knowledge Extraction techniques for novice user's query\nprocessing is more robust than the use of the probability theory. Moreover,\nboth possibilistic and probabilistic pertinence feedback can be effectively\nemployed to improve the effectiveness of novice user's query processing.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2012 13:30:37 GMT"}], "update_date": "2012-06-06", "authors_parsed": [["Omri", "Mohamed Nazih", ""]]}, {"id": "1206.0968", "submitter": "Omri Mohamed Nazih", "authors": "Kamel Garrouch, Mohamed Nazih Omri and Bachir Elayeb", "title": "Pertinent Information retrieval based on Possibilistic Bayesian network\n  : origin and possibilistic perspective", "comments": "The International Conference on Computing & e-Systems - 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a synthesis of work performed on tow information\nretrieval models: Bayesian network information retrieval model witch encode\n(in) dependence relation between terms and possibilistic network information\nretrieval model witch make use of necessity and possibility measures to\nrepresent the fuzziness of pertinence measure. It is known that the use of a\ngeneral Bayesian network methodology as the basis for an IR system is difficult\nto tackle. The problem mainly appears because of the large number of variables\ninvolved and the computational efforts needed to both determine the\nrelationships between variables and perform the inference processes. To resolve\nthese problems, many models have been proposed such as BNR model. Generally,\nBayesian network models doesn't consider the fuzziness of natural language in\nthe relevance measure of a document to a given query and possibilistic models\ndoesn't undertake the dependence relations between terms used to index\ndocuments. As a first solution we propose a hybridization of these two models\nin one that will undertake both the relationship between terms and the\nintrinsic fuzziness of natural language. We believe that the translation of\nBayesian network model from the probabilistic framework to possibilistic one\nwill allow a performance improvement of BNRM.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2012 15:45:06 GMT"}], "update_date": "2012-06-06", "authors_parsed": [["Garrouch", "Kamel", ""], ["Omri", "Mohamed Nazih", ""], ["Elayeb", "Bachir", ""]]}, {"id": "1206.0976", "submitter": "Omri Mohamed Nazih", "authors": "Amen Ajroud, Mohamed Nazih Omri, Habib Youssef and Salem Benferhat", "title": "Loopy Belief Propagation in Bayesian Networks : origin and possibilistic\n  perspectives", "comments": "The International Conference on Computing & e-Systems - 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a synthesis of the work performed on two inference\nalgorithms: the Pearl's belief propagation (BP) algorithm applied to Bayesian\nnetworks without loops (i.e. polytree) and the Loopy belief propagation (LBP)\nalgorithm (inspired from the BP) which is applied to networks containing\nundirected cycles. It is known that the BP algorithm, applied to Bayesian\nnetworks with loops, gives incorrect numerical results i.e. incorrect posterior\nprobabilities. Murphy and al. [7] find that the LBP algorithm converges on\nseveral networks and when this occurs, LBP gives a good approximation of the\nexact posterior probabilities. However this algorithm presents an oscillatory\nbehaviour when it is applied to QMR (Quick Medical Reference) network [15].\nThis phenomenon prevents the LBP algorithm from converging towards a good\napproximation of posterior probabilities. We believe that the translation of\nthe inference computation problem from the probabilistic framework to the\npossibilistic framework will allow performance improvement of LBP algorithm. We\nhope that an adaptation of this algorithm to a possibilistic causal network\nwill show an improvement of the convergence of LBP.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2012 16:14:16 GMT"}], "update_date": "2012-06-06", "authors_parsed": [["Ajroud", "Amen", ""], ["Omri", "Mohamed Nazih", ""], ["Youssef", "Habib", ""], ["Benferhat", "Salem", ""]]}, {"id": "1206.1011", "submitter": "Mohamed ElArnaouty", "authors": "Mohamed Elarnaoty, Samir AbdelRahman, and Aly Fahmy", "title": "A Machine Learning Approach For Opinion Holder Extraction In Arabic\n  Language", "comments": null, "journal-ref": "Mohamed Elarnaoty, Samir AbdelRahman and Aly Fahmy. \"A Machine\n  Learning Approach for Opinion Holder Extraction in Arabic Language\",\n  ISSN:0976-2191, vol 3, March 2012", "doi": "10.5121/ijaia.2012.3205", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion mining aims at extracting useful subjective information from reliable\namounts of text. Opinion mining holder recognition is a task that has not been\nconsidered yet in Arabic Language. This task essentially requires deep\nunderstanding of clauses structures. Unfortunately, the lack of a robust,\npublicly available, Arabic parser further complicates the research. This paper\npresents a leading research for the opinion holder extraction in Arabic news\nindependent from any lexical parsers. We investigate constructing a\ncomprehensive feature set to compensate the lack of parsing structural\noutcomes. The proposed feature set is tuned from English previous works coupled\nwith our proposed semantic field and named entities features. Our feature\nanalysis is based on Conditional Random Fields (CRF) and semi-supervised\npattern recognition techniques. Different research models are evaluated via\ncross-validation experiments achieving 54.03 F-measure. We publicly release our\nown research outcome corpus and lexicon for opinion mining community to\nencourage further research.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2012 20:50:59 GMT"}], "update_date": "2012-06-06", "authors_parsed": [["Elarnaoty", "Mohamed", ""], ["AbdelRahman", "Samir", ""], ["Fahmy", "Aly", ""]]}, {"id": "1206.1042", "submitter": "Omri Mohamed Nazih", "authors": "Mohamed Nazih Omri", "title": "Relevance Feedback for Goal's Extraction from Fuzzy Semantic Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1206.0925,\n  arXiv:1206.1615", "journal-ref": "Asian Journal of Information Technology(AJIT). Vol. 3, No. 4,\n  258-265, (2004)", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a short survey of fuzzy and Semantic approaches to\nKnowledge Extraction. The goal of such approaches is to define flexible\nKnowledge Extraction Systems able to deal with the inherent vagueness and\nuncertainty of the Extraction process. It has long been recognised that\ninteractivity improves the effectiveness of Knowledge Extraction systems.\nNovice user's queries is the most natural and interactive medium of\ncommunication and recent progress in recognition is making it possible to build\nsystems that interact with the user. However, given the typical novice user's\nqueries submitted to Knowledge Extraction systems, it is easy to imagine that\nthe effects of goal recognition errors in novice user's queries must be\nseverely destructive on the system's effectiveness. The experimental work\nreported in this paper shows that the use of classical Knowledge Extraction\ntechniques for novice user's query processing is robust to considerably high\nlevels of goal recognition errors. Moreover, both standard relevance feedback\nand pseudo relevance feedback can be effectively employed to improve the\neffectiveness of novice user's query processing.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2012 12:49:44 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2013 16:15:50 GMT"}], "update_date": "2013-01-25", "authors_parsed": [["Omri", "Mohamed Nazih", ""]]}, {"id": "1206.1147", "submitter": "Jia Zeng", "authors": "Jia Zeng, Zhi-Qiang Liu and Xiao-Qin Cao", "title": "Memory-Efficient Topic Modeling", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the simplest probabilistic topic modeling techniques, latent\nDirichlet allocation (LDA) has found many important applications in text\nmining, computer vision and computational biology. Recent training algorithms\nfor LDA can be interpreted within a unified message passing framework. However,\nmessage passing requires storing previous messages with a large amount of\nmemory space, increasing linearly with the number of documents or the number of\ntopics. Therefore, the high memory usage is often a major problem for topic\nmodeling of massive corpora containing a large number of topics. To reduce the\nspace complexity, we propose a novel algorithm without storing previous\nmessages for training LDA: tiny belief propagation (TBP). The basic idea of TBP\nrelates the message passing algorithms with the non-negative matrix\nfactorization (NMF) algorithms, which absorb the message updating into the\nmessage passing process, and thus avoid storing previous messages. Experimental\nresults on four large data sets confirm that TBP performs comparably well or\neven better than current state-of-the-art training algorithms for LDA but with\na much less memory consumption. TBP can do topic modeling when massive corpora\ncannot fit in the computer memory, for example, extracting thematic topics from\n7 GB PUBMED corpora on a common desktop computer with 2GB memory.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2012 08:34:43 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2012 14:07:26 GMT"}], "update_date": "2012-06-11", "authors_parsed": [["Zeng", "Jia", ""], ["Liu", "Zhi-Qiang", ""], ["Cao", "Xiao-Qin", ""]]}, {"id": "1206.1339", "submitter": "Bernhard Haslhofer", "authors": "Christian Mader and Bernhard Haslhofer and Antoine Isaac", "title": "Finding Quality Issues in SKOS Vocabularies", "comments": "12 pages, to be published in TPDL 2012 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Simple Knowledge Organization System (SKOS) is a standard model for\ncontrolled vocabularies on the Web. However, SKOS vocabularies often differ in\nterms of quality, which reduces their applicability across system boundaries.\nHere we investigate how we can support taxonomists in improving SKOS\nvocabularies by pointing out quality issues that go beyond the integrity\nconstraints defined in the SKOS specification. We identified potential\nquantifiable quality issues and formalized them into computable quality\nchecking functions that can find affected resources in a given SKOS vocabulary.\nWe implemented these functions in the qSKOS quality assessment tool, analyzed\n15 existing vocabularies, and found possible quality issues in all of them.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2012 20:12:28 GMT"}], "update_date": "2012-06-08", "authors_parsed": [["Mader", "Christian", ""], ["Haslhofer", "Bernhard", ""], ["Isaac", "Antoine", ""]]}, {"id": "1206.1492", "submitter": "Georg Singer", "authors": "Georg Singer, Ulrich Norbisrath, Dirk Lewandowski", "title": "Ordinary Search Engine Users Carrying Out Complex Search Tasks", "comments": "60 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web search engines have become the dominant tools for finding information on\nthe Internet. Due to their popularity, users apply them to a wide range of\nsearch needs, from simple look-ups to rather complex information tasks. This\npaper presents the results of a study to investigate the characteristics of\nthese complex information needs in the context of Web search engines. The aim\nof the study is to find out more about (1) what makes complex search tasks\ndistinct from simple tasks and if it is possible to find simple measures for\ndescribing their complexity, (2) if search success for a task can be predicted\nby means of unique measures, and (3) if successful searchers show a different\nbehavior than unsuccessful ones. The study includes 60 people who carried out a\nset of 12 search tasks with current commercial search engines. Their behavior\nwas logged with the Search-Logger tool. The results confirm that complex tasks\nshow significantly different characteristics than simple tasks. Yet it seems to\nbe difficult to distinguish successful from unsuccessful search behaviors. Good\nsearchers can be differentiated from bad searchers by means of measurable\nparameters. The implications of these findings for search engine vendors are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2012 13:45:05 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2012 10:30:36 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2012 13:29:45 GMT"}], "update_date": "2012-07-05", "authors_parsed": [["Singer", "Georg", ""], ["Norbisrath", "Ulrich", ""], ["Lewandowski", "Dirk", ""]]}, {"id": "1206.1494", "submitter": "Georg Singer", "authors": "Georg Singer, Ulrich Norbisrath, Dirk Lewandowski", "title": "Impact of Gender and Age on performing Search Tasks Online", "comments": "10 pages", "journal-ref": null, "doi": "10.1524/9783486718782.23", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More and more people use the Internet to work on duties of their daily work\nroutine. To find the right information online, Web search engines are the tools\nof their choice. Apart from finding facts, people use Web search engines to\nalso execute rather complex and time consuming search tasks. So far search\nengines follow the one-for-all approach to serve its users and little is known\nabout the impact of gender and age on people's Web search behavior. In this\narticle we present a study that examines (1) how female and male web users\ncarry out simple and complex search tasks and what are the differences between\nthe two user groups, and (2) how the age of the users impacts their search\nperformance. The laboratory study was done with 56 ordinary people each\ncarrying out 12 search tasks. Our findings confirm that age impacts behavior\nand search performance significantly, while gender influences were smaller than\nexpected.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2012 13:49:41 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["Singer", "Georg", ""], ["Norbisrath", "Ulrich", ""], ["Lewandowski", "Dirk", ""]]}, {"id": "1206.1615", "submitter": "Omri Mohamed Nazih", "authors": "Mohamed Nazih Omri", "title": "Objects and Goals Extraction from Semantic Networks : Applications of\n  Fuzzy SetS Theory", "comments": "arXiv admin note: text overlap with arXiv:1206.1042, arXiv:1206.0925", "journal-ref": "Conf\\'erence Internationale : Science \\'Electroniques,\n  Technologies de l'Information et des T\\'el\\'ecommunications(SETIT), p.\n  275-282, Mahdia, Tunisie, 2003", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a short survey of fuzzy and Semantic approaches to\nKnowledge Extraction. The goal of such approaches is to define flexible\nKnowledge Extraction Systems able to deal with the inherent vagueness and\nuncertainty of the Extraction process. In this survey we address if and how\nsome approaches met their goal.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2012 21:04:15 GMT"}], "update_date": "2012-06-11", "authors_parsed": [["Omri", "Mohamed Nazih", ""]]}, {"id": "1206.1624", "submitter": "Omri Mohamed Nazih", "authors": "Mohamed nazih Omri and Noureddine Chouigui", "title": "Measure of Similarity between Fuzzy Concepts for Optimization of Fuzzy\n  Semantic Nets", "comments": "14th International Conference on Systems Science. Wroclaw, Poland,\n  2001", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method to measure the similarity between different\nfuzzy concepts in order to optimize Semantic networks. The problem approached\nis the minimization of the time of research and identification of user's\nObjects and Goals. Indeed, it concerns to determine to each instant the\ntotality of Objects (respectively Goals) among which one can identify rapidly\nthe most satisfactory for the user's Object and Goal. Alone Objects and most\nsimilar Goals to Objects and researched Goals of the viewpoint of attribute\nvalues will be processed, what will avoid the analysis of all Objects and\nsystem Goals far of needs of the user.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2012 21:36:08 GMT"}], "update_date": "2012-06-11", "authors_parsed": [["Omri", "Mohamed nazih", ""], ["Chouigui", "Noureddine", ""]]}, {"id": "1206.1754", "submitter": "Shuai Yuan", "authors": "Shuai Yuan, Ahmad Zainal Abidin, Marc Sloan, Jun Wang", "title": "Internet Advertising: An Interplay among Advertisers, Online Publishers,\n  Ad Exchanges and Web Users", "comments": "44 pages, 7 figures, 6 tables. Submitted to Information Processing\n  and Management", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet advertising is a fast growing business which has proved to be\nsignificantly important in digital economics. It is vitally important for both\nweb search engines and online content providers and publishers because web\nadvertising provides them with major sources of revenue. Its presence is\nincreasingly important for the whole media industry due to the influence of the\nWeb. For advertisers, it is a smarter alternative to traditional marketing\nmedia such as TVs and newspapers. As the web evolves and data collection\ncontinues, the design of methods for more targeted, interactive, and friendly\nadvertising may have a major impact on the way our digital economy evolves, and\nto aid societal development.\n  Towards this goal mathematically well-grounded Computational Advertising\nmethods are becoming necessary and will continue to develop as a fundamental\ntool towards the Web. As a vibrant new discipline, Internet advertising\nrequires effort from different research domains including Information\nRetrieval, Machine Learning, Data Mining and Analytic, Statistics, Economics,\nand even Psychology to predict and understand user behaviours. In this paper,\nwe provide a comprehensive survey on Internet advertising, discussing and\nclassifying the research issues, identifying the recent technologies, and\nsuggesting its future directions. To have a comprehensive picture, we first\nstart with a brief history, introduction, and classification of the industry\nand present a schematic view of the new advertising ecosystem. We then\nintroduce four major participants, namely advertisers, online publishers, ad\nexchanges and web users; and through analysing and discussing the major\nresearch problems and existing solutions from their perspectives respectively,\nwe discover and aggregate the fundamental problems that characterise the\nnewly-formed research field and capture its potential future prospects.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2012 13:28:30 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2012 21:42:57 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Yuan", "Shuai", ""], ["Abidin", "Ahmad Zainal", ""], ["Sloan", "Marc", ""], ["Wang", "Jun", ""]]}, {"id": "1206.1852", "submitter": "Omri Mohamed Nazih", "authors": "Mohamed Nazih Omri", "title": "Optimization of Fuzzy Semantic Networks Based on Galois Lattice and\n  Bayesian Formalism", "comments": "arXiv admin note: text overlap with arXiv:1206.1794", "journal-ref": "Eighth international Conference on Information Processing and\n  Management of Uncertainty in Knowledge-Based Systems. P. 1844-1850. Madrid,\n  Espagne, 2000", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method of optimization, based on both Bayesian Analysis\ntechnical and Galois Lattice of Fuzzy Semantic Network. The technical System we\nuse learns by interpreting an unknown word using the links created between this\nnew word and known words. The main link is provided by the context of the\nquery. When novice's query is confused with an unknown verb (goal) applied to a\nknown noun denoting either an object in the ideal user's Network or an object\nin the user's Network, the system infer that this new verb corresponds to one\nof the known goal. With the learning of new words in natural language as the\ninterpretation, which was produced in agreement with the user, the system\nimproves its representation scheme at each experiment with a new user and, in\naddition, takes advantage of previous discussions with users. The semantic Net\nof user objects thus obtained by learning is not always optimal because some\nrelationships between couple of user objects can be generalized and others\nsuppressed according to values of forces that characterize them. Indeed, to\nsimplify the obtained Net, we propose to proceed to an Inductive Bayesian\nAnalysis, on the Net obtained from Galois lattice. The objective of this\nanalysis can be seen as an operation of filtering of the obtained descriptive\ngraph.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2012 21:53:36 GMT"}], "update_date": "2012-06-12", "authors_parsed": [["Omri", "Mohamed Nazih", ""]]}, {"id": "1206.2010", "submitter": "Michele Filannino", "authors": "Michele Filannino", "title": "Temporal expression normalisation in natural language texts", "comments": "7 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Automatic annotation of temporal expressions is a research challenge of great\ninterest in the field of information extraction. In this report, I describe a\nnovel rule-based architecture, built on top of a pre-existing system, which is\nable to normalise temporal expressions detected in English texts. Gold standard\ntemporally-annotated resources are limited in size and this makes research\ndifficult. The proposed system outperforms the state-of-the-art systems with\nrespect to TempEval-2 Shared Task (value attribute) and achieves substantially\nbetter results with respect to the pre-existing system on top of which it has\nbeen developed. I will also introduce a new free corpus consisting of 2822\nunique annotated temporal expressions. Both the corpus and the system are\nfreely available on-line.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2012 09:32:07 GMT"}], "update_date": "2012-06-12", "authors_parsed": [["Filannino", "Michele", ""]]}, {"id": "1206.2123", "submitter": "Philipp Schaer", "authors": "Philipp Schaer, Philipp Mayr, Thomas L\\\"uke", "title": "Extending Term Suggestion with Author Names", "comments": "6 pages; to be published in Proceedings of Theory and Practice of\n  Digital Libraries 2012 (TPDL 2012)", "journal-ref": null, "doi": "10.1007/978-3-642-33290-6_34", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Term suggestion or recommendation modules can help users to formulate their\nqueries by mapping their personal vocabularies onto the specialized vocabulary\nof a digital library. While we examined actual user queries of the social\nsciences digital library Sowiport we could see that nearly one third of the\nusers were explicitly looking for author names rather than terms. Common term\nrecommenders neglect this fact. By picking up the idea of polyrepresentation we\ncould show that in a standardized IR evaluation setting we can significantly\nincrease the retrieval performances by adding topical-related author names to\nthe query. This positive effect only appears when the query is additionally\nexpanded with thesaurus terms. By just adding the author names to a query we\noften observe a query drift which results in worse results.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2012 08:29:36 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Schaer", "Philipp", ""], ["Mayr", "Philipp", ""], ["L\u00fcke", "Thomas", ""]]}, {"id": "1206.2126", "submitter": "Philipp Schaer", "authors": "Thomas L\\\"uke, Philipp Schaer, Philipp Mayr", "title": "Improving Retrieval Results with discipline-specific Query Expansion", "comments": "6 pages; to be published in Proceedings of Theory and Practice of\n  Digital Libraries 2012 (TPDL 2012)", "journal-ref": null, "doi": "10.1007/978-3-642-33290-6_44", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing the right terms to describe an information need is becoming more\ndifficult as the amount of available information increases.\nSearch-Term-Recommendation (STR) systems can help to overcome these problems.\nThis paper evaluates the benefits that may be gained from the use of STRs in\nQuery Expansion (QE). We create 17 STRs, 16 based on specific disciplines and\none giving general recommendations, and compare the retrieval performance of\nthese STRs. The main findings are: (1) QE with specific STRs leads to\nsignificantly better results than QE with a general STR, (2) QE with specific\nSTRs selected by a heuristic mechanism of topic classification leads to better\nresults than the general STR, however (3) selecting the best matching specific\nSTR in an automatic way is a major challenge of this process.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2012 08:32:41 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["L\u00fcke", "Thomas", ""], ["Schaer", "Philipp", ""], ["Mayr", "Philipp", ""]]}, {"id": "1206.2465", "submitter": "Georg Singer", "authors": "Kristiina Singer, Georg Singer, Krista Lepik, Ulrich Norbisrath, and\n  Pille Pruulmann-Vengerfeldt", "title": "Search Strategies of Library Search Experts", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines like Google, Yahoo or Bing are an excellent support for\nfinding documents, but this strength also imposes a limitation. As they are\noptimized for document retrieval tasks, they perform less well when it comes to\nmore complex search needs. Complex search tasks are usually described as\nopen-ended, abstract and poorly defined information needs with a multifaceted\ncharacter. In this paper we will present the results of an experiment carried\nout with information professionals from libraries and museums in the course of\na search contest. The aim of the experiment was to analyze the search\nstrategies of experienced information workers trying to tackle search tasks of\nvarying complexity and get qualitative results on the impact of time pressure\non such an experiment.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2012 08:46:11 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2012 10:02:23 GMT"}], "update_date": "2012-06-27", "authors_parsed": [["Singer", "Kristiina", ""], ["Singer", "Georg", ""], ["Lepik", "Krista", ""], ["Norbisrath", "Ulrich", ""], ["Pruulmann-Vengerfeldt", "Pille", ""]]}, {"id": "1206.2484", "submitter": "Puneet  Singh", "authors": "Puneet Singh, Ashutosh Kapoor, Vishal Kaushik and Hima Bindu\n  Maringanti", "title": "Architecture for Automated Tagging and Clustering of Song Files\n  According to Mood", "comments": "7 pages", "journal-ref": "IJCSI Volume 7, Issue 4, No 2, pp 11-17, July 2010", "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music is one of the basic human needs for recreation and entertainment. As\nsong files are digitalized now a days, and digital libraries are expanding\ncontinuously, which makes it difficult to recall a song. Thus need of a new\nclassification system other than genre is very obvious and mood based\nclassification system serves the purpose very well. In this paper we will\npresent a well-defined architecture to classify songs into different mood-based\ncategories, using audio content analysis, affective value of song lyrics to map\na song onto a psychological-based emotion space and information from online\nsources. In audio content analysis we will use music features such as\nintensity, timbre and rhythm including their subfeatures to map music in a\n2-Dimensional emotional space. In lyric based classification 1-Dimensional\nemotional space is used. Both the results are merged onto a 2-Dimensional\nemotional space, which will classify song into a particular mood category.\nFinally clusters of mood based song files are formed and arranged according to\ndata acquired from various Internet sources.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2012 10:28:11 GMT"}], "update_date": "2012-06-13", "authors_parsed": [["Singh", "Puneet", ""], ["Kapoor", "Ashutosh", ""], ["Kaushik", "Vishal", ""], ["Maringanti", "Hima Bindu", ""]]}, {"id": "1206.2510", "submitter": "David Novak", "authors": "David Novak, Petr Volny, Pavel Zezula", "title": "Generic Subsequence Matching Framework: Modularity, Flexibility,\n  Efficiency", "comments": "This is an extended version of a paper published on DEXA 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subsequence matching has appeared to be an ideal approach for solving many\nproblems related to the fields of data mining and similarity retrieval. It has\nbeen shown that almost any data class (audio, image, biometrics, signals) is or\ncan be represented by some kind of time series or string of symbols, which can\nbe seen as an input for various subsequence matching approaches. The variety of\ndata types, specific tasks and their partial or full solutions is so wide that\nthe choice, implementation and parametrization of a suitable solution for a\ngiven task might be complicated and time-consuming; a possibly fruitful\ncombination of fragments from different research areas may not be obvious nor\neasy to realize. The leading authors of this field also mention the\nimplementation bias that makes difficult a proper comparison of competing\napproaches. Therefore we present a new generic Subsequence Matching Framework\n(SMF) that tries to overcome the aforementioned problems by a uniform frame\nthat simplifies and speeds up the design, development and evaluation of\nsubsequence matching related systems. We identify several relatively separate\nsubtasks solved differently over the literature and SMF enables to combine them\nin straightforward manner achieving new quality and efficiency. This framework\ncan be used in many application domains and its components can be reused\neffectively. Its strictly modular architecture and openness enables also\ninvolvement of efficient solutions from different fields, for instance\nefficient metric-based indexes. This is an extended version of a paper\npublished on DEXA 2012.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2012 12:39:04 GMT"}], "update_date": "2012-06-13", "authors_parsed": [["Novak", "David", ""], ["Volny", "Petr", ""], ["Zezula", "Pavel", ""]]}, {"id": "1206.2523", "submitter": "Gabriele Fici", "authors": "Golnaz Badkobeh, Gabriele Fici, Steve Kroon, Zsuzsanna Lipt\\'ak", "title": "Binary Jumbled String Matching for Highly Run-Length Compressible Texts", "comments": "v2: only small cosmetic changes; v3: new title, weakened conjectures\n  on size of Corner Index (we no longer conjecture it to be always linear in\n  size of RLE); removed experimental part on random strings (these are valid\n  but limited in their predictive power w.r.t. general strings); v3 published\n  in IPL", "journal-ref": "Information Processing Letters, 113: 604-608 (2013)", "doi": "10.1016/j.ipl.2013.05.007", "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Binary Jumbled String Matching problem is defined as: Given a string $s$\nover $\\{a,b\\}$ of length $n$ and a query $(x,y)$, with $x,y$ non-negative\nintegers, decide whether $s$ has a substring $t$ with exactly $x$ $a$'s and $y$\n$b$'s. Previous solutions created an index of size O(n) in a pre-processing\nstep, which was then used to answer queries in constant time. The fastest\nalgorithms for construction of this index have running time $O(n^2/\\log n)$\n[Burcsi et al., FUN 2010; Moosa and Rahman, IPL 2010], or $O(n^2/\\log^2 n)$ in\nthe word-RAM model [Moosa and Rahman, JDA 2012]. We propose an index\nconstructed directly from the run-length encoding of $s$. The construction time\nof our index is $O(n+\\rho^2\\log \\rho)$, where O(n) is the time for computing\nthe run-length encoding of $s$ and $\\rho$ is the length of this encoding---this\nis no worse than previous solutions if $\\rho = O(n/\\log n)$ and better if $\\rho\n= o(n/\\log n)$. Our index $L$ can be queried in $O(\\log \\rho)$ time. While\n$|L|= O(\\min(n, \\rho^{2}))$ in the worst case, preliminary investigations have\nindicated that $|L|$ may often be close to $\\rho$. Furthermore, the algorithm\nfor constructing the index is conceptually simple and easy to implement. In an\nattempt to shed light on the structure and size of our index, we characterize\nit in terms of the prefix normal forms of $s$ introduced in [Fici and Lipt\\'ak,\nDLT 2011].\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2012 13:33:32 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2012 21:38:40 GMT"}, {"version": "v3", "created": "Fri, 31 May 2013 17:32:12 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Badkobeh", "Golnaz", ""], ["Fici", "Gabriele", ""], ["Kroon", "Steve", ""], ["Lipt\u00e1k", "Zsuzsanna", ""]]}, {"id": "1206.2528", "submitter": "Georg Singer", "authors": "Georg Singer, Ulrich Norbisrath, Dirk Lewandowski", "title": "Ordinary Search Engine Users assessing Difficulty, Effort, and Outcome\n  for Simple and Complex Search Tasks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines are the preferred tools for finding information on the Web.\nThey are advancing to be the common helpers to answer any of our search needs.\nWe use them to carry out simple look-up tasks and also to work on rather time\nconsuming and more complex search tasks. Yet, we do not know very much about\nthe user performance while carrying out those tasks -- especially not for\nordinary users. The aim of this study was to get more insight into whether Web\nusers manage to assess difficulty, time effort, query effort, and task outcome\nof search tasks, and if their judging performance relates to task complexity.\nOur study was conducted with a systematically selected sample of 56 people with\na wide demographic background. They carried out a set of 12 search tasks with\ncommercial Web search engines in a laboratory environment. The results confirm\nthat it is hard for normal Web users to judge the difficulty and effort to\ncarry out complex search tasks. The judgments are more reliable for simple\ntasks than for complex ones. Task complexity is an indicator for judging\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2012 13:43:08 GMT"}], "update_date": "2012-06-13", "authors_parsed": [["Singer", "Georg", ""], ["Norbisrath", "Ulrich", ""], ["Lewandowski", "Dirk", ""]]}, {"id": "1206.3078", "submitter": "Saurabh  Pal", "authors": "Saurabh Pal", "title": "Mining Educational Data Using Classification to Decrease Dropout Rate of\n  Students", "comments": "5 pages. arXiv admin note: substantial text overlap with\n  arXiv:1203.2987, arXiv:1203.3832, arXiv:1202.4815, arXiv:1201.3418,\n  arXiv:1201.3417, and with arXiv:1002.1144 by other authors", "journal-ref": "International journal of multidisciplinary sciences and\n  engineering, vol. 3, no. 5, May 2012, 35-39", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades, number of Higher Education Institutions (HEI) grows\nrapidly in India. Since most of the institutions are opened in private mode\ntherefore, a cut throat competition rises among these institutions while\nattracting the student to got admission. This is the reason for institutions to\nfocus on the strength of students not on the quality of education. This paper\npresents a data mining application to generate predictive models for\nengineering student's dropout management. Given new records of incoming\nstudents, the predictive model can produce short accurate prediction list\nidentifying students who tend to need the support from the student dropout\nprogram most. The results show that the machine learning algorithm is able to\nestablish effective predictive model from the existing student dropout data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2012 12:10:23 GMT"}], "update_date": "2012-06-15", "authors_parsed": [["Pal", "Saurabh", ""]]}, {"id": "1206.3254", "submitter": "Amit Gruber", "authors": "Amit Gruber, Michal Rosen-Zvi, Yair Weiss", "title": "Latent Topic Models for Hypertext", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-230-239", "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent topic models have been successfully applied as an unsupervised topic\ndiscovery technique in large document collections. With the proliferation of\nhypertext document collection such as the Internet, there has also been great\ninterest in extending these approaches to hypertext [6, 9]. These approaches\ntypically model links in an analogous fashion to how they model words - the\ndocument-link co-occurrence matrix is modeled in the same way that the\ndocument-word co-occurrence matrix is modeled in standard topic models. In this\npaper we present a probabilistic generative model for hypertext document\ncollections that explicitly models the generation of links. Specifically, links\nfrom a word w to a document d depend directly on how frequent the topic of w is\nin d, in addition to the in-degree of d. We show how to perform EM learning on\nthis model efficiently. By not modeling links as analogous to words, we end up\nusing far fewer free parameters and obtain better link prediction results.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:30:14 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Gruber", "Amit", ""], ["Rosen-Zvi", "Michal", ""], ["Weiss", "Yair", ""]]}, {"id": "1206.3278", "submitter": "David Mimno", "authors": "David Mimno, Andrew McCallum", "title": "Topic Models Conditioned on Arbitrary Features with\n  Dirichlet-multinomial Regression", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-411-418", "categories": "cs.IR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although fully generative models have been successfully used to model the\ncontents of text documents, they are often awkward to apply to combinations of\ntext data and document metadata. In this paper we propose a\nDirichlet-multinomial regression (DMR) topic model that includes a log-linear\nprior on document-topic distributions that is a function of observed features\nof the document, such as author, publication venue, references, and dates. We\nshow that by selecting appropriate features, DMR topic models can meet or\nexceed the performance of several previously published topic models designed\nfor specific data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:42:17 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Mimno", "David", ""], ["McCallum", "Andrew", ""]]}, {"id": "1206.3298", "submitter": "Chong Wang", "authors": "Chong Wang, David Blei, David Heckerman", "title": "Continuous Time Dynamic Topic Models", "comments": "Appears in Proceedings of the Twenty-Fourth Conference on Uncertainty\n  in Artificial Intelligence (UAI2008)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2008-PG-579-586", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop the continuous time dynamic topic model (cDTM). The\ncDTM is a dynamic topic model that uses Brownian motion to model the latent\ntopics through a sequential collection of documents, where a \"topic\" is a\npattern of word use that we expect to evolve over the course of the collection.\nWe derive an efficient variational approximate inference algorithm that takes\nadvantage of the sparsity of observations in text, a property that lets us\neasily handle many time points. In contrast to the cDTM, the original\ndiscrete-time dynamic topic model (dDTM) requires that time be discretized.\nMoreover, the complexity of variational inference for the dDTM grows quickly as\ntime granularity increases, a drawback which limits fine-grained\ndiscretization. We demonstrate the cDTM on two news corpora, reporting both\npredictive perplexity and the novel task of time stamp prediction.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2012 15:56:33 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 22:57:04 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Wang", "Chong", ""], ["Blei", "David", ""], ["Heckerman", "David", ""]]}, {"id": "1206.3320", "submitter": "Zi-Ke Zhang Mr.", "authors": "Jinhu Liu, Chengcheng Yang, Zi-Ke Zhang", "title": "A two-step Recommendation Algorithm via Iterative Local Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems can change our life a lot and help us select suitable and\nfavorite items much more conveniently and easily. As a consequence, various\nkinds of algorithms have been proposed in last few years to improve the\nperformance. However, all of them face one critical problem: data sparsity. In\nthis paper, we proposed a two-step recommendation algorithm via iterative local\nleast squares (ILLS). Firstly, we obtain the ratings matrix which is\nconstructed via users' behavioral records, and it is normally very sparse.\nSecondly, we preprocess the \"ratings\" matrix through ProbS which can convert\nthe sparse data to a dense one. Then we use ILLS to estimate those missing\nvalues. Finally, the recommendation list is generated. Experimental results on\nthe three datasets: MovieLens, Netflix, RYM, suggest that the proposed method\ncan enhance the algorithmic accuracy of AUC. Especially, it performs much\nbetter in dense datasets. Furthermore, since this methods can improve those\nmissing value more accurately via iteration which might show light in\ndiscovering those inactive users' purchasing intention and eventually solving\ncold-start problem.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2012 20:23:24 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Liu", "Jinhu", ""], ["Yang", "Chengcheng", ""], ["Zhang", "Zi-Ke", ""]]}, {"id": "1206.3667", "submitter": "Sudhir Ahuja", "authors": "Sudhir Ahuja, Mr. Rinkaj Goyal", "title": "Information Retrieval in Intelligent Systems: Current Scenario & Issues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web space is the huge repository of data. Everyday lots of new information\nget added to this web space. The more the information, more is demand for tools\nto access that information. Answering users' queries about the online\ninformation intelligently is one of the great challenges in information\nretrieval in intelligent systems. In this paper, we will start with the brief\nintroduction on information retrieval and intelligent systems and explain how\nswoogle, the semantic search engine, uses its algorithms and techniques to\nsearch for the desired contents in the web. We then continue with the\nclustering technique that is used to group the similar things together and\ndiscuss the machine learning technique called Self-organizing maps [6] or SOM,\nwhich is a data visualization technique that reduces the dimensions of data\nthrough the use of self-organizing neural networks. We then discuss how SOM is\nused to visualize the contents of the data, by following some lines of\nalgorithm, in the form of maps. So, we could say that websites or machines can\nbe used to retrieve the information that what exactly users want from them.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2012 13:46:33 GMT"}], "update_date": "2012-06-19", "authors_parsed": [["Ahuja", "Sudhir", ""], ["Goyal", "Mr. Rinkaj", ""]]}, {"id": "1206.4110", "submitter": "Duc Son Pham", "authors": "Truyen T. Tran and Duc Son Pham", "title": "ConeRANK: Ranking as Learning Generalized Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We propose a new data mining approach in ranking documents based on the\nconcept of cone-based generalized inequalities between vectors. A partial\nordering between two vectors is made with respect to a proper cone and thus\nlearning the preferences is formulated as learning proper cones. A pairwise\nlearning-to-rank algorithm (ConeRank) is proposed to learn a non-negative\nsubspace, formulated as a polyhedral cone, over document-pair differences. The\nalgorithm is regularized by controlling the `volume' of the cone. The\nexperimental studies on the latest and largest ranking dataset LETOR 4.0 shows\nthat ConeRank is competitive against other recent ranking approaches.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2012 02:24:55 GMT"}], "update_date": "2012-06-21", "authors_parsed": [["Tran", "Truyen T.", ""], ["Pham", "Duc Son", ""]]}, {"id": "1206.4300", "submitter": "Sebastiano Vigna", "authors": "Sebastiano Vigna", "title": "Quasi-Succinct Indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed inverted indices in use today are based on the idea of gap\ncompression: documents pointers are stored in increasing order, and the gaps\nbetween successive document pointers are stored using suitable codes which\nrepresent smaller gaps using less bits. Additional data such as counts and\npositions is stored using similar techniques. A large body of research has been\nbuilt in the last 30 years around gap compression, including theoretical\nmodeling of the gap distribution, specialized instantaneous codes suitable for\ngap encoding, and ad hoc document reorderings which increase the efficiency of\ninstantaneous codes. This paper proposes to represent an index using a\ndifferent architecture based on quasi-succinct representation of monotone\nsequences. We show that, besides being theoretically elegant and simple, the\nnew index provides expected constant-time operations and, in practice,\nsignificant performance improvements on conjunctive, phrasal and proximity\nqueries.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2012 19:47:12 GMT"}], "update_date": "2012-06-20", "authors_parsed": [["Vigna", "Sebastiano", ""]]}, {"id": "1206.4603", "submitter": "Jason Weston", "authors": "Jason Weston (Google), Chong Wang (Princeton University), Ron Weiss\n  (Google), Adam Berenzweig (Google)", "title": "Latent Collaborative Retrieval", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieval tasks typically require a ranking of items given a query.\nCollaborative filtering tasks, on the other hand, learn to model user's\npreferences over items. In this paper we study the joint problem of\nrecommending items to a user with respect to a given query, which is a\nsurprisingly common task. This setup differs from the standard collaborative\nfiltering one in that we are given a query x user x item tensor for training\ninstead of the more traditional user x item matrix. Compared to document\nretrieval we do have a query, but we may or may not have content features (we\nwill consider both cases) and we can also take account of the user's profile.\nWe introduce a factorized model for this new task that optimizes the top-ranked\nitems returned for the given query and user. We report empirical results where\nit outperforms several baselines.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 14:41:20 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Weston", "Jason", "", "Google"], ["Wang", "Chong", "", "Princeton University"], ["Weiss", "Ron", "", "Google"], ["Berenzweig", "Adam", "", "Google"]]}, {"id": "1206.4622", "submitter": "Aaron Defazio", "authors": "Aaron Defazio (ANU), Tiberio Caetano (NICTA and Australian National\n  University)", "title": "A Graphical Model Formulation of Collaborative Filtering Neighbourhood\n  Methods with Fast Maximum Entropy Training", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item neighbourhood methods for collaborative filtering learn a weighted graph\nover the set of items, where each item is connected to those it is most similar\nto. The prediction of a user's rating on an item is then given by that rating\nof neighbouring items, weighted by their similarity. This paper presents a new\nneighbourhood approach which we call item fields, whereby an undirected\ngraphical model is formed over the item graph. The resulting prediction rule is\na simple generalization of the classical approaches, which takes into account\nnon-local information in the graph, allowing its best results to be obtained\nwhen using drastically fewer edges than other neighbourhood approaches. A fast\napproximate maximum entropy training method based on the Bethe approximation is\npresented, which uses a simple gradient ascent procedure. When using\nprecomputed sufficient statistics on the Movielens datasets, our method is\nfaster than maximum likelihood approaches by two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:05:52 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Defazio", "Aaron", "", "ANU"], ["Caetano", "Tiberio", "", "NICTA and Australian National\n  University"]]}, {"id": "1206.4631", "submitter": "Edoardo Airoldi", "authors": "Edoardo M Airoldi, Jonathan M Bischof", "title": "A Poisson convolution model for characterizing topical content with word\n  frequency and exclusivity", "comments": "Originally appeared in ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ongoing challenge in the analysis of document collections is how to\nsummarize content in terms of a set of inferred themes that can be interpreted\nsubstantively in terms of topics. The current practice of parametrizing the\nthemes in terms of most frequent words limits interpretability by ignoring the\ndifferential use of words across topics. We argue that words that are both\ncommon and exclusive to a theme are more effective at characterizing topical\ncontent. We consider a setting where professional editors have annotated\ndocuments to a collection of topic categories, organized into a tree, in which\nleaf-nodes correspond to the most specific topics. Each document is annotated\nto multiple categories, at different levels of the tree. We introduce a\nhierarchical Poisson convolution model to analyze annotated documents in this\nsetting. The model leverages the structure among categories defined by\nprofessional editors to infer a clear semantic description for each topic in\nterms of words that are both frequent and exclusive. We carry out a large\nrandomized experiment on Amazon Turk to demonstrate that topic summaries based\non the FREX score are more interpretable than currently established frequency\nbased summaries, and that the proposed model produces more efficient estimates\nof exclusivity than with currently models. We also develop a parallelized\nHamiltonian Monte Carlo sampler that allows the inference to scale to millions\nof documents.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:11:38 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2012 17:32:26 GMT"}, {"version": "v3", "created": "Mon, 28 Jul 2014 03:02:39 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Airoldi", "Edoardo M", ""], ["Bischof", "Jonathan M", ""]]}, {"id": "1206.4647", "submitter": "Laurent Charlin", "authors": "Laurent Charlin (University of Toronto), Rich Zemel (University of\n  Toronto), Craig Boutilier (University of Toronto)", "title": "Active Learning for Matching Problems", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective learning of user preferences is critical to easing user burden in\nvarious types of matching problems. Equally important is active query selection\nto further reduce the amount of preference information users must provide. We\naddress the problem of active learning of user preferences for matching\nproblems, introducing a novel method for determining probabilistic matchings,\nand developing several new active learning strategies that are sensitive to the\nspecific matching objective. Experiments with real-world data sets spanning\ndiverse domains demonstrate that matching-sensitive active learning\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:22:24 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Charlin", "Laurent", "", "University of Toronto"], ["Zemel", "Rich", "", "University of\n  Toronto"], ["Boutilier", "Craig", "", "University of Toronto"]]}, {"id": "1206.4667", "submitter": "Kendrick Boyd", "authors": "Kendrick Boyd (University of Wisconsin Madison), Vitor Santos Costa\n  (University of Porto), Jesse Davis (KU Leuven), David Page (University of\n  Wisconsin Madison)", "title": "Unachievable Region in Precision-Recall Space and Its Effect on\n  Empirical Evaluation", "comments": "ICML2012, fixed citations to use correct tech report number", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision-recall (PR) curves and the areas under them are widely used to\nsummarize machine learning results, especially for data sets exhibiting class\nskew. They are often used analogously to ROC curves and the area under ROC\ncurves. It is known that PR curves vary as class skew changes. What was not\nrecognized before this paper is that there is a region of PR space that is\ncompletely unachievable, and the size of this region depends only on the skew.\nThis paper precisely characterizes the size of that region and discusses its\nimplications for empirical evaluation methodology in machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:33:05 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2012 18:54:06 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Boyd", "Kendrick", "", "University of Wisconsin Madison"], ["Costa", "Vitor Santos", "", "University of Porto"], ["Davis", "Jesse", "", "KU Leuven"], ["Page", "David", "", "University of\n  Wisconsin Madison"]]}, {"id": "1206.4684", "submitter": "Sanjay Purushotham", "authors": "Sanjay Purushotham (Univ. of Southern California), Yan Liu (Univ. of\n  Southern California), C.-C. Jay Kuo (Univ. of Southern California)", "title": "Collaborative Topic Regression with Social Matrix Factorization for\n  Recommendation Systems", "comments": "ICML2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network websites, such as Facebook, YouTube, Lastfm etc, have become a\npopular platform for users to connect with each other and share content or\nopinions. They provide rich information for us to study the influence of user's\nsocial circle in their decision process. In this paper, we are interested in\nexamining the effectiveness of social network information to predict the user's\nratings of items. We propose a novel hierarchical Bayesian model which jointly\nincorporates topic modeling and probabilistic matrix factorization of social\nnetworks. A major advantage of our model is to automatically infer useful\nlatent topics and social information as well as their importance to\ncollaborative filtering from the training data. Empirical experiments on two\nlarge-scale datasets show that our algorithm provides a more effective\nrecommendation system than the state-of-the art approaches. Our results reveal\ninteresting insight that the social circles have more influence on people's\ndecisions about the usefulness of information (e.g., bookmarking preference on\nDelicious) than personal taste (e.g., music preference on Lastfm). We also\nexamine and discuss solutions on potential information leak in many\nrecommendation systems that utilize social information.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2012 15:41:06 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Purushotham", "Sanjay", "", "Univ. of Southern California"], ["Liu", "Yan", "", "Univ. of\n  Southern California"], ["Kuo", "C. -C. Jay", "", "Univ. of Southern California"]]}, {"id": "1206.4802", "submitter": "Philipp Schaer", "authors": "Philipp Schaer", "title": "Better Than Their Reputation? On the Reliability of Relevance\n  Assessments with Students", "comments": "12 pages, to be published in Proceedings of Conference and Labs of\n  the Evaluation Forum 2012 (CLEF 2012)", "journal-ref": null, "doi": "10.1007/978-3-642-33247-0_14", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last three years we conducted several information retrieval\nevaluation series with more than 180 LIS students who made relevance\nassessments on the outcomes of three specific retrieval services. In this study\nwe do not focus on the retrieval performance of our system but on the relevance\nassessments and the inter-assessor reliability. To quantify the agreement we\napply Fleiss' Kappa and Krippendorff's Alpha. When we compare these two\nstatistical measures on average Kappa values were 0.37 and Alpha values 0.15.\nWe use the two agreement measures to drop too unreliable assessments from our\ndata set. When computing the differences between the unfiltered and the\nfiltered data set we see a root mean square error between 0.02 and 0.12. We see\nthis as a clear indicator that disagreement affects the reliability of\nretrieval evaluations. We suggest not to work with unfiltered results or to\nclearly document the disagreement rates.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2012 08:06:15 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Schaer", "Philipp", ""]]}, {"id": "1206.4883", "submitter": "Zakaria Elberrichi", "authors": "Zakaria Elberrichi, Malika Taibi, Amel Belaggoun", "title": "Multilingual Medical Documents Classification Based on MesH Domain\n  Ontology", "comments": "IJCSI International Journal of Computer Science Issues, Vol. 9, Issue\n  2, No 2, March 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with the semantic Web and ontologies. It addresses the\nissue of the classification of multilingual Web documents, based on domain\nontology. The objective is being able, using a model, to classify documents in\ndifferent languages. We will try to solve this problematic using two different\napproaches. The two approaches will have two elementary stages: the creation of\nthe model using machine learning algorithms on a labeled corpus, then the\nclassification of documents after detecting their languages and mapping their\nterms into the concepts of the language of reference (English). But each one\nwill deal with the multilingualism with a different approach. One supposes the\nontology is monolingual, whereas the other considers it multilingual. To show\nthe feasibility and the importance of our work, we implemented it on a domain\nthat attracts nowadays a lot of attention from the data mining community: the\nbiomedical domain. The selected documents are from the biomedical benchmark\ncorpus Ohsumed, and the associated ontology is the thesaurus MeSH (Medical\nSubject Headings). The main idea in our work is a new document representation,\nthe masterpiece of all good classification, based on concept. The experimental\nresults show that the recommended ideas are promising.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2012 13:56:50 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Elberrichi", "Zakaria", ""], ["Taibi", "Malika", ""], ["Belaggoun", "Amel", ""]]}, {"id": "1206.4958", "submitter": "Peiyou Song", "authors": "Peiyou Song, Anhei Shu, Anyu Zhou, Dan Wallach, Jedidiah R. Crandall", "title": "A Pointillism Approach for Natural Language Processing of Social Media", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Chinese language poses challenges for natural language processing based\non the unit of a word even for formal uses of the Chinese language, social\nmedia only makes word segmentation in Chinese even more difficult. In this\ndocument we propose a pointillism approach to natural language processing.\nRather than words that have individual meanings, the basic unit of a\npointillism approach is trigrams of characters. These grams take on meaning in\naggregate when they appear together in a way that is correlated over time.\n  Our results from three kinds of experiments show that when words and topics\ndo have a meme-like trend, they can be reconstructed from only trigrams. For\nexample, for 4-character idioms that appear at least 99 times in one day in our\ndata, the unconstrained precision (that is, precision that allows for deviation\nfrom a lexicon when the result is just as correct as the lexicon version of the\nword or phrase) is 0.93. For longer words and phrases collected from\nWiktionary, including neologisms, the unconstrained precision is 0.87. We\nconsider these results to be very promising, because they suggest that it is\nfeasible for a machine to reconstruct complex idioms, phrases, and neologisms\nwith good precision without any notion of words. Thus the colorful and baroque\nuses of language that typify social media in challenging languages such as\nChinese may in fact be accessible to machines.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2012 18:17:50 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Song", "Peiyou", ""], ["Shu", "Anhei", ""], ["Zhou", "Anyu", ""], ["Wallach", "Dan", ""], ["Crandall", "Jedidiah R.", ""]]}, {"id": "1206.5248", "submitter": "Joshua Dillon", "authors": "Joshua Dillon, Yi Mao, Guy Lebanon, Jian Zhang", "title": "Statistical Translation, Heat Kernels and Expected Distances", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-93-100", "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional structured data such as text and images is often poorly\nunderstood and misrepresented in statistical modeling. The standard histogram\nrepresentation suffers from high variance and performs poorly in general. We\nexplore novel connections between statistical translation, heat kernels on\nmanifolds and graphs, and expected distances. These connections provide a new\nframework for unsupervised metric learning for text documents. Experiments\nindicate that the resulting distances are generally superior to their more\nstandard counterparts.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 14:55:04 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Dillon", "Joshua", ""], ["Mao", "Yi", ""], ["Lebanon", "Guy", ""], ["Zhang", "Jian", ""]]}, {"id": "1206.5267", "submitter": "Benjamin Marlin", "authors": "Benjamin Marlin, Richard S. Zemel, Sam Roweis, Malcolm Slaney", "title": "Collaborative Filtering and the Missing at Random Assumption", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-267-275", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rating prediction is an important application, and a popular research topic\nin collaborative filtering. However, both the validity of learning algorithms,\nand the validity of standard testing procedures rest on the assumption that\nmissing ratings are missing at random (MAR). In this paper we present the\nresults of a user study in which we collect a random sample of ratings from\ncurrent users of an online radio service. An analysis of the rating data\ncollected in the study shows that the sample of random ratings has markedly\ndifferent properties than ratings of user-selected songs. When asked to report\non their own rating behaviour, a large number of users indicate they believe\ntheir opinion of a song does affect whether they choose to rate that song, a\nviolation of the MAR condition. Finally, we present experimental results\nshowing that incorporating an explicit model of the missing data mechanism can\nlead to significant improvements in prediction performance on the random sample\nof ratings.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:03:41 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Marlin", "Benjamin", ""], ["Zemel", "Richard S.", ""], ["Roweis", "Sam", ""], ["Slaney", "Malcolm", ""]]}, {"id": "1206.5270", "submitter": "Wei Li", "authors": "Wei Li, David Blei, Andrew McCallum", "title": "Nonparametric Bayes Pachinko Allocation", "comments": "Appears in Proceedings of the Twenty-Third Conference on Uncertainty\n  in Artificial Intelligence (UAI2007)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2007-PG-243-250", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in topic models have explored complicated structured\ndistributions to represent topic correlation. For example, the pachinko\nallocation model (PAM) captures arbitrary, nested, and possibly sparse\ncorrelations between topics using a directed acyclic graph (DAG). While PAM\nprovides more flexibility and greater expressive power than previous models\nlike latent Dirichlet allocation (LDA), it is also more difficult to determine\nthe appropriate topic structure for a specific dataset. In this paper, we\npropose a nonparametric Bayesian prior for PAM based on a variant of the\nhierarchical Dirichlet process (HDP). Although the HDP can capture topic\ncorrelations defined by nested data structure, it does not automatically\ndiscover such correlations from unstructured data. By assuming an HDP-based\nprior for PAM, we are able to learn both the number of topics and how the\ntopics are correlated. We evaluate our model on synthetic and real-world text\ndatasets, and show that nonparametric PAM achieves performance matching the\nbest of PAM without manually tuning the number of topics.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2012 15:04:47 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Li", "Wei", ""], ["Blei", "David", ""], ["McCallum", "Andrew", ""]]}, {"id": "1206.5582", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Debajyoti Mukhopadhyay, Archana Chougule", "title": "A Survey on Web Service Discovery Approaches", "comments": "12 pages, 1 figure", "journal-ref": "Advances in Computer Science, Eng. & Appl., AISC 166, pp.\n  1001-1012, Springerlink, 2012", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web services are playing an important role in e-business and e-commerce\napplications. As web service applications are interoperable and can work on any\nplatform, large scale distributed systems can be developed easily using web\nservices. Finding most suitable web service from vast collection of web\nservices is very crucial for successful execution of applications. Traditional\nweb service discovery approach is a keyword based search using UDDI. Various\nother approaches for discovering web services are also available. Some of the\ndiscovery approaches are syntax based while other are semantic based. Having\nsystem for service discovery which can work automatically is also the concern\nof service discovery approaches. As these approaches are different, one\nsolution may be better than another depending on requirements. Selecting a\nspecific service discovery system is a hard task. In this paper, we give an\noverview of different approaches for web service discovery described in\nliterature. We present a survey of how these approaches differ from each other.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2012 06:00:56 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Mukhopadhyay", "Debajyoti", ""], ["Chougule", "Archana", ""]]}, {"id": "1206.5584", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Sukanta Sinha, Rana Duttagupta, Debajyoti Mukhopadhyay", "title": "Web-page Prediction for Domain Specific Web-search using Boolean Bit\n  Mask", "comments": "10 pages, 3 figures", "journal-ref": "Advances in Computer Science, Eng. & Appl., AISC 167, pp. 211-220,\n  Springerlink, 2012", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search Engine is a Web-page retrieval tool. Nowadays Web searchers utilize\ntheir time using an efficient search engine. To improve the performance of the\nsearch engine, we are introducing a unique mechanism which will give Web\nsearchers more prominent search results. In this paper, we are going to discuss\na domain specific Web search prototype which will generate the predicted\nWeb-page list for user given search string using Boolean bit mask.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2012 06:07:20 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Sinha", "Sukanta", ""], ["Duttagupta", "Rana", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "1206.6322", "submitter": "Nabendu Chaki Dr.", "authors": "Soumya Sen, Anjan Dutta, Agostino Cortesi, Nabendu Chaki", "title": "A New Scale for Attribute Dependency in Large Database Systems", "comments": "12 pages - paper accepted for presentation and publication in CISIM\n  2012 International Confrence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large, data centric applications are characterized by its different\nattributes. In modern day, a huge majority of the large data centric\napplications are based on relational model. The databases are collection of\ntables and every table consists of numbers of attributes. The data is accessed\ntypically through SQL queries. The queries that are being executed could be\nanalyzed for different types of optimizations. Analysis based on different\nattributes used in a set of query would guide the database administrators to\nenhance the speed of query execution. A better model in this context would help\nin predicting the nature of upcoming query set. An effective prediction model\nwould guide in different applications of database, data warehouse, data mining\netc. In this paper, a numeric scale has been proposed to enumerate the strength\nof associations between independent data attributes. The proposed scale is\nbuilt based on some probabilistic analysis of the usage of the attributes in\ndifferent queries. Thus this methodology aims to predict future usage of\nattributes based on the current usage.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:09:46 GMT"}], "update_date": "2012-06-28", "authors_parsed": [["Sen", "Soumya", ""], ["Dutta", "Anjan", ""], ["Cortesi", "Agostino", ""], ["Chaki", "Nabendu", ""]]}, {"id": "1206.6411", "submitter": "Junfeng He", "authors": "Junfeng He (Columbia University), Sanjiv Kumar (Google Research),\n  Shih-Fu Chang (Columbia University)", "title": "On the Difficulty of Nearest Neighbor Search", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast approximate nearest neighbor (NN) search in large databases is becoming\npopular. Several powerful learning-based formulations have been proposed\nrecently. However, not much attention has been paid to a more fundamental\nquestion: how difficult is (approximate) nearest neighbor search in a given\ndata set? And which data properties affect the difficulty of nearest neighbor\nsearch and how? This paper introduces the first concrete measure called\nRelative Contrast that can be used to evaluate the influence of several crucial\ndata characteristics such as dimensionality, sparsity, and database size\nsimultaneously in arbitrary normed metric spaces. Moreover, we present a\ntheoretical analysis to prove how the difficulty measure (relative contrast)\ndetermines/affects the complexity of Local Sensitive Hashing, a popular\napproximate NN search method. Relative contrast also provides an explanation\nfor a family of heuristic hashing algorithms with good practical performance\nbased on PCA. Finally, we show that most of the previous works in measuring NN\nsearch meaningfulness/difficulty can be derived as special asymptotic cases for\ndense vectors of the proposed measure.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["He", "Junfeng", "", "Columbia University"], ["Kumar", "Sanjiv", "", "Google Research"], ["Chang", "Shih-Fu", "", "Columbia University"]]}, {"id": "1206.6441", "submitter": "Athina Spiliopoulou", "authors": "Athina Spiliopoulou (University of Edinburgh), Amos Storkey\n  (University of Edinburgh)", "title": "A Topic Model for Melodic Sequences", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the problem of learning a probabilistic model for melody directly\nfrom musical sequences belonging to the same genre. This is a challenging task\nas one needs to capture not only the rich temporal structure evident in music,\nbut also the complex statistical dependencies among different music components.\nTo address this problem we introduce the Variable-gram Topic Model, which\ncouples the latent topic formalism with a systematic model for contextual\ninformation. We evaluate the model on next-step prediction. Additionally, we\npresent a novel way of model evaluation, where we directly compare model\nsamples with data sequences using the Maximum Mean Discrepancy of string\nkernels, to assess how close is the model distribution to the data\ndistribution. We show that the model has the highest performance under both\nevaluation measures when compared to LDA, the Topic Bigram and related\nnon-topic models.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Spiliopoulou", "Athina", "", "University of Edinburgh"], ["Storkey", "Amos", "", "University of Edinburgh"]]}, {"id": "1206.6481", "submitter": "Yuhong Guo", "authors": "Yuhong Guo (Temple University), Min Xiao (Temple University)", "title": "Cross Language Text Classification via Subspace Co-Regularized\n  Multi-View Learning", "comments": "Appears in Proceedings of the 29th International Conference on\n  Machine Learning (ICML 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many multilingual text classification problems, the documents in different\nlanguages often share the same set of categories. To reduce the labeling cost\nof training a classification model for each individual language, it is\nimportant to transfer the label knowledge gained from one language to another\nlanguage by conducting cross language classification. In this paper we develop\na novel subspace co-regularized multi-view learning method for cross language\ntext classification. This method is built on parallel corpora produced by\nmachine translation. It jointly minimizes the training error of each classifier\nin each language while penalizing the distance between the subspace\nrepresentations of parallel documents. Our empirical study on a large set of\ncross language text classification tasks shows the proposed method consistently\noutperforms a number of inductive methods, domain adaptation methods, and\nmulti-view learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 19:59:59 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Guo", "Yuhong", "", "Temple University"], ["Xiao", "Min", "", "Temple University"]]}, {"id": "1206.6858", "submitter": "Guy Lebanon", "authors": "Guy Lebanon", "title": "Sequential Document Representations and Simplicial Curves", "comments": "Appears in Proceedings of the Twenty-Second Conference on Uncertainty\n  in Artificial Intelligence (UAI2006)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2006-PG-273-280", "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popular bag of words assumption represents a document as a histogram of\nword occurrences. While computationally efficient, such a representation is\nunable to maintain any sequential information. We present a continuous and\ndifferentiable sequential document representation that goes beyond the bag of\nwords assumption, and yet is efficient and effective. This representation\nemploys smooth curves in the multinomial simplex to account for sequential\ninformation. We discuss the representation and its geometric properties and\ndemonstrate its applicability for the task of text classification.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2012 16:26:46 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Lebanon", "Guy", ""]]}, {"id": "1206.7112", "submitter": "Yi-Hao Kao", "authors": "Yi-Hao Kao and Benjamin Van Roy and Daniel Rubin and Jiajing Xu and\n  Jessica Faruque and Sandy Napel", "title": "A Hybrid Method for Distance Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a measure of distance among vectors in a\nfeature space and propose a hybrid method that simultaneously learns from\nsimilarity ratings assigned to pairs of vectors and class labels assigned to\nindividual vectors. Our method is based on a generative model in which class\nlabels can provide information that is not encoded in feature vectors but yet\nrelates to perceived similarity between objects. Experiments with synthetic\ndata as well as a real medical image retrieval problem demonstrate that\nleveraging class labels through use of our method improves retrieval\nperformance significantly.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2012 19:33:47 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Kao", "Yi-Hao", ""], ["Van Roy", "Benjamin", ""], ["Rubin", "Daniel", ""], ["Xu", "Jiajing", ""], ["Faruque", "Jessica", ""], ["Napel", "Sandy", ""]]}]