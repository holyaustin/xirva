[{"id": "1111.0753", "submitter": "Sourav Dutta", "authors": "Sourav Dutta, Souvik Bhattacherjee and Ankur Narang", "title": "Towards \"Intelligent Compression\" in Streams: A Biased Reservoir\n  Sampling based Bloom Filter Approach", "comments": "11 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": "IBM TechReport RI11015", "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the explosion of information stored world-wide,data intensive computing\nhas become a central area of research.Efficient management and processing of\nthis massively exponential amount of data from diverse sources,such as\ntelecommunication call data records,online transaction records,etc.,has become\na necessity.Removing redundancy from such huge(multi-billion records) datasets\nresulting in resource and compute efficiency for downstream processing\nconstitutes an important area of study. \"Intelligent compression\" or\ndeduplication in streaming scenarios,for precise identification and elimination\nof duplicates from the unbounded datastream is a greater challenge given the\nrealtime nature of data arrival.Stable Bloom Filters(SBF) address this problem\nto a certain extent.However,SBF suffers from a high false negative rate(FNR)\nand slow convergence rate,thereby rendering it inefficient for applications\nwith low FNR tolerance.In this paper, we present a novel Reservoir Sampling\nbased Bloom Filter,(RSBF) data structure,based on the combined concepts of\nreservoir sampling and Bloom filters for approximate detection of duplicates in\ndata streams.Using detailed theoretical analysis we prove analytical bounds on\nits false positive rate(FPR),false negative rate(FNR) and convergence rates\nwith low memory requirements.We show that RSBF offers the currently lowest FN\nand convergence rates,and are better than those of SBF while using the same\nmemory.Using empirical analysis on real-world datasets(3 million records) and\nsynthetic datasets with around 1 billion records,we demonstrate upto 2x\nimprovement in FNR with better convergence rates as compared to SBF,while\nexhibiting comparable FPR.To the best of our knowledge,this is the first\nattempt to integrate reservoir sampling method with Bloom filters for\ndeduplication in streaming scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2011 08:45:44 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Dutta", "Sourav", ""], ["Bhattacherjee", "Souvik", ""], ["Narang", "Ankur", ""]]}, {"id": "1111.1093", "submitter": "Sabu Thampi m", "authors": "Sabu M. Thampi, Ann Jisma Jacob", "title": "Securing Biometric Images using Reversible Watermarking", "comments": "8 pages, 7 figures", "journal-ref": "International Journal of Image Processing (IJIP), Volume:\n  5,Issue:4, September/October 2011", "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometric security is a fast growing area. Protecting biometric data is very\nimportant since it can be misused by attackers. In order to increase security\nof biometric data there are different methods in which watermarking is widely\naccepted. A more acceptable, new important development in this area is\nreversible watermarking in which the original image can be completely restored\nand the watermark can be retrieved. But reversible watermarking in biometrics\nis an understudied area. Reversible watermarking maintains high quality of\nbiometric data. This paper proposes Rotational Replacement of LSB as a\nreversible watermarking scheme for biometric images. PSNR is the regular method\nused for quality measurement of biometric data. In this paper we also show that\nSSIM Index is a better alternate for effective quality assessment for\nreversible watermarked biometric data by comparing with the well known\nreversible watermarking scheme using Difference Expansion.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2011 10:50:45 GMT"}], "update_date": "2011-11-07", "authors_parsed": [["Thampi", "Sabu M.", ""], ["Jacob", "Ann Jisma", ""]]}, {"id": "1111.1497", "submitter": "Rishiraj Saha Roy", "authors": "Rishiraj Saha Roy, Niloy Ganguly, Monojit Choudhury and Srivatsan\n  Laxman", "title": "An IR-based Evaluation Framework for Web Search Query Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper presents the first evaluation framework for Web search query\nsegmentation based directly on IR performance. In the past, segmentation\nstrategies were mainly validated against manual annotations. Our work shows\nthat the goodness of a segmentation algorithm as judged through evaluation\nagainst a handful of human annotated segmentations hardly reflects its\neffectiveness in an IR-based setup. In fact, state-of the-art algorithms are\nshown to perform as good as, and sometimes even better than human annotations\n-- a fact masked by previous validations. The proposed framework also provides\nus an objective understanding of the gap between the present best and the best\npossible segmentation algorithm. We draw these conclusions based on an\nextensive evaluation of six segmentation strategies, including three most\nrecent algorithms, vis-a-vis segmentations from three human annotators. The\nevaluation framework also gives insights about which segments should be\nnecessarily detected by an algorithm for achieving the best retrieval results.\nThe meticulously constructed dataset used in our experiments has been made\npublic for use by the research community.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2011 07:26:27 GMT"}, {"version": "v2", "created": "Sun, 18 Dec 2011 17:33:28 GMT"}, {"version": "v3", "created": "Tue, 20 Dec 2011 11:22:38 GMT"}, {"version": "v4", "created": "Tue, 18 Sep 2012 03:26:22 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Roy", "Rishiraj Saha", ""], ["Ganguly", "Niloy", ""], ["Choudhury", "Monojit", ""], ["Laxman", "Srivatsan", ""]]}, {"id": "1111.1570", "submitter": "Frederico  Durao", "authors": "Frederico Durao, Peter Dolog", "title": "Semantic Grounding Strategies for Tagbased Recommender Systems", "comments": "13 pages, 5 figures", "journal-ref": "International Journal of Web & Semantic Technology (IJWesT) Vol.2,\n  No.4, 2011, 67-79", "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Recommender systems usually operate on similarities between recommended items\nor users. Tag based recommender systems utilize similarities on tags. The tags\nare however mostly free user entered phrases. Therefore, similarities computed\nwithout their semantic groundings might lead to less relevant recommendations.\nIn this paper, we study a semantic grounding used for tag similarity calculus.\nWe show a comprehensive analysis of semantic grounding given by 20 ontologies\nfrom different domains. The study besides other things reveals that currently\navailable OWL ontologies are very narrow and the percentage of the similarity\nexpansions is rather small. WordNet scores slightly better as it is broader but\nnot much as it does not support several semantic relationships. Furthermore,\nthe study reveals that even with such number of expansions, the recommendations\nchange considerably.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2011 13:31:39 GMT"}], "update_date": "2011-11-08", "authors_parsed": [["Durao", "Frederico", ""], ["Dolog", "Peter", ""]]}, {"id": "1111.1648", "submitter": "Archana Shukla", "authors": "Archana Shukla", "title": "Sentiment Analysis of Document Based on Annotation", "comments": "14 pages, 14 figures, published in IJWEST Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present a tool which tells the quality of document or its usefulness based\non annotations. Annotation may include comments, notes, observation,\nhighlights, underline, explanation, question or help etc. comments are used for\nevaluative purpose while others are used for summarization or for expansion\nalso. Further these comments may be on another annotation. Such annotations are\nreferred as meta-annotation. All annotation may not get equal weightage. My\ntool considered highlights, underline as well as comments to infer the\ncollective sentiment of annotators. Collective sentiments of annotators are\nclassified as positive, negative, objectivity. My tool computes collective\nsentiment of annotations in two manners. It counts all the annotation present\non the documents as well as it also computes sentiment scores of all annotation\nwhich includes comments to obtain the collective sentiments about the document\nor to judge the quality of document. I demonstrate the use of tool on research\npaper.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2011 17:11:50 GMT"}], "update_date": "2011-11-08", "authors_parsed": [["Shukla", "Archana", ""]]}, {"id": "1111.2948", "submitter": "Marcos Domingues", "authors": "Marcos A. Domingues, Alipio Mario Jorge, Carlos Soares", "title": "Using Contextual Information as Virtual Items on Top-N Recommender\n  Systems", "comments": "Workshop on Context-Aware Recommender Systems (CARS'09) in\n  conjunction with the 3rd ACM Conference on Recommender Systems (RecSys'09)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, recommender systems for the Web deal with applications that\nhave two dimensions, users and items. Based on access logs that relate these\ndimensions, a recommendation model can be built and used to identify a set of N\nitems that will be of interest to a certain user. In this paper we propose a\nmethod to complement the information in the access logs with contextual\ninformation without changing the recommendation algorithm. The method consists\nin representing context as virtual items. We empirically test this method with\ntwo top-N recommender systems, an item-based collaborative filtering technique\nand association rules, on three data sets. The results show that our method is\nable to take advantage of the context (new dimensions) when it is informative.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2011 17:53:10 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2011 10:20:57 GMT"}], "update_date": "2011-11-16", "authors_parsed": [["Domingues", "Marcos A.", ""], ["Jorge", "Alipio Mario", ""], ["Soares", "Carlos", ""]]}, {"id": "1111.3281", "submitter": "Reza Farrahi Moghaddam", "authors": "Reza Farrahi Moghaddam and Mohamed Cheriet and Thomas Milo and Robert\n  Wisnovsky", "title": "A prototype system for handwritten sub-word recognition: Toward\n  Arabic-manuscript transliteration", "comments": "8 pages, 7 figures, 6 tables", "journal-ref": null, "doi": "10.1109/ISSPA.2012.6310473", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prototype system for the transliteration of diacritics-less Arabic\nmanuscripts at the sub-word or part of Arabic word (PAW) level is developed.\nThe system is able to read sub-words of the input manuscript using a set of\nskeleton-based features. A variation of the system is also developed which\nreads archigraphemic Arabic manuscripts, which are dot-less, into\narchigraphemes transliteration. In order to reduce the complexity of the\noriginal highly multiclass problem of sub-word recognition, it is redefined\ninto a set of binary descriptor classifiers. The outputs of trained binary\nclassifiers are combined to generate the sequence of sub-word letters. SVMs are\nused to learn the binary classifiers. Two specific Arabic databases have been\ndeveloped to train and test the system. One of them is a database of the Naskh\nstyle. The initial results are promising. The systems could be trained on other\nscripts found in Arabic manuscripts.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2011 17:03:42 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Moghaddam", "Reza Farrahi", ""], ["Cheriet", "Mohamed", ""], ["Milo", "Thomas", ""], ["Wisnovsky", "Robert", ""]]}, {"id": "1111.6116", "submitter": "Norman Gray", "authors": "Norman Gray and Robert G Mann and Dave Morris and Mark Holliman and\n  Keith Noddle", "title": "AstroDAbis: Annotations and Cross-Matches for Remote Catalogues", "comments": "4 pages, 1 figure, to appear in Proceedings of ADASS XXI, Paris, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astronomers are good at sharing data, but poorer at sharing knowledge.\n  Almost all astronomical data ends up in open archives, and access to these is\nbeing simplified by the development of the global Virtual Observatory (VO).\nThis is a great advance, but the fundamental problem remains that these\narchives contain only basic observational data, whereas all the astrophysical\ninterpretation of that data -- which source is a quasar, which a low-mass star,\nand which an image artefact -- is contained in journal papers, with very little\nlinkage back from the literature to the original data archives. It is therefore\ncurrently impossible for an astronomer to pose a query like \"give me all\nsources in this data archive that have been identified as quasars\" and this\nlimits the effective exploitation of these archives, as the user of an archive\nhas no direct means of taking advantage of the knowledge derived by its\nprevious users.\n  The AstroDAbis service aims to address this, in a prototype service enabling\nastronomers to record annotations and cross-identifications in the AstroDAbis\nservice, annotating objects in other catalogues. We have deployed two\ninterfaces to the annotations, namely one astronomy-specific one using the TAP\nprotocol}, and a second exploiting generic Linked Open Data (LOD) and RDF\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2011 21:22:42 GMT"}], "update_date": "2011-11-29", "authors_parsed": [["Gray", "Norman", ""], ["Mann", "Robert G", ""], ["Morris", "Dave", ""], ["Holliman", "Mark", ""], ["Noddle", "Keith", ""]]}, {"id": "1111.6349", "submitter": "Awny Sayed", "authors": "Awny Sayed", "title": "XML Information Retrieval Systems: A Survey", "comments": "10 pages, 25 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuous growth in the XML information repositories has been matched by\nincreasing efforts in development of XML retrieval systems, in large parts\naiming at supporting content-oriented XML retrieval. These systems exploit the\navailable structural information, as market up in XML documents, in order to\nreturn documents components- the so called XML elements-instead of the\ncomplement documents in repose to the user query. In this paper, we provide an\noverview of the different XML information retrieval systems and classify them\naccording to their storage and query evaluation strategies.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2011 05:45:43 GMT"}], "update_date": "2011-11-29", "authors_parsed": [["Sayed", "Awny", ""]]}, {"id": "1111.6387", "submitter": "Kassimi My Abdellah", "authors": "My Abdellah Kassimi and Omar El beqqali", "title": "3D Model Retrieval Based on Semantic and Shape Indexes", "comments": "IJCSI International Journal of Computer Science Issues, Vol. 8, Issue\n  3, May 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The size of 3D models used on the web or stored in databases is becoming\nincreasingly high. Then, an efficient method that allows users to find similar\n3D objects for a given 3D model query has become necessary. Keywords and the\ngeometry of a 3D model cannot meet the needs of users' retrieval because they\ndo not include the semantic information. In this paper, a new method has been\nproposed to 3D models retrieval using semantic concepts combined with shape\nindexes. To obtain these concepts, we use the machine learning methods to label\n3D models by k-means algorithm in measures and shape indexes space. Moreover,\nsemantic concepts have been organized and represented by ontology language OWL\nand spatial relationships are used to disambiguate among models of similar\nappearance. The SPARQL query language has been used to question the information\ndisplayed in this language and to compute the similarity between two 3D models.\nWe interpret our results using the Princeton Shape Benchmark Database and the\nresults show the performance of the proposed new approach to retrieval 3D\nmodels. Keywords: 3D Model, 3D retrieval, measures, shape indexes, semantic,\nontology\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2011 10:07:41 GMT"}], "update_date": "2011-11-29", "authors_parsed": [["Kassimi", "My Abdellah", ""], ["beqqali", "Omar El", ""]]}, {"id": "1111.6631", "submitter": "Arash Sangari Mr.", "authors": "Arash Sangari, Adel Ardalan, Larry Lambe, Hamid Eghbalnia and Amir H.\n  Assadi", "title": "Mathematical Analysis and Computational Integration of Massive\n  Heterogeneous Data from the Human Retina", "comments": "9 pages, 3 figures, submitted and accepted in Damor2012 conference:\n  http://www.uninova.pt/damor2012/index.php?page=authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.IR math.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern epidemiology integrates knowledge from heterogeneous collections of\ndata consisting of numerical, descriptive and imaging. Large-scale\nepidemiological studies use sophisticated statistical analysis, mathematical\nmodels using differential equations and versatile analytic tools that handle\nnumerical data. In contrast, knowledge extraction from images and descriptive\ninformation in the form of text and diagrams remain a challenge for most\nfields, in particular, for diseases of the eye. In this article we provide a\nroadmap towards extraction of knowledge from text and images with focus on\nforthcoming applications to epidemiological investigation of retinal diseases,\nespecially from existing massive heterogeneous collections of data distributed\naround the globe.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2011 22:01:19 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2012 19:52:29 GMT"}], "update_date": "2012-10-11", "authors_parsed": [["Sangari", "Arash", ""], ["Ardalan", "Adel", ""], ["Lambe", "Larry", ""], ["Eghbalnia", "Hamid", ""], ["Assadi", "Amir H.", ""]]}, {"id": "1111.6640", "submitter": "Scott Hand", "authors": "Scott Hand", "title": "A Markov Random Field Topic Space Model for Document Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel statistical approach to intelligent document\nretrieval. It seeks to offer a more structured and extensible mathematical\napproach to the term generalization done in the popular Latent Semantic\nAnalysis (LSA) approach to document indexing. A Markov Random Field (MRF) is\npresented that captures relationships between terms and documents as\nprobabilistic dependence assumptions between random variables. From there, it\nuses the MRF-Gibbs equivalence to derive joint probabilities as well as local\nprobabilities for document variables. A parameter learning method is proposed\nthat utilizes rank reduction with singular value decomposition in a matter\nsimilar to LSA to reduce dimensionality of document-term relationships to that\nof a latent topic space. Experimental results confirm the ability of this\napproach to effectively and efficiently retrieve documents from substantial\ndata sets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2011 22:33:10 GMT"}], "update_date": "2011-11-30", "authors_parsed": [["Hand", "Scott", ""]]}]