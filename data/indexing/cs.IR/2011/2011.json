[{"id": "2011.00061", "submitter": "Marzieh Fadaee", "authors": "Marzieh Fadaee, Olga Gureenkova, Fernando Rejon Barrera, Carsten\n  Schnober, Wouter Weerkamp, Jakub Zavrel", "title": "A New Neural Search and Insights Platform for Navigating and Organizing\n  AI Research", "comments": "Accepted to Workshop on Scholarly Document Processing (SDP) at EMNLP\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To provide AI researchers with modern tools for dealing with the explosive\ngrowth of the research literature in their field, we introduce a new platform,\nAI Research Navigator, that combines classical keyword search with neural\nretrieval to discover and organize relevant literature. The system provides\nsearch at multiple levels of textual granularity, from sentences to\naggregations across documents, both in natural language and through navigation\nin a domain-specific Knowledge Graph. We give an overview of the overall\narchitecture of the system and of the components for document analysis,\nquestion answering, search, analytics, expert search, and recommendations.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 19:12:25 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Fadaee", "Marzieh", ""], ["Gureenkova", "Olga", ""], ["Barrera", "Fernando Rejon", ""], ["Schnober", "Carsten", ""], ["Weerkamp", "Wouter", ""], ["Zavrel", "Jakub", ""]]}, {"id": "2011.00270", "submitter": "Kenta Iida", "authors": "Kenta Iida and Hitoshi Kiya", "title": "A Privacy-Preserving Content-Based Image Retrieval Scheme Allowing Mixed\n  Use Of Encrypted And Plain Images", "comments": "This paper will be presented at APSIPA ASC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel content based-image retrieval scheme\nallowing the mixed use of encrypted and plain images for the first time. In the\nproposed scheme, images are encrypted by a block-scrambling method developed\nfor encryption-then-compression (EtC) systems. The encrypted images, referred\nto as EtC images, can be compressed with JPEG, as well as for plain images.\nImage descriptors used for the proposed retrieval is designed to avoid the\neffect of image encryption. As a result, the use of EtC images and the\ndescriptors allows us to carry out retrieval of both encrypted images and plain\nones. In an experiment, the proposed scheme is demonstrated to have the same\nperformance as conventional retrieval methods with plain images, even under the\nmixed use of plain images and EtC ones.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 13:28:59 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Iida", "Kenta", ""], ["Kiya", "Hitoshi", ""]]}, {"id": "2011.00377", "submitter": "Swati Padhee", "authors": "Ankita Agarwal and Preetham Salehundam and Swati Padhee and William L.\n  Romine and Tanvi Banerjee", "title": "Leveraging Natural Language Processing to Mine Issues on Twitter During\n  the COVID-19 Pandemic", "comments": "11 pages, 5 figures, 5 tables. Long version of accepted Paper at IEEE\n  Big Data 2020 (https://bigdataieee.org/BigData2020/AcceptedPapers.html)", "journal-ref": null, "doi": null, "report-no": "BigD560", "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent global outbreak of the coronavirus disease (COVID-19) has spread\nto all corners of the globe. The international travel ban, panic buying, and\nthe need for self-quarantine are among the many other social challenges brought\nabout in this new era. Twitter platforms have been used in various public\nhealth studies to identify public opinion about an event at the local and\nglobal scale. To understand the public concerns and responses to the pandemic,\na system that can leverage machine learning techniques to filter out irrelevant\ntweets and identify the important topics of discussion on social media\nplatforms like Twitter is needed. In this study, we constructed a system to\nidentify the relevant tweets related to the COVID-19 pandemic throughout\nJanuary 1st, 2020 to April 30th, 2020, and explored topic modeling to identify\nthe most discussed topics and themes during this period in our data set.\nAdditionally, we analyzed the temporal changes in the topics with respect to\nthe events that occurred during this pandemic. We found out that eight topics\nwere sufficient to identify the themes in our corpus. These topics depicted a\ntemporal trend. The dominant topics vary over time and align with the events\nrelated to the COVID-19 pandemic.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 22:26:26 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 02:42:05 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Agarwal", "Ankita", ""], ["Salehundam", "Preetham", ""], ["Padhee", "Swati", ""], ["Romine", "William L.", ""], ["Banerjee", "Tanvi", ""]]}, {"id": "2011.00422", "submitter": "Yujie Lu", "authors": "Yujie Lu, Shengyu Zhang, Yingxuan Huang, Luyao Wang, Xinyao Yu, Zhou\n  Zhao, Fei Wu", "title": "Future-Aware Diverse Trends Framework for Recommendation", "comments": null, "journal-ref": "The Web Conference 2021", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In recommender systems, modeling user-item behaviors is essential for user\nrepresentation learning. Existing sequential recommenders consider the\nsequential correlations between historically interacted items for capturing\nusers' historical preferences. However, since users' preferences are by nature\ntime-evolving and diversified, solely modeling the historical preference\n(without being aware of the time-evolving trends of preferences) can be\ninferior for recommending complementary or fresh items and thus hurt the\neffectiveness of recommender systems. In this paper, we bridge the gap between\nthe past preference and potential future preference by proposing the\nfuture-aware diverse trends (FAT) framework. By future-aware, for each\ninspected user, we construct the future sequences from other similar users,\nwhich comprise of behaviors that happen after the last behavior of the\ninspected user, based on a proposed neighbor behavior extractor. By diverse\ntrends, supposing the future preferences can be diversified, we propose the\ndiverse trends extractor and the time-aware mechanism to represent the possible\ntrends of preferences for a given user with multiple vectors. We leverage both\nthe representations of historical preference and possible future trends to\nobtain the final recommendation. The quantitative and qualitative results from\nrelatively extensive experiments on real-world datasets demonstrate the\nproposed framework not only outperforms the state-of-the-art sequential\nrecommendation methods across various metrics, but also makes complementary and\nfresh recommendations.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 04:37:28 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 10:06:54 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lu", "Yujie", ""], ["Zhang", "Shengyu", ""], ["Huang", "Yingxuan", ""], ["Wang", "Luyao", ""], ["Yu", "Xinyao", ""], ["Zhao", "Zhou", ""], ["Wu", "Fei", ""]]}, {"id": "2011.00427", "submitter": "Xiaochen Liu", "authors": "Xiaochen Liu", "title": "Efficient Pipelines for Vision-Based Context Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context awareness is an essential part of mobile and ubiquitous computing.\nIts goal is to unveil situational information about mobile users like locations\nand activities. The sensed context can enable many services like navigation,\nAR, and smarting shopping. Such context can be sensed in different ways\nincluding visual sensors. There is an emergence of vision sources deployed\nworldwide. The cameras could be installed on roadside, in-house, and on mobile\nplatforms. This trend provides huge amount of vision data that could be used\nfor context sensing. However, the vision data collection and analytics are\nstill highly manual today. It is hard to deploy cameras at large scale for data\ncollection. Organizing and labeling context from the data are also labor\nintensive. In recent years, advanced vision algorithms and deep neural networks\nare used to help analyze vision data. But this approach is limited by data\nquality, labeling effort, and dependency on hardware resources. In summary,\nthere are three major challenges for today's vision-based context sensing\nsystems: data collection and labeling at large scale, process large data\nvolumes efficiently with limited hardware resources, and extract accurate\ncontext out of vision data. The thesis explores the design space that consists\nof three dimensions: sensing task, sensor types, and task locations. Our prior\nwork explores several points in this design space. We make contributions by (1)\ndeveloping efficient and scalable solutions for different points in the design\nspace of vision-based sensing tasks; (2) achieving state-of-the-art accuracy in\nthose applications; (3) and developing guidelines for designing such sensing\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 05:09:13 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Liu", "Xiaochen", ""]]}, {"id": "2011.00479", "submitter": "Kevin Roitero", "authors": "Kevin Roitero", "title": "Cheap IR Evaluation: Fewer Topics, No Relevance Judgements, and\n  Crowdsourced Assessments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To evaluate Information Retrieval (IR) effectiveness, a possible approach is\nto use test collections, which are composed of a collection of documents, a set\nof description of information needs (called topics), and a set of relevant\ndocuments to each topic. Test collections are modelled in a competition\nscenario: for example, in the well known TREC initiative, participants run\ntheir own retrieval systems over a set of topics and they provide a ranked list\nof retrieved documents; some of the retrieved documents (usually the first\nranked) constitute the so called pool, and their relevance is evaluated by\nhuman assessors; the document list is then used to compute effectiveness\nmetrics and rank the participant systems. Private Web Search companies also run\ntheir in-house evaluation exercises; although the details are mostly unknown,\nand the aims are somehow different, the overall approach shares several issues\nwith the test collection approach.\n  The aim of this work is to: (i) develop and improve some state-of-the-art\nwork on the evaluation of IR effectiveness while saving resources, and (ii)\npropose a novel, more principled and engineered, overall approach to test\ncollection based effectiveness evaluation.\n  [...]\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 11:22:05 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Roitero", "Kevin", ""]]}, {"id": "2011.00518", "submitter": "Rujing Yao", "authors": "Rujing Yao, Yingchun Ye, Ji Zhang, Shuxiao Li and Ou Wu", "title": "AI Marker-based Large-scale AI Literature Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knowledge contained in academic literature is interesting to mine.\nInspired by the idea of molecular markers tracing in the field of biochemistry,\nthree named entities, namely, methods, datasets and metrics are used as AI\nmarkers for AI literature. These entities can be used to trace the research\nprocess described in the bodies of papers, which opens up new perspectives for\nseeking and mining more valuable academic information. Firstly, the entity\nextraction model is used in this study to extract AI markers from large-scale\nAI literature. Secondly, original papers are traced for AI markers. Statistical\nand propagation analysis are performed based on tracing results. Finally, the\nco-occurrences of AI markers are used to achieve clustering. The evolution\nwithin method clusters and the influencing relationships amongst different\nresearch scene clusters are explored. The above-mentioned mining based on AI\nmarkers yields many meaningful discoveries. For example, the propagation of\neffective methods on the datasets is rapidly increasing with the development of\ntime; effective methods proposed by China in recent years have increasing\ninfluence on other countries, whilst France is the opposite. Saliency\ndetection, a classic computer vision research scene, is the least likely to be\naffected by other research scenes.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 14:48:46 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 04:13:16 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Yao", "Rujing", ""], ["Ye", "Yingchun", ""], ["Zhang", "Ji", ""], ["Li", "Shuxiao", ""], ["Wu", "Ou", ""]]}, {"id": "2011.00550", "submitter": "Xinyi Dai", "authors": "Xinyi Dai, Jiawei Hou, Qing Liu, Yunjia Xi, Ruiming Tang, Weinan\n  Zhang, Xiuqiang He, Jun Wang, Yong Yu", "title": "U-rank: Utility-oriented Learning to Rank with Implicit Feedback", "comments": null, "journal-ref": null, "doi": "10.1145/3340531.3412756", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to rank with implicit feedback is one of the most important tasks in\nmany real-world information systems where the objective is some specific\nutility, e.g., clicks and revenue. However, we point out that existing methods\nbased on probabilistic ranking principle do not necessarily achieve the highest\nutility. To this end, we propose a novel ranking framework called U-rank that\ndirectly optimizes the expected utility of the ranking list. With a\nposition-aware deep click-through rate prediction model, we address the\nattention bias considering both query-level and item-level features. Due to the\nitem-specific attention bias modeling, the optimization for expected utility\ncorresponds to a maximum weight matching on the item-position bipartite graph.\nWe base the optimization of this objective in an efficient Lambdaloss\nframework, which is supported by both theoretical and empirical analysis. We\nconduct extensive experiments for both web search and recommender systems over\nthree benchmark datasets and two proprietary datasets, where the performance\ngain of U-rank over state-of-the-arts is demonstrated. Moreover, our proposed\nU-rank has been deployed on a large-scale commercial recommender and a large\nimprovement over the production baseline has been observed in an online A/B\ntesting.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 16:29:05 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Dai", "Xinyi", ""], ["Hou", "Jiawei", ""], ["Liu", "Qing", ""], ["Xi", "Yunjia", ""], ["Tang", "Ruiming", ""], ["Zhang", "Weinan", ""], ["He", "Xiuqiang", ""], ["Wang", "Jun", ""], ["Yu", "Yong", ""]]}, {"id": "2011.00565", "submitter": "Bilal Tahir", "authors": "Muntaha Iqbal, Kamran Amjad, Bilal Tahir, Muhammad Amir Mehmood", "title": "CURE: Collection for Urdu Information Retrieval Evaluation and Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urdu is a widely spoken language with 163 million speakers worldwide across\nthe globe. Information Retrieval (IR) for Urdu entails special consideration of\nresearch community due to its rich morphological features and a large number of\nspeakers. In general, IR evaluation task is not extensively explored for Urdu.\nThe most important missing element is the availability of a standardized\nevaluation corpus specific to Urdu. In this research work, we propose and\nconstruct a standard test collection of Urdu documents for IR evaluation and\nnamed it Collection for Urdu Retrieval Evaluation (CURE). We select 1,096\nunique documents against 50 diverse queries from a large collection of 0.5\nmillion crawled documents using two IR models. The purpose of test collection\nis the evaluation of IR models, ranking algorithms, and different natural\nlanguage processing techniques. Next, we perform binary relevance judgment on\nthe selected documents. We also built two other language resources for\nlemmatization and query expansion specific to our test collection. Evaluation\nof test collection is carried out using four retrieval models as well using the\nstop-words list, lemmatization, and query expansion. Furthermore, error\nanalysis was performed for each query with different NLP techniques. To the\nbest of our knowledge, this work is the first attempt for preparing a\nstandardized information retrieval evaluation test collection for the Urdu\nlanguage.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 17:16:49 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Iqbal", "Muntaha", ""], ["Amjad", "Kamran", ""], ["Tahir", "Bilal", ""], ["Mehmood", "Muhammad Amir", ""]]}, {"id": "2011.00650", "submitter": "Konstantina Dritsa Mrs.", "authors": "Konstantina Dritsa, Thodoris Sotiropoulos, Haris Skarpetis, Panos\n  Louridas", "title": "Search Engine Similarity Analysis: A Combined Content and Rankings\n  Approach", "comments": "Shorter version of this paper was accepted in the 21st International\n  Conference on Web Information Systems Engineering (WISE 2020). The final\n  authenticated version is available online at\n  https://doi.org/10.1007/978-3-030-62008-0_2", "journal-ref": null, "doi": "10.1007/978-3-030-62008-0_2", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How different are search engines? The search engine wars are a favorite topic\nof on-line analysts, as two of the biggest companies in the world, Google and\nMicrosoft, battle for prevalence of the web search space. Differences in search\nengine popularity can be explained by their effectiveness or other factors,\nsuch as familiarity with the most popular first engine, peer imitation, or\nforce of habit. In this work we present a thorough analysis of the affinity of\nthe two major search engines, Google and Bing, along with DuckDuckGo, which\ngoes to great lengths to emphasize its privacy-friendly credentials. To do so,\nwe collected search results using a comprehensive set of 300 unique queries for\ntwo time periods in 2016 and 2019, and developed a new similarity metric that\nleverages both the content and the ranking of search responses. We evaluated\nthe characteristics of the metric against other metrics and approaches that\nhave been proposed in the literature, and used it to (1) investigate the\nsimilarities of search engine results, (2) the evolution of their affinity over\ntime, (3) what aspects of the results influence similarity, and (4) how the\nmetric differs over different kinds of search services. We found that Google\nstands apart, but Bing and DuckDuckGo are largely indistinguishable from each\nother.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 23:57:24 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 17:11:10 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Dritsa", "Konstantina", ""], ["Sotiropoulos", "Thodoris", ""], ["Skarpetis", "Haris", ""], ["Louridas", "Panos", ""]]}, {"id": "2011.00696", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Sergey Feldman, Nazli Goharian, Doug Downey, Arman\n  Cohan", "title": "ABNIRML: Analyzing the Behavior of Neural IR Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous studies have demonstrated the effectiveness of pretrained\ncontextualized language models such as BERT and T5 for ad-hoc search. However,\nit is not well-understood why these methods are so effective, what makes some\nvariants more effective than others, and what pitfalls they may have. We\npresent a new comprehensive framework for Analyzing the Behavior of Neural IR\nModeLs (ABNIRML), which includes new types of diagnostic tests that allow us to\nprobe several characteristics---such as sensitivity to word order---that are\nnot addressed by previous techniques. To demonstrate the value of the\nframework, we conduct an extensive empirical study that yields insights into\nthe factors that contribute to the neural model's gains, and identify potential\nunintended biases the models exhibit. We find evidence that recent neural\nranking models have fundamentally different characteristics from prior ranking\nmodels. For instance, these models can be highly influenced by altered document\nword order, sentence order and inflectional endings. They can also exhibit\nunexpected behaviors when additional content is added to documents, or when\ndocuments are expressed with different levels of fluency or formality. We find\nthat these differences can depend on the architecture and not just the\nunderlying language model.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 03:07:38 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["MacAvaney", "Sean", ""], ["Feldman", "Sergey", ""], ["Goharian", "Nazli", ""], ["Downey", "Doug", ""], ["Cohan", "Arman", ""]]}, {"id": "2011.00701", "submitter": "Xiao Zhang", "authors": "Jiapeng Liu, Xiao Zhang, Dan Goldwasser, Xiao Wang", "title": "Cross-Lingual Document Retrieval with Smooth Learning", "comments": "COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual document search is an information retrieval task in which the\nqueries' language differs from the documents' language. In this paper, we study\nthe instability of neural document search models and propose a novel end-to-end\nrobust framework that achieves improved performance in cross-lingual search\nwith different documents' languages. This framework includes a novel measure of\nthe relevance, smooth cosine similarity, between queries and documents, and a\nnovel loss function, Smooth Ordinal Search Loss, as the objective. We further\nprovide theoretical guarantee on the generalization error bound for the\nproposed framework. We conduct experiments to compare our approach with other\ndocument search models, and observe significant gains under commonly used\nranking metrics on the cross-lingual document retrieval task in a variety of\nlanguages.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 03:17:39 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Liu", "Jiapeng", ""], ["Zhang", "Xiao", ""], ["Goldwasser", "Dan", ""], ["Wang", "Xiao", ""]]}, {"id": "2011.00866", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Praveenkumar Kanumala, Stephen Guo, Kannan\n  Achan", "title": "An End-to-End ML System for Personalized Conversational Voice Models in\n  Walmart E-Commerce", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for and making decisions about products is becoming increasingly\neasier in the e-commerce space, thanks to the evolution of recommender systems.\nPersonalization and recommender systems have gone hand-in-hand to help\ncustomers fulfill their shopping needs and improve their experiences in the\nprocess. With the growing adoption of conversational platforms for shopping, it\nhas become important to build personalized models at scale to handle the large\ninflux of data and perform inference in real-time. In this work, we present an\nend-to-end machine learning system for personalized conversational voice\ncommerce. We include components for implicit feedback to the model, model\ntraining, evaluation on update, and a real-time inference engine. Our system\npersonalizes voice shopping for Walmart Grocery customers and is currently\navailable via Google Assistant, Siri and Google Home devices.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 10:14:55 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Kanumala", "Praveenkumar", ""], ["Guo", "Stephen", ""], ["Achan", "Kannan", ""]]}, {"id": "2011.00867", "submitter": "Benjamin Murray Mr", "authors": "Benjamin Murray, Eric Kerfoot, Mark S. Graham, Carole H. Sudre, Erika\n  Molteni, Liane S. Canas, Michela Antonelli, Kerstin Klaser, Alessia Visconti,\n  Andrew T. Chan, Paul W. Franks, Richard Davies, Jonathan Wolf, Tim Spector,\n  Claire J. Steves, Marc Modat, Sebastien Ourselin", "title": "Accessible Data Curation and Analytics for International-Scale Citizen\n  Science Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Covid Symptom Study, a smartphone-based surveillance study on COVID-19\nsymptoms in the population, is an exemplar of big data citizen science. Over\n4.7 million participants and 189 million unique assessments have been logged\nsince its introduction in March 2020. The success of the Covid Symptom Study\ncreates technical challenges around effective data curation for two reasons.\nFirstly, the scale of the dataset means that it can no longer be easily\nprocessed using standard software on commodity hardware. Secondly, the size of\nthe research group means that replicability and consistency of key analytics\nused across multiple publications becomes an issue. We present ExeTera, an open\nsource data curation software designed to address scalability challenges and to\nenable reproducible research across an international research group for\ndatasets such as the Covid Symptom Study dataset.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 10:17:32 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 14:58:58 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Murray", "Benjamin", ""], ["Kerfoot", "Eric", ""], ["Graham", "Mark S.", ""], ["Sudre", "Carole H.", ""], ["Molteni", "Erika", ""], ["Canas", "Liane S.", ""], ["Antonelli", "Michela", ""], ["Klaser", "Kerstin", ""], ["Visconti", "Alessia", ""], ["Chan", "Andrew T.", ""], ["Franks", "Paul W.", ""], ["Davies", "Richard", ""], ["Wolf", "Jonathan", ""], ["Spector", "Tim", ""], ["Steves", "Claire J.", ""], ["Modat", "Marc", ""], ["Ourselin", "Sebastien", ""]]}, {"id": "2011.00944", "submitter": "Yan Zhang", "authors": "Yan Zhang, Ivor W. Tsang, Hongzhi Yin, Guowu Yang, Defu Lian, and\n  Jingjing Li", "title": "Deep Pairwise Hashing for Cold-start Recommendation", "comments": "13 pages, 8 figures, 6 tables, submitted to TKDE", "journal-ref": "2020", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation efficiency and data sparsity problems have been regarded as\ntwo challenges of improving performance for online recommendation. Most of the\nprevious related work focus on improving recommendation accuracy instead of\nefficiency. In this paper, we propose a Deep Pairwise Hashing (DPH) to map\nusers and items to binary vectors in Hamming space, where a user's preference\nfor an item can be efficiently calculated by Hamming distance, which\nsignificantly improves the efficiency of online recommendation. To alleviate\ndata sparsity and cold-start problems, the user-item interactive information\nand item content information are unified to learn effective representations of\nitems and users. Specifically, we first pre-train robust item representation\nfrom item content data by a Denoising Auto-encoder instead of other\ndeterministic deep learning frameworks; then we finetune the entire framework\nby adding a pairwise loss objective with discrete constraints; moreover, DPH\naims to minimize a pairwise ranking loss that is consistent with the ultimate\ngoal of recommendation. Finally, we adopt the alternating optimization method\nto optimize the proposed model with discrete constraints. Extensive experiments\non three different datasets show that DPH can significantly advance the\nstate-of-the-art frameworks regarding data sparsity and item cold-start\nrecommendation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 12:55:45 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhang", "Yan", ""], ["Tsang", "Ivor W.", ""], ["Yin", "Hongzhi", ""], ["Yang", "Guowu", ""], ["Lian", "Defu", ""], ["Li", "Jingjing", ""]]}, {"id": "2011.00953", "submitter": "Yan Zhang", "authors": "Yan Zhang, Ivor W. Tsang, Lixin Duan", "title": "Collaborative Generative Hashing for Marketing and Fast Cold-start\n  Recommendation", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cold-start has being a critical issue in recommender systems with the\nexplosion of data in e-commerce. Most existing studies proposed to alleviate\nthe cold-start problem are also known as hybrid recommender systems that learn\nrepresentations of users and items by combining user-item interactive and\nuser/item content information. However, previous hybrid methods regularly\nsuffered poor efficiency bottlenecking in online recommendations with\nlarge-scale items, because they were designed to project users and items into\ncontinuous latent space where the online recommendation is expensive. To this\nend, we propose a collaborative generated hashing (CGH) framework to improve\nthe efficiency by denoting users and items as binary codes, then fast hashing\nsearch techniques can be used to speed up the online recommendation. In\naddition, the proposed CGH can generate potential users or items for marketing\napplication where the generative network is designed with the principle of\nMinimum Description Length (MDL), which is used to learn compact and\ninformative binary codes. Extensive experiments on two public datasets show the\nadvantages for recommendations in various settings over competing baselines and\nanalyze its feasibility in marketing application.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 13:15:11 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhang", "Yan", ""], ["Tsang", "Ivor W.", ""], ["Duan", "Lixin", ""]]}, {"id": "2011.00956", "submitter": "Binbin Jin", "authors": "Binbin Jin, Defu Lian, Zheng Liu, Qi Liu, Jianhui Ma, Xing Xie, Enhong\n  Chen", "title": "Sampling-Decomposable Generative Adversarial Recommender", "comments": "Thirty-fourth Conference on Neural Information Processing Systems\n  (NeurIPS2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation techniques are important approaches for alleviating\ninformation overload. Being often trained on implicit user feedback, many\nrecommenders suffer from the sparsity challenge due to the lack of explicitly\nnegative samples. The GAN-style recommenders (i.e., IRGAN) addresses the\nchallenge by learning a generator and a discriminator adversarially, such that\nthe generator produces increasingly difficult samples for the discriminator to\naccelerate optimizing the discrimination objective. However, producing samples\nfrom the generator is very time-consuming, and our empirical study shows that\nthe discriminator performs poor in top-k item recommendation. To this end, a\ntheoretical analysis is made for the GAN-style algorithms, showing that the\ngenerator of limit capacity is diverged from the optimal generator. This may\ninterpret the limitation of discriminator's performance. Based on these\nfindings, we propose a Sampling-Decomposable Generative Adversarial Recommender\n(SD-GAR). In the framework, the divergence between some generator and the\noptimum is compensated by self-normalized importance sampling; the efficiency\nof sample generation is improved with a sampling-decomposable generator, such\nthat each sample can be generated in O(1) with the Vose-Alias method.\nInterestingly, due to decomposability of sampling, the generator can be\noptimized with the closed-form solutions in an alternating manner, being\ndifferent from policy gradient in the GAN-style algorithms. We extensively\nevaluate the proposed algorithm with five real-world recommendation datasets.\nThe results show that SD-GAR outperforms IRGAN by 12.4% and the SOTA\nrecommender by 10% on average. Moreover, discriminator training can be 20x\nfaster on the dataset with more than 120K items.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 13:19:10 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Jin", "Binbin", ""], ["Lian", "Defu", ""], ["Liu", "Zheng", ""], ["Liu", "Qi", ""], ["Ma", "Jianhui", ""], ["Xie", "Xing", ""], ["Chen", "Enhong", ""]]}, {"id": "2011.01018", "submitter": "Mathilde Brousmiche", "authors": "Mathilde Brousmiche and St\\'ephane Dupont and Jean Rouat", "title": "AVECL-UMONS database for audio-visual event classification and\n  localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the AVECL-UMons dataset for audio-visual event classification\nand localization in the context of office environments. The audio-visual\ndataset is composed of 11 event classes recorded at several realistic positions\nin two different rooms. Two types of sequences are recorded according to the\nnumber of events in the sequence. The dataset comprises 2662 unilabel sequences\nand 2724 multilabel sequences corresponding to a total of 5.24 hours. The\ndataset is publicly accessible online :\nhttps://zenodo.org/record/3965492#.X09wsobgrCI.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 14:26:02 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Brousmiche", "Mathilde", ""], ["Dupont", "St\u00e9phane", ""], ["Rouat", "Jean", ""]]}, {"id": "2011.01035", "submitter": "Akshar Nair", "authors": "Nikhil Fernandes, Alexandra Gkolia, Nicolas Pizzo, James Davenport,\n  Akshar Nair", "title": "Unification of HDP and LDA Models for Optimal Topic Clustering of\n  Subject Specific Question Banks", "comments": "8 pages, 5 figures, Submitted to EAAI21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an increasingly popular trend in Universities for curriculum\ntransformation to make teaching more interactive and suitable for online\ncourses. An increase in the popularity of online courses would result in an\nincrease in the number of course-related queries for academics. This, coupled\nwith the fact that if lectures were delivered in a video on demand format,\nthere would be no fixed time where the majority of students could ask\nquestions. When questions are asked in a lecture there is a negligible chance\nof having similar questions repeatedly, but asynchronously this is more likely.\nIn order to reduce the time spent on answering each individual question,\nclustering them is an ideal choice. There are different unsupervised models fit\nfor text clustering, of which the Latent Dirichlet Allocation model is the most\ncommonly used. We use the Hierarchical Dirichlet Process to determine an\noptimal topic number input for our LDA model runs. Due to the probabilistic\nnature of these topic models, the outputs of them vary for different runs. The\ngeneral trend we found is that not all the topics were being used for\nclustering on the first run of the LDA model, which results in a less effective\nclustering. To tackle probabilistic output, we recursively use the LDA model on\nthe effective topics being used until we obtain an efficiency ratio of 1.\nThrough our experimental results we also establish a reasoning on how Zeno's\nparadox is avoided.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 18:21:20 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Fernandes", "Nikhil", ""], ["Gkolia", "Alexandra", ""], ["Pizzo", "Nicolas", ""], ["Davenport", "James", ""], ["Nair", "Akshar", ""]]}, {"id": "2011.01113", "submitter": "Mhd Wesam Al-Nabki", "authors": "Mhd Wesam Al-Nabki, Eduardo Fidalgo, Enrique Alegre, Roc\\'io\n  Alaiz-Rodr\\'iguez", "title": "Short Text Classification Approach to Identify Child Sexual Exploitation\n  Material", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Producing or sharing Child Sexual Exploitation Material (CSEM) is a serious\ncrime fought vigorously by Law Enforcement Agencies (LEAs). When an LEA seizes\na computer from a potential producer or consumer of CSEM, they need to analyze\nthe suspect's hard disk's files looking for pieces of evidence. However, a\nmanual inspection of the file content looking for CSEM is a time-consuming\ntask. In most cases, it is unfeasible in the amount of time available for the\nSpanish police using a search warrant. Instead of analyzing its content,\nanother approach that can be used to speed up the process is to identify CSEM\nby analyzing the file names and their absolute paths. The main challenge for\nthis task lies behind dealing with short text distorted deliberately by the\nowners of this material using obfuscated words and user-defined naming\npatterns. This paper presents and compares two approaches based on short text\nclassification to identify CSEM files. The first one employs two independent\nsupervised classifiers, one for the file name and the other for the path, and\ntheir outputs are later on fused into a single score. Conversely, the second\napproach uses only the file name classifier to iterate over the file's absolute\npath. Both approaches operate at the character n-grams level, while binary and\northographic features enrich the file name representation, and a binary\nLogistic Regression model is used for classification. The presented file\nclassifier achieved an average class recall of 0.98. This solution could be\nintegrated into forensic tools and services to support Law Enforcement Agencies\nto identify CSEM without tackling every file's visual content, which is\ncomputationally much more highly demanding.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 09:37:16 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 09:39:29 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Al-Nabki", "Mhd Wesam", ""], ["Fidalgo", "Eduardo", ""], ["Alegre", "Enrique", ""], ["Alaiz-Rodr\u00edguez", "Roc\u00edo", ""]]}, {"id": "2011.01393", "submitter": "Xu Chen", "authors": "Yunpeng Weng and Xu Chen and Liang Chen and Wei Liu", "title": "GAIN: Graph Attention & Interaction Network for Inductive\n  Semi-Supervised Learning over Large-scale Graphs", "comments": "Accepted by IEEE Transactions on Knowledge and Data Engineering\n  (TKDE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have led to state-of-the-art performance on a\nvariety of machine learning tasks such as recommendation, node classification\nand link prediction. Graph neural network models generate node embeddings by\nmerging nodes features with the aggregated neighboring nodes information. Most\nexisting GNN models exploit a single type of aggregator (e.g., mean-pooling) to\naggregate neighboring nodes information, and then add or concatenate the output\nof aggregator to the current representation vector of the center node. However,\nusing only a single type of aggregator is difficult to capture the different\naspects of neighboring information and the simple addition or concatenation\nupdate methods limit the expressive capability of GNNs. Not only that, existing\nsupervised or semi-supervised GNN models are trained based on the loss function\nof the node label, which leads to the neglect of graph structure information.\nIn this paper, we propose a novel graph neural network architecture, Graph\nAttention \\& Interaction Network (GAIN), for inductive learning on graphs.\nUnlike the previous GNN models that only utilize a single type of aggregation\nmethod, we use multiple types of aggregators to gather neighboring information\nin different aspects and integrate the outputs of these aggregators through the\naggregator-level attention mechanism. Furthermore, we design a graph\nregularized loss to better capture the topological relationship of the nodes in\nthe graph. Additionally, we first present the concept of graph feature\ninteraction and propose a vector-wise explicit feature interaction mechanism to\nupdate the node embeddings. We conduct comprehensive experiments on two\nnode-classification benchmarks and a real-world financial news dataset. The\nexperiments demonstrate our GAIN model outperforms current state-of-the-art\nperformances on all the tasks.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 00:20:24 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Weng", "Yunpeng", ""], ["Chen", "Xu", ""], ["Chen", "Liang", ""], ["Liu", "Wei", ""]]}, {"id": "2011.01421", "submitter": "Md Tahmid Rahman Laskar", "authors": "Md Tahmid Rahman Laskar, Enamul Hoque, Jimmy Xiangji Huang", "title": "WSL-DS: Weakly Supervised Learning with Distant Supervision for Query\n  Focused Multi-Document Abstractive Summarization", "comments": "COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Query Focused Multi-Document Summarization (QF-MDS) task, a set of\ndocuments and a query are given where the goal is to generate a summary from\nthese documents based on the given query. However, one major challenge for this\ntask is the lack of availability of labeled training datasets. To overcome this\nissue, in this paper, we propose a novel weakly supervised learning approach\nvia utilizing distant supervision. In particular, we use datasets similar to\nthe target dataset as the training data where we leverage pre-trained sentence\nsimilarity models to generate the weak reference summary of each individual\ndocument in a document set from the multi-document gold reference summaries.\nThen, we iteratively train our summarization model on each single-document to\nalleviate the computational complexity issue that occurs while training neural\nsummarization models in multiple documents (i.e., long sequences) at once.\nExperimental results in Document Understanding Conferences (DUC) datasets show\nthat our proposed approach sets a new state-of-the-art result in terms of\nvarious evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 02:02:55 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Laskar", "Md Tahmid Rahman", ""], ["Hoque", "Enamul", ""], ["Huang", "Jimmy Xiangji", ""]]}, {"id": "2011.01453", "submitter": "Xue Jun Wang", "authors": "Xue Jun Wang, Maura R. Grossman, Seung Gyu Hyun", "title": "Participation in TREC 2020 COVID Track Using Continuous Active Learning", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our participation in all five rounds of the TREC 2020 COVID Track\n(TREC-COVID). The goal of TREC-COVID is to contribute to the response to the\nCOVID-19 pandemic by identifying answers to many pressing questions and\nbuilding infrastructure to improve search systems [8]. All five rounds of this\nTrack challenged participants to perform a classic ad-hoc search task on the\nnew data collection CORD-19. Our solution addressed this challenge by applying\nthe Continuous Active Learning model (CAL) and its variations. Our results\nshowed us to be amongst the top scoring manual runs and we remained competitive\nwithin all categories of submissions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 03:43:43 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wang", "Xue Jun", ""], ["Grossman", "Maura R.", ""], ["Hyun", "Seung Gyu", ""]]}, {"id": "2011.01504", "submitter": "Harsh Patel", "authors": "Harsh Patel", "title": "BioNerFlair: biomedical named entity recognition using flair embedding\n  and sequence tagger", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: The proliferation of Biomedical research articles has made the\ntask of information retrieval more important than ever. Scientists and\nResearchers are having difficulty in finding articles that contain information\nrelevant to them. Proper extraction of biomedical entities like Disease,\nDrug/chem, Species, Gene/protein, can considerably improve the filtering of\narticles resulting in better extraction of relevant information. Performance on\nBioNer benchmarks has progressively improved because of progression in\ntransformers-based models like BERT, XLNet, OpenAI, GPT2, etc. These models\ngive excellent results; however, they are computationally expensive and we can\nachieve better scores for domain-specific tasks using other contextual\nstring-based models and LSTM-CRF based sequence tagger. Results: We introduce\nBioNerFlair, a method to train models for biomedical named entity recognition\nusing Flair plus GloVe embeddings and Bidirectional LSTM-CRF based sequence\ntagger. With almost the same generic architecture widely used for named entity\nrecognition, BioNerFlair outperforms previous state-of-the-art models. I\nperformed experiments on 8 benchmarks datasets for biomedical named entity\nrecognition. Compared to current state-of-the-art models, BioNerFlair achieves\nthe best F1-score of 90.17 beyond 84.72 on the BioCreative II gene mention\n(BC2GM) corpus, best F1-score of 94.03 beyond 92.36 on the BioCreative IV\nchemical and drug (BC4CHEMD) corpus, best F1-score of 88.73 beyond 78.58 on the\nJNLPBA corpus, best F1-score of 91.1 beyond 89.71 on the NCBI disease corpus,\nbest F1-score of 85.48 beyond 78.98 on the Species-800 corpus, while near best\nresults was observed on BC5CDR-chem, BC3CDR-disease, and LINNAEUS corpus.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 06:46:45 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Patel", "Harsh", ""]]}, {"id": "2011.01580", "submitter": "Si Sun", "authors": "Chenyan Xiong, Zhenghao Liu, Si Sun, Zhuyun Dai, Kaitao Zhang, Shi Yu,\n  Zhiyuan Liu, Hoifung Poon, Jianfeng Gao and Paul Bennett", "title": "CMT in TREC-COVID Round 2: Mitigating the Generalization Gaps from Web\n  to Special Domain Search", "comments": "5 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural rankers based on deep pretrained language models (LMs) have been shown\nto improve many information retrieval benchmarks. However, these methods are\naffected by their the correlation between pretraining domain and target domain\nand rely on massive fine-tuning relevance labels. Directly applying pretraining\nmethods to specific domains may result in suboptimal search quality because\nspecific domains may have domain adaption problems, such as the COVID domain.\nThis paper presents a search system to alleviate the special domain adaption\nproblem. The system utilizes the domain-adaptive pretraining and few-shot\nlearning technologies to help neural rankers mitigate the domain discrepancy\nand label scarcity problems. Besides, we also integrate dense retrieval to\nalleviate traditional sparse retrieval's vocabulary mismatch obstacle. Our\nsystem performs the best among the non-manual runs in Round 2 of the TREC-COVID\ntask, which aims to retrieve useful information from scientific literature\nrelated to COVID-19. Our code is publicly available at\nhttps://github.com/thunlp/OpenMatch.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 09:10:48 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Xiong", "Chenyan", ""], ["Liu", "Zhenghao", ""], ["Sun", "Si", ""], ["Dai", "Zhuyun", ""], ["Zhang", "Kaitao", ""], ["Yu", "Shi", ""], ["Liu", "Zhiyuan", ""], ["Poon", "Hoifung", ""], ["Gao", "Jianfeng", ""], ["Bennett", "Paul", ""]]}, {"id": "2011.01637", "submitter": "Ant\\'onio S\\'a Pinto", "authors": "A. S\\'a Pinto, I. Domingues, and M. E. P. Davies", "title": "Shift If You Can: Counting and Visualising Correction Operations for\n  Beat Tracking Evaluation", "comments": "ISMIR 2020 Late Breaking/Demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this late-breaking abstract we propose a modified approach for beat\ntracking evaluation which poses the problem in terms of the effort required to\ntransform a sequence of beat detections such that they maximise the well-known\nF-measure calculation when compared to a sequence of ground truth annotations.\nCentral to our approach is the inclusion of a shifting operation conducted over\nan additional, larger, tolerance window, which can substitute the combination\nof insertions and deletions. We describe a straightforward calculation of\nannotation efficiency and combine this with an informative visualisation which\ncan be of use for the qualitative evaluation of beat tracking systems. We make\nour implementation and visualisation code freely available in a GitHub\nrepository.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 11:26:03 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Pinto", "A. S\u00e1", ""], ["Domingues", "I.", ""], ["Davies", "M. E. P.", ""]]}, {"id": "2011.01731", "submitter": "Shanlei Mu", "authors": "Wayne Xin Zhao, Shanlei Mu, Yupeng Hou, Zihan Lin, Kaiyuan Li, Yushuo\n  Chen, Yujie Lu, Hui Wang, Changxin Tian, Xingyu Pan, Yingqian Min, Zhichao\n  Feng, Xinyan Fan, Xu Chen, Pengfei Wang, Wendi Ji, Yaliang Li, Xiaoling Wang\n  and Ji-Rong Wen", "title": "RecBole: Towards a Unified, Comprehensive and Efficient Framework for\n  Recommendation Algorithms", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there are a large number of recommendation algorithms\nproposed in the literature, from traditional collaborative filtering to neural\nnetwork algorithms. However, the concerns about how to standardize open source\nimplementation of recommendation algorithms continually increase in the\nresearch community.\n  In the light of this challenge, we propose a unified, comprehensive and\nefficient recommender system library called RecBole, which provides a unified\nframework to develop and reproduce recommender systems for research purpose. In\nthis library, we implement 53 recommendation models on 27 benchmark datasets,\ncovering the categories of general recommendation, sequential recommendation,\ncontext-aware recommendation and knowledge-based recommendation. We implement\nthe RecBole library based on PyTorch, which is one of the most popular deep\nlearning frameworks. Our library is featured in many aspects, including general\nand extensible data structures, comprehensive benchmark models and datasets,\nefficient GPU-accelerated execution, and extensive and standard evaluation\nprotocols. We provide a series of auxiliary functions, tools, and scripts to\nfacilitate the use of this library, such as automatic parameter tuning and\nbreak-point resume. Such a framework is useful to standardize the\nimplementation and evaluation of recommender systems. The project and documents\nare released at https://recbole.io.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 14:26:55 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 10:42:19 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Zhao", "Wayne Xin", ""], ["Mu", "Shanlei", ""], ["Hou", "Yupeng", ""], ["Lin", "Zihan", ""], ["Li", "Kaiyuan", ""], ["Chen", "Yushuo", ""], ["Lu", "Yujie", ""], ["Wang", "Hui", ""], ["Tian", "Changxin", ""], ["Pan", "Xingyu", ""], ["Min", "Yingqian", ""], ["Feng", "Zhichao", ""], ["Fan", "Xinyan", ""], ["Chen", "Xu", ""], ["Wang", "Pengfei", ""], ["Ji", "Wendi", ""], ["Li", "Yaliang", ""], ["Wang", "Xiaoling", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2011.01881", "submitter": "Kiran Sharma Dr.", "authors": "Kiran Sharma and Parul Khurana", "title": "Growth and dynamics of Econophysics: A bibliometric and network analysis", "comments": "14 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitization of publications, advancement in communication technology, and\nthe availability of bibliographic data have made it easier for the researchers\nto study the growth and dynamics of any discipline. We present a study on\n\"Econophysics\" metadata extracted from the Web of Science managed by the\nClarivate Analytics from 2000-2019. The study highlights the growth and\ndynamics of the discipline by measures of a number of publications, citations\non publications, other disciplines contribution, institutions participation,\ncountry-wise spread, etc. We investigate the impact of self-citations on\ncitations with every five-year interval. Also, we find the contribution of\nother disciplines by analyzing the cited references. Results emerged from\nmicro, meso and macro-level analysis of collaborations show that the\ndistributions among authors collaboration and affiliations of authors follow a\npower law. Thus, very few authors keep producing most of the papers and are\nfrom a few institutions. We find that China is leading in the production of a\nnumber of authors and a number of papers; however, shares more of national\ncollaboration rather than international, whereas the USA shares more\ninternational collaboration. Finally, we demonstrate the evolution of the\nauthor's collaborations and affiliations networks from 2000-2019. Overall the\nanalysis reveals the \"small-world\" property of the network with average path\nlength 5. As a consequence of our analysis, this study can serve as in-depth\nknowledge to understand the growth and dynamics of the Econophysics network\nboth qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 17:57:51 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Sharma", "Kiran", ""], ["Khurana", "Parul", ""]]}, {"id": "2011.02066", "submitter": "Yunhe Feng", "authors": "Yunhe Feng, Daniel Saelid, Ke Li, Ruoyuan Gao, Chirag Shah", "title": "University of Washington at TREC 2020 Fairness Ranking Track", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  InfoSeeking Lab's FATE (Fairness Accountability Transparency Ethics) group at\nUniversity of Washington participated in 2020 TREC Fairness Ranking Track. This\nreport describes that track, assigned data and tasks, our group definitions,\nand our results. Our approach to bringing fairness in retrieval and re-ranking\ntasks with Semantic Scholar data was to extract various dimensions of author\nidentity. These dimensions included gender and location. We developed modules\nfor these extractions in a way that allowed us to plug them in for either of\nthe tasks as needed. After trying different combinations of relative weights\nassigned to relevance, gender, and location information, we chose five runs for\nretrieval and five runs for re-ranking tasks. The results showed that our runs\nperformed below par for re-ranking task, but above average for retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 23:40:54 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 06:55:24 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Feng", "Yunhe", ""], ["Saelid", "Daniel", ""], ["Li", "Ke", ""], ["Gao", "Ruoyuan", ""], ["Shah", "Chirag", ""]]}, {"id": "2011.02084", "submitter": "Michael Lui", "authors": "Michael Lui, Yavuz Yetim, \\\"Ozg\\\"ur \\\"Ozkan, Zhuoran Zhao, Shin-Yeh\n  Tsai, Carole-Jean Wu, and Mark Hempstead", "title": "Understanding Capacity-Driven Scale-Out Neural Recommendation Inference", "comments": "16 pages + references, 16 Figures. Additive revision to clarify\n  distinction between this work and other DLRM-like models and add\n  Acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning recommendation models have grown to the terabyte scale.\nTraditional serving schemes--that load entire models to a single server--are\nunable to support this scale. One approach to support this scale is with\ndistributed serving, or distributed inference, which divides the memory\nrequirements of a single large model across multiple servers.\n  This work is a first-step for the systems research community to develop novel\nmodel-serving solutions, given the huge system design space. Large-scale deep\nrecommender systems are a novel workload and vital to study, as they consume up\nto 79% of all inference cycles in the data center. To that end, this work\ndescribes and characterizes scale-out deep learning recommendation inference\nusing data-center serving infrastructure. This work specifically explores\nlatency-bounded inference systems, compared to the throughput-oriented training\nsystems of other recent works. We find that the latency and compute overheads\nof distributed inference are largely a result of a model's static embedding\ntable distribution and sparsity of input inference requests. We further\nevaluate three embedding table mapping strategies of three DLRM-like models and\nspecify challenging design trade-offs in terms of end-to-end latency, compute\noverhead, and resource efficiency. Overall, we observe only a marginal latency\noverhead when the data-center scale recommendation models are served with the\ndistributed inference manner--P99 latency is increased by only 1% in the best\ncase configuration. The latency overheads are largely a result of the commodity\ninfrastructure used and the sparsity of embedding tables. Even more\nencouragingly, we also show how distributed inference can account for\nefficiency improvements in data-center scale recommendation serving.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 00:51:40 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 16:31:05 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Lui", "Michael", ""], ["Yetim", "Yavuz", ""], ["\u00d6zkan", "\u00d6zg\u00fcr", ""], ["Zhao", "Zhuoran", ""], ["Tsai", "Shin-Yeh", ""], ["Wu", "Carole-Jean", ""], ["Hempstead", "Mark", ""]]}, {"id": "2011.02100", "submitter": "Lin Meng", "authors": "Zhiwei Liu, Lin Meng, Fei Jiang, Jiawei Zhang, Philip S. Yu", "title": "Deoscillated Graph Collaborative Filtering", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Filtering (CF) signals are crucial for a Recommender\nSystem~(RS) model to learn user and item embeddings. High-order information can\nalleviate the cold-start issue of CF-based methods, which is modelled through\npropagating the information over the user-item bipartite graph. Recent Graph\nNeural Networks~(GNNs) propose to stack multiple aggregation layers to\npropagate high-order signals. However, the oscillation problem, varying\nlocality of bipartite graph, and the fix propagation pattern spoil the ability\nof multi-layer structure to propagate information. The oscillation problem\nresults from the bipartite structure, as the information from users only\npropagates to items. Besides oscillation problem, varying locality suggests the\ndensity of nodes should be considered in the propagation process. Moreover, the\nlayer-fixed propagation pattern introduces redundant information between\nlayers. In order to tackle these problems, we propose a new RS model, named as\n\\textbf{D}eoscillated \\textbf{G}raph \\textbf{C}ollaborative\n\\textbf{F}iltering~(DGCF). We introduce cross-hop propagation layers in it to\nbreak the bipartite propagating structure, thus resolving the oscillation\nproblem. Additionally, we design innovative locality-adaptive layers which\nadaptively propagate information. Stacking multiple cross-hop propagation\nlayers and locality layers constitutes the DGCF model, which models high-order\nCF signals adaptively to the locality of nodes and layers. Extensive\nexperiments on real-world datasets show the effectiveness of DGCF. Detailed\nanalyses indicate that DGCF solves oscillation problem, adaptively learns local\nfactor, and has layer-wise propagation pattern. Our code is available online at\nhttps://github.com/JimLiu96/DeosciRec.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 02:26:53 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 19:30:24 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Liu", "Zhiwei", ""], ["Meng", "Lin", ""], ["Jiang", "Fei", ""], ["Zhang", "Jiawei", ""], ["Yu", "Philip S.", ""]]}, {"id": "2011.02177", "submitter": "Michael Fromm", "authors": "Michael Fromm, Max Berrendorf, Sandra Obermeier, Thomas Seidl, Evgeniy\n  Faerman", "title": "Diversity Aware Relevance Learning for Argument Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we focus on the problem of retrieving relevant arguments for a\nquery claim covering diverse aspects. State-of-the-art methods rely on explicit\nmappings between claims and premises, and thus are unable to utilize large\navailable collections of premises without laborious and costly manual\nannotation. Their diversity approach relies on removing duplicates via\nclustering which does not directly ensure that the selected premises cover all\naspects. This work introduces a new multi-step approach for the argument\nretrieval problem. Rather than relying on ground-truth assignments, our\napproach employs a machine learning model to capture semantic relationships\nbetween arguments. Beyond that, it aims to cover diverse facets of the query,\ninstead of trying to identify duplicates explicitly. Our empirical evaluation\ndemonstrates that our approach leads to a significant improvement in the\nargument retrieval task even though it requires less data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 08:37:44 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 12:29:18 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 15:02:40 GMT"}, {"version": "v4", "created": "Wed, 17 Mar 2021 11:25:56 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Fromm", "Michael", ""], ["Berrendorf", "Max", ""], ["Obermeier", "Sandra", ""], ["Seidl", "Thomas", ""], ["Faerman", "Evgeniy", ""]]}, {"id": "2011.02248", "submitter": "Xiaocong Chen", "authors": "Xiaocong Chen and Lina Yao and Aixin Sun and Xianzhi Wang and Xiwei Xu\n  and Liming Zhu", "title": "Generative Inverse Deep Reinforcement Learning for Online Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning enables an agent to capture user's interest\nthrough interactions with the environment dynamically. It has attracted great\ninterest in the recommendation research. Deep reinforcement learning uses a\nreward function to learn user's interest and to control the learning process.\nHowever, most reward functions are manually designed; they are either\nunrealistic or imprecise to reflect the high variety, dimensionality, and\nnon-linearity properties of the recommendation problem. That makes it difficult\nfor the agent to learn an optimal policy to generate the most satisfactory\nrecommendations. To address the above issue, we propose a novel generative\ninverse reinforcement learning approach, namely InvRec, which extracts the\nreward function from user's behaviors automatically, for online recommendation.\nWe conduct experiments on an online platform, VirtualTB, and compare with\nseveral state-of-the-art methods to demonstrate the feasibility and\neffectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 12:12:25 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Chen", "Xiaocong", ""], ["Yao", "Lina", ""], ["Sun", "Aixin", ""], ["Wang", "Xianzhi", ""], ["Xu", "Xiwei", ""], ["Zhu", "Liming", ""]]}, {"id": "2011.02260", "submitter": "Fei Sun", "authors": "Shiwen Wu, Fei Sun, Wentao Zhang, Bin Cui", "title": "Graph Neural Networks in Recommender Systems: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Owing to the superiority of GNN in learning on graph data and its efficacy in\ncapturing collaborative signals and sequential patterns, utilizing GNN\ntechniques in recommender systems has gain increasing interests in academia and\nindustry. In this survey, we provide a comprehensive review of the most recent\nworks on GNN-based recommender systems. We proposed a classification scheme for\norganizing existing works. For each category, we briefly clarify the main\nissues, and detail the corresponding strategies adopted by the representative\nmodels. We also discuss the advantages and limitations of the existing\nstrategies. Furthermore, we suggest several promising directions for future\nresearches. We hope this survey can provide readers with a general\nunderstanding of the recent progress in this field, and shed some light on\nfuture developments.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 12:57:47 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 11:41:15 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wu", "Shiwen", ""], ["Sun", "Fei", ""], ["Zhang", "Wentao", ""], ["Cui", "Bin", ""]]}, {"id": "2011.02273", "submitter": "Sahan Bulathwela", "authors": "Sahan Bulathwela and Maria Perez-Ortiz and Emine Yilmaz and John\n  Shawe-Taylor", "title": "VLEngagement: A Dataset of Scientific Video Lectures for Evaluating\n  Population-based Engagement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the emergence of e-learning and personalised education, the production\nand distribution of digital educational resources have boomed. Video lectures\nhave now become one of the primary modalities to impart knowledge to masses in\nthe current digital age. The rapid creation of video lecture content challenges\nthe currently established human-centred moderation and quality assurance\npipeline, demanding for more efficient, scalable and automatic solutions for\nmanaging learning resources. Although a few datasets related to engagement with\neducational videos exist, there is still an important need for data and\nresearch aimed at understanding learner engagement with scientific video\nlectures. This paper introduces VLEngagement, a novel dataset that consists of\ncontent-based and video-specific features extracted from publicly available\nscientific video lectures and several metrics related to user engagement. We\nintroduce several novel tasks related to predicting and understanding\ncontext-agnostic engagement in video lectures, providing preliminary baselines.\nThis is the largest and most diverse publicly available dataset to our\nknowledge that deals with such tasks. The extraction of Wikipedia topic-based\nfeatures also allows associating more sophisticated Wikipedia based features to\nthe dataset to improve the performance in these tasks. The dataset, helper\ntools and example code snippets are available publicly at\nhttps://github.com/sahanbull/context-agnostic-engagement\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 14:20:19 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Bulathwela", "Sahan", ""], ["Perez-Ortiz", "Maria", ""], ["Yilmaz", "Emine", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "2011.02426", "submitter": "Subramanyam Natarajan", "authors": "Arvind Srinivasan, Aprameya Bharadwaj, Aveek Saha, Subramanyam\n  Natarajan", "title": "Graph Based Temporal Aggregation for Video Retrieval", "comments": "6 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale video retrieval is a field of study with a lot of ongoing\nresearch. Most of the work in the field is on video retrieval through text\nqueries using techniques such as VSE++. However, there is little research done\non video retrieval through image queries, and the work that has been done in\nthis field either uses image queries from within the video dataset or iterates\nthrough videos frame by frame. These approaches are not generalized for queries\nfrom outside the dataset and do not scale well for large video datasets. To\novercome these issues, we propose a new approach for video retrieval through\nimage queries where an undirected graph is constructed from the combined set of\nframes from all videos to be searched. The node features of this graph are used\nin the task of video retrieval. Experimentation is done on the MSR-VTT dataset\nby using query images from outside the dataset. To evaluate this novel approach\nP@5, P@10 and P@20 metrics are calculated. Two different ResNet models namely,\nResNet-152 and ResNet-50 are used in this study.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 17:23:14 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Srinivasan", "Arvind", ""], ["Bharadwaj", "Aprameya", ""], ["Saha", "Aveek", ""], ["Natarajan", "Subramanyam", ""]]}, {"id": "2011.02552", "submitter": "Fabrizio Sebastiani", "authors": "Alejandro Moreo and Fabrizio Sebastiani", "title": "Re-Assessing the \"Classify and Count\" Quantification Method", "comments": "This is the final version of the paper, identical to the one that is\n  going to appear on the Proceedings of the 43rd European Conference on\n  Information Retrieval (ECIR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Learning to quantify (a.k.a.\\ quantification) is a task concerned with\ntraining unbiased estimators of class prevalence via supervised learning. This\ntask originated with the observation that \"Classify and Count\" (CC), the\ntrivial method of obtaining class prevalence estimates, is often a biased\nestimator, and thus delivers suboptimal quantification accuracy; following this\nobservation, several methods for learning to quantify have been proposed that\nhave been shown to outperform CC. In this work we contend that previous works\nhave failed to use properly optimised versions of CC. We thus reassess the real\nmerits of CC (and its variants), and argue that, while still inferior to some\ncutting-edge methods, they deliver near-state-of-the-art accuracy once (a)\nhyperparameter optimisation is performed, and (b) this optimisation is\nperformed by using a true quantification loss instead of a standard\nclassification-based loss. Experiments on three publicly available binary\nsentiment classification datasets support these conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 21:47:39 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 17:32:23 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Moreo", "Alejandro", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "2011.02602", "submitter": "Chin-Chia Michael Yeh", "authors": "Chin-Chia Michael Yeh, Zhongfang Zhuang, Yan Zheng, Liang Wang,\n  Junpeng Wang, Wei Zhang", "title": "Merchant Category Identification Using Credit Card Transactions", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital payment volume has proliferated in recent years with the rapid growth\nof small businesses and online shops. When processing these digital\ntransactions, recognizing each merchant's real identity (i.e., business type)\nis vital to ensure the integrity of payment processing systems. Conventionally,\nthis problem is formulated as a time series classification problem solely using\nthe merchant transaction history. However, with the large scale of the data,\nand changing behaviors of merchants and consumers over time, it is extremely\nchallenging to achieve satisfying performance from off-the-shelf classification\nmethods. In this work, we approach this problem from a multi-modal learning\nperspective, where we use not only the merchant time series data but also the\ninformation of merchant-merchant relationship (i.e., affinity) to verify the\nself-reported business type (i.e., merchant category) of a given merchant.\nSpecifically, we design two individual encoders, where one is responsible for\nencoding temporal information and the other is responsible for affinity\ninformation, and a mechanism to fuse the outputs of the two encoders to\naccomplish the identification task. Our experiments on real-world credit card\ntransaction data between 71,668 merchants and 433,772,755 customers have\ndemonstrated the effectiveness and efficiency of the proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 01:21:30 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Yeh", "Chin-Chia Michael", ""], ["Zhuang", "Zhongfang", ""], ["Zheng", "Yan", ""], ["Wang", "Liang", ""], ["Wang", "Junpeng", ""], ["Zhang", "Wei", ""]]}, {"id": "2011.02665", "submitter": "Tony Gracious", "authors": "Tony Gracious, Ambedkar Dukkipati", "title": "Adversarial Context Aware Network Embeddings for Textual Networks", "comments": "8 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning of textual networks poses a significant challenge as\nit involves capturing amalgamated information from two modalities: (i)\nunderlying network structure, and (ii) node textual attributes. For this, most\nexisting approaches learn embeddings of text and network structure by enforcing\nembeddings of connected nodes to be similar. Then for achieving a modality\nfusion they use the similarities between text embedding of a node with the\nstructure embedding of its connected node and vice versa. This implies that\nthese approaches require edge information for learning embeddings and they\ncannot learn embeddings of unseen nodes. In this paper we propose an approach\nthat achieves both modality fusion and the capability to learn embeddings of\nunseen nodes. The main feature of our model is that it uses an adversarial\nmechanism between text embedding based discriminator, and structure embedding\nbased generator to learn efficient representations. Then for learning\nembeddings of unseen nodes, we use the supervision provided by the text\nembedding based discriminator. In addition this, we propose a novel\narchitecture for learning text embedding that can combine both mutual attention\nand topological attention mechanism, which give more flexible text embeddings.\nThrough extensive experiments on real-world datasets, we demonstrate that our\nmodel makes substantial gains over several state-of-the-art benchmarks. In\ncomparison with previous state-of-the-art, it gives up to 7% improvement in\nperformance in predicting links among nodes seen in the training and up to 12%\nimprovement in performance in predicting links involving nodes not seen in\ntraining. Further, in the node classification task, it gives up to 2%\nimprovement in performance.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 05:20:01 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Gracious", "Tony", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "2011.02690", "submitter": "Jan Botha", "authors": "Jan A. Botha, Zifei Shan, Daniel Gillick", "title": "Entity Linking in 100 Languages", "comments": "13 pages, 3 figures, 8 tables; published at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new formulation for multilingual entity linking, where\nlanguage-specific mentions resolve to a language-agnostic Knowledge Base. We\ntrain a dual encoder in this new setting, building on prior work with improved\nfeature representation, negative mining, and an auxiliary entity-pairing task,\nto obtain a single entity retrieval model that covers 100+ languages and 20\nmillion entities. The model outperforms state-of-the-art results from a far\nmore limited cross-lingual linking task. Rare entities and low-resource\nlanguages pose challenges at this large-scale, so we advocate for an increased\nfocus on zero- and few-shot evaluation. To this end, we provide Mewsli-9, a\nlarge new multilingual dataset (http://goo.gle/mewsli-dataset) matched to our\nsetting, and show how frequency-based analysis provided key insights for our\nmodel and training enhancements.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 07:28:35 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Botha", "Jan A.", ""], ["Shan", "Zifei", ""], ["Gillick", "Daniel", ""]]}, {"id": "2011.03080", "submitter": "Momchil Hardalov", "authors": "Momchil Hardalov, Todor Mihaylov, Dimitrina Zlatkova, Yoan Dinkov,\n  Ivan Koychev, Preslav Nakov", "title": "EXAMS: A Multi-Subject High School Examinations Dataset for\n  Cross-Lingual and Multilingual Question Answering", "comments": "EMNLP 2020, 17 pages, 6 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose EXAMS -- a new benchmark dataset for cross-lingual and\nmultilingual question answering for high school examinations. We collected more\nthan 24,000 high-quality high school exam questions in 16 languages, covering 8\nlanguage families and 24 school subjects from Natural Sciences and Social\nSciences, among others.\n  EXAMS offers a fine-grained evaluation framework across multiple languages\nand subjects, which allows precise analysis and comparison of various models.\nWe perform various experiments with existing top-performing multilingual\npre-trained models and we show that EXAMS offers multiple challenges that\nrequire multilingual knowledge and reasoning in multiple domains. We hope that\nEXAMS will enable researchers to explore challenging reasoning and knowledge\ntransfer methods and pre-trained models for school question answering in\nvarious languages which was not possible before. The data, code, pre-trained\nmodels, and evaluation are available at https://github.com/mhardalov/exams-qa.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 20:06:50 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Hardalov", "Momchil", ""], ["Mihaylov", "Todor", ""], ["Zlatkova", "Dimitrina", ""], ["Dinkov", "Yoan", ""], ["Koychev", "Ivan", ""], ["Nakov", "Preslav", ""]]}, {"id": "2011.03123", "submitter": "Alberto Calderone Dr.", "authors": "Alberto Calderone", "title": "PubSqueezer: A Text-Mining Web Tool to Transform Unstructured Documents\n  into Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The amount of scientific papers published every day is daunting and\nconstantly increasing. Keeping up with literature represents a challenge. If\none wants to start exploring new topics it is hard to have a big picture\nwithout reading lots of articles. Furthermore, as one reads through literature,\nmaking mental connections is crucial to ask new questions which might lead to\ndiscoveries. In this work, I present a web tool which uses a Text Mining\nstrategy to transform large collections of unstructured biomedical articles\ninto structured data. Generated results give a quick overview on complex topics\nwhich can possibly suggest not explicitly reported information. In particular,\nI show two Data Science analyses. First, I present a literature based rare\ndiseases network build using this tool in the hope that it will help clarify\nsome aspects of these less popular pathologies. Secondly, I show how a\nliterature based analysis conducted with PubSqueezer results allows to describe\nknown facts about SARS-CoV-2. In one sentence, data generated with PubSqueezer\nmake it easy to use scientific literate in any computational analysis such as\nmachine learning, natural language processing etc.\n  Availability: http://www.pubsqueezer.com\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 22:23:18 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 07:51:35 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Calderone", "Alberto", ""]]}, {"id": "2011.03228", "submitter": "Tomasz Dwojak", "authors": "Tomasz Dwojak, Micha{\\l} Pietruszka, {\\L}ukasz Borchmann, Jakub\n  Ch{\\l}\\k{e}dowski, Filip Grali\\'nski", "title": "From Dataset Recycling to Multi-Property Extraction and Beyond", "comments": "Accepted at CoNLL 2020; this article supersedes arXiv: 2006.08281", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates various Transformer architectures on the WikiReading\nInformation Extraction and Machine Reading Comprehension dataset. The proposed\ndual-source model outperforms the current state-of-the-art by a large margin.\nNext, we introduce WikiReading Recycled-a newly developed public dataset and\nthe task of multiple property extraction. It uses the same data as WikiReading\nbut does not inherit its predecessor's identified disadvantages. In addition,\nwe provide a human-annotated test set with diagnostic subsets for a detailed\nanalysis of model performance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 08:22:12 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Dwojak", "Tomasz", ""], ["Pietruszka", "Micha\u0142", ""], ["Borchmann", "\u0141ukasz", ""], ["Ch\u0142\u0119dowski", "Jakub", ""], ["Grali\u0144ski", "Filip", ""]]}, {"id": "2011.03327", "submitter": "Parth Patwa", "authors": "Parth Patwa, Shivam Sharma, Srinivas Pykl, Vineeth Guptha, Gitanjali\n  Kumari, Md Shad Akhtar, Asif Ekbal, Amitava Das, Tanmoy Chakraborty", "title": "Fighting an Infodemic: COVID-19 Fake News Dataset", "comments": "Published at CONSTRAINT-2021, Collocated with AAAI-2021", "journal-ref": null, "doi": "10.1007/978-3-030-73696-5_3", "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with COVID-19 pandemic we are also fighting an `infodemic'. Fake news\nand rumors are rampant on social media. Believing in rumors can cause\nsignificant harm. This is further exacerbated at the time of a pandemic. To\ntackle this, we curate and release a manually annotated dataset of 10,700\nsocial media posts and articles of real and fake news on COVID-19. We benchmark\nthe annotated dataset with four machine learning baselines - Decision Tree,\nLogistic Regression, Gradient Boost, and Support Vector Machine (SVM). We\nobtain the best performance of 93.46% F1-score with SVM. The data and code is\navailable at: https://github.com/parthpatwa/covid19-fake-news-dectection\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 13:09:37 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 04:46:19 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 12:11:21 GMT"}, {"version": "v4", "created": "Wed, 26 May 2021 15:38:55 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Patwa", "Parth", ""], ["Sharma", "Shivam", ""], ["Pykl", "Srinivas", ""], ["Guptha", "Vineeth", ""], ["Kumari", "Gitanjali", ""], ["Akhtar", "Md Shad", ""], ["Ekbal", "Asif", ""], ["Das", "Amitava", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "2011.03371", "submitter": "Samaneh Saadat", "authors": "Samaneh Saadat, Gita Sukthankar", "title": "Explaining Differences in Classes of Discrete Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there are many machine learning methods to classify and cluster\nsequences, they fail to explain what are the differences in groups of sequences\nthat make them distinguishable. Although in some cases having a black box model\nis sufficient, there is a need for increased explainability in research areas\nfocused on human behaviors. For example, psychologists are less interested in\nhaving a model that predicts human behavior with high accuracy and more\nconcerned with identifying differences between actions that lead to divergent\nhuman behavior. This paper presents techniques for understanding differences\nbetween classes of discrete sequences. Approaches introduced in this paper can\nbe utilized to interpret black box machine learning models on sequences. The\nfirst approach compares k-gram representations of sequences using the\nsilhouette score. The second method characterizes differences by analyzing the\ndistance matrix of subsequences. As a case study, we trained black box\nsupervised learning methods to classify sequences of GitHub teams and then\nutilized our sequence analysis techniques to measure and characterize\ndifferences between event sequences of teams with bots and teams without bots.\nIn our second case study, we classified Minecraft event sequences to infer\ntheir high-level actions and analyzed differences between low-level event\nsequences of actions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 14:13:30 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Saadat", "Samaneh", ""], ["Sukthankar", "Gita", ""]]}, {"id": "2011.03413", "submitter": "Mathias Wolfgang Jesse", "authors": "Mathias Jesse and Dietmar Jannach", "title": "Digital Nudging with Recommender Systems: Survey and Future Directions", "comments": null, "journal-ref": "JIn Computers in Human Behavior Reports 3, p. 100052 (2021)", "doi": "10.1016/j.chbr.2020.100052", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are nowadays a pervasive part of our online user\nexperience, where they either serve as information filters or provide us with\nsuggestions for additionally relevant content. These systems thereby influence\nwhich information is easily accessible to us and thus affect our\ndecision-making processes though the automated selection and ranking of the\npresented content. Automated recommendations can therefore be seen as digital\nnudges, because they determine different aspects of the choice architecture for\nusers.\n  In this work, we examine the relationship between digital nudging and\nrecommender systems, topics that so far were mostly investigated in isolation.\nThrough a systematic literature search, we first identified 87 nudging\nmechanisms, which we categorize in a novel taxonomy. A subsequent analysis then\nshows that only a small part of these nudging mechanisms was previously\ninvestigated in the context of recommender systems. This indicates that there\nis a huge potential to develop future recommender systems that leverage the\npower of digital nudging in order to influence the decision-making of users. In\nthis work, we therefore outline potential ways of integrating nudging\nmechanisms into recommender systems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 15:08:43 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 11:50:25 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Jesse", "Mathias", ""], ["Jannach", "Dietmar", ""]]}, {"id": "2011.03424", "submitter": "Sara Latifi", "authors": "Sara Latifi, Noemi Mauro, Dietmar Jannach", "title": "Session-aware Recommendation: A Surprising Quest for the\n  State-of-the-art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are designed to help users in situations of information\noverload. In recent years, we observed increased interest in session-based\nrecommendation scenarios, where the problem is to make item suggestions to\nusers based only on interactions observed in an ongoing session. However, in\ncases where interactions from previous user sessions are available, the\nrecommendations can be personalized according to the users' long-term\npreferences, a process called session-aware recommendation. Today, research in\nthis area is scattered and many existing works only compare session-aware with\nsession-based models. This makes it challenging to understand what represents\nthe state-of-the-art. To close this research gap, we benchmarked recent\nsession-aware algorithms against each other and against a number of\nsession-based recommendation algorithms and trivial extensions thereof. Our\ncomparison, to some surprise, revealed that (i) item simple techniques based on\nnearest neighbors consistently outperform recent neural techniques and that\n(ii) session-aware models were mostly not better than approaches that do not\nuse long-term preference information. Our work therefore not only points to\npotential methodological issues where new methods are compared to weak\nbaselines, but also indicates that there remains a huge potential for more\nsophisticated session-aware recommendation algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 15:18:01 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Latifi", "Sara", ""], ["Mauro", "Noemi", ""], ["Jannach", "Dietmar", ""]]}, {"id": "2011.03451", "submitter": "Rongcheng Tu", "authors": "Rong-Cheng Tu, Xian-Ling Mao, Rongxin Tu, Binbin Bian, Wei Wei, Heyan\n  Huang", "title": "Deep Cross-modal Hashing via Margin-dynamic-softmax Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their high retrieval efficiency and low storage cost for cross-modal\nsearch task, cross-modal hashing methods have attracted considerable attention.\nFor the supervised cross-modal hashing methods, how to make the learned hash\ncodes preserve semantic information sufficiently contained in the label of\ndatapoints is the key to further enhance the retrieval performance. Hence,\nalmost all supervised cross-modal hashing methods usually depends on defining a\nsimilarity between datapoints with the label information to guide the hashing\nmodel learning fully or partly. However, the defined similarity between\ndatapoints can only capture the label information of datapoints partially and\nmisses abundant semantic information, then hinders the further improvement of\nretrieval performance. Thus, in this paper, different from previous works, we\npropose a novel cross-modal hashing method without defining the similarity\nbetween datapoints, called Deep Cross-modal Hashing via\n\\textit{Margin-dynamic-softmax Loss} (DCHML). Specifically, DCHML first trains\na proxy hashing network to transform each category information of a dataset\ninto a semantic discriminative hash code, called proxy hash code. Each proxy\nhash code can preserve the semantic information of its corresponding category\nwell. Next, without defining the similarity between datapoints to supervise the\ntraining process of the modality-specific hashing networks , we propose a novel\n\\textit{margin-dynamic-softmax loss} to directly utilize the proxy hashing\ncodes as supervised information. Finally, by minimizing the novel\n\\textit{margin-dynamic-softmax loss}, the modality-specific hashing networks\ncan be trained to generate hash codes which can simultaneously preserve the\ncross-modal similarity and abundant semantic information well.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 16:02:35 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 15:02:27 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Tu", "Rong-Cheng", ""], ["Mao", "Xian-Ling", ""], ["Tu", "Rongxin", ""], ["Bian", "Binbin", ""], ["Wei", "Wei", ""], ["Huang", "Heyan", ""]]}, {"id": "2011.03452", "submitter": "Xuan Bi", "authors": "Xuan Bi, Gediminas Adomavicius, William Li, Annie Qu", "title": "Improving Sales Forecasting Accuracy: A Tensor Factorization Approach\n  with Demand Awareness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to accessible big data collections from consumers, products, and stores,\nadvanced sales forecasting capabilities have drawn great attention from many\ncompanies especially in the retail business because of its importance in\ndecision making. Improvement of the forecasting accuracy, even by a small\npercentage, may have a substantial impact on companies' production and\nfinancial planning, marketing strategies, inventory controls, supply chain\nmanagement, and eventually stock prices. Specifically, our research goal is to\nforecast the sales of each product in each store in the near future. Motivated\nby tensor factorization methodologies for personalized context-aware\nrecommender systems, we propose a novel approach called the Advanced Temporal\nLatent-factor Approach to Sales forecasting (ATLAS), which achieves accurate\nand individualized prediction for sales by building a single\ntensor-factorization model across multiple stores and products. Our\ncontribution is a combination of: tensor framework (to leverage information\nacross stores and products), a new regularization function (to incorporate\ndemand dynamics), and extrapolation of tensor into future time periods using\nstate-of-the-art statistical (seasonal auto-regressive integrated\nmoving-average models) and machine-learning (recurrent neural networks) models.\nThe advantages of ATLAS are demonstrated on eight product category datasets\ncollected by the Information Resource, Inc., where a total of 165 million\nweekly sales transactions from more than 1,500 grocery stores over 15,560\nproducts are analyzed.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 16:04:40 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Bi", "Xuan", ""], ["Adomavicius", "Gediminas", ""], ["Li", "William", ""], ["Qu", "Annie", ""]]}, {"id": "2011.03536", "submitter": "Micha{\\l} Gajda", "authors": "Micha{\\l} J. Gajda, Dmitry Krylov", "title": "Fast XML/HTML for Haskell: XML TypeLift", "comments": null, "journal-ref": null, "doi": "10.5281/zenodo.3929549", "report-no": null, "categories": "cs.PL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents and compares a range of parsers with and without data\nmapping for conversion between XML and Haskell. The best performing parser\ncompetes favorably with the fastest tools available in other languages and is,\nthus, suitable for use in large-scale data analysis. The best performing parser\nalso allows software developers of intermediate-level Haskell programming\nskills to start processing large numbers of XML documents soon after finding\nthe relevant XML Schema from a simple internet search, without the need for\nspecialist prior knowledge or skills. We hope that this unique combination of\nparser performance and usability will provide a new standard for XML mapping to\nhigh-level languages.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 20:26:36 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Gajda", "Micha\u0142 J.", ""], ["Krylov", "Dmitry", ""]]}, {"id": "2011.03796", "submitter": "Lionel Tabourier", "authors": "Pedro Ramaciotti Morales, Lionel Tabourier, Rapha\\\"el\n  Fournier-S'niehotta", "title": "Testing the Impact of Semantics and Structure on Recommendation Accuracy\n  and Diversity", "comments": "22 pages, 8 figures, accepted to conference ASONAM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Heterogeneous Information Network (HIN) formalism is very flexible and\nenables complex recommendations models. We evaluate the effect of different\nparts of a HIN on the accuracy and the diversity of recommendations, then\ninvestigate if these effects are only due to the semantic content encoded in\nthe network. We use recently-proposed diversity measures which are based on the\nnetwork structure and better suited to the HIN formalism. Finally, we randomly\nshuffle the edges of some parts of the HIN, to empty the network from its\nsemantic content, while leaving its structure relatively unaffected. We show\nthat the semantic content encoded in the network data has a limited importance\nfor the performance of a recommender system and that structure is crucial.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 16:07:18 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 14:40:09 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Morales", "Pedro Ramaciotti", ""], ["Tabourier", "Lionel", ""], ["Fournier-S'niehotta", "Rapha\u00ebl", ""]]}, {"id": "2011.03890", "submitter": "Roman Levin", "authors": "Emil Noordeh, Roman Levin, Ruochen Jiang, Harris Shadmany", "title": "Echo Chambers in Collaborative Filtering Based Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems underpin the serving of nearly all online content in\nthe modern age. From Youtube and Netflix recommendations, to Facebook feeds and\nGoogle searches, these systems are designed to filter content to the predicted\npreferences of users. Recently, these systems have faced growing criticism with\nrespect to their impact on content diversity, social polarization, and the\nhealth of public discourse. In this work we simulate the recommendations given\nby collaborative filtering algorithms on users in the MovieLens data set. We\nfind that prolonged exposure to system-generated recommendations substantially\ndecreases content diversity, moving individual users into \"echo-chambers\"\ncharacterized by a narrow range of content. Furthermore, our work suggests that\nonce these echo-chambers have been established, it is difficult for an\nindividual user to break out by manipulating solely their own rating vector.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 02:35:47 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Noordeh", "Emil", ""], ["Levin", "Roman", ""], ["Jiang", "Ruochen", ""], ["Shadmany", "Harris", ""]]}, {"id": "2011.04006", "submitter": "Yi Tay", "authors": "Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri,\n  Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, Donald Metzler", "title": "Long Range Arena: A Benchmark for Efficient Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformers do not scale very well to long sequence lengths largely because\nof quadratic self-attention complexity. In the recent months, a wide spectrum\nof efficient, fast Transformers have been proposed to tackle this problem, more\noften than not claiming superior or comparable model quality to vanilla\nTransformer models. To this date, there is no well-established consensus on how\nto evaluate this class of models. Moreover, inconsistent benchmarking on a wide\nspectrum of tasks and datasets makes it difficult to assess relative model\nquality amongst many models. This paper proposes a systematic and unified\nbenchmark, LRA, specifically focused on evaluating model quality under\nlong-context scenarios. Our benchmark is a suite of tasks consisting of\nsequences ranging from $1K$ to $16K$ tokens, encompassing a wide range of data\ntypes and modalities such as text, natural, synthetic images, and mathematical\nexpressions requiring similarity, structural, and visual-spatial reasoning. We\nsystematically evaluate ten well-established long-range Transformer models\n(Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers,\nSynthesizers, Sparse Transformers, and Longformers) on our newly proposed\nbenchmark suite. LRA paves the way towards better understanding this class of\nefficient Transformer models, facilitates more research in this direction, and\npresents new challenging tasks to tackle. Our benchmark code will be released\nat https://github.com/google-research/long-range-arena.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 15:53:56 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Tay", "Yi", ""], ["Dehghani", "Mostafa", ""], ["Abnar", "Samira", ""], ["Shen", "Yikang", ""], ["Bahri", "Dara", ""], ["Pham", "Philip", ""], ["Rao", "Jinfeng", ""], ["Yang", "Liu", ""], ["Ruder", "Sebastian", ""], ["Metzler", "Donald", ""]]}, {"id": "2011.04106", "submitter": "Jieming Zhu", "authors": "Jieming Zhu, Jinyang Liu, Weiqi Li, Jincai Lai, Xiuqiang He, Liang\n  Chen, Zibin Zheng", "title": "Ensembled CTR Prediction via Knowledge Distillation", "comments": "Published in CIKM'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning-based models have been widely studied for\nclick-through rate (CTR) prediction and lead to improved prediction accuracy in\nmany industrial applications. However, current research focuses primarily on\nbuilding complex network architectures to better capture sophisticated feature\ninteractions and dynamic user behaviors. The increased model complexity may\nslow down online inference and hinder its adoption in real-time applications.\nInstead, our work targets at a new model training strategy based on knowledge\ndistillation (KD). KD is a teacher-student learning framework to transfer\nknowledge learned from a teacher model to a student model. The KD strategy not\nonly allows us to simplify the student model as a vanilla DNN model but also\nachieves significant accuracy improvements over the state-of-the-art teacher\nmodels. The benefits thus motivate us to further explore the use of a powerful\nensemble of teachers for more accurate student model training. We also propose\nsome novel techniques to facilitate ensembled CTR prediction, including teacher\ngating and early stopping by distillation loss. We conduct comprehensive\nexperiments against 12 existing models and across three industrial datasets.\nBoth offline and online A/B testing results show the effectiveness of our\nKD-based training strategy.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 23:37:58 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Zhu", "Jieming", ""], ["Liu", "Jinyang", ""], ["Li", "Weiqi", ""], ["Lai", "Jincai", ""], ["He", "Xiuqiang", ""], ["Chen", "Liang", ""], ["Zheng", "Zibin", ""]]}, {"id": "2011.04137", "submitter": "Andrew Browne", "authors": "Alex Carderas, Ye Yuan, Itamar Livnat, Ryan Yanagihara, Rosita Saul,\n  Gabrielle Montes De Oca, Kai Zheng, Andrew W. Browne", "title": "Automated data extraction of bar chart raster images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: To develop software utilizing optical character recognition toward\nthe automatic extraction of data from bar charts for meta-analysis. Methods: We\nutilized a multistep data extraction approach that included figure extraction,\ntext detection, and image disassembly. PubMed Central papers that were\nprocessed in this manner included clinical trials regarding macular\ndegeneration, a disease causing blindness with a heavy disease burden and many\nclinical trials. Bar chart characteristics were extracted in both an automated\nand manual fashion. These two approaches were then compared for accuracy. These\ncharacteristics were then compared using a Bland-Altman analysis. Results:\nBased on Bland-Altman analysis, 91.8% of data points were within the limits of\nagreement. By comparing our automated data extraction with manual data\nextraction, automated data extraction yielded the following accuracies: X-axis\nlabels 79.5%, Y-tick values 88.6%, Y-axis label 88.6%, Bar value <5% error\n88.0%. Discussion: Based on our analysis, we achieved an agreement between\nautomated data extraction and manual data extraction. A major source of error\nwas the incorrect delineation of 7s as 2s by optical character recognition\nlibrary. We also would benefit from adding redundancy checks in the form of a\ndeep neural network to boost our bar detection accuracy. Further refinements to\nthis method are justified to extract tabulated and line graph data to\nfacilitate automated data gathering for meta-analysis.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 01:45:36 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Carderas", "Alex", ""], ["Yuan", "Ye", ""], ["Livnat", "Itamar", ""], ["Yanagihara", "Ryan", ""], ["Saul", "Rosita", ""], ["De Oca", "Gabrielle Montes", ""], ["Zheng", "Kai", ""], ["Browne", "Andrew W.", ""]]}, {"id": "2011.04219", "submitter": "Anay Mehrotra", "authors": "Anay Mehrotra and L. Elisa Celis", "title": "Mitigating Bias in Set Selection with Noisy Protected Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DS cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subset selection algorithms are ubiquitous in AI-driven applications,\nincluding, online recruiting portals and image search engines, so it is\nimperative that these tools are not discriminatory on the basis of protected\nattributes such as gender or race. Currently, fair subset selection algorithms\nassume that the protected attributes are known as part of the dataset. However,\nprotected attributes may be noisy due to errors during data collection or if\nthey are imputed (as is often the case in real-world settings). While a wide\nbody of work addresses the effect of noise on the performance of machine\nlearning algorithms, its effect on fairness remains largely unexamined. We find\nthat in the presence of noisy protected attributes, in attempting to increase\nfairness without considering noise, one can, in fact, decrease the fairness of\nthe result!\n  Towards addressing this, we consider an existing noise model in which there\nis probabilistic information about the protected attributes (e.g., [58, 34, 20,\n46]), and ask is fair selection possible under noisy conditions? We formulate a\n``denoised'' selection problem which functions for a large class of fairness\nmetrics; given the desired fairness goal, the solution to the denoised problem\nviolates the goal by at most a small multiplicative amount with high\nprobability. Although this denoised problem turns out to be NP-hard, we give a\nlinear-programming based approximation algorithm for it. We evaluate this\napproach on both synthetic and real-world datasets. Our empirical results show\nthat this approach can produce subsets which significantly improve the fairness\nmetrics despite the presence of noisy protected attributes, and, compared to\nprior noise-oblivious approaches, has better Pareto-tradeoffs between utility\nand fairness.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 06:45:15 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 17:56:05 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Mehrotra", "Anay", ""], ["Celis", "L. Elisa", ""]]}, {"id": "2011.04395", "submitter": "Hao Wang", "authors": "Hao Wang, Bing Ruan", "title": "MatRec: Matrix Factorization for Highly Skewed Dataset", "comments": null, "journal-ref": null, "doi": "10.1145/3422713.3422735", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems is one of the most successful AI technologies applied in\nthe internet cooperations. Popular internet products such as TikTok, Amazon,\nand YouTube have all integrated recommender systems as their core product\nfeature. Although recommender systems have received great success, it is well\nknown for highly skewed datasets, engineers and researchers need to adjust\ntheir methods to tackle the specific problem to yield good results. Inability\nto deal with highly skewed dataset usually generates hard computational\nproblems for big data clusters and unsatisfactory results for customers. In\nthis paper, we propose a new algorithm solving the problem in the framework of\nmatrix factorization. We model the data skewness factors in the theoretic\nmodeling of the approach with easy to interpret and easy to implement formulas.\nWe prove in experiments our method generates comparably favorite results with\npopular recommender system algorithms such as Learning to Rank , Alternating\nLeast Squares and Deep Matrix Factorization.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 12:55:38 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Wang", "Hao", ""], ["Ruan", "Bing", ""]]}, {"id": "2011.04521", "submitter": "Marina Litvak Dr", "authors": "Natalia Vanetik, Marina Litvak, Sergey Shevchuk, and Lior Reznik", "title": "Automated Discovery of Mathematical Definitions in Text with Deep Neural\n  Networks", "comments": "42 pages, 10 figures, currently under review in Expert Systems with\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic definition extraction from texts is an important task that has\nnumerous applications in several natural language processing fields such as\nsummarization, analysis of scientific texts, automatic taxonomy generation,\nontology generation, concept identification, and question answering. For\ndefinitions that are contained within a single sentence, this problem can be\nviewed as a binary classification of sentences into definitions and\nnon-definitions. In this paper, we focus on automatic detection of one-sentence\ndefinitions in mathematical texts, which are difficult to separate from\nsurrounding text. We experiment with several data representations, which\ninclude sentence syntactic structure and word embeddings, and apply deep\nlearning methods such as the Convolutional Neural Network (CNN) and the Long\nShort-Term Memory network (LSTM), in order to identify mathematical\ndefinitions. Our experiments demonstrate the superiority of CNN and its\ncombination with LSTM, when applied on the syntactically-enriched input\nrepresentation. We also present a new dataset for definition extraction from\nmathematical texts. We demonstrate that this dataset is beneficial for training\nsupervised models aimed at extraction of mathematical definitions. Our\nexperiments with different domains demonstrate that mathematical definitions\nrequire special treatment, and that using cross-domain learning is inefficient\nfor that task.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 15:57:53 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Vanetik", "Natalia", ""], ["Litvak", "Marina", ""], ["Shevchuk", "Sergey", ""], ["Reznik", "Lior", ""]]}, {"id": "2011.04779", "submitter": "Mohamed Karim Belaid", "authors": "Mohamed Karim Belaid", "title": "After All, Only The Last Neuron Matters: Comparing Multi-modal Fusion\n  Functions for Scene Graph Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  From object segmentation to word vector representations, Scene Graph\nGeneration (SGG) became a complex task built upon numerous research results. In\nthis paper, we focus on the last module of this model: the fusion function. The\nrole of this latter is to combine three hidden states. We perform an ablation\ntest in order to compare different implementations. First, we reproduce the\nstate-of-the-art results using SUM, and GATE functions. Then we expand the\noriginal solution by adding more model-agnostic functions: an adapted version\nof DIST and a mixture between MFB and GATE. On the basis of the\nstate-of-the-art configuration, DIST performed the best Recall @ K, which makes\nit now part of the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 21:27:32 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Belaid", "Mohamed Karim", ""]]}, {"id": "2011.04830", "submitter": "Rodger Benham", "authors": "Rodger Benham, Alistair Moffat, J. Shane Culpepper", "title": "RMITB at TREC COVID 2020", "comments": "7 pages, 6 figures, 1 table, Text REtrieval Conference COVID Track\n  Workshop Paper, for more workshop papers, see\n  https://ir.nist.gov/covidSubmit/bib.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engine users rarely express an information need using the same query,\nand small differences in queries can lead to very different result sets. These\nuser query variations have been exploited in past TREC CORE tracks to\ncontribute diverse, highly-effective runs in offline evaluation campaigns with\nthe goal of producing reusable test collections. In this paper, we document the\nquery fusion runs submitted to the first and second round of TREC COVID, using\nten queries per topic created by the first author. In our analysis, we focus\nprimarily on the effects of having our second priority run omitted from the\njudgment pool. This run is of particular interest, as it surfaced a number of\nrelevant documents that were not judged until later rounds of the task. If the\nadditional judgments were included in the first round, the performance of this\nrun increased by 35 rank positions when using RBP p=0.5, highlighting the\nimportance of judgment depth and coverage in assessment tasks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 23:30:56 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Benham", "Rodger", ""], ["Moffat", "Alistair", ""], ["Culpepper", "J. Shane", ""]]}, {"id": "2011.05057", "submitter": "Anas Al-Oraiqat Dr.", "authors": "Yelyzaveta Meleshko, Oleksandr Drieiev, Anas Mahmoud Al-Oraiqat", "title": "The improved model of user similarity coefficients computation For\n  recommendation systems", "comments": "10 pages, 5 figures", "journal-ref": "ADVANCED INFORMATION SYSTEMS, ISSN 2522-9052, 2020", "doi": "10.20998/2522-9052.2020.3.06", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subject matter of the article is a model of calculating the user\nsimilarity coefficients of the recommendation systems. The goal is the\ndevelopment of the improved model of user similarity coefficients calculation\nfor recommendation systems to optimize the time of forming recommendation\nlists. The tasks to be solved are: to investigate the probability of changing\nuser preferences of a recommendation system by comparing their similarity\ncoefficients in time, to investigate which distribution function describes the\nchanges of similarity coefficients of users in time. The methods used are:\ngraph theory, probability theory, radioactivity theory, algorithm theory.\nConclusions. In the course of the researches, the model of user similarity\ncoefficients calculating for the recommendation systems has been improved. The\nmodel differs from the known ones in that it takes into account the\nrecalculation period of similarity coefficients for the individual user and\naverage recalculation period of similarity coefficients for all users of the\nsystem or a specific group of users. The software has been developed, in which\na series of experiments was conducted to test the effectiveness of the\ndeveloped method. The conducted experiments showed that the developed method in\ngeneral increases the quality of the recommendation system without significant\nfluctuations of Precision and Recall of the system. Precision and Recall can\ndecrease slightly or increase, depending on the characteristics of the incoming\ndata set. The use of the proposed solutions will increase the application\nperiod of the previously calculated similarity coefficients of users for the\nprediction of preferences without their recalculation and, accordingly, it will\nshorten the time of formation and issuance of recommendation lists up to 2\ntimes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 11:49:54 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Meleshko", "Yelyzaveta", ""], ["Drieiev", "Oleksandr", ""], ["Al-Oraiqat", "Anas Mahmoud", ""]]}, {"id": "2011.05061", "submitter": "Riku Togashi", "authors": "Riku Togashi, Mayu Otani, Shin'ichi Satoh", "title": "Alleviating Cold-Start Problems in Recommendation through\n  Pseudo-Labelling over Knowledge Graph", "comments": "WSDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving cold-start problems is indispensable to provide meaningful\nrecommendation results for new users and items. Under sparsely observed data,\nunobserved user-item pairs are also a vital source for distilling latent users'\ninformation needs. Most present works leverage unobserved samples for\nextracting negative signals. However, such an optimisation strategy can lead to\nbiased results toward already popular items by frequently handling new items as\nnegative instances. In this study, we tackle the cold-start problems for new\nusers/items by appropriately leveraging unobserved samples. We propose a\nknowledge graph (KG)-aware recommender based on graph neural networks, which\naugments labelled samples through pseudo-labelling. Our approach aggressively\nemploys unobserved samples as positive instances and brings new items into the\nspotlight. To avoid exhaustive label assignments to all possible pairs of users\nand items, we exploit a KG for selecting probably positive items for each user.\nWe also utilise an improved negative sampling strategy and thereby suppress the\nexacerbation of popularity biases. Through experiments, we demonstrate that our\napproach achieves improvements over the state-of-the-art KG-aware recommenders\nin a variety of scenarios; in particular, our methodology successfully improves\nrecommendation performance for cold-start users/items.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 11:54:44 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Togashi", "Riku", ""], ["Otani", "Mayu", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "2011.05119", "submitter": "Mostafa Khalaji", "authors": "Mostafa Khalaji", "title": "TRSM-RS: A Movie Recommender System Based on Users' Gender and New\n  Weighted Similarity Measure", "comments": "11 pages, 17th Iran Media Technology Exhibition and Conference, At:\n  Tehran, Iran, November 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the growing data on the Internet, recommender systems have been able to\npredict users' preferences and offer related movies. Collaborative filtering is\none of the most popular algorithms in these systems. The main purpose of\ncollaborative filtering is to find the users or the same items using the rating\nmatrix. By increasing the number of users and items, this algorithm suffers\nfrom the scalability problem. On the other hand, due to the unavailability of a\nlarge number of user preferences for different items, there is a cold start\nproblem for a new user or item that has a significant impact on system\nperformance. The purpose of this paper is to design a movie recommender system\nnamed TRSM-RS using users' demographic information (just users' gender) along\nwith the new weighted similarity measure. By segmenting users based on their\ngender, the scalability problem is improved, and by considering the reliability\nof the users' similarity as the weight in the new similarity measure (Tanimoto\nReliability Similarity Measure, TRSM), the effect of the cold-start problem is\nundermined and the performance of the system is improved. Experiments were\nperformed on the MovieLens dataset and the system was evaluated using mean\nabsolute error (MAE), Accuracy, Precision, and Recall metrics. The results of\nthe experiments indicate improved performance (accuracy and precision) and\nsystem error rate compared to other research methods of the researchers. The\nmaximum improved MAE rate of the system for men and women is 5.5% and 13.8%,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 14:41:40 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Khalaji", "Mostafa", ""]]}, {"id": "2011.05208", "submitter": "Zekarias Tilahun Kefato", "authors": "Zekarias T. Kefato and Sarunas Girdzijauskas and Nasrullah Sheikh and\n  Alberto Montresor", "title": "Dynamic Embeddings for Interaction Prediction", "comments": "Accepted for the International World Wide Web Conference Committee,\n  WWW'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recommender systems (RSs), predicting the next item that a user interacts\nwith is critical for user retention. While the last decade has seen an\nexplosion of RSs aimed at identifying relevant items that match user\npreferences, there is still a range of aspects that could be considered to\nfurther improve their performance. For example, often RSs are centered around\nthe user, who is modeled using her recent sequence of activities. Recent\nstudies, however, have shown the effectiveness of modeling the mutual\ninteractions between users and items using separate user and item embeddings.\nBuilding on the success of these studies, we propose a novel method called\nDeePRed that addresses some of their limitations. In particular, we avoid\nrecursive and costly interactions between consecutive short-term embeddings by\nusing long-term (stationary) embeddings as a proxy. This enable us to train\nDeePRed using simple mini-batches without the overhead of specialized\nmini-batches proposed in previous studies. Moreover, DeePRed's effectiveness\ncomes from the aforementioned design and a multi-way attention mechanism that\ninspects user-item compatibility. Experiments show that DeePRed outperforms the\nbest state-of-the-art approach by at least 14% on next item prediction task,\nwhile gaining more than an order of magnitude speedup over the best performing\nbaselines. Although this study is mainly concerned with temporal interaction\nnetworks, we also show the power and flexibility of DeePRed by adapting it to\nthe case of static interaction networks, substituting the short- and long-term\naspects with local and global ones.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 16:04:46 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 20:35:36 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kefato", "Zekarias T.", ""], ["Girdzijauskas", "Sarunas", ""], ["Sheikh", "Nasrullah", ""], ["Montresor", "Alberto", ""]]}, {"id": "2011.05546", "submitter": "Yiren Liu", "authors": "Yiren Liu, Kuan-Ying Lee", "title": "E-commerce Query-based Generation based on User Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing number of merchandise on e-commerce platforms, users tend\nto refer to reviews of other shoppers to decide which product they should buy.\nHowever, with so many reviews of a product, users often have to spend lots of\ntime browsing through reviews talking about product attributes they do not care\nabout. We want to establish a system that can automatically summarize and\nanswer user's product specific questions.\n  In this study, we propose a novel seq2seq based text generation model to\ngenerate answers to user's question based on reviews posted by previous users.\nGiven a user question and/or target sentiment polarity, we extract aspects of\ninterest and generate an answer that summarizes previous relevant user reviews.\nSpecifically, our model performs attention between input reviews and target\naspects during encoding and is conditioned on both review rating and input\ncontext during decoding. We also incorporate a pre-trained auxiliary rating\nclassifier to improve model performance and accelerate convergence during\ntraining. Experiments using real-world e-commerce dataset show that our model\nachieves improvement in performance compared to previously introduced models.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 04:58:31 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Liu", "Yiren", ""], ["Lee", "Kuan-Ying", ""]]}, {"id": "2011.05614", "submitter": "Jiangcheng Qin", "authors": "Jiangcheng Qin, Baisong Liu", "title": "A Novel Privacy-Preserved Recommender System Framework based on\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender System (RS) is currently an effective way to solve information\noverload. To meet users' next click behavior, RS needs to collect users'\npersonal information and behavior to achieve a comprehensive and profound user\npreference perception. However, these centrally collected data are\nprivacy-sensitive, and any leakage may cause severe problems to both users and\nservice providers. This paper proposed a novel privacy-preserved recommender\nsystem framework (PPRSF), through the application of federated learning\nparadigm, to enable the recommendation algorithm to be trained and carry out\ninference without centrally collecting users' private data. The PPRSF not only\nable to reduces the privacy leakage risk, satisfies legal and regulatory\nrequirements but also allows various recommendation algorithms to be applied.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 08:07:58 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Qin", "Jiangcheng", ""], ["Liu", "Baisong", ""]]}, {"id": "2011.05625", "submitter": "Guorui Zhou", "authors": "Guorui Zhou, Weijie Bian, Kailun Wu, Lejian Ren, Qi Pi, Yujing Zhang,\n  Can Xiao, Xiang-Rong Sheng, Na Mou, Xinchen Luo, Chi Zhang, Xianjie Qiao,\n  Shiming Xiang, Kun Gai, Xiaoqiang Zhu, Jian Xu", "title": "CAN: Revisiting Feature Co-Action for Click-Through Rate Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the success of deep learning, recent industrial Click-Through\nRate (CTR) prediction models have made the transition from traditional shallow\napproaches to deep approaches. Deep Neural Networks (DNNs) are known for its\nability to learn non-linear interactions from raw feature automatically,\nhowever, the non-linear feature interaction is learned in an implicit manner.\nThe non-linear interaction may be hard to capture and explicitly model the\n\\textit{co-action} of raw feature is beneficial for CTR prediction.\n\\textit{Co-action} refers to the collective effects of features toward final\nprediction.\n  In this paper, we argue that current CTR models do not fully explore the\npotential of feature co-action. We conduct experiments and show that the effect\nof feature co-action is underestimated seriously. Motivated by our observation,\nwe propose feature Co-Action Network (CAN) to explore the potential of feature\nco-action. The proposed model can efficiently and effectively capture the\nfeature co-action, which improves the model performance while reduce the\nstorage and computation consumption. Experiment results on public and\nindustrial datasets show that CAN outperforms state-of-the-art CTR models by a\nlarge margin. Up to now, CAN has been deployed in the Alibaba display\nadvertisement system, obtaining averaging 12\\% improvement on CTR and 8\\% on\nRPM.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 08:33:07 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Zhou", "Guorui", ""], ["Bian", "Weijie", ""], ["Wu", "Kailun", ""], ["Ren", "Lejian", ""], ["Pi", "Qi", ""], ["Zhang", "Yujing", ""], ["Xiao", "Can", ""], ["Sheng", "Xiang-Rong", ""], ["Mou", "Na", ""], ["Luo", "Xinchen", ""], ["Zhang", "Chi", ""], ["Qiao", "Xianjie", ""], ["Xiang", "Shiming", ""], ["Gai", "Kun", ""], ["Zhu", "Xiaoqiang", ""], ["Xu", "Jian", ""]]}, {"id": "2011.05742", "submitter": "Shuai Zhang", "authors": "Shuai Zhang, Huoyu Liu, Aston Zhang, Yue Hu, Ce Zhang, Yumeng Li,\n  Tanchao Zhu, Shaojian He, Wenwu Ou", "title": "Learning User Representations with Hypercuboids for Recommender Systems", "comments": "Accepted by WSDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling user interests is crucial in real-world recommender systems. In this\npaper, we present a new user interest representation model for personalized\nrecommendation. Specifically, the key novelty behind our model is that it\nexplicitly models user interests as a hypercuboid instead of a point in the\nspace. In our approach, the recommendation score is learned by calculating a\ncompositional distance between the user hypercuboid and the item. This helps to\nalleviate the potential geometric inflexibility of existing collaborative\nfiltering approaches, enabling a greater extent of modeling capability.\nFurthermore, we present two variants of hypercuboids to enhance the capability\nin capturing the diversities of user interests. A neural architecture is also\nproposed to facilitate user hypercuboid learning by capturing the activity\nsequences (e.g., buy and rate) of users. We demonstrate the effectiveness of\nour proposed model via extensive experiments on both public and commercial\ndatasets. Empirical results show that our approach achieves very promising\nresults, outperforming existing state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 12:50:00 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Zhang", "Shuai", ""], ["Liu", "Huoyu", ""], ["Zhang", "Aston", ""], ["Hu", "Yue", ""], ["Zhang", "Ce", ""], ["Li", "Yumeng", ""], ["Zhu", "Tanchao", ""], ["He", "Shaojian", ""], ["Ou", "Wenwu", ""]]}, {"id": "2011.05928", "submitter": "Namyong Park", "authors": "Namyong Park, Andrey Kan, Christos Faloutsos, Xin Luna Dong", "title": "J-Recs: Principled and Scalable Recommendation Justification", "comments": "ICDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online recommendation is an essential functionality across a variety of\nservices, including e-commerce and video streaming, where items to buy, watch,\nor read are suggested to users. Justifying recommendations, i.e., explaining\nwhy a user might like the recommended item, has been shown to improve user\nsatisfaction and persuasiveness of the recommendation. In this paper, we\ndevelop a method for generating post-hoc justifications that can be applied to\nthe output of any recommendation algorithm. Existing post-hoc methods are often\nlimited in providing diverse justifications, as they either use only one of\nmany available types of input data, or rely on the predefined templates. We\naddress these limitations of earlier approaches by developing J-Recs, a method\nfor producing concise and diverse justifications. J-Recs is a recommendation\nmodel-agnostic method that generates diverse justifications based on various\ntypes of product and user data (e.g., purchase history and product attributes).\nThe challenge of jointly processing multiple types of data is addressed by\ndesigning a principled graph-based approach for justification generation. In\naddition to theoretical analysis, we present an extensive evaluation on\nsynthetic and real-world data. Our results show that J-Recs satisfies desirable\nproperties of justifications, and efficiently produces effective\njustifications, matching user preferences up to 20% more accurately than\nbaselines.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 17:37:52 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Park", "Namyong", ""], ["Kan", "Andrey", ""], ["Faloutsos", "Christos", ""], ["Dong", "Xin Luna", ""]]}, {"id": "2011.06237", "submitter": "Bhanu Prakash Reddy Guda", "authors": "Samarth Aggarwal, Rohin Garg, Abhilasha Sancheti, Bhanu Prakash Reddy\n  Guda, Iftikhar Ahamath Burhanuddin", "title": "Goal-driven Command Recommendations for Analysts", "comments": "14th ACM Conference on Recommender Systems (RecSys 2020)", "journal-ref": null, "doi": "10.1145/3383313.3412255", "report-no": null, "categories": "cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent times have seen data analytics software applications become an\nintegral part of the decision-making process of analysts. The users of these\nsoftware applications generate a vast amount of unstructured log data. These\nlogs contain clues to the user's goals, which traditional recommender systems\nmay find difficult to model implicitly from the log data. With this assumption,\nwe would like to assist the analytics process of a user through command\nrecommendations. We categorize the commands into software and data categories\nbased on their purpose to fulfill the task at hand. On the premise that the\nsequence of commands leading up to a data command is a good predictor of the\nlatter, we design, develop, and validate various sequence modeling techniques.\nIn this paper, we propose a framework to provide goal-driven data command\nrecommendations to the user by leveraging unstructured logs. We use the log\ndata of a web-based analytics software to train our neural network models and\nquantify their performance, in comparison to relevant and competitive\nbaselines. We propose a custom loss function to tailor the recommended data\ncommands according to the goal information provided exogenously. We also\npropose an evaluation metric that captures the degree of goal orientation of\nthe recommendations. We demonstrate the promise of our approach by evaluating\nthe models with the proposed metric and showcasing the robustness of our models\nin the case of adversarial examples, where the user activity is misaligned with\nselected goal, through offline evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 07:26:52 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Aggarwal", "Samarth", ""], ["Garg", "Rohin", ""], ["Sancheti", "Abhilasha", ""], ["Guda", "Bhanu Prakash Reddy", ""], ["Burhanuddin", "Iftikhar Ahamath", ""]]}, {"id": "2011.06490", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Joachim Denzler", "title": "Content-based Image Retrieval and the Semantic Gap in the Deep Learning\n  Era", "comments": "CBIR workshop at ICPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based image retrieval has seen astonishing progress over the past\ndecade, especially for the task of retrieving images of the same object that is\ndepicted in the query image. This scenario is called instance or object\nretrieval and requires matching fine-grained visual patterns between images.\nSemantics, however, do not play a crucial role. This brings rise to the\nquestion: Do the recent advances in instance retrieval transfer to more generic\nimage retrieval scenarios? To answer this question, we first provide a brief\noverview of the most relevant milestones of instance retrieval. We then apply\nthem to a semantic image retrieval task and find that they perform inferior to\nmuch less sophisticated and more generic methods in a setting that requires\nimage understanding. Following this, we review existing approaches to closing\nthis so-called semantic gap by integrating prior world knowledge. We conclude\nthat the key problem for the further advancement of semantic image retrieval\nlies in the lack of a standardized task definition and an appropriate benchmark\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 17:00:08 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["Denzler", "Joachim", ""]]}, {"id": "2011.06807", "submitter": "Zekun Li", "authors": "Zekun Li, Yujia Zheng, Shu Wu, Xiaoyu Zhang, Liang Wang", "title": "Heterogeneous Graph Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based collaborative filtering (CF) algorithms have gained increasing\nattention. Existing work in this literature usually models the user-item\ninteractions as a bipartite graph, where users and items are two isolated node\nsets and edges between them indicate their interactions. Then, the unobserved\npreference of users can be exploited by modeling high-order connectivity on the\nbipartite graph. In this work, we propose to model user-item interactions as a\nheterogeneous graph which consists of not only user-item edges indicating their\ninteraction but also user-user edges indicating their similarity. We develop\nheterogeneous graph collaborative filtering (HGCF), a GCN-based framework which\ncan explicitly capture both the interaction signal and similarity signal\nthrough embedding propagation on the heterogeneous graph. Since the\nheterogeneous graph is more connected than the bipartite graph, the sparsity\nissue can be alleviated and the demand for expensive high-order connectivity\nmodeling can be lowered. Extensive experiments conducted on three public\nbenchmarks demonstrate its superiority over the state-of-the-arts. Further\nanalysis verifies the importance of user-user edges in the graph, justifying\nthe rationality and effectiveness of HGCF.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 08:34:53 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 07:34:12 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Li", "Zekun", ""], ["Zheng", "Yujia", ""], ["Wu", "Shu", ""], ["Zhang", "Xiaoyu", ""], ["Wang", "Liang", ""]]}, {"id": "2011.06829", "submitter": "Erick Elejalde", "authors": "Elejalde Erick and Galanopoulos Damianos and Niederee Claudia and\n  Mezaris Vasileios", "title": "Migration-Related Semantic Concepts for the Retrieval of Relevant Video\n  Content", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-71711-7_34", "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Migration, and especially irregular migration, is a critical issue for border\nagencies and society in general. Migration-related situations and decisions are\ninfluenced by various factors, including the perceptions about migration routes\nand target countries. An improved understanding of such factors can be achieved\nby systematic automated analyses of media and social media channels, and the\nvideos and images published in them. However, the multifaceted nature of\nmigration and the variety of ways migration-related aspects are expressed in\nimages and videos make the finding and automated analysis of migration-related\nmultimedia content a challenging task. We propose a novel approach that\neffectively bridges the gap between a substantiated domain understanding -\nencapsulated into a set of Migration-related semantic concepts - and the\nexpression of such concepts in a video, by introducing an advanced video\nanalysis and retrieval method for this purpose.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 09:37:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Erick", "Elejalde", ""], ["Damianos", "Galanopoulos", ""], ["Claudia", "Niederee", ""], ["Vasileios", "Mezaris", ""]]}, {"id": "2011.07203", "submitter": "Mahmoud Sayed", "authors": "Jason R. Baron, Mahmoud F. Sayed, Douglas W. Oard", "title": "Providing More Efficient Access To Government Records: A Use Case\n  Involving Application of Machine Learning to Improve FOIA Review for the\n  Deliberative Process Privilege", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  At present, the review process for material that is exempt from disclosure\nunder the Freedom of Information Act (FOIA) in the United States of America,\nand under many similar government transparency regimes worldwide, is entirely\nmanual. Public access to the records of their government is thus inhibited by\nthe long backlogs of material awaiting such reviews. This paper studies one\naspect of that problem by first creating a new public test collection with\nannotations for one class of exempt material, the deliberative process\nprivilege, and then by using that test collection to study the ability of\ncurrent text classification techniques to identify those materials that are\nexempt from release under that privilege. Results show that when the system is\ntrained and evaluated using annotations from the same reviewer that even\ndifficult cases can often be reliably detected, but that differences in\nreviewer interpretations, differences in record custodians, and that\ndifferences in topics of the records used for training and testing pose\nadditional challenges.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 02:56:49 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Baron", "Jason R.", ""], ["Sayed", "Mahmoud F.", ""], ["Oard", "Douglas W.", ""]]}, {"id": "2011.07208", "submitter": "Md Tahmid Rahman Laskar", "authors": "Md Tahmid Rahman Laskar, Enamul Hoque, Jimmy Xiangji Huang", "title": "Utilizing Bidirectional Encoder Representations from Transformers for\n  Answer Selection", "comments": "Accepted to the AMMCS 2019 Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training a transformer-based model for the language modeling task in a\nlarge dataset and then fine-tuning it for downstream tasks has been found very\nuseful in recent years. One major advantage of such pre-trained language models\nis that they can effectively absorb the context of each word in a sentence.\nHowever, for tasks such as the answer selection task, the pre-trained language\nmodels have not been extensively used yet. To investigate their effectiveness\nin such tasks, in this paper, we adopt the pre-trained Bidirectional Encoder\nRepresentations from Transformer (BERT) language model and fine-tune it on two\nQuestion Answering (QA) datasets and three Community Question Answering (CQA)\ndatasets for the answer selection task. We find that fine-tuning the BERT model\nfor the answer selection task is very effective and observe a maximum\nimprovement of 13.1% in the QA datasets and 18.7% in the CQA datasets compared\nto the previous state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 03:15:26 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Laskar", "Md Tahmid Rahman", ""], ["Hoque", "Enamul", ""], ["Huang", "Jimmy Xiangji", ""]]}, {"id": "2011.07222", "submitter": "Risul Islam", "authors": "Risul Islam, Md Omar Faruk Rokon, Ahmad Darki, Michalis Faloutsos", "title": "HackerScope: The Dynamics of a Massive Hacker Online Ecosystem", "comments": "8 pages, 7 figures, and 4 tables. In press of ASONAM'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Authors of malicious software are not hiding as much as one would assume:\nthey have a visible online footprint. Apart from online forums, this footprint\nappears in software development platforms, where authors create\npublicly-accessible malware repositories to share and collaborate. With the\nexception of a few recent efforts, the existence and the dynamics of this\ncommunity has received surprisingly limited attention. The goal of our work is\nto analyze this ecosystem of hackers in order to: (a) understand their\ncollaborative patterns, and (b) identify and profile its most influential\nauthors. We develop HackerScope, a systematic approach for analyzing the\ndynamics of this hacker ecosystem. Leveraging our targeted data collection, we\nconduct an extensive study of 7389 authors of malware repositories on GitHub,\nwhich we combine with their activity on four security forums. From a modeling\npoint of view, we study the ecosystem using three network representations: (a)\nthe author-author network, (b) the author-repository network, and (c)\ncross-platform egonets. Our analysis leads to the following key observations:\n(a) the ecosystem is growing at an accelerating rate as the number of new\nmalware authors per year triples every 2 years, (b) it is highly collaborative,\nmore so than the rest of GitHub authors, and (c) it includes influential and\nprofessional hackers. We find 30 authors maintain an online \"brand\" across\nGitHub and our security forums. Our study is a significant step towards using\npublic online information for understanding the malicious hacker community.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 05:19:54 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Islam", "Risul", ""], ["Rokon", "Md Omar Faruk", ""], ["Darki", "Ahmad", ""], ["Faloutsos", "Michalis", ""]]}, {"id": "2011.07226", "submitter": "Risul Islam", "authors": "Risul Islam, Md Omar Faruk Rokon, Evangelos E. Papalexakis, Michalis\n  Faloutsos", "title": "TenFor: A Tensor-Based Tool to Extract Interesting Events from Security\n  Forums", "comments": "8 pages, 5 figures, and 4 tables. In Press of ASONAM'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  How can we get a security forum to \"tell\" us its activities and events of\ninterest? We take a unique angle: we want to identify these activities without\nany a priori knowledge, which is a key difference compared to most of the\nprevious problem formulations. Despite some recent efforts, mining security\nforums to extract useful information has received relatively little attention,\nwhile most of them are usually searching for specific information. We propose\nTenFor, an unsupervised tensor-based approach, to systematically identify\nimportant events in a three-dimensional space: (a) user, (b) thread, and (c)\ntime. Our method consists of three high-level steps: (a) a tensor-based\nclustering across the three dimensions, (b) an extensive cluster profiling that\nuses both content and behavioral features, and (c) a deeper investigation,\nwhere we identify key users and threads within the events of interest. In\naddition, we implement our approach as a powerful and easy-to-use platform for\npractitioners. In our evaluation, we find that 83% of our clusters capture\nmeaningful events and we find more meaningful clusters compared to previous\napproaches. Our approach and our platform constitute an important step towards\ndetecting activities of interest from a forum in an unsupervised learning\nfashion in practice.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 05:59:07 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Islam", "Risul", ""], ["Rokon", "Md Omar Faruk", ""], ["Papalexakis", "Evangelos E.", ""], ["Faloutsos", "Michalis", ""]]}, {"id": "2011.07307", "submitter": "Shen Gao", "authors": "Shen Gao, Xiuying Chen, Zhaochun Ren, Dongyan Zhao and Rui Yan", "title": "Meaningful Answer Generation of E-Commerce Question-Answering", "comments": "Accepted By TOIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In e-commerce portals, generating answers for product-related questions has\nbecome a crucial task. In this paper, we focus on the task of product-aware\nanswer generation, which learns to generate an accurate and complete answer\nfrom large-scale unlabeled e-commerce reviews and product attributes. However,\nsafe answer problems pose significant challenges to text generation tasks, and\ne-commerce question-answering task is no exception. To generate more meaningful\nanswers, in this paper, we propose a novel generative neural model, called the\nMeaningful Product Answer Generator (MPAG), which alleviates the safe answer\nproblem by taking product reviews, product attributes, and a prototype answer\ninto consideration. Product reviews and product attributes are used to provide\nmeaningful content, while the prototype answer can yield a more diverse answer\npattern. To this end, we propose a novel answer generator with a review\nreasoning module and a prototype answer reader. Our key idea is to obtain the\ncorrect question-aware information from a large scale collection of reviews and\nlearn how to write a coherent and meaningful answer from an existing prototype\nanswer. To be more specific, we propose a read-and-write memory consisting of\nselective writing units to conduct reasoning among these reviews. We then\nemploy a prototype reader consisting of comprehensive matching to extract the\nanswer skeleton from the prototype answer. Finally, we propose an answer editor\nto generate the final answer by taking the question and the above parts as\ninput. Conducted on a real-world dataset collected from an e-commerce platform,\nextensive experimental results show that our model achieves state-of-the-art\nperformance in terms of both automatic metrics and human evaluations. Human\nevaluation also demonstrates that our model can consistently generate specific\nand proper answers.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 14:05:30 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Gao", "Shen", ""], ["Chen", "Xiuying", ""], ["Ren", "Zhaochun", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "2011.07359", "submitter": "Gourab K Patro", "authors": "Ashmi Banerjee, Gourab K Patro, Linus W. Dietz, Abhijnan Chakraborty", "title": "Analyzing 'Near Me' Services: Potential for Exposure Bias in\n  Location-based Retrieval", "comments": "International Workshop on Fair and Interpretable Learning Algorithms\n  (FILA 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of smartphones has led to the increased popularity of\nlocation-based search and recommendation systems. Online platforms like Google\nand Yelp allow location-based search in the form of nearby feature to query for\nhotels or restaurants in the vicinity. Moreover, hotel booking platforms like\nBooking[dot]com, Expedia, or Trivago allow travelers searching for\naccommodations using either their desired location as a search query or near a\nparticular landmark. Since the popularity of different locations in a city\nvaries, certain locations may get more queries than other locations. Thus, the\nexposure received by different establishments at these locations may be very\ndifferent from their intrinsic quality as captured in their ratings.\n  Today, many small businesses (shops, hotels, or restaurants) rely on such\nonline platforms for attracting customers. Thus, receiving less exposure than\nthat is expected can be unfavorable for businesses. It could have a negative\nimpact on their revenue and potentially lead to economic starvation or even\nshutdown. By gathering and analyzing data from three popular platforms, we\nobserve that many top-rated hotels and restaurants get less exposure vis-a-vis\ntheir quality, which could be detrimental for them. Following a meritocratic\nnotion, we define and quantify such exposure disparity due to location-based\nsearches on these platforms. We attribute this exposure disparity mainly to two\nkinds of biases -- Popularity Bias and Position Bias. Our experimental\nevaluation on multiple datasets reveals that although the platforms are doing\nwell in delivering distance-based results, exposure disparity exists for\nindividual businesses and needs to be reduced for business sustainability.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 18:39:39 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Banerjee", "Ashmi", ""], ["Patro", "Gourab K", ""], ["Dietz", "Linus W.", ""], ["Chakraborty", "Abhijnan", ""]]}, {"id": "2011.07363", "submitter": "Risul Islam", "authors": "Risul Islam, Md Omar Faruk Rokon, Evangelos E. Papalexakis, Michalis\n  Faloutsos", "title": "RecTen: A Recursive Hierarchical Low Rank Tensor Factorization Method to\n  Discover Hierarchical Patterns in Multi-modal Data", "comments": "9 pages, 9 figures, 1 table, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  How can we expand the tensor decomposition to reveal a hierarchical structure\nof the multi-modal data in a self-adaptive way? Current tensor decomposition\nprovides only a single layer of clusters. We argue that with the abundance of\nmultimodal data and time-evolving networks nowadays, the ability to identify\nemerging hierarchies is important. To this effect, we propose RecTen, a\nrecursive hierarchical soft clustering approach based on tensor decomposition.\nOur approach enables us to: (a) recursively decompose clusters identified in\nthe previous step, and (b) identify the right conditions for terminating this\nprocess. In the absence of proper ground truth, we evaluate our approach with\nsynthetic data and test its sensitivity to different parameters. We also apply\nRecTen on five real datasets which involve the activities of users in online\ndiscussion platforms, such as security forums. This analysis helps us reveal\nclusters of users with interesting behaviors, including but not limited to\nearly detection of some real events like ransomware outbreaks, the emergence of\na blackmarket of decryption tools, and romance scamming. To maximize the\nusefulness of our approach, we develop a tool which can help the data analysts\nand overall research community by identifying hierarchical structures. RecTen\nis an unsupervised approach which can be used to take the pulse of the large\nmulti-modal data and let the data discover its own hidden structures by itself.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 18:44:35 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Islam", "Risul", ""], ["Rokon", "Md Omar Faruk", ""], ["Papalexakis", "Evangelos E.", ""], ["Faloutsos", "Michalis", ""]]}, {"id": "2011.07368", "submitter": "Bhaskar Mitra", "authors": "Bhaskar Mitra, Sebastian Hofstatter, Hamed Zamani and Nick Craswell", "title": "Conformer-Kernel with Query Term Independence at TREC 2020 Deep Learning\n  Track", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We benchmark Conformer-Kernel models under the strict blind evaluation\nsetting of the TREC 2020 Deep Learning track. In particular, we study the\nimpact of incorporating: (i) Explicit term matching to complement matching\nbased on learned representations (i.e., the \"Duet principle\"), (ii) query term\nindependence (i.e., the \"QTI assumption\") to scale the model to the full\nretrieval setting, and (iii) the ORCAS click data as an additional document\ndescription field. We find evidence which supports that all three\naforementioned strategies can lead to improved retrieval quality.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 19:03:24 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 23:57:45 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Mitra", "Bhaskar", ""], ["Hofstatter", "Sebastian", ""], ["Zamani", "Hamed", ""], ["Craswell", "Nick", ""]]}, {"id": "2011.07546", "submitter": "Ruchit Agrawal", "authors": "Ruchit Agrawal, Simon Dixon", "title": "Learning Frame Similarity using Siamese networks for Audio-to-Score\n  Alignment", "comments": "Accepted at EUSIPCO 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Audio-to-score alignment aims at generating an accurate mapping between a\nperformance audio and the score of a given piece. Standard alignment methods\nare based on Dynamic Time Warping (DTW) and employ handcrafted features, which\ncannot be adapted to different acoustic conditions. We propose a method to\novercome this limitation using learned frame similarity for audio-to-score\nalignment. We focus on offline audio-to-score alignment of piano music.\nExperiments on music data from different acoustic conditions demonstrate that\nour method achieves higher alignment accuracy than a standard DTW-based method\nthat uses handcrafted features, and generates robust alignments whilst being\nadaptable to different domains at the same time.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 14:58:03 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Agrawal", "Ruchit", ""], ["Dixon", "Simon", ""]]}, {"id": "2011.07636", "submitter": "Amanpreet Singh", "authors": "Amanpreet Singh, Niranjan Balasubramanian", "title": "Open4Business(O4B): An Open Access Dataset for Summarizing Business\n  Documents", "comments": "7 pages, 3 figures, accepted in Workshop on Dataset Curation and\n  Security-NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A major challenge in fine-tuning deep learning models for automatic\nsummarization is the need for large domain specific datasets. One of the\nbarriers to curating such data from resources like online publications is\nnavigating the license regulations applicable to their re-use, especially for\ncommercial purposes. As a result, despite the availability of several business\njournals there are no large scale datasets for summarizing business documents.\nIn this work, we introduce Open4Business(O4B),a dataset of 17,458 open access\nbusiness articles and their reference summaries. The dataset introduces a new\nchallenge for summarization in the business domain, requiring highly\nabstractive and more concise summaries as compared to other existing datasets.\nAdditionally, we evaluate existing models on it and consequently show that\nmodels trained on O4B and a 7x larger non-open access dataset achieve\ncomparable performance on summarization. We release the dataset, along with the\ncode which can be leveraged to similarly gather data for multiple domains.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 22:00:07 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 21:50:18 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2020 21:19:23 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Singh", "Amanpreet", ""], ["Balasubramanian", "Niranjan", ""]]}, {"id": "2011.07734", "submitter": "Jiawei Chen", "authors": "Can Wang, Jiawei Chen, Sheng Zhou, Qihao Shi, Yan Feng, Chun Chen", "title": "SamWalker++: recommendation with informative sampling strategy", "comments": "14pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation from implicit feedback is a highly challenging task due to the\nlack of reliable negative feedback data. Existing methods address this\nchallenge by treating all the un-observed data as negative (dislike) but\ndownweight the confidence of these data. However, this treatment causes two\nproblems: (1) Confidence weights of the unobserved data are usually assigned\nmanually, which lack flexibility and may create empirical bias on evaluating\nuser's preference. (2) To handle massive volume of the unobserved feedback\ndata, most of the existing methods rely on stochastic inference and data\nsampling strategies. However, since a user is only aware of a very small\nfraction of items in a large dataset, it is difficult for existing samplers to\nselect informative training instances in which the user really dislikes the\nitem rather than does not know it.\n  To address the above two problems, we propose two novel recommendation\nmethods SamWalker and SamWalker++ that support both adaptive confidence\nassignment and efficient model learning. SamWalker models data confidence with\na social network-aware function, which can adaptively specify different weights\nto different data according to users' social contexts. However, the social\nnetwork information may not be available in many recommender systems, which\nhinders application of SamWalker. Thus, we further propose SamWalker++, which\ndoes not require any side information and models data confidence with a\nconstructed pseudo-social network. We also develop fast random-walk-based\nsampling strategies for our SamWalker and SamWalker++ to adaptively draw\ninformative training instances, which can speed up gradient estimation and\nreduce sampling variance. Extensive experiments on five real-world datasets\ndemonstrate the superiority of the proposed SamWalker and SamWalker++.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 05:40:42 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Wang", "Can", ""], ["Chen", "Jiawei", ""], ["Zhou", "Sheng", ""], ["Shi", "Qihao", ""], ["Feng", "Yan", ""], ["Chen", "Chun", ""]]}, {"id": "2011.07739", "submitter": "Jiawei Chen", "authors": "Jiawei Chen, Chengquan Jiang, Can Wang, Sheng Zhou, Yan Feng, Chun\n  Chen, Martin Ester, Xiangnan He", "title": "CoSam: An Efficient Collaborative Adaptive Sampler for Recommendation", "comments": "21pages, submitting to TOIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling strategies have been widely applied in many recommendation systems\nto accelerate model learning from implicit feedback data. A typical strategy is\nto draw negative instances with uniform distribution, which however will\nseverely affect model's convergency, stability, and even recommendation\naccuracy. A promising solution for this problem is to over-sample the\n``difficult'' (a.k.a informative) instances that contribute more on training.\nBut this will increase the risk of biasing the model and leading to non-optimal\nresults. Moreover, existing samplers are either heuristic, which require domain\nknowledge and often fail to capture real ``difficult'' instances; or rely on a\nsampler model that suffers from low efficiency.\n  To deal with these problems, we propose an efficient and effective\ncollaborative sampling method CoSam, which consists of: (1) a collaborative\nsampler model that explicitly leverages user-item interaction information in\nsampling probability and exhibits good properties of normalization, adaption,\ninteraction information awareness, and sampling efficiency; and (2) an\nintegrated sampler-recommender framework, leveraging the sampler model in\nprediction to offset the bias caused by uneven sampling. Correspondingly, we\nderive a fast reinforced training algorithm of our framework to boost the\nsampler performance and sampler-recommender collaboration. Extensive\nexperiments on four real-world datasets demonstrate the superiority of the\nproposed collaborative sampler model and integrated sampler-recommender\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 06:18:00 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Chen", "Jiawei", ""], ["Jiang", "Chengquan", ""], ["Wang", "Can", ""], ["Zhou", "Sheng", ""], ["Feng", "Yan", ""], ["Chen", "Chun", ""], ["Ester", "Martin", ""], ["He", "Xiangnan", ""]]}, {"id": "2011.07783", "submitter": "Ziyang Wang", "authors": "Ziyang Wang, Wei Wei, Xian-Ling Mao, Guibing Guo, Pan Zhou and\n  Shanshan Feng", "title": "User-based Network Embedding for Collective Opinion Spammer Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the huge commercial interests behind online reviews, a\ntremendousamount of spammers manufacture spam reviews for product reputation\nmanipulation. To further enhance the influence of spam reviews, spammers often\ncollaboratively post spam reviewers within a short period of time, the\nactivities of whom are called collective opinion spam campaign. As the goals\nand members of the spam campaign activities change frequently, and some\nspammers also imitate normal purchases to conceal identity, which makes the\nspammer detection challenging. In this paper, we propose an unsupervised\nnetwork embedding-based approach to jointly exploiting different types of\nrelations, e.g., direct common behaviour relation and indirect co-reviewed\nrelation to effectively represent the relevances of users for detecting the\ncollective opinion spammers. The average improvements of our method over the\nstate-of-the-art solutions on dataset AmazonCn and YelpHotel are\n[14.09%,12.04%] and [16.25%,12.78%] in terms of AP and AUC, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 08:24:34 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Wang", "Ziyang", ""], ["Wei", "Wei", ""], ["Mao", "Xian-Ling", ""], ["Guo", "Guibing", ""], ["Zhou", "Pan", ""], ["Feng", "Shanshan", ""]]}, {"id": "2011.07931", "submitter": "Wenshuo Guo", "authors": "Karl Krauth, Sarah Dean, Alex Zhao, Wenshuo Guo, Mihaela Curmei,\n  Benjamin Recht, Michael I. Jordan", "title": "Do Offline Metrics Predict Online Performance in Recommender Systems?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems operate in an inherently dynamical setting. Past\nrecommendations influence future behavior, including which data points are\nobserved and how user preferences change. However, experimenting in production\nsystems with real user dynamics is often infeasible, and existing\nsimulation-based approaches have limited scale. As a result, many\nstate-of-the-art algorithms are designed to solve supervised learning problems,\nand progress is judged only by offline metrics. In this work we investigate the\nextent to which offline metrics predict online performance by evaluating eleven\nrecommenders across six controlled simulated environments. We observe that\noffline metrics are correlated with online performance over a range of\nenvironments. However, improvements in offline metrics lead to diminishing\nreturns in online performance. Furthermore, we observe that the ranking of\nrecommenders varies depending on the amount of initial offline data available.\nWe study the impact of adding exploration strategies, and observe that their\neffectiveness, when compared to greedy recommendation, is highly dependent on\nthe recommendation algorithm. We provide the environments and recommenders\ndescribed in this paper as Reclab: an extensible ready-to-use simulation\nframework at https://github.com/berkeley-reclab/RecLab.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 01:41:13 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Krauth", "Karl", ""], ["Dean", "Sarah", ""], ["Zhao", "Alex", ""], ["Guo", "Wenshuo", ""], ["Curmei", "Mihaela", ""], ["Recht", "Benjamin", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2011.07959", "submitter": "Rahul Yedida", "authors": "Rahul Yedida, Saad Mohammad Abrar, Cleber Melo-Filho, Eugene Muratov,\n  Rada Chirkova, Alexander Tropsha", "title": "Text Mining to Identify and Extract Novel Disease Treatments From\n  Unstructured Datasets", "comments": "initial submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective: We aim to learn potential novel cures for diseases from\nunstructured text sources. More specifically, we seek to extract drug-disease\npairs of potential cures to diseases by a simple reasoning over the structure\nof spoken text.\n  Materials and Methods: We use Google Cloud to transcribe podcast episodes of\nan NPR radio show. We then build a pipeline for systematically pre-processing\nthe text to ensure quality input to the core classification model, which feeds\nto a series of post-processing steps for obtaining filtered results. Our\nclassification model itself uses a language model pre-trained on PubMed text.\nThe modular nature of our pipeline allows for ease of future developments in\nthis area by substituting higher quality components at each stage of the\npipeline. As a validation measure, we use ROBOKOP, an engine over a medical\nknowledge graph with only validated pathways, as a ground truth source for\nchecking the existence of the proposed pairs. For the proposed pairs not found\nin ROBOKOP, we provide further verification using Chemotext.\n  Results: We found 30.4% of our proposed pairs in the ROBOKOP database. For\nexample, our model successfully identified that Omeprazole can help treat\nheartburn.We discuss the significance of this result, showing some examples of\nthe proposed pairs.\n  Discussion and Conclusion: The agreement of our results with the existing\nknowledge source indicates a step in the right direction. Given the\nplug-and-play nature of our framework, it is easy to add, remove, or modify\nparts to improve the model as necessary. We discuss the results showing some\nexamples, and note that this is a potentially new line of research that has\nfurther scope to be explored. Although our approach was originally oriented on\nradio podcast transcripts, it is input-agnostic and could be applied to any\nsource of textual data and to any problem of interest.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 19:52:49 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Yedida", "Rahul", ""], ["Abrar", "Saad Mohammad", ""], ["Melo-Filho", "Cleber", ""], ["Muratov", "Eugene", ""], ["Chirkova", "Rada", ""], ["Tropsha", "Alexander", ""]]}, {"id": "2011.07964", "submitter": "Martin Hole\\v{c}ek", "authors": "Martin Hole\\v{c}ek", "title": "Learning from similarity and information extraction from structured\n  documents", "comments": "17 pages, 9 figures, manuscript for the IJDAR journal special issue\n  for ICDAR conference", "journal-ref": "Hole\\v{c}ek, M. 2021 Learning from similarity and information\n  extraction from structured documents; International Journal on Document\n  Analysis and Recognition (IJDAR) 2021/06/11", "doi": "10.1007/s10032-021-00375-3", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automation of document processing is gaining recent attention due to the\ngreat potential to reduce manual work through improved methods and hardware.\nNeural networks have been successfully applied before - even though they have\nbeen trained only on relatively small datasets with hundreds of documents so\nfar. To successfully explore deep learning techniques and improve the\ninformation extraction results, a dataset with more than twenty-five thousand\ndocuments has been compiled, anonymized and is published as a part of this\nwork. We will expand our previous work where we proved that convolutions, graph\nconvolutions and self-attention can work together and exploit all the\ninformation present in a structured document. Taking the fully trainable method\none step further, we will now design and examine various approaches to using\nsiamese networks, concepts of similarity, one-shot learning and context/memory\nawareness. The aim is to improve micro F1 of per-word classification on the\nhuge real-world document dataset. The results verify the hypothesis that\ntrainable access to a similar (yet still different) page together with its\nalready known target information improves the information extraction.\nFurthermore, the experiments confirm that all proposed architecture parts are\nall required to beat the previous results. The best model improves the previous\nstate-of-the-art results by an 8.25 gain in F1 score. Qualitative analysis is\nprovided to verify that the new model performs better for all target classes.\nAdditionally, multiple structural observations about the causes of the\nunderperformance of some architectures are revealed. All the source codes,\nparameters and implementation details are published together with the dataset\nin the hope to push the research boundaries since all the techniques used in\nthis work are not problem-specific and can be generalized for other tasks and\ncontexts.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 21:34:52 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 21:36:56 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hole\u010dek", "Martin", ""]]}, {"id": "2011.07999", "submitter": "Diego D\\'iaz-Dom\\'inguez", "authors": "Diego D\\'iaz-Dom\\'inguez and Gonzalo Navarro", "title": "A grammar compressor for collections of reads with applications to the\n  construction of the BWT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a grammar for DNA sequencing reads from which we can compute the\nBWT directly. Our motivation is to perform in succinct space genomic analyses\nthat require complex string queries not yet supported by repetition-based\nself-indexes. Our approach is to store the set of reads as a grammar, but when\nrequired, compute its BWT to carry out the analysis by using self-indexes. Our\nexperiments in real data showed that the space reduction we achieve with our\ncompressor is competitive with LZ-based methods and better than entropy-based\napproaches. Compared to other popular grammars, in this kind of data, we\nachieve, on average, 12\\% of extra compression and require less working space\nand time.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 03:16:02 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["D\u00edaz-Dom\u00ednguez", "Diego", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "2011.08071", "submitter": "Ha Thanh Nguyen", "authors": "Ha-Thanh Nguyen, Hai-Yen Thi Vuong, Phuong Minh Nguyen, Binh Tran\n  Dang, Quan Minh Bui, Sinh Trong Vu, Chau Minh Nguyen, Vu Tran, Ken Satoh,\n  Minh Le Nguyen", "title": "JNLP Team: Deep Learning for Legal Processing in COLIEE 2020", "comments": "Also be published in JURISIN2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose deep learning based methods for automatic systems of legal\nretrieval and legal question-answering in COLIEE 2020. These systems are all\ncharacterized by being pre-trained on large amounts of data before being\nfinetuned for the specified tasks. This approach helps to overcome the data\nscarcity and achieve good performance, thus can be useful for tackling related\nproblems in information retrieval, and decision support in the legal domain.\nBesides, the approach can be explored to deal with other domain specific\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 06:14:11 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Nguyen", "Ha-Thanh", ""], ["Vuong", "Hai-Yen Thi", ""], ["Nguyen", "Phuong Minh", ""], ["Dang", "Binh Tran", ""], ["Bui", "Quan Minh", ""], ["Vu", "Sinh Trong", ""], ["Nguyen", "Chau Minh", ""], ["Tran", "Vu", ""], ["Satoh", "Ken", ""], ["Nguyen", "Minh Le", ""]]}, {"id": "2011.08072", "submitter": "Swati Padhee", "authors": "Amanuel Alambo, Cori Lohstroh, Erik Madaus, Swati Padhee, Brandy\n  Foster, Tanvi Banerjee, Krishnaprasad Thirunarayan, Michael Raymer", "title": "Topic-Centric Unsupervised Multi-Document Summarization of Scientific\n  and News Articles", "comments": "6 pages, 6 Figures, 8 Tables. Accepted at IEEE Big Data 2020\n  (https://bigdataieee.org/BigData2020/AcceptedPapers.html)", "journal-ref": null, "doi": null, "report-no": "BigD420", "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in natural language processing have enabled automation of a\nwide range of tasks, including machine translation, named entity recognition,\nand sentiment analysis. Automated summarization of documents, or groups of\ndocuments, however, has remained elusive, with many efforts limited to\nextraction of keywords, key phrases, or key sentences. Accurate abstractive\nsummarization has yet to be achieved due to the inherent difficulty of the\nproblem, and limited availability of training data. In this paper, we propose a\ntopic-centric unsupervised multi-document summarization framework to generate\nextractive and abstractive summaries for groups of scientific articles across\n20 Fields of Study (FoS) in Microsoft Academic Graph (MAG) and news articles\nfrom DUC-2004 Task 2. The proposed algorithm generates an abstractive summary\nby developing salient language unit selection and text generation techniques.\nOur approach matches the state-of-the-art when evaluated on automated\nextractive evaluation metrics and performs better for abstractive summarization\non five human evaluation metrics (entailment, coherence, conciseness,\nreadability, and grammar). We achieve a kappa score of 0.68 between two\nco-author linguists who evaluated our results. We plan to publicly share\nMAG-20, a human-validated gold standard dataset of topic-clustered research\narticles and their summaries to promote research in abstractive summarization.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 04:04:21 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Alambo", "Amanuel", ""], ["Lohstroh", "Cori", ""], ["Madaus", "Erik", ""], ["Padhee", "Swati", ""], ["Foster", "Brandy", ""], ["Banerjee", "Tanvi", ""], ["Thirunarayan", "Krishnaprasad", ""], ["Raymer", "Michael", ""]]}, {"id": "2011.08090", "submitter": "Arash Shaban-Nejad", "authors": "Nariman Ammar, Arash Shaban-Nejad", "title": "Explainable Artificial Intelligence Recommendation System by Leveraging\n  the Semantics of Adverse Childhood Experiences: Proof-of-Concept Prototype\n  Development", "comments": "15 Pages, 7 Figures", "journal-ref": "JMIR Med Inform. 2020 Nov 4;8(11):e18752", "doi": "10.2196/18752", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study of adverse childhood experiences and their consequences has emerged\nover the past 20 years. In this study, we aimed to leverage explainable\nartificial intelligence, and propose a proof-of-concept prototype for a\nknowledge-driven evidence-based recommendation system to improve surveillance\nof adverse childhood experiences. We used concepts from an ontology that we\nhave developed to build and train a question-answering agent using the Google\nDialogFlow engine. In addition to the question-answering agent, the initial\nprototype includes knowledge graph generation and recommendation components\nthat leverage third-party graph technology. To showcase the framework\nfunctionalities, we here present a prototype design and demonstrate the main\nfeatures through four use case scenarios motivated by an initiative currently\nimplemented at a children hospital in Memphis, Tennessee. Ongoing development\nof the prototype requires implementing an optimization algorithm of the\nrecommendations, incorporating a privacy layer through a personal health\nlibrary, and conducting a clinical trial to assess both usability and\nusefulness of the implementation. This semantic-driven explainable artificial\nintelligence prototype can enhance health care practitioners ability to provide\nexplanations for the decisions they make.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 17:45:15 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Ammar", "Nariman", ""], ["Shaban-Nejad", "Arash", ""]]}, {"id": "2011.08091", "submitter": "Fabrizio Sebastiani", "authors": "Alejandro Moreo and Fabrizio Sebastiani", "title": "Tweet Sentiment Quantification: An Experimental Re-Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment quantification is the task of estimating the relative frequency (or\n\"prevalence\") of sentiment-related classes (such as Positive, Neutral,\nNegative) in a sample of unlabelled texts; this is especially important when\nthese texts are tweets, since most sentiment classification endeavours carried\nout on Twitter data actually have quantification (and not the classification of\nindividual tweets) as their ultimate goal. It is well-known that solving\nquantification via \"classify and count\" (i.e., by classifying all unlabelled\nitems via a standard classifier and counting the items that have been assigned\nto a given class) is suboptimal in terms of accuracy, and that more accurate\nquantification methods exist. In 2016, Gao and Sebastiani carried out a\nsystematic comparison of quantification methods on the task of tweet sentiment\nquantification. In hindsight, we observe that the experimental protocol\nfollowed in that work is flawed, and that its results are thus unreliable. We\nnow re-evaluate those quantification methods on the very same datasets, this\ntime following a now consolidated and much more robust experimental protocol,\nthat involves 5775 as many experiments as run in the original study. Our\nexperimentation yields results dramatically different from those obtained by\nGao and Sebastiani, and thus provide a different, much more solid understanding\nof the relative strengths and weaknesses of different sentiment quantification\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 21:41:34 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 10:49:24 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Moreo", "Alejandro", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "2011.08127", "submitter": "Akshar Nair", "authors": "Alexandra Gkolia, Nikhil Fernandes, Nicolas Pizzo, James Davenport and\n  Akshar Nair", "title": "The Influence of Domain-Based Preprocessing on Subject-Specific\n  Clustering", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sudden change of moving the majority of teaching online at Universities\ndue to the global Covid-19 pandemic has caused an increased amount of workload\nfor academics. One of the contributing factors is answering a high volume of\nqueries coming from students. As these queries are not limited to the\nsynchronous time frame of a lecture, there is a high chance of many of them\nbeing related or even equivalent. One way to deal with this problem is to\ncluster these questions depending on their topic. In our previous work, we\naimed to find an improved method of clustering that would give us a high\nefficiency, using a recurring LDA model. Our data set contained questions\nposted online from a Computer Science course at the University of Bath. A\nsignificant number of these questions contained code excerpts, which we found\ncaused a problem in clustering, as certain terms were being considered as\ncommon words in the English language and not being recognised as specific code\nterms. To address this, we implemented tagging of these technical terms using\nPython, as part of preprocessing the data set. In this paper, we explore the\nrealms of tagging data sets, focusing on identifying code excerpts and\nproviding empirical results in order to justify our reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 17:47:19 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Gkolia", "Alexandra", ""], ["Fernandes", "Nikhil", ""], ["Pizzo", "Nicolas", ""], ["Davenport", "James", ""], ["Nair", "Akshar", ""]]}, {"id": "2011.08225", "submitter": "Noy Cohen-Shapira", "authors": "Noy Cohen-Shapira and Lior Rokach", "title": "Automatic selection of clustering algorithms using supervised graph\n  embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The widespread adoption of machine learning (ML) techniques and the extensive\nexpertise required to apply them have led to increased interest in automated ML\nsolutions that reduce the need for human intervention. One of the main\nchallenges in applying ML to previously unseen problems is algorithm selection\n- the identification of high-performing algorithm(s) for a given dataset, task,\nand evaluation measure. This study addresses the algorithm selection challenge\nfor data clustering, a fundamental task in data mining that is aimed at\ngrouping similar objects. We present MARCO-GE, a novel meta-learning approach\nfor the automated recommendation of clustering algorithms. MARCO-GE first\ntransforms datasets into graphs and then utilizes a graph convolutional neural\nnetwork technique to extract their latent representation. Using the embedding\nrepresentations obtained, MARCO-GE trains a ranking meta-model capable of\naccurately recommending top-performing algorithms for a new dataset and\nclustering evaluation measure. Extensive evaluation on 210 datasets, 13\nclustering algorithms, and 10 clustering measures demonstrates the\neffectiveness of our approach and its superiority in terms of predictive and\ngeneralization performance over state-of-the-art clustering meta-learning\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 19:13:20 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 19:09:22 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Cohen-Shapira", "Noy", ""], ["Rokach", "Lior", ""]]}, {"id": "2011.08431", "submitter": "Zhenghao Zhang", "authors": "Zhenghao Zhang, Jianbin Huang and Qinglin Tan", "title": "Association Rules Enhanced Knowledge Graph Attention Network", "comments": "19 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:1906.01195 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most existing knowledge graphs suffer from incompleteness. Embedding\nknowledge graphs into continuous vector spaces has recently attracted\nincreasing interest in knowledge base completion. However, in most existing\nembedding methods, only fact triplets are utilized, and logical rules have not\nbeen thoroughly studied for the knowledge base completion task. To overcome the\nproblem, we propose an association rules enhanced knowledge graph attention\nnetwork (AR-KGAT). The AR-KGAT captures both entity and relation features for\nhigh-order neighborhoods of any given entity in an end-to-end manner under the\ngraph attention network framework. The major component of AR-KGAT is an encoder\nof an effective neighborhood aggregator, which addresses the problems by\naggregating neighbors with both association-rules-based and graph-based\nattention weights. Additionally, the proposed model also encapsulates the\nrepresentations from multi-hop neighbors of nodes to refine their embeddings.\nThe decoder enables AR-KGAT to be translational between entities and relations\nwhile keeping the superior link prediction performance. A logic-like inference\npattern is utilized as constraints for knowledge graph embedding. Then, the\nglobal loss is minimized over both atomic and complex formulas to achieve the\nembedding task. In this manner, we learn embeddings compatible with triplets\nand rules, which are certainly more predictive for knowledge acquisition and\ninference. We conduct extensive experiments on two benchmark datasets: WN18RR\nand FB15k-237, for two knowledge graph completion tasks: the link prediction\nand triplet classification to evaluate the proposed AR-KGAT model. The results\nshow that the proposed AR-KGAT model achieves significant and consistent\nimprovements over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 13:18:55 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Zhang", "Zhenghao", ""], ["Huang", "Jianbin", ""], ["Tan", "Qinglin", ""]]}, {"id": "2011.08606", "submitter": "Deeksha Sinha", "authors": "Vivek F. Farias, Andrew A. Li, and Deeksha Sinha", "title": "Optimizing Offer Sets in Sub-Linear Time", "comments": "30 pages, 3 figures. Proceedings of the 21st ACM Conference on\n  Economics and Computation. 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalization and recommendations are now accepted as core competencies in\njust about every online setting, ranging from media platforms to e-commerce to\nsocial networks. While the challenge of estimating user preferences has\ngarnered significant attention, the operational problem of using such\npreferences to construct personalized offer sets to users is still a challenge,\nparticularly in modern settings where a massive number of items and a\nmillisecond response time requirement mean that even enumerating all of the\nitems is impossible. Faced with such settings, existing techniques are either\n(a) entirely heuristic with no principled justification, or (b) theoretically\nsound, but simply too slow to work.\n  Thus motivated, we propose an algorithm for personalized offer set\noptimization that runs in time sub-linear in the number of items while enjoying\na uniform performance guarantee. Our algorithm works for an extremely general\nclass of problems and models of user choice that includes the mixed multinomial\nlogit model as a special case. We achieve a sub-linear runtime by leveraging\nthe dimensionality reduction from learning an accurate latent factor model,\nalong with existing sub-linear time approximate near neighbor algorithms. Our\nalgorithm can be entirely data-driven, relying on samples of the user, where a\n`sample' refers to the user interaction data typically collected by firms. We\nevaluate our approach on a massive content discovery dataset from Outbrain that\nincludes millions of advertisements. Results show that our implementation\nindeed runs fast and with increased performance relative to existing fast\nheuristics.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 13:02:56 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Farias", "Vivek F.", ""], ["Li", "Andrew A.", ""], ["Sinha", "Deeksha", ""]]}, {"id": "2011.09553", "submitter": "Yue Feng", "authors": "Yue Feng, Yang Wang, Hang Li", "title": "A Sequence-to-Sequence Approach to Dialogue State Tracking", "comments": "Accepted by ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is concerned with dialogue state tracking (DST) in a task-oriented\ndialogue system. Building a DST module that is highly effective is still a\nchallenging issue, although significant progresses have been made recently.\nThis paper proposes a new approach to dialogue state tracking, referred to as\nSeq2Seq-DU, which formalizes DST as a sequence-to-sequence problem. Seq2Seq-DU\nemploys two BERT-based encoders to respectively encode the utterances in the\ndialogue and the descriptions of schemas, an attender to calculate attentions\nbetween the utterance embeddings and the schema embeddings, and a decoder to\ngenerate pointers to represent the current state of dialogue. Seq2Seq-DU has\nthe following advantages. It can jointly model intents, slots, and slot values;\nit can leverage the rich representations of utterances and schemas based on\nBERT; it can effectively deal with categorical and non-categorical slots, and\nunseen schemas. In addition, Seq2Seq-DU can also be used in the NLU (natural\nlanguage understanding) module of a dialogue system. Experimental results on\nbenchmark datasets in different settings (SGD, MultiWOZ2.2, MultiWOZ2.1,\nWOZ2.0, DSTC2, M2M, SNIPS, and ATIS) show that Seq2Seq-DU outperforms the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 21:42:44 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 02:48:28 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Feng", "Yue", ""], ["Wang", "Yang", ""], ["Li", "Hang", ""]]}, {"id": "2011.09580", "submitter": "Luis Miguel Vaquero Gonzalez", "authors": "Kentaro Takiguchi, Niall Twomey, Luis M. Vaquero", "title": "Non-Linear Multiple Field Interactions Neural Document Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Ranking tasks are usually based on the text of the main body of the page and\nthe actions (clicks) of users on the page. There are other elements that could\nbe leveraged to better contextualise the ranking experience (e.g. text in other\nfields, query made by the user, images, etc). We present one of the first\nin-depth analyses of field interaction for multiple field ranking in two\nseparate datasets. While some works have taken advantage of full document\nstructure, some aspects remain unexplored. In this work we build on previous\nanalyses to show how query-field interactions, non-linear field interactions,\nand the architecture of the underlying neural model affect performance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 23:13:33 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Takiguchi", "Kentaro", ""], ["Twomey", "Niall", ""], ["Vaquero", "Luis M.", ""]]}, {"id": "2011.09752", "submitter": "Athanasios Lagopoulos", "authors": "Athanasios Lagopoulos, Grigorios Tsoumakas", "title": "From Protocol to Screening: A Hybrid Learning Approach for\n  Technology-Assisted Systematic Literature Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the medical domain, a Systematic Literature Review (SLR) attempts to\ncollect all empirical evidence, that fit pre-specified eligibility criteria, in\norder to answer a specific research question. The process of preparing an SLR\nconsists of multiple tasks that are labor-intensive and time-consuming,\ninvolving large monetary costs. Technology-assisted review (TAR) methods\nautomate the different processes of creating an SLR and they are particularly\nfocused on reducing the burden of screening for reviewers. We present a novel\nmethod for TAR that implements a full pipeline from the research protocol to\nthe screening of the relevant papers. Our pipeline overcomes the need of a\nBoolean query constructed by specialists and consists of three different\ncomponents: the primary retrieval engine, the inter-review ranker and the\nintra-review ranker, combining learning-to-rank techniques with a relevance\nfeedback method. In addition, we contribute an updated version of the Task 2 of\nthe CLEF 2019 eHealth Lab dataset, which we make publicly available. Empirical\nresults on this dataset show that our approach can achieve state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 10:03:01 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Lagopoulos", "Athanasios", ""], ["Tsoumakas", "Grigorios", ""]]}, {"id": "2011.10106", "submitter": "Firoj Alam", "authors": "Md. Arid Hasan, Jannatul Tajrin, Shammur Absar Chowdhury, Firoj Alam", "title": "Sentiment Classification in Bangla Textual Content: A Comparative Study", "comments": "Accepted at ICCIT-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis has been widely used to understand our views on social and\npolitical agendas or user experiences over a product. It is one of the cores\nand well-researched areas in NLP. However, for low-resource languages, like\nBangla, one of the prominent challenge is the lack of resources. Another\nimportant limitation, in the current literature for Bangla, is the absence of\ncomparable results due to the lack of a well-defined train/test split. In this\nstudy, we explore several publicly available sentiment labeled datasets and\ndesigned classifiers using both classical and deep learning algorithms. In our\nstudy, the classical algorithms include SVM and Random Forest, and deep\nlearning algorithms include CNN, FastText, and transformer-based models. We\ncompare these models in terms of model performance and time-resource\ncomplexity. Our finding suggests transformer-based models, which have not been\nexplored earlier for Bangla, outperform all other models. Furthermore, we\ncreated a weighted list of lexicon content based on the valence score per\nclass. We then analyzed the content for high significance entries per class, in\nthe datasets. For reproducibility, we make publicly available data splits and\nthe ranked lexicon list. The presented results can be used for future studies\nas a benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 21:06:28 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Hasan", "Md. Arid", ""], ["Tajrin", "Jannatul", ""], ["Chowdhury", "Shammur Absar", ""], ["Alam", "Firoj", ""]]}, {"id": "2011.10173", "submitter": "Ziyang Wang", "authors": "Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, Minghui\n  Qiu, Shanshan Feng", "title": "Exploring Global Information for Session-based Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommendation (SBR) is a challenging task, which aims at\nrecommending items based on anonymous behavior sequences. Most existing SBR\nstudies model the user preferences based only on the current session while\nneglecting the item-transition information from the other sessions, which\nsuffer from the inability of modeling the complicated item-transition pattern.\nTo address the limitations, we introduce global item-transition information to\nstrength the modeling of the dynamic item-transition. For fully exploiting the\nglobal item-transition information, two ways of exploring global information\nfor SBR are studied in this work. Specifically, we first propose a basic\nGNN-based framework (BGNN), which solely uses session-level item-transition\ninformation on session graph. Based on BGNN, we propose a novel approach,\ncalled Session-based Recommendation with Global Information (SRGI), which\ninfers the user preferences via fully exploring global item-transitions over\nall sessions from two different perspectives: (i) Fusion-based Model (SRGI-FM),\nwhich recursively incorporates the neighbor embeddings of each node on global\ngraph into the learning process of session level item representation; and (ii)\nConstrained-based Model (SRGI-CM), which treats the global-level\nitem-transition information as a constraint to ensure the learned item\nembeddings are consistent with the global item-transition. Extensive\nexperiments conducted on three popular benchmark datasets demonstrate that both\nSRGI-FM and SRGI-CM outperform the state-of-the-art methods consistently.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 02:22:31 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 16:23:58 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wang", "Ziyang", ""], ["Wei", "Wei", ""], ["Cong", "Gao", ""], ["Li", "Xiao-Li", ""], ["Mao", "Xian-Ling", ""], ["Qiu", "Minghui", ""], ["Feng", "Shanshan", ""]]}, {"id": "2011.10187", "submitter": "Sarkar Snigdha Sarathi Das", "authors": "Md. Ashraful Islam, Mir Mahathir Mohammad, Sarkar Snigdha Sarathi Das,\n  Mohammed Eunus Ali", "title": "A Survey on Deep Learning Based Point-Of-Interest (POI) Recommendations", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location-based Social Networks (LBSNs) enable users to socialize with friends\nand acquaintances by sharing their check-ins, opinions, photos, and reviews.\nHuge volume of data generated from LBSNs opens up a new avenue of research that\ngives birth to a new sub-field of recommendation systems, known as\nPoint-of-Interest (POI) recommendation. A POI recommendation technique\nessentially exploits users' historical check-ins and other multi-modal\ninformation such as POI attributes and friendship network, to recommend the\nnext set of POIs suitable for a user. A plethora of earlier works focused on\ntraditional machine learning techniques by using hand-crafted features from the\ndataset. With the recent surge of deep learning research, we have witnessed a\nlarge variety of POI recommendation works utilizing different deep learning\nparadigms. These techniques largely vary in problem formulations, proposed\ntechniques, used datasets, and features, etc. To the best of our knowledge,\nthis work is the first comprehensive survey of all major deep learning-based\nPOI recommendation works. Our work categorizes and critically analyzes the\nrecent POI recommendation works based on different deep learning paradigms and\nother relevant features. This review can be considered a cookbook for\nresearchers or practitioners working in the area of POI recommendation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 02:55:52 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Islam", "Md. Ashraful", ""], ["Mohammad", "Mir Mahathir", ""], ["Das", "Sarkar Snigdha Sarathi", ""], ["Ali", "Mohammed Eunus", ""]]}, {"id": "2011.10216", "submitter": "Joel Jang", "authors": "Joel Jang, Yoonjeon Kim, Kyoungho Choi, Sungho Suh", "title": "Sequential Targeting: an incremental learning approach for data\n  imbalance in text classification", "comments": "9 pages, 7 figures, submitted to the journal of Expert Systems with\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classification tasks require a balanced distribution of data to ensure the\nlearner to be trained to generalize over all classes. In real-world datasets,\nhowever, the number of instances vary substantially among classes. This\ntypically leads to a learner that promotes bias towards the majority group due\nto its dominating property. Therefore, methods to handle imbalanced datasets\nare crucial for alleviating distributional skews and fully utilizing the\nunder-represented data, especially in text classification. While addressing the\nimbalance in text data, most methods utilize sampling methods on the numerical\nrepresentation of the data, which limits its efficiency on how effective the\nrepresentation is. We propose a novel training method, Sequential\nTargeting(ST), independent of the effectiveness of the representation method,\nwhich enforces an incremental learning setting by splitting the data into\nmutually exclusive subsets and training the learner adaptively. To address\nproblems that arise within incremental learning, we apply elastic weight\nconsolidation. We demonstrate the effectiveness of our method through\nexperiments on simulated benchmark datasets (IMDB) and data collected from\nNAVER.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 04:54:00 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 02:33:08 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Jang", "Joel", ""], ["Kim", "Yoonjeon", ""], ["Choi", "Kyoungho", ""], ["Suh", "Sungho", ""]]}, {"id": "2011.10239", "submitter": "Fangrui Liu", "authors": "Fangrui Liu, Zheng Liu", "title": "Shuffle and Learn: Minimizing Mutual Information for Unsupervised\n  Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unsupervised binary representation allows fast data retrieval without any\nannotations, enabling practical application like fast person re-identification\nand multimedia retrieval. It is argued that conflicts in binary space are one\nof the major barriers to high-performance unsupervised hashing as current\nmethods failed to capture the precise code conflicts in the full domain. A\nnovel relaxation method called Shuffle and Learn is proposed to tackle code\nconflicts in the unsupervised hash. Approximated derivatives for joint\nprobability and the gradients for the binary layer are introduced to bridge the\nupdate from the hash to the input. Proof on $\\epsilon$-Convergence of joint\nprobability with approximated derivatives is provided to guarantee the\npreciseness on update applied on the mutual information. The proposed algorithm\nis carried out with iterative global updates to minimize mutual information,\ndiverging the code before regular unsupervised optimization. Experiments\nsuggest that the proposed method can relax the code optimization from local\noptimum and help to generate binary representations that are more\ndiscriminative and informative without any annotations. Performance benchmarks\non image retrieval with the unsupervised binary code are conducted on three\nopen datasets, and the model achieves state-of-the-art accuracy on image\nretrieval task for all those datasets. Datasets and reproducible code are\nprovided.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 07:14:55 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Liu", "Fangrui", ""], ["Liu", "Zheng", ""]]}, {"id": "2011.10337", "submitter": "Shivam Pal", "authors": "Shivam Pal, Vipul Arora, Pawan Goyal", "title": "Finding Prerequisite Relations between Concepts using Textbook", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A prerequisite is anything that you need to know or understand first before\nattempting to learn or understand something new. In the current work, we\npresent a method of finding prerequisite relations between concepts using\nrelated textbooks. Previous researchers have focused on finding these relations\nusing Wikipedia link structure through unsupervised and supervised learning\napproaches. In the current work, we have proposed two methods, one is\nstatistical method and another is learning-based method. We mine the rich and\nstructured knowledge available in the textbooks to find the content for those\nconcepts and the order in which they are discussed. Using this information,\nproposed statistical method estimates explicit as well as implicit prerequisite\nrelations between concepts. During experiments, we have found performance of\nproposed statistical method is better than the popular RefD method, which uses\nWikipedia link structure. And proposed learning-based method has shown a\nsignificant increase in the efficiency of supervised learning method when\ncompared with graph and text-based learning-based approaches.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 10:58:31 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Pal", "Shivam", ""], ["Arora", "Vipul", ""], ["Goyal", "Pawan", ""]]}, {"id": "2011.10358", "submitter": "Dinesh Kumar Vishwakarma Dr", "authors": "Ashima Yadav, Dinesh Kumar Vishwakarma", "title": "A Deep Language-independent Network to analyze the impact of COVID-19 on\n  the World via Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Towards the end of 2019, Wuhan experienced an outbreak of novel coronavirus,\nwhich soon spread all over the world, resulting in a deadly pandemic that\ninfected millions of people around the globe. The government and public health\nagencies followed many strategies to counter the fatal virus. However, the\nvirus severely affected the social and economic lives of the people. In this\npaper, we extract and study the opinion of people from the top five worst\naffected countries by the virus, namely USA, Brazil, India, Russia, and South\nAfrica. We propose a deep language-independent Multilevel Attention-based\nConv-BiGRU network (MACBiG-Net), which includes embedding layer, word-level\nencoded attention, and sentence-level encoded attention mechanism to extract\nthe positive, negative, and neutral sentiments. The embedding layer encodes the\nsentence sequence into a real-valued vector. The word-level and sentence-level\nencoding is performed by a 1D Conv-BiGRU based mechanism, followed by\nword-level and sentence-level attention, respectively. We further develop a\nCOVID-19 Sentiment Dataset by crawling the tweets from Twitter. Extensive\nexperiments on our proposed dataset demonstrate the effectiveness of the\nproposed MACBiG-Net. Also, attention-weights visualization and in-depth results\nanalysis shows that the proposed network has effectively captured the\nsentiments of the people.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 11:59:16 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Yadav", "Ashima", ""], ["Vishwakarma", "Dinesh Kumar", ""]]}, {"id": "2011.10456", "submitter": "Noemi Mauro", "authors": "Noemi Mauro and Liliana Ardissono and Giovanna Petrone", "title": "User and Item-aware Estimation of Review Helpfulness", "comments": null, "journal-ref": "Information Processing & Management, Volume 58, Issue 1, 2021.\n  ISSN 0306-4573", "doi": "10.1016/j.ipm.2020.102434", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In online review sites, the analysis of user feedback for assessing its\nhelpfulness for decision-making is usually carried out by locally studying the\nproperties of individual reviews. However, global properties should be\nconsidered as well to precisely evaluate the quality of user feedback. In this\npaper we investigate the role of deviations in the properties of reviews as\nhelpfulness determinants with the intuition that \"out of the core\" feedback\nhelps item evaluation. We propose a novel helpfulness estimation model that\nextends previous ones with the analysis of deviations in rating, length and\npolarity with respect to the reviews written by the same person, or concerning\nthe same item. A regression analysis carried out on two large datasets of\nreviews extracted from Yelp social network shows that user-based deviations in\nreview length and rating clearly influence perceived helpfulness. Moreover, an\nexperiment on the same datasets shows that the integration of our helpfulness\nestimation model improves the performance of a collaborative recommender system\nby enhancing the selection of high-quality data for rating estimation. Our\nmodel is thus an effective tool to select relevant user feedback for\ndecision-making.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 15:35:56 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Mauro", "Noemi", ""], ["Ardissono", "Liliana", ""], ["Petrone", "Giovanna", ""]]}, {"id": "2011.10690", "submitter": "Parshan Pakiman", "authors": "Boxiao Chen, Selvaprabu Nadarajah, Parshan Pakiman, Stefanus Jasin", "title": "Self-adapting Robustness in Demand Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We study dynamic pricing over a finite number of periods in the presence of\ndemand model ambiguity. Departing from the typical no-regret learning\nenvironment, where price changes are allowed at any time, pricing decisions are\nmade at pre-specified points in time and each price can be applied to a large\nnumber of arrivals. In this environment, which arises in retailing, a pricing\ndecision based on an incorrect demand model can significantly impact cumulative\nrevenue. We develop an adaptively-robust-learning (ARL) pricing policy that\nlearns the true model parameters from the data while actively managing demand\nmodel ambiguity. It optimizes an objective that is robust with respect to a\nself-adapting set of demand models, where a given model is included in this set\nonly if the sales data revealed from prior pricing decisions makes it\n\"probable\". As a result, it gracefully transitions from being robust when\ndemand model ambiguity is high to minimizing regret when this ambiguity\ndiminishes upon receiving more data. We characterize the stochastic behavior of\nARL's self-adapting ambiguity sets and derive a regret bound that highlights\nthe link between the scale of revenue loss and the customer arrival pattern. We\nalso show that ARL, by being conscious of both model ambiguity and revenue,\nbridges the gap between a distributionally robust policy and a\nfollow-the-leader policy, which focus on model ambiguity and revenue,\nrespectively. We numerically find that the ARL policy, or its extension\nthereof, exhibits superior performance compared to distributionally robust,\nfollow-the-leader, and upper-confidence-bound policies in terms of expected\nrevenue and/or value at risk.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 01:15:54 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Chen", "Boxiao", ""], ["Nadarajah", "Selvaprabu", ""], ["Pakiman", "Parshan", ""], ["Jasin", "Stefanus", ""]]}, {"id": "2011.10834", "submitter": "Adolfo Almeida", "authors": "A. Almeida, J.P. de Villiers, A. De Freitas, M. Velayudan", "title": "Exploring the multimodal information from video content using deep\n  learning features of appearance, audio and action for video recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the popularisation of media streaming, a number of video streaming\nservices are continuously buying new video content to mine the potential profit\nfrom them. As such, the newly added content has to be handled well to be\nrecommended to suitable users. In this paper, we address the new item\ncold-start problem by exploring the potential of various deep learning features\nto provide video recommendations. The deep learning features investigated\ninclude features that capture the visual-appearance, audio and motion\ninformation from video content. We also explore different fusion methods to\nevaluate how well these feature modalities can be combined to fully exploit the\ncomplementary information captured by them. Experiments on a real-world video\ndataset for movie recommendations show that deep learning features outperform\nhand-crafted features. In particular, recommendations generated with deep\nlearning audio features and action-centric deep learning features are superior\nto MFCC and state-of-the-art iDT features. In addition, the combination of\nvarious deep learning features with hand-crafted features and textual metadata\nyields significant improvement in recommendations compared to combining only\nthe former.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 18:00:28 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Almeida", "A.", ""], ["de Villiers", "J. P.", ""], ["De Freitas", "A.", ""], ["Velayudan", "M.", ""]]}, {"id": "2011.10919", "submitter": "Kazem Jahanbakhsh", "authors": "Kazem Jahanbakhsh", "title": "Applying Multi-armed Bandit Algorithms to Computational Advertising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last two decades, we have seen extensive industrial research in the\narea of computational advertising. In this paper, our goal is to study the\nperformance of various online learning algorithms to identify and display the\nbest ads/offers with the highest conversion rates to web users. We formulate\nour ad-selection problem as a Multi-Armed Bandit problem which is a classical\nparadigm in Machine Learning. We have been applying machine learning, data\nmining, probability, and statistics to analyze big data in the ad-tech space\nand devise efficient ad selection strategies. This article highlights some of\nour findings in the area of computational advertising from 2011 to 2015.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 03:23:13 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Jahanbakhsh", "Kazem", ""]]}, {"id": "2011.11938", "submitter": "Junyou He", "authors": "Junyou He, Guibao Mei, Feng Xing, Xiaorui Yang, Yongjun Bao, Weipeng\n  Yan", "title": "DADNN: Multi-Scene CTR Prediction via Domain-Aware Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click through rate(CTR) prediction is a core task in advertising systems. The\nbooming e-commerce business in our company, results in a growing number of\nscenes. Most of them are so-called long-tail scenes, which means that the\ntraffic of a single scene is limited, but the overall traffic is considerable.\nTypical studies mainly focus on serving a single scene with a well designed\nmodel. However, this method brings excessive resource consumption both on\noffline training and online serving. Besides, simply training a single model\nwith data from multiple scenes ignores the characteristics of their own. To\naddress these challenges, we propose a novel but practical model named\nDomain-Aware Deep Neural Network(DADNN) by serving multiple scenes with only\none model. Specifically, shared bottom block among all scenes is applied to\nlearn a common representation, while domain-specific heads maintain the\ncharacteristics of every scene. Besides, knowledge transfer is introduced to\nenhance the opportunity of knowledge sharing among different scenes. In this\npaper, we study two instances of DADNN where its shared bottom block is\nmultilayer perceptron(MLP) and Multi-gate Mixture-of-Experts(MMoE)\nrespectively, for which we denote as DADNN-MLP and DADNN-MMoE.Comprehensive\noffline experiments on a real production dataset from our company show that\nDADNN outperforms several state-of-the-art methods for multi-scene CTR\nprediction. Extensive online A/B tests reveal that DADNN-MLP contributes up to\n6.7% CTR and 3.0% CPM(Cost Per Mille) promotion compared with a well-engineered\nDCN model. Furthermore, DADNN-MMoE outperforms DADNN-MLP with a relative\nimprovement of 2.2% and 2.7% on CTR and CPM respectively. More importantly,\nDADNN utilizes a single model for multiple scenes which saves a lot of offline\ntraining and online serving resources.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 07:30:52 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["He", "Junyou", ""], ["Mei", "Guibao", ""], ["Xing", "Feng", ""], ["Yang", "Xiaorui", ""], ["Bao", "Yongjun", ""], ["Yan", "Weipeng", ""]]}, {"id": "2011.11950", "submitter": "Nikitha Rao", "authors": "Nikitha Rao, Chetan Bansal and Joe Guan", "title": "Search4Code: Code Search Intent Classification Using Weak Supervision", "comments": "Dataset for this paper is available here:\n  https://github.com/microsoft/Search4Code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developers use search for various tasks such as finding code, documentation,\ndebugging information, etc. In particular, web search is heavily used by\ndevelopers for finding code examples and snippets during the coding process.\nRecently, natural language based code search has been an active area of\nresearch. However, the lack of real-world large-scale datasets is a significant\nbottleneck. In this work, we propose a weak supervision based approach for\ndetecting code search intent in search queries for C# and Java programming\nlanguages. We evaluate the approach against several baselines on a real-world\ndataset comprised of over 1 million queries mined from Bing web search engine\nand show that the CNN based model can achieve an accuracy of 77% and 76% for C#\nand Java respectively. Furthermore, we are also releasing Search4Code, the\nfirst large-scale real-world dataset of code search queries mined from Bing web\nsearch engine. We hope that the dataset will aid future research on code\nsearch.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 08:06:53 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 01:54:17 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 15:01:41 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Rao", "Nikitha", ""], ["Bansal", "Chetan", ""], ["Guan", "Joe", ""]]}, {"id": "2011.11970", "submitter": "Abhilash Nandy", "authors": "Manish Agrawal, Abhilash Nandy", "title": "A Novel Multimodal Music Genre Classifier using Hierarchical Attention\n  and Convolutional Neural Network", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music genre classification is one of the trending topics in regards to the\ncurrent Music Information Retrieval (MIR) Research. Since, the dependency of\ngenre is not only limited to the audio profile, we also make use of textual\ncontent provided as lyrics of the corresponding song. We implemented a CNN\nbased feature extractor for spectrograms in order to incorporate the acoustic\nfeatures and a Hierarchical Attention Network based feature extractor for\nlyrics. We then go on to classify the music track based upon the resulting\nfused feature vector.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 09:02:35 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Agrawal", "Manish", ""], ["Nandy", "Abhilash", ""]]}, {"id": "2011.12349", "submitter": "Batuhan Bardak", "authors": "Batuhan Bardak and Mehmet Tan", "title": "Improving Clinical Outcome Predictions Using Convolution over Medical\n  Entities with Multimodal Learning", "comments": "21 pages, 2 figures, Submitted to Elsevier (Artificial Intelligence\n  in Medicine)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Early prediction of mortality and length of stay(LOS) of a patient is vital\nfor saving a patient's life and management of hospital resources. Availability\nof electronic health records(EHR) makes a huge impact on the healthcare domain\nand there has seen several works on predicting clinical problems. However, many\nstudies did not benefit from the clinical notes because of the sparse, and high\ndimensional nature. In this work, we extract medical entities from clinical\nnotes and use them as additional features besides time-series features to\nimprove our predictions. We propose a convolution based multimodal\narchitecture, which not only learns effectively combining medical entities and\ntime-series ICU signals of patients, but also allows us to compare the effect\nof different embedding techniques such as Word2vec, FastText on medical\nentities. In the experiments, our proposed method robustly outperforms all\nother baseline models including different multimodal architectures for all\nclinical tasks. The code for the proposed method is available at\nhttps://github.com/tanlab/ConvolutionMedicalNer.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 20:08:39 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 09:40:41 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Bardak", "Batuhan", ""], ["Tan", "Mehmet", ""]]}, {"id": "2011.12586", "submitter": "Linhao Luo", "authors": "Linhao Luo, Liqi Yang, Ju Xin, Yixiang Fang, Xiaofeng Zhang, Xiaofei\n  Yang, Kai Chen, Zhiyuan Zhang, Kai Liu", "title": "RRCN: A Reinforced Random Convolutional Network based Reciprocal\n  Recommendation Approach for Online Dating", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the reciprocal recommendation, especially for online dating\napplications, has attracted more and more research attention. Different from\nconventional recommendation problems, the reciprocal recommendation aims to\nsimultaneously best match users' mutual preferences. Intuitively, the mutual\npreferences might be affected by a few key attributes that users like or\ndislike. Meanwhile, the interactions between users' attributes and their key\nattributes are also important for key attributes selection. Motivated by these\nobservations, in this paper we propose a novel reinforced random convolutional\nnetwork (RRCN) approach for the reciprocal recommendation task. In particular,\nwe technically propose a novel random CNN component that can randomly convolute\nnon-adjacent features to capture their interaction information and learn\nfeature embeddings of key attributes to make the final recommendation.\nMoreover, we design a reinforcement learning based strategy to integrate with\nthe random CNN component to select salient attributes to form the candidate set\nof key attributes. We evaluate the proposed RRCN against a number of both\nbaselines and the state-of-the-art approaches on two real-world datasets, and\nthe promising results have demonstrated the superiority of RRCN against the\ncompared approaches in terms of a number of evaluation criteria.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 08:55:17 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Luo", "Linhao", ""], ["Yang", "Liqi", ""], ["Xin", "Ju", ""], ["Fang", "Yixiang", ""], ["Zhang", "Xiaofeng", ""], ["Yang", "Xiaofei", ""], ["Chen", "Kai", ""], ["Zhang", "Zhiyuan", ""], ["Liu", "Kai", ""]]}, {"id": "2011.12620", "submitter": "Shah Miah Prof", "authors": "Soliman Aljarboa, Shah J. Miah", "title": "An Integration of UTAUT and Task-Technology Fit Frameworks for Assessing\n  the Acceptance of Clinical Decision Support Systems in the Context of a\n  Developing Country", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is to create a basis of theoretical contribution for a new PhD\nthesis in the area of Clinical Decision Support Systems (CDSS) acceptance. Over\nthe past three years, we conducted qualitative research into three distinctive\nphases to develop an extended Task-Technology Fit (TTF) Framework. These phases\nare for initiating requirement generation of the framework, discovering the\nfactors of the framework through perspectives, and evaluating the new proposed\nframework. The new condition is related to developing country in which various\nsectors such as healthcare is mostly under attention. We conduct a new\ninspective for assisting decisions to support technology and its usefulness in\nthis sector to integrate with other frameworks for assisting the value, use,\nand how can be better accepted in the context of healthcare professionals.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 10:20:46 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Aljarboa", "Soliman", ""], ["Miah", "Shah J.", ""]]}, {"id": "2011.12683", "submitter": "Jiarui Jin", "authors": "Jiarui Jin, Kounianhua Du, Weinan Zhang, Jiarui Qin, Yuchen Fang, Yong\n  Yu, Zheng Zhang, Alexander J. Smola", "title": "GraphHINGE: Learning Interaction Models of Structured Neighborhood on\n  Heterogeneous Information Network", "comments": "TOIS (Special Issue on Graph Technologies for User Modeling and\n  Recommendation). arXiv admin note: text overlap with arXiv:2007.00216", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Heterogeneous information network (HIN) has been widely used to characterize\nentities of various types and their complex relations. Recent attempts either\nrely on explicit path reachability to leverage path-based semantic relatedness\nor graph neighborhood to learn heterogeneous network representations before\npredictions. These weakly coupled manners overlook the rich interactions among\nneighbor nodes, which introduces an early summarization issue. In this paper,\nwe propose GraphHINGE (Heterogeneous INteract and aggreGatE), which captures\nand aggregates the interactive patterns between each pair of nodes through\ntheir structured neighborhoods. Specifically, we first introduce\nNeighborhood-based Interaction (NI) module to model the interactive patterns\nunder the same metapaths, and then extend it to Cross Neighborhood-based\nInteraction (CNI) module to deal with different metapaths. Next, in order to\naddress the complexity issue on large-scale networks, we formulate the\ninteraction modules via a convolutional framework and learn the parameters\nefficiently with fast Fourier transform. Furthermore, we design a novel\nneighborhood-based selection (NS) mechanism, a sampling strategy, to filter\nhigh-order neighborhood information based on their low-order performance. The\nextensive experiments on six different types of heterogeneous graphs\ndemonstrate the performance gains by comparing with state-of-the-arts in both\nclick-through rate prediction and top-N recommendation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 12:30:04 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 21:47:00 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Jin", "Jiarui", ""], ["Du", "Kounianhua", ""], ["Zhang", "Weinan", ""], ["Qin", "Jiarui", ""], ["Fang", "Yuchen", ""], ["Yu", "Yong", ""], ["Zhang", "Zheng", ""], ["Smola", "Alexander J.", ""]]}, {"id": "2011.12684", "submitter": "Lucas Chaves Lima", "authors": "Lucas Chaves Lima, Casper Hansen, Christian Hansen, Dongsheng Wang,\n  Maria Maistro, Birger Larsen, Jakob Grue Simonsen and Christina Lioma", "title": "Denmark's Participation in the Search Engine TREC COVID-19 Challenge:\n  Lessons Learned about Searching for Precise Biomedical Scientific Information\n  on COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This report describes the participation of two Danish universities,\nUniversity of Copenhagen and Aalborg University, in the international search\nengine competition on COVID-19 (the 2020 TREC-COVID Challenge) organised by the\nU.S. National Institute of Standards and Technology (NIST) and its Text\nRetrieval Conference (TREC) division. The aim of the competition was to find\nthe best search engine strategy for retrieving precise biomedical scientific\ninformation on COVID-19 from the largest, at that point in time, dataset of\ncurated scientific literature on COVID-19 -- the COVID-19 Open Research Dataset\n(CORD-19). CORD-19 was the result of a call to action to the tech community by\nthe U.S. White House in March 2020, and was shortly thereafter posted on Kaggle\nas an AI competition by the Allen Institute for AI, the Chan Zuckerberg\nInitiative, Georgetown University's Center for Security and Emerging\nTechnology, Microsoft, and the National Library of Medicine at the US National\nInstitutes of Health. CORD-19 contained over 200,000 scholarly articles (of\nwhich more than 100,000 were with full text) about COVID-19, SARS-CoV-2, and\nrelated coronaviruses, gathered from curated biomedical sources. The TREC-COVID\nchallenge asked for the best way to (a) retrieve accurate and precise\nscientific information, in response to some queries formulated by biomedical\nexperts, and (b) rank this information decreasingly by its relevance to the\nquery.\n  In this document, we describe the TREC-COVID competition setup, our\nparticipation to it, and our resulting reflections and lessons learned about\nthe state-of-art technology when faced with the acute task of retrieving\nprecise scientific information from a rapidly growing corpus of literature, in\nresponse to highly specialised queries, in the middle of a pandemic.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 12:30:38 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 12:42:21 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Lima", "Lucas Chaves", ""], ["Hansen", "Casper", ""], ["Hansen", "Christian", ""], ["Wang", "Dongsheng", ""], ["Maistro", "Maria", ""], ["Larsen", "Birger", ""], ["Simonsen", "Jakob Grue", ""], ["Lioma", "Christina", ""]]}, {"id": "2011.12843", "submitter": "Homa Hosseinmardi", "authors": "Homa Hosseinmardi, Amir Ghasemian, Aaron Clauset, David M. Rothschild,\n  Markus Mobius, Duncan J. Watts", "title": "Evaluating the scale, growth, and origins of right-wing echo chambers on\n  YouTube", "comments": "29 pages, 21 figures, 15 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although it is understudied relative to other social media platforms, YouTube\nis arguably the largest and most engaging online media consumption platform in\nthe world. Recently, YouTube's outsize influence has sparked concerns that its\nrecommendation algorithm systematically directs users to radical right-wing\ncontent. Here we investigate these concerns with large scale longitudinal data\nof individuals' browsing behavior spanning January 2016 through December 2019.\nConsistent with previous work, we find that political news content accounts for\na relatively small fraction (11%) of consumption on YouTube, and is dominated\nby mainstream and largely centrist sources. However, we also find evidence for\na small but growing \"echo chamber\" of far-right content consumption. Users in\nthis community show higher engagement and greater \"stickiness\" than users who\nconsume any other category of content. Moreover, YouTube accounts for an\nincreasing fraction of these users' overall online news consumption. Finally,\nwhile the size, intensity, and growth of this echo chamber present real\nconcerns, we find no evidence that they are caused by YouTube recommendations.\nRather, consumption of radical content on YouTube appears to reflect broader\npatterns of news consumption across the web. Our results emphasize the\nimportance of measuring consumption directly rather than inferring it from\nrecommendations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:00:20 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Hosseinmardi", "Homa", ""], ["Ghasemian", "Amir", ""], ["Clauset", "Aaron", ""], ["Rothschild", "David M.", ""], ["Mobius", "Markus", ""], ["Watts", "Duncan J.", ""]]}, {"id": "2011.13091", "submitter": "Kiran Sharma Dr.", "authors": "Kiran Sharma", "title": "Patterns of retractions from 1981-2020 : Does a fraud lead to another\n  fraud?", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Misconduct accounts for the majority of retracted scientific publications and\nthis database reveals the disturbing trend in\nscience~\\citep{fang2012misconduct, brainard2018massive}. The objective of the\nstudy is to find the association among the authors' collaboration, the number\nof retracted papers, the number of retracted citations, journal impact factor,\nand research areas. We present a detailed analysis of 12231 research papers\nindexed by Web of Science (WoS) as retracted publications from 1981-2020. The\nstudy demonstrates the collaboration patterns of retracted publications where\n61.5% of authors have only one and 24.6% have two retracted papers; however, 2%\nof authors have more than 10 retracted papers. To study the impact of citing\nretracted papers, we investigated the retracted papers with citations. The\nstudy reveals that 55.2% of retracted papers have been cited at least once,\nwhere 25.4% of papers are such papers where at least one citation turned out to\nbe a retraction. This shows the impact of scientific misconduct or fraud on new\nresearch. The number of retractions is independent of the journal impact factor\nand as compared to high impact papers, low impact papers are attracting more\ncitations. We also investigate the citations received by retracted papers\npublished in higher as well as lower impact factor journals. 1/4th of the\npapers are retracted citations that cited the retracted papers; however, there\nis no significant relationship exists between the higher impact or lower impact\njournals with retractions or citations. Finally, how the average team size and\naverage retracted citations vary among different research areas are studied.\nThe study provides an insight that how a fraud leads to another fraud in the\nscientific world. Also, the rising trend of citations of retracted papers is a\nserious concern.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 02:08:05 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Sharma", "Kiran", ""]]}, {"id": "2011.13114", "submitter": "Christopher Rauch", "authors": "Mat Kelly (1), Jane Greenberg (1), Christopher B. Rauch (1), Sam\n  Grabus (1), Joan P. Boone (1), John A. Kunze (2), Peter Melville Logan (3)\n  ((1) Drexel University, (2) California Digital Library, (3) Temple\n  University)", "title": "A Computational Approach to Historical Ontologies", "comments": "6 pages, 5 figures. To be published in Proceedings of the 2020 IEEE\n  International Conference on Big Data (IEEE Big Data 2020)", "journal-ref": "2020 IEEE International Conference on Big Data (Big Data),\n  Atlanta, GA, USA, 2020, pp. 1878-1883", "doi": "10.1109/BigData50022.2020.9378268", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a use case exploring the application of the Archival\nResource Key (ARK) persistent identifier for promoting and maintaining\nontologies. In particular, we look at improving computation with an in-house\nontology server in the context of temporally aligned vocabularies. This effort\ndemonstrates the utility of ARKs in preparing historical ontologies for\ncomputational archival science.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 03:53:56 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kelly", "Mat", ""], ["Greenberg", "Jane", ""], ["Rauch", "Christopher B.", ""], ["Grabus", "Sam", ""], ["Boone", "Joan P.", ""], ["Kunze", "John A.", ""], ["Logan", "Peter Melville", ""]]}, {"id": "2011.13253", "submitter": "Sundeep Teki", "authors": "Rutvik Vijjali, Prathyush Potluri, Siddharth Kumar, Sundeep Teki", "title": "Two Stage Transformer Model for COVID-19 Fake News Detection and Fact\n  Checking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid advancement of technology in online communication via social media\nplatforms has led to a prolific rise in the spread of misinformation and fake\nnews. Fake news is especially rampant in the current COVID-19 pandemic, leading\nto people believing in false and potentially harmful claims and stories.\nDetecting fake news quickly can alleviate the spread of panic, chaos and\npotential health hazards. We developed a two stage automated pipeline for\nCOVID-19 fake news detection using state of the art machine learning models for\nnatural language processing. The first model leverages a novel fact checking\nalgorithm that retrieves the most relevant facts concerning user claims about\nparticular COVID-19 claims. The second model verifies the level of truth in the\nclaim by computing the textual entailment between the claim and the true facts\nretrieved from a manually curated COVID-19 dataset. The dataset is based on a\npublicly available knowledge source consisting of more than 5000 COVID-19 false\nclaims and verified explanations, a subset of which was internally annotated\nand cross-validated to train and evaluate our models. We evaluate a series of\nmodels based on classical text-based features to more contextual Transformer\nbased models and observe that a model pipeline based on BERT and ALBERT for the\ntwo stages respectively yields the best results.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 11:50:45 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Vijjali", "Rutvik", ""], ["Potluri", "Prathyush", ""], ["Kumar", "Siddharth", ""], ["Teki", "Sundeep", ""]]}, {"id": "2011.13284", "submitter": "Catherine Kobus", "authors": "Alexandre Arnold and G\\'erard Dupont and F\\'elix Furger and Catherine\n  Kobus and Fran\\c{c}ois Lancelot", "title": "A question-answering system for aircraft pilots' documentation", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The aerospace industry relies on massive collections of complex and technical\ndocuments covering system descriptions, manuals or procedures. This paper\npresents a question answering (QA) system that would help aircraft pilots\naccess information in this documentation by naturally interacting with the\nsystem and asking questions in natural language. After describing each module\nof the dialog system, we present a multi-task based approach for the QA module\nwhich enables performance improvement on a Flight Crew Operating Manual (FCOM)\ndataset. A method to combine scores from the retriever and the QA modules is\nalso presented.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 13:33:47 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Arnold", "Alexandre", ""], ["Dupont", "G\u00e9rard", ""], ["Furger", "F\u00e9lix", ""], ["Kobus", "Catherine", ""], ["Lancelot", "Fran\u00e7ois", ""]]}, {"id": "2011.13436", "submitter": "Hansi Zeng", "authors": "Hansi Zeng, Qingyao Ai", "title": "A Hierarchical Self-attentive Convolution Network for Review Modeling in\n  Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using reviews to learn user and item representations is important for\nrecommender system. Current review based methods can be divided into two\ncategories: (1) the Convolution Neural Network (CNN) based models that extract\nn-gram features from user/item reviews; (2) the Recurrent Neural Network (RNN)\nbased models that learn global contextual representations from reviews for\nusers and items. Despite their success, both CNN and RNN based models in\nprevious studies suffer from their own drawbacks. While CNN based models are\nweak in modeling long-dependency relation in text, RNN based models are slow in\ntraining and inference due to their incapability with parallel computing. To\nalleviate these problems, we propose a new text encoder module for review\nmodeling in recommendation by combining convolution networks with\nself-attention networks to model local and global interactions in text\ntogether.As different words, sentences, reviews have different importance for\nmodeling user and item representations, we construct review models\nhierarchically in sentence-level, review-level, and user/item level by encoding\nwords for sentences, encoding sentences for reviews, and encoding reviews for\nuser and item representations. Experiments on Amazon Product Benchmark show\nthat our model can achieve significant better performance comparing to the\nstate-of-the-art review based recommendation models.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 18:45:23 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Zeng", "Hansi", ""], ["Ai", "Qingyao", ""]]}, {"id": "2011.13482", "submitter": "Zhenlong Li Dr.", "authors": "Yago Martin, Zhenlong Li, Yue Ge", "title": "Towards real-time population estimates: introducing Twitter daily\n  estimates of residents and non-residents at the county level", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study of migrations and mobility has historically been severely limited\nby the absence of reliable data or the temporal sparsity of the available data.\nUsing geospatial digital trace data, the study of population movements can be\nmuch more precisely and dynamically measured. Our research seeks to develop a\nnear real-time (one-day lag) Twitter census that gives a more temporally\ngranular picture of local and non-local population at the county level.\nLeveraging geotagged tweets to determine the home location of all active\nTwitter users, we contribute to the field of digital and computational\ndemography by obtaining accurate daily Twitter population stocks (residents and\nnon-residents). Internal validation reveals over 80% of accuracy when compared\nwith users self-reported home location. External validation results suggest\nthese stocks correlate with available statistics of residents/non-residents at\nthe county level and can accurately reflect regular (seasonal tourism) and\nnon-regular events such as the Great American Solar Eclipse of 2017. The\nfindings demonstrate that Twitter holds potential to introduce the dynamic\ncomponent often lacking in population estimates.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 22:01:07 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Martin", "Yago", ""], ["Li", "Zhenlong", ""], ["Ge", "Yue", ""]]}, {"id": "2011.13534", "submitter": "Nishant Subramani", "authors": "Nishant Subramani and Alexandre Matton and Malcolm Greaves and Adrian\n  Lam", "title": "A Survey of Deep Learning Approaches for OCR and Document Understanding", "comments": "Accepted to the ML-RSA Workshop at NeurIPS2020. 15 pages (10 +\n  References)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Documents are a core part of many businesses in many fields such as law,\nfinance, and technology among others. Automatic understanding of documents such\nas invoices, contracts, and resumes is lucrative, opening up many new avenues\nof business. The fields of natural language processing and computer vision have\nseen tremendous progress through the development of deep learning such that\nthese methods have started to become infused in contemporary document\nunderstanding systems. In this survey paper, we review different techniques for\ndocument understanding for documents written in English and consolidate\nmethodologies present in literature to act as a jumping-off point for\nresearchers exploring this area.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 03:05:59 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 23:48:39 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Subramani", "Nishant", ""], ["Matton", "Alexandre", ""], ["Greaves", "Malcolm", ""], ["Lam", "Adrian", ""]]}, {"id": "2011.13556", "submitter": "R. Ghosh", "authors": "R K Ghosh, Vinay R and Arnab Bhattacharyya", "title": "Eco-Routing Using Open Street Maps", "comments": "16 pages, 17 figures, 41 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A vehicle's fuel consumption depends on its type, the speed, the condition,\nand the gradients of the road on which it is moving. We developed a Routing\nEngine for finding an eco-route (one with low fuel consumption) between a\nsource and a destination. Open Street Maps has data on road conditions. We used\nCGIAR-CSI road elevation data 16[4] to integrate the road gradients into the\nproposed route-finding algorithm that modifies Open Street Routing Machine\n(OSRM). It allowed us to dynamically predict a vehicle's velocity, considering\nboth the conditions and the road segment's slope. Using Highway EneRgy\nAssessment (HERA) methodology, we calculated the fuel consumed by a vehicle\ngiven its type and velocity. We have created both web and mobile interfaces\nthrough which users can specify Geo coordinates or human-readable addresses of\na source and a destination. The user interface graphically displays the route\nobtained from the proposed Routing Engine with a detailed travel itinerary.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 04:52:22 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Ghosh", "R K", ""], ["R", "Vinay", ""], ["Bhattacharyya", "Arnab", ""]]}, {"id": "2011.13802", "submitter": "Tasos Spiliotopoulos", "authors": "Tasos Spiliotopoulos, Ian Oakley", "title": "Post or Tweet: Lessons from a Study of Facebook and Twitter Usage", "comments": "Published in the CHI 2016 Workshop on Following user pathways: Using\n  cross platform and mixed methods analysis in social media studies, San Jose,\n  CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This workshop paper reports on an ongoing mixed-methods study on the two\narguably most popular social network sites, Facebook and Twitter, for the same\nusers. The overarching goal of the study is to shed light into the nuances of\nsocial media selection and cross-platform use by combining survey data about\nparticipants' motivations with usage data collected via API extraction. We\ndescribe the set-up of the study and focus our discussion on the challenges and\ninsights relating to participant recruiting and data collection, handling and\ndimensionalizing usage data, and comparing usage data across sites.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 15:55:02 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Spiliotopoulos", "Tasos", ""], ["Oakley", "Ian", ""]]}, {"id": "2011.13832", "submitter": "Nabarun Mondal Mr", "authors": "Nabarun Mondal, Mrunal Lohia", "title": "Supervised Text Classification using Text Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised text classification is a classical and active area of ML research.\nIn large enterprise, solutions to this problem has significant importance. This\nis specifically true in ticketing systems where prediction of the type and\nsubtype of tickets given new incoming ticket text to find out optimal routing\nis a multi billion dollar industry.\n  In this paper authors describe a class of industrial standard algorithms\nwhich can accurately ( 86\\% and above ) predict classification of any text\ngiven prior labelled text data - by novel use of any text search engine.\n  These algorithms were used to automate routing of issue tickets to the\nappropriate team. This class of algorithms has far reaching consequences for a\nwide variety of industrial applications, IT support, RPA script triggering,\neven legal domain where massive set of pre labelled data are already available.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 19:51:51 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 19:53:45 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Mondal", "Nabarun", ""], ["Lohia", "Mrunal", ""]]}, {"id": "2011.14137", "submitter": "Abdul Wahab", "authors": "Abdul Wahab, Muhammad Anas Tahir, Naveed Iqbal, Faisal Shafait, Syed\n  Muhammad Raza Kazmi", "title": "Short-Term Load Forecasting using Bi-directional Sequential Models and\n  Feature Engineering for Small Datasets", "comments": "8 pages, 13 figures, 5 tables. Submitted to IEEE Transactions on\n  Power Systems, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Electricity load forecasting enables the grid operators to optimally\nimplement the smart grid's most essential features such as demand response and\nenergy efficiency. Electricity demand profiles can vary drastically from one\nregion to another on diurnal, seasonal and yearly scale. Hence to devise a load\nforecasting technique that can yield the best estimates on diverse datasets,\nspecially when the training data is limited, is a big challenge. This paper\npresents a deep learning architecture for short-term load forecasting based on\nbidirectional sequential models in conjunction with feature engineering that\nextracts the hand-crafted derived features in order to aid the model for better\nlearning and predictions. In the proposed architecture, named as Deep Derived\nFeature Fusion (DeepDeFF), the raw input and hand-crafted features are trained\nat separate levels and then their respective outputs are combined to make the\nfinal prediction. The efficacy of the proposed methodology is evaluated on\ndatasets from five countries with completely different patterns. The results\ndemonstrate that the proposed technique is superior to the existing state of\nthe art.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 14:11:35 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wahab", "Abdul", ""], ["Tahir", "Muhammad Anas", ""], ["Iqbal", "Naveed", ""], ["Shafait", "Faisal", ""], ["Kazmi", "Syed Muhammad Raza", ""]]}, {"id": "2011.14146", "submitter": "Isa Inuwa-Dutse", "authors": "Isa Inuwa-Dutse", "title": "Towards Combating Pandemic-related Misinformation in Social Media", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conventional preventive measures during pandemic include social distancing\nand lockdown. Such measures in the time of social media brought about a new set\nof challenges - vulnerability to the toxic impact of online misinformation is\nhigh. A case in point is the prevailing COVID-19; as the virus propagates, so\ndoes the associated misinformation and fake news about it leading to infodemic.\nSince the outbreak, there has been a surge of studies investigating various\naspects of the pandemic. Of interest to this chapter include studies centring\non datasets from online social media platforms where the bulk of the public\ndiscourse happen. Consequently, the main goal is to support the fight against\nnegative infodemic by (1) contributing a diverse set of curated relevant\ndatasets (2) recommending relevant areas to study using the datasets (3)\ndiscussion on how relevant datasets, strategies and state-of-the-art IT tools\ncan be leveraged in managing the pandemic.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 15:30:14 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 02:26:25 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Inuwa-Dutse", "Isa", ""]]}, {"id": "2011.14280", "submitter": "Hrithwik Shalu", "authors": "Sudhir Kumar Suman, Hrithwik Shalu, Lakshya A Agrawal, Archit Agrawal,\n  Juned Kadiwala", "title": "A Novel Sentiment Analysis Engine for Preliminary Depression Status\n  Estimation on Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text sentiment analysis for preliminary depression status estimation of users\non social media is a widely exercised and feasible method, However, the immense\nvariety of users accessing the social media websites and their ample mix of\nvocabularies makes it difficult for commonly applied deep learning-based\nclassifiers to perform. To add to the situation, the lack of adaptability of\ntraditional supervised machine learning could hurt at many levels. We propose a\ncloud-based smartphone application, with a deep learning-based backend to\nprimarily perform depression detection on Twitter social media. The backend\nmodel consists of a RoBERTa based siamese sentence classifier that compares a\ngiven tweet (Query) with a labeled set of tweets with known sentiment (\nStandard Corpus ). The standard corpus is varied over time with expert opinion\nso as to improve the model's reliability. A psychologist ( with the patient's\npermission ) could leverage the application to assess the patient's depression\nstatus prior to counseling, which provides better insight into the mental\nhealth status of a patient. In addition, to the same, the psychologist could be\nreferred to cases of similar characteristics, which could in turn help in more\neffective treatment. We evaluate our backend model after fine-tuning it on a\npublicly available dataset. The find tuned model is made to predict depression\non a large set of tweet samples with random noise factors. The model achieved\npinnacle results, with a testing accuracy of 87.23% and an AUC of 0.8621.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 04:42:53 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Suman", "Sudhir Kumar", ""], ["Shalu", "Hrithwik", ""], ["Agrawal", "Lakshya A", ""], ["Agrawal", "Archit", ""], ["Kadiwala", "Juned", ""]]}, {"id": "2011.14333", "submitter": "Na Li", "authors": "Na Li, Renyu Zhu, Xiaoxu Zhou, Xiangnan He, Wenyuan Cai, Ming Gao,\n  Aoying Zhou", "title": "On Disambiguating Authors: Collaboration Network Reconstruction in a\n  Bottom-up Manner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Author disambiguation arises when different authors share the same name,\nwhich is a critical task in digital libraries, such as DBLP, CiteULike,\nCiteSeerX, etc. While the state-of-the-art methods have developed various paper\nembedding-based methods performing in a top-down manner, they primarily focus\non the ego-network of a target name and overlook the low-quality collaborative\nrelations existed in the ego-network. Thus, these methods can be suboptimal for\ndisambiguating authors.\n  In this paper, we model the author disambiguation as a collaboration network\nreconstruction problem, and propose an incremental and unsupervised author\ndisambiguation method, namely IUAD, which performs in a bottom-up manner.\nInitially, we build a stable collaboration network based on stable\ncollaborative relations. To further improve the recall, we build a\nprobabilistic generative model to reconstruct the complete collaboration\nnetwork. In addition, for newly published papers, we can incrementally judge\nwho publish them via only computing the posterior probabilities. We have\nconducted extensive experiments on a large-scale DBLP dataset to evaluate IUAD.\nThe experimental results demonstrate that IUAD not only achieves the promising\nperformance, but also outperforms comparable baselines significantly. Codes are\navailable at https://github.com/papergitgit/IUAD.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 10:40:58 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Li", "Na", ""], ["Zhu", "Renyu", ""], ["Zhou", "Xiaoxu", ""], ["He", "Xiangnan", ""], ["Cai", "Wenyuan", ""], ["Gao", "Ming", ""], ["Zhou", "Aoying", ""]]}, {"id": "2011.14386", "submitter": "Eisa Alanazi", "authors": "Btool Hamoui, Abdulaziz Alashaikh, Eisa Alanazi", "title": "Google Searches and COVID-19 Cases in Saudi Arabia: A Correlation Study", "comments": "7 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: The outbreak of the new coronavirus disease (COVID-19) has\naffected human life to a great extent on a worldwide scale. During the\ncoronavirus pandemic, public health professionals at the early outbreak faced\nan extraordinary challenge to track and quantify the spread of disease.\nObjective: To investigate whether a digital surveillance model using google\ntrends (GT) is feasible to monitor the outbreak of coronavirus in the Kingdom\nof Saudi Arabia. Methods: We retrieve GT data using ten common COVID-19\nsymptoms related keywords from March 2, 2020, to October 31, 2020. Spearman\ncorrelation were performed to determine the correlation between COVID-19 cases\nand the Google search terms. Results: GT data related to Cough and Sore Throat\nwere the most searched symptoms by the Internet users in Saudi Arabia. The\nhighest daily correlation found with the Loss of Smell followed by Loss of\nTaste and Diarrhea. Strong correlation as well was found between the weekly\nconfirmed cases and the same symptoms: Loss of Smell, Loss of Taste and\nDiarrhea. Conclusions: We conducted an investigation study utilizing Internet\nsearches related to COVID-19 symptoms for surveillance of the pandemic spread.\nThis study documents that google searches can be used as a supplementary\nsurveillance tool in COVID-19 monitoring in Saudi Arabia.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 15:11:37 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Hamoui", "Btool", ""], ["Alashaikh", "Abdulaziz", ""], ["Alanazi", "Eisa", ""]]}, {"id": "2011.14615", "submitter": "Qi Yang", "authors": "Aleksandr Farseev, Qi Yang, Andrey Filchenkov, Kirill Lepikhin, Yu-Yi\n  Chu-Farseeva, Daron-Benjamin Loo", "title": "SoMin.ai: Personality-Driven Content Generation Platform", "comments": "WSDM 2021 - Demonstration", "journal-ref": null, "doi": "10.1145/3437963.3441714", "report-no": null, "categories": "cs.MM cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical demonstration, we showcase the World's first\npersonality-driven marketing content generation platform, called SoMin.ai. The\nplatform combines deep multi-view personality profiling framework and style\ngenerative adversarial networks facilitating the automatic creation of content\nthat appeals to different human personality types. The platform can be used for\nthe enhancement of the social networking user experience as well as for content\nmarketing routines. Guided by the MBTI personality type, automatically derived\nfrom a user social network content, SoMin.ai generates new social media content\nbased on the preferences of other users with a similar personality type aiming\nat enhancing the user experience on social networking venues as well\ndiversifying the efforts of marketers when crafting new content for digital\nmarketing campaigns. The real-time user feedback to the platform via the\nplatform's GUI fine-tunes the content generation model and the evaluation\nresults demonstrate the promising performance of the proposed multi-view\npersonality profiling framework when being applied in the content generation\nscenario. By leveraging content generation at a large scale, marketers will be\nable to execute more effective digital marketing campaigns at a lower cost.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 08:33:39 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 11:39:23 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Farseev", "Aleksandr", ""], ["Yang", "Qi", ""], ["Filchenkov", "Andrey", ""], ["Lepikhin", "Kirill", ""], ["Chu-Farseeva", "Yu-Yi", ""], ["Loo", "Daron-Benjamin", ""]]}, {"id": "2011.14616", "submitter": "Andre Greiner-Petter", "authors": "Andr\\'e Greiner-Petter", "title": "Automatic Mathematical Information Retrieval to Perform Translations up\n  to Computer Algebra Systems", "comments": "Doctoral Consortium Paper at the Joint Conference on Digital\n  Libraries (JCDL), Fort Worth, TX, USA, June 03-07, 2018", "journal-ref": "Bulletin of IEEE Technical Committee on Digital Libraries 15.1\n  (Jan. 2019)", "doi": null, "report-no": null, "categories": "cs.IR cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In mathematics, LaTeX is the de facto standard to prepare documents, e.g.,\nscientific publications. While some formulae are still developed using pen and\npaper, more complicated mathematical expressions used more and more often with\ncomputer algebra systems. Mathematical expressions are often manually\ntranscribed to computer algebra systems. The goal of my doctoral thesis is to\nimprove the efficiency of this workflow. My envisioned method will\nautomatically semantically enrich mathematical expressions so that they can be\nimported to computer algebra systems and other systems that can take advantage\nof the semantics, such as search engines or automatic plagiarism detection\nsystems. These imports should preserve the essential semantic features of the\nexpression.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 08:36:58 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Greiner-Petter", "Andr\u00e9", ""]]}, {"id": "2011.14618", "submitter": "Kavita Vaishnaw", "authors": "Heer Ambavi (1), Kavita Vaishnaw (1), Udit Vyas (1), Abhisht Tiwari\n  (1) and Mayank Singh (1) ((1) Indian Institute of Technology Gandhinagar)", "title": "CovidExplorer: A Multi-faceted AI-based Search and Visualization Engine\n  for COVID-19 Information", "comments": "4 pages, 7 figures, The associated system can be accessed at\n  http://covidexplorer.in, To be published in the Proceedings of the 29th ACM\n  International Conference on Information and Knowledge Management (CIKM '20)\n  (October 19-23, 2020)(Virtual Event, Ireland)", "journal-ref": null, "doi": "10.1145/3340531.3417428", "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The entire world is engulfed in the fight against the COVID-19 pandemic,\nleading to a significant surge in research experiments, government policies,\nand social media discussions. A multi-modal information access and data\nvisualization platform can play a critical role in supporting research aimed at\nunderstanding and developing preventive measures for the pandemic. In this\npaper, we present a multi-faceted AI-based search and visualization engine,\nCovidExplorer. Our system aims to help researchers understand current\nstate-of-the-art COVID-19 research, identify research articles relevant to\ntheir domain, and visualize real-time trends and statistics of COVID-19 cases.\nIn contrast to other existing systems, CovidExplorer also brings in\nIndia-specific topical discussions on social media to study different aspects\nof COVID-19. The system, demo video, and the datasets are available at\nhttp://covidexplorer.in.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 08:42:13 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ambavi", "Heer", "", "Indian Institute of Technology Gandhinagar"], ["Vaishnaw", "Kavita", "", "Indian Institute of Technology Gandhinagar"], ["Vyas", "Udit", "", "Indian Institute of Technology Gandhinagar"], ["Tiwari", "Abhisht", "", "Indian Institute of Technology Gandhinagar"], ["Singh", "Mayank", "", "Indian Institute of Technology Gandhinagar"]]}]