[{"id": "1712.00044", "submitter": "Hussam Hamdan", "authors": "Hussam Hamdan, Jean-Gabriel Ganascia", "title": "Graph Centrality Measures for Boosting Popularity-Based Entity Linking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Entity Linking systems use collective graph-based methods to\ndisambiguate the entity mentions within a document. Most of them have focused\non graph construction and initial weighting of the candidate entities, less\nattention has been devoted to compare the graph ranking algorithms. In this\nwork, we focus on the graph-based ranking algorithms, therefore we propose to\napply five centrality measures: Degree, HITS, PageRank, Betweenness and\nCloseness. A disambiguation graph of candidate entities is constructed for each\ndocument using the popularity method, then centrality measures are applied to\nchoose the most relevant candidate to boost the results of entity popularity\nmethod. We investigate the effectiveness of each centrality measure on the\nperformance across different domains and datasets. Our experiments show that a\nsimple and fast centrality measure such as Degree centrality can outperform\nother more time-consuming measures.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 19:39:23 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Hamdan", "Hussam", ""], ["Ganascia", "Jean-Gabriel", ""]]}, {"id": "1712.00334", "submitter": "Fabio Paolizzo", "authors": "Fabio Paolizzo", "title": "Enabling Embodied Analogies in Intelligent Music Systems", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present methodology is aimed at cross-modal machine learning and uses\nmultidisciplinary tools and methods drawn from a broad range of areas and\ndisciplines, including music, systematic musicology, dance, motion capture,\nhuman-computer interaction, computational linguistics and audio signal\nprocessing. Main tasks include: (1) adapting wisdom-of-the-crowd approaches to\nembodiment in music and dance performance to create a dataset of music and\nmusic lyrics that covers a variety of emotions, (2) applying\naudio/language-informed machine learning techniques to that dataset to identify\nautomatically the emotional content of the music and the lyrics, and (3)\nintegrating motion capture data from a Vicon system and dancers performing on\nthat music.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 08:27:08 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Paolizzo", "Fabio", ""]]}, {"id": "1712.00731", "submitter": "Hongwei Wang", "authors": "Hongwei Wang, Jia Wang, Miao Zhao, Jiannong Cao, Minyi Guo", "title": "Joint Topic-Semantic-aware Social Recommendation for Online Voting", "comments": "The 26th ACM International Conference on Information and Knowledge\n  Management (CIKM 2017)", "journal-ref": null, "doi": "10.1145/3132847.3132889", "report-no": null, "categories": "stat.ML cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online voting is an emerging feature in social networks, in which users can\nexpress their attitudes toward various issues and show their unique interest.\nOnline voting imposes new challenges on recommendation, because the propagation\nof votings heavily depends on the structure of social networks as well as the\ncontent of votings. In this paper, we investigate how to utilize these two\nfactors in a comprehensive manner when doing voting recommendation. First, due\nto the fact that existing text mining methods such as topic model and semantic\nmodel cannot well process the content of votings that is typically short and\nambiguous, we propose a novel Topic-Enhanced Word Embedding (TEWE) method to\nlearn word and document representation by jointly considering their topics and\nsemantics. Then we propose our Joint Topic-Semantic-aware social Matrix\nFactorization (JTS-MF) model for voting recommendation. JTS-MF model calculates\nsimilarity among users and votings by combining their TEWE representation and\nstructural information of social networks, and preserves this\ntopic-semantic-social similarity during matrix factorization. To evaluate the\nperformance of TEWE representation and JTS-MF model, we conduct extensive\nexperiments on real online voting dataset. The results prove the efficacy of\nour approach against several state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 08:11:43 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Wang", "Hongwei", ""], ["Wang", "Jia", ""], ["Zhao", "Miao", ""], ["Cao", "Jiannong", ""], ["Guo", "Minyi", ""]]}, {"id": "1712.00732", "submitter": "Hongwei Wang", "authors": "Hongwei Wang, Fuzheng Zhang, Min Hou, Xing Xie, Minyi Guo, Qi Liu", "title": "SHINE: Signed Heterogeneous Information Network Embedding for Sentiment\n  Link Prediction", "comments": "The 11th ACM International Conference on Web Search and Data Mining\n  (WSDM 2018)", "journal-ref": null, "doi": "10.1145/3159652.3159666", "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online social networks people often express attitudes towards others,\nwhich forms massive sentiment links among users. Predicting the sign of\nsentiment links is a fundamental task in many areas such as personal\nadvertising and public opinion analysis. Previous works mainly focus on textual\nsentiment classification, however, text information can only disclose the \"tip\nof the iceberg\" about users' true opinions, of which the most are unobserved\nbut implied by other sources of information such as social relation and users'\nprofile. To address this problem, in this paper we investigate how to predict\npossibly existing sentiment links in the presence of heterogeneous information.\nFirst, due to the lack of explicit sentiment links in mainstream social\nnetworks, we establish a labeled heterogeneous sentiment dataset which consists\nof users' sentiment relation, social relation and profile knowledge by\nentity-level sentiment extraction method. Then we propose a novel and flexible\nend-to-end Signed Heterogeneous Information Network Embedding (SHINE) framework\nto extract users' latent representations from heterogeneous networks and\npredict the sign of unobserved sentiment links. SHINE utilizes multiple deep\nautoencoders to map each user into a low-dimension feature space while\npreserving the network structure. We demonstrate the superiority of SHINE over\nstate-of-the-art baselines on link prediction and node recommendation in two\nreal-world datasets. The experimental results also prove the efficacy of SHINE\nin cold start scenario.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 08:21:31 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Wang", "Hongwei", ""], ["Zhang", "Fuzheng", ""], ["Hou", "Min", ""], ["Xie", "Xing", ""], ["Guo", "Minyi", ""], ["Liu", "Qi", ""]]}, {"id": "1712.01264", "submitter": "Anh Nguyen Duc", "authors": "Anh Nguyen Duc, Hilde Gudvangen", "title": "A Context-aware Recommender System for Hyperlocal News: A Conceptual\n  Framework", "comments": "This is the author's version of the work, Norwegian Big Data\n  Symposium (NOBIDS) 2016, Trondheim, Norway", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems (RSs) have been popular in variety of application domains\ndue to the increased demand for filtering and sorting items and information.\nToday, there is a numerous approaches and algorithms of data filtering and\nrecommendations. This works presents a conceptual framework for constructing a\nmobile RS in hyper-local news domain. The mobile RS is designed to deal with\nspecific requirements of news readers, such as spatial- temporal relevance,\nrecency, real-time update and validated news. The implementation of the RS in a\ndistributed file system is also discussed.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 22:34:25 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Duc", "Anh Nguyen", ""], ["Gudvangen", "Hilde", ""]]}, {"id": "1712.01329", "submitter": "Dana Kianfar", "authors": "Mircea Mironenco, Dana Kianfar, Ke Tran, Evangelos Kanoulas,\n  Efstratios Gavves", "title": "Examining Cooperation in Visual Dialog Models", "comments": "9 pages, 5 figures, 2 tables, code at\n  http://github.com/danakianfar/Examining-Cooperation-in-VDM/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a blackbox intervention method for visual dialog\nmodels, with the aim of assessing the contribution of individual linguistic or\nvisual components. Concretely, we conduct structured or randomized\ninterventions that aim to impair an individual component of the model, and\nobserve changes in task performance. We reproduce a state-of-the-art visual\ndialog model and demonstrate that our methodology yields surprising insights,\nnamely that both dialog and image information have minimal contributions to\ntask performance. The intervention method presented here can be applied as a\nsanity check for the strength and robustness of each component in visual dialog\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 20:16:52 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Mironenco", "Mircea", ""], ["Kianfar", "Dana", ""], ["Tran", "Ke", ""], ["Kanoulas", "Evangelos", ""], ["Gavves", "Efstratios", ""]]}, {"id": "1712.01562", "submitter": "Ritvik Shrivastava", "authors": "Kuntal Dey, Ritvik Shrivastava, Saroj Kaushik and L. Venkata\n  Subramaniam", "title": "EmTaggeR: A Word Embedding Based Novel Method for Hashtag Recommendation\n  on Twitter", "comments": "Accepted at the IEEE International Conference on Data Mining (ICDM)\n  2017 ACUMEN Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hashtag recommendation problem addresses recommending (suggesting) one or\nmore hashtags to explicitly tag a post made on a given social network platform,\nbased upon the content and context of the post. In this work, we propose a\nnovel methodology for hashtag recommendation for microblog posts, specifically\nTwitter. The methodology, EmTaggeR, is built upon a training-testing framework\nthat builds on the top of the concept of word embedding. The training phase\ncomprises of learning word vectors associated with each hashtag, and deriving a\nword embedding for each hashtag. We provide two training procedures, one in\nwhich each hashtag is trained with a separate word embedding model applicable\nin the context of that hashtag, and another in which each hashtag obtains its\nembedding from a global context. The testing phase constitutes computing the\naverage word embedding of the test post, and finding the similarity of this\nembedding with the known embeddings of the hashtags. The tweets that contain\nthe most-similar hashtag are extracted, and all the hashtags that appear in\nthese tweets are ranked in terms of embedding similarity scores. The top-K\nhashtags that appear in this ranked list, are recommended for the given test\npost. Our system produces F1 score of 50.83%, improving over the LDA baseline\nby around 6.53 times, outperforming the best-performing system known in the\nliterature that provides a lift of 6.42 times. EmTaggeR is a fast, scalable and\nlightweight system, which makes it practical to deploy in real-life\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 10:29:14 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Dey", "Kuntal", ""], ["Shrivastava", "Ritvik", ""], ["Kaushik", "Saroj", ""], ["Subramaniam", "L. Venkata", ""]]}, {"id": "1712.01913", "submitter": "Alexey Grigorev", "authors": "Alexey Grigorev", "title": "Approaching the Ad Placement Problem with Online Linear Classification:\n  The winning solution to the NIPS'17 Ad Placement Challenge", "comments": "NIPS'17 Workshop on Causal Inference and Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of computational advertising is to select the most suitable\nadvertisement candidate from a set of possible options. The candidate is\nselected in such a way that the user is most likely to positively react to it:\nclick and perform certain actions afterwards.\n  Choosing the best option is done by a \"policy\" -- an algorithm which learns\nfrom historical data and then is used for future actions. This way the policy\nshould deliver better targeted content with higher chances of interactions.\n  Constructing the policy is a difficult problem and many researches and\npractitioners from both the industry and the academia are concerned with it. To\nadvance the collaboration in this area, the organizers of NIPS'17 Workshop on\nCausal Inference and Machine Learning challenged the community to develop the\nbest policy algorithm. The challenge is based on the data generously provided\nby Criteo from the logs of their production system.\n  In this report we describe the winning approach to the challenge: our team\nwas able to achieve the IPS of 55.6 and secured the first position. Our\nsolution is made available on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 20:37:49 GMT"}, {"version": "v2", "created": "Fri, 8 Dec 2017 09:47:25 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Grigorev", "Alexey", ""]]}, {"id": "1712.01930", "submitter": "Kyriaki Kalimeri", "authors": "Kyriaki Kalimeri, Mariano G. Beiro, Matteo Delfino, Robert Raleigh and\n  Ciro Cattuto", "title": "Predicting Demographics, Moral Foundations, and Human Values from\n  Digital Behaviors", "comments": null, "journal-ref": null, "doi": "10.1016/j.chb.2018.11.024", "report-no": null, "categories": "cs.CY cs.AI cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personal electronic devices including smartphones give access to behavioural\nsignals that can be used to learn about the characteristics and preferences of\nindividuals. In this study, we explore the connection between demographic and\npsychological attributes and the digital behavioural records, for a cohort of\n7,633 people, closely representative of the US population with respect to\ngender, age, geographical distribution, education, and income. Along with the\ndemographic data, we collected self-reported assessments on validated\npsychometric questionnaires for moral traits and basic human values and\ncombined this information with passively collected multi-modal digital data\nfrom web browsing behaviour and smartphone usage. A machine learning framework\nwas then designed to infer both the demographic and psychological attributes\nfrom the behavioural data. In a cross-validated setting, our models predicted\ndemographic attributes with good accuracy as measured by the weighted AUROC\nscore (Area Under the Receiver Operating Characteristic), but were less\nperformant for the moral traits and human values. These results call for\nfurther investigation since they are still far from unveiling individuals'\npsychological fabric. This connection, along with the most predictive features\nthat we provide for each attribute, might prove useful for designing\npersonalised services, communication strategies, and interventions, and can be\nused to sketch a portrait of people with a similar worldview.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 21:20:34 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 14:33:55 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2018 16:08:49 GMT"}, {"version": "v4", "created": "Wed, 21 Nov 2018 15:21:02 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Kalimeri", "Kyriaki", ""], ["Beiro", "Mariano G.", ""], ["Delfino", "Matteo", ""], ["Raleigh", "Robert", ""], ["Cattuto", "Ciro", ""]]}, {"id": "1712.02084", "submitter": "Pierre-Francois Marteau", "authors": "Pierre-Fran\\c{c}ois Marteau (EXPRESSION)", "title": "Sequence Covering for Efficient Host-Based Intrusion Detection", "comments": null, "journal-ref": null, "doi": "10.1109/TIFS.2018.2868614", "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new similarity measure, the covering similarity, that\nwe formally define for evaluating the similarity between a symbolic sequence\nand a set of symbolic sequences. A pair-wise similarity can also be directly\nderived from the covering similarity to compare two symbolic sequences. An\nefficient implementation to compute the covering similarity is proposed that\nuses a suffix tree data-structure, but other implementations, based on suffix\narray for instance, are possible and possibly necessary for handling large\nscale problems. We have used this similarity to isolate attack sequences from\nnormal sequences in the scope of Host-based Intrusion Detection. We have\nassessed the covering similarity on two well-known benchmarks in the field. In\nview of the results reported on these two datasets for the state of the art\nmethods, and according to the comparative study we have carried out based on\nthree challenging similarity measures commonly used for string processing or in\nbioinformatics, we show that the covering similarity is particularly relevant\nto address the detection of anomalies in sequences of system calls\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 08:49:42 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 09:58:49 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Marteau", "Pierre-Fran\u00e7ois", "", "EXPRESSION"]]}, {"id": "1712.02316", "submitter": "Mahdi Namazifar", "authors": "Mahdi Namazifar", "title": "Named Entity Sequence Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Recognition (NER) aims at locating and classifying named\nentities in text. In some use cases of NER, including cases where detected\nnamed entities are used in creating content recommendations, it is crucial to\nhave a reliable confidence level for the detected named entities. In this work\nwe study the problem of finding confidence levels for detected named entities.\nWe refer to this problem as Named Entity Sequence Classification (NESC). We\nframe NESC as a binary classification problem and we use NER as well as\nrecurrent neural networks to find the probability of candidate named entity is\na real named entity. We apply this approach to Tweet texts and we show how we\ncould find named entities with high confidence levels from Tweets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 18:33:55 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Namazifar", "Mahdi", ""]]}, {"id": "1712.02342", "submitter": "Cong Quan", "authors": "Libing Wu, Cong Quan, Chenliang Li, Qian Wang, Bolong Zheng", "title": "A Context-Aware User-Item Representation Learning for Item\n  Recommendation", "comments": "Submitted to the IEEE Transactions on Knowledge and Data Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both reviews and user-item interactions (i.e., rating scores) have been\nwidely adopted for user rating prediction. However, these existing techniques\nmainly extract the latent representations for users and items in an independent\nand static manner. That is, a single static feature vector is derived to encode\nher preference without considering the particular characteristics of each\ncandidate item. We argue that this static encoding scheme is difficult to fully\ncapture the users' preference. In this paper, we propose a novel context-aware\nuser-item representation learning model for rating prediction, named CARL.\nNamely, CARL derives a joint representation for a given user-item pair based on\ntheir individual latent features and latent feature interactions. Then, CARL\nadopts Factorization Machines to further model higher-order feature\ninteractions on the basis of the user-item pair for rating prediction.\nSpecifically, two separate learning components are devised in CARL to exploit\nreview data and interaction data respectively: review-based feature learning\nand interaction-based feature learning. In review-based learning component,\nwith convolution operations and attention mechanism, the relevant features for\na user-item pair are extracted by jointly considering their corresponding\nreviews. However, these features are only review-driven and may not be\ncomprehensive. Hence, interaction-based learning component further extracts\ncomplementary features from interaction data alone, also on the basis of\nuser-item pairs. The final rating score is then derived with a dynamic linear\nfusion mechanism. Experiments on five real-world datasets show that CARL\nachieves significantly better rating prediction accuracy than existing\nstate-of-the-art alternatives. Also, with attention mechanism, we show that the\nrelevant information in reviews can be highlighted to interpret the rating\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 08:47:48 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 11:54:53 GMT"}, {"version": "v3", "created": "Tue, 26 Dec 2017 17:05:23 GMT"}, {"version": "v4", "created": "Wed, 27 Dec 2017 06:37:24 GMT"}, {"version": "v5", "created": "Fri, 29 Dec 2017 06:05:39 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Wu", "Libing", ""], ["Quan", "Cong", ""], ["Li", "Chenliang", ""], ["Wang", "Qian", ""], ["Zheng", "Bolong", ""]]}, {"id": "1712.02820", "submitter": "Heri Ramampiaro", "authors": "Basant Agarwal, Heri Ramampiaro, Helge Langseth, Massimiliano Ruocco", "title": "A Deep Network Model for Paraphrase Detection in Short Text Messages", "comments": null, "journal-ref": "B Agarwal, H. Ramampiaro, H Langseth, M Ruocco, (2018), \"A Deep\n  Network Model for Paraphrase Detection in Short Text Messages\". In\n  Information Processing & Management Journal (IPM), 54(6), pp. 922-937.\n  Elsevier", "doi": "10.1016/j.ipm.2018.06.005", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with paraphrase detection. The ability to detect\nsimilar sentences written in natural language is crucial for several\napplications, such as text mining, text summarization, plagiarism detection,\nauthorship authentication and question answering. Given two sentences, the\nobjective is to detect whether they are semantically identical. An important\ninsight from this work is that existing paraphrase systems perform well when\napplied on clean texts, but they do not necessarily deliver good performance\nagainst noisy texts. Challenges with paraphrase detection on user generated\nshort texts, such as Twitter, include language irregularity and noise. To cope\nwith these challenges, we propose a novel deep neural network-based approach\nthat relies on coarse-grained sentence modeling using a convolutional neural\nnetwork and a long short-term memory model, combined with a specific\nfine-grained word-level similarity matching model. Our experimental results\nshow that the proposed approach outperforms existing state-of-the-art\napproaches on user-generated noisy social media data, such as Twitter texts,\nand achieves highly competitive performance on a cleaner corpus.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 19:10:45 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Agarwal", "Basant", ""], ["Ramampiaro", "Heri", ""], ["Langseth", "Helge", ""], ["Ruocco", "Massimiliano", ""]]}, {"id": "1712.02912", "submitter": "Fabien Andr\\'e", "authors": "Fabien Andr\\'e", "title": "Exploiting Modern Hardware for High-Dimensional Nearest Neighbor Search", "comments": "PhD Thesis, 123 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.IR cs.MM cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many multimedia information retrieval or machine learning problems require\nefficient high-dimensional nearest neighbor search techniques. For instance,\nmultimedia objects (images, music or videos) can be represented by\nhigh-dimensional feature vectors. Finding two similar multimedia objects then\ncomes down to finding two objects that have similar feature vectors. In the\ncurrent context of mass use of social networks, large scale multimedia\ndatabases or large scale machine learning applications are more and more\ncommon, calling for efficient nearest neighbor search approaches.\n  This thesis builds on product quantization, an efficient nearest neighbor\nsearch technique that compresses high-dimensional vectors into short codes.\nThis makes it possible to store very large databases entirely in RAM, enabling\nlow response times. We propose several contributions that exploit the\ncapabilities of modern CPUs, especially SIMD and the cache hierarchy, to\nfurther decrease response times offered by product quantization.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 02:14:17 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Andr\u00e9", "Fabien", ""]]}, {"id": "1712.02987", "submitter": "Chen Zhu", "authors": "Hao Lin, Hengshu Zhu, Yuan Zuo, Chen Zhu, Junjie Wu, Hui Xiong", "title": "Collaborative Company Profiling: Insights from an Employee's Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Company profiling is an analytical process to build an indepth understanding\nof company's fundamental characteristics. It serves as an effective way to gain\nvital information of the target company and acquire business intelligence.\nTraditional approaches for company profiling rely heavily on the availability\nof rich finance information about the company, such as finance reports and SEC\nfilings, which may not be readily available for many private companies.\nHowever, the rapid prevalence of online employment services enables a new\nparadigm - to obtain the variety of company's information from their employees'\nonline ratings and comments. This, in turn, raises the challenge to develop\ncompany profiles from an employee's perspective. To this end, in this paper, we\npropose a method named Company Profiling based Collaborative Topic Regression\n(CPCTR), for learning the latent structural patterns of companies. By\nformulating a joint optimization framework, CPCTR has the ability in\ncollaboratively modeling both textual (e.g., reviews) and numerical information\n(e.g., salaries and ratings). Indeed, with the identified patterns, including\nthe positive/negative opinions and the latent variable that influences salary,\nwe can effectively carry out opinion analysis and salary prediction. Extensive\nexperiments were conducted on a real-world data set to validate the\neffectiveness of CPCTR. The results show that our method provides a\ncomprehensive understanding of company characteristics and delivers a more\neffective prediction of salaries than other baselines.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 09:16:56 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Lin", "Hao", ""], ["Zhu", "Hengshu", ""], ["Zuo", "Yuan", ""], ["Zhu", "Chen", ""], ["Wu", "Junjie", ""], ["Xiong", "Hui", ""]]}, {"id": "1712.03158", "submitter": "Thijs Laarhoven", "authors": "Thijs Laarhoven", "title": "Graph-based time-space trade-offs for approximate near neighbors", "comments": "26 pages, 4 figures", "journal-ref": "34th International Symposium on Computational Geometry (SoCG), pp.\n  57:1-57:14, 2018", "doi": "10.4230/LIPIcs.SoCG.2018.57", "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take a first step towards a rigorous asymptotic analysis of graph-based\napproaches for finding (approximate) nearest neighbors in high-dimensional\nspaces, by analyzing the complexity of (randomized) greedy walks on the\napproximate near neighbor graph. For random data sets of size $n = 2^{o(d)}$ on\nthe $d$-dimensional Euclidean unit sphere, using near neighbor graphs we can\nprovably solve the approximate nearest neighbor problem with approximation\nfactor $c > 1$ in query time $n^{\\rho_q + o(1)}$ and space $n^{1 + \\rho_s +\no(1)}$, for arbitrary $\\rho_q, \\rho_s \\geq 0$ satisfying \\begin{align} (2c^2 -\n1) \\rho_q + 2 c^2 (c^2 - 1) \\sqrt{\\rho_s (1 - \\rho_s)} \\geq c^4. \\end{align}\nGraph-based near neighbor searching is especially competitive with hash-based\nmethods for small $c$ and near-linear memory, and in this regime the asymptotic\nscaling of a greedy graph-based search matches the recent optimal hash-based\ntrade-offs of Andoni-Laarhoven-Razenshteyn-Waingarten [SODA'17]. We further\nstudy how the trade-offs scale when the data set is of size $n =\n2^{\\Theta(d)}$, and analyze asymptotic complexities when applying these results\nto lattice sieving.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 16:26:22 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Laarhoven", "Thijs", ""]]}, {"id": "1712.03190", "submitter": "Hossein Azgomi", "authors": "Hossein Azgomi, Masumeh Ghasemi Mahsayeh, Masoud Mohammadi, Milad\n  Moradi", "title": "A Method for Finding Similar Documents Relying on Adding Repetition of\n  Symbols in Length Based Filtering", "comments": null, "journal-ref": "Indian Journal of Scientific Research, 2(1):81-84, 2014", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A basic topic in mining of massive dataset is finding similar items. As an\nexample, finding similar documents can be recommended. In this case many\nmethods are existed. For example, Shingling method and length based filtering\nare one of them. In Shingling method, from each document, substrings have been\nselected with symbol name and, they are placed on one set. For finding similar\ndocuments, the similarities of sets that related with them have been\ncalculated. In Length based filtering just documents which close these lengths\nhave been compared. These methods don't consider repetition of symbols. With\nconsidering the repetition can calculate length of documents with more\naccurately. In this paper we suggested a method for finding similar documents\nwith considering the repetition of symbols. This method separated documents to\nbetter form. The main goal of this paper is presentation a method for finding\nsimilar documents with take fewer comparisons and time indeed.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 17:32:45 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Azgomi", "Hossein", ""], ["Mahsayeh", "Masumeh Ghasemi", ""], ["Mohammadi", "Masoud", ""], ["Moradi", "Milad", ""]]}, {"id": "1712.03249", "submitter": "Tobias Moers", "authors": "Florian Krebs, Bruno Lubascher, Tobias Moers, Pieter Schaap, Gerasimos\n  Spanakis", "title": "Social Emotion Mining Techniques for Facebook Posts Reaction Prediction", "comments": "10 pages, 13 figures and accepted at ICAART 2018. (Dataset:\n  https://github.com/jerryspan/FacebookR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As of February 2016 Facebook allows users to express their experienced\nemotions about a post by using five so-called `reactions'. This research paper\nproposes and evaluates alternative methods for predicting these reactions to\nuser posts on public pages of firms/companies (like supermarket chains). For\nthis purpose, we collected posts (and their reactions) from Facebook pages of\nlarge supermarket chains and constructed a dataset which is available for other\nresearches. In order to predict the distribution of reactions of a new post,\nneural network architectures (convolutional and recurrent neural networks) were\ntested using pretrained word embeddings. Results of the neural networks were\nimproved by introducing a bootstrapping approach for sentiment and emotion\nmining on the comments for each post. The final model (a combination of neural\nnetwork and a baseline emotion miner) is able to predict the reaction\ndistribution on Facebook posts with a mean squared error (or misclassification\nrate) of 0.135.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 19:05:50 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Krebs", "Florian", ""], ["Lubascher", "Bruno", ""], ["Moers", "Tobias", ""], ["Schaap", "Pieter", ""], ["Spanakis", "Gerasimos", ""]]}, {"id": "1712.03404", "submitter": "Dayong Tian", "authors": "Dayong Tian and Maoguo Gong and Deyun Zhou and Jiao Shi and Yu Lei", "title": "Semi-supervised Multimodal Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieving nearest neighbors across correlated data in multiple modalities,\nsuch as image-text pairs on Facebook and video-tag pairs on YouTube, has become\na challenging task due to the huge amount of data. Multimodal hashing methods\nthat embed data into binary codes can boost the retrieving speed and reduce\nstorage requirement. As unsupervised multimodal hashing methods are usually\ninferior to supervised ones, while the supervised ones requires too much\nmanually labeled data, the proposed method in this paper utilizes a part of\nlabels to design a semi-supervised multimodal hashing method. It first computes\nthe transformation matrices for data matrices and label matrix. Then, with\nthese transformation matrices, fuzzy logic is introduced to estimate a label\nmatrix for unlabeled data. Finally, it uses the estimated label matrix to learn\nhashing functions for data in each modality to generate a unified binary code\nmatrix. Experiments show that the proposed semi-supervised method with 50%\nlabels can get a medium performance among the compared supervised ones and\nachieve an approximate performance to the best supervised method with 90%\nlabels. With only 10% labels, the proposed method can still compete with the\nworst compared supervised one.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 15:38:18 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Tian", "Dayong", ""], ["Gong", "Maoguo", ""], ["Zhou", "Deyun", ""], ["Shi", "Jiao", ""], ["Lei", "Yu", ""]]}, {"id": "1712.03585", "submitter": "Daniyal Shahrokhian", "authors": "Daniyal Shahrokhian, Alejandro Vera de Juan", "title": "SneakPeek: Interest Mining of Images based on User Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, eye tracking is the most used technology to detect areas of\ninterest. This kind of technology requires specialized equipment recording\nuser's eyes. In this paper, we propose SneakPeek, a different approach to\ndetect areas of interest on images displayed in web pages based on the zooming\nand panning actions of the users through the image. We have validated our\nproposed solution with a group of test subjects that have performed a test in\nour on-line prototype. Being this the first iteration of the algorithm, we have\nfound both good and bad results, depending on the type of image. In specific,\nSneakPeek works best with medium/big objects in medium/big sized images. The\nreason behind it is the limitation on detection when smartphone screens keep\ngetting bigger and bigger. SneakPeek can be adapted to any website by simply\nadapting the controller interface for the specific case.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 20:37:19 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Shahrokhian", "Daniyal", ""], ["de Juan", "Alejandro Vera", ""]]}, {"id": "1712.03622", "submitter": "George Philipp", "authors": "George Philipp, Ryen W. White", "title": "Interactions between Health Searchers and Search Engines", "comments": "SIGIR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web is an important resource for understanding and diagnosing medical\nconditions. Based on exposure to online content, people may develop undue\nhealth concerns, believing that common and benign symptoms are explained by\nserious illnesses. In this paper, we investigate potential strategies to mine\nqueries and searcher histories for clues that could help search engines choose\nthe most appropriate information to present in response to exploratory medical\nqueries. To do this, we performed a longitudinal study of health search\nbehavior using the logs of a popular search engine. We found that query\nvariations which might appear innocuous (e.g. \"bad headache\" vs \"severe\nheadache\") may hold valuable information about the searcher which could be used\nby search engines to improve performance. Furthermore, we investigated how\nmedically concerned users respond differently to search engine result pages\n(SERPs) and find that their disposition for clicking on concerning pages is\npronounced, potentially leading to a self-reinforcement of concern. Finally, we\nstudied to which degree variations in the SERP impact future search and\nreal-world health-seeking behavior and obtained some surprising results (e.g.,\nviewing concerning pages may lead to a short-term reduction of real-world\nhealth seeking).\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 01:44:54 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Philipp", "George", ""], ["White", "Ryen W.", ""]]}, {"id": "1712.03941", "submitter": "Gautam Singh", "authors": "Gautam Singh, Gargi Dasgupta, Yu Deng", "title": "Fast Nearest-Neighbor Classification using RNN in Domains with Large\n  Number of Classes", "comments": "12 Pages, 2 Theorems, 6 Figures, 1 Algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In scenarios involving text classification where the number of classes is\nlarge (in multiples of 10000s) and training samples for each class are few and\noften verbose, nearest neighbor methods are effective but very slow in\ncomputing a similarity score with training samples of every class. On the other\nhand, machine learning models are fast at runtime but training them adequately\nis not feasible using few available training samples per class. In this paper,\nwe propose a hybrid approach that cascades 1) a fast but less-accurate\nrecurrent neural network (RNN) model and 2) a slow but more-accurate\nnearest-neighbor model using bag of syntactic features.\n  Using the cascaded approach, our experiments, performed on data set from IT\nsupport services where customer complaint text needs to be classified to return\ntop-$N$ possible error codes, show that the query-time of the slow system is\nreduced to $1/6^{th}$ while its accuracy is being improved. Our approach\noutperforms an LSH-based baseline for query-time reduction. We also derive a\nlower bound on the accuracy of the cascaded model in terms of the accuracies of\nthe individual models. In any two-stage approach, choosing the right number of\ncandidates to pass on to the second stage is crucial. We prove a result that\naids in choosing this cutoff number for the cascaded system.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 18:48:55 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Singh", "Gautam", ""], ["Dasgupta", "Gargi", ""], ["Deng", "Yu", ""]]}, {"id": "1712.04116", "submitter": "Peixian Chen", "authors": "Peixian Chen, Zhourong Chen and Nevin L. Zhang", "title": "A Novel Document Generation Process for Topic Detection based on\n  Hierarchical Latent Tree Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel document generation process based on hierarchical latent\ntree models (HLTMs) learned from data. An HLTM has a layer of observed word\nvariables at the bottom and multiple layers of latent variables on top. For\neach document, we first sample values for the latent variables layer by layer\nvia logic sampling, then draw relative frequencies for the words conditioned on\nthe values of the latent variables, and finally generate words for the document\nusing the relative word frequencies. The motivation for the work is to take\nword counts into consideration with HLTMs. In comparison with LDA-based\nhierarchical document generation processes, the new process achieves\ndrastically better model fit with much fewer parameters. It also yields more\nmeaningful topics and topic hierarchies. It is the new state-of-the-art for the\nhierarchical topic detection.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 04:07:10 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 02:46:02 GMT"}, {"version": "v3", "created": "Fri, 28 Jun 2019 03:15:45 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Chen", "Peixian", ""], ["Chen", "Zhourong", ""], ["Zhang", "Nevin L.", ""]]}, {"id": "1712.04671", "submitter": "Karen Pinel-Sauvagnat", "authors": "Gilles Hubert, Jose G. Moreno, Karen Pinel-Sauvagnat, Yoann Pitarch", "title": "Everything You Always Wanted to Know About TREC RTS* (*But Were Afraid\n  to Ask)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TREC Real-Time Summarization (RTS) track provides a framework for\nevaluating systems monitoring the Twitter stream and pushing tweets to users\naccording to given profiles. It includes metrics, files, settings and\nhypothesis provided by the organizers. In this work, we perform a thorough\nanalysis of each component of the framework used in 2016 and 2017 and found\nsome limitations for the Scenario A of this track. Our main findings point out\nthe weakness of the metrics and give clear recommendations to fairly reuse the\ncollection.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 09:25:50 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Hubert", "Gilles", ""], ["Moreno", "Jose G.", ""], ["Pinel-Sauvagnat", "Karen", ""], ["Pitarch", "Yoann", ""]]}, {"id": "1712.05191", "submitter": "Sachin Pawar", "authors": "Sachin Pawar, Girish K. Palshikar, Pushpak Bhattacharyya", "title": "Relation Extraction : A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of the Internet, large amount of digital text is generated\neveryday in the form of news articles, research publications, blogs, question\nanswering forums and social media. It is important to develop techniques for\nextracting information automatically from these documents, as lot of important\ninformation is hidden within them. This extracted information can be used to\nimprove access and management of knowledge hidden in large text corpora.\nSeveral applications such as Question Answering, Information Retrieval would\nbenefit from this information. Entities like persons and organizations, form\nthe most basic unit of the information. Occurrences of entities in a sentence\nare often linked through well-defined relations; e.g., occurrences of person\nand organization in a sentence may be linked through relations such as employed\nat. The task of Relation Extraction (RE) is to identify such relations\nautomatically. In this paper, we survey several important supervised,\nsemi-supervised and unsupervised RE techniques. We also cover the paradigms of\nOpen Information Extraction (OIE) and Distant Supervision. Finally, we describe\nsome of the recent trends in the RE techniques and possible future research\ndirections. This survey would be useful for three kinds of readers - i)\nNewcomers in the field who want to quickly learn about RE; ii) Researchers who\nwant to know how the various RE techniques evolved over time and what are\npossible future research directions and iii) Practitioners who just need to\nknow which RE technique works best in various settings.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 12:04:10 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Pawar", "Sachin", ""], ["Palshikar", "Girish K.", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1712.05197", "submitter": "Francisco Raposo", "authors": "Francisco Raposo, David Martins de Matos, Ricardo Ribeiro, Suhua Tang,\n  Yi Yu", "title": "Towards Deep Modeling of Music Semantics using EEG Regularizers", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SD eess.AS q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling of music audio semantics has been previously tackled through\nlearning of mappings from audio data to high-level tags or latent unsupervised\nspaces. The resulting semantic spaces are theoretically limited, either because\nthe chosen high-level tags do not cover all of music semantics or because audio\ndata itself is not enough to determine music semantics. In this paper, we\npropose a generic framework for semantics modeling that focuses on the\nperception of the listener, through EEG data, in addition to audio data. We\nimplement this framework using a novel end-to-end 2-view Neural Network (NN)\narchitecture and a Deep Canonical Correlation Analysis (DCCA) loss function\nthat forces the semantic embedding spaces of both views to be maximally\ncorrelated. We also detail how the EEG dataset was collected and use it to\ntrain our proposed model. We evaluate the learned semantic space in a transfer\nlearning context, by using it as an audio feature extractor in an independent\ndataset and proxy task: music audio-lyrics cross-modal retrieval. We show that\nour embedding model outperforms Spotify features and performs comparably to a\nstate-of-the-art embedding model that was trained on 700 times more data. We\nfurther discuss improvements to the model that are likely to improve its\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 12:27:11 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 15:57:29 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Raposo", "Francisco", ""], ["de Matos", "David Martins", ""], ["Ribeiro", "Ricardo", ""], ["Tang", "Suhua", ""], ["Yu", "Yi", ""]]}, {"id": "1712.05403", "submitter": "Yi Tay", "authors": "Yi Tay, Anh Tuan Luu, Siu Cheung Hui", "title": "Learning to Attend via Word-Aspect Associative Fusion for Aspect-based\n  Sentiment Analysis", "comments": "Accepted to AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based sentiment analysis (ABSA) tries to predict the polarity of a\ngiven document with respect to a given aspect entity. While neural network\narchitectures have been successful in predicting the overall polarity of\nsentences, aspect-specific sentiment analysis still remains as an open problem.\nIn this paper, we propose a novel method for integrating aspect information\ninto the neural model. More specifically, we incorporate aspect information\ninto the neural model by modeling word-aspect relationships. Our novel model,\n\\textit{Aspect Fusion LSTM} (AF-LSTM) learns to attend based on associative\nrelationships between sentence words and aspect which allows our model to\nadaptively focus on the correct words given an aspect term. This ameliorates\nthe flaws of other state-of-the-art models that utilize naive concatenations to\nmodel word-aspect similarity. Instead, our model adopts circular convolution\nand circular correlation to model the similarity between aspect and words and\nelegantly incorporates this within a differentiable neural attention framework.\nFinally, our model is end-to-end differentiable and highly related to\nconvolution-correlation (holographic like) memories. Our proposed neural model\nachieves state-of-the-art performance on benchmark datasets, outperforming\nATAE-LSTM by $4\\%-5\\%$ on average across multiple datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 12:46:44 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Tay", "Yi", ""], ["Luu", "Anh Tuan", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1712.05574", "submitter": "Avikalp Srivastava", "authors": "Avikalp Srivastava, Madhav Datt", "title": "Soft Seeded SSL Graphs for Unsupervised Semantic Similarity-based\n  Retrieval", "comments": "Published in Proceedings of the 2017 ACM Conference on Information\n  and Knowledge Management (CIKM '17)", "journal-ref": null, "doi": "10.1145/3132847.3133162", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic similarity based retrieval is playing an increasingly important role\nin many IR systems such as modern web search, question-answering, similar\ndocument retrieval etc. Improvements in retrieval of semantically similar\ncontent are very significant to applications like Quora, Stack Overflow, Siri\netc. We propose a novel unsupervised model for semantic similarity based\ncontent retrieval, where we construct semantic flow graphs for each query, and\nintroduce the concept of \"soft seeding\" in graph based semi-supervised learning\n(SSL) to convert this into an unsupervised model.\n  We demonstrate the effectiveness of our model on an equivalent question\nretrieval problem on the Stack Exchange QA dataset, where our unsupervised\napproach significantly outperforms the state-of-the-art unsupervised models,\nand produces comparable results to the best supervised models. Our research\nprovides a method to tackle semantic similarity based retrieval without any\ntraining data, and allows seamless extension to different domain QA\ncommunities, as well as to other semantic equivalence tasks.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 08:22:23 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Srivastava", "Avikalp", ""], ["Datt", "Madhav", ""]]}, {"id": "1712.05956", "submitter": "Stefan Heindorf", "authors": "Stefan Heindorf (1), Martin Potthast (2), Gregor Engels (1), Benno\n  Stein (3) ((1) Paderborn University, (2) Leipzig University, (3)\n  Bauhaus-Universit\\\"at Weimar)", "title": "Overview of the Wikidata Vandalism Detection Task at WSDM Cup 2017", "comments": "Overview of the Wikidata Vandalism Detection Task at WSDM Cup 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on the Wikidata vandalism detection task at the WSDM Cup 2017. The\ntask received five submissions for which this paper describes their evaluation\nand a comparison to state of the art baselines. Unlike previous work, we recast\nWikidata vandalism detection as an online learning problem, requiring\nparticipant software to predict vandalism in near real-time. The\nbest-performing approach achieves a ROC-AUC of 0.947 at a PR-AUC of 0.458. In\nparticular, this task was organized as a software submission task: to maximize\nreproducibility as well as to foster future research and development on this\ntask, the participants were asked to submit their working software to the TIRA\nexperimentation platform along with the source code for open source release.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 13:19:41 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Heindorf", "Stefan", ""], ["Potthast", "Martin", ""], ["Engels", "Gregor", ""], ["Stein", "Benno", ""]]}, {"id": "1712.05997", "submitter": "Amir Karami", "authors": "Amir Karami", "title": "Taming Wild High Dimensional Text Data with a Fuzzy Lash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bag of words (BOW) represents a corpus in a matrix whose elements are the\nfrequency of words. However, each row in the matrix is a very high-dimensional\nsparse vector. Dimension reduction (DR) is a popular method to address sparsity\nand high-dimensionality issues. Among different strategies to develop DR\nmethod, Unsupervised Feature Transformation (UFT) is a popular strategy to map\nall words on a new basis to represent BOW. The recent increase of text data and\nits challenges imply that DR area still needs new perspectives. Although a wide\nrange of methods based on the UFT strategy has been developed, the fuzzy\napproach has not been considered for DR based on this strategy. This research\ninvestigates the application of fuzzy clustering as a DR method based on the\nUFT strategy to collapse BOW matrix to provide a lower-dimensional\nrepresentation of documents instead of the words in a corpus. The quantitative\nevaluation shows that fuzzy clustering produces superior performance and\nfeatures to Principal Components Analysis (PCA) and Singular Value\nDecomposition (SVD), two popular DR methods based on the UFT strategy.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 17:57:57 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Karami", "Amir", ""]]}, {"id": "1712.06076", "submitter": "Rushin Gindra", "authors": "Rushin Gindra (1 and 2), Srushti Kotak (1 and 2), Asmita Natekar (1\n  and 2) and Grishma Sharma (1 and 3) ((1) K. J. Somaiya College of\n  Engineering, (2) Undergraduate Scholar, (3) Assistant Professor and Project\n  Adviser)", "title": "Using Deep learning methods for generation of a personalized list of\n  shuffled songs", "comments": "My Undergraduate Project. Future research may/may not be in the same\n  field", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shuffle mode, where songs are played in a randomized order that is\ndecided upon for all tracks at once, is widely found and known to exist in\nmusic player systems. There are only few music enthusiasts who use this mode\nsince it either is too random to suit their mood or it keeps on repeating the\nsame list every time. In this paper, we propose to build a convolutional deep\nbelief network(CDBN) that is trained to perform genre recognition based on\naudio features retrieved from the records of the Million Song Dataset. The\nlearned parameters shall be used to initialize a multi-layer perceptron which\ntakes extracted features of user's playlist as input alongside the metadata to\nclassify to various categories. These categories will be shuffled\nretrospectively based on the metadata to autonomously provide with a list that\nis efficacious in playing songs that are desired by humans in normal\nconditions.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 09:18:20 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2019 06:10:48 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Gindra", "Rushin", "", "1 and 2"], ["Kotak", "Srushti", "", "1 and 2"], ["Natekar", "Asmita", "", "1\n  and 2"], ["Sharma", "Grishma", "", "1 and 3"]]}, {"id": "1712.06704", "submitter": "Kriste Krstovski", "authors": "Kriste Krstovski, Michael J. Kurtz, David A. Smith and Alberto\n  Accomazzi", "title": "Multilingual Topic Models", "comments": "18 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific publications have evolved several features for mitigating\nvocabulary mismatch when indexing, retrieving, and computing similarity between\narticles. These mitigation strategies range from simply focusing on high-value\narticle sections, such as titles and abstracts, to assigning keywords, often\nfrom controlled vocabularies, either manually or through automatic annotation.\nVarious document representation schemes possess different cost-benefit\ntradeoffs. In this paper, we propose to model different representations of the\nsame article as translations of each other, all generated from a common latent\nrepresentation in a multilingual topic model. We start with a methodological\noverview on latent variable models for parallel document representations that\ncould be used across many information science tasks. We then show how solving\nthe inference problem of mapping diverse representations into a shared topic\nspace allows us to evaluate representations based on how topically similar they\nare to the original article. In addition, our proposed approach provides means\nto discover where different concept vocabularies require improvement.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 22:45:20 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Krstovski", "Kriste", ""], ["Kurtz", "Michael J.", ""], ["Smith", "David A.", ""], ["Accomazzi", "Alberto", ""]]}, {"id": "1712.06919", "submitter": "Rafael Crescenzi", "authors": "Rafael Crescenzi, Marcelo Fernandez, Federico A. Garcia Calabria,\n  Pablo Albani, Diego Tauziet, Adriana Baravalle, Andr\\'es Sebasti\\'an\n  D'Ambrosio (Austral University)", "title": "A Production Oriented Approach for Vandalism Detection in Wikidata - The\n  Buffaloberry Vandalism Detector at WSDM Cup 2017", "comments": "Vandalism Detector at WSDM Cup 2017, see arXiv:1712.05956", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikidata is a free and open knowledge base from the Wikimedia Foundation,\nthat not only acts as a central storage of structured data for other projects\nof the organization, but also for a growing array of information systems,\nincluding search engines. Like Wikipedia, Wikidata's content can be created and\nedited by anyone; which is the main source of its strength, but also allows for\nmalicious users to vandalize it, risking the spreading of misinformation\nthrough all the systems that rely on it as a source of structured facts. Our\ntask at the WSDM Cup 2017 was to come up with a fast and reliable prediction\nsystem that narrows down suspicious edits for human revision. Elaborating on\nprevious works by Heindorf et al. we were able to outperform all other\ncontestants, while incorporating new interesting features, unifying the\nprogramming language used to only Python and refactoring the feature extractor\ninto a simpler and more compact code base.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 13:39:37 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Crescenzi", "Rafael", "", "Austral University"], ["Fernandez", "Marcelo", "", "Austral University"], ["Calabria", "Federico A. Garcia", "", "Austral University"], ["Albani", "Pablo", "", "Austral University"], ["Tauziet", "Diego", "", "Austral University"], ["Baravalle", "Adriana", "", "Austral University"], ["D'Ambrosio", "Andr\u00e9s Sebasti\u00e1n", "", "Austral University"]]}, {"id": "1712.06920", "submitter": "Alexey Grigorev", "authors": "Alexey Grigorev (Searchmetrics GmbH)", "title": "Large-Scale Vandalism Detection with Linear Classifiers - The\n  Conkerberry Vandalism Detector at WSDM Cup 2017", "comments": "Vandalism Detector at WSDM Cup 2017, see arXiv:1712.05956", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays many artificial intelligence systems rely on knowledge bases for\nenriching the information they process. Such Knowledge Bases are usually\ndifficult to obtain and therefore they are crowdsourced: they are available for\neveryone on the internet to suggest edits and add new information.\nUnfortunately, they are sometimes targeted by vandals who put inaccurate or\noffensive information there. This is especially bad for the systems that use\nthese Knowledge Bases: for them it is important to use reliable information to\nmake correct inferences.\n  One of such knowledge bases is Wikidata, and to fight vandals the organizers\nof WSDM Cup 2017 challenged participants to build a model for detecting\nmistrustful edits. In this paper we present the second place solution to the\ncup: we show that it is possible to achieve competitive performance with simple\nlinear classification. With our approach we can achieve AU ROC of 0.938 on the\ntest data. Additionally, compared to other approaches, ours is significantly\nfaster. The solution is made available on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 13:39:47 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Grigorev", "Alexey", "", "Searchmetrics GmbH"]]}, {"id": "1712.06921", "submitter": "Tomoya Yamazaki", "authors": "Tomoya Yamazaki, Mei Sasaki, Naoya Murakami, Takuya Makabe, Hiroki\n  Iwasawa (Yahoo Japan Corporation)", "title": "Ensemble Models for Detecting Wikidata Vandalism with Stacking - Team\n  Honeyberry Vandalism Detector at WSDM Cup 2017", "comments": "Vandalism Detector at WSDM Cup 2017, see arXiv:1712.05956", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The WSDM Cup 2017 is a binary classification task for classifying Wikidata\nrevisions into vandalism and non-vandalism. This paper describes our method\nusing some machine learning techniques such as under-sampling, feature\nselection, stacking and ensembles of models. We confirm the validity of each\ntechnique by calculating AUC-ROC of models using such techniques and not using\nthem. Additionally, we analyze the results and gain useful insights into\nimproving models for the vandalism detection task. The AUC-ROC of our final\nsubmission after the deadline resulted in 0.94412.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 13:39:52 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Yamazaki", "Tomoya", "", "Yahoo Japan Corporation"], ["Sasaki", "Mei", "", "Yahoo Japan Corporation"], ["Murakami", "Naoya", "", "Yahoo Japan Corporation"], ["Makabe", "Takuya", "", "Yahoo Japan Corporation"], ["Iwasawa", "Hiroki", "", "Yahoo Japan Corporation"]]}, {"id": "1712.06922", "submitter": "Qi Zhu", "authors": "Qi Zhu, Hongwei Ng, Liyuan Liu, Ziwei Ji, Bingjie Jiang, Jiaming Shen,\n  Huan Gui (University of Illinois Urbana-Champaign)", "title": "Wikidata Vandalism Detection - The Loganberry Vandalism Detector at WSDM\n  Cup 2017", "comments": "Vandalism Detector at WSDM Cup 2017, see arXiv:1712.05956", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikidata is the new, large-scale knowledge base of the Wikimedia Foundation.\nAs it can be edited by anyone, entries frequently get vandalized, leading to\nthe possibility that it might spread of falsified information if such posts are\nnot detected. The WSDM 2017 Wiki Vandalism Detection Challenge requires us to\nsolve this problem by computing a vandalism score denoting the likelihood that\na revision corresponds to an act of vandalism and performance is measured using\nthe ROC-AUC obtained on a held-out test set. This paper provides the details of\nour submission that obtained an ROC-AUC score of 0.91976 in the final\nevaluation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 13:40:06 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Zhu", "Qi", "", "University of Illinois Urbana-Champaign"], ["Ng", "Hongwei", "", "University of Illinois Urbana-Champaign"], ["Liu", "Liyuan", "", "University of Illinois Urbana-Champaign"], ["Ji", "Ziwei", "", "University of Illinois Urbana-Champaign"], ["Jiang", "Bingjie", "", "University of Illinois Urbana-Champaign"], ["Shen", "Jiaming", "", "University of Illinois Urbana-Champaign"], ["Gui", "Huan", "", "University of Illinois Urbana-Champaign"]]}, {"id": "1712.07525", "submitter": "Ayush Singhal", "authors": "Ayush Singhal, Pradeep Sinha, Rakesh Pant", "title": "Use of Deep Learning in Modern Recommendation System: A Summary of\n  Recent Works", "comments": "6 pages, 1 figure, 1 table, \"Published with International Journal of\n  Computer Applications (IJCA)\"", "journal-ref": "International Journal of Computer Applications 180(7):17-22,\n  December 2017", "doi": "10.5120/ijca2017916055", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the exponential increase in the amount of digital information over the\ninternet, online shops, online music, video and image libraries, search engines\nand recommendation system have become the most convenient ways to find relevant\ninformation within a short time. In the recent times, deep learning's advances\nhave gained significant attention in the field of speech recognition, image\nprocessing and natural language processing. Meanwhile, several recent studies\nhave shown the utility of deep learning in the area of recommendation systems\nand information retrieval as well. In this short review, we cover the recent\nadvances made in the field of recommendation using various variants of deep\nlearning technology. We organize the review in three parts: Collaborative\nsystem, Content based system and Hybrid system. The review also discusses the\ncontribution of deep learning integrated recommendation systems into several\napplication domains. The review concludes by discussion of the impact of deep\nlearning in recommendation system in various domain and whether deep learning\nhas shown any significant improvement over the conventional systems for\nrecommendation. Finally, we also provide future directions of research which\nare possible based on the current state of use of deep learning in\nrecommendation systems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 15:25:31 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Singhal", "Ayush", ""], ["Sinha", "Pradeep", ""], ["Pant", "Rakesh", ""]]}, {"id": "1712.07691", "submitter": "Guangyuan Piao", "authors": "Guangyuan Piao and John G. Breslin", "title": "Inferring User Interests in Microblogging Social Networks: A Survey", "comments": "pre-print, accepted at UMUAI, final version DOI\n  10.1007/s11257-018-9207-8", "journal-ref": null, "doi": "10.1007/s11257-018-9207-8", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing popularity of microblogging services such as Twitter in\nrecent years, an increasing number of users are using these services in their\ndaily lives. The huge volume of information generated by users raises new\nopportunities in various applications and areas. Inferring user interests plays\na significant role in providing personalized recommendations on microblogging\nservices, and also on third-party applications providing social logins via\nthese services, especially in cold-start situations. In this survey, we review\nuser modeling strategies with respect to inferring user interests from previous\nstudies. To this end, we focus on four dimensions of inferring user interest\nprofiles: (1) data collection, (2) representation of user interest profiles,\n(3) construction and enhancement of user interest profiles, and (4) the\nevaluation of the constructed profiles. Through this survey, we aim to provide\nan overview of state-of-the-art user modeling strategies for inferring user\ninterest profiles on microblogging social networks with respect to the four\ndimensions. For each dimension, we review and summarize previous studies based\non specified criteria. Finally, we discuss some challenges and opportunities\nfor future work in this research domain.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 20:08:47 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 23:48:21 GMT"}, {"version": "v3", "created": "Tue, 13 Mar 2018 23:24:07 GMT"}, {"version": "v4", "created": "Tue, 19 Jun 2018 12:47:20 GMT"}, {"version": "v5", "created": "Tue, 14 Aug 2018 21:43:32 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Piao", "Guangyuan", ""], ["Breslin", "John G.", ""]]}, {"id": "1712.07727", "submitter": "Ramesh Baral", "authors": "Ramesh Baral and Tao Li", "title": "PERS: A Personalized and Explainable POI Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Location-Based Social Networks (LBSN) (e.g., Facebook) have many factors\n(for instance, ratings, check-in time, etc.) that play a crucial role for the\nPoint-of-Interest (POI) recommendations. Unlike ratings, the reviews can help\nusers to elaborate their opinion and share the extent of consumption experience\nin terms of the relevant factors of interest (aspects). Though some of the\nexisting recommendation systems have been using the user reviews, most of them\nare less transparent and non-interpretable. These reasons have induced\nconsiderable attention towards explainable and interpretable recommendation. To\nthe best of our knowledge, this is the first paper to exploit the user reviews\nto incorporate the sentiment and opinions on different aspects for the\npersonalized and explainable POI recommendation. In this paper, we propose a\nmodel termed as PERS (Personalized Explainable POI Recommender System) which\nmodels the review-aspect category correlation by exploiting deep neural\nnetwork, formulates the user-aspect category bipartite relation as a bipartite\ngraph, and models the explainable recommendation using bipartite core-based and\nranking-based methods. The major contributions of this paper are: (i) it models\nusers and locations based on the aspects posted by user via reviews, (ii) it\nexploits a deep neural network to model the review-aspect category correlation,\n(iii) it provisions the incorporation of multiple contexts (e.g., categorical,\nspatial, etc.) in the POI recommendation model, (iv) it formulates the\npreference of users' on aspect category as a bipartite relation, represents it\nas a location-aspect category bipartite graph, and models the explainable\nrecommendation with the notion of ordered dense subgraph extraction using\nbipartite core-based and ranking-based approaches, and (v) it evaluates the\ngenerated recommendation with three real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 22:09:38 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Baral", "Ramesh", ""], ["Li", "Tao", ""]]}, {"id": "1712.08081", "submitter": "Hannah Bast", "authors": "Hannah Bast, Bj\\\"orn Buchhold, Elmar Haussmann", "title": "Overview of the Triple Scoring Task at the WSDM Cup 2017", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an overview of the triple scoring task at the WSDM Cup\n2017, including a description of the task and the dataset, an overview of the\nparticipating teams and their results, and a brief account of the methods\nemployed. In a nutshell, the task was to compute relevance scores for\nknowledge-base triples from relations, where such scores make sense. Due to the\nway the ground truth was constructed, scores were required to be integers from\nthe range 0..7. For example, reasonable scores for the triples \"Tim Burton\nprofession Director\" and \"Tim Burton profession Actor\" would be 7 and 2,\nrespectively, because Tim Burton is well-known as a director, but he acted only\nin a few lesser known movies.\n  The triple scoring task attracted considerable interest, with 52 initial\nregistrations and 21 teams who submitted a valid run before the deadline. The\nwinning team achieved an accuracy of 87%, that is, for that fraction of the\ntriples from the test set (which was revealed only after the deadline) the\ndifference to the score from the ground truth was at most 2. The best result\nfor the average difference from the test set scores was 1.50.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 16:54:35 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Bast", "Hannah", ""], ["Buchhold", "Bj\u00f6rn", ""], ["Haussmann", "Elmar", ""]]}, {"id": "1712.08091", "submitter": "Tien Do Huu", "authors": "Tien Huu Do, Duc Minh Nguyen, Evaggelia Tsiligianni, Bruno Cornelis\n  and Nikos Deligiannis", "title": "Multiview Deep Learning for Predicting Twitter Users' Location", "comments": "Submitted to the IEEE Transactions on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of predicting the location of users on large social networks like\nTwitter has emerged from real-life applications such as social unrest detection\nand online marketing. Twitter user geolocation is a difficult and active\nresearch topic with a vast literature. Most of the proposed methods follow\neither a content-based or a network-based approach. The former exploits\nuser-generated content while the latter utilizes the connection or interaction\nbetween Twitter users. In this paper, we introduce a novel method combining the\nstrength of both approaches. Concretely, we propose a multi-entry neural\nnetwork architecture named MENET leveraging the advances in deep learning and\nmultiview learning. The generalizability of MENET enables the integration of\nmultiple data representations. In the context of Twitter user geolocation, we\nrealize MENET with textual, network, and metadata features. Considering the\nnatural distribution of Twitter users across the concerned geographical area,\nwe subdivide the surface of the earth into multi-scale cells and train MENET\nwith the labels of the cells. We show that our method outperforms the state of\nthe art by a large margin on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 17:15:41 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Do", "Tien Huu", ""], ["Nguyen", "Duc Minh", ""], ["Tsiligianni", "Evaggelia", ""], ["Cornelis", "Bruno", ""], ["Deligiannis", "Nikos", ""]]}, {"id": "1712.08350", "submitter": "Valentin Zmiycharov", "authors": "Valentin Zmiycharov (1), Dimitar Alexandrov (1), Preslav Nakov (2),\n  Ivan Koychev (1), Yasen Kiprov (1) ((1) Sofia University \"St. Kliment\n  Ohridski\", (2) Qatar Computing Research Institute)", "title": "Finding People's Professions and Nationalities Using Distant Supervision\n  - The FMI@SU \"goosefoot\" team at the WSDM Cup 2017 Triple Scoring Task", "comments": "Triple Scorer at WSDM Cup 2017, see arXiv:1712.08081", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the system that our FMI@SU student's team built for participating\nin the Triple Scoring task at the WSDM Cup 2017. Given a triple from a\n\"type-like\" relation, profession or nationality, the goal is to produce a\nscore, on a scale from 0 to 7, that measures the relevance of the statement\nexpressed by the triple: e.g., how well does the profession of an Actor fit for\nQuentin Tarantino? We propose a distant supervision approach using information\ncrawled from Wikipedia, DeletionPedia, and DBpedia, together with task-specific\nword embeddings, TF-IDF weights, and role occurrence order, which we combine in\na linear regression model. The official evaluation ranked our submission 1st on\nKendall's Tau, 7th on Average score difference, and 9th on Accuracy, out of 21\nparticipating teams.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 09:01:11 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Zmiycharov", "Valentin", ""], ["Alexandrov", "Dimitar", ""], ["Nakov", "Preslav", ""], ["Koychev", "Ivan", ""], ["Kiprov", "Yasen", ""]]}, {"id": "1712.08351", "submitter": "Masahiro Sato", "authors": "Masahiro Sato (Fuji Xerox Co., Ltd.)", "title": "Predicting Triple Scoring with Crowdsourcing-specific Features - The\n  fiddlehead Triple Scorer at WSDM Cup 2017", "comments": "Triple Scorer at WSDM Cup 2017, see arXiv:1712.08081", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Triple Scoring Task at the WSDM Cup 2017 involves the prediction of the\nrelevance scores between persons and professions/nationalities. The ground\ntruth of the relevance scores was obtained by counting the vote of seven\ncrowdworkers. I confirmed that features related to task difficulty correlate\nwith the discrepancy among crowdworkers' judgement. This means such features\nare useful for predicting whether a score is in the middle or not. Hence, the\nfeatures were incorporated into the prediction model of the crowdsourced\nrelevance scores. The introduced features improve the average score difference\nof the prediction. The final ranking of my prediction was 4th for average score\ndifference and 12th for both accuracy and Kendall's tau.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 09:03:06 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Sato", "Masahiro", "", "Fuji Xerox Co., Ltd."]]}, {"id": "1712.08352", "submitter": "Edgard Marx", "authors": "Edgard Marx (1 and 2), Tommaso Soru (1), Andr\\'e Valdestilhas (1) ((1)\n  University of Leipzig, (2) Leipzig University of Applied Sciences)", "title": "Triple Scoring Using a Hybrid Fact Validation Approach - The Catsear\n  Triple Scorer at WSDM Cup 2017", "comments": "Triple Scorer at WSDM Cup 2017, see arXiv:1712.08081", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the continuous increase of data daily published in knowledge bases\nacross the Web, one of the main issues is regarding information relevance. In\nmost knowledge bases, a triple (i.e., a statement composed by subject,\npredicate, and object) can be only true or false. However, triples can be\nassigned a score to have information sorted by relevance. In this work, we\ndescribe the participation of the Catsear team in the Triple Scoring Challenge\nat the WSDM Cup 2017. The Catsear approach scores triples by combining the\nanswers coming from three different sources using a linear regression\nclassifier. We show how our approach achieved an Accuracy2 value of 79.58% and\nthe overall 4th place.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 09:04:55 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Marx", "Edgard", "", "1 and 2"], ["Soru", "Tommaso", ""], ["Valdestilhas", "Andr\u00e9", ""]]}, {"id": "1712.08353", "submitter": "Vibhor Kanojia", "authors": "Vibhor Kanojia, Riku Togashi, Hideyuki Maeda (Yahoo! Japan)", "title": "Relevance Score of Triplets Using Knowledge Graph Embedding - The\n  Pigweed Triple Scorer at WSDM Cup 2017", "comments": "Triple Scorer at WSDM Cup 2017, see arXiv:1712.08081", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Knowledge Bases such as Freebase and Wikidata mention multiple\nprofessions and nationalities for a particular entity. The goal of the WSDM Cup\n2017 Triplet Scoring Challenge was to calculate relevance scores between an\nentity and its professions/nationalities. Such scores are a fundamental\ningredient when ranking results in entity search. This paper proposes a novel\napproach to ensemble an advanced Knowledge Graph Embedding Model with a simple\nbag-of-words model. The former deals with hidden pragmatics and deep semantics\nwhereas the latter handles text-based retrieval and low-level semantics.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 09:05:32 GMT"}], "update_date": "2017-12-28", "authors_parsed": [["Kanojia", "Vibhor", "", "Yahoo! Japan"], ["Togashi", "Riku", "", "Yahoo! Japan"], ["Maeda", "Hideyuki", "", "Yahoo! Japan"]]}, {"id": "1712.08354", "submitter": "Faegheh Hasibi", "authors": "Faegheh Hasibi (1) Dar\\'io Garigliotti (2), Shuo Zhang (2), Krisztian\n  Balog (2) ((1) NTNU Trondheim, (2) University of Stavanger)", "title": "Supervised Ranking of Triples for Type-Like Relations - The Cress Triple\n  Scorer at the WSDM Cup 2017", "comments": "Triple Scorer at WSDM Cup 2017, see arXiv:1712.08081", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our participation in the Triple Scoring task of WSDM Cup\n2017, which aims at ranking triples from a knowledge base for two type-like\nrelations: profession and nationality. We introduce a supervised ranking method\nalong with the features we designed for this task. Our system has been top\nranked with respect to average score difference and 2nd best in terms of\nKendall's tau.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 09:06:04 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Hasibi", "Faegheh", "", "NTNU Trondheim"], ["Garigliotti", "Dar\u00edo", "", "University of Stavanger"], ["Zhang", "Shuo", "", "University of Stavanger"], ["Balog", "Krisztian", "", "University of Stavanger"]]}, {"id": "1712.08355", "submitter": "Frank Dorssers", "authors": "Frank Dorssers (1), Arjen P. de Vries (1), Wouter Alink (2), Roberto\n  Cornacchia (2) ((1) Radboud University, (2) Spinque)", "title": "Ranking Triples using Entity Links in a Large Web Crawl - The Chicory\n  Triple Scorer at WSDM Cup 2017", "comments": "Triple Scorer at WSDM Cup 2017, see arXiv:1712.08081", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the participation of team Chicory in the Triple Ranking\nChallenge of the WSDM Cup 2017. Our approach deploys a large collection of\nentity tagged web data to estimate the correctness of the relevance relation\nexpressed by the triples, in combination with a baseline approach using\nWikipedia abstracts following [1]. Relevance estimations are drawn from\nClueWeb12 annotated by Google's entity linker, available publicly as the FACC1\ndataset. Our implementation is automatically generated from a so-called 'search\nstrategy' that specifies declaratively how the input data are combined into a\nfinal ranking of triples.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 09:07:20 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Dorssers", "Frank", "", "Radboud University"], ["de Vries", "Arjen P.", "", "Radboud University"], ["Alink", "Wouter", "", "Spinque"], ["Cornacchia", "Roberto", "", "Spinque"]]}, {"id": "1712.08356", "submitter": "Quan Wang", "authors": "Boyang Ding, Quan Wang, Bin Wang (Chinese Academy of Sciences)", "title": "Leveraging Text and Knowledge Bases for Triple Scoring: An Ensemble\n  Approach - The BOKCHOY Triple Scorer at WSDM Cup 2017", "comments": "Triple Scorer at WSDM Cup 2017, see arXiv:1712.08081", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our winning solution for the WSDM Cup 2017 triple scoring task. We\ndevise an ensemble of four base scorers, so as to leverage the power of both\ntext and knowledge bases for that task. Then we further refine the outputs of\nthe ensemble by trigger word detection, achieving even better predictive\naccuracy. The code is available at https://github.com/wsdm-cup-2017/bokchoy.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 09:08:34 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Ding", "Boyang", "", "Chinese Academy of Sciences"], ["Wang", "Quan", "", "Chinese Academy of Sciences"], ["Wang", "Bin", "", "Chinese Academy of Sciences"]]}, {"id": "1712.08357", "submitter": "Liang-Wei Chen", "authors": "Liang-Wei Chen, Bhargav Mangipudi, Jayachandu Bandlamudi, Richa\n  Sehgal, Yun Hao, Meng Jiang, Huan Gui (University of Illinois at\n  Urbana-Champaign)", "title": "Integrating Knowledge from Latent and Explicit Features for Triple\n  Scoring - Team Radicchio's Triple Scorer at WSDM Cup 2017", "comments": "Triple Scorer at WSDM Cup 2017, see arXiv:1712.08081", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of the triple scoring task in WSDM Cup 2017 is to compute\nrelevance scores for knowledge-base triples of type-like relations. For\nexample, consider Julius Caesar who has had various professions, including\nPolitician and Author. For two given triples (Julius Caesar, profession,\nPolitician) and (Julius Caesar, profession, Author), the former triple is\nlikely to have a higher relevance score (also called \"triple score\") because\nJulius Caesar was well-known as a politician and not as an author. Accurate\nprediction of such triple scores greatly benefits real-world applications, such\nas information retrieval or knowledge base query. In these scenarios, being\nable to rank all relations (Profession/Nationality) can help improve the user\nexperience. We propose a triple scoring model which integrates knowledge from\nboth latent features and explicit features via an ensemble approach. The latent\nfeatures consist of representations for a person learned by using a word2vec\nmodel and representations for profession/nationality values extracted from a\npre-trained GloVe embedding model. In addition, we extract explicit features\nfor person entities from the Freebase knowledge base. Experimental results show\nthat the proposed method performs competitively at WSDM Cup 2017, ranking at\nthe third place with an accuracy of 79.72% for predicting within two places of\nthe ground truth score.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 09:09:16 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Chen", "Liang-Wei", "", "University of Illinois at\n  Urbana-Champaign"], ["Mangipudi", "Bhargav", "", "University of Illinois at\n  Urbana-Champaign"], ["Bandlamudi", "Jayachandu", "", "University of Illinois at\n  Urbana-Champaign"], ["Sehgal", "Richa", "", "University of Illinois at\n  Urbana-Champaign"], ["Hao", "Yun", "", "University of Illinois at\n  Urbana-Champaign"], ["Jiang", "Meng", "", "University of Illinois at\n  Urbana-Champaign"], ["Gui", "Huan", "", "University of Illinois at\n  Urbana-Champaign"]]}, {"id": "1712.08359", "submitter": "Yael Brumer", "authors": "Yael Brumer (1), Bracha Shapira (1), Lior Rokach (1), Oren Barkan (2)\n  ((1) Ben-Gurion University of the Negev, (2) Tel Aviv University)", "title": "Predicting Relevance Scores for Triples from Type-Like Relations using\n  Neural Embedding - The Cabbage Triple Scorer at WSDM Cup 2017", "comments": "Triple Scorer at WSDM Cup 2017, see arXiv:1712.08081", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The WSDM Cup 2017 Triple scoring challenge is aimed at calculating and\nassigning relevance scores for triples from type-like relations. Such scores\nare a fundamental ingredient for ranking results in entity search. In this\npaper, we propose a method that uses neural embedding techniques to accurately\ncalculate an entity score for a triple based on its nearest neighbor. We strive\nto develop a new latent semantic model with a deep structure that captures the\nsemantic and syntactic relations between words. Our method has been ranked\namong the top performers with accuracy - 0.74, average score difference - 1.74,\nand average Kendall's Tau - 0.35.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 09:09:52 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Brumer", "Yael", "", "Ben-Gurion University of the Negev"], ["Shapira", "Bracha", "", "Ben-Gurion University of the Negev"], ["Rokach", "Lior", "", "Ben-Gurion University of the Negev"], ["Barkan", "Oren", "", "Tel Aviv University"]]}, {"id": "1712.08360", "submitter": "Esraa Ali", "authors": "Esraa Ali, Annalina Caputo, S\\'eamus Lawless (Trinity College Dublin)", "title": "Triple Scoring Using Paragraph Vector - The Gailan Triple Scorer at WSDM\n  Cup 2017", "comments": "Triple Scorer at WSDM Cup 2017, see arXiv:1712.08081", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe our solution to the WSDM Cup 2017 Triple Scoring\ntask. Our approach generates a relevance score based on the textual description\nof the triple's subject and value (Object). It measures how similar (related)\nthe text description of the subject is to the text description of its values.\nThe generated similarity score can then be used to rank the multiple values\nassociated with this subject. We utilize the Paragraph Vector algorithm to\nrepresent the unstructured text into fixed length vectors. The fixed length\nrepresentation is then employed to calculate the similarity (relevance) score\nbetween the subject and its multiple values. Our experimental results have\nshown that the suggested approach is promising and suitable to solve this\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 09:10:15 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Ali", "Esraa", "", "Trinity College Dublin"], ["Caputo", "Annalina", "", "Trinity College Dublin"], ["Lawless", "S\u00e9amus", "", "Trinity College Dublin"]]}, {"id": "1712.08370", "submitter": "Shenglan Liu", "authors": "Lin Feng, Shenlan Liu, Jianing Yao", "title": "Music Genre Classification with Paralleling Recurrent Convolutional\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been demonstrated its effectiveness and efficiency in music\ngenre classification. However, the existing achievements still have several\nshortcomings which impair the performance of this classification task. In this\npaper, we propose a hybrid architecture which consists of the paralleling CNN\nand Bi-RNN blocks. They focus on spatial features and temporal frame orders\nextraction respectively. Then the two outputs are fused into one powerful\nrepresentation of musical signals and fed into softmax function for\nclassification. The paralleling network guarantees the extracting features\nrobust enough to represent music. Moreover, the experiments prove our proposed\narchitecture improve the music genre classification performance and the\nadditional Bi-RNN block is a supplement for CNNs.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 09:49:26 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Feng", "Lin", ""], ["Liu", "Shenlan", ""], ["Yao", "Jianing", ""]]}, {"id": "1712.08437", "submitter": "Alexander Ponomarenko", "authors": "Alexander Ponomarenko, Irina Utkina and Mikhail Batsyn", "title": "A Model of Optimal Network Structure for Decentralized Nearest Neighbor\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the approaches for the nearest neighbor search problem is to build a\nnetwork which nodes correspond to the given set of indexed objects. In this\ncase the search of the closest object can be thought as a search of a node in a\nnetwork. A procedure in a network is called decentralized if it uses only local\ninformation about visited nodes and its neighbors. Networks, which structure\nallows efficient performing the nearest neighbour search by a decentralised\nsearch procedure started from any node, are of particular interest especially\nfor pure distributed systems. Several algorithms that construct such networks\nhave been proposed in literature. However, the following questions arise: \"Are\nthere network models in which decentralised search can be performed faster?\";\n\"What are the optimal networks for the decentralised search?\"; \"What are their\nproperties?\". In this paper we partially give answers to these questions. We\npropose a mathematical programming model for the problem of determining an\noptimal network structure for decentralized nearest neighbor search. We have\nfound an exact solution for a regular lattice of size 4x4 and heuristic\nsolutions for sizes from 5x5 to 7x7. As a distance function we use L1 , L2 and\nL_inf metrics. We hope that our results and the proposed model will initiate\nstudy of optimal network structures for decentralised nearest neighbour search.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 13:34:15 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Ponomarenko", "Alexander", ""], ["Utkina", "Irina", ""], ["Batsyn", "Mikhail", ""]]}, {"id": "1712.08550", "submitter": "Tianxiang Gao", "authors": "Tianxiang Gao, Weiming Bao, Jinning Li, Xiaofeng Gao, Boyuan Kong, Yan\n  Tang, Guihai Chen, Xuan Li", "title": "DancingLines: An Analytical Scheme to Depict Cross-Platform Event\n  Popularity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, events usually burst and are propagated online through multiple\nmodern media like social networks and search engines. There exists various\nresearch discussing the event dissemination trends on individual medium, while\nfew studies focus on event popularity analysis from a cross-platform\nperspective. Challenges come from the vast diversity of events and media,\nlimited access to aligned datasets across different media and a great deal of\nnoise in the datasets. In this paper, we design DancingLines, an innovative\nscheme that captures and quantitatively analyzes event popularity between\npairwise text media. It contains two models: TF-SW, a semantic-aware popularity\nquantification model, based on an integrated weight coefficient leveraging\nWord2Vec and TextRank; and wDTW-CD, a pairwise event popularity time series\nalignment model matching different event phases adapted from Dynamic Time\nWarping. We also propose three metrics to interpret event popularity trends\nbetween pairwise social platforms. Experimental results on eighteen real-world\nevent datasets from an influential social network and a popular search engine\nvalidate the effectiveness and applicability of our scheme. DancingLines is\ndemonstrated to possess broad application potentials for discovering the\nknowledge of various aspects related to events and different media.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 16:24:15 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Gao", "Tianxiang", ""], ["Bao", "Weiming", ""], ["Li", "Jinning", ""], ["Gao", "Xiaofeng", ""], ["Kong", "Boyuan", ""], ["Tang", "Yan", ""], ["Chen", "Guihai", ""], ["Li", "Xuan", ""]]}, {"id": "1712.08673", "submitter": "Nausheen Fatma", "authors": "Nausheen Fatma (1), Manoj K. Chinnakotla (2), Manish Shrivastava (1)\n  ((1) IIIT Hyderabad, (2) Microsoft)", "title": "Relevance Scoring of Triples Using Ordinal Logistic Classification - The\n  Celosia Triple Scorer at WSDM Cup 2017", "comments": "Triple Scorer at WSDM Cup 2017, see arXiv:1712.08081", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report our participation in the Task 2: Triple Scoring of\nWSDM Cup challenge 2017. In this task, we were provided with triples of\n\"type-like\" relations which were given human-annotated relevance scores ranging\nfrom 0 to 7, with 7 being the \"most relevant\" and 0 being the \"least relevant\".\nThe task focuses on two such relations: profession and nationality. We built a\nsystem which could automatically predict the relevance scores for unseen\ntriples. Our model is primarily a supervised machine learning based one in\nwhich we use well-designed features which are used to a make a Logistic Ordinal\nRegression based classification model. The proposed system achieves an overall\naccuracy score of 0.73 and Kendall's tau score of 0.36.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 22:30:05 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Fatma", "Nausheen", "", "IIIT Hyderabad"], ["Chinnakotla", "Manoj K.", "", "Microsoft"], ["Shrivastava", "Manish", "", "IIIT Hyderabad"]]}, {"id": "1712.08674", "submitter": "Prashant Shiralkar", "authors": "Prashant Shiralkar, Mihai Avram, Giovanni Luca Ciampaglia, Filippo\n  Menczer, Alessandro Flammini (Indiana University Bloomington)", "title": "RelSifter: Scoring Triples from Type-like Relations - The Samphire\n  Triple Scorer at WSDM Cup 2017", "comments": "Triple Scorer at WSDM Cup 2017, see arXiv:1712.08081", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RelSifter, a supervised learning approach to the problem of\nassigning relevance scores to triples expressing type-like relations such as\n'profession' and 'nationality.' To provide additional contextual information\nabout individuals and relations we supplement the data provided as part of the\nWSDM 2017 Triple Score contest with Wikidata and DBpedia, two large-scale\nknowledge graphs (KG). Our hypothesis is that any type relation, i.e., a\nspecific profession like 'actor' or 'scientist,' can be described by the set of\ntypical \"activities\" of people known to have that type relation. For example,\nactors are known to star in movies, and scientists are known for their academic\naffiliations. In a KG, this information is to be found on a properly defined\nsubset of the second-degree neighbors of the type relation. This form of local\ninformation can be used as part of a learning algorithm to predict relevance\nscores for new, unseen triples. When scoring 'profession' and 'nationality'\ntriples our experiments based on this approach result in an accuracy equal to\n73% and 78%, respectively. These performance metrics are roughly equivalent or\nonly slightly below the state of the art prior to the present contest. This\nsuggests that our approach can be effective for evaluating facts, despite the\nskewness in the number of facts per individual mined from KGs.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 22:31:37 GMT"}], "update_date": "2017-12-28", "authors_parsed": [["Shiralkar", "Prashant", "", "Indiana University Bloomington"], ["Avram", "Mihai", "", "Indiana University Bloomington"], ["Ciampaglia", "Giovanni Luca", "", "Indiana University Bloomington"], ["Menczer", "Filippo", "", "Indiana University Bloomington"], ["Flammini", "Alessandro", "", "Indiana University Bloomington"]]}, {"id": "1712.08685", "submitter": "Nick Duffield", "authors": "Nesreen K. Ahmed and Nick Duffield and Liangzhen Xia", "title": "Sampling for Approximate Bipartite Network Projection", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.IR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite networks manifest as a stream of edges that represent transactions,\ne.g., purchases by retail customers. Many machine learning applications employ\nneighborhood-based measures to characterize the similarity among the nodes,\nsuch as the pairwise number of common neighbors (CN) and related metrics. While\nthe number of node pairs that share neighbors is potentially enormous, only a\nrelatively small proportion of them have many common neighbors. This motivates\nfinding a weighted sampling approach to preferentially sample these node pairs.\nThis paper presents a new sampling algorithm that provides a fixed size\nunbiased estimate of the similarity matrix resulting from a bipartite graph\nstream projection. The algorithm has two components. First, it maintains a\nreservoir of sampled bipartite edges with sampling weights that favor selection\nof high similarity nodes. Second, arriving edges generate a stream of\n\\textsl{similarity updates} based on their adjacency with the current sample.\nThese updates are aggregated in a second reservoir sample-based stream\naggregator to yield the final unbiased estimate. Experiments on real world\ngraphs show that a 10% sample at each stage yields estimates of high similarity\nedges with weighted relative errors of about 1%.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 23:56:44 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 03:46:41 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Duffield", "Nick", ""], ["Xia", "Liangzhen", ""]]}, {"id": "1712.08865", "submitter": "Koki Tsuyuzaki", "authors": "Koki Tsuyuzaki, Itoshi Nikaido", "title": "Biological Systems as Heterogeneous Information Networks: A Mini-review\n  and Perspectives", "comments": "8 pages, 5 figures, HeteroNAM'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the real world, most objects and data have multiple types of attributes\nand inter-connections. Such data structures are named \"Heterogeneous\nInformation Networks\" (HIN) and have been widely researched. Biological systems\nare also considered to be highly complicated HIN. In this work, we review\nvarious applications of HIN methods to biological and chemical data, discuss\nsome advanced topics, and describe some future research directions.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 02:15:31 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Tsuyuzaki", "Koki", ""], ["Nikaido", "Itoshi", ""]]}, {"id": "1712.08941", "submitter": "Fabio Crestani Prof.", "authors": "Kasturi Dewi Varathan and Anastasia Giachanou and Fabio Crestani", "title": "Comparative Opinion Mining: A Review", "comments": null, "journal-ref": "Journal of the Association for Information Science and Technology,\n  68(4), 2017", "doi": "10.1002/asi.23716", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion mining refers to the use of natural language processing, text\nanalysis and computational linguistics to identify and extract subjective\ninformation in textual material. Opinion mining, also known as sentiment\nanalysis, has received a lot of attention in recent times, as it provides a\nnumber of tools to analyse the public opinion on a number of different topics.\nComparative opinion mining is a subfield of opinion mining that deals with\nidentifying and extracting information that is expressed in a comparative form\n(e.g.~\"paper X is better than the Y\"). Comparative opinion mining plays a very\nimportant role when ones tries to evaluate something, as it provides a\nreference point for the comparison. This paper provides a review of the area of\ncomparative opinion mining. It is the first review that cover specifically this\ntopic as all previous reviews dealt mostly with general opinion mining. This\nsurvey covers comparative opinion mining from two different angles. One from\nperspective of techniques and the other from perspective of comparative opinion\nelements. It also incorporates preprocessing tools as well as dataset that were\nused by the past researchers that can be useful to the future researchers in\nthe field of comparative opinion mining.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 16:16:07 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Varathan", "Kasturi Dewi", ""], ["Giachanou", "Anastasia", ""], ["Crestani", "Fabio", ""]]}, {"id": "1712.09010", "submitter": "Yao Wu", "authors": "Yao Wu, Tianzhen Wu, Ziyi Xiong, Yuncheng Wu, Hong Chen, Cuiping Li,\n  Xiaoying Zhang", "title": "HelPal: A Search System for Mobile Crowd Service", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Proliferation of ubiquitous mobile devices makes location based services\nprevalent. Mobile users are able to volunteer as providers of specific services\nand in the meanwhile to search these services. For example, drivers may be\ninterested in tracking available nearby users who are willing to help with\nmotor repair or are willing to provide travel directions or first aid. With the\ndiffusion of mobile users, it is necessary to provide scalable means of\nenabling such users to connect with other nearby users so that they can help\neach other with specific services. Motivated by these observations, we design\nand implement a general location based system HelPal for mobile users to\nprovide and enjoy instant service, which is called mobile crowd service. In\nthis demo, we introduce a mobile crowd service system featured with several\nnovel techniques. We sketch the system architecture and illustrate scenarios\nvia several cases. Demonstration shows the user-friendly search interface for\nusers to conveniently find skilled and qualified nearby service providers.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 05:22:24 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Wu", "Yao", ""], ["Wu", "Tianzhen", ""], ["Xiong", "Ziyi", ""], ["Wu", "Yuncheng", ""], ["Chen", "Hong", ""], ["Li", "Cuiping", ""], ["Zhang", "Xiaoying", ""]]}, {"id": "1712.09043", "submitter": "Qibing Li", "authors": "Qibing Li, Xiaolin Zheng and Xinyue Wu", "title": "Neural Collaborative Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural networks have yielded state-of-the-art\nperformance on several tasks. Although some recent works have focused on\ncombining deep learning with recommendation, we highlight three issues of\nexisting models. First, these models cannot work on both explicit and implicit\nfeedback, since the network structures are specially designed for one\nparticular case. Second, due to the difficulty on training deep neural\nnetworks, existing explicit models do not fully exploit the expressive\npotential of deep learning. Third, neural network models are easier to overfit\non the implicit setting than shallow models. To tackle these issues, we present\na generic recommender framework called Neural Collaborative Autoencoder (NCAE)\nto perform collaborative filtering, which works well for both explicit feedback\nand implicit feedback. NCAE can effectively capture the subtle hidden\nrelationships between interactions via a non-linear matrix factorization\nprocess. To optimize the deep architecture of NCAE, we develop a three-stage\npre-training mechanism that combines supervised and unsupervised feature\nlearning. Moreover, to prevent overfitting on the implicit setting, we propose\nan error reweighting module and a sparsity-aware data-augmentation strategy.\nExtensive experiments on three real-world datasets demonstrate that NCAE can\nsignificantly advance the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 08:48:43 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 04:39:05 GMT"}, {"version": "v3", "created": "Wed, 19 Dec 2018 06:40:14 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Li", "Qibing", ""], ["Zheng", "Xiaolin", ""], ["Wu", "Xinyue", ""]]}, {"id": "1712.09059", "submitter": "Wei Zhao", "authors": "Wei Zhao, Haixia Chai, Benyou Wang, Jianbo Ye, Min Yang, Zhou Zhao,\n  Xiaojun Chen", "title": "Leveraging Long and Short-term Information in Content-aware Movie\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Movie recommendation systems provide users with ranked lists of movies based\non individual's preferences and constraints. Two types of models are commonly\nused to generate ranking results: long-term models and session-based models.\nWhile long-term models represent the interactions between users and movies that\nare supposed to change slowly across time, session-based models encode the\ninformation of users' interests and changing dynamics of movies' attributes in\nshort terms. In this paper, we propose an LSIC model, leveraging Long and\nShort-term Information in Content-aware movie recommendation using adversarial\ntraining. In the adversarial process, we train a generator as an agent of\nreinforcement learning which recommends the next movie to a user sequentially.\nWe also train a discriminator which attempts to distinguish the generated list\nof movies from the real records. The poster information of movies is integrated\nto further improve the performance of movie recommendation, which is\nspecifically essential when few ratings are available. The experiments\ndemonstrate that the proposed model has robust superiority over competitors and\nsets the state-of-the-art. We will release the source code of this work after\npublication.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 12:08:07 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 04:39:15 GMT"}, {"version": "v3", "created": "Wed, 2 May 2018 14:06:26 GMT"}, {"version": "v4", "created": "Sun, 6 May 2018 16:20:29 GMT"}, {"version": "v5", "created": "Tue, 26 Jun 2018 04:58:20 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Zhao", "Wei", ""], ["Chai", "Haixia", ""], ["Wang", "Benyou", ""], ["Ye", "Jianbo", ""], ["Yang", "Min", ""], ["Zhao", "Zhou", ""], ["Chen", "Xiaojun", ""]]}, {"id": "1712.09123", "submitter": "Shameem Puthiya Parambath Mr.", "authors": "Shameem A Puthiya Parambath, Nishant Vijayakumar, Sanjay Chawla", "title": "SAGA: A Submodular Greedy Algorithm For Group Recommendation", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a unified framework and an algorithm for the\nproblem of group recommendation where a fixed number of items or alternatives\ncan be recommended to a group of users. The problem of group recommendation\narises naturally in many real world contexts, and is closely related to the\nbudgeted social choice problem studied in economics. We frame the group\nrecommendation problem as choosing a subgraph with the largest group consensus\nscore in a completely connected graph defined over the item affinity matrix. We\npropose a fast greedy algorithm with strong theoretical guarantees, and show\nthat the proposed algorithm compares favorably to the state-of-the-art group\nrecommendation algorithms according to commonly used relevance and coverage\nperformance measures on benchmark dataset.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 20:03:46 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Parambath", "Shameem A Puthiya", ""], ["Vijayakumar", "Nishant", ""], ["Chawla", "Sanjay", ""]]}, {"id": "1712.09183", "submitter": "Yen Hao Huang", "authors": "Yen-Hao Huang, Lin-Hung Wei, Yi-Shin Chen", "title": "Detection of the Prodromal Phase of Bipolar Disorder from Psychological\n  and Phonological Aspects in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seven out of ten people with bipolar disorder are initially misdiagnosed and\nthirty percent of individuals with bipolar disorder will commit suicide.\nIdentifying the early phases of the disorder is one of the key components for\nreducing the full development of the disorder. In this study, we aim at\nleveraging the data from social media to design predictive models, which\nutilize the psychological and phonological features, to determine the onset\nperiod of bipolar disorder and provide insights on its prodrome. This study\nmakes these discoveries possible by employing a novel data collection process,\ncoined as Time-specific Subconscious Crowdsourcing, which helps collect a\nreliable dataset that supplements diagnosis information from people suffering\nfrom bipolar disorder. Our experimental results demonstrate that the proposed\nmodels could greatly contribute to the regular assessments of people with\nbipolar disorder, which is important in the primary care setting.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 05:56:29 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Huang", "Yen-Hao", ""], ["Wei", "Lin-Hung", ""], ["Chen", "Yi-Shin", ""]]}, {"id": "1712.09528", "submitter": "Stefan Heindorf", "authors": "Martin Potthast (1), Stefan Heindorf (2), Hannah Bast (3) ((1) Leipzig\n  University, (2) Paderborn University, (3) University of Freiburg)", "title": "Proceedings of the WSDM Cup 2017: Vandalism Detection and Triple Scoring", "comments": "Proceedings of the WSDM Cup 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The WSDM Cup 2017 was a data mining challenge held in conjunction with the\n10th International Conference on Web Search and Data Mining (WSDM). It\naddressed key challenges of knowledge bases today: quality assurance and entity\nsearch. For quality assurance, we tackle the task of vandalism detection, based\non a dataset of more than 82 million user-contributed revisions of the Wikidata\nknowledge base, all of which annotated with regard to whether or not they are\nvandalism. For entity search, we tackle the task of triple scoring, using a\ndataset that comprises relevance scores for triples from type-like relations\nincluding occupation and country of citizenship, based on about 10,000 human\nrelevance judgements. For reproducibility sake, participants were asked to\nsubmit their software on TIRA, a cloud-based evaluation platform, and they were\nincentivized to share their approaches open source.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 08:49:08 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Potthast", "Martin", ""], ["Heindorf", "Stefan", ""], ["Bast", "Hannah", ""]]}, {"id": "1712.09550", "submitter": "Jean-Michel Renders", "authors": "Jean-Michel Renders (NaverLabs Europe)", "title": "Active Search for High Recall: a Non-Stationary Extension of Thompson\n  Sampling", "comments": "12 pages, 0 figures. Long version (with full details and appendices)\n  of the short paper accepted for ECIR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Active Search, where a maximum of relevant objects\n- ideally all relevant objects - should be retrieved with the minimum effort or\nminimum time. Typically, there are two main challenges to face when tackling\nthis problem: first, the class of relevant objects has often low prevalence\nand, secondly, this class can be multi-faceted or multi-modal: objects could be\nrelevant for completely different reasons. To solve this problem and its\nassociated issues, we propose an approach based on a non-stationary (aka\nrestless) extension of Thompson Sampling, a well-known strategy for Multi-Armed\nBandits problems. The collection is first soft-clustered into a finite set of\ncomponents and a posterior distribution of getting a relevant object inside\neach cluster is updated after receiving the user feedback about the proposed\ninstances. The \"next instance\" selection strategy is a mixed, two-level\ndecision process, where both the soft clusters and their instances are\nconsidered. This method can be considered as an insurance, where the cost of\nthe insurance is an extra exploration effort in the short run, for achieving a\nnearly \"total\" recall with less efforts in the long run.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 11:21:35 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 18:54:36 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Renders", "Jean-Michel", "", "NaverLabs Europe"]]}, {"id": "1712.10110", "submitter": "Su Yan", "authors": "Su Yan, Wei Lin, Tianshu Wu, Daorui Xiao, Xu Zheng, Bo Wu, Kaipeng Liu", "title": "Beyond Keywords and Relevance: A Personalized Ad Retrieval Framework in\n  E-Commerce Sponsored Search", "comments": null, "journal-ref": "Proceedings of the 2018 World Wide Web Conference Pages 1919-1928", "doi": "10.1145/3178876.3186172", "report-no": null, "categories": "cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  On most sponsored search platforms, advertisers bid on some keywords for\ntheir advertisements (ads). Given a search request, ad retrieval module\nrewrites the query into bidding keywords, and uses these keywords as keys to\nselect Top N ads through inverted indexes. In this way, an ad will not be\nretrieved even if queries are related when the advertiser does not bid on\ncorresponding keywords. Moreover, most ad retrieval approaches regard rewriting\nand ad-selecting as two separated tasks, and focus on boosting relevance\nbetween search queries and ads. Recently, in e-commerce sponsored search more\nand more personalized information has been introduced, such as user profiles,\nlong-time and real-time clicks. Personalized information makes ad retrieval\nable to employ more elements (e.g. real-time clicks) as search signals and\nretrieval keys, however it makes ad retrieval more difficult to measure ads\nretrieved through different signals. To address these problems, we propose a\nnovel ad retrieval framework beyond keywords and relevance in e-commerce\nsponsored search. Firstly, we employ historical ad click data to initialize a\nhierarchical network representing signals, keys and ads, in which personalized\ninformation is introduced. Then we train a model on top of the hierarchical\nnetwork by learning the weights of edges. Finally we select the best edges\naccording to the model, boosting RPM/CTR. Experimental results on our\ne-commerce platform demonstrate that our ad retrieval framework achieves good\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 03:48:25 GMT"}, {"version": "v2", "created": "Mon, 1 Jan 2018 13:10:17 GMT"}, {"version": "v3", "created": "Thu, 8 Mar 2018 08:10:56 GMT"}, {"version": "v4", "created": "Mon, 9 Apr 2018 02:56:34 GMT"}, {"version": "v5", "created": "Mon, 23 Apr 2018 18:32:58 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Yan", "Su", ""], ["Lin", "Wei", ""], ["Wu", "Tianshu", ""], ["Xiao", "Daorui", ""], ["Zheng", "Xu", ""], ["Wu", "Bo", ""], ["Liu", "Kaipeng", ""]]}, {"id": "1712.10190", "submitter": "Victor Thompson Vt", "authors": "Victor Thompson", "title": "Detecting Cross-Lingual Plagiarism Using Simulated Word Embeddings", "comments": "This is a mildly edited version that is currently undergoing review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual plagiarism (CLP) occurs when texts written in one language are\ntranslated into a different language and used without acknowledging the\noriginal sources. One of the most common methods for detecting CLP requires\nonline machine translators (such as Google or Microsoft translate) which are\nnot always available, and given that plagiarism detection typically involves\nlarge document comparison, the amount of translations required would overwhelm\nan online machine translator, especially when detecting plagiarism over the\nweb. In addition, when translated texts are replaced with their synonyms, using\nonline machine translators to detect CLP would result in poor performance. This\npaper addresses the problem of cross-lingual plagiarism detection (CLPD) by\nproposing a model that uses simulated word embeddings to reproduce the\npredictions of an online machine translator (Google translate) when detecting\nCLP. The simulated embeddings comprise of translated words in different\nlanguages mapped in a common space, and replicated to increase the prediction\nprobability of retrieving the translations of a word (and their synonyms) from\nthe model. Unlike most existing models, the proposed model does not require\nparallel corpora, and accommodates multiple languages (multi-lingual). We\ndemonstrated the effectiveness of the proposed model in detecting CLP in\nstandard datasets that contain CLP cases, and evaluated its performance against\na state-of-the-art baseline that relies on online machine translator (T+MA\nmodel). Evaluation results revealed that the proposed model is not only\neffective in detecting CLP, it outperformed the baseline. The results indicate\nthat CLP could be detected with state-of-the-art performances by leveraging the\nprediction accuracy of an internet translator with word embeddings, without\nrelying on internet translators.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 11:46:13 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 14:59:22 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Thompson", "Victor", ""]]}, {"id": "1712.10309", "submitter": "Victor Thompson Vt", "authors": "Victor Thompson", "title": "Methods for Detecting Paraphrase Plagiarism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paraphrase plagiarism is one of the difficult challenges facing plagiarism\ndetection systems. Paraphrasing occur when texts are lexically or syntactically\naltered to look different, but retain their original meaning. Most plagiarism\ndetection systems (many of which are commercial based) are designed to detect\nword co-occurrences and light modifications, but are unable to detect severe\nsemantic and structural alterations such as what is seen in many academic\ndocuments. Hence many paraphrase plagiarism cases go undetected. In this paper,\nwe approached the problem of paraphrase plagiarism by proposing methods for\ndetecting the most common techniques (phenomena) used in paraphrasing texts\n(namely; lexical substitution, insertion/deletion and word and phrase\nreordering), and combined the methods into a paraphrase detection model. We\nevaluated our proposed methods and model on collections containing paraphrase\ntexts. Experimental results show significant improvement in performance when\nthe methods were combined (the proposed model) as opposed to running them\nindividually. The results also show that the proposed paraphrase detection\nmodel outperformed a standard baseline (based on greedy string tilling), and\nprevious studies.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 18:53:12 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Thompson", "Victor", ""]]}]