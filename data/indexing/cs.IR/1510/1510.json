[{"id": "1510.00585", "submitter": "Bidyut Kr. Patra", "authors": "Ranveer Singh, Bidyut Kr. Patra and Bibhas Adhikari", "title": "A Complex Network Approach for Collaborative Recommendation", "comments": "22 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) is the most widely used and successful approach\nfor personalized service recommendations. Among the collaborative\nrecommendation approaches, neighborhood based approaches enjoy a huge amount of\npopularity, due to their simplicity, justifiability, efficiency and stability.\nNeighborhood based collaborative filtering approach finds K nearest neighbors\nto an active user or K most similar rated items to the target item for\nrecommendation. Traditional similarity measures use ratings of co-rated items\nto find similarity between a pair of users. Therefore, traditional similarity\nmeasures cannot compute effective neighbors in sparse dataset. In this paper,\nwe propose a two-phase approach, which generates user-user and item-item\nnetworks using traditional similarity measures in the first phase. In the\nsecond phase, two hybrid approaches HB1, HB2, which utilize structural\nsimilarity of both the network for finding K nearest neighbors and K most\nsimilar items to a target items are introduced. To show effectiveness of the\nmeasures, we compared performances of neighborhood based CFs using\nstate-of-the-art similarity measures with our proposed structural similarity\nmeasures based CFs. Recommendation results on a set of real data show that\nproposed measures based CFs outperform existing measures based CFs in various\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2015 13:05:42 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Singh", "Ranveer", ""], ["Patra", "Bidyut Kr.", ""], ["Adhikari", "Bibhas", ""]]}, {"id": "1510.00819", "submitter": "Jai Manral", "authors": "Jai Manral", "title": "Intelligent Search Optimization using Artificial Fuzzy Logics", "comments": "Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information on the web is prodigious; searching relevant information is\ndifficult making web users to rely on search engines for finding relevant\ninformation on the web. Search engines index and categorize web pages according\nto their contents using crawlers and rank them accordingly. For given user\nquery they retrieve millions of webpages and display them to users according to\nweb-page rank. Every search engine has their own algorithms based on certain\nparameters for ranking web-pages. Search Engine Optimization (SEO) is that\ntechnique by which webmasters try to improve ranking of their websites by\noptimizing it according to search engines ranking parameters. It is the aim of\nthis research to identify the most popular SEO techniques used by search\nengines for ranking web-pages and to establish their importance for indexing\nand categorizing web data. The research tries to establish that using more SEO\nparameters in ranking algorithms helps in retrieving better search results thus\nincreasing user satisfaction.\n  In the accomplished research, a web based Meta search engine is proposed to\naggregates search results from different search engines and rank web-pages\nbased on new page ranking algorithm which will assign heuristic page rank to\nweb-pages based on SEO parameters such as title tag, Meta description, sitemap\netc. The research also provides insight into techniques which webmasters can\nuse for better ranking their websites in Google and Bing.\n  Initial results has shown that using certain SEO parameters in present\nranking algorithm helps in retrieving more useful results for user queries.\nThese results generated from Meta search engine outperformed existing search\nengines in terms of better retrieved search results and high precision.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2015 13:28:50 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Manral", "Jai", ""]]}, {"id": "1510.01006", "submitter": "Rion Brattig Correia", "authors": "Rion Brattig Correia and Lang Li and Luis M. Rocha", "title": "Monitoring Potential Drug Interactions and Reactions via Network\n  Analysis of Instagram User Timelines", "comments": "Pacific Symposium on Biocomputing. 21:492-503", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.IR q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much recent research aims to identify evidence for Drug-Drug Interactions\n(DDI) and Adverse Drug reactions (ADR) from the biomedical scientific\nliterature. In addition to this \"Bibliome\", the universe of social media\nprovides a very promising source of large-scale data that can help identify DDI\nand ADR in ways that have not been hitherto possible. Given the large number of\nusers, analysis of social media data may be useful to identify under-reported,\npopulation-level pathology associated with DDI, thus further contributing to\nimprovements in population health. Moreover, tapping into this data allows us\nto infer drug interactions with natural products--including cannabis--which\nconstitute an array of DDI very poorly explored by biomedical research thus\nfar. Our goal is to determine the potential of Instagram for public health\nmonitoring and surveillance for DDI, ADR, and behavioral pathology at large.\nUsing drug, symptom, and natural product dictionaries for identification of the\nvarious types of DDI and ADR evidence, we have collected ~7000 timelines. We\nreport on 1) the development of a monitoring tool to easily observe user-level\ntimelines associated with drug and symptom terms of interest, and 2)\npopulation-level behavior via the analysis of co-occurrence networks computed\nfrom user timelines at three different scales: monthly, weekly, and daily\noccurrences. Analysis of these networks further reveals 3) drug and symptom\ndirect and indirect associations with greater support in user timelines, as\nwell as 4) clusters of symptoms and drugs revealed by the collective behavior\nof the observed population. This demonstrates that Instagram contains much\ndrug- and pathology specific data for public health monitoring of DDI and ADR,\nand that complex network analysis provides an important toolbox to extract\nhealth-related associations and their support from large-scale social media\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 00:42:18 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2016 19:16:52 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Correia", "Rion Brattig", ""], ["Li", "Lang", ""], ["Rocha", "Luis M.", ""]]}, {"id": "1510.01562", "submitter": "Benjamin Piwowarski", "authors": "Benjamin Piwowarski and Sylvain Lamprier and Nicolas Despres", "title": "Parameterized Neural Network Language Models for Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Retrieval (IR) models need to deal with two difficult issues,\nvocabulary mismatch and term dependencies. Vocabulary mismatch corresponds to\nthe difficulty of retrieving relevant documents that do not contain exact query\nterms but semantically related terms. Term dependencies refers to the need of\nconsidering the relationship between the words of the query when estimating the\nrelevance of a document. A multitude of solutions has been proposed to solve\neach of these two problems, but no principled model solve both. In parallel, in\nthe last few years, language models based on neural networks have been used to\ncope with complex natural language processing tasks like emotion and paraphrase\ndetection. Although they present good abilities to cope with both term\ndependencies and vocabulary mismatch problems, thanks to the distributed\nrepresentation of words they are based upon, such models could not be used\nreadily in IR, where the estimation of one language model per document (or\nquery) is required. This is both computationally unfeasible and prone to\nover-fitting. Based on a recent work that proposed to learn a generic language\nmodel that can be modified through a set of document-specific parameters, we\nexplore use of new neural network models that are adapted to ad-hoc IR tasks.\nWithin the language model IR framework, we propose and study the use of a\ngeneric language model as well as a document-specific language model. Both can\nbe used as a smoothing component, but the latter is more adapted to the\ndocument at hand and has the potential of being used as a full document\nlanguage model. We experiment with such models and analyze their results on\nTREC-1 to 8 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 13:07:31 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Piwowarski", "Benjamin", ""], ["Lamprier", "Sylvain", ""], ["Despres", "Nicolas", ""]]}, {"id": "1510.01784", "submitter": "Ruining He", "authors": "Ruining He, Julian McAuley", "title": "VBPR: Visual Bayesian Personalized Ranking from Implicit Feedback", "comments": "AAAI'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern recommender systems model people and items by discovering or `teasing\napart' the underlying dimensions that encode the properties of items and users'\npreferences toward them. Critically, such dimensions are uncovered based on\nuser feedback, often in implicit form (such as purchase histories, browsing\nlogs, etc.); in addition, some recommender systems make use of side\ninformation, such as product attributes, temporal information, or review text.\nHowever one important feature that is typically ignored by existing\npersonalized recommendation and ranking methods is the visual appearance of the\nitems being considered. In this paper we propose a scalable factorization model\nto incorporate visual signals into predictors of people's opinions, which we\napply to a selection of large, real-world datasets. We make use of visual\nfeatures extracted from product images using (pre-trained) deep networks, on\ntop of which we learn an additional layer that uncovers the visual dimensions\nthat best explain the variation in people's feedback. This not only leads to\nsignificantly more accurate personalized ranking methods, but also helps to\nalleviate cold start issues, and qualitatively to analyze the visual dimensions\nthat influence people's opinions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2015 23:46:15 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["He", "Ruining", ""], ["McAuley", "Julian", ""]]}, {"id": "1510.01991", "submitter": "Ji Wan", "authors": "Ji Wan, Sheng Tang, Yongdong Zhang, Jintao Li, Pengcheng Wu, Steven\n  C.H. Hoi", "title": "HDIdx: High-Dimensional Indexing for Efficient Approximate Nearest\n  Neighbor Search", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2015.11.104", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast Nearest Neighbor (NN) search is a fundamental challenge in large-scale\ndata processing and analytics, particularly for analyzing multimedia contents\nwhich are often of high dimensionality. Instead of using exact NN search,\nextensive research efforts have been focusing on approximate NN search\nalgorithms. In this work, we present \"HDIdx\", an efficient high-dimensional\nindexing library for fast approximate NN search, which is open-source and\nwritten in Python. It offers a family of state-of-the-art algorithms that\nconvert input high-dimensional vectors into compact binary codes, making them\nvery efficient and scalable for NN search with very low space complexity.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 15:39:39 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Wan", "Ji", ""], ["Tang", "Sheng", ""], ["Zhang", "Yongdong", ""], ["Li", "Jintao", ""], ["Wu", "Pengcheng", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1510.02177", "submitter": "Khan Muhammad", "authors": "Khan Muhammad, Irfan Mehmood, Mi Young Lee, Su Mi Ji, Sung Wook Baik", "title": "Ontology-based Secure Retrieval of Semantically Significant Visual\n  Contents", "comments": "A short paper of 11 pages for secure visual contents retrieval.The\n  original version can be accessed at this link:\n  http://www.kingpc.or.kr/inc_html/index.html", "journal-ref": "Khan Muhammad, Irfan Mehmood, Mi Young Lee, Su Mi Ji, Sung Wook\n  Baik, \"Ontology-based Secure Retrieval of Semantically Significant Visual\n  Contents,\" JOURNAL OF KOREAN INSTITUTE OF NEXT GENERATION COMPUTING, vol. 11,\n  pp. 87-96, 2015", "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification is an enthusiastic research field where large amount of\nimage data is classified into various classes based on their visual contents.\nResearchers have presented various low-level features-based techniques for\nclassifying images into different categories. However, efficient and effective\nclassification and retrieval is still a challenging problem due to complex\nnature of visual contents. In addition, the traditional information retrieval\ntechniques are vulnerable to security risks, making it easy for attackers to\nretrieve personal visual contents such as patients records and law enforcement\nagencies databases. Therefore, we propose a novel ontology-based framework\nusing image steganography for secure image classification and information\nretrieval. The proposed framework uses domain-specific ontology for mapping the\nlow-level image features to high-level concepts of ontologies which\nconsequently results in efficient classification. Furthermore, the proposed\nmethod utilizes image steganography for hiding the image semantics as a secret\nmessage inside them, making the information retrieval process secure from third\nparties. The proposed framework minimizes the computational complexity of\ntraditional techniques, increasing its suitability for secure and real-time\nvisual contents retrieval from personalized image databases. Experimental\nresults confirm the efficiency, effectiveness, and security of the proposed\nframework as compared with other state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 01:09:32 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Muhammad", "Khan", ""], ["Mehmood", "Irfan", ""], ["Lee", "Mi Young", ""], ["Ji", "Su Mi", ""], ["Baik", "Sung Wook", ""]]}, {"id": "1510.02348", "submitter": "Jian Gao", "authors": "Ling-Jiao Chen, Zi-Ke Zhang, Jin-Hu Liu, Jian Gao, Tao Zhou", "title": "A vertex similarity index for better personalized recommendation", "comments": "11 pages, 3 figures, 2 tables in Physica A, 2016", "journal-ref": "Physica A: Statistical Mechanics and its Applications 466 (2017):\n  607-615", "doi": "10.1016/j.physa.2016.09.057", "report-no": null, "categories": "cs.IR physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems benefit us in tackling the problem of information\noverload by predicting our potential choices among diverse niche objects. So\nfar, a variety of personalized recommendation algorithms have been proposed and\nmost of them are based on similarities, such as collaborative filtering and\nmass diffusion. Here, we propose a novel vertex similarity index named CosRA,\nwhich combines advantages of both the cosine index and the resource-allocation\n(RA) index. By applying the CosRA index to real recommender systems including\nMovieLens, Netflix and RYM, we show that the CosRA-based method has better\nperformance in accuracy, diversity and novelty than some benchmark methods.\nMoreover, the CosRA index is free of parameters, which is a significant\nadvantage in real applications. Further experiments show that the introduction\nof two turnable parameters cannot remarkably improve the overall performance of\nthe CosRA index.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 14:50:50 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 14:18:47 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Chen", "Ling-Jiao", ""], ["Zhang", "Zi-Ke", ""], ["Liu", "Jin-Hu", ""], ["Gao", "Jian", ""], ["Zhou", "Tao", ""]]}, {"id": "1510.02755", "submitter": "Koushiki Sarkar", "authors": "Koushiki Sarkar, Ritwika Law", "title": "A Novel Approach to Document Classification using WordNet", "comments": "(Working Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content based Document Classification is one of the biggest challenges in the\ncontext of free text mining. Current algorithms on document classifications\nmostly rely on cluster analysis based on bag-of-words approach. However that\nmethod is still being applied to many modern scientific dilemmas. It has\nestablished a strong presence in fields like economics and social science to\nmerit serious attention from the researchers. In this paper we would like to\npropose and explore an alternative grounded more securely on the dictionary\nclassification and correlatedness of words and phrases. It is expected that\napplication of our existing knowledge about the underlying classification\nstructure may lead to improvement of the classifier's performance.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2015 09:24:27 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2015 17:56:52 GMT"}], "update_date": "2015-12-15", "authors_parsed": [["Sarkar", "Koushiki", ""], ["Law", "Ritwika", ""]]}, {"id": "1510.03004", "submitter": "Elizeu Santos-Neto", "authors": "Elizeu Santos-Neto, Flavio Figueiredo, Nigini Oliveira, Nazareno\n  Andrade, Jussara Almeida, Matei Ripeanu", "title": "Assessing the Value of Peer-Produced Information for Exploratory Search", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tagging is a popular feature that supports several collaborative tasks,\nincluding search, as tags produced by one user can help others finding relevant\ncontent. However, task performance depends on the existence of 'good' tags. A\nfirst step towards creating incentives for users to produce 'good' tags is the\nquantification of their value in the first place. This work fills this gap by\ncombining qualitative and quantitative research methods. In particular, using\ncontextual interviews, we first determine aspects that influence users'\nperception of tags' value for exploratory search. Next, we formalize some of\nthe identified aspects and propose an information-theoretical method with\nprovable properties that quantifies the two most important aspects (according\nto the qualitative analysis) that influence the perception of tag value: the\nability of a tag to reduce the search space while retrieving relevant items to\nthe user. The evaluation on real data shows that our method is accurate: tags\nthat users consider more important have higher value than tags users have not\nexpressed interest.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 02:58:49 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Santos-Neto", "Elizeu", ""], ["Figueiredo", "Flavio", ""], ["Oliveira", "Nigini", ""], ["Andrade", "Nazareno", ""], ["Almeida", "Jussara", ""], ["Ripeanu", "Matei", ""]]}, {"id": "1510.03025", "submitter": "Abhay Prakash", "authors": "Abhay Prakash", "title": "Mining Interesting Trivia for Entities from Wikipedia", "comments": "Part of the work presented in this dissertation has been published at\n  IJCAI 2015 as following: \"Abhay Prakash, Manoj K. Chinnakotla, Dhaval Patel\n  and Puneet Garg. Did you know?- Mining Interesting Trivia for Entities from\n  Wikipedia, In 24th International Joint Conference on Artificial Intelligence\n  (IJCAI, 2015).\" (Paper Link:\n  http://ijcai.org/papers15/Papers/IJCAI15-446.pdf)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trivia is any fact about an entity, which is interesting due to any of the\nfollowing characteristics - unusualness, uniqueness, unexpectedness or\nweirdness. Such interesting facts are provided in 'Did You Know?' section at\nmany places. Although trivia are facts of little importance to be known, but we\nhave presented their usage in user engagement purpose. Such fun facts generally\nspark intrigue and draws user to engage more with the entity, thereby promoting\nrepeated engagement. The thesis has cited some case studies, which show the\nsignificant impact of using trivia for increasing user engagement or for wide\npublicity of the product/service.\n  In this thesis, we propose a novel approach for mining entity trivia from\ntheir Wikipedia pages. Given an entity, our system extracts relevant sentences\nfrom its Wikipedia page and produces a list of sentences ranked based on their\ninterestingness as trivia. At the heart of our system lies an interestingness\nranker which learns the notion of interestingness, through a rich set of\ndomain-independent linguistic and entity based features. Our ranking model is\ntrained by leveraging existing user-generated trivia data available on the Web\ninstead of creating new labeled data for movie domain. For other domains like\nsports, celebrities, countries etc. labeled data would have to be created as\ndescribed in thesis. We evaluated our system on movies domain and celebrity\ndomain, and observed that the system performs significantly better than the\ndefined baselines. A thorough qualitative analysis of the results revealed that\nour engineered rich set of features indeed help in surfacing interesting trivia\nin the top ranks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 09:05:23 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Prakash", "Abhay", ""]]}, {"id": "1510.03051", "submitter": "Ashish Anand", "authors": "Gargi Priyadarshini and Ashish Anand", "title": "Inferring disease correlation from healthcare data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic Health Records maintained in health care settings are a potential\nsource of substantial clinical knowledge. The massive volume of data,\nunstructured nature of records and obligatory requirement of domain\nacquaintance together pose a challenge in knowledge extraction from it. The aim\nof this study is to overcome this challenge with a methodical analysis,\nabstraction and summarization of such data. This is an attempt to explain\nclinical observations through bio-medical and genomic data. Discharge summaries\nof obesity patients were processed to extract coherent patterns. This was\nsupported by Machine Learning and Natural Language Processing based\ntechnologies and concept mapping tool along with biomedical, clinical and\ngenomic knowledge bases. Semantic relations between diseases were extracted and\nfiltered through Chi square test to remove spurious relations. The remaining\nrelations were validated against biomedical literature and gene interaction\nnetworks. A collection of binary relations of diseases was derived from the\ndata. One set implied co-morbidity while the other set contained diseases which\nare risk factors of others. Validation against bio-medical literature increased\nthe prospect of correlation between diseases. Gene interaction network revealed\nthat the diseases are related and their corresponding genes are in close\nproximity. Conclusion: This study focuses on deducing meaningful relations\nbetween diseases from discharge summaries. For analytical purpose, the scope\nhas been limited to a few common, well-researched diseases. It can be extended\nto incorporate relatively unknown, complex diseases and discover new traits to\nhelp in clinical assessments.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 13:36:22 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Priyadarshini", "Gargi", ""], ["Anand", "Ashish", ""]]}, {"id": "1510.03299", "submitter": "Qian Yu", "authors": "Peng Zhang, Qian Yu, Yuexian Hou, Dawei Song, Jingfei Li, Bin Hu", "title": "Further Theoretical Study of Distribution Separation Method for\n  Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a Distribution Separation Method (DSM) is proposed for relevant\nfeedback in information retrieval, which aims to approximate the true relevance\ndistribution by separating a seed irrelevance distribution from the mixture\none. While DSM achieved a promising empirical performance, theoretical analysis\nof DSM is still need further study and comparison with other relative retrieval\nmodel. In this article, we first generalize DSM's theoretical property, by\nproving that its minimum correlation assumption is equivalent to the maximum\n(original and symmetrized) KL-Divergence assumption. Second, we also\nanalytically show that the EM algorithm in a well-known Mixture Model is\nessentially a distribution separation process and can be simplified using the\nlinear separation algorithm in DSM. Some empirical results are also presented\nto support our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2015 14:27:18 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2015 09:48:27 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Zhang", "Peng", ""], ["Yu", "Qian", ""], ["Hou", "Yuexian", ""], ["Song", "Dawei", ""], ["Li", "Jingfei", ""], ["Hu", "Bin", ""]]}, {"id": "1510.04029", "submitter": "Nadine Kroher", "authors": "Nadine Kroher, Jos\\'e-Miguel D\\'iaz-B\\'a\\~nez, Joaquin Mora, Emilia\n  G\\'omez", "title": "Corpus COFLA: A research corpus for the Computational study of Flamenco\n  Music", "comments": "24 pages, submitted to the ACM Journal of Computing and Cultural\n  Heritage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flamenco is a music tradition from Southern Spain which attracts a growing\ncommunity of enthusiasts around the world. Its unique melodic and rhythmic\nelements, the typically spontaneous and improvised interpretation and its\ndiversity regarding styles make this still largely undocumented art form a\nparticularly interesting material for musicological studies. In prior works it\nhas already been demonstrated that research on computational analysis of\nflamenco music, despite it being a relatively new field, can provide powerful\ntools for the discovery and diffusion of this genre. In this paper we present\ncorpusCOFLA, a data framework for the development of such computational tools.\nThe proposed collection of audio recordings and meta-data serves as a pool for\ncreating annotated subsets which can be used in development and evaluation of\nalgorithms for specific music information retrieval tasks. First, we describe\nthe design criteria for the corpus creation and then provide various examples\nof subsets drawn from the corpus. We showcase possible research applications in\nthe context of computational study of flamenco music and give perspectives\nregarding further development of the corpus.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 10:14:29 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Kroher", "Nadine", ""], ["D\u00edaz-B\u00e1\u00f1ez", "Jos\u00e9-Miguel", ""], ["Mora", "Joaquin", ""], ["G\u00f3mez", "Emilia", ""]]}, {"id": "1510.04039", "submitter": "Nadine Kroher", "authors": "Nadine Kroher, Emilia G\\'omez", "title": "Automatic Transcription of Flamenco Singing from Polyphonic Music\n  Recordings", "comments": "Submitted to the IEEE Transactions on Audio, Speech and Language\n  Processing", "journal-ref": null, "doi": "10.1109/TASLP.2016.2531284", "report-no": null, "categories": "cs.SD cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic note-level transcription is considered one of the most challenging\ntasks in music information retrieval. The specific case of flamenco singing\ntranscription poses a particular challenge due to its complex melodic\nprogressions, intonation inaccuracies, the use of a high degree of\nornamentation and the presence of guitar accompaniment. In this study, we\nexplore the limitations of existing state of the art transcription systems for\nthe case of flamenco singing and propose a specific solution for this genre: We\nfirst extract the predominant melody and apply a novel contour filtering\nprocess to eliminate segments of the pitch contour which originate from the\nguitar accompaniment. We formulate a set of onset detection functions based on\nvolume and pitch characteristics to segment the resulting vocal pitch contour\ninto discrete note events. A quantised pitch label is assigned to each note\nevent by combining global pitch class probabilities with local pitch contour\nstatistics. The proposed system outperforms state of the art singing\ntranscription systems with respect to voicing accuracy, onset detection and\noverall performance when evaluated on flamenco singing datasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 10:53:00 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Kroher", "Nadine", ""], ["G\u00f3mez", "Emilia", ""]]}, {"id": "1510.04389", "submitter": "Yusuke Matsui", "authors": "Yusuke Matsui, Kota Ito, Yuji Aramaki, Toshihiko Yamasaki, Kiyoharu\n  Aizawa", "title": "Sketch-based Manga Retrieval using Manga109 Dataset", "comments": "13 pages", "journal-ref": "Multimedia Tools and Applications, Volume 76, Issue 20, 2017", "doi": "10.1007/s11042-016-4020-z", "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manga (Japanese comics) are popular worldwide. However, current e-manga\narchives offer very limited search support, including keyword-based search by\ntitle or author, or tag-based categorization. To make the manga search\nexperience more intuitive, efficient, and enjoyable, we propose a content-based\nmanga retrieval system. First, we propose a manga-specific image-describing\nframework. It consists of efficient margin labeling, edge orientation histogram\nfeature description, and approximate nearest-neighbor search using product\nquantization. Second, we propose a sketch-based interface as a natural way to\ninteract with manga content. The interface provides sketch-based querying,\nrelevance feedback, and query retouch. For evaluation, we built a novel dataset\nof manga images, Manga109, which consists of 109 comic books of 21,142 pages\ndrawn by professional manga artists. To the best of our knowledge, Manga109 is\ncurrently the biggest dataset of manga images available for research. We\nconducted a comparative study, a localization evaluation, and a large-scale\nqualitative study. From the experiments, we verified that: (1) the retrieval\naccuracy of the proposed method is higher than those of previous methods; (2)\nthe proposed method can localize an object instance with reasonable runtime and\naccuracy; and (3) sketch querying is useful for manga search.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 03:47:46 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Matsui", "Yusuke", ""], ["Ito", "Kota", ""], ["Aramaki", "Yuji", ""], ["Yamasaki", "Toshihiko", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1510.04501", "submitter": "Alan Freihof Tygel", "authors": "Alan Tygel, S\\\"oren Auer, Jeremy Debattista, Fabrizio Orlandi, Maria\n  Luiza Machado Campos", "title": "Towards Cleaning-up Open Data Portals: A Metadata Reconciliation\n  Approach", "comments": "8 pages,10 Figures - Under Revision for ICSC2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an approach for metadata reconciliation, curation and\nlinking for Open Governamental Data Portals (ODPs). ODPs have been lately the\nstandard solution for governments willing to put their public data available\nfor the society. Portal managers use several types of metadata to organize the\ndatasets, one of the most important ones being the tags. However, the tagging\nprocess is subject to many problems, such as synonyms, ambiguity or\nincoherence, among others. As our empiric analysis of ODPs shows, these issues\nare currently prevalent in most ODPs and effectively hinders the reuse of Open\nData. In order to address these problems, we develop and implement an approach\nfor tag reconciliation in Open Data Portals, encompassing local actions related\nto individual portals, and global actions for adding a semantic metadata layer\nabove individual portals. The local part aims to enhance the quality of tags in\na single portal, and the global part is meant to interlink ODPs by establishing\nrelations between tags.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 12:29:56 GMT"}], "update_date": "2015-10-16", "authors_parsed": [["Tygel", "Alan", ""], ["Auer", "S\u00f6ren", ""], ["Debattista", "Jeremy", ""], ["Orlandi", "Fabrizio", ""], ["Campos", "Maria Luiza Machado", ""]]}, {"id": "1510.04780", "submitter": "Kan Ren", "authors": "Chenhao Zhu, Kan Ren, Xuan Liu, Haofen Wang, Yiding Tian and Yong Yu", "title": "A Graph Traversal Based Approach to Answer Non-Aggregation Questions\n  Over DBpedia", "comments": "In the proceedings of the 5th Joint International Semantic Technology\n  (JIST2015): https://link.springer.com/chapter/10.1007/978-3-319-31676-5_16", "journal-ref": null, "doi": "10.1007/978-3-319-31676-5_16", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a question answering system over DBpedia, filling the gap between\nuser information needs expressed in natural language and a structured query\ninterface expressed in SPARQL over the underlying knowledge base (KB). Given\nthe KB, our goal is to comprehend a natural language query and provide\ncorresponding accurate answers. Focusing on solving the non-aggregation\nquestions, in this paper, we construct a subgraph of the knowledge base from\nthe detected entities and propose a graph traversal method to solve both the\nsemantic item mapping problem and the disambiguation problem in a joint way.\nCompared with existing work, we simplify the process of query intention\nunderstanding and pay more attention to the answer path ranking. We evaluate\nour method on a non-aggregation question dataset and further on a complete\ndataset. Experimental results show that our method achieves best performance\ncompared with several state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 05:35:59 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 03:08:54 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Zhu", "Chenhao", ""], ["Ren", "Kan", ""], ["Liu", "Xuan", ""], ["Wang", "Haofen", ""], ["Tian", "Yiding", ""], ["Yu", "Yong", ""]]}, {"id": "1510.04972", "submitter": "Weiyi Sun", "authors": "Weiyi Sun, Anna Rumshisky, Ozlem Uzuner", "title": "Normalization of Relative and Incomplete Temporal Expressions in\n  Clinical Narratives", "comments": "Draft version", "journal-ref": "Journal of the American Medical Informatics Association (2015)", "doi": "10.1093/jamia/ocu004", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the RI-TIMEXes in temporally annotated corpora and propose two\nhypotheses regarding the normalization of RI-TIMEXes in the clinical narrative\ndomain: the anchor point hypothesis and the anchor relation hypothesis. We\nannotate the RI-TIMEXes in three corpora to study the characteristics of\nRI-TMEXes in different domains. This informed the design of our RI-TIMEX\nnormalization system for the clinical domain, which consists of an anchor point\nclassifier, an anchor relation classifier and a rule-based RI-TIMEX text span\nparser. We experiment with different feature sets and perform error analysis\nfor each system component. The annotation confirmed the hypotheses that we can\nsimplify the RI-TIMEXes normalization task using two multi-label classifiers.\nOur system achieves anchor point classification, anchor relation classification\nand rule-based parsing accuracy of 74.68%, 87.71% and 57.2% (82.09% under\nrelaxed matching criteria) respectively on the held-out test set of the 2012\ni2b2 temporal relation challenge. Experiments with feature sets reveals some\ninteresting findings such as the verbal tense feature does not inform the\nanchor relation classification in clinical narratives as much as the tokens\nnear the RI-TIMEX. Error analysis shows that underrepresented anchor point and\nanchor relation classes are difficult to detect. We formulate the RI-TIMEX\nnormalization problem as a pair of multi-label classification problems.\nConsidering only the RI-TIMEX extraction and normalization, the system achieves\nstatistically significant improvement over the RI-TIMEX results of the best\nsystems in the 2012 i2b2 challenge.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 18:35:13 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Sun", "Weiyi", ""], ["Rumshisky", "Anna", ""], ["Uzuner", "Ozlem", ""]]}, {"id": "1510.05120", "submitter": "Philipp Mayr", "authors": "Philipp Mayr, Ingo Frommholz, Guillaume Cabanac", "title": "Bibliometric-Enhanced Information Retrieval: 3rd International BIR\n  Workshop", "comments": "4 pages, 38th European Conference on IR Research, ECIR 2016, Padova,\n  Italy. arXiv admin note: substantial text overlap with arXiv:1501.02646", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The BIR workshop brings together experts in Bibliometrics and Information\nRetrieval. While sometimes perceived as rather loosely related, these research\nareas share various interests and face similar challenges. Our motivation as\norganizers of the BIR workshop stemmed from a twofold observation. First, both\ncommunities only partly overlap, albeit sharing various interests. Second, it\nwill be profitable for both sides to tackle some of the emerging problems that\nscholars face today when they have to identify relevant and high quality\nliterature in the fast growing number of electronic publications available\nworldwide. Bibliometric techniques are not yet used widely to enhance retrieval\nprocesses in digital libraries, although they offer value-added effects for\nusers. Information professionals working in libraries and archives, however,\nare increasingly confronted with applying bibliometric techniques in their\nservices. The first BIR workshop in 2014 set the research agenda by introducing\neach group to the other, illustrating state-of-the-art methods, reporting on\ncurrent research problems, and brainstorming about common interests. The second\nworkshop in 2015 further elaborated these themes. This third BIR workshop aims\nto foster a common ground for the incorporation of bibliometric-enhanced\nservices into scholarly search engine interfaces. In particular we will address\nspecific communities, as well as studies on large, cross-domain collections\nlike Mendeley and ResearchGate. This third BIR workshop addresses explicitly\nboth scholarly and industrial researchers.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2015 11:51:26 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2015 04:51:19 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Mayr", "Philipp", ""], ["Frommholz", "Ingo", ""], ["Cabanac", "Guillaume", ""]]}, {"id": "1510.05301", "submitter": "Haruna Isah", "authors": "Haruna Isah, Daniel Neagu, Paul Trundle", "title": "Social Media Analysis for Product Safety using Text Mining and Sentiment\n  Analysis", "comments": "2014 14th UK Workshop on Computational Intelligence (UKCI)", "journal-ref": null, "doi": "10.1109/UKCI.2014.6930158", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing incidents of counterfeiting and associated economic and health\nconsequences necessitate the development of active surveillance systems capable\nof producing timely and reliable information for all stake holders in the\nanti-counterfeiting fight. User generated content from social media platforms\ncan provide early clues about product allergies, adverse events and product\ncounterfeiting. This paper reports a work in progresswith contributions\nincluding: the development of a framework for gathering and analyzing the views\nand experiences of users of drug and cosmetic products using machine learning,\ntext mining and sentiment analysis, the application of the proposed framework\non Facebook comments and data from Twitter for brand analysis, and the\ndescription of how to develop a product safety lexicon and training data for\nmodeling a machine learning classifier for drug and cosmetic product sentiment\nprediction. The initial brand and product comparison results signify the\nusefulness of text mining and sentiment analysis on social media data while the\nuse of machine learning classifier for predicting the sentiment orientation\nprovides a useful tool for users, product manufacturers, regulatory and\nenforcement agencies to monitor brand or product sentiment trends in order to\nact in the event of sudden or significant rise in negative sentiment.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 20:05:08 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Isah", "Haruna", ""], ["Neagu", "Daniel", ""], ["Trundle", "Paul", ""]]}, {"id": "1510.05544", "submitter": "Neil Shah", "authors": "Neil Shah, Alex Beutel, Bryan Hooi, Leman Akoglu, Stephan Gunnemann,\n  Disha Makhija, Mohit Kumar, Christos Faloutsos", "title": "EdgeCentric: Anomaly Detection in Edge-Attributed Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a network with attributed edges, how can we identify anomalous\nbehavior? Networks with edge attributes are commonplace in the real world. For\nexample, edges in e-commerce networks often indicate how users rated products\nand services in terms of number of stars, and edges in online social and\nphonecall networks contain temporal information about when friendships were\nformed and when users communicated with each other -- in such cases, edge\nattributes capture information about how the adjacent nodes interact with other\nentities in the network. In this paper, we aim to utilize exactly this\ninformation to discern suspicious from typical node behavior. Our work has a\nnumber of notable contributions, including (a) formulation: while most other\ngraph-based anomaly detection works use structural graph connectivity or node\ninformation, we focus on the new problem of leveraging edge information, (b)\nmethodology: we introduce EdgeCentric, an intuitive and scalable\ncompression-based approach for detecting edge-attributed graph anomalies, and\n(c) practicality: we show that EdgeCentric successfully spots numerous such\nanomalies in several large, edge-attributed real-world graphs, including the\nFlipkart e-commerce graph with over 3 million product reviews between 1.1\nmillion users and 545 thousand products, where it achieved 0.87 precision over\nthe top 100 results.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 15:47:48 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 21:36:48 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Shah", "Neil", ""], ["Beutel", "Alex", ""], ["Hooi", "Bryan", ""], ["Akoglu", "Leman", ""], ["Gunnemann", "Stephan", ""], ["Makhija", "Disha", ""], ["Kumar", "Mohit", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1510.05911", "submitter": "Tim Weninger PhD", "authors": "Baoxu Shi, Tim Weninger", "title": "Discriminative Predicate Path Mining for Fact Checking in Knowledge\n  Graphs", "comments": "17 pages, 4 Figures. To Appear in Knowledge Based Systems", "journal-ref": null, "doi": "10.1016/j.knosys.2016.04.015", "report-no": null, "categories": "cs.DB cs.AI cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional fact checking by experts and analysts cannot keep pace with the\nvolume of newly created information. It is important and necessary, therefore,\nto enhance our ability to computationally determine whether some statement of\nfact is true or false. We view this problem as a link-prediction task in a\nknowledge graph, and present a discriminative path-based method for fact\nchecking in knowledge graphs that incorporates connectivity, type information,\nand predicate interactions. Given a statement S of the form (subject,\npredicate, object), for example, (Chicago, capitalOf, Illinois), our approach\nmines discriminative paths that alternatively define the generalized statement\n(U.S. city, predicate, U.S. state) and uses the mined rules to evaluate the\nveracity of statement S. We evaluate our approach by examining thousands of\nclaims related to history, geography, biology, and politics using a public,\nmillion node knowledge graph extracted from Wikipedia and PubMedDB. Not only\ndoes our approach significantly outperform related models, we also find that\nthe discriminative predicate path model is easily interpretable and provides\nsensible reasons for the final determination.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 14:31:31 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2016 14:43:30 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Shi", "Baoxu", ""], ["Weninger", "Tim", ""]]}, {"id": "1510.06153", "submitter": "Aaron Li", "authors": "Aaron Q Li, Yuntian Deng, Kublai Jing, Joseph W Robinson", "title": "Creating Scalable and Interactive Web Applications Using High\n  Performance Latent Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project we outline a modularized, scalable system for comparing\nAmazon products in an interactive and informative way using efficient latent\nvariable models and dynamic visualization. We demonstrate how our system can\nbuild on the structure and rich review information of Amazon products in order\nto provide a fast, multifaceted, and intuitive comparison. By providing a\ncondensed per-topic comparison visualization to the user, we are able to\ndisplay aggregate information from the entire set of reviews while providing an\ninterface that is at least as compact as the \"most helpful reviews\" currently\ndisplayed by Amazon, yet far more informative.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 07:29:23 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Li", "Aaron Q", ""], ["Deng", "Yuntian", ""], ["Jing", "Kublai", ""], ["Robinson", "Joseph W", ""]]}, {"id": "1510.06786", "submitter": "Vivek Kulkarni", "authors": "Vivek Kulkarni, Bryan Perozzi, Steven Skiena", "title": "Freshman or Fresher? Quantifying the Geographic Variation of Internet\n  Language", "comments": "11 pages (updated submission)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new computational technique to detect and analyze statistically\nsignificant geographic variation in language. Our meta-analysis approach\ncaptures statistical properties of word usage across geographical regions and\nuses statistical methods to identify significant changes specific to regions.\nWhile previous approaches have primarily focused on lexical variation between\nregions, our method identifies words that demonstrate semantic and syntactic\nvariation as well.\n  We extend recently developed techniques for neural language models to learn\nword representations which capture differing semantics across geographical\nregions. In order to quantify this variation and ensure robust detection of\ntrue regional differences, we formulate a null model to determine whether\nobserved changes are statistically significant. Our method is the first such\napproach to explicitly account for random variation due to chance while\ndetecting regional variation in word meaning.\n  To validate our model, we study and analyze two different massive online data\nsets: millions of tweets from Twitter spanning not only four different\ncountries but also fifty states, as well as millions of phrases contained in\nthe Google Book Ngrams. Our analysis reveals interesting facets of language\nchange at multiple scales of geographic resolution -- from neighboring states\nto distant continents.\n  Finally, using our model, we propose a measure of semantic distance between\nlanguages. Our analysis of British and American English over a period of 100\nyears reveals that semantic variation between these dialects is shrinking.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 22:53:10 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 14:40:36 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Kulkarni", "Vivek", ""], ["Perozzi", "Bryan", ""], ["Skiena", "Steven", ""]]}, {"id": "1510.07025", "submitter": "Dawen Liang", "authors": "Dawen Liang, Laurent Charlin, James McInerney, David M. Blei", "title": "Modeling User Exposure in Recommendation", "comments": "11 pages, 4 figures. WWW'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering analyzes user preferences for items (e.g., books,\nmovies, restaurants, academic papers) by exploiting the similarity patterns\nacross users. In implicit feedback settings, all the items, including the ones\nthat a user did not consume, are taken into consideration. But this assumption\ndoes not accord with the common sense understanding that users have a limited\nscope and awareness of items. For example, a user might not have heard of a\ncertain paper, or might live too far away from a restaurant to experience it.\nIn the language of causal analysis, the assignment mechanism (i.e., the items\nthat a user is exposed to) is a latent variable that may change for various\nuser/item combinations. In this paper, we propose a new probabilistic approach\nthat directly incorporates user exposure to items into collaborative filtering.\nThe exposure is modeled as a latent variable and the model infers its value\nfrom data. In doing so, we recover one of the most successful state-of-the-art\napproaches as a special case of our model, and provide a plug-in method for\nconditioning exposure on various forms of exposure covariates (e.g., topics in\ntext, venue locations). We show that our scalable inference algorithm\noutperforms existing benchmarks in four different domains both with and without\nexposure covariates.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 19:39:38 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2016 18:58:44 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Liang", "Dawen", ""], ["Charlin", "Laurent", ""], ["McInerney", "James", ""], ["Blei", "David M.", ""]]}, {"id": "1510.07035", "submitter": "Aaron Li", "authors": "Joseph W Robinson, Aaron Q Li", "title": "Fast Latent Variable Models for Inference and Visualization on Mobile\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project we outline Vedalia, a high performance distributed network\nfor performing inference on latent variable models in the context of Amazon\nreview visualization. We introduce a new model, RLDA, which extends Latent\nDirichlet Allocation (LDA) [Blei et al., 2003] for the review space by\nincorporating auxiliary data available in online reviews to improve modeling\nwhile simultaneously remaining compatible with pre-existing fast sampling\ntechniques such as [Yao et al., 2009; Li et al., 2014a] to achieve high\nperformance. The network is designed such that computation is efficiently\noffloaded to the client devices using the Chital system [Robinson & Li, 2015],\nimproving response times and reducing server costs. The resulting system is\nable to rapidly compute a large number of specialized latent variable models\nwhile requiring minimal server resources.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 05:26:09 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Robinson", "Joseph W", ""], ["Li", "Aaron Q", ""]]}, {"id": "1510.07064", "submitter": "Somwrita Sarkar", "authors": "Somwrita Sarkar and Sanjay Chawla and Peter A. Robinson and Santo\n  Fortunato", "title": "Eigenvector dynamics under perturbation of modular networks", "comments": "7 pages, 4 figures", "journal-ref": "Physical Review E, 93, 062312, 2016", "doi": "10.1103/PhysRevE.93.062312", "report-no": null, "categories": "physics.soc-ph cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotation dynamics of eigenvectors of modular network adjacency matrices under\nrandom perturbations are presented. In the presence of $q$ communities, the\nnumber of eigenvectors corresponding to the $q$ largest eigenvalues form a\n\"community\" eigenspace and rotate together, but separately from that of the\n\"bulk\" eigenspace spanned by all the other eigenvectors. Using this property,\nthe number of modules or clusters in a network can be estimated in an\nalgorithm-independent way. A general argument and derivation for the\ntheoretical detectability limit for sparse modular networks with $q$\ncommunities is presented, beyond which modularity persists in the system but\ncannot be detected. It is shown that for detecting the clusters or modules\nusing the adjacency matrix, there is a \"band\" in which it is hard to detect the\nclusters even before the theoretical detectability limit is reached, and for\nwhich the theoretically predicted detectability limit forms the sufficient\nupper bound. Analytic estimations of these bounds are presented, and\nempirically demonstrated.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2015 21:04:17 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 05:13:05 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Sarkar", "Somwrita", ""], ["Chawla", "Sanjay", ""], ["Robinson", "Peter A.", ""], ["Fortunato", "Santo", ""]]}, {"id": "1510.07197", "submitter": "Xiang Ren", "authors": "Xiang Ren, Yuanhua Lv, Kuansan Wang, Jiawei Han", "title": "Comparative Document Analysis for Large Text Corpora", "comments": "Submission to WWW'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel research problem on joint discovery of\ncommonalities and differences between two individual documents (or document\nsets), called Comparative Document Analysis (CDA). Given any pair of documents\nfrom a document collection, CDA aims to automatically identify sets of quality\nphrases to summarize the commonalities of both documents and highlight the\ndistinctions of each with respect to the other informatively and concisely. Our\nsolution uses a general graph-based framework to derive novel measures on\nphrase semantic commonality and pairwise distinction}, and guides the selection\nof sets of phrases by solving two joint optimization problems. We develop an\niterative algorithm to integrate the maximization of phrase commonality or\ndistinction measure with the learning of phrase-document semantic relevance in\na mutually enhancing way. Experiments on text corpora from two different\ndomains---scientific publications and news---demonstrate the effectiveness and\nrobustness of the proposed method on comparing individual documents. Our case\nstudy on comparing news articles published at different dates shows the power\nof the proposed method on comparing document sets.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 01:22:05 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Ren", "Xiang", ""], ["Lv", "Yuanhua", ""], ["Wang", "Kuansan", ""], ["Han", "Jiawei", ""]]}, {"id": "1510.07385", "submitter": "Jean-Valere Cossu", "authors": "Jean-Val\\`ere Cossu (LIA), Ludovic Bonnefoy (LIA), Xavier Bost (LIA),\n  Marc El B\\`eze (LIA)", "title": "How to merge three different methods for information filtering ?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter is now a gold marketing tool for entities concerned with online\nreputation. To automatically monitor online reputation of entities , systems\nhave to deal with ambiguous entity names, polarity detection and topic\ndetection. We propose three approaches to tackle the first issue: monitoring\nTwitter in order to find relevant tweets about a given entity. Evaluated within\nthe framework of the RepLab-2013 Filtering task, each of them has been shown\ncompetitive with state-of-the-art approaches. Mainly we investigate on how much\nmerging strategies may impact performances on a filtering task according to the\nevaluation measure.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 07:17:36 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Cossu", "Jean-Val\u00e8re", "", "LIA"], ["Bonnefoy", "Ludovic", "", "LIA"], ["Bost", "Xavier", "", "LIA"], ["B\u00e8ze", "Marc El", "", "LIA"]]}, {"id": "1510.07545", "submitter": "Tobias Schnabel", "authors": "Tobias Schnabel, Paul N. Bennett, Susan T. Dumais and Thorsten\n  Joachims", "title": "Using Shortlists to Support Decision Making and Improve Recommender\n  System Performance", "comments": "11 pages in WWW 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study shortlists as an interface component for recommender\nsystems with the dual goal of supporting the user's decision process, as well\nas improving implicit feedback elicitation for increased recommendation\nquality. A shortlist is a temporary list of candidates that the user is\ncurrently considering, e.g., a list of a few movies the user is currently\nconsidering for viewing. From a cognitive perspective, shortlists serve as\ndigital short-term memory where users can off-load the items under\nconsideration -- thereby decreasing their cognitive load. From a machine\nlearning perspective, adding items to the shortlist generates a new implicit\nfeedback signal as a by-product of exploration and decision making which can\nimprove recommendation quality. Shortlisting therefore provides additional data\nfor training recommendation systems without the increases in cognitive load\nthat requesting explicit feedback would incur.\n  We perform an user study with a movie recommendation setup to compare\ninterfaces that offer shortlist support with those that do not. From the user\nstudies we conclude: (i) users make better decisions with a shortlist; (ii)\nusers prefer an interface with shortlist support; and (iii) the additional\nimplicit feedback from sessions with a shortlist improves the quality of\nrecommendations by nearly a factor of two.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2015 16:49:07 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2016 12:16:23 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Schnabel", "Tobias", ""], ["Bennett", "Paul N.", ""], ["Dumais", "Susan T.", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1510.08487", "submitter": "Nemanja Spasojevic", "authors": "Adithya Rao, Nemanja Spasojevic, Zhisheng Li and Trevor DSouza", "title": "Klout Score: Measuring Influence Across Multiple Social Networks", "comments": "8 pages, 2015 IEEE International Big Data Conference - Workshop on\n  Mining Big Data in Social Networks", "journal-ref": null, "doi": "10.1109/BigData.2015.7364017", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the Klout Score, an influence scoring system that\nassigns scores to 750 million users across 9 different social networks on a\ndaily basis. We propose a hierarchical framework for generating an influence\nscore for each user, by incorporating information for the user from multiple\nnetworks and communities. Over 3600 features that capture signals of\ninfluential interactions are aggregated across multiple dimensions for each\nuser. The features are scalably generated by processing over 45 billion\ninteractions from social networks every day, as well as by incorporating\nfactors that indicate real world influence. Supervised models trained from\nlabeled data determine the weights for features, and the final Klout Score is\nobtained by hierarchically combining communities and networks. We validate the\ncorrectness of the score by showing that users with higher scores are able to\nspread information more effectively in a network. Finally, we use several\ncomparisons to other ranking systems to show that highly influential and\nrecognizable users across different domains have high Klout scores.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 21:04:10 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Rao", "Adithya", ""], ["Spasojevic", "Nemanja", ""], ["Li", "Zhisheng", ""], ["DSouza", "Trevor", ""]]}, {"id": "1510.08628", "submitter": "Jianfei Chen", "authors": "Jianfei Chen, Kaiwei Li, Jun Zhu, Wenguang Chen", "title": "WarpLDA: a Cache Efficient O(1) Algorithm for Latent Dirichlet\n  Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing efficient and scalable algorithms for Latent Dirichlet Allocation\n(LDA) is of wide interest for many applications. Previous work has developed an\nO(1) Metropolis-Hastings sampling method for each token. However, the\nperformance is far from being optimal due to random accesses to the parameter\nmatrices and frequent cache misses.\n  In this paper, we first carefully analyze the memory access efficiency of\nexisting algorithms for LDA by the scope of random access, which is the size of\nthe memory region in which random accesses fall, within a short period of time.\nWe then develop WarpLDA, an LDA sampler which achieves both the best O(1) time\ncomplexity per token and the best O(K) scope of random access. Our empirical\nresults in a wide range of testing conditions demonstrate that WarpLDA is\nconsistently 5-15x faster than the state-of-the-art Metropolis-Hastings based\nLightLDA, and is comparable or faster than the sparsity aware F+LDA. With\nWarpLDA, users can learn up to one million topics from hundreds of millions of\ndocuments in a few hours, at an unprecedentedly throughput of 11G tokens per\nsecond.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 10:33:20 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 06:29:30 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Chen", "Jianfei", ""], ["Li", "Kaiwei", ""], ["Zhu", "Jun", ""], ["Chen", "Wenguang", ""]]}, {"id": "1510.08897", "submitter": "Kyriaki Dimitriadou", "authors": "Kyriaki Dimitriadou and Olga Papaemmanouil and Yanlei Diao", "title": "AIDE: An Automated Sample-based Approach for Interactive Data\n  Exploration", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we argue that database systems be augmented with an automated\ndata exploration service that methodically steers users through the data in a\nmeaningful way. Such an automated system is crucial for deriving insights from\ncomplex datasets found in many big data applications such as scientific and\nhealthcare applications as well as for reducing the human effort of data\nexploration. Towards this end, we present AIDE, an Automatic Interactive Data\nExploration framework that assists users in discovering new interesting data\npatterns and eliminate expensive ad-hoc exploratory queries.\n  AIDE relies on a seamless integration of classification algorithms and data\nmanagement optimization techniques that collectively strive to accurately learn\nthe user interests based on his relevance feedback on strategically collected\nsamples. We present a number of exploration techniques as well as optimizations\nthat minimize the number of samples presented to the user while offering\ninteractive performance. AIDE can deliver highly accurate query predictions for\nvery common conjunctive queries with small user effort while, given a\nreasonable number of samples, it can predict with high accuracy complex\ndisjunctive queries. It provides interactive performance as it limits the user\nwait time per iteration of exploration to less than a few seconds.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 20:50:05 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Dimitriadou", "Kyriaki", ""], ["Papaemmanouil", "Olga", ""], ["Diao", "Yanlei", ""]]}]