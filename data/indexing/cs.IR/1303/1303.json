[{"id": "1303.0073", "submitter": "Uri Kartoun", "authors": "Uri Kartoun", "title": "A Method for Comparing Hedge Funds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents new machine learning methods: signal composition, which\nclassifies time-series regardless of length, type, and quantity; and\nself-labeling, a supervised-learning enhancement. The paper describes further\nthe implementation of the methods on a financial search engine system to\nidentify behavioral similarities among time-series representing monthly returns\nof 11,312 hedge funds operated during approximately one decade (2000 - 2010).\nThe presented approach of cross-category and cross-location classification\nassists the investor to identify alternative investments.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 03:38:35 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 21:20:08 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Kartoun", "Uri", ""]]}, {"id": "1303.0093", "submitter": "Tomasz Kajdanowicz", "authors": "Przemyslaw Kazienko, Katarzyna Musial and Tomasz Kajdanowicz", "title": "Multidimensional Social Network in the Social Recommender System", "comments": "social recommender system;Multidimensional social network (MSN);Web\n  2.0;multi-layered social network;multimedia sharing system (MSS);recommender\n  system;social network analysis", "journal-ref": "Kazienko, P.; Musial, K.; Kajdanowicz, T.; , \"Multidimensional\n  Social Network in the Social Recommender System,\" Systems, Man and\n  Cybernetics, Part A: Systems and Humans, IEEE Transactions on , vol.41, no.4,\n  pp.746-759, July 2011", "doi": "10.1109/TSMCA.2011.2132707", "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All online sharing systems gather data that reflects users' collective\nbehaviour and their shared activities. This data can be used to extract\ndifferent kinds of relationships, which can be grouped into layers, and which\nare basic components of the multidimensional social network proposed in the\npaper. The layers are created on the basis of two types of relations between\nhumans, i.e. direct and object-based ones which respectively correspond to\neither social or semantic links between individuals. For better understanding\nof the complexity of the social network structure, layers and their profiles\nwere identified and studied on two, spanned in time, snapshots of the Flickr\npopulation. Additionally, for each layer, a separate strength measure was\nproposed. The experiments on the Flickr photo sharing system revealed that the\nrelationships between users result either from semantic links between objects\nthey operate on or from social connections of these users. Moreover, the\ndensity of the social network increases in time. The second part of the study\nis devoted to building a social recommender system that supports the creation\nof new relations between users in a multimedia sharing system. Its main goal is\nto generate personalized suggestions that are continuously adapted to users'\nneeds depending on the personal weights assigned to each layer in the\nmultidimensional social network. The conducted experiments confirmed the\nusefulness of the proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 06:17:24 GMT"}], "update_date": "2013-03-04", "authors_parsed": [["Kazienko", "Przemyslaw", ""], ["Musial", "Katarzyna", ""], ["Kajdanowicz", "Tomasz", ""]]}, {"id": "1303.0283", "submitter": "Uri Kartoun", "authors": "Uri Kartoun", "title": "Inverse Signal Classification for Financial Instruments", "comments": "arXiv admin note: substantial text overlap with arXiv:1303.0073", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents new machine learning methods: signal composition, which\nclassifies time-series regardless of length, type, and quantity; and\nself-labeling, a supervised-learning enhancement. The paper describes further\nthe implementation of the methods on a financial search engine system using a\ncollection of 7,881 financial instruments traded during 2011 to identify\ninverse behavior among the time-series.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 03:45:42 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 21:17:56 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Kartoun", "Uri", ""]]}, {"id": "1303.0284", "submitter": "Tomasz Kajdanowicz", "authors": "Katarzyna Musial, Przemyslaw Kazienkol and Tomasz Kajdanowicz", "title": "Social Recommendations within the Multimedia Sharing Systems", "comments": "recommender system, multirelational social network, multimedia\n  sharing system, social network analysis, Best Paper Award. arXiv admin note:\n  text overlap with arXiv:1303.0093", "journal-ref": "Musial K., Kazienko P., Kajdanowicz T.: Social Recommendations\n  within the Multimedia Sharing Systems. The First World Summit on the\n  Knowledge Society, WSKS'08, Lecture Notes in Computer Science LNCS 5288,\n  2008, pp. 364-372", "doi": "10.1007/978-3-540-87781-3_40", "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The social recommender system that supports the creation of new relations\nbetween users in the multimedia sharing system is presented in the paper. To\ngenerate suggestions the new concept of the multirelational social network was\nintroduced. It covers both direct as well as object-based relationships that\nreflect social and semantic links between users. The main goal of the new\nmethod is to create the personalized suggestions that are continuously adapted\nto users' needs depending on the personal weights assigned to each layer from\nthe social network. The conducted experiments confirmed the usefulness of the\nproposed model.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 07:09:39 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Musial", "Katarzyna", ""], ["Kazienkol", "Przemyslaw", ""], ["Kajdanowicz", "Tomasz", ""]]}, {"id": "1303.0407", "submitter": "Qasem Abu Al-Haija", "authors": "Ahmad Al Badawi and Qasem Abu Al-Haija", "title": "IRS for Computer Character Sequences Filtration: a new software tool and\n  algorithm to support the IRS at tokenization process", "comments": "5 pages, 6 figures, corresponding author is Qasem Abu Al-Haija", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), The Science and Information Organization (SAI), Vol. 4,\n  No. 2, 2013,", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tokenization is the task of chopping it up into pieces, called tokens,\nperhaps at the same time throwing away certain characters, such as punctuation.\nA token is an instance of token a sequence of characters in some particular\ndocument that are grouped together as a useful semantic unit for processing.\nNew software tool and algorithm to support the IRS at tokenization process are\npresented. Our proposed tool will filter out the three computer character\nSequences: IP-Addresses, Web URLs, Date, and Email Addresses. Our tool will use\nthe pattern matching algorithms and filtration methods. After this process, the\nIRS can start a new tokenization process on the new retrieved text which will\nbe free of these sequences.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2013 17:43:38 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Badawi", "Ahmad Al", ""], ["Al-Haija", "Qasem Abu", ""]]}, {"id": "1303.0445", "submitter": "Kanagavalli V R", "authors": "Kanagavalli V R and Raja. K", "title": "Detecting and resolving spatial ambiguity in text using named entity\n  extraction and self learning fuzzy logic techniques", "comments": "National Conference on Recent Trends in Data Mining and Distributed\n  Systems September 2011", "journal-ref": null, "doi": null, "report-no": "ISBN 978-81-909042-5-4 P.no.71-76", "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information extraction identifies useful and relevant text in a document and\nconverts unstructured text into a form that can be loaded into a database\ntable. Named entity extraction is a main task in the process of information\nextraction and is a classification problem in which words are assigned to one\nor more semantic classes or to a default non-entity class. A word which can\nbelong to one or more classes and which has a level of uncertainty in it can be\nbest handled by a self learning Fuzzy Logic Technique. This paper proposes a\nmethod for detecting the presence of spatial uncertainty in the text and\ndealing with spatial ambiguity using named entity extraction techniques coupled\nwith self learning fuzzy logic techniques\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 01:21:58 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["R", "Kanagavalli V", ""], ["K", "Raja.", ""]]}, {"id": "1303.0481", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Situation-Aware Approach to Improve Context-based Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel situation aware approach to improve a\ncontext based recommender system. To build situation aware user profiles, we\nrely on evidence issued from retrieval situations. A retrieval situation refers\nto the social spatio temporal context of the user when he interacts with the\nrecommender system. A situation is represented as a combination of social\nspatio temporal concepts inferred from ontological knowledge given social\ngroup, location and time information. User's interests are inferred from past\nuser's interaction with the recommender system related to the identified\nsituations. They are represented using concepts issued from a domain ontology.\nWe also propose a method to dynamically adapt the system to the user's\ninterest's evolution.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 09:32:44 GMT"}, {"version": "v2", "created": "Sun, 30 Mar 2014 08:09:43 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1303.0484", "submitter": "Folke Mitzlaff", "authors": "Folke Mitzlaff and Gerd Stumme", "title": "Onomastics 2.0 - The Power of Social Co-Occurrences", "comments": "Historically, this is the first paper on the analysis of names in the\n  context of the name search engine 'nameling'. arXiv admin note: text overlap\n  with arXiv:1302.4412", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Onomastics is \"the science or study of the origin and forms of proper names\nof persons or places.\" [\"Onomastics\". Merriam-Webster.com, 2013.\nhttp://www.merriam-webster.com (11 February 2013)]. Especially personal names\nplay an important role in daily life, as all over the world future parents are\nfacing the task of finding a suitable given name for their child. This choice\nis influenced by different factors, such as the social context, language,\ncultural background and, in particular, personal taste.\n  With the rise of the Social Web and its applications, users more and more\ninteract digitally and participate in the creation of heterogeneous,\ndistributed, collaborative data collections. These sources of data also reflect\ncurrent and new naming trends as well as new emerging interrelations among\nnames.\n  The present work shows, how basic approaches from the field of social network\nanalysis and information retrieval can be applied for discovering relations\namong names, thus extending Onomastics by data mining techniques. The\nconsidered approach starts with building co-occurrence graphs relative to data\nfrom the Social Web, respectively for given names and city names. As a main\nresult, correlations between semantically grounded similarities among names\n(e.g., geographical distance for city names) and structural graph based\nsimilarities are observed.\n  The discovered relations among given names are the foundation of \"nameling\"\n[http://nameling.net], a search engine and academic research platform for given\nnames which attracted more than 30,000 users within four months,\nunderpinningthe relevance of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 10:11:44 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Mitzlaff", "Folke", ""], ["Stumme", "Gerd", ""]]}, {"id": "1303.0485", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Optimizing an Utility Function for Exploration / Exploitation Trade-off\n  in Context-Aware Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a dynamic exploration/ exploitation (exr/exp)\nstrategy for contextual recommender systems (CRS). Specifically, our methods\ncan adaptively balance the two aspects of exr/exp by automatically learning the\noptimal tradeoff. This consists of optimizing a utility function represented by\na linearized form of the probability distributions of the rewards of the\nclicked and the non-clicked documents already recommended. Within an offline\nsimulation framework we apply our algorithms to a CRS and conduct an evaluation\nwith real event log data. The experimental results and detailed analysis\ndemonstrate that our algorithms outperform existing algorithms in terms of\nclick-through-rate (CTR).\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 10:23:38 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 08:31:54 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1303.0489", "submitter": "Leena Patil Leena Homraj Patil", "authors": "Leena H. Patil, Mohammed Atique", "title": "A Semantic approach for effective document clustering using WordNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Now a days, the text document is spontaneously increasing over the internet,\ne-mail and web pages and they are stored in the electronic database format. To\narrange and browse the document it becomes difficult. To overcome such problem\nthe document preprocessing, term selection, attribute reduction and maintaining\nthe relationship between the important terms using background knowledge,\nWordNet, becomes an important parameters in data mining. In these paper the\ndifferent stages are formed, firstly the document preprocessing is done by\nremoving stop words, stemming is performed using porter stemmer algorithm, word\nnet thesaurus is applied for maintaining relationship between the important\nterms, global unique words, and frequent word sets get generated, Secondly,\ndata matrix is formed, and thirdly terms are extracted from the documents by\nusing term selection approaches tf-idf, tf-df, and tf2 based on their minimum\nthreshold value. Further each and every document terms gets preprocessed, where\nthe frequency of each term within the document is counted for representation.\nThe purpose of this approach is to reduce the attributes and find the effective\nterm selection method using WordNet for better clustering accuracy. Experiments\nare evaluated on Reuters Transcription Subsets, wheat, trade, money grain, and\nship, Reuters 21578, Classic 30, 20 News group (atheism), 20 News group\n(Hardware), 20 News group (Computer Graphics) etc.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 12:19:18 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Patil", "Leena H.", ""], ["Atique", "Mohammed", ""]]}, {"id": "1303.0566", "submitter": "Taher Zaki", "authors": "T. Zaki (1 and 2), M. Amrouch (1), D. Mammass (1), A. Ennaji (2) ((1)\n  IRFSIC Laboratory, Ibn Zohr University Agadir Morocco, (2) LITIS Laboratory,\n  University of Rouen France)", "title": "Arabic documents classification using fuzzy R.B.F. classifier with\n  sliding window", "comments": "5 pages, 2 figures", "journal-ref": "Journal of Computing , eISSN 2151-9617 , Volume 5, Issue 1,\n  January 2013", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper, we propose a system for contextual and semantic Arabic\ndocuments classification by improving the standard fuzzy model. Indeed,\npromoting neighborhood semantic terms that seems absent in this model by using\na radial basis modeling. In order to identify the relevant documents to the\nquery. This approach calculates the similarity between related terms by\ndetermining the relevance of each relative to documents (NEAR operator), based\non a kernel function. The use of sliding window improves the process of\nclassification. The results obtained on a arabic dataset of press show very\ngood performance compared with the literature.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2013 20:50:12 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Zaki", "T.", "", "1 and 2"], ["Amrouch", "M.", ""], ["Mammass", "D.", ""], ["Ennaji", "A.", ""]]}, {"id": "1303.0597", "submitter": "Tao Zhu", "authors": "Tao Zhu, David Phipps, Adam Pridgen, Jedidiah R. Crandall and Dan S.\n  Wallach", "title": "The Velocity of Censorship: High-Fidelity Detection of Microblog Post\n  Deletions", "comments": "arXiv admin note: substantial text overlap with arXiv:1211.6166", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weibo and other popular Chinese microblogging sites are well known for\nexercising internal censorship, to comply with Chinese government requirements.\nThis research seeks to quantify the mechanisms of this censorship: how fast and\nhow comprehensively posts are deleted.Our analysis considered 2.38 million\nposts gathered over roughly two months in 2012, with our attention focused on\nrepeatedly visiting \"sensitive\" users. This gives us a view of censorship\nevents within minutes of their occurrence, albeit at a cost of our data no\nlonger representing a random sample of the general Weibo population. We also\nhave a larger 470 million post sampling from Weibo's public timeline, taken\nover a longer time period, that is more representative of a random sample.\n  We found that deletions happen most heavily in the first hour after a post\nhas been submitted. Focusing on original posts, not reposts/retweets, we\nobserved that nearly 30% of the total deletion events occur within 5- 30\nminutes. Nearly 90% of the deletions happen within the first 24 hours.\nLeveraging our data, we also considered a variety of hypotheses about the\nmechanisms used by Weibo for censorship, such as the extent to which Weibo's\ncensors use retrospective keyword-based censorship, and how repost/retweet\npopularity interacts with censorship. We also used natural language processing\ntechniques to analyze which topics were more likely to be censored.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 04:15:03 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2013 01:21:03 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Zhu", "Tao", ""], ["Phipps", "David", ""], ["Pridgen", "Adam", ""], ["Crandall", "Jedidiah R.", ""], ["Wallach", "Dan S.", ""]]}, {"id": "1303.0646", "submitter": "Stefano Braghin", "authors": "Anwitaman Datta and Stefano Braghin and Jackson Tan Teck Yong", "title": "The Zen of Multidisciplinary Team Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to accomplish complex tasks, it is often necessary to compose a team\nconsisting of experts with diverse competencies. However, for proper\nfunctioning, it is also preferable that a team be socially cohesive. A team\nrecommendation system, which facilitates the search for potential team members\ncan be of great help both for (i) individuals who need to seek out\ncollaborators and (ii) managers who need to build a team for some specific\ntasks.\n  A decision support system which readily helps summarize such metrics, and\npossibly rank the teams in a personalized manner according to the end users'\npreferences, can be a great tool to navigate what would otherwise be an\ninformation avalanche.\n  In this work we present a general framework of how to compose such subsystems\ntogether to build a composite team recommendation system, and instantiate it\nfor a case study of academic teams.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 09:07:04 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Datta", "Anwitaman", ""], ["Braghin", "Stefano", ""], ["Yong", "Jackson Tan Teck", ""]]}, {"id": "1303.0665", "submitter": "Florent Garcin", "authors": "Florent Garcin, Christos Dimitrakakis and Boi Faltings", "title": "Personalized News Recommendation with Context Trees", "comments": null, "journal-ref": "Proceedings of the 7th ACM conference on Recommender systems\n  (2013), pp. 105--112", "doi": "10.1145/2507157.2507166", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The profusion of online news articles makes it difficult to find interesting\narticles, a problem that can be assuaged by using a recommender system to bring\nthe most relevant news stories to readers. However, news recommendation is\nchallenging because the most relevant articles are often new content seen by\nfew users. In addition, they are subject to trends and preference changes over\ntime, and in many cases we do not have sufficient information to profile the\nreader.\n  In this paper, we introduce a class of news recommendation systems based on\ncontext trees. They can provide high-quality news recommendation to anonymous\nvisitors based on present browsing behaviour. We show that context-tree\nrecommender systems provide good prediction accuracy and recommendation\nnovelty, and they are sufficiently flexible to capture the unique properties of\nnews articles.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 10:34:13 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 16:19:43 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Garcin", "Florent", ""], ["Dimitrakakis", "Christos", ""], ["Faltings", "Boi", ""]]}, {"id": "1303.0667", "submitter": "Dipasree Pal", "authors": "Dipasree Pal, Mandar Mitra, Kalyankumar Datta", "title": "Query Expansion Using Term Distribution and Term Association", "comments": "19 pages, 1 figure, 2 result tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good term selection is an important issue for an automatic query expansion\n(AQE) technique. AQE techniques that select expansion terms from the target\ncorpus usually do so in one of two ways. Distribution based term selection\ncompares the distribution of a term in the (pseudo) relevant documents with\nthat in the whole corpus / random distribution. Two well-known\ndistribution-based methods are based on Kullback-Leibler Divergence (KLD) and\nBose-Einstein statistics (Bo1). Association based term selection, on the other\nhand, uses information about how a candidate term co-occurs with the original\nquery terms. Local Context Analysis (LCA) and Relevance-based Language Model\n(RM3) are examples of association-based methods. Our goal in this study is to\ninvestigate how these two classes of methods may be combined to improve\nretrieval effectiveness. We propose the following combination-based approach.\nCandidate expansion terms are first obtained using a distribution based method.\nThis set is then refined based on the strength of the association of terms with\nthe original query terms. We test our methods on 11 TREC collections. The\nproposed combinations generally yield better results than each individual\nmethod, as well as other state-of-the-art AQE approaches. En route to our\nprimary goal, we also propose some modifications to LCA and Bo1 which lead to\nimproved performance.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2013 10:34:41 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Pal", "Dipasree", ""], ["Mitra", "Mandar", ""], ["Datta", "Kalyankumar", ""]]}, {"id": "1303.1441", "submitter": "Kamal Sarkar", "authors": "Kamal Sarkar", "title": "A Hybrid Approach to Extract Keyphrases from Medical Documents", "comments": null, "journal-ref": "International Journal of Computer Applications 63(18):14-19,\n  February 2013. Published by Foundation of Computer Science, New York, USA", "doi": "10.5120/10565-5528", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrases are the phrases, consisting of one or more words, representing the\nimportant concepts in the articles. Keyphrases are useful for a variety of\ntasks such as text summarization, automatic indexing,\nclustering/classification, text mining etc. This paper presents a hybrid\napproach to keyphrase extraction from medical documents. The keyphrase\nextraction approach presented in this paper is an amalgamation of two methods:\nthe first one assigns weights to candidate keyphrases based on an effective\ncombination of features such as position, term frequency, inverse document\nfrequency and the second one assign weights to candidate keyphrases using some\nknowledge about their similarities to the structure and characteristics of\nkeyphrases available in the memory (stored list of keyphrases). An efficient\ncandidate keyphrase identification method as the first component of the\nproposed keyphrase extraction system has also been introduced in this paper.\nThe experimental results show that the proposed hybrid approach performs better\nthan some state-of-the art keyphrase extraction approaches.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 20:09:05 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2014 16:34:35 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Sarkar", "Kamal", ""]]}, {"id": "1303.1695", "submitter": "R K Bisht", "authors": "Raj Kishor Bisht and Ila Pant Bisht", "title": "Effect of Query Formation on Web Search Engine Results", "comments": "6 pages, journal", "journal-ref": "International Journal of Natural Language Computing, 2013, Vol 2,\n  No. 1, 31-36", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query in a search engine is generally based on natural language. A query can\nbe expressed in more than one way without changing its meaning as it depends on\nthinking of human being at a particular moment. Aim of the searcher is to get\nmost relevant results immaterial of how the query has been expressed. In the\npresent paper, we have examined the results of search engine for change in\ncoverage and similarity of first few results when a query is entered in two\nsemantically same but in different formats. Searching has been made through\nGoogle search engine. Fifteen pairs of queries have been chosen for the study.\nThe t-test has been used for the purpose and the results have been checked on\nthe basis of total documents found, similarity of first five and first ten\ndocuments found in the results of a query entered in two different formats. It\nhas been found that the total coverage is same but first few results are\nsignificantly different.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 14:15:19 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Bisht", "Raj Kishor", ""], ["Bisht", "Ila Pant", ""]]}, {"id": "1303.1703", "submitter": "Fatiha Boubekeur", "authors": "Fatiha Boubekeur and Wassila Azzoug", "title": "Concept-based indexing in text information retrieval", "comments": "18 pages, 5 tables, 3 figures", "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 5, No 1, February 2013", "doi": "10.5121/ijcsit", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional information retrieval systems rely on keywords to index documents\nand queries. In such systems, documents are retrieved based on the number of\nshared keywords with the query. This lexical-focused retrieval leads to\ninaccurate and incomplete results when different keywords are used to describe\nthe documents and queries. Semantic-focused retrieval approaches attempt to\novercome this problem by relying on concepts rather than on keywords to\nindexing and retrieval. The goal is to retrieve documents that are semantically\nrelevant to a given user query. This paper addresses this issue by proposing a\nsolution at the indexing level. More precisely, we propose a novel approach for\nsemantic indexing based on concepts identified from a linguistic resource. In\nparticular, our approach relies on the joint use of WordNet and WordNetDomains\nlexical databases for concept identification. Furthermore, we propose a\nsemantic-based concept weighting scheme that relies on a novel definition of\nconcept centrality. The resulting system is evaluated on the TIME test\ncollection. Experimental results show the effectiveness of our proposition over\ntraditional IR approaches.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2013 14:46:17 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Boubekeur", "Fatiha", ""], ["Azzoug", "Wassila", ""]]}, {"id": "1303.2156", "submitter": "Heng Gao", "authors": "Heng Gao, Yongbao Li, Qiudan Li, Daniel Zeng", "title": "The Powerful Model Adpredictor for Search Engine Switching Detection\n  Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The purpose of the Switching Detection Challenge in the 2013 WSCD workshop\nwas to predict users' search engine swithcing actions given records about\nsearch sessions and logs.Our solution adopted the powerful prediction model\nAdpredictor and utilized the method of feature engineering. We successfully\napplied the click through rate (CTR) prediction model Adpredicitor into our\nsolution framework, and then the discovery of effective features and the\nmultiple classification of different switching type make our model outperforms\nmany competitors. We achieved an AUC score of 0.84255 on the private\nleaderboard and ranked the 5th among all the competitors in the competition.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2013 02:01:04 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Gao", "Heng", ""], ["Li", "Yongbao", ""], ["Li", "Qiudan", ""], ["Zeng", "Daniel", ""]]}, {"id": "1303.2277", "submitter": "Guilherme de Castro Mendes Gomes", "authors": "Guilherme de Castro Mendes Gomes, Vitor Campos de Oliveira, Jussara\n  Marques de Almeida and Marcos Andr\\'e Gon\\c{c}alves", "title": "Is Learning to Rank Worth It? A Statistical Analysis of Learning to Rank\n  Methods", "comments": "7 pages, 10 tables, 14 references. Original (short) paper published\n  in the Brazilian Symposium on Databases, 2012 (SBBD2012). Current revision\n  submitted to the Journal of Information and Data Management (JIDM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Learning to Rank (L2R) research field has experienced a fast paced growth\nover the last few years, with a wide variety of benchmark datasets and\nbaselines available for experimentation. We here investigate the main\nassumption behind this field, which is that, the use of sophisticated L2R\nalgorithms and models, produce significant gains over more traditional and\nsimple information retrieval approaches. Our experimental results surprisingly\nindicate that many L2R algorithms, when put up against the best individual\nfeatures of each dataset, may not produce statistically significant\ndifferences, even if the absolute gains may seem large. We also find that most\nof the reported baselines are statistically tied, with no clear winner.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2013 23:28:16 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Gomes", "Guilherme de Castro Mendes", ""], ["de Oliveira", "Vitor Campos", ""], ["de Almeida", "Jussara Marques", ""], ["Gon\u00e7alves", "Marcos Andr\u00e9", ""]]}, {"id": "1303.2308", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Improving adaptation of ubiquitous recommander systems by using\n  reinforcement learning and collaborative filtering", "comments": "arXiv admin note: text overlap with arXiv:1301.4351", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide development of mobile applications provides a considerable amount of\ndata of all types (images, texts, sounds, videos, etc.). Thus, two main issues\nhave to be considered: assist users in finding information and reduce search\nand navigation time. In this sense, context-based recommender systems (CBRS)\npropose the user the adequate information depending on her/his situation. Our\nwork consists in applying machine learning techniques and reasoning process in\norder to bring a solution to some of the problems concerning the acceptance of\nrecommender systems by users, namely avoiding the intervention of experts,\nreducing cold start problem, speeding learning process and adapting to the\nuser's interest. To achieve this goal, we propose a fundamental modification in\nterms of how we model the learning of the CBRS. Inspired by models of human\nreasoning developed in robotic, we combine reinforcement learning and\ncase-based reasoning to define a contextual recommendation process based on\ndifferent context dimensions (cognitive, social, temporal, geographic). This\npaper describes an ongoing work on the implementation of a CBRS based on a\nhybrid Q-learning (HyQL) algorithm which combines Q-learning, collaborative\nfiltering and case-based reasoning techniques. It also presents preliminary\nresults by comparing HyQL and the standard Q-Learning w.r.t. solving the cold\nstart problem.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2013 11:02:24 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 08:20:39 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1303.2438", "submitter": "GuangGang Geng", "authors": "Guang-Gang Geng, Xiu-Tao Yang, Wei Wang, Chi-Jie Meng", "title": "A Taxonomy of Hyperlink Hiding Techniques", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden links are designed solely for search engines rather than visitors. To\nget high search engine rankings, link hiding techniques are usually used for\nthe profitability of black industries, such as illicit game servers, false\nmedical services, illegal gambling, and less attractive high-profit industry,\netc. This paper investigates hyperlink hiding techniques on the Web, and gives\na detailed taxonomy. We believe the taxonomy can help develop appropriate\ncountermeasures. Study on 5,583,451 Chinese sites' home pages indicate that\nlink hidden techniques are very prevalent on the Web. We also tried to explore\nthe attitude of Google towards link hiding spam by analyzing the PageRank\nvalues of relative links. The results show that more should be done to punish\nthe hidden link spam.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 06:49:38 GMT"}, {"version": "v2", "created": "Thu, 3 Apr 2014 06:45:25 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Geng", "Guang-Gang", ""], ["Yang", "Xiu-Tao", ""], ["Wang", "Wei", ""], ["Meng", "Chi-Jie", ""]]}, {"id": "1303.2446", "submitter": "Tobias Kuhn", "authors": "Tobias Kuhn, Paolo Emilio Barbano, Mate Levente Nagy and Michael\n  Krauthammer", "title": "Broadening the Scope of Nanopublications", "comments": "To appear in the Proceedings of the 10th Extended Semantic Web\n  Conference (ESWC 2013)", "journal-ref": null, "doi": "10.1007/978-3-642-38288-8_33", "report-no": "LNCS 7882", "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an approach for extending the existing concept of\nnanopublications --- tiny entities of scientific results in RDF representation\n--- to broaden their application range. The proposed extension uses English\nsentences to represent informal and underspecified scientific claims. These\nsentences follow a syntactic and semantic scheme that we call AIDA (Atomic,\nIndependent, Declarative, Absolute), which provides a uniform and succinct\nrepresentation of scientific assertions. Such AIDA nanopublications are\ncompatible with the existing nanopublication concept and enjoy most of its\nadvantages such as information sharing, interlinking of scientific findings,\nand detailed attribution, while being more flexible and applicable to a much\nwider range of scientific results. We show that users are able to create AIDA\nsentences for given scientific results quickly and at high quality, and that it\nis feasible to automatically extract and interlink AIDA nanopublications from\nexisting unstructured data sources. To demonstrate our approach, a web-based\ninterface is introduced, which also exemplifies the use of nanopublications for\nnon-scientific content, including meta-nanopublications that describe other\nnanopublications.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 08:02:37 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Kuhn", "Tobias", ""], ["Barbano", "Paolo Emilio", ""], ["Nagy", "Mate Levente", ""], ["Krauthammer", "Michael", ""]]}, {"id": "1303.2651", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Hybrid Q-Learning Applied to Ubiquitous recommender system", "comments": "arXiv admin note: substantial text overlap with arXiv:1301.4351,\n  arXiv:1303.2308", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ubiquitous information access becomes more and more important nowadays and\nresearch is aimed at making it adapted to users. Our work consists in applying\nmachine learning techniques in order to bring a solution to some of the\nproblems concerning the acceptance of the system by users. To achieve this, we\npropose a fundamental shift in terms of how we model the learning of\nrecommender system: inspired by models of human reasoning developed in robotic,\nwe combine reinforcement learning and case-base reasoning to define a\nrecommendation process that uses these two approaches for generating\nrecommendations on different context dimensions (social, temporal, geographic).\nWe describe an implementation of the recommender system based on this\nframework. We also present preliminary results from experiments with the system\nand show how our approach increases the recommendation quality.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2013 12:51:03 GMT"}, {"version": "v2", "created": "Sun, 30 Mar 2014 08:26:31 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1303.3164", "submitter": "Uma Sawant", "authors": "Uma Sawant and Soumen Chakrabarti", "title": "Features and Aggregators for Web-scale Entity Search", "comments": "10 pages, 12 figures including tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on two research issues in entity search: scoring a document or\nsnippet that potentially supports a candidate entity, and aggregating scores\nfrom different snippets into an entity score. Proximity scoring has been\nstudied in IR outside the scope of entity search. However, aggregation has been\nhardwired except in a few cases where probabilistic language models are used.\nWe instead explore simple, robust, discriminative ranking algorithms, with\ninformative snippet features and broad families of aggregation functions. Our\nfirst contribution is a study of proximity-cognizant snippet features. In\ncontrast with prior work which uses hardwired \"proximity kernels\" that\nimplement a fixed decay with distance, we present a \"universal\" feature\nencoding which jointly expresses the perplexity (informativeness) of a query\nterm match and the proximity of the match to the entity mention. Our second\ncontribution is a study of aggregation functions. Rather than train the ranking\nalgorithm on snippets and then aggregate scores, we directly train on entities\nsuch that the ranking algorithm takes into account the aggregation function\nbeing used. Our third contribution is an extensive Web-scale evaluation of the\nabove algorithms on two data sets having quite different properties and\nbehavior. The first one is the W3C dataset used in TREC-scale enterprise\nsearch, with pre-annotated entity mentions. The second is a Web-scale\nopen-domain entity search dataset consisting of 500 million Web pages, which\ncontain about 8 billion token spans annotated automatically with two million\nentities from 200,000 entity types in Wikipedia. On the TREC dataset, the\nperformance of our system is comparable to the currently prevalent systems. On\nthe much larger and noisier Web dataset, our system delivers significantly\nbetter performance than all other systems, with 8% MAP improvement over the\nclosest competitor.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 14:06:49 GMT"}], "update_date": "2013-03-14", "authors_parsed": [["Sawant", "Uma", ""], ["Chakrabarti", "Soumen", ""]]}, {"id": "1303.3229", "submitter": "Radu Dragusin", "authors": "Radu Dragusin (1 and 2), Paula Petcu (1 and 3), Christina Lioma (1 and\n  2), Birger Larsen (4), Henrik L. J{\\o}rgensen (5), Ingemar J. Cox (1 and 6),\n  Lars Kai Hansen (1), Peter Ingwersen (4), Ole Winther (1) ((1) DTU Compute,\n  Technical University of Denmark, Denmark, (2) Department of Computer Science,\n  University of Copenhagen, Denmark, (3) Findwise, Copenhagen, Denmark, (4)\n  Information Systems and Interaction Design, Royal School of Library and\n  Information Science, Copenhagen, Denmark, (5) Department of Clinical\n  Biochemistry, Bispebjerg Hospital, Copenhagen, Denmark, (6) Department of\n  Computer Science, University College London, London, United Kingdom)", "title": "FindZebra: A search engine for rare diseases", "comments": null, "journal-ref": "International Journal of Medical Informatics, Available online 23\n  February 2013, ISSN 1386-5056", "doi": "10.1016/j.ijmedinf.2013.01.005", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The web has become a primary information resource about illnesses\nand treatments for both medical and non-medical users. Standard web search is\nby far the most common interface for such information. It is therefore of\ninterest to find out how well web search engines work for diagnostic queries\nand what factors contribute to successes and failures. Among diseases, rare (or\norphan) diseases represent an especially challenging and thus interesting class\nto diagnose as each is rare, diverse in symptoms and usually has scattered\nresources associated with it. Methods: We use an evaluation approach for web\nsearch engines for rare disease diagnosis which includes 56 real life\ndiagnostic cases, state-of-the-art evaluation measures, and curated information\nresources. In addition, we introduce FindZebra, a specialized (vertical) rare\ndisease search engine. FindZebra is powered by open source search technology\nand uses curated freely available online medical information. Results:\nFindZebra outperforms Google Search in both default setup and customised to the\nresources used by FindZebra. We extend FindZebra with specialized\nfunctionalities exploiting medical ontological information and UMLS medical\nconcepts to demonstrate different ways of displaying the retrieved results to\nmedical experts. Conclusions: Our results indicate that a specialized search\nengine can improve the diagnostic quality without compromising the ease of use\nof the currently widely popular web search engines. The proposed evaluation\napproach can be valuable for future development and benchmarking. The FindZebra\nsearch engine is available at http://www.findzebra.com/.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 17:47:57 GMT"}], "update_date": "2013-03-14", "authors_parsed": [["Dragusin", "Radu", "", "1 and 2"], ["Petcu", "Paula", "", "1 and 3"], ["Lioma", "Christina", "", "1 and\n  2"], ["Larsen", "Birger", "", "1 and 6"], ["J\u00f8rgensen", "Henrik L.", "", "1 and 6"], ["Cox", "Ingemar J.", "", "1 and 6"], ["Hansen", "Lars Kai", ""], ["Ingwersen", "Peter", ""], ["Winther", "Ole", ""]]}, {"id": "1303.3964", "submitter": "Mahyuddin K. M.  Nasution", "authors": "Mahyuddin K. M. Nasution", "title": "Simple Search Engine Model: Selective Properties", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the relationship between query and search engine by\nexploring the selective properties based on a simple search engine. We used the\nset theory and utilized the words and terms for defining singleton and\ndoubleton in the event spaces and then provided their implementation for\nproving the existence of the shadow of micro-cluster.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2013 10:21:33 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Nasution", "Mahyuddin K. M.", ""]]}, {"id": "1303.4087", "submitter": "Rafi Muhammad", "authors": "Muhammad Rafi, Mohammad Shahid Shaikh", "title": "An improved semantic similarity measure for document clustering based on\n  topic maps", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major computational burden, while performing document clustering, is the\ncalculation of similarity measure between a pair of documents. Similarity\nmeasure is a function that assigns a real number between 0 and 1 to a pair of\ndocuments, depending upon the degree of similarity between them. A value of\nzero means that the documents are completely dissimilar whereas a value of one\nindicates that the documents are practically identical. Traditionally,\nvector-based models have been used for computing the document similarity. The\nvector-based models represent several features present in documents. These\napproaches to similarity measures, in general, cannot account for the semantics\nof the document. Documents written in human languages contain contexts and the\nwords used to describe these contexts are generally semantically related.\nMotivated by this fact, many researchers have proposed seman-tic-based\nsimilarity measures by utilizing text annotation through external thesauruses\nlike WordNet (a lexical database). In this paper, we define a semantic\nsimilarity measure based on documents represented in topic maps. Topic maps are\nrapidly becoming an industrial standard for knowledge representation with a\nfocus for later search and extraction. The documents are transformed into a\ntopic map based coded knowledge and the similarity between a pair of documents\nis represented as a correlation between the common patterns (sub-trees). The\nexperimental studies on the text mining datasets reveal that this new\nsimilarity measure is more effective as compared to commonly used similarity\nmeasures in text clustering.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2013 18:28:02 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Rafi", "Muhammad", ""], ["Shaikh", "Mohammad Shahid", ""]]}, {"id": "1303.4402", "submitter": "Julian McAuley", "authors": "Julian McAuley and Jure Leskovec", "title": "From Amateurs to Connoisseurs: Modeling the Evolution of User Expertise\n  through Online Reviews", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending products to consumers means not only understanding their tastes,\nbut also understanding their level of experience. For example, it would be a\nmistake to recommend the iconic film Seven Samurai simply because a user enjoys\nother action movies; rather, we might conclude that they will eventually enjoy\nit -- once they are ready. The same is true for beers, wines, gourmet foods --\nor any products where users have acquired tastes: the `best' products may not\nbe the most `accessible'. Thus our goal in this paper is to recommend products\nthat a user will enjoy now, while acknowledging that their tastes may have\nchanged over time, and may change again in the future. We model how tastes\nchange due to the very act of consuming more products -- in other words, as\nusers become more experienced. We develop a latent factor recommendation system\nthat explicitly accounts for each user's level of experience. We find that such\na model not only leads to better recommendations, but also allows us to study\nthe role of user experience and expertise on a novel dataset of fifteen million\nbeer, wine, food, and movie reviews.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 20:01:19 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["McAuley", "Julian", ""], ["Leskovec", "Jure", ""]]}, {"id": "1303.4702", "submitter": "Ed Summers", "authors": "Thomas Steiner, Seth van Hooland, Ed Summers", "title": "MJ no more: Using Concurrent Wikipedia Edit Spikes with Social Network\n  Plausibility Checks for Breaking News Detection", "comments": null, "journal-ref": "Proceedings of the 22nd international conference on World Wide Web\n  companion (WWW '13 Companion), 2013. International World Wide Web Conferences\n  Steering Committee, Republic and Canton of Geneva, Switzerland, 791-794", "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We have developed an application called Wikipedia Live Monitor that monitors\narticle edits on different language versions of Wikipedia, as they happen in\nrealtime. Wikipedia articles in different languages are highly interlinked. For\nexample, the English article en:2013_Russian_meteor_event on the topic of the\nFebruary 15 meteoroid that exploded over the region of Chelyabinsk Oblast,\nRussia, is interlinked with the Russian article on the same topic. As we\nmonitor multiple language versions of Wikipedia in parallel, we can exploit\nthis fact to detect concurrent edit spikes of Wikipedia articles covering the\nsame topics, both in only one, and in different languages. We treat such\nconcurrent edit spikes as signals for potential breaking news events, whose\nplausibility we then check with full-text cross-language searches on multiple\nsocial networks. Unlike the reverse approach of monitoring social networks\nfirst, and potentially checking plausibility on Wikipedia second, the approach\nproposed in this paper has the advantage of being less prone to false-positive\nalerts, while being equally sensitive to true-positive events, however, at only\na fraction of the processing cost.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2013 19:14:45 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2013 09:08:57 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2013 16:12:45 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Steiner", "Thomas", ""], ["van Hooland", "Seth", ""], ["Summers", "Ed", ""]]}, {"id": "1303.5250", "submitter": "Marc Sloan Mr", "authors": "Marc Sloan and Jun Wang", "title": "Iterative Expectation for Multi Period Information Retrieval", "comments": "8 pages, 3 tables, published at the Workshop on Web Search Click Data\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Information Retrieval (IR) models make use of offline statistical\ntechniques to score documents for ranking over a single period, rather than use\nan online, dynamic system that is responsive to users over time. In this paper,\nwe explicitly formulate a general Multi Period Information Retrieval problem,\nwhere we consider retrieval as a stochastic yet controllable process. The\nranking action during the process continuously controls the retrieval system's\ndynamics, and an optimal ranking policy is found in order to maximise the\noverall users' satisfaction over the multiple periods as much as possible. Our\nderivations show interesting properties about how the posterior probability of\nthe documents relevancy evolves from users feedbacks through clicks, and\nprovides a plug-in framework for incorporating different click models. Based on\nthe Multi-Armed Bandit theory, we propose a simple implementation of our\nframework using a dynamic ranking rule that takes rank bias and exploration of\ndocuments into account. We use TREC data to learn a suitable exploration\nparameter for our model, and then analyse its performance and a number of\nvariants using a search log data set; the experiments suggest an ability to\nexplore document relevance dynamically over time using user feedback in a way\nthat can handle rank bias.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 13:11:24 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Sloan", "Marc", ""], ["Wang", "Jun", ""]]}, {"id": "1303.5367", "submitter": "Piotr Dendek", "authors": "Piotr Jan Dendek and Artur Czeczko and Mateusz Fedoryszak and Adam\n  Kawa and Piotr Wendykier and Lukasz Bolikowski", "title": "Taming the zoo - about algorithms implementation in the ecosystem of\n  Apache Hadoop", "comments": "This paper (with changed content) appeared under the title \"Content\n  Analysis of Scientific Articles in Apache Hadoop Ecosystem\" in \"Intelligent\n  Tools for Building a Scientific Information Platform: From Research to\n  Implementation\", \"Studies in Computational Intelligence\", Volume 541, 2014,\n  http://link.springer.com/book/10.1007/978-3-319-04714-0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content Analysis System (CoAnSys) is a research framework for mining\nscientific publications using Apache Hadoop. This article describes the\nalgorithms currently implemented in CoAnSys including classification,\ncategorization and citation matching of scientific publications. The size of\nthe input data classifies these algorithms in the range of big data problems,\nwhich can be efficiently solved on Hadoop clusters.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 18:56:06 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2013 10:41:16 GMT"}, {"version": "v3", "created": "Sun, 16 Mar 2014 22:27:02 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Dendek", "Piotr Jan", ""], ["Czeczko", "Artur", ""], ["Fedoryszak", "Mateusz", ""], ["Kawa", "Adam", ""], ["Wendykier", "Piotr", ""], ["Bolikowski", "Lukasz", ""]]}, {"id": "1303.5867", "submitter": "Srikantaiah K C", "authors": "Srikantaiah K C, Suraj M, Venugopal K R, L M Patnaik", "title": "Similarity based Dynamic Web Data Extraction and Integration System from\n  Search Engine Result Pages for Web Content Mining", "comments": "8 pages", "journal-ref": "ACEEE International Journal on Information Technology, Volume 3,\n  Issue 1, 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an explosive growth of information in the World Wide Web thus posing\na challenge to Web users to extract essential knowledge from the Web. Search\nengines help us to narrow down the search in the form of Search Engine Result\nPages (SERP). Web Content Mining is one of the techniques that help users to\nextract useful information from these SERPs. In this paper, we propose two\nsimilarity based mechanisms; WDES, to extract desired SERPs and store them in\nthe local depository for offline browsing and WDICS, to integrate the requested\ncontents and enable the user to perform the intended analysis and extract the\ndesired information. Our experimental results show that WDES and WDICS\noutperform DEPTA [1] in terms of Precision and Recall.\n", "versions": [{"version": "v1", "created": "Sat, 23 Mar 2013 17:40:32 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["C", "Srikantaiah K", ""], ["M", "Suraj", ""], ["R", "Venugopal K", ""], ["Patnaik", "L M", ""]]}, {"id": "1303.5988", "submitter": "Yao Hengshuai", "authors": "Hengshuai Yao and Dale Schuurmans", "title": "Reinforcement Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new framework for web page ranking -- reinforcement ranking --\nthat improves the stability and accuracy of Page Rank while eliminating the\nneed for computing the stationary distribution of random walks. Instead of\nrelying on teleportation to ensure a well defined Markov chain, we develop a\nreverse-time reinforcement learning framework that determines web page\nauthority based on the solution of a reverse Bellman equation. In particular,\nfor a given reward function and surfing policy we recover a well defined\nauthority score from a reverse-time perspective: looking back from a web page,\nwhat is the total incoming discounted reward brought by the surfer from the\npage's predecessors? This results in a novel form of reverse-time\ndynamic-programming/reinforcement-learning problem that achieves several\nadvantages over Page Rank based methods: First, stochasticity, ergodicity, and\nirreducibility of the underlying Markov chain is no longer required for\nwell-posedness. Second, the method is less sensitive to graph topology and more\nstable in the presence of dangling pages. Third, not only does the reverse\nBellman iteration yield a more efficient power iteration, it allows for faster\nupdating in the presence of graph changes. Finally, our experiments demonstrate\nimprovements in ranking quality.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2013 20:28:05 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Yao", "Hengshuai", ""], ["Schuurmans", "Dale", ""]]}, {"id": "1303.6361", "submitter": "Conrad Sanderson", "authors": "Sandra Mau, Shaokang Chen, Conrad Sanderson, Brian C. Lovell", "title": "Video Face Matching using Subset Selection and Clustering of\n  Probabilistic Multi-Region Histograms", "comments": null, "journal-ref": "International Conference of Image and Vision Computing New Zealand\n  (IVCNZ), 2010", "doi": "10.1109/IVCNZ.2010.6148860", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balancing computational efficiency with recognition accuracy is one of the\nmajor challenges in real-world video-based face recognition. A significant\ndesign decision for any such system is whether to process and use all possible\nfaces detected over the video frames, or whether to select only a few \"best\"\nfaces. This paper presents a video face recognition system based on\nprobabilistic Multi-Region Histograms to characterise performance trade-offs\nin: (i) selecting a subset of faces compared to using all faces, and (ii)\ncombining information from all faces via clustering. Three face selection\nmetrics are evaluated for choosing a subset: face detection confidence, random\nsubset, and sequential selection. Experiments on the recently introduced MOBIO\ndataset indicate that the usage of all faces through clustering always\noutperformed selecting only a subset of faces. The experiments also show that\nthe face selection metric based on face detection confidence generally provides\nbetter recognition performance than random or sequential sampling. Moreover,\nthe optimal number of faces varies drastically across selection metric and\nsubsets of MOBIO. Given the trade-offs between computational effort,\nrecognition accuracy and robustness, it is recommended that face feature\nclustering would be most advantageous in batch processing (particularly for\nvideo-based watchlists), whereas face selection methods should be limited to\napplications with significant computational restrictions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 01:34:42 GMT"}], "update_date": "2013-03-27", "authors_parsed": [["Mau", "Sandra", ""], ["Chen", "Shaokang", ""], ["Sanderson", "Conrad", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1303.6369", "submitter": "Qian-Ming Zhang", "authors": "Qian-Ming Zhang, An Zeng, Ming-Sheng Shang", "title": "Extracting the information backbone in online system", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0062624", "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information overload is a serious problem in modern society and many\nsolutions such as recommender system have been proposed to filter out\nirrelevant information. In the literature, researchers mainly dedicated to\nimprove the recommendation performance (accuracy and diversity) of the\nalgorithms while overlooked the influence of topology of the online user-object\nbipartite networks. In this paper, we find that some information provided by\nthe bipartite networks is not only redundant but also misleading. With such\n\"less can be more\" feature, we design some algorithms to improve the\nrecommendation performance by eliminating some links from the original\nnetworks. Moreover, we propose a hybrid method combining the time-aware and\ntopology-aware link removal algorithms to extract the backbone which contains\nthe essential information for the recommender systems. From the practical point\nof view, our method can improve the performance and reduce the computational\ntime of the recommendation system, thus improve both of their effectiveness and\nefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 02:32:15 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Zhang", "Qian-Ming", ""], ["Zeng", "An", ""], ["Shang", "Ming-Sheng", ""]]}, {"id": "1303.6906", "submitter": "Mateusz Fedoryszak", "authors": "Mateusz Fedoryszak, Dominika Tkaczyk and {\\L}ukasz Bolikowski", "title": "Large scale citation matching using Apache Hadoop", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the process of citation matching links from bibliography entries to\nreferenced publications are created. Such links are indicators of topical\nsimilarity between linked texts, are used in assessing the impact of the\nreferenced document and improve navigation in the user interfaces of digital\nlibraries. In this paper we present a citation matching method and show how to\nscale it up to handle great amounts of data using appropriate indexing and a\nMapReduce paradigm in the Hadoop environment.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 16:33:00 GMT"}], "update_date": "2013-03-28", "authors_parsed": [["Fedoryszak", "Mateusz", ""], ["Tkaczyk", "Dominika", ""], ["Bolikowski", "\u0141ukasz", ""]]}, {"id": "1303.7149", "submitter": "Andre Vellino", "authors": "Andr\\'e Vellino", "title": "Usage-based vs. Citation-based Methods for Recommending Scholarly\n  Research Articles", "comments": "4 pages, 4 figures, ACM Recommender Systems Workshop 2012, Dublin\n  Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two principal data sources for collaborative filtering recommenders\nin scholarly digital libraries: usage data obtained from harvesting a large,\ndistributed collection of Open URL web logs and citation data obtained from the\njournal articles. This study explores the characteristics of recommendations\ngenerated by implementations of these two methods: the 'bX' system by ExLibris\nand an experimental citation-based recommender, Sarkanto. Recommendations from\neach system were compared according to their semantic similarity to the seed\narticle that was used to generate them. Since the full text of the articles was\nnot available for all the recommendations in both systems, the semantic\nsimilarity between the seed article and the recommended articles was deemed to\nbe the semantic distance between the journals in which the articles were\npublished. The semantic distance between journals was computed from the\n\"semantic vectors\" distance between all the terms in the full-text of the\navailable articles in that journal and this study shows that citation-based\nrecommendations are more semantically diverse than usage-based ones. These\nrecommenders are complementary since most of the time, when one recommender\nproduces recommendations the other does not.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 15:27:16 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2013 02:21:53 GMT"}], "update_date": "2013-04-01", "authors_parsed": [["Vellino", "Andr\u00e9", ""]]}, {"id": "1303.7264", "submitter": "Yaojia Zhu", "authors": "Yaojia Zhu, Xiaoran Yan, Lise Getoor and Cristopher Moore", "title": "Scalable Text and Link Analysis with Mixed-Topic Link Models", "comments": "11 pages, 4 figures", "journal-ref": "Proc. 19th SIGKDD Conference on Knowledge Discovery and Data\n  Mining (KDD) 2013, 473-481", "doi": "10.1145/2487575.2487693", "report-no": null, "categories": "cs.LG cs.IR cs.SI physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data sets contain rich information about objects, as well as pairwise\nrelations between them. For instance, in networks of websites, scientific\npapers, and other documents, each node has content consisting of a collection\nof words, as well as hyperlinks or citations to other nodes. In order to\nperform inference on such data sets, and make predictions and recommendations,\nit is useful to have models that are able to capture the processes which\ngenerate the text at each node and the links between them. In this paper, we\ncombine classic ideas in topic modeling with a variant of the mixed-membership\nblock model recently developed in the statistical physics community. The\nresulting model has the advantage that its parameters, including the mixture of\ntopics of each document and the resulting overlapping communities, can be\ninferred with a simple and scalable expectation-maximization algorithm. We test\nour model on three data sets, performing unsupervised topic classification and\nlink prediction. For both tasks, our model outperforms several existing\nstate-of-the-art methods, achieving higher accuracy with significantly less\ncomputation, analyzing a data set with 1.3 million words and 44 thousand links\nin a few minutes.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2013 22:34:51 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Zhu", "Yaojia", ""], ["Yan", "Xiaoran", ""], ["Getoor", "Lise", ""], ["Moore", "Cristopher", ""]]}, {"id": "1303.7310", "submitter": "Niraj Kumar", "authors": "Niraj Kumar, Rashmi Gangadharaiah, Kannan Srinathan and Vasudeva Varma", "title": "Exploring the Role of Logically Related Non-Question Phrases for\n  Answering Why-Questions", "comments": "Got accepted in NLDB-2013; as Paper ID: 23; Title: \"Exploring the\n  Role of Logically Related Non-Question Phrases for Answering Why-Questions\",\n  Withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we show that certain phrases although not present in a given\nquestion/query, play a very important role in answering the question. Exploring\nthe role of such phrases in answering questions not only reduces the dependency\non matching question phrases for extracting answers, but also improves the\nquality of the extracted answers. Here matching question phrases means phrases\nwhich co-occur in given question and candidate answers. To achieve the above\ndiscussed goal, we introduce a bigram-based word graph model populated with\nsemantic and topical relatedness of terms in the given document. Next, we apply\nan improved version of ranking with a prior-based approach, which ranks all\nwords in the candidate document with respect to a set of root words (i.e.\nnon-stopwords present in the question and in the candidate document). As a\nresult, terms logically related to the root words are scored higher than terms\nthat are not related to the root words. Experimental results show that our\ndevised system performs better than state-of-the-art for the task of answering\nWhy-questions.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 06:31:40 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Kumar", "Niraj", ""], ["Gangadharaiah", "Rashmi", ""], ["Srinathan", "Kannan", ""], ["Varma", "Vasudeva", ""]]}]