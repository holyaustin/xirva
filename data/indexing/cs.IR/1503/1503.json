[{"id": "1503.00244", "submitter": "Kamran Kowsari", "authors": "Nima Bari, Roman Vichr, Kamran Kowsari, Simon Y. Berkovich", "title": "23-bit Metaknowledge Template Towards Big Data Knowledge Discovery and\n  Management", "comments": "IEEE Data Science and Advanced Analytics (DSAA'2014)", "journal-ref": null, "doi": "10.1109/DSAA.2014.7058121", "report-no": null, "categories": "cs.DB cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global influence of Big Data is not only growing but seemingly endless.\nThe trend is leaning towards knowledge that is attained easily and quickly from\nmassive pools of Big Data. Today we are living in the technological world that\nDr. Usama Fayyad and his distinguished research fellows discussed in the\nintroductory explanations of Knowledge Discovery in Databases (KDD) predicted\nnearly two decades ago. Indeed, they were precise in their outlook on Big Data\nanalytics. In fact, the continued improvement of the interoperability of\nmachine learning, statistics, database building and querying fused to create\nthis increasingly popular science- Data Mining and Knowledge Discovery. The\nnext generation computational theories are geared towards helping to extract\ninsightful knowledge from even larger volumes of data at higher rates of speed.\nAs the trend increases in popularity, the need for a highly adaptive solution\nfor knowledge discovery will be necessary. In this research paper, we are\nintroducing the investigation and development of 23 bit-questions for a\nMetaknowledge template for Big Data Processing and clustering purposes. This\nresearch aims to demonstrate the construction of this methodology and proves\nthe validity and the beneficial utilization that brings Knowledge Discovery\nfrom Big Data.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 09:41:11 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Bari", "Nima", ""], ["Vichr", "Roman", ""], ["Kowsari", "Kamran", ""], ["Berkovich", "Simon Y.", ""]]}, {"id": "1503.00245", "submitter": "Kamran Kowsari", "authors": "Nima Bari, Roman Vichr, Kamran Kowsari, Simon Y. Berkovich", "title": "Novel Metaknowledge-based Processing Technique for Multimedia Big Data\n  clustering challenges", "comments": "IEEE Multimedia Big Data (BigMM 2015)", "journal-ref": null, "doi": "10.1109/BigMM.2015.78", "report-no": null, "categories": "cs.DB cs.AI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past research has challenged us with the task of showing relational patterns\nbetween text-based data and then clustering for predictive analysis using Golay\nCode technique. We focus on a novel approach to extract metaknowledge in\nmultimedia datasets. Our collaboration has been an on-going task of studying\nthe relational patterns between datapoints based on metafeatures extracted from\nmetaknowledge in multimedia datasets. Those selected are significant to suit\nthe mining technique we applied, Golay Code algorithm. In this research paper\nwe summarize findings in optimization of metaknowledge representation for\n23-bit representation of structured and unstructured multimedia data in order\nto\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 09:53:15 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Bari", "Nima", ""], ["Vichr", "Roman", ""], ["Kowsari", "Kamran", ""], ["Berkovich", "Simon Y.", ""]]}, {"id": "1503.00303", "submitter": "Xin Luna Dong", "authors": "Xian Li and Xin Luna Dong and Kenneth Lyons and Weiyi Meng and Divesh\n  Srivastava", "title": "Truth Finding on the Deep Web: Is the Problem Solved?", "comments": "VLDB'2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of useful information available on the Web has been growing at a\ndramatic pace in recent years and people rely more and more on the Web to\nfulfill their information needs. In this paper, we study truthfulness of Deep\nWeb data in two domains where we believed data are fairly clean and data\nquality is important to people's lives: {\\em Stock} and {\\em Flight}. To our\nsurprise, we observed a large amount of inconsistency on data from different\nsources and also some sources with quite low accuracy. We further applied on\nthese two data sets state-of-the-art {\\em data fusion} methods that aim at\nresolving conflicts and finding the truth, analyzed their strengths and\nlimitations, and suggested promising research directions. We wish our study can\nincrease awareness of the seriousness of conflicting data on the Web and in\nturn inspire more research in our community to tackle this problem.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 16:47:30 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Li", "Xian", ""], ["Dong", "Xin Luna", ""], ["Lyons", "Kenneth", ""], ["Meng", "Weiyi", ""], ["Srivastava", "Divesh", ""]]}, {"id": "1503.00439", "submitter": "Dr. Deepali Virmani", "authors": "Savneet Kaur, Deepali Virmani, Satbir Jain", "title": "A Novel Framework for Intelligent Information Retrieval in Wireless\n  Sensor Networks", "comments": "5 pages, 4 figures; ERCICA 2014 - Emerging Research in Computing,\n  Information, Communication and Applications (Vol 2), Elsevier Science and\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the development of the low-cost, power-efficient embedded\ndevices, coupled with the rising need for support of new information processing\nparadigms such as smart spaces and military surveillance systems, have led to\nactive research in large-scale, highly distributed sensor networks of small,\nwireless, low-power, unattended sensors and actuators. While applications keep\ndiversifying, one common property they share is the need for an efficient\nnetwork architecture tailored towards information retrieval in sensor networks.\nPrevious solutions designed for traditional networks serve as good references;\nhowever, due to the vast differences between previous paradigms and needs of\nsensor networks, a framework is required to gather and impart only the required\ninformation .To achieve this goal in this paper we have proposed a framework\nfor intelligent information retrieval and dissemination to desired destination\nnode. The proposed frame work combines three major concern areas in WSNs i.e.\ndata aggregation, information retrieval and data dissemination in a single\nscenario. In the proposed framework data aggregation is responsible for\ncombining information from all nodes and removing the redundant data.\nInformation retrieval filters the processed data to obtain final information\ntermed as intelligent data to be disseminated to the required destination node.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 08:40:08 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Kaur", "Savneet", ""], ["Virmani", "Deepali", ""], ["Jain", "Satbir", ""]]}, {"id": "1503.00841", "submitter": "Biao Liu", "authors": "Biao Liu, Minlie Huang", "title": "Robustly Leveraging Prior Knowledge in Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Prior knowledge has been shown very useful to address many natural language\nprocessing tasks. Many approaches have been proposed to formalise a variety of\nknowledge, however, whether the proposed approach is robust or sensitive to the\nknowledge supplied to the model has rarely been discussed. In this paper, we\npropose three regularization terms on top of generalized expectation criteria,\nand conduct extensive experiments to justify the robustness of the proposed\nmethods. Experimental results demonstrate that our proposed methods obtain\nremarkable improvements and are much more robust than baselines.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 06:59:28 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Liu", "Biao", ""], ["Huang", "Minlie", ""]]}, {"id": "1503.01082", "submitter": "Philipp Mayr", "authors": "Zeljko Carevic, Thomas Krichel, Philipp Mayr", "title": "Assessing a human mediated current awareness service", "comments": "12 pages, 5 figures, accepted paper at the 14th International\n  Symposium of Information Science (ISI 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an approach for analyzing the behavior of editors\nin the large current awareness service \"NEP: New Economics Papers\". We\nprocessed data from more than 38,000 issues derived from 90 different NEP\nreports over the past ten years. The aim of our analysis was to gain an inside\nto the editor behaviour when creating an issue and to look for factors that\ninfluence the success of a report. In our study we looked at the following\nfeatures: average editing time, the average number of papers in an issue and\nthe editor effort measured on presorted issues as relative search length (RSL).\nWe found an average issue size of 12.4 documents per issue. The average editing\ntime is rather low with 14.5 minute. We get to the point that the success of a\nreport is mainly driven by its topic and the number of subscribers, as well as\nproactive action by the editor to promote the report in her community.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 20:03:00 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2015 12:44:08 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Carevic", "Zeljko", ""], ["Krichel", "Thomas", ""], ["Mayr", "Philipp", ""]]}, {"id": "1503.01549", "submitter": "Wesam Elshamy", "authors": "William Hsu, Mohammed Abduljabbar, Ryuichi Osuga, Max Lu, Wesam\n  Elshamy", "title": "Visualization of Clandestine Labs from Seizure Reports: Thematic Mapping\n  and Data Mining Research Directions", "comments": "In Proceedings of The 2nd European Workshop on Human-Computer\n  Interaction and Information Retrieval EuroHCIR2012, pages 43--46, Nijmegen,\n  the Netherlands, 24th/25th August 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of spatiotemporal event visualization based on reports entails\nsubtasks ranging from named entity recognition to relationship extraction and\nmapping of events. We present an approach to event extraction that is driven by\ndata mining and visualization goals, particularly thematic mapping and trend\nanalysis. This paper focuses on bridging the information extraction and\nvisualization tasks and investigates topic modeling approaches. We develop a\nstatic, finite topic model and examine the potential benefits and feasibility\nof extending this to dynamic topic modeling with a large number of topics and\ncontinuous time. We describe an experimental test bed for event mapping that\nuses this end-to-end information retrieval system, and report preliminary\nresults on a geoinformatics problem: tracking of methamphetamine lab seizure\nevents across time and space.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 06:22:15 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Hsu", "William", ""], ["Abduljabbar", "Mohammed", ""], ["Osuga", "Ryuichi", ""], ["Lu", "Max", ""], ["Elshamy", "Wesam", ""]]}, {"id": "1503.01558", "submitter": "Jonathan Malmaud", "authors": "Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick Johnston, Andrew\n  Rabinovich, and Kevin Murphy", "title": "What's Cookin'? Interpreting Cooking Videos using Text, Speech and\n  Vision", "comments": "To appear in NAACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for aligning a sequence of instructions to a video\nof someone carrying out a task. In particular, we focus on the cooking domain,\nwhere the instructions correspond to the recipe. Our technique relies on an HMM\nto align the recipe steps to the (automatically generated) speech transcript.\nWe then refine this alignment using a state-of-the-art visual food detector,\nbased on a deep convolutional neural network. We show that our technique\noutperforms simpler techniques based on keyword spotting. It also enables\ninteresting applications, such as automatically illustrating recipes with\nkeyframes, and searching within a video for events of interest.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 07:07:48 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2015 04:11:49 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2015 18:55:22 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Malmaud", "Jonathan", ""], ["Huang", "Jonathan", ""], ["Rathod", "Vivek", ""], ["Johnston", "Nick", ""], ["Rabinovich", "Andrew", ""], ["Murphy", "Kevin", ""]]}, {"id": "1503.01647", "submitter": "Zhangyang Wang", "authors": "Zhangyang Wang, Xianming Liu, Shiyu Chang, Jiayu Zhou, Guo-Jun Qi,\n  Thomas S. Huang", "title": "Decentralized Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a decentralized recommender system by formulating the\npopular collaborative filleting (CF) model into a decentralized matrix\ncompletion form over a set of users. In such a way, data storages and\ncomputations are fully distributed. Each user could exchange limited\ninformation with its local neighborhood, and thus it avoids the centralized\nfusion. Advantages of the proposed system include a protection on user privacy,\nas well as better scalability and robustness. We compare our proposed algorithm\nwith several state-of-the-art algorithms on the FlickerUserFavor dataset, and\ndemonstrate that the decentralized algorithm can gain a competitive performance\nto others.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 14:34:02 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Wang", "Zhangyang", ""], ["Liu", "Xianming", ""], ["Chang", "Shiyu", ""], ["Zhou", "Jiayu", ""], ["Qi", "Guo-Jun", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1503.02781", "submitter": "Jonathan Tuke", "authors": "Matthew Roughan, Jonathan Tuke", "title": "Unravelling Graph-Exchange File Formats", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A graph is used to represent data in which the relationships between the\nobjects in the data are at least as important as the objects themselves. Over\nthe last two decades nearly a hundred file formats have been proposed or used\nto provide portable access to such data. This paper seeks to review these\nformats, and provide some insight to both reduce the ongoing creation of\nunnecessary formats, and guide the development of new formats where needed.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 06:23:56 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Roughan", "Matthew", ""], ["Tuke", "Jonathan", ""]]}, {"id": "1503.02801", "submitter": "Jiaming Xu", "authors": "Jiaming Xu, Bo Xu, Guanhua Tian, Jun Zhao, Fangyuan Wang, Hongwei Hao", "title": "Short Text Hashing Improved by Integrating Multi-Granularity Topics and\n  Tags", "comments": "12 pages, accepted at CICLing 2015", "journal-ref": null, "doi": "10.1007/978-3-319-18111-0_33", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to computational and storage efficiencies of compact binary codes,\nhashing has been widely used for large-scale similarity search. Unfortunately,\nmany existing hashing methods based on observed keyword features are not\neffective for short texts due to the sparseness and shortness. Recently, some\nresearchers try to utilize latent topics of certain granularity to preserve\nsemantic similarity in hash codes beyond keyword matching. However, topics of\ncertain granularity are not adequate to represent the intrinsic semantic\ninformation. In this paper, we present a novel unified approach for short text\nHashing using Multi-granularity Topics and Tags, dubbed HMTT. In particular, we\npropose a selection method to choose the optimal multi-granularity topics\ndepending on the type of dataset, and design two distinct hashing strategies to\nincorporate multi-granularity topics. We also propose a simple and effective\nmethod to exploit tags to enhance the similarity of related texts. We carry out\nextensive experiments on one short text dataset as well as on one normal text\ndataset. The results demonstrate that our approach is effective and\nsignificantly outperforms baselines on several evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 07:51:59 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Xu", "Jiaming", ""], ["Xu", "Bo", ""], ["Tian", "Guanhua", ""], ["Zhao", "Jun", ""], ["Wang", "Fangyuan", ""], ["Hao", "Hongwei", ""]]}, {"id": "1503.03168", "submitter": "Kalyani Desikan", "authors": "G. Hannah Grace, Kalyani Desikan", "title": "Experimental Estimation of Number of Clusters Based on Cluster Quality", "comments": "12 pages, 9 figures", "journal-ref": "Journal of mathematics and computer science, Vol12 (2014), 304-315", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text Clustering is a text mining technique which divides the given set of\ntext documents into significant clusters. It is used for organizing a huge\nnumber of text documents into a well-organized form. In the majority of the\nclustering algorithms, the number of clusters must be specified apriori, which\nis a drawback of these algorithms. The aim of this paper is to show\nexperimentally how to determine the number of clusters based on cluster\nquality. Since partitional clustering algorithms are well-suited for clustering\nlarge document datasets, we have confined our analysis to a partitional\nclustering algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 10:34:06 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Grace", "G. Hannah", ""], ["Desikan", "Kalyani", ""]]}, {"id": "1503.03606", "submitter": "Nagaraja S", "authors": "Nagaraja S. and Prabhakar C.J.", "title": "Low-Level Features for Image Retrieval Based on Extraction of\n  Directional Binary Patterns and Its Oriented Gradients Histogram", "comments": "7 Figures, 5 Tables 16 Pages in Computer Applications: An\n  International Journal (CAIJ), Vol.2, No.1, February 2015", "journal-ref": null, "doi": "10.5121/caij.2015.2102", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach for image retrieval based on\nextraction of low level features using techniques such as Directional Binary\nCode, Haar Wavelet transform and Histogram of Oriented Gradients. The DBC\ntexture descriptor captures the spatial relationship between any pair of\nneighbourhood pixels in a local region along a given direction, while Local\nBinary Patterns descriptor considers the relationship between a given pixel and\nits surrounding neighbours. Therefore, DBC captures more spatial information\nthan LBP and its variants, also it can extract more edge information than LBP.\nHence, we employ DBC technique in order to extract grey level texture feature\nfrom each RGB channels individually and computed texture maps are further\ncombined which represents colour texture features of an image. Then, we\ndecomposed the extracted colour texture map and original image using Haar\nwavelet transform. Finally, we encode the shape and local features of wavelet\ntransformed images using Histogram of Oriented Gradients for content based\nimage retrieval. The performance of proposed method is compared with existing\nmethods on two databases such as Wang's corel image and Caltech 256. The\nevaluation results show that our approach outperforms the existing methods for\nimage retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 06:45:01 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["S.", "Nagaraja", ""], ["J.", "Prabhakar C.", ""]]}, {"id": "1503.03607", "submitter": "Najva Izadpanah", "authors": "Najva Izadpanah", "title": "A divisive hierarchical clustering-based method for indexing image\n  information", "comments": null, "journal-ref": "Signal & Image Processing : An International Journal (SIPIJ)\n  Vol.6, No.1, February 2015", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most practical applications of image retrieval, high-dimensional feature\nvectors are required, but current multi-dimensional indexing structures lose\ntheir efficiency with growth of dimensions. Our goal is to propose a divisive\nhierarchical clustering-based multi-dimensional indexing structure which is\nefficient in high-dimensional feature spaces. A projection pursuit method has\nbeen used for finding a component of the data, which data's projections onto it\nmaximizes the approximation of negentropy for preparing essential information\nin order to partitioning of the data space. Various tests and experimental\nresults on high-dimensional datasets indicate the performance of proposed\nmethod in comparison with others.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 06:51:06 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Izadpanah", "Najva", ""]]}, {"id": "1503.03650", "submitter": "Weiqing Wang", "authors": "Weiqing Wang, Hongzhi Yin, Ling Chen, Yizhou Sun, Shazia Sadiq,\n  Xiaofang Zhou", "title": "Geo-SAGE: A Geographical Sparse Additive Generative Model for Spatial\n  Item Recommendation", "comments": "10 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of location-based social networks (LBSNs), spatial\nitem recommendation has become an important means to help people discover\nattractive and interesting venues and events, especially when users travel out\nof town. However, this recommendation is very challenging compared to the\ntraditional recommender systems. A user can visit only a limited number of\nspatial items, leading to a very sparse user-item matrix. Most of the items\nvisited by a user are located within a short distance from where he/she lives,\nwhich makes it hard to recommend items when the user travels to a far away\nplace. Moreover, user interests and behavior patterns may vary dramatically\nacross different geographical regions. In light of this, we propose Geo-SAGE, a\ngeographical sparse additive generative model for spatial item recommendation\nin this paper. Geo-SAGE considers both user personal interests and the\npreference of the crowd in the target region, by exploiting both the\nco-occurrence pattern of spatial items and the content of spatial items. To\nfurther alleviate the data sparsity issue, Geo-SAGE exploits the geographical\ncorrelation by smoothing the crowd's preferences over a well-designed spatial\nindex structure called spatial pyramid. We conduct extensive experiments to\nevaluate the performance of our Geo-SAGE model on two real large-scale\ndatasets. The experimental results clearly demonstrate our Geo-SAGE model\noutperforms the state-of-the-art in the two tasks of both out-of-town and\nhome-town recommendations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 09:44:11 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Wang", "Weiqing", ""], ["Yin", "Hongzhi", ""], ["Chen", "Ling", ""], ["Sun", "Yizhou", ""], ["Sadiq", "Shazia", ""], ["Zhou", "Xiaofang", ""]]}, {"id": "1503.03660", "submitter": "Zeon Trevor Fernando", "authors": "Zeon Trevor Fernando", "title": "Capturing, Documenting and Visualizing Search Contexts for building\n  Multimedia Corpora", "comments": "Undergraduate (B.Tech Hons, Computer Science) Thesis Report, 2014,\n  Vellore Institute of Technology, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Social Science research, multimedia documents are often collected to\nanswer particular research questions like: \"Which of the aesthetic properties\nof a photo are considered important on the web\" or \"How has Street Art\ndeveloped over the past 50 years\". Therefore, a researcher generally issues\nmultiple queries to a number of search engines. This activity may span over\nlong time intervals and results in a collection which can be further analyzed.\nDocumenting the collection building process which includes the context of the\ncarried out searches is imperative for social scientists to reproduce their\nresearch. Such context documentation consists of several user actions and\nsearch attributes like: the issued queries; the results clicked and saved;\nduration a particular result was viewed for; the set of results that was\ndisplayed but neither clicked, nor saved; as well as user annotations like\ncomments or tags. In this work we will describe a search process tracking\nmodule and a search history visualization module. These modules can be\nintegrated into keyword based search systems through a REST API which was\ndeveloped to help capture, document and revisit past search contexts while\nbuilding a web corpora. Finally, we detail the implementation of how the module\nwas integrated into the LearnWeb2.0 platform - a multimedia web2.0 search and\nsharing application which can obtain resources from various web2.0 tools such\nas Youtube, Bing, Flickr, etc using keyword search.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 10:11:59 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Fernando", "Zeon Trevor", ""]]}, {"id": "1503.03701", "submitter": "Alessandro Perina", "authors": "Nebojsa Jojic and Alessandro Perina and Dongwoo Kim", "title": "Hierarchical learning of grids of microtopics", "comments": "To Appear in Uncertainty in Artificial Intelligence - UAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The counting grid is a grid of microtopics, sparse word/feature\ndistributions. The generative model associated with the grid does not use these\nmicrotopics individually. Rather, it groups them in overlapping rectangular\nwindows and uses these grouped microtopics as either mixture or admixture\ncomponents. This paper builds upon the basic counting grid model and it shows\nthat hierarchical reasoning helps avoid bad local minima, produces better\nclassification accuracy and, most interestingly, allows for extraction of large\nnumbers of coherent microtopics even from small datasets. We evaluate this in\nterms of consistency, diversity and clarity of the indexed content, as well as\nin a user study on word intrusion tasks. We demonstrate that these models work\nwell as a technique for embedding raw images and discuss interesting parallels\nbetween hierarchical CG models and other deep architectures.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 12:59:25 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 16:38:24 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2015 16:46:07 GMT"}, {"version": "v4", "created": "Wed, 8 Jun 2016 15:05:38 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Jojic", "Nebojsa", ""], ["Perina", "Alessandro", ""], ["Kim", "Dongwoo", ""]]}, {"id": "1503.03753", "submitter": "Senjuti Basu Roy", "authors": "Senjuti Basu Roy, Laks V. S. Lakshmanan, Rui Liu", "title": "From Group Recommendations to Group Formation", "comments": "14 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant recent interest in the area of group\nrecommendations, where, given groups of users of a recommender system, one\nwants to recommend top-k items to a group that maximize the satisfaction of the\ngroup members, according to a chosen semantics of group satisfaction. Examples\nsemantics of satisfaction of a recommended itemset to a group include the\nso-called least misery (LM) and aggregate voting (AV). We consider the\ncomplementary problem of how to form groups such that the users in the formed\ngroups are most satisfied with the suggested top-k recommendations. We assume\nthat the recommendations will be generated according to one of the two group\nrecommendation semantics - LM or AV. Rather than assuming groups are given, or\nrely on ad hoc group formation dynamics, our framework allows a strategic\napproach for forming groups of users in order to maximize satisfaction. We show\nthat the problem is NP-hard to solve optimally under both semantics.\nFurthermore, we develop two efficient algorithms for group formation under LM\nand show that they achieve bounded absolute error. We develop efficient\nheuristic algorithms for group formation under AV. We validate our results and\ndemonstrate the scalability and effectiveness of our group formation algorithms\non two large real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 17:34:43 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Roy", "Senjuti Basu", ""], ["Lakshmanan", "Laks V. S.", ""], ["Liu", "Rui", ""]]}, {"id": "1503.03920", "submitter": "Samar Alqhtani Mrs", "authors": "Samar M. Alqhtani, Suhuai Luo and Brian Regan", "title": "Fusing Text and Image for Event Detection in Twitter", "comments": "9 Pages, 4 figuers", "journal-ref": null, "doi": "10.5121/ijma.2015.7103", "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, we develop an accurate and effective event detection\nmethod to detect events from a Twitter stream, which uses visual and textual\ninformation to improve the performance of the mining process. The method\nmonitors a Twitter stream to pick up tweets having texts and images and stores\nthem into a database. This is followed by applying a mining algorithm to detect\nan event. The procedure starts with detecting events based on text only by\nusing the feature of the bag-of-words which is calculated using the term\nfrequency-inverse document frequency (TF-IDF) method. Then it detects the event\nbased on image only by using visual features including histogram of oriented\ngradients (HOG) descriptors, grey-level cooccurrence matrix (GLCM), and color\nhistogram. K nearest neighbours (Knn) classification is used in the detection.\nThe final decision of the event detection is made based on the reliabilities of\ntext only detection and image only detection. The experiment result showed that\nthe proposed method achieved high accuracy of 0.94, comparing with 0.89 with\ntexts only, and 0.86 with images only.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 01:11:08 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Alqhtani", "Samar M.", ""], ["Luo", "Suhuai", ""], ["Regan", "Brian", ""]]}, {"id": "1503.03957", "submitter": "Prabhjot Singh", "authors": "Prabhjot Singh, Sumit Dhawan, Shubham Agarwal, Narina Thakur", "title": "Implementation of an efficient Fuzzy Logic based Information Retrieval\n  System", "comments": "arXiv admin note: substantial text overlap with\n  http://ntz-develop.blogspot.in/ ,\n  http://www.micsymposium.org/mics2012/submissions/mics2012_submission_8.pdf ,\n  http://www.slideshare.net/JeffreyStricklandPhD/predictive-modeling-and-analytics-selectchapters-41304405\n  by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper exemplifies the implementation of an efficient Information\nRetrieval (IR) System to compute the similarity between a dataset and a query\nusing Fuzzy Logic. TREC dataset has been used for the same purpose. The dataset\nis parsed to generate keywords index which is used for the similarity\ncomparison with the user query. Each query is assigned a score value based on\nits fuzzy similarity with the index keywords. The relevant documents are\nretrieved based on the score value. The performance and accuracy of the\nproposed fuzzy similarity model is compared with Cosine similarity model using\nPrecision-Recall curves. The results prove the dominance of Fuzzy Similarity\nbased IR system.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 05:21:02 GMT"}], "update_date": "2015-05-15", "authors_parsed": [["Singh", "Prabhjot", ""], ["Dhawan", "Sumit", ""], ["Agarwal", "Shubham", ""], ["Thakur", "Narina", ""]]}, {"id": "1503.03961", "submitter": "Runwei Qiang", "authors": "Runwei Qiang, Feifan Fan, Chao Lv, Jianwu Yang", "title": "Knowledge-based Query Expansion in Real-Time Microblog Search", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the length of microblog texts, such as tweets, is strictly limited to\n140 characters, traditional Information Retrieval techniques suffer from the\nvocabulary mismatch problem severely and cannot yield good performance in the\ncontext of microblogosphere. To address this critical challenge, in this paper,\nwe propose a new language modeling approach for microblog retrieval by\ninferring various types of context information. In particular, we expand the\nquery using knowledge terms derived from Freebase so that the expanded one can\nbetter reflect users' search intent. Besides, in order to further satisfy\nusers' real-time information need, we incorporate temporal evidences into the\nexpansion method, which can boost recent tweets in the retrieval results with\nrespect to a given topic. Experimental results on two official TREC Twitter\ncorpora demonstrate the significant superiority of our approach over baseline\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 05:54:04 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Qiang", "Runwei", ""], ["Fan", "Feifan", ""], ["Lv", "Chao", ""], ["Yang", "Jianwu", ""]]}, {"id": "1503.04424", "submitter": "Fabrizio Sebastiani", "authors": "Walid Magdy and Hassan Sajjad and Tarek El-Ganainy and Fabrizio\n  Sebastiani", "title": "Bridging Social Media via Distant Supervision", "comments": null, "journal-ref": null, "doi": "10.1007/s13278-015-0275-z", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Microblog classification has received a lot of attention in recent years.\nDifferent classification tasks have been investigated, most of them focusing on\nclassifying microblogs into a small number of classes (five or less) using a\ntraining set of manually annotated tweets. Unfortunately, labelling data is\ntedious and expensive, and finding tweets that cover all the classes of\ninterest is not always straightforward, especially when some of the classes do\nnot frequently arise in practice. In this paper we study an approach to tweet\nclassification based on distant supervision, whereby we automatically transfer\nlabels from one social medium to another for a single-label multi-class\nclassification task. In particular, we apply YouTube video classes to tweets\nlinking to these videos. This provides for free a virtually unlimited number of\nlabelled instances that can be used as training data. The classification\nexperiments we have run show that training a tweet classifier via these\nautomatically labelled data achieves substantially better performance than\ntraining the same classifier with a limited amount of manually labelled data;\nthis is advantageous, given that the automatically labelled data come at no\ncost. Further investigation of our approach shows its robustness when applied\nwith different numbers of classes and across different languages.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2015 13:22:03 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Magdy", "Walid", ""], ["Sajjad", "Hassan", ""], ["El-Ganainy", "Tarek", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1503.05543", "submitter": "Alexander Alemi", "authors": "Alexander A Alemi, Paul Ginsparg", "title": "Text Segmentation based on Semantic Word Embeddings", "comments": "10 pages, 4 figures. KDD2015 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of semantic word embeddings in text segmentation\nalgorithms, including the C99 segmentation algorithm and new algorithms\ninspired by the distributed word vector representation. By developing a general\nframework for discussing a class of segmentation objectives, we study the\neffectiveness of greedy versus exact optimization approaches and suggest a new\niterative refinement technique for improving the performance of greedy\nstrategies. We compare our results to known benchmarks, using known metrics. We\ndemonstrate state-of-the-art performance for an untrained method with our\nContent Vector Segmentation (CVS) on the Choi test set. Finally, we apply the\nsegmentation procedure to an in-the-wild dataset consisting of text extracted\nfrom scholarly articles in the arXiv.org database.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 19:44:06 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Alemi", "Alexander A", ""], ["Ginsparg", "Paul", ""]]}, {"id": "1503.05702", "submitter": "Xianwen Wang", "authors": "Xianwen Wang, Chen Liu, Wenli Mao and Zhichao Fang", "title": "The Open Access Advantage Considering Citation, Article Usage and Social\n  Media Attention", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": "10.1007/s11192-015-1547-0", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we compare the difference in the impact between open access\n(OA) and non-open access (non-OA) articles. 1761 Nature Communications articles\npublished from 1 Jan. 2012 to 31 Aug. 2013 are selected as our research\nobjects, including 587 OA articles and 1174 non-OA articles. Citation data and\ndaily updated article-level metrics data are harvested directly from the\nplatform of nature.com. Data is analyzed from the static versus\ntemporal-dynamic perspectives. The OA citation advantage is confirmed, and the\nOA advantage is also applicable when extending the comparing from citation to\narticle views and social media attention. More important, we find that OA\npapers not only have the great advantage of total downloads, but also have the\nfeature of keeping sustained and steady downloads for a long time. For article\ndownloads, non-OA papers only have a short period of attention, when the\nadvantage of OA papers exists for a much longer time.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 10:41:22 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Wang", "Xianwen", ""], ["Liu", "Chen", ""], ["Mao", "Wenli", ""], ["Fang", "Zhichao", ""]]}, {"id": "1503.05781", "submitter": "Alexei Yavlinsky", "authors": "Alexei Yavlinsky", "title": "Memantic: A Medical Knowledge Discovery Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that constructs and maintains an up-to-date co-occurrence\nnetwork of medical concepts based on continuously mining the latest biomedical\nliterature. Users can explore this network visually via a concise online\ninterface to quickly discover important and novel relationships between medical\nentities. This enables users to rapidly gain contextual understanding of their\nmedical topics of interest, and we believe this constitutes a significant user\nexperience improvement over contemporary search engines operating in the\nbiomedical literature domain.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 14:31:28 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Yavlinsky", "Alexei", ""]]}, {"id": "1503.05951", "submitter": "Kai Li", "authors": "Kai Li, Guojun Qi, Jun Ye, Kien A. Hua", "title": "Rank Subspace Learning for Compact Hash Codes", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The era of Big Data has spawned unprecedented interests in developing hashing\nalgorithms for efficient storage and fast nearest neighbor search. Most\nexisting work learn hash functions that are numeric quantizations of feature\nvalues in projected feature space. In this work, we propose a novel hash\nlearning framework that encodes feature's rank orders instead of numeric values\nin a number of optimal low-dimensional ranking subspaces. We formulate the\nranking subspace learning problem as the optimization of a piece-wise linear\nconvex-concave function and present two versions of our algorithm: one with\nindependent optimization of each hash bit and the other exploiting a sequential\nlearning framework. Our work is a generalization of the Winner-Take-All (WTA)\nhash family and naturally enjoys all the numeric stability benefits of rank\ncorrelation measures while being optimized to achieve high precision at very\nshort code length. We compare with several state-of-the-art hashing algorithms\nin both supervised and unsupervised domain, showing superior performance in a\nnumber of data sets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 21:34:33 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Li", "Kai", ""], ["Qi", "Guojun", ""], ["Ye", "Jun", ""], ["Hua", "Kien A.", ""]]}, {"id": "1503.06410", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": "KIT-14-001", "categories": "cs.IR cs.CL cs.LG cs.NE stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The F-measure or F-score is one of the most commonly used single number\nmeasures in Information Retrieval, Natural Language Processing and Machine\nLearning, but it is based on a mistake, and the flawed assumptions render it\nunsuitable for use in most contexts! Fortunately, there are better\nalternatives.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 11:32:34 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 05:42:44 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "1503.06483", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Maryam Yammahi, Nima Bari, Roman Vichr, Faisal Alsaby,\n  Simon Y. Berkovich", "title": "Construction of FuzzyFind Dictionary using Golay Coding Transformation\n  for Searching Applications", "comments": null, "journal-ref": null, "doi": "10.14569/IJACSA.2015.060313", "report-no": null, "categories": "cs.DB cs.AI cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching through a large volume of data is very critical for companies,\nscientists, and searching engines applications due to time complexity and\nmemory complexity. In this paper, a new technique of generating FuzzyFind\nDictionary for text mining was introduced. We simply mapped the 23 bits of the\nEnglish alphabet into a FuzzyFind Dictionary or more than 23 bits by using more\nFuzzyFind Dictionary, and reflecting the presence or absence of particular\nletters. This representation preserves closeness of word distortions in terms\nof closeness of the created binary vectors within Hamming distance of 2\ndeviations. This paper talks about the Golay Coding Transformation Hash Table\nand how it can be used on a FuzzyFind Dictionary as a new technology for using\nin searching through big data. This method is introduced by linear time\ncomplexity for generating the dictionary and constant time complexity to access\nthe data and update by new data sets, also updating for new data sets is linear\ntime depends on new data points. This technique is based on searching only for\nletters of English that each segment has 23 bits, and also we have more than\n23-bit and also it could work with more segments as reference table.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 21:46:12 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Kowsari", "Kamran", ""], ["Yammahi", "Maryam", ""], ["Bari", "Nima", ""], ["Vichr", "Roman", ""], ["Alsaby", "Faisal", ""], ["Berkovich", "Simon Y.", ""]]}, {"id": "1503.06555", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Sumitkumar Kanoje, Sheetal Girase, Debajyoti Mukhopadhyay", "title": "User Profiling for Recommendation System", "comments": "5 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation system is a type of information filtering systems that\nrecommend various objects from a vast variety and quantity of items which are\nof the user interest. This results in guiding an individual in personalized way\nto interesting or useful objects in a large space of possible options. Such\nsystems also help many businesses to achieve more profits to sustain in their\nfiled against their rivals. But looking at the amount of information which a\nbusiness holds it becomes difficult to identify the items of user interest.\nTherefore personalization or user profiling is one of the challenging tasks\nthat give access to user relevant information which can be used in solving the\ndifficult task of classification and ranking items according to an individuals\ninterest. Profiling can be done in various ways such assupervised or\nunsupervised, individual or group profiling, distributive or and non\ndistributive profiling. Our focus in this paper will be on the dataset which we\nwill use, we identify some interesting facts by using Weka Tool that can be\nused for recommending the items from dataset. Our aim is to present a novel\ntechnique to achieve user profiling in recommendation system.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 08:47:35 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Kanoje", "Sumitkumar", ""], ["Girase", "Sheetal", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "1503.06562", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Dheeraj kumar Bokde, Sheetal Girase, Debajyoti Mukhopadhyay", "title": "An Item-Based Collaborative Filtering using Dimensionality Reduction\n  Techniques on Mahout Framework", "comments": "6 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Filtering is the most widely used prediction technique in\nRecommendation System. Most of the current CF recommender systems maintains\nsingle criteria user rating in user item matrix. However, recent studies\nindicate that recommender system depending on multi criteria can improve\nprediction and accuracy levels of recommendation by considering the user\npreferences in multi aspects of items. This gives birth to Multi Criteria\nCollaborative Filtering. In MC CF users provide the rating on multiple aspects\nof an item in new dimensions,thereby increasing the size of rating matrix,\nsparsity and scalability problem. Appropriate dimensionality reduction\ntechniques are thus needed to take care of these challenges to reduce the\ndimension of user item rating matrix to improve the prediction accuracy and\nefficiency of CF recommender system. The process of dimensionality reduction\nmaps the high dimensional input space into lower dimensional space. Thus, the\nobjective of this paper is to propose an efficient MC CF algorithm using\ndimensionality reduction technique to improve the recommendation quality and\nprediction accuracy. Dimensionality reduction techniques such as Singular Value\nDecomposition and Principal Component Analysis are used to solve the\nscalability and alleviate the sparsity problems in overall rating. The proposed\nMC CF approach will be implemented using Apache Mahout, which allows processing\nof massive dataset stored in distributed/non-distributed file system.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 09:09:07 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Bokde", "Dheeraj kumar", ""], ["Girase", "Sheetal", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "1503.06598", "submitter": "Mikhail Galkin", "authors": "Mikhail Galkin and Dmitry Mouromtsev and S\\\"oren Auer", "title": "Identifying Web Tables - Supporting a Neglected Type of Content on the\n  Web", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of the data in the Internet facilitates the improvement of\nextraction and processing tools. The trend in the open data publishing\nencourages the adoption of structured formats like CSV and RDF. However, there\nis still a plethora of unstructured data on the Web which we assume contain\nsemantics. For this reason, we propose an approach to derive semantics from web\ntables which are still the most popular publishing tool on the Web. The paper\nalso discusses methods and services of unstructured data extraction and\nprocessing as well as machine learning techniques to enhance such a workflow.\nThe eventual result is a framework to process, publish and visualize linked\nopen data. The software enables tables extraction from various open data\nsources in the HTML format and an automatic export to the RDF format making the\ndata linked. The paper also gives the evaluation of machine learning techniques\nin conjunction with string similarity functions to be applied in a tables\nrecognition task.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 11:05:26 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Galkin", "Mikhail", ""], ["Mouromtsev", "Dmitry", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "1503.06609", "submitter": "Mansaf Alam Dr", "authors": "Mansaf Alam and Kishwar Sadaf", "title": "Web Search Result Clustering based on Cuckoo Search and Consensus\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering of web search result document has emerged as a promising tool for\nimproving retrieval performance of an Information Retrieval (IR) system. Search\nresults often plagued by problems like synonymy, polysemy, high volume etc.\nClustering other than resolving these problems also provides the user the\neasiness to locate his/her desired information. In this paper, a method, called\nWSRDC-CSCC, is introduced to cluster web search result using cuckoo search\nmeta-heuristic method and Consensus clustering. Cuckoo search provides a solid\nfoundation for consensus clustering. As a local clustering function, k-means\ntechnique is used. The final number of cluster is not depended on this k.\nConsensus clustering finds the natural grouping of the objects. The proposed\nalgorithm is compared to another clustering method which is based on cuckoo\nsearch and Bayesian Information Criterion. The experimental results show that\nproposed algorithm finds the actual number of clusters with great value of\nprecision, recall and F-measure as compared to the other method\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 11:52:49 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Alam", "Mansaf", ""], ["Sadaf", "Kishwar", ""]]}, {"id": "1503.06666", "submitter": "David Martins de Matos", "authors": "Francisco Raposo, Ricardo Ribeiro, David Martins de Matos", "title": "Using Generic Summarization to Improve Music Information Retrieval Tasks", "comments": "24 pages, 10 tables; Submitted to IEEE/ACM Transactions on Audio,\n  Speech and Language Processing", "journal-ref": "IEEE/ACM Transactions on Audio, Speech and Language Processing,\n  vol. 24, n. 6, March 2016", "doi": "10.1109/TASLP.2016.2541299", "report-no": null, "categories": "cs.IR cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to satisfy processing time constraints, many MIR tasks process only\na segment of the whole music signal. This practice may lead to decreasing\nperformance, since the most important information for the tasks may not be in\nthose processed segments. In this paper, we leverage generic summarization\nalgorithms, previously applied to text and speech summarization, to summarize\nitems in music datasets. These algorithms build summaries, that are both\nconcise and diverse, by selecting appropriate segments from the input signal\nwhich makes them good candidates to summarize music as well. We evaluate the\nsummarization process on binary and multiclass music genre classification\ntasks, by comparing the performance obtained using summarized datasets against\nthe performances obtained using continuous segments (which is the traditional\nmethod used for addressing the previously mentioned time constraints) and full\nsongs of the same original dataset. We show that GRASSHOPPER, LexRank, LSA,\nMMR, and a Support Sets-based Centrality model improve classification\nperformance when compared to selected 30-second baselines. We also show that\nsummarized datasets lead to a classification performance whose difference is\nnot statistically significant from using full songs. Furthermore, we make an\nargument stating the advantages of sharing summarized datasets for future MIR\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 14:48:24 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2015 18:38:22 GMT"}, {"version": "v3", "created": "Wed, 9 Mar 2016 16:24:42 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Raposo", "Francisco", ""], ["Ribeiro", "Ricardo", ""], ["de Matos", "David Martins", ""]]}, {"id": "1503.07284", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "Arijit De and Sunil Kumar Kopparapu", "title": "A Rule-Based Short Query Intent Identification System", "comments": "5 pages, 2010 International Conference on Signal and Image Processing\n  (ICSIP)", "journal-ref": null, "doi": "10.1109/ICSIP.2010.5697471", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using SMS (Short Message System), cell phones can be used to query for\ninformation about various topics. In an SMS based search system, one of the key\nproblems is to identify a domain (broad topic) associated with the user query;\nso that a more comprehensive search can be carried out by the domain specific\nsearch engine. In this paper we use a rule based approach, to identify the\ndomain, called Short Query Intent Identification System (SQIIS). We construct\ntwo different rule-bases using different strategies to suit query intent\nidentification. We evaluate the two rule-bases experimentally.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 05:35:05 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["De", "Arijit", ""], ["Kopparapu", "Sunil Kumar", ""]]}, {"id": "1503.07294", "submitter": "Issa Atoum", "authors": "Wendy Tan Wei Syn, Bong Chih How, Issa Atoum", "title": "Using Latent Semantic Analysis to Identify Quality in Use (QU)\n  Indicators from User Reviews", "comments": "4 Figures in The International Conference on Artificial Intelligence\n  and Pattern Recognition (AIPR2014),2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes a novel approach to categorize users' reviews according\nto the three Quality in Use (QU) indicators defined in ISO: effectiveness,\nefficiency and freedom from risk. With the tremendous amount of reviews\npublished each day, there is a need to automatically summarize user reviews to\ninform us if any of the software able to meet requirement of a company\naccording to the quality requirements. We implemented the method of Latent\nSemantic Analysis (LSA) and its subspace to predict QU indicators. We build a\nreduced dimensionality universal semantic space from Information System\njournals and Amazon reviews. Next, we projected set of indicators' measurement\nscales into the universal semantic space and represent them as subspace. In the\nsubspace, we can map similar measurement scales to the unseen reviews and\npredict the QU indicators. Our preliminary study able to obtain the average of\nF-measure, 0.3627.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 06:42:05 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Syn", "Wendy Tan Wei", ""], ["How", "Bong Chih", ""], ["Atoum", "Issa", ""]]}, {"id": "1503.07387", "submitter": "Daniel Lemire", "authors": "Jeff Plaisance, Nathan Kurz, Daniel Lemire", "title": "Vectorized VByte Decoding", "comments": "First International Symposium on Web Algorithms (June 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the ubiquitous technique of VByte compression, which represents\neach integer as a variable length sequence of bytes. The low 7 bits of each\nbyte encode a portion of the integer, and the high bit of each byte is reserved\nas a continuation flag. This flag is set to 1 for all bytes except the last,\nand the decoding of each integer is complete when a byte with a high bit of 0\nis encountered. VByte decoding can be a performance bottleneck especially when\nthe unpredictable lengths of the encoded integers cause frequent branch\nmispredictions. Previous attempts to accelerate VByte decoding using SIMD\nvector instructions have been disappointing, prodding search engines such as\nGoogle to use more complicated but faster-to-decode formats for\nperformance-critical code. Our decoder (Masked VByte) is 2 to 4 times faster\nthan a conventional scalar VByte decoder, making the format once again\ncompetitive with regard to speed.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 14:52:06 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 19:26:20 GMT"}, {"version": "v3", "created": "Wed, 16 Mar 2016 14:50:54 GMT"}, {"version": "v4", "created": "Sat, 14 Jan 2017 04:05:36 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Plaisance", "Jeff", ""], ["Kurz", "Nathan", ""], ["Lemire", "Daniel", ""]]}, {"id": "1503.07405", "submitter": "Arkaitz Zubiaga", "authors": "Bo Wang, Arkaitz Zubiaga, Maria Liakata, Rob Procter", "title": "Making the Most of Tweet-Inherent Features for Social Spam Detection on\n  Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social spam produces a great amount of noise on social media services such as\nTwitter, which reduces the signal-to-noise ratio that both end users and data\nmining applications observe. Existing techniques on social spam detection have\nfocused primarily on the identification of spam accounts by using extensive\nhistorical and network-based data. In this paper we focus on the detection of\nspam tweets, which optimises the amount of data that needs to be gathered by\nrelying only on tweet-inherent features. This enables the application of the\nspam detection system to a large set of tweets in a timely fashion, potentially\napplicable in a real-time or near real-time setting. Using two large\nhand-labelled datasets of tweets containing spam, we study the suitability of\nfive classification algorithms and four different feature sets to the social\nspam detection task. Our results show that, by using the limited set of\nfeatures readily available in a tweet, we can achieve encouraging results which\nare competitive when compared against existing spammer detection systems that\nmake use of additional, costly user features. Our study is the first that\nattempts at generalising conclusions on the optimal classifiers and sets of\nfeatures for social spam detection over different datasets.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 14:58:59 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Wang", "Bo", ""], ["Zubiaga", "Arkaitz", ""], ["Liakata", "Maria", ""], ["Procter", "Rob", ""]]}, {"id": "1503.07474", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Sumitkumar Kanoje, Sheetal Girase, Debajyoti Mukhopadhyay", "title": "User Profiling Trends, Techniques and Applications", "comments": "6 pages, 1 figure in IJAFRC, Vol.1, Issue 11, November 2014, ISSN:\n  2348-4853. arXiv admin note: text overlap with arXiv:1503.06555", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Personalization of information has taken recommender systems at a very\nhigh level. With personalization these systems can generate user specific\nrecommendations accurately and efficiently. User profiling helps\npersonalization, where information retrieval is done to personalize a scenario\nwhich maintains a separate user profile for individual user. The main objective\nof this paper is to explore this field of personalization in context of user\nprofiling, to help researchers make aware of the user profiling. Various\ntrends, techniques and Applications have been discussed in paper which will\nfulfill this motto.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 17:52:21 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Kanoje", "Sumitkumar", ""], ["Girase", "Sheetal", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "1503.07475", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Dheeraj kumar Bokde, Sheetal Girase, Debajyoti Mukhopadhyay", "title": "Role of Matrix Factorization Model in Collaborative Filtering Algorithm:\n  A Survey", "comments": "8 pages, 1 figure in IJAFRC, Vol.1, Issue 12, December 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation Systems apply Information Retrieval techniques to select the\nonline information relevant to a given user. Collaborative Filtering is\ncurrently most widely used approach to build Recommendation System. CF\ntechniques uses the user behavior in form of user item ratings as their\ninformation source for prediction. There are major challenges like sparsity of\nrating matrix and growing nature of data which is faced by CF algorithms. These\nchallenges are been well taken care by Matrix Factorization. In this paper we\nattempt to present an overview on the role of different MF model to address the\nchallenges of CF algorithms, which can be served as a roadmap for research in\nthis area.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 17:54:43 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Bokde", "Dheeraj kumar", ""], ["Girase", "Sheetal", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "1503.07816", "submitter": "Abdelkhalak Bahri bahriinfo", "authors": "Bahri Abdelkhalak and Hamid Zouaki", "title": "Content-Based Bird Retrieval using Shape context, Color moments and Bag\n  of Features", "comments": "5 pages, 2 figures, IJCSI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new descriptor for birds search. First, our work\nwas carried on the choice of a descriptor. This choice is usually driven by the\napplication requirements such as robustness to noise, stability with respect to\nbias, the invariance to geometrical transformations or tolerance to occlusions.\nIn this context, we introduce a descriptor which combines the shape and color\ndescriptors to have an effectiveness description of birds. The proposed\ndescriptor is an adaptation of a descriptor based on the contours defined in\narticle Belongie et al. [5] combined with color moments [19]. Specifically,\npoints of interest are extracted from each image and information's in the\nregion in the vicinity of these points are represented by descriptors of shape\ncontext concatenated with color moments. Thus, the approach bag of visual words\nis applied to the latter. The experimental results show the effectiveness of\nour descriptor for the bird search by content.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 11:02:14 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Abdelkhalak", "Bahri", ""], ["Zouaki", "Hamid", ""]]}, {"id": "1503.08248", "submitter": "Xirong Li", "authors": "Xirong Li and Tiberio Uricchio and Lamberto Ballan and Marco Bertini\n  and Cees G. M. Snoek and Alberto Del Bimbo", "title": "Socializing the Semantic Gap: A Comparative Survey on Image Tag\n  Assignment, Refinement and Retrieval", "comments": "to appear in ACM Computing Surveys", "journal-ref": "ACM Computing Surveys, Volume 49 Issue 1, 14:1-14:39, June 2016", "doi": "10.1145/2906152", "report-no": null, "categories": "cs.IR cs.CV cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Where previous reviews on content-based image retrieval emphasize on what can\nbe seen in an image to bridge the semantic gap, this survey considers what\npeople tag about an image. A comprehensive treatise of three closely linked\nproblems, i.e., image tag assignment, refinement, and tag-based image retrieval\nis presented. While existing works vary in terms of their targeted tasks and\nmethodology, they rely on the key functionality of tag relevance, i.e.\nestimating the relevance of a specific tag with respect to the visual content\nof a given image and its social context. By analyzing what information a\nspecific method exploits to construct its tag relevance function and how such\ninformation is exploited, this paper introduces a taxonomy to structure the\ngrowing literature, understand the ingredients of the main works, clarify their\nconnections and difference, and recognize their merits and limitations. For a\nhead-to-head comparison between the state-of-the-art, a new experimental\nprotocol is presented, with training sets containing 10k, 100k and 1m images\nand an evaluation on three test sets, contributed by various research groups.\nEleven representative works are implemented and evaluated. Putting all this\ntogether, the survey aims to provide an overview of the past and foster\nprogress for the near future.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 00:10:16 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2015 05:33:21 GMT"}, {"version": "v3", "created": "Wed, 23 Mar 2016 05:45:31 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Li", "Xirong", ""], ["Uricchio", "Tiberio", ""], ["Ballan", "Lamberto", ""], ["Bertini", "Marco", ""], ["Snoek", "Cees G. M.", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1503.08463", "submitter": "S. K. Sahay", "authors": "Rajendra Kumar Roul, Saransh Varshneya, Ashu Kalra, Sanjay Kumar Sahay", "title": "A Novel Modified Apriori Approach for Web Document Clustering", "comments": "11 Pages, 5 Figures", "journal-ref": "Springer, Smart Innovation Systems and Technologies, Vol. 33,\n  2015, p. 159-171; Proceedings of the ICCIDM, Dec. 2014", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional apriori algorithm can be used for clustering the web\ndocuments based on the association technique of data mining. But this algorithm\nhas several limitations due to repeated database scans and its weak association\nrule analysis. In modern world of large databases, efficiency of traditional\napriori algorithm would reduce manifolds. In this paper, we proposed a new\nmodified apriori approach by cutting down the repeated database scans and\nimproving association analysis of traditional apriori algorithm to cluster the\nweb documents. Further we improve those clusters by applying Fuzzy C-Means\n(FCM), K-Means and Vector Space Model (VSM) techniques separately. For\nexperimental purpose, we use Classic3 and Classic4 datasets of Cornell\nUniversity having more than 10,000 documents and run both traditional apriori\nand our modified apriori approach on it. Experimental results show that our\napproach outperforms the traditional apriori algorithm in terms of database\nscan and improvement on association of analysis. We found out that FCM is\nbetter than K-Means and VSM in terms of F-measure of clusters of different\nsizes.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 17:40:18 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Roul", "Rajendra Kumar", ""], ["Varshneya", "Saransh", ""], ["Kalra", "Ashu", ""], ["Sahay", "Sanjay Kumar", ""]]}, {"id": "1503.08535", "submitter": "Junyu Xuan", "authors": "Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo", "title": "Infinite Author Topic Model based on Mixed Gamma-Negative Binomial\n  Process", "comments": "10 pages, 5 figures, submitted to KDD conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating the side information of text corpus, i.e., authors, time\nstamps, and emotional tags, into the traditional text mining models has gained\nsignificant interests in the area of information retrieval, statistical natural\nlanguage processing, and machine learning. One branch of these works is the\nso-called Author Topic Model (ATM), which incorporates the authors's interests\nas side information into the classical topic model. However, the existing ATM\nneeds to predefine the number of topics, which is difficult and inappropriate\nin many real-world settings. In this paper, we propose an Infinite Author Topic\n(IAT) model to resolve this issue. Instead of assigning a discrete probability\non fixed number of topics, we use a stochastic process to determine the number\nof topics from the data itself. To be specific, we extend a gamma-negative\nbinomial process to three levels in order to capture the\nauthor-document-keyword hierarchical structure. Furthermore, each document is\nassigned a mixed gamma process that accounts for the multi-author's\ncontribution towards this document. An efficient Gibbs sampling inference\nalgorithm with each conditional distribution being closed-form is developed for\nthe IAT model. Experiments on several real-world datasets show the capabilities\nof our IAT model to learn the hidden topics, authors' interests on these topics\nand the number of topics simultaneously.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 05:03:37 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Xuan", "Junyu", ""], ["Lu", "Jie", ""], ["Zhang", "Guangquan", ""], ["Da Xu", "Richard Yi", ""], ["Luo", "Xiangfeng", ""]]}, {"id": "1503.08542", "submitter": "Junyu Xuan", "authors": "Junyu Xuan, Jie Lu, Guangquan Zhang, Richard Yi Da Xu, Xiangfeng Luo", "title": "Nonparametric Relational Topic Models through Dependent Gamma Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Relational Topic Models provide a way to discover the hidden\ntopics from a document network. Many theoretical and practical tasks, such as\ndimensional reduction, document clustering, link prediction, benefit from this\nrevealed knowledge. However, existing relational topic models are based on an\nassumption that the number of hidden topics is known in advance, and this is\nimpractical in many real-world applications. Therefore, in order to relax this\nassumption, we propose a nonparametric relational topic model in this paper.\nInstead of using fixed-dimensional probability distributions in its generative\nmodel, we use stochastic processes. Specifically, a gamma process is assigned\nto each document, which represents the topic interest of this document.\nAlthough this method provides an elegant solution, it brings additional\nchallenges when mathematically modeling the inherent network structure of\ntypical document network, i.e., two spatially closer documents tend to have\nmore similar topics. Furthermore, we require that the topics are shared by all\nthe documents. In order to resolve these challenges, we use a subsampling\nstrategy to assign each document a different gamma process from the global\ngamma process, and the subsampling probabilities of documents are assigned with\na Markov Random Field constraint that inherits the document network structure.\nThrough the designed posterior inference algorithm, we can discover the hidden\ntopics and its number simultaneously. Experimental results on both synthetic\nand real-world network datasets demonstrate the capabilities of learning the\nhidden topics and, more importantly, the number of topics.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 05:40:41 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Xuan", "Junyu", ""], ["Lu", "Jie", ""], ["Zhang", "Guangquan", ""], ["Da Xu", "Richard Yi", ""], ["Luo", "Xiangfeng", ""]]}, {"id": "1503.08558", "submitter": "Konstantin Avrachenkov", "authors": "Konstantin Avrachenkov (INRIA Sophia Antipolis), Vivek Borkar (EE-IIT)", "title": "Whittle Index Policy for Crawling Ephemeral Content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a task of scheduling a crawler to retrieve content from several\nsites with ephemeral content. A user typically loses interest in ephemeral\ncontent, like news or posts at social network groups, after several days or\nhours. Thus, development of timely crawling policy for such ephemeral\ninformation sources is very important. We first formulate this problem as an\noptimal control problem with average reward. The reward can be measured in the\nnumber of clicks or relevant search requests. The problem in its initial\nformulation suffers from the curse of dimensionality and quickly becomes\nintractable even with moderate number of information sources. Fortunately, this\nproblem admits a Whittle index, which leads to problem decomposition and to a\nvery simple and efficient crawling policy. We derive the Whittle index and\nprovide its theoretical justification.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 06:45:31 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Avrachenkov", "Konstantin", "", "INRIA Sophia Antipolis"], ["Borkar", "Vivek", "", "EE-IIT"]]}, {"id": "1503.08581", "submitter": "Ioannis Partalas", "authors": "Ioannis Partalas, Aris Kosmopoulos, Nicolas Baskiotis, Thierry\n  Artieres, George Paliouras, Eric Gaussier, Ion Androutsopoulos, Massih-Reza\n  Amini, Patrick Galinari", "title": "LSHTC: A Benchmark for Large-Scale Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSHTC is a series of challenges which aims to assess the performance of\nclassification systems in large-scale classification in a a large number of\nclasses (up to hundreds of thousands). This paper describes the dataset that\nhave been released along the LSHTC series. The paper details the construction\nof the datsets and the design of the tracks as well as the evaluation measures\nthat we implemented and a quick overview of the results. All of these datasets\nare available online and runs may still be submitted on the online server of\nthe challenges.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 08:03:47 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Partalas", "Ioannis", ""], ["Kosmopoulos", "Aris", ""], ["Baskiotis", "Nicolas", ""], ["Artieres", "Thierry", ""], ["Paliouras", "George", ""], ["Gaussier", "Eric", ""], ["Androutsopoulos", "Ion", ""], ["Amini", "Massih-Reza", ""], ["Galinari", "Patrick", ""]]}, {"id": "1503.08604", "submitter": "Sebastiano Vigna", "authors": "Paolo Boldi, Corrado Monti, Massimo Santini, Sebastiano Vigna", "title": "Liquid FM: Recommending Music through Viscous Democracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most modern recommendation systems use the approach of collaborative\nfiltering: users that are believed to behave alike are used to produce\nrecommendations. In this work we describe an application (Liquid FM) taking a\ncompletely different approach. Liquid FM is a music recommendation system that\nmakes the user responsible for the recommended items. Suggestions are the\nresult of a voting scheme, employing the idea of viscous democracy. Liquid FM\ncan also be thought of as the first testbed for this voting system. In this\npaper we outline the design and architecture of the application, both from the\ntheoretical and from the implementation viewpoints.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2015 09:10:22 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Boldi", "Paolo", ""], ["Monti", "Corrado", ""], ["Santini", "Massimo", ""], ["Vigna", "Sebastiano", ""]]}]