[{"id": "1502.00094", "submitter": "Roman Dovgopol", "authors": "Roman Dovgopol, Matt Nohelty", "title": "Twitter Hash Tag Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The rise in popularity of microblogging services like Twitter has led to\nincreased use of content annotation strategies like the hashtag. Hashtags\nprovide users with a tagging mechanism to help organize, group, and create\nvisibility for their posts. This is a simple idea but can be challenging for\nthe user in practice which leads to infrequent usage. In this paper, we will\ninvestigate various methods of recommending hashtags as new posts are created\nto encourage more widespread adoption and usage. Hashtag recommendation comes\nwith numerous challenges including processing huge volumes of streaming data\nand content which is small and noisy. We will investigate preprocessing methods\nto reduce noise in the data and determine an effective method of hashtag\nrecommendation based on the popular classification algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 12:15:53 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Dovgopol", "Roman", ""], ["Nohelty", "Matt", ""]]}, {"id": "1502.00319", "submitter": "Mahdi Salarian mr", "authors": "Mahdi Salarian, Rashid Ansari", "title": "Efficient refinement of GPS-based localization in urban areas using\n  visual information and sensor parameter", "comments": "this paper has been withdrawn bt the authors due to couple of errors\n  in exprimental part", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient method is proposed for refining GPS-acquired location\ncoordinates in urban areas using camera images, Google Street View (GSV) and\nsensor parameters. The main goal is to compensate for GPS location imprecision\nin dense area of cities due to proximity to walls and buildings. Avail-able\nmethods for better localization often use visual information by using query\nimages acquired with camera-equipped mobile devices and applying image\nretrieval techniques to find the closest match in a GPS-referenced image data\nset. The search areas required for reliable search are about 1-2 sq. Km and the\naccuracy is typically 25-100 meters. Here we describe a method based on image\nretrieval where a reliable search can be confined to areas of 0.01 sq. Km and\nthe accuracy in our experiments is less than 10 meters. To test our procedure\nwe created a database by acquiring all Google Street View images close to what\nis seen by a pedestrian in a large region of downtown Chicago and saved all\ncoordinates and orientation data to be used for confining our search region.\nPrior knowledge from approximate position of query image is leveraged to\naddress complexity and accuracy issues of our search in a large scale\ngeo-tagged data set. One key aspect that differentiates our work is that it\nutilizes the sensor information of GPS SOS and the camera orientation in\nimproving localization. Finally we demonstrate retrieval-based technique are\nless accurate in sparse open areas compared with purely GPS measurement. The\neffectiveness of our approach is discussed in detail and experimental results\nshow improved performance when compared with regular approaches.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 21:56:39 GMT"}, {"version": "v2", "created": "Sun, 15 Feb 2015 06:52:09 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2015 18:08:37 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Salarian", "Mahdi", ""], ["Ansari", "Rashid", ""]]}, {"id": "1502.00524", "submitter": "Ricard Marxer", "authors": "Ricard Marxer and Hendrik Purwins", "title": "Unsupervised Incremental Learning and Prediction of Music Signals", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TASLP.2016.2530409", "report-no": null, "categories": "cs.SD cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A system is presented that segments, clusters and predicts musical audio in\nan unsupervised manner, adjusting the number of (timbre) clusters\ninstantaneously to the audio input. A sequence learning algorithm adapts its\nstructure to a dynamically changing clustering tree. The flow of the system is\nas follows: 1) segmentation by onset detection, 2) timbre representation of\neach segment by Mel frequency cepstrum coefficients, 3) discretization by\nincremental clustering, yielding a tree of different sound classes (e.g.\ninstruments) that can grow or shrink on the fly driven by the instantaneous\nsound events, resulting in a discrete symbol sequence, 4) extraction of\nstatistical regularities of the symbol sequence, using hierarchical N-grams and\nthe newly introduced conceptual Boltzmann machine, and 5) prediction of the\nnext sound event in the sequence. The system's robustness is assessed with\nrespect to complexity and noisiness of the signal. Clustering in isolation\nyields an adjusted Rand index (ARI) of 82.7% / 85.7% for data sets of singing\nvoice and drums. Onset detection jointly with clustering achieve an ARI of\n81.3% / 76.3% and the prediction of the entire system yields an ARI of 27.2% /\n39.2%.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 15:45:38 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2015 14:37:45 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Marxer", "Ricard", ""], ["Purwins", "Hendrik", ""]]}, {"id": "1502.00527", "submitter": "Maksims Volkovs", "authors": "Maksims Volkovs", "title": "Context Models For Web Search Personalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our solution to the Yandex Personalized Web Search Challenge. The\naim of this challenge was to use the historical search logs to personalize\ntop-N document rankings for a set of test users. We used over 100 features\nextracted from user- and query-depended contexts to train neural net and\ntree-based learning-to-rank and regression models. Our final submission, which\nwas a blend of several different models, achieved an NDCG@10 of 0.80476 and\nplaced 4'th amongst the 194 teams winning 3'rd prize.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 15:50:39 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Volkovs", "Maksims", ""]]}, {"id": "1502.00804", "submitter": "Ronan Cummins", "authors": "Ronan Cummins, Jiaul Hoque Paik, and Yuanhua Lv", "title": "A Polya Urn Document Language Model for Improved Information Retrieval", "comments": "37 page journal submission (accepted for publication in TOIS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multinomial language model has been one of the most effective models of\nretrieval for over a decade. However, the multinomial distribution does not\nmodel one important linguistic phenomenon relating to term-dependency, that is\nthe tendency of a term to repeat itself within a document (i.e. word\nburstiness). In this article, we model document generation as a random process\nwith reinforcement (a multivariate Polya process) and develop a Dirichlet\ncompound multinomial language model that captures word burstiness directly.\n  We show that the new reinforced language model can be computed as efficiently\nas current retrieval models, and with experiments on an extensive set of TREC\ncollections, we show that it significantly outperforms the state-of-the-art\nlanguage model for a number of standard effectiveness metrics. Experiments also\nshow that the tuning parameter in the proposed model is more robust than in the\nmultinomial language model. Furthermore, we develop a constraint for the\nverbosity hypothesis and show that the proposed model adheres to the\nconstraint. Finally, we show that the new language model essentially introduces\na measure closely related to idf which gives theoretical justification for\ncombining the term and document event spaces in tf-idf type schemes.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 10:41:12 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 23:39:28 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Cummins", "Ronan", ""], ["Paik", "Jiaul Hoque", ""], ["Lv", "Yuanhua", ""]]}, {"id": "1502.01057", "submitter": "Li Zhou", "authors": "Li Zhou", "title": "Personalized Web Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalization is important for search engines to improve user experience.\nMost of the existing work do pure feature engineering and extract a lot of\nsession-style features and then train a ranking model. Here we proposed a novel\nway to model both long term and short term user behavior using Multi-armed\nbandit algorithm. Our algorithm can generalize session information across users\nwell, and as an Explore-Exploit style algorithm, it can generalize to new urls\nand new users well. Experiments show that our algorithm can improve performance\nover the default ranking and outperforms several popular Multi-armed bandit\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 22:37:37 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Zhou", "Li", ""]]}, {"id": "1502.01657", "submitter": "Sudeep Pillai", "authors": "Michael Fleder, Michael S. Kester, Sudeep Pillai", "title": "Bitcoin Transaction Graph Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoins have recently become an increasingly popular cryptocurrency through\nwhich users trade electronically and more anonymously than via traditional\nelectronic transfers. Bitcoin's design keeps all transactions in a public\nledger. The sender and receiver for each transaction are identified only by\ncryptographic public-key ids. This leads to a common misconception that it\ninherently provides anonymous use. While Bitcoin's presumed anonymity offers\nnew avenues for commerce, several recent studies raise user-privacy concerns.\nWe explore the level of anonymity in the Bitcoin system. Our approach is\ntwo-fold: (i) We annotate the public transaction graph by linking bitcoin\npublic keys to \"real\" people - either definitively or statistically. (ii) We\nrun the annotated graph through our graph-analysis framework to find and\nsummarize activity of both known and unknown users.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 17:55:27 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Fleder", "Michael", ""], ["Kester", "Michael S.", ""], ["Pillai", "Sudeep", ""]]}, {"id": "1502.01916", "submitter": "Daniel Lemire", "authors": "Wayne Xin Zhao, Xudong Zhang, Daniel Lemire, Dongdong Shan, Jian-Yun\n  Nie, Hongfei Yan, Ji-Rong Wen", "title": "A General SIMD-based Approach to Accelerating Compression Algorithms", "comments": null, "journal-ref": "ACM Trans. Inf. Syst. 33, 3, Article 15 (March 2015)", "doi": "10.1145/2735629", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Compression algorithms are important for data oriented tasks, especially in\nthe era of Big Data. Modern processors equipped with powerful SIMD instruction\nsets, provide us an opportunity for achieving better compression performance.\nPrevious research has shown that SIMD-based optimizations can multiply decoding\nspeeds. Following these pioneering studies, we propose a general approach to\naccelerate compression algorithms. By instantiating the approach, we have\ndeveloped several novel integer compression algorithms, called Group-Simple,\nGroup-Scheme, Group-AFOR, and Group-PFD, and implemented their corresponding\nvectorized versions. We evaluate the proposed algorithms on two public TREC\ndatasets, a Wikipedia dataset and a Twitter dataset. With competitive\ncompression ratios and encoding speeds, our SIMD-based algorithms outperform\nstate-of-the-art non-vectorized algorithms with respect to decoding speeds.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 15:11:47 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Zhao", "Wayne Xin", ""], ["Zhang", "Xudong", ""], ["Lemire", "Daniel", ""], ["Shan", "Dongdong", ""], ["Nie", "Jian-Yun", ""], ["Yan", "Hongfei", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "1502.01963", "submitter": "Peter Mutschke", "authors": "Peter Mutschke, Philipp Mayr, Andrea Scharnhorst", "title": "Editorial for the Proceedings of the Workshop Knowledge Maps and\n  Information Retrieval (KMIR2014) at Digital Libraries 2014", "comments": "URL workshop proceedings: http://ceur-ws.org/Vol-1311/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge maps are promising tools for visualizing the structure of\nlarge-scale information spaces, but still far away from being applicable for\nsearching. The first international workshop on \"Knowledge Maps and Information\nRetrieval (KMIR)\", held as part of the International Conference on Digital\nLibraries 2014 in London, aimed at bringing together experts in Information\nRetrieval (IR) and knowledge mapping in order to discuss the potential of\ninteractive knowledge maps for information seeking purposes.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 17:35:34 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Mutschke", "Peter", ""], ["Mayr", "Philipp", ""], ["Scharnhorst", "Andrea", ""]]}, {"id": "1502.01965", "submitter": "Peter Mutschke", "authors": "Peter Mutschke, Karima Haddou ou Moussa", "title": "How can heat maps of indexing vocabularies be utilized for information\n  seeking purposes?", "comments": "URL workshop proceedings: http://ceur-ws.org/Vol-1311/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to browse an information space in a structured way by exploiting\nsimilarities and dissimilarities between information objects is crucial for\nknowledge discovery. Knowledge maps use visualizations to gain insights into\nthe structure of large-scale information spaces, but are still far away from\nbeing applicable for searching. The paper proposes a use case for enhancing\nsearch term recommendations by heat map visualizations of co-word\nrelation-ships taken from indexing vocabulary. By contrasting areas of\ndifferent \"heat\" the user is enabled to indicate mainstream areas of the field\nin question more easily.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 17:38:48 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Mutschke", "Peter", ""], ["Moussa", "Karima Haddou ou", ""]]}, {"id": "1502.02233", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Adham Beykikhoshk, Ognjen Arandjelovic, Dinh Phung, Svetha Venkatesh", "title": "Hierarchical Dirichlet process for tracking complex topical structure\n  evolution and its application to autism research literature", "comments": "In Proc. Pacific-Asia Conference on Knowledge Discovery and Data\n  Mining (PAKDD), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a novel framework for the discovery of the topical\ncontent of a data corpus, and the tracking of its complex structural changes\nacross the temporal dimension. In contrast to previous work our model does not\nimpose a prior on the rate at which documents are added to the corpus nor does\nit adopt the Markovian assumption which overly restricts the type of changes\nthat the model can capture. Our key technical contribution is a framework based\non (i) discretization of time into epochs, (ii) epoch-wise topic discovery\nusing a hierarchical Dirichlet process-based model, and (iii) a temporal\nsimilarity graph which allows for the modelling of complex topic changes:\nemergence and disappearance, evolution, and splitting and merging. The power of\nthe proposed framework is demonstrated on the medical literature corpus\nconcerned with the autism spectrum disorder (ASD) - an increasingly important\nresearch subject of significant social and healthcare importance. In addition\nto the collected ASD literature corpus which we will make freely available, our\ncontributions also include two free online tools we built as aids to ASD\nresearchers. These can be used for semantically meaningful navigation and\nsearching, as well as knowledge discovery from this large and rapidly growing\ncorpus of literature.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 10:25:20 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Beykikhoshk", "Adham", ""], ["Arandjelovic", "Ognjen", ""], ["Phung", "Dinh", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1502.02277", "submitter": "Seung-Hoon Na", "authors": "Seung-Hoon Na and In-Su Kang and Jong-Hyeok Lee", "title": "Improving Term Frequency Normalization for Multi-topical Documents, and\n  Application to Language Modeling Approaches", "comments": "8 pages, conference paper, published in ECIR '08", "journal-ref": "Advances in Information Retrieval Lecture Notes in Computer\n  Science Volume 4956, 2008, pp 382-393", "doi": "10.1007/978-3-540-78646-7_35", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Term frequency normalization is a serious issue since lengths of documents\nare various. Generally, documents become long due to two different reasons -\nverbosity and multi-topicality. First, verbosity means that the same topic is\nrepeatedly mentioned by terms related to the topic, so that term frequency is\nmore increased than the well-summarized one. Second, multi-topicality indicates\nthat a document has a broad discussion of multi-topics, rather than single\ntopic. Although these document characteristics should be differently handled,\nall previous methods of term frequency normalization have ignored these\ndifferences and have used a simplified length-driven approach which decreases\nthe term frequency by only the length of a document, causing an unreasonable\npenalization. To attack this problem, we propose a novel TF normalization\nmethod which is a type of partially-axiomatic approach. We first formulate two\nformal constraints that the retrieval model should satisfy for documents having\nverbose and multi-topicality characteristic, respectively. Then, we modify\nlanguage modeling approaches to better satisfy these two constraints, and\nderive novel smoothing methods. Experimental results show that the proposed\nmethod increases significantly the precision for keyword queries, and\nsubstantially improves MAP (Mean Average Precision) for verbose queries.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 17:32:44 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Na", "Seung-Hoon", ""], ["Kang", "In-Su", ""], ["Lee", "Jong-Hyeok", ""]]}, {"id": "1502.02777", "submitter": "Jared Lorince", "authors": "Jared Lorince, Sam Zorowitz, Jaimie Murdock, Peter M. Todd", "title": "The Wisdom of the Few? \"Supertaggers\" in Collaborative Tagging Systems", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": "10.1561/106.00000002", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A folksonomy is ostensibly an information structure built up by the \"wisdom\nof the crowd\", but is the \"crowd\" really doing the work? Tagging is in fact a\nsharply skewed process in which a small minority of \"supertagger\" users\ngenerate an overwhelming majority of the annotations. Using data from three\nlarge-scale social tagging platforms, we explore (a) how to best quantify the\nimbalance in tagging behavior and formally define a supertagger, (b) how\nsupertaggers differ from other users in their tagging patterns, and (c) if\neffects of motivation and expertise inform our understanding of what makes a\nsupertagger. Our results indicate that such prolific users not only tag more\nthan their counterparts, but in quantifiably different ways. Specifically, we\nfind that supertaggers are more likely to label content in the long tail of\nless popular items, that they show differences in patterns of content tagged\nand terms utilized, and are measurably different with respect to tagging\nexpertise and motivation. These findings suggest we should question the extent\nto which folksonomies achieve crowdsourced classification via the \"wisdom of\nthe crowd\", especially for broad folksonomies like Last.fm as opposed to narrow\nfolksonomies like Flickr.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 04:33:39 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2015 16:21:54 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Lorince", "Jared", ""], ["Zorowitz", "Sam", ""], ["Murdock", "Jaimie", ""], ["Todd", "Peter M.", ""]]}, {"id": "1502.03190", "submitter": "Xiahong Lin", "authors": "Xiahong Lin, Zhi Wang, Lifeng Sun", "title": "MAP: Microblogging Assisted Profiling of TV Shows", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-14445-0_38", "report-no": null, "categories": "cs.IR cs.MM cs.SI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Online microblogging services that have been increasingly used by people to\nshare and exchange information, have emerged as a promising way to profiling\nmultimedia contents, in a sense to provide users a socialized abstraction and\nunderstanding of these contents. In this paper, we propose a microblogging\nprofiling framework, to provide a social demonstration of TV shows. Challenges\nfor this study lie in two folds: First, TV shows are generally offline, i.e.,\nmost of them are not originally from the Internet, and we need to create a\nconnection between these TV shows with online microblogging services; Second,\ncontents in a microblogging service are extremely noisy for video profiling,\nand we need to strategically retrieve the most related information for the TV\nshow profiling.To address these challenges, we propose a MAP, a\nmicroblogging-assisted profiling framework, with contributions as follows: i)\nWe propose a joint user and content retrieval scheme, which uses information\nabout both actors and topics of a TV show to retrieve related microblogs; ii)\nWe propose a social-aware profiling strategy, which profiles a video according\nto not only its content, but also the social relationship of its microblogging\nusers and its propagation in the social network; iii) We present some\ninteresting analysis, based on our framework to profile real-world TV shows.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 04:13:30 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Lin", "Xiahong", ""], ["Wang", "Zhi", ""], ["Sun", "Lifeng", ""]]}, {"id": "1502.03215", "submitter": "Pratyaydipta Rudra", "authors": "Smarajit Bose, Amita Pal, Jhimli Mallick, Sunil Kumar and Pratyaydipta\n  Rudra", "title": "A Hybrid Approach for Improved Content-based Image Retrieval using\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of Content-Based Image Retrieval (CBIR) methods is essentially\nto extract, from large (image) databases, a specified number of images similar\nin visual and semantic content to a so-called query image. To bridge the\nsemantic gap that exists between the representation of an image by low-level\nfeatures (namely, colour, shape, texture) and its high-level semantic content\nas perceived by humans, CBIR systems typically make use of the relevance\nfeedback (RF) mechanism. RF iteratively incorporates user-given inputs\nregarding the relevance of retrieved images, to improve retrieval efficiency.\nOne approach is to vary the weights of the features dynamically via feature\nreweighting. In this work, an attempt has been made to improve retrieval\naccuracy by enhancing a CBIR system based on color features alone, through\nimplicit incorporation of shape information obtained through prior segmentation\nof the images. Novel schemes for feature reweighting as well as for\ninitialization of the relevant set for improved relevance feedback, have also\nbeen proposed for boosting performance of RF- based CBIR. At the same time, new\nmeasures for evaluation of retrieval accuracy have been suggested, to overcome\nthe limitations of existing measures in the RF context. Results of extensive\nexperiments have been presented to illustrate the effectiveness of the proposed\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 08:23:05 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Bose", "Smarajit", ""], ["Pal", "Amita", ""], ["Mallick", "Jhimli", ""], ["Kumar", "Sunil", ""], ["Rudra", "Pratyaydipta", ""]]}, {"id": "1502.03519", "submitter": "Xin Luna Dong", "authors": "Xin Luna Dong, Evgeniy Gabrilovich, Kevin Murphy, Van Dang, Wilko\n  Horn, Camillo Lugaresi, Shaohua Sun, Wei Zhang", "title": "Knowledge-Based Trust: Estimating the Trustworthiness of Web Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of web sources has been traditionally evaluated using exogenous\nsignals such as the hyperlink structure of the graph. We propose a new approach\nthat relies on endogenous signals, namely, the correctness of factual\ninformation provided by the source. A source that has few false facts is\nconsidered to be trustworthy. The facts are automatically extracted from each\nsource by information extraction methods commonly used to construct knowledge\nbases. We propose a way to distinguish errors made in the extraction process\nfrom factual errors in the web source per se, by using joint inference in a\nnovel multi-layer probabilistic model. We call the trustworthiness score we\ncomputed Knowledge-Based Trust (KBT). On synthetic data, we show that our\nmethod can reliably compute the true trustworthiness levels of the sources. We\nthen apply it to a database of 2.8B facts extracted from the web, and thereby\nestimate the trustworthiness of 119M webpages. Manual evaluation of a subset of\nthe results confirms the effectiveness of the method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 02:45:06 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Dong", "Xin Luna", ""], ["Gabrilovich", "Evgeniy", ""], ["Murphy", "Kevin", ""], ["Dang", "Van", ""], ["Horn", "Wilko", ""], ["Lugaresi", "Camillo", ""], ["Sun", "Shaohua", ""], ["Zhang", "Wei", ""]]}, {"id": "1502.03630", "submitter": "Min Yang", "authors": "Min Yang, Tianyi Cui, Wenting Tu", "title": "Ordering-sensitive and Semantic-aware Topic Modeling", "comments": "To appear in proceedings of AAAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling of textual corpora is an important and challenging problem. In\nmost previous work, the \"bag-of-words\" assumption is usually made which ignores\nthe ordering of words. This assumption simplifies the computation, but it\nunrealistically loses the ordering information and the semantic of words in the\ncontext. In this paper, we present a Gaussian Mixture Neural Topic Model\n(GMNTM) which incorporates both the ordering of words and the semantic meaning\nof sentences into topic modeling. Specifically, we represent each topic as a\ncluster of multi-dimensional vectors and embed the corpus into a collection of\nvectors generated by the Gaussian mixture model. Each word is affected not only\nby its topic, but also by the embedding vector of its surrounding words and the\ncontext. The Gaussian mixture components and the topic of documents, sentences\nand words can be learnt jointly. Extensive experiments show that our model can\nlearn better topics and more accurate word distributions for each topic.\nQuantitatively, comparing to state-of-the-art topic modeling approaches, GMNTM\nobtains significantly better performance in terms of perplexity, retrieval\naccuracy and classification accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 12:32:39 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Yang", "Min", ""], ["Cui", "Tianyi", ""], ["Tu", "Wenting", ""]]}, {"id": "1502.03682", "submitter": "Matthias Samwald", "authors": "Jose Antonio Mi\\~narro-Gim\\'enez, Oscar Mar\\'in-Alonso, Matthias\n  Samwald", "title": "Applying deep learning techniques on medical corpora from the World Wide\n  Web: a prototypical system and evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  BACKGROUND: The amount of biomedical literature is rapidly growing and it is\nbecoming increasingly difficult to keep manually curated knowledge bases and\nontologies up-to-date. In this study we applied the word2vec deep learning\ntoolkit to medical corpora to test its potential for identifying relationships\nfrom unstructured text. We evaluated the efficiency of word2vec in identifying\nproperties of pharmaceuticals based on mid-sized, unstructured medical text\ncorpora available on the web. Properties included relationships to diseases\n('may treat') or physiological processes ('has physiological effect'). We\ncompared the relationships identified by word2vec with manually curated\ninformation from the National Drug File - Reference Terminology (NDF-RT)\nontology as a gold standard. RESULTS: Our results revealed a maximum accuracy\nof 49.28% which suggests a limited ability of word2vec to capture linguistic\nregularities on the collected medical corpora compared with other published\nresults. We were able to document the influence of different parameter settings\non result accuracy and found and unexpected trade-off between ranking quality\nand accuracy. Pre-processing corpora to reduce syntactic variability proved to\nbe a good strategy for increasing the utility of the trained vector models.\nCONCLUSIONS: Word2vec is a very efficient implementation for computing vector\nrepresentations and for its ability to identify relationships in textual data\nwithout any prior domain knowledge. We found that the ranking and retrieved\nresults generated by word2vec were not of sufficient quality for automatic\npopulation of knowledge bases and ontologies, but could serve as a starting\npoint for further manual curation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 14:44:15 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Mi\u00f1arro-Gim\u00e9nez", "Jose Antonio", ""], ["Mar\u00edn-Alonso", "Oscar", ""], ["Samwald", "Matthias", ""]]}, {"id": "1502.04032", "submitter": "Catarina Moreira", "authors": "Andreas Wichert and Catarina Moreira", "title": "On Projection Based Operators in Lp space for Exact Similarity Search", "comments": null, "journal-ref": "Fundamenta Informaticae: Annales Societatis Mathematicae Polonae,\n  136: 1-14, 2015", "doi": "10.3233/FI-2015-1166", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate exact indexing for high dimensional Lp norms based on the\n1-Lipschitz property and projection operators. The orthogonal projection that\nsatisfies the 1-Lipschitz property for the Lp norm is described. The adaptive\nprojection defined by the first principal component is introduced.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 10:56:10 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Wichert", "Andreas", ""], ["Moreira", "Catarina", ""]]}, {"id": "1502.04163", "submitter": "Zhang Junlin", "authors": "Zhang Junlin, Cai Heng, Huang Tongwen, Xue Huiping", "title": "A Distributional Representation Model For Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a very concise deep learning approach for\ncollaborative filtering that jointly models distributional representation for\nusers and items. The proposed framework obtains better performance when\ncompared against current state-of-art algorithms and that made the\ndistributional representation model a promising direction for further research\nin the collaborative filtering.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 03:23:53 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Junlin", "Zhang", ""], ["Heng", "Cai", ""], ["Tongwen", "Huang", ""], ["Huiping", "Xue", ""]]}, {"id": "1502.04331", "submitter": "Seung-Hoon Na", "authors": "Seung-Hoon Na", "title": "Two-Stage Document Length Normalization for Information Retrieval", "comments": "40 pages (to appear in ACM TOIS)", "journal-ref": "ACM Transactions on Information Systems (TOIS), vol. 33, no. 2,\n  2015", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard approach for term frequency normalization is based only on the\ndocument length. However, it does not distinguish the verbosity from the scope,\nthese being the two main factors determining the document length. Because the\nverbosity and scope have largely different effects on the increase in term\nfrequency, the standard approach can easily suffer from insufficient or\nexcessive penalization depending on the specific type of long document. To\novercome these problems, this paper proposes two-stage normalization by\nperforming verbosity and scope normalization separately, and by employing\ndifferent penalization functions. In verbosity normalization, each document is\npre-normalized by dividing the term frequency by the verbosity of the document.\nIn scope normalization, an existing retrieval model is applied in a\nstraightforward manner to the pre-normalized document, finally leading us to\nformulate our proposed verbosity normalized (VN) retrieval model. Experimental\nresults carried out on standard TREC collections demonstrate that the VN model\nleads to marginal but statistically significant improvements over standard\nretrieval models.\n", "versions": [{"version": "v1", "created": "Sun, 15 Feb 2015 16:49:09 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Na", "Seung-Hoon", ""]]}, {"id": "1502.04348", "submitter": "Gregory Hasseler", "authors": "Norman Ahmed, Jason Bryant, Gregory Hasseler, Matthew Paulini", "title": "Semantic Modeling of Analytic-based Relationships with Direct\n  Qualification", "comments": "Proceedings of the 2015 IEEE 9th International Conference on Semantic\n  Computing (IEEE ICSC 2015)", "journal-ref": null, "doi": "10.1109/ICOSC.2015.7050845", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Successfully modeling state and analytics-based semantic relationships of\ndocuments enhances representation, importance, relevancy, provenience, and\npriority of the document. These attributes are the core elements that form the\nmachine-based knowledge representation for documents. However, modeling\ndocument relationships that can change over time can be inelegant, limited,\ncomplex or overly burdensome for semantic technologies. In this paper, we\npresent Direct Qualification (DQ), an approach for modeling any semantically\nreferenced document, concept, or named graph with results from associated\napplied analytics. The proposed approach supplements the traditional\nsubject-object relationships by providing a third leg to the relationship; the\nqualification of how and why the relationship exists. To illustrate, we show a\nprototype of an event-based system with a realistic use case for applying DQ to\nrelevancy analytics of PageRank and Hyperlink-Induced Topic Search (HITS).\n", "versions": [{"version": "v1", "created": "Sun, 15 Feb 2015 19:01:14 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Ahmed", "Norman", ""], ["Bryant", "Jason", ""], ["Hasseler", "Gregory", ""], ["Paulini", "Matthew", ""]]}, {"id": "1502.04609", "submitter": "Derek Greene", "authors": "Derek Greene, Daniel Archambault, V\\'aclav Bel\\'ak, P\\'adraig\n  Cunningham", "title": "TextLuas: Tracking and Visualizing Document and Term Clusters in Dynamic\n  Text Data", "comments": "21 page version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For large volumes of text data collected over time, a key knowledge discovery\ntask is identifying and tracking clusters. These clusters may correspond to\nemerging themes, popular topics, or breaking news stories in a corpus.\nTherefore, recently there has been increased interest in the problem of\nclustering dynamic data. However, there exists little support for the\ninteractive exploration of the output of these analysis techniques,\nparticularly in cases where researchers wish to simultaneously explore both the\nchange in cluster structure over time and the change in the textual content\nassociated with clusters. In this paper, we propose a model for tracking\ndynamic clusters characterized by the evolutionary events of each cluster.\nMotivated by this model, the TextLuas system provides an implementation for\ntracking these dynamic clusters and visualizing their evolution using a metro\nmap metaphor. To provide overviews of cluster content, we adapt the tag cloud\nrepresentation to the dynamic clustering scenario. We demonstrate the TextLuas\nsystem on two different text corpora, where they are shown to elucidate the\nevolution of key themes. We also describe how TextLuas was applied to a problem\nin bibliographic network research.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 19:13:02 GMT"}], "update_date": "2016-08-07", "authors_parsed": [["Greene", "Derek", ""], ["Archambault", "Daniel", ""], ["Bel\u00e1k", "V\u00e1clav", ""], ["Cunningham", "P\u00e1draig", ""]]}, {"id": "1502.04662", "submitter": "Tim Althoff", "authors": "Tim Althoff, Xin Luna Dong, Kevin Murphy, Safa Alai, Van Dang, Wei\n  Zhang", "title": "TimeMachine: Timeline Generation for Knowledge-Base Entities", "comments": "To appear at ACM SIGKDD KDD'15. 12pp, 7 fig. With appendix. Demo and\n  other info available at http://cs.stanford.edu/~althoff/timemachine/", "journal-ref": null, "doi": "10.1145/2783258.2783325", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method called TIMEMACHINE to generate a timeline of events and\nrelations for entities in a knowledge base. For example for an actor, such a\ntimeline should show the most important professional and personal milestones\nand relationships such as works, awards, collaborations, and family\nrelationships. We develop three orthogonal timeline quality criteria that an\nideal timeline should satisfy: (1) it shows events that are relevant to the\nentity; (2) it shows events that are temporally diverse, so they distribute\nalong the time axis, avoiding visual crowding and allowing for easy user\ninteraction, such as zooming in and out; and (3) it shows events that are\ncontent diverse, so they contain many different types of events (e.g., for an\nactor, it should show movies and marriages and awards, not just movies). We\npresent an algorithm to generate such timelines for a given time period and\nscreen size, based on submodular optimization and web-co-occurrence statistics\nwith provable performance guarantees. A series of user studies using Mechanical\nTurk shows that all three quality criteria are crucial to produce quality\ntimelines and that our algorithm significantly outperforms various baseline and\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 18:53:01 GMT"}, {"version": "v2", "created": "Sat, 21 Feb 2015 07:02:11 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2015 20:39:26 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Althoff", "Tim", ""], ["Dong", "Xin Luna", ""], ["Murphy", "Kevin", ""], ["Alai", "Safa", ""], ["Dang", "Van", ""], ["Zhang", "Wei", ""]]}, {"id": "1502.04696", "submitter": "Gregory Hasseler", "authors": "Jason Bryant, Gregory Hasseler, Timothy Lebo, Matthew Paulini", "title": "Enhancing Information Awareness Through Directed Qualification of\n  Semantic Relevancy Scoring Operations", "comments": "Proceedings of the 19th International Command and Control Research\n  and Technology Symposium. arXiv admin note: substantial text overlap with\n  arXiv:1502.04348", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Successfully managing analytics-based semantic relationships and their\nprovenance enables determinations of document importance and priority,\nfurthering capabilities for machine-based relevancy scoring operations.\nSemantic technologies are well suited for modeling explicit and fully qualified\nrelationships but struggle with modeling relationships that are qualified in\nnature, or resultant from applied analytics. Our work seeks to implement the\nautonomous Directed Qualification of analytic-based relationships by pairing\nthe Prov-O Ontology (W3C Recommendation) with a relevancy ontology supporting\nanalytics terminology. This work results in the capability for any semantically\nreferenced document, concept, or named graph to be associated with the results\nof applied analytics as Direct Qualification (DQ) modeled relational nodes.\nThis new capability will enable role, identity, or any other content-based\nmeasures of relevancy and analytics-based metrics for semantically described\ndocuments.\n", "versions": [{"version": "v1", "created": "Sun, 15 Feb 2015 19:13:26 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Bryant", "Jason", ""], ["Hasseler", "Gregory", ""], ["Lebo", "Timothy", ""], ["Paulini", "Matthew", ""]]}, {"id": "1502.04823", "submitter": "Hui Zhang", "authors": "Hui Zhang, Kiduk Yang, Elin Jacob", "title": "Topic Level Disambiguation for Weak Queries", "comments": null, "journal-ref": "Journal of Information Science Theory and Practice, 1(3), 33-46", "doi": "10.1633/JISTaP.2013.1.3.3", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite limited success, information retrieval (IR) systems today are not\nintelligent or reliable. IR systems return poor search results when users\nformulate their information needs into incomplete or ambiguous queries (i.e.,\nweak queries). Therefore, one of the main challenges in modern IR research is\nto provide consistent results across all queries by improving the performance\non weak queries. However, existing IR approaches such as query expansion are\nnot overly effective because they make little effort to analyze and exploit the\nmeanings of the queries. Furthermore, word sense disambiguation approaches,\nwhich rely on textual context, are ineffective against weak queries that are\ntypically short. Motivated by the demand for a robust IR system that can\nconsistently provide highly accurate results, the proposed study implemented a\nnovel topic detection that leveraged both the language model and structural\nknowledge of Wikipedia and systematically evaluated the effect of query\ndisambiguation and topic-based retrieval approaches on TREC collections. The\nresults not only confirm the effectiveness of the proposed topic detection and\ntopic-based retrieval approaches but also demonstrate that query disambiguation\ndoes not improve IR as expected.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 08:14:51 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Zhang", "Hui", ""], ["Yang", "Kiduk", ""], ["Jacob", "Elin", ""]]}, {"id": "1502.04885", "submitter": "Guillaume Pitel", "authors": "Guillaume Pitel and Geoffroy Fouquier", "title": "Count-Min-Log sketch: Approximately counting with approximate counters", "comments": "7 pages, 3 figures. Some preliminary notes can be found on\n  http://blog.guillaume-pitel.fr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Count-Min Sketch is a widely adopted algorithm for approximate event counting\nin large scale processing. However, the original version of the\nCount-Min-Sketch (CMS) suffers of some deficiences, especially if one is\ninterested by the low-frequency items, such as in text-mining related tasks.\nSeveral variants of CMS have been proposed to compensate for the high relative\nerror for low-frequency events, but the proposed solutions tend to correct the\nerrors instead of preventing them. In this paper, we propose the Count-Min-Log\nsketch, which uses logarithm-based, approximate counters instead of linear\ncounters to improve the average relative error of CMS at constant memory\nfootprint.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 13:17:17 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Pitel", "Guillaume", ""], ["Fouquier", "Geoffroy", ""]]}, {"id": "1502.05131", "submitter": "Ju-Chiang Wang", "authors": "Ju-Chiang Wang, Yi-Hsuan Yang, Hsin-Min Wang", "title": "Affective Music Information Retrieval", "comments": "40 pages, 18 figures, 5 tables, author version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Much of the appeal of music lies in its power to convey emotions/moods and to\nevoke them in listeners. In consequence, the past decade witnessed a growing\ninterest in modeling emotions from musical signals in the music information\nretrieval (MIR) community. In this article, we present a novel generative\napproach to music emotion modeling, with a specific focus on the\nvalence-arousal (VA) dimension model of emotion. The presented generative\nmodel, called \\emph{acoustic emotion Gaussians} (AEG), better accounts for the\nsubjectivity of emotion perception by the use of probability distributions.\nSpecifically, it learns from the emotion annotations of multiple subjects a\nGaussian mixture model in the VA space with prior constraints on the\ncorresponding acoustic features of the training music pieces. Such a\ncomputational framework is technically sound, capable of learning in an online\nfashion, and thus applicable to a variety of applications, including\nuser-independent (general) and user-dependent (personalized) emotion\nrecognition and emotion-based music retrieval. We report evaluations of the\naforementioned applications of AEG on a larger-scale emotion-annotated corpora,\nAMG1608, to demonstrate the effectiveness of AEG and to showcase how\nevaluations are conducted for research on emotion-based MIR. Directions of\nfuture work are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 06:29:45 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Wang", "Ju-Chiang", ""], ["Yang", "Yi-Hsuan", ""], ["Wang", "Hsin-Min", ""]]}, {"id": "1502.05168", "submitter": "Rekha Vaidyanathan", "authors": "Rekha Vaidyanathan, Sujoy Das, Namita Srivastava", "title": "Query Expansion Strategy based on Pseudo Relevance Feedback and Term\n  Weight Scheme for Monolingual Retrieval", "comments": null, "journal-ref": "International Journal of Computer Applications 105(8):1-6,\n  November 2014", "doi": "10.5120/18394-9646", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query Expansion using Pseudo Relevance Feedback is a useful and a popular\ntechnique for reformulating the query. In our proposed query expansion method,\nwe assume that relevant information can be found within a document near the\ncentral idea. The document is normally divided into sections, paragraphs and\nlines. The proposed method tries to extract keywords that are closer to the\ncentral theme of the document. The expansion terms are obtained by\nequi-frequency partition of the documents obtained from pseudo relevance\nfeedback and by using tf-idf scores. The idf factor is calculated for number of\npartitions in documents. The group of words for query expansion is selected\nusing the following approaches: the highest score, average score and a group of\nwords that has maximum number of keywords. As each query behaved differently\nfor different methods, the effect of these methods in selecting the words for\nquery expansion is investigated. From this initial study, we extend the\nexperiment to develop a rule-based statistical model that automatically selects\nthe best group of words incorporating the tf-idf scoring and the 3 approaches\nexplained here, in the future. The experiments were performed on FIRE 2011\nAdhoc Hindi and English test collections on 50 queries each, using Terrier as\nretrieval engine.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 09:55:37 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Vaidyanathan", "Rekha", ""], ["Das", "Sujoy", ""], ["Srivastava", "Namita", ""]]}, {"id": "1502.05441", "submitter": "Ahmad Hassanat", "authors": "Ahmad B.A. Hassanat and Ghada Awad Altarawneh", "title": "Rule-and Dictionary-based Solution for Variations in Written Arabic\n  Names in Social Networks, Big Data, Accounting Systems and Large Databases", "comments": null, "journal-ref": "Research Journal of Applied Sciences, Engineering and Technology,\n  2014, 8(14): 1630-1638", "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem that some Arabic names can be written in\nmultiple ways. When someone searches for only one form of a name, neither exact\nnor approximate matching is appropriate for returning the multiple variants of\nthe name. Exact matching requires the user to enter all forms of the name for\nthe search, and approximate matching yields names not among the variations of\nthe one being sought. In this paper, we attempt to solve the problem with a\ndictionary of all Arabic names mapped to their different (alternative) writing\nforms. We generated alternatives based on rules we derived from reviewing the\nfirst names of 9.9 million citizens and former citizens of Jordan. This\ndictionary can be used for both standardizing the written form when inserting a\nnew name into a database and for searching for the name and all its alternative\nwritten forms. Creating the dictionary automatically based on rules resulted in\nat least 7% erroneous acceptance errors and 7.9% erroneous rejection errors. We\naddressed the errors by manually editing the dictionary. The dictionary can be\nof help to real world-databases, with the qualification that manual editing\ndoes not guarantee 100% correctness.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 23:16:00 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Hassanat", "Ahmad B. A.", ""], ["Altarawneh", "Ghada Awad", ""]]}, {"id": "1502.05472", "submitter": "Fabrizio Sebastiani", "authors": "Diego Marcheggiani and Fabrizio Sebastiani", "title": "On the Effects of Low-Quality Training Data on Information Extraction\n  from Clinical Reports", "comments": "Submitted for publication", "journal-ref": null, "doi": "10.1145/3106235", "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In the last five years there has been a flurry of work on information\nextraction from clinical documents, i.e., on algorithms capable of extracting,\nfrom the informal and unstructured texts that are generated during everyday\nclinical practice, mentions of concepts relevant to such practice. Most of this\nliterature is about methods based on supervised learning, i.e., methods for\ntraining an information extraction system from manually annotated examples.\nWhile a lot of work has been devoted to devising learning methods that generate\nmore and more accurate information extractors, no work has been devoted to\ninvestigating the effect of the quality of training data on the learning\nprocess. Low quality in training data often derives from the fact that the\nperson who has annotated the data is different from the one against whose\njudgment the automatically annotated data must be evaluated. In this paper we\ntest the impact of such data quality issues on the accuracy of information\nextraction systems as applied to the clinical domain. We do this by comparing\nthe accuracy deriving from training data annotated by the authoritative coder\n(i.e., the one who has also annotated the test data, and by whose judgment we\nmust abide), with the accuracy deriving from training data annotated by a\ndifferent coder. The results indicate that, although the disagreement between\nthe two coders (as measured on the training set) is substantial, the difference\nis (surprisingly enough) not always statistically significant.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 06:04:40 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 08:08:49 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Marcheggiani", "Diego", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1502.05491", "submitter": "Fabrizio Sebastiani", "authors": "Andrea Esuli and Fabrizio Sebastiani", "title": "Optimizing Text Quantifiers for Multivariate Loss Functions", "comments": "In press in ACM Transactions on Knowledge Discovery from Data, 2015", "journal-ref": null, "doi": "10.1145/2700406", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We address the problem of \\emph{quantification}, a supervised learning task\nwhose goal is, given a class, to estimate the relative frequency (or\n\\emph{prevalence}) of the class in a dataset of unlabelled items.\nQuantification has several applications in data and text mining, such as\nestimating the prevalence of positive reviews in a set of reviews of a given\nproduct, or estimating the prevalence of a given support issue in a dataset of\ntranscripts of phone calls to tech support. So far, quantification has been\naddressed by learning a general-purpose classifier, counting the unlabelled\nitems which have been assigned the class, and tuning the obtained counts\naccording to some heuristics. In this paper we depart from the tradition of\nusing general-purpose classifiers, and use instead a supervised learning model\nfor \\emph{structured prediction}, capable of generating classifiers directly\noptimized for the (multivariate and non-linear) function used for evaluating\nquantification accuracy. The experiments that we have run on 5500 binary\nhigh-dimensional datasets (averaging more than 14,000 documents each) show that\nthis method is more accurate, more stable, and more efficient than existing,\nstate-of-the-art quantification methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 08:06:54 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 14:45:59 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Esuli", "Andrea", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1502.05535", "submitter": "Dmytro Filatov", "authors": "Dmytro Filatov and Taras Filatov", "title": "Evolutionary algorithm based adaptive navigation in information\n  retrieval interfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer interfaces in general, especially in information retrieval tasks,\nit is important to be able to quickly find and retrieve information. State of\nthe art approach, used, for example, in search engines, is not effective as it\nintroduces losses of meanings due to context to keywords back and forth\ntranslation. Authors argue it increases the time and reduces the accuracy of\ninformation retrieval compared to what it could be in the system that employs\nmodern information retrieval and text mining methods while presenting results\nin an adaptive human- computer interface where system effectively learns what\noperator needs through iterative interaction. In current work, a combination of\nadaptive navigational interface and real time collaborative feedback analysis\nfor documents relevance weighting is proposed as an viable alternative to\nprevailing \"telegraphic\" approach in information retrieval systems. Adaptive\nnavigation is provided through a dynamic links panel controlled by an\nevolutionary algorithm. Documents relevance is initially established with\nstandard information retrieval techniques and is further refined in real time\nthrough interaction of users with the system. Introduced concepts of\nmultidimensional Knowledge Map and Weighted Point of Interest allow finding\nrelevant documents and users with common interests through a trivial\ncalculation. Browsing search approach, the ability of the algorithm to adapt\nnavigation to users interests, collaborative refinement and the self-organising\nfeatures of the system are the main factors making such architecture effective\nin various fields where non-structured knowledge shall be represented to the\nusers.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 11:47:28 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Filatov", "Dmytro", ""], ["Filatov", "Taras", ""]]}, {"id": "1502.05955", "submitter": "Edith Cohen", "authors": "Edith Cohen", "title": "Stream Sampling for Frequency Cap Statistics", "comments": "21 pages, 4 figures, preliminary version will appear in KDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unaggregated data, in streamed or distributed form, is prevalent and come\nfrom diverse application domains which include interactions of users with web\nservices and IP traffic. Data elements have {\\em keys} (cookies, users,\nqueries) and elements with different keys interleave. Analytics on such data\ntypically utilizes statistics stated in terms of the frequencies of keys. The\ntwo most common statistics are {\\em distinct}, which is the number of active\nkeys in a specified segment, and {\\em sum}, which is the sum of the frequencies\nof keys in the segment. Both are special cases of {\\em cap} statistics, defined\nas the sum of frequencies {\\em capped} by a parameter $T$, which are popular in\nonline advertising platforms. Aggregation by key, however, is costly, requiring\nstate proportional to the number of distinct keys, and therefore we are\ninterested in estimating these statistics or more generally, sampling the data,\nwithout aggregation. We present a sampling framework for unaggregated data that\nuses a single pass (for streams) or two passes (for distributed data) and state\nproportional to the desired sample size. Our design provides the first\neffective solution for general frequency cap statistics. Our $\\ell$-capped\nsamples provide estimates with tight statistical guarantees for cap statistics\nwith $T=\\Theta(\\ell)$ and nonnegative unbiased estimates of {\\em any} monotone\nnon-decreasing frequency statistics. An added benefit of our unified design is\nfacilitating {\\em multi-objective samples}, which provide estimates with\nstatistical guarantees for a specified set of different statistics, using a\nsingle, smaller sample.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 17:53:45 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2015 13:49:41 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Cohen", "Edith", ""]]}, {"id": "1502.05957", "submitter": "Paul Vitanyi", "authors": "Andrew R. Cohen (Dept Electri. Comput. Eng., Drexel Univ.), Paul M.B.\n  Vitanyi (CWI and University of Amsterdam)", "title": "Web Similarity in Sets of Search Terms using Database Queries", "comments": "LaTeX 18 pages, 3 tables. A precursor is arXiv:1308.3177", "journal-ref": "SN COMPUT. SCI. 1, 161(2020)", "doi": "10.1007/s42979-020-00148-5", "report-no": null, "categories": "cs.IR cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalized web distance (NWD) is a similarity or normalized semantic distance\nbased on the World Wide Web or another large electronic database, for instance\nWikipedia, and a search engine that returns reliable aggregate page counts. For\nsets of search terms the NWD gives a common similarity (common semantics) on a\nscale from 0 (identical) to 1 (completely different). The NWD approximates the\nsimilarity of members of a set according to all (upper semi)computable\nproperties. We develop the theory and give applications of classifying using\nAmazon, Wikipedia, and the NCBI website from the National Institutes of Health.\nThe last gives new correlations between health hazards. A restriction of the\nNWD to a set of two yields the earlier normalized google distance (NGD) but no\ncombination of the NGD's of pairs in a set can extract the information the NWD\nextracts from the set. The NWD enables a new contextual (different databases)\nlearning approachbased on Kolmogorov complexity theory that incorporates\nknowledge from these databases.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 17:55:58 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 16:27:48 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Cohen", "Andrew R.", "", "Dept Electri. Comput. Eng., Drexel Univ."], ["Vitanyi", "Paul M. B.", "", "CWI and University of Amsterdam"]]}, {"id": "1502.06084", "submitter": "Jieming Zhu", "authors": "Jieming Zhu, Pinjia He, Zibin Zheng, Michael R. Lyu", "title": "A Privacy-Preserving QoS Prediction Framework for Web Service\n  Recommendation", "comments": "This paper is published in IEEE International Conference on Web\n  Services (ICWS'15)", "journal-ref": null, "doi": "10.1109/ICWS.2015.41", "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QoS-based Web service recommendation has recently gained much attention for\nproviding a promising way to help users find high-quality services. To\nfacilitate such recommendations, existing studies suggest the use of\ncollaborative filtering techniques for personalized QoS prediction. These\napproaches, by leveraging partially observed QoS values from users, can achieve\nhigh accuracy of QoS predictions on the unobserved ones. However, the\nrequirement to collect users' QoS data likely puts user privacy at risk, thus\nmaking them unwilling to contribute their usage data to a Web service\nrecommender system. As a result, privacy becomes a critical challenge in\ndeveloping practical Web service recommender systems. In this paper, we make\nthe first attempt to cope with the privacy concerns for Web service\nrecommendation. Specifically, we propose a simple yet effective\nprivacy-preserving framework by applying data obfuscation techniques, and\nfurther develop two representative privacy-preserving QoS prediction approaches\nunder this framework. Evaluation results from a publicly-available QoS dataset\nof real-world Web services demonstrate the feasibility and effectiveness of our\nprivacy-preserving QoS prediction approaches. We believe our work can serve as\na good starting point to inspire more research efforts on privacy-preserving\nWeb service recommendation.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 08:14:39 GMT"}, {"version": "v2", "created": "Sat, 2 May 2015 04:43:00 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Zhu", "Jieming", ""], ["He", "Pinjia", ""], ["Zheng", "Zibin", ""], ["Lyu", "Michael R.", ""]]}, {"id": "1502.06124", "submitter": "Dmytro Filatov", "authors": "Dmytro Filatov, Taras Filatov", "title": "Unified vector space mapping for knowledge representation systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most significant problems which inhibits further developments in\nthe areas of Knowledge Representation and Artificial Intelligence is a problem\nof semantic alignment or knowledge mapping. The progress in its solution will\nbe greatly beneficial for further advances of information retrieval, ontology\nalignment, relevance calculation, text mining, natural language processing etc.\nIn the paper the concept of multidimensional global knowledge map, elaborated\nthrough unsupervised extraction of dependencies from large documents corpus, is\nproposed. In addition, the problem of direct Human - Knowledge Representation\nSystem interface is addressed and a concept of adaptive decoder proposed for\nthe purpose of interaction with previously described unified mapping model. In\ncombination these two approaches are suggested as basis for a development of a\nnew generation of knowledge representation systems.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 18:29:57 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Filatov", "Dmytro", ""], ["Filatov", "Taras", ""]]}, {"id": "1502.06161", "submitter": "Thiago Marzag\\~ao", "authors": "Thiago Marzag\\~ao", "title": "Using NLP to measure democracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses natural language processing to create the first machine-coded\ndemocracy index, which I call Automated Democracy Scores (ADS). The ADS are\nbased on 42 million news articles from 6,043 different sources and cover all\nindependent countries in the 1993-2012 period. Unlike the democracy indices we\nhave today the ADS are replicable and have standard errors small enough to\nactually distinguish between cases.\n  The ADS are produced with supervised learning. Three approaches are tried: a)\na combination of Latent Semantic Analysis and tree-based regression methods; b)\na combination of Latent Dirichlet Allocation and tree-based regression methods;\nand c) the Wordscores algorithm. The Wordscores algorithm outperforms the\nalternatives, so it is the one on which the ADS are based.\n  There is a web application where anyone can change the training set and see\nhow the results change: democracy-scores.org\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 01:30:32 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Marzag\u00e3o", "Thiago", ""]]}, {"id": "1502.06922", "submitter": "Hamid Palangi", "authors": "Hamid Palangi, Li Deng, Yelong Shen, Jianfeng Gao, Xiaodong He,\n  Jianshu Chen, Xinying Song, Rabab Ward", "title": "Deep Sentence Embedding Using Long Short-Term Memory Networks: Analysis\n  and Application to Information Retrieval", "comments": "To appear in IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing", "journal-ref": null, "doi": "10.1109/TASLP.2016.2520371", "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a model that addresses sentence embedding, a hot topic in\ncurrent natural language processing research, using recurrent neural networks\nwith Long Short-Term Memory (LSTM) cells. Due to its ability to capture long\nterm memory, the LSTM-RNN accumulates increasingly richer information as it\ngoes through the sentence, and when it reaches the last word, the hidden layer\nof the network provides a semantic representation of the whole sentence. In\nthis paper, the LSTM-RNN is trained in a weakly supervised manner on user\nclick-through data logged by a commercial web search engine. Visualization and\nanalysis are performed to understand how the embedding process works. The model\nis found to automatically attenuate the unimportant words and detects the\nsalient keywords in the sentence. Furthermore, these detected keywords are\nfound to automatically activate different cells of the LSTM-RNN, where words\nbelonging to a similar topic activate the same cell. As a semantic\nrepresentation of the sentence, the embedding vector can be used in many\ndifferent applications. These automatic keyword detection and topic allocation\nabilities enabled by the LSTM-RNN allow the network to perform document\nretrieval, a difficult language processing task, where the similarity between\nthe query and documents can be measured by the distance between their\ncorresponding sentence embedding vectors computed by the LSTM-RNN. On a web\nsearch task, the LSTM-RNN embedding is shown to significantly outperform\nseveral existing state of the art methods. We emphasize that the proposed model\ngenerates sentence embedding vectors that are specially useful for web document\nretrieval tasks. A comparison with a well known general sentence embedding\nmethod, the Paragraph Vector, is performed. The results show that the proposed\nmethod in this paper significantly outperforms it for web document retrieval\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 19:39:27 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2015 06:11:19 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2016 06:35:23 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Palangi", "Hamid", ""], ["Deng", "Li", ""], ["Shen", "Yelong", ""], ["Gao", "Jianfeng", ""], ["He", "Xiaodong", ""], ["Chen", "Jianshu", ""], ["Song", "Xinying", ""], ["Ward", "Rabab", ""]]}, {"id": "1502.07015", "submitter": "Thanh-Cong Dinh", "authors": "Thanh-Cong Dinh, Hyerim Bae, Jaehun Park and Joonsoo Bae", "title": "A framework to discover potential ideas of new product development from\n  crowdsourcing application", "comments": "International Conference on Computer, Networks, Systems, and\n  Industrial Applications (CNSI 2012), Jeju Island, Korea, July 16-18, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study idea mining from crowdsourcing applications which\nencourage a group of people, who are usually undefined and very large sized, to\ngenerate ideas for new product development (NPD). In order to isolate the\nrelatively small number of potential ones among ideas from crowd, decision\nmakers not only have to identify the key textual information representing the\nideas, but they also need to consider online opinions of people who gave\ncomments and votes on the ideas. Due to the extremely large size of text data\ngenerated by people on the Internet, identifying textual information has been\ncarried out in manual ways, and has been considered very time consuming and\ncostly. To overcome the ineffectiveness, this paper introduces a novel\nframework that can help decision makers discover ideas having the potential to\nbe used in an NPD process. To achieve this, a semi-automatic text mining\ntechnique that retrieves useful text patterns from ideas posted on\ncrowdsourcing application is proposed. Then, we provide an online learning\nalgorithm to evaluate whether the idea is potential or not. Finally to verify\nthe effectiveness of our algorithm, we conducted experiments on the data, which\nare collected from an existing crowd sourcing website.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 00:13:44 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Dinh", "Thanh-Cong", ""], ["Bae", "Hyerim", ""], ["Park", "Jaehun", ""], ["Bae", "Joonsoo", ""]]}, {"id": "1502.07041", "submitter": "Jamil Ahmad", "authors": "Jamil Ahmad, Muhammad Sajjad, Irfan Mehmood, Seungmin Rho and Sung\n  Wook Baik", "title": "Describing Colors, Textures and Shapes for Content Based Image Retrieval\n  - A Survey", "comments": null, "journal-ref": "(2014), Journal of Platform Technology 2(4): 34-48", "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Visual media has always been the most enjoyed way of communication. From the\nadvent of television to the modern day hand held computers, we have witnessed\nthe exponential growth of images around us. Undoubtedly it's a fact that they\ncarry a lot of information in them which needs be utilized in an effective\nmanner. Hence intense need has been felt to efficiently index and store large\nimage collections for effective and on- demand retrieval. For this purpose\nlow-level features extracted from the image contents like color, texture and\nshape has been used. Content based image retrieval systems employing these\nfeatures has proven very successful. Image retrieval has promising applications\nin numerous fields and hence has motivated researchers all over the world. New\nand improved ways to represent visual content are being developed each day.\nTremendous amount of research has been carried out in the last decade. In this\npaper we will present a detailed overview of some of the powerful color,\ntexture and shape descriptors for content based image retrieval. A comparative\nanalysis will also be carried out for providing an insight into outstanding\nchallenges in this field.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 03:51:43 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Ahmad", "Jamil", ""], ["Sajjad", "Muhammad", ""], ["Mehmood", "Irfan", ""], ["Rho", "Seungmin", ""], ["Baik", "Sung Wook", ""]]}, {"id": "1502.07058", "submitter": "Adam Harley", "authors": "Adam W. Harley, Alex Ufkes, and Konstantinos G. Derpanis", "title": "Evaluation of Deep Convolutional Nets for Document Image Classification\n  and Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new state-of-the-art for document image classification\nand retrieval, using features learned by deep convolutional neural networks\n(CNNs). In object and scene analysis, deep neural nets are capable of learning\na hierarchical chain of abstraction from pixel inputs to concise and\ndescriptive representations. The current work explores this capacity in the\nrealm of document analysis, and confirms that this representation strategy is\nsuperior to a variety of popular hand-crafted alternatives. Experiments also\nshow that (i) features extracted from CNNs are robust to compression, (ii) CNNs\ntrained on non-document images transfer well to document analysis tasks, and\n(iii) enforcing region-specific feature-learning is unnecessary given\nsufficient training data. This work also makes available a new labelled subset\nof the IIT-CDIP collection, containing 400,000 document images across 16\ncategories, useful for training new CNNs for document analysis.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 05:58:43 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Harley", "Adam W.", ""], ["Ufkes", "Alex", ""], ["Derpanis", "Konstantinos G.", ""]]}, {"id": "1502.07157", "submitter": "Pierre-Francois Marteau", "authors": "Pierre-Fran\\c{c}ois Marteau (IRISA), Guiyao Ke (IRISA)", "title": "Exploiting a comparability mapping to improve bi-lingual data\n  categorization: a three-mode data analysis perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address in this paper the co-clustering and co-classification of bilingual\ndata laying in two linguistic similarity spaces when a comparability measure\ndefining a mapping between these two spaces is available. A new approach that\nwe can characterized as a three-mode analysis scheme, is proposed to mix the\ncomparability measure with the two similarity measures. Our aim is to improve\njointly the accuracy of classification and clustering tasks performed in each\nof the two linguistic spaces, as well as the quality of the final alignment of\ncomparable clusters that can be obtained. We used first some purely synthetic\nrandom data sets to assess our formal similarity-comparability mixing model. We\nthen propose two variants of the comparability measure that has been defined by\n(Li and Gaussier 2010) in the context of bilingual lexicon extraction to adapt\nit to clustering or categorizing tasks. These two variant measures are\nsubsequently used to evaluate our similarity-comparability mixing model in the\ncontext of the co-classification and co-clustering of comparable textual data\nsets collected from Wikipedia categories for the English and French languages.\nOur experiments show clear improvements in clustering and classification\naccuracies when mixing comparability with similarity measures, with, as\nexpected, a higher robustness obtained when the two comparability variant\nmeasures that we propose are used. We believe that this approach is\nparticularly well suited for the construction of thematic comparable corpora of\ncontrollable quality.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 13:07:41 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 19:30:15 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Marteau", "Pierre-Fran\u00e7ois", "", "IRISA"], ["Ke", "Guiyao", "", "IRISA"]]}, {"id": "1502.07938", "submitter": "Monica Jha Miss", "authors": "Rakesh Chandra Balabantaray, Chandrali Sarma, Monica Jha", "title": "Document Clustering using K-Means and K-Medoids", "comments": null, "journal-ref": "International Journal of Knowledge Based Computer Systems, Volume\n  1 Issue 1 (2013)", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the huge upsurge of information in day-to-days life, it has become\ndifficult to assemble relevant information in nick of time. But people, always\nare in dearth of time, they need everything quick. Hence clustering was\nintroduced to gather the relevant information in a cluster. There are several\nalgorithms for clustering information out of which in this paper, we accomplish\nK-means and K-Medoids clustering algorithm and a comparison is carried out to\nfind which algorithm is best for clustering. On the best clusters formed,\ndocument summarization is executed based on sentence weight to focus on key\npoint of the whole document, which makes it easier for people to ascertain the\ninformation they want and thus read only those documents which is relevant in\ntheir point of view.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 15:44:46 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Balabantaray", "Rakesh Chandra", ""], ["Sarma", "Chandrali", ""], ["Jha", "Monica", ""]]}, {"id": "1502.08033", "submitter": "Hung Nghiep Tran", "authors": "Vu Le Anh, Vo Hoang Hai, Hung Nghiep Tran, Jason J. Jung", "title": "SciRecSys: A Recommendation System for Scientific Publication by\n  Discovering Keyword Relationships", "comments": null, "journal-ref": "International Conference on Computational Collective Intelligence\n  (ICCCI 2014)", "doi": "10.1007/978-3-319-11289-3_8", "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new approach for discovering various relationships\namong keywords over the scientific publications based on a Markov Chain model.\nIt is an important problem since keywords are the basic elements for\nrepresenting abstract objects such as documents, user profiles, topics and many\nthings else. Our model is very effective since it combines four important\nfactors in scientific publications: content, publicity, impact and randomness.\nParticularly, a recommendation system (called SciRecSys) has been presented to\nsupport users to efficiently find out relevant articles.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 19:35:24 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Anh", "Vu Le", ""], ["Hai", "Vo Hoang", ""], ["Tran", "Hung Nghiep", ""], ["Jung", "Jason J.", ""]]}]