[{"id": "1402.0422", "submitter": "Andrea Lancichinetti", "authors": "Andrea Lancichinetti, M. Irmak Sirer, Jane X. Wang, Daniel Acuna,\n  Konrad K\\\"ording, Lu\\'is A. Nunes Amaral", "title": "A high-reproducibility and high-accuracy method for automated topic\n  classification", "comments": "23 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of human knowledge sits in large databases of unstructured text.\nLeveraging this knowledge requires algorithms that extract and record metadata\non unstructured text documents. Assigning topics to documents will enable\nintelligent search, statistical characterization, and meaningful\nclassification. Latent Dirichlet allocation (LDA) is the state-of-the-art in\ntopic classification. Here, we perform a systematic theoretical and numerical\nanalysis that demonstrates that current optimization techniques for LDA often\nyield results which are not accurate in inferring the most suitable model\nparameters. Adapting approaches for community detection in networks, we propose\na new algorithm which displays high-reproducibility and high-accuracy, and also\nhas high computational efficiency. We apply it to a large set of documents in\nthe English Wikipedia and reveal its hierarchical structure. Our algorithm\npromises to make \"big data\" text analysis systems more reliable.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 16:45:13 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Lancichinetti", "Andrea", ""], ["Sirer", "M. Irmak", ""], ["Wang", "Jane X.", ""], ["Acuna", "Daniel", ""], ["K\u00f6rding", "Konrad", ""], ["Amaral", "Lu\u00eds A. Nunes", ""]]}, {"id": "1402.0543", "submitter": "Bill Rea", "authors": "Jan Koeman and William Rea", "title": "How Does Latent Semantic Analysis Work? A Visualisation Approach", "comments": "13 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By using a small example, an analogy to photographic compression, and a\nsimple visualization using heatmaps, we show that latent semantic analysis\n(LSA) is able to extract what appears to be semantic meaning of words from a\nset of documents by blurring the distinctions between the words.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 23:09:28 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Koeman", "Jan", ""], ["Rea", "William", ""]]}, {"id": "1402.0556", "submitter": "Vahed Qazvinian", "authors": "Vahed Qazvinian, Dragomir R. Radev, Saif M. Mohammad, Bonnie Dorr,\n  David Zajic, Michael Whidby, Taesun Moon", "title": "Generating Extractive Summaries of Scientific Paradigms", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 46, pages\n  165-201, 2013", "doi": "10.1613/jair.3732", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers and scientists increasingly find themselves in the position of\nhaving to quickly understand large amounts of technical material. Our goal is\nto effectively serve this need by using bibliometric text mining and\nsummarization techniques to generate summaries of scientific literature. We\nshow how we can use citations to produce automatically generated, readily\nconsumable, technical extractive summaries. We first propose C-LexRank, a model\nfor summarizing single scientific articles based on citations, which employs\ncommunity detection and extracts salient information-rich sentences. Next, we\nfurther extend our experiments to summarize a set of papers, which cover the\nsame scientific topic. We generate extractive summaries of a set of Question\nAnswering (QA) and Dependency Parsing (DP) papers, their abstracts, and their\ncitation sentences and show that citations have unique information amenable to\ncreating a summary.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:33:10 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Qazvinian", "Vahed", ""], ["Radev", "Dragomir R.", ""], ["Mohammad", "Saif M.", ""], ["Dorr", "Bonnie", ""], ["Zajic", "David", ""], ["Whidby", "Michael", ""], ["Moon", "Taesun", ""]]}, {"id": "1402.0574", "submitter": "Kira Radinsky", "authors": "Kira Radinsky, Sagie Davidovich, Shaul Markovitch", "title": "Learning to Predict from Textual Data", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 45, pages\n  641-684, 2012", "doi": "10.1613/jair.3865", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a current news event, we tackle the problem of generating plausible\npredictions of future events it might cause. We present a new methodology for\nmodeling and predicting such future news events using machine learning and data\nmining techniques. Our Pundit algorithm generalizes examples of causality pairs\nto infer a causality predictor. To obtain precisely labeled causality examples,\nwe mine 150 years of news articles and apply semantic natural language modeling\ntechniques to headlines containing certain predefined causality patterns. For\ngeneralization, the model uses a vast number of world knowledge ontologies.\nEmpirical evaluation on real news articles shows that our Pundit algorithm\nperforms as well as non-expert humans.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 01:39:12 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Radinsky", "Kira", ""], ["Davidovich", "Sagie", ""], ["Markovitch", "Shaul", ""]]}, {"id": "1402.0728", "submitter": "Dominik Kowald", "authors": "Dominik Kowald, Paul Seitlinger, Christoph Trattner, Tobias Ley", "title": "Forgetting the Words but Remembering the Meaning: Modeling Forgetting in\n  a Verbal and Semantic Tag Recommender", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assume that recommender systems are more successful, when they are based\non a thorough understanding of how people process information. In the current\npaper we test this assumption in the context of social tagging systems.\nCognitive research on how people assign tags has shown that they draw on two\ninterconnected levels of knowledge in their memory: on a conceptual level of\nsemantic fields or topics, and on a lexical level that turns patterns on the\nsemantic level into words. Another strand of tagging research reveals a strong\nimpact of time dependent forgetting on users' tag choices, such that recently\nused tags have a higher probability being reused than \"older\" tags. In this\npaper, we align both strands by implementing a computational theory of human\nmemory that integrates the two-level conception and the process of forgetting\nin form of a tag recommender and test it in three large-scale social tagging\ndatasets (drawn from BibSonomy, CiteULike and Flickr).\n  As expected, our results reveal a selective effect of time: forgetting is\nmuch more pronounced on the lexical level of tags. Second, an extensive\nevaluation based on this observation shows that a tag recommender\ninterconnecting both levels and integrating time dependent forgetting on the\nlexical level results in high accuracy predictions and outperforms other\nwell-established algorithms, such as Collaborative Filtering, Pairwise\nInteraction Tensor Factorization, FolkRank and two alternative time dependent\napproaches. We conclude that tag recommenders can benefit from going beyond the\nmanifest level of word co-occurrences, and from including forgetting processes\non the lexical level.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 13:31:10 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 08:37:04 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Kowald", "Dominik", ""], ["Seitlinger", "Paul", ""], ["Trattner", "Christoph", ""], ["Ley", "Tobias", ""]]}, {"id": "1402.1270", "submitter": "Abderrahim Mohammed El Amine", "authors": "Abderrahim Mohammed El Amine", "title": "Vers une interface pour l enrichissement des requetes en arabe dans un\n  systeme de recherche d information", "comments": "9 pages, in French", "journal-ref": "CIIA'2009 : 2eme Conference Internationale sur Informatique et ses\n  Applications, Saida - Algerie, 03 -04 Mai 2009", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This presentation focuses on the automatic expansion of Arabic request using\nmorphological analyzer and Arabic Wordnet. The expanded request is sent to\nGoogle.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 08:29:19 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Amine", "Abderrahim Mohammed El", ""]]}, {"id": "1402.1668", "submitter": "John Osborne", "authors": "John David Osborne, Binod Gyawali, Thamar Solorio", "title": "Evaluation of YTEX and MetaMap for clinical concept recognition", "comments": "6 pages, working notes to the ShareClef eHealth 2013 Shared Task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We used MetaMap and YTEX as a basis for the construc- tion of two separate\nsystems to participate in the 2013 ShARe/CLEF eHealth Task 1[9], the\nrecognition of clinical concepts. No modifications were directly made to these\nsystems, but output concepts were filtered using stop concepts, stop concept\ntext and UMLS semantic type. Con- cept boundaries were also adjusted using a\nsmall collection of rules to increase precision on the strict task. Overall\nMetaMap had better per- formance than YTEX on the strict task, primarily due to\na 20% perfor- mance improvement in precision. In the relaxed task YTEX had\nbetter performance in both precision and recall giving it an overall F-Score\n4.6% higher than MetaMap on the test data. Our results also indicated a 1.3%\nhigher accuracy for YTEX in UMLS CUI mapping.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 15:33:44 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Osborne", "John David", ""], ["Gyawali", "Binod", ""], ["Solorio", "Thamar", ""]]}, {"id": "1402.1892", "submitter": "Zachary Lipton", "authors": "Zachary Chase Lipton, Charles Elkan, Balakrishnan Narayanaswamy", "title": "Thresholding Classifiers to Maximize F1 Score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides new insight into maximizing F1 scores in the context of\nbinary classification and also in the context of multilabel classification. The\nharmonic mean of precision and recall, F1 score is widely used to measure the\nsuccess of a binary classifier when one class is rare. Micro average, macro\naverage, and per instance average F1 scores are used in multilabel\nclassification. For any classifier that produces a real-valued output, we\nderive the relationship between the best achievable F1 score and the\ndecision-making threshold that achieves this optimum. As a special case, if the\nclassifier outputs are well-calibrated conditional probabilities, then the\noptimal threshold is half the optimal F1 score. As another special case, if the\nclassifier is completely uninformative, then the optimal behavior is to\nclassify all examples as positive. Since the actual prevalence of positive\nexamples typically is low, this behavior can be considered undesirable. As a\ncase study, we discuss the results, which can be surprising, of applying this\nprocedure when predicting 26,853 labels for Medline documents.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2014 21:14:29 GMT"}, {"version": "v2", "created": "Wed, 14 May 2014 01:29:47 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Lipton", "Zachary Chase", ""], ["Elkan", "Charles", ""], ["Narayanaswamy", "Balakrishnan", ""]]}, {"id": "1402.1946", "submitter": "Prajwal Thakare", "authors": "Prajwal R Thakare, K. Hanumantha Rao", "title": "Anomaly Detection Based on Access Behavior and Document Rank Algorithm", "comments": "6 pages, 3 figures", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  6(39):4-6, December 2013", "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed denial of service(DDos) attack is ongoing dangerous threat to the\nInternet. Commonly, DDos attacks are carried out at the network layer, e.g. SYN\nflooding, ICMP flooding and UDP flooding, which are called Distributed denial\nof service attacks. The intention of these DDos attacks is to utilize the\nnetwork bandwidth and deny service to authorize users of the victim systems.\nObtain from the low layers, new application-layer-based DDos attacks utilizing\nauthorize HTTP requests to overload victim resources are more undetectable.\nWhen these are taking place during crowd events of any popular website, this is\nthe case is very serious. The state-of-art approaches cannot handle the\nsituation where there is no considerable deviation between the normal and the\nattackers activity. The page rank and proximity graph representation of online\nweb accesses takes much time in practice. There should be less computational\ncomplexity, than of proximity graph search. Hence proposing Web Access Table\nmechanism to hold the data such as \"who accessed what and how many times, and\ntheir rank on average\" to find the anomalous web access behavior. The system\ntakes less computational complexity and may produce considerable time\ncomplexity.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 12:50:58 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Thakare", "Prajwal R", ""], ["Rao", "K. Hanumantha", ""]]}, {"id": "1402.1947", "submitter": "Farrukh Arslan", "authors": "Farrukh Arslan", "title": "Classification Tree Diagrams in Health Informatics Applications", "comments": "In the Proceedings of 7th International Conference on the Theory and\n  Application of Diagrams 2012. 7th International Conference on the Theory and\n  Application of Diagrams 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health informatics deal with the methods used to optimize the acquisition,\nstorage and retrieval of medical data, and classify information in healthcare\napplications. Healthcare analysts are particularly interested in various\ncomputer informatics areas such as; knowledge representation from data, anomaly\ndetection, outbreak detection methods and syndromic surveillance applications.\nAlthough various parametric and non-parametric approaches are being proposed to\nclassify information from data, classification tree diagrams provide an\ninteractive visualization to analysts as compared to other methods. In this\nwork we discuss application of classification tree diagrams to classify\ninformation from medical data in healthcare applications.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 13:02:51 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Arslan", "Farrukh", ""]]}, {"id": "1402.2073", "submitter": "Tobias Kuhn", "authors": "Tobias Kuhn, Mate Levente Nagy, ThaiBinh Luong, Michael Krauthammer", "title": "Mining Images in Biomedical Publications: Detection and Analysis of Gel\n  Diagrams", "comments": "arXiv admin note: substantial text overlap with arXiv:1209.1481", "journal-ref": "Journal of Biomedical Semantics 2014, 5:10", "doi": "10.1186/2041-1480-5-10", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Authors of biomedical publications use gel images to report experimental\nresults such as protein-protein interactions or protein expressions under\ndifferent conditions. Gel images offer a concise way to communicate such\nfindings, not all of which need to be explicitly discussed in the article text.\nThis fact together with the abundance of gel images and their shared common\npatterns makes them prime candidates for automated image mining and parsing. We\nintroduce an approach for the detection of gel images, and present a workflow\nto analyze them. We are able to detect gel segments and panels at high\naccuracy, and present preliminary results for the identification of gene names\nin these images. While we cannot provide a complete solution at this point, we\npresent evidence that this kind of image mining is feasible.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 09:16:13 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Kuhn", "Tobias", ""], ["Nagy", "Mate Levente", ""], ["Luong", "ThaiBinh", ""], ["Krauthammer", "Michael", ""]]}, {"id": "1402.2145", "submitter": "Niloofar Rastin", "authors": "Niloofar Rastin and Mansoor Zolghadri Jahromi", "title": "Using content features to enhance performance of user-based\n  collaborative filtering performance of user-based collaborative filtering", "comments": "10 pages, journal", "journal-ref": "International journal of artificial intelligence and applications,\n  vol. 5, no. 1, pp. 53-62, January 2014", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based and collaborative filtering methods are the most successful\nsolutions in recommender systems. Content based method is based on items\nattributes. This method checks the features of users favourite items and then\nproposes the items which have the most similar characteristics with those\nitems. Collaborative filtering method is based on the determination of similar\nitems or similar users, which are called item-based and user-based\ncollaborative filtering, respectively.In this paper we propose a hybrid method\nthat integrates collaborative filtering and content-based methods. The proposed\nmethod can be viewed as user-based Collaborative filtering technique. However\nto find users with similar taste with active user, we used content features of\nthe item under investigation to put more emphasis on users rating for similar\nitems. In other words two users are similar if their ratings are similar on\nitems that have similar context. This is achieved by assigning a weight to each\nrating when calculating the similarity of two users.We used movielens data set\nto access the performance of the proposed method in comparison with basic\nuser-based collaborative filtering and other popular methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 13:52:33 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2014 12:16:22 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Rastin", "Niloofar", ""], ["Jahromi", "Mansoor Zolghadri", ""]]}, {"id": "1402.2232", "submitter": "Vipeen Bopche Vikas", "authors": "V Rajakumar, Vipeen V Bopche", "title": "Image Search Reranking", "comments": null, "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  6(41):5-6, December 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing methods for image search reranking suffer from the\nunfaithfulness of the assumptions under which the text-based images search\nresult. The resulting images contain more irrelevant images. Hence the re\nranking concept arises to re rank the retrieved images based on the text around\nthe image and data of data of image and visual feature of image. A number of\nmethods are differentiated for this re-ranking. The high ranked images are used\nas noisy data and a k means algorithm for classification is learned to rectify\nthe ranking further. We are study the affect ability of the cross validation\nmethod to this training data. The pre eminent originality of the overall method\nis in collecting text/metadata of image and visual features in order to achieve\nan automatic ranking of the images. Supervision is initiated to learn the model\nweights offline, previous to reranking process. While model learning needs\nmanual labeling of the results for a some limited queries, the resulting model\nis query autonomous and therefore applicable to any other query .Examples are\ngiven for a selection of other classes like vehicles, animals and other\nclasses.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 18:28:18 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Rajakumar", "V", ""], ["Bopche", "Vipeen V", ""]]}, {"id": "1402.2351", "submitter": "Flavio Figueiredo", "authors": "Flavio Figueiredo, Jussara M. Almeida, Marcos Andr\\'e Gon\\c{c}alves,\n  Fabr\\'icio Benevenuto", "title": "TrendLearner: Early Prediction of Popularity Trends of User Generated\n  Content", "comments": "To appear at Elsevier Information Sciences Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We here focus on the problem of predicting the popularity trend of user\ngenerated content (UGC) as early as possible. Taking YouTube videos as case\nstudy, we propose a novel two-step learning approach that: (1) extracts\npopularity trends from previously uploaded objects, and (2) predicts trends for\nnew content. Unlike previous work, our solution explicitly addresses the\ninherent tradeoff between prediction accuracy and remaining interest in the\ncontent after prediction, solving it on a per-object basis. Our experimental\nresults show great improvements of our solution over alternatives, and its\napplicability to improve the accuracy of state-of-the-art popularity prediction\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 02:36:26 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2014 15:53:33 GMT"}, {"version": "v3", "created": "Mon, 7 Apr 2014 02:44:32 GMT"}, {"version": "v4", "created": "Sun, 14 Feb 2016 20:39:52 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Figueiredo", "Flavio", ""], ["Almeida", "Jussara M.", ""], ["Gon\u00e7alves", "Marcos Andr\u00e9", ""], ["Benevenuto", "Fabr\u00edcio", ""]]}, {"id": "1402.2427", "submitter": "Georg Groh", "authors": "Jan Hauffa, Tobias Lichtenberg, Georg Groh", "title": "An evaluation of keyword extraction from online communication for the\n  characterisation of social relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The set of interpersonal relationships on a social network service or a\nsimilar online community is usually highly heterogenous. The concept of tie\nstrength captures only one aspect of this heterogeneity. Since the unstructured\ntext content of online communication artefacts is a salient source of\ninformation about a social relationship, we investigate the utility of keywords\nextracted from the message body as a representation of the relationship's\ncharacteristics as reflected by the conversation topics. Keyword extraction is\nperformed using standard natural language processing methods. Communication\ndata and human assessments of the extracted keywords are obtained from Facebook\nusers via a custom application. The overall positive quality assessment\nprovides evidence that the keywords indeed convey relevant information about\nthe relationship.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 10:27:43 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Hauffa", "Jan", ""], ["Lichtenberg", "Tobias", ""], ["Groh", "Georg", ""]]}, {"id": "1402.2509", "submitter": "Subha", "authors": "M.Subha, K.Saravanan", "title": "Achieve Better Ranking Accuracy Using CloudRank Framework for Cloud\n  Services", "comments": "6 pages, 10 figures, Published with International Journal of\n  Engineering Trends and Technology (IJETT)", "journal-ref": "International Journal of Engineering Trends and Technology (IJETT)\n  6(6):307-312, December 2013", "doi": null, "report-no": null, "categories": "cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building high quality cloud applications becomes an urgently required\nresearch problem. Nonfunctional performance of cloud services is usually\ndescribed by quality-of-service (QoS). In cloud applications, cloud services\nare invoked remotely by internet connections. The QoS Ranking of cloud services\nfor a user cannot be transferred directly to another user, since the locations\nof the cloud applications are quite different. Personalized QoS Ranking is\nrequired to evaluate all candidate services at the user - side but it is\nimpractical in reality. To get QoS values, the service candidates are usually\nrequired and it is very expensive. To avoid time consuming and expensive\nrealworld service invocations, this paper proposes a CloudRank framework which\npredicts the QoS ranking directly without predicting the corresponding QoS\nvalues. This framework provides an accurate ranking but the QoS values are same\nin both algorithms so, an optimal VM allocation policy is used to improve the\nQoS performance of cloud services and it also provides better ranking accuracy\nthan CloudRank2 algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 14:51:11 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Subha", "M.", ""], ["Saravanan", "K.", ""]]}, {"id": "1402.2562", "submitter": "Nathalie Chaignaud", "authors": "Nathalie Chaignaud (LITIS), Val\\'erie Delavigne (LiDiFra), Maryvonne\n  Holzem (LiDiFra), Jean-Philippe Kotowicz (LITIS), Alain Loisel (LITIS)", "title": "\\'Etude cognitive des processus de construction d'une requ\\^ete dans un\n  syst\\`eme de gestion de connaissances m\\'edicales", "comments": "29 pages", "journal-ref": "Revue Revue Technique et Science Informatiques 29 (2010) 991-1021", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the Cogni-CISMeF project, which aims at improving\nmedical information search in the CISMeF system (Catalog and Index of\nFrench-language health resources) by including a conversational agent to\ninteract with the user in natural language. To study the cognitive processes\ninvolved during the information search, a bottom-up methodology was adopted.\nExperimentation has been set up to obtain human dialogs between a user (playing\nthe role of patient) dealing with medical information search and a CISMeF\nexpert refining the request. The analysis of these dialogs underlined the use\nof discursive evidence: vocabulary, reformulation, implicit or explicit\nexpression of user intentions, conversational sequences, etc. A model of\nartificial agent is proposed. It leads the user in its information search by\nproposing to him examples, assistance and choices. This model was implemented\nand integrated in the CISMeF system. ---- Cet article d\\'ecrit le projet\nCogni-CISMeF qui propose un module de dialogue Homme-Machine \\`a int\\'egrer\ndans le syst\\`eme d'indexation de connaissances m\\'edicales CISMeF (Catalogue\net Index des Sites M\\'edicaux Francophones). Nous avons adopt\\'e une d\\'emarche\nde mod\\'elisation cognitive en proc\\'edant \\`a un recueil de corpus de\ndialogues entre un utilisateur (jouant le r\\^ole d'un patient) d\\'esirant une\ninformation m\\'edicale et un expert CISMeF af inant cette demande pour\nconstruire la requ\\^ete. Nous avons analys\\'e la structure des dialogues ainsi\nobtenus et avons \\'etudi\\'e un certain nombre d'indices discursifs :\nvocabulaire employ\\'e, marques de reformulation, commentaires m\\'eta et\n\\'epilinguistiques, expression implicite ou explicite des intentions de\nl'utilisateur, encha\\^inement conversationnel, etc. De cette analyse, nous\navons construit un mod\\`ele d'agent artificiel dot\\'e de capacit\\'es cognitives\ncapables d'aider l'utilisateur dans sa t\\^ache de recherche d'information. Ce\nmod\\`ele a \\'et\\'e impl\\'ement\\'e et int\\'egr\\'e dans le syst\\`eme CISMeF.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 20:46:24 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Chaignaud", "Nathalie", "", "LITIS"], ["Delavigne", "Val\u00e9rie", "", "LiDiFra"], ["Holzem", "Maryvonne", "", "LiDiFra"], ["Kotowicz", "Jean-Philippe", "", "LITIS"], ["Loisel", "Alain", "", "LITIS"]]}, {"id": "1402.2695", "submitter": "Laura Deal", "authors": "Laura Deal", "title": "Visualizing Digital Collections", "comments": "30 pages, 4 figures. This is a preprint of an article submitted for\n  consideration in the Technical Services Quarterly (2014); Technical Services\n  Quarterly is available online at:\n  http://www.tandfonline.com/toc/wtsq20/current", "journal-ref": null, "doi": "10.1080/07317131.2015.972871", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualizations can greatly enhance search in digital collections by\nproviding information about the scope and context of a collection and allowing\nusers to more easily browse and explore the contents. This article discusses\nthe benefits of incorporating visualizations into digital collections based on\nthe experiences of the Cold War International History Project (CWIHP) in\ndeveloping a user-friendly tool for searching and visualizing the project's\ncomplex set of historical documents. The paper concludes with a tutorial on\nusing the free Library of Congress tool Viewshare to create visualizations\nbased on real data from the CWIHP Digital Archive.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 23:19:14 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Deal", "Laura", ""]]}, {"id": "1402.3010", "submitter": "Eray Ozkural", "authors": "Eray \\\"Ozkural, Cevdet Aykanat", "title": "1-D and 2-D Parallel Algorithms for All-Pairs Similarity Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All-pairs similarity problem asks to find all vector pairs in a set of\nvectors the similarities of which surpass a given similarity threshold, and it\nis a computational kernel in data mining and information retrieval for several\ntasks. We investigate the parallelization of a recent fast sequential\nalgorithm. We propose effective 1-D and 2-D data distribution strategies that\npreserve the essential optimizations in the fast algorithm. 1-D parallel\nalgorithms distribute either dimensions or vectors, whereas the 2-D parallel\nalgorithm distributes data both ways. Additional contributions to the 1-D\nvertical distribution include a local pruning strategy to reduce the number of\ncandidates, a recursive pruning algorithm, and block processing to reduce\nimbalance. The parallel algorithms were programmed in OCaml which affords much\nconvenience. Our experiments indicate that the performance depends on the\ndataset, therefore a variety of parallelizations is useful.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 00:14:33 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["\u00d6zkural", "Eray", ""], ["Aykanat", "Cevdet", ""]]}, {"id": "1402.3070", "submitter": "Parth Gupta", "authors": "Parth Gupta, Rafael E. Banchs and Paolo Rosso", "title": "Squeezing bottlenecks: exploring the limits of autoencoder semantic\n  representation capabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive study on the use of autoencoders for modelling\ntext data, in which (differently from previous studies) we focus our attention\non the following issues: i) we explore the suitability of two different models\nbDA and rsDA for constructing deep autoencoders for text data at the sentence\nlevel; ii) we propose and evaluate two novel metrics for better assessing the\ntext-reconstruction capabilities of autoencoders; and iii) we propose an\nautomatic method to find the critical bottleneck dimensionality for text\nlanguage representations (below which structural information is lost).\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 09:54:01 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Gupta", "Parth", ""], ["Banchs", "Rafael E.", ""], ["Rosso", "Paolo", ""]]}, {"id": "1402.3405", "submitter": "Daniele Cerra", "authors": "Daniele Cerra, Mihai Datcu, and Peter Reinartz", "title": "Authorship Analysis based on Data Compression", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2014.01.019", "report-no": null, "categories": "cs.CL cs.DL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to perform authorship analysis using the Fast Compression\nDistance (FCD), a similarity measure based on compression with dictionaries\ndirectly extracted from the written texts. The FCD computes a similarity\nbetween two documents through an effective binary search on the intersection\nset between the two related dictionaries. In the reported experiments the\nproposed method is applied to documents which are heterogeneous in style,\nwritten in five different languages and coming from different historical\nperiods. Results are comparable to the state of the art and outperform\ntraditional compression-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 09:25:59 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Cerra", "Daniele", ""], ["Datcu", "Mihai", ""], ["Reinartz", "Peter", ""]]}, {"id": "1402.3470", "submitter": "Thomas Bosch", "authors": "Thomas Bosch, Andias Wira-Alam and Brigitte Mathiak", "title": "Designing an Ontology for the Data Documentation Initiative", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ontology of the DDI 3 data model will be designed by following the\nontology engineering methodology to be evolved based on state-of-the-art\nmethodologies. Hence DDI 3 data and metadata can be represented in form of a\nstandard web interchange format RDF and processed by highly available RDF\ntools. As a consequence the DDI community has the possibility to publish and\nlink LOD data sets to become part of the LOD cloud.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 14:12:17 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Bosch", "Thomas", ""], ["Wira-Alam", "Andias", ""], ["Mathiak", "Brigitte", ""]]}, {"id": "1402.3891", "submitter": "Vinodhini G", "authors": "Vinodhini G Chandrasekaran RM", "title": "Performance Evaluation of Machine Learning Classifiers in Sentiment\n  Mining", "comments": "4 pages 2 tables, International Journal of Computer Trends and\n  Technology, volume 4, Issue 6, june 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the use of machine learning classifiers is of great value in\nsolving a variety of problems in text classification. Sentiment mining is a\nkind of text classification in which, messages are classified according to\nsentiment orientation such as positive or negative. This paper extends the idea\nof evaluating the performance of various classifiers to show their\neffectiveness in sentiment mining of online product reviews. The product\nreviews are collected from Amazon reviews. To evaluate the performance of\nclassifiers various evaluation methods like random sampling, linear sampling\nand bootstrap sampling are used. Our results shows that support vector machine\nwith bootstrap sampling method outperforms others classifiers and sampling\nmethods in terms of misclassification rate.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 05:24:42 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["RM", "Vinodhini G Chandrasekaran", ""]]}, {"id": "1402.4417", "submitter": "Pankaj Malhotra", "authors": "Pankaj Malhotra, Puneet Agarwal, Gautam Shroff", "title": "Incremental Entity Resolution from Linked Documents", "comments": "15 pages, 8 figures, patented work", "journal-ref": null, "doi": null, "report-no": "TR-DAIF-2014-1", "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many government applications we often find that information about\nentities, such as persons, are available in disparate data sources such as\npassports, driving licences, bank accounts, and income tax records. Similar\nscenarios are commonplace in large enterprises having multiple customer,\nsupplier, or partner databases. Each data source maintains different aspects of\nan entity, and resolving entities based on these attributes is a well-studied\nproblem. However, in many cases documents in one source reference those in\nothers; e.g., a person may provide his driving-licence number while applying\nfor a passport, or vice-versa. These links define relationships between\ndocuments of the same entity (as opposed to inter-entity relationships, which\nare also often used for resolution). In this paper we describe an algorithm to\ncluster documents that are highly likely to belong to the same entity by\nexploiting inter-document references in addition to attribute similarity. Our\ntechnique uses a combination of iterative graph-traversal, locality-sensitive\nhashing, iterative match-merge, and graph-clustering to discover unique\nentities based on a document corpus. A unique feature of our technique is that\nnew sets of documents can be added incrementally while having to re-resolve\nonly a small subset of a previously resolved entity-document collection. We\npresent performance and quality results on two data-sets: a real-world database\nof companies and a large synthetically generated `population' database. We also\ndemonstrate benefit of using inter-document references for clustering in the\nform of enhanced recall of documents for resolution.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 17:45:31 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Malhotra", "Pankaj", ""], ["Agarwal", "Puneet", ""], ["Shroff", "Gautam", ""]]}, {"id": "1402.4653", "submitter": "Sohan Seth", "authors": "Sohan Seth, John Shawe-Taylor, Samuel Kaski", "title": "Retrieval of Experiments by Efficient Estimation of Marginal Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of retrieving relevant experiments given a query\nexperiment. By experiment, we mean a collection of measurements from a set of\n`covariates' and the associated `outcomes'. While similar experiments can be\nretrieved by comparing available `annotations', this approach ignores the\nvaluable information available in the measurements themselves. To incorporate\nthis information in the retrieval task, we suggest employing a retrieval metric\nthat utilizes probabilistic models learned from the measurements. We argue that\nsuch a metric is a sensible measure of similarity between two experiments since\nit permits inclusion of experiment-specific prior knowledge. However, accurate\nmodels are often not analytical, and one must resort to storing posterior\nsamples which demands considerable resources. Therefore, we study strategies to\nselect informative posterior samples to reduce the computational load while\nmaintaining the retrieval performance. We demonstrate the efficacy of our\napproach on simulated data with simple linear regression as the models, and\nreal world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 13:21:40 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Seth", "Sohan", ""], ["Shawe-Taylor", "John", ""], ["Kaski", "Samuel", ""]]}, {"id": "1402.4888", "submitter": "Johnvictor D", "authors": "D. Johnvictor, G. Selvavinayagam", "title": "Survey on Sparse Coded Features for Content Based Face Image Retrieval", "comments": "4 pages,3 figures,1 table, Published with International Journal of\n  Computer Trends and Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  8(1):30-33, February 2014. ISSN:2231-2803", "doi": "10.14445/22312803/IJCTT-V8P106", "report-no": null, "categories": "cs.IR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content based image retrieval, a technique which uses visual contents of\nimage to search images from large scale image databases according to users'\ninterests. This paper provides a comprehensive survey on recent technology used\nin the area of content based face image retrieval. Nowadays digital devices and\nphoto sharing sites are getting more popularity, large human face photos are\navailable in database. Multiple types of facial features are used to represent\ndiscriminality on large scale human facial image database. Searching and mining\nof facial images are challenging problems and important research issues. Sparse\nrepresentation on features provides significant improvement in indexing related\nimages to query image.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 04:32:40 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Johnvictor", "D.", ""], ["Selvavinayagam", "G.", ""]]}, {"id": "1402.5176", "submitter": "Ko-Jen Hsiao", "authors": "Ko-Jen Hsiao, Jeff Calder, Alfred O. Hero III", "title": "Pareto-depth for Multiple-query Image Retrieval", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2014.2378057", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most content-based image retrieval systems consider either one single query,\nor multiple queries that include the same object or represent the same semantic\ninformation. In this paper we consider the content-based image retrieval\nproblem for multiple query images corresponding to different image semantics.\nWe propose a novel multiple-query information retrieval algorithm that combines\nthe Pareto front method (PFM) with efficient manifold ranking (EMR). We show\nthat our proposed algorithm outperforms state of the art multiple-query\nretrieval algorithms on real-world image databases. We attribute this\nperformance improvement to concavity properties of the Pareto fronts, and prove\na theoretical result that characterizes the asymptotic concavity of the fronts.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 00:42:48 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Hsiao", "Ko-Jen", ""], ["Calder", "Jeff", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1402.5255", "submitter": "Christian von der Weth", "authors": "Christian von der Weth, Manfred Hauswirth", "title": "Analysing Parallel and Passive Web Browsing Behavior and its Effects on\n  Website Metrics", "comments": "22 pages, 11 figures, 3 tables, 29 references. arXiv admin note: text\n  overlap with arXiv:1307.1542", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Getting deeper insights into the online browsing behavior of Web users has\nbeen a major research topic since the advent of the WWW. It provides useful\ninformation to optimize website design, Web browser design, search engines\nofferings, and online advertisement. We argue that new technologies and new\nservices continue to have significant effects on the way how people browse the\nWeb. For example, listening to music clips on YouTube or to a radio station on\nLast.fm does not require users to sit in front of their computer. Social media\nand networking sites like Facebook or micro-blogging sites like Twitter have\nattracted new types of users that previously were less inclined to go online.\nThese changes in how people browse the Web feature new characteristics which\nare not well understood so far. In this paper, we provide novel and unique\ninsights by presenting first results of DOBBS, our long-term effort to create a\ncomprehensive and representative dataset capturing online user behavior. We\nfirstly investigate the concepts of parallel browsing and passive browsing,\nshowing that browsing the Web is no longer a dedicated task for many users.\nBased on these results, we then analyze their impact on the calculation of a\nuser's dwell time -- i.e., the time the user spends on a webpage -- which has\nbecome an important metric to quantify the popularity of websites.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 11:15:02 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["von der Weth", "Christian", ""], ["Hauswirth", "Manfred", ""]]}, {"id": "1402.5565", "submitter": "David M. Johnson", "authors": "David M. Johnson, Caiming Xiong and Jason J. Corso", "title": "Semi-Supervised Nonlinear Distance Metric Learning via Forests of\n  Max-Margin Cluster Hierarchies", "comments": "Manuscript submitted to SIGKDD on 21 Feb 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning is a key problem for many data mining and machine learning\napplications, and has long been dominated by Mahalanobis methods. Recent\nadvances in nonlinear metric learning have demonstrated the potential power of\nnon-Mahalanobis distance functions, particularly tree-based functions. We\npropose a novel nonlinear metric learning method that uses an iterative,\nhierarchical variant of semi-supervised max-margin clustering to construct a\nforest of cluster hierarchies, where each individual hierarchy can be\ninterpreted as a weak metric over the data. By introducing randomness during\nhierarchy training and combining the output of many of the resulting\nsemi-random weak hierarchy metrics, we can obtain a powerful and robust\nnonlinear metric model. This method has two primary contributions: first, it is\nsemi-supervised, incorporating information from both constrained and\nunconstrained points. Second, we take a relaxed approach to constraint\nsatisfaction, allowing the method to satisfy different subsets of the\nconstraints at different levels of the hierarchy rather than attempting to\nsimultaneously satisfy all of them. This leads to a more robust learning\nalgorithm. We compare our method to a number of state-of-the-art benchmarks on\n$k$-nearest neighbor classification, large-scale image retrieval and\nsemi-supervised clustering problems, and find that our algorithm yields results\ncomparable or superior to the state-of-the-art, and is significantly more\nrobust to noise.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 00:26:48 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Johnson", "David M.", ""], ["Xiong", "Caiming", ""], ["Corso", "Jason J.", ""]]}, {"id": "1402.5774", "submitter": "Da-Cheng Nie", "authors": "Da-Cheng Nie, Ya-Hui An, Qiang Dong, Yan Fu and Tao Zhou", "title": "Information Filtering via Balanced Diffusion on Bipartite Networks", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent decade has witnessed the increasing popularity of recommender systems,\nwhich help users acquire relevant commodities and services from overwhelming\nresources on Internet. Some simple physical diffusion processes have been used\nto design effective recommendation algorithms for user-object bipartite\nnetworks, typically mass diffusion (MD) and heat conduction (HC) algorithms\nwhich have different advantages respectively on accuracy and diversity. In this\npaper, we investigate the effect of weight assignment in the hybrid of MD and\nHC, and find that a new hybrid algorithm of MD and HC with balanced weights\nwill achieve the optimal recommendation results, we name it balanced diffusion\n(BD) algorithm. Numerical experiments on three benchmark data sets, MovieLens,\nNetflix and RateYourMusic (RYM), show that the performance of BD algorithm\noutperforms the existing diffusion-based methods on the three important\nrecommendation metrics, accuracy, diversity and novelty. Specifically, it can\nnot only provide accurately recommendation results, but also yield higher\ndiversity and novelty in recommendations by accurately recommending unpopular\nobjects.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 10:26:39 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Nie", "Da-Cheng", ""], ["An", "Ya-Hui", ""], ["Dong", "Qiang", ""], ["Fu", "Yan", ""], ["Zhou", "Tao", ""]]}, {"id": "1402.6010", "submitter": "Linhong Zhu", "authors": "Linhong Zhu and Aram Galstyan and James Cheng and Kristina Lerman", "title": "Tripartite Graph Clustering for Dynamic Sentiment Analysis on Social\n  Media", "comments": "A short version is in Proceeding of the 2014 ACM SIGMOD International\n  Conference on Management of data", "journal-ref": null, "doi": "10.1145/2588555.2593682", "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing popularity of social media (e.g, Twitter) allows users to easily\nshare information with each other and influence others by expressing their own\nsentiments on various subjects. In this work, we propose an unsupervised\n\\emph{tri-clustering} framework, which analyzes both user-level and tweet-level\nsentiments through co-clustering of a tripartite graph. A compelling feature of\nthe proposed framework is that the quality of sentiment clustering of tweets,\nusers, and features can be mutually improved by joint clustering. We further\ninvestigate the evolution of user-level sentiments and latent feature vectors\nin an online framework and devise an efficient online algorithm to sequentially\nupdate the clustering of tweets, users and features with newly arrived data.\nThe online framework not only provides better quality of both dynamic\nuser-level and tweet-level sentiment analysis, but also improves the\ncomputational and storage efficiency. We verified the effectiveness and\nefficiency of the proposed approaches on the November 2012 California ballot\nTwitter data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 22:58:28 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2014 18:50:55 GMT"}, {"version": "v3", "created": "Thu, 12 Jun 2014 18:49:21 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Zhu", "Linhong", ""], ["Galstyan", "Aram", ""], ["Cheng", "James", ""], ["Lerman", "Kristina", ""]]}, {"id": "1402.6132", "submitter": "Wei Zeng", "authors": "Wei Zeng, An Zeng, Hao Liu, Ming-Sheng Shang and Tao Zhou", "title": "Uncovering the information core in recommender systems", "comments": "14pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of the Internet and overwhelming amount of information\nthat people are confronted with, recommender systems have been developed to\neffiectively support users' decision-making process in online systems. So far,\nmuch attention has been paid to designing new recommendation algorithms and\nimproving existent ones. However, few works considered the different\ncontributions from different users to the performance of a recommender system.\nSuch studies can help us improve the recommendation efficiency by excluding\nirrelevant users. In this paper, we argue that in each online system there\nexists a group of core users who carry most of the information for\nrecommendation. With them, the recommender systems can already generate\nsatisfactory recommendation. Our core user extraction method enables the\nrecommender systems to achieve 90% of the accuracy by taking only 20% of the\ndata into account.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 11:08:02 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Zeng", "Wei", ""], ["Zeng", "An", ""], ["Liu", "Hao", ""], ["Shang", "Ming-Sheng", ""], ["Zhou", "Tao", ""]]}, {"id": "1402.6238", "submitter": "Jobin Wilson", "authors": "Jobin Wilson, Santanu Chaudhury, Brejesh Lall, Prateek Kapadia", "title": "Improving Collaborative Filtering based Recommenders using Topic\n  Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard Collaborative Filtering (CF) algorithms make use of interactions\nbetween users and items in the form of implicit or explicit ratings alone for\ngenerating recommendations. Similarity among users or items is calculated\npurely based on rating overlap in this case,without considering explicit\nproperties of users or items involved, limiting their applicability in domains\nwith very sparse rating spaces. In many domains such as movies, news or\nelectronic commerce recommenders, considerable contextual data in text form\ndescribing item properties is available along with the rating data, which could\nbe utilized to improve recommendation quality.In this paper, we propose a novel\napproach to improve standard CF based recommenders by utilizing latent\nDirichlet allocation (LDA) to learn latent properties of items, expressed in\nterms of topic proportions, derived from their textual description. We infer\nuser's topic preferences or persona in the same latent space,based on her\nhistorical ratings. While computing similarity between users, we make use of a\ncombined similarity measure involving rating overlap as well as similarity in\nthe latent topic space. This approach alleviates sparsity problem as it allows\ncalculation of similarity between users even if they have not rated any items\nin common. Our experiments on multiple public datasets indicate that the\nproposed hybrid approach significantly outperforms standard user Based and item\nBased CF recommenders in terms of classification accuracy metrics such as\nprecision, recall and f-measure.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 16:52:05 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Wilson", "Jobin", ""], ["Chaudhury", "Santanu", ""], ["Lall", "Brejesh", ""], ["Kapadia", "Prateek", ""]]}, {"id": "1402.6926", "submitter": "Peter Foster", "authors": "Peter Foster, Matthias Mauch and Simon Dixon", "title": "Sequential Complexity as a Descriptor for Musical Similarity", "comments": "13 pages, 9 figures, 8 tables. Accepted version", "journal-ref": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  vol. 22 no. 12, pp. 1965-1977, 2014", "doi": "10.1109/TASLP.2014.2357676", "report-no": null, "categories": "cs.IR cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose string compressibility as a descriptor of temporal structure in\naudio, for the purpose of determining musical similarity. Our descriptors are\nbased on computing track-wise compression rates of quantised audio features,\nusing multiple temporal resolutions and quantisation granularities. To verify\nthat our descriptors capture musically relevant information, we incorporate our\ndescriptors into similarity rating prediction and song year prediction tasks.\nWe base our evaluation on a dataset of 15500 track excerpts of Western popular\nmusic, for which we obtain 7800 web-sourced pairwise similarity ratings. To\nassess the agreement among similarity ratings, we perform an evaluation under\ncontrolled conditions, obtaining a rank correlation of 0.33 between intersected\nsets of ratings. Combined with bag-of-features descriptors, we obtain\nperformance gains of 31.1% and 10.9% for similarity rating prediction and song\nyear prediction. For both tasks, analysis of selected descriptors reveals that\nrepresenting features at multiple time scales benefits prediction accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 14:51:48 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2014 15:14:37 GMT"}, {"version": "v3", "created": "Sun, 28 Sep 2014 23:33:44 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Foster", "Peter", ""], ["Mauch", "Matthias", ""], ["Dixon", "Simon", ""]]}, {"id": "1402.7200", "submitter": "Venugopal K r", "authors": "Leena Giri G, Srikanth P L, S H Manjula, K R Venugopal, L M Patnaik", "title": "Mathematical Model of Semantic Look - An Efficient Context Driven Search\n  Engine", "comments": "12 pages", "journal-ref": "International Journal of Information Processing, 7(2), 20-31, 2013", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The WorldWideWeb (WWW) is a huge conservatory of web pages. Search Engines\nare key applications that fetch web pages for the user query. In the current\ngeneration web architecture, search engines treat keywords provided by the user\nas isolated keywords without considering the context of the user query. This\nresults in a lot of unrelated pages or links being displayed to the user.\nSemantic Web is based on the current web with a revised framework to display a\nmore precise result set as response to a user query. The current web pages need\nto be annotated by finding relevant meta data to be added to each of them, so\nthat they become useful to Semantic Web search engines. Semantic Look explores\nthe context of user query by processing the Semantic information recorded in\nthe web pages. It is compared with an existing algorithm called OntoLook and it\nis shown that Semantic Look is a better optimized search engine by being more\nthan twice as fast as OntoLook.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 10:55:24 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["G", "Leena Giri", ""], ["L", "Srikanth P", ""], ["Manjula", "S H", ""], ["Venugopal", "K R", ""], ["Patnaik", "L M", ""]]}]