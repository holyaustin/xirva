[{"id": "1212.0074", "submitter": "Kyumars Sheykh Esmaili", "authors": "Kyumars Sheykh Esmaili", "title": "Challenges in Kurdish Text Processing", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite having a large number of speakers, the Kurdish language is among the\nless-resourced languages. In this work we highlight the challenges and problems\nin providing the required tools and techniques for processing texts written in\nKurdish. From a high-level perspective, the main challenges are: the inherent\ndiversity of the language, standardization and segmentation issues, and the\nlack of language resources.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2012 07:01:27 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Esmaili", "Kyumars Sheykh", ""]]}, {"id": "1212.0190", "submitter": "Xu He", "authors": "Xu He, Fan Min, William Zhu", "title": "A Comparative Study of Discretization Approaches for Granular\n  Association Rule Mining", "comments": "17 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:1210.0065", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granular association rule mining is a new relational data mining approach to\nreveal patterns hidden in multiple tables. The current research of granular\nassociation rule mining considers only nominal data. In this paper, we study\nthe impact of discretization approaches on mining semantically richer and\nstronger rules from numeric data. Specifically, the Equal Width approach and\nthe Equal Frequency approach are adopted and compared. The setting of interval\nnumbers is a key issue in discretization approaches, so we compare different\nsettings through experiments on a well-known real life data set. Experimental\nresults show that: 1) discretization is an effective preprocessing technique in\nmining stronger rules; 2) the Equal Frequency approach helps generating more\nrules than the Equal Width approach; 3) with certain settings of interval\nnumbers, we can obtain much more rules than others.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2012 06:51:40 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 02:36:56 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2012 02:29:40 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["He", "Xu", ""], ["Min", "Fan", ""], ["Zhu", "William", ""]]}, {"id": "1212.0763", "submitter": "Modou Gueye M.", "authors": "Modou Gueye, Talel Abdessalem, Hubert Naacke", "title": "Dynamic recommender system : using cluster-based biases to improve the\n  accuracy of the predictions", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is today accepted that matrix factorization models allow a high quality of\nrating prediction in recommender systems. However, a major drawback of matrix\nfactorization is its static nature that results in a progressive declining of\nthe accuracy of the predictions after each factorization. This is due to the\nfact that the new obtained ratings are not taken into account until a new\nfactorization is computed, which can not be done very often because of the high\ncost of matrix factorization.\n  In this paper, aiming at improving the accuracy of recommender systems, we\npropose a cluster-based matrix factorization technique that enables online\nintegration of new ratings. Thus, we significantly enhance the obtained\npredictions between two matrix factorizations. We use finer-grained user biases\nby clustering similar items into groups, and allocating in these groups a bias\nto each user. The experiments we did on large datasets demonstrated the\nefficiency of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 13:00:27 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["Gueye", "Modou", ""], ["Abdessalem", "Talel", ""], ["Naacke", "Hubert", ""]]}, {"id": "1212.0960", "submitter": "Hyun Joon Jung", "authors": "Hyun Joon Jung and Matthew Lease", "title": "Evaluating Classifiers Without Expert Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the challenge of evaluating a set of classifiers, as\ndone in shared task evaluations like the KDD Cup or NIST TREC, without expert\nlabels. While expert labels provide the traditional cornerstone for evaluating\nstatistical learners, limited or expensive access to experts represents a\npractical bottleneck. Instead, we seek methodology for estimating performance\nof the classifiers which is more scalable than expert labeling yet preserves\nhigh correlation with evaluation based on expert labels. We consider both: 1)\nusing only labels automatically generated by the classifiers (blind\nevaluation); and 2) using labels obtained via crowdsourcing. While\ncrowdsourcing methods are lauded for scalability, using such data for\nevaluation raises serious concerns given the prevalence of label noise. In\nregard to blind evaluation, two broad strategies are investigated: combine &\nscore and score & combine methods infer a single pseudo-gold label set by\naggregating classifier labels; classifiers are then evaluated based on this\nsingle pseudo-gold label set. On the other hand, score & combine methods: 1)\nsample multiple label sets from classifier outputs, 2) evaluate classifiers on\neach label set, and 3) average classifier performance across label sets. When\nadditional crowd labels are also collected, we investigate two alternative\navenues for exploiting them: 1) direct evaluation of classifiers; or 2)\nsupervision of combine & score methods. To assess generality of our techniques,\nclassifier performance is measured using four common classification metrics,\nwith statistical significance tests. Finally, we measure both score and rank\ncorrelations between estimated classifier performance vs. actual performance\naccording to expert judgments. Rigorous evaluation of classifiers from the TREC\n2011 Crowdsourcing Track shows reliable evaluation can be achieved without\nreliance on expert labels.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 08:15:36 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Jung", "Hyun Joon", ""], ["Lease", "Matthew", ""]]}, {"id": "1212.1068", "submitter": "Leonardo Ermann", "authors": "Leonardo Ermann, Klaus M. Frahm and Dima L. Shepelyansky", "title": "Spectral properties of Google matrix of Wikipedia and other networks", "comments": "10 pages, 9 figures", "journal-ref": "Eur. Phys. J. B 86, 193 (2013)", "doi": "10.1140/epjb/e2013-31090-8", "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the properties of eigenvalues and eigenvectors of the Google matrix\nof the Wikipedia articles hyperlink network and other real networks. With the\nhelp of the Arnoldi method we analyze the distribution of eigenvalues in the\ncomplex plane and show that eigenstates with significant eigenvalue modulus are\nlocated on well defined network communities. We also show that the correlator\nbetween PageRank and CheiRank vectors distinguishes different organizations of\ninformation flow on BBC and Le Monde web sites.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 15:58:19 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Ermann", "Leonardo", ""], ["Frahm", "Klaus M.", ""], ["Shepelyansky", "Dima L.", ""]]}, {"id": "1212.1131", "submitter": "Lior Rokach", "authors": "Gilad Katz, Guy Shani, Bracha Shapira, Lior Rokach", "title": "Using Wikipedia to Boost SVD Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singular Value Decomposition (SVD) has been used successfully in recent years\nin the area of recommender systems. In this paper we present how this model can\nbe extended to consider both user ratings and information from Wikipedia. By\nmapping items to Wikipedia pages and quantifying their similarity, we are able\nto use this information in order to improve recommendation accuracy, especially\nwhen the sparsity is high. Another advantage of the proposed approach is the\nfact that it can be easily integrated into any other SVD implementation,\nregardless of additional parameters that may have been added to it. Preliminary\nexperimental results on the MovieLens dataset are encouraging.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 19:03:39 GMT"}], "update_date": "2012-12-06", "authors_parsed": [["Katz", "Gilad", ""], ["Shani", "Guy", ""], ["Shapira", "Bracha", ""], ["Rokach", "Lior", ""]]}, {"id": "1212.1464", "submitter": "Manuel Gomez Rodriguez", "authors": "Manuel Gomez Rodriguez, Jure Leskovec, Bernhard Sch\\\"olkopf", "title": "Structure and Dynamics of Information Pathways in Online Media", "comments": "To Appear at the 6th International Conference on Web Search and Data\n  Mining (WSDM '13)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion of information, spread of rumors and infectious diseases are all\ninstances of stochastic processes that occur over the edges of an underlying\nnetwork. Many times networks over which contagions spread are unobserved, and\nsuch networks are often dynamic and change over time. In this paper, we\ninvestigate the problem of inferring dynamic networks based on information\ndiffusion data. We assume there is an unobserved dynamic network that changes\nover time, while we observe the results of a dynamic process spreading over the\nedges of the network. The task then is to infer the edges and the dynamics of\nthe underlying network.\n  We develop an on-line algorithm that relies on stochastic convex optimization\nto efficiently solve the dynamic network inference problem. We apply our\nalgorithm to information diffusion among 3.3 million mainstream media and blog\nsites and experiment with more than 179 million different pieces of information\nspreading over the network in a one year period. We study the evolution of\ninformation pathways in the online media space and find interesting insights.\nInformation pathways for general recurrent topics are more stable across time\nthan for on-going news events. Clusters of news media sites and blogs often\nemerge and vanish in matter of days for on-going news events. Major social\nmovements and events involving civil population, such as the Libyan's civil war\nor Syria's uprise, lead to an increased amount of information pathways among\nblogs as well as in the overall increase in the network centrality of blogs and\nsocial media sites.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 21:01:07 GMT"}], "update_date": "2012-12-10", "authors_parsed": [["Rodriguez", "Manuel Gomez", ""], ["Leskovec", "Jure", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1212.1478", "submitter": "Bohdan Pavlyshenko", "authors": "Bohdan Pavlyshenko", "title": "The Clustering of Author's Texts of English Fiction in the Vector Space\n  of Semantic Fields", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering of text documents in the vector space of semantic fields and\nin the semantic space with orthogonal basis has been analysed. It is shown that\nusing the vector space model with the basis of semantic fields is effective in\nthe cluster analysis algorithms of author's texts in English fiction. The\nanalysis of the author's texts distribution in cluster structure showed the\npresence of the areas of semantic space that represent the author's ideolects\nof individual authors. SVD factorization of the semantic fields matrix makes it\npossible to reduce significantly the dimension of the semantic space in the\ncluster analysis of author's texts.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 21:28:19 GMT"}], "update_date": "2012-12-10", "authors_parsed": [["Pavlyshenko", "Bohdan", ""]]}, {"id": "1212.1625", "submitter": "Daniel Faria", "authors": "Daniel Faria, Catia Pesquita, Emanuel Santos, Francisco M. Couto,\n  Cosmin Stroe, Isabel F. Cruz", "title": "Testing the AgreementMaker System in the Anatomy Task of OAEI 2012", "comments": "4 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The AgreementMaker system was the leading system in the anatomy task of the\nOntology Alignment Evaluation Initiative (OAEI) competition in 2011. While\nAgreementMaker did not compete in OAEI 2012, here we report on its performance\nin the 2012 anatomy task, using the same configurations of AgreementMaker\nsubmitted to OAEI 2011. Additionally, we also test AgreementMaker using an\nupdated version of the UBERON ontology as a mediating ontology, and otherwise\nidentical configurations. AgreementMaker achieved an F-measure of 91.8% with\nthe 2011 configurations, and an F-measure of 92.2% with the updated UBERON\nontology. Thus, AgreementMaker would have been the second best system had it\ncompeted in the anatomy task of OAEI 2012, and only 0.1% below the F-measure of\nthe best system.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2012 14:59:36 GMT"}], "update_date": "2012-12-10", "authors_parsed": [["Faria", "Daniel", ""], ["Pesquita", "Catia", ""], ["Santos", "Emanuel", ""], ["Couto", "Francisco M.", ""], ["Stroe", "Cosmin", ""], ["Cruz", "Isabel F.", ""]]}, {"id": "1212.1918", "submitter": "Juan Manuel Torres Moreno", "authors": "Juan-Manuel Torres-Moreno, Patricia Vel\\'azquez-Morales, Jean-Guy\n  Meunier", "title": "Condens\\'es de textes par des m\\'ethodes num\\'eriques", "comments": "Conf\\'erence JADT 2002, Saint-Malo/France. 12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Since information in electronic form is already a standard, and that the\nvariety and the quantity of information become increasingly large, the methods\nof summarizing or automatic condensation of texts is a critical phase of the\nanalysis of texts. This article describes CORTEX a system based on numerical\nmethods, which allows obtaining a condensation of a text, which is independent\nof the topic and of the length of the text. The structure of the system enables\nit to find the abstracts in French or Spanish in very short times.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2012 20:55:52 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Torres-Moreno", "Juan-Manuel", ""], ["Vel\u00e1zquez-Morales", "Patricia", ""], ["Meunier", "Jean-Guy", ""]]}, {"id": "1212.2006", "submitter": "Jiwei Li", "authors": "Jiwei Li and Sujian Li", "title": "A Novel Feature-based Bayesian Model for Query Focused Multi-document\n  Summarization", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in equation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both supervised learning methods and LDA based topic model have been\nsuccessfully applied in the field of query focused multi-document\nsummarization. In this paper, we propose a novel supervised approach that can\nincorporate rich sentence features into Bayesian topic models in a principled\nway, thus taking advantages of both topic model and feature based supervised\nlearning methods. Experiments on TAC2008 and TAC2009 demonstrate the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 09:41:12 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2013 17:28:14 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Li", "Jiwei", ""], ["Li", "Sujian", ""]]}, {"id": "1212.2036", "submitter": "Jiwei Li", "authors": "Jiwei Li and Sujian Li", "title": "Query-focused Multi-document Summarization: Combining a Novel Topic\n  Model with Graph-based Semi-supervised Learning", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in equation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based semi-supervised learning has proven to be an effective approach\nfor query-focused multi-document summarization. The problem of previous\nsemi-supervised learning is that sentences are ranked without considering the\nhigher level information beyond sentence level. Researches on general\nsummarization illustrated that the addition of topic level can effectively\nimprove the summary quality. Inspired by previous researches, we propose a\ntwo-layer (i.e. sentence layer and topic layer) graph-based semi-supervised\nlearning approach. At the same time, we propose a novel topic model which makes\nfull use of the dependence between sentences and words. Experimental results on\nDUC and TAC data sets demonstrate the effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 11:35:29 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2013 17:24:00 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2013 17:13:33 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Li", "Jiwei", ""], ["Li", "Sujian", ""]]}, {"id": "1212.2065", "submitter": "Youssef Bassil", "authors": "Youssef Bassil", "title": "A Survey on Information Retrieval, Text Categorization, and Web Crawling", "comments": "LACSC - Lebanese Association for Computational Sciences,\n  http://www.lacsc.org", "journal-ref": "Journal of Computer Science & Research (JCSCR), Vol. 1, No. 6, pp.\n  1-11, 2012", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a survey discussing Information Retrieval concepts, methods,\nand applications. It goes deep into the document and query modelling involved\nin IR systems, in addition to pre-processing operations such as removing stop\nwords and searching by synonym techniques. The paper also tackles text\ncategorization along with its application in neural networks and machine\nlearning. Finally, the architecture of web crawlers is to be discussed shedding\nthe light on how internet spiders index web documents and how they allow users\nto search for items on the web.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 13:57:59 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Bassil", "Youssef", ""]]}, {"id": "1212.2145", "submitter": "Shuanghong Yang", "authors": "Shuang-Hong Yang", "title": "A Scale-Space Theory for Text", "comments": "9 pages, 6 figures; Nature language processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scale-space theory has been established primarily by the computer vision and\nsignal processing communities as a well-founded and promising framework for\nmulti-scale processing of signals (e.g., images). By embedding an original\nsignal into a family of gradually coarsen signals parameterized with a\ncontinuous scale parameter, it provides a formal framework to capture the\nstructure of a signal at different scales in a consistent way. In this paper,\nwe present a scale space theory for text by integrating semantic and spatial\nfilters, and demonstrate how natural language documents can be understood,\nprocessed and analyzed at multiple resolutions, and how this scale-space\nrepresentation can be used to facilitate a variety of NLP and text analysis\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 17:39:44 GMT"}], "update_date": "2012-12-14", "authors_parsed": [["Yang", "Shuang-Hong", ""]]}, {"id": "1212.2150", "submitter": "Shuanghong Yang", "authors": "Shuang-Hong Yang", "title": "Collaborative Competitive filtering II: Optimal Recommendation and\n  Collaborative Games", "comments": "10 pages, 5 figures; Recommender system, Collaborative filtering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have emerged as a new weapon to help online firms to\nrealize many of their strategic goals (e.g., to improve sales, revenue,\ncustomer experience etc.). However, many existing techniques commonly approach\nthese goals by seeking to recover preference (e.g., estimating ratings) in a\nmatrix completion framework. This paper aims to bridge this significant gap\nbetween the clearly-defined strategic objectives and the not-so-well-justified\nproxy.\n  We show it is advantageous to think of a recommender system as an analogy to\na monopoly economic market with the system as the sole seller, users as the\nbuyers and items as the goods. This new perspective motivates a game-theoretic\nformulation for recommendation that enables us to identify the optimal\nrecommendation policy by explicit optimizing certain strategic goals. In this\npaper, we revisit and extend our prior work, the Collaborative-Competitive\nFiltering preference model, towards a game-theoretic framework. The proposed\nframework consists of two components. First, a conditional preference model\nthat characterizes how a user would respond to a recommendation action; Second,\nknowing in advance how the user would respond, how a recommender system should\nact (i.e., recommend) strategically to maximize its goals. We show how\nobjectives such as click-through rate, sales revenue and consumption diversity\ncan be optimized explicitly in this framework. Experiments are conducted on a\ncommercial recommender system and demonstrate promising results.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 17:57:07 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Yang", "Shuang-Hong", ""]]}, {"id": "1212.2287", "submitter": "Jimmy Lin", "authors": "Nima Asadi, Jimmy Lin, and Arjen P. de Vries", "title": "Runtime Optimizations for Prediction with Tree-Based Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-based models have proven to be an effective solution for web ranking as\nwell as other problems in diverse domains. This paper focuses on optimizing the\nruntime performance of applying such models to make predictions, given an\nalready-trained model. Although exceedingly simple conceptually, most\nimplementations of tree-based models do not efficiently utilize modern\nsuperscalar processor architectures. By laying out data structures in memory in\na more cache-conscious fashion, removing branches from the execution flow using\na technique called predication, and micro-batching predictions using a\ntechnique called vectorization, we are able to better exploit modern processor\narchitectures and significantly improve the speed of tree-based models over\nhard-coded if-else blocks. Our work contributes to the exploration of\narchitecture-conscious runtime implementations of machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 03:20:46 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2013 16:33:08 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Asadi", "Nima", ""], ["Lin", "Jimmy", ""], ["de Vries", "Arjen P.", ""]]}, {"id": "1212.2442", "submitter": "Craig Boutilier", "authors": "Craig Boutilier, Richard S. Zemel, Benjamin Marlin", "title": "Active Collaborative Filtering", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-98-106", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) allows the preferences of multiple users to be\npooled to make recommendations regarding unseen products. We consider in this\npaper the problem of online and interactive CF: given the current ratings\nassociated with a user, what queries (new ratings) would most improve the\nquality of the recommendations made? We cast this terms of expected value of\ninformation (EVOI); but the online computational cost of computing optimal\nqueries is prohibitive. We show how offline prototyping and computation of\nbounds on EVOI can be used to dramatically reduce the required online\ncomputation. The framework we develop is general, but we focus on derivations\nand empirical study in the specific case of the multiple-cause vector\nquantization model.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:04:12 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Boutilier", "Craig", ""], ["Zemel", "Richard S.", ""], ["Marlin", "Benjamin", ""]]}, {"id": "1212.2453", "submitter": "David Azari", "authors": "David Azari, Eric J. Horvitz, Susan Dumais, Eric Brill", "title": "Web-Based Question Answering: A Decision-Making Perspective", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-11-19", "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an investigation of the use of probabilistic models and\ncost-benefit analyses to guide resource-intensive procedures used by a\nWeb-based question answering system. We first provide an overview of research\non question-answering systems. Then, we present details on AskMSR, a prototype\nweb-based question answering system. We discuss Bayesian analyses of the\nquality of answers generated by the system and show how we can endow the system\nwith the ability to make decisions about the number of queries issued to a\nsearch engine, given the cost of queries and the expected value of query\nresults in refining an ultimate answer. Finally, we review the results of a set\nof experiments.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:03:27 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Azari", "David", ""], ["Horvitz", "Eric J.", ""], ["Dumais", "Susan", ""], ["Brill", "Eric", ""]]}, {"id": "1212.2477", "submitter": "Shyong (Tony) K. Lam", "authors": "Shyong (Tony) K. Lam, David M Pennock, Dan Cosley, Steve Lawrence", "title": "1 Billion Pages = 1 Million Dollars? Mining the Web to Play \"Who Wants\n  to be a Millionaire?\"", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-337-345", "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit the redundancy and volume of information on the web to build a\ncomputerized player for the ABC TV game show 'Who Wants To Be A Millionaire?'\nThe player consists of a question-answering module and a decision-making\nmodule. The question-answering module utilizes question transformation\ntechniques, natural language parsing, multiple information retrieval\nalgorithms, and multiple search engines; results are combined in the spirit of\nensemble learning using an adaptive weighting scheme. Empirically, the system\ncorrectly answers about 75% of questions from the Millionaire CD-ROM, 3rd\nedition - general-interest trivia questions often about popular culture and\ncommon knowledge. The decision-making module chooses from allowable actions in\nthe game in order to maximize expected risk-adjusted winnings, where the\nestimated probability of answering correctly is a function of past performance\nand confidence in in correctly answering the current question. When given a six\nquestion head start (i.e., when starting from the $2,000 level), we find that\nthe system performs about as well on average as humans starting at the\nbeginning. Our system demonstrates the potential of simple but well-chosen\ntechniques for mining answers from unstructured information such as the web.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:06:15 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Shyong", "", "", "Tony"], ["Lam", "K.", ""], ["Pennock", "David M", ""], ["Cosley", "Dan", ""], ["Lawrence", "Steve", ""]]}, {"id": "1212.2478", "submitter": "Rong Jin", "authors": "Rong Jin, Luo Si, ChengXiang Zhai", "title": "Preference-based Graphic Models for Collaborative Filtering", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-329-336", "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is a very useful general technique for exploiting the\npreference patterns of a group of users to predict the utility of items to a\nparticular user. Previous research has studied several probabilistic graphic\nmodels for collaborative filtering with promising results. However, while these\nmodels have succeeded in capturing the similarity among users and items in one\nway or the other, none of them has considered the fact that users with similar\ninterests in items can have very different rating patterns; some users tend to\nassign a higher rating to all items than other users. In this paper, we propose\nand study of two new graphic models that address the distinction between user\npreferences and ratings. In one model, called the decoupled model, we introduce\ntwo different variables to decouple a users preferences FROM his ratings. IN\nthe other, called the preference model, we model the orderings OF items\npreferred BY a USER, rather than the USERs numerical ratings of items.\nEmpirical study over two datasets of movie ratings shows that appropriate\nmodeling of the distinction between user preferences and ratings improves the\nperformance substantially and consistently. Specifically, the proposed\ndecoupled model outperforms all five existing approaches that we compare with\nsignificantly, but the preference model is not very successful. These results\nsuggest that explicit modeling of the underlying user preferences is very\nimportant for collaborative filtering, but we can not afford ignoring the\nrating information completely.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:06:09 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Jin", "Rong", ""], ["Si", "Luo", ""], ["Zhai", "ChengXiang", ""]]}, {"id": "1212.2508", "submitter": "Kai Yu", "authors": "Kai Yu, Anton Schwaighofer, Volker Tresp, Wei-Ying Ma, HongJiang Zhang", "title": "Collaborative Ensemble Learning: Combining Collaborative and\n  Content-Based Information Filtering via Hierarchical Bayes", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-616-623", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) and content-based filtering (CBF) have widely\nbeen used in information filtering applications. Both approaches have their\nstrengths and weaknesses which is why researchers have developed hybrid\nsystems. This paper proposes a novel approach to unify CF and CBF in a\nprobabilistic framework, named collaborative ensemble learning. It uses\nprobabilistic SVMs to model each user's profile (as CBF does).At the prediction\nphase, it combines a society OF users profiles, represented by their respective\nSVM models, to predict an active users preferences(the CF idea).The combination\nscheme is embedded in a probabilistic framework and retains an intuitive\nexplanation.Moreover, collaborative ensemble learning does not require a global\ntraining stage and thus can incrementally incorporate new data.We report\nresults based on two data sets. For the Reuters-21578 text data set, we\nsimulate user ratings under the assumption that each user is interested in only\none category. In the second experiment, we use users' opinions on a set of 642\nart images that were collected through a web-based survey. For both data sets,\ncollaborative ensemble achieved excellent performance in terms of\nrecommendation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:08:51 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Yu", "Kai", ""], ["Schwaighofer", "Anton", ""], ["Tresp", "Volker", ""], ["Ma", "Wei-Ying", ""], ["Zhang", "HongJiang", ""]]}, {"id": "1212.2509", "submitter": "Joel Young", "authors": "Joel Young, Thomas L. Dean", "title": "Exploiting Locality in Searching the Web", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-608-615", "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Published experiments on spidering the Web suggest that, given training data\nin the form of a (relatively small) subgraph of the Web containing a subset of\na selected class of target pages, it is possible to conduct a directed search\nand find additional target pages significantly faster (with fewer page\nretrievals) than by performing a blind or uninformed random or systematic\nsearch, e.g., breadth-first search. If true, this claim motivates a number of\npractical applications. Unfortunately, these experiments were carried out in\nspecialized domains or under conditions that are difficult to replicate. We\npresent and apply an experimental framework designed to reexamine and resolve\nthe basic claims of the earlier work, so that the supporting experiments can be\nreplicated and built upon. We provide high-performance tools for building\nexperimental spiders, make use of the ground truth and static nature of the\nWT10g TREC Web corpus, and rely on simple well understand machine learning\ntechniques to conduct our experiments. In this paper, we describe the basic\nframework, motivate the experimental design, and report on our findings\nsupporting and qualifying the conclusions of the earlier research.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:08:47 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Young", "Joel", ""], ["Dean", "Thomas L.", ""]]}, {"id": "1212.2587", "submitter": "Abdelkrim Bouramoul", "authors": "Abdelkrim Bouramoul, Mohamed-Khireddine Kholladi, Bich-Li\\^en Doan", "title": "An ontology-based approach for semantics ranking of the web search\n  engines results", "comments": "6 pages, 5 figures, appears in: (ICMCS), 2012 International\n  Conference on Multimedia Computing and Systems, Print ISBN: 978-1-4673-1518-0", "journal-ref": "In The 3rd International Conference on Multimedia Computing and\n  Systems, IEEE. 797-802, Tanger, Morocco, 2012", "doi": "10.1109/ICMCS.2012.6320318", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work falls in the areas of information retrieval and semantic web, and\naims to improve the evaluation of web search tools. Indeed, the huge number of\ninformation on the web as well as the growth of new inexperienced users creates\nnew challenges for information retrieval; certainly the current search engines\n(such as Google, Bing and Yahoo) offer an efficient way to browse the web\ncontent. However, this type of tool does not take into account the semantic\ndriven by the query terms and document words. This paper proposes a new\nsemantic based approach for the evaluation of information retrieval systems;\nthe goal is to increase the selectivity of search tools and to improve how\nthese tools are evaluated. The test of the proposed approach for the evaluation\nof search engines has proved its applicability to real search tools. The\nresults showed that semantic evaluation is a promising way to improve the\nperformance and behavior of search engines as well as the relevance of the\nresults that they return.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 18:58:13 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Bouramoul", "Abdelkrim", ""], ["Kholladi", "Mohamed-Khireddine", ""], ["Doan", "Bich-Li\u00ean", ""]]}, {"id": "1212.2676", "submitter": "Aaron Gerow", "authors": "Aaron Gerow and Mark Keane", "title": "Mining the Web for the Voice of the Herd to Track Stock Market Bubbles", "comments": "Proceedings of the 22nd International Joint Conference on Artificial\n  Intelligence (IJCAI '11), Barcelona, Spain, 16-22 July, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR physics.soc-ph q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that power-law analyses of financial commentaries from newspaper\nweb-sites can be used to identify stock market bubbles, supplementing\ntraditional volatility analyses. Using a four-year corpus of 17,713 online,\nfinance-related articles (10M+ words) from the Financial Times, the New York\nTimes, and the BBC, we show that week-to-week changes in power-law\ndistributions reflect market movements of the Dow Jones Industrial Average\n(DJI), the FTSE-100, and the NIKKEI-225. Notably, the statistical regularities\nin language track the 2007 stock market bubble, showing emerging structure in\nthe language of commentators, as progressively greater agreement arose in their\npositive perceptions of the market. Furthermore, during the bubble period, a\nmarked divergence in positive language occurs as revealed by a Kullback-Leibler\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 23:47:56 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Gerow", "Aaron", ""], ["Keane", "Mark", ""]]}, {"id": "1212.2791", "submitter": "Llu\\'is Belanche-Mu\\~noz", "authors": "Llu\\'is A. Belanche", "title": "Understanding (dis)similarity measures", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": "LSI-12-16-R", "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intuitively, the concept of similarity is the notion to measure an inexact\nmatching between two entities of the same reference set. The notions of\nsimilarity and its close relative dissimilarity are widely used in many fields\nof Artificial Intelligence. Yet they have many different and often partial\ndefinitions or properties, usually restricted to one field of application and\nthus incompatible with other uses. This paper contributes to the design and\nunderstanding of similarity and dissimilarity measures for Artificial\nIntelligence. A formal dual definition for each concept is proposed, joined\nwith a set of fundamental properties. The behavior of the properties under\nseveral transformations is studied and revealed as an important matter to bear\nin mind. We also develop several practical examples that work out the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 12:27:53 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Belanche", "Llu\u00eds A.", ""]]}, {"id": "1212.3013", "submitter": "Gabriele Modena", "authors": "K. Massoudi, G. Modena", "title": "Product/Brand extraction from WikiPedia", "comments": "17 pages. Manuscript first creation date: November 27, 2009. At the\n  time of first creation both authors were affiliated with the University of\n  Amsterdam (The Netherlands)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the task of extracting product and brand pages from\nwikipedia. We present an experimental environment and setup built on top of a\ndataset of wikipedia pages we collected. We introduce a method for recognition\nof product pages modelled as a boolean probabilistic classification task. We\nshow that this approach can lead to promising results and we discuss\nalternative approaches we considered.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 23:25:46 GMT"}], "update_date": "2012-12-14", "authors_parsed": [["Massoudi", "K.", ""], ["Modena", "G.", ""]]}, {"id": "1212.3023", "submitter": "Mahyuddin K. M.  Nasution", "authors": "Mahyuddin K. M. Nasution, Shahrul Azman Mohd Noah", "title": "Keyword Extraction for Identifying Social Actors", "comments": "7 pages, nothing, draft to ICOCSIM 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the social actor has become one of tasks in Artificial\nIntelligence, whereby extracting keyword from Web snippets depend on the use of\nweb is steadily gaining ground in this research. We develop therefore an\napproach based on overlap principle for utilizing a collection of features in\nweb snippets, where use of keyword will eliminate the un-relevant web pages.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2012 00:34:23 GMT"}], "update_date": "2012-12-14", "authors_parsed": [["Nasution", "Mahyuddin K. M.", ""], ["Noah", "Shahrul Azman Mohd", ""]]}, {"id": "1212.3228", "submitter": "Peiyou Song", "authors": "Peiyou Song, Anhei Shu, David Phipps, Dan Wallach, Mohit Tiwari,\n  Jedidiah Crandall, George Luger", "title": "Language Without Words: A Pointillist Model for Natural Language\n  Processing", "comments": "5 pages, 2 figures", "journal-ref": "The 6th International Conference on Soft Computing and Intelligent\n  Systems (SCIS-ISIS 2012) Kobe, Japan", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores two separate questions: Can we perform natural language\nprocessing tasks without a lexicon?; and, Should we? Existing natural language\nprocessing techniques are either based on words as units or use units such as\ngrams only for basic classification tasks. How close can a machine come to\nreasoning about the meanings of words and phrases in a corpus without using any\nlexicon, based only on grams?\n  Our own motivation for posing this question is based on our efforts to find\npopular trends in words and phrases from online Chinese social media. This form\nof written Chinese uses so many neologisms, creative character placements, and\ncombinations of writing systems that it has been dubbed the \"Martian Language.\"\nReaders must often use visual queues, audible queues from reading out loud, and\ntheir knowledge and understanding of current events to understand a post. For\nanalysis of popular trends, the specific problem is that it is difficult to\nbuild a lexicon when the invention of new ways to refer to a word or concept is\neasy and common. For natural language processing in general, we argue in this\npaper that new uses of language in social media will challenge machines'\nabilities to operate with words as the basic unit of understanding, not only in\nChinese but potentially in other languages.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 20:19:58 GMT"}], "update_date": "2012-12-14", "authors_parsed": [["Song", "Peiyou", ""], ["Shu", "Anhei", ""], ["Phipps", "David", ""], ["Wallach", "Dan", ""], ["Tiwari", "Mohit", ""], ["Crandall", "Jedidiah", ""], ["Luger", "George", ""]]}, {"id": "1212.3390", "submitter": "A Majumder", "authors": "Anirban Majumder and Nisheeth Shrivastava", "title": "Know Your Personalization: Learning Topic level Personalization in\n  Online Services", "comments": "privacy, personalization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online service platforms (OSPs), such as search engines, news-websites,\nad-providers, etc., serve highly pe rsonalized content to the user, based on\nthe profile extracted from his history with the OSP. Although personalization\n(generally) leads to a better user experience, it also raises privacy concerns\nfor the user---he does not know what is present in his profile and more\nimportantly, what is being used to per sonalize content for him. In this paper,\nwe capture OSP's personalization for an user in a new data structure called the\nperson alization vector ($\\eta$), which is a weighted vector over a set of\ntopics, and present techniques to compute it for users of an OSP. Our approach\ntreats OSPs as black-boxes, and extracts $\\eta$ by mining only their output,\nspecifical ly, the personalized (for an user) and vanilla (without any user\ninformation) contents served, and the differences in these content. We\nformulate a new model called Latent Topic Personalization (LTP) that captures\nthe personalization vector into a learning framework and present efficient\ninference algorithms for it. We do extensive experiments for search result\npersonalization using both data from real Google users and synthetic datasets.\nOur results show high accuracy (R-pre = 84%) of LTP in finding personalized\ntopics. For Google data, our qualitative results show how LTP can also\nidentifies evidences---queries for results on a topic with high $\\eta$ value\nwere re-ranked. Finally, we show how our approach can be used to build a new\nPrivacy evaluation framework focused at end-user privacy on commercial OSPs.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 04:12:21 GMT"}], "update_date": "2012-12-17", "authors_parsed": [["Majumder", "Anirban", ""], ["Shrivastava", "Nisheeth", ""]]}, {"id": "1212.3493", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Alejandro Molina, Juan-Manuel Torres-Moreno, Iria da Cunha, Eric\n  SanJuan, Gerardo Sierra", "title": "Sentence Compression in Spanish driven by Discourse Segmentation and\n  Language Models", "comments": "7 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works demonstrated that Automatic Text Summarization (ATS) by\nsentences extraction may be improved using sentence compression. In this work\nwe present a sentence compressions approach guided by level-sentence discourse\nsegmentation and probabilistic language models (LM). The results presented here\nshow that the proposed solution is able to generate coherent summaries with\ngrammatical compressed sentences. The approach is simple enough to be\ntransposed into other languages.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 14:51:22 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2012 13:10:47 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Molina", "Alejandro", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["da Cunha", "Iria", ""], ["SanJuan", "Eric", ""], ["Sierra", "Gerardo", ""]]}, {"id": "1212.3536", "submitter": "Valmir Barbosa", "authors": "Flavio B. Gonzaga, Valmir C. Barbosa, Geraldo B. Xex\\'eo", "title": "The network structure of mathematical knowledge according to the\n  Wikipedia, MathWorld, and DLMF online libraries", "comments": null, "journal-ref": "Network Science 2 (2014), 367-386", "doi": "10.1017/nws.2014.20", "report-no": null, "categories": "cs.SI cs.IR math.HO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the network structure of Wikipedia (restricted to its mathematical\nportion), MathWorld, and DLMF. We approach these three online mathematical\nlibraries from the perspective of several global and local network-theoretic\nfeatures, providing for each one the appropriate value or distribution, along\nwith comparisons that, if possible, also include the whole of the Wikipedia or\nthe Web. We identify some distinguishing characteristics of all three\nlibraries, most of them supposedly traceable to the libraries' shared nature of\nrelating to a very specialized domain. Among these characteristics are the\npresence of a very large strongly connected component in each of the\ncorresponding directed graphs, the complete absence of any clear power laws\ndescribing the distribution of local features, and the rise to prominence of\nsome local features (e.g., stress centrality) that can be used to effectively\nsearch for keywords in the libraries.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 17:29:57 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Gonzaga", "Flavio B.", ""], ["Barbosa", "Valmir C.", ""], ["Xex\u00e9o", "Geraldo B.", ""]]}, {"id": "1212.3540", "submitter": "Dima Kagan", "authors": "Yehonatan Bitton, Michael Fire, Dima Kagan, Bracha Shapira, Lior\n  Rokach, Judit Bar-Ilan", "title": "Social Network Based Search for Experts", "comments": "Participated in HCIR 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our system illustrates how information retrieved from social networks can be\nused for suggesting experts for specific tasks. The system is designed to\nfacilitate the task of finding the appropriate person(s) for a job, as a\nconference committee member, an advisor, etc. This short description will\ndemonstrate how the system works in the context of the HCIR2012 published\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 17:35:31 GMT"}], "update_date": "2012-12-17", "authors_parsed": [["Bitton", "Yehonatan", ""], ["Fire", "Michael", ""], ["Kagan", "Dima", ""], ["Shapira", "Bracha", ""], ["Rokach", "Lior", ""], ["Bar-Ilan", "Judit", ""]]}, {"id": "1212.3634", "submitter": "Hanane Froud", "authors": "Hanane Froud, Abdelmonaim Lachkar, and Said Alaoui Ouatik", "title": "A comparative study of root-based and stem-based approaches for\n  measuring the similarity between arabic words for arabic text mining\n  applications", "comments": null, "journal-ref": "Advanced Computing An International Journal (ACIJ), November 2012,\n  Volume 3, Number 6", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation of semantic information contained in the words is needed for\nany Arabic Text Mining applications. More precisely, the purpose is to better\ntake into account the semantic dependencies between words expressed by the\nco-occurrence frequencies of these words. There have been many proposals to\ncompute similarities between words based on their distributions in contexts. In\nthis paper, we compare and contrast the effect of two preprocessing techniques\napplied to Arabic corpus: Rootbased (Stemming), and Stem-based (Light Stemming)\napproaches for measuring the similarity between Arabic words with the well\nknown abstractive model -Latent Semantic Analysis (LSA)- with a wide variety of\ndistance functions and similarity measures, such as the Euclidean Distance,\nCosine Similarity, Jaccard Coefficient, and the Pearson Correlation\nCoefficient. The obtained results show that, on the one hand, the variety of\nthe corpus produces more accurate results; on the other hand, the Stem-based\napproach outperformed the Root-based one because this latter affects the words\nmeanings.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 23:34:07 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Froud", "Hanane", ""], ["Lachkar", "Abdelmonaim", ""], ["Ouatik", "Said Alaoui", ""]]}, {"id": "1212.3906", "submitter": "Mahyuddin K. M.  Nasution", "authors": "Mahyuddin K. M. Nasution", "title": "Simple Search Engine Model: Adaptive Properties", "comments": "6 pages, noting, draf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the relationship between query and search engine by\nexploring the adaptive properties based on a simple search engine. We used set\ntheory and utilized the words and terms for defining singleton space of event\nin a search engine model, and then provided the inclusion between one singleton\nto another.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 07:13:50 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Nasution", "Mahyuddin K. M.", ""]]}, {"id": "1212.3964", "submitter": "Sourav Dutta", "authors": "Suman K. Bera, Sourav Dutta, Ankur Narang and Souvik Bhattacherjee", "title": "Advanced Bloom Filter Based Algorithms for Efficient Approximate Data\n  De-Duplication in Streams", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications involving telecommunication call data records, web pages, online\ntransactions, medical records, stock markets, climate warning systems, etc.,\nnecessitate efficient management and processing of such massively exponential\namount of data from diverse sources. De-duplication or Intelligent Compression\nin streaming scenarios for approximate identification and elimination of\nduplicates from such unbounded data stream is a greater challenge given the\nreal-time nature of data arrival. Stable Bloom Filters (SBF) addresses this\nproblem to a certain extent. .\n  In this work, we present several novel algorithms for the problem of\napproximate detection of duplicates in data streams. We propose the Reservoir\nSampling based Bloom Filter (RSBF) combining the working principle of reservoir\nsampling and Bloom Filters. We also present variants of the novel Biased\nSampling based Bloom Filter (BSBF) based on biased sampling concepts. We also\npropose a randomized load balanced variant of the sampling Bloom Filter\napproach to efficiently tackle the duplicate detection. In this work, we thus\nprovide a generic framework for de-duplication using Bloom Filters. Using\ndetailed theoretical analysis we prove analytical bounds on the false positive\nrate, false negative rate and convergence rate of the proposed structures. We\nexhibit that our models clearly outperform the existing methods. We also\ndemonstrate empirical analysis of the structures using real-world datasets (3\nmillion records) and also with synthetic datasets (1 billion records) capturing\nvarious input distributions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 11:47:09 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Bera", "Suman K.", ""], ["Dutta", "Sourav", ""], ["Narang", "Ankur", ""], ["Bhattacherjee", "Souvik", ""]]}, {"id": "1212.4522", "submitter": "Yunchao Gong", "authors": "Yunchao Gong and Qifa Ke and Michael Isard and Svetlana Lazebnik", "title": "A Multi-View Embedding Space for Modeling Internet Images, Tags, and\n  their Semantics", "comments": "To Appear: International Journal of Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of modeling Internet images and\nassociated text or tags for tasks such as image-to-image search, tag-to-image\nsearch, and image-to-tag search (image annotation). We start with canonical\ncorrelation analysis (CCA), a popular and successful approach for mapping\nvisual and textual features to the same latent space, and incorporate a third\nview capturing high-level image semantics, represented either by a single\ncategory or multiple non-mutually-exclusive concepts. We present two ways to\ntrain the three-view embedding: supervised, with the third view coming from\nground-truth labels or search keywords; and unsupervised, with semantic themes\nautomatically obtained by clustering the tags. To ensure high accuracy for\nretrieval tasks while keeping the learning process scalable, we combine\nmultiple strong visual features and use explicit nonlinear kernel mappings to\nefficiently approximate kernel CCA. To perform retrieval, we use a specially\ndesigned similarity function in the embedded space, which substantially\noutperforms the Euclidean distance. The resulting system produces compelling\nqualitative results and outperforms a number of two-view baselines on retrieval\ntasks on three large-scale Internet image datasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2012 22:02:43 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2013 19:14:58 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["Gong", "Yunchao", ""], ["Ke", "Qifa", ""], ["Isard", "Michael", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1212.4702", "submitter": "Mahyuddin K. M.  Nasution", "authors": "Mahyuddin K. M. Nasution", "title": "Simple Search Engine Model: Adaptive Properties for Doubleton", "comments": "5 pages, nothing, a draf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the relationship between query and search engine by\nexploring the adaptive properties for doubleton as a space of event based on a\nsimple search engine. We employ set theory for defining doubleton and generate\nsome properties.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2012 15:18:09 GMT"}], "update_date": "2012-12-20", "authors_parsed": [["Nasution", "Mahyuddin K. M.", ""]]}, {"id": "1212.5331", "submitter": "Ameer Albaham", "authors": "Ameer Tawfik Albaham, Naomie Salim", "title": "Adapting Voting Techniques for Online Forum Thread Retrieval", "comments": "The original publication is available at\n  http://www.springerlink.com/. Fixing minor typos. arXiv admin note: text\n  overlap with arXiv:1212.5590", "journal-ref": "Advanced Machine Learning Technologies and Applications, 2012,\n  322, 439-448", "doi": "10.1007/978-3-642-35326-0_44", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online forums or message boards are rich knowledge-based communities. In\nthese communities, thread retrieval is an essential tool facilitating\ninformation access. However, the issue on thread search is how to combine\nevidence from text units(messages) to estimate thread relevance. In this paper,\nwe first rank a list of messages, then we score threads by aggregating their\nranked messages' scores. To aggregate the message scores, we adopt several\nvoting techniques that have been applied in ranking aggregates tasks such as\nblog distillation and expert finding. The experimental result shows that many\nvoting techniques should be preferred over a baseline that treats a thread as a\nconcatenation of its message texts.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 04:22:13 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2013 08:37:47 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["Albaham", "Ameer Tawfik", ""], ["Salim", "Naomie", ""]]}, {"id": "1212.5423", "submitter": "Shameem Puthiya Parambath Mr.", "authors": "Shameem A Puthiya Parambath", "title": "Topic Extraction and Bundling of Related Scientific Articles", "comments": "NeSeFo 2012", "journal-ref": null, "doi": null, "report-no": "NeSeFo 2012", "categories": "cs.IR cs.DL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic classification of scientific articles based on common\ncharacteristics is an interesting problem with many applications in digital\nlibrary and information retrieval systems. Properly organized articles can be\nuseful for automatic generation of taxonomies in scientific writings, textual\nsummarization, efficient information retrieval etc. Generating article bundles\nfrom a large number of input articles, based on the associated features of the\narticles is tedious and computationally expensive task. In this report we\npropose an automatic two-step approach for topic extraction and bundling of\nrelated articles from a set of scientific articles in real-time. For topic\nextraction, we make use of Latent Dirichlet Allocation (LDA) topic modeling\ntechniques and for bundling, we make use of hierarchical agglomerative\nclustering techniques.\n  We run experiments to validate our bundling semantics and compare it with\nexisting models in use. We make use of an online crowdsourcing marketplace\nprovided by Amazon called Amazon Mechanical Turk to carry out experiments. We\nexplain our experimental setup and empirical results in detail and show that\nour method is advantageous over existing ones.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 13:25:00 GMT"}, {"version": "v2", "created": "Fri, 1 May 2015 15:30:45 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Parambath", "Shameem A Puthiya", ""]]}, {"id": "1212.5442", "submitter": "Gerald Kembellec", "authors": "G\\'erald Kembellec, Claire Scopsi (DICEN CNAM)", "title": "\\'Etude compar\\'ee de quatre logiciels de gestion de r\\'ef\\'erences\n  bibliographiques libres ou gratuits", "comments": "11 pages", "journal-ref": "Documentation et Bibliotheques 58, 4 (2012) 187-197", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is the result of the analysis of various bibliographic reference\nmanagement tools, especially those that are free. The use of editorial tools by\nbibliographic editors has evolved rapidly since 2007. But, until recently, free\nsoftware has fallen short when it comes to ergonomics or use. The functional\nand technical panorama offered by free software is the result of the comparison\nof JabRef, Mendeley Desktop, BibDesk and Zotero software undertaken in January\n2012 by two research professors affiliated with the Institut national\nfran\\c{c}ais des techniques de la documentation (INTD).\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 14:00:23 GMT"}], "update_date": "2012-12-24", "authors_parsed": [["Kembellec", "G\u00e9rald", "", "DICEN CNAM"], ["Scopsi", "Claire", "", "DICEN CNAM"]]}, {"id": "1212.5590", "submitter": "Ameer Albaham", "authors": "Ameer Tawfik Albaham, Naomie Salim", "title": "Online Forum Thread Retrieval using Pseudo Cluster Selection and Voting\n  Techniques", "comments": "The original publication is available at\n  http://www.springerlink.com/. arXiv admin note: substantial text overlap with\n  arXiv:1212.5331", "journal-ref": "Advances in Intelligent Systems and Applications, Volume 1, 2013,\n  20, 297-306", "doi": "10.1007/978-3-642-35452-6_31", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online forums facilitate knowledge seeking and sharing on the Web. However,\nthe shared knowledge is not fully utilized due to information overload. Thread\nretrieval is one method to overcome information overload. In this paper, we\npropose a model that combines two existing approaches: the Pseudo Cluster\nSelection and the Voting Techniques. In both, a retrieval system first scores a\nlist of messages and then ranks threads by aggregating their scored messages.\nThey differ on what and how to aggregate. The pseudo cluster selection focuses\non input, while voting techniques focus on the aggregation method. Our combined\nmodels focus on the input and the aggregation methods. The result shows that\nsome combined models are statistically superior to baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 07:21:06 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Albaham", "Ameer Tawfik", ""], ["Salim", "Naomie", ""]]}, {"id": "1212.5633", "submitter": "Romain Deveaud", "authors": "Pierre Joulin, Romain Deveaud, Eric SanJuan-Ibekwe, Jean-Marc\n  Francony, Fran\\c{c}oise Para", "title": "Design, implementation and experiment of a YeSQL Web Crawler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel, \"focusable\", scalable, distributed web crawler based on\nGNU/Linux and PostgreSQL that we designed to be easily extendible and which we\nhave released under a GNU public licence. We also report a first use case\nrelated to an analysis of Twitter's streams about the french 2012 presidential\nelections and the URL's it contains.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2012 23:20:24 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Joulin", "Pierre", ""], ["Deveaud", "Romain", ""], ["SanJuan-Ibekwe", "Eric", ""], ["Francony", "Jean-Marc", ""], ["Para", "Fran\u00e7oise", ""]]}, {"id": "1212.5650", "submitter": "Ke Zhou", "authors": "Ke Zhou, Hongyuan Zha, Gui-Rong Xue and Yong Yu", "title": "Learning the Gain Values and Discount Factors of DCG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation metrics are an essential part of a ranking system, and in the past\nmany evaluation metrics have been proposed in information retrieval and Web\nsearch. Discounted Cumulated Gains (DCG) has emerged as one of the evaluation\nmetrics widely adopted for evaluating the performance of ranking functions used\nin Web search. However, the two sets of parameters, gain values and discount\nfactors, used in DCG are determined in a rather ad-hoc way. In this paper we\nfirst show that DCG is generally not coherent, meaning that comparing the\nperformance of ranking functions using DCG very much depends on the particular\ngain values and discount factors used. We then propose a novel methodology that\ncan learn the gain values and discount factors from user preferences over\nrankings. Numerical simulations illustrate the effectiveness of our proposed\nmethods. Please contact the authors for the full version of this work.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2012 03:33:12 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Zhou", "Ke", ""], ["Zha", "Hongyuan", ""], ["Xue", "Gui-Rong", ""], ["Yu", "Yong", ""]]}, {"id": "1212.6110", "submitter": "Makiko Konoshima", "authors": "Makiko Konoshima and Yui Noma", "title": "Hyperplane Arrangements and Locality-Sensitive Hashing with Lift", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality-sensitive hashing converts high-dimensional feature vectors, such as\nimage and speech, into bit arrays and allows high-speed similarity calculation\nwith the Hamming distance. There is a hashing scheme that maps feature vectors\nto bit arrays depending on the signs of the inner products between feature\nvectors and the normal vectors of hyperplanes placed in the feature space. This\nhashing can be seen as a discretization of the feature space by hyperplanes. If\nlabels for data are given, one can determine the hyperplanes by using learning\nalgorithms. However, many proposed learning methods do not consider the\nhyperplanes' offsets. Not doing so decreases the number of partitioned regions,\nand the correlation between Hamming distances and Euclidean distances becomes\nsmall. In this paper, we propose a lift map that converts learning algorithms\nwithout the offsets to the ones that take into account the offsets. With this\nmethod, the learning methods without the offsets give the discretizations of\nspaces as if it takes into account the offsets. For the proposed method, we\ninput several high-dimensional feature data sets and studied the relationship\nbetween the statistical characteristics of data, the number of hyperplanes, and\nthe effect of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2012 02:14:41 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Konoshima", "Makiko", ""], ["Noma", "Yui", ""]]}, {"id": "1212.6177", "submitter": "Scott Ainsworth", "authors": "Scott G. Ainsworth and Ahmed AlSum and Hany SalahEldeen and Michele C.\n  Weigle and Michael L. Nelson", "title": "How Much of the Web Is Archived?", "comments": "This is the long version of the short paper by the same title\n  published at JCDL'11. 10 pages, 5 figures, 7 tables. Version 2 includes minor\n  typographical corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the Internet Archive's Wayback Machine is the largest and most\nwell-known web archive, there have been a number of public web archives that\nhave emerged in the last several years. With varying resources, audiences and\ncollection development policies, these archives have varying levels of overlap\nwith each other. While individual archives can be measured in terms of number\nof URIs, number of copies per URI, and intersection with other archives, to\ndate there has been no answer to the question \"How much of the Web is\narchived?\" We study the question by approximating the Web using sample URIs\nfrom DMOZ, Delicious, Bitly, and search engine indexes; and, counting the\nnumber of copies of the sample URIs exist in various public web archives. Each\nsample set provides its own bias. The results from our sample sets indicate\nthat range from 35%-90% of the Web has at least one archived copy, 17%-49% has\nbetween 2-5 copies, 1%-8% has 6-10 copies, and 8%-63% has more than 10 copies\nin public web archives. The number of URI copies varies as a function of time,\nbut no more than 31.3% of URIs are archived more than once per month.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2012 13:36:55 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2013 04:18:21 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Ainsworth", "Scott G.", ""], ["AlSum", "Ahmed", ""], ["SalahEldeen", "Hany", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1212.6193", "submitter": "Uma Sawant", "authors": "Uma Sawant and Soumen Chakrabarti", "title": "Learning Joint Query Interpretation and Response Ranking", "comments": "11 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to information extraction and semantic Web efforts, search on\nunstructured text is increasingly refined using semantic annotations and\nstructured knowledge bases. However, most users cannot become familiar with the\nschema of knowledge bases and ask structured queries. Interpreting free-format\nqueries into a more structured representation is of much current interest. The\ndominant paradigm is to segment or partition query tokens by purpose\n(references to types, entities, attribute names, attribute values, relations)\nand then launch the interpreted query on structured knowledge bases. Given that\nstructured knowledge extraction is never complete, here we use a data\nrepresentation that retains the unstructured text corpus, along with structured\nannotations (mentions of entities and relationships) on it. We propose two new,\nnatural formulations for joint query interpretation and response ranking that\nexploit bidirectional flow of information between the knowledge base and the\ncorpus.One, inspired by probabilistic language models, computes expected\nresponse scores over the uncertainties of query interpretation. The other is\nbased on max-margin discriminative learning, with latent variables representing\nthose uncertainties. In the context of typed entity search, both formulations\nbridge a considerable part of the accuracy gap between a generic query that\ndoes not constrain the type at all, and the upper bound where the \"perfect\"\ntarget entity type of each query is provided by humans. Our formulations are\nalso superior to a two-stage approach of first choosing a target type using\nrecent query type prediction techniques, and then launching a type-restricted\nentity search query.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2012 15:28:27 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Sawant", "Uma", ""], ["Chakrabarti", "Soumen", ""]]}]