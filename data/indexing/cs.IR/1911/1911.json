[{"id": "1911.00069", "submitter": "Jian Ni", "authors": "Jian Ni, Radu Florian", "title": "Neural Cross-Lingual Relation Extraction Based on Bilingual Word\n  Embedding Mapping", "comments": "11 pages, Conference on Empirical Methods in Natural Language\n  Processing (EMNLP), 2019", "journal-ref": null, "doi": "10.18653/v1/D19-1038", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation extraction (RE) seeks to detect and classify semantic relationships\nbetween entities, which provides useful information for many NLP applications.\nSince the state-of-the-art RE models require large amounts of manually\nannotated data and language-specific resources to achieve high accuracy, it is\nvery challenging to transfer an RE model of a resource-rich language to a\nresource-poor language. In this paper, we propose a new approach for\ncross-lingual RE model transfer based on bilingual word embedding mapping. It\nprojects word embeddings from a target language to a source language, so that a\nwell-trained source-language neural network RE model can be directly applied to\nthe target language. Experiment results show that the proposed approach\nachieves very good performance for a number of target languages on both\nin-house and open datasets, using a small bilingual dictionary with only 1K\nword pairs.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 19:30:54 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Ni", "Jian", ""], ["Florian", "Radu", ""]]}, {"id": "1911.00234", "submitter": "Rishi Hazra", "authors": "Rishi Hazra and Parag Dutta and Shubham Gupta and Mohammed Abdul\n  Qaathir and Ambedkar Dukkipati", "title": "Active$^2$ Learning: Actively reducing redundancies in Active Learning\n  methods for Sequence Tagging and Machine Translation", "comments": "Accepted in NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning is a powerful tool for natural language processing (NLP)\nproblems, successful solutions to these problems rely heavily on large amounts\nof annotated samples. However, manually annotating data is expensive and\ntime-consuming. Active Learning (AL) strategies reduce the need for huge\nvolumes of labeled data by iteratively selecting a small number of examples for\nmanual annotation based on their estimated utility in training the given model.\nIn this paper, we argue that since AL strategies choose examples independently,\nthey may potentially select similar examples, all of which may not contribute\nsignificantly to the learning process. Our proposed approach,\nActive$\\mathbf{^2}$ Learning (A$\\mathbf{^2}$L), actively adapts to the deep\nlearning model being trained to eliminate such redundant examples chosen by an\nAL strategy. We show that A$\\mathbf{^2}$L is widely applicable by using it in\nconjunction with several different AL strategies and NLP tasks. We empirically\ndemonstrate that the proposed approach is further able to reduce the data\nrequirements of state-of-the-art AL strategies by $\\approx \\mathbf{3-25\\%}$ on\nan absolute scale on multiple NLP tasks while achieving the same performance\nwith virtually no additional computation overhead.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 07:31:02 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 16:50:09 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 06:56:32 GMT"}, {"version": "v4", "created": "Tue, 6 Apr 2021 09:22:16 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Hazra", "Rishi", ""], ["Dutta", "Parag", ""], ["Gupta", "Shubham", ""], ["Qaathir", "Mohammed Abdul", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1911.00262", "submitter": "Marko Mihajlovic", "authors": "Marko Mihajlovic, Ning Xiong", "title": "Finding the most similar textual documents using Case-Based Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, huge amounts of unstructured textual data on the Internet\nare a big difficulty for AI algorithms to provide the best recommendations for\nusers and their search queries. Since the Internet became widespread, a lot of\nresearch has been done in the field of Natural Language Processing (NLP) and\nmachine learning. Almost every solution transforms documents into Vector Space\nModels (VSM) in order to apply AI algorithms over them. One such approach is\nbased on Case-Based Reasoning (CBR). Therefore, the most important part of\nthose systems is to compute the similarity between numerical data points. In\n2016, the new similarity TS-SS metric is proposed, which showed\nstate-of-the-art results in the field of textual mining for unsupervised\nlearning. However, no one before has investigated its performances for\nsupervised learning (classification task). In this work, we devised a CBR\nsystem capable of finding the most similar documents for a given query aiming\nto investigate performances of the new state-of-the-art metric, TS-SS, in\naddition to the two other geometrical similarity measures --- Euclidean\ndistance and Cosine similarity --- that showed the best predictive results over\nseveral benchmark corpora. The results show surprising inappropriateness of\nTS-SS measure for high dimensional features.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 08:46:35 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Mihajlovic", "Marko", ""], ["Xiong", "Ning", ""]]}, {"id": "1911.00359", "submitter": "Marie-Anne Lachaux", "authors": "Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav\n  Chaudhary, Francisco Guzm\\'an, Armand Joulin, Edouard Grave", "title": "CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training text representations have led to significant improvements in\nmany areas of natural language processing. The quality of these models benefits\ngreatly from the size of the pretraining corpora as long as its quality is\npreserved. In this paper, we describe an automatic pipeline to extract massive\nhigh-quality monolingual datasets from Common Crawl for a variety of languages.\nOur pipeline follows the data processing introduced in fastText (Mikolov et\nal., 2017; Grave et al., 2018), that deduplicates documents and identifies\ntheir language. We augment this pipeline with a filtering step to select\ndocuments that are close to high quality corpora like Wikipedia.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 13:09:28 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 00:03:54 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Wenzek", "Guillaume", ""], ["Lachaux", "Marie-Anne", ""], ["Conneau", "Alexis", ""], ["Chaudhary", "Vishrav", ""], ["Guzm\u00e1n", "Francisco", ""], ["Joulin", "Armand", ""], ["Grave", "Edouard", ""]]}, {"id": "1911.00558", "submitter": "Lingling Yang", "authors": "Lingling Yang, Dongyang Li, Yao Lu", "title": "Prediction Modeling and Analysis for Telecom Customer Churn in Two\n  Months", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A practical churn customer prediction model is critical to retain customers\nfor telecom companies in the saturated and competitive market. Previous studies\nfocus on predicting churn customers in current or next month, in which telecom\ncompanies don't have enough time to develop and carry out churn management\nstrategies. In this paper, we propose a new T+2 churn customer prediction\nmodel, in which the churn customers in two months are recognized and the\none-month window T+1 is reserved to carry out churn management strategies.\nHowever, the predictions for churn customers in two months are much more\ndifficult than in current or next month because of the weaker correlation\nbetween the customer information and churn states. Two characteristics of\ntelecom dataset, the discrimination between churn and non-churn customers is\ncomplicated and the class imbalance problem is serious, are observed. To\ndiscriminate the churn customers accurately, random forest (RF) classifier is\nchosen because RF solves the nonlinear separable problem with low bias and low\nvariance and handles high feature spaces and large number of training examples.\nTo overcome the imbalance problem, synthetic minority over-sampling with\nborderline or tomek link, in which the distribution of the samples remains and\nthe number of the training examples becomes larger, is applied. Overall, a\nprecision ratio of about 50% with a recall ratio of about 50% is achieved in\nthe T+2 churn prediction. The proposed prediction model provides an accurate\nand operable churn customer prediction model for telecom companies.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 19:15:07 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Yang", "Lingling", ""], ["Li", "Dongyang", ""], ["Lu", "Yao", ""]]}, {"id": "1911.00760", "submitter": "Sendong Zhao", "authors": "Sendong Zhao, Chang Su, Andrea Sboner, Fei Wang", "title": "GRAPHENE: A Precise Biomedical Literature Retrieval Engine with Graph\n  Augmented Deep Learning and External Knowledge Empowerment", "comments": "CIKM 2019", "journal-ref": null, "doi": "10.1145/3357384.3358038", "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective biomedical literature retrieval (BLR) plays a central role in\nprecision medicine informatics. In this paper, we propose GRAPHENE, which is a\ndeep learning based framework for precise BLR. GRAPHENE consists of three main\ndifferent modules 1) graph-augmented document representation learning; 2) query\nexpansion and representation learning and 3) learning to rank biomedical\narticles. The graph-augmented document representation learning module\nconstructs a document-concept graph containing biomedical concept nodes and\ndocument nodes so that global biomedical related concept from external\nknowledge source can be captured, which is further connected to a BiLSTM so\nboth local and global topics can be explored. Query expansion and\nrepresentation learning module expands the query with abbreviations and\ndifferent names, and then builds a CNN-based model to convolve the expanded\nquery and obtain a vector representation for each query. Learning to rank\nminimizes a ranking loss between biomedical articles with the query to learn\nthe retrieval function. Experimental results on applying our system to TREC\nPrecision Medicine track data are provided to demonstrate its effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 18:05:20 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 20:39:33 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Zhao", "Sendong", ""], ["Su", "Chang", ""], ["Sboner", "Andrea", ""], ["Wang", "Fei", ""]]}, {"id": "1911.00852", "submitter": "Masoud Mansoury", "authors": "Masoud Mansoury, Himan Abdollahpouri, Joris Rombouts, Mykola\n  Pechenizkiy", "title": "The Relationship between the Consistency of Users' Ratings and\n  Recommendation Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness in recommender systems has recently received attention from\nresearchers. Unfair recommendations have negative impact on the effectiveness\nof recommender systems as it may degrade users' satisfaction, loyalty, and at\nworst, it can lead to or perpetuate undesirable social dynamics. One of the\nfactors that may impact fairness is calibration, the degree to which users'\npreferences on various item categories are reflected in the recommendations\nthey receive.\n  The ability of a recommendation algorithm for generating effective\nrecommendations may depend on the meaningfulness of the input data and the\namount of information available in users' profile. In this paper, we aim to\nexplore the relationship between the consistency of users' ratings behavior and\nthe degree of calibrated recommendations they receive. We conduct our analysis\non different groups of users based on the consistency of their ratings. Our\nexperimental results on a movie dataset and several recommendation algorithms\nshow that there is a positive correlation between the consistency of users'\nratings behavior and the degree of calibration in their recommendations,\nmeaning that user groups with higher inconsistency in their ratings receive\nless calibrated recommendations.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 08:10:33 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Mansoury", "Masoud", ""], ["Abdollahpouri", "Himan", ""], ["Rombouts", "Joris", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "1911.00886", "submitter": "Yikai Wang", "authors": "Yikai Wang, Liang Zhang, Quanyu Dai, Fuchun Sun, Bo Zhang, Yang He,\n  Weipeng Yan, Yongjun Bao", "title": "Regularized Adversarial Sampling and Deep Time-aware Attention for\n  Click-Through Rate Prediction", "comments": "CIKM 2019 Long Paper, 10 pages", "journal-ref": null, "doi": "10.1145/3357384.3357936", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving the performance of click-through rate (CTR) prediction remains one\nof the core tasks in online advertising systems. With the rise of deep\nlearning, CTR prediction models with deep networks remarkably enhance model\ncapacities. In deep CTR models, exploiting users' historical data is essential\nfor learning users' behaviors and interests. As existing CTR prediction works\nneglect the importance of the temporal signals when embed users' historical\nclicking records, we propose a time-aware attention model which explicitly uses\nabsolute temporal signals for expressing the users' periodic behaviors and\nrelative temporal signals for expressing the temporal relation between items.\nBesides, we propose a regularized adversarial sampling strategy for negative\nsampling which eases the classification imbalance of CTR data and can make use\nof the strong guidance provided by the observed negative CTR samples. The\nadversarial sampling strategy significantly improves the training efficiency,\nand can be co-trained with the time-aware attention model seamlessly.\nExperiments are conducted on real-world CTR datasets from both in-station and\nout-station advertising places.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 13:40:57 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Wang", "Yikai", ""], ["Zhang", "Liang", ""], ["Dai", "Quanyu", ""], ["Sun", "Fuchun", ""], ["Zhang", "Bo", ""], ["He", "Yang", ""], ["Yan", "Weipeng", ""], ["Bao", "Yongjun", ""]]}, {"id": "1911.00936", "submitter": "Daeryong Kim", "authors": "Daeryong Kim and Bongwon Suh", "title": "Enhancing VAEs for Collaborative Filtering: Flexible Priors & Gating\n  Mechanisms", "comments": null, "journal-ref": "In Thirteenth ACM Conference on Recommender Systems (RecSys '19),\n  September 16-20, 2019, Copenhagen, Denmark. ACM, New York, NY, USA, 5 pages", "doi": "10.1145/3298689.3347015", "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network based models for collaborative filtering have started to gain\nattention recently. One branch of research is based on using deep generative\nmodels to model user preferences where variational autoencoders were shown to\nproduce state-of-the-art results. However, there are some potentially\nproblematic characteristics of the current variational autoencoder for CF. The\nfirst is the too simplistic prior that VAEs incorporate for learning the latent\nrepresentations of user preference. The other is the model's inability to learn\ndeeper representations with more than one hidden layer for each network. Our\ngoal is to incorporate appropriate techniques to mitigate the aforementioned\nproblems of variational autoencoder CF and further improve the recommendation\nperformance. Our work is the first to apply flexible priors to collaborative\nfiltering and show that simple priors (in original VAEs) may be too restrictive\nto fully model user preferences and setting a more flexible prior gives\nsignificant gains. We experiment with the VampPrior, originally proposed for\nimage generation, to examine the effect of flexible priors in CF. We also show\nthat VampPriors coupled with gating mechanisms outperform SOTA results\nincluding the Variational Autoencoder for Collaborative Filtering by meaningful\nmargins on 2 popular benchmark datasets (MovieLens & Netflix).\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 17:42:57 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Kim", "Daeryong", ""], ["Suh", "Bongwon", ""]]}, {"id": "1911.00964", "submitter": "Tolgahan Cakaloglu", "authors": "Tolgahan Cakaloglu, Xiaowei Xu", "title": "MRNN: A Multi-Resolution Neural Network with Duplex Attention for\n  Document Retrieval in the Context of Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The primary goal of ad-hoc retrieval (document retrieval in the context of\nquestion answering) is to find relevant documents satisfied the information\nneed posted in a natural language query. It requires a good understanding of\nthe query and all the documents in a corpus, which is difficult because the\nmeaning of natural language texts depends on the context, syntax,and semantics.\nRecently deep neural networks have been used to rank search results in response\nto a query. In this paper, we devise a multi-resolution neural network(MRNN) to\nleverage the whole hierarchy of representations for document retrieval. The\nproposed MRNN model is capable of deriving a representation that integrates\nrepresentations of different levels of abstraction from all the layers of the\nlearned hierarchical representation.Moreover, a duplex attention component is\ndesigned to refinethe multi-resolution representation so that an optimal\ncontextfor matching the query and document can be determined. More\nspecifically, the first attention mechanism determines optimal context from the\nlearned multi-resolution representation for the query and document. The latter\nattention mechanism aims to fine-tune the representation so that the query and\nthe relevant document are closer in proximity. The empirical study shows that\nMRNN with the duplex attention is significantly superior to existing models\nused for ad-hoc retrieval on benchmark datasets including SQuAD, WikiQA,\nQUASAR, and TrecQA.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 20:38:38 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Cakaloglu", "Tolgahan", ""], ["Xu", "Xiaowei", ""]]}, {"id": "1911.00985", "submitter": "Karol Chlasta", "authors": "Karol Chlasta", "title": "Sentiment analysis model for Twitter data in Polish language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text mining analysis of tweets gathered during Polish presidential election\non May 10th, 2015. The project included implementation of engine to retrieve\ninformation from Twitter, building document corpora, corpora cleaning, and\ncreating Term-Document Matrix. Each tweet from the text corpora was assigned a\ncategory based on its sentiment score. The score was calculated using the\nnumber of positive and/or negative emoticons and Polish words in each document.\nThe result data set was used to train and test four machine learning\nclassifiers, to select these providing most accurate automatic tweet\nclassification results. The Naive Bayes and Maximum Entropy algorithms achieved\nthe best accuracy of respectively 71.76% and 77.32%. All implementation tasks\nwere completed using R programming language.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 22:06:03 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Chlasta", "Karol", ""]]}, {"id": "1911.01026", "submitter": "Jeremy Wohlwend", "authors": "Jeremy Wohlwend, Ethan R. Elenberg, Samuel Altschul, Shawn Henry, Tao\n  Lei", "title": "Metric Learning for Dynamic Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional text classifiers are limited to predicting over a fixed set of\nlabels. However, in many real-world applications the label set is frequently\nchanging. For example, in intent classification, new intents may be added over\ntime while others are removed. We propose to address the problem of dynamic\ntext classification by replacing the traditional, fixed-size output layer with\na learned, semantically meaningful metric space. Here the distances between\ntextual inputs are optimized to perform nearest-neighbor classification across\noverlapping label sets. Changing the label set does not involve removing\nparameters, but rather simply adding or removing support points in the metric\nspace. Then the learned metric can be fine-tuned with only a few additional\ntraining examples. We demonstrate that this simple strategy is robust to\nchanges in the label space. Furthermore, our results show that learning a\nnon-Euclidean metric can improve performance in the low data regime, suggesting\nthat further work on metric spaces may benefit low-resource research.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 04:27:29 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Wohlwend", "Jeremy", ""], ["Elenberg", "Ethan R.", ""], ["Altschul", "Samuel", ""], ["Henry", "Shawn", ""], ["Lei", "Tao", ""]]}, {"id": "1911.01097", "submitter": "Auriol Degbelo", "authors": "Auriol Degbelo and Brhane Bahrishum Teka", "title": "Spatial Search Strategies for Open Government Data: A Systematic\n  Comparison", "comments": "Paper accepted to GIR'19: 13th Workshop on Geographic Information\n  Retrieval (Lyon, France)", "journal-ref": null, "doi": "10.1145/3371140.3371142", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of open government datasets on the Web calls for\nways to enable their efficient access and searching. There is however an\noverall lack of understanding regarding spatial search strategies which would\nperform best in this context. To address this gap, this work has assessed the\nimpact of different spatial search strategies on performance and user relevance\njudgment. We harvested machine-readable spatial datasets and their metadata\nfrom three English-based open government data portals, performed metadata\nenhancement, developed a prototype and performed both a theoretical and\nuser-based evaluation. The results highlight that (i) switching between area of\noverlap and Hausdorff distance for spatial similarity computation does not have\nany substantial impact on performance; and (ii) the use of Hausdorff distance\ninduces slightly better user relevance ratings than the use of area of overlap.\nThe data collected and the insights gleaned may serve as a baseline against\nwhich future work can compare.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 09:56:45 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Degbelo", "Auriol", ""], ["Teka", "Brhane Bahrishum", ""]]}, {"id": "1911.01256", "submitter": "Arijit Das", "authors": "Arijit Das and Diganta Saha", "title": "A Novel Approach to Enhance the Performance of Semantic Search in\n  Bengali using Neural Net and other Classification Techniques", "comments": "12 pages, 5 figures", "journal-ref": "IJEAT Vol 9 Issue 3 year 2020 ISSN 2249-8958", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search has for a long time been an important tool for users to retrieve\ninformation. Syntactic search is matching documents or objects containing\nspecific keywords like user-history, location, preference etc. to improve the\nresults. However, it is often possible that the query and the best answer have\nno term or very less number of terms in common and syntactic search can not\nperform properly in such cases. Semantic search, on the other hand, resolves\nthese issues but suffers from lack of annotation, absence of WordNet in case of\nlow resource languages. In this work, we have demonstrated an end to end\nprocedure to improve the performance of semantic search using semi-supervised\nand unsupervised learning algorithms. An available Bengali repository was\nchosen to have seven types of semantic properties primarily to develop the\nsystem. Performance has been tested using Support Vector Machine, Naive Bayes,\nDecision Tree and Artificial Neural Network (ANN). Our system has achieved the\nefficiency to predict the correct semantics using knowledge base over the time\nof learning. A repository containing around a million sentences, a product of\nTDIL project of Govt. of India, was used to test our system at first instance.\nThen the testing has been done for other languages. Being a cognitive system it\nmay be very useful for improving user satisfaction in e-Governance or\nm-Governance in the multilingual environment and also for other applications.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 14:47:33 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 18:22:01 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Das", "Arijit", ""], ["Saha", "Diganta", ""]]}, {"id": "1911.01270", "submitter": "Rabah Tighilt Ferhat", "authors": "Amal Ait Brahim, Rabah Tighilt Ferhat, Gilles Zurfluh", "title": "Incremental extraction of a NoSQL database model using an MDA-based\n  process", "comments": null, "journal-ref": "Publication of the 2019 World Congress in Computer Science,\n  Computer Engineering, and Applied Computing (CSCE 19 ) July 29 - August 01 ,\n  2019 | Las Vegas, Nevada, USA", "doi": null, "report-no": "ISBN: 1-60132-505-3", "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the need to use NoSQL systems to store and exploit big data\nhas been steadily increasing. Most of these systems are characterized by the\nproperty \"schema less\" which means absence of the data model when creating a\ndatabase. This property brings an undeniable flexibility by allowing the\nevolution of the model during the exploitation of the base. However, the\nexpression of queries requires a precise knowledge of this model. In this\npaper, we propose an incremental process to extract the model while operating\nthe document-oriented NoSQL database. To do this, we use the Model Driven\nArchitecture (MDA) that provides a formal framework for automatic model\ntransformation. From the insert, delete and update queries executed on the\ndatabase, we propose formal transformation rules with QVT to generate the\nphysical model of the NoSQL database. An experimentation of the extraction\nprocess was performed on a medical application.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 15:07:28 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Brahim", "Amal Ait", ""], ["Ferhat", "Rabah Tighilt", ""], ["Zurfluh", "Gilles", ""]]}, {"id": "1911.01556", "submitter": "Qian Yu", "authors": "Qian Yu, Lidong Bing, Qiong Zhang, Wai Lam, Luo Si", "title": "Review-based Question Generation with Adaptive Instance Transfer and\n  Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews provide rich information about products and service, while it\nremains inefficient for potential consumers to exploit the reviews for\nfulfilling their specific information need. We propose to explore question\ngeneration as a new way of exploiting review information. One major challenge\nof this task is the lack of review-question pairs for training a neural\ngeneration model. We propose an iterative learning framework for handling this\nchallenge via adaptive transfer and augmentation of the training instances with\nthe help of the available user-posed question-answer data. To capture the\naspect characteristics in reviews, the augmentation and generation procedures\nincorporate related features extracted via unsupervised learning. Experiments\non data from 10 categories of a popular E-commerce site demonstrate the\neffectiveness of the framework, as well as the usefulness of the new task.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 01:19:50 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 04:11:31 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Yu", "Qian", ""], ["Bing", "Lidong", ""], ["Zhang", "Qiong", ""], ["Lam", "Wai", ""], ["Si", "Luo", ""]]}, {"id": "1911.01565", "submitter": "Yadan Luo", "authors": "Zijian Wang, Zheng Zhang, Yadan Luo, Zi Huang", "title": "Deep Collaborative Discrete Hashing with Semantic-Invariant Structure", "comments": null, "journal-ref": "SIGIR 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing deep hashing approaches fail to fully explore semantic correlations\nand neglect the effect of linguistic context on visual attention learning,\nleading to inferior performance. This paper proposes a dual-stream learning\nframework, dubbed Deep Collaborative Discrete Hashing (DCDH), which constructs\na discriminative common discrete space by collaboratively incorporating the\nshared and individual semantics deduced from visual features and semantic\nlabels. Specifically, the context-aware representations are generated by\nemploying the outer product of visual embeddings and semantic encodings.\nMoreover, we reconstruct the labels and introduce the focal loss to take\nadvantage of frequent and rare concepts. The common binary code space is built\non the joint learning of the visual representations attended by language, the\nsemantic-invariant structure construction and the label distribution\ncorrection. Extensive experiments demonstrate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 01:46:52 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Wang", "Zijian", ""], ["Zhang", "Zheng", ""], ["Luo", "Yadan", ""], ["Huang", "Zi", ""]]}, {"id": "1911.01600", "submitter": "Hamada Nayel", "authors": "Hamada A. Nayel and Shashrekha H. L", "title": "Integrating Dictionary Feature into A Deep Learning Model for Disease\n  Named Entity Recognition", "comments": "16 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, Deep Learning (DL) models are becoming important due to\ntheir demonstrated success at overcoming complex learning problems. DL models\nhave been applied effectively for different Natural Language Processing (NLP)\ntasks such as part-of-Speech (PoS) tagging and Machine Translation (MT).\nDisease Named Entity Recognition (Disease-NER) is a crucial task which aims at\nextracting disease Named Entities (NEs) from text. In this paper, a DL model\nfor Disease-NER using dictionary information is proposed and evaluated on\nNational Center for Biotechnology Information (NCBI) disease corpus and BC5CDR\ndataset. Word embeddings trained over general domain texts as well as\nbiomedical texts have been used to represent input to the proposed model. This\nstudy also compares two different Segment Representation (SR) schemes, namely\nIOB2 and IOBES for Disease-NER. The results illustrate that using dictionary\ninformation, pre-trained word embeddings, character embeddings and CRF with\nglobal score improves the performance of Disease-NER system.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 03:50:16 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Nayel", "Hamada A.", ""], ["L", "Shashrekha H.", ""]]}, {"id": "1911.01690", "submitter": "Zhuo Wang", "authors": "Zhuo Wang, Runlong Hu, Qian Chen, Pei Gao, and Xiaowei Xu", "title": "ColluEagle: Collusive review spammer detection using Markov random\n  fields", "comments": "16 pages, 12 figures", "journal-ref": "Data mining and knowledge discovery, 2020, 34(6): 1621-1641", "doi": "10.1007/s10618-020-00693-w", "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product reviews are extremely valuable for online shoppers in providing\npurchase decisions. Driven by immense profit incentives, fraudsters\ndeliberately fabricate untruthful reviews to distort the reputation of online\nproducts. As online reviews become more and more important, group spamming,\ni.e., a team of fraudsters working collaboratively to attack a set of target\nproducts, becomes a new fashion. Previous works use review network effects,\ni.e. the relationships among reviewers, reviews, and products, to detect fake\nreviews or review spammers, but ignore time effects, which are critical in\ncharacterizing group spamming. In this paper, we propose a novel Markov random\nfield (MRF)-based method (ColluEagle) to detect collusive review spammers, as\nwell as review spam campaigns, considering both network effects and time\neffects. First we identify co-review pairs, a review phenomenon that happens\nbetween two reviewers who review a common product in a similar way, and then\nmodel reviewers and their co-review pairs as a pairwise-MRF, and use loopy\nbelief propagation to evaluate the suspiciousness of reviewers. We further\ndesign a high quality yet easy-to-compute node prior for ColluEagle, through\nwhich the review spammer groups can also be subsequently identified.\nExperiments show that ColluEagle can not only detect collusive spammers with\nhigh precision, significantly outperforming state-of-the-art baselines ---\nFraudEagle and SpEagle, but also identify highly suspicious review spammer\ncampaigns.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 09:57:36 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Wang", "Zhuo", ""], ["Hu", "Runlong", ""], ["Chen", "Qian", ""], ["Gao", "Pei", ""], ["Xu", "Xiaowei", ""]]}, {"id": "1911.01770", "submitter": "Matthias Fontanellaz", "authors": "Matthias Fontanellaz and Stergios Christodoulidis and Stavroula\n  Mougiakakou", "title": "Self-Attention and Ingredient-Attention Based Model for Recipe Retrieval\n  from Image Queries", "comments": "MADiMa 2019,5th International Workshop on Multimedia Assisted Dietary\n  Management", "journal-ref": null, "doi": "10.1145/3347448.3357163", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct computer vision based-nutrient content estimation is a demanding task,\ndue to deformation and occlusions of ingredients, as well as high intra-class\nand low inter-class variability between meal classes. In order to tackle these\nissues, we propose a system for recipe retrieval from images. The recipe\ninformation can subsequently be used to estimate the nutrient content of the\nmeal. In this study, we utilize the multi-modal Recipe1M dataset, which\ncontains over 1 million recipes accompanied by over 13 million images. The\nproposed model can operate as a first step in an automatic pipeline for the\nestimation of nutrition content by supporting hints related to ingredient and\ninstruction. Through self-attention, our model can directly process raw recipe\ntext, making the upstream instruction sentence embedding process redundant and\nthus reducing training time, while providing desirable retrieval results.\nFurthermore, we propose the use of an ingredient attention mechanism, in order\nto gain insight into which instructions, parts of instructions or single\ninstruction words are of importance for processing a single ingredient within a\ncertain recipe. Attention-based recipe text encoding contributes to solving the\nissue of high intra-class/low inter-class variability by focusing on\npreparation steps specific to the meal. The experimental results demonstrate\nthe potential of such a system for recipe retrieval from images. A comparison\nwith respect to two baseline methods is also presented.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 13:42:30 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Fontanellaz", "Matthias", ""], ["Christodoulidis", "Stergios", ""], ["Mougiakakou", "Stavroula", ""]]}, {"id": "1911.01874", "submitter": "Abel Dasylva Dr.", "authors": "Abel Dasylva, Arthur Goussanou, David Ajavon and Hanan Abousaleh", "title": "Revisiting the probabilistic method of record linkage", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DB cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In theory, the probabilistic linkage method provides two distinct advantages\nover non-probabilistic methods, including minimal rates of linkage error and\naccurate measures of these rates for data users. However, implementations can\nfall short of these expectations either because the conditional independence\nassumption is made, or because a model with interactions is used but lacks the\nidentification property. In official statistics, this is currently the main\nchallenge to the automated production and use of linked data. To address this\nchallenge, a new methodology is described for proper linkage problems, where\nmatched records may be identified with a probability that is bounded away from\nzero, regardless of the population size. It models the number of neighbours of\na given record, i.e. the number of resembling records. To be specific, the\nproposed model is a finite mixture where each component is the sum of a\nBernoulli variable with an independent Poisson variable. It has the\nidentification property and yields solutions for many longstanding problems,\nincluding the evaluation of blocking criteria and the estimation of linkage\nerrors for probabilistic or non-probabilistic linkages, all without clerical\nreviews or conditional independence assumptions. Thus it also enables\nunsupervised machine learning solutions for record linkage problems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 15:28:46 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Dasylva", "Abel", ""], ["Goussanou", "Arthur", ""], ["Ajavon", "David", ""], ["Abousaleh", "Hanan", ""]]}, {"id": "1911.02079", "submitter": "Hui Guan", "authors": "Hui Guan, Andrey Malevich, Jiyan Yang, Jongsoo Park, Hector Yuen", "title": "Post-Training 4-bit Quantization on Embedding Tables", "comments": "Accepted in MLSys@NeurIPS'19 (http://learningsys.org/neurips19/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous representations have been widely adopted in recommender systems\nwhere a large number of entities are represented using embedding vectors. As\nthe cardinality of the entities increases, the embedding components can easily\ncontain millions of parameters and become the bottleneck in both storage and\ninference due to large memory consumption. This work focuses on post-training\n4-bit quantization on the continuous embeddings. We propose row-wise uniform\nquantization with greedy search and codebook-based quantization that\nconsistently outperforms state-of-the-art quantization approaches on reducing\naccuracy degradation. We deploy our uniform quantization technique on a\nproduction model in Facebook and demonstrate that it can reduce the model size\nto only 13.89% of the single-precision version while the model quality stays\nneutral.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 20:43:51 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Guan", "Hui", ""], ["Malevich", "Andrey", ""], ["Yang", "Jiyan", ""], ["Park", "Jongsoo", ""], ["Yuen", "Hector", ""]]}, {"id": "1911.02174", "submitter": "Ahmed Elmisery", "authors": "Ahmed M. Elmisery, Mirela Sertovic", "title": "Privacy Preserving Threat Hunting in Smart Home Environments", "comments": "In Proc. the International Conference on Advances in Cyber Security,\n  Penang, Malaysia, July 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent proliferation of smart home environments offers new and\ntransformative circumstances for various domains with a commitment to enhancing\nthe quality of life and experience. Most of these environments combine\ndifferent gadgets offered by multiple stakeholders in a dynamic and\ndecentralized manner, which in turn presents new challenges from the\nperspective of digital investigation. In addition, a plentiful amount of data\nrecords got generated because of the day to day interactions between these\ngadgets and homeowners, which poses difficulty in managing and analyzing such\ndata. The analysts should endorse new digital investigation approaches to\ntackle the current limitations in traditional approaches when used in these\nenvironments. The digital evidence in such environments can be found inside the\nrecords of logfiles that store the historical events occurred inside the smart\nhome. Threat hunting can leverage the collective nature of these gadgets to\ngain deeper insights into the best way for responding to new threats, which in\nturn can be valuable in reducing the impact of breaches. Nevertheless, this\napproach depends mainly on the readiness of smart homeowners to share their own\npersonal usage logs that have been extracted from their smart home\nenvironments. However, they might disincline to employ such service due to the\nsensitive nature of the information logged by their personal gateways. In this\npaper, we presented an approach to enable smart homeowners to share their usage\nlogs in a privacy preserving manner. A distributed threat hunting approach has\nbeen developed to permit the composition of diverse threat classes without\nrevealing the logged records to other involved parties. Furthermore, a scenario\nwas proposed to depict a proactive threat Intelligence sharing for the\ndetection of potential threats in smart home environments with some\nexperimental results.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 02:58:32 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 12:16:35 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Elmisery", "Ahmed M.", ""], ["Sertovic", "Mirela", ""]]}, {"id": "1911.02247", "submitter": "Arthur Bra\\v{z}inskas", "authors": "Arthur Bra\\v{z}inskas, Mirella Lapata and Ivan Titov", "title": "Unsupervised Opinion Summarization as Copycat-Review Generation", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion summarization is the task of automatically creating summaries that\nreflect subjective information expressed in multiple documents, such as product\nreviews. While the majority of previous work has focused on the extractive\nsetting, i.e., selecting fragments from input reviews to produce a summary, we\nlet the model generate novel sentences and hence produce abstractive summaries.\nRecent progress in summarization has seen the development of supervised models\nwhich rely on large quantities of document-summary pairs. Since such training\ndata is expensive to acquire, we instead consider the unsupervised setting, in\nother words, we do not use any summaries in training. We define a generative\nmodel for a review collection which capitalizes on the intuition that when\ngenerating a new review given a set of other reviews of a product, we should be\nable to control the \"amount of novelty\" going into the new review or,\nequivalently, vary the extent to which it deviates from the input. At test\ntime, when generating summaries, we force the novelty to be minimal, and\nproduce a text reflecting consensus opinions. We capture this intuition by\ndefining a hierarchical variational autoencoder model. Both individual reviews\nand the products they correspond to are associated with stochastic latent\ncodes, and the review generator (\"decoder\") has direct access to the text of\ninput reviews through the pointer-generator mechanism. Experiments on Amazon\nand Yelp datasets, show that setting at test time the review's latent code to\nits mean, allows the model to produce fluent and coherent summaries reflecting\ncommon opinions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 08:20:13 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 15:49:31 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Bra\u017einskas", "Arthur", ""], ["Lapata", "Mirella", ""], ["Titov", "Ivan", ""]]}, {"id": "1911.02248", "submitter": "Fan Wang", "authors": "Fan Wang, Xiaomin Fang, Lihang Liu, Hao Tian, Zhiming Peng", "title": "MBCAL: Sample Efficient and Variance Reduced Reinforcement Learning for\n  Recommender Systems", "comments": null, "journal-ref": "1st International Workshop on Industrial Recommendation Systems\n  (IRS), KDD 2020", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recommender systems such as news feed stream, it is essential to optimize\nthe long-term utilities in the continuous user-system interaction processes.\nPrevious works have proved the capability of reinforcement learning in this\nproblem. However, there are many practical challenges to implement deep\nreinforcement learning in online systems, including low sample efficiency,\nuncontrollable risks, and excessive variances. To address these issues, we\npropose a novel reinforcement learning method, namely model-based\ncounterfactual advantage learning (MBCAL). The proposed method takes advantage\nof the characteristics of recommender systems and draws ideas from the\nmodel-based reinforcement learning method for higher sample efficiency. It has\ntwo components: an environment model that predicts the instant user behavior\none-by-one in an auto-regressive form, and a future advantage model that\npredicts the future utility. To alleviate the impact of excessive variance when\nlearning the future advantage model, we employ counterfactual comparisons\nderived from the environment model. In consequence, the proposed method\npossesses high sample efficiency and significantly lower variance; Also, it is\nable to use existing user logs to avoid the risks of starting from scratch. In\ncontrast to its capability, its implementation cost is relatively low, which\nfits well with practical systems. Theoretical analysis and elaborate\nexperiments are presented. Results show that the proposed method transcends the\nother supervised learning and RL-based methods in both sample efficiency and\nasymptotic performances.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 08:26:32 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 04:45:44 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Wang", "Fan", ""], ["Fang", "Xiaomin", ""], ["Liu", "Lihang", ""], ["Tian", "Hao", ""], ["Peng", "Zhiming", ""]]}, {"id": "1911.02671", "submitter": "Li Xiong", "authors": "Lee Xiong, Chuan Hu, Chenyan Xiong, Daniel Campos, Arnold Overwijk", "title": "Open Domain Web Keyphrase Extraction Beyond Language Modeling", "comments": null, "journal-ref": "EMNLP-IJCNLP 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies keyphrase extraction in real-world scenarios where\ndocuments are from diverse domains and have variant content quality. We curate\nand release OpenKP, a large scale open domain keyphrase extraction dataset with\nnear one hundred thousand web documents and expert keyphrase annotations. To\nhandle the variations of domain and content quality, we develop BLING-KPE, a\nneural keyphrase extraction model that goes beyond language understanding using\nvisual presentations of documents and weak supervision from search queries.\nExperimental results on OpenKP confirm the effectiveness of BLING-KPE and the\ncontributions of its neural architecture, visual features, and search log weak\nsupervision. Zero-shot evaluations on DUC-2001 demonstrate the improved\ngeneralization ability of learning from the open domain data compared to a\nspecific domain.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 23:12:56 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Xiong", "Lee", ""], ["Hu", "Chuan", ""], ["Xiong", "Chenyan", ""], ["Campos", "Daniel", ""], ["Overwijk", "Arnold", ""]]}, {"id": "1911.02733", "submitter": "Xue Mengge", "authors": "Xue Mengge, Yu Bowen, Liu Tingwen, Zhang Yue, Meng Erli, Wang Bin", "title": "Porous Lattice-based Transformer Encoder for Chinese NER", "comments": "9 pages, 4 figures", "journal-ref": "COLING 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating lattices into character-level Chinese named entity recognition\nis an effective method to exploit explicit word information. Recent works\nextend recurrent and convolutional neural networks to model lattice inputs.\nHowever, due to the DAG structure or the variable-sized potential word set for\nlattice inputs, these models prevent the convenient use of batched computation,\nresulting in serious inefficient. In this paper, we propose a porous\nlattice-based transformer encoder for Chinese named entity recognition, which\nis capable to better exploit the GPU parallelism and batch the computation\nowing to the mask mechanism in transformer. We first investigate the\nlattice-aware self-attention coupled with relative position representations to\nexplore effective word information in the lattice structure. Besides, to\nstrengthen the local dependencies among neighboring tokens, we propose a novel\nporous structure during self-attentional computation processing, in which every\ntwo non-neighboring tokens are connected through a shared pivot node.\nExperimental results on four datasets show that our model performs up to 9.47\ntimes faster than state-of-the-art models, while is roughly on a par with its\nperformance. The source code of this paper can be obtained from\nhttps://github.com/xxx/xxx.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 02:58:17 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 14:46:51 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 12:52:24 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Mengge", "Xue", ""], ["Bowen", "Yu", ""], ["Tingwen", "Liu", ""], ["Yue", "Zhang", ""], ["Erli", "Meng", ""], ["Bin", "Wang", ""]]}, {"id": "1911.02747", "submitter": "Zhenxin Fu", "authors": "Zhenxin Fu, Feng Ji, Wenpeng Hu, Wei Zhou, Dongyan Zhao, Haiqing Chen,\n  Rui Yan", "title": "Query-bag Matching with Mutual Coverage for Information-seeking\n  Conversations in E-commerce", "comments": "CIKM 2019 Short", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information-seeking conversation system aims at satisfying the information\nneeds of users through conversations. Text matching between a user query and a\npre-collected question is an important part of the information-seeking\nconversation in E-commerce. In the practical scenario, a sort of questions\nalways correspond to a same answer. Naturally, these questions can form a bag.\nLearning the matching between user query and bag directly may improve the\nconversation performance, denoted as query-bag matching. Inspired by such\nopinion, we propose a query-bag matching model which mainly utilizes the mutual\ncoverage between query and bag and measures the degree of the content in the\nquery mentioned by the bag, and vice verse. In addition, the learned bag\nrepresentation in word level helps find the main points of a bag in a fine\ngrade and promotes the query-bag matching performance. Experiments on two\ndatasets show the effectiveness of our model.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 04:11:03 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Fu", "Zhenxin", ""], ["Ji", "Feng", ""], ["Hu", "Wenpeng", ""], ["Zhou", "Wei", ""], ["Zhao", "Dongyan", ""], ["Chen", "Haiqing", ""], ["Yan", "Rui", ""]]}, {"id": "1911.02752", "submitter": "Rocky Chen", "authors": "Tong Chen, Hongzhi Yin, Quoc Viet Hung Nguyen, Wen-Chih Peng, Xue Li,\n  Xiaofang Zhou", "title": "Sequence-Aware Factorization Machines for Temporal Predictive Analytics", "comments": "To appear in ICDE'20, Dallas, Texas, USA. Code link updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various web applications like targeted advertising and recommender\nsystems, the available categorical features (e.g., product type) are often of\ngreat importance but sparse. As a widely adopted solution, models based on\nFactorization Machines (FMs) are capable of modelling high-order interactions\namong features for effective sparse predictive analytics. As the volume of\nweb-scale data grows exponentially over time, sparse predictive analytics\ninevitably involves dynamic and sequential features. However, existing FM-based\nmodels assume no temporal orders in the data, and are unable to capture the\nsequential dependencies or patterns within the dynamic features, impeding the\nperformance and adaptivity of these methods. Hence, in this paper, we propose a\nnovel Sequence-Aware Factorization Machine (SeqFM) for temporal predictive\nanalytics, which models feature interactions by fully investigating the effect\nof sequential dependencies. As static features (e.g., user gender) and dynamic\nfeatures (e.g., user interacted items) express different semantics, we\ninnovatively devise a multi-view self-attention scheme that separately models\nthe effect of static features, dynamic features and the mutual interactions\nbetween static and dynamic features in three different views. In SeqFM, we\nfurther map the learned representations of feature interactions to the desired\noutput with a shared residual network. To showcase the versatility and\ngeneralizability of SeqFM, we test SeqFM in three popular application scenarios\nfor FM-based models, namely ranking, classification and regression tasks.\nExtensive experimental results on six large-scale datasets demonstrate the\nsuperior effectiveness and efficiency of SeqFM.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 04:29:53 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 01:44:19 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Chen", "Tong", ""], ["Yin", "Hongzhi", ""], ["Nguyen", "Quoc Viet Hung", ""], ["Peng", "Wen-Chih", ""], ["Li", "Xue", ""], ["Zhou", "Xiaofang", ""]]}, {"id": "1911.02850", "submitter": "Rishiraj Saha Roy", "authors": "Magdalena Kaiser, Rishiraj Saha Roy, Gerhard Weikum", "title": "CROWN: Conversational Passage Ranking by Reasoning over Word Networks", "comments": "TREC 2019, 14 pages", "journal-ref": "TREC 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information needs around a topic cannot be satisfied in a single turn; users\ntypically ask follow-up questions referring to the same theme and a system must\nbe capable of understanding the conversational context of a request to retrieve\ncorrect answers. In this paper, we present our submission to the TREC\nConversational Assistance Track 2019, in which such a conversational setting is\nexplored. We propose a simple unsupervised method for conversational passage\nranking by formulating the passage score for a query as a combination of\nsimilarity and coherence. To be specific, passages are preferred that contain\nwords semantically similar to the words used in the question, and where such\nwords appear close by. We built a word-proximity network (WPN) from a large\ncorpus, where words are nodes and there is an edge between two nodes if they\nco-occur in the same passages in a statistically significant way, within a\ncontext window. Our approach, named CROWN, improved nDCG scores over a provided\nIndri baseline on the CAsT training data. On the evaluation data for CAsT, our\nbest run submission achieved above-average performance with respect to AP@5 and\nnDCG@1000.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 11:02:21 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 23:54:15 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 10:36:32 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Kaiser", "Magdalena", ""], ["Roy", "Rishiraj Saha", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1911.02896", "submitter": "Jinhyuk Lee", "authors": "Jinhyuk Lee, Minjoon Seo, Hannaneh Hajishirzi, Jaewoo Kang", "title": "Contextualized Sparse Representations for Real-Time Open-Domain Question\n  Answering", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain question answering can be formulated as a phrase retrieval\nproblem, in which we can expect huge scalability and speed benefit but often\nsuffer from low accuracy due to the limitation of existing phrase\nrepresentation models. In this paper, we aim to improve the quality of each\nphrase embedding by augmenting it with a contextualized sparse representation\n(Sparc). Unlike previous sparse vectors that are term-frequency-based (e.g.,\ntf-idf) or directly learned (only few thousand dimensions), we leverage\nrectified self-attention to indirectly learn sparse vectors in n-gram\nvocabulary space. By augmenting the previous phrase retrieval model (Seo et\nal., 2019) with Sparc, we show 4%+ improvement in CuratedTREC and SQuAD-Open.\nOur CuratedTREC score is even better than the best known retrieve & read model\nwith at least 45x faster inference speed.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 13:34:54 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 06:36:08 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Lee", "Jinhyuk", ""], ["Seo", "Minjoon", ""], ["Hajishirzi", "Hannaneh", ""], ["Kang", "Jaewoo", ""]]}, {"id": "1911.02984", "submitter": "Vittorio Castelli", "authors": "Vittorio Castelli, Rishav Chakravarti, Saswati Dana, Anthony Ferritto,\n  Radu Florian, Martin Franz, Dinesh Garg, Dinesh Khandelwal, Scott McCarley,\n  Mike McCawley, Mohamed Nasr, Lin Pan, Cezar Pendus, John Pitrelli, Saurabh\n  Pujar, Salim Roukos, Andrzej Sakrajda, Avirup Sil, Rosario Uceda-Sosa, Todd\n  Ward, Rong Zhang", "title": "The TechQA Dataset", "comments": "Long version of conference paper to be submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce TechQA, a domain-adaptation question answering dataset for the\ntechnical support domain. The TechQA corpus highlights two real-world issues\nfrom the automated customer support domain. First, it contains actual questions\nposed by users on a technical forum, rather than questions generated\nspecifically for a competition or a task. Second, it has a real-world size --\n600 training, 310 dev, and 490 evaluation question/answer pairs -- thus\nreflecting the cost of creating large labeled datasets with actual data.\nConsequently, TechQA is meant to stimulate research in domain adaptation rather\nthan being a resource to build QA systems from scratch. The dataset was\nobtained by crawling the IBM Developer and IBM DeveloperWorks forums for\nquestions with accepted answers that appear in a published IBM Technote---a\ntechnical document that addresses a specific technical issue. We also release a\ncollection of the 801,998 publicly available Technotes as of April 4, 2019 as a\ncompanion resource that might be used for pretraining, to learn representations\nof the IT domain language.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 02:04:39 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Castelli", "Vittorio", ""], ["Chakravarti", "Rishav", ""], ["Dana", "Saswati", ""], ["Ferritto", "Anthony", ""], ["Florian", "Radu", ""], ["Franz", "Martin", ""], ["Garg", "Dinesh", ""], ["Khandelwal", "Dinesh", ""], ["McCarley", "Scott", ""], ["McCawley", "Mike", ""], ["Nasr", "Mohamed", ""], ["Pan", "Lin", ""], ["Pendus", "Cezar", ""], ["Pitrelli", "John", ""], ["Pujar", "Saurabh", ""], ["Roukos", "Salim", ""], ["Sakrajda", "Andrzej", ""], ["Sil", "Avirup", ""], ["Uceda-Sosa", "Rosario", ""], ["Ward", "Todd", ""], ["Zhang", "Rong", ""]]}, {"id": "1911.02989", "submitter": "Jimmy Lin", "authors": "Peng Shi and Jimmy Lin", "title": "Cross-Lingual Relevance Transfer for Document Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown the surprising ability of multi-lingual BERT to serve\nas a zero-shot cross-lingual transfer model for a number of language processing\ntasks. We combine this finding with a similarly-recently proposal on\nsentence-level relevance modeling for document retrieval to demonstrate the\nability of multi-lingual BERT to transfer models of relevance across languages.\nExperiments on test collections in five different languages from diverse\nlanguage families (Chinese, Arabic, French, Hindi, and Bengali) show that\nmodels trained with English data improve ranking quality, without any special\nprocessing, both for (non-English) mono-lingual retrieval as well as\ncross-lingual retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 02:19:52 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Shi", "Peng", ""], ["Lin", "Jimmy", ""]]}, {"id": "1911.02991", "submitter": "Joy Bose", "authors": "Joy Bose, Sumanta Mukherjee", "title": "Semi-Supervised Method using Gaussian Random Fields for Boilerplate\n  Removal in Web Browsers", "comments": "4 pages, 1 figure, IEEE INDICON conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boilerplate removal refers to the problem of removing noisy content from a\nwebpage such as ads and extracting relevant content that can be used by various\nservices. This can be useful in several features in web browsers such as ad\nblocking, accessibility tools such as read out loud, translation, summarization\netc. In order to create a training dataset to train a model for boilerplate\ndetection and removal, labeling or tagging webpage data manually can be tedious\nand time consuming. Hence, a semi-supervised model, in which some of the\nwebpage elements are labeled manually and labels for others are inferred based\non some parameters, can be useful. In this paper we present a solution for\nextraction of relevant content from a webpage that relies on semi-supervised\nlearning using Gaussian Random Fields. We first represent the webpage as a\ngraph, with text elements as nodes and the edge weights representing similarity\nbetween nodes. After this, we label a few nodes in the graph using heuristics\nand label the remaining nodes by a weighted measure of similarity to the\nalready labeled nodes. We describe the system architecture and a few\npreliminary results on a dataset of webpages.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 02:23:33 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Bose", "Joy", ""], ["Mukherjee", "Sumanta", ""]]}, {"id": "1911.03270", "submitter": "Ekaterina Artemova", "authors": "Taisiya Glushkova and Ekaterina Artemova", "title": "Char-RNN and Active Learning for Hashtag Segmentation", "comments": "to appear in Cicling2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We explore the abilities of character recurrent neural network (char-RNN) for\nhashtag segmentation. Our approach to the task is the following: we generate\nsynthetic training dataset according to frequent n-grams that satisfy\npredefined morpho-syntactic patterns to avoid any manual annotation. The active\nlearning strategy limits the training dataset and selects informative training\nsubset. The approach does not require any language-specific settings and is\ncompared for two languages, which differ in inflection degree.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 14:03:55 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Glushkova", "Taisiya", ""], ["Artemova", "Ekaterina", ""]]}, {"id": "1911.03338", "submitter": "Yaroslav Koshka", "authors": "Yaroslav Koshka, M.A. Novotny", "title": "Comparison of D-Wave Quantum Annealing and Classical Simulated Annealing\n  for Local Minima Determination", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machines trained with different numbers of iterations\nwere used to provide a diverse set of energy functions each containing many\nlocal valleys (LVs) with different energies, widths, escape barrier heights,\netc. They were used to verify the previously reported possibility of using the\nD-Wave quantum annealer (QA) to find potentially important LVs in the energy\nfunctions of Ising spin glasses that may be missed by classical searches. For\nclassical search, extensive simulated annealing (SA) was conducted to find as\nmany LVs as possible regardless of the computational cost. SA was conducted\nlong enough to ensure that the number of SA-found LVs approaches that and\neventually significantly exceeds the number of the LVs found by a single call\nsubmitted to the D-Wave. Even after a prohibitively long SA search, as many as\n30-50% of the D-Wave-found LVs remained not found by the SA. In order to\nestablish if LVs found only by the D-Wave represent potentially important\nregions of the configuration space, they were compared to those that were found\nby both techniques. While the LVs found by the D-Wave but missed by SA\npredominantly had higher energies and lower escape barriers, there was a\nsignificant fraction having intermediate values of the energy and barrier\nheight. With respect to most other important LV parameters, the LVs found only\nby the D-Wave were distributed in a wide range of the parameters' values. It\nwas established that for large or small, shallow or deep, wide or narrow LVs,\nthe LVs found only by the D-Wave are distinguished by a few-times smaller size\nof the LV basin of attraction (BoA). Apparently, the size of the BoA is not or\nat least is less important for QA search compared to the classical search,\nallowing QA to easily find many potentially important (e.g., wide and deep) LVs\nmissed by even prohibitively lengthy classical searches.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 15:53:51 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Koshka", "Yaroslav", ""], ["Novotny", "M. A.", ""]]}, {"id": "1911.03353", "submitter": "Tan Yan", "authors": "Tan Yan, Heyan Huang, Xian-Ling Mao", "title": "SEPT: Improving Scientific Named Entity Recognition with Span\n  Representation", "comments": "This work is outdated. The result should not be trusted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new scientific named entity recognizer called SEPT, which\nstands for Span Extractor with Pre-trained Transformers. In recent papers, span\nextractors have been demonstrated to be a powerful model compared with sequence\nlabeling models. However, we discover that with the development of pre-trained\nlanguage models, the performance of span extractors appears to become similar\nto sequence labeling models. To keep the advantages of span representation, we\nmodified the model by under-sampling to balance the positive and negative\nsamples and reduce the search space. Furthermore, we simplify the origin\nnetwork architecture to combine the span extractor with BERT. Experiments\ndemonstrate that even simplified architecture achieves the same performance and\nSEPT achieves a new state of the art result in scientific named entity\nrecognition even without relation information involved.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 16:19:26 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 04:25:25 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Yan", "Tan", ""], ["Huang", "Heyan", ""], ["Mao", "Xian-Ling", ""]]}, {"id": "1911.03598", "submitter": "Lili Yu", "authors": "Lili Yu, Howard Chen, Sida Wang, Tao Lei, Yoav Artzi", "title": "Interactive Classification by Asking Informative Questions", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the potential for interaction in natural language classification. We\nadd a limited form of interaction for intent classification, where users\nprovide an initial query using natural language, and the system asks for\nadditional information using binary or multi-choice questions. At each turn,\nour system decides between asking the most informative question or making the\nfinal classification prediction.The simplicity of the model allows for\nbootstrapping of the system without interaction data, instead relying on simple\ncrowdsourcing tasks. We evaluate our approach on two domains, showing the\nbenefit of interaction and the advantage of learning to balance between asking\nadditional questions and making the final prediction.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 03:05:50 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 19:47:51 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Yu", "Lili", ""], ["Chen", "Howard", ""], ["Wang", "Sida", ""], ["Lei", "Tao", ""], ["Artzi", "Yoav", ""]]}, {"id": "1911.03626", "submitter": "Chun Yuan Yuan", "authors": "Qianwen Ma, Chunyuan Yuan, Wei Zhou, Jizhong Han, Songlin Hu", "title": "Beyond Statistical Relations: Integrating Knowledge Relations into Style\n  Correlations for Multi-Label Music Style Classification", "comments": "Accepted as WSDM 2020 Regular Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Automatically labeling multiple styles for every song is a comprehensive\napplication in all kinds of music websites. Recently, some researches explore\nreview-driven multi-label music style classification and exploit style\ncorrelations for this task. However, their methods focus on mining the\nstatistical relations between different music styles and only consider shallow\nstyle relations. Moreover, these statistical relations suffer from the\nunderfitting problem because some music styles have little training data.\n  To tackle these problems, we propose a novel knowledge relations integrated\nframework (KRF) to capture the complete style correlations, which jointly\nexploits the inherent relations between music styles according to external\nknowledge and their statistical relations. Based on the two types of relations,\nwe use a graph convolutional network to learn the deep correlations between\nstyles automatically. Experimental results show that our framework\nsignificantly outperforms state-of-the-art methods. Further studies demonstrate\nthat our framework can effectively alleviate the underfitting problem and learn\nmeaningful style correlations. The source code can be available at\nhttps://github.com/Makwen1995/MusicGenre.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 06:55:39 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 08:30:13 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Ma", "Qianwen", ""], ["Yuan", "Chunyuan", ""], ["Zhou", "Wei", ""], ["Han", "Jizhong", ""], ["Hu", "Songlin", ""]]}, {"id": "1911.03845", "submitter": "Xueying Bai", "authors": "Xueying Bai, Jian Guan, Hongning Wang", "title": "Model-Based Reinforcement Learning with Adversarial Training for Online\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is well suited for optimizing policies of recommender\nsystems. Current solutions mostly focus on model-free approaches, which require\nfrequent interactions with the real environment, and thus are expensive in\nmodel learning. Offline evaluation methods, such as importance sampling, can\nalleviate such limitations, but usually request a large amount of logged data\nand do not work well when the action space is large. In this work, we propose a\nmodel-based reinforcement learning solution which models user-agent interaction\nfor offline policy learning via a generative adversarial network. To reduce\nbias in the learned model and policy, we use a discriminator to evaluate the\nquality of generated data and scale the generated rewards. Our theoretical\nanalysis and empirical evaluations demonstrate the effectiveness of our\nsolution in learning policies from the offline and generated data.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 04:24:04 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 01:40:22 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 20:47:53 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Bai", "Xueying", ""], ["Guan", "Jian", ""], ["Wang", "Hongning", ""]]}, {"id": "1911.03854", "submitter": "Kai Nakamura", "authors": "Kai Nakamura, Sharon Levy, William Yang Wang", "title": "r/Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake\n  News Detection", "comments": "Accepted LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fake news has altered society in negative ways in politics and culture. It\nhas adversely affected both online social network systems as well as offline\ncommunities and conversations. Using automatic machine learning classification\nmodels is an efficient way to combat the widespread dissemination of fake news.\nHowever, a lack of effective, comprehensive datasets has been a problem for\nfake news research and detection model development. Prior fake news datasets do\nnot provide multimodal text and image data, metadata, comment data, and\nfine-grained fake news categorization at the scale and breadth of our dataset.\nWe present Fakeddit, a novel multimodal dataset consisting of over 1 million\nsamples from multiple categories of fake news. After being processed through\nseveral stages of review, the samples are labeled according to 2-way, 3-way,\nand 6-way classification categories through distant supervision. We construct\nhybrid text+image models and perform extensive experiments for multiple\nvariations of classification, demonstrating the importance of the novel aspect\nof multimodality and fine-grained classification unique to Fakeddit.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 05:06:38 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 17:55:57 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Nakamura", "Kai", ""], ["Levy", "Sharon", ""], ["Wang", "William Yang", ""]]}, {"id": "1911.03869", "submitter": "Pratyay Banerjee", "authors": "Pratyay Banerjee, Kuntal Kumar Pal, Murthy Devarakonda, Chitta Baral", "title": "Knowledge Guided Named Entity Recognition for BioMedical Text", "comments": "6 pages, 2 figures, 5 tables, WIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we formulate the NER task as a multi-answer knowledge guided QA\ntask (KGQA) which helps to predict entities only by assigning B, I and O tags\nwithout associating entity types with the tags. We provide different knowledge\ncontexts, such as, entity types, questions, definitions and examples along with\nthe text and train on a combined dataset of 18 biomedical corpora. This\nformulation (a) enables systems to jointly learn NER specific features from\nvaried NER datasets, (b) can use knowledge-text attention to identify words\nhaving higher similarity to provided knowledge, improving performance, (c)\nreduces system confusion by reducing the prediction classes to B, I, O only,\nand (d) makes detection of nested entities easier. We perform extensive\nexperiments of this KGQA formulation on 18 biomedical NER datasets, and through\nexperiments we note that knowledge helps in achieving better performance. Our\nproblem formulation is able to achieve state-of-the-art results in 12 datasets.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 07:05:25 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 03:15:54 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 07:42:01 GMT"}, {"version": "v4", "created": "Fri, 18 Sep 2020 04:38:31 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Banerjee", "Pratyay", ""], ["Pal", "Kuntal Kumar", ""], ["Devarakonda", "Murthy", ""], ["Baral", "Chitta", ""]]}, {"id": "1911.03871", "submitter": "Magda Friedjungov\\'a", "authors": "Petra Kubern\\'atov\\'a, Magda Friedjungov\\'a, Max van Duijn", "title": "Constructing a Data Visualization Recommender System", "comments": "Conference paper, DATA 2018, part of the Communications in Computer\n  and Information Science book series", "journal-ref": "Communications in Computer and Information Science 862 (2019) 1-25", "doi": "10.1007/978-3-030-26636-3_1", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing a suitable visualization for data is a difficult task. Current data\nvisualization recommender systems exist to aid in choosing a visualization, yet\nsuffer from issues such as low accessibility and indecisiveness. In this study,\nwe first define a step-by-step guide on how to build a data visualization\nrecommender system. We then use this guide to create a model for a data\nvisualization recommender system for non-experts that aims to resolve the\nissues of current solutions. The result is a question-based model that uses a\ndecision tree and a data visualization classification hierarchy in order to\nrecommend a visualization. Furthermore, it incorporates both task-driven and\ndata characteristics-driven perspectives, whereas existing solutions seem to\neither convolute these or focus on one of the two exclusively. Based on testing\nagainst existing solutions, it is shown that the new model reaches similar\nresults while being simpler, clearer, more versatile, extendable and\ntransparent. The presented guide can be used as a manual for anyone building a\ndata visualization recommender system. The resulting model can be applied in\nthe development of new data visualization software or as part of a learning\ntool.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 07:24:39 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Kubern\u00e1tov\u00e1", "Petra", ""], ["Friedjungov\u00e1", "Magda", ""], ["van Duijn", "Max", ""]]}, {"id": "1911.03883", "submitter": "Jiarui Qin", "authors": "Jiarui Qin, Kan Ren, Yuchen Fang, Weinan Zhang, Yong Yu", "title": "Sequential Recommendation with Dual Side Neighbor-based Collaborative\n  Relation Modeling", "comments": "WSDM 2020", "journal-ref": null, "doi": "10.1145/3336191.3371842", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential recommendation task aims to predict user preference over items in\nthe future given user historical behaviors. The order of user behaviors implies\nthat there are resourceful sequential patterns embedded in the behavior history\nwhich reveal the underlying dynamics of user interests. Various sequential\nrecommendation methods are proposed to model the dynamic user behaviors.\nHowever, most of the models only consider the user's own behaviors and\ndynamics, while ignoring the collaborative relations among users and items,\ni.e., similar tastes of users or analogous properties of items. Without\nmodeling collaborative relations, those methods suffer from the lack of\nrecommendation diversity and thus may have worse performance. Worse still, most\nexisting methods only consider the user-side sequence and ignore the temporal\ndynamics on the item side. To tackle the problems of the current sequential\nrecommendation models, we propose Sequential Collaborative Recommender (SCoRe)\nwhich effectively mines high-order collaborative information using\ncross-neighbor relation modeling and, additionally utilizes both user-side and\nitem-side historical sequences to better capture user and item dynamics.\nExperiments on three real-world yet large-scale datasets demonstrate the\nsuperiority of the proposed model over strong baselines.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 09:28:20 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Qin", "Jiarui", ""], ["Ren", "Kan", ""], ["Fang", "Yuchen", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""]]}, {"id": "1911.03974", "submitter": "Antonio Busson", "authors": "Pedro V. A. de Freitas, Paulo R. C. Mendes, Gabriel N. P. dos Santos,\n  Antonio Jos\\'e G. Busson, \\'Alan Livio Guedes, S\\'ergio Colcher, Ruy Luiz\n  Milidi\\'u", "title": "A Multimodal CNN-based Tool to Censure Inappropriate Video Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the extensive use of video-sharing platforms and services for their\nstorage, the amount of such media on the internet has become massive. This\nvolume of data makes it difficult to control the kind of content that may be\npresent in such video files. One of the main concerns regarding the video\ncontent is if it has an inappropriate subject matter, such as nudity, violence,\nor other potentially disturbing content. More than telling if a video is either\nappropriate or inappropriate, it is also important to identify which parts of\nit contain such content, for preserving parts that would be discarded in a\nsimple broad analysis. In this work, we present a multimodal~(using audio and\nimage features) architecture based on Convolutional Neural Networks (CNNs) for\ndetecting inappropriate scenes in video files. In the task of classifying video\nfiles, our model achieved 98.95\\% and 98.94\\% of F1-score for the appropriate\nand inappropriate classes, respectively. We also present a censoring tool that\nautomatically censors inappropriate segments of a video file.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 18:26:24 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["de Freitas", "Pedro V. A.", ""], ["Mendes", "Paulo R. C.", ""], ["Santos", "Gabriel N. P. dos", ""], ["Busson", "Antonio Jos\u00e9 G.", ""], ["Guedes", "\u00c1lan Livio", ""], ["Colcher", "S\u00e9rgio", ""], ["Milidi\u00fa", "Ruy Luiz", ""]]}, {"id": "1911.04013", "submitter": "Vishal Anand", "authors": "Vishal Anand, Ravi Shukla, Ashwani Gupta and Abhishek Kumar", "title": "Customized video filtering on YouTube", "comments": "13 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inappropriate and profane content on social media is exponentially increasing\nand big corporations are becoming more aware of the type of content on which\nthey are advertising and how it may affect their brand reputation. But with a\nhuge surge in content being posted online it becomes seemingly difficult to\nfilter out related videos on which they can run their ads without compromising\nbrand name. Advertising on youtube videos generates a huge amount of revenue\nfor corporations. It becomes increasingly important for such corporations to\nadvertise on only the videos that don't hurt the feelings, community or harmony\nof the audience at large. In this paper, we propose a system to identify\ninappropriate content on YouTube and leverage it to perform a first of its\nkind, large scale, quantitative characterization that reveals some of the risks\nof YouTube ads consumption on inappropriate videos. Customization of the\narchitecture have also been included to serve different requirements of\ncorporations. Our analysis reveals that YouTube is still plagued by such\ndisturbing videos and its currently deployed countermeasures are ineffective in\nterms of detecting them in a timely manner. Our framework tries to fill this\ngap by providing a handy, add on solution to filter the videos and help\ncorporations and companies to push ads on the platform without worrying about\nthe content on which the ads are displayed.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 00:05:17 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 13:31:25 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Anand", "Vishal", ""], ["Shukla", "Ravi", ""], ["Gupta", "Ashwani", ""], ["Kumar", "Abhishek", ""]]}, {"id": "1911.04099", "submitter": "Liang Zhang", "authors": "Liang Zhang, Guannan Liu, Junjie Wu", "title": "Beyond Similarity: Relation Embedding with Dual Attentions for\n  Item-based Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the effectiveness and ease of use, Item-based Collaborative Filtering\n(ICF) methods have been broadly used in industry in recent years. The key of\nICF lies in the similarity measurement between items, which however is a\ncoarse-grained numerical value that can hardly capture users' fine-grained\npreferences toward different latent aspects of items from a representation\nlearning perspective. In this paper, we propose a model called REDA (latent\nRelation Embedding with Dual Attentions) to address this challenge. REDA is\nessentially a deep learning based recommendation method that employs an item\nrelation embedding scheme through a neural network structure for inter-item\nrelations representation. A relational user embedding is then proposed by\naggregating the relation embeddings between all purchased items of a user,\nwhich not only better characterizes user preferences but also alleviates the\ndata sparsity problem. Moreover, to capture valid meta-knowledge that reflects\nusers' desired latent aspects and meanwhile suppress their explosive growth\ntowards overfitting, we further propose a dual attentions mechanism, including\na memory attention and a weight attention. A relation-wise optimization method\nis finally developed for model inference by constructing a personalized ranking\nloss for item relations. Extensive experiments are implemented on real-world\ndatasets and the proposed model is shown to greatly outperform state-of-the-art\nmethods, especially when the data is sparse.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 06:26:13 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Zhang", "Liang", ""], ["Liu", "Guannan", ""], ["Wu", "Junjie", ""]]}, {"id": "1911.04229", "submitter": "Qiang Liu", "authors": "Qiang Liu, Shu Wu, Liang Wang", "title": "Learning Preferences and Demands in Visual Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual information is an important factor in recommender systems, in which\nusers' selections consist of two components: \\emph{preferences} and\n\\emph{demands}. Some studies has been done for modeling users' preferences in\nvisual recommendation. However, conventional methods models items in a common\nvisual feature space, which may fail in capturing \\emph{styles} of items. We\npropose a DeepStyle method for learning style features of items. DeepStyle\neliminates the categorical information of items, which is dominant in the\noriginal visual feature space, based on a Convolutional Neural Networks (CNN)\narchitecture. For modeling users' demands on different categories of items, the\nproblem can be formulated as recommendation with contextual and sequential\ninformation. To solve this problem, we propose a Context-Aware Gated Recurrent\nUnit (CA-GRU) method, which can capture sequential and contextual information\nsimultaneously. Furthermore, the aggregation of prediction on preferences and\ndemands, i.e., prediction generated by DeepStyle and CA-GRU, can model users'\nselection behaviors more completely. Experiments conducted on real-world\ndatasets illustrates the effectiveness of our proposed methods in visual\nrecommendation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 13:13:10 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Liu", "Qiang", ""], ["Wu", "Shu", ""], ["Wang", "Liang", ""]]}, {"id": "1911.04273", "submitter": "Igor De Oliveira Nunes", "authors": "Igor de Oliveira Nunes, Gabriel Matos Cardoso Leite, Daniel Ratton\n  Figueiredo", "title": "A Contextual Hierarchical Graph Model for Generating Random Sequences of\n  Objects with Application to Music Playlists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending the right content in large scale multimedia streaming services\nis an important and challenging problem that has received much attention in the\npast decade. A key ingredient for successful recommendations is an effective\nsimilarity metric between two objects, and models that leverage the current\ncontext to constrain the recommendations. This work proposes a model for random\nobject generation that introduces two key novel elements: (i) a similarity\nmetric based on the distance between objects in a given object sequence, that\nis also used to measure similarity between meta-data associated with the\nobjects, such as artists and genres; (ii) a hierarchical graph model with\ndifferent graphs each associated with a different meta-data. A biased random\nwalk in each graph that are coupled and synchronized dictate the random\ngeneration of objects, leveraging the current context to constrain randomness.\nThe proposed model is fully parameterized from sequences of objects, requiring\nno external parameters or tuning. The model is applied to a large music dataset\nwith over 1 million playlists generating a hierarchy with three layers (genre,\nartist, track). Results indicate its superiority in generating actual full\nplaylists against two baseline models.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 13:54:17 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Nunes", "Igor de Oliveira", ""], ["Leite", "Gabriel Matos Cardoso", ""], ["Figueiredo", "Daniel Ratton", ""]]}, {"id": "1911.04427", "submitter": "Manirupa Das", "authors": "Manirupa Das, Juanxi Li, Eric Fosler-Lussier, Simon Lin, Soheil\n  Moosavinasab, Steve Rust, Yungui Huang and Rajiv Ramnath", "title": "Sequence-to-Set Semantic Tagging: End-to-End Multi-label Prediction\n  using Neural Attention for Complex Query Reformulation and Automated Text\n  Categorization", "comments": "8 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel contexts may often arise in complex querying scenarios such as in\nevidence-based medicine (EBM) involving biomedical literature, that may not\nexplicitly refer to entities or canonical concept forms occurring in any fact-\nor rule-based knowledge source such as an ontology like the UMLS. Moreover,\nhidden associations between candidate concepts meaningful in the current\ncontext, may not exist within a single document, but within the collection, via\nalternate lexical forms. Therefore, inspired by the recent success of\nsequence-to-sequence neural models in delivering the state-of-the-art in a wide\nrange of NLP tasks, we develop a novel sequence-to-set framework with neural\nattention for learning document representations that can effect term transfer\nwithin the corpus, for semantically tagging a large collection of documents. We\ndemonstrate that our proposed method can be effective in both a supervised\nmulti-label classification setup for text categorization, as well as in a\nunique unsupervised setting with no human-annotated document labels that uses\nno external knowledge resources and only corpus-derived term statistics to\ndrive the training. Further, we show that semi-supervised training using our\narchitecture on large amounts of unlabeled data can augment performance on the\ntext categorization task when limited labeled data is available. Our approach\nto generate document encodings employing our sequence-to-set models for\ninference of semantic tags, gives to the best of our knowledge, the\nstate-of-the-art for both, the unsupervised query expansion task for the TREC\nCDS 2016 challenge dataset when evaluated on an Okapi BM25--based document\nretrieval system; and also over the MLTM baseline (Soleimani et al, 2016), for\nboth supervised and semi-supervised multi-label prediction tasks on the\ndel.icio.us and Ohsumed datasets. We will make our code and data publicly\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 18:13:05 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Das", "Manirupa", ""], ["Li", "Juanxi", ""], ["Fosler-Lussier", "Eric", ""], ["Lin", "Simon", ""], ["Moosavinasab", "Soheil", ""], ["Rust", "Steve", ""], ["Huang", "Yungui", ""], ["Ramnath", "Rajiv", ""]]}, {"id": "1911.04654", "submitter": "Xinyan Dai", "authors": "Xinyan Dai, Xiao Yan, Kelvin K. W. Ng, Jie Liu, James Cheng", "title": "Norm-Explicit Quantization: Improving Vector Quantization for Maximum\n  Inner Product Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector quantization (VQ) techniques are widely used in similarity search for\ndata compression, fast metric computation and etc. Originally designed for\nEuclidean distance, existing VQ techniques (e.g., PQ, AQ) explicitly or\nimplicitly minimize the quantization error. In this paper, we present a new\nangle to analyze the quantization error, which decomposes the quantization\nerror into norm error and direction error. We show that quantization errors in\nnorm have much higher influence on inner products than quantization errors in\ndirection, and small quantization error does not necessarily lead to good\nperformance in maximum inner product search (MIPS). Based on this observation,\nwe propose norm-explicit quantization (NEQ) --- a general paradigm that\nimproves existing VQ techniques for MIPS. NEQ quantizes the norms of items in a\ndataset explicitly to reduce errors in norm, which is crucial for MIPS. For the\ndirection vectors, NEQ can simply reuse an existing VQ technique to quantize\nthem without modification. We conducted extensive experiments on a variety of\ndatasets and parameter configurations. The experimental results show that NEQ\nimproves the performance of various VQ techniques for MIPS, including PQ, OPQ,\nRQ and AQ.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 03:35:17 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 05:56:21 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Dai", "Xinyan", ""], ["Yan", "Xiao", ""], ["Ng", "Kelvin K. W.", ""], ["Liu", "Jie", ""], ["Cheng", "James", ""]]}, {"id": "1911.04655", "submitter": "Xinyan Dai", "authors": "Xinyan Dai, Xiao Yan, Kaiwen Zhou, Han Yang, Kelvin K. W. Ng, James\n  Cheng, Yu Fan", "title": "Hyper-Sphere Quantization: Communication-Efficient SGD for Federated\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high cost of communicating gradients is a major bottleneck for federated\nlearning, as the bandwidth of the participating user devices is limited.\nExisting gradient compression algorithms are mainly designed for data centers\nwith high-speed network and achieve $O(\\sqrt{d} \\log d)$ per-iteration\ncommunication cost at best, where $d$ is the size of the model. We propose\nhyper-sphere quantization (HSQ), a general framework that can be configured to\nachieve a continuum of trade-offs between communication efficiency and gradient\naccuracy. In particular, at the high compression ratio end, HSQ provides a low\nper-iteration communication cost of $O(\\log d)$, which is favorable for\nfederated learning. We prove the convergence of HSQ theoretically and show by\nexperiments that HSQ significantly reduces the communication cost of model\ntraining without hurting convergence accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 03:36:09 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 11:00:41 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Dai", "Xinyan", ""], ["Yan", "Xiao", ""], ["Zhou", "Kaiwen", ""], ["Yang", "Han", ""], ["Ng", "Kelvin K. W.", ""], ["Cheng", "James", ""], ["Fan", "Yu", ""]]}, {"id": "1911.04660", "submitter": "Juliano Henrique Foleiss", "authors": "Juliano Henrique Foleiss, Tiago Fernandes Tavares", "title": "Random Projections of Mel-Spectrograms as Low-Level Features for\n  Automatic Music Genre Classification", "comments": "Submitted to IEEE Signal Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we analyse the random projections of Mel-spectrograms as\nlow-level features for music genre classification. This approach was compared\nto handcrafted features, features learned using an auto-encoder and features\nobtained from a transfer learning setting. Tests in five different well-known,\npublicly available datasets show that random projections leads to results\ncomparable to learned features and outperforms features obtained via transfer\nlearning in a shallow learning scenario. Random projections do not require\nusing extensive specialist knowledge and, simultaneously, requires less\ncomputational power for training than other projection-based low-level\nfeatures. Therefore, they can be are a viable choice for usage in shallow\nlearning content-based music genre classification.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 03:58:11 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Foleiss", "Juliano Henrique", ""], ["Tavares", "Tiago Fernandes", ""]]}, {"id": "1911.04669", "submitter": "Yekun Chai", "authors": "Yekun Chai, Naomi Saphra, Adam Lopez", "title": "How to Evaluate Word Representations of Informal Domain?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diverse word representations have surged in most state-of-the-art natural\nlanguage processing (NLP) applications. Nevertheless, how to efficiently\nevaluate such word embeddings in the informal domain such as Twitter or forums,\nremains an ongoing challenge due to the lack of sufficient evaluation dataset.\nWe derived a large list of variant spelling pairs from UrbanDictionary with the\nautomatic approaches of weakly-supervised pattern-based bootstrapping and\nself-training linear-chain conditional random field (CRF). With these extracted\nrelation pairs we promote the odds of eliding the text normalization procedure\nof traditional NLP pipelines and directly adopting representations of\nnon-standard words in the informal domain. Our code is available.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 04:38:19 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 04:32:00 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Chai", "Yekun", ""], ["Saphra", "Naomi", ""], ["Lopez", "Adam", ""]]}, {"id": "1911.04690", "submitter": "Kerl Chen", "authors": "Wenqiang Chen, Lizhang Zhan, Yuanlong Ci, Minghua Yang, Chen Lin,\n  Dugang Liu", "title": "FLEN: Leveraging Field for Scalable CTR Prediction", "comments": "KDD'20 DLP workshop, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-Through Rate (CTR) prediction has been an indispensable component for\nmany industrial applications, such as recommendation systems and online\nadvertising. CTR prediction systems are usually based on multi-field\ncategorical features, i.e., every feature is categorical and belongs to one and\nonly one field. Modeling feature conjunctions is crucial for CTR prediction\naccuracy. However, it requires a massive number of parameters to explicitly\nmodel all feature conjunctions, which is not scalable for real-world production\nsystems. In this paper, we describe a novel Field-Leveraged Embedding Network\n(FLEN) which has been deployed in the commercial recommender system in Meitu\nand serves the main traffic. FLEN devises a field-wise bi-interaction pooling\ntechnique. By suitably exploiting field information, the field-wise\nbi-interaction pooling captures both inter-field and intra-field feature\nconjunctions with a small number of model parameters and an acceptable time\ncomplexity for industrial applications. We show that a variety of\nstate-of-the-art CTR models can be expressed under this technique. Furthermore,\nwe develop Dicefactor: a dropout technique to prevent independent latent\nfeatures from co-adapting. Extensive experiments, including offline evaluations\nand online A/B testing on real production systems, demonstrate the\neffectiveness and efficiency of FLEN against the state-of-the-arts. Notably,\nFLEN has obtained 5.19% improvement on CTR with 1/6 of memory usage and\ncomputation time, compared to last version (i.e. NFM).\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 05:54:45 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 11:15:39 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 06:46:18 GMT"}, {"version": "v4", "created": "Mon, 28 Sep 2020 10:22:19 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Chen", "Wenqiang", ""], ["Zhan", "Lizhang", ""], ["Ci", "Yuanlong", ""], ["Yang", "Minghua", ""], ["Lin", "Chen", ""], ["Liu", "Dugang", ""]]}, {"id": "1911.04824", "submitter": "Andres Ferraro", "authors": "Andres Ferraro, Dmitry Bogdanov, Xavier Serra, Jay Ho Jeon, Jason Yoon", "title": "How Low Can You Go? Reducing Frequency and Time Resolution in Current\n  CNN Architectures for Music Auto-tagging", "comments": "The 28th European Signal Processing Conference (EUSIPCO)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic tagging of music is an important research topic in Music\nInformation Retrieval and audio analysis algorithms proposed for this task have\nachieved improvements with advances in deep learning. In particular, many\nstate-of-the-art systems use Convolutional Neural Networks and operate on\nmel-spectrogram representations of the audio. In this paper, we compare\ncommonly used mel-spectrogram representations and evaluate model performances\nthat can be achieved by reducing the input size in terms of both lesser amount\nof frequency bands and larger frame rates. We use the MagnaTagaTune dataset for\ncomprehensive performance comparisons and then compare selected configurations\non the larger Million Song Dataset. The results of this study can serve\nresearchers and practitioners in their trade-off decision between accuracy of\nthe models, data storage size and training and inference times.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 12:50:10 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 18:53:02 GMT"}, {"version": "v3", "created": "Sun, 28 Jun 2020 10:13:16 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Ferraro", "Andres", ""], ["Bogdanov", "Dmitry", ""], ["Serra", "Xavier", ""], ["Jeon", "Jay Ho", ""], ["Yoon", "Jason", ""]]}, {"id": "1911.04827", "submitter": "Andres Ferraro", "authors": "Andres Ferraro, Dmitry Bogdanov, Xavier Serra, Jason Yoon", "title": "Artist and style exposure bias in collaborative filtering based music\n  recommendations", "comments": "Presented at Workshop on Designing Human-Centric MIR Systems, ISMIR\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Algorithms have an increasing influence on the music that we consume and\nunderstanding their behavior is fundamental to make sure they give a fair\nexposure to all artists across different styles. In this on-going work we\ncontribute to this research direction analyzing the impact of collaborative\nfiltering recommendations from the perspective of artist and music style\nexposure given by the system. We first analyze the distribution of the\nrecommendations considering the exposure of different styles or genres and\ncompare it to the users' listening behavior. This comparison suggests that the\nsystem is reinforcing the popularity of the items. Then, we simulate the effect\nof the system in the long term with a feedback loop. From this simulation we\ncan see how the system gives less opportunity to the majority of artists,\nconcentrating the users on fewer items. The results of our analysis demonstrate\nthe need for a better evaluation methodology for current music recommendation\nalgorithms, not only limited to user-focused relevance metrics.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 12:57:30 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Ferraro", "Andres", ""], ["Bogdanov", "Dmitry", ""], ["Serra", "Xavier", ""], ["Yoon", "Jason", ""]]}, {"id": "1911.04879", "submitter": "Manvi Breja", "authors": "Manvi Breja and Sanjay Kumar Jain", "title": "A Survey on Why-Type Question Answering Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines such as Google, Yahoo and Baidu yield information in the form\nof a relevant set of web pages according to the need of the user. Question\nAnswering Systems reduce the time taken to get an answer, to a query asked in\nnatural language by providing the one most relevant answer. To the best of our\nknowledge, major research in Why-type questions began in early 2000's and our\nwork on Why-type questions can help explore newer avenues for fact-finding and\nanalysis. The paper presents a survey on Why-type Question Answering System,\ndetails the architecture, the processes involved in the system and suggests\nfurther areas of research.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 14:25:53 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Breja", "Manvi", ""], ["Jain", "Sanjay Kumar", ""]]}, {"id": "1911.04973", "submitter": "Tristan Carsault", "authors": "Tristan Carsault, J\\'er\\^ome Nika and Philippe Esling", "title": "Using musical relationships between chord labels in automatic chord\n  extraction tasks", "comments": "Accepted for publication in ISMIR, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent researches on Automatic Chord Extraction (ACE) have focused on the\nimprovement of models based on machine learning. However, most models still\nfail to take into account the prior knowledge underlying the labeling alphabets\n(chord labels). Furthermore, recent works have shown that ACE performances are\nconverging towards a glass ceiling. Therefore, this prompts the need to focus\non other aspects of the task, such as the introduction of musical knowledge in\nthe representation, the improvement of the models towards more complex chord\nalphabets and the development of more adapted evaluation methods.\n  In this paper, we propose to exploit specific properties and relationships\nbetween chord labels in order to improve the learning of statistical ACE\nmodels. Hence, we analyze the interdependence of the representations of chords\nand their associated distances, the precision of the chord alphabets, and the\nimpact of the reduction of the alphabet before or after training of the model.\nFurthermore, we propose new training losses based on musical theory. We show\nthat these improve the results of ACE systems based on Convolutional Neural\nNetworks. By performing an in-depth analysis of our results, we uncover a set\nof related insights on ACE tasks based on statistical models, and also\nformalize the musical meaning of some classification errors.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 16:04:22 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 15:32:30 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Carsault", "Tristan", ""], ["Nika", "J\u00e9r\u00f4me", ""], ["Esling", "Philippe", ""]]}, {"id": "1911.05100", "submitter": "Djordje Gligorijevic", "authors": "Djordje Gligorijevic, Jelena Gligorijevic and Aaron Flores", "title": "Time-Aware Prospective Modeling of Users for Online Display Advertising", "comments": "Accepted at AdKDD 2019 workshop at KDD'19 conference, Anchorage,\n  Alaska, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prospective display advertising poses a great challenge for large advertising\nplatforms as the strongest predictive signals of users are not eligible to be\nused in the conversion prediction systems. To that end efforts are made to\ncollect as much information as possible about each user from various data\nsources and to design powerful models that can capture weaker signals\nultimately obtaining good quality of conversion prediction probability\nestimates. In this study we propose a novel time-aware approach to model\nheterogeneous sequences of users' activities and capture implicit signals of\nusers' conversion intents. On two real-world datasets we show that our approach\noutperforms other, previously proposed approaches, while providing\ninterpretability of signal impact to conversion probability.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 19:06:59 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Gligorijevic", "Djordje", ""], ["Gligorijevic", "Jelena", ""], ["Flores", "Aaron", ""]]}, {"id": "1911.05161", "submitter": "Alvin Dey", "authors": "Alvin Dey, Harsh Kumar Jain, Vikash Kumar Pandey, Tanmoy Chakraborty", "title": "All It Takes is 20 Questions!: A Knowledge Graph Based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  20 Questions (20Q) is a two-player game. One player is the answerer, and the\nother is a questioner. The answerer chooses an entity from a specified domain\nand does not reveal this to the other player. The questioner can ask at most 20\nquestions to the answerer to guess the entity. The answerer can reply to the\nquestions asked by saying yes/no/maybe. In this paper, we propose a novel\napproach based on the knowledge graph for designing the 20Q game on Bollywood\nmovies. The system assumes the role of the questioner and asks questions to\npredict the movie thought by the answerer. It uses a probabilistic learning\nmodel for template-based question generation and answers prediction. A dataset\nof interrelated entities is represented as a weighted knowledge graph, which\nupdates as the game progresses by asking questions. An evolutionary approach\nhelps the model to gain a better understanding of user choices and predicts the\nanswer in fewer questions over time. Experimental results show that our model\nwas able to predict the correct movie in less than 10 questions for more than\nhalf of the times the game was played. This kind of model can be used to design\napplications that can detect diseases by asking questions based on symptoms,\nimproving recommendation systems, etc.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 22:05:28 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Dey", "Alvin", ""], ["Jain", "Harsh Kumar", ""], ["Pandey", "Vikash Kumar", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "1911.05276", "submitter": "Jae-woong Lee", "authors": "Jae-woong Lee, Minjin Choi, Jongwuk Lee, and Hyunjung Shim", "title": "Collaborative Distillation for Top-N Recommendation", "comments": "10 pages, ICDM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation (KD) is a well-known method to reduce inference\nlatency by compressing a cumbersome teacher model to a small student model.\nDespite the success of KD in the classification task, applying KD to\nrecommender models is challenging due to the sparsity of positive feedback, the\nambiguity of missing feedback, and the ranking problem associated with the\ntop-N recommendation. To address the issues, we propose a new KD model for the\ncollaborative filtering approach, namely collaborative distillation (CD).\nSpecifically, (1) we reformulate a loss function to deal with the ambiguity of\nmissing feedback. (2) We exploit probabilistic rank-aware sampling for the\ntop-N recommendation. (3) To train the proposed model effectively, we develop\ntwo training strategies for the student model, called the teacher- and the\nstudent-guided training methods, selecting the most useful feedback from the\nteacher model. Via experimental results, we demonstrate that the proposed model\noutperforms the state-of-the-art method by 2.7-33.2% and 2.7-29.1% in hit rate\n(HR) and normalized discounted cumulative gain (NDCG), respectively. Moreover,\nthe proposed model achieves the performance comparable to the teacher model.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 03:43:35 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Lee", "Jae-woong", ""], ["Choi", "Minjin", ""], ["Lee", "Jongwuk", ""], ["Shim", "Hyunjung", ""]]}, {"id": "1911.05395", "submitter": "Christine Bauer", "authors": "Christine Bauer", "title": "Allowing for equal opportunities for artists in music recommendation", "comments": "3 pages, position paper, 1st Workshop on Designing Human-Centric MIR\n  Systems, Delft, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Promoting diversity in the music sector is widely discussed on the media.\nWhile the major problem may lie deep in our society, music information\nretrieval contributes to promoting diversity or may create unequal\nopportunities for artists. For example, considering the known problem of\npopularity bias in music recommendation, it is important to investigate whether\nthe short head of popular music artists and the long tail of less popular ones\nshow similar patterns of diversity---in terms of, for example, age, gender, or\nethnic origin---or the popularity bias amplifies a positive or negative effect.\nI advocate for reasonable opportunities for artists---for (currently) popular\nartists and artists in the long-tail alike---in music recommender systems. In\nthis work, I represent the position that we need to develop a deep\nunderstanding of the biases and inequalities because it is the essential basis\nto design approaches for music recommendation that provide reasonable\nopportunities. Thus, research needs to investigate the various reasons that\nhinder equal opportunity and diversity in music recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 10:58:30 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Bauer", "Christine", ""]]}, {"id": "1911.05405", "submitter": "Paheli Bhattacharya", "authors": "Paheli Bhattacharya, Shounak Paul, Kripabandhu Ghosh, Saptarshi Ghosh\n  and Adam Wyner", "title": "Identification of Rhetorical Roles of Sentences in Indian Legal\n  Judgments", "comments": "Accepted at the 32nd International Conference on Legal Knowledge and\n  Information Systems (JURIX) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically understanding the rhetorical roles of sentences in a legal case\njudgement is an important problem to solve, since it can help in several\ndownstream tasks like summarization of legal judgments, legal search, and so\non. The task is challenging since legal case documents are usually not\nwell-structured, and these rhetorical roles may be subjective (as evident from\nvariation of opinions between legal experts). In this paper, we address this\ntask for judgments from the Supreme Court of India. We label sentences in 50\ndocuments using multiple human annotators, and perform an extensive analysis of\nthe human-assigned labels. We also attempt automatic identification of the\nrhetorical roles of sentences. While prior approaches towards this task used\nConditional Random Fields over manually handcrafted features, we explore the\nuse of deep neural models which do not require hand-crafting of features.\nExperiments show that neural models perform much better in this task than\nbaseline methods which use handcrafted features.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 11:21:20 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Bhattacharya", "Paheli", ""], ["Paul", "Shounak", ""], ["Ghosh", "Kripabandhu", ""], ["Ghosh", "Saptarshi", ""], ["Wyner", "Adam", ""]]}, {"id": "1911.05676", "submitter": "M. O\\u{g}uzhan K\\\"ulekci", "authors": "M. O\\u{g}uzhan K\\\"ulekci, Yasin \\\"Ozt\\\"urk, Elif Altunok, Can\n  Alt{\\i}ni\\u{g}ne", "title": "Enumerative Data Compression with Non-Uniquely Decodable Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-uniquely decodable codes can be defined as the codes that cannot be\nuniquely decoded without additional disambiguation information. These are\nmainly the class of non-prefix-free codes, where a codeword can be a prefix of\nother(s), and thus, the codeword boundary information is essential for correct\ndecoding. Although the codeword bit stream consumes significantly less space\nwhen compared to prefix--free codes, the additional disambiguation information\nmakes it difficult to catch the performance of prefix-free codes in total.\nPrevious studies considered compression with non-prefix-free codes by\nintegrating rank/select dictionaries or wavelet trees to mark the code-word\nboundaries. In this study we focus on another dimension with a block--wise\nenumeration scheme that improves the compression ratios of the previous studies\nsignificantly. Experiments conducted on a known corpus showed that the proposed\nscheme successfully represents a source within its entropy, even performing\nbetter than the Huffman and arithmetic coding in some cases. The non-uniquely\ndecodable codes also provides an intrinsic security feature due to lack of\nunique-decodability. We investigate this dimension as an opportunity to provide\ncompressed data security without (or with less) encryption, and discuss various\npossible practical advantages supported by such codes.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 17:55:06 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["K\u00fclekci", "M. O\u011fuzhan", ""], ["\u00d6zt\u00fcrk", "Yasin", ""], ["Altunok", "Elif", ""], ["Alt\u0131ni\u011fne", "Can", ""]]}, {"id": "1911.05727", "submitter": "Erik Blasch", "authors": "Erik Blasch, James Sung, Tao Nguyen, Chandra P. Daniel, Alisa P. Mason", "title": "Artificial Intelligence Strategies for National Security and Safety\n  Standards", "comments": "Presented at AAAI FSS-19: Artificial Intelligence in Government and\n  Public Sector, Arlington, Virginia, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in artificial intelligence (AI) have lead to an explosion of\nmultimedia applications (e.g., computer vision (CV) and natural language\nprocessing (NLP)) for different domains such as commercial, industrial, and\nintelligence. In particular, the use of AI applications in a national security\nenvironment is often problematic because the opaque nature of the systems leads\nto an inability for a human to understand how the results came about. A\nreliance on 'black boxes' to generate predictions and inform decisions is\npotentially disastrous. This paper explores how the application of standards\nduring each stage of the development of an AI system deployed and used in a\nnational security environment would help enable trust. Specifically, we focus\non the standards outlined in Intelligence Community Directive 203 (Analytic\nStandards) to subject machine outputs to the same rigorous standards as\nanalysis performed by humans.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 12:49:32 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Blasch", "Erik", ""], ["Sung", "James", ""], ["Nguyen", "Tao", ""], ["Daniel", "Chandra P.", ""], ["Mason", "Alisa P.", ""]]}, {"id": "1911.06111", "submitter": "Andrew O. Arnold", "authors": "Andrew O. Arnold, William W. Cohen", "title": "Instance-based Transfer Learning for Multilingual Deep Retrieval", "comments": null, "journal-ref": "The Web Conference Workshop on Multilingual Search, 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of search in the multilingual setting. Examining the\nproblems of next-sentence prediction and inverse cloze, we show that at large\nscale, instance-based transfer learning is surprisingly effective in the\nmultilingual setting, leading to positive transfer on all of the 35 target\nlanguages and two tasks tested. We analyze this improvement and argue that the\nmost natural explanation, namely direct vocabulary overlap between languages,\nonly partially explains the performance gains: in fact, we demonstrate\ntarget-language improvement can occur after adding data from an auxiliary\nlanguage even with no vocabulary in common with the target. This surprising\nresult is due to the effect of transitive vocabulary overlaps between pairs of\nauxiliary and target languages.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 22:23:30 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 18:11:37 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 15:22:31 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Arnold", "Andrew O.", ""], ["Cohen", "William W.", ""]]}, {"id": "1911.06147", "submitter": "Sergio Gast\\'on Burdisso", "authors": "Sergio G. Burdisso, Marcelo Errecalde, Manuel Montes-y-G\\'omez", "title": "t-SS3: a text classifier with dynamic n-grams for early risk detection\n  over text streams", "comments": "Highlights: (*) A classifier that is able to dynamically learn and\n  recognize important word n-grams. (*) A novel text classifier having the\n  ability to visually explain its rationale. (*) Support for incremental\n  learning and text classification over streams. (*) Efficient model for\n  addressing early risk detection problems", "journal-ref": "Pattern Recognition Letters, Elsevier, 2020", "doi": "10.1016/j.patrec.2020.07.001", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recently introduced classifier, called SS3, has shown to be well suited to\ndeal with early risk detection (ERD) problems on text streams. It obtained\nstate-of-the-art performance on early depression and anorexia detection on\nReddit in the CLEF's eRisk open tasks. SS3 was created to deal with ERD\nproblems naturally since: it supports incremental training and classification\nover text streams, and it can visually explain its rationale. However, SS3\nprocesses the input using a bag-of-word model lacking the ability to recognize\nimportant word sequences. This aspect could negatively affect the\nclassification performance and also reduces the descriptiveness of visual\nexplanations. In the standard document classification field, it is very common\nto use word n-grams to try to overcome some of these limitations.\nUnfortunately, when working with text streams, using n-grams is not trivial\nsince the system must learn and recognize which n-grams are important \"on the\nfly\". This paper introduces t-SS3, an extension of SS3 that allows it to\nrecognize useful patterns over text streams dynamically. We evaluated our model\nin the eRisk 2017 and 2018 tasks on early depression and anorexia detection.\nExperimental results suggest that t-SS3 is able to improve both current results\nand the richness of visual explanations.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 22:06:40 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 23:04:03 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Burdisso", "Sergio G.", ""], ["Errecalde", "Marcelo", ""], ["Montes-y-G\u00f3mez", "Manuel", ""]]}, {"id": "1911.06400", "submitter": "Xiao Fan Liu", "authors": "Zeng-Xian Lin, Xiao Fan Liu", "title": "Tracking the circulation routes of fresh coins in Bitcoin: A way of\n  identifying coin miners with transaction network structural properties", "comments": "in Chinese", "journal-ref": "Journal of Nanjing University of Information Science &\n  Technology(Natural Science Edition), 2018(4): 450-455", "doi": "10.13878/j.cnki.jnuist.2018.04.009", "report-no": null, "categories": "q-fin.GN cs.CR cs.IR econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin draws the highest degree of attention among cryptocurrencies, while\ncoin mining is one of the most important fashion of profiting in the Bitcoin\necosystem. This paper constructs fresh coin circulation networks by tracking\nthe fresh coin transfer routes with transaction referencing in Bitcoin\nblockchain. This paper proposes a heuristic algorithm to identifying coin\nminers by comparing coin circulation networks from different mining pools and\nthereby inferring the common profit distribution schemes of Bitcoin mining\npools. Furthermore, this paper characterizes the increasing trend of Bitcoin\nminer numbers during recent years.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 07:43:39 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Lin", "Zeng-Xian", ""], ["Liu", "Xiao Fan", ""]]}, {"id": "1911.06407", "submitter": "Ahmed El-Kishky", "authors": "Hyungsul Kim, Ahmed El-Kishky, Xiang Ren, Jiawei Han", "title": "Mining News Events from Comparable News Corpora: A Multi-Attribute\n  Proximity Network Modeling Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ProxiModel, a novel event mining framework for extracting\nhigh-quality structured event knowledge from large, redundant, and noisy news\ndata sources. The proposed model differentiates itself from other approaches by\nmodeling both the event correlation within each individual document as well as\nacross the corpus. To facilitate this, we introduce the concept of a\nproximity-network, a novel space-efficient data structure to facilitate\nscalable event mining. This proximity network captures the corpus-level\nco-occurence statistics for candidate event descriptors, event attributes, as\nwell as their connections. We probabilistically model the proximity network as\na generative process with sparsity-inducing regularization. This allows us to\nefficiently and effectively extract high-quality and interpretable news events.\nExperiments on three different news corpora demonstrate that the proposed\nmethod is effective and robust at generating high-quality event descriptors and\nattributes. We briefly detail many interesting applications from our proposed\nframework such as news summarization, event tracking and multi-dimensional\nanalysis on news. Finally, we explore a case study on visualizing the events\nfor a Japan Tsunami news corpus and demonstrate ProxiModel's ability to\nautomatically summarize emerging news events.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 22:22:12 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Kim", "Hyungsul", ""], ["El-Kishky", "Ahmed", ""], ["Ren", "Xiang", ""], ["Han", "Jiawei", ""]]}, {"id": "1911.06478", "submitter": "Mingi Ji", "authors": "Mingi Ji, Weonyoung Joo, Kyungwoo Song, Yoon-Yeong Kim, Il-Chul Moon", "title": "Sequential Recommendation with Relation-Aware Kernelized Self-Attention", "comments": "8 pages, 5 figures, AAAI", "journal-ref": "AAAI 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies identified that sequential Recommendation is improved by the\nattention mechanism. By following this development, we propose Relation-Aware\nKernelized Self-Attention (RKSA) adopting a self-attention mechanism of the\nTransformer with augmentation of a probabilistic model. The original\nself-attention of Transformer is a deterministic measure without\nrelation-awareness. Therefore, we introduce a latent space to the\nself-attention, and the latent space models the recommendation context from\nrelation as a multivariate skew-normal distribution with a kernelized\ncovariance matrix from co-occurrences, item characteristics, and user\ninformation. This work merges the self-attention of the Transformer and the\nsequential recommendation by adding a probabilistic model of the recommendation\ntask specifics. We experimented RKSA over the benchmark datasets, and RKSA\nshows significant improvements compared to the recent baseline models. Also,\nRKSA were able to produce a latent space model that answers the reasons for\nrecommendation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 04:54:54 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Ji", "Mingi", ""], ["Joo", "Weonyoung", ""], ["Song", "Kyungwoo", ""], ["Kim", "Yoon-Yeong", ""], ["Moon", "Il-Chul", ""]]}, {"id": "1911.06815", "submitter": "Preslav Nakov", "authors": "Seunghak Yu, Giovanni Da San Martino, Preslav Nakov", "title": "Experiments in Detecting Persuasion Techniques in the News", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.02517", "journal-ref": "NeurIPS-2019 workshop on AI for Social Good", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent political events, like the 2016 US Presidential elections or the\n2018 Brazilian elections have raised the attention of institutions and of the\ngeneral public on the role of Internet and social media in influencing the\noutcome of these events. We argue that a safe democracy is one in which\ncitizens have tools to make them aware of propaganda campaigns. We propose a\nnovel task: performing fine-grained analysis of texts by detecting all\nfragments that contain propaganda techniques as well as their type. We further\ndesign a novel multi-granularity neural network, and we show that it\noutperforms several strong BERT-based baselines.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 07:14:35 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Yu", "Seunghak", ""], ["Martino", "Giovanni Da San", ""], ["Nakov", "Preslav", ""]]}, {"id": "1911.06910", "submitter": "Bo Peng", "authors": "Bo Peng, Renqiang Min, Xia Ning", "title": "CNN-based Dual-Chain Models for Knowledge Graph Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge graph learning plays a critical role in integrating domain specific\nknowledge bases when deploying machine learning and data mining models in\npractice. Existing methods on knowledge graph learning primarily focus on\nmodeling the relations among entities as translations among the relations and\nentities, and many of these methods are not able to handle zero-shot problems,\nwhen new entities emerge. In this paper, we present a new convolutional neural\nnetwork (CNN)-based dual-chain model. Different from translation based methods,\nin our model, interactions among relations and entities are directly captured\nvia CNN over their embeddings. Moreover, a secondary chain of learning is\nconducted simultaneously to incorporate additional information and to enable\nbetter performance. We also present an extension of this model, which\nincorporates descriptions of entities and learns a second set of entity\nembeddings from the descriptions. As a result, the extended model is able to\neffectively handle zero-shot problems. We conducted comprehensive experiments,\ncomparing our methods with 15 methods on 8 benchmark datasets. Extensive\nexperimental results demonstrate that our proposed methods achieve or\noutperform the state-of-the-art results on knowledge graph learning, and\noutperform other methods on zero-shot problems. In addition, our methods\napplied to real-world biomedical data are able to produce results that conform\nto expert domain knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 23:24:17 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 13:40:35 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Peng", "Bo", ""], ["Min", "Renqiang", ""], ["Ning", "Xia", ""]]}, {"id": "1911.07199", "submitter": "Quanzhi Li", "authors": "Quanzhi Li, Qiong Zhang, Luo Si, Yingchi Liu", "title": "Rumor Detection on Social Media: Datasets, Methods and Opportunities", "comments": "10 pages", "journal-ref": "EMNLP 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media platforms have been used for information and news gathering, and\nthey are very valuable in many applications. However, they also lead to the\nspreading of rumors and fake news. Many efforts have been taken to detect and\ndebunk rumors on social media by analyzing their content and social context\nusing machine learning techniques. This paper gives an overview of the recent\nstudies in the rumor detection field. It provides a comprehensive list of\ndatasets used for rumor detection, and reviews the important studies based on\nwhat types of information they exploit and the approaches they take. And more\nimportantly, we also present several new directions for future research.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 09:40:24 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Li", "Quanzhi", ""], ["Zhang", "Qiong", ""], ["Si", "Luo", ""], ["Liu", "Yingchi", ""]]}, {"id": "1911.07200", "submitter": "Manish Agnihotri", "authors": "Manish Agnihotri, Adiyta Rathod, Aditya Jajodia, Chethan Sharma", "title": "Common Artist Music Assistance", "comments": "8 pages, 11 figures, ICCSE 2018 Malaysia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In today's world of growing number of songs, the need of finding apposite\nmusic content according to a user's interest is crucial. Furthermore,\nrecommendations suitable to one user may be irrelevant to another. In this\npaper, we propose a recommendation system for users with common-artist music\nlistening patterns. We use \"random walk with restart\" algorithm to get relevant\nrecommendations and conduct experiments to find the optimal values of multiple\nparameters.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 09:41:30 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Agnihotri", "Manish", ""], ["Rathod", "Adiyta", ""], ["Jajodia", "Aditya", ""], ["Sharma", "Chethan", ""]]}, {"id": "1911.07317", "submitter": "Massih-Reza Amini", "authors": "Philippe Mulhem and Lorraine Goeuriot and Massih-Reza Amini and\n  Nayanika Dogra", "title": "Quels corpus d'entra\\^inement pour l'expansion de requ\\^etes par\n  plongement de mots : application \\`a la recherche de microblogs culturels", "comments": "23 pages. in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe here an experimental framework and the results obtained on\nmicroblogs retrieval. We study the contribution one popular approach, i.e.,\nwords embeddings, and investigate the impact of the training set on the learned\nembedding. We focus on query expansion for the retrieval of tweets on the CLEF\nCMC 2016 corpus. Our results show that using embeddings trained on a corpus in\nthe same domain as the indexed documents did not necessarily lead to better\nretrieval results.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 19:05:46 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Mulhem", "Philippe", ""], ["Goeuriot", "Lorraine", ""], ["Amini", "Massih-Reza", ""], ["Dogra", "Nayanika", ""]]}, {"id": "1911.07405", "submitter": "Qiang Huang Huang", "authors": "Qiang Huang, Jianhui Bu, Weijian Xie, Shengwen Yang, Weijia Wu, Liping\n  Liu", "title": "Multi-task Sentence Encoding Model for Semantic Retrieval in Question\n  Answering Systems", "comments": "IJCNN 2019 - International Joint Conference on Neural Networks,\n  Budapest Hungary, 14-19 July 2019", "journal-ref": null, "doi": null, "report-no": "paper N-20437.pdf", "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Question Answering (QA) systems are used to provide proper responses to\nusers' questions automatically. Sentence matching is an essential task in the\nQA systems and is usually reformulated as a Paraphrase Identification (PI)\nproblem. Given a question, the aim of the task is to find the most similar\nquestion from a QA knowledge base. In this paper, we propose a Multi-task\nSentence Encoding Model (MSEM) for the PI problem, wherein a connected graph is\nemployed to depict the relation between sentences, and a multi-task learning\nmodel is applied to address both the sentence matching and sentence intent\nclassification problem. In addition, we implement a general semantic retrieval\nframework that combines our proposed model and the Approximate Nearest Neighbor\n(ANN) technology, which enables us to find the most similar question from all\navailable candidates very quickly during online serving. The experiments show\nthe superiority of our proposed method as compared with the existing sentence\nmatching models.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 03:11:36 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Huang", "Qiang", ""], ["Bu", "Jianhui", ""], ["Xie", "Weijian", ""], ["Yang", "Shengwen", ""], ["Wu", "Weijia", ""], ["Liu", "Liping", ""]]}, {"id": "1911.07429", "submitter": "Furao Shen", "authors": "Yahui Liu, Furao Shen, Jian Zhao", "title": "Pairwise Interactive Graph Attention Network for Context-Aware\n  Recommendation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-aware recommender systems (CARS), which consider rich side\ninformation to improve recommendation performance, have caught more and more\nattention in both academia and industry. How to predict user preferences from\ndiverse contextual features is the core of CARS. Several recent models pay\nattention to user behaviors and use specifically designed structures to extract\nadaptive user interests from history behaviors. However, few works take item\nhistory interactions into consideration, which leads to the insufficiency of\nitem feature representation and item attraction extraction. From these\nobservations, we model the user-item interaction as a dynamic interaction graph\n(DIG) and proposed a GNN-based model called Pairwise Interactive Graph\nAttention Network (PIGAT) to capture dynamic user interests and item\nattractions simultaneously. PIGAT introduces the attention mechanism to\nconsider the importance of each interacted user/item to both the user and the\nitem, which captures user interests, item attractions and their influence on\nthe recommendation context. Moreover, confidence embeddings are applied to\ninteractions to distinguish the confidence of interactions occurring at\ndifferent times. Then more expressive user/item representations and adaptive\ninteraction features are generated, which benefits the recommendation\nperformance especially when involving long-tail items. We conduct experiments\non three real-world datasets to demonstrate the effectiveness of PIGAT.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 05:06:05 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Liu", "Yahui", ""], ["Shen", "Furao", ""], ["Zhao", "Jian", ""]]}, {"id": "1911.07574", "submitter": "Wen-Yen Chang", "authors": "Wen-Yen Chang and Wen-Huan Chiang and Shao-Hao Lu and Tingfan Wu and\n  Min Sun", "title": "Bias-Aware Heapified Policy for Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data efficiency of learning-based algorithms is more and more important\nsince high-quality and clean data is expensive as well as hard to collect. In\norder to achieve high model performance with the least number of samples,\nactive learning is a technique that queries the most important subset of data\nfrom the original dataset. In active learning domain, one of the mainstream\nresearch is the heuristic uncertainty-based method which is useful for the\nlearning-based system. Recently, a few works propose to apply policy\nreinforcement learning (PRL) for querying important data. It seems more general\nthan heuristic uncertainty-based method owing that PRL method depends on data\nfeature which is reliable than human prior. However, there have two problems -\nsample inefficiency of policy learning and overconfidence, when applying PRL on\nactive learning. To be more precise, sample inefficiency of policy learning\noccurs when sampling within a large action space, in the meanwhile, class\nimbalance can lead to the overconfidence. In this paper, we propose a\nbias-aware policy network called Heapified Active Learning (HAL), which\nprevents overconfidence, and improves sample efficiency of policy learning by\nheapified structure without ignoring global inforamtion(overview of the whole\nunlabeled set). In our experiment, HAL outperforms other baseline methods on\nMNIST dataset and duplicated MNIST. Last but not least, we investigate the\ngeneralization of the HAL policy learned on MNIST dataset by directly applying\nit on MNIST-M. We show that the agent can generalize and outperform\ndirectly-learned policy under constrained labeled sets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 12:08:09 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Chang", "Wen-Yen", ""], ["Chiang", "Wen-Huan", ""], ["Lu", "Shao-Hao", ""], ["Wu", "Tingfan", ""], ["Sun", "Min", ""]]}, {"id": "1911.07673", "submitter": "Fabrizio Orlandi", "authors": "Ademar Crotti Junior and Fabrizio Orlandi and Declan O'Sullivan and\n  Christian Dirschl and Quentin Reul", "title": "Using Mapping Languages for Building Legal Knowledge Graphs from XML\n  Files", "comments": "Presented at the 2nd International Contextualized Knowledge Graphs\n  Workshop (CKG'19) at the 18th International Semantic Web Conference (ISWC)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents our experience on building RDF knowledge graphs for an\nindustrial use case in the legal domain. The information contained in legal\ninformation systems are often accessed through simple keyword interfaces and\npresented as a simple list of hits. In order to improve search accuracy one may\navail of knowledge graphs, where the semantics of the data can be made\nexplicit. Significant research effort has been invested in the area of building\nknowledge graphs from semi-structured text documents, such as XML, with the\nprevailing approach being the use of mapping languages. In this paper, we\npresent a semantic model for representing legal documents together with an\nindustrial use case. We also present a set of use case requirements based on\nthe proposed semantic model, which are used to compare and discuss the use of\nstate-of-the-art mapping languages for building knowledge graphs for legal\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 14:50:31 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Junior", "Ademar Crotti", ""], ["Orlandi", "Fabrizio", ""], ["O'Sullivan", "Declan", ""], ["Dirschl", "Christian", ""], ["Reul", "Quentin", ""]]}, {"id": "1911.07698", "submitter": "Maurizio Ferrari Dacrema", "authors": "Maurizio Ferrari Dacrema and Simone Boglio and Paolo Cremonesi and\n  Dietmar Jannach", "title": "A Troubling Analysis of Reproducibility and Progress in Recommender\n  Systems Research", "comments": "Source code and full results available at:\n  https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation", "journal-ref": "ACM Transactions on Information Systems 39, 2, Article 20 (January\n  2021), 49 pages", "doi": "10.1145/3434185", "report-no": null, "categories": "cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of algorithms that generate personalized ranked item lists is a\ncentral topic of research in the field of recommender systems. In the past few\nyears, in particular, approaches based on deep learning (neural) techniques\nhave become dominant in the literature. For all of them, substantial progress\nover the state-of-the-art is claimed. However, indications exist of certain\nproblems in today's research practice, e.g., with respect to the choice and\noptimization of the baselines used for comparison, raising questions about the\npublished claims. In order to obtain a better understanding of the actual\nprogress, we have tried to reproduce recent results in the area of neural\nrecommendation approaches based on collaborative filtering. The worrying\noutcome of the analysis of these recent works-all were published at prestigious\nscientific conferences between 2015 and 2018-is that 11 out of the 12\nreproducible neural approaches can be outperformed by conceptually simple\nmethods, e.g., based on the nearest-neighbor heuristics. None of the\ncomputationally complex neural methods was actually consistently better than\nalready existing learning-based techniques, e.g., using matrix factorization or\nlinear models. In our analysis, we discuss common issues in today's research\npractice, which, despite the many papers that are published on the topic, have\napparently led the field to a certain level of stagnation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 15:27:09 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 10:18:34 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 14:09:01 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Dacrema", "Maurizio Ferrari", ""], ["Boglio", "Simone", ""], ["Cremonesi", "Paolo", ""], ["Jannach", "Dietmar", ""]]}, {"id": "1911.07716", "submitter": "Farhad Pourkamali-Anaraki", "authors": "Farhad Pourkamali-Anaraki and Michael B. Wakin", "title": "The Effectiveness of Variational Autoencoders for Active Learning", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high cost of acquiring labels is one of the main challenges in deploying\nsupervised machine learning algorithms. Active learning is a promising approach\nto control the learning process and address the difficulties of data labeling\nby selecting labeled training examples from a large pool of unlabeled\ninstances. In this paper, we propose a new data-driven approach to active\nlearning by choosing a small set of labeled data points that are both\ninformative and representative. To this end, we present an efficient geometric\ntechnique to select a diverse core-set in a low-dimensional latent space\nobtained by training a Variational Autoencoder (VAE). Our experiments\ndemonstrate an improvement in accuracy over two related techniques and, more\nimportantly, signify the representation power of generative modeling for\ndeveloping new active learning methods in high-dimensional data settings.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 15:42:20 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Pourkamali-Anaraki", "Farhad", ""], ["Wakin", "Michael B.", ""]]}, {"id": "1911.07918", "submitter": "Vivek Gupta", "authors": "Vivek Gupta, Ankit Saw, Pegah Nokhiz, Harshit Gupta, Partha Talukdar", "title": "Improving Document Classification with Multi-Sense Embeddings", "comments": "8 Pages, 7 Figures, 12 Tables, under review at ECAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Efficient representation of text documents is an important building block in\nmany NLP tasks. Research on long text categorization has shown that simple\nweighted averaging of word vectors for sentence representation often\noutperforms more sophisticated neural models. Recently proposed Sparse\nComposite Document Vector (SCDV) (Mekala et. al, 2017) extends this approach\nfrom sentences to documents using soft clustering over word vectors. However,\nSCDV disregards the multi-sense nature of words, and it also suffers from the\ncurse of higher dimensionality. In this work, we address these shortcomings and\npropose SCDV-MS. SCDV-MS utilizes multi-sense word embeddings and learns a\nlower dimensional manifold. Through extensive experiments on multiple\nreal-world datasets, we show that SCDV-MS embeddings outperform previous\nstate-of-the-art embeddings on multi-class and multi-label text categorization\ntasks. Furthermore, SCDV-MS embeddings are more efficient than SCDV in terms of\ntime and space complexity on textual classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 20:30:06 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Gupta", "Vivek", ""], ["Saw", "Ankit", ""], ["Nokhiz", "Pegah", ""], ["Gupta", "Harshit", ""], ["Talukdar", "Partha", ""]]}, {"id": "1911.08054", "submitter": "Himank Yadav", "authors": "Himank Yadav, Zhengxiao Du, Thorsten Joachims", "title": "Policy-Gradient Training of Fair and Unbiased Ranking Functions", "comments": null, "journal-ref": null, "doi": "10.1145/3404835.3462953", "report-no": null, "categories": "cs.LG cs.CY cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While implicit feedback (e.g., clicks, dwell times, etc.) is an abundant and\nattractive source of data for learning to rank, it can produce unfair ranking\npolicies for both exogenous and endogenous reasons. Exogenous reasons typically\nmanifest themselves as biases in the training data, which then get reflected in\nthe learned ranking policy and often lead to rich-get-richer dynamics.\nMoreover, even after the correction of such biases, reasons endogenous to the\ndesign of the learning algorithm can still lead to ranking policies that do not\nallocate exposure among items in a fair way. To address both exogenous and\nendogenous sources of unfairness, we present the first learning-to-rank\napproach that addresses both presentation bias and merit-based fairness of\nexposure simultaneously. Specifically, we define a class of amortized\nfairness-of-exposure constraints that can be chosen based on the needs of an\napplication, and we show how these fairness criteria can be enforced despite\nthe selection biases in implicit feedback data. The key result is an efficient\nand flexible policy-gradient algorithm, called FULTR, which is the first to\nenable the use of counterfactual estimators for both utility estimation and\nfairness constraints. Beyond the theoretical justification of the framework, we\nshow empirically that the proposed algorithm can learn accurate and fair\nranking policies from biased and noisy feedback.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 02:45:42 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 11:15:39 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yadav", "Himank", ""], ["Du", "Zhengxiao", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1911.08113", "submitter": "Preslav Nakov", "authors": "Todor Mihaylov, Preslav Nakov", "title": "Hunting for Troll Comments in News Community Forums", "comments": null, "journal-ref": "ACL-2016", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are different definitions of what a troll is. Certainly, a troll can be\nsomebody who teases people to make them angry, or somebody who offends people,\nor somebody who wants to dominate any single discussion, or somebody who tries\nto manipulate people's opinion (sometimes for money), etc. The last definition\nis the one that dominates the public discourse in Bulgaria and Eastern Europe,\nand this is our focus in this paper. In our work, we examine two types of\nopinion manipulation trolls: paid trolls that have been revealed from leaked\nreputation management contracts and mentioned trolls that have been called such\nby several different people. We show that these definitions are sensible: we\nbuild two classifiers that can distinguish a post by such a paid troll from one\nby a non-troll with 81-82% accuracy; the same classifier achieves 81-82%\naccuracy on so called mentioned troll vs. non-troll posts.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 06:42:23 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Mihaylov", "Todor", ""], ["Nakov", "Preslav", ""]]}, {"id": "1911.08125", "submitter": "Preslav Nakov", "authors": "Momchil Hardalov, Ivan Koychev, Preslav Nakov", "title": "In Search of Credible News", "comments": "Credibility, veracity, fact checking, humor detection", "journal-ref": "AIMSA-2016", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding fake online news. This is an important\nproblem as news of questionable credibility have recently been proliferating in\nsocial media at an alarming scale. As this is an understudied problem,\nespecially for languages other than English, we first collect and release to\nthe research community three new balanced credible vs. fake news datasets\nderived from four online sources. We then propose a language-independent\napproach for automatically distinguishing credible from fake news, based on a\nrich feature set. In particular, we use linguistic (n-gram),\ncredibility-related (capitalization, punctuation, pronoun use, sentiment\npolarity), and semantic (embeddings and DBPedia data) features. Our experiments\non three different testsets show that our model can distinguish credible from\nfake news with very high accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 07:06:22 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Hardalov", "Momchil", ""], ["Koychev", "Ivan", ""], ["Nakov", "Preslav", ""]]}, {"id": "1911.08151", "submitter": "Jiahuan Pei", "authors": "Jiahuan Pei, Pengjie Ren, Christof Monz, Maarten de Rijke", "title": "Retrospective and Prospective Mixture-of-Generators for Task-oriented\n  Dialogue Response Generation", "comments": "The paper is accepted by 24th European Conference on Artificial\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue response generation (DRG) is a critical component of task-oriented\ndialogue systems (TDSs). Its purpose is to generate proper natural language\nresponses given some context, e.g., historical utterances, system states, etc.\nState-of-the-art work focuses on how to better tackle DRG in an end-to-end way.\nTypically, such studies assume that each token is drawn from a single\ndistribution over the output vocabulary, which may not always be optimal.\nResponses vary greatly with different intents, e.g., domains, system actions.\n  We propose a novel mixture-of-generators network (MoGNet) for DRG, where we\nassume that each token of a response is drawn from a mixture of distributions.\nMoGNet consists of a chair generator and several expert generators. Each expert\nis specialized for DRG w.r.t. a particular intent. The chair coordinates\nmultiple experts and combines the output they have generated to produce more\nappropriate responses. We propose two strategies to help the chair make better\ndecisions, namely, a retrospective mixture-of-generators (RMoG) and prospective\nmixture-of-generators (PMoG). The former only considers the historical\nexpert-generated responses until the current time step while the latter also\nconsiders possible expert-generated responses in the future by encouraging\nexploration. In order to differentiate experts, we also devise a\nglobal-and-local (GL) learning scheme that forces each expert to be specialized\ntowards a particular intent using a local loss and trains the chair and all\nexperts to coordinate using a global loss.\n  We carry out extensive experiments on the MultiWOZ benchmark dataset. MoGNet\nsignificantly outperforms state-of-the-art methods in terms of both automatic\nand human evaluations, demonstrating its effectiveness for DRG.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 08:20:45 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 11:04:28 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Pei", "Jiahuan", ""], ["Ren", "Pengjie", ""], ["Monz", "Christof", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1911.08225", "submitter": "Rodrigo Santos", "authors": "Marcio Ferreira Moreno, Rodrigo Costa Mesquita Santos, Wallas Henrique\n  Sousa dos Santos, Sandro Rama Fiorini, Reinaldo Mozart da Gama Silva", "title": "Multimedia Search and Temporal Reasoning", "comments": "International Conference on Information Systems (ICIS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Properly modelling dynamic information that changes over time still is an\nopen issue. Most modern knowledge bases are unable to represent relationships\nthat are valid only during a given time interval. In this work, we revisit a\nprevious extension to the hyperknowledge framework to deal with temporal facts\nand propose a temporal query language and engine. We validate our proposal by\ndiscussing a qualitative analysis of the modelling of a real-world use case in\nthe Oil & Gas industry.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 12:29:19 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Moreno", "Marcio Ferreira", ""], ["Santos", "Rodrigo Costa Mesquita", ""], ["Santos", "Wallas Henrique Sousa dos", ""], ["Fiorini", "Sandro Rama", ""], ["Silva", "Reinaldo Mozart da Gama", ""]]}, {"id": "1911.08340", "submitter": "Martin Andrews", "authors": "Martin Andrews, Sam Witteveen", "title": "Unsupervised Natural Question Answering with a Small Model", "comments": "Accepted paper for FEVER workshop at EMNLP-IJCNLP 2019. (4 pages +\n  references)", "journal-ref": null, "doi": "10.18653/v1/D19-6606", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent (2019-02) demonstration of the power of huge language models such\nas GPT-2 to memorise the answers to factoid questions raises questions about\nthe extent to which knowledge is being embedded directly within these large\nmodels. This short paper describes an architecture through which much smaller\nmodels can also answer such questions - by making use of 'raw' external\nknowledge. The contribution of this work is that the methods presented here\nrely on unsupervised learning techniques, complementing the unsupervised\ntraining of the Language Model. The goal of this line of research is to be able\nto add knowledge explicitly, without extensive training.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 15:18:39 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Andrews", "Martin", ""], ["Witteveen", "Sam", ""]]}, {"id": "1911.08743", "submitter": "Preslav Nakov", "authors": "Todor Mihaylov, Preslav Nakov", "title": "SemanticZ at SemEval-2016 Task 3: Ranking Relevant Answers in Community\n  Question Answering Using Semantic Similarity Based on Fine-tuned Word\n  Embeddings", "comments": "community question answering, semantic similarity", "journal-ref": "SemEval-2016", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our system for finding good answers in a community forum, as\ndefined in SemEval-2016, Task 3 on Community Question Answering. Our approach\nrelies on several semantic similarity features based on fine-tuned word\nembeddings and topics similarities. In the main Subtask C, our primary\nsubmission was ranked third, with a MAP of 51.68 and accuracy of 69.94. In\nSubtask A, our primary submission was also third, with MAP of 77.58 and\naccuracy of 73.39.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 07:16:16 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Mihaylov", "Todor", ""], ["Nakov", "Preslav", ""]]}, {"id": "1911.08755", "submitter": "Preslav Nakov", "authors": "Shafiq Joty, Alberto Barr\\'on-Cede\\~no, Giovanni Da San Martino,\n  Simone Filice, Llu\\'is M\\`arquez, Alessandro Moschitti, Preslav Nakov", "title": "Global Thread-Level Inference for Comment Classification in Community\n  Question Answering", "comments": "community question answering, thread-level inference, graph-cut,\n  inductive logic programming", "journal-ref": "EMNLP-2015", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community question answering, a recent evolution of question answering in the\nWeb context, allows a user to quickly consult the opinion of a number of people\non a particular topic, thus taking advantage of the wisdom of the crowd. Here\nwe try to help the user by deciding automatically which answers are good and\nwhich are bad for a given question. In particular, we focus on exploiting the\noutput structure at the thread level in order to make more consistent global\ndecisions. More specifically, we exploit the relations between pairs of\ncomments at any distance in the thread, which we incorporate in a graph-cut and\nin an ILP frameworks. We evaluated our approach on the benchmark dataset of\nSemEval-2015 Task 3. Results improved over the state of the art, confirming the\nimportance of using thread level information.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 08:09:36 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Joty", "Shafiq", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["Martino", "Giovanni Da San", ""], ["Filice", "Simone", ""], ["M\u00e0rquez", "Llu\u00eds", ""], ["Moschitti", "Alessandro", ""], ["Nakov", "Preslav", ""]]}, {"id": "1911.08762", "submitter": "Preslav Nakov", "authors": "Preslav Nakov", "title": "Paraphrasing Verbs for Noun Compound Interpretation", "comments": "noun compounds, paraphrasing verbs, semantic interpretation,\n  multi-word expressions, MWEs", "journal-ref": "MWE-2008", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important challenge for the automatic analysis of English written text is\nthe abundance of noun compounds: sequences of nouns acting as a single noun. In\nour view, their semantics is best characterized by the set of all possible\nparaphrasing verbs, with associated weights, e.g., malaria mosquito is carry\n(23), spread (16), cause (12), transmit (9), etc. Using Amazon's Mechanical\nTurk, we collect paraphrasing verbs for 250 noun-noun compounds previously\nproposed in the linguistic literature, thus creating a valuable resource for\nnoun compound interpretation. Using these verbs, we further construct a dataset\nof pairs of sentences representing a special kind of textual entailment task,\nwhere a binary decision is to be made about whether an expression involving a\nverb and two nouns can be transformed into a noun compound, while preserving\nthe sentence meaning.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 08:29:10 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Nakov", "Preslav", ""]]}, {"id": "1911.08976", "submitter": "Martin Andrews", "authors": "Yew Ken Chia, Sam Witteveen, Martin Andrews", "title": "Red Dragon AI at TextGraphs 2019 Shared Task: Language Model Assisted\n  Explanation Generation", "comments": "Accepted paper for TextGraphs-13 workshop at EMNLP-IJCNLP 2019. (5\n  pages including references)", "journal-ref": null, "doi": "10.18653/v1/D19-5311", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TextGraphs-13 Shared Task on Explanation Regeneration asked participants\nto develop methods to reconstruct gold explanations for elementary science\nquestions. Red Dragon AI's entries used the language of the questions and\nexplanation text directly, rather than a constructing a separate graph-like\nrepresentation. Our leaderboard submission placed us 3rd in the competition,\nbut we present here three methods of increasing sophistication, each of which\nscored successively higher on the test set after the competition close.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 15:41:47 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Chia", "Yew Ken", ""], ["Witteveen", "Sam", ""], ["Andrews", "Martin", ""]]}, {"id": "1911.08994", "submitter": "Zhifeng Jia", "authors": "Jiacheng Dai, Zhifeng Jia, Xiaofeng Gao and Guihai Chen", "title": "A Hierarchical Optimizer for Recommendation System Based on Shortest\n  Path Algorithm", "comments": "The report is in a length of 2 pages, and includes 3 figures", "journal-ref": null, "doi": null, "report-no": "Report-no: 341", "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-k Nearest Geosocial Keyword (T-kNGK) query on geosocial network is\ndefined to give users k recommendations based on some keywords and designated\nspatial range, and can be realized by shortest path algorithms. However,\nshortest path algorithm cannot provide convincing recommendations, so we design\na hierarchical optimizer consisting of classifiers and a constant optimizer to\noptimize the result by some features of the service providers.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 03:23:27 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Dai", "Jiacheng", ""], ["Jia", "Zhifeng", ""], ["Gao", "Xiaofeng", ""], ["Chen", "Guihai", ""]]}, {"id": "1911.09356", "submitter": "Cristina Cornelio PhD", "authors": "Mustafa Canim, Cristina Cornelio, Arun Iyengar, Ryan Musa, Mariano\n  Rodrigez Muro", "title": "Schemaless Queries over Document Tables with Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unstructured enterprise data such as reports, manuals and guidelines often\ncontain tables. The traditional way of integrating data from these tables is\nthrough a two-step process of table detection/extraction and mapping the table\nlayouts to an appropriate schema. This can be an expensive process. In this\npaper we show that by using semantic technologies (RDF/SPARQL and database\ndependencies) paired with a simple but powerful way to transform tables with\nnon-relational layouts, it is possible to offer query answering services over\nthese tables with minimal manual work or domain-specific mappings. Our method\nenables users to exploit data in tables embedded in documents with little\neffort, not only for simple retrieval queries, but also for structured queries\nthat require joining multiple interrelated tables.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 09:20:24 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Canim", "Mustafa", ""], ["Cornelio", "Cristina", ""], ["Iyengar", "Arun", ""], ["Musa", "Ryan", ""], ["Muro", "Mariano Rodrigez", ""]]}, {"id": "1911.09360", "submitter": "Pierre-Francois Marteau", "authors": "Pierre-Fran\\c{c}ois Marteau (EXPRESSION, UBS)", "title": "On the separation of shape and temporal patterns in time series\n  -Application to signature authentication-", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.IR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we address the problem of separation of shape and time\ncomponents in time series. The concept ofshape that we tackle is termed\ntemporally neutral to consider that it may possibly exist outside of any\ntemporal specification, as it is the case for a geometric form. We propose to\nexploit and adapt a probabilistic temporal alignment algorithm, initially\ndesigned to estimate the centroid of a set of time series, to build some\nheuristicelements of solution to this separation problem. We show on some\ncontrolled synthetic data that this algorithm meets empirically our initial\nobjectives. We finally evaluate it on real data, in the context of some on-line\nhandwritten signature authentication benchmarks. On the three evaluated tasks,\nour approach based on the separation of signature shape and associated temporal\npatterns is positioned slightly above the current state of the art\ndemonstrating the applicative benefit of this separating problem.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 09:32:32 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 14:21:41 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Marteau", "Pierre-Fran\u00e7ois", "", "EXPRESSION, UBS"]]}, {"id": "1911.09471", "submitter": "Sahan Bulathwela", "authors": "Sahan Bulathwela, Maria Perez-Ortiz, Emine Yilmaz and John\n  Shawe-Taylor", "title": "TrueLearn: A Family of Bayesian Algorithms to Match Lifelong Learners to\n  Open Educational Resources", "comments": "In Proceedings of AAAI Conference on Artificial Intelligence 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advances in computer-assisted learning systems and the\navailability of open educational resources today promise a pathway to providing\ncost-efficient, high-quality education to large masses of learners. One of the\nmost ambitious use cases of computer-assisted learning is to build a lifelong\nlearning recommendation system. Unlike short-term courses, lifelong learning\npresents unique challenges, requiring sophisticated recommendation models that\naccount for a wide range of factors such as background knowledge of learners or\nnovelty of the material while effectively maintaining knowledge states of\nmasses of learners for significantly longer periods of time (ideally, a\nlifetime). This work presents the foundations towards building a dynamic,\nscalable and transparent recommendation system for education, modelling\nlearner's knowledge from implicit data in the form of engagement with open\neducational resources. We i) use a text ontology based on Wikipedia to\nautomatically extract knowledge components of educational resources and, ii)\npropose a set of online Bayesian strategies inspired by the well-known areas of\nitem response theory and knowledge tracing. Our proposal, TrueLearn, focuses on\nrecommendations for which the learner has enough background knowledge (so they\nare able to understand and learn from the material), and the material has\nenough novelty that would help the learner improve their knowledge about the\nsubject and keep them engaged. We further construct a large open educational\nvideo lectures dataset and test the performance of the proposed algorithms,\nwhich show clear promise towards building an effective educational\nrecommendation system.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 13:56:40 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Bulathwela", "Sahan", ""], ["Perez-Ortiz", "Maria", ""], ["Yilmaz", "Emine", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1911.09572", "submitter": "Wenxin Hu", "authors": "Wenxin Hu, Xiaofeng Zhang, Gang Yang", "title": "Automatically Generating Macro Research Reports from a Piece of News", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically generating macro research reports from economic news is an\nimportant yet challenging task. As we all know, it requires the macro analysts\nto write such reports within a short period of time after the important\neconomic news are released. This motivates our work, i.e., using AI techniques\nto save manual cost. The goal of the proposed system is to generate macro\nresearch reports as the draft for macro analysts. Essentially, the core\nchallenge is the long text generation issue. To address this issue, we propose\na novel deep learning technique based approach which includes two components,\ni.e., outline generation and macro research report generation.For the model\nperformance evaluation, we first crawl a large news-to-report dataset and then\nevaluate our approach on this dataset, and the generated reports are given for\nthe subjective evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 16:05:02 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Hu", "Wenxin", ""], ["Zhang", "Xiaofeng", ""], ["Yang", "Gang", ""]]}, {"id": "1911.09732", "submitter": "Yu Meng", "authors": "Yu Meng, Maryam Karimzadehgan, Honglei Zhuang, Donald Metzler", "title": "Separate and Attend in Personal Email Search", "comments": "WSDM 2020", "journal-ref": null, "doi": "10.1145/3336191.3371775", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In personal email search, user queries often impose different requirements on\ndifferent aspects of the retrieved emails. For example, the query \"my recent\nflight to the US\" requires emails to be ranked based on both textual contents\nand recency of the email documents, while other queries such as \"medical\nhistory\" do not impose any constraints on the recency of the email. Recent deep\nlearning-to-rank models for personal email search often directly concatenate\ndense numerical features (e.g., document age) with embedded sparse features\n(e.g., n-gram embeddings). In this paper, we first show with a set of\nexperiments on synthetic datasets that direct concatenation of dense and sparse\nfeatures does not lead to the optimal search performance of deep neural ranking\nmodels. To effectively incorporate both sparse and dense email features into\npersonal email search ranking, we propose a novel neural model, SepAttn.\nSepAttn first builds two separate neural models to learn from sparse and dense\nfeatures respectively, and then applies an attention mechanism at the\nprediction level to derive the final prediction from these two models. We\nconduct a comprehensive set of experiments on a large-scale email search\ndataset, and demonstrate that our SepAttn model consistently improves the\nsearch quality over the baseline models.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 20:19:28 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Meng", "Yu", ""], ["Karimzadehgan", "Maryam", ""], ["Zhuang", "Honglei", ""], ["Metzler", "Donald", ""]]}, {"id": "1911.09735", "submitter": "Son Doan <", "authors": "Son Doan, Quoc-Hung Ngo, Ai Kawazoe, Nigel Collier", "title": "Global Health Monitor: A Web-based System for Detecting and Mapping\n  Infectious Diseases", "comments": "6 pages, 3 figures, Proc. of IJCNLP 2008", "journal-ref": "Proc. of the International Joint Conference on Natural Language\n  Processing (IJCNLP) 2008, pages 951-956", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Global Health Monitor, an online Web-based system for\ndetecting and mapping infectious disease outbreaks that appear in news stories.\nThe system analyzes English news stories from news feed providers, classifies\nthem for topical relevance and plots them onto a Google map using geo-coding\ninformation, helping public health workers to monitor the spread of diseases in\na geo-temporal context. The background knowledge for the system is contained in\nthe BioCaster ontology (BCO) (Collier et al., 2007a) which includes both\ninformation on infectious diseases as well as geographical locations with their\nlatitudes/longitudes. The system consists of four main stages: topic\nclassification, named entity recognition (NER), disease/location detection and\nvisualization. Evaluation of the system shows that it achieved high accuracy on\na gold standard corpus. The system is now in practical use. Running on a\nclustercomputer, it monitors more than 1500 news feeds 24/7, updating the map\nevery hour.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 20:26:29 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Doan", "Son", ""], ["Ngo", "Quoc-Hung", ""], ["Kawazoe", "Ai", ""], ["Collier", "Nigel", ""]]}, {"id": "1911.09787", "submitter": "Busra Celikkaya", "authors": "Ming Zhu, Busra Celikkaya, Parminder Bhatia, Chandan K. Reddy", "title": "LATTE: Latent Type Modeling for Biomedical Entity Linking", "comments": "AAAI 2020 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity linking is the task of linking mentions of named entities in natural\nlanguage text, to entities in a curated knowledge-base. This is of significant\nimportance in the biomedical domain, where it could be used to semantically\nannotate a large volume of clinical records and biomedical literature, to\nstandardized concepts described in an ontology such as Unified Medical Language\nSystem (UMLS). We observe that with precise type information, entity\ndisambiguation becomes a straightforward task. However, fine-grained type\ninformation is usually not available in biomedical domain. Thus, we propose\nLATTE, a LATent Type Entity Linking model, that improves entity linking by\nmodeling the latent fine-grained type information about mentions and entities.\nUnlike previous methods that perform entity linking directly between the\nmentions and the entities, LATTE jointly does entity disambiguation, and latent\nfine-grained type learning, without direct supervision. We evaluate our model\non two biomedical datasets: MedMentions, a large scale public dataset annotated\nwith UMLS concepts, and a de-identified corpus of dictated doctor's notes that\nhas been annotated with ICD concepts. Extensive experimental evaluation shows\nour model achieves significant performance improvements over several\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 23:55:15 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 23:17:27 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Zhu", "Ming", ""], ["Celikkaya", "Busra", ""], ["Bhatia", "Parminder", ""], ["Reddy", "Chandan K.", ""]]}, {"id": "1911.09798", "submitter": "Sebastian Bruch", "authors": "Sebastian Bruch", "title": "An Alternative Cross Entropy Loss for Learning-to-Rank", "comments": null, "journal-ref": null, "doi": "10.1145/3442381.3449794", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Listwise learning-to-rank methods form a powerful class of ranking algorithms\nthat are widely adopted in applications such as information retrieval. These\nalgorithms learn to rank a set of items by optimizing a loss that is a function\nof the entire set -- as a surrogate to a typically non-differentiable ranking\nmetric. Despite their empirical success, existing listwise methods are based on\nheuristics and remain theoretically ill-understood. In particular, none of the\nempirically successful loss functions are related to ranking metrics. In this\nwork, we propose a cross entropy-based learning-to-rank loss function that is\ntheoretically sound, is a convex bound on NDCG -- a popular ranking metric --\nand is consistent with NDCG under learning scenarios common in information\nretrieval. Furthermore, empirical evaluation of an implementation of the\nproposed method with gradient boosting machines on benchmark learning-to-rank\ndatasets demonstrates the superiority of our proposed formulation over existing\nalgorithms in quality and robustness.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 00:58:11 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 01:14:31 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 19:41:23 GMT"}, {"version": "v4", "created": "Fri, 22 May 2020 12:51:42 GMT"}, {"version": "v5", "created": "Thu, 4 Feb 2021 19:02:56 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Bruch", "Sebastian", ""]]}, {"id": "1911.09818", "submitter": "Jing Pan", "authors": "Jing Pan, Weian Sheng, Santanu Dey", "title": "Order Matters at Fanatics Recommending Sequentially Ordered Products by\n  LSTM Embedded with Word2Vec", "comments": "5 pages, 2 figures, KDD 2019 Workshop, Deep Learning on Graphics,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A unique challenge for e-commerce recommendation is that customers are often\ninterested in products that are more advanced than their already purchased\nproducts, but not reversed. The few existing recommender systems modeling\nunidirectional sequence output a limited number of categories or continuous\nvariables. To model the ordered sequence, we design the first recommendation\nsystem that both embed purchased items with Word2Vec, and model the sequence\nwith stateless LSTM RNN. The click-through rate of this recommender system in\nproduction outperforms its solely Word2Vec based predecessor. Developed in\n2017, it was perhaps the first published real-world application that makes\ndistributed predictions of a single machine trained Keras model on Spark slave\nnodes at a scale of more than 0.4 million columns per row.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 02:39:41 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Pan", "Jing", ""], ["Sheng", "Weian", ""], ["Dey", "Santanu", ""]]}, {"id": "1911.09821", "submitter": "Canran Xu", "authors": "Canran Xu, Ming Wu", "title": "Learning Feature Interactions with Lorentzian Factorization Machine", "comments": "8 pages, 5 figures, accepted to AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning representations for feature interactions to model user behaviors is\ncritical for recommendation system and click-trough rate (CTR) predictions.\nRecent advances in this area are empowered by deep learning methods which could\nlearn sophisticated feature interactions and achieve the state-of-the-art\nresult in an end-to-end manner. These approaches require large number of\ntraining parameters integrated with the low-level representations, and thus are\nmemory and computational inefficient. In this paper, we propose a new model\nnamed \"LorentzFM\" that can learn feature interactions embedded in a hyperbolic\nspace in which the violation of triangle inequality for Lorentz distances is\navailable. To this end, the learned representation is benefited by the peculiar\ngeometric properties of hyperbolic triangles, and result in a significant\nreduction in the number of parameters (20\\% to 80\\%) because all the top deep\nlearning layers are not required. With such a lightweight architecture,\nLorentzFM achieves comparable and even materially better results than the deep\nlearning methods such as DeepFM, xDeepFM and Deep \\& Cross in both\nrecommendation and CTR prediction tasks.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 02:43:39 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Xu", "Canran", ""], ["Wu", "Ming", ""]]}, {"id": "1911.09872", "submitter": "Ghazaleh Beigi", "authors": "Ghazaleh Beigi, Ahmadreza Mosallanezhad, Ruocheng Guo, Hamidreza\n  Alvari, Alexander Nou, Huan Liu", "title": "Privacy-Aware Recommendation with Private-Attribute Protection using\n  Adversarial Learning", "comments": "The Thirteenth ACM International Conference on Web Search and Data\n  Mining (WSDM 2020)", "journal-ref": null, "doi": "10.1145/3336191.3371832", "report-no": null, "categories": "cs.SI cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation is one of the critical applications that helps users find\ninformation relevant to their interests. However, a malicious attacker can\ninfer users' private information via recommendations. Prior work obfuscates\nuser-item data before sharing it with recommendation system. This approach does\nnot explicitly address the quality of recommendation while performing data\nobfuscation. Moreover, it cannot protect users against private-attribute\ninference attacks based on recommendations. This work is the first attempt to\nbuild a Recommendation with Attribute Protection (RAP) model which\nsimultaneously recommends relevant items and counters private-attribute\ninference attacks. The key idea of our approach is to formulate this problem as\nan adversarial learning problem with two main components: the private attribute\ninference attacker, and the Bayesian personalized recommender. The attacker\nseeks to infer users' private-attribute information according to their items\nlist and recommendations. The recommender aims to extract users' interests\nwhile employing the attacker to regularize the recommendation process.\nExperiments show that the proposed model both preserves the quality of\nrecommendation service and protects users against private-attribute inference\nattacks.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 06:22:21 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Beigi", "Ghazaleh", ""], ["Mosallanezhad", "Ahmadreza", ""], ["Guo", "Ruocheng", ""], ["Alvari", "Hamidreza", ""], ["Nou", "Alexander", ""], ["Liu", "Huan", ""]]}, {"id": "1911.09882", "submitter": "Nikki Lijing Kuang", "authors": "Nikki Lijing Kuang and Clement H.C. Leung", "title": "Analysis of Evolutionary Behavior in Self-Learning Media Search Engines", "comments": "IEEE BigData 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diversity of intrinsic qualities of multimedia entities tends to impede\ntheir effective retrieval. In a SelfLearning Search Engine architecture, the\nsubtle nuances of human perceptions and deep knowledge are taught and captured\nthrough unsupervised reinforcement learning, where the degree of reinforcement\nmay be suitably calibrated. Such architectural paradigm enables indexes to\nevolve naturally while accommodating the dynamic changes of user interests. It\noperates by continuously constructing indexes over time, while injecting\nprogressive improvement in search performance. For search operations to be\neffective, convergence of index learning is of crucial importance to ensure\nefficiency and robustness. In this paper, we develop a Self-Learning Search\nEngine architecture based on reinforcement learning using a Markov Decision\nProcess framework. The balance between exploration and exploitation is achieved\nthrough evolutionary exploration Strategies. The evolutionary index learning\nbehavior is then studied and formulated using stochastic analysis. Experimental\nresults are presented which corroborate the steady convergence of the index\nevolution mechanism. Index Term\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 06:43:56 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Kuang", "Nikki Lijing", ""], ["Leung", "Clement H. C.", ""]]}, {"id": "1911.09891", "submitter": "Nikki Lijing Kuang", "authors": "Nikki Lijing Kuang and Clement H.C. Leung", "title": "Performance Effectiveness of Multimedia Information Search Using the\n  Epsilon-Greedy Algorithm", "comments": "8 pages, 10 figures. IEEE ICMLA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the search and retrieval of multimedia objects, it is impractical to\neither manually or automatically extract the contents for indexing since most\nof the multimedia contents are not machine extractable, while manual extraction\ntends to be highly laborious and time-consuming. However, by systematically\ncapturing and analyzing the feedback patterns of human users, vital information\nconcerning the multimedia contents can be harvested for effective indexing and\nsubsequent search. By learning from the human judgment and mental evaluation of\nusers, effective search indices can be gradually developed and built up, and\nsubsequently be exploited to find the most relevant multimedia objects. To\navoid hovering around a local maximum, we apply the epsilon-greedy method to\nsystematically explore the search space. Through such methodic exploration, we\nshow that the proposed approach is able to guarantee that the most relevant\nobjects can always be discovered, even though initially it may have been\noverlooked or not regarded as relevant. The search behavior of the present\napproach is quantitatively analyzed, and closed-form expressions are obtained\nfor the performance of two variants of the epsilon-greedy algorithm, namely\nEGSE-A and EGSE-B. Simulations and experiments on real data set have been\nperformed which show good agreement with the theoretical findings. The present\nmethod is able to leverage exploration in an effective way to significantly\nraise the performance of multimedia information search, and enables the certain\ndiscovery of relevant objects which may be otherwise undiscoverable.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 07:12:05 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Kuang", "Nikki Lijing", ""], ["Leung", "Clement H. C.", ""]]}, {"id": "1911.09969", "submitter": "Ruixue Liu", "authors": "Meng Chen, Ruixue Liu, Lei Shen, Shaozu Yuan, Jingyan Zhou, Youzheng\n  Wu, Xiaodong He, Bowen Zhou", "title": "The JDDC Corpus: A Large-Scale Multi-Turn Chinese Dialogue Dataset for\n  E-commerce Customer Service", "comments": "This paper is accepted by LREC 2020 (International Conference on\n  Language Resources and Evaluation )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human conversations are complicated and building a human-like dialogue agent\nis an extremely challenging task. With the rapid development of deep learning\ntechniques, data-driven models become more and more prevalent which need a huge\namount of real conversation data. In this paper, we construct a large-scale\nreal scenario Chinese E-commerce conversation corpus, JDDC, with more than 1\nmillion multi-turn dialogues, 20 million utterances, and 150 million words. The\ndataset reflects several characteristics of human-human conversations, e.g.,\ngoal-driven, and long-term dependency among the context. It also covers various\ndialogue types including task-oriented, chitchat and question-answering. Extra\nintent information and three well-annotated challenge sets are also provided.\nThen, we evaluate several retrieval-based and generative models to provide\nbasic benchmark performance on the JDDC corpus. And we hope JDDC can serve as\nan effective testbed and benefit the development of fundamental research in\ndialogue task\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 10:55:50 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 08:38:17 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2020 08:12:30 GMT"}, {"version": "v4", "created": "Tue, 24 Mar 2020 15:09:18 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Chen", "Meng", ""], ["Liu", "Ruixue", ""], ["Shen", "Lei", ""], ["Yuan", "Shaozu", ""], ["Zhou", "Jingyan", ""], ["Wu", "Youzheng", ""], ["He", "Xiaodong", ""], ["Zhou", "Bowen", ""]]}, {"id": "1911.09976", "submitter": "Xinshao Wang Mr", "authors": "Xinshao Wang, Elyor Kodirov, Yang Hua, Neil Robertson", "title": "Instance Cross Entropy for Deep Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loss functions play a crucial role in deep metric learning thus a variety of\nthem have been proposed. Some supervise the learning process by pairwise or\ntripletwise similarity constraints while others take advantage of structured\nsimilarity information among multiple data points. In this work, we approach\ndeep metric learning from a novel perspective. We propose instance cross\nentropy (ICE) which measures the difference between an estimated instance-level\nmatching distribution and its ground-truth one. ICE has three main appealing\nproperties. Firstly, similar to categorical cross entropy (CCE), ICE has clear\nprobabilistic interpretation and exploits structured semantic similarity\ninformation for learning supervision. Secondly, ICE is scalable to infinite\ntraining data as it learns on mini-batches iteratively and is independent of\nthe training set size. Thirdly, motivated by our relative weight analysis,\nseamless sample reweighting is incorporated. It rescales samples' gradients to\ncontrol the differentiation degree over training examples instead of truncating\nthem by sample mining. In addition to its simplicity and intuitiveness,\nextensive experiments on three real-world benchmarks demonstrate the\nsuperiority of ICE.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 11:12:48 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Wang", "Xinshao", ""], ["Kodirov", "Elyor", ""], ["Hua", "Yang", ""], ["Robertson", "Neil", ""]]}, {"id": "1911.10130", "submitter": "Amey Girish Parundekar", "authors": "Amey Parundekar, Susan Elias, Ashwin Ashok", "title": "A Data Set of Internet Claims and Comparison of their Sentiments with\n  Credibility", "comments": "8 pages, 6 figures, A paper accepted at the Truth Discovery and Fact\n  Checking: Theory and Practice SIGKDD 2019 Workshop, August 5th, Anchorage,\n  Alaska", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this modern era, communication has become faster and easier. This means\nfallacious information can spread as fast as reality. Considering the damage\nthat fake news kindles on the psychology of people and the fact that such news\nproliferates faster than truth, we need to study the phenomenon that helps\nspread fake news. An unbiased data set that depends on reality for rating news\nis necessary to construct predictive models for its classification. This paper\ndescribes the methodology to create such a data set. We collect our data from\nsnopes.com which is a fact-checking organization. Furthermore, we intend to\ncreate this data set not only for classification of the news but also to find\npatterns that reason the intent behind misinformation. We also formally define\nan Internet Claim, its credibility, and the sentiment behind such a claim. We\ntry to realize the relationship between the sentiment of a claim with its\ncredibility. This relationship pours light on the bigger picture behind the\npropagation of misinformation. We pave the way for further research based on\nthe methodology described in this paper to create the data set and usage of\npredictive modeling along with research-based on psychology/mentality of people\nto understand why fake news spreads much faster than reality.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 16:35:37 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Parundekar", "Amey", ""], ["Elias", "Susan", ""], ["Ashok", "Ashwin", ""]]}, {"id": "1911.10232", "submitter": "Kai Ni", "authors": "Amit Pande, Kai Ni and Venkataramani Kini", "title": "SWAG: Item Recommendations using Convolutions on Weighted Graphs", "comments": "10 pages, 8 figures, 2019 IEEE BigData special session", "journal-ref": "2019 IEEE International Conference on Big Data", "doi": "10.1109/BigData47090.2019.9005633", "report-no": null, "categories": "cs.IR cs.LG stat.CO stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recent advancements in deep neural networks for graph-structured data have\nled to state-of-the-art performance on recommender system benchmarks. In this\nwork, we present a Graph Convolutional Network (GCN) algorithm SWAG (Sample\nWeight and AGgregate), which combines efficient random walks and graph\nconvolutions on weighted graphs to generate embeddings for nodes (items) that\nincorporate both graph structure as well as node feature information such as\nitem-descriptions and item-images. The three important SWAG operations that\nenable us to efficiently generate node embeddings based on graph structures are\n(a) Sampling of graph to homogeneous structure, (b) Weighting the sampling,\nwalks and convolution operations, and (c) using AGgregation functions for\ngenerating convolutions. The work is an adaptation of graphSAGE over weighted\ngraphs. We deploy SWAG at Target and train it on a graph of more than 500K\nproducts sold online with over 50M edges. Offline and online evaluations reveal\nthe benefit of using a graph-based approach and the benefits of weighing to\nproduce high quality embeddings and product recommendations.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 19:51:58 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Pande", "Amit", ""], ["Ni", "Kai", ""], ["Kini", "Venkataramani", ""]]}, {"id": "1911.10418", "submitter": "Yue Zhao", "authors": "Yue Zhao and Maciej K. Hryniewicki", "title": "DCSO: Dynamic Combination of Detector Scores for Outlier Ensembles", "comments": "ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),\n  Outlier Detection De-constructed Workshop, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting and combining the outlier scores of different base detectors used\nwithin outlier ensembles can be quite challenging in the absence of ground\ntruth. In this paper, an unsupervised outlier detector combination framework\ncalled DCSO is proposed, demonstrated and assessed for the dynamic selection of\nmost competent base detectors, with an emphasis on data locality. The proposed\nDCSO framework first defines the local region of a test instance by its k\nnearest neighbors and then identifies the top-performing base detectors within\nthe local region. Experimental results on ten benchmark datasets demonstrate\nthat DCSO provides consistent performance improvement over existing static\ncombination approaches in mining outlying objects. To facilitate\ninterpretability and reliability of the proposed method, DCSO is analyzed using\nboth theoretical frameworks and visualization techniques, and presented\nalongside empirical parameter setting instructions that can be used to improve\nthe overall performance.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 21:16:00 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Zhao", "Yue", ""], ["Hryniewicki", "Maciej K.", ""]]}, {"id": "1911.10421", "submitter": "Preslav Nakov", "authors": "Iris Hendrickx, Preslav Nakov, Stan Szpakowicz, Zornitsa Kozareva,\n  Diarmuid \\'O S\\'eaghdha, Tony Veale", "title": "SemEval-2013 Task 4: Free Paraphrases of Noun Compounds", "comments": "noun compounds, paraphrasing verbs, semantic interpretation,\n  multi-word expressions, MWEs", "journal-ref": "SemEval-2013", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe SemEval-2013 Task 4: the definition, the data, the\nevaluation and the results. The task is to capture some of the meaning of\nEnglish noun compounds via paraphrasing. Given a two-word noun compound, the\nparticipating system is asked to produce an explicitly ranked list of its\nfree-form paraphrases. The list is automatically compared and evaluated against\na similarly ranked list of paraphrases proposed by human annotators, recruited\nand managed through Amazon's Mechanical Turk. The comparison of raw paraphrases\nis sensitive to syntactic and morphological variation. The \"gold\" ranking is\nbased on the relative popularity of paraphrases among annotators. To make the\nranking more reliable, highly similar paraphrases are grouped, so as to\ndownplay superficial differences in syntax and morphology. Three systems\nparticipated in the task. They all beat a simple baseline on one of the two\nevaluation measures, but not on both measures. This shows that the task is\ndifficult.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 21:42:23 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Hendrickx", "Iris", ""], ["Nakov", "Preslav", ""], ["Szpakowicz", "Stan", ""], ["Kozareva", "Zornitsa", ""], ["S\u00e9aghdha", "Diarmuid \u00d3", ""], ["Veale", "Tony", ""]]}, {"id": "1911.10422", "submitter": "Preslav Nakov", "authors": "Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid\n  \\'O S\\'eaghdha, Sebastian Pad\\'o, Marco Pennacchiotti, Lorenza Romano, Stan\n  Szpakowicz", "title": "SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations\n  Between Pairs of Nominals", "comments": "semantic relations, nominals", "journal-ref": "SemEval-2010", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In response to the continuing research interest in computational semantic\nanalysis, we have proposed a new task for SemEval-2010: multi-way\nclassification of mutually exclusive semantic relations between pairs of\nnominals. The task is designed to compare different approaches to the problem\nand to provide a standard testbed for future research. In this paper, we define\nthe task, describe the creation of the datasets, and discuss the results of the\nparticipating 28 systems submitted by 10 teams.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 21:49:10 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Hendrickx", "Iris", ""], ["Kim", "Su Nam", ""], ["Kozareva", "Zornitsa", ""], ["Nakov", "Preslav", ""], ["S\u00e9aghdha", "Diarmuid \u00d3", ""], ["Pad\u00f3", "Sebastian", ""], ["Pennacchiotti", "Marco", ""], ["Romano", "Lorenza", ""], ["Szpakowicz", "Stan", ""]]}, {"id": "1911.10699", "submitter": "Ruijia Wang", "authors": "Xiao Wang, Ruijia Wang, Chuan Shi, Guojie Song, Qingyong Li", "title": "Multi-Component Graph Convolutional Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interactions of users and items in recommender system could be naturally\nmodeled as a user-item bipartite graph. In recent years, we have witnessed an\nemerging research effort in exploring user-item graph for collaborative\nfiltering methods. Nevertheless, the formation of user-item interactions\ntypically arises from highly complex latent purchasing motivations, such as\nhigh cost performance or eye-catching appearance, which are indistinguishably\nrepresented by the edges. The existing approaches still remain the differences\nbetween various purchasing motivations unexplored, rendering the inability to\ncapture fine-grained user preference. Therefore, in this paper we propose a\nnovel Multi-Component graph convolutional Collaborative Filtering (MCCF)\napproach to distinguish the latent purchasing motivations underneath the\nobserved explicit user-item interactions. Specifically, there are two\nelaborately designed modules, decomposer and combiner, inside MCCF. The former\nfirst decomposes the edges in user-item graph to identify the latent components\nthat may cause the purchasing relationship; the latter then recombines these\nlatent components automatically to obtain unified embeddings for prediction.\nFurthermore, the sparse regularizer and weighted random sample strategy are\nutilized to alleviate the overfitting problem and accelerate the optimization.\nEmpirical results on three real datasets and a synthetic dataset not only show\nthe significant performance gains of MCCF, but also well demonstrate the\nnecessity of considering multiple components.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 04:41:43 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Wang", "Xiao", ""], ["Wang", "Ruijia", ""], ["Shi", "Chuan", ""], ["Song", "Guojie", ""], ["Li", "Qingyong", ""]]}, {"id": "1911.10708", "submitter": "Idris Abdulmumin", "authors": "Idris Abdulmumin and Bashir Shehu Galadanci", "title": "hauWE: Hausa Words Embedding for Natural Language Processing", "comments": "In Proceedings of the 2019 2nd International Conference of the IEEE\n  Nigeria Computer Chapter", "journal-ref": null, "doi": "10.1109/NigeriaComputConf45974.2019.8949674", "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Words embedding (distributed word vector representations) have become an\nessential component of many natural language processing (NLP) tasks such as\nmachine translation, sentiment analysis, word analogy, named entity recognition\nand word similarity. Despite this, the only work that provides word vectors for\nHausa language is that of Bojanowski et al. [1] trained using fastText,\nconsisting of only a few words vectors. This work presents words embedding\nmodels using Word2Vec's Continuous Bag of Words (CBoW) and Skip Gram (SG)\nmodels. The models, hauWE (Hausa Words Embedding), are bigger and better than\nthe only previous model, making them more useful in NLP tasks. To compare the\nmodels, they were used to predict the 10 most similar words to 30 randomly\nselected Hausa words. hauWE CBoW's 88.7% and hauWE SG's 79.3% prediction\naccuracy greatly outperformed Bojanowski et al. [1]'s 22.3%.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 05:46:56 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Abdulmumin", "Idris", ""], ["Galadanci", "Bashir Shehu", ""]]}, {"id": "1911.10763", "submitter": "Benjamin Sznajder", "authors": "Liat Ein-Dor, Eyal Shnarch, Lena Dankin, Alon Halfon, Benjamin\n  Sznajder, Ariel Gera, Carlos Alzate, Martin Gleize, Leshem Choshen, Yufang\n  Hou, Yonatan Bilu, Ranit Aharonov and Noam Slonim", "title": "Corpus Wide Argument Mining -- a Working Solution", "comments": null, "journal-ref": "AAAI 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main tasks in argument mining is the retrieval of argumentative\ncontent pertaining to a given topic. Most previous work addressed this task by\nretrieving a relatively small number of relevant documents as the initial\nsource for such content. This line of research yielded moderate success, which\nis of limited use in a real-world system. Furthermore, for such a system to\nyield a comprehensive set of relevant arguments, over a wide range of topics,\nit requires leveraging a large and diverse corpus in an appropriate manner.\nHere we present a first end-to-end high-precision, corpus-wide argument mining\nsystem. This is made possible by combining sentence-level queries over an\nappropriate indexing of a very large corpus of newspaper articles, with an\niterative annotation scheme. This scheme addresses the inherent label bias in\nthe data and pinpoints the regions of the sample space whose manual labeling is\nrequired to obtain high-precision among top-ranked candidates.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 08:29:37 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ein-Dor", "Liat", ""], ["Shnarch", "Eyal", ""], ["Dankin", "Lena", ""], ["Halfon", "Alon", ""], ["Sznajder", "Benjamin", ""], ["Gera", "Ariel", ""], ["Alzate", "Carlos", ""], ["Gleize", "Martin", ""], ["Choshen", "Leshem", ""], ["Hou", "Yufang", ""], ["Bilu", "Yonatan", ""], ["Aharonov", "Ranit", ""], ["Slonim", "Noam", ""]]}, {"id": "1911.10953", "submitter": "Amir Karami", "authors": "Amir Karami, Aryya Gangopadhyay, Bin Zhou, Hadi Kharrazi", "title": "FLATM: A Fuzzy Logic Approach Topic Model for Medical Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges for text analysis in medical domains is analyzing\nlarge-scale medical documents. As a consequence, finding relevant documents has\nbecome more difficult. One of the popular methods to retrieve information based\non discovering the themes in the documents is topic modeling. The themes in the\ndocuments help to retrieve documents on the same topic with and without a\nquery. In this paper, we present a novel approach to topic modeling using fuzzy\nclustering. To evaluate our model, we experiment with two text datasets of\nmedical documents. The evaluation metrics carried out through document\nclassification and document modeling show that our model produces better\nperformance than LDA, indicating that fuzzy set theory can improve the\nperformance of topic models in medical domains.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 14:55:11 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Karami", "Amir", ""], ["Gangopadhyay", "Aryya", ""], ["Zhou", "Bin", ""], ["Kharrazi", "Hadi", ""]]}, {"id": "1911.11017", "submitter": "Jiankai Sun", "authors": "Jiankai Sun, Jie Zhao, Huan Sun, and Srinivasan Parthasarathy", "title": "An End-to-End Framework for Cold Question Routing in Community Question\n  Answering Services", "comments": "arXiv admin note: text overlap with arXiv:1807.00462", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Routing newly posted questions (a.k.a cold questions) to potential answerers\nwith the suitable expertise in Community Question Answering sites (CQAs) is an\nimportant and challenging task. The existing methods either focus only on\nembedding the graph structural information and are less effective for newly\nposted questions, or adopt manually engineered feature vectors that are not as\nrepresentative as the graph embedding methods. Therefore, we propose to address\nthe challenge of leveraging heterogeneous graph and textual information for\ncold question routing by designing an end-to-end framework that jointly learns\nCQA node embeddings and finds best answerers for cold questions. We conducted\nextensive experiments to confirm the usefulness of incorporating the textual\ninformation from question tags and demonstrate that an end-2-end framework can\nachieve promising performances on routing newly posted questions asked by both\nexisting users and newly registered users.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 07:59:32 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Sun", "Jiankai", ""], ["Zhao", "Jie", ""], ["Sun", "Huan", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1911.11060", "submitter": "Saad Farooq", "authors": "Saad Farooq", "title": "A Survey on Adversarial Information Retrieval on the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey paper discusses different forms of malicious techniques that can\naffect how an information retrieval model retrieves documents for a query and\ntheir remedies.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 17:23:31 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 14:57:18 GMT"}, {"version": "v3", "created": "Sat, 15 Feb 2020 13:03:58 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Farooq", "Saad", ""]]}, {"id": "1911.11061", "submitter": "Tommy Jones", "authors": "Tommy Jones", "title": "A Coefficient of Determination for Probabilistic Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research proposes a new (old) metric for evaluating goodness of fit in\ntopic models, the coefficient of determination, or $R^2$. Within the context of\ntopic modeling, $R^2$ has the same interpretation that it does when used in a\nbroader class of statistical models. Reporting $R^2$ with topic models\naddresses two current problems in topic modeling: a lack of standard\ncross-contextual evaluation metrics for topic modeling and ease of\ncommunication with lay audiences. The author proposes that $R^2$ should be\nreported as a standard metric when constructing topic models.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 00:55:30 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 03:07:01 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Jones", "Tommy", ""]]}, {"id": "1911.11062", "submitter": "Md Saiful Islam", "authors": "Arnab Sen Sharma, Maruf Ahmed Mridul, Md Saiful Islam", "title": "Automatic Detection of Satire in Bangla Documents: A CNN Approach Based\n  on Hybrid Feature Extraction Model", "comments": "5 pages, Conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Widespread of satirical news in online communities is an ongoing trend. The\nnature of satires is so inherently ambiguous that sometimes it's too hard even\nfor humans to understand whether it's actually satire or not. So, research\ninterest has grown in this field. The purpose of this research is to detect\nBangla satirical news spread in online news portals as well as social media. In\nthis paper, we propose a hybrid technique for extracting features from text\ndocuments combining Word2Vec and TF-IDF. Using our proposed feature extraction\ntechnique, with standard CNN architecture we could detect whether a Bangla text\ndocument is satire or not with an accuracy of more than 96%.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 20:37:03 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Sharma", "Arnab Sen", ""], ["Mridul", "Maruf Ahmed", ""], ["Islam", "Md Saiful", ""]]}, {"id": "1911.11063", "submitter": "Kezia Irene", "authors": "Hanqing Huang, Kezia Irene, Nahyun Ryu", "title": "Voice Search and Typed Search Performance Comparison on Baidu Search\n  System", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the voice search system is getting more and more developed, some\npeople still have difficulties when searching for information with the voice\nsearch system. This paper is a pilot study to compare the search performance of\npeople using voice search and typed search using Baidu search system. We\nsurveyed and interviewed 40 Chinese students who have been using the Baidu\nsearch system. Afterward, we analyzed 8 people who had a middle to advanced\nsearching ability by their behaviors, search results, and average query length.\nWe found that there are a lot of variations among the participants' time when\nsearching for different queries, and there were some interesting behaviors that\nwere displayed by a number of participants. We conclude that more participants\nare needed to make a firm conclusion on the performance comparison between the\nvoice search and typed search.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 10:40:26 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Huang", "Hanqing", ""], ["Irene", "Kezia", ""], ["Ryu", "Nahyun", ""]]}, {"id": "1911.11064", "submitter": "Nourah ALRossais", "authors": "Nourah ALRossais, Daniel Kudenko", "title": "Generating Stereotypes Automatically For Complex Categorical Features", "comments": "Accepted to KaRS 2019 Second Workshop on Knowledge-Aware and\n  Conversational Recommender Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the context of stereotypes creation for recommender systems, we found that\ncertain types of categorical variables pose particular challenges if simple\nclustering procedures were employed with the objective to create stereotypes. A\ncategorical variable is defined to be complex when it cannot be easily\ntranslated into a numerical variable, when the semantic of the categories\npotentially plays an important role in the optimal determination of\nstereotypes, and when it is also multi-choice (e.g., each item can be labelled\nwith one or more categories that may be applicable, in a non pre-defined\nnumber).\n  The main objective of this paper is to analyse the possibility of obtaining a\nviable recommendation system that operates on stereotypes generated directly\nvia the feature's metadata similarities, without using ratings information at\nthe time the generation of the classes. The encouraging results using\nintegrated MovieLens and Imdb data set show that the proposed algorithm\nperforms better than other categorical clustering algorithms like k-modes when\nclustering complex categorical features. Notably, the representation of complex\ncategorical features can help to alleviate cold-start issues in recommender\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 13:02:32 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["ALRossais", "Nourah", ""], ["Kudenko", "Daniel", ""]]}, {"id": "1911.11065", "submitter": "Siamak Shakeri", "authors": "Siamak Shakeri, Abhinav Sethy, Cheng Cheng", "title": "Knowledge Distillation in Document Retrieval", "comments": "Published at Amazon Machine Learning Conference(AMLC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex deep learning models now achieve state of the art performance for\nmany document retrieval tasks. The best models process the query or claim\njointly with the document. However for fast scalable search it is desirable to\nhave document embeddings which are independent of the claim. In this paper we\nshow that knowledge distillation can be used to encourage a model that\ngenerates claim independent document encodings to mimic the behavior of a more\ncomplex model which generates claim dependent encodings. We explore this\napproach in document retrieval for a fact extraction and verification task. We\nshow that by using the soft labels from a complex cross attention teacher\nmodel, the performance of claim independent student LSTM or CNN models is\nimproved across all the ranking metrics. The student models we use are 12x\nfaster in runtime and 20x smaller in number of parameters than the teacher\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 21:02:54 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Shakeri", "Siamak", ""], ["Sethy", "Abhinav", ""], ["Cheng", "Cheng", ""]]}, {"id": "1911.11066", "submitter": "Rajkumar R", "authors": "R Rajkumar, Dr. M V Sudhamani", "title": "Crawler for Image Acquisition from World Wide Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the advancement in computer communication and storage technologies,\nlarge amount of image data is available on World Wide Web (WWW). In order to\nlocate a particular set of images the available search engines may be used with\nthe help of keywords. Here, the filtering of unwanted data is not done. For the\npurpose of retrieving relevant images with appropriate keyword(s) an image\ncrawler is designed and implemented. Here, keyword(s) are submitted as query\nand with the help of sender engine, images are downloaded along with metadata\nlike URL, filename, file size, file access date and time etc.,. Later, with the\nhelp of URL, images already present in repository and newly downloaded are\ncompared for uniqueness. Only unique URLs are in turn considered and stored in\nrepository. The images in the repository are used to build novel Content Based\nImage Retrieval (CBIR) system in future. This repository may be used for\nvarious purposes. This image crawler tool is useful in building image datasets\nwhich can be used by any CBIR system for training and testing purposes.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 04:14:21 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Rajkumar", "R", ""], ["Sudhamani", "Dr. M V", ""]]}, {"id": "1911.11067", "submitter": "Bokun Kong", "authors": "Bokun Kong", "title": "Analysing Russian Trolls via NLP tools", "comments": "53 pages, 8 figures, 16 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The fifty-eighth American presidential election in 2016 still arouse fierce\ncontroversyat present. A portion of politicians as well as medium and voters\nbelieve that theRussian government interfered with the election of 2016 by\ncontrolling malicioussocial media accounts on twitter, such as trolls and bots\naccounts. Both of them willbroadcast fake news, derail the conversations about\nelection, and mislead people.Therefore, this paper will focus on analysing some\nof the twitter dataset about theelection of 2016 by using NLP methods and\nlooking for some interesting patterns ofwhether the Russian government\ninterfered with the election or not. We apply topicmodel on the given twitter\ndataset to extract some interesting topics and analysethe meaning, then we\nimplement supervised topic model to retrieve the relationshipbetween topics to\ncategory which is left troll or right troll, and analyse the\npattern.Additionally, we will do sentiment analysis to analyse the attitude of\nthe tweet. Afterextracting typical tweets from interesting topic, sentiment\nanalysis offers the ability toknow whether the tweet supports this topic or\nnot. Based on comprehensive analysisand evaluation, we find interesting\npatterns of the dataset as well as some meaningfultopics.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 08:30:54 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Kong", "Bokun", ""]]}, {"id": "1911.11069", "submitter": "Arthi Krishna", "authors": "Arthi Krishna, Ye Jin, Christine Foster, Greg Gabel, Britt Hanley and\n  Abdou Youssef", "title": "Query Expansion for Patent Searching using Word Embedding and\n  Professional Crowdsourcing", "comments": "Presented at AAAI FSS-19: Artificial Intelligence in Government and\n  Public Sector, Arlington, Virginia, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The patent examination process includes a search of previous work to verify\nthat a patent application describes a novel invention. Patent examiners\nprimarily use keyword-based searches to uncover prior art. A critical part of\nkeyword searching is query expansion, which is the process of including\nalternate terms such as synonyms and other related words, since the same\nconcepts are often described differently in the literature. Patent terminology\nis often domain specific. By curating technology-specific corpora and training\nword embedding models based on these corpora, we are able to automatically\nidentify the most relevant expansions of a given word or phrase. We compare the\nperformance of several automated query expansion techniques against expert\nspecified expansions. Furthermore, we explore a novel mechanism to extract\nrelated terms not just based on one input term but several terms in conjunction\nby computing their centroid and identifying the nearest neighbors to this\ncentroid. Highly skilled patent examiners are often the best and most reliable\nsource of identifying related terms. By designing a user interface that allows\nexaminers to interact with the word embedding suggestions, we are able to use\nthese interactions to power crowdsourced modes of related terms. Learning from\nusers allows us to overcome several challenges such as identifying words that\nare bleeding edge and have not been published in the corpus yet. This paper\nstudies the effectiveness of word embedding and crowdsourced models across 11\ndisparate technical areas.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 22:34:02 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Krishna", "Arthi", ""], ["Jin", "Ye", ""], ["Foster", "Christine", ""], ["Gabel", "Greg", ""], ["Hanley", "Britt", ""], ["Youssef", "Abdou", ""]]}, {"id": "1911.11070", "submitter": "Joanna Misztal-Radecka", "authors": "Joanna Misztal-Radecka, Dominik Rusiecki, Micha{\\l} \\.Zmuda, Artur\n  Bujak", "title": "Trend-responsive User Segmentation Enabling Traceable Publishing\n  Insights. A Case Study of a Real-world Large-scale News Recommendation System", "comments": null, "journal-ref": "7th International Workshop on News Recommendation and Analytics\n  (INRA 2019), in conjunction with RecSys 2019, September 19, 2019, Copenhagen,\n  Denmark", "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The traditional offline approaches are no longer sufficient for building\nmodern recommender systems in domains such as online news services, mainly due\nto the high dynamics of environment changes and necessity to operate on a large\nscale with high data sparsity. The ability to balance exploration with\nexploitation makes the multi-armed bandits an efficient alternative to the\nconventional methods, and a robust user segmentation plays a crucial role in\nproviding the context for such online recommendation algorithms. In this work,\nwe present an unsupervised and trend-responsive method for segmenting users\naccording to their semantic interests, which has been integrated with a\nreal-world system for large-scale news recommendations. The results of an\nonline A/B test show significant improvements compared to a global-optimization\nalgorithm on several services with different characteristics. Based on the\nexperimental results as well as the exploration of segments descriptions and\ntrend dynamics, we propose extensions to this approach that address particular\nreal-world challenges for different use-cases. Moreover, we describe a method\nof generating traceable publishing insights facilitating the creation of\ncontent that serves the diversity of all users needs.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 18:42:16 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Misztal-Radecka", "Joanna", ""], ["Rusiecki", "Dominik", ""], ["\u017bmuda", "Micha\u0142", ""], ["Bujak", "Artur", ""]]}, {"id": "1911.11139", "submitter": "Amin Omidvar", "authors": "Amin Omidvar, Hossein Poormodheji, Aijun An, Gordon Edall", "title": "Learning to Determine the Quality of News Headlines", "comments": "10 Pages, Accepted at the 12th International Conference on Agents and\n  Artificial Intelligence (ICAART) 2020", "journal-ref": null, "doi": "10.5220/0009367504010409", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, most newsreaders read the online version of news articles rather than\ntraditional paper-based newspapers. Also, news media publishers rely heavily on\nthe income generated from subscriptions and website visits made by newsreaders.\nThus, online user engagement is a very important issue for online newspapers.\nMuch effort has been spent on writing interesting headlines to catch the\nattention of online users. On the other hand, headlines should not be\nmisleading (e.g., clickbaits); otherwise, readers would be disappointed when\nreading the content. In this paper, we propose four indicators to determine the\nquality of published news headlines based on their click count and dwell time,\nwhich are obtained by website log analysis. Then, we use soft target\ndistribution of the calculated quality indicators to train our proposed deep\nlearning model which can predict the quality of unpublished news headlines. The\nproposed model not only processes the latent features of both headline and body\nof the article to predict its headline quality but also considers the semantic\nrelation between headline and body as well. To evaluate our model, we use a\nreal dataset from a major Canadian newspaper. Results show our proposed model\noutperforms other state-of-the-art NLP models.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 00:09:30 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 23:42:19 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Omidvar", "Amin", ""], ["Poormodheji", "Hossein", ""], ["An", "Aijun", ""], ["Edall", "Gordon", ""]]}, {"id": "1911.11214", "submitter": "Naeemul Hassan", "authors": "Sima Bhowmik, Md Main Uddin Rony, Md Mahfuzul Haque, Kristen Alley\n  Swain, Naeemul Hassan", "title": "Examining the Role of Clickbait Headlines to Engage Readers with\n  Reliable Health-related Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clickbait headlines are frequently used to attract readers to read articles.\nAlthough this headline type has turned out to be a technique to engage readers\nwith misleading items, it is still unknown whether the technique can be used to\nattract readers to reliable pieces. This study takes the opportunity to test\nits efficacy to engage readers with reliable health articles. A set of online\nsurveys would be conducted to test readers' engagement with and perception\nabout clickbait headlines with reliable articles. After that, we would design\nan automation system to generate clickabit headlines to maximize user\nengagement.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 20:29:01 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Bhowmik", "Sima", ""], ["Rony", "Md Main Uddin", ""], ["Haque", "Md Mahfuzul", ""], ["Swain", "Kristen Alley", ""], ["Hassan", "Naeemul", ""]]}, {"id": "1911.11240", "submitter": "Julian Risch", "authors": "Julian Risch and Ralf Krestel", "title": "My Approach = Your Apparatus? Entropy-Based Topic Modeling on Multiple\n  Domain-Specific Text Collections", "comments": null, "journal-ref": "Proceedings of the 18th ACM/IEEE Joint Conference on Digital\n  Libraries (JCDL). bll. 283-292 (2018)", "doi": "10.1145/3197026.3197038", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparative text mining extends from genre analysis and political bias\ndetection to the revelation of cultural and geographic differences, through to\nthe search for prior art across patents and scientific papers. These\napplications use cross-collection topic modeling for the exploration,\nclustering, and comparison of large sets of documents, such as digital\nlibraries. However, topic modeling on documents from different collections is\nchallenging because of domain-specific vocabulary. We present a\ncross-collection topic model combined with automatic domain term extraction and\nphrase segmentation. This model distinguishes collection-specific and\ncollection-independent words based on information entropy and reveals\ncommonalities and differences of multiple text collections. We evaluate our\nmodel on patents, scientific papers, newspaper articles, forum posts, and\nWikipedia articles. In comparison to state-of-the-art cross-collection topic\nmodeling, our model achieves up to 13% higher topic coherence, up to 4% lower\nperplexity, and up to 31% higher document classification accuracy. More\nimportantly, our approach is the first topic model that ensures disjunct\ngeneral and specific word distributions, resulting in clear-cut topic\nrepresentations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 21:29:59 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Risch", "Julian", ""], ["Krestel", "Ralf", ""]]}, {"id": "1911.11358", "submitter": "Saurav Manchanda", "authors": "Saurav Manchanda and George Karypis", "title": "CAWA: An Attention-Network for Credit Attribution", "comments": "To appear in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Credit attribution is the task of associating individual parts in a document\nwith their most appropriate class labels. It is an important task with\napplications to information retrieval and text summarization. When labeled\ntraining data is available, traditional approaches for sequence tagging can be\nused for credit attribution. However, generating such labeled datasets is\nexpensive and time-consuming. In this paper, we present \"Credit Attribution\nWith Attention (CAWA)\", a neural-network-based approach, that instead of using\nsentence-level labeled data, uses the set of class labels that are associated\nwith an entire document as a source of distant-supervision. CAWA combines an\nattention mechanism with a multilabel classifier into an end-to-end learning\nframework to perform credit attribution. CAWA labels the individual sentences\nfrom the input document using the resultant attention-weights. CAWA improves\nupon the state-of-the-art credit attribution approach by not constraining a\nsentence to belong to just one class, but modeling each sentence as a\ndistribution over all classes, leading to better modeling of\nsemantically-similar classes. Experiments on the credit attribution task on a\nvariety of datasets show that the sentence class labels generated by CAWA\noutperform the competing approaches. Additionally, on the multilabel text\nclassification task, CAWA performs better than the competing credit attribution\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 06:02:33 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Manchanda", "Saurav", ""], ["Karypis", "George", ""]]}, {"id": "1911.11403", "submitter": "Preslav Nakov", "authors": "Preslav Nakov, Llu\\'is M\\`arquez, Walid Magdy, Alessandro Moschitti,\n  James Glass, Bilal Randeree", "title": "SemEval-2015 Task 3: Answer Selection in Community Question Answering", "comments": "community question answering, answer selection, English, Arabic", "journal-ref": "SemEval-2015", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community Question Answering (cQA) provides new interesting research\ndirections to the traditional Question Answering (QA) field, e.g., the\nexploitation of the interaction between users and the structure of related\nposts. In this context, we organized SemEval-2015 Task 3 on \"Answer Selection\nin cQA\", which included two subtasks: (a) classifying answers as \"good\", \"bad\",\nor \"potentially relevant\" with respect to the question, and (b) answering a\nYES/NO question with \"yes\", \"no\", or \"unsure\", based on the list of all\nanswers. We set subtask A for Arabic and English on two relatively different\ncQA domains, i.e., the Qatar Living website for English, and a Quran-related\nwebsite for Arabic. We used crowdsourcing on Amazon Mechanical Turk to label a\nlarge English training dataset, which we released to the research community.\nThirteen teams participated in the challenge with a total of 61 submissions: 24\nprimary and 37 contrastive. The best systems achieved an official score\n(macro-averaged F1) of 57.19 and 63.7 for the English subtasks A and B, and\n78.55 for the Arabic subtask A.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 08:40:49 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Nakov", "Preslav", ""], ["M\u00e0rquez", "Llu\u00eds", ""], ["Magdy", "Walid", ""], ["Moschitti", "Alessandro", ""], ["Glass", "James", ""], ["Randeree", "Bilal", ""]]}, {"id": "1911.11433", "submitter": "Anush Sankaran", "authors": "Ameya Prabhu, Riddhiman Dasgupta, Anush Sankaran, Srikanth\n  Tamilselvam, Senthil Mani", "title": "\"You might also like this model\": Data Driven Approach for Recommending\n  Deep Learning Models for Unknown Image Datasets", "comments": "NeurIPS 2019, New in ML Group", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an unknown (new) classification dataset, choosing an appropriate deep\nlearning architecture is often a recursive, time-taking, and laborious process.\nIn this research, we propose a novel technique to recommend a suitable\narchitecture from a repository of known models. Further, we predict the\nperformance accuracy of the recommended architecture on the given unknown\ndataset, without the need for training the model. We propose a model encoder\napproach to learn a fixed length representation of deep learning architectures\nalong with its hyperparameters, in an unsupervised fashion. We manually curate\na repository of image datasets with corresponding known deep learning models\nand show that the predicted accuracy is a good estimator of the actual\naccuracy. We discuss the implications of the proposed approach for three\nbenchmark images datasets and also the challenges in using the approach for\ntext modality. To further increase the reproducibility of the proposed\napproach, the entire implementation is made publicly available along with the\ntrained models.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 10:01:35 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 20:45:57 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Prabhu", "Ameya", ""], ["Dasgupta", "Riddhiman", ""], ["Sankaran", "Anush", ""], ["Tamilselvam", "Srikanth", ""], ["Mani", "Senthil", ""]]}, {"id": "1911.11473", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen, Dai Quoc Nguyen, Son Bao Pham, The Duy Bui", "title": "A Fast Template-based Approach to Automatically Identify Primary Text\n  Content of a Web Page", "comments": "In Proceedings of the 2009 International Conference on Knowledge and\n  Systems Engineering (KSE 2009)", "journal-ref": null, "doi": "10.1109/KSE.2009.39", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines have become an indispensable tool for browsing information on\nthe Internet. The user, however, is often annoyed by redundant results from\nirrelevant Web pages. One reason is because search engines also look at\nnon-informative blocks of Web pages such as advertisement, navigation links,\netc. In this paper, we propose a fast algorithm called FastContentExtractor to\nautomatically detect main content blocks in a Web page by improving the\nContentExtractor algorithm. By automatically identifying and storing templates\nrepresenting the structure of content blocks in a website, content blocks of a\nnew Web page from the Website can be extracted quickly. The hierarchical order\nof the output blocks is also maintained which guarantees that the extracted\ncontent blocks are in the same order as the original ones.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 11:49:16 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Nguyen", "Dat Quoc", ""], ["Nguyen", "Dai Quoc", ""], ["Pham", "Son Bao", ""], ["Bui", "The Duy", ""]]}, {"id": "1911.11486", "submitter": "Yue Wang", "authors": "Yue Wang, Chenwei Zhang, Shen Wang, Philip S. Yu, Lu Bai, Lixin Cui,\n  Guandong Xu", "title": "Generative Temporal Link Prediction via Self-tokenized Sequence Modeling", "comments": "accepted by World Wide Web Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize networks with evolving structures as temporal networks and\npropose a generative link prediction model, Generative Link Sequence Modeling\n(GLSM), to predict future links for temporal networks. GLSM captures the\ntemporal link formation patterns from the observed links with a sequence\nmodeling framework and has the ability to generate the emerging links by\ninferring from the probability distribution on the potential future links. To\navoid overfitting caused by treating each link as a unique token, we propose a\nself-tokenization mechanism to transform each raw link in the network to an\nabstract aggregation token automatically. The self-tokenization is seamlessly\nintegrated into the sequence modeling framework, which allows the proposed GLSM\nmodel to have the generalization capability to discover link formation patterns\nbeyond raw link sequences. We compare GLSM with the existing state-of-art\nmethods on five real-world datasets. The experimental results demonstrate that\nGLSM obtains future positive links effectively in a generative fashion while\nachieving the best performance (2-10\\% improvements on AUC) among other\nalternatives.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 12:14:01 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 13:17:26 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Wang", "Yue", ""], ["Zhang", "Chenwei", ""], ["Wang", "Shen", ""], ["Yu", "Philip S.", ""], ["Bai", "Lu", ""], ["Cui", "Lixin", ""], ["Xu", "Guandong", ""]]}, {"id": "1911.11543", "submitter": "Shruti Jadon", "authors": "Tanvi Sahay, Ankita Mehta, Shruti Jadon", "title": "Schema Matching using Machine Learning", "comments": "7 pages, 2 figures, 2 tables", "journal-ref": null, "doi": "10.1109/SPIN48934.2020.9071272", "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Schema Matching is a method of finding attributes that are either similar to\neach other linguistically or represent the same information. In this project,\nwe take a hybrid approach at solving this problem by making use of both the\nprovided data and the schema name to perform one to one schema matching and\nintroduce the creation of a global dictionary to achieve one to many schema\nmatching. We experiment with two methods of one to one matching and compare\nboth based on their F-scores, precision, and recall. We also compare our method\nwith the ones previously suggested and highlight differences between them.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 02:40:09 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Sahay", "Tanvi", ""], ["Mehta", "Ankita", ""], ["Jadon", "Shruti", ""]]}, {"id": "1911.11623", "submitter": "Dat Quoc Nguyen", "authors": "Tien-Thanh Vu, Dat Quoc Nguyen", "title": "A Vietnamese information retrieval system for product-price", "comments": "In Proceedings of the 2011 IEEE International Conference on Granular\n  Computing (GrC 2011)", "journal-ref": null, "doi": "10.1109/GRC.2011.6122681", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A price information retrieval (IR) system allows users to search and view\ndifferences among prices of specific products. Building product-price driven IR\nsystem is a challenging and active research area. Approaches entirely depending\nproducts information provided by shops via interface environment encounter\nlimitations of database. While automatic systems specifically require product\nnames and commercial websites for their input. For both paradigms, approaches\nof building product-price IR system for Vietnamese are still very limited. In\nthis paper, we introduce an automatic Vietnamese IR system for product-price by\nidentifying and storing Xpath patterns to extract prices of products from\ncommercial websites. Experiments of our system show promising results.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 15:14:40 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Vu", "Tien-Thanh", ""], ["Nguyen", "Dat Quoc", ""]]}, {"id": "1911.11698", "submitter": "Emeric Dynomant", "authors": "Emeric Dynomant, St\\'efan J. Darmoni, \\'Emeline Lejeune, Ga\\\"etan\n  Kerdelhu\\'e, Jean-Philippe Leroy, Vincent Lequertier, St\\'ephane Canu, Julien\n  Grosjean", "title": "Doc2Vec on the PubMed corpus: study of a new approach to generate\n  related articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PubMed is the biggest and most used bibliographic database worldwide, hosting\nmore than 26M biomedical publications. One of its useful features is the\n\"similar articles\" section, allowing the end-user to find scientific articles\nlinked to the consulted document in term of context. The aim of this study is\nto analyze whether it is possible to replace the statistic model PubMed Related\nArticles (pmra) with a document embedding method. Doc2Vec algorithm was used to\ntrain models allowing to vectorize documents. Six of its parameters were\noptimised by following a grid-search strategy to train more than 1,900 models.\nParameters combination leading to the best accuracy was used to train models on\nabstracts from the PubMed database. Four evaluations tasks were defined to\ndetermine what does or does not influence the proximity between documents for\nboth Doc2Vec and pmra. The two different Doc2Vec architectures have different\nabilities to link documents about a common context. The terminological\nindexing, words and stems contents of linked documents are highly similar\nbetween pmra and Doc2Vec PV-DBOW architecture. These algorithms are also more\nlikely to bring closer documents having a similar size. In contrary, the manual\nevaluation shows much better results for the pmra algorithm. While the pmra\nalgorithm links documents by explicitly using terminological indexing in its\nformula, Doc2Vec does not need a prior indexing. It can infer relations between\ndocuments sharing a similar indexing, without any knowledge about them,\nparticularly regarding the PV-DBOW architecture. In contrary, the human\nevaluation, without any clear agreement between evaluators, implies future\nstudies to better understand this difference between PV-DBOW and pmra\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 17:06:02 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Dynomant", "Emeric", ""], ["Darmoni", "St\u00e9fan J.", ""], ["Lejeune", "\u00c9meline", ""], ["Kerdelhu\u00e9", "Ga\u00ebtan", ""], ["Leroy", "Jean-Philippe", ""], ["Lequertier", "Vincent", ""], ["Canu", "St\u00e9phane", ""], ["Grosjean", "Julien", ""]]}, {"id": "1911.11750", "submitter": "Nino Arsov", "authors": "Nino Arsov, Milan Dukovski, Blagoja Evkoski, Stefan Cvetkovski", "title": "A Measure of Similarity in Textual Data Using Spearman's Rank\n  Correlation Coefficient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, many diverse advances have occurred in the field of\ninformation extraction from data. Information extraction in its simplest form\ntakes place in computing environments, where structured data can be extracted\nthrough a series of queries. The continuous expansion of quantities of data\nhave therefore provided an opportunity for knowledge extraction (KE) from a\ntextual document (TD). A typical problem of this kind is the extraction of\ncommon characteristics and knowledge from a group of TDs, with the possibility\nto group such similar TDs in a process known as clustering. In this paper we\npresent a technique for such KE among a group of TDs related to the common\ncharacteristics and meaning of their content. Our technique is based on the\nSpearman's Rank Correlation Coefficient (SRCC), for which the conducted\nexperiments have proven to be comprehensive measure to achieve a high-quality\nKE.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 18:38:59 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Arsov", "Nino", ""], ["Dukovski", "Milan", ""], ["Evkoski", "Blagoja", ""], ["Cvetkovski", "Stefan", ""]]}, {"id": "1911.11807", "submitter": "Florian Hartmann", "authors": "Florian Hartmann, Sunah Suh, Arkadiusz Komarzewski, Tim D. Smith,\n  Ilana Segall", "title": "Federated Learning for Ranking Browser History Suggestions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning is a new subfield of machine learning that allows fitting\nmodels without collecting the training data itself. Instead of sharing data,\nusers collaboratively train a model by only sending weight updates to a server.\nTo improve the ranking of suggestions in the Firefox URL bar, we make use of\nFederated Learning to train a model on user interactions in a\nprivacy-preserving way. This trained model replaces a handcrafted heuristic,\nand our results show that users now type over half a character less to find\nwhat they are looking for. To be able to deploy our system to real users\nwithout degrading their experience during training, we design the optimization\nprocess to be robust. To this end, we use a variant of Rprop for optimization,\nand implement additional safeguards. By using a numerical gradient\napproximation technique, our system is able to optimize anything in Firefox\nthat is currently based on handcrafted heuristics. Our paper shows that\nFederated Learning can be used successfully to train models in\nprivacy-respecting ways.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 19:45:28 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Hartmann", "Florian", ""], ["Suh", "Sunah", ""], ["Komarzewski", "Arkadiusz", ""], ["Smith", "Tim D.", ""], ["Segall", "Ilana", ""]]}, {"id": "1911.11942", "submitter": "Ruihong Qiu", "authors": "Ruihong Qiu, Jingjing Li, Zi Huang, Hongzhi Yin", "title": "Rethinking the Item Order in Session-based Recommendation with Graph\n  Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3357384.3358010", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting a user's preference in a short anonymous interaction session\ninstead of long-term history is a challenging problem in the real-life\nsession-based recommendation, e.g., e-commerce and media stream. Recent\nresearch of the session-based recommender system mainly focuses on sequential\npatterns by utilizing the attention mechanism, which is straightforward for the\nsession's natural sequence sorted by time. However, the user's preference is\nmuch more complicated than a solely consecutive time pattern in the transition\nof item choices. In this paper, therefore, we study the item transition pattern\nby constructing a session graph and propose a novel model which collaboratively\nconsiders the sequence order and the latent order in the session graph for a\nsession-based recommender system. We formulate the next item recommendation\nwithin the session as a graph classification problem. Specifically, we propose\na weighted attention graph layer and a Readout function to learn embeddings of\nitems and sessions for the next item recommendation. Extensive experiments have\nbeen conducted on two benchmark E-commerce datasets, Yoochoose and Diginetica,\nand the experimental results show that our model outperforms other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 04:10:02 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 12:02:41 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Qiu", "Ruihong", ""], ["Li", "Jingjing", ""], ["Huang", "Zi", ""], ["Yin", "Hongzhi", ""]]}, {"id": "1911.12085", "submitter": "Preslav Nakov", "authors": "Su Nam Kim, Preslav Nakov", "title": "Large-Scale Noun Compound Interpretation Using Bootstrapping and the Web\n  as a Corpus", "comments": "noun compounds, paraphrasing verbs, paraphrases, semantic\n  interpretation, bootstrapping, semi-supervised learning", "journal-ref": "EMNLP-2011", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Responding to the need for semantic lexical resources in natural language\nprocessing applications, we examine methods to acquire noun compounds (NCs),\ne.g., \"orange juice\", together with suitable fine-grained semantic\ninterpretations, e.g., \"squeezed from\", which are directly usable as\nparaphrases. We employ bootstrapping and web statistics, and utilize the\nrelationship between NCs and paraphrasing patterns to jointly extract NCs and\nsuch patterns in multiple alternating iterations. In evaluation, we found that\nhaving one compound noun fixed yields both a higher number of semantically\ninterpreted NCs and improved accuracy due to stronger semantic restrictions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 11:25:43 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Kim", "Su Nam", ""], ["Nakov", "Preslav", ""]]}, {"id": "1911.12091", "submitter": "Preslav Nakov", "authors": "Liane Guillou, Christian Hardmeier, Preslav Nakov, Sara Stymne, J\\\"org\n  Tiedemann, Yannick Versley, Mauro Cettolo, Bonnie Webber, Andrei\n  Popescu-Belis", "title": "Findings of the 2016 WMT Shared Task on Cross-lingual Pronoun Prediction", "comments": "cross-lingual pronoun prediction, WMT, shared task, English, German,\n  French", "journal-ref": "WMT-2016", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the design, the evaluation setup, and the results of the 2016 WMT\nshared task on cross-lingual pronoun prediction. This is a classification task\nin which participants are asked to provide predictions on what pronoun class\nlabel should replace a placeholder value in the target-language text, provided\nin lemmatised and PoS-tagged form. We provided four subtasks, for the\nEnglish-French and English-German language pairs, in both directions. Eleven\nteams participated in the shared task; nine for the English-French subtask,\nfive for French-English, nine for English-German, and six for German-English.\nMost of the submissions outperformed two strong language-model based baseline\nsystems, with systems using deep recurrent neural networks outperforming those\nusing other architectures for most language pairs.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 11:45:15 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Guillou", "Liane", ""], ["Hardmeier", "Christian", ""], ["Nakov", "Preslav", ""], ["Stymne", "Sara", ""], ["Tiedemann", "J\u00f6rg", ""], ["Versley", "Yannick", ""], ["Cettolo", "Mauro", ""], ["Webber", "Bonnie", ""], ["Popescu-Belis", "Andrei", ""]]}, {"id": "1911.12481", "submitter": "Da Xu", "authors": "Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan", "title": "Product Knowledge Graph Embedding for E-commerce", "comments": null, "journal-ref": null, "doi": "10.1145/3336191.3371778", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new product knowledge graph (PKG) embedding\napproach for learning the intrinsic product relations as product knowledge for\ne-commerce. We define the key entities and summarize the pivotal product\nrelations that are critical for general e-commerce applications including\nmarketing, advertisement, search ranking and recommendation. We first provide a\ncomprehensive comparison between PKG and ordinary knowledge graph (KG) and then\nillustrate why KG embedding methods are not suitable for PKG learning. We\nconstruct a self-attention-enhanced distributed representation learning model\nfor learning PKG embeddings from raw customer activity data in an end-to-end\nfashion. We design an effective multi-task learning schema to fully leverage\nthe multi-modal e-commerce data. The Poincare embedding is also employed to\nhandle complex entity structures. We use a real-world dataset from\ngrocery.walmart.com to evaluate the performances on knowledge completion,\nsearch ranking and recommendation. The proposed approach compares favourably to\nbaselines in knowledge completion and downstream tasks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 01:53:47 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Xu", "Da", ""], ["Ruan", "Chuanwei", ""], ["Korpeoglu", "Evren", ""], ["Kumar", "Sushant", ""], ["Achan", "Kannan", ""]]}, {"id": "1911.12544", "submitter": "Preslav Nakov", "authors": "Veselin Raychev, Preslav Nakov", "title": "Language-Independent Sentiment Analysis Using Subjectivity and\n  Positional Information", "comments": "sentiment analysis, subjectivity", "journal-ref": "RANLP-2009", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel language-independent approach to the task of determining\nthe polarity, positive or negative, of the author's opinion on a specific topic\nin natural language text. In particular, weights are assigned to attributes,\nindividual words or word bi-grams, based on their position and on their\nlikelihood of being subjective. The subjectivity of each attribute is estimated\nin a two-step process, where first the probability of being subjective is\ncalculated for each sentence containing the attribute, and then these\nprobabilities are used to alter the attribute's weights for polarity\nclassification. The evaluation results on a standard dataset of movie reviews\nshows 89.85% classification accuracy, which rivals the best previously\npublished results for this dataset for systems that use no additional\nlinguistic information nor external resources.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 05:55:44 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Raychev", "Veselin", ""], ["Nakov", "Preslav", ""]]}, {"id": "1911.12559", "submitter": "Florian Boudin", "authors": "Ygor Gallina and Florian Boudin and B\\'eatrice Daille", "title": "KPTimes: A Large-Scale Dataset for Keyphrase Generation on News\n  Documents", "comments": "Accepted at the International Conference on Natural Language\n  Generation (INLG), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Keyphrase generation is the task of predicting a set of lexical units that\nconveys the main content of a source text. Existing datasets for keyphrase\ngeneration are only readily available for the scholarly domain and include\nnon-expert annotations. In this paper we present KPTimes, a large-scale dataset\nof news texts paired with editor-curated keyphrases. Exploring the dataset, we\nshow how editors tag documents, and how their annotations differ from those\nfound in existing datasets. We also train and evaluate state-of-the-art neural\nkeyphrase generation models on KPTimes to gain insights on how well they\nperform on the news domain. The dataset is available online at\nhttps://github.com/ygorg/KPTimes .\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 07:12:30 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Gallina", "Ygor", ""], ["Boudin", "Florian", ""], ["Daille", "B\u00e9atrice", ""]]}, {"id": "1911.12618", "submitter": "Jaime Ram\\'irez Castillo", "authors": "Jaime Ram\\'irez, M. Julia Flores", "title": "Machine learning for music genre: multifaceted review and\n  experimentation with audioset", "comments": null, "journal-ref": null, "doi": "10.1007/s10844-019-00582-9", "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music genre classification is one of the sub-disciplines of music information\nretrieval (MIR) with growing popularity among researchers, mainly due to the\nalready open challenges. Although research has been prolific in terms of number\nof published works, the topic still suffers from a problem in its foundations:\nthere is no clear and formal definition of what genre is. Music categorizations\nare vague and unclear, suffering from human subjectivity and lack of agreement.\nIn its first part, this paper offers a survey trying to cover the many\ndifferent aspects of the matter. Its main goal is give the reader an overview\nof the history and the current state-of-the-art, exploring techniques and\ndatasets used to the date, as well as identifying current challenges, such as\nthis ambiguity of genre definitions or the introduction of human-centric\napproaches. The paper pays special attention to new trends in machine learning\napplied to the music annotation problem. Finally, we also include a music genre\nclassification experiment that compares different machine learning models using\nAudioset.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 09:57:28 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ram\u00edrez", "Jaime", ""], ["Flores", "M. Julia", ""]]}, {"id": "1911.12637", "submitter": "Carlos Badenes-Olmedo", "authors": "Carlos Badenes-Olmedo, Jose-Luis Redondo-Garcia and Oscar Corcho", "title": "Legal document retrieval across languages: topic hierarchies based on\n  synsets", "comments": "IberLegal Workshop co-located with Jurix 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-lingual annotations of legislative texts enable us to explore major\nthemes covered in multilingual legal data and are a key facilitator of semantic\nsimilarity when searching for similar documents. Multilingual probabilistic\ntopic models have recently emerged as a group of semi-supervised machine\nlearning models that can be used to perform thematic explorations on\ncollections of texts in multiple languages. However, these approaches require\ntheme-aligned training data to create a language-independent space, which\nlimits the amount of scenarios where this technique can be used. In this work,\nwe provide an unsupervised document similarity algorithm based on hierarchies\nof multi-lingual concepts to describe topics across languages. The algorithm\ndoes not require parallel or comparable corpora, or any other type of\ntranslation resource. Experiments performed on the English, Spanish, French and\nPortuguese editions of JCR-Acquis corpora reveal promising results on\nclassifying and sorting documents by similar content.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 10:49:36 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Badenes-Olmedo", "Carlos", ""], ["Redondo-Garcia", "Jose-Luis", ""], ["Corcho", "Oscar", ""]]}, {"id": "1911.12848", "submitter": "Sonali Rajesh Shah", "authors": "Sonali Rajesh Shah (1) and Abhishek Kaushik (1) ((1) Dublin Business\n  School)", "title": "Sentiment Analysis On Indian Indigenous Languages: A Review On\n  Multilingual Opinion Mining", "comments": null, "journal-ref": null, "doi": "10.20944/preprints201911.0338.v1", "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increase in the use of smartphones has laid to the use of the internet and\nsocial media platforms. The most commonly used social media platforms are\nTwitter, Facebook, WhatsApp and Instagram. People are sharing their personal\nexperiences, reviews, feedbacks on the web. The information which is available\non the web is unstructured and enormous. Hence, there is a huge scope of\nresearch on understanding the sentiment of the data available on the web.\nSentiment Analysis (SA) can be carried out on the reviews, feedbacks,\ndiscussions available on the web. There has been extensive research carried out\non SA in the English language, but data on the web also contains different\nother languages which should be analyzed. This paper aims to analyze, review\nand discuss the approaches, algorithms, challenges faced by the researchers\nwhile carrying out the SA on Indigenous languages.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 20:00:40 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Shah", "Sonali Rajesh", ""], ["Kaushik", "Abhishek", ""]]}, {"id": "1911.12866", "submitter": "Yunan Zhang", "authors": "Yunan Zhang, Heting Gao, Tarek Abdelzaher", "title": "Macross: Urban Dynamics Modeling based on Metapath Guided Cross-Modal\n  Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the ongoing rapid urbanization takes place with an ever-increasing speed,\nfully modeling urban dynamics becomes more and more challenging, but also a\nnecessity for socioeconomic development. It is challenging because human\nactivities and constructions are ubiquitous; urban landscape and life content\nchange anywhere and anytime. It's crucial due to the fact that only up-to-date\nurban dynamics can enable governors to optimize their city planning strategy\nand help individuals organize their daily lives in a more efficient way.\nPrevious geographic topic model based methods attempt to solve this problem but\nsuffer from high computational cost and memory consumption, limiting their\nscalability to city level applications. Also, strong prior assumptions make\nsuch models fail to capture certain patterns by nature.\n  To bridge the gap, we propose Macross, a metapath guided embedding approach\nto jointly model location, time and text information. Given a dataset of\ngeo-tagged social media posts, we extract and aggregate location and time and\nconstruct a heterogeneous information network using the aggregated space and\ntime. Metapath2vec based approach is used to construct vector representations\nfor times, locations and frequent words such that co-occurrence pairs of nodes\nare closer in latent space. The vector representations will be used to infer\nrelated time, locations or keywords for a user query. Experiments done on\nenormous datasets show our model can generate comparable if not better quality\nquery results compared to state of the art models and outperform some\ncutting-edge models for activity recovery and classification.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 21:09:52 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Zhang", "Yunan", ""], ["Gao", "Heting", ""], ["Abdelzaher", "Tarek", ""]]}, {"id": "1911.13096", "submitter": "Rujing Yao", "authors": "Rujing Yao, Linlin Hou, Yingchun Ye, Ou Wu, Ji Zhang, Jian Wu", "title": "Method and Dataset Mining in Scientific Papers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literature analysis facilitates researchers better understanding the\ndevelopment of science and technology. The conventional literature analysis\nfocuses on the topics, authors, abstracts, keywords, references, etc., and\nrarely pays attention to the content of papers. In the field of machine\nlearning, the involved methods (M) and datasets (D) are key information in\npapers. The extraction and mining of M and D are useful for discipline analysis\nand algorithm recommendation. In this paper, we propose a novel entity\nrecognition model, called MDER, and constructe datasets from the papers of the\nPAKDD conferences (2009-2019). Some preliminary experiments are conducted to\nassess the extraction performance and the mining results are visualized.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 13:19:45 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Yao", "Rujing", ""], ["Hou", "Linlin", ""], ["Ye", "Yingchun", ""], ["Wu", "Ou", ""], ["Zhang", "Ji", ""], ["Wu", "Jian", ""]]}, {"id": "1911.13279", "submitter": "Saikat Chakraborty", "authors": "Saikat Chakraborty", "title": "A Graph-based Ranking Approach to Extract Key-frames for Static Video\n  Summarization", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video abstraction has become one of the efficient approaches to grasp the\ncontent of a video without seeing it entirely. Key frame-based static video\nsummarization falls under this category. In this paper, we propose a\ngraph-based approach which summarizes the video with best user satisfaction. We\ntreated each video frame as a node of the graph and assigned a rank to each\nnode by our proposed VidRank algorithm. We developed three different models of\nVidRank algorithm and performed a comparative study on those models. A\ncomprehensive evaluation of 50 videos from open video database using objective\nand semi-objective measures indicates the superiority of our static video\nsummary generation method.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 18:24:44 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Chakraborty", "Saikat", ""]]}]