[{"id": "1405.0190", "submitter": "Klesti Hoxha", "authors": "Klesti Hoxha, Alda Kika, Eriglen Gani, Silvana Greca", "title": "Towards a Modular Recommender System for Research Papers written in\n  Albanian", "comments": "8 pages", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), Volume 5 Issue 4, 2014", "doi": "10.14569/IJACSA.2014.050423", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years there has been an increase in scientific papers\npublications in Albania and its neighboring countries that have large\ncommunities of Albanian speaking researchers. Many of these papers are written\nin Albanian. It is a very time consuming task to find papers related to the\nresearchers' work, because there is no concrete system that facilitates this\nprocess. In this paper we present the design of a modular intelligent search\nsystem for articles written in Albanian. The main part of it is the recommender\nmodule that facilitates searching by providing relevant articles to the users\n(in comparison with a given one). We used a cosine similarity based heuristics\nthat differentiates the importance of term frequencies based on their location\nin the article. We did not notice big differences on the recommendation results\nwhen using different combinations of the importance factors of the keywords,\ntitle, abstract and body. We got similar results when using only the title and\nabstract in comparison with the other combinations. Because we got fairly good\nresults in this initial approach, we believe that similar recommender systems\nfor documents written in Albanian can be build also in contexts not related to\nscientific publishing.\n", "versions": [{"version": "v1", "created": "Thu, 1 May 2014 15:32:09 GMT"}], "update_date": "2014-05-02", "authors_parsed": [["Hoxha", "Klesti", ""], ["Kika", "Alda", ""], ["Gani", "Eriglen", ""], ["Greca", "Silvana", ""]]}, {"id": "1405.0546", "submitter": "Antti Puurula", "authors": "Antti Puurula, Jesse Read, Albert Bifet", "title": "Kaggle LSHTC4 Winning Solution", "comments": "Kaggle LSHTC winning solution description", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our winning submission to the 2014 Kaggle competition for Large Scale\nHierarchical Text Classification (LSHTC) consists mostly of an ensemble of\nsparse generative models extending Multinomial Naive Bayes. The\nbase-classifiers consist of hierarchically smoothed models combining document,\nlabel, and hierarchy level Multinomials, with feature pre-processing using\nvariants of TF-IDF and BM25. Additional diversification is introduced by\ndifferent types of folds and random search optimization for different measures.\nThe ensemble algorithm optimizes macroFscore by predicting the documents for\neach label, instead of the usual prediction of labels per document. Scores for\ndocuments are predicted by weighted voting of base-classifier outputs with a\nvariant of Feature-Weighted Linear Stacking. The number of documents per label\nis chosen using label priors and thresholding of vote scores. This document\ndescribes the models and software used to build our solution. Reproducing the\nresults for our solution can be done by running the scripts included in the\nKaggle package. A package omitting precomputed result files is also\ndistributed. All code is open source, released under GNU GPL 2.0, and GPL 3.0\nfor Weka and Meka dependencies.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 01:41:27 GMT"}, {"version": "v2", "created": "Fri, 9 May 2014 04:57:19 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Puurula", "Antti", ""], ["Read", "Jesse", ""], ["Bifet", "Albert", ""]]}, {"id": "1405.0580", "submitter": "Prabhjot Kaur", "authors": "Prabhjot Kaur", "title": "Web Content Classification: A Survey", "comments": "5 pages, 1 figure. arXiv admin note: text overlap with\n  arXiv:1307.1024, arXiv:1310.4647 by other authors", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  V10(2):97-101, Apr 2014", "doi": "10.14445/22312803/IJCTT-V10P117", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the information contained within the web is increasing day by day,\norganizing this information could be a necessary requirement.The data mining\nprocess is to extract information from a data set and transform it into an\nunderstandable structure for further use. Classification of web page content is\nessential to many tasks in web information retrieval such as maintaining web\ndirectories and focused crawling.The uncontrolled type of nature of web content\npresents additional challenges to web page classification as compared to the\ntraditional text classification, but the interconnected nature of hypertext\nalso provides features that can assist the process. In this paper the web\nclassification is discussed in detail and its importance in field of data\nmining is explored.\n", "versions": [{"version": "v1", "created": "Sat, 3 May 2014 12:57:50 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Kaur", "Prabhjot", ""]]}, {"id": "1405.0647", "submitter": "Djamal Ziani", "authors": "Djamal Ziani", "title": "Feature Selection On Boolean Symbolic Objects", "comments": "20 pages, 10 figures", "journal-ref": "Ziani D., Feature Selection on Boolean Symbolic Objects, in\n  International Journal of Computational Sciences and Information Technology\n  (IJCSITY), November 2013, volume 1, Number 4, 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the boom in IT technology, the data sets used in application are more\nand more larger and are described by a huge number of attributes, therefore,\nthe feature selection become an important discipline in Knowledge discovery and\ndata mining, allowing the experts to select the most relevant features to\nimprove the quality of their studies and to reduce the time processing of their\nalgorithm. In addition to that, the data used by the applications become\nricher. They are now represented by a set of complex and structured objects,\ninstead of simple numerical matrixes. The purpose of our algorithm is to do\nfeature selection on rich data, called Boolean Symbolic Objects (BSOs). These\nobjects are described by multivalued features. The BSOs are considered as\nhigher level units which can model complex data, such as cluster of\nindividuals, aggregated data or taxonomies. In this paper we will introduce a\nnew feature selection criterion for BSOs, and we will explain how we improved\nits complexity.\n", "versions": [{"version": "v1", "created": "Sun, 4 May 2014 05:02:53 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Ziani", "Djamal", ""]]}, {"id": "1405.0749", "submitter": "Salman Hooshmand", "authors": "Seyed M. Mirtaheri, Mustafa Emre Din\\c{c}kt\\\"urk, Salman Hooshmand,\n  Gregor V. Bochmann, Guy-Vincent Jourdan, Iosif Viorel Onut", "title": "A Brief History of Web Crawlers", "comments": null, "journal-ref": "Proc. of CASCON 2013, Toronto, Nov. 2013", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web crawlers visit internet applications, collect data, and learn about new\nweb pages from visited pages. Web crawlers have a long and interesting history.\nEarly web crawlers collected statistics about the web. In addition to\ncollecting statistics about the web and indexing the applications for search\nengines, modern crawlers can be used to perform accessibility and vulnerability\nchecks on the application. Quick expansion of the web, and the complexity added\nto web applications have made the process of crawling a very challenging one.\nThroughout the history of web crawling many researchers and industrial groups\naddressed different issues and challenges that web crawlers face. Different\nsolutions have been proposed to reduce the time and cost of crawling.\nPerforming an exhaustive crawl is a challenging question. Additionally\ncapturing the model of a modern web application and extracting data from it\nautomatically is another open question. What follows is a brief history of\ndifferent technique and algorithms used from the early days of crawling up to\nthe recent days. We introduce criteria to evaluate the relative performance of\nweb crawlers. Based on these criteria we plot the evolution of web crawlers and\ncompare their performance\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 00:06:25 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Mirtaheri", "Seyed M.", ""], ["Din\u00e7kt\u00fcrk", "Mustafa Emre", ""], ["Hooshmand", "Salman", ""], ["Bochmann", "Gregor V.", ""], ["Jourdan", "Guy-Vincent", ""], ["Onut", "Iosif Viorel", ""]]}, {"id": "1405.0770", "submitter": "Yonghong Yu", "authors": "Yonghong Yu, Can Wang, Yang Gao", "title": "Attributes Coupling based Item Enhanced Matrix Factorization Technique\n  for Recommender Systems", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender system has attracted lots of attentions since it helps users\nalleviate the information overload problem. Matrix factorization technique is\none of the most widely employed collaborative filtering techniques in the\nresearch of recommender systems due to its effectiveness and efficiency in\ndealing with very large user-item rating matrices. Recently, based on the\nintuition that additional information provides useful insights for matrix\nfactorization techniques, several recommendation algorithms have utilized\nadditional information to improve the performance of matrix factorization\nmethods. However, the majority focus on dealing with the cold start user\nproblem and ignore the cold start item problem. In addition, there are few\nsuitable similarity measures for these content enhanced matrix factorization\napproaches to compute the similarity between categorical items. In this paper,\nwe propose attributes coupling based item enhanced matrix factorization method\nby incorporating item attribute information into matrix factorization technique\nas well as adapting the coupled object similarity to capture the relationship\nbetween items. Item attribute information is formed as an item relationship\nregularization term to regularize the process of matrix factorization.\nSpecifically, the similarity between items is measured by the Coupled Object\nSimilarity considering coupling between items. Experimental results on two real\ndata sets show that our proposed method outperforms state-of-the-art\nrecommendation algorithms and can effectively cope with the cold start item\nproblem when more item attribute information is available.\n", "versions": [{"version": "v1", "created": "Mon, 5 May 2014 02:36:52 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Yu", "Yonghong", ""], ["Wang", "Can", ""], ["Gao", "Yang", ""]]}, {"id": "1405.1486", "submitter": "Danai Koutra", "authors": "Danai Koutra, Paul Bennett, Eric Horvitz", "title": "Events and Controversies: Influences of a Shocking News Event on\n  Information Seeking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been suggested that online search and retrieval contributes to the\nintellectual isolation of users within their preexisting ideologies, where\npeople's prior views are strengthened and alternative viewpoints are\ninfrequently encountered. This so-called \"filter bubble\" phenomenon has been\ncalled out as especially detrimental when it comes to dialog among people on\ncontroversial, emotionally charged topics, such as the labeling of genetically\nmodified food, the right to bear arms, the death penalty, and online privacy.\nWe seek to identify and study information-seeking behavior and access to\nalternative versus reinforcing viewpoints following shocking, emotional, and\nlarge-scale news events. We choose for a case study to analyze search and\nbrowsing on gun control/rights, a strongly polarizing topic for both citizens\nand leaders of the United States. We study the period of time preceding and\nfollowing a mass shooting to understand how its occurrence, follow-on\ndiscussions, and debate may have been linked to changes in the patterns of\nsearching and browsing. We employ information-theoretic measures to quantify\nthe diversity of Web domains of interest to users and understand the browsing\npatterns of users. We use these measures to characterize the influence of news\nevents on these web search and browsing patterns.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 02:00:41 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Koutra", "Danai", ""], ["Bennett", "Paul", ""], ["Horvitz", "Eric", ""]]}, {"id": "1405.1511", "submitter": "Neha Gupta", "authors": "Neha Gupta, Ponnurangam Kumaraguru", "title": "Exploration of gaps in Bitly's spam detection and relevant counter\n  measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existence of spam URLs over emails and Online Social Media (OSM) has become a\ngrowing phenomenon. To counter the dissemination issues associated with long\ncomplex URLs in emails and character limit imposed on various OSM (like\nTwitter), the concept of URL shortening gained a lot of traction. URL\nshorteners take as input a long URL and give a short URL with the same landing\npage in return. With its immense popularity over time, it has become a prime\ntarget for the attackers giving them an advantage to conceal malicious content.\nBitly, a leading service in this domain is being exploited heavily to carry out\nphishing attacks, work from home scams, pornographic content propagation, etc.\nThis imposes additional performance pressure on Bitly and other URL shorteners\nto be able to detect and take a timely action against the illegitimate content.\nIn this study, we analyzed a dataset marked as suspicious by Bitly in the month\nof October 2013 to highlight some ground issues in their spam detection\nmechanism. In addition, we identified some short URL based features and coupled\nthem with two domain specific features to classify a Bitly URL as malicious /\nbenign and achieved a maximum accuracy of 86.41%. To the best of our knowledge,\nthis is the first large scale study to highlight the issues with Bitly's spam\ndetection policies and proposing a suitable countermeasure.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 06:02:40 GMT"}], "update_date": "2014-05-08", "authors_parsed": [["Gupta", "Neha", ""], ["Kumaraguru", "Ponnurangam", ""]]}, {"id": "1405.1740", "submitter": "Kutlu Emre  Yilmaz", "authors": "Kutlu Emre Y{\\i}lmaz, Ahmet Arslan, Ozgur Yilmazel", "title": "Turkish Text Retrieval Experiments Using Lemur Toolkit", "comments": "3 pages", "journal-ref": "IADIS AC 2009: Rome, Italy", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We used Lemur Toolkit, an open source toolkit designed for Information\nRetrieval (IR) research, for our automated indexing and retrieval experiments\non a TREC-like test collection for Turkish. We study and compare three\nretrieval models Lemur supports, especially Language modeling approach to IR,\ncombined with language specific preprocessing techniques. Our experiments show\nthat all retrieval models benefits from language specific preprocessing in\nterms of retrieval quality. Also Language Modeling approach is the best\nperforming retrieval model when language specific preprocessing applied.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 20:20:35 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Y\u0131lmaz", "Kutlu Emre", ""], ["Arslan", "Ahmet", ""], ["Yilmazel", "Ozgur", ""]]}, {"id": "1405.1814", "submitter": "Gayathri V", "authors": "Thangaraj M and Gayathri V", "title": "A Retrieval Mechanism for Multi-versioned Digital Collection Using TAG", "comments": "6 pages, 7 figures, Published with International Journal of Computer\n  & Organization Trends (IJCOT)", "journal-ref": "Dr. M. Thangaraj , V. Gayathri. \"A Retrieval Mechanism for\n  Multi-versioned Digital Collection Using TAG\", International Journal of\n  Computer & organization Trends (IJCOT),V7(1):14-19 April 2014. ISSN:2249-2593", "doi": "10.14445/22492593/IJCOT-V7P303", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the marvellous growth of the digital library in each year, the problems\nwith indexing and searching a digital library is increased in a high rate. When\nthe researchers search for the earlier versions, only a few recent versions in\nthe back volumes can be retrieved soon. It is unpredictable that researchers\nrequire the earlier versions in a specific boundary. In order to facilitate the\nresearchers, who may access any version at any time, we propose a VTAG\ntechnique for indexing. Our experiments indicate that the proposed retrieval\ntechnique, VTAG, effectively retrieves any version in considerable amount of\ntime than the existing method.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 06:48:03 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["M", "Thangaraj", ""], ["V", "Gayathri", ""]]}, {"id": "1405.1837", "submitter": "Emanuel Lacic", "authors": "Emanuel Lacic, Dominik Kowald, Lukas Eberhard, Christoph Trattner,\n  Denis Parra, Leandro Marinho", "title": "Utilizing Online Social Network and Location-Based Data to Recommend\n  Products and Categories in Online Marketplaces", "comments": "20 pages book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has unveiled the importance of online social networks for\nimproving the quality of recommender systems and encouraged the research\ncommunity to investigate better ways of exploiting the social information for\nrecommendations. To contribute to this sparse field of research, in this paper\nwe exploit users' interactions along three data sources (marketplace, social\nnetwork and location-based) to assess their performance in a barely studied\ndomain: recommending products and domains of interests (i.e., product\ncategories) to people in an online marketplace environment. To that end we\ndefined sets of content- and network-based user similarity features for each\ndata source and studied them isolated using an user-based Collaborative\nFiltering (CF) approach and in combination via a hybrid recommender algorithm,\nto assess which one provides the best recommendation performance.\nInterestingly, in our experiments conducted on a rich dataset collected from\nSecondLife, a popular online virtual world, we found that recommenders relying\non user similarity features obtained from the social network data clearly\nyielded the best results in terms of accuracy in case of predicting products,\nwhereas the features obtained from the marketplace and location-based data\nsources also obtained very good results in case of predicting categories. This\nfinding indicates that all three types of data sources are important and should\nbe taken into account depending on the level of specialization of the\nrecommendation task.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 08:43:55 GMT"}, {"version": "v2", "created": "Mon, 8 Sep 2014 07:48:08 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Lacic", "Emanuel", ""], ["Kowald", "Dominik", ""], ["Eberhard", "Lukas", ""], ["Trattner", "Christoph", ""], ["Parra", "Denis", ""], ["Marinho", "Leandro", ""]]}, {"id": "1405.1842", "submitter": "Emanuel Laci\\'c", "authors": "Emanuel Lacic, Dominik Kowald, Christoph Trattner", "title": "SocRecM: A Scalable Social Recommender Engine for Online Marketplaces", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present work-in-progress on SocRecM, a novel social\nrecommendation framework for online marketplaces. We demonstrate that SocRecM\nis not only easy to integrate with existing Web technologies through a RESTful,\nscalable and easy-to-extend service-based architecture but also reveal the\nextent to which various social features and recommendation approaches are\nuseful in an online social marketplace environment.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 09:00:25 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Lacic", "Emanuel", ""], ["Kowald", "Dominik", ""], ["Trattner", "Christoph", ""]]}, {"id": "1405.2048", "submitter": "Jeffrey Sukharev", "authors": "Jeffrey Sukharev, Leonid Zhukov, Alexandrin Popescul", "title": "Learning Alternative Name Spellings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Name matching is a key component of systems for entity resolution or record\nlinkage. Alternative spellings of the same names are a com- mon occurrence in\nmany applications. We use the largest collection of genealogy person records in\nthe world together with user search query logs to build name matching models.\nThe procedure for building a crowd-sourced training set is outlined together\nwith the presentation of our method. We cast the problem of learning\nalternative spellings as a machine translation problem at the character level.\nWe use in- formation retrieval evaluation methodology to show that this method\nsubstantially outperforms on our data a number of standard well known phonetic\nand string similarity methods in terms of precision and re- call. Additionally,\nwe rigorously compare the performance of standard methods when compared with\neach other. Our result can lead to a significant practical impact in entity\nresolution applications.\n", "versions": [{"version": "v1", "created": "Wed, 7 May 2014 19:47:51 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Sukharev", "Jeffrey", ""], ["Zhukov", "Leonid", ""], ["Popescul", "Alexandrin", ""]]}, {"id": "1405.2210", "submitter": "Dirk Lewandowski", "authors": "Dirk Lewandowski", "title": "Evaluating the retrieval effectiveness of Web search engines using a\n  representative query sample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engine retrieval effectiveness studies are usually small-scale, using\nonly limited query samples. Furthermore, queries are selected by the\nresearchers. We address these issues by taking a random representative sample\nof 1,000 informational and 1,000 navigational queries from a major German\nsearch engine and comparing Google's and Bing's results based on this sample.\nJurors were found through crowdsourcing, data was collected using specialised\nsoftware, the Relevance Assessment Tool (RAT). We found that while Google\noutperforms Bing in both query types, the difference in the performance for\ninformational queries was rather low. However, for navigational queries, Google\nfound the correct answer in 95.3 per cent of cases whereas Bing only found the\ncorrect answer 76.6 per cent of the time. We conclude that search engine\nperformance on navigational queries is of great importance, as users in this\ncase can clearly identify queries that have returned correct results. So,\nperformance on this query type may contribute to explaining user satisfaction\nwith search engines.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2014 12:06:55 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Lewandowski", "Dirk", ""]]}, {"id": "1405.2212", "submitter": "Dirk Lewandowski", "authors": "Dirk Lewandowski", "title": "Why we need an independent index of the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The path to greater diversity, as we have seen, cannot be achieved by merely\nhoping for a new search engine nor will government support for a single\nalternative achieve this goal. What is instead required is to create the\nconditions that will make establishing such a search engine possible in the\nfirst place. I describe how building and maintaining a proprietary index is the\ngreatest deterrent to such an undertaking. We must first overcome this\nobstacle. Doing so will still not solve the problem of the lack of diversity in\nthe search engine marketplace. But it may establish the conditions necessary to\nachieve that desired end.\n", "versions": [{"version": "v1", "created": "Fri, 9 May 2014 12:10:11 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Lewandowski", "Dirk", ""]]}, {"id": "1405.2386", "submitter": "Srayan Datta", "authors": "Srayan Datta", "title": "Predicting Central Topics in a Blog Corpus from a Networks Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's content-centric Internet, blogs are becoming increasingly popular\nand important from a data analysis perspective. According to Wikipedia, there\nwere over 156 million public blogs on the Internet as of February 2011. Blogs\nare a reflection of our contemporary society. The contents of different blog\nposts are important from social, psychological, economical and political\nperspectives. Discovery of important topics in the blogosphere is an area which\nstill needs much exploring. We try to come up with a procedure using\nprobabilistic topic modeling and network centrality measures which identifies\nthe central topics in a blog corpus.\n", "versions": [{"version": "v1", "created": "Sat, 10 May 2014 03:43:14 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Datta", "Srayan", ""]]}, {"id": "1405.2584", "submitter": "Rahul Tejwani", "authors": "Rahul Tejwani (University at Buffalo)", "title": "Sentiment Analysis: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis (also known as opinion mining) refers to the use of\nnatural language processing, text analysis and computational linguistics to\nidentify and extract subjective information in source materials. Mining\nopinions expressed in the user generated content is a challenging yet\npractically very useful problem. This survey would cover various approaches and\nmethodology used in Sentiment Analysis and Opinion Mining in general. The focus\nwould be on Internet text like, Product review, tweets and other social media.\n", "versions": [{"version": "v1", "created": "Sun, 11 May 2014 21:05:28 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Tejwani", "Rahul", "", "University at Buffalo"]]}, {"id": "1405.3117", "submitter": "Georgiana Ifrim", "authors": "Bichen Shi, Georgiana Ifrim, Neil Hurley", "title": "Be In The Know: Connecting News Articles to Relevant Twitter\n  Conversations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of data-driven journalism, data analytics can deliver tools to\nsupport journalists in connecting to new and developing news stories, e.g., as\nechoed in micro-blogs such as Twitter, the new citizen-driven media. In this\npaper, we propose a framework for tracking and automatically connecting news\narticles to Twitter conversations as captured by Twitter hashtags. For example,\nsuch a system could alert journalists about news that get a lot of Twitter\nreaction, so that they can investigate those conversations for new developments\nin the story, promote their article to a set of interested consumers, or\ndiscover general sentiment towards the story. Mapping articles to appropriate\nhashtags is nevertheless very challenging, due to different language styles\nused in articles versus tweets, the streaming aspect of news and tweets, as\nwell as the user behavior when marking certain tweet-terms as hashtags. As a\ncase-study, we continuously track the RSS feeds of Irish Times news articles\nand a focused Twitter stream over a two months period, and present a system\nthat assigns hashtags to each article, based on its Twitter echo. We propose a\nmachine learning approach for classifying and ranking article-hashtag pairs.\nOur empirical study shows that our system delivers high precision for this\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 13 May 2014 11:55:11 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Shi", "Bichen", ""], ["Ifrim", "Georgiana", ""], ["Hurley", "Neil", ""]]}, {"id": "1405.3353", "submitter": "Minh-Quoc Nghiem", "authors": "Minh-Quoc Nghiem, Giovanni Yoko Kristianto, Goran Topic, Akiko Aizawa", "title": "Which one is better: presentation-based or content-based math search?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Mathematical content is a valuable information source and retrieving this\ncontent has become an important issue. This paper compares two searching\nstrategies for math expressions: presentation-based and content-based\napproaches. Presentation-based search uses state-of-the-art math search system\nwhile content-based search uses semantic enrichment of math expressions to\nconvert math expressions into their content forms and searching is done using\nthese content-based expressions. By considering the meaning of math\nexpressions, the quality of search system is improved over presentation-based\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 03:44:32 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Nghiem", "Minh-Quoc", ""], ["Kristianto", "Giovanni Yoko", ""], ["Topic", "Goran", ""], ["Aizawa", "Akiko", ""]]}, {"id": "1405.3518", "submitter": "Yoon Kim", "authors": "Yoon Kim, Owen Zhang", "title": "Credibility Adjusted Term Frequency: A Supervised Term Weighting Scheme\n  for Sentiment Analysis and Text Classification", "comments": null, "journal-ref": "Proceedings of the 5th Workshop on Computational Approaches to\n  Subjectivity, Sentiment and Social Media Analysis. June, 2014. 79--83", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a simple but novel supervised weighting scheme for adjusting term\nfrequency in tf-idf for sentiment analysis and text classification. We compare\nour method to baseline weighting schemes and find that it outperforms them on\nmultiple benchmarks. The method is robust and works well on both snippets and\nlonger documents.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 14:50:59 GMT"}, {"version": "v2", "created": "Sat, 28 Jun 2014 17:18:54 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Kim", "Yoon", ""], ["Zhang", "Owen", ""]]}, {"id": "1405.3557", "submitter": "Iaakov Exman", "authors": "Iaakov Exman, Gilad Amar and Ran Shaltiel", "title": "The Interestingness Tool for Search in the Web", "comments": "13 pages, 6 figures, 4 tables, 3rd SKY'2012 International Workshop on\n  Software Knowledge, Barcelona, Spain, October 2012, SciTePress, Portugal", "journal-ref": null, "doi": "10.5220/0004178900540063", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interestingness,as the composition of Relevance and Unexpectedness, has been\ntested by means of Web search cases studies and led to promising results. But\nfor thorough investigation and routine practical application one needs a\nflexible and robust tool. This work describes such an Interestingness based\nsearch tool, its software architecture and actual implementation. One of its\nflexibility traits is the choice of Interestingness functions: it may work with\nMatch-Mismatch and Tf-Idf, among other functions. The tool has been\nexperimentally verified by application to various domains of interest. It has\nbeen validated by comparison of results with those of commercial search engines\nand results from differing Interestingness functions.\n", "versions": [{"version": "v1", "created": "Wed, 14 May 2014 16:03:14 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Exman", "Iaakov", ""], ["Amar", "Gilad", ""], ["Shaltiel", "Ran", ""]]}, {"id": "1405.3726", "submitter": "Xi Qiu", "authors": "Xi Qiu and Christopher Stewart", "title": "Topic words analysis based on LDA model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network analysis (SNA), which is a research field describing and\nmodeling the social connection of a certain group of people, is popular among\nnetwork services. Our topic words analysis project is a SNA method to visualize\nthe topic words among emails from Obama.com to accounts registered in Columbus,\nOhio. Based on Latent Dirichlet Allocation (LDA) model, a popular topic model\nof SNA, our project characterizes the preference of senders for target group of\nreceptors. Gibbs sampling is used to estimate topic and word distribution. Our\ntraining and testing data are emails from the carbon-free server\nDatagreening.com. We use parallel computing tool BashReduce for word processing\nand generate related words under each latent topic to discovers typical\ninformation of political news sending specially to local Columbus receptors.\nRunning on two instances using paralleling tool BashReduce, our project\ncontributes almost 30% speedup processing the raw contents, comparing with\nprocessing contents on one instance locally. Also, the experimental result\nshows that the LDA model applied in our project provides precision rate 53.96%\nhigher than TF-IDF model finding target words, on the condition that\nappropriate size of topic words list is selected.\n", "versions": [{"version": "v1", "created": "Thu, 15 May 2014 02:15:01 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Qiu", "Xi", ""], ["Stewart", "Christopher", ""]]}, {"id": "1405.4095", "submitter": "Xuzhen Zhu", "authors": "Xuzhen Zhu, Hui Tian, Shimin Cai", "title": "Personalized recommendation with corrected similarity", "comments": "13 pages, 2 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:0805.4127 by other authors", "journal-ref": null, "doi": "10.1088/1742-5468/2014/07/P07004", "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized recommendation attracts a surge of interdisciplinary researches.\nEspecially, similarity based methods in applications of real recommendation\nsystems achieve great success. However, the computations of similarities are\noverestimated or underestimated outstandingly due to the defective strategy of\nunidirectional similarity estimation. In this paper, we solve this drawback by\nleveraging mutual correction of forward and backward similarity estimations,\nand propose a new personalized recommendation index, i.e., corrected similarity\nbased inference (CSI). Through extensive experiments on four benchmark\ndatasets, the results show a greater improvement of CSI in comparison with\nthese mainstream baselines. And the detailed analysis is presented to unveil\nand understand the origin of such difference between CSI and mainstream\nindices.\n", "versions": [{"version": "v1", "created": "Fri, 16 May 2014 08:50:59 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Zhu", "Xuzhen", ""], ["Tian", "Hui", ""], ["Cai", "Shimin", ""]]}, {"id": "1405.4402", "submitter": "Jia Zeng", "authors": "Yi Wang, Xuemin Zhao, Zhenlong Sun, Hao Yan, Lifeng Wang, Zhihui Jin,\n  Liubin Wang, Yang Gao, Ching Law and Jia Zeng", "title": "Peacock: Learning Long-Tail Topic Features for Industrial Applications", "comments": "23 pages, 11 figures, ACM Transactions on Intelligent Systems and\n  Technology, 2015", "journal-ref": "ACM Transactions on Intelligent Systems and Technology, Vol. 6,\n  No. 4, Article 47, 2015", "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet allocation (LDA) is a popular topic modeling technique in\nacademia but less so in industry, especially in large-scale applications\ninvolving search engine and online advertising systems. A main underlying\nreason is that the topic models used have been too small in scale to be useful;\nfor example, some of the largest LDA models reported in literature have up to\n$10^3$ topics, which cover difficultly the long-tail semantic word sets. In\nthis paper, we show that the number of topics is a key factor that can\nsignificantly boost the utility of topic-modeling systems. In particular, we\nshow that a \"big\" LDA model with at least $10^5$ topics inferred from $10^9$\nsearch queries can achieve a significant improvement on industrial search\nengine and online advertising systems, both of which serving hundreds of\nmillions of users. We develop a novel distributed system called Peacock to\nlearn big LDA models from big data. The main features of Peacock include\nhierarchical distributed architecture, real-time prediction and topic\nde-duplication. We empirically demonstrate that the Peacock system is capable\nof providing significant benefits via highly scalable LDA topic models for\nseveral industrial applications.\n", "versions": [{"version": "v1", "created": "Sat, 17 May 2014 14:36:52 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 09:56:44 GMT"}, {"version": "v3", "created": "Sat, 6 Dec 2014 09:54:43 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Wang", "Yi", ""], ["Zhao", "Xuemin", ""], ["Sun", "Zhenlong", ""], ["Yan", "Hao", ""], ["Wang", "Lifeng", ""], ["Jin", "Zhihui", ""], ["Wang", "Liubin", ""], ["Gao", "Yang", ""], ["Law", "Ching", ""], ["Zeng", "Jia", ""]]}, {"id": "1405.5147", "submitter": "Everaldo Aguiar", "authors": "Everaldo Aguiar, Saurabh Nagrecha, Nitesh V. Chawla", "title": "Predicting Online Video Engagement Using Clickstreams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the nascent days of e-content delivery, having a superior product was\nenough to give companies an edge against the competition. With today's fiercely\ncompetitive market, one needs to be multiple steps ahead, especially when it\ncomes to understanding consumers. Focusing on a large set of web portals owned\nand managed by a private communications company, we propose methods by which\nthese sites' clickstream data can be used to provide a deep understanding of\ntheir visitors, as well as their interests and preferences. We further expand\nthe use of this data to show that it can be effectively used to predict user\nengagement to video streams.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 16:32:59 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Aguiar", "Everaldo", ""], ["Nagrecha", "Saurabh", ""], ["Chawla", "Nitesh V.", ""]]}, {"id": "1405.5447", "submitter": "Hosein Azarbonyad", "authors": "Hosein Azarbonyad, Azadeh Shakery, Heshaam Faili", "title": "Learning to Exploit Different Translation Resources for Cross Language\n  Information Retrieval", "comments": null, "journal-ref": "International Journal of Information and Communication Technology\n  Research, Volume 6, Issue 1, pp. 55-68, 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the important factors that affects the performance of Cross Language\nInformation Retrieval(CLIR)is the quality of translations being employed in\nCLIR. In order to improve the quality of translations, it is important to\nexploit available resources efficiently. Employing different translation\nresources with different characteristics has many challenges. In this paper, we\npropose a method for exploiting available translation resources simultaneously.\nThis method employs Learning to Rank(LTR) for exploiting different translation\nresources. To apply LTR methods for query translation, we define different\ntranslation relation based features in addition to context based features. We\nuse the contextual information contained in translation resources for\nextracting context based features.The proposed method uses LTR to construct a\ntranslation ranking model based on defined features. The constructed model is\nused for ranking translation candidates of query words. To evaluate the\nproposed method we do English-Persian CLIR, in which we employ the translation\nranking model to find translations of English queries and employ the\ntranslations to retrieve Persian documents. Experimental results show that our\napproach significantly outperforms single resource based CLIR methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 May 2014 11:45:06 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Azarbonyad", "Hosein", ""], ["Shakery", "Azadeh", ""], ["Faili", "Heshaam", ""]]}, {"id": "1405.5509", "submitter": "V Chitraa", "authors": "V. Chitraa, Antony Selvadoss Thanamani", "title": "Web Log Data Analysis by Enhanced Fuzzy C Means Clustering", "comments": "15 pages, International Journal on Computational Sciences &\n  Applications April 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  World Wide Web is a huge repository of information and there is a tremendous\nincrease in the volume of information daily. The number of users are also\nincreasing day by day. To reduce users browsing time lot of research is taken\nplace. Web Usage Mining is a type of web mining in which mining techniques are\napplied in log data to extract the behaviour of users. Clustering plays an\nimportant role in a broad range of applications like Web analysis, CRM,\nmarketing, medical diagnostics, computational biology, and many others.\nClustering is the grouping of similar instances or objects. The key factor for\nclustering is some sort of measure that can determine whether two objects are\nsimilar or dissimilar . In this paper a novel clustering method to partition\nuser sessions into accurate clusters is discussed. The accuracy and various\nperformance measures of the proposed algorithm shows that the proposed method\nis a better method for web log mining.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 18:26:06 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Chitraa", "V.", ""], ["Thanamani", "Antony Selvadoss", ""]]}, {"id": "1405.5869", "submitter": "Ping Li", "authors": "Anshumali Shrivastava and Ping Li", "title": "Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search\n  (MIPS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first provably sublinear time algorithm for approximate\n\\emph{Maximum Inner Product Search} (MIPS). Our proposal is also the first\nhashing algorithm for searching with (un-normalized) inner product as the\nunderlying similarity measure. Finding hashing schemes for MIPS was considered\nhard. We formally show that the existing Locality Sensitive Hashing (LSH)\nframework is insufficient for solving MIPS, and then we extend the existing LSH\nframework to allow asymmetric hashing schemes. Our proposal is based on an\ninteresting mathematical phenomenon in which inner products, after independent\nasymmetric transformations, can be converted into the problem of approximate\nnear neighbor search. This key observation makes efficient sublinear hashing\nscheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, we\nprovide an explicit construction of provably fast hashing scheme for MIPS. The\nproposed construction and the extended LSH framework could be of independent\ntheoretical interest. Our proposed algorithm is simple and easy to implement.\nWe evaluate the method, for retrieving inner products, in the collaborative\nfiltering task of item recommendations on Netflix and Movielens datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 May 2014 19:42:57 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1405.6223", "submitter": "Fangfang Li", "authors": "Fangfang Li, Guandong Xu, Longbing Cao", "title": "Coupled Item-based Matrix Factorization", "comments": "7 pages submitted to AAAI2014. arXiv admin note: substantial text\n  overlap with arXiv:1404.7467", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The essence of the challenges cold start and sparsity in Recommender Systems\n(RS) is that the extant techniques, such as Collaborative Filtering (CF) and\nMatrix Factorization (MF), mainly rely on the user-item rating matrix, which\nsometimes is not informative enough for predicting recommendations. To solve\nthese challenges, the objective item attributes are incorporated as\ncomplementary information. However, most of the existing methods for inferring\nthe relationships between items assume that the attributes are \"independently\nand identically distributed (iid)\", which does not always hold in reality. In\nfact, the attributes are more or less coupled with each other by some implicit\nrelationships. Therefore, in this pa-per we propose an attribute-based coupled\nsimilarity measure to capture the implicit relationships between items. We then\nintegrate the implicit item coupling into MF to form the Coupled Item-based\nMatrix Factorization (CIMF) model. Experimental results on two open data sets\ndemonstrate that CIMF outperforms the benchmark methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 00:42:16 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Li", "Fangfang", ""], ["Xu", "Guandong", ""], ["Cao", "Longbing", ""]]}, {"id": "1405.6285", "submitter": "David Rodrigues", "authors": "David M.S. Rodrigues and Vitorino Ramos", "title": "Traversing News with Ant Colony Optimisation and Negative Pheromones", "comments": "accepted as preprint for oral presentation at ECCS'14 in Lucca, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has seen the rapid development of the online newsroom. News\npublished online are the main outlet of news surpassing traditional printed\nnewspapers. This poses challenges to the production and to the consumption of\nthose news. With those many sources of information available it is important to\nfind ways to cluster and organise the documents if one wants to understand this\nnew system. A novel bio inspired approach to the problem of traversing the news\nis presented. It finds Hamiltonian cycles over documents published by the\nnewspaper The Guardian. A Second Order Swarm Intelligence algorithm based on\nAnt Colony Optimisation was developed that uses a negative pheromone to mark\nunrewarding paths with a \"no-entry\" signal. This approach follows recent\nfindings of negative pheromone usage in real ants.\n", "versions": [{"version": "v1", "created": "Sat, 24 May 2014 09:08:49 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Rodrigues", "David M. S.", ""], ["Ramos", "Vitorino", ""]]}, {"id": "1405.6287", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "\\'Etude des dimensions sp\\'ecifiques du contexte dans un syst\\`eme de\n  filtrage d'informations", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of business information systems, e-commerce and access to\nknowledge, the relevance of the information provided to use is a key fact to\nthe success of information systems. Therefore the quality of access is\ndetermined by access to the right information at the right time, at the right\nplace. In this context, it is important to consider the users needs when access\nto information and his contextual situation in order to provide relevant\ninformation, tailored to their needs and context use. In what follows we\ndescribe the prelude to a project that tries to combine all of these needs to\nimprove information systems.\n", "versions": [{"version": "v1", "created": "Sat, 24 May 2014 10:16:11 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1405.6667", "submitter": "Puneet Singh Ludu", "authors": "Puneet Singh Ludu", "title": "Inferring gender of a Twitter user using celebrities it follows", "comments": "Submitted at CSE department, SUNY Buffalo, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of user gender classification in social media,\nwith an application to Twitter. The approach automatically predicts gender by\nleveraging observable information such as the tweet behavior, linguistic\ncontent of the user's Twitter feed and the celebrities followed by the user.\nThis paper first evaluates linguistic content based features using LIWC\ndictionary and popular neighborhood features using Wikipedia and Freebase. Then\naugments both features which yielded a significant increase in the accuracy for\ngender prediction. Results show that rich linguistic features combined with\npopular neighborhood prove valuables and promising for additional user\nclassification needs.\n", "versions": [{"version": "v1", "created": "Mon, 26 May 2014 18:25:35 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Ludu", "Puneet Singh", ""]]}, {"id": "1405.6886", "submitter": "Rasmus Troelsg{\\aa}rd", "authors": "Rasmus Troelsg{\\aa}rd, Bj{\\o}rn Sand Jensen, Lars Kai Hansen", "title": "A Topic Model Approach to Multi-Modal Similarity", "comments": "topic modelling workshop at NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calculating similarities between objects defined by many heterogeneous data\nmodalities is an important challenge in many multimedia applications. We use a\nmulti-modal topic model as a basis for defining such a similarity between\nobjects. We propose to compare the resulting similarities from different model\nrealizations using the non-parametric Mantel test. The approach is evaluated on\na music dataset.\n", "versions": [{"version": "v1", "created": "Tue, 27 May 2014 12:34:24 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Troelsg\u00e5rd", "Rasmus", ""], ["Jensen", "Bj\u00f8rn Sand", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1405.7519", "submitter": "Deepali Virmani", "authors": "Deepali Virmani, Vikrant Malhotra, Ridhi Tyagi", "title": "Aspect Based Sentiment Analysis to Extract Meticulous Opinion Value", "comments": "IJCSIT, MAY 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion Mining and Sentiment Analysis is a process of identifying opinions in\nlarge unstructured/structured data and then analysing polarity of those\nopinions. Opinion mining and sentiment analysis have found vast application in\nanalysing online ratings, analysing product based reviews, e-governance, and\nmanaging hostile content over the internet. This paper proposes an algorithm to\nimplement aspect level sentiment analysis. The algorithm takes input from the\nremarks submitted by various teachers of a student. An aspect tree is formed\nwhich has various levels and weights are assigned to each branch to identify\nlevel of aspect. Aspect value is calculated by the algorithm by means of the\nproposed aspect tree. Dictionary based method is implemented to evaluate the\npolarity of the remark. The algorithm returns the aspect value clubbed with\nopinion value and sentiment value which helps in concluding the summarized\nvalue of remark.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 11:05:29 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Virmani", "Deepali", ""], ["Malhotra", "Vikrant", ""], ["Tyagi", "Ridhi", ""]]}, {"id": "1405.7544", "submitter": "Hai Nguyen T", "authors": "Hai Thanh Nguyen, J\\'er\\'emie Mary and Philippe Preux", "title": "Cold-start Problems in Recommendation Systems via Contextual-bandit\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a cold-start problem in recommendation systems where\nwe have completely new users entered the systems. There is not any interaction\nor feedback of the new users with the systems previoustly, thus no ratings are\navailable. Trivial approaches are to select ramdom items or the most popular\nones to recommend to the new users. However, these methods perform poorly in\nmany case. In this research, we provide a new look of this cold-start problem\nin recommendation systems. In fact, we cast this cold-start problem as a\ncontextual-bandit problem. No additional information on new users and new items\nis needed. We consider all the past ratings of previous users as contextual\ninformation to be integrated into the recommendation framework. To solve this\ntype of the cold-start problems, we propose a new efficient method which is\nbased on the LinUCB algorithm for contextual-bandit problems. The experiments\nwere conducted on three different publicly-available data sets, namely\nMovielens, Netflix and Yahoo!Music. The new proposed methods were also compared\nwith other state-of-the-art techniques. Experiments showed that our new method\nsignificantly improves upon all these methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 May 2014 13:05:39 GMT"}], "update_date": "2014-05-30", "authors_parsed": [["Nguyen", "Hai Thanh", ""], ["Mary", "J\u00e9r\u00e9mie", ""], ["Preux", "Philippe", ""]]}, {"id": "1405.7713", "submitter": "Sophia Katrenko", "authors": "Sophia Katrenko, Pieter Adriaans, Maarten van Someren", "title": "Using Local Alignments for Relation Recognition", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  1-48, 2010", "doi": "10.1613/jair.2964", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the problem of marrying structural similarity with\nsemantic relatedness for Information Extraction from text. Aiming at accurate\nrecognition of relations, we introduce local alignment kernels and explore\nvarious possibilities of using them for this task. We give a definition of a\nlocal alignment (LA) kernel based on the Smith-Waterman score as a sequence\nsimilarity measure and proceed with a range of possibilities for computing\nsimilarity between elements of sequences. We show how distributional similarity\nmeasures obtained from unlabeled data can be incorporated into the learning\ntask as semantic knowledge. Our experiments suggest that the LA kernel yields\npromising results on various biomedical corpora outperforming two baselines by\na large margin. Additional series of experiments have been conducted on the\ndata sets of seven general relation types, where the performance of the LA\nkernel is comparable to the current state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:51:47 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Katrenko", "Sophia", ""], ["Adriaans", "Pieter", ""], ["van Someren", "Maarten", ""]]}, {"id": "1405.7857", "submitter": "Peter Mutschke", "authors": "Peter Mutschke, Andrea Scharnhorst, Christophe Gu\\'eret, Philipp Mayr,\n  Preben Hansen, Aida Slavic", "title": "Knowledge Maps and Information Retrieval (KMIR)", "comments": "6 pages, accepted workshop proposal for Digital Libraries 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information systems usually show as a particular point of failure the\nvagueness between user search terms and the knowledge orders of the information\nspace in question. Some kind of guided searching therefore becomes more and\nmore important in order to precisely discover information without knowing the\nright search terms. Knowledge maps of digital library collections are promising\nnavigation tools through knowledge spaces but still far away from being\napplicable for searching digital libraries. However, there is no continuous\nknowledge exchange between the \"map makers\" on the one hand and the Information\nRetrieval (IR) specialists on the other hand. Thus, there is also a lack of\nmodels that properly combine insights of the two strands. The proposed workshop\naims at bringing together these two communities: experts in IR reflecting on\nvisual enhanced search interfaces and experts in knowledge mapping reflecting\non visualizations of the content of a collection that might also present a\ncontext for a search term in a visual manner. The intention of the workshop is\nto raise awareness of the potential of interactive knowledge maps for\ninformation seeking purposes and to create a common ground for experiments\naiming at the incorporation of knowledge maps into IR models at the level of\nthe user interface.\n", "versions": [{"version": "v1", "created": "Fri, 30 May 2014 13:30:15 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Mutschke", "Peter", ""], ["Scharnhorst", "Andrea", ""], ["Gu\u00e9ret", "Christophe", ""], ["Mayr", "Philipp", ""], ["Hansen", "Preben", ""], ["Slavic", "Aida", ""]]}, {"id": "1405.7868", "submitter": "Priya Bajaj", "authors": "Priya Bajaj and Supriya Raheja", "title": "A Vague Improved Markov Model Approach for Web Page Prediction", "comments": "8 pages, 4 figures, 1 table, International Journal of Computer\n  Science & Engineering Survey (IJCSES) Vol.5, No.2, April 2014", "journal-ref": null, "doi": "10.5121/ijcses.2014.5205", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today most of the information in all areas is available over the web. It\nincreases the web utilization as well as attracts the interest of researchers\nto improve the effectiveness of web access and web utilization. As the number\nof web clients gets increased, the bandwidth sharing is performed that\ndecreases the web access efficiency. Web page prefetching improves the\neffectiveness of web access by availing the next required web page before the\nuser demand. It is an intelligent predictive mining that analyze the user web\naccess history and predict the next page. In this work, vague improved markov\nmodel is presented to perform the prediction. In this work, vague rules are\nsuggested to perform the pruning at different levels of markov model. Once the\nprediction table is generated, the association mining will be implemented to\nidentify the most effective next page. In this paper, an integrated model is\nsuggested to improve the prediction accuracy and effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 07:52:20 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Bajaj", "Priya", ""], ["Raheja", "Supriya", ""]]}, {"id": "1405.7869", "submitter": "Priya Bajaj", "authors": "Priya Bajaj and Supriya Raheja", "title": "Integrating Vague Association Mining with Markov Model", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing demand of world wide web raises the need of predicting the\nuser's web page request.The most widely used approach to predict the web pages\nis the pattern discovery process of Web usage mining. This process involves\ninevitability of many techniques like Markov model, association rules and\nclustering. Fuzzy theory with different techniques has been introduced for the\nbetter results. Our focus is on Markov models. This paper is introducing the\nvague Rules with Markov models for more accuracy using the vague set theory.\n", "versions": [{"version": "v1", "created": "Thu, 8 May 2014 07:44:39 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Bajaj", "Priya", ""], ["Raheja", "Supriya", ""]]}, {"id": "1405.7975", "submitter": "Ercan Canhasi", "authors": "Ercan Canhasi", "title": "Multi-layered graph-based multi-document summarization model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-document summarization is a process of automatic generation of a\ncompressed version of the given collection of documents. Recently, the\ngraph-based models and ranking algorithms have been actively investigated by\nthe extractive document summarization community. While most work to date\nfocuses on homogeneous connecteness of sentences and heterogeneous connecteness\nof documents and sentences (e.g. sentence similarity weighted by document\nimportance), in this paper we present a novel 3-layered graph model that\nemphasizes not only sentence and document level relations but also the\ninfluence of under sentence level relations (e.g. a part of sentence\nsimilarity).\n", "versions": [{"version": "v1", "created": "Sat, 17 May 2014 22:21:00 GMT"}], "update_date": "2014-06-02", "authors_parsed": [["Canhasi", "Ercan", ""]]}]