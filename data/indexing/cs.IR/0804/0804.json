[{"id": "0804.0317", "submitter": "Maurice H. T. Ling", "authors": "Maurice HT Ling, Christophe Lefevre, Kevin R. Nicholas", "title": "Parts-of-Speech Tagger Errors Do Not Necessarily Degrade Accuracy in\n  Extracting Information from Biomedical Text", "comments": null, "journal-ref": "Ling, Maurice HT, Lefevre, Christophe, Nicholas, Kevin R. 2008.\n  Parts-of-Speech Tagger Errors Do Not Necessarily Degrade Accuracy in\n  Extracting Information from Biomedical Text. The Python Papers 3 (1): 65-80", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A recent study reported development of Muscorian, a generic text processing\ntool for extracting protein-protein interactions from text that achieved\ncomparable performance to biomedical-specific text processing tools. This\nresult was unexpected since potential errors from a series of text analysis\nprocesses is likely to adversely affect the outcome of the entire process. Most\nbiomedical entity relationship extraction tools have used biomedical-specific\nparts-of-speech (POS) tagger as errors in POS tagging and are likely to affect\nsubsequent semantic analysis of the text, such as shallow parsing. This study\naims to evaluate the parts-of-speech (POS) tagging accuracy and attempts to\nexplore whether a comparable performance is obtained when a generic POS tagger,\nMontyTagger, was used in place of MedPost, a tagger trained in biomedical text.\nOur results demonstrated that MontyTagger, Muscorian's POS tagger, has a POS\ntagging accuracy of 83.1% when tested on biomedical text. Replacing MontyTagger\nwith MedPost did not result in a significant improvement in entity relationship\nextraction from text; precision of 55.6% from MontyTagger versus 56.8% from\nMedPost on directional relationships and 86.1% from MontyTagger compared to\n81.8% from MedPost on nondirectional relationships. This is unexpected as the\npotential for poor POS tagging by MontyTagger is likely to affect the outcome\nof the information extraction. An analysis of POS tagging errors demonstrated\nthat 78.5% of tagging errors are being compensated by shallow parsing. Thus,\ndespite 83.1% tagging accuracy, MontyTagger has a functional tagging accuracy\nof 94.6%.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2008 09:34:13 GMT"}], "update_date": "2008-04-03", "authors_parsed": [["Ling", "Maurice HT", ""], ["Lefevre", "Christophe", ""], ["Nicholas", "Kevin R.", ""]]}, {"id": "0804.2057", "submitter": "Jos\\'e R. P\\'erez-Ag\\\"uera Phd.", "authors": "Jos\\'e R. P\\'erez-Ag\\\"uera and Lourdes Araujo", "title": "Comparing and Combining Methods for Automatic Query Expansion", "comments": "12 pages", "journal-ref": "Advances in Natural Language Processing and Applications. Research\n  in Computing Science 33, 2008, pp. 177-188", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query expansion is a well known method to improve the performance of\ninformation retrieval systems. In this work we have tested different approaches\nto extract the candidate query terms from the top ranked documents returned by\nthe first-pass retrieval.\n  One of them is the cooccurrence approach, based on measures of cooccurrence\nof the candidate and the query terms in the retrieved documents. The other one,\nthe probabilistic approach, is based on the probability distribution of terms\nin the collection and in the top ranked set.\n  We compare the retrieval improvement achieved by expanding the query with\nterms obtained with different methods belonging to both approaches. Besides, we\nhave developed a na\\\"ive combination of both kinds of method, with which we\nhave obtained results that improve those obtained with any of them separately.\nThis result confirms that the information provided by each approach is of a\ndifferent nature and, therefore, can be used in a combined manner.\n", "versions": [{"version": "v1", "created": "Sun, 13 Apr 2008 11:38:28 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["P\u00e9rez-Ag\u00fcera", "Jos\u00e9 R.", ""], ["Araujo", "Lourdes", ""]]}, {"id": "0804.2354", "submitter": "Andrew Krizhanovsky A", "authors": "A. V. Smirnov, A. A. Krizhanovsky", "title": "Information filtering based on wiki index database", "comments": "9 pages, 1 table, 2 figures, 8th International FLINS Conference on\n  Computational Intelligence in Decision and Control, Madrid, Spain, September\n  21-24, 2008; v2: typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a profile-based approach to information filtering by\nan analysis of the content of text documents. The Wikipedia index database is\ncreated and used to automatically generate the user profile from the user\ndocument collection. The problem-oriented Wikipedia subcorpora are created\n(using knowledge extracted from the user profile) for each topic of user\ninterests. The index databases of these subcorpora are applied to filtering\ninformation flow (e.g., mails, news). Thus, the analyzed texts are classified\ninto several topics explicitly presented in the user profile. The paper\nconcentrates on the indexing part of the approach. The architecture of an\napplication implementing the Wikipedia indexing is described. The indexing\nmethod is evaluated using the Russian and Simple English Wikipedia.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2008 11:05:59 GMT"}, {"version": "v2", "created": "Thu, 8 May 2008 12:35:01 GMT"}], "update_date": "2008-05-08", "authors_parsed": [["Smirnov", "A. V.", ""], ["Krizhanovsky", "A. A.", ""]]}, {"id": "0804.3599", "submitter": "Lillian Lee", "authors": "Oren Kurland, Lillian Lee", "title": "Respect My Authority! HITS Without Hyperlinks, Utilizing Cluster-Based\n  Language Models", "comments": null, "journal-ref": "Proceedings of SIGIR 2006, pp 83--90", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to improving the precision of an initial document\nranking wherein we utilize cluster information within a graph-based framework.\nThe main idea is to perform re-ranking based on centrality within bipartite\ngraphs of documents (on one side) and clusters (on the other side), on the\npremise that these are mutually reinforcing entities. Links between entities\nare created via consideration of language models induced from them.\n  We find that our cluster-document graphs give rise to much better retrieval\nperformance than previously proposed document-only graphs do. For example,\nauthority-based re-ranking of documents via a HITS-style cluster-based approach\noutperforms a previously-proposed PageRank-inspired algorithm applied to\nsolely-document graphs. Moreover, we also show that computing authority scores\nfor clusters constitutes an effective method for identifying clusters\ncontaining a large percentage of relevant documents.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2008 20:02:14 GMT"}], "update_date": "2008-04-24", "authors_parsed": [["Kurland", "Oren", ""], ["Lee", "Lillian", ""]]}, {"id": "0804.3671", "submitter": "Julien Fayolle", "authors": "Frederique Bassino, Julien Clement, Julien Fayolle, Pierre Nicodeme", "title": "Constructions for Clumps Statistics", "comments": "12 p., 2 figs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a component of the word statistics known as clump; starting from\na finite set of words, clumps are maximal overlapping sets of these\noccurrences. This parameter has first been studied by Schbath with the aim of\ncounting the number of occurrences of words in random texts. Later work with\nsimilar probabilistic approach used the Chen-Stein approximation for a compound\nPoisson distribution, where the number of clumps follows a law close to\nPoisson. Presently there is no combinatorial counterpart to this approach, and\nwe fill the gap here. We emphasize the fact that, in contrast with the\nprobabilistic approach which only provides asymptotic results, the\ncombinatorial approach provides exact results that are useful when considering\nshort sequences.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2008 10:38:15 GMT"}], "update_date": "2008-04-24", "authors_parsed": [["Bassino", "Frederique", ""], ["Clement", "Julien", ""], ["Fayolle", "Julien", ""], ["Nicodeme", "Pierre", ""]]}, {"id": "0804.4305", "submitter": "\\'Alvaro Francisco Huertas-Rosero", "authors": "Alvaro Francisco Huertas-Rosero", "title": "An Algorigtm for Singular Value Decomposition of Matrices in Blocks", "comments": "14 pages, 8 figures. Technical Report Replacement with updated\n  reference of publication", "journal-ref": "DCS Technical Report Series, TR-2008-269", "doi": null, "report-no": "TR-2008-269", "categories": "math.NA cs.IR math.AC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Two methods to decompose block matrices analogous to Singular Matrix\nDecomposition are proposed, one yielding the so called economy decomposition,\nand other yielding the full decomposition. This method is devised to avoid\nhandling matrices bigger than the biggest blocks, so it is particularly\nappropriate when a limitation on the size of matrices exists. The method is\ntested on a document-term matrix (17780x3204) divided in 4 blocks, the\nupper-left corner being 215x215.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2008 23:29:07 GMT"}, {"version": "v2", "created": "Sat, 7 Jun 2008 18:00:47 GMT"}], "update_date": "2008-06-07", "authors_parsed": [["Huertas-Rosero", "Alvaro Francisco", ""]]}, {"id": "0804.4451", "submitter": "Jian Ma", "authors": "Jian Ma and Zengqi Sun", "title": "Dependence Structure Estimation via Copula", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependence strucuture estimation is one of the important problems in machine\nlearning domain and has many applications in different scientific areas. In\nthis paper, a theoretical framework for such estimation based on copula and\ncopula entropy -- the probabilistic theory of representation and measurement of\nstatistical dependence, is proposed. Graphical models are considered as a\nspecial case of the copula framework. A method of the framework for estimating\nmaximum spanning copula is proposed. Due to copula, the method is irrelevant to\nthe properties of individual variables, insensitive to outlier and able to deal\nwith non-Gaussianity. Experiments on both simulated data and real dataset\ndemonstrated the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2008 17:14:53 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 00:29:28 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Ma", "Jian", ""], ["Sun", "Zengqi", ""]]}]