[{"id": "1610.00192", "submitter": "Tanay Kumar Saha", "authors": "Tanay Kumar Saha, Mourad Ouzzani, Hossam M. Hammady, Ahmed K.\n  Elmagarmid, Wajdi Dhifli, and Mohammad Al Hasan", "title": "A large scale study of SVM based methods for abstract screening in\n  systematic reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major task in systematic reviews is abstract screening, i.e., excluding,\noften hundreds or thousand of, irrelevant citations returned from a database\nsearch based on titles and abstracts. Thus, a systematic review platform that\ncan automate the abstract screening process is of huge importance. Several\nmethods have been proposed for this task. However, it is very hard to clearly\nunderstand the applicability of these methods in a systematic review platform\nbecause of the following challenges: (1) the use of non-overlapping metrics for\nthe evaluation of the proposed methods, (2) usage of features that are very\nhard to collect, (3) using a small set of reviews for the evaluation, and (4)\nno solid statistical testing or equivalence grouping of the methods. In this\npaper, we use feature representation that can be extracted per citation. We\nevaluate SVM-based methods (commonly used) on a large set of reviews ($61$) and\nmetrics ($11$) to provide equivalence grouping of methods based on a solid\nstatistical test. Our analysis also includes a strong variability of the\nmetrics using $500$x$2$ cross validation. While some methods shine for\ndifferent metrics and for different datasets, there is no single method that\ndominates the pack. Furthermore, we observe that in some cases relevant\n(included) citations can be found after screening only 15-20% of them via a\ncertainty based sampling. A few included citations present outlying\ncharacteristics and can only be found after a very large number of screening\nsteps. Finally, we present an ensemble algorithm for producing a $5$-star\nrating of citations based on their relevance. Such algorithm combines the best\nmethods from our evaluation and through its $5$-star rating outputs a more\neasy-to-consume prediction.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 21:11:38 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 12:56:11 GMT"}, {"version": "v3", "created": "Tue, 16 Jan 2018 00:52:51 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Saha", "Tanay Kumar", ""], ["Ouzzani", "Mourad", ""], ["Hammady", "Hossam M.", ""], ["Elmagarmid", "Ahmed K.", ""], ["Dhifli", "Wajdi", ""], ["Hasan", "Mohammad Al", ""]]}, {"id": "1610.00219", "submitter": "Junxian He", "authors": "Junxian He, Ying Huang, Changfeng Liu, Jiaming Shen, Yuting Jia,\n  Xinbing Wang", "title": "Text Network Exploration via Heterogeneous Web of Topics", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A text network refers to a data type that each vertex is associated with a\ntext document and the relationship between documents is represented by edges.\nThe proliferation of text networks such as hyperlinked webpages and academic\ncitation networks has led to an increasing demand for quickly developing a\ngeneral sense of a new text network, namely text network exploration. In this\npaper, we address the problem of text network exploration through constructing\na heterogeneous web of topics, which allows people to investigate a text\nnetwork associating word level with document level. To achieve this, a\nprobabilistic generative model for text and links is proposed, where three\ndifferent relationships in the heterogeneous topic web are quantified. We also\ndevelop a prototype demo system named TopicAtlas to exhibit such heterogeneous\ntopic web, and demonstrate how this system can facilitate the task of text\nnetwork exploration. Extensive qualitative analyses are included to verify the\neffectiveness of this heterogeneous topic web. Besides, we validate our model\non real-life text networks, showing that it preserves good performance on\nobjective evaluation metrics.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 03:35:11 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["He", "Junxian", ""], ["Huang", "Ying", ""], ["Liu", "Changfeng", ""], ["Shen", "Jiaming", ""], ["Jia", "Yuting", ""], ["Wang", "Xinbing", ""]]}, {"id": "1610.00248", "submitter": "Mark Scanlon", "authors": "Mark Scanlon", "title": "Battling the Digital Forensic Backlog through Data Deduplication", "comments": "Scanlon, M., Battling the Digital Forensic Backlog through Data\n  Deduplication, 6th IEEE International Conference on Innovative Computing\n  Technology (INTECH 2016), Dublin, Ireland, August 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In everyday life. Technological advancement can be found in many facets of\nlife, including personal computers, mobile devices, wearables, cloud services,\nvideo gaming, web-powered messaging, social media, Internet-connected devices,\netc. This technological influence has resulted in these technologies being\nemployed by criminals to conduct a range of crimes -- both online and offline.\nBoth the number of cases requiring digital forensic analysis and the sheer\nvolume of information to be processed in each case has increased rapidly in\nrecent years. As a result, the requirement for digital forensic investigation\nhas ballooned, and law enforcement agencies throughout the world are scrambling\nto address this demand. While more and more members of law enforcement are\nbeing trained to perform the required investigations, the supply is not keeping\nup with the demand. Current digital forensic techniques are arduously\ntime-consuming and require a significant amount of man power to execute. This\npaper discusses a novel solution to combat the digital forensic backlog. This\nsolution leverages a deduplication-based paradigm to eliminate the\nreacquisition, redundant storage, and reanalysis of previously processed data.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 09:11:38 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Scanlon", "Mark", ""]]}, {"id": "1610.00369", "submitter": "Asif Hassan", "authors": "A. Hassan, M. R. Amin, N. Mohammed, A. K. A. Azad", "title": "Sentiment Analysis on Bangla and Romanized Bangla Text (BRBT) using Deep\n  Recurrent models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment Analysis (SA) is an action research area in the digital age. With\nrapid and constant growth of online social media sites and services, and the\nincreasing amount of textual data such as - statuses, comments, reviews etc.\navailable in them, application of automatic SA is on the rise. However, most of\nthe research works on SA in natural language processing (NLP) are based on\nEnglish language. Despite being the sixth most widely spoken language in the\nworld, Bangla still does not have a large and standard dataset. Because of\nthis, recent research works in Bangla have failed to produce results that can\nbe both comparable to works done by others and reusable as stepping stones for\nfuture researchers to progress in this field. Therefore, we first tried to\nprovide a textual dataset - that includes not just Bangla, but Romanized Bangla\ntexts as well, is substantial, post-processed and multiple validated, ready to\nbe used in SA experiments. We tested this dataset in Deep Recurrent model,\nspecifically, Long Short Term Memory (LSTM), using two types of loss functions\n- binary crossentropy and categorical crossentropy, and also did some\nexperimental pre-training by using data from one validation to pre-train the\nother and vice versa. Lastly, we documented the results along with some\nanalysis on them, which were promising.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 23:45:23 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 02:13:05 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Hassan", "A.", ""], ["Amin", "M. R.", ""], ["Mohammed", "N.", ""], ["Azad", "A. K. A.", ""]]}, {"id": "1610.00572", "submitter": "Mauro Cettolo", "authors": "Mauro Cettolo", "title": "An Arabic-Hebrew parallel corpus of TED talks", "comments": "To appear in Proceedings of the AMTA 2016 Workshop on Semitic Machine\n  Translation (SeMaT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an Arabic-Hebrew parallel corpus of TED talks built upon WIT3,\nthe Web inventory that repurposes the original content of the TED website in a\nway which is more convenient for MT researchers. The benchmark consists of\nabout 2,000 talks, whose subtitles in Arabic and Hebrew have been accurately\naligned and rearranged in sentences, for a total of about 3.5M tokens per\nlanguage. Talks have been partitioned in train, development and test sets\nsimilarly in all respects to the MT tasks of the IWSLT 2016 evaluation\ncampaign. In addition to describing the benchmark, we list the problems\nencountered in preparing it and the novel methods designed to solve them.\nBaseline MT results and some measures on sentence length are provided as an\nextrinsic evaluation of the quality of the benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 14:44:58 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Cettolo", "Mauro", ""]]}, {"id": "1610.00574", "submitter": "Sepehr Eghbali", "authors": "Sepehr Eghbali and Ladan Tahvildari", "title": "Fast Cosine Similarity Search in Binary Space with Angular Multi-index\n  Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large dataset of binary codes and a binary query point, we address\nhow to efficiently find $K$ codes in the dataset that yield the largest cosine\nsimilarities to the query. The straightforward answer to this problem is to\ncompare the query with all items in the dataset, but this is practical only for\nsmall datasets. One potential solution to enhance the search time and achieve\nsublinear cost is to use a hash table populated with binary codes of the\ndataset and then look up the nearby buckets to the query to retrieve the\nnearest neighbors. However, if codes are compared in terms of cosine similarity\nrather than the Hamming distance, then the main issue is that the order of\nbuckets to probe is not evident. To examine this issue, we first elaborate on\nthe connection between the Hamming distance and the cosine similarity. Doing\nthis allows us to systematically find the probing sequence in the hash table.\nHowever, solving the nearest neighbor search with a single table is only\npractical for short binary codes. To address this issue, we propose the angular\nmulti-index hashing search algorithm which relies on building multiple hash\ntables on binary code substrings. The proposed search algorithm solves the\nexact angular $K$ nearest neighbor problem in a time that is often orders of\nmagnitude faster than the linear scan baseline and even approximation methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 23:16:37 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 12:55:36 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Eghbali", "Sepehr", ""], ["Tahvildari", "Ladan", ""]]}, {"id": "1610.00735", "submitter": "Yanshan Wang", "authors": "Yanshan Wang and Hongfang Liu", "title": "MatLM: a Matrix Formulation for Probabilistic Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic language models are widely used in Information Retrieval (IR)\nto rank documents by the probability that they generate the query. However, the\nimplementation of the probabilistic representations with programming languages\nthat favor matrix calculations is challenging. In this paper, we utilize matrix\nrepresentations to reformulate the probabilistic language models. The matrix\nrepresentation is a superstructure for the probabilistic language models to\norganize the calculated probabilities and a potential formalism for\nstandardization of language models and for further mathematical analysis. It\nfacilitates implementations by matrix friendly programming languages. In this\npaper, we consider the matrix formulation of conventional language model with\nDirichlet smoothing, and two language models based on Latent Dirichlet\nAllocation (LDA), i.e., LBDM and LDI. We release a Java software\npackage--MatLM--implementing the proposed models. Code is available at:\nhttps://github.com/yanshanwang/JGibbLDA-v.1.0-MatLM.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 20:36:56 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Wang", "Yanshan", ""], ["Liu", "Hongfang", ""]]}, {"id": "1610.01206", "submitter": "Yingming Li", "authors": "Yingming Li, Ming Yang, and Zhongfei Zhang", "title": "A Survey of Multi-View Representation Learning", "comments": "Accepted by IEEE Transactions on Knowledge and Data Engineering", "journal-ref": null, "doi": "10.1109/TKDE.2018.2872063", "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, multi-view representation learning has become a rapidly growing\ndirection in machine learning and data mining areas. This paper introduces two\ncategories for multi-view representation learning: multi-view representation\nalignment and multi-view representation fusion. Consequently, we first review\nthe representative methods and theories of multi-view representation learning\nbased on the perspective of alignment, such as correlation-based alignment.\nRepresentative examples are canonical correlation analysis (CCA) and its\nseveral extensions. Then from the perspective of representation fusion we\ninvestigate the advancement of multi-view representation learning that ranges\nfrom generative methods including multi-modal topic learning, multi-view sparse\ncoding, and multi-view latent space Markov networks, to neural network-based\nmethods including multi-modal autoencoders, multi-view convolutional neural\nnetworks, and multi-modal recurrent neural networks. Further, we also\ninvestigate several important applications of multi-view representation\nlearning. Overall, this survey aims to provide an insightful overview of\ntheoretical foundation and state-of-the-art developments in the field of\nmulti-view representation learning and to help researchers find the most\nappropriate tools for particular applications.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 17:14:15 GMT"}, {"version": "v2", "created": "Sun, 27 Nov 2016 03:11:53 GMT"}, {"version": "v3", "created": "Thu, 24 Aug 2017 08:08:22 GMT"}, {"version": "v4", "created": "Fri, 1 Sep 2017 05:52:06 GMT"}, {"version": "v5", "created": "Wed, 24 Oct 2018 02:34:53 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Li", "Yingming", ""], ["Yang", "Ming", ""], ["Zhang", "Zhongfei", ""]]}, {"id": "1610.01327", "submitter": "Christina Lioma Assoc. Prof", "authors": "Christina Lioma and Birger Larsen and Wei Lu and Yong Huang", "title": "A Study of Factuality, Objectivity and Relevance: Three Desiderata in\n  Large-Scale Information Retrieval?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the information processed by Information Retrieval (IR) systems is\nunreliable, biased, and generally untrustworthy [1], [2], [3]. Yet, factuality\n& objectivity detection is not a standard component of IR systems, even though\nit has been possible in Natural Language Processing (NLP) in the last decade.\nMotivated by this, we ask if and how factuality & objectivity detection may\nbenefit IR. We answer this in two parts. First, we use state-of-the-art NLP to\ncompute the probability of document factuality & objectivity in two TREC\ncollections, and analyse its relation to document relevance. We find that\nfactuality is strongly and positively correlated to document relevance, but\nobjectivity is not. Second, we study the impact of factuality & objectivity to\nretrieval effectiveness by treating them as query independent features that we\ncombine with a competitive language modelling baseline. Experiments with 450\nTREC queries show that factuality improves precision >10% over strong\nbaselines, especially for uncurated data used in web search; objectivity gives\nmixed results. An overall clear trend is that document factuality & objectivity\nis much more beneficial to IR when searching uncurated (e.g. web) documents vs.\ncurated (e.g. state documentation and newswire articles). To our knowledge,\nthis is the first study of factuality & objectivity for back-end IR,\ncontributing novel findings about the relation between relevance and\nfactuality/objectivity, and statistically significant gains to retrieval\neffectiveness in the competitive web search task.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 09:24:49 GMT"}, {"version": "v2", "created": "Sat, 8 Oct 2016 20:28:46 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Lioma", "Christina", ""], ["Larsen", "Birger", ""], ["Lu", "Wei", ""], ["Huang", "Yong", ""]]}, {"id": "1610.01366", "submitter": "Giambattista Amati Dr.", "authors": "Giambattista Amati and Simone Angelini and Marco Bianchi and Luca\n  Costantini and Giuseppe Marcone", "title": "A cumulative approach to quantification for sentiment analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We estimate sentiment categories proportions for retrieval within large\nretrieval sets. In general, estimates are produced by counting the\nclassification outcomes and then by adjusting such category sizes taking into\naccount misclassification error matrix. However, both the accuracy of the\nclassifier and the precision of the retrieval produce a large number of errors\nthat makes difficult the application of an aggregative approach to sentiment\nanalysis as a reliable and efficient estimation of proportions for sentiment\ncategories.\n  The challenge for real time analytics during retrieval is thus to overcome\nmisclassification errors, and more importantly, to apply sentiment\nclassification or any other similar post-processing analytics at retrieval\ntime. We present a non-aggregative approach that can be applied to very large\nretrieval sets of queries.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 11:30:28 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Amati", "Giambattista", ""], ["Angelini", "Simone", ""], ["Bianchi", "Marco", ""], ["Costantini", "Luca", ""], ["Marcone", "Giuseppe", ""]]}, {"id": "1610.01520", "submitter": "Edgar Altszyler", "authors": "Edgar Altszyler, Mariano Sigman, Sidarta Ribeiro and Diego Fern\\'andez\n  Slezak", "title": "Comparative study of LSA vs Word2vec embeddings in small corpora: a case\n  study in dreams database", "comments": null, "journal-ref": "Conscious Cogn. 2017 Nov;56:178-187", "doi": "10.1016/j.concog.2017.09.004", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings have been extensively studied in large text datasets.\nHowever, only a few studies analyze semantic representations of small corpora,\nparticularly relevant in single-person text production studies. In the present\npaper, we compare Skip-gram and LSA capabilities in this scenario, and we test\nboth techniques to extract relevant semantic patterns in single-series dreams\nreports. LSA showed better performance than Skip-gram in small size training\ncorpus in two semantic tests. As a study case, we show that LSA can capture\nrelevant words associations in dream reports series, even in cases of small\nnumber of dreams or low-frequency words. We propose that LSA can be used to\nexplore words associations in dreams reports, which could bring new insight\ninto this classic research area of psychology\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 16:47:17 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 15:43:33 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Altszyler", "Edgar", ""], ["Sigman", "Mariano", ""], ["Ribeiro", "Sidarta", ""], ["Slezak", "Diego Fern\u00e1ndez", ""]]}, {"id": "1610.01546", "submitter": "Yueming Sun", "authors": "Yueming Sun, Yi Zhang, Yunfei Chen, Roger Jin", "title": "Conversational Recommendation System with Unsupervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We will demonstrate a conversational products recommendation agent. This\nsystem shows how we combine research in personalized recommendation systems\nwith research in dialogue systems to build a virtual sales agent. Based on new\ndeep learning technologies we developed, the virtual agent is capable of\nlearning how to interact with users, how to answer user questions, what is the\nnext question to ask, and what to recommend when chatting with a human user.\n  Normally a descent conversational agent for a particular domain requires tens\nof thousands of hand labeled conversational data or hand written rules. This is\na major barrier when launching a conversation agent for a new domain. We will\nexplore and demonstrate the effectiveness of the learning solution even when\nthere is no hand written rules or hand labeled training data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 05:46:49 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Sun", "Yueming", ""], ["Zhang", "Yi", ""], ["Chen", "Yunfei", ""], ["Jin", "Roger", ""]]}, {"id": "1610.01858", "submitter": "Muhammad Imran", "authors": "Muhammad Imran, Sanjay Chawla, Carlos Castillo", "title": "A Robust Framework for Classifying Evolving Document Streams in an\n  Expert-Machine-Crowd Setting", "comments": "Accepted at ICDM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emerging challenge in the online classification of social media data\nstreams is to keep the categories used for classification up-to-date. In this\npaper, we propose an innovative framework based on an Expert-Machine-Crowd\n(EMC) triad to help categorize items by continuously identifying novel concepts\nin heterogeneous data streams often riddled with outliers. We unify constrained\nclustering and outlier detection by formulating a novel optimization problem:\nCOD-Means. We design an algorithm to solve the COD-Means problem and show that\nCOD-Means will not only help detect novel categories but also seamlessly\ndiscover human annotation errors and improve the overall quality of the\ncategorization process. Experiments on diverse real data sets demonstrate that\nour approach is both effective and efficient.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 13:20:07 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Imran", "Muhammad", ""], ["Chawla", "Sanjay", ""], ["Castillo", "Carlos", ""]]}, {"id": "1610.01901", "submitter": "Tongfei Chen", "authors": "Tongfei Chen, Benjamin Van Durme", "title": "Discriminative Information Retrieval for Knowledge Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for discriminative Information Retrieval (IR) atop\nlinguistic features, trained to improve the recall of tasks such as answer\ncandidate passage retrieval, the initial step in text-based Question Answering\n(QA). We formalize this as an instance of linear feature-based IR (Metzler and\nCroft, 2007), illustrating how a variety of knowledge discovery tasks are\ncaptured under this approach, leading to a 44% improvement in recall for\ncandidate triage for QA.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 15:04:10 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Chen", "Tongfei", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "1610.02085", "submitter": "Tim Althoff", "authors": "Tim Althoff, Ryen W. White, Eric Horvitz", "title": "Influence of Pok\\'emon Go on Physical Activity: Study and Implications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical activity helps people maintain a healthy weight and reduces the risk\nfor several chronic diseases. Although this knowledge is widely recognized,\nadults and children in many countries around the world do not get recommended\namounts of physical activity. While many interventions are found to be\nineffective at increasing physical activity or reaching inactive populations,\nthere have been anecdotal reports of increased physical activity due to novel\nmobile games that embed game play in the physical world. The most recent and\nsalient example of such a game is Pok\\'emon Go, which has reportedly reached\ntens of millions of users in the US and worldwide.\n  We study the effect of Pok\\'emon Go on physical activity through a\ncombination of signals from large-scale corpora of wearable sensor data and\nsearch engine logs for 32 thousand users over a period of three months.\nPok\\'emon Go players are identified through search engine queries and activity\nis measured through accelerometry. We find that Pok\\'emon Go leads to\nsignificant increases in physical activity over a period of 30 days, with\nparticularly engaged users (i.e., those making multiple search queries for\ndetails about game usage) increasing their activity by 1473 steps a day on\naverage, a more than 25% increase compared to their prior activity level\n($p<10^{-15}$). In the short time span of the study, we estimate that Pok\\'emon\nGo has added a total of 144 billion steps to US physical activity. Furthermore,\nPok\\'emon Go has been able to increase physical activity across men and women\nof all ages, weight status, and prior activity levels showing this form of game\nleads to increases in physical activity with significant implications for\npublic health. We find that Pok\\'emon Go is able to reach low activity\npopulations while all four leading mobile health apps studied in this work\nlargely draw from an already very active population.\n", "versions": [{"version": "v1", "created": "Thu, 6 Oct 2016 21:58:00 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 00:50:50 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Althoff", "Tim", ""], ["White", "Ryen W.", ""], ["Horvitz", "Eric", ""]]}, {"id": "1610.02496", "submitter": "Kaiwei Li", "authors": "Kaiwei Li, Jianfei Chen, Wenguang Chen, Jun Zhu", "title": "SaberLDA: Sparsity-Aware Learning of Topic Models on GPUs", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete\ncount data such as text and images. Applications require LDA to handle both\nlarge datasets and a large number of topics. Though distributed CPU systems\nhave been used, GPU-based systems have emerged as a promising alternative\nbecause of the high computational power and memory bandwidth of GPUs. However,\nexisting GPU-based LDA systems cannot support a large number of topics because\nthey use algorithms on dense data structures whose time and space complexity is\nlinear to the number of topics. In this paper, we propose SaberLDA, a GPU-based\nLDA system that implements a sparsity-aware algorithm to achieve sublinear time\ncomplexity and scales well to learn a large number of topics. To address the\nchallenges introduced by sparsity, we propose a novel data layout, a new\nwarp-based sampling kernel, and an efficient sparse count matrix updating\nalgorithm that improves locality, makes efficient utilization of GPU warps, and\nreduces memory consumption. Experiments show that SaberLDA can learn from\nbillions-token-scale data with up to 10,000 topics, which is almost two orders\nof magnitude larger than that of the previous GPU-based systems. With a single\nGPU card, SaberLDA is able to learn 10,000 topics from a dataset of billions of\ntokens in a few hours, which is only achievable with clusters with tens of\nmachines before.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 07:57:00 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 12:39:07 GMT"}], "update_date": "2016-10-13", "authors_parsed": [["Li", "Kaiwei", ""], ["Chen", "Jianfei", ""], ["Chen", "Wenguang", ""], ["Zhu", "Jun", ""]]}, {"id": "1610.02502", "submitter": "J. Shane Culpepper", "authors": "J. Shane Culpepper and Charles L. A. Clarke and Jimmy Lin", "title": "Dynamic Trade-Off Prediction in Multi-Stage Retrieval Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern multi-stage retrieval systems are comprised of a candidate generation\nstage followed by one or more reranking stages. In such an architecture, the\nquality of the final ranked list may not be sensitive to the quality of initial\ncandidate pool, especially in terms of early precision. This provides several\nopportunities to increase retrieval efficiency without significantly\nsacrificing effectiveness. In this paper, we explore a new approach to\ndynamically predicting two different parameters in the candidate generation\nstage which can directly affect the overall efficiency and effectiveness of the\nentire system. Previous work exploring this tradeoff has focused on global\nparameter settings that apply to all queries, even though optimal settings vary\nacross queries. In contrast, we propose a technique which makes a parameter\nprediction that maximizes efficiency within a effectiveness envelope on a per\nquery basis, using only static pre-retrieval features. The query-specific\ntradeoff point between effectiveness and efficiency is decided using a\nclassifier cascade that weighs possible efficiency gains against effectiveness\nlosses over a range of possible parameter cutoffs to make the prediction. The\ninteresting twist in our new approach is to train classifiers without requiring\nexplicit relevance judgments. We show that our framework is generalizable by\napplying it to two different retrieval parameters - selecting k in common top-k\nquery retrieval algorithms, and setting a quality threshold, $\\rho$, for\nscore-at-a-time approximate query evaluation algorithms. Experimental results\nshow that substantial efficiency gains are achievable depending on the dynamic\nparameter choice. In addition, our framework provides a versatile tool that can\nbe used to estimate the effectiveness-efficiency tradeoffs that are possible\nbefore selecting and tuning algorithms to make machine learned predictions.\n", "versions": [{"version": "v1", "created": "Sat, 8 Oct 2016 08:59:03 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Culpepper", "J. Shane", ""], ["Clarke", "Charles L. A.", ""], ["Lin", "Jimmy", ""]]}, {"id": "1610.03048", "submitter": "Hua Sun", "authors": "Hua Sun and Syed A. Jafar", "title": "Optimal Download Cost of Private Information Retrieval for Arbitrary\n  Message Length", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A private information retrieval scheme is a mechanism that allows a user to\nretrieve any one out of $K$ messages from $N$ non-communicating replicated\ndatabases, each of which stores all $K$ messages, without revealing anything\nabout the identity of the desired message index to any individual database. If\nthe size of each message is $L$ bits and the total download required by a PIR\nscheme from all $N$ databases is $D$ bits, then $D$ is called the download cost\nand the ratio $L/D$ is called an achievable rate. For fixed $K,N\\in\\mathbb{N}$,\nthe capacity of PIR, denoted by $C$, is the supremum of achievable rates over\nall PIR schemes and over all message sizes, and was recently shown to be\n$C=(1+1/N+1/N^2+\\cdots+1/N^{K-1})^{-1}$. In this work, for arbitrary $K, N$, we\nexplore the minimum download cost $D_L$ across all PIR schemes (not restricted\nto linear schemes) for arbitrary message lengths $L$ under arbitrary choices of\nalphabet (not restricted to finite fields) for the message and download\nsymbols. If the same $M$-ary alphabet is used for the message and download\nsymbols, then we show that the optimal download cost in $M$-ary symbols is\n$D_L=\\lceil\\frac{L}{C}\\rceil$. If the message symbols are in $M$-ary alphabet\nand the downloaded symbols are in $M'$-ary alphabet, then we show that the\noptimal download cost in $M'$-ary symbols, $D_L\\in\\left\\{\\left\\lceil\n\\frac{L'}{C}\\right\\rceil,\\left\\lceil \\frac{L'}{C}\\right\\rceil-1,\\left\\lceil\n\\frac{L'}{C}\\right\\rceil-2\\right\\}$, where $L'= \\lceil L \\log_{M'} M\\rceil$.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 19:51:49 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Sun", "Hua", ""], ["Jafar", "Syed A.", ""]]}, {"id": "1610.03106", "submitter": "Hussam Hamdan", "authors": "Hussam Hamdan, Patrice Bellot, Frederic Bechet", "title": "Supervised Term Weighting Metrics for Sentiment Analysis in Short Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Term weighting metrics assign weights to terms in order to discriminate the\nimportant terms from the less crucial ones. Due to this characteristic, these\nmetrics have attracted growing attention in text classification and recently in\nsentiment analysis. Using the weights given by such metrics could lead to more\naccurate document representation which may improve the performance of the\nclassification. While previous studies have focused on proposing or comparing\ndifferent weighting metrics at two-classes document level sentiment analysis,\nthis study propose to analyse the results given by each metric in order to find\nout the characteristics of good and bad weighting metrics. Therefore we present\nan empirical study of fifteen global supervised weighting metrics with four\nlocal weighting metrics adopted from information retrieval, we also give an\nanalysis to understand the behavior of each metric by observing and analysing\nhow each metric distributes the terms and deduce some characteristics which may\ndistinguish the good and bad metrics. The evaluation has been done using\nSupport Vector Machine on three different datasets: Twitter, restaurant and\nlaptop reviews.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 21:52:47 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Hamdan", "Hussam", ""], ["Bellot", "Patrice", ""], ["Bechet", "Frederic", ""]]}, {"id": "1610.03120", "submitter": "Hussam Hamdan", "authors": "Hussam Hamdan", "title": "Correlation-Based Method for Sentiment Classification", "comments": "I'm not convinced about the significance of this paper in its actual\n  state", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic supervised classification algorithms are efficient, but\ntime-consuming, complicated and not interpretable, which makes it difficult to\nanalyze their results that limits the possibility to improve them based on real\nobservations. In this paper, we propose a new and a simple classifier to\npredict a sentiment label of a short text. This model keeps the capacity of\nhuman interpret-ability and can be extended to integrate NLP techniques in a\nmore interpretable way. Our model is based on a correlation metric which\nmeasures the degree of association between a sentiment label and a word. Ten\ncorrelation metrics are proposed and evaluated intrinsically. And then a\nclassifier based on each metric is proposed, evaluated and compared to the\nclassic classification algorithms which have proved their performance in many\nstudies. Our model outperforms these algorithms with several correlation\nmetrics.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 22:35:21 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 22:37:49 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Hamdan", "Hussam", ""]]}, {"id": "1610.03147", "submitter": "Pan Zhou Prof.", "authors": "Yifan Hou, Pan Zhou, Ting Wang, Li Yu, Yuchong Hu, Dapeng Wu", "title": "Context-Aware Online Learning for Course Recommendation of MOOC Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Massive Open Online Course (MOOC) has expanded significantly in recent\nyears. With the widespread of MOOC, the opportunity to study the fascinating\ncourses for free has attracted numerous people of diverse educational\nbackgrounds all over the world. In the big data era, a key research topic for\nMOOC is how to mine the needed courses in the massive course databases in cloud\nfor each individual student accurately and rapidly as the number of courses is\nincreasing fleetly. In this respect, the key challenge is how to realize\npersonalized course recommendation as well as to reduce the computing and\nstorage costs for the tremendous course data. In this paper, we propose a big\ndata-supported, context-aware online learning-based course recommender system\nthat could handle the dynamic and infinitely massive datasets, which recommends\ncourses by using personalized context information and historical statistics.\nThe context-awareness takes the personal preferences into consideration, making\nthe recommendation suitable for people with different backgrounds. Besides, the\nalgorithm achieves the sublinear regret performance, which means it can\ngradually recommend the mostly preferred and matched courses to students. In\naddition, our storage module is expanded to the distributed-connected storage\nnodes, where the devised algorithm can handle massive course storage problems\nfrom heterogeneous sources of course datasets. Comparing to existing\nalgorithms, our proposed algorithms achieve the linear time complexity and\nspace complexity. Experiment results verify the superiority of our algorithms\nwhen comparing with existing ones in the MOOC big data setting.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 01:02:15 GMT"}, {"version": "v2", "created": "Sun, 16 Oct 2016 03:34:37 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Hou", "Yifan", ""], ["Zhou", "Pan", ""], ["Wang", "Ting", ""], ["Yu", "Li", ""], ["Hu", "Yuchong", ""], ["Wu", "Dapeng", ""]]}, {"id": "1610.04002", "submitter": "Richard McCreadie", "authors": "Richard McCreadie, Craig Macdonald and Iadh Ounis", "title": "Emergency Identification and Analysis with EAIMS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media platforms are now a key source of information for a large\nsegment of the public. As such, these platforms have a great potential as a\nmeans to provide real-time information to emergency management agencies.\nMoreover, during an emergency, these agencies are very interested in social\nmedia as a means to find public-driven response efforts, as well as to track\nhow their handling of that emergency is being perceived. However, there is\ncurrently a lack advanced tools designed for monitoring social media during\nemergencies. The Emergency Analysis Identification and Management System\n(EAIMS) is a prototype service that aims to fill this technology gap by\nproviding richer analytic and exploration tools than current solutions. In\nparticular, EAIMS provides real-time detection of emergency events, related\ninformation finding, information access and credibility analysis tools for use\nover social media during emergencies.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 10:02:01 GMT"}], "update_date": "2016-10-14", "authors_parsed": [["McCreadie", "Richard", ""], ["Macdonald", "Craig", ""], ["Ounis", "Iadh", ""]]}, {"id": "1610.04086", "submitter": "Zhi-Hua Zhou", "authors": "Ming Pang and Wei Gao and Min Tao and Zhi-Hua Zhou", "title": "Unorganized Malicious Attacks Detection", "comments": null, "journal-ref": "NIPS 2018", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender system has attracted much attention during the past decade. Many\nattack detection algorithms have been developed for better recommendations,\nmostly focusing on shilling attacks, where an attack organizer produces a large\nnumber of user profiles by the same strategy to promote or demote an item. This\nwork considers a different attack style: unorganized malicious attacks, where\nattackers individually utilize a small number of user profiles to attack\ndifferent items without any organizer. This attack style occurs in many real\napplications, yet relevant study remains open. We first formulate the\nunorganized malicious attacks detection as a matrix completion problem, and\npropose the Unorganized Malicious Attacks detection (UMA) approach, a proximal\nalternating splitting augmented Lagrangian method. We verify, both\ntheoretically and empirically, the effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2016 14:02:49 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 14:42:32 GMT"}, {"version": "v3", "created": "Sun, 18 Feb 2018 00:47:15 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Pang", "Ming", ""], ["Gao", "Wei", ""], ["Tao", "Min", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1610.04530", "submitter": "Qiwen Wang Ms", "authors": "Qiwen Wang, Mikael Skoglund", "title": "Symmetric Private Information Retrieval For MDS Coded Distributed\n  Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A user wants to retrieve a file from a database without revealing the\nidentity of the file retrieved at the database, which is known as the problem\nof private information retrieval (PIR). If it is further required that the user\nobtains no information about the database other than the desired file, the\nconcept of symmetric private information retrieval (SPIR) is introduced to\nguarantee privacy for both parties. In this paper, the problem of SPIR is\nstudied for a database stored among $N$ nodes in a distributed way, by using an\n$(N,M)$-MDS storage code. The information-theoretic capacity of SPIR, defined\nas the maximum number of symbols of the desired file retrieved per downloaded\nsymbol, for the coded database is derived. It is shown that the SPIR capacity\nfor coded database is $1-\\frac{M}{N}$, when the amount of the shared common\nrandomness of distributed nodes (unavailable at the user) is at least\n$\\frac{M}{N-M}$ times the file size. Otherwise, the SPIR capacity for the coded\ndatabase equals zero.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 16:56:39 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Wang", "Qiwen", ""], ["Skoglund", "Mikael", ""]]}, {"id": "1610.04533", "submitter": "Issa Atoum", "authors": "Issa Atoum, Ahmed Otoom and Narayanan Kulathuramaiyer", "title": "A Comprehensive Comparative Study of Word and Sentence Similarity\n  Measures", "comments": "7 pages,4 figures", "journal-ref": "International Journal of Computer Applications,2016,135(1),\n  Foundation of Computer Science (FCS), NY, USA", "doi": "10.5120/ijca2016908259", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence similarity is considered the basis of many natural language tasks\nsuch as information retrieval, question answering and text summarization. The\nsemantic meaning between compared text fragments is based on the words semantic\nfeatures and their relationships. This article reviews a set of word and\nsentence similarity measures and compares them on benchmark datasets. On the\nstudied datasets, results showed that hybrid semantic measures perform better\nthan both knowledge and corpus based measures.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 19:33:47 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Atoum", "Issa", ""], ["Otoom", "Ahmed", ""], ["Kulathuramaiyer", "Narayanan", ""]]}, {"id": "1610.04814", "submitter": "Mahamad Suhil", "authors": "D S Guru and Mahamad Suhil", "title": "Term-Class-Max-Support (TCMS): A Simple Text Document Categorization\n  Approach Using Term-Class Relevance Measure", "comments": "4 Pages, 4 Figures; 2016 Intl. Conference on Advances in Computing,\n  Communications and Informatics (ICACCI), Sept. 21-24, 2016, Jaipur, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a simple text categorization method using term-class relevance\nmeasures is proposed. Initially, text documents are processed to extract\nsignificant terms present in them. For every term extracted from a document, we\ncompute its importance in preserving the content of a class through a novel\nterm-weighting scheme known as Term_Class Relevance (TCR) measure proposed by\nGuru and Suhil (2015) [1]. In this way, for every term, its relevance for all\nthe classes present in the corpus is computed and stored in the knowledgebase.\nDuring testing, the terms present in the test document are extracted and the\nterm-class relevance of each term is obtained from the stored knowledgebase. To\nachieve quick search of term weights, Btree indexing data structure has been\nadapted. Finally, the class which receives maximum support in terms of\nterm-class relevance is decided to be the class of the given test document. The\nproposed method works in logarithmic complexity in testing time and simple to\nimplement when compared to any other text categorization techniques available\nin literature. The experiments conducted on various benchmarking datasets have\nrevealed that the performance of the proposed method is satisfactory and\nencouraging.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 03:40:13 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Guru", "D S", ""], ["Suhil", "Mahamad", ""]]}, {"id": "1610.04850", "submitter": "Alexander Fonarev", "authors": "Alexander Fonarev, Alexander Mikhalev, Pavel Serdyukov, Gleb Gusev,\n  Ivan Oseledets", "title": "Efficient Rectangular Maximal-Volume Algorithm for Rating Elicitation in\n  Collaborative Filtering", "comments": "IEEE International Conference on Data Mining (ICDM) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cold start problem in Collaborative Filtering can be solved by asking new\nusers to rate a small seed set of representative items or by asking\nrepresentative users to rate a new item. The question is how to build a seed\nset that can give enough preference information for making good\nrecommendations. One of the most successful approaches, called Representative\nBased Matrix Factorization, is based on Maxvol algorithm. Unfortunately, this\napproach has one important limitation --- a seed set of a particular size\nrequires a rating matrix factorization of fixed rank that should coincide with\nthat size. This is not necessarily optimal in the general case. In the current\npaper, we introduce a fast algorithm for an analytical generalization of this\napproach that we call Rectangular Maxvol. It allows the rank of factorization\nto be lower than the required size of the seed set. Moreover, the paper\nincludes the theoretical analysis of the method's error, the complexity\nanalysis of the existing methods and the comparison to the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 16 Oct 2016 12:50:37 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Fonarev", "Alexander", ""], ["Mikhalev", "Alexander", ""], ["Serdyukov", "Pavel", ""], ["Gusev", "Gleb", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1610.05819", "submitter": "Ashwinkumar Ganesan", "authors": "Ashwinkumar Ganesan, Tim Oates, Matt Schmill", "title": "Finding Representative Points in Multivariate Data Using PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of representation has been used in various fields of study from data\nanalysis to political science. In this paper, we define representativeness and\ndescribe a method to isolate data points that can represent the entire data\nset. Also, we show how the minimum set of representative data points can be\ngenerated. We use data from GLOBE (a project to study the effects on Land\nChange based on a set of parameters that include temperature, forest cover,\nhuman population, atmospheric parameters and many other variables) to test &\nvalidate the algorithm. Principal Component Analysis (PCA) is used to reduce\nthe dimensions of the multivariate data set, so that the representative points\ncan be generated efficiently and its Representativeness has been compared\nagainst Random Sampling of points from the data set.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2016 22:35:49 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Ganesan", "Ashwinkumar", ""], ["Oates", "Tim", ""], ["Schmill", "Matt", ""]]}, {"id": "1610.06085", "submitter": "Saqib Iqbal", "authors": "Saqib Iqbal, Ali Zulqurnain, Yaqoob Wani, Khalid Hussain", "title": "The survey of sentiment and opinion mining for behavior analysis of\n  social media", "comments": "21-28, International Journal of Computer Science & Engineering Survey\n  (IJCSES), Vol.6, No.5, October 2015", "journal-ref": null, "doi": "10.5121/ijcses.2015.6502", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, internet has changed the world into a global village. Social Media\nhas reduced the gaps among the individuals. Previously communication was a time\nconsuming and expensive task between the people. Social Media has earned fame\nbecause it is a cheaper and faster communication provider. Besides, social\nmedia has allowed us to reduce the gaps of physical distance, it also generates\nand preserves huge amount of data. The data are very valuable and it presents\nassociation degree between people and their opinions. The comprehensive\nanalysis of the methods which are used on user behavior prediction is presented\nin this paper. This comparison will provide a detailed information, pros and\ncons in the domain of sentiment and opinion mining.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 09:09:06 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Iqbal", "Saqib", ""], ["Zulqurnain", "Ali", ""], ["Wani", "Yaqoob", ""], ["Hussain", "Khalid", ""]]}, {"id": "1610.06382", "submitter": "Andreas M. Wahl", "authors": "Andreas M. Wahl, Gregor Endler, Peter K. Schwab, Sebastian Herbst,\n  Richard Lenz", "title": "Anfrage-getriebener Wissenstransfer zur Unterstuetzung von\n  Datenanalysten", "comments": "in German", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In larger organizations, multiple teams of data scientists have to integrate\ndata from heterogeneous data sources as preparation for data analysis tasks.\nWriting effective analytical queries requires data scientists to have in-depth\nknowledge of the existence, semantics, and usage context of data sources. Once\ngathered, such knowledge is informally shared within a specific team of data\nscientists, but usually is neither formalized nor shared with other teams.\nPotential synergies remain unused. We therefore introduce a novel approach\nwhich extends data management systems with additional knowledge-sharing\ncapabilities to facilitate user collaboration without altering established data\nanalysis workflows. Relevant collective knowledge from the query log is\nextracted to support data source discovery and incremental data integration.\nExtracted knowledge is formalized and provided at query time.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 12:41:31 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Wahl", "Andreas M.", ""], ["Endler", "Gregor", ""], ["Schwab", "Peter K.", ""], ["Herbst", "Sebastian", ""], ["Lenz", "Richard", ""]]}, {"id": "1610.06431", "submitter": "Ilya Safro", "authors": "Neela Avudaiappan, Alexander Herzog, Sneha Kadam, Yuheng Du, Jason\n  Thatcher, Ilya Safro", "title": "Detecting and Summarizing Emergent Events in Microblogs and Social Media\n  Streams by Dynamic Centralities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for detecting and summarizing emergent keywords have been extensively\nstudied since social media and microblogging activities have started to play an\nimportant role in data analysis and decision making. We present a system for\nmonitoring emergent keywords and summarizing a document stream based on the\ndynamic semantic graphs of streaming documents. We introduce the notion of\ndynamic eigenvector centrality for ranking emergent keywords, and present an\nalgorithm for summarizing emergent events that is based on the minimum weight\nset cover. We demonstrate our system with an analysis of streaming Twitter data\nrelated to public security events.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 14:32:37 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Avudaiappan", "Neela", ""], ["Herzog", "Alexander", ""], ["Kadam", "Sneha", ""], ["Du", "Yuheng", ""], ["Thatcher", "Jason", ""], ["Safro", "Ilya", ""]]}, {"id": "1610.06468", "submitter": "Jimmy Lin", "authors": "Charles L. A. Clarke, Gordon V. Cormack, Jimmy Lin, Adam Roegiest", "title": "Ten Blue Links on Mars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores a simple question: How would we provide a high-quality\nsearch experience on Mars, where the fundamental physical limit is\nspeed-of-light propagation delays on the order of tens of minutes? On Earth,\nusers are accustomed to nearly instantaneous response times from search\nengines. Is it possible to overcome orders-of-magnitude longer latency to\nprovide a tolerable user experience on Mars? In this paper, we formulate the\nsearching from Mars problem as a tradeoff between \"effort\" (waiting for\nresponses from Earth) and \"data transfer\" (pre-fetching or caching data on\nMars). The contribution of our work is articulating this design space and\npresenting two case studies that explore the effectiveness of baseline\ntechniques, using publicly available data from the TREC Total Recall and\nSessions Tracks. We intend for this research problem to be aspirational and\ninspirational - even if one is not convinced by the premise of Mars\ncolonization, there are Earth-based scenarios such as searching from a rural\nvillage in India that share similar constraints, thus making the problem worthy\nof exploration and attention from researchers.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 15:57:39 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Clarke", "Charles L. A.", ""], ["Cormack", "Gordon V.", ""], ["Lin", "Jimmy", ""], ["Roegiest", "Adam", ""]]}, {"id": "1610.07119", "submitter": "Yi Tay", "authors": "Minh C. Phan, Yi Tay, Tuan-Anh Nguyen Pham", "title": "Cross Device Matching for Online Advertising with Neural Feature\n  Ensembles : First Place Solution at CIKM Cup 2016", "comments": "4 pages Competition Report for CIKM Cup", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the 1st place winning approach for the CIKM Cup 2016 Challenge.\nIn this paper, we provide an approach to reasonably identify same users across\nmultiple devices based on browsing logs. Our approach regards a candidate\nranking problem as pairwise classification and utilizes an unsupervised neural\nfeature ensemble approach to learn latent features of users. Combined with\ntraditional hand crafted features, each user pair feature is fed into a\nsupervised classifier in order to perform pairwise classification. Lastly, we\npropose supervised and unsupervised inference techniques.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 03:25:05 GMT"}, {"version": "v2", "created": "Sun, 19 Feb 2017 03:33:03 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Phan", "Minh C.", ""], ["Tay", "Yi", ""], ["Pham", "Tuan-Anh Nguyen", ""]]}, {"id": "1610.07328", "submitter": "Chen Luo", "authors": "Chen Luo, Anshumali Shrivastava", "title": "SSH (Sketch, Shingle, & Hash) for Indexing Massive-Scale Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Similarity search on time series is a frequent operation in large-scale\ndata-driven applications. Sophisticated similarity measures are standard for\ntime series matching, as they are usually misaligned. Dynamic Time Warping or\nDTW is the most widely used similarity measure for time series because it\ncombines alignment and matching at the same time. However, the alignment makes\nDTW slow. To speed up the expensive similarity search with DTW, branch and\nbound based pruning strategies are adopted. However, branch and bound based\npruning are only useful for very short queries (low dimensional time series),\nand the bounds are quite weak for longer queries. Due to the loose bounds\nbranch and bound pruning strategy boils down to a brute-force search.\n  To circumvent this issue, we design SSH (Sketch, Shingle, & Hashing), an\nefficient and approximate hashing scheme which is much faster than the\nstate-of-the-art branch and bound searching technique: the UCR suite. SSH uses\na novel combination of sketching, shingling and hashing techniques to produce\n(probabilistic) indexes which align (near perfectly) with DTW similarity\nmeasure. The generated indexes are then used to create hash buckets for\nsub-linear search. Our results show that SSH is very effective for longer time\nsequence and prunes around 95% candidates, leading to the massive speedup in\nsearch with DTW. Empirical results on two large-scale benchmark time series\ndata show that our proposed method can be around 20 times faster than the\nstate-of-the-art package (UCR suite) without any significant loss in accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 08:33:44 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Luo", "Chen", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1610.07363", "submitter": "Arkaitz Zubiaga", "authors": "Arkaitz Zubiaga, Maria Liakata, Rob Procter", "title": "Learning Reporting Dynamics during Breaking News for Rumour Detection in\n  Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breaking news leads to situations of fast-paced reporting in social media,\nproducing all kinds of updates related to news stories, albeit with the caveat\nthat some of those early updates tend to be rumours, i.e., information with an\nunverified status at the time of posting. Flagging information that is\nunverified can be helpful to avoid the spread of information that may turn out\nto be false. Detection of rumours can also feed a rumour tracking system that\nultimately determines their veracity. In this paper we introduce a novel\napproach to rumour detection that learns from the sequential dynamics of\nreporting during breaking news in social media to detect rumours in new\nstories. Using Twitter datasets collected during five breaking news stories, we\nexperiment with Conditional Random Fields as a sequential classifier that\nleverages context learnt during an event for rumour detection, which we compare\nwith the state-of-the-art rumour detection system as well as other baselines.\nIn contrast to existing work, our classifier does not need to observe tweets\nquerying a piece of information to deem it a rumour, but instead we detect\nrumours from the tweet alone by exploiting context learnt during the event. Our\nclassifier achieves competitive performance, beating the state-of-the-art\nclassifier that relies on querying tweets with improved precision and recall,\nas well as outperforming our best baseline with nearly 40% improvement in terms\nof F1 score. The scale and diversity of our experiments reinforces the\ngeneralisability of our classifier.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 11:25:24 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Zubiaga", "Arkaitz", ""], ["Liakata", "Maria", ""], ["Procter", "Rob", ""]]}, {"id": "1610.07703", "submitter": "Ilya Safro", "authors": "Chris Gropp, Alexander Herzog, Ilya Safro, Paul W. Wilson, Amy W. Apon", "title": "Scalable Dynamic Topic Modeling with Clustered Latent Dirichlet\n  Allocation (CLDA)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling, a method for extracting the underlying themes from a\ncollection of documents, is an increasingly important component of the design\nof intelligent systems enabling the sense-making of highly dynamic and diverse\nstreams of text data. Traditional methods such as Dynamic Topic Modeling (DTM)\ndo not lend themselves well to direct parallelization because of dependencies\nfrom one time step to another. In this paper, we introduce and empirically\nanalyze Clustered Latent Dirichlet Allocation (CLDA), a method for extracting\ndynamic latent topics from a collection of documents. Our approach is based on\ndata decomposition in which the data is partitioned into segments, followed by\ntopic modeling on the individual segments. The resulting local models are then\ncombined into a global solution using clustering. The decomposition and\nresulting parallelization leads to very fast runtime even on very large\ndatasets. Our approach furthermore provides insight into how the composition of\ntopics changes over time and can also be applied using other data partitioning\nstrategies over any discrete features of the data, such as geographic features\nor classes of users. In this paper CLDA is applied successfully to seventeen\nyears of NIPS conference papers (2,484 documents and 3,280,697 words),\nseventeen years of computer science journal abstracts (533,560 documents and\n32,551,540 words), and to forty years of the PubMed corpus (4,025,978 documents\nand 273,853,980 words).\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 01:50:24 GMT"}, {"version": "v2", "created": "Sun, 15 Oct 2017 04:06:39 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 14:37:41 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Gropp", "Chris", ""], ["Herzog", "Alexander", ""], ["Safro", "Ilya", ""], ["Wilson", "Paul W.", ""], ["Apon", "Amy W.", ""]]}, {"id": "1610.08078", "submitter": "Tanay Kumar Saha", "authors": "Tanay Kumar Saha, Shafiq Joty, Naeemul Hassan and Mohammad Al Hasan", "title": "Dis-S2V: Discourse Informed Sen2Vec", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector representation of sentences is important for many text processing\ntasks that involve clustering, classifying, or ranking sentences. Recently,\ndistributed representation of sentences learned by neural models from unlabeled\ndata has been shown to outperform the traditional bag-of-words representation.\nHowever, most of these learning methods consider only the content of a sentence\nand disregard the relations among sentences in a discourse by and large.\n  In this paper, we propose a series of novel models for learning latent\nrepresentations of sentences (Sen2Vec) that consider the content of a sentence\nas well as inter-sentence relations. We first represent the inter-sentence\nrelations with a language network and then use the network to induce contextual\ninformation into the content-based Sen2Vec models. Two different approaches are\nintroduced to exploit the information in the network. Our first approach\nretrofits (already trained) Sen2Vec vectors with respect to the network in two\ndifferent ways: (1) using the adjacency relations of a node, and (2) using a\nstochastic sampling method which is more flexible in sampling neighbors of a\nnode. The second approach uses a regularizer to encode the information in the\nnetwork into the existing Sen2Vec model. Experimental results show that our\nproposed models outperform existing methods in three fundamental information\nsystem tasks demonstrating the effectiveness of our approach. The models\nleverage the computational power of multi-core CPUs to achieve fine-grained\ncomputational efficiency. We make our code publicly available upon acceptance.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 20:19:35 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Saha", "Tanay Kumar", ""], ["Joty", "Shafiq", ""], ["Hassan", "Naeemul", ""], ["Hasan", "Mohammad Al", ""]]}, {"id": "1610.08095", "submitter": "Mengting Wan", "authors": "Mengting Wan, Julian McAuley", "title": "Modeling Ambiguity, Subjectivity, and Diverging Viewpoints in Opinion\n  Question Answering Systems", "comments": "10 pages, accepted by ICDM'2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product review websites provide an incredible lens into the wide variety of\nopinions and experiences of different people, and play a critical role in\nhelping users discover products that match their personal needs and\npreferences. To help address questions that can't easily be answered by reading\nothers' reviews, some review websites also allow users to pose questions to the\ncommunity via a question-answering (QA) system. As one would expect, just as\nopinions diverge among different reviewers, answers to such questions may also\nbe subjective, opinionated, and divergent. This means that answering such\nquestions automatically is quite different from traditional QA tasks, where it\nis assumed that a single `correct' answer is available. While recent work\nintroduced the idea of question-answering using product reviews, it did not\naccount for two aspects that we consider in this paper: (1) Questions have\nmultiple, often divergent, answers, and this full spectrum of answers should\nsomehow be used to train the system; and (2) What makes a `good' answer depends\non the asker and the answerer, and these factors should be incorporated in\norder for the system to be more personalized. Here we build a new QA dataset\nwith 800 thousand questions---and over 3.1 million answers---and show that\nexplicitly accounting for personalization and ambiguity leads both to\nquantitatively better answers, but also a more nuanced view of the range of\nsupporting, but subjective, opinions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 21:08:15 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Wan", "Mengting", ""], ["McAuley", "Julian", ""]]}, {"id": "1610.08117", "submitter": "Parvez Ahammad", "authors": "Heju Jiang, Scott Algatt, Parvez Ahammad", "title": "A recommender system for efficient discovery of new anomalies in\n  large-scale access logs", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, non-standard recommender system for large-scale security\npolicy management(SPM). Our system Helios discovers and recommends unknown and\nunseen anomalies in large-scale access logs with minimal supervision and no\nstarting information on users and items. Typical recommender systems assume\navailability of user- and item-related information, but such information is not\nusually available in access logs. To resolve this problem, we first use\ndiscrete categorical labels to construct categorical combinations from access\nlogs in a bootstrapping manner. Then, we utilize rank statistics of entity rank\nand order categorical combinations for recommendation. From a double-sided cold\nstart, with minimal supervision, Helios learns to recommend most salient\nanomalies at large-scale, and provides visualizations to security experts to\nexplain rationale behind the recommendations. Our experiments show Helios to be\nsuitable for large-scale applications: from cold starts, in less than 60\nminutes, Helios can analyze roughly 4.6 billion records in logs of 400GB with\nabout 300 million potential categorical combinations, then generate ranked\ncategorical combinations as recommended discoveries. We also show that, even\nwith limited computing resources, Helios accelerates unknown and unseen anomaly\ndiscovery process for SPM by 1 to 3 orders of magnitude, depending on use\ncases. In addition, Helios' design is flexible with metrics and measurement\nfields used for discoveries and recommendations. Overall, our system leads to\nmore efficient and customizable SPM processes with faster discoveries of unseen\nand unknown anomalies.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 23:29:34 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Jiang", "Heju", ""], ["Algatt", "Scott", ""], ["Ahammad", "Parvez", ""]]}, {"id": "1610.08136", "submitter": "Fernando Diaz", "authors": "Bhaskar Mitra, Fernando Diaz, Nick Craswell", "title": "Learning to Match Using Local and Distributed Representations of Text\n  for Web Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models such as latent semantic analysis and those based on neural embeddings\nlearn distributed representations of text, and match the query against the\ndocument in the latent semantic space. In traditional information retrieval\nmodels, on the other hand, terms have discrete or local representations, and\nthe relevance of a document is determined by the exact matches of query terms\nin the body text. We hypothesize that matching with distributed representations\ncomplements matching with traditional local representations, and that a\ncombination of the two is favorable. We propose a novel document ranking model\ncomposed of two separate deep neural networks, one that matches the query and\nthe document using a local representation, and another that matches the query\nand the document using learned distributed representations. The two networks\nare jointly trained as part of a single neural network. We show that this\ncombination or `duet' performs significantly better than either neural network\nindividually on a Web page ranking task, and also significantly outperforms\ntraditional baselines and other recently proposed models based on neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 01:10:05 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Mitra", "Bhaskar", ""], ["Diaz", "Fernando", ""], ["Craswell", "Nick", ""]]}, {"id": "1610.08442", "submitter": "Elad Yom-Tov", "authors": "Luca Soldaini and Elad Yom-Tov", "title": "Inferring individual attributes from search engine queries and auxiliary\n  information", "comments": null, "journal-ref": null, "doi": "10.1145/3038912.3052629", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet data has surfaced as a primary source for investigation of different\naspects of human behavior. A crucial step in such studies is finding a suitable\ncohort (i.e., a set of users) that shares a common trait of interest to\nresearchers. However, direct identification of users sharing this trait is\noften impossible, as the data available to researchers is usually anonymized to\npreserve user privacy. To facilitate research on specific topics of interest,\nespecially in medicine, we introduce an algorithm for identifying a trait of\ninterest in anonymous users. We illustrate how a small set of labeled examples,\ntogether with statistical information about the entire population, can be\naggregated to obtain labels on unseen examples. We validate our approach using\nlabeled data from the political domain.\n  We provide two applications of the proposed algorithm to the medical domain.\nIn the first, we demonstrate how to identify users whose search patterns\nindicate they might be suffering from certain types of cancer. In the second,\nwe detail an algorithm to predict the distribution of diseases given their\nincidence in a subset of the population at study, making it possible to predict\ndisease spread from partial epidemiological data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 18:08:16 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Soldaini", "Luca", ""], ["Yom-Tov", "Elad", ""]]}, {"id": "1610.08597", "submitter": "Sanjaya Wijeratne", "authors": "Sanjaya Wijeratne, Lakshika Balasuriya, Derek Doran, Amit Sheth", "title": "Word Embeddings to Enhance Twitter Gang Member Profile Identification", "comments": "7 pages, 1 figure, 2 tables, Published at IJCAI Workshop on Semantic\n  Machine Learning (SML 2016)", "journal-ref": "IJCAI Workshop on Semantic Machine Learning (SML 2016). pp. 18-24.\n  CEUR-WS, New York City, NY (07 2016)", "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gang affiliates have joined the masses who use social media to share thoughts\nand actions publicly. Interestingly, they use this public medium to express\nrecent illegal actions, to intimidate others, and to share outrageous images\nand statements. Agencies able to unearth these profiles may thus be able to\nanticipate, stop, or hasten the investigation of gang-related crimes. This\npaper investigates the use of word embeddings to help identify gang members on\nTwitter. Building on our previous work, we generate word embeddings that\ntranslate what Twitter users post in their profile descriptions, tweets,\nprofile images, and linked YouTube content to a real vector format amenable for\nmachine learning classification. Our experimental results show that pre-trained\nword embeddings can boost the accuracy of supervised learning algorithms\ntrained over gang members social media posts.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 03:21:49 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Wijeratne", "Sanjaya", ""], ["Balasuriya", "Lakshika", ""], ["Doran", "Derek", ""], ["Sheth", "Amit", ""]]}, {"id": "1610.09077", "submitter": "Danis Wilson", "authors": "Danis J. Wilson and Wei Zhang", "title": "Integrating Topic Models and Latent Factors for Recommendation", "comments": "11 pages, 3 figures, version 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research of personalized recommendation techniques today has mostly\nparted into two mainstream directions, i.e., the factorization-based approaches\nand topic models. Practically, they aim to benefit from the numerical ratings\nand textual reviews, correspondingly, which compose two major information\nsources in various real-world systems. However, although the two approaches are\nsupposed to be correlated for their same goal of accurate recommendation, there\nstill lacks a clear theoretical understanding of how their objective functions\ncan be mathematically bridged to leverage the numerical ratings and textual\nreviews collectively, and why such a bridge is intuitively reasonable to match\nup their learning procedures for the rating prediction and top-N recommendation\ntasks, respectively.\n  In this work, we exposit with mathematical analysis that, the vector-level\nrandomization functions to coordinate the optimization objectives of\nfactorizational and topic models unfortunately do not exist at all, although\nthey are usually pre-assumed and intuitively designed in the literature.\nFortunately, we also point out that one can avoid the seeking of such a\nrandomization function by optimizing a Joint Factorizational Topic (JFT) model\ndirectly. We apply our JFT model to restaurant recommendation, and study its\nperformance in both normal and cross-city recommendation scenarios, where the\nlatter is an extremely difficult task for its inherent cold-start nature.\nExperimental results on real-world datasets verified the appealing performance\nof our approach against previous methods, on both rating prediction and top-N\nrecommendation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 04:20:54 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 20:36:43 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Wilson", "Danis J.", ""], ["Zhang", "Wei", ""]]}, {"id": "1610.09225", "submitter": "Venkata Sasank Pagolu", "authors": "Venkata Sasank Pagolu, Kamal Nayan Reddy Challa, Ganapati Panda,\n  Babita Majhi", "title": "Sentiment Analysis of Twitter Data for Predicting Stock Market Movements", "comments": "6 pages 4 figures Conference Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting stock market movements is a well-known problem of interest.\nNow-a-days social media is perfectly representing the public sentiment and\nopinion about current events. Especially, twitter has attracted a lot of\nattention from researchers for studying the public sentiments. Stock market\nprediction on the basis of public sentiments expressed on twitter has been an\nintriguing field of research. Previous studies have concluded that the\naggregate public mood collected from twitter may well be correlated with Dow\nJones Industrial Average Index (DJIA). The thesis of this work is to observe\nhow well the changes in stock prices of a company, the rises and falls, are\ncorrelated with the public opinions being expressed in tweets about that\ncompany. Understanding author's opinion from a piece of text is the objective\nof sentiment analysis. The present paper have employed two different textual\nrepresentations, Word2vec and N-gram, for analyzing the public sentiments in\ntweets. In this paper, we have applied sentiment analysis and supervised\nmachine learning principles to the tweets extracted from twitter and analyze\nthe correlation between stock market movements of a company and sentiments in\ntweets. In an elaborate way, positive news and tweets in social media about a\ncompany would definitely encourage people to invest in the stocks of that\ncompany and as a result the stock price of that company would increase. At the\nend of the paper, it is shown that a strong correlation exists between the rise\nand falls in stock prices with the public sentiments in tweets.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 14:07:43 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Pagolu", "Venkata Sasank", ""], ["Challa", "Kamal Nayan Reddy", ""], ["Panda", "Ganapati", ""], ["Majhi", "Babita", ""]]}, {"id": "1610.09226", "submitter": "Pavlina Fragkou", "authors": "Pavlina Fragkou", "title": "Text Segmentation using Named Entity Recognition and Co-reference\n  Resolution in English and Greek Texts", "comments": "32 pages. arXiv admin note: text overlap with arXiv:1308.0661,\n  arXiv:1204.2847 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine the benefit of performing named entity recognition\n(NER) and co-reference resolution to an English and a Greek corpus used for\ntext segmentation. The aim here is to examine whether the combination of text\nsegmentation and information extraction can be beneficial for the\nidentification of the various topics that appear in a document. NER was\nperformed manually in the English corpus and was compared with the output\nproduced by publicly available annotation tools while, an already existing tool\nwas used for the Greek corpus. Produced annotations from both corpora were\nmanually corrected and enriched to cover four types of named entities.\nCo-reference resolution i.e., substitution of every reference of the same\ninstance with the same named entity identifier was subsequently performed. The\nevaluation, using five text segmentation algorithms for the English corpus and\nfour for the Greek corpus leads to the conclusion that, the benefit highly\ndepends on the segment's topic, the number of named entity instances appearing\nin it, as well as the segment's length.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 14:09:33 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Fragkou", "Pavlina", ""]]}, {"id": "1610.09274", "submitter": "Guang-He Lee", "authors": "Guang-He Lee, Shao-Wen Yang, Shou-De Lin", "title": "Toward Implicit Sample Noise Modeling: Deviation-driven Matrix\n  Factorization", "comments": "6 pages + 1 reference page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective function of a matrix factorization model usually aims to\nminimize the average of a regression error contributed by each element.\nHowever, given the existence of stochastic noises, the implicit deviations of\nsample data from their true values are almost surely diverse, which makes each\ndata point not equally suitable for fitting a model. In this case, simply\naveraging the cost among data in the objective function is not ideal.\nIntuitively we would like to emphasize more on the reliable instances (i.e.,\nthose contain smaller noise) while training a model. Motivated by such\nobservation, we derive our formula from a theoretical framework for optimal\nweighting under heteroscedastic noise distribution. Specifically, by modeling\nand learning the deviation of data, we design a novel matrix factorization\nmodel. Our model has two advantages. First, it jointly learns the deviation and\nconducts dynamic reweighting of instances, allowing the model to converge to a\nbetter solution. Second, during learning the deviated instances are assigned\nlower weights, which leads to faster convergence since the model does not need\nto overfit the noise. The experiments are conducted in clean recommendation and\nnoisy sensor datasets to test the effectiveness of the model in various\nscenarios. The results show that our model outperforms the state-of-the-art\nfactorization and deep learning models in both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 15:33:25 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Lee", "Guang-He", ""], ["Yang", "Shao-Wen", ""], ["Lin", "Shou-De", ""]]}, {"id": "1610.09428", "submitter": "Moontae Lee", "authors": "Moontae Lee, Seok Hyun Jin, David Mimno", "title": "Beyond Exchangeability: The Chinese Voting Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many online communities present user-contributed responses such as reviews of\nproducts and answers to questions. User-provided helpfulness votes can\nhighlight the most useful responses, but voting is a social process that can\ngain momentum based on the popularity of responses and the polarity of existing\nvotes. We propose the Chinese Voting Process (CVP) which models the evolution\nof helpfulness votes as a self-reinforcing process dependent on position and\npresentation biases. We evaluate this model on Amazon product reviews and more\nthan 80 StackExchange forums, measuring the intrinsic quality of individual\nresponses and behavioral coefficients of different communities.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 23:38:22 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Lee", "Moontae", ""], ["Jin", "Seok Hyun", ""], ["Mimno", "David", ""]]}, {"id": "1610.09516", "submitter": "Sanjaya Wijeratne", "authors": "Lakshika Balasuriya, Sanjaya Wijeratne, Derek Doran, Amit Sheth", "title": "Finding Street Gang Members on Twitter", "comments": "8 pages, 9 figures, 2 tables, Published as a full paper at 2016\n  IEEE/ACM International Conference on Advances in Social Networks Analysis and\n  Mining (ASONAM 2016)", "journal-ref": "The 2016 IEEE/ACM Int. Conf. on Advances in Social Networks\n  Analysis and Mining. vol. 8, pp. 685-692. San Francisco, CA, USA (2016)", "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most street gang members use Twitter to intimidate others, to present\noutrageous images and statements to the world, and to share recent illegal\nactivities. Their tweets may thus be useful to law enforcement agencies to\ndiscover clues about recent crimes or to anticipate ones that may occur.\nFinding these posts, however, requires a method to discover gang member Twitter\nprofiles. This is a challenging task since gang members represent a very small\npopulation of the 320 million Twitter users. This paper studies the problem of\nautomatically finding gang members on Twitter. It outlines a process to curate\none of the largest sets of verifiable gang member profiles that have ever been\nstudied. A review of these profiles establishes differences in the language,\nimages, YouTube links, and emojis gang members use compared to the rest of the\nTwitter population. Features from this review are used to train a series of\nsupervised classifiers. Our classifier achieves a promising F1 score with a low\nfalse positive rate.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 14:30:57 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Balasuriya", "Lakshika", ""], ["Wijeratne", "Sanjaya", ""], ["Doran", "Derek", ""], ["Sheth", "Amit", ""]]}, {"id": "1610.09982", "submitter": "Sanjay Chakraborty", "authors": "Lopamudra Dey, Sanjay Chakraborty, Anuraag Biswas, Beepa Bose, Sweta\n  Tiwari", "title": "Sentiment Analysis of Review Datasets Using Naive Bayes and K-NN\n  Classifier", "comments": "Volume-8, Issue-4, pp.54-62, 2016", "journal-ref": null, "doi": "10.5815/ijieeb.2016.04.07", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of Web 2.0 has led to an increase in the amount of sentimental\ncontent available in the Web. Such content is often found in social media web\nsites in the form of movie or product reviews, user comments, testimonials,\nmessages in discussion forums etc. Timely discovery of the sentimental or\nopinionated web content has a number of advantages, the most important of all\nbeing monetization. Understanding of the sentiments of human masses towards\ndifferent entities and products enables better services for contextual\nadvertisements, recommendation systems and analysis of market trends. The focus\nof our project is sentiment focussed web crawling framework to facilitate the\nquick discovery of sentimental contents of movie reviews and hotel reviews and\nanalysis of the same. We use statistical methods to capture elements of\nsubjective style and the sentence polarity. The paper elaborately discusses two\nsupervised machine learning algorithms: K-Nearest Neighbour(K-NN) and Naive\nBayes and compares their overall accuracy, precisions as well as recall values.\nIt was seen that in case of movie reviews Naive Bayes gave far better results\nthan K-NN but for hotel reviews these algorithms gave lesser, almost same\naccuracies.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 15:45:41 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Dey", "Lopamudra", ""], ["Chakraborty", "Sanjay", ""], ["Biswas", "Anuraag", ""], ["Bose", "Beepa", ""], ["Tiwari", "Sweta", ""]]}, {"id": "1610.10000", "submitter": "Xueqing Liu", "authors": "Xueqing Liu, Chengxiang Zhai, Wei Han, Onur Gungor", "title": "Numerical Facet Range Partition: Evaluation Metric and Methods", "comments": "accepted to WWW 2017 Industry Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faceted navigation is a very useful component in today's search engines. It\nis especially useful when user has an exploratory information need or prefer\ncertain attribute values than others. Existing work has tried to optimize\nfaceted systems in many aspects, but little work has been done on optimizing\nnumerical facet ranges (e.g., price ranges of product). In this paper, we\nintroduce for the first time the research problem on numerical facet range\npartition and formally frame it as an optimization problem. To enable\nquantitative evaluation of a partition algorithm, we propose an evaluation\nmetric to be applied to search engine logs. We further propose two range\npartition algorithms that computationally optimize the defined metric.\nExperimental results on a two-month search log from a major e-Commerce engine\nshow that our proposed method can significantly outperform baseline.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 16:25:15 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 22:02:18 GMT"}, {"version": "v3", "created": "Mon, 27 Feb 2017 19:20:51 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Liu", "Xueqing", ""], ["Zhai", "Chengxiang", ""], ["Han", "Wei", ""], ["Gungor", "Onur", ""]]}, {"id": "1610.10001", "submitter": "Leonid Boytsov", "authors": "Leonid Boytsov, David Novak, Yury Malkov, Eric Nyberg", "title": "Off the Beaten Path: Let's Replace Term-Based Retrieval with k-NN Search", "comments": null, "journal-ref": null, "doi": "10.1145/2983323.2983815", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieval pipelines commonly rely on a term-based search to obtain candidate\nrecords, which are subsequently re-ranked. Some candidates are missed by this\napproach, e.g., due to a vocabulary mismatch. We address this issue by\nreplacing the term-based search with a generic k-NN retrieval algorithm, where\na similarity function can take into account subtle term associations. While an\nexact brute-force k-NN search using this similarity function is slow, we\ndemonstrate that an approximate algorithm can be nearly two orders of magnitude\nfaster at the expense of only a small loss in accuracy. A retrieval pipeline\nusing an approximate k-NN search can be more effective and efficient than the\nterm-based pipeline. This opens up new possibilities for designing effective\nretrieval pipelines. Our software (including data-generating code) and\nderivative data based on the Stack Overflow collection is available online.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 16:27:08 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Boytsov", "Leonid", ""], ["Novak", "David", ""], ["Malkov", "Yury", ""], ["Nyberg", "Eric", ""]]}]