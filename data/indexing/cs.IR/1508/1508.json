[{"id": "1508.00027", "submitter": "Miha Grcar", "authors": "Petra Kralj Novak, Miha Grcar, Borut Sluban, Igor Mozetic", "title": "Analysis of Financial News with NewsStream", "comments": null, "journal-ref": null, "doi": null, "report-no": "IJS-DP-11965", "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unstructured data, such as news and blogs, can provide valuable insights into\nthe financial world. We present the NewsStream portal, an intuitive and\neasy-to-use tool for news analytics, which supports interactive querying and\nvisualizations of the documents at different levels of detail. It relies on a\nscalable architecture for real-time processing of a continuous stream of\ntextual data, which incorporates data acquisition, cleaning, natural-language\npreprocessing and semantic annotation components. It has been running for over\ntwo years and collected over 18 million news articles and blog posts. The\nNewsStream portal can be used to answer the questions when, how often, in what\ncontext, and with what sentiment was a financial entity or term mentioned in a\ncontinuous stream of news and blogs, and therefore providing a complement to\nnews aggregators. We illustrate some features of our system in three use cases:\nrelations between the rating agencies and the PIIGS countries, reflection of\nfinancial news on credit default swap (CDS) prices, the emergence of the\nBitcoin digital currency, and visualizing how the world is connected through\nnews.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2015 20:44:04 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 08:23:18 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Novak", "Petra Kralj", ""], ["Grcar", "Miha", ""], ["Sluban", "Borut", ""], ["Mozetic", "Igor", ""]]}, {"id": "1508.00189", "submitter": "Devendra Sachan", "authors": "Devendra Singh Sachan, Shailesh Kumar", "title": "Class Vectors: Embedding representation of Document Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed representations of words and paragraphs as semantic embeddings in\nhigh dimensional data are used across a number of Natural Language\nUnderstanding tasks such as retrieval, translation, and classification. In this\nwork, we propose \"Class Vectors\" - a framework for learning a vector per class\nin the same embedding space as the word and paragraph embeddings. Similarity\nbetween these class vectors and word vectors are used as features to classify a\ndocument to a class. In experiment on several sentiment analysis tasks such as\nYelp reviews and Amazon electronic product reviews, class vectors have shown\nbetter or comparable results in classification while learning very meaningful\nclass embeddings.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 04:17:40 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Sachan", "Devendra Singh", ""], ["Kumar", "Shailesh", ""]]}, {"id": "1508.00973", "submitter": "Peixian Chen", "authors": "Peixian Chen, Nevin L. Zhang, Leonard K.M. Poon, Zhourong Chen", "title": "Progressive EM for Latent Tree Models and Hierarchical Topic Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical latent tree analysis (HLTA) is recently proposed as a new method\nfor topic detection. It differs fundamentally from the LDA-based methods in\nterms of topic definition, topic-document relationship, and learning method. It\nhas been shown to discover significantly more coherent topics and better topic\nhierarchies. However, HLTA relies on the Expectation-Maximization (EM)\nalgorithm for parameter estimation and hence is not efficient enough to deal\nwith large datasets. In this paper, we propose a method to drastically speed up\nHLTA using a technique inspired by recent advances in the moments method.\nEmpirical experiments show that our method greatly improves the efficiency of\nHLTA. It is as efficient as the state-of-the-art LDA-based method for\nhierarchical topic detection and finds substantially better topics and topic\nhierarchies.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 05:00:32 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Chen", "Peixian", ""], ["Zhang", "Nevin L.", ""], ["Poon", "Leonard K. M.", ""], ["Chen", "Zhourong", ""]]}, {"id": "1508.01011", "submitter": "Dongxu Zhang", "authors": "Dongxu Zhang, Tianyi Luo, Dong Wang and Rong Liu", "title": "Learning from LDA using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation (LDA) is a three-level hierarchical Bayesian\nmodel for topic inference. In spite of its great success, inferring the latent\ntopic distribution with LDA is time-consuming. Motivated by the transfer\nlearning approach proposed by~\\newcite{hinton2015distilling}, we present a\nnovel method that uses LDA to supervise the training of a deep neural network\n(DNN), so that the DNN can approximate the costly LDA inference with less\ncomputation. Our experiments on a document classification task show that a\nsimple DNN can learn the LDA behavior pretty well, while the inference is\nspeeded up tens or hundreds of times.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 09:22:25 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Zhang", "Dongxu", ""], ["Luo", "Tianyi", ""], ["Wang", "Dong", ""], ["Liu", "Rong", ""]]}, {"id": "1508.01067", "submitter": "Jing Su", "authors": "Jing Su and Ois\\'in Boydell and Derek Greene and Gerard Lynch", "title": "Topic Stability over Noisy Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modelling techniques such as LDA have recently been applied to speech\ntranscripts and OCR output. These corpora may contain noisy or erroneous texts\nwhich may undermine topic stability. Therefore, it is important to know how\nwell a topic modelling algorithm will perform when applied to noisy data. In\nthis paper we show that different types of textual noise will have diverse\neffects on the stability of different topic models. From these observations, we\npropose guidelines for text corpus generation, with a focus on automatic speech\ntranscription. We also suggest topic model selection methods for noisy corpora.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 13:18:51 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Su", "Jing", ""], ["Boydell", "Ois\u00edn", ""], ["Greene", "Derek", ""], ["Lynch", "Gerard", ""]]}, {"id": "1508.01177", "submitter": "Melanie JI M\\\"uller", "authors": "Lucas Bernardi and Jaap Kamps and Julia Kiseleva and Melanie JI\n  M\\\"uller", "title": "The Continuous Cold Start Problem in e-Commerce Recommender Systems", "comments": "6 pages, 3 figures. 2nd Workshop on New Trends in Content-Based\n  Recommender Systems, RecSys 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many e-commerce websites use recommender systems to recommend items to users.\nWhen a user or item is new, the system may fail because not enough information\nis available on this user or item. Various solutions to this `cold-start\nproblem' have been proposed in the literature. However, many real-life\ne-commerce applications suffer from an aggravated, recurring version of\ncold-start even for known users or items, since many users visit the website\nrarely, change their interests over time, or exhibit different personas. This\npaper exposes the `Continuous Cold Start' (CoCoS) problem and its consequences\nfor content- and context-based recommendation from the viewpoint of typical\ne-commerce applications, illustrated with examples from a major travel\nrecommendation website, Booking.com.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 19:03:49 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Bernardi", "Lucas", ""], ["Kamps", "Jaap", ""], ["Kiseleva", "Julia", ""], ["M\u00fcller", "Melanie JI", ""]]}, {"id": "1508.01420", "submitter": "Luis Marujo", "authors": "Lu\\'is Marujo, Jos\\'e Port\\^elo, Wang Ling, David Martins de Matos,\n  Jo\\~ao P. Neto, Anatole Gershman, Jaime Carbonell, Isabel Trancoso, Bhiksha\n  Raj", "title": "Privacy-Preserving Multi-Document Summarization", "comments": "4 pages, In Proceedings of 2nd ACM SIGIR Workshop on\n  Privacy-Preserving Information Retrieval, August 2015. arXiv admin note: text\n  overlap with arXiv:1407.5416", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art extractive multi-document summarization systems are usually\ndesigned without any concern about privacy issues, meaning that all documents\nare open to third parties. In this paper we propose a privacy-preserving\napproach to multi-document summarization. Our approach enables other parties to\nobtain summaries without learning anything else about the original documents'\ncontent. We use a hashing scheme known as Secure Binary Embeddings to convert\ndocuments representation containing key phrases and bag-of-words into bit\nstrings, allowing the computation of approximate distances, instead of exact\nones. Our experiments indicate that our system yields similar results to its\nnon-private counterpart on standard multi-document evaluation datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 14:30:47 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Marujo", "Lu\u00eds", ""], ["Port\u00ealo", "Jos\u00e9", ""], ["Ling", "Wang", ""], ["de Matos", "David Martins", ""], ["Neto", "Jo\u00e3o P.", ""], ["Gershman", "Anatole", ""], ["Carbonell", "Jaime", ""], ["Trancoso", "Isabel", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1508.01571", "submitter": "Humberto Corona", "authors": "Humberto Corona, Michael P. O'Mahony", "title": "A Mood-based Genre Classification of Television Content", "comments": "in ACM Workshop on Recommendation Systems for Television and Online\n  Video 2014 Foster City, California USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of television content helps users organise and navigate\nthrough the large list of channels and programs now available. In this paper,\nwe address the problem of television content classification by exploiting text\ninformation extracted from program transcriptions. We present an analysis which\nadapts a model for sentiment that has been widely and successfully applied in\nother fields such as music or blog posts. We use a real-world dataset obtained\nfrom the Boxfish API to compare the performance of classifiers trained on a\nnumber of different feature sets. Our experiments show that, over a large\ncollection of television content, program genres can be represented in a\nthree-dimensional space of valence, arousal and dominance, and that promising\nclassification results can be achieved using features based on this\nrepresentation. This finding supports the use of the proposed representation of\ntelevision content as a feature space for similarity computation and\nrecommendation generation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 23:53:30 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Corona", "Humberto", ""], ["O'Mahony", "Michael P.", ""]]}, {"id": "1508.01648", "submitter": "Shiva Asadianfam", "authors": "Shiva Asadianfam, Mahboubeh Shamsi, Sima Asadianfam", "title": "Predicting academic major of students using bayesian networks to the\n  case of iran", "comments": "7 pages, 4 figures. http://airccse.org/journal/ijcax/. in\n  International Journal of Computer-Aided Technologies (IJCAx) Vol.2, No.3,\n  July 2015", "journal-ref": null, "doi": "10.5121/ijcax.2015.2304", "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, which took place current year in the city of Maragheh in IRAN.\nNumber of high school students in the fields of study: mathematics,\nExperimental Sciences, humanities, vocational, business and science were\nstudied and compared. The purpose of this research is to predict the academic\nmajor of high school students using Bayesian networks. The effective factors\nhave been used in academic major selection for the first time as an effective\nindicator of Bayesian networks. Evaluation of Impacts of indicators on each\nother, discretization data and processing them was performed by GeNIe. The\nproper course would be advised for students to continue their education.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 10:19:58 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Asadianfam", "Shiva", ""], ["Shamsi", "Mahboubeh", ""], ["Asadianfam", "Sima", ""]]}, {"id": "1508.01672", "submitter": "An Zeng", "authors": "An Zeng, Chi Ho Yeung, Matus Medo, Yi-Cheng Zhang", "title": "Modeling mutual feedback between users and recommender systems", "comments": "13 pages, 5 figures", "journal-ref": "Journal of Statistical Mechanics P07020 (2015)", "doi": "10.1088/1742-5468/2015/07/P07020", "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems daily influence our decisions on the Internet. While\nconsiderable attention has been given to issues such as recommendation accuracy\nand user privacy, the long-term mutual feedback between a recommender system\nand the decisions of its users has been neglected so far. We propose here a\nmodel of network evolution which allows us to study the complex dynamics\ninduced by this feedback, including the hysteresis effect which is typical for\nsystems with non-linear dynamics. Despite the popular belief that\nrecommendation helps users to discover new things, we find that the long-term\nuse of recommendation can contribute to the rise of extremely popular items and\nthus ultimately narrow the user choice. These results are supported by\nmeasurements of the time evolution of item popularity inequality in real\nsystems. We show that this adverse effect of recommendation can be tamed by\nsacrificing part of short-term recommendation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 12:30:22 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Zeng", "An", ""], ["Yeung", "Chi Ho", ""], ["Medo", "Matus", ""], ["Zhang", "Yi-Cheng", ""]]}, {"id": "1508.01696", "submitter": "Kasra Madadipouya", "authors": "Kasra Madadipouya", "title": "A Location-Based Movie Recommender System Using Collaborative Filtering", "comments": "7 pages in International Journal in Foundations of Computer Science &\n  Technology (IJFCST), Vol.5, No.4, July 2015", "journal-ref": null, "doi": "10.5121/ijfcst.2015.5402", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Available recommender systems mostly provide recommendations based on the\nusers preferences by utilizing traditional methods such as collaborative\nfiltering which only relies on the similarities between users and items.\nHowever, collaborative filtering might lead to provide poor recommendation\nbecause it does not rely on other useful available data such as users locations\nand hence the accuracy of the recommendations could be very low and\ninefficient. This could be very obvious in the systems that locations would\naffect users preferences highly such as movie recommender systems. In this\npaper a new location-based movie recommender system based on the collaborative\nfiltering is introduced for enhancing the accuracy and the quality of\nrecommendations. In this approach, users locations have been utilized and take\nin consideration in the entire processing of the recommendations and peer\nselections. The potential of the proposed approach in providing novel and\nbetter quality recommendations have been discussed through experiments in real\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 14:03:41 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Madadipouya", "Kasra", ""]]}, {"id": "1508.01929", "submitter": "Michal R\\r{u}\\v{z}i\\v{c}ka", "authors": "Martin L\\'i\\v{s}ka, Petr Sojka, Michal R\\r{u}\\v{z}i\\v{c}ka", "title": "Combining Text and Formula Queries in Math Information Retrieval:\n  Evaluation of Query Results Merging Strategies", "comments": null, "journal-ref": null, "doi": "10.1145/2810355.2810359", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specific to Math Information Retrieval is combining text with mathematical\nformulae both in documents and in queries. Rigorous evaluation of query\nexpansion and merging strategies combining math and standard textual keyword\nterms in a query are given. It is shown that techniques similar to those known\nfrom textual query processing may be applied in math information retrieval as\nwell, and lead to a cutting edge performance. Striping and merging partial\nresults from subqueries is one technique that improves results measured by\ninformation retrieval evaluation metrics like Bpref.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 17:18:40 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["L\u00ed\u0161ka", "Martin", ""], ["Sojka", "Petr", ""], ["R\u016f\u017ei\u010dka", "Michal", ""]]}, {"id": "1508.02127", "submitter": "Manvi", "authors": "Manvi, Komal Kumar Bhatia, Ashutosh Dixit", "title": "A novel design of hidden web crawler using ontology", "comments": "7 pages,8 figures,2 tables, International Journal of Engineering\n  Trends & Technology (IJETT),August 2015, ISSN: 2231-5381", "journal-ref": null, "doi": "10.14445/22315381/IJETT-V26P204", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Web is content hidden behind HTML forms. Since it represents a large\nportion of the structured, unstructured and dynamic data on the Web, accessing\nDeep-Web content has been a long challenge for the database community. This\npaper describes a crawler for accessing Deep-Web using Ontologies. Performance\nevaluation of the proposed work showed that this new approach has promising\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 04:56:08 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Manvi", "", ""], ["Bhatia", "Komal Kumar", ""], ["Dixit", "Ashutosh", ""]]}, {"id": "1508.02315", "submitter": "Justin F Brunelle", "authors": "Justin F. Brunelle, Michele C. Weigle, and Michael L. Nelson", "title": "Archiving Deferred Representations Using a Two-Tiered Crawling Approach", "comments": "To appear at iPRES2015 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web resources are increasingly interactive, resulting in resources that are\nincreasingly difficult to archive. The archival difficulty is based on the use\nof client-side technologies (e.g., JavaScript) to change the client-side state\nof a representation after it has initially loaded. We refer to these\nrepresentations as deferred representations. We can better archive deferred\nrepresentations using tools like headless browsing clients. We use 10,000 seed\nUniversal Resource Identifiers (URIs) to explore the impact of including\nPhantomJS -- a headless browsing tool -- into the crawling process by comparing\nthe performance of wget (the baseline), PhantomJS, and Heritrix. Heritrix\ncrawled 2.065 URIs per second, 12.15 times faster than PhantomJS and 2.4 times\nfaster than wget. However, PhantomJS discovered 531,484 URIs, 1.75 times more\nthan Heritrix and 4.11 times more than wget. To take advantage of the\nperformance benefits of Heritrix and the URI discovery of PhantomJS, we\nrecommend a tiered crawling strategy in which a classifier predicts whether a\nrepresentation will be deferred or not, and only resources with deferred\nrepresentations are crawled with PhantomJS while resources without deferred\nrepresentations are crawled with Heritrix. We show that this approach is 5.2\ntimes faster than using only PhantomJS and creates a frontier (set of URIs to\nbe crawled) 1.8 times larger than using only Heritrix.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 16:39:18 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Brunelle", "Justin F.", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1508.02496", "submitter": "Olivier Mor\\`ere", "authors": "Vijay Chandrasekhar, Jie Lin, Olivier Mor\\`ere, Hanlin Goh, Antoine\n  Veillard", "title": "A Practical Guide to CNNs and Fisher Vectors for Image Instance\n  Retrieval", "comments": "Deep Convolutional Neural Networks for instance retrieval, Fisher\n  Vectors, instance retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With deep learning becoming the dominant approach in computer vision, the use\nof representations extracted from Convolutional Neural Nets (CNNs) is quickly\ngaining ground on Fisher Vectors (FVs) as favoured state-of-the-art global\nimage descriptors for image instance retrieval. While the good performance of\nCNNs for image classification are unambiguously recognised, which of the two\nhas the upper hand in the image retrieval context is not entirely clear yet. In\nthis work, we propose a comprehensive study that systematically evaluates FVs\nand CNNs for image retrieval. The first part compares the performances of FVs\nand CNNs on multiple publicly available data sets. We investigate a number of\ndetails specific to each method. For FVs, we compare sparse descriptors based\non interest point detectors with dense single-scale and multi-scale variants.\nFor CNNs, we focus on understanding the impact of depth, architecture and\ntraining data on retrieval results. Our study shows that no descriptor is\nsystematically better than the other and that performance gains can usually be\nobtained by using both types together. The second part of the study focuses on\nthe impact of geometrical transformations such as rotations and scale changes.\nFVs based on interest point detectors are intrinsically resilient to such\ntransformations while CNNs do not have a built-in mechanism to ensure such\ninvariance. We show that performance of CNNs can quickly degrade in presence of\nrotations while they are far less affected by changes in scale. We then propose\na number of ways to incorporate the required invariances in the CNN pipeline.\nOverall, our work is intended as a reference guide offering practically useful\nand simply implementable guidelines to anyone looking for state-of-the-art\nglobal descriptors best suited to their specific image instance retrieval\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 07:15:07 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2015 10:51:50 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2015 11:30:22 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Chandrasekhar", "Vijay", ""], ["Lin", "Jie", ""], ["Mor\u00e8re", "Olivier", ""], ["Goh", "Hanlin", ""], ["Veillard", "Antoine", ""]]}, {"id": "1508.02552", "submitter": "Mansaf Alam Dr", "authors": "Mansaf Alam and Kishwar Sadaf", "title": "Web Search Result Clustering based on Heuristic Search and k-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Giving user a simple and well organized web search result has been a topic of\nactive information Retrieval (IR) research. Irrespective of how small or\nambiguous a query is, a user always wants the desired result on the first\ndisplay of an IR system. Clustering of an IR system result can render a way,\nwhich fulfills the actual information need of a user. In this paper, an\napproach to cluster an IR system result is presented.The approach is a\ncombination of heuristics and k-means technique using cosine similarity. Our\nheuristic approach detects the initial value of k for creating initial\ncentroids. This eliminates the problem of external specification of the value\nk, which may lead to unwanted result if wrongly specified. The centroids\ncreated in this way are more specific and meaningful in the context of web\nsearch result. Another advantage of the proposed method is the removal of the\nobjective means function of k-means which makes cluster sizes same. The end\nresult of the proposed approach consists of different clusters of documents\nhaving different sizes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 11:00:54 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Alam", "Mansaf", ""], ["Sadaf", "Kishwar", ""]]}, {"id": "1508.02884", "submitter": "Ernesto Diaz-Aviles", "authors": "Ernesto Diaz-Aviles (1), Fabio Pinelli (1), Karol Lynch (1), Zubair\n  Nabi (1), Yiannis Gkoufas (1), Eric Bouillet (1), Francesco Calabrese (1),\n  Eoin Coughlan (2), Peter Holland (2), Jason Salzwedel (2) ((1) IBM Research\n  -- Ireland, (2) IBM Now Factory -- Ireland, (3) Vodacom -- South Africa)", "title": "Towards Real-time Customer Experience Prediction for Telecommunication\n  Operators", "comments": "IEEE 2015 BigData Conference (to appear). Keywords: Telecom\n  operators; Customer Care; Big Data; Predictive Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telecommunications operators (telcos) traditional sources of income, voice\nand SMS, are shrinking due to customers using over-the-top (OTT) applications\nsuch as WhatsApp or Viber. In this challenging environment it is critical for\ntelcos to maintain or grow their market share, by providing users with as good\nan experience as possible on their network.\n  But the task of extracting customer insights from the vast amounts of data\ncollected by telcos is growing in complexity and scale everey day. How can we\nmeasure and predict the quality of a user's experience on a telco network in\nreal-time? That is the problem that we address in this paper.\n  We present an approach to capture, in (near) real-time, the mobile customer\nexperience in order to assess which conditions lead the user to place a call to\na telco's customer care center. To this end, we follow a supervised learning\napproach for prediction and train our 'Restricted Random Forest' model using,\nas a proxy for bad experience, the observed customer transactions in the telco\ndata feed before the user places a call to a customer care center.\n  We evaluate our approach using a rich dataset provided by a major African\ntelecommunication's company and a novel big data architecture for both the\ntraining and scoring of predictive models. Our empirical study shows our\nsolution to be effective at predicting user experience by inferring if a\ncustomer will place a call based on his current context.\n  These promising results open new possibilities for improved customer service,\nwhich will help telcos to reduce churn rates and improve customer experience,\nboth factors that directly impact their revenue growth.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 11:43:11 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2015 15:26:48 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Diaz-Aviles", "Ernesto", ""], ["Pinelli", "Fabio", ""], ["Lynch", "Karol", ""], ["Nabi", "Zubair", ""], ["Gkoufas", "Yiannis", ""], ["Bouillet", "Eric", ""], ["Calabrese", "Francesco", ""], ["Coughlan", "Eoin", ""], ["Holland", "Peter", ""], ["Salzwedel", "Jason", ""]]}, {"id": "1508.03110", "submitter": "Michael B Hynes", "authors": "Manda Winlaw, Michael B. Hynes, Anthony Caterini, Hans De Sterck", "title": "Algorithmic Acceleration of Parallel ALS for Collaborative Filtering:\n  Speeding up Distributed Big Data Recommendation in Spark", "comments": "Proceedings of ICPADS 2015, Melbourne, AU. 10 pages; 6 figures; 4\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.IR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering algorithms are important building blocks in many\npractical recommendation systems. For example, many large-scale data processing\nenvironments include collaborative filtering models for which the Alternating\nLeast Squares (ALS) algorithm is used to compute latent factor matrix\ndecompositions. In this paper, we propose an approach to accelerate the\nconvergence of parallel ALS-based optimization methods for collaborative\nfiltering using a nonlinear conjugate gradient (NCG) wrapper around the ALS\niterations. We also provide a parallel implementation of the accelerated\nALS-NCG algorithm in the Apache Spark distributed data processing environment,\nand an efficient line search technique as part of the ALS-NCG implementation\nthat requires only one pass over the data on distributed datasets. In serial\nnumerical experiments on a linux workstation and parallel numerical experiments\non a 16 node cluster with 256 computing cores, we demonstrate that the combined\nALS-NCG method requires many fewer iterations and less time than standalone ALS\nto reach movie rankings with high accuracy on the MovieLens 20M dataset. In\nparallel, ALS-NCG can achieve an acceleration factor of 4 or greater in clock\ntime when an accurate solution is desired; furthermore, the acceleration factor\nincreases as greater numerical precision is required in the solution. In\naddition, the NCG acceleration mechanism is efficient in parallel and scales\nlinearly with problem size on synthetic datasets with up to nearly 1 billion\nratings. The acceleration mechanism is general and may also be applicable to\nother optimization methods for collaborative filtering.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 03:37:04 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 16:53:49 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2016 23:52:03 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Winlaw", "Manda", ""], ["Hynes", "Michael B.", ""], ["Caterini", "Anthony", ""], ["De Sterck", "Hans", ""]]}, {"id": "1508.03298", "submitter": "Gilad Katz", "authors": "Gilad Katz and Bracha Shapira", "title": "Enabling Complex Wikipedia Queries - Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report we present a database schema used to store Wikipedia\nso it can be easily used in query-intensive applications. In addition to\nstoring the information in a way that makes it highly accessible, our schema\nenables users to easily formulate complex queries using information such as the\nanchor-text of links and their location in the page, the titles and number of\nredirect pages for each page and the paragraph structure of entity pages. We\nhave successfully used the schema in domains such as recommender systems,\ninformation retrieval and sentiment analysis. In order to assist other\nresearchers, we now make the schema and its content available online.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 18:35:06 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Katz", "Gilad", ""], ["Shapira", "Bracha", ""]]}, {"id": "1508.03856", "submitter": "Sheikh Muhammad Sarwar", "authors": "Sheikh Muhammad Sarwar, Mahamudul Hasan, Dmitry I. Ignatov", "title": "Two-stage Cascaded Classifier for Purchase Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe our machine learning solution for the RecSys\nChallenge, 2015. We have proposed a time efficient two-stage cascaded\nclassifier for the prediction of buy sessions and purchased items within such\nsessions. Based on the model, several interesting features found, and formation\nof our own test bed, we have achieved a reasonable score. Usage of Random\nForests helps us to cope with the effect of the multiplicity of good models\ndepending on varying subsets of features in the purchased items prediction and,\nin its turn, boosting is used as a suitable technique to overcome severe class\nimbalance of the buy-session prediction.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 19:27:35 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Sarwar", "Sheikh Muhammad", ""], ["Hasan", "Mahamudul", ""], ["Ignatov", "Dmitry I.", ""]]}, {"id": "1508.03868", "submitter": "Brendan Jou", "authors": "Brendan Jou, Tao Chen, Nikolaos Pappas, Miriam Redi, Mercan Topkara,\n  Shih-Fu Chang", "title": "Visual Affect Around the World: A Large-scale Multilingual Visual\n  Sentiment Ontology", "comments": "11 pages, to appear at ACM MM'15", "journal-ref": null, "doi": "10.1145/2733373.2806246", "report-no": null, "categories": "cs.MM cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every culture and language is unique. Our work expressly focuses on the\nuniqueness of culture and language in relation to human affect, specifically\nsentiment and emotion semantics, and how they manifest in social multimedia. We\ndevelop sets of sentiment- and emotion-polarized visual concepts by adapting\nsemantic structures called adjective-noun pairs, originally introduced by Borth\net al. (2013), but in a multilingual context. We propose a new\nlanguage-dependent method for automatic discovery of these adjective-noun\nconstructs. We show how this pipeline can be applied on a social multimedia\nplatform for the creation of a large-scale multilingual visual sentiment\nconcept ontology (MVSO). Unlike the flat structure in Borth et al. (2013), our\nunified ontology is organized hierarchically by multilingual clusters of\nvisually detectable nouns and subclusters of emotionally biased versions of\nthese nouns. In addition, we present an image-based prediction task to show how\ngeneralizable language-specific models are in a multilingual context. A new,\npublicly available dataset of >15.6K sentiment-biased visual concepts across 12\nlanguages with language-specific detector banks, >7.36M images and their\nmetadata is also released.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 21:43:59 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2015 16:33:13 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2015 19:07:14 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Jou", "Brendan", ""], ["Chen", "Tao", ""], ["Pappas", "Nikolaos", ""], ["Redi", "Miriam", ""], ["Topkara", "Mercan", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1508.03902", "submitter": "EPTCS", "authors": "Van Tien Hoang (IMT Lucca, Italy), Angelo Spognardi (IIT-CNR, Pisa,\n  Italy), Francesco Tiezzi (University of Camerino, Italy), Marinella Petrocchi\n  (IIT-CNR, Pisa, Italy), Rocco De Nicola (IMT Lucca, Italy)", "title": "Domain-specific queries and Web search personalization: some\n  investigations", "comments": "In Proceedings WWV 2015, arXiv:1508.03389", "journal-ref": "EPTCS 188, 2015, pp. 51-58", "doi": "10.4204/EPTCS.188.6", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major search engines deploy personalized Web results to enhance users'\nexperience, by showing them data supposed to be relevant to their interests.\nEven if this process may bring benefits to users while browsing, it also raises\nconcerns on the selection of the search results. In particular, users may be\nunknowingly trapped by search engines in protective information bubbles, called\n\"filter bubbles\", which can have the undesired effect of separating users from\ninformation that does not fit their preferences. This paper moves from early\nresults on quantification of personalization over Google search query results.\nInspired by previous works, we have carried out some experiments consisting of\nsearch queries performed by a battery of Google accounts with differently\nprepared profiles. Matching query results, we quantify the level of\npersonalization, according to topics of the queries and the profile of the\naccounts. This work reports initial results and it is a first step a for more\nextensive investigation to measure Web search personalization.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 01:51:11 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Hoang", "Van Tien", "", "IMT Lucca, Italy"], ["Spognardi", "Angelo", "", "IIT-CNR, Pisa,\n  Italy"], ["Tiezzi", "Francesco", "", "University of Camerino, Italy"], ["Petrocchi", "Marinella", "", "IIT-CNR, Pisa, Italy"], ["De Nicola", "Rocco", "", "IMT Lucca, Italy"]]}, {"id": "1508.03981", "submitter": "Olga Kolchyna", "authors": "Olga Kolchyna, Th'arsis T. P. Souza, Tomaso Aste, Philip C. Treleaven", "title": "In Quest of Significance: Identifying Types of Twitter Sentiment Events\n  that Predict Spikes in Sales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the power of Twitter events to predict consumer sales events by\nanalysing sales for 75 companies from the retail sector and over 150 million\ntweets mentioning those companies along with their sentiment. We suggest an\napproach for events identification on Twitter extending existing methodologies\nof event study. We also propose a robust method for clustering Twitter events\ninto different types based on their shape, which captures the varying dynamics\nof information propagation through the social network. We provide empirical\nevidence that through events differentiation based on their shape we can\nclearly identify types of Twitter events that have a more significant power to\npredict spikes in sales than the aggregated Twitter signal.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 11:35:13 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Kolchyna", "Olga", ""], ["Souza", "Th'arsis T. P.", ""], ["Aste", "Tomaso", ""], ["Treleaven", "Philip C.", ""]]}, {"id": "1508.04066", "submitter": "Tim Weninger PhD", "authors": "Tim Weninger, Rodrigo Palacios, Valter Crescenzi, Thomas Gottron,\n  Paolo Merialdo", "title": "Web Content Extraction - a Meta-Analysis of its Past and Thoughts on its\n  Future", "comments": "Accepted for publication in SIGKDD Explorations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a meta-analysis of several Web content extraction\nalgorithms, and make recommendations for the future of content extraction on\nthe Web. First, we find that nearly all Web content extractors do not consider\na very large, and growing, portion of modern Web pages. Second, it is well\nunderstood that wrapper induction extractors tend to break as the Web changes;\nheuristic/feature engineering extractors were thought to be immune to a Web\nsite's evolution, but we find that this is not the case: heuristic content\nextractor performance also tends to degrade over time due to the evolution of\nWeb site forms and practices. We conclude with recommendations for future work\nthat address these and other findings.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 15:52:23 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 14:13:14 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Weninger", "Tim", ""], ["Palacios", "Rodrigo", ""], ["Crescenzi", "Valter", ""], ["Gottron", "Thomas", ""], ["Merialdo", "Paolo", ""]]}, {"id": "1508.04562", "submitter": "Jingwei Zhang", "authors": "Jingwei Zhang, Aaron Gerow, Jaan Altosaar, James Evans, Richard Jean\n  So", "title": "Fast, Flexible Models for Discovering Topic Correlation across\n  Weakly-Related Collections", "comments": "EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weak topic correlation across document collections with different numbers of\ntopics in individual collections presents challenges for existing\ncross-collection topic models. This paper introduces two probabilistic topic\nmodels, Correlated LDA (C-LDA) and Correlated HDP (C-HDP). These address\nproblems that can arise when analyzing large, asymmetric, and potentially\nweakly-related collections. Topic correlations in weakly-related collections\ntypically lie in the tail of the topic distribution, where they would be\noverlooked by models unable to fit large numbers of topics. To efficiently\nmodel this long tail for large-scale analysis, our models implement a parallel\nsampling algorithm based on the Metropolis-Hastings and alias methods (Yuan et\nal., 2015). The models are first evaluated on synthetic data, generated to\nsimulate various collection-level asymmetries. We then present a case study of\nmodeling over 300k documents in collections of sciences and humanities research\nfrom JSTOR.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 08:30:37 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Zhang", "Jingwei", ""], ["Gerow", "Aaron", ""], ["Altosaar", "Jaan", ""], ["Evans", "James", ""], ["So", "Richard Jean", ""]]}, {"id": "1508.04819", "submitter": "Brian Keegan", "authors": "Brian C. Keegan, Shakked Lev, Ofer Arazy", "title": "Analyzing Organizational Routines in Online Knowledge Collaborations: A\n  Case for Sequence Analysis in CSCW", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC cs.IR physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research into socio-technical systems like Wikipedia has overlooked important\nstructural patterns in the coordination of distributed work. This paper argues\nfor a conceptual reorientation towards sequences as a fundamental unit of\nanalysis for understanding work routines in online knowledge collaboration. We\noutline a research agenda for researchers in computer-supported cooperative\nwork (CSCW) to understand the relationships, patterns, antecedents, and\nconsequences of sequential behavior using methods already developed in fields\nlike bio-informatics. Using a data set of 37,515 revisions from 16,616 unique\neditors to 96 Wikipedia articles as a case study, we analyze the prevalence and\nsignificance of different sequences of editing patterns. We illustrate the\nmixed method potential of sequence approaches by interpreting the frequent\npatterns as general classes of behavioral motifs. We conclude by discussing the\nmethodological opportunities for using sequence analysis for expanding existing\napproaches to analyzing and theorizing about co-production routines in online\nknowledge collaboration.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 22:36:21 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 15:17:09 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Keegan", "Brian C.", ""], ["Lev", "Shakked", ""], ["Arazy", "Ofer", ""]]}, {"id": "1508.05163", "submitter": "Yustinus Soelistio Eko", "authors": "Yustinus Eko Soelistio, Martinus Raditia Sigit Surendra", "title": "Simple Text Mining for Sentiment Analysis of Political Figure Using\n  Naive Bayes Classifier Method", "comments": "5 pages, published in the Proceedings of the 7th ICTS", "journal-ref": null, "doi": "10.12962/p9772338185001.a18", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text mining can be applied to many fields. One of the application is using\ntext mining in digital newspaper to do politic sentiment analysis. In this\npaper sentiment analysis is applied to get information from digital news\narticles about its positive or negative sentiment regarding particular\npolitician. This paper suggests a simple model to analyze digital newspaper\nsentiment polarity using naive Bayes classifier method. The model uses a set of\ninitial data to begin with which will be updated when new information appears.\nThe model showed promising result when tested and can be implemented to some\nother sentiment analysis problems.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 01:40:54 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Soelistio", "Yustinus Eko", ""], ["Surendra", "Martinus Raditia Sigit", ""]]}, {"id": "1508.05470", "submitter": "Leonid Boytsov", "authors": "Bilegsaikhan Naidan, Leonid Boytsov, Yury Malkov, David Novak", "title": "Non-Metric Space Library Manual", "comments": "Methodology paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document covers a library for fast similarity (k-NN)search. It describes\nonly search methods and distances (spaces). Details about building, installing,\nPython bindings can be found\nonline:https://github.com/searchivarius/nmslib/tree/v1.8/. Even though the\nlibrary contains a variety of exact metric-space access methods, our main focus\nis on more generic and approximate search methods, in particular, on methods\nfor non-metric spaces. NMSLIB is possibly the first library with a principled\nsupport for non-metric space searching.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 04:43:36 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 15:45:46 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2019 01:23:59 GMT"}, {"version": "v4", "created": "Fri, 7 Jun 2019 00:49:39 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Naidan", "Bilegsaikhan", ""], ["Boytsov", "Leonid", ""], ["Malkov", "Yury", ""], ["Novak", "David", ""]]}, {"id": "1508.05565", "submitter": "Weicong Ding", "authors": "Weicong Ding, Prakash Ishwar, Venkatesh Saligrama", "title": "Necessary and Sufficient Conditions and a Provably Efficient Algorithm\n  for Separable Topic Discovery", "comments": "Typo corrected; Revised argument in Lemma 3 and 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop necessary and sufficient conditions and a novel provably\nconsistent and efficient algorithm for discovering topics (latent factors) from\nobservations (documents) that are realized from a probabilistic mixture of\nshared latent factors that have certain properties. Our focus is on the class\nof topic models in which each shared latent factor contains a novel word that\nis unique to that factor, a property that has come to be known as separability.\nOur algorithm is based on the key insight that the novel words correspond to\nthe extreme points of the convex hull formed by the row-vectors of a suitably\nnormalized word co-occurrence matrix. We leverage this geometric insight to\nestablish polynomial computation and sample complexity bounds based on a few\nisotropic random projections of the rows of the normalized word co-occurrence\nmatrix. Our proposed random-projections-based algorithm is naturally amenable\nto an efficient distributed implementation and is attractive for modern\nweb-scale distributed data mining applications.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 03:44:26 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 18:26:33 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Ding", "Weicong", ""], ["Ishwar", "Prakash", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1508.05764", "submitter": "Mariano Beguerisse-D\\'iaz", "authors": "Mariano Beguerisse-D\\'iaz, Amy K. McLennan, Guillermo\n  Gardu\\~no-Hern\\'andez, Mauricio Barahona, Stanley J. Ulijaszek", "title": "The 'who' and 'what' of #diabetes on Twitter", "comments": "25 pages, 11 figures, 7 tables. Supplemental spreadsheet available\n  from http://journals.sagepub.com/doi/suppl/10.1177/2055207616688841, Digital\n  Health, Vol 3, 2017", "journal-ref": null, "doi": "10.1177/2055207616688841", "report-no": null, "categories": "physics.soc-ph cs.CY cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media are being increasingly used for health promotion, yet the\nlandscape of users, messages and interactions in such fora is poorly\nunderstood. Studies of social media and diabetes have focused mostly on\npatients, or public agencies addressing it, but have not looked broadly at all\nthe participants or the diversity of content they contribute. We study Twitter\nconversations about diabetes through the systematic analysis of 2.5 million\ntweets collected over 8 months and the interactions between their authors. We\naddress three questions: (1) what themes arise in these tweets?, (2) who are\nthe most influential users?, (3) which type of users contribute to which\nthemes? We answer these questions using a mixed-methods approach, integrating\ntechniques from anthropology, network science and information retrieval such as\nthematic coding, temporal network analysis, and community and topic detection.\nDiabetes-related tweets fall within broad thematic groups: health information,\nnews, social interaction, and commercial. At the same time, humorous messages\nand references to popular culture appear consistently, more than any other type\nof tweet. We classify authors according to their temporal 'hub' and 'authority'\nscores. Whereas the hub landscape is diffuse and fluid over time, top\nauthorities are highly persistent across time and comprise bloggers, advocacy\ngroups and NGOs related to diabetes, as well as for-profit entities without\nspecific diabetes expertise. Top authorities fall into seven interest\ncommunities as derived from their Twitter follower network. Our findings have\nimplications for public health professionals and policy makers who seek to use\nsocial media as an engagement tool and to inform policy design.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 11:51:19 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2015 17:42:13 GMT"}, {"version": "v3", "created": "Wed, 9 Nov 2016 21:19:36 GMT"}, {"version": "v4", "created": "Mon, 30 Jan 2017 18:52:48 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Beguerisse-D\u00edaz", "Mariano", ""], ["McLennan", "Amy K.", ""], ["Gardu\u00f1o-Hern\u00e1ndez", "Guillermo", ""], ["Barahona", "Mauricio", ""], ["Ulijaszek", "Stanley J.", ""]]}, {"id": "1508.06034", "submitter": "Jun-Ping Ng", "authors": "Jun-Ping Ng, Viktoria Abrecht", "title": "Better Summarization Evaluation with Word Embeddings for ROUGE", "comments": "Pre-print - To appear in proceedings of the Conference on Empirical\n  Methods in Natural Language Processing (EMNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ROUGE is a widely adopted, automatic evaluation measure for text\nsummarization. While it has been shown to correlate well with human judgements,\nit is biased towards surface lexical similarities. This makes it unsuitable for\nthe evaluation of abstractive summarization, or summaries with substantial\nparaphrasing. We study the effectiveness of word embeddings to overcome this\ndisadvantage of ROUGE. Specifically, instead of measuring lexical overlaps,\nword embeddings are used to compute the semantic similarity of the words used\nin summaries instead. Our experimental results show that our proposal is able\nto achieve better correlations with human judgements when measured with the\nSpearman and Kendall rank coefficients.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 05:04:53 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Ng", "Jun-Ping", ""], ["Abrecht", "Viktoria", ""]]}, {"id": "1508.06257", "submitter": "Homa Hosseinmardi", "authors": "Homa Hosseinmardi, Sabrina Arredondo Mattson, Rahat Ibn Rafiq, Richard\n  Han, Qin Lv, Shivakant Mishr", "title": "Prediction of Cyberbullying Incidents on the Instagram Social Network", "comments": "arXiv admin note: text overlap with arXiv:1503.03909", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyberbullying is a growing problem affecting more than half of all American\nteens. The main goal of this paper is to investigate fundamentally new\napproaches to understand and automatically detect and predict incidents of\ncyberbullying in Instagram, a media-based mobile social network. In this work,\nwe have collected a sample data set consisting of Instagram images and their\nassociated comments. We then designed a labeling study and employed human\ncontributors at the crowd-sourced CrowdFlower website to label these media\nsessions for cyberbullying. A detailed analysis of the labeled data is then\npresented, including a study of relationships between cyberbullying and a host\nof features such as cyberaggression, profanity, social graph features, temporal\ncommenting behavior, linguistic content, and image content. Using the labeled\ndata, we further design and evaluate the performance of classifiers to\nautomatically detect and pre- dict incidents of cyberbullying and\ncyberaggression.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 19:27:23 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Hosseinmardi", "Homa", ""], ["Mattson", "Sabrina Arredondo", ""], ["Rafiq", "Rahat Ibn", ""], ["Han", "Richard", ""], ["Lv", "Qin", ""], ["Mishr", "Shivakant", ""]]}, {"id": "1508.06374", "submitter": "Alexander Koplenig", "authors": "Alexander Koplenig", "title": "A fully data-driven method to identify (correlated) changes in\n  diachronic corpora", "comments": "typological changes only: reference-source-not-found-errors removed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a method for measuring synchronic corpus (dis-)similarity put\nforward by Kilgarriff (2001) is adapted and extended to identify trends and\ncorrelated changes in diachronic text data, using the Corpus of Historical\nAmerican English (Davies 2010a) and the Google Ngram Corpora (Michel et al.\n2010a). This paper shows that this fully data-driven method, which extracts\nword types that have undergone the most pronounced change in frequency in a\ngiven period of time, is computationally very cheap and that it allows\ninterpretations of diachronic trends that are both intuitively plausible and\nmotivated from the perspective of information theory. Furthermore, it\ndemonstrates that the method is able to identify correlated linguistic changes\nand diachronic shifts that can be linked to historical events. Finally, it can\nhelp to improve diachronic POS tagging and complement existing NLP approaches.\nThis indicates that the approach can facilitate an improved understanding of\ndiachronic processes in language change.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 06:18:51 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2015 06:29:43 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Koplenig", "Alexander", ""]]}, {"id": "1508.07744", "submitter": "Gilles Louppe", "authors": "Gilles Louppe, Hussein Al-Natsheh, Mateusz Susik, Eamonn Maguire", "title": "Ethnicity sensitive author disambiguation using semi-supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Author name disambiguation in bibliographic databases is the problem of\ngrouping together scientific publications written by the same person,\naccounting for potential homonyms and/or synonyms. Among solutions to this\nproblem, digital libraries are increasingly offering tools for authors to\nmanually curate their publications and claim those that are theirs. Indirectly,\nthese tools allow for the inexpensive collection of large annotated training\ndata, which can be further leveraged to build a complementary automated\ndisambiguation system capable of inferring patterns for identifying\npublications written by the same person. Building on more than 1 million\npublicly released crowdsourced annotations, we propose an automated author\ndisambiguation solution exploiting this data (i) to learn an accurate\nclassifier for identifying coreferring authors and (ii) to guide the clustering\nof scientific publications by distinct authors in a semi-supervised way. To the\nbest of our knowledge, our analysis is the first to be carried out on data of\nthis size and coverage. With respect to the state of the art, we validate the\ngeneral pipeline used in most existing solutions, and improve by: (i) proposing\nphonetic-based blocking strategies, thereby increasing recall; and (ii) adding\nstrong ethnicity-sensitive features for learning a linkage function, thereby\ntailoring disambiguation to non-Western author names whenever necessary.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 09:49:27 GMT"}, {"version": "v2", "created": "Wed, 4 May 2016 06:35:55 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Louppe", "Gilles", ""], ["Al-Natsheh", "Hussein", ""], ["Susik", "Mateusz", ""], ["Maguire", "Eamonn", ""]]}]