[{"id": "2007.00054", "submitter": "G. G. Md. Nawaz Ali", "authors": "Md. Mokhlesur Rahman, G. G. Md. Nawaz Ali, Xue Jun Li, Kamal Chandra\n  Paul, Peter H.J. Chong", "title": "Twitter and Census Data Analytics to Explore Socioeconomic Factors for\n  Post-COVID-19 Reopening Sentiment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Investigating and classifying sentiments of social media users towards an\nitem, situation, and system are very popular among the researchers. However,\nthey rarely discuss the underlying socioeconomic factor associations for such\nsentiments. This study attempts to explore the factors associated with positive\nand negative sentiments of the people about reopening the economy, in the\nUnited States (US) amidst the COVID-19 global crisis. It takes into\nconsideration the situational uncertainties (i.e., changes in work and travel\npattern due to lockdown policies), economic downturn and associated trauma, and\nemotional factors such as depression. To understand the sentiment of the people\nabout the reopening economy, Twitter data was collected, representing the 51\nstates including Washington DC of the US. State-wide socioeconomic\ncharacteristics of the people, built environment data, and the number of\nCOVID-19 related cases were collected and integrated with Twitter data to\nperform the analysis. A binary logit model was used to identify the factors\nthat influence people toward a positive or negative sentiment. The results from\nthe logit model demonstrate that family households, people with low education\nlevels, people in the labor force, low-income people, and people with higher\nhouse rent are more interested in reopening the economy. In contrast,\nhouseholds with a high number of members and high income are less interested to\nreopen the economy. The model can correctly classify 56.18% of the sentiments.\nThe Pearson chi2 test indicates that overall this model has high\ngoodness-of-fit. This study provides a clear indication to the policymakers\nwhere to allocate resources and what policy options they can undertake to\nimprove the socioeconomic situations of the people and mitigate the impacts of\npandemics in the current situation and as well as in the future.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 18:30:50 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Rahman", "Md. Mokhlesur", ""], ["Ali", "G. G. Md. Nawaz", ""], ["Li", "Xue Jun", ""], ["Paul", "Kamal Chandra", ""], ["Chong", "Peter H. J.", ""]]}, {"id": "2007.00177", "submitter": "Homanga Bharadhwaj", "authors": "Homanga Bharadhwaj, Dylan Turpin, Animesh Garg, Ashton Anderson", "title": "De-anonymization of authors through arXiv submissions during\n  double-blind review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the effects of releasing arXiv preprints of\npapers that are undergoing a double-blind review process. In particular, we ask\nthe following research question: What is the relation between de-anonymization\nof authors through arXiv preprints and acceptance of a research paper at a\n(nominally) double-blind venue? Under two conditions: papers that are released\non arXiv before the review phase and papers that are not, we examine the\ncorrelation between the reputation of their authors with the review scores and\nacceptance decisions. By analyzing a dataset of ICLR 2020 and ICLR 2019\nsubmissions (n=5050), we find statistically significant evidence of positive\ncorrelation between percentage acceptance and papers with high reputation\nreleased on arXiv. In order to understand this observed association better, we\nperform additional analyses based on self-specified confidence scores of\nreviewers and observe that less confident reviewers are more likely to assign\nhigh review scores to papers with well known authors and low review scores to\npapers with less known authors, where reputation is quantified in terms of\nnumber of Google Scholar citations. We emphasize upfront that our results are\npurely correlational and we neither can nor intend to make any causal claims. A\nblog post accompanying the paper and our scraping code will be linked in the\nproject website https://sites.google.com/view/deanon-arxiv/home\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 01:40:06 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Bharadhwaj", "Homanga", ""], ["Turpin", "Dylan", ""], ["Garg", "Animesh", ""], ["Anderson", "Ashton", ""]]}, {"id": "2007.00194", "submitter": "Yisong Miao", "authors": "Wenqiang Lei, Gangyi Zhang, Xiangnan He, Yisong Miao, Xiang Wang,\n  Liang Chen, Tat-Seng Chua", "title": "Interactive Path Reasoning on Graph for Conversational Recommendation", "comments": "KDD 2020, 11 pages, 6 figures", "journal-ref": null, "doi": "10.1145/3394486.3403258", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional recommendation systems estimate user preference on items from\npast interaction history, thus suffering from the limitations of obtaining\nfine-grained and dynamic user preference. Conversational recommendation system\n(CRS) brings revolutions to those limitations by enabling the system to\ndirectly ask users about their preferred attributes on items. However, existing\nCRS methods do not make full use of such advantage -- they only use the\nattribute feedback in rather implicit ways such as updating the latent user\nrepresentation. In this paper, we propose Conversational Path Reasoning (CPR),\na generic framework that models conversational recommendation as an interactive\npath reasoning problem on a graph. It walks through the attribute vertices by\nfollowing user feedback, utilizing the user preferred attributes in an explicit\nway. By leveraging on the graph structure, CPR is able to prune off many\nirrelevant candidate attributes, leading to better chance of hitting user\npreferred attributes. To demonstrate how CPR works, we propose a simple yet\neffective instantiation named SCPR (Simple CPR). We perform empirical studies\non the multi-round conversational recommendation scenario, the most realistic\nCRS setting so far that considers multiple rounds of asking attributes and\nrecommending items. Through extensive experiments on two datasets Yelp and\nLastFM, we validate the effectiveness of our SCPR, which significantly\noutperforms the state-of-the-art CRS methods EAR (arXiv:2002.09102) and CRM\n(arXiv:1806.03277). In particular, we find that the more attributes there are,\nthe more advantages our method can achieve.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 03:03:45 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Lei", "Wenqiang", ""], ["Zhang", "Gangyi", ""], ["He", "Xiangnan", ""], ["Miao", "Yisong", ""], ["Wang", "Xiang", ""], ["Chen", "Liang", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2007.00216", "submitter": "Jiarui Jin", "authors": "Jiarui Jin, Jiarui Qin, Yuchen Fang, Kounianhua Du, Weinan Zhang, Yong\n  Yu, Zheng Zhang, Alexander J. Smola", "title": "An Efficient Neighborhood-based Interaction Model for Recommendation on\n  Heterogeneous Graph", "comments": "KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an influx of heterogeneous information network (HIN) based\nrecommender systems in recent years since HIN is capable of characterizing\ncomplex graphs and contains rich semantics. Although the existing approaches\nhave achieved performance improvement, while practical, they still face the\nfollowing problems. On one hand, most existing HIN-based methods rely on\nexplicit path reachability to leverage path-based semantic relatedness between\nusers and items, e.g., metapath-based similarities. These methods are hard to\nuse and integrate since path connections are sparse or noisy, and are often of\ndifferent lengths. On the other hand, other graph-based methods aim to learn\neffective heterogeneous network representations by compressing node together\nwith its neighborhood information into single embedding before prediction. This\nweakly coupled manner in modeling overlooks the rich interactions among nodes,\nwhich introduces an early summarization issue. In this paper, we propose an\nend-to-end Neighborhood-based Interaction Model for Recommendation (NIRec) to\naddress the above problems. Specifically, we first analyze the significance of\nlearning interactions in HINs and then propose a novel formulation to capture\nthe interactive patterns between each pair of nodes through their\nmetapath-guided neighborhoods. Then, to explore complex interactions between\nmetapaths and deal with the learning complexity on large-scale networks, we\nformulate interaction in a convolutional way and learn efficiently with fast\nFourier transform. The extensive experiments on four different types of\nheterogeneous graphs demonstrate the performance gains of NIRec comparing with\nstate-of-the-arts. To the best of our knowledge, this is the first work\nproviding an efficient neighborhood-based interaction model in the HIN-based\nrecommendations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 04:04:52 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Jin", "Jiarui", ""], ["Qin", "Jiarui", ""], ["Fang", "Yuchen", ""], ["Du", "Kounianhua", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""], ["Zhang", "Zheng", ""], ["Smola", "Alexander J.", ""]]}, {"id": "2007.00228", "submitter": "Yipeng Zhang", "authors": "Yipeng Zhang, Hanjia Lyu, Yubao Liu, Xiyang Zhang, Yu Wang, Jiebo Luo", "title": "Monitoring Depression Trend on Twitter during the COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has severely affected people's daily lives and caused\ntremendous economic loss worldwide. However, its influence on people's mental\nhealth conditions has not received as much attention. To study this subject, we\nchoose social media as our main data resource and create by far the largest\nEnglish Twitter depression dataset containing 2,575 distinct identified\ndepression users with their past tweets. To examine the effect of depression on\npeople's Twitter language, we train three transformer-based depression\nclassification models on the dataset, evaluate their performance with\nprogressively increased training sizes, and compare the model's \"tweet\nchunk\"-level and user-level performances. Furthermore, inspired by\npsychological studies, we create a fusion classifier that combines deep\nlearning model scores with psychological text features and users' demographic\ninformation and investigate these features' relations to depression signals.\nFinally, we demonstrate our model's capability of monitoring both group-level\nand population-level depression trends by presenting two of its applications\nduring the COVID-19 pandemic. We hope this study can raise awareness among\nresearchers and the general public of COVID-19's impact on people's mental\nhealth.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 04:28:20 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 00:37:31 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Zhang", "Yipeng", ""], ["Lyu", "Hanjia", ""], ["Liu", "Yubao", ""], ["Zhang", "Xiyang", ""], ["Wang", "Yu", ""], ["Luo", "Jiebo", ""]]}, {"id": "2007.00303", "submitter": "Hai-Jun Zhou", "authors": "Hai-Jun Zhou and Qinyi Liao", "title": "Circumventing spin glass traps by microcanonical spontaneous symmetry\n  breaking", "comments": "11 pages; final version as accepted by PRE; minor corrections to\n  reference and affiliation name", "journal-ref": "Phys. Rev. E 103, 042112 (2021)", "doi": "10.1103/PhysRevE.103.042112", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The planted p-spin interaction model is a paradigm of random-graph systems\npossessing both a ferromagnetic phase and a disordered phase with the latter\nsplitting into many spin glass states at low temperatures. Conventional\nsimulated annealing dynamics is easily blocked by these low-energy spin glass\nstates. Here we demonstrate that, actually this planted system is exponentially\ndominated by a microcanonical polarized phase at intermediate energy densities.\nThere is a discontinuous microcanonical spontaneous symmetry breaking\ntransition from the paramagnetic phase to the microcanonical polarized phase.\nThis transition can serve as a mechanism to avoid all the spin glass traps, and\nit is accelerated by the restart strategy of microcanonical random walk. We\nalso propose an unsupervised learning problem on microcanonically sampled\nconfigurations for inferring the planted ground state.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 07:55:39 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 13:26:08 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 03:35:15 GMT"}, {"version": "v4", "created": "Wed, 7 Apr 2021 00:44:25 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhou", "Hai-Jun", ""], ["Liao", "Qinyi", ""]]}, {"id": "2007.00380", "submitter": "Casper Hansen", "authors": "Casper Hansen and Christian Hansen and Jakob Grue Simonsen and Stephen\n  Alstrup and Christina Lioma", "title": "Unsupervised Semantic Hashing with Pairwise Reconstruction", "comments": "Accepted at SIGIR'20", "journal-ref": null, "doi": "10.1145/3397271.3401220", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Hashing is a popular family of methods for efficient similarity\nsearch in large-scale datasets. In Semantic Hashing, documents are encoded as\nshort binary vectors (i.e., hash codes), such that semantic similarity can be\nefficiently computed using the Hamming distance. Recent state-of-the-art\napproaches have utilized weak supervision to train better performing hashing\nmodels. Inspired by this, we present Semantic Hashing with Pairwise\nReconstruction (PairRec), which is a discrete variational autoencoder based\nhashing model. PairRec first encodes weakly supervised training pairs (a query\ndocument and a semantically similar document) into two hash codes, and then\nlearns to reconstruct the same query document from both of these hash codes\n(i.e., pairwise reconstruction). This pairwise reconstruction enables our model\nto encode local neighbourhood structures within the hash code directly through\nthe decoder. We experimentally compare PairRec to traditional and\nstate-of-the-art approaches, and obtain significant performance improvements in\nthe task of document similarity search.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 10:54:27 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Hansen", "Casper", ""], ["Hansen", "Christian", ""], ["Simonsen", "Jakob Grue", ""], ["Alstrup", "Stephen", ""], ["Lioma", "Christina", ""]]}, {"id": "2007.00398", "submitter": "Minesh Mathew", "authors": "Minesh Mathew, Dimosthenis Karatzas, C.V. Jawahar", "title": "DocVQA: A Dataset for VQA on Document Images", "comments": "accepted at WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new dataset for Visual Question Answering (VQA) on document\nimages called DocVQA. The dataset consists of 50,000 questions defined on\n12,000+ document images. Detailed analysis of the dataset in comparison with\nsimilar datasets for VQA and reading comprehension is presented. We report\nseveral baseline results by adopting existing VQA and reading comprehension\nmodels. Although the existing models perform reasonably well on certain types\nof questions, there is large performance gap compared to human performance\n(94.36% accuracy). The models need to improve specifically on questions where\nunderstanding structure of the document is crucial. The dataset, code and\nleaderboard are available at docvqa.org\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 11:37:40 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 04:13:51 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 05:39:39 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Mathew", "Minesh", ""], ["Karatzas", "Dimosthenis", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2007.00636", "submitter": "John Leung", "authors": "John Kalung Leung, Igor Griva, William G. Kennedy", "title": "Using Affective Features from Media Content Metadata for Better Movie\n  Recommendations", "comments": "8 pages, 10 tables, 1 figure, and presented in KDIR2020 Proceedings\n  of the 12th International Joint Conference on Knowledge Discovery, Knowledge\n  Engineering and Knowledge Management", "journal-ref": "In Proceedings of the 12th International Joint Conference on\n  Knowledge Discovery, Knowledge Engineering and Knowledge Management - Volume\n  1: KDIR, 161-168, 2020", "doi": "10.5220/0010056201610168", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper investigates the causality in the decision making of movie\nrecommendations through the users' affective profiles. We advocate a method of\nassigning emotional tags to a movie by the auto-detection of the affective\nfeatures in the movie's overview. We apply a text-based Emotion Detection and\nRecognition model, which trained by tweets short messages and transfers the\nlearned model to detect movie overviews' implicit affective features. We\nvectorized the affective movie tags to represent the mood embeddings of the\nmovie. We obtain the user's emotional features by taking the average of all the\nmovies' affective vectors the user has watched. We apply five-distance metrics\nto rank the Top-N movie recommendations against the user's emotion profile. We\nfound Cosine Similarity distance metrics performed better than other distance\nmetrics measures. We conclude that by replacing the top-N recommendations\ngenerated by the Recommender with the reranked recommendations list made by the\nCosine Similarity distance metrics, the user will effectively get affective\naware top-N recommendations while making the Recommender feels like an Emotion\nAware Recommender.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 17:36:24 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 19:17:29 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Leung", "John Kalung", ""], ["Griva", "Igor", ""], ["Kennedy", "William G.", ""]]}, {"id": "2007.00709", "submitter": "Seethalakshmi Gopalakrishnan", "authors": "Hossein Hematialam, Luciana Garbayo, Seethalakshmi Gopalakrishnan,\n  Wlodek Zadrozny", "title": "Computing Conceptual Distances between Breast Cancer Screening\n  Guidelines: An Implementation of a Near-Peer Epistemic Model of Medical\n  Disagreement", "comments": "39 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": "3285697", "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using natural language processing tools, we investigate the differences of\nrecommendations in medical guidelines for the same decision problem -- breast\ncancer screening. We show that these differences arise from knowledge brought\nto the problem by different medical societies, as reflected in the conceptual\nvocabularies used by the different groups of authors.The computational models\nwe build and analyze agree with the near-peer epistemic model of expert\ndisagreement proposed by Garbayo. Even though the article is a case study\nfocused on one set of guidelines, the proposed methodology is broadly\napplicable. In addition to proposing a novel graph-based similarity model for\ncomparing collections of documents, we perform an extensive analysis of the\nmodel performance. In a series of a few dozen experiments, in three broad\ncategories, we show, at a very high statistical significance level of 3-4\nstandard deviations for our best models, that the high similarity between\nexpert annotated model and our concept based, automatically created,\ncomputational models is not accidental. Our best model achieves roughly 70%\nsimilarity. We also describe possible extensions of this work.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 19:21:10 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 02:01:05 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Hematialam", "Hossein", ""], ["Garbayo", "Luciana", ""], ["Gopalakrishnan", "Seethalakshmi", ""], ["Zadrozny", "Wlodek", ""]]}, {"id": "2007.00747", "submitter": "Yusuf Sermet", "authors": "Yusuf Sermet and Ibrahim Demir", "title": "A Semantic Web Framework for Automated Smart Assistants: COVID-19 Case\n  Study", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 pandemic elucidated that knowledge systems will be instrumental in\ncases where accurate information needs to be communicated to a substantial\ngroup of people with different backgrounds and technological resources.\nHowever, several challenges and obstacles hold back the wide adoption of\nvirtual assistants by public health departments and organizations. This paper\npresents the Instant Expert, an open-source semantic web framework to build and\nintegrate voice-enabled smart assistants (i.e. chatbots) for any web platform\nregardless of the underlying domain and technology. The component allows\nnon-technical domain experts to effortlessly incorporate an operational\nassistant with voice recognition capability into their websites. Instant Expert\nis capable of automatically parsing, processing, and modeling Frequently Asked\nQuestions pages as an information resource as well as communicating with an\nexternal knowledge engine for ontology-powered inference and dynamic data\nutilization. The presented framework utilizes advanced web technologies to\nensure reusability and reliability, and an inference engine for natural\nlanguage understanding powered by deep learning and heuristic algorithms. A use\ncase for creating an informatory assistant for COVID-19 based on the Centers\nfor Disease Control and Prevention (CDC) data is presented to demonstrate the\nframework's usage and benefits.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 20:47:44 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 17:28:08 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Sermet", "Yusuf", ""], ["Demir", "Ibrahim", ""]]}, {"id": "2007.00808", "submitter": "Li Xiong", "authors": "Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul\n  Bennett, Junaid Ahmed, Arnold Overwijk", "title": "Approximate Nearest Neighbor Negative Contrastive Learning for Dense\n  Text Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conducting text retrieval in a dense learned representation space has many\nintriguing advantages over sparse retrieval. Yet the effectiveness of dense\nretrieval (DR) often requires combination with sparse retrieval. In this paper,\nwe identify that the main bottleneck is in the training mechanisms, where the\nnegative instances used in training are not representative of the irrelevant\ndocuments in testing. This paper presents Approximate nearest neighbor Negative\nContrastive Estimation (ANCE), a training mechanism that constructs negatives\nfrom an Approximate Nearest Neighbor (ANN) index of the corpus, which is\nparallelly updated with the learning process to select more realistic negative\ntraining instances. This fundamentally resolves the discrepancy between the\ndata distribution used in the training and testing of DR. In our experiments,\nANCE boosts the BERT-Siamese DR model to outperform all competitive dense and\nsparse retrieval baselines. It nearly matches the accuracy of\nsparse-retrieval-and-BERT-reranking using dot-product in the ANCE-learned\nrepresentation space and provides almost 100x speed-up.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 23:15:56 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 22:17:19 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Xiong", "Lee", ""], ["Xiong", "Chenyan", ""], ["Li", "Ye", ""], ["Tang", "Kwok-Fung", ""], ["Liu", "Jialin", ""], ["Bennett", "Paul", ""], ["Ahmed", "Junaid", ""], ["Overwijk", "Arnold", ""]]}, {"id": "2007.00814", "submitter": "Omar Khattab", "authors": "Omar Khattab, Christopher Potts, Matei Zaharia", "title": "Relevance-guided Supervision for OpenQA with ColBERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems for Open-Domain Question Answering (OpenQA) generally depend on a\nretriever for finding candidate passages in a large corpus and a reader for\nextracting answers from those passages. In much recent work, the retriever is a\nlearned component that uses coarse-grained vector representations of questions\nand passages. We argue that this modeling choice is insufficiently expressive\nfor dealing with the complexity of natural language questions. To address this,\nwe define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT\nto OpenQA. ColBERT creates fine-grained interactions between questions and\npassages. We propose a weak supervision strategy that iteratively uses ColBERT\nto create its own training data. This greatly improves OpenQA retrieval on both\nNatural Questions and TriviaQA, and the resulting end-to-end OpenQA system\nattains state-of-the-art performance on both of those datasets.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 23:50:58 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Khattab", "Omar", ""], ["Potts", "Christopher", ""], ["Zaharia", "Matei", ""]]}, {"id": "2007.00875", "submitter": "Nikka Mofid", "authors": "Chetanya Rastogi, Nikka Mofid, Fang-I Hsiao", "title": "Can We Achieve More with Less? Exploring Data Augmentation for Toxic\n  Comment Classification", "comments": "11 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles one of the greatest limitations in Machine Learning: Data\nScarcity. Specifically, we explore whether high accuracy classifiers can be\nbuilt from small datasets, utilizing a combination of data augmentation\ntechniques and machine learning algorithms. In this paper, we experiment with\nEasy Data Augmentation (EDA) and Backtranslation, as well as with three popular\nlearning algorithms, Logistic Regression, Support Vector Machine (SVM), and\nBidirectional Long Short-Term Memory Network (Bi-LSTM). For our\nexperimentation, we utilize the Wikipedia Toxic Comments dataset so that in the\nprocess of exploring the benefits of data augmentation, we can develop a model\nto detect and classify toxic speech in comments to help fight back against\ncyberbullying and online harassment. Ultimately, we found that data\naugmentation techniques can be used to significantly boost the performance of\nclassifiers and are an excellent strategy to combat lack of data in NLP\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 04:43:31 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Rastogi", "Chetanya", ""], ["Mofid", "Nikka", ""], ["Hsiao", "Fang-I", ""]]}, {"id": "2007.00927", "submitter": "Michael Saidani", "authors": "Michael Saidani (LGI), Bernard Yannou (LGI), Yann Leroy (LGI),\n  Fran\\c{c}ois Cluzel (LGI), Harrison Kim", "title": "How circular economy and industrial ecology concepts are intertwined? A\n  bibliometric and text mining analysis", "comments": null, "journal-ref": "Online Symposium on Circular Economy and Sustainability, Jul 2020,\n  Alexandroupolis, Greece", "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining new insights from both bibliometric and text mining analyses, with\nprior relevant research conversations on circular economy (CE) and industrial\necology (IE), this paper aims to clarify the recent development trends and\nrelations between these concepts, including their representations and\napplications. On this basis, discussions are made and recommendations provided\non how CE and IE approaches, tools, and indicators can complement each other to\nenable and catalyze a more circular and sustainable development, by supporting\nsustainable policy-making and monitoring sound CE strategies in industrial\npractices.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 07:06:27 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Saidani", "Michael", "", "LGI"], ["Yannou", "Bernard", "", "LGI"], ["Leroy", "Yann", "", "LGI"], ["Cluzel", "Fran\u00e7ois", "", "LGI"], ["Kim", "Harrison", ""]]}, {"id": "2007.01230", "submitter": "Zichang Liu", "authors": "Zichang Liu, Zhaozhuo Xu, Alan Ji, Jonathan Li, Beidi Chen, Anshumali\n  Shrivastava", "title": "Climbing the WOL: Training for Cheaper Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient inference for wide output layers (WOLs) is an essential yet\nchallenging task in large scale machine learning. Most approaches reduce this\nproblem to approximate maximum inner product search (MIPS), which relies\nheavily on the observation that for a given model, ground truth labels\ncorrespond to logits of highest value during full model inference. However,\nsuch an assumption is restrictive in practice. In this paper, we argue that\napproximate MIPS subroutines, despite having sub-linear computation time, are\nsub-optimal because they are tailored for retrieving large inner products with\nhigh recall instead of retrieving the correct labels. With WOL, the labels\noften have moderate inner products, which makes approximate MIPS more\nchallenging. We propose an alternative problem formulation, called Label\nSuperior Sampling (LSS), where the objective is to tailor the system to ensure\nretrieval of the correct label. Accordingly, we propose a novel learned hash\napproach, which is significantly more efficient and sufficient for high\ninference accuracy than MIPS baselines. Our extensive evaluation indicates that\nLSS can match or even outperform full inference accuracy with around 5x speed\nup and 87% energy reduction.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 16:26:26 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 03:11:54 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Liu", "Zichang", ""], ["Xu", "Zhaozhuo", ""], ["Ji", "Alan", ""], ["Li", "Jonathan", ""], ["Chen", "Beidi", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "2007.01455", "submitter": "John Leung", "authors": "John Kalung Leung, Igor Griva, William G. Kennedy", "title": "Text-based Emotion Aware Recommender", "comments": "13 pages, 8 tables, International Conference on Natural Language\n  Computing and AI (NLCAI2020) July25-26, London, United Kingdom", "journal-ref": "David C. Wyld et al. (Eds): CCSEA, BIoT, DKMP, CLOUD, NLCAI, SIPRO\n  - 2020 pp. 101-114, 2020", "doi": "10.5121/csit.2020.101009", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the concept of users' emotion vectors (UVECs) and movies' emotion\nvectors (MVECs) as building components of Emotion Aware Recommender System. We\nbuilt a comparative platform that consists of five recommenders based on\ncontent-based and collaborative filtering algorithms. We employed a Tweets\nAffective Classifier to classify movies' emotion profiles through movie\noverviews. We construct MVECs from the movie emotion profiles. We track users'\nmovie watching history to formulate UVECs by taking the average of all the\nMVECs from all the movies a user has watched. With the MVECs, we built an\nEmotion Aware Recommender as one of the comparative platforms' algorithms. We\nevaluated the top-N recommendation lists generated by these Recommenders and\nfound the top-N list of Emotion Aware Recommender showed serendipity\nrecommendations.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 01:43:29 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 13:02:01 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Leung", "John Kalung", ""], ["Griva", "Igor", ""], ["Kennedy", "William G.", ""]]}, {"id": "2007.01510", "submitter": "Yusi Zhang", "authors": "Yusi Zhang, Chuanjie Liu, Angen Luo, Hui Xue, Xuan Shan, Yuxiang Luo,\n  Yiqian Xia, Yuanchi Yan, Haidong Wang", "title": "MIRA: Leveraging Multi-Intention Co-click Information in Web-scale\n  Document Retrieval using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of deep recall model in industrial web search, which is,\ngiven a user query, retrieve hundreds of most relevance documents from billions\nof candidates. The common framework is to train two encoding models based on\nneural embedding which learn the distributed representations of queries and\ndocuments separately and match them in the latent semantic space. However, all\nthe exiting encoding models only leverage the information of the document\nitself, which is often not sufficient in practice when matching with query\nterms, especially for the hard tail queries. In this work we aim to leverage\nthe additional information for each document from its co-click neighbour to\nhelp document retrieval. The challenges include how to effectively extract\ninformation and eliminate noise when involving co-click information in deep\nmodel while meet the demands of billion-scale data size for real time online\ninference.\n  To handle the noise in co-click relations, we firstly propose a web-scale\nMulti-Intention Co-click document Graph(MICG) which builds the co-click\nconnections between documents on click intention level but not on document\nlevel. Then we present an encoding framework MIRA based on Bert and graph\nattention networks which leverages a two-factor attention mechanism to\naggregate neighbours. To meet the online latency requirements, we only involve\nneighbour information in document side, which can save the time-consuming query\nneighbor search in real time serving. We conduct extensive offline experiments\non both public dataset and private web-scale dataset from two major commercial\nsearch engines demonstrating the effectiveness and scalability of the proposed\nmethod compared with several baselines. And a further case study reveals that\nco-click relations mainly help improve web search quality from two aspects: key\nconcept enhancing and query term complementary.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 06:32:48 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Zhang", "Yusi", ""], ["Liu", "Chuanjie", ""], ["Luo", "Angen", ""], ["Xue", "Hui", ""], ["Shan", "Xuan", ""], ["Luo", "Yuxiang", ""], ["Xia", "Yiqian", ""], ["Yan", "Yuanchi", ""], ["Wang", "Haidong", ""]]}, {"id": "2007.01764", "submitter": "Xiang Wang", "authors": "Xiang Wang, Hongye Jin, An Zhang, Xiangnan He, Tong Xu, Tat-Seng Chua", "title": "Disentangled Graph Collaborative Filtering", "comments": "SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401137", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning informative representations of users and items from the interaction\ndata is of crucial importance to collaborative filtering (CF). Present\nembedding functions exploit user-item relationships to enrich the\nrepresentations, evolving from a single user-item instance to the holistic\ninteraction graph. Nevertheless, they largely model the relationships in a\nuniform manner, while neglecting the diversity of user intents on adopting the\nitems, which could be to pass time, for interest, or shopping for others like\nfamilies. Such uniform approach to model user interests easily results in\nsuboptimal representations, failing to model diverse relationships and\ndisentangle user intents in representations.\n  In this work, we pay special attention to user-item relationships at the\nfiner granularity of user intents. We hence devise a new model, Disentangled\nGraph Collaborative Filtering (DGCF), to disentangle these factors and yield\ndisentangled representations. Specifically, by modeling a distribution over\nintents for each user-item interaction, we iteratively refine the intent-aware\ninteraction graphs and representations. Meanwhile, we encourage independence of\ndifferent intents. This leads to disentangled representations, effectively\ndistilling information pertinent to each intent. We conduct extensive\nexperiments on three benchmark datasets, and DGCF achieves significant\nimprovements over several state-of-the-art models like NGCF, DisenGCN, and\nMacridVAE. Further analyses offer insights into the advantages of DGCF on the\ndisentanglement of user intents and interpretability of representations. Our\ncodes are available in\nhttps://github.com/xiangwang1223/disentangled_graph_collaborative_filtering.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 15:37:25 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Wang", "Xiang", ""], ["Jin", "Hongye", ""], ["Zhang", "An", ""], ["He", "Xiangnan", ""], ["Xu", "Tong", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2007.01788", "submitter": "Antonios Anastasopoulos", "authors": "Antonios Anastasopoulos, Alessandro Cattelan, Zi-Yi Dou, Marcello\n  Federico, Christian Federman, Dmitriy Genzel, Francisco Guzm\\'an, Junjie Hu,\n  Macduff Hughes, Philipp Koehn, Rosie Lazar, Will Lewis, Graham Neubig,\n  Mengmeng Niu, Alp \\\"Oktem, Eric Paquin, Grace Tang, and Sylwia Tur", "title": "TICO-19: the Translation Initiative for Covid-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic is the worst pandemic to strike the world in over a\ncentury. Crucial to stemming the tide of the SARS-CoV-2 virus is communicating\nto vulnerable populations the means by which they can protect themselves. To\nthis end, the collaborators forming the Translation Initiative for COvid-19\n(TICO-19) have made test and development data available to AI and MT\nresearchers in 35 different languages in order to foster the development of\ntools and resources for improving access to information about COVID-19 in these\nlanguages. In addition to 9 high-resourced, \"pivot\" languages, the team is\ntargeting 26 lesser resourced languages, in particular languages of Africa,\nSouth Asia and South-East Asia, whose populations may be the most vulnerable to\nthe spread of the virus. The same data is translated into all of the languages\nrepresented, meaning that testing or development can be done for any pairing of\nlanguages in the set. Further, the team is converting the test and development\ndata into translation memories (TMXs) that can be used by localizers from and\nto any of the languages.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:26:17 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 14:13:51 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Anastasopoulos", "Antonios", ""], ["Cattelan", "Alessandro", ""], ["Dou", "Zi-Yi", ""], ["Federico", "Marcello", ""], ["Federman", "Christian", ""], ["Genzel", "Dmitriy", ""], ["Guzm\u00e1n", "Francisco", ""], ["Hu", "Junjie", ""], ["Hughes", "Macduff", ""], ["Koehn", "Philipp", ""], ["Lazar", "Rosie", ""], ["Lewis", "Will", ""], ["Neubig", "Graham", ""], ["Niu", "Mengmeng", ""], ["\u00d6ktem", "Alp", ""], ["Paquin", "Eric", ""], ["Tang", "Grace", ""], ["Tur", "Sylwia", ""]]}, {"id": "2007.01800", "submitter": "Jingxuan Tu", "authors": "Jingxuan Tu, Marc Verhagen, Brent Cochran, James Pustejovsky", "title": "Exploration and Discovery of the COVID-19 Literature through Semantic\n  Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are developing semantic visualization techniques in order to enhance\nexploration and enable discovery over large datasets of complex networks of\nrelations. Semantic visualization is a method of enabling exploration and\ndiscovery over large datasets of complex networks by exploiting the semantics\nof the relations in them. This involves (i) NLP to extract named entities,\nrelations and knowledge graphs from the original data; (ii) indexing the output\nand creating representations for all relevant entities and relations that can\nbe visualized in many different ways, e.g., as tag clouds, heat maps, graphs,\netc.; (iii) applying parameter reduction operations to the extracted relations,\ncreating \"relation containers\", or functional entities that can also be\nvisualized using the same methods, allowing the visualization of multiple\nrelations, partial pathways, and exploration across multiple dimensions. Our\nhope is that this will enable the discovery of novel inferences over relations\nin complex data that otherwise would go unnoticed. We have applied this to\nanalysis of the recently released CORD-19 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:40:37 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Tu", "Jingxuan", ""], ["Verhagen", "Marc", ""], ["Cochran", "Brent", ""], ["Pustejovsky", "James", ""]]}, {"id": "2007.01978", "submitter": "Yingjie Hu", "authors": "Yingjie Hu", "title": "Building benchmarking frameworks for supporting replicability and\n  reproducibility: spatial and textual analysis as an example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replicability and reproducibility (R&R) are critical for the long-term\nprosperity of a scientific discipline. In GIScience, researchers have discussed\nR&R related to different research topics and problems, such as local spatial\nstatistics, digital earth, and metadata (Fotheringham, 2009; Goodchild, 2012;\nAnselin et al., 2014). This position paper proposes to further support R&R by\nbuilding benchmarking frameworks in order to facilitate the replication of\nprevious research for effective and effcient comparisons of methods and\nsoftware tools developed for addressing the same or similar problems.\nParticularly, this paper will use geoparsing, an important research problem in\nspatial and textual analysis, as an example to explain the values of such\nbenchmarking frameworks.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 01:16:49 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Hu", "Yingjie", ""]]}, {"id": "2007.02033", "submitter": "Maxime Meyer", "authors": "Mehdi Regina and Maxime Meyer and S\\'ebastien Goutal", "title": "Text Data Augmentation: Towards better detection of spear-phishing\n  emails", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text data augmentation, i.e., the creation of new textual data from an\nexisting text, is challenging. Indeed, augmentation transformations should take\ninto account language complexity while being relevant to the target Natural\nLanguage Processing (NLP) task (e.g., Machine Translation, Text\nClassification). Initially motivated by an application of Business Email\nCompromise (BEC) detection, we propose a corpus and task agnostic augmentation\nframework used as a service to augment English texts within our company. Our\nproposal combines different methods, utilizing BERT language model, multi-step\nback-translation and heuristics. We show that our augmentation framework\nimproves performances on several text classification tasks using publicly\navailable models and corpora as well as on a BEC detection task. We also\nprovide a comprehensive argumentation about the limitations of our augmentation\nframework.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 07:45:04 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 14:54:10 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Regina", "Mehdi", ""], ["Meyer", "Maxime", ""], ["Goutal", "S\u00e9bastien", ""]]}, {"id": "2007.02095", "submitter": "Lixin Zou", "authors": "Lixin Zou, Long Xia, Yulong Gu, Xiangyu Zhao, Weidong Liu, Jimmy\n  Xiangji Huang, Dawei Yin", "title": "Neural Interactive Collaborative Filtering", "comments": null, "journal-ref": null, "doi": "10.1145/3397271.3401181", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study collaborative filtering in an interactive setting, in\nwhich the recommender agents iterate between making recommendations and\nupdating the user profile based on the interactive feedback. The most\nchallenging problem in this scenario is how to suggest items when the user\nprofile has not been well established, i.e., recommend for cold-start users or\nwarm-start users with taste drifting. Existing approaches either rely on overly\npessimistic linear exploration strategy or adopt meta-learning based algorithms\nin a full exploitation way. In this work, to quickly catch up with the user's\ninterests, we propose to represent the exploration policy with a neural network\nand directly learn it from the feedback data. Specifically, the exploration\npolicy is encoded in the weights of multi-channel stacked self-attention neural\nnetworks and trained with efficient Q-learning by maximizing users' overall\nsatisfaction in the recommender systems. The key insight is that the satisfied\nrecommendations triggered by the exploration recommendation can be viewed as\nthe exploration bonus (delayed reward) for its contribution on improving the\nquality of the user profile. Therefore, the proposed exploration policy, to\nbalance between learning the user profile and making accurate recommendations,\ncan be directly optimized by maximizing users' long-term satisfaction with\nreinforcement learning. Extensive experiments and analysis conducted on three\nbenchmark collaborative filtering datasets have demonstrated the advantage of\nour method over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 13:35:39 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zou", "Lixin", ""], ["Xia", "Long", ""], ["Gu", "Yulong", ""], ["Zhao", "Xiangyu", ""], ["Liu", "Weidong", ""], ["Huang", "Jimmy Xiangji", ""], ["Yin", "Dawei", ""]]}, {"id": "2007.02124", "submitter": "Judy Gichoya", "authors": "Ningcheng Li, Guy Maresh, Maxwell Cretcher, Khashayar Farsad, Ramsey\n  Al-Hakim, John Kaufman, Judy Gichoya", "title": "A Modern Non-SQL Approach to Radiology-Centric Search Engine Design with\n  Clinical Validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Healthcare data is increasing in size at an unprecedented speed with much\nattention on big data analysis and Artificial Intelligence application for\nquality assurance, clinical training, severity triaging, and decision support.\nRadiology is well-suited for innovation given its intrinsically paired\nlinguistic and visual data. Previous attempts to unlock this information\ngoldmine were encumbered by heterogeneity of human language, proprietary search\nalgorithms, and lack of medicine-specific search performance matrices. We\npresent a de novo process of developing a document-based, secure, efficient,\nand accurate search engine in the context of Radiology. We assess our\nimplementation of the search engine with comparison to pre-existing manually\ncollected clinical databases used previously for clinical research projects in\naddition to computational performance benchmarks and survey feedback. By\nleveraging efficient database architecture, search capability, and clinical\nthinking, radiologists are at the forefront of harnessing the power of\nhealthcare data.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 15:21:49 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Li", "Ningcheng", ""], ["Maresh", "Guy", ""], ["Cretcher", "Maxwell", ""], ["Farsad", "Khashayar", ""], ["Al-Hakim", "Ramsey", ""], ["Kaufman", "John", ""], ["Gichoya", "Judy", ""]]}, {"id": "2007.02144", "submitter": "Antony Samuels", "authors": "Antony Samuels, John Mcgonical", "title": "Sentiment Analysis on Social Media Content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Nowadays, people from all around the world use social media sites to share\ninformation. Twitter for example is a platform in which users send, read posts\nknown as tweets and interact with different communities. Users share their\ndaily lives, post their opinions on everything such as brands and places.\nCompanies can benefit from this massive platform by collecting data related to\nopinions on them. The aim of this paper is to present a model that can perform\nsentiment analysis of real data collected from Twitter. Data in Twitter is\nhighly unstructured which makes it difficult to analyze. However, our proposed\nmodel is different from prior work in this field because it combined the use of\nsupervised and unsupervised machine learning algorithms. The process of\nperforming sentiment analysis as follows: Tweet extracted directly from Twitter\nAPI, then cleaning and discovery of data performed. After that the data were\nfed into several models for the purpose of training. Each tweet extracted\nclassified based on its sentiment whether it is a positive, negative or\nneutral. Data were collected on two subjects McDonalds and KFC to show which\nrestaurant has more popularity. Different machine learning algorithms were\nused. The result from these models were tested using various testing metrics\nlike cross validation and f-score. Moreover, our model demonstrates strong\nperformance on mining texts extracted directly from Twitter.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 17:03:30 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 02:42:14 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Samuels", "Antony", ""], ["Mcgonical", "John", ""]]}, {"id": "2007.02237", "submitter": "Antony Samuels", "authors": "Antony Samuels, John Mcgonical", "title": "Sentiment Analysis on Customer Responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Sentiment analysis is one of the fastest spreading research areas in computer\nscience, making it challenging to keep track of all the activities in the area.\nWe present a customer feedback reviews on product, where we utilize opinion\nmining, text mining and sentiments, which has affected the surrounded world by\nchanging their opinion on a specific product. Data used in this study are\nonline product reviews collected from Amazon.com. We performed a comparative\nsentiment analysis of retrieved reviews. This research paper provides you with\nsentimental analysis of various smart phone opinions on smart phones dividing\nthem Positive, Negative and Neutral Behaviour.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 04:50:40 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Samuels", "Antony", ""], ["Mcgonical", "John", ""]]}, {"id": "2007.02238", "submitter": "Antony Samuels", "authors": "Antony Samuels, John Mcgonical", "title": "News Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Modern technological era has reshaped traditional lifestyle in several\ndomains. The medium of publishing news and events has become faster with the\nadvancement of Information Technology. IT has also been flooded with immense\namounts of data, which is being published every minute of every day, by\nmillions of users, in the shape of comments, blogs, news sharing through blogs,\nsocial media micro-blogging websites and many more. Manual traversal of such\nhuge data is a challenging job, thus, sophisticated methods are acquired to\nperform this task automatically and efficiently. News reports events that\ncomprise of emotions - good, bad, neutral. Sentiment analysis is utilized to\ninvestigate human emotions present in textual information. This paper presents\na lexicon-based approach for sentiment analysis of news articles. The\nexperiments have been performed on BBC news data set, which expresses the\napplicability and validation of the adopted approach.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 05:15:35 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Samuels", "Antony", ""], ["Mcgonical", "John", ""]]}, {"id": "2007.02304", "submitter": "Shuiqiao Yang", "authors": "Hui Yin, Shuiqiao Yang, Jianxin Li", "title": "Detecting Topic and Sentiment Dynamics Due to COVID-19 Pandemic Using\n  Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of the novel Coronavirus Disease (COVID-19) has greatly\ninfluenced people's daily lives across the globe. Emergent measures and\npolicies (e.g., lockdown, social distancing) have been taken by governments to\ncombat this highly infectious disease. However, people's mental health is also\nat risk due to the long-time strict social isolation rules. Hence, monitoring\npeople's mental health across various events and topics will be extremely\nnecessary for policy makers to make the appropriate decisions. On the other\nhand, social media have been widely used as an outlet for people to publish and\nshare their personal opinions and feelings. The large scale social media posts\n(e.g., tweets) provide an ideal data source to infer the mental health for\npeople during this pandemic period. In this work, we propose a novel framework\nto analyze the topic and sentiment dynamics due to COVID-19 from the massive\nsocial media posts. Based on a collection of 13 million tweets related to\nCOVID-19 over two weeks, we found that the positive sentiment shows higher\nratio than the negative sentiment during the study period. When zooming into\nthe topic-level analysis, we find that different aspects of COVID-19 have been\nconstantly discussed and show comparable sentiment polarities. Some topics like\n``stay safe home\" are dominated with positive sentiment. The others such as\n``people death\" are consistently showing negative sentiment. Overall, the\nproposed framework shows insightful findings based on the analysis of the\ntopic-level sentiment dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 12:05:30 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Yin", "Hui", ""], ["Yang", "Shuiqiao", ""], ["Li", "Jianxin", ""]]}, {"id": "2007.02445", "submitter": "Kirill Shevkunov", "authors": "Kirill Shevkunov and Liudmila Prokhorenkova", "title": "Overlapping Spaces for Compact Graph Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various non-trivial spaces are becoming popular for embedding structured data\nsuch as graphs, texts, or images. Following spherical and hyperbolic spaces,\nmore general product spaces have been proposed. However, searching for the best\nconfiguration of product space is a resource-intensive procedure, which reduces\nthe practical applicability of the idea. We generalize the concept of product\nspace and introduce an overlapping space that does not have the configuration\nsearch problem. The main idea is to allow subsets of coordinates to be shared\nbetween spaces of different types (Euclidean, hyperbolic, spherical). As a\nresult, parameter optimization automatically learns the optimal configuration.\nAdditionally, overlapping spaces allow for more compact representations since\ntheir geometry is more complex. Our experiments confirm that overlapping spaces\noutperform the competitors in graph embedding tasks. Here, we consider both\ndistortion setup, where the aim is to preserve distances, and ranking setup,\nwhere the relative order should be preserved. The proposed method effectively\nsolves the problem and outperforms the competitors in both settings. We also\nperform an empirical analysis in a realistic information retrieval task, where\nwe compare all spaces by incorporating them into DSSM. In this case, the\nproposed overlapping space consistently achieves nearly optimal results without\nany configuration tuning. This allows for reducing training time, which can be\nsignificant in large-scale applications.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 20:55:47 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 10:21:18 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Shevkunov", "Kirill", ""], ["Prokhorenkova", "Liudmila", ""]]}, {"id": "2007.02474", "submitter": "Yingqiang Ge", "authors": "Yingqiang Ge, Shuya Zhao, Honglu Zhou, Changhua Pei, Fei Sun, Wenwu\n  Ou, Yongfeng Zhang", "title": "Understanding Echo Chambers in E-commerce Recommender Systems", "comments": null, "journal-ref": null, "doi": "10.1145/3397271.3401431", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized recommendation benefits users in accessing contents of interests\neffectively. Current research on recommender systems mostly focuses on matching\nusers with proper items based on user interests. However, significant efforts\nare missing to understand how the recommendations influence user preferences\nand behaviors, e.g., if and how recommendations result in \\textit{echo\nchambers}. Extensive efforts have been made in examining the phenomenon in\nonline media and social network systems. Meanwhile, there are growing concerns\nthat recommender systems might lead to the self-reinforcing of user's interests\ndue to narrowed exposure of items, which may be the potential cause of echo\nchamber. In this paper, we aim to analyze the echo chamber phenomenon in\nAlibaba Taobao -- one of the largest e-commerce platforms in the world. Echo\nchamber means the effect of user interests being reinforced through repeated\nexposure to similar contents. Based on the definition, we examine the presence\nof echo chamber in two steps. First, we explore whether user interests have\nbeen reinforced. Second, we check whether the reinforcement results from the\nexposure of similar contents. Our evaluations are enhanced with robust metrics,\nincluding cluster validity and statistical significance. Experiments are\nperformed on extensive collections of real-world data consisting of user\nclicks, purchases, and browse logs from Alibaba Taobao. Evidence suggests the\ntendency of echo chamber in user click behaviors, while it is relatively\nmitigated in user purchase behaviors. Insights from the results guide the\nrefinement of recommendation algorithms in real-world e-commerce systems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 00:09:26 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Ge", "Yingqiang", ""], ["Zhao", "Shuya", ""], ["Zhou", "Honglu", ""], ["Pei", "Changhua", ""], ["Sun", "Fei", ""], ["Ou", "Wenwu", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2007.02478", "submitter": "Yingqiang Ge", "authors": "Yingqiang Ge, Shuyuan Xu, Shuchang Liu, Zuohui Fu, Fei Sun, Yongfeng\n  Zhang", "title": "Learning Personalized Risk Preferences for Recommendation", "comments": null, "journal-ref": null, "doi": "10.1145/3397271.3401056", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of e-commerce has made people accustomed to shopping online.\nBefore making purchases on e-commerce websites, most consumers tend to rely on\nrating scores and review information to make purchase decisions. With this\ninformation, they can infer the quality of products to reduce the risk of\npurchase. Specifically, items with high rating scores and good reviews tend to\nbe less risky, while items with low rating scores and bad reviews might be\nrisky to purchase. On the other hand, the purchase behaviors will also be\ninfluenced by consumers' tolerance of risks, known as the risk attitudes.\nEconomists have studied risk attitudes for decades. These studies reveal that\npeople are not always rational enough when making decisions, and their risk\nattitudes may vary in different circumstances.\n  Most existing works over recommendation systems do not consider users' risk\nattitudes in modeling, which may lead to inappropriate recommendations to\nusers. For example, suggesting a risky item to a risk-averse person or a\nconservative item to a risk-seeking person may result in the reduction of user\nexperience. In this paper, we propose a novel risk-aware recommendation\nframework that integrates machine learning and behavioral economics to uncover\nthe risk mechanism behind users' purchasing behaviors. Concretely, we first\ndevelop statistical methods to estimate the risk distribution of each item and\nthen draw the Nobel-award winning Prospect Theory into our model to learn how\nusers choose from probabilistic alternatives that involve risks, where the\nprobabilities of the outcomes are uncertain. Experiments on several e-commerce\ndatasets demonstrate that our approach can achieve better performance than many\nclassical recommendation approaches, and further analyses also verify the\nadvantages of risk-aware recommendation beyond accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 00:16:20 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Ge", "Yingqiang", ""], ["Xu", "Shuyuan", ""], ["Liu", "Shuchang", ""], ["Fu", "Zuohui", ""], ["Sun", "Fei", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2007.02492", "submitter": "Vincent Nguyen", "authors": "Vincent Nguyen, Maciek Rybinski, Sarvnaz Karimi, Zhenchang Xing", "title": "Searching Scientific Literature for Answers on COVID-19 Questions", "comments": "4 pages + 1 page of references, submitted to ACL COVID-19 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding answers related to a pandemic of a novel disease raises new\nchallenges for information seeking and retrieval, as the new information\nbecomes available gradually. TREC COVID search track aims to assist in creating\nsearch tools to aid scientists, clinicians, policy makers and others with\nsimilar information needs in finding reliable answers from the scientific\nliterature. We experiment with different ranking algorithms as part of our\nparticipation in this challenge. We propose a novel method for neural\nretrieval, and demonstrate its effectiveness on the TREC COVID search.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 01:34:25 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Nguyen", "Vincent", ""], ["Rybinski", "Maciek", ""], ["Karimi", "Sarvnaz", ""], ["Xing", "Zhenchang", ""]]}, {"id": "2007.02620", "submitter": "Djoerd Hiemstra", "authors": "Djoerd Hiemstra", "title": "Reducing Misinformation in Query Autocompletions", "comments": "Published at the 2nd International Symposium on Open Search\n  Technology, 12-14 October 2020, CERN, Geneva, Switzerland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Query autocompletions help users of search engines to speed up their searches\nby recommending completions of partially typed queries in a drop down box.\nThese recommended query autocompletions are usually based on large logs of\nqueries that were previously entered by the search engine's users. Therefore,\nmisinformation entered -- either accidentally or purposely to manipulate the\nsearch engine -- might end up in the search engine's recommendations,\npotentially harming organizations, individuals, and groups of people. This\npaper proposes an alternative approach for generating query autocompletions by\nextracting anchor texts from a large web crawl, without the need to use query\nlogs. Our evaluation shows that even though query log autocompletions perform\nbetter for shorter queries, anchor text autocompletions outperform query log\nautocompletions for queries of 2 words or more.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 10:20:12 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 11:34:22 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Hiemstra", "Djoerd", ""]]}, {"id": "2007.02747", "submitter": "Ruihong Qiu", "authors": "Ruihong Qiu, Hongzhi Yin, Zi Huang, Tong Chen", "title": "GAG: Global Attributed Graph Neural Network for Streaming Session-based\n  Recommendation", "comments": null, "journal-ref": null, "doi": "10.1145/3397271.3401109", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Streaming session-based recommendation (SSR) is a challenging task that\nrequires the recommender system to do the session-based recommendation (SR) in\nthe streaming scenario. In the real-world applications of e-commerce and social\nmedia, a sequence of user-item interactions generated within a certain period\nare grouped as a session, and these sessions consecutively arrive in the form\nof streams. Most of the recent SR research has focused on the static setting\nwhere the training data is first acquired and then used to train a\nsession-based recommender model. They need several epochs of training over the\nwhole dataset, which is infeasible in the streaming setting. Besides, they can\nhardly well capture long-term user interests because of the neglect or the\nsimple usage of the user information. Although some streaming recommendation\nstrategies have been proposed recently, they are designed for streams of\nindividual interactions rather than streams of sessions. In this paper, we\npropose a Global Attributed Graph (GAG) neural network model with a Wasserstein\nreservoir for the SSR problem. On one hand, when a new session arrives, a\nsession graph with a global attribute is constructed based on the current\nsession and its associate user. Thus, the GAG can take both the global\nattribute and the current session into consideration to learn more\ncomprehensive representations of the session and the user, yielding a better\nperformance in the recommendation. On the other hand, for the adaptation to the\nstreaming session scenario, a Wasserstein reservoir is proposed to help\npreserve a representative sketch of the historical data. Extensive experiments\non two real-world datasets have been conducted to verify the superiority of the\nGAG model compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:41:19 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 12:13:36 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Qiu", "Ruihong", ""], ["Yin", "Hongzhi", ""], ["Huang", "Zi", ""], ["Chen", "Tong", ""]]}, {"id": "2007.02847", "submitter": "Guandong Xu", "authors": "Hamad Zogan, Imran Razzak, Xianzhi Wang, Shoaib Jameel, Guandong Xu", "title": "Explainable Depression Detection with Multi-Modalities Using a Hybrid\n  Deep Learning Model on Social Media", "comments": "23 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model interpretability has become important to engenders appropriate user\ntrust by providing the insight into the model prediction. However, most of the\nexisting machine learning methods provide no interpretability for depression\nprediction, hence their predictions are obscure to human. In this work, we\npropose interpretive Multi-Modal Depression Detection with Hierarchical\nAttention Network MDHAN, for detection depressed users on social media and\nexplain the model prediction. We have considered user posts along with\nTwitter-based multi-modal features, specifically, we encode user posts using\ntwo levels of attention mechanisms applied at the tweet-level and word-level,\ncalculate each tweet and words' importance, and capture semantic sequence\nfeatures from the user timelines (posts). Our experiments show that MDHAN\noutperforms several popular and robust baseline methods, demonstrating the\neffectiveness of combining deep learning with multi-modal features. We also\nshow that our model helps improve predictive performance when detecting\ndepression in users who are posting messages publicly on social media. MDHAN\nachieves excellent performance and ensures adequate evidence to explain the\nprediction.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 12:11:22 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 09:33:07 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Zogan", "Hamad", ""], ["Razzak", "Imran", ""], ["Wang", "Xianzhi", ""], ["Jameel", "Shoaib", ""], ["Xu", "Guandong", ""]]}, {"id": "2007.03020", "submitter": "Lakshya Kumar", "authors": "Shreyas Mangalgi, Lakshya Kumar and Ravindra Babu Tallamraju", "title": "Deep Contextual Embeddings for Address Classification in E-commerce", "comments": "9 Pages, 8 Figures, AI for fashion supply chain, KDD2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-commerce customers in developing nations like India tend to follow no fixed\nformat while entering shipping addresses. Parsing such addresses is challenging\nbecause of a lack of inherent structure or hierarchy. It is imperative to\nunderstand the language of addresses, so that shipments can be routed without\ndelays. In this paper, we propose a novel approach towards understanding\ncustomer addresses by deriving motivation from recent advances in Natural\nLanguage Processing (NLP). We also formulate different pre-processing steps for\naddresses using a combination of edit distance and phonetic algorithms. Then we\napproach the task of creating vector representations for addresses using\nWord2Vec with TF-IDF, Bi-LSTM and BERT based approaches. We compare these\napproaches with respect to sub-region classification task for North and South\nIndian cities. Through experiments, we demonstrate the effectiveness of\ngeneralized RoBERTa model, pre-trained over a large address corpus for language\nmodelling task. Our proposed RoBERTa model achieves a classification accuracy\nof around 90% with minimal text preprocessing for sub-region classification\ntask outperforming all other approaches. Once pre-trained, the RoBERTa model\ncan be fine-tuned for various downstream tasks in supply chain like pincode\nsuggestion and geo-coding. The model generalizes well for such tasks even with\nlimited labelled data. To the best of our knowledge, this is the first of its\nkind research proposing a novel approach of understanding customer addresses in\ne-commerce domain by pre-training language models and fine-tuning them for\ndifferent purposes.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 19:06:34 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Mangalgi", "Shreyas", ""], ["Kumar", "Lakshya", ""], ["Tallamraju", "Ravindra Babu", ""]]}, {"id": "2007.03106", "submitter": "Sarvesh Soni", "authors": "Sarvesh Soni and Kirk Roberts", "title": "An Evaluation of Two Commercial Deep Learning-Based Information\n  Retrieval Systems for COVID-19 Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has resulted in a tremendous need for access to the\nlatest scientific information, primarily through the use of text mining and\nsearch tools. This has led to both corpora for biomedical articles related to\nCOVID-19 (such as the CORD-19 corpus (Wang et al., 2020)) as well as search\nengines to query such data. While most research in search engines is performed\nin the academic field of information retrieval (IR), most academic search\nengines$\\unicode{x2013}$though rigorously evaluated$\\unicode{x2013}$are\nsparsely utilized, while major commercial web search engines (e.g., Google,\nBing) dominate. This relates to COVID-19 because it can be expected that\ncommercial search engines deployed for the pandemic will gain much higher\ntraction than those produced in academic labs, and thus leads to questions\nabout the empirical performance of these search tools. This paper seeks to\nempirically evaluate two such commercial search engines for COVID-19, produced\nby Google and Amazon, in comparison to the more academic prototypes evaluated\nin the context of the TREC-COVID track (Roberts et al., 2020). We performed\nseveral steps to reduce bias in the available manual judgments in order to\nensure a fair comparison of the two systems with those submitted to TREC-COVID.\nWe find that the top-performing system from TREC-COVID on bpref metric\nperformed the best among the different systems evaluated in this study on all\nthe metrics. This has implications for developing biomedical retrieval systems\nfor future health crises as well as trust in popular health search engines.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 22:49:46 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 22:49:39 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Soni", "Sarvesh", ""], ["Roberts", "Kirk", ""]]}, {"id": "2007.03137", "submitter": "Adewale Adeagbo", "authors": "Adewale Adeagbo", "title": "Predicting Afrobeats Hit Songs Using Spotify Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study approached the Hit Song Science problem with the aim of predicting\nwhich songs in the Afrobeats genre will become popular among Spotify listeners.\nA dataset of 2063 songs was generated through the Spotify Web API, with the\nprovided audio features. Random Forest and Gradient Boosting algorithms proved\nto be successful with approximately F1 scores of 86%.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 00:14:30 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 13:13:56 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Adeagbo", "Adewale", ""]]}, {"id": "2007.03183", "submitter": "Manqing Dong", "authors": "Manqing Dong and Feng Yuan and Lina Yao and Xiwei Xu and Liming Zhu", "title": "MAMO: Memory-Augmented Meta-Optimization for Cold-start Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A common challenge for most current recommender systems is the cold-start\nproblem. Due to the lack of user-item interactions, the fine-tuned recommender\nsystems are unable to handle situations with new users or new items. Recently,\nsome works introduce the meta-optimization idea into the recommendation\nscenarios, i.e. predicting the user preference by only a few of past interacted\nitems. The core idea is learning a global sharing initialization parameter for\nall users and then learning the local parameters for each user separately.\nHowever, most meta-learning based recommendation approaches adopt\nmodel-agnostic meta-learning for parameter initialization, where the global\nsharing parameter may lead the model into local optima for some users. In this\npaper, we design two memory matrices that can store task-specific memories and\nfeature-specific memories. Specifically, the feature-specific memories are used\nto guide the model with personalized parameter initialization, while the\ntask-specific memories are used to guide the model fast predicting the user\npreference. And we adopt a meta-optimization approach for optimizing the\nproposed method. We test the model on two widely used recommendation datasets\nand consider four cold-start situations. The experimental results show the\neffectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 03:25:15 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Dong", "Manqing", ""], ["Yuan", "Feng", ""], ["Yao", "Lina", ""], ["Xu", "Xiwei", ""], ["Zhu", "Liming", ""]]}, {"id": "2007.03225", "submitter": "Paheli Bhattacharya", "authors": "Paheli Bhattacharya, Kripabandhu Ghosh, Arindam Pal, Saptarshi Ghosh", "title": "Hier-SPCNet: A Legal Statute Hierarchy-based Heterogeneous Network for\n  Computing Legal Case Document Similarity", "comments": "Accepted at the 43rd International ACM SIGIR Conference on Research\n  and Development in Information Retrieval, 2020 (Short Paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing similarity between two legal case documents is an important and\nchallenging task in Legal IR, for which text-based and network-based measures\nhave been proposed in literature. All prior network-based similarity methods\nconsidered a precedent citation network among case documents only (PCNet).\nHowever, this approach misses an important source of legal knowledge -- the\nhierarchy of legal statutes that are applicable in a given legal jurisdiction\n(e.g., country). We propose to augment the PCNet with the hierarchy of legal\nstatutes, to form a heterogeneous network Hier-SPCNet, having citation links\nbetween case documents and statutes, as well as citation and hierarchy links\namong the statutes. Experiments over a set of Indian Supreme Court case\ndocuments show that our proposed heterogeneous network enables significantly\nbetter document similarity estimation, as compared to existing approaches using\nPCNet. We also show that the proposed network-based method can complement\ntext-based measures for better estimation of legal document similarity.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 06:30:46 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Bhattacharya", "Paheli", ""], ["Ghosh", "Kripabandhu", ""], ["Pal", "Arindam", ""], ["Ghosh", "Saptarshi", ""]]}, {"id": "2007.03383", "submitter": "Kang Liu", "authors": "Kang Liu, Feng Xue, and Richang Hong", "title": "RGCF: Refined Graph Convolution Collaborative Filtering with concise and\n  expressive embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolution Network (GCN) has attracted significant attention and\nbecome the most popular method for learning graph representations. In recent\nyears, many efforts have been focused on integrating GCN into the recommender\ntasks and have made remarkable progress. At its core is to explicitly capture\nhigh-order connectivities between the nodes in user-item bipartite graph.\nHowever, we theoretically and empirically find an inherent drawback existed in\nthese GCN-based recommendation methods, where GCN is directly applied to\naggregate neighboring nodes will introduce noise and information redundancy.\nConsequently, the these models' capability of capturing high-order\nconnectivities among different nodes is limited, leading to suboptimal\nperformance of the recommender tasks. The main reason is that the the nonlinear\nnetwork layer inside GCN structure is not suitable for extracting non-sematic\nfeatures(such as one-hot ID feature) in the collaborative filtering scenarios.\nIn this work, we develop a new GCN-based Collaborative Filtering model, named\nRefined Graph convolution Collaborative Filtering(RGCF), where the construction\nof the embeddings of users (items) are delicately redesigned from several\naspects during the aggregation on the graph. Compared to the state-of-the-art\nGCN-based recommendation, RGCF is more capable for capturing the implicit\nhigh-order connectivities inside the graph and the resultant vector\nrepresentations are more expressive. We conduct extensive experiments on three\npublic million-size datasets, demonstrating that our RGCF significantly\noutperforms state-of-the-art models. We release our code at\nhttps://github.com/hfutmars/RGCF.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 12:26:10 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 04:32:32 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liu", "Kang", ""], ["Xue", "Feng", ""], ["Hong", "Richang", ""]]}, {"id": "2007.03505", "submitter": "Gabriele D'Angelo", "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D'Angelo", "title": "On the Efficiency of Decentralized File Storage for Personal Information\n  Management Systems", "comments": "To appear in the Proceedings of the 25th IEEE Symposium on Computers\n  and Communications (ISCC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.IR cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an architecture, based on Distributed Ledger Technologies\n(DLTs) and Decentralized File Storage (DFS) systems, to support the use of\nPersonal Information Management Systems (PIMS). DLT and DFS are used to manage\ndata sensed by mobile users equipped with devices with sensing capability. DLTs\nguarantee the immutability, traceability and verifiability of references to\npersonal data, that are stored in DFS. In fact, the inclusion of data digests\nin the DLT makes it possible to obtain an unalterable reference and a\ntamper-proof log, while remaining compliant with the regulations on personal\ndata, i.e. GDPR. We provide an experimental evaluation on the feasibility of\nthe use of DFS. Three different scenarios have been studied: i) a proprietary\nIPFS approach with a dedicated node interfacing with the data producers, ii) a\npublic IPFS service and iii) Sia Skynet. Results show that through proper\nconfiguration of the system infrastructure, it is viable to build a\ndecentralized Personal Data Storage (PDS).\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:41:18 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Zichichi", "Mirko", ""], ["Ferretti", "Stefano", ""], ["D'Angelo", "Gabriele", ""]]}, {"id": "2007.03634", "submitter": "Aditya Pal", "authors": "Aditya Pal, Chantat Eksombatchai, Yitong Zhou, Bo Zhao, Charles\n  Rosenberg, Jure Leskovec", "title": "PinnerSage: Multi-Modal User Embedding Framework for Recommendations at\n  Pinterest", "comments": "10 pages, 7 figures", "journal-ref": "KDD 2020", "doi": "10.1145/3394486.3403280", "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Latent user representations are widely adopted in the tech industry for\npowering personalized recommender systems. Most prior work infers a single high\ndimensional embedding to represent a user, which is a good starting point but\nfalls short in delivering a full understanding of the user's interests. In this\nwork, we introduce PinnerSage, an end-to-end recommender system that represents\neach user via multi-modal embeddings and leverages this rich representation of\nusers to provides high quality personalized recommendations. PinnerSage\nachieves this by clustering users' actions into conceptually coherent clusters\nwith the help of a hierarchical clustering method (Ward) and summarizes the\nclusters via representative pins (Medoids) for efficiency and interpretability.\nPinnerSage is deployed in production at Pinterest and we outline the several\ndesign decisions that makes it run seamlessly at a very large scale. We conduct\nseveral offline and online A/B experiments to show that our method\nsignificantly outperforms single embedding methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:13:20 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Pal", "Aditya", ""], ["Eksombatchai", "Chantat", ""], ["Zhou", "Yitong", ""], ["Zhao", "Bo", ""], ["Rosenberg", "Charles", ""], ["Leskovec", "Jure", ""]]}, {"id": "2007.03771", "submitter": "Kartikey Pant", "authors": "Kartikey Pant and Tanvi Dadu", "title": "Cross-lingual Inductive Transfer to Detect Offensive Language", "comments": "Accepted at OffenseEval 2020 to be held at COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing use of social media and its availability, many instances of\nthe use of offensive language have been observed across multiple languages and\ndomains. This phenomenon has given rise to the growing need to detect the\noffensive language used in social media cross-lingually. In OffensEval 2020,\nthe organizers have released the \\textit{multilingual Offensive Language\nIdentification Dataset} (mOLID), which contains tweets in five different\nlanguages, to detect offensive language. In this work, we introduce a\ncross-lingual inductive approach to identify the offensive language in tweets\nusing the contextual word embedding \\textit{XLM-RoBERTa} (XLM-R). We show that\nour model performs competitively on all five languages, obtaining the fourth\nposition in the English task with an F1-score of $0.919$ and eighth position in\nthe Turkish task with an F1-score of $0.781$. Further experimentation proves\nthat our model works competitively in a zero-shot learning environment, and is\nextensible to other languages.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 20:10:31 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Pant", "Kartikey", ""], ["Dadu", "Tanvi", ""]]}, {"id": "2007.03777", "submitter": "Huaiyi Huang", "authors": "Huaiyi Huang, Yuqi Zhang, Qingqiu Huang, Zhengkui Guo, Ziwei Liu, and\n  Dahua Lin", "title": "Placepedia: Comprehensive Place Understanding with Multi-Faceted\n  Annotations", "comments": "To appear in ECCV 2020. Dataset is available at:\n  https://hahehi.github.io/placepedia.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place is an important element in visual understanding. Given a photo of a\nbuilding, people can often tell its functionality, e.g. a restaurant or a shop,\nits cultural style, e.g. Asian or European, as well as its economic type, e.g.\nindustry oriented or tourism oriented. While place recognition has been widely\nstudied in previous work, there remains a long way towards comprehensive place\nunderstanding, which is far beyond categorizing a place with an image and\nrequires information of multiple aspects. In this work, we contribute\nPlacepedia, a large-scale place dataset with more than 35M photos from 240K\nunique places. Besides the photos, each place also comes with massive\nmulti-faceted information, e.g. GDP, population, etc., and labels at multiple\nlevels, including function, city, country, etc.. This dataset, with its large\namount of data and rich annotations, allows various studies to be conducted.\nParticularly, in our studies, we develop 1) PlaceNet, a unified framework for\nmulti-level place recognition, and 2) a method for city embedding, which can\nproduce a vector representation for a city that captures both visual and\nmulti-faceted side information. Such studies not only reveal key challenges in\nplace understanding, but also establish connections between visual observations\nand underlying socioeconomic/cultural implications.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 20:17:01 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 08:17:10 GMT"}, {"version": "v3", "created": "Sun, 12 Jul 2020 16:38:50 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 08:56:05 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Huang", "Huaiyi", ""], ["Zhang", "Yuqi", ""], ["Huang", "Qingqiu", ""], ["Guo", "Zhengkui", ""], ["Liu", "Ziwei", ""], ["Lin", "Dahua", ""]]}, {"id": "2007.03805", "submitter": "Tuan Manh Lai", "authors": "Tuan Manh Lai, Trung Bui, Nedim Lipka", "title": "ISA: An Intelligent Shopping Assistant", "comments": "Accepted by AACL 2020 (Demo)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growth of e-commerce, brick-and-mortar stores are still the\npreferred destinations for many people. In this paper, we present ISA, a\nmobile-based intelligent shopping assistant that is designed to improve\nshopping experience in physical stores. ISA assists users by leveraging\nadvanced techniques in computer vision, speech processing, and natural language\nprocessing. An in-store user only needs to take a picture or scan the barcode\nof the product of interest, and then the user can talk to the assistant about\nthe product. The assistant can also guide the user through the purchase process\nor recommend other similar products to the user. We take a data-driven approach\nin building the engines of ISA's natural language processing component, and the\nengines achieve good performance.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 21:57:34 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 05:42:24 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Lai", "Tuan Manh", ""], ["Bui", "Trung", ""], ["Lipka", "Nedim", ""]]}, {"id": "2007.03824", "submitter": "Ikechukwu Onyenwe", "authors": "Ikechukwu Onyenwe, Samuel Nwagbo, Njideka Mbeledogu, Ebele Onyedinma", "title": "The impact of political party/candidate on the election results from a\n  sentiment analysis perspective using #AnambraDecides2017 tweets", "comments": "This is a pre-print of an article published in Social Network\n  Analysis and Mining (2020) 10:55. The final authenticated version is\n  available online at: https://doi.org/10.1007/s13278-020-00667-2. This article\n  comprises of 17 pages, 15 figures an 34 references", "journal-ref": "Soc. Netw. Anal. Min. 10, 55 (2020)", "doi": "10.1007/s13278-020-00667-2", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates empirically the impact of political party control over\nits candidates or vice versa on winning an election using a natural language\nprocessing technique called sentiment analysis (SA). To do this, a set of 7430\ntweets bearing or related to #AnambraDecides2017 was streamed during the\nNovember 18, 2017, Anambra State gubernatorial election. These are Twitter\ndiscussions on the top five political parties and their candidates termed\npolitical actors in this paper. We conduct polarity and subjectivity sentiment\nanalyses on all the tweets considering time as a useful dimension of SA.\nFurthermore, we use the word frequency to find words most associated with the\npolitical actors in a given time. We find most talked about topics using a\ntopic modeling algorithm and how the computed sentiments and most frequent\nwords are related to the topics per political actor. Among other things, we\ndeduced from the experimental results that even though a political party serves\nas a platform that sales the personality of a candidate, the acceptance of the\ncandidate/party adds to the winning of an election. For example, we found the\nwinner of the election Willie Obiano benefiting from the values his party share\namong the people of the State. Associating his name with his party, All\nProgressive Grand Alliance (APGA) displays more positive sentiments and the\nsubjective sentiment analysis indicates that Twitter users mentioning APGA are\nless emotionally subjective in their tweets than the other parties.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 23:41:56 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Onyenwe", "Ikechukwu", ""], ["Nwagbo", "Samuel", ""], ["Mbeledogu", "Njideka", ""], ["Onyedinma", "Ebele", ""]]}, {"id": "2007.04002", "submitter": "Daisuke Moriwaki", "authors": "Daisuke Moriwaki and Yuta Hayakawa and Isshu Munemasa and Yuta Saito\n  and Akira Matsui", "title": "Unbiased Lift-based Bidding System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional bidding strategies for online display ad auction heavily relies\non observed performance indicators such as clicks or conversions. A bidding\nstrategy naively pursuing these easily observable metrics, however, fails to\noptimize the profitability of the advertisers. Rather, the bidding strategy\nthat leads to the maximum revenue is a strategy pursuing the performance lift\nof showing ads to a specific user. Therefore, it is essential to predict the\nlift-effect of showing ads to each user on their target variables from observed\nlog data. However, there is a difficulty in predicting the lift-effect, as the\ntraining data gathered by a past bidding strategy may have a strong bias\ntowards the winning impressions. In this study, we develop Unbiased Lift-based\nBidding System, which maximizes the advertisers' profit by accurately\npredicting the lift-effect from biased log data. Our system is the first to\nenable high-performing lift-based bidding strategy by theoretically alleviating\nthe inherent bias in the log. Real-world, large-scale A/B testing successfully\ndemonstrates the superiority and practicability of the proposed system.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 10:05:53 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 02:09:17 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Moriwaki", "Daisuke", ""], ["Hayakawa", "Yuta", ""], ["Munemasa", "Isshu", ""], ["Saito", "Yuta", ""], ["Matsui", "Akira", ""]]}, {"id": "2007.04032", "submitter": "Yuanhang Zhou", "authors": "Kun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuanhang Zhou, Ji-Rong Wen,\n  Jingsong Yu", "title": "Improving Conversational Recommender Systems via Knowledge Graph based\n  Semantic Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational recommender systems (CRS) aim to recommend high-quality items\nto users through interactive conversations. Although several efforts have been\nmade for CRS, two major issues still remain to be solved. First, the\nconversation data itself lacks of sufficient contextual information for\naccurately understanding users' preference. Second, there is a semantic gap\nbetween natural language expression and item-level user preference. To address\nthese issues, we incorporate both word-oriented and entity-oriented knowledge\ngraphs (KG) to enhance the data representations in CRSs, and adopt Mutual\nInformation Maximization to align the word-level and entity-level semantic\nspaces. Based on the aligned semantic representations, we further develop a\nKG-enhanced recommender component for making accurate recommendations, and a\nKG-enhanced dialog component that can generate informative keywords or entities\nin the response text. Extensive experiments have demonstrated the effectiveness\nof our approach in yielding better performance on both recommendation and\nconversation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 11:14:23 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Zhou", "Kun", ""], ["Zhao", "Wayne Xin", ""], ["Bian", "Shuqing", ""], ["Zhou", "Yuanhang", ""], ["Wen", "Ji-Rong", ""], ["Yu", "Jingsong", ""]]}, {"id": "2007.04125", "submitter": "Peter Hillmann", "authors": "Matthias Schopp, Peter Hillmann", "title": "Agile Approach for IT Forensics Management", "comments": "Journal of Internet Technology and Secured Transactions (JITST) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.IR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The forensic investigation of cyber attacks and IT incidents is becoming\nincreasingly difficult due to increasing complexity and intensify networking.\nEspecially with Advanced Attacks (AT) like the increasing Advanced Persistent\nThreats an agile approach is indispensable. Several systems are involved in an\nattack (multi-host attacks). Current forensic models and procedures show\nconsiderable deficits in the process of analyzing such attacks. For this\npurpose, this paper presents the novel flower model, which uses agile methods\nand forms a new forensic management approach. In this way, the growing\nchallenges of ATs are met. In the forensic investigation of such attacks, big\ndata problems have to be solved due to the amount of data that needs to be\nanalyzed. The proposed model meets this requirement by precisely defining the\nquestions that need to be answered in an early state and collecting only the\nevidence usable in court proceedings that is needed to answer these questions.\nAdditionally, the novel flower model for AT is presented that meets the\ndifferent phases of an investigation process.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 13:48:50 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Schopp", "Matthias", ""], ["Hillmann", "Peter", ""]]}, {"id": "2007.04249", "submitter": "Subramaniam Kazhuparambil Mr.", "authors": "Subramaniam Kazhuparambil (1) and Abhishek Kaushik (1 and 2) ((1)\n  Dublin Business School, (2) Dublin City University)", "title": "Cooking Is All About People: Comment Classification On Cookery Channels\n  Using BERT and Classification Models (Malayalam-English Mix-Code)", "comments": "Rectified typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The scope of a lucrative career promoted by Google through its video\ndistribution platform YouTube has attracted a large number of users to become\ncontent creators. An important aspect of this line of work is the feedback\nreceived in the form of comments which show how well the content is being\nreceived by the audience. However, volume of comments coupled with spam and\nlimited tools for comment classification makes it virtually impossible for a\ncreator to go through each and every comment and gather constructive feedback.\nAutomatic classification of comments is a challenge even for established\nclassification models, since comments are often of variable lengths riddled\nwith slang, symbols and abbreviations. This is a greater challenge where\ncomments are multilingual as the messages are often rife with the respective\nvernacular. In this work, we have evaluated top-performing classification\nmodels for classifying comments which are a mix of different combinations of\nEnglish and Malayalam (only English, only Malayalam and Mix of English and\nMalayalam). The statistical analysis of results indicates that Multinomial\nNaive Bayes, K-Nearest Neighbors (KNN), Support Vector Machine (SVM), Random\nForest and Decision Trees offer similar level of accuracy in comment\nclassification. Further, we have also evaluated 3 multilingual transformer\nbased language models (BERT, DISTILBERT and XLM) and compared their performance\nto the traditional machine learning classification techniques. XLM was the\ntop-performing BERT model with an accuracy of 67.31. Random Forest with Term\nFrequency Vectorizer was the best performing model out of all the traditional\nclassification models with an accuracy of 63.59.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 19:07:06 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 12:57:24 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 08:40:09 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Kazhuparambil", "Subramaniam", "", "1 and 2"], ["Kaushik", "Abhishek", "", "1 and 2"]]}, {"id": "2007.04357", "submitter": "Sagar Uprety Mr.", "authors": "Sagar Uprety and Dimitris Gkoumas and Dawei Song", "title": "A Survey of Quantum Theory Inspired Approaches to Information Retrieval", "comments": "Accepted for publication at ACM Computing Surveys on May 20, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since 2004, researchers have been using the mathematical framework of Quantum\nTheory (QT) in Information Retrieval (IR). QT offers a generalized probability\nand logic framework. Such a framework has been shown capable of unifying the\nrepresentation, ranking and user cognitive aspects of IR, and helpful in\ndeveloping more dynamic, adaptive and context-aware IR systems. Although\nQuantum-inspired IR is still a growing area, a wide array of work in different\naspects of IR has been done and produced promising results. This paper presents\na survey of the research done in this area, aiming to show the landscape of the\nfield and draw a road-map of future directions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 18:24:59 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Uprety", "Sagar", ""], ["Gkoumas", "Dimitris", ""], ["Song", "Dawei", ""]]}, {"id": "2007.04524", "submitter": "Yingjie Hu", "authors": "Jimin Wang, Yingjie Hu", "title": "Enhancing spatial and textual analysis with EUPEG: an extensible and\n  unified platform for evaluating geoparsers", "comments": null, "journal-ref": "Transactions in GIS, 2019, 23(6), 1393-1419", "doi": "10.1111/tgis.12579", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rich amount of geographic information exists in unstructured texts, such as\nWeb pages, social media posts, housing advertisements, and historical archives.\nGeoparsers are useful tools that extract structured geographic information from\nunstructured texts, thereby enabling spatial analysis on textual data. While a\nnumber of geoparsers were developed, they were tested on different datasets\nusing different metrics. Consequently, it is difficult to compare existing\ngeoparsers or to compare a new geoparser with existing ones. In recent years,\nresearchers created open and annotated corpora for testing geoparsers. While\nthese corpora are extremely valuable, much effort is still needed for a\nresearcher to prepare these datasets and deploy geoparsers for comparative\nexperiments. This paper presents EUPEG: an Extensible and Unified Platform for\nEvaluating Geoparsers. EUPEG is an open source and Web based benchmarking\nplatform which hosts a majority of open corpora, geoparsers, and performance\nmetrics reported in the literature. It enables direct comparison of the hosted\ngeoparsers, and a new geoparser can be connected to EUPEG and compared with\nother geoparsers. The main objective of EUPEG is to reduce the time and effort\nthat researchers have to spend in preparing datasets and baselines, thereby\nincreasing the efficiency and effectiveness of comparative experiments.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 03:00:02 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Wang", "Jimin", ""], ["Hu", "Yingjie", ""]]}, {"id": "2007.04526", "submitter": "Shiwei Zhang", "authors": "Shiwei Zhang, Xiuzhen Zhang, Jey Han Lau, Jeffrey Chan, and Cecile\n  Paris", "title": "Less is More: Rejecting Unreliable Reviews for Product Question\n  Answering", "comments": "ECML-PKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Promptly and accurately answering questions on products is important for\ne-commerce applications. Manually answering product questions (e.g. on\ncommunity question answering platforms) results in slow response and does not\nscale. Recent studies show that product reviews are a good source for\nreal-time, automatic product question answering (PQA). In the literature, PQA\nis formulated as a retrieval problem with the goal to search for the most\nrelevant reviews to answer a given product question. In this paper, we focus on\nthe issue of answerability and answer reliability for PQA using reviews. Our\ninvestigation is based on the intuition that many questions may not be\nanswerable with a finite set of reviews. When a question is not answerable, a\nsystem should return nil answers rather than providing a list of irrelevant\nreviews, which can have significant negative impact on user experience.\nMoreover, for answerable questions, only the most relevant reviews that answer\nthe question should be included in the result. We propose a conformal\nprediction based framework to improve the reliability of PQA systems, where we\nreject unreliable answers so that the returned results are more concise and\naccurate at answering the product question, including returning nil answers for\nunanswerable questions. Experiments on a widely used Amazon dataset show\nencouraging results of our proposed framework. More broadly, our results\ndemonstrate a novel and effective application of conformal methods to a\nretrieval task.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 03:08:55 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Zhang", "Shiwei", ""], ["Zhang", "Xiuzhen", ""], ["Lau", "Jey Han", ""], ["Chan", "Jeffrey", ""], ["Paris", "Cecile", ""]]}, {"id": "2007.04697", "submitter": "Anastasija Nikiforova", "authors": "Anastasija Nikiforova", "title": "Open Data Quality Evaluation: A Comparative Analysis of Open Data in\n  Latvia", "comments": "24 pages, 2 tables, 3 figures, Baltic J. Modern Computing", "journal-ref": "Baltic J. Modern Computing, Vol. 6(2018), No. 4, 363-386", "doi": "10.22364/bjmc.2018.6.4.04", "report-no": null, "categories": "cs.DB cs.CY cs.IR stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays open data is entering the mainstream - it is free available for\nevery stakeholder and is often used in business decision-making. It is\nimportant to be sure data is trustable and error-free as its quality problems\ncan lead to huge losses. The research discusses how (open) data quality could\nbe assessed. It also covers main points which should be considered developing a\ndata quality management solution. One specific approach is applied to several\nLatvian open data sets. The research provides a step-by-step open data sets\nanalysis guide and summarizes its results. It is also shown there could exist\ndifferences in data quality depending on data supplier (centralized and\ndecentralized data releases) and, unfortunately, trustable data supplier cannot\nguarantee data quality problems absence. There are also underlined common data\nquality problems detected not only in Latvian open data but also in open data\nof 3 European countries.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 10:43:28 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Nikiforova", "Anastasija", ""]]}, {"id": "2007.04782", "submitter": "Igor Andr\\'e Pegoraro Santana", "authors": "Igor Andr\\'e Pegoraro Santana, Marcos Aurelio Domingues", "title": "A Systematic Review on Context-Aware Recommender Systems using Deep\n  Learning and Embeddings", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender Systems are tools that improve how users find relevant\ninformation in web systems, so they do not face too much information. In order\nto generate better recommendations, the context of information should be used\nin the recommendation process. Context-Aware Recommender Systems were created,\naccomplishing state-of-the-art results and improving traditional recommender\nsystems. There are many approaches to build recommender systems, and two of the\nmost prominent advances in area have been the use of Embeddings to represent\nthe data in the recommender system, and the use of Deep Learning architectures\nto generate the recommendations to the user. A systematic review adopts a\nformal and systematic method to perform a bibliographic review, and it is used\nto identify and evaluate all the research in certain area of study, by\nanalyzing the relevant research published. A systematic review was conducted to\nunderstand how the Deep Learning and Embeddings techniques are being applied to\nimprove Context-Aware Recommender Systems. We summarized the architectures that\nare used to create those and the domains that they are used.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:23:40 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Santana", "Igor Andr\u00e9 Pegoraro", ""], ["Domingues", "Marcos Aurelio", ""]]}, {"id": "2007.04833", "submitter": "Hengrui Zhang", "authors": "Qitian Wu, Hengrui Zhang, Xiaofeng Gao, Junchi Yan, Hongyuan Zha", "title": "Towards Open-World Recommendation: An Inductive Model-based\n  Collaborative Filtering Approach", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommendation models can effectively estimate underlying user interests and\npredict one's future behaviors by factorizing an observed user-item rating\nmatrix into products of two sets of latent factors. However, the user-specific\nembedding factors can only be learned in a transductive way, making it\ndifficult to handle new users on-the-fly. In this paper, we propose an\ninductive collaborative filtering framework that contains two representation\nmodels. The first model follows conventional matrix factorization which\nfactorizes a group of key users' rating matrix to obtain meta latents. The\nsecond model resorts to attention-based structure learning that estimates\nhidden relations from query to key users and learns to leverage meta latents to\ninductively compute embeddings for query users via neural message passing. Our\nmodel enables inductive representation learning for users and meanwhile\nguarantees equivalent representation capacity as matrix factorization.\nExperiments demonstrate that our model achieves promising results for\nrecommendation on few-shot users with limited training ratings and new unseen\nusers which are commonly encountered in open-world recommender systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 14:31:25 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 15:25:21 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Wu", "Qitian", ""], ["Zhang", "Hengrui", ""], ["Gao", "Xiaofeng", ""], ["Yan", "Junchi", ""], ["Zha", "Hongyuan", ""]]}, {"id": "2007.05039", "submitter": "Timothy Hazen", "authors": "Timothy J. Hazen and Alexandra Olteanu and Gabriella Kazai and\n  Fernando Diaz and Michael Golebiewski", "title": "On the Social and Technical Challenges of Web Search Autosuggestion\n  Moderation", "comments": "17 Pages, 4 images displayed within 3 latex figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Past research shows that users benefit from systems that support them in\ntheir writing and exploration tasks. The autosuggestion feature of Web search\nengines is an example of such a system: It helps users in formulating their\nqueries by offering a list of suggestions as they type. Autosuggestions are\ntypically generated by machine learning (ML) systems trained on a corpus of\nsearch logs and document representations. Such automated methods can become\nprone to issues that result in problematic suggestions that are biased, racist,\nsexist or in other ways inappropriate. While current search engines have become\nincreasingly proficient at suppressing such problematic suggestions, there are\nstill persistent issues that remain. In this paper, we reflect on past efforts\nand on why certain issues still linger by covering explored solutions along a\nprototypical pipeline for identifying, detecting, and addressing problematic\nautosuggestions. To showcase their complexity, we discuss several dimensions of\nproblematic suggestions, difficult issues along the pipeline, and why our\ndiscussion applies to the increasing number of applications beyond web search\nthat implement similar textual suggestion features. By outlining persistent\nsocial and technical challenges in moderating web search suggestions, we\nprovide a renewed call for action.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 19:22:00 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Hazen", "Timothy J.", ""], ["Olteanu", "Alexandra", ""], ["Kazai", "Gabriella", ""], ["Diaz", "Fernando", ""], ["Golebiewski", "Michael", ""]]}, {"id": "2007.05163", "submitter": "Leonard Poon", "authors": "Leonard K. M. Poon and Nevin L. Zhang and Haoran Xie and Gary Cheng", "title": "Handling Collocations in Hierarchical Latent Tree Analysis for Topic\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling has been one of the most active research areas in machine\nlearning in recent years. Hierarchical latent tree analysis (HLTA) has been\nrecently proposed for hierarchical topic modeling and has shown superior\nperformance over state-of-the-art methods. However, the models used in HLTA\nhave a tree structure and cannot represent the different meanings of multiword\nexpressions sharing the same word appropriately. Therefore, we propose a method\nfor extracting and selecting collocations as a preprocessing step for HLTA. The\nselected collocations are replaced with single tokens in the bag-of-words model\nbefore running HLTA. Our empirical evaluation shows that the proposed method\nled to better performance of HLTA on three of the four data sets tested.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 04:56:36 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Poon", "Leonard K. M.", ""], ["Zhang", "Nevin L.", ""], ["Xie", "Haoran", ""], ["Cheng", "Gary", ""]]}, {"id": "2007.05186", "submitter": "Xuan Shan", "authors": "Xuan Shan, Chuanjie Liu, Yiqian Xia, Qi Chen, Yusi Zhang, Kaize Ding,\n  Yaobo Liang, Angen Luo, Yuxiang Luo", "title": "GLOW : Global Weighted Self-Attention Network for Web Search", "comments": "8pages,2figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep matching models aim to facilitate search engines retrieving more\nrelevant documents by mapping queries and documents into semantic vectors in\nthe first-stage retrieval. When leveraging BERT as the deep matching model, the\nattention score across two words are solely built upon local contextualized\nword embeddings. It lacks prior global knowledge to distinguish the importance\nof different words, which has been proved to play a critical role in\ninformation retrieval tasks. In addition to this, BERT only performs attention\nacross sub-words tokens which weakens whole word attention representation. We\npropose a novel Global Weighted Self-Attention (GLOW) network for web document\nsearch. GLOW fuses global corpus statistics into the deep matching model. By\nadding prior weights into attention generation from global information, like\nBM25, GLOW successfully learns weighted attention scores jointly with query\nmatrix $Q$ and key matrix $K$. We also present an efficient whole word weight\nsharing solution to bring prior whole word knowledge into sub-words level\nattention. It aids Transformer to learn whole word level attention. To make our\nmodels applicable to complicated web search scenarios, we introduce combined\nfields representation to accommodate documents with multiple fields even with\nvariable number of instances. We demonstrate GLOW is more efficient to capture\nthe topical and semantic representation both in queries and documents.\nIntrinsic evaluation and experiments conducted on public data sets reveal GLOW\nto be a general framework for document retrieve task. It significantly\noutperforms BERT and other competitive baselines by a large margin while\nretaining the same model complexity with BERT.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 06:18:07 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 16:21:30 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 03:02:50 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Shan", "Xuan", ""], ["Liu", "Chuanjie", ""], ["Xia", "Yiqian", ""], ["Chen", "Qi", ""], ["Zhang", "Yusi", ""], ["Ding", "Kaize", ""], ["Liang", "Yaobo", ""], ["Luo", "Angen", ""], ["Luo", "Yuxiang", ""]]}, {"id": "2007.05302", "submitter": "Andreas Vogelsang", "authors": "Kim Julian G\\\"ulle, Nicholas Ford, Patrick Ebel, Florian Brokhausen,\n  Andreas Vogelsang", "title": "Topic Modeling on User Stories using Word Mover's Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Requirements elicitation has recently been complemented with crowd-based\ntechniques, which continuously involve large, heterogeneous groups of users who\nexpress their feedback through a variety of media. Crowd-based elicitation has\ngreat potential for engaging with (potential) users early on but also results\nin large sets of raw and unstructured feedback. Consolidating and analyzing\nthis feedback is a key challenge for turning it into sensible user\nrequirements. In this paper, we focus on topic modeling as a means to identify\ntopics within a large set of crowd-generated user stories and compare three\napproaches: (1) a traditional approach based on Latent Dirichlet Allocation,\n(2) a combination of word embeddings and principal component analysis, and (3)\na combination of word embeddings and Word Mover's Distance. We evaluate the\napproaches on a publicly available set of 2,966 user stories written and\ncategorized by crowd workers. We found that a combination of word embeddings\nand Word Mover's Distance is most promising. Depending on the word embeddings\nwe use in our approaches, we manage to cluster the user stories in two ways:\none that is closer to the original categorization and another that allows new\ninsights into the dataset, e.g. to find potentially new categories.\nUnfortunately, no measure exists to rate the quality of our results\nobjectively. Still, our findings provide a basis for future work towards\nanalyzing crowd-sourced user stories.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 11:05:42 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 09:22:50 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["G\u00fclle", "Kim Julian", ""], ["Ford", "Nicholas", ""], ["Ebel", "Patrick", ""], ["Brokhausen", "Florian", ""], ["Vogelsang", "Andreas", ""]]}, {"id": "2007.05891", "submitter": "Yi Tay", "authors": "Yi Tay, Zhe Zhao, Dara Bahri, Donald Metzler, Da-Cheng Juan", "title": "HyperGrid: Efficient Multi-Task Transformers with Grid-wise Decomposable\n  Hyper Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving state-of-the-art performance on natural language understanding\ntasks typically relies on fine-tuning a fresh model for every task.\nConsequently, this approach leads to a higher overall parameter cost, along\nwith higher technical maintenance for serving multiple models. Learning a\nsingle multi-task model that is able to do well for all the tasks has been a\nchallenging and yet attractive proposition. In this paper, we propose\n\\textsc{HyperGrid}, a new approach for highly effective multi-task learning.\nThe proposed approach is based on a decomposable hypernetwork that learns\ngrid-wise projections that help to specialize regions in weight matrices for\ndifferent tasks. In order to construct the proposed hypernetwork, our method\nlearns the interactions and composition between a global (task-agnostic) state\nand a local task-specific state. We apply our proposed \\textsc{HyperGrid} on\nthe current state-of-the-art T5 model, demonstrating strong performance across\nthe GLUE and SuperGLUE benchmarks when using only a single multi-task model.\nOur method helps bridge the gap between fine-tuning and multi-task learning\napproaches.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 02:49:16 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Tay", "Yi", ""], ["Zhao", "Zhe", ""], ["Bahri", "Dara", ""], ["Metzler", "Donald", ""], ["Juan", "Da-Cheng", ""]]}, {"id": "2007.05911", "submitter": "Fuzhen Zhuang", "authors": "Dongbo Xi, Fuzhen Zhuang, Yongchun Zhu, Pengpeng Zhao, Xiangliang\n  Zhang and Qing He", "title": "Graph Factorization Machines for Cross-Domain Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graph neural networks (GNNs) have been successfully applied to\nrecommender systems. In recommender systems, the user's feedback behavior on an\nitem is usually the result of multiple factors acting at the same time.\nHowever, a long-standing challenge is how to effectively aggregate multi-order\ninteractions in GNN. In this paper, we propose a Graph Factorization Machine\n(GFM) which utilizes the popular Factorization Machine to aggregate multi-order\ninteractions from neighborhood for recommendation. Meanwhile, cross-domain\nrecommendation has emerged as a viable method to solve the data sparsity\nproblem in recommender systems. However, most existing cross-domain\nrecommendation methods might fail when confronting the graph-structured data.\nIn order to tackle the problem, we propose a general cross-domain\nrecommendation framework which can be applied not only to the proposed GFM, but\nalso to other GNN models. We conduct experiments on four pairs of datasets to\ndemonstrate the superior performance of the GFM. Besides, based on general\ncross-domain recommendation experiments, we also demonstrate that our\ncross-domain framework could not only contribute to the cross-domain\nrecommendation task with the GFM, but also be universal and expandable for\nvarious existing GNN models.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 04:56:10 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Xi", "Dongbo", ""], ["Zhuang", "Fuzhen", ""], ["Zhu", "Yongchun", ""], ["Zhao", "Pengpeng", ""], ["Zhang", "Xiangliang", ""], ["He", "Qing", ""]]}, {"id": "2007.05917", "submitter": "Vivek Kumar Singh Ph.D.", "authors": "Prashasti Singh, Vivek Kumar Singh, Parveen Arora, Sujit Bhattacharya", "title": "India's rank and global share in scientific research -- how data sourced\n  from different databases can produce varying outcomes", "comments": "12 pages Submitted to Journal of Scientific Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  India is emerging as a major knowledge producer of the world in terms of\nproportionate share of global research output and the overall research\nproductivity rank. Many recent reports, both of commissioned studies from\nGovernment of India as well as independent international agencies, show India\nat different ranks of global research productivity (variations as large as from\n3rd to 9th place). The paper examines this contradiction; tries to analyse as\nto why different reports places India at different ranks and what may be the\nreasons thereof. The research output data for India, along with the ten most\nproductive countries in the world, is analysed from three major scholarly\ndatabases: Web of Science, Scopus and Dimensions for this purpose. Results show\nthat both, the endogenous factors (such as database coverage variation and\ndifferent subject classification schemes) and the exogenous factors (such as\nsubject selection and publication counting methodology) cause the variations in\ndifferent reports. This paper reports first part of the analysis, focusing\nmainly on variations due to use of data from different databases. The policy\nimplications of the study are also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 06:08:52 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 03:45:33 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Singh", "Prashasti", ""], ["Singh", "Vivek Kumar", ""], ["Arora", "Parveen", ""], ["Bhattacharya", "Sujit", ""]]}, {"id": "2007.05919", "submitter": "Vivek Kumar Singh Ph.D.", "authors": "Vivek Kumar Singh, Parveen Arora, Ashraf Uddin, Sujit Bhattacharya", "title": "India's rank and global share in scientific research -- how publication\n  counting method and subject selection can vary the outcomes", "comments": null, "journal-ref": "Journal of Scientific and Industrial Research 2021", "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the last two decades, India has emerged as a major knowledge producer\nin the world, however different reports put it at different ranks, varying from\n3rd to 9th places. The recent commissioned study reports of Department of\nScience and Technology (DST) done by Elsevier and Clarivate Analytics, rank\nIndia at 5thand 9th places, respectively. On the other hand, an independent\nreport by National Science Foundation (NSF) of United States (US), ranks India\nat 3rd place on research output in Science and Engineering area. Interestingly,\nboth, the Elsevier and the NSF reports use Scopus data, and yet surprisingly\ntheir outcomes are different. This article, therefore, attempts to investigate\nas to how the use of same database can still produce different outcomes, due to\ndifferences in methodological approaches. The publication counting method used\nand the subject selection approach are the two main exogenous factors\nidentified to cause these variations. The implications of the analytical\noutcomes are discussed with special focus on policy perspectives.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 06:13:08 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 03:56:05 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 10:14:28 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Singh", "Vivek Kumar", ""], ["Arora", "Parveen", ""], ["Uddin", "Ashraf", ""], ["Bhattacharya", "Sujit", ""]]}, {"id": "2007.06390", "submitter": "Sherzod Hakimov", "authors": "Golsa Tahmasebzadeh, Sherzod Hakimov, Eric M\\\"uller-Budack, Ralph\n  Ewerth", "title": "A Feature Analysis for Multimodal News Retrieval", "comments": "CLEOPATRA Workshop co-located with ESWC 2020", "journal-ref": "CLEOPATRA Workshop co-located with ESWC 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Content-based information retrieval is based on the information contained in\ndocuments rather than using metadata such as keywords. Most information\nretrieval methods are either based on text or image. In this paper, we\ninvestigate the usefulness of multimodal features for cross-lingual news search\nin various domains: politics, health, environment, sport, and finance. To this\nend, we consider five feature types for image and text and compare the\nperformance of the retrieval system using different combinations. Experimental\nresults show that retrieval results can be improved when considering both\nvisual and textual information. In addition, it is observed that among textual\nfeatures entity overlap outperforms word embeddings, while geolocation\nembeddings achieve better performance among visual features in the retrieval\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 14:09:29 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 08:38:47 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Tahmasebzadeh", "Golsa", ""], ["Hakimov", "Sherzod", ""], ["M\u00fcller-Budack", "Eric", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2007.06434", "submitter": "Qingquan Song", "authors": "Qingquan Song, Dehua Cheng, Hanning Zhou, Jiyan Yang, Yuandong Tian,\n  Xia Hu", "title": "Towards Automated Neural Interaction Discovery for Click-Through Rate\n  Prediction", "comments": null, "journal-ref": null, "doi": "10.1145/3394486.3403137", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-Through Rate (CTR) prediction is one of the most important machine\nlearning tasks in recommender systems, driving personalized experience for\nbillions of consumers. Neural architecture search (NAS), as an emerging field,\nhas demonstrated its capabilities in discovering powerful neural network\narchitectures, which motivates us to explore its potential for CTR predictions.\nDue to 1) diverse unstructured feature interactions, 2) heterogeneous feature\nspace, and 3) high data volume and intrinsic data randomness, it is challenging\nto construct, search, and compare different architectures effectively for\nrecommendation models. To address these challenges, we propose an automated\ninteraction architecture discovering framework for CTR prediction named\nAutoCTR. Via modularizing simple yet representative interactions as virtual\nbuilding blocks and wiring them into a space of direct acyclic graphs, AutoCTR\nperforms evolutionary architecture exploration with learning-to-rank guidance\nat the architecture level and achieves acceleration using low-fidelity model.\nEmpirical analysis demonstrates the effectiveness of AutoCTR on different\ndatasets comparing to human-crafted architectures. The discovered architecture\nalso enjoys generalizability and transferability among different datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 04:33:01 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Song", "Qingquan", ""], ["Cheng", "Dehua", ""], ["Zhou", "Hanning", ""], ["Yang", "Jiyan", ""], ["Tian", "Yuandong", ""], ["Hu", "Xia", ""]]}, {"id": "2007.06616", "submitter": "Mariano Maisonnave", "authors": "Mariano Maisonnave, Fernando Delbianco, Fernando Tohm\\'e, Ana\n  Maguitman", "title": "Assessing the behavior and performance of a supervised term-weighting\n  technique for topic-based retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article analyses and evaluates FDD\\b{eta}, a supervised term-weighting\nscheme that can be applied for query-term selection in topic-based retrieval.\nFDD\\b{eta} weights terms based on two factors representing the descriptive and\ndiscriminating power of the terms with respect to the given topic. It then\ncombines these two factor through the use of an adjustable parameter that\nallows to favor different aspects of retrieval, such as precision, recall or a\nbalance between both. The article makes the following contributions: (1) it\npresents an extensive analysis of the behavior of FDD\\b{eta} as a function of\nits adjustable parameter; (2) it compares FDD\\b{eta} against eighteen\ntraditional and state-of-the-art weighting scheme; (3) it evaluates the\nperformance of disjunctive queries built by combining terms selected using the\nanalyzed methods; (4) it introduces a new public data set with news labeled as\nrelevant or irrelevant to the economic domain. The analysis and evaluations are\nperformed on three data sets: two well-known text data sets, namely 20\nNewsgroups and Reuters-21578, and the newly released data set. It is possible\nto conclude that despite its simplicity, FDD\\b{eta} is competitive with\nstate-of-the-art methods and has the important advantage of offering\nflexibility at the moment of adapting to specific task goals. The results also\ndemonstrate that FDD\\b{eta} offers a useful mechanism to explore different\napproaches to build complex queries.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 18:44:32 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 18:27:30 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Maisonnave", "Mariano", ""], ["Delbianco", "Fernando", ""], ["Tohm\u00e9", "Fernando", ""], ["Maguitman", "Ana", ""]]}, {"id": "2007.06758", "submitter": "Chaoran Huang", "authors": "May Altulyan, Lina Yao, Xianzhi Wang, Chaoran Huang, Salil S Kanhere,\n  Quan Z Sheng", "title": "Recommender Systems for the Internet of Things: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation represents a vital stage in developing and promoting the\nbenefits of the Internet of Things (IoT). Traditional recommender systems fail\nto exploit ever-growing, dynamic, and heterogeneous IoT data. This paper\npresents a comprehensive review of the state-of-the-art recommender systems, as\nwell as related techniques and application in the vibrant field of IoT. We\ndiscuss several limitations of applying recommendation systems to IoT and\npropose a reference framework for comparing existing studies to guide future\nresearch and practices.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 01:24:44 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Altulyan", "May", ""], ["Yao", "Lina", ""], ["Wang", "Xianzhi", ""], ["Huang", "Chaoran", ""], ["Kanhere", "Salil S", ""], ["Sheng", "Quan Z", ""]]}, {"id": "2007.06927", "submitter": "Karlson Pfannschmidt", "authors": "Karlson Pfannschmidt, Eyke H\\\"ullermeier", "title": "Learning Choice Functions via Pareto-Embeddings", "comments": "5 pages, 4 figures, presented at KI 2020, 43. German Conference on\n  Artificial Intelligence, Bamberg, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning to choose from a given set of objects,\nwhere each object is represented by a feature vector. Traditional approaches in\nchoice modelling are mainly based on learning a latent, real-valued utility\nfunction, thereby inducing a linear order on choice alternatives. While this\napproach is suitable for discrete (top-1) choices, it is not straightforward\nhow to use it for subset choices. Instead of mapping choice alternatives to the\nreal number line, we propose to embed them into a higher-dimensional utility\nspace, in which we identify choice sets with Pareto-optimal points. To this\nend, we propose a learning algorithm that minimizes a differentiable loss\nfunction suitable for this task. We demonstrate the feasibility of learning a\nPareto-embedding on a suite of benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 09:34:44 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Pfannschmidt", "Karlson", ""], ["H\u00fcllermeier", "Eyke", ""]]}, {"id": "2007.06954", "submitter": "Yinping Yang Dr", "authors": "Raj Kumar Gupta, Ajay Vishwanath, Yinping Yang", "title": "Global Reactions to COVID-19 on Twitter: A Labelled Dataset with Latent\n  Topic, Sentiment and Emotion Attributes", "comments": "Updated with the complete 2020 data (28 Jan 2020-1 Jan 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a large, labelled dataset on people's responses and\nexpressions related to the COVID-19 pandemic over the Twitter platform. From 28\nJanuary 2020 to 1 Jan 2021, we retrieved over 132 million public Twitter posts\n(i.e., tweets) from more than 20 million unique users using four keywords:\n\"corona\", \"wuhan\", \"nCov\" and \"covid\". Leveraging natural language processing\ntechniques and pre-trained machine learning-based emotion analytic algorithms,\nwe labelled each tweet with seventeen latent semantic attributes, including a)\nten binary attributes indicating the tweet's relevance or irrelevance to the\ntop ten detected topics, b) five quantitative emotion intensity attributes\nindicating the degree of intensity of the valence or sentiment (from extremely\nnegative to extremely positive), and the degree of intensity of fear, of anger,\nof sadness and of joy emotions (from barely noticeable to extremely high\nintensity), and c) two qualitative attributes indicating the sentiment category\nand the dominant emotion category the tweet is mainly expressing. We report the\ndescriptive statistics around the topic, sentiment and emotion attributes, and\ntheir temporal distributions, and discuss the dataset's possible usage in\ncommunication, psychology, public health, economics, and epidemiology research.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 10:30:47 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 11:39:23 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 05:49:29 GMT"}, {"version": "v4", "created": "Fri, 7 Aug 2020 10:39:40 GMT"}, {"version": "v5", "created": "Sat, 5 Sep 2020 04:12:15 GMT"}, {"version": "v6", "created": "Tue, 16 Feb 2021 13:31:40 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Gupta", "Raj Kumar", ""], ["Vishwanath", "Ajay", ""], ["Yang", "Yinping", ""]]}, {"id": "2007.07060", "submitter": "Dharmen Punjani", "authors": "Dharmen Punjani, Markos Iliakis, Theodoros Stefou, Kuldeep Singh,\n  Andreas Both, Manolis Koubarakis, Iosif Angelidis, Konstantina Bereta, Themis\n  Beris, Dimitris Bilidas, Theofilos Ioannidis, Nikolaos Karalis, Christoph\n  Lange, Despina-Athanasia Pantazi, Christos Papaloukas, Georgios Stamoulis", "title": "Template-Based Question Answering over Linked Geospatial Data", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large amounts of geospatial data have been made available recently on the\nlinked open data cloud and the portals of many national cartographic agencies\n(e.g., OpenStreetMap data, administrative geographies of various countries, or\nland cover/land use data sets). These datasets use various geospatial\nvocabularies and can be queried using SPARQL or its OGC-standardized extension\nGeoSPARQL. In this paper, we go beyond these approaches to offer a\nquestion-answering engine for natural language questions on top of linked\ngeospatial data sources. Our system has been implemented as re-usable\ncomponents of the Frankenstein question answering architecture. We give a\ndetailed description of the system's architecture, its underlying algorithms,\nand its evaluation using a set of 201 natural language questions. The set of\nquestions is offered to the research community as a gold standard dataset for\nthe comparative evaluation of future geospatial question answering engines.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 14:35:48 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 15:41:33 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Punjani", "Dharmen", ""], ["Iliakis", "Markos", ""], ["Stefou", "Theodoros", ""], ["Singh", "Kuldeep", ""], ["Both", "Andreas", ""], ["Koubarakis", "Manolis", ""], ["Angelidis", "Iosif", ""], ["Bereta", "Konstantina", ""], ["Beris", "Themis", ""], ["Bilidas", "Dimitris", ""], ["Ioannidis", "Theofilos", ""], ["Karalis", "Nikolaos", ""], ["Lange", "Christoph", ""], ["Pantazi", "Despina-Athanasia", ""], ["Papaloukas", "Christos", ""], ["Stamoulis", "Georgios", ""]]}, {"id": "2007.07073", "submitter": "Sara Scaramuccia", "authors": "Sara Scaramuccia, Simon Nanty, Florent Masseglia", "title": "Feedback Clustering for Online Travel Agencies Searches: a Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding choices performed by online customers is a growing need in the\ntravel industry. In many practical situations, the only available information\nis the flight search query performed by the customer with no additional profile\nknowledge. In general, customer flight bookings are driven by prices, duration,\nnumber of connections, and so on. However, not all customers might assign the\nsame importance to each of those criteria. Here comes the need of grouping\ntogether all flight searches performed by the same kind of customer, that is\nhaving the same booking criteria. The effectiveness of some set of\nrecommendations, for a single cluster, can be measured in terms of the number\nof bookings historically performed. This effectiveness measure plays the role\nof a feedback, that is an external knowledge which can be recombined to\niteratively obtain a final segmentation. In this paper, we describe our Online\nTravel Agencies (OTA) flight search use case and highlight its specific\nfeatures. We address the flight search segmentation problem motivated above by\nproposing a novel algorithm called Split-or-Merge (S/M). This algorithm is a\nvariation of the Split-Merge-Evolve (SME) method. The SME method has already\nbeen introduced in the community as an iterative process updating a clustering\ngiven by the K-means algorithm by splitting and merging clusters subject to\nfeedback independent evaluations. No previous application of the SME method to\nthe real-word data is reported in literature to the best of our knowledge.\nHere, we provide experimental evaluations over real-world data to the SME and\nthe S/M methods. The impact on our domain-specific metrics obtained under the\nSME and the S/M methods suggests that feedback clustering techniques can be\nvery promising in the handling of the domain of OTA flight searches.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 11:19:42 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Scaramuccia", "Sara", ""], ["Nanty", "Simon", ""], ["Masseglia", "Florent", ""]]}, {"id": "2007.07079", "submitter": "Tanner Fiez", "authors": "Tanner Fiez, Nihar B. Shah, Lillian Ratliff", "title": "A SUPER* Algorithm to Optimize Paper Bidding in Peer Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of applications involve sequential arrival of users, and require\nshowing each user an ordering of items. A prime example (which forms the focus\nof this paper) is the bidding process in conference peer review where reviewers\nenter the system sequentially, each reviewer needs to be shown the list of\nsubmitted papers, and the reviewer then \"bids\" to review some papers. The order\nof the papers shown has a significant impact on the bids due to primacy\neffects. In deciding on the ordering of papers to show, there are two competing\ngoals: (i) obtaining sufficiently many bids for each paper, and (ii) satisfying\nreviewers by showing them relevant items. In this paper, we begin by developing\na framework to study this problem in a principled manner. We present an\nalgorithm called SUPER*, inspired by the A* algorithm, for this goal.\nTheoretically, we show a local optimality guarantee of our algorithm and prove\nthat popular baselines are considerably suboptimal. Moreover, under a community\nmodel for the similarities, we prove that SUPER* is near-optimal whereas the\npopular baselines are considerably suboptimal. In experiments on real data from\nICLR 2018 and synthetic data, we find that SUPER* considerably outperforms\nbaselines deployed in existing systems, consistently reducing the number of\npapers with fewer than requisite bids by 50-75% or more, and is also robust to\nvarious real world complexities.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 06:44:49 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 17:47:45 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Fiez", "Tanner", ""], ["Shah", "Nihar B.", ""], ["Ratliff", "Lillian", ""]]}, {"id": "2007.07081", "submitter": "Ilia Kravets", "authors": "Ilia Kravets, Tal Heletz, Hayit Greenspan", "title": "Nodule2vec: a 3D Deep Learning System for Pulmonary Nodule Retrieval\n  Using Semantic Representation", "comments": "to appear at MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based retrieval supports a radiologist decision making process by\npresenting the doctor the most similar cases from the database containing both\nhistorical diagnosis and further disease development history. We present a deep\nlearning system that transforms a 3D image of a pulmonary nodule from a CT scan\ninto a low-dimensional embedding vector. We demonstrate that such a vector\nrepresentation preserves semantic information about the nodule and offers a\nviable approach for content-based image retrieval (CBIR). We discuss the\ntheoretical limitations of the available datasets and overcome them by applying\ntransfer learning of the state-of-the-art lung nodule detection model. We\nevaluate the system using the LIDC-IDRI dataset of thoracic CT scans. We devise\na similarity score and show that it can be utilized to measure similarity 1)\nbetween annotations of the same nodule by different radiologists and 2) between\nthe query nodule and the top four CBIR results. A comparison between doctors\nand algorithm scores suggests that the benefit provided by the system to the\nradiologist end-user is comparable to obtaining a second radiologist's opinion.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 16:26:16 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Kravets", "Ilia", ""], ["Heletz", "Tal", ""], ["Greenspan", "Hayit", ""]]}, {"id": "2007.07082", "submitter": "Vladimir Bernstein", "authors": "Vladimir Bernstein and Andrei Afanassenkov", "title": "Unsupervised Data Extraction from Computer-generated Documents with\n  Single Line Formatting", "comments": "23 pages, 17 figures, 6 tables, 5 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processing large amounts of data is an essential problem of the big data era.\nMost of the data exchange is done via direct communication (using APIs) and\nwell-structured file formats (JSON, XML, EDI, etc.), but a significant portion\nof the data is transferred using arbitrary formatted computer-generated\ndocuments (such as invoices, purchase orders, financial reports, etc.), which\nrequire sophisticated processing and human intervention for data interpretation\nand extraction. The currently available solutions, ranging from manual data\nentry to low-level scripting and data extraction tools, are costly and require\nhuman intervention. This paper describes the principle methodology for\nunsupervised, fully automatic data extraction from a wide range of\ncomputer-generated documents, assuming that their formatting reflects the\noriginal structure of the data sources. The presented methodology falls into\nthe category of unsupervised machine learning and consists of the three main\nparts: (1) - detecting repeating patterns of text formatting by employing the\nrelative feature space clustering and adaptive weighted feature score maps, (2)\n- detecting hierarchical formatting structures via collapsing and noise\nfiltering procedure applied to the repeating formatting patterns and (3) -\nautomatic configuration of the interactive data extraction tool (SiMX\nTextConverter) for fully automated processing.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 22:34:49 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 20:07:49 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Bernstein", "Vladimir", ""], ["Afanassenkov", "Andrei", ""]]}, {"id": "2007.07084", "submitter": "Shihao Li", "authors": "Shihao Li (1), Dekun Yang (1), Bufeng Zhang (1) ((1) Alibaba Inc)", "title": "MRIF: Multi-resolution Interest Fusion for Recommendation", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main task of personalized recommendation is capturing users' interests\nbased on their historical behaviors. Most of recent advances in recommender\nsystems mainly focus on modeling users' preferences accurately using deep\nlearning based approaches. There are two important properties of users'\ninterests, one is that users' interests are dynamic and evolve over time, the\nother is that users' interests have different resolutions, or temporal-ranges\nto be precise, such as long-term and short-term preferences. Existing\napproaches either use Recurrent Neural Networks (RNNs) to address the drifts in\nusers' interests without considering different temporal-ranges, or design two\ndifferent networks to model long-term and short-term preferences separately.\nThis paper presents a multi-resolution interest fusion model (MRIF) that takes\nboth properties of users' interests into consideration. The proposed model is\ncapable to capture the dynamic changes in users' interests at different\ntemporal-ranges, and provides an effective way to combine a group of\nmulti-resolution user interests to make predictions. Experiments show that our\nmethod outperforms state-of-the-art recommendation methods consistently.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 02:32:15 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Li", "Shihao", "", "Alibaba Inc"], ["Yang", "Dekun", "", "Alibaba Inc"], ["Zhang", "Bufeng", "", "Alibaba Inc"]]}, {"id": "2007.07085", "submitter": "Wenhui Yu", "authors": "Wenhui Yu and Xiao Lin and Junfeng Ge and Wenwu Ou and Zheng Qin", "title": "Semi-supervised Collaborative Filtering by Text-enhanced Domain\n  Adaptation", "comments": "KDD 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sparsity is an inherent challenge in the recommender systems, where most\nof the data is collected from the implicit feedbacks of users. This causes two\ndifficulties in designing effective algorithms: first, the majority of users\nonly have a few interactions with the system and there is no enough data for\nlearning; second, there are no negative samples in the implicit feedbacks and\nit is a common practice to perform negative sampling to generate negative\nsamples. However, this leads to a consequence that many potential positive\nsamples are mislabeled as negative ones and data sparsity would exacerbate the\nmislabeling problem. To solve these difficulties, we regard the problem of\nrecommendation on sparse implicit feedbacks as a semi-supervised learning task,\nand explore domain adaption to solve it. We transfer the knowledge learned from\ndense data to sparse data and we focus on the most challenging case -- there is\nno user or item overlap. In this extreme case, aligning embeddings of two\ndatasets directly is rather sub-optimal since the two latent spaces encode very\ndifferent information. As such, we adopt domain-invariant textual features as\nthe anchor points to align the latent spaces. To align the embeddings, we\nextract the textual features for each user and item and feed them into a domain\nclassifier with the embeddings of users and items. The embeddings are trained\nto puzzle the classifier and textual features are fixed as anchor points. By\ndomain adaptation, the distribution pattern in the source domain is transferred\nto the target domain. As the target part can be supervised by domain\nadaptation, we abandon negative sampling in target dataset to avoid label\nnoise. We adopt three pairs of real-world datasets to validate the\neffectiveness of our transfer strategy. Results show that our models outperform\nexisting models significantly.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 05:28:05 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Yu", "Wenhui", ""], ["Lin", "Xiao", ""], ["Ge", "Junfeng", ""], ["Ou", "Wenwu", ""], ["Qin", "Zheng", ""]]}, {"id": "2007.07102", "submitter": "Gerald Onwujekwe", "authors": "Gerald Onwujekwe, Kweku-Muata Osei-Bryson, Nnatubemugo Ngwum", "title": "A Framework for Capturing and Analyzing Unstructured and Semi-structured\n  Data for a Knowledge Management System", "comments": null, "journal-ref": "9th International Conference on Advanced Information Technologies\n  and Applications (ICAITA 2020)", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mainstream knowledge management researchers generally agree that knowledge\nextracted from unstructured data and semi-structured data have become\nimperative for organizational strategic decision making. In this research, we\ndevelop a framework that captures and analyses unstructured data using machine\nlearning techniques and integrates knowledge and insight gained from the data\ninto traditional knowledge management systems. Unlike most frameworks published\nin the literature that focuses on a specific type of unstructured data, our\nframeworks cut across the varieties of unstructured data ranging from textual\ndata from social network sites, online forums, discussion boards, reviews to\naudio data, image data and video data. We highlight some pre-processing and\nprocessing techniques for these data and also highlight some standard output.\nWe evaluate the framework by developing a textual data application programming\ninterface (API) using python and beautiful soup and we perform sentiment\nanalysis on the students review data collected through the API.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 15:21:44 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Onwujekwe", "Gerald", ""], ["Osei-Bryson", "Kweku-Muata", ""], ["Ngwum", "Nnatubemugo", ""]]}, {"id": "2007.07177", "submitter": "Mark Hamilton", "authors": "Mark Hamilton, Stephanie Fu, Mindren Lu, Johnny Bui, Darius Bopp,\n  Zhenbang Chen, Felix Tran, Margaret Wang, Marina Rogers, Lei Zhang, Chris\n  Hoder, William T. Freeman", "title": "MosAIc: Finding Artistic Connections across Culture with Conditional\n  Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce MosAIc, an interactive web app that allows users to find pairs\nof semantically related artworks that span different cultures, media, and\nmillennia. To create this application, we introduce Conditional Image Retrieval\n(CIR) which combines visual similarity search with user supplied filters or\n\"conditions\". This technique allows one to find pairs of similar images that\nspan distinct subsets of the image corpus. We provide a generic way to adapt\nexisting image retrieval data-structures to this new domain and provide\ntheoretical bounds on our approach's efficiency. To quantify the performance of\nCIR systems, we introduce new datasets for evaluating CIR methods and show that\nCIR performs non-parametric style transfer. Finally, we demonstrate that our\nCIR data-structures can identify \"blind spots\" in Generative Adversarial\nNetworks (GAN) where they fail to properly model the true data distribution.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:50:29 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 18:25:23 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 01:08:22 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hamilton", "Mark", ""], ["Fu", "Stephanie", ""], ["Lu", "Mindren", ""], ["Bui", "Johnny", ""], ["Bopp", "Darius", ""], ["Chen", "Zhenbang", ""], ["Tran", "Felix", ""], ["Wang", "Margaret", ""], ["Rogers", "Marina", ""], ["Zhang", "Lei", ""], ["Hoder", "Chris", ""], ["Freeman", "William T.", ""]]}, {"id": "2007.07203", "submitter": "Weihao Gao", "authors": "Weihao Gao, Xiangjun Fan, Chong Wang, Jiankai Sun, Kai Jia, Wenzhi\n  Xiao, Ruofan Ding, Xingyan Bin, Hui Yang, Xiaobing Liu", "title": "Deep Retrieval: Learning A Retrievable Structure for Large-Scale\n  Recommendations", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the core problems in large-scale recommendations is to retrieve top\nrelevant candidates accurately and efficiently, preferably in sub-linear time.\nPrevious approaches are mostly based on a two-step procedure: first learn an\ninner-product model, and then use some approximate nearest neighbor (ANN)\nsearch algorithm to find top candidates. In this paper, we present Deep\nRetrieval (DR), to learn a retrievable structure directly with user-item\ninteraction data (e.g. clicks) without resorting to the Euclidean space\nassumption in ANN algorithms. DR's structure encodes all candidate items into a\ndiscrete latent space. Those latent codes for the candidates are model\nparameters and learnt together with other neural network parameters to maximize\nthe same objective function. With the model learnt, a beam search over the\nstructure is performed to retrieve the top candidates for reranking.\nEmpirically, we first demonstrate that DR, with sub-linear computational\ncomplexity, can achieve almost the same accuracy as the brute-force baseline on\ntwo public datasets. Moreover, we show that, in a live production\nrecommendation system, a deployed DR approach significantly outperforms a\nwell-tuned ANN baseline in terms of engagement metrics. To the best of our\nknowledge, DR is among the first non-ANN algorithms successfully deployed at\nthe scale of hundreds of millions of items for industrial recommendation\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 06:23:51 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 05:45:30 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Gao", "Weihao", ""], ["Fan", "Xiangjun", ""], ["Wang", "Chong", ""], ["Sun", "Jiankai", ""], ["Jia", "Kai", ""], ["Xiao", "Wenzhi", ""], ["Ding", "Ruofan", ""], ["Bin", "Xingyan", ""], ["Yang", "Hui", ""], ["Liu", "Xiaobing", ""]]}, {"id": "2007.07204", "submitter": "Wenhui Yu", "authors": "Wenhui Yu and Zheng Qin", "title": "Sampler Design for Implicit Feedback Data by Noisy-label Robust Learning", "comments": "SIGIR 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit feedback data is extensively explored in recommendation as it is\neasy to collect and generally applicable. However, predicting users' preference\non implicit feedback data is a challenging task since we can only observe\npositive (voted) samples and unvoted samples. It is difficult to distinguish\nbetween the negative samples and unlabeled positive samples from the unvoted\nones. Existing works, such as Bayesian Personalized Ranking (BPR), sample\nunvoted items as negative samples uniformly, therefore suffer from a critical\nnoisy-label issue. To address this gap, we design an adaptive sampler based on\nnoisy-label robust learning for implicit feedback data.\n  To formulate the issue, we first introduce Bayesian Point-wise Optimization\n(BPO) to learn a model, e.g., Matrix Factorization (MF), by maximum likelihood\nestimation. We predict users' preferences with the model and learn it by\nmaximizing likelihood of observed data labels, i.e., a user prefers her\npositive samples and has no interests in her unvoted samples. However, in\nreality, a user may have interests in some of her unvoted samples, which are\nindeed positive samples mislabeled as negative ones. We then consider the risk\nof these noisy labels, and propose a Noisy-label Robust BPO (NBPO). NBPO also\nmaximizes the observation likelihood while connects users' preference and\nobserved labels by the likelihood of label flipping based on the Bayes'\ntheorem. In NBPO, a user prefers her true positive samples and shows no\ninterests in her true negative samples, hence the optimization quality is\ndramatically improved. Extensive experiments on two public real-world datasets\nshow the significant improvement of our proposed optimization methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 05:31:53 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Yu", "Wenhui", ""], ["Qin", "Zheng", ""]]}, {"id": "2007.07217", "submitter": "Longbing Cao", "authors": "Longbing Cao", "title": "Non-IID Recommender Systems: A Review and Framework of Recommendation\n  Paradigm Shifting", "comments": null, "journal-ref": "Engineering, 2: 212-224, 2016", "doi": "10.1016/J.ENG.2016.02.013", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recommendation plays an increasingly critical role in our living,\nstudy, work, and entertainment, the recommendations we receive are often for\nirrelevant, duplicate, or uninteresting products and services. A critical\nreason for such bad recommendations lies in the intrinsic assumption that\nrecommended users and items are independent and identically distributed (IID)\nin existing theories and systems. Another phenomenon is that, while tremendous\nefforts have been made to model specific aspects of users or items, the overall\nuser and item characteristics and their non-IIDness have been overlooked. In\nthis paper, the non-IID nature and characteristics of recommendation are\ndiscussed, followed by the non-IID theoretical framework in order to build a\ndeep and comprehensive understanding of the intrinsic nature of recommendation\nproblems, from the perspective of both couplings and heterogeneity. This\nnon-IID recommendation research triggers the paradigm shift from IID to non-IID\nrecommendation research and can hopefully deliver informed, relevant,\npersonalized, and actionable recommendations. It creates exciting new\ndirections and fundamental solutions to address various complexities including\ncold-start, sparse data-based, cross-domain, group-based, and shilling\nattack-related issues.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 11:24:33 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Cao", "Longbing", ""]]}, {"id": "2007.07224", "submitter": "Ting-Hsiang Wang", "authors": "Ting-Hsiang Wang, Qingquan Song, Xiaotian Han, Zirui Liu, Haifeng Jin,\n  Xia Hu", "title": "AutoRec: An Automated Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic recommender systems are often required to adapt to ever-changing\ndata and tasks or to explore different models systematically. To address the\nneed, we present AutoRec, an open-source automated machine learning (AutoML)\nplatform extended from the TensorFlow ecosystem and, to our knowledge, the\nfirst framework to leverage AutoML for model search and hyperparameter tuning\nin deep recommendation models. AutoRec also supports a highly flexible pipeline\nthat accommodates both sparse and dense inputs, rating prediction and\nclick-through rate (CTR) prediction tasks, and an array of recommendation\nmodels. Lastly, AutoRec provides a simple, user-friendly API. Experiments\nconducted on the benchmark datasets reveal AutoRec is reliable and can identify\nmodels which resemble the best model without prior knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 17:04:53 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Wang", "Ting-Hsiang", ""], ["Song", "Qingquan", ""], ["Han", "Xiaotian", ""], ["Liu", "Zirui", ""], ["Jin", "Haifeng", ""], ["Hu", "Xia", ""]]}, {"id": "2007.07229", "submitter": "Narjes Nikzad-Khasmakhi", "authors": "N. Nikzad-Khasmakhi, M. A. Balafar, M.Reza Feizi-Derakhshi, Cina\n  Motamed", "title": "BERTERS: Multimodal Representation Learning for Expert Recommendation\n  System with Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of an expert recommendation system is to trace a set of\ncandidates' expertise and preferences, recognize their expertise patterns, and\nidentify experts. In this paper, we introduce a multimodal classification\napproach for expert recommendation system (BERTERS). In our proposed system,\nthe modalities are derived from text (articles published by candidates) and\ngraph (their co-author connections) information. BERTERS converts text into a\nvector using the Bidirectional Encoder Representations from Transformer (BERT).\nAlso, a graph Representation technique called ExEm is used to extract the\nfeatures of candidates from the co-author network. Final representation of a\ncandidate is the concatenation of these vectors and other features. Eventually,\na classifier is built on the concatenation of features. This multimodal\napproach can be used in both the academic community and the community question\nanswering. To verify the effectiveness of BERTERS, we analyze its performance\non multi-label classification and visualization tasks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 12:30:16 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Nikzad-Khasmakhi", "N.", ""], ["Balafar", "M. A.", ""], ["Feizi-Derakhshi", "M. Reza", ""], ["Motamed", "Cina", ""]]}, {"id": "2007.07269", "submitter": "Joel R. Bock", "authors": "Joel R. Bock and Akhilesh Maewal", "title": "Adversarial learning for product recommendation", "comments": null, "journal-ref": "AI 2020, 1(3), 376-388", "doi": "10.3390/ai1030025", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Product recommendation can be considered as a problem in data fusion--\nestimation of the joint distribution between individuals, their behaviors, and\ngoods or services of interest. This work proposes a conditional, coupled\ngenerative adversarial network (RecommenderGAN) that learns to produce samples\nfrom a joint distribution between (view, buy) behaviors found in extremely\nsparse implicit feedback training data. User interaction is represented by two\nmatrices having binary-valued elements. In each matrix, nonzero values indicate\nwhether a user viewed or bought a specific item in a given product category,\nrespectively. By encoding actions in this manner, the model is able to\nrepresent entire, large scale product catalogs. Conversion rate statistics\ncomputed on trained GAN output samples ranged from 1.323 to 1.763 percent.\nThese statistics are found to be significant in comparison to null hypothesis\ntesting results. The results are shown comparable to published conversion rates\naggregated across many industries and product types. Our results are\npreliminary, however they suggest that the recommendations produced by the\nmodel may provide utility for consumers and digital retailers.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 23:35:36 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 15:01:29 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Bock", "Joel R.", ""], ["Maewal", "Akhilesh", ""]]}, {"id": "2007.07388", "submitter": "Kai Shu", "authors": "Kai Shu, Amrita Bhattacharjee, Faisal Alatawi, Tahora Nazer, Kaize\n  Ding, Mansooreh Karami, and Huan Liu", "title": "Combating Disinformation in a Social Media Age", "comments": "WIREs Data Mining and Knowledge Discovery", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The creation, dissemination, and consumption of disinformation and fabricated\ncontent on social media is a growing concern, especially with the ease of\naccess to such sources, and the lack of awareness of the existence of such\nfalse information. In this paper, we present an overview of the techniques\nexplored to date for the combating of disinformation with various forms. We\nintroduce different forms of disinformation, discuss factors related to the\nspread of disinformation, elaborate on the inherent challenges in detecting\ndisinformation, and show some approaches to mitigating disinformation via\neducation, research, and collaboration. Looking ahead, we present some\npromising future research directions on disinformation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 22:38:48 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Shu", "Kai", ""], ["Bhattacharjee", "Amrita", ""], ["Alatawi", "Faisal", ""], ["Nazer", "Tahora", ""], ["Ding", "Kaize", ""], ["Karami", "Mansooreh", ""], ["Liu", "Huan", ""]]}, {"id": "2007.07486", "submitter": "Stefan Langer", "authors": "Stefan Langer, Liza Obermeier, Andr\\'e Ebert, Markus Friedrich, Emma\n  Munisamy, Claudia Linnhoff-Popien", "title": "Content-based Recommendations for Radio Stations with Deep Learned Audio\n  Fingerprints", "comments": "Informatik 2020, LectureNotes in Informatics(LNI),Gesellschaft f\\\"ur\n  Informatik, Bonn2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world of linear radio broadcasting is characterized by a wide variety of\nstations and played content. That is why finding stations playing the preferred\ncontent is a tough task for a potential listener, especially due to the\noverwhelming number of offered choices. Here, recommender systems usually step\nin but existing content-based approaches rely on metadata and thus are\nconstrained by the available data quality. Other approaches leverage user\nbehavior data and thus do not exploit any domain-specific knowledge and are\nfurthermore disadvantageous regarding privacy concerns. Therefore, we propose a\nnew pipeline for the generation of audio-based radio station fingerprints\nrelying on audio stream crawling and a Deep Autoencoder. We show that the\nproposed fingerprints are especially useful for characterizing radio stations\nby their audio content and thus are an excellent representation for meaningful\nand reliable radio station recommendations. Furthermore, the proposed modules\nare part of the HRADIO Communication Platform, which enables hybrid radio\nfeatures to radio stations. It is released with a flexible open source license\nand enables especially small- and medium-sized businesses, to provide\ncustomized and high quality radio services to potential listeners.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 05:15:30 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Langer", "Stefan", ""], ["Obermeier", "Liza", ""], ["Ebert", "Andr\u00e9", ""], ["Friedrich", "Markus", ""], ["Munisamy", "Emma", ""], ["Linnhoff-Popien", "Claudia", ""]]}, {"id": "2007.07639", "submitter": "Simone Santini", "authors": "Emilio Aced Fuentes and Simone Santini", "title": "Network navigation using Page Rank random walks", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.IR cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a formalism based on a continuous time approximation, to study\nthe characteristics of Page Rank random walks. We find that the diffusion of\nthe occupancy probability has a dynamics that exponentially \"forgets\" the\ninitial conditions and settles to a steady state that depends only on the\ncharacteristics of the network. In the special case in which the walk begins\nfrom a single node, we find that the largest eigenvalue of the transition value\n(lambda=1) does not contribute to the dynamic and that the probability is\nconstant in the direction of the corresponding eigenvector. We study the\nprocess of visiting new node, which we find to have a dynamic similar to that\nof the occupancy probability. Finally, we determine the average transit time\nbetween nodes <T>, which we find to exhibit certain connection with the\ncorresponding time for Levy walks. The relevance of these results reside in\nthat Page Rank, which are a more reasonable model for the searching behavior of\nindividuals, can be shown to exhibit features similar to Levy walks, which in\nturn have been shown to be a reasonable model of a common large scale search\nstrategy known as \"Area Restricted Search\".\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 11:54:01 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 16:58:35 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Fuentes", "Emilio Aced", ""], ["Santini", "Simone", ""]]}, {"id": "2007.07753", "submitter": "Peter Hillmann", "authors": "Sandro Passarelli, Cem G\\\"undogan, Lars Stiemert, Matthias Schopp,\n  Peter Hillmann", "title": "NERD: Neural Network for Edict of Risky Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.DC cs.IR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber incidents can have a wide range of cause from a simple connection loss\nto an insistent attack. Once a potential cyber security incidents and system\nfailures have been identified, deciding how to proceed is often complex.\nEspecially, if the real cause is not directly in detail determinable.\nTherefore, we developed the concept of a Cyber Incident Handling Support\nSystem. The developed system is enriched with information by multiple sources\nsuch as intrusion detection systems and monitoring tools. It uses over twenty\nkey attributes like sync-package ratio to identify potential security incidents\nand to classify the data into different priority categories. Afterwards, the\nsystem uses artificial intelligence to support the further decision-making\nprocess and to generate corresponding reports to brief the Board of Directors.\nOriginating from this information, appropriate and detailed suggestions are\nmade regarding the causes and troubleshooting measures. Feedback from users\nregarding the problem solutions are included into future decision-making by\nusing labelled flow data as input for the learning process. The prototype shows\nthat the decision making can be sustainably improved and the Cyber Incident\nHandling process becomes much more effective.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 14:24:48 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Passarelli", "Sandro", ""], ["G\u00fcndogan", "Cem", ""], ["Stiemert", "Lars", ""], ["Schopp", "Matthias", ""], ["Hillmann", "Peter", ""]]}, {"id": "2007.07846", "submitter": "Jimmy Lin", "authors": "Edwin Zhang, Nikhil Gupta, Raphael Tang, Xiao Han, Ronak Pradeep,\n  Kuang Lu, Yue Zhang, Rodrigo Nogueira, Kyunghyun Cho, Hui Fang, Jimmy Lin", "title": "Covidex: Neural Ranking Models and Keyword Search Infrastructure for the\n  COVID-19 Open Research Dataset", "comments": "arXiv admin note: text overlap with arXiv:2004.05125", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Covidex, a search engine that exploits the latest neural ranking\nmodels to provide information access to the COVID-19 Open Research Dataset\ncurated by the Allen Institute for AI. Our system has been online and serving\nusers since late March 2020. The Covidex is the user application component of\nour three-pronged strategy to develop technologies for helping domain experts\ntackle the ongoing global pandemic. In addition, we provide robust and\neasy-to-use keyword search infrastructure that exploits mature fusion-based\nmethods as well as standalone neural ranking models that can be incorporated\ninto other applications. These techniques have been evaluated in the ongoing\nTREC-COVID challenge: Our infrastructure and baselines have been adopted by\nmany participants, including some of the highest-scoring runs in rounds 1, 2,\nand 3. In round 3, we report the highest-scoring run that takes advantage of\nprevious training data and the second-highest fully automatic run.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:26:01 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Zhang", "Edwin", ""], ["Gupta", "Nikhil", ""], ["Tang", "Raphael", ""], ["Han", "Xiao", ""], ["Pradeep", "Ronak", ""], ["Lu", "Kuang", ""], ["Zhang", "Yue", ""], ["Nogueira", "Rodrigo", ""], ["Cho", "Kyunghyun", ""], ["Fang", "Hui", ""], ["Lin", "Jimmy", ""]]}, {"id": "2007.07987", "submitter": "Xiao Wang", "authors": "Xiao Wang, Craig Macdonald, Iadh Ounis", "title": "Deep Reinforced Query Reformulation for Information Retrieval", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Query reformulations have long been a key mechanism to alleviate the\nvocabulary-mismatch problem in information retrieval, for example by expanding\nthe queries with related query terms or by generating paraphrases of the\nqueries. In this work, we propose a deep reinforced query reformulation (DRQR)\nmodel to automatically generate new reformulations of the query. To encourage\nthe model to generate queries which can achieve high performance when\nperforming the retrieval task, we incorporate query performance prediction into\nour reward function. In addition, to evaluate the quality of the reformulated\nquery in the context of information retrieval, we first train our DRQR model,\nthen apply the retrieval ranking model on the obtained reformulated query.\nExperiments are conducted on the TREC 2020 Deep Learning track MSMARCO document\nranking dataset. Our results show that our proposed model outperforms several\nquery reformulation model baselines when performing retrieval task. In\naddition, improvements are also observed when combining with various retrieval\nmodels, such as query expansion and BERT.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 20:40:54 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Wang", "Xiao", ""], ["Macdonald", "Craig", ""], ["Ounis", "Iadh", ""]]}, {"id": "2007.07996", "submitter": "Preslav Nakov", "authors": "Firoj Alam, Fahim Dalvi, Shaden Shaar, Nadir Durrani, Hamdy Mubarak,\n  Alex Nikolov, Giovanni Da San Martino, Ahmed Abdelali, Hassan Sajjad, Kareem\n  Darwish, Preslav Nakov", "title": "Fighting the COVID-19 Infodemic in Social Media: A Holistic Perspective\n  and a Call to Arms", "comments": "COVID-19, Infodemic, Disinformation, Misinformation, Fake News, Call\n  to Arms, Crowdsourcing Annotations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the outbreak of the COVID-19 pandemic, people turned to social media to\nread and to share timely information including statistics, warnings, advice,\nand inspirational stories. Unfortunately, alongside all this useful\ninformation, there was also a new blending of medical and political\nmisinformation and disinformation, which gave rise to the first global\ninfodemic. While fighting this infodemic is typically thought of in terms of\nfactuality, the problem is much broader as malicious content includes not only\nfake news, rumors, and conspiracy theories, but also promotion of fake cures,\npanic, racism, xenophobia, and mistrust in the authorities, among others. This\nis a complex problem that needs a holistic approach combining the perspectives\nof journalists, fact-checkers, policymakers, government entities, social media\nplatforms, and society as a whole. Taking them into account we define an\nannotation schema and detailed annotation instructions, which reflect these\nperspectives. We performed initial annotations using this schema, and our\ninitial experiments demonstrated sizable improvements over the baselines. Now,\nwe issue a call to arms to the research community and beyond to join the fight\nby supporting our crowdsourcing annotation efforts.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 21:18:30 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 08:52:10 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Alam", "Firoj", ""], ["Dalvi", "Fahim", ""], ["Shaar", "Shaden", ""], ["Durrani", "Nadir", ""], ["Mubarak", "Hamdy", ""], ["Nikolov", "Alex", ""], ["Martino", "Giovanni Da San", ""], ["Abdelali", "Ahmed", ""], ["Sajjad", "Hassan", ""], ["Darwish", "Kareem", ""], ["Nakov", "Preslav", ""]]}, {"id": "2007.07997", "submitter": "Preslav Nakov", "authors": "Alberto Barron-Cedeno, Tamer Elsayed, Preslav Nakov, Giovanni Da San\n  Martino, Maram Hasanain, Reem Suwaileh, Fatima Haouari, Nikolay Babulkov,\n  Bayan Hamdan, Alex Nikolov, Shaden Shaar, and Zien Sheikh Ali", "title": "Overview of CheckThat! 2020: Automatic Identification and Verification\n  of Claims in Social Media", "comments": "Check-Worthiness Estimation, Fact-Checking, Veracity, Evidence-based\n  Verification, Detecting Previously Fact-Checked Claims, Social Media\n  Verification, Computational Journalism, COVID-19", "journal-ref": "CLEF-2020", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an overview of the third edition of the CheckThat! Lab at CLEF\n2020. The lab featured five tasks in two different languages: English and\nArabic. The first four tasks compose the full pipeline of claim verification in\nsocial media: Task 1 on check-worthiness estimation, Task 2 on retrieving\npreviously fact-checked claims, Task 3 on evidence retrieval, and Task 4 on\nclaim verification. The lab is completed with Task 5 on check-worthiness\nestimation in political debates and speeches. A total of 67 teams registered to\nparticipate in the lab (up from 47 at CLEF 2019), and 23 of them actually\nsubmitted runs (compared to 14 at CLEF 2019). Most teams used deep neural\nnetworks based on BERT, LSTMs, or CNNs, and achieved sizable improvements over\nthe baselines on all tasks. Here we describe the tasks setup, the evaluation\nresults, and a summary of the approaches used by the participants, and we\ndiscuss some lessons learned. Last but not least, we release to the research\ncommunity all datasets from the lab as well as the evaluation scripts, which\nshould enable further research in the important tasks of check-worthiness\nestimation and automatic claim verification.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 21:19:32 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Barron-Cedeno", "Alberto", ""], ["Elsayed", "Tamer", ""], ["Nakov", "Preslav", ""], ["Martino", "Giovanni Da San", ""], ["Hasanain", "Maram", ""], ["Suwaileh", "Reem", ""], ["Haouari", "Fatima", ""], ["Babulkov", "Nikolay", ""], ["Hamdan", "Bayan", ""], ["Nikolov", "Alex", ""], ["Shaar", "Shaden", ""], ["Ali", "Zien Sheikh", ""]]}, {"id": "2007.08024", "submitter": "Preslav Nakov", "authors": "Giovanni Da San Martino, Stefano Cresci, Alberto Barron-Cedeno,\n  Seunghak Yu, Roberto Di Pietro, Preslav Nakov", "title": "A Survey on Computational Propaganda Detection", "comments": "propaganda detection, disinformation, misinformation, fake news,\n  media bias", "journal-ref": "IJCAI-2020", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propaganda campaigns aim at influencing people's mindset with the purpose of\nadvancing a specific agenda. They exploit the anonymity of the Internet, the\nmicro-profiling ability of social networks, and the ease of automatically\ncreating and managing coordinated networks of accounts, to reach millions of\nsocial network users with persuasive messages, specifically targeted to topics\neach individual user is sensitive to, and ultimately influencing the outcome on\na targeted issue. In this survey, we review the state of the art on\ncomputational propaganda detection from the perspective of Natural Language\nProcessing and Network Analysis, arguing about the need for combined efforts\nbetween these communities. We further discuss current challenges and future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 22:25:51 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Martino", "Giovanni Da San", ""], ["Cresci", "Stefano", ""], ["Barron-Cedeno", "Alberto", ""], ["Yu", "Seunghak", ""], ["Di Pietro", "Roberto", ""], ["Nakov", "Preslav", ""]]}, {"id": "2007.08304", "submitter": "Xu Chen", "authors": "Chenyang Li and Xu Chen and Ya Zhang and Siheng Chen and Dan Lv and\n  Yanfeng Wang", "title": "Dual Graph Embedding for Object-Tag LinkPrediction on the Knowledge\n  Graph", "comments": "8 pages. It has been accepted by IEEE International Conference on\n  Knowledge Graphs (ICKG) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KGs) composed of users, objects, and tags are widely used\nin web applications ranging from E-commerce, social media sites to news\nportals. This paper concentrates on an attractive application which aims to\npredict the object-tag links in the KG for better tag recommendation and object\nexplanation. When predicting the object-tag links, both the first-order and\nhigh-order proximities between entities in the KG propagate essential\nsimilarity information for better prediction. Most existing methods focus on\npreserving the first-order proximity between entities in the KG. However, they\ncannot capture the high-order proximities in an explicit way, and the adopted\nmargin-based criterion cannot measure the first-order proximity on the global\nstructure accurately. In this paper, we propose a novel approach named Dual\nGraph Embedding (DGE) that models both the first-order and high-order\nproximities in the KG via an auto-encoding architecture to facilitate better\nobject-tag relation inference. Here the dual graphs contain an object graph and\na tag graph that explicitly depict the high-order object-object and tag-tag\nproximities in the KG. The dual graph encoder in DGE then encodes these\nhigh-order proximities in the dual graphs into entity embeddings. The decoder\nformulates a skip-gram objective that maximizes the first-order proximity\nbetween observed object-tag pairs over the global proximity structure. With the\nsupervision of the decoder, the embeddings derived by the encoder will be\nrefined to capture both the first-order and high-order proximities in the KG\nfor better link prediction. Extensive experiments on three real-world datasets\ndemonstrate that DGE outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 12:46:32 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Li", "Chenyang", ""], ["Chen", "Xu", ""], ["Zhang", "Ya", ""], ["Chen", "Siheng", ""], ["Lv", "Dan", ""], ["Wang", "Yanfeng", ""]]}, {"id": "2007.08308", "submitter": "Xu Chen", "authors": "Jingchao Su and Xu Chen and Ya Zhang and Siheng Chen and Dan Lv and\n  Chenyang Li", "title": "Collaborative Adversarial Learning for RelationalLearning on Multiple\n  Bipartite Graphs", "comments": "8 pages. It has been accepted by IEEE International Conference on\n  Knowledge Graphs (ICKG) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational learning aims to make relation inference by exploiting the\ncorrelations among different types of entities. Exploring relational learning\non multiple bipartite graphs has been receiving attention because of its\npopular applications such as recommendations. How to make efficient relation\ninference with few observed links is the main problem on multiple bipartite\ngraphs. Most existing approaches attempt to solve the sparsity problem via\nlearning shared representations to integrate knowledge from multi-source data\nfor shared entities. However, they merely model the correlations from one\naspect (e.g. distribution, representation), and cannot impose sufficient\nconstraints on different relations of the shared entities. One effective way of\nmodeling the multi-domain data is to learn the joint distribution of the shared\nentities across domains.In this paper, we propose Collaborative Adversarial\nLearning (CAL) that explicitly models the joint distribution of the shared\nentities across multiple bipartite graphs. The objective of CAL is formulated\nfrom a variational lower bound that maximizes the joint log-likelihoods of the\nobservations. In particular, CAL consists of distribution-level and\nfeature-level alignments for knowledge from multiple bipartite graphs. The\ntwo-level alignment acts as two different constraints on different relations of\nthe shared entities and facilitates better knowledge transfer for relational\nlearning on multiple bipartite graphs. Extensive experiments on two real-world\ndatasets have shown that the proposed model outperforms the existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 12:55:06 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Su", "Jingchao", ""], ["Chen", "Xu", ""], ["Zhang", "Ya", ""], ["Chen", "Siheng", ""], ["Lv", "Dan", ""], ["Li", "Chenyang", ""]]}, {"id": "2007.08557", "submitter": "Lili Mou", "authors": "Jingjing Li, Zichao Li, Lili Mou, Xin Jiang, Michael R. Lyu, Irwin\n  King", "title": "Unsupervised Text Generation by Learning from Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present TGLS, a novel framework to unsupervised Text\nGeneration by Learning from Search. We start by applying a strong search\nalgorithm (in particular, simulated annealing) towards a heuristically defined\nobjective that (roughly) estimates the quality of sentences. Then, a\nconditional generative model learns from the search results, and meanwhile\nsmooth out the noise of search. The alternation between search and learning can\nbe repeated for performance bootstrapping. We demonstrate the effectiveness of\nTGLS on two real-world natural language generation tasks, paraphrase generation\nand text formalization. Our model significantly outperforms unsupervised\nbaseline methods in both tasks. Especially, it achieves comparable performance\nwith the state-of-the-art supervised methods in paraphrase generation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 04:34:48 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Li", "Jingjing", ""], ["Li", "Zichao", ""], ["Mou", "Lili", ""], ["Jiang", "Xin", ""], ["Lyu", "Michael R.", ""], ["King", "Irwin", ""]]}, {"id": "2007.08617", "submitter": "Chris Thomas", "authors": "Christopher Thomas and Adriana Kovashka", "title": "Preserving Semantic Neighborhoods for Robust Cross-modal Retrieval", "comments": null, "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of multimodal data (e.g. social media posts) has inspired\ninterest in cross-modal retrieval methods. Popular approaches rely on a variety\nof metric learning losses, which prescribe what the proximity of image and text\nshould be, in the learned space. However, most prior methods have focused on\nthe case where image and text convey redundant information; in contrast,\nreal-world image-text pairs convey complementary information with little\noverlap. Further, images in news articles and media portray topics in a\nvisually diverse fashion; thus, we need to take special care to ensure a\nmeaningful image representation. We propose novel within-modality losses which\nencourage semantic coherency in both the text and image subspaces, which does\nnot necessarily align with visual coherency. Our method ensures that not only\nare paired images and texts close, but the expected image-image and text-text\nrelationships are also observed. Our approach improves the results of\ncross-modal retrieval on four datasets compared to five baselines.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 20:32:54 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Thomas", "Christopher", ""], ["Kovashka", "Adriana", ""]]}, {"id": "2007.08628", "submitter": "Yang Feng", "authors": "Yang Feng, Yubao Liu, Jiebo Luo", "title": "Universal Model for Multi-Domain Medical Image Retrieval", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.03701", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical Image Retrieval (MIR) helps doctors quickly find similar patients'\ndata, which can considerably aid the diagnosis process. MIR is becoming\nincreasingly helpful due to the wide use of digital imaging modalities and the\ngrowth of the medical image repositories. However, the popularity of various\ndigital imaging modalities in hospitals also poses several challenges to MIR.\nUsually, one image retrieval model is only trained to handle images from one\nmodality or one source. When there are needs to retrieve medical images from\nseveral sources or domains, multiple retrieval models need to be maintained,\nwhich is cost ineffective. In this paper, we study an important but unexplored\ntask: how to train one MIR model that is applicable to medical images from\nmultiple domains? Simply fusing the training data from multiple domains cannot\nsolve this problem because some domains become over-fit sooner when trained\ntogether using existing methods. Therefore, we propose to distill the knowledge\nin multiple specialist MIR models into a single multi-domain MIR model via\nuniversal embedding to solve this problem. Using skin disease, x-ray, and\nretina image datasets, we validate that our proposed universal model can\neffectively accomplish multi-domain MIR.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 23:22:04 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Feng", "Yang", ""], ["Liu", "Yubao", ""], ["Luo", "Jiebo", ""]]}, {"id": "2007.08709", "submitter": "Bodo Billerbeck", "authors": "Bodo Billerbeck, Justin Zobel, Nicholas Lester, Nick Craswell", "title": "Scalable Methods for Calculating Term Co-Occurrence Frequencies", "comments": "5 pages, 1 table, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search techniques make use of elementary information such as term frequencies\nand document lengths in computation of similarity weighting. They can also\nexploit richer statistics, in particular the number of documents in which any\ntwo terms co-occur. In this paper we propose alternative methods for computing\nthis statistic, a challenging task because the number of distinct pairs of\nterms is vast -- around 100,000 in a typical 1000-word news article, for\nexample. In contrast, we do not employ approximation algorithms, as we want to\nbe able to find exact counts. We explore their efficiency, finding that a\nna\\\"ive approach based on a dictionary is indeed very slow, while methods based\non a combination of inverted indexes and linear scanning provide both massive\nspeed-ups and better observed asymptotic behaviour. Our careful implementation\nshows that, with our novel list-pairs approach it is possible to process over\nseveral hundred thousand documents per hour.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 01:21:40 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Billerbeck", "Bodo", ""], ["Zobel", "Justin", ""], ["Lester", "Nicholas", ""], ["Craswell", "Nick", ""]]}, {"id": "2007.08710", "submitter": "Alireza Tabebordbar", "authors": "Alireza Tabebordbar", "title": "Augmented Understanding and Automated Adaptation of Curation Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past years, there has been many efforts to curate and increase the\nadded value of the raw data. Data curation has been defined as activities and\nprocesses an analyst undertakes to transform the raw data into contextualized\ndata and knowledge. Data curation enables decision-makers and data analyst to\nextract value and derive insight from the raw data. However, to curate the raw\ndata, an analyst needs to carry out various curation tasks including,\nextraction linking, classification, and indexing, which are error-prone,\ntedious and challenging. Besides, deriving insight require analysts to spend a\nlong period of time to scan and analyze the curation environments. This problem\nis exacerbated when the curation environment is large, and the analyst needs to\ncurate a varied and comprehensive list of data. To address these challenges, in\nthis dissertation, we present techniques, algorithms and systems for augmenting\nanalysts in curation tasks. We propose: ~(1) a feature-based and automated\ntechnique for curating the raw data. ~(2) We propose an autonomic approach for\nadapting data curation rules. ~(3) We provide a solution to augment users in\nformulating their preferences while curating data in large scale information\nspaces. ~(4) We implement a set of APIs for automating the basic curation\ntasks, including Named Entity extraction, POS tags, classification, and etc.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 01:27:12 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Tabebordbar", "Alireza", ""]]}, {"id": "2007.08725", "submitter": "Shilong Wang", "authors": "Shilong Wang (1), Hang Liu (2), Anil Gaihre (2), Hengyong Yu (1) ((1)\n  University of Massachusetts Lowell, (2) Stevens Institute of Technology)", "title": "EZLDA: Efficient and Scalable LDA on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LDA is a statistical approach for topic modeling with a wide range of\napplications. However, there exist very few attempts to accelerate LDA on GPUs\nwhich come with exceptional computing and memory throughput capabilities. To\nthis end, we introduce EZLDA which achieves efficient and scalable LDA training\non GPUs with the following three contributions: First, EZLDA introduces\nthree-branch sampling method which takes advantage of the convergence\nheterogeneity of various tokens to reduce the redundant sampling task. Second,\nto enable sparsity-aware format for both D and W on GPUs with fast sampling and\nupdating, we introduce hybrid format for W along with corresponding token\npartition to T and inverted index designs. Third, we design a hierarchical\nworkload balancing solution to address the extremely skewed workload imbalance\nproblem on GPU and scaleEZLDA across multiple GPUs. Taken together, EZLDA\nachieves superior performance over the state-of-the-art attempts with lower\nmemory consumption.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 02:40:03 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Wang", "Shilong", ""], ["Liu", "Hang", ""], ["Gaihre", "Anil", ""], ["Yu", "Hengyong", ""]]}, {"id": "2007.08954", "submitter": "Jinming Zhao Ms", "authors": "Jinming Zhao, Ming Liu, Longxiang Gao, Yuan Jin, Lan Du, He Zhao, He\n  Zhang and Gholamreza Haffari", "title": "SummPip: Unsupervised Multi-Document Summarization with Sentence Graph\n  Compression", "comments": "accepted to SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401327", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining training data for multi-document summarization (MDS) is time\nconsuming and resource-intensive, so recent neural models can only be trained\nfor limited domains. In this paper, we propose SummPip: an unsupervised method\nfor multi-document summarization, in which we convert the original documents to\na sentence graph, taking both linguistic and deep representation into account,\nthen apply spectral clustering to obtain multiple clusters of sentences, and\nfinally compress each cluster to generate the final summary. Experiments on\nMulti-News and DUC-2004 datasets show that our method is competitive to\nprevious unsupervised methods and is even comparable to the neural supervised\napproaches. In addition, human evaluation shows our system produces consistent\nand complete summaries compared to human written ones.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 13:01:15 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 10:20:12 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhao", "Jinming", ""], ["Liu", "Ming", ""], ["Gao", "Longxiang", ""], ["Jin", "Yuan", ""], ["Du", "Lan", ""], ["Zhao", "He", ""], ["Zhang", "He", ""], ["Haffari", "Gholamreza", ""]]}, {"id": "2007.09060", "submitter": "Prateek Verma", "authors": "Camille Noufi and Prateek Verma, Jonathan Berger", "title": "Self-Supervised Representation Learning for Vocal Music Context", "comments": "Working on more updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In music and speech, meaning is derived at multiple levels of context.\nAffect, for example, can be inferred both by a short sound token and by sonic\npatterns over a longer temporal window such as an entire recording. In this\npaper we focus on inferring meaning from this dichotomy of contexts. We show\nhow contextual representations of short sung vocal lines can be implicitly\nlearned from fundamental frequency ($F_0$) and thus be used as a meaningful\nfeature space for downstream Music Information Retrieval (MIR) tasks. We\npropose three self-supervised deep learning paradigms which leverage pseudotask\nlearning of these two levels of context to produce latent representation\nspaces. We evaluate the usefulness of these representations by embedding unseen\nvocal contours into each space and conducting downstream classification tasks.\nOur results show that contextual representation can enhance downstream\nclassification by as much as 15 % as compared to using traditional statistical\ncontour features.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 15:41:00 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 02:10:05 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 17:46:54 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Noufi", "Camille", ""], ["Verma", "Prateek", ""], ["Berger", "Jonathan", ""]]}, {"id": "2007.09186", "submitter": "Kristjan Arumae", "authors": "Parminder Bhatia, Lan Liu, Kristjan Arumae, Nima Pourdamghani, Suyog\n  Deshpande, Ben Snively, Mona Mona, Colby Wise, George Price, Shyam Ramaswamy,\n  Xiaofei Ma, Ramesh Nallapati, Zhiheng Huang, Bing Xiang, Taha Kass-Hout", "title": "AWS CORD-19 Search: A Neural Search Engine for COVID-19 Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease (COVID-19) has been declared as a pandemic by WHO with\nthousands of cases being reported each day. Numerous scientific articles are\nbeing published on the disease raising the need for a service which can\norganize, and query them in a reliable fashion. To support this cause we\npresent AWS CORD-19 Search (ACS), a public, COVID-19 specific, neural search\nengine that is powered by several machine learning systems to support natural\nlanguage based searches. ACS with capabilities such as document ranking,\npassage ranking, question answering and topic classification provides a\nscalable solution to COVID-19 researchers and policy makers in their search and\ndiscovery for answers to high priority scientific questions. We present a\nquantitative evaluation and qualitative analysis of the system against other\nleading COVID-19 search platforms. ACS is top performing across these systems\nyielding quality results which we detail with relevant examples in this work.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 18:41:29 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 01:38:58 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 05:59:53 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Bhatia", "Parminder", ""], ["Liu", "Lan", ""], ["Arumae", "Kristjan", ""], ["Pourdamghani", "Nima", ""], ["Deshpande", "Suyog", ""], ["Snively", "Ben", ""], ["Mona", "Mona", ""], ["Wise", "Colby", ""], ["Price", "George", ""], ["Ramaswamy", "Shyam", ""], ["Ma", "Xiaofei", ""], ["Nallapati", "Ramesh", ""], ["Huang", "Zhiheng", ""], ["Xiang", "Bing", ""], ["Kass-Hout", "Taha", ""]]}, {"id": "2007.09368", "submitter": "Saptarshi Ghosh Dr.", "authors": "Ritam Dutt, Moumita Basu, Kripabandhu Ghosh, Saptarshi Ghosh", "title": "Utilizing Microblogs for Assisting Post-Disaster Relief Operations via\n  Matching Resource Needs and Availabilities", "comments": null, "journal-ref": "Information Processing and Management, Elsevier, vol. 56, issue 5,\n  pages 1680--1697, September 2019", "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During a disaster event, two types of information that are especially useful\nfor coordinating relief operations are needs and availabilities of resources\n(e.g., food, water, medicines) in the affected region. Information posted on\nmicroblogging sites is increasingly being used for assisting post-disaster\nrelief operations. In this context, two practical challenges are (i)~to\nidentify tweets that inform about resource needs and availabilities (termed as\nneed-tweets and availability-tweets respectively), and (ii)~to automatically\nmatch needs with appropriate availabilities. While several works have addressed\nthe first problem, there has been little work on automatically matching needs\nwith availabilities. The few prior works that attempted matching only\nconsidered the resources, and no attempt has been made to understand other\naspects of needs/availabilities that are essential for matching in practice. In\nthis work, we develop a methodology for understanding five important aspects of\nneed-tweets and availability-tweets, including what resource and what quantity\nis needed/available, the geographical location of the need/availability, and\nwho needs / is providing the resource. Understanding these aspects helps us to\naddress the need-availability matching problem considering not only the\nresources, but also other factors such as the geographical proximity between\nthe need and the availability. To our knowledge, this study is the first\nattempt to develop methods for understanding the semantics of need-tweets and\navailability-tweets. We also develop a novel methodology for matching\nneed-tweets with availability-tweets, considering both resource similarity and\ngeographical proximity. Experiments on two datasets corresponding to two\ndisaster events, demonstrate that our proposed methods perform substantially\nbetter matching than those in prior works.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 08:43:37 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Dutt", "Ritam", ""], ["Basu", "Moumita", ""], ["Ghosh", "Kripabandhu", ""], ["Ghosh", "Saptarshi", ""]]}, {"id": "2007.09377", "submitter": "Alexander Veretennikov Borisovich", "authors": "Alexander B. Veretennikov", "title": "About a structure of easily updatable full-text indexes", "comments": "Indexing: Scopus. This is the English translation performed by the\n  author of the original Russian paper. The original version is available\n  online at: http://ceur-ws.org/Vol-1894/", "journal-ref": "CEUR Workshop Proceedings, vol. 1894, 2017, pp. 30-40. 48th\n  International Youth School Conference \"Modern Problems in Mathematics and its\n  Applications\", 06-Feb-2017", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider strategies to organize easily updatable associative arrays in\nexternal memory. These arrays are used for full-text search. We study indexes\nwith different keys: single word form, two word forms, and sequences of word\nforms. The storage structure depends on the size of the key's data. The results\nof the experiments are given in the context of the proximity full-text search,\nwhich is performed by means of additional indexes.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 09:30:52 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Veretennikov", "Alexander B.", ""]]}, {"id": "2007.09464", "submitter": "Sowmya Kamath", "authors": "Sowmya Kamath S and Karthik K", "title": "A Bag of Visual Words Model for Medical Image Retrieval", "comments": "In the proceedings of the 7th International Engineering Symposium\n  (IES 2018), Kumamoto University, Kumamoto, Japan, Mar 7-9, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical Image Retrieval is a challenging field in Visual information\nretrieval, due to the multi-dimensional and multi-modal context of the\nunderlying content. Traditional models often fail to take the intrinsic\ncharacteristics of data into consideration, and have thus achieved limited\naccuracy when applied to medical images. The Bag of Visual Words (BoVW) is a\ntechnique that can be used to effectively represent intrinsic image features in\nvector space, so that applications like image classification and similar-image\nsearch can be optimized. In this paper, we present a MedIR approach based on\nthe BoVW model for content-based medical image retrieval. As medical images as\nmulti-dimensional, they exhibit underlying cluster and manifold information\nwhich enhances semantic relevance and allows for label uniformity. Hence, the\nBoVW features extracted for each image are used to train a supervised machine\nlearning classifier based on positive and negative training images, for\nextending content based image retrieval. During experimental validation, the\nproposed model performed very well, achieving a Mean Average Precision of\n88.89% during top-3 image retrieval experiments.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 16:21:30 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["S", "Sowmya Kamath", ""], ["K", "Karthik", ""]]}, {"id": "2007.09536", "submitter": "Yu Meng", "authors": "Yu Meng, Yunyi Zhang, Jiaxin Huang, Yu Zhang, Chao Zhang, Jiawei Han", "title": "Hierarchical Topic Mining via Joint Spherical Tree and Text Embedding", "comments": "KDD 2020 Research Track. (Code: https://github.com/yumeng5/JoSH)", "journal-ref": null, "doi": "10.1145/3394486.3403242", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining a set of meaningful topics organized into a hierarchy is intuitively\nappealing since topic correlations are ubiquitous in massive text corpora. To\naccount for potential hierarchical topic structures, hierarchical topic models\ngeneralize flat topic models by incorporating latent topic hierarchies into\ntheir generative modeling process. However, due to their purely unsupervised\nnature, the learned topic hierarchy often deviates from users' particular needs\nor interests. To guide the hierarchical topic discovery process with minimal\nuser supervision, we propose a new task, Hierarchical Topic Mining, which takes\na category tree described by category names only, and aims to mine a set of\nrepresentative terms for each category from a text corpus to help a user\ncomprehend his/her interested topics. We develop a novel joint tree and text\nembedding method along with a principled optimization procedure that allows\nsimultaneous modeling of the category tree structure and the corpus generative\nprocess in the spherical space for effective category-representative term\ndiscovery. Our comprehensive experiments show that our model, named JoSH, mines\na high-quality set of hierarchical topics with high efficiency and benefits\nweakly-supervised hierarchical text classification tasks.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 23:30:47 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Meng", "Yu", ""], ["Zhang", "Yunyi", ""], ["Huang", "Jiaxin", ""], ["Zhang", "Yu", ""], ["Zhang", "Chao", ""], ["Han", "Jiawei", ""]]}, {"id": "2007.09703", "submitter": "Isa Inuwa-Dutse", "authors": "Isa Inuwa-Dutse, Ioannis Korkontzelos", "title": "A curated collection of COVID-19 online datasets", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the defining moments of the year 2020 is the outbreak of Coronavirus\nDisease (Covid-19), a deadly virus affecting the body's respiratory system to\nthe point of needing a breathing aid via ventilators. As of June 21, 2020 there\nare 12,929,306 confirmed cases and 569,738 confirmed deaths across 216\ncountries, areas or territories. The scale of spread and impact of the pandemic\nleft many nations grappling with preventive and curative approaches. The\ninfamous lockdown measure introduced to mitigate the virus spread has altered\nmany aspects of our social routines in which demand for online-based services\nskyrocketed. As the virus propagate, so does misinformation and fake news\naround it via online social media, which seems to favour virality over\nveracity. With a majority of the populace confined to their homes for a long\nperiod, vulnerability to the toxic impact of online misinformation is high. A\ncase in point is the various myths and disinformation associated with the\nCovid-19, which, if left unchecked, could lead to a catastrophic outcome and\nhamper the fight against the virus.\n  While the scientific community is actively engaged in identifying the virus\ntreatment, there is a growing interest in combating the associated harmful\ninfodemic. To this end, researchers have been curating and documenting various\ndatasets about Covid-19. In line with existing studies, we provide an expansive\ncollection of curated datasets to support the fight against the pandemic,\nespecially concerning misinformation. The collection consists of 3 categories\nof Twitter data, information about standard practices from credible sources and\na chronicle of global situation reports. We describe how to retrieve the\nhydrated version of the data and proffer some research problems that could be\naddressed using the data.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 16:14:58 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Inuwa-Dutse", "Isa", ""], ["Korkontzelos", "Ioannis", ""]]}, {"id": "2007.09798", "submitter": "Mucun Tian", "authors": "Mucun Tian, Chun Guo, Vito Ostuni, Zhen Zhu", "title": "Counterfactual Learning to Rank using Heterogeneous Treatment Effect\n  Estimation", "comments": "9 pages; to be published in SIGIR eCom'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-to-Rank (LTR) models trained from implicit feedback (e.g. clicks)\nsuffer from inherent biases. A well-known one is the position bias -- documents\nin top positions are more likely to receive clicks due in part to their\nposition advantages. To unbiasedly learn to rank, existing counterfactual\nframeworks first estimate the propensity (probability) of missing clicks with\nintervention data from a small portion of search traffic, and then use inverse\npropensity score (IPS) to debias LTR algorithms on the whole data set. These\napproaches often assume the propensity only depends on the position of the\ndocument, which may cause high estimation variance in applications where the\nsearch context (e.g. query, user) varies frequently. While context-dependent\npropensity models reduce variance, accurate estimations may require\nrandomization or intervention on a large amount of traffic, which may not be\nrealistic in real-world systems, especially for long tail queries. In this\nwork, we employ heterogeneous treatment effect estimation techniques to\nestimate position bias when intervention click data is limited. We then use\nsuch estimations to debias the observed click distribution and re-draw a new\nde-biased data set, which can be used for any LTR algorithms. We conduct\nsimulations with varying experiment conditions and show the effectiveness of\nthe proposed method in regimes with long tail queries and sparse clicks.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 22:12:33 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Tian", "Mucun", ""], ["Guo", "Chun", ""], ["Ostuni", "Vito", ""], ["Zhu", "Zhen", ""]]}, {"id": "2007.09878", "submitter": "Xiangyang Mou", "authors": "Xiangyang Mou, Mo Yu, Bingsheng Yao, Chenghao Yang, Xiaoxiao Guo,\n  Saloni Potdar, Hui Su", "title": "Frustratingly Hard Evidence Retrieval for QA Over Books", "comments": "ACL 2020 NUSE Workshop, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of progress has been made to improve question answering (QA) in recent\nyears, but the special problem of QA over narrative book stories has not been\nexplored in-depth. We formulate BookQA as an open-domain QA task given its\nsimilar dependency on evidence retrieval. We further investigate how\nstate-of-the-art open-domain QA approaches can help BookQA. Besides achieving\nstate-of-the-art on the NarrativeQA benchmark, our study also reveals the\ndifficulty of evidence retrieval in books with a wealth of experiments and\nanalysis - which necessitates future effort on novel solutions for evidence\nretrieval in BookQA.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 04:10:08 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Mou", "Xiangyang", ""], ["Yu", "Mo", ""], ["Yao", "Bingsheng", ""], ["Yang", "Chenghao", ""], ["Guo", "Xiaoxiao", ""], ["Potdar", "Saloni", ""], ["Su", "Hui", ""]]}, {"id": "2007.10276", "submitter": "Juan Banda", "authors": "Ramya Tekumalla, Juan M. Banda", "title": "Characterizing drug mentions in COVID-19 Twitter Chatter", "comments": "7 pages, 2 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the classification of COVID-19 as a global pandemic, there have been\nmany attempts to treat and contain the virus. Although there is no specific\nantiviral treatment recommended for COVID-19, there are several drugs that can\npotentially help with symptoms. In this work, we mined a large twitter dataset\nof 424 million tweets of COVID-19 chatter to identify discourse around drug\nmentions. While seemingly a straightforward task, due to the informal nature of\nlanguage use in Twitter, we demonstrate the need of machine learning alongside\ntraditional automated methods to aid in this task. By applying these\ncomplementary methods, we are able to recover almost 15% additional data,\nmaking misspelling handling a needed task as a pre-processing step when dealing\nwith social media data.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:56:46 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 15:35:23 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Tekumalla", "Ramya", ""], ["Banda", "Juan M.", ""]]}, {"id": "2007.10296", "submitter": "Fatemeh Sarvi", "authors": "Fatemeh Sarvi, Nikos Voskarides, Lois Mooiman, Sebastian Schelter and\n  Maarten de Rijke", "title": "A Comparison of Supervised Learning to Match Methods for Product Search", "comments": "10 pages, 5 figures, Accepted at SIGIR Workshop on eCommerce 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The vocabulary gap is a core challenge in information retrieval (IR). In\ne-commerce applications like product search, the vocabulary gap is reported to\nbe a bigger challenge than in more traditional application areas in IR, such as\nnews search or web search. As recent learning to match methods have made\nimportant advances in bridging the vocabulary gap for these traditional IR\nareas, we investigate their potential in the context of product search. In this\npaper we provide insights into using recent learning to match methods for\nproduct search. We compare both effectiveness and efficiency of these methods\nin a product search setting and analyze their performance on two product search\ndatasets, with 50,000 queries each. One is an open dataset made available as\npart of a community benchmark activity at CIKM 2016. The other is a proprietary\nquery log obtained from a European e-commerce platform. This comparison is\nconducted towards a better understanding of trade-offs in choosing a preferred\nmodel for this task. We find that (1) models that have been specifically\ndesigned for short text matching, like MV-LSTM and DRMMTKS, are consistently\namong the top three methods in all experiments; however, taking efficiency and\naccuracy into account at the same time, ARC-I is the preferred model for real\nworld use cases; and (2) the performance from a state-of-the-art BERT-based\nmodel is mediocre, which we attribute to the fact that the text BERT is\npre-trained on is very different from the text we have in product search. We\nalso provide insights into factors that can influence model behavior for\ndifferent types of query, such as the length of retrieved list, and query\ncomplexity, and discuss the implications of our findings for e-commerce\npractitioners, with respect to choosing a well performing method.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:28:57 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Sarvi", "Fatemeh", ""], ["Voskarides", "Nikos", ""], ["Mooiman", "Lois", ""], ["Schelter", "Sebastian", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2007.10434", "submitter": "Bhaskar Mitra", "authors": "Bhaskar Mitra, Sebastian Hofstatter, Hamed Zamani and Nick Craswell", "title": "Conformer-Kernel with Query Term Independence for Document Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Transformer-Kernel (TK) model has demonstrated strong reranking\nperformance on the TREC Deep Learning benchmark---and can be considered to be\nan efficient (but slightly less effective) alternative to BERT-based ranking\nmodels. In this work, we extend the TK architecture to the full retrieval\nsetting by incorporating the query term independence assumption. Furthermore,\nto reduce the memory complexity of the Transformer layers with respect to the\ninput sequence length, we propose a new Conformer layer. We show that the\nConformer's GPU memory requirement scales linearly with input sequence length,\nmaking it a more viable option when ranking long documents. Finally, we\ndemonstrate that incorporating explicit term matching signal into the model can\nbe particularly useful in the full retrieval setting. We present preliminary\nresults from our work in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 19:47:28 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Mitra", "Bhaskar", ""], ["Hofstatter", "Sebastian", ""], ["Zamani", "Hamed", ""], ["Craswell", "Nick", ""]]}, {"id": "2007.10610", "submitter": "Tongyu Lu", "authors": "Tongyu Lu, Lyucheng Yan, Gus Xia", "title": "Word Representation for Rhythms", "comments": "Construction of this paper is ill. The first author decides that this\n  paper needs thorough rewriting and more experiments should be done", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a word representation strategy for rhythm patterns. Using\n1034 pieces of Nottingham Dataset, a rhythm word dictionary whose size is 450\n(without control tokens) is generated. BERT model is created to explore\nsyntactic potentials of rhythm words. Our model is able to find overall music\nstructures and cluster different meters. In a larger scheme, a think mode -\nmusic as language - is proposed for systematic considerations.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 05:39:12 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 11:12:34 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2020 06:56:35 GMT"}, {"version": "v4", "created": "Thu, 3 Sep 2020 10:53:53 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Lu", "Tongyu", ""], ["Yan", "Lyucheng", ""], ["Xia", "Gus", ""]]}, {"id": "2007.10718", "submitter": "M. F. Mridha", "authors": "M. F. Mridha, Md. Saifur Rahman, Abu Quwsar Ohi", "title": "Human Abnormality Detection Based on Bengali Text", "comments": "The paper is accepted in IEEE Region 10 Symposium (TENSYMP) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of natural language processing and human-computer interaction,\nhuman attitudes and sentiments have attracted the researchers. However, in the\nfield of human-computer interaction, human abnormality detection has not been\ninvestigated extensively and most works depend on image-based information. In\nnatural language processing, effective meaning can potentially convey by all\nwords. Each word may bring out difficult encounters because of their semantic\nconnection with ideas or categories. In this paper, an efficient and effective\nhuman abnormality detection model is introduced, that only uses Bengali text.\nThis proposed model can recognize whether the person is in a normal or abnormal\nstate by analyzing their typed Bengali text. To the best of our knowledge, this\nis the first attempt in developing a text based human abnormality detection\nsystem. We have created our Bengali dataset (contains 2000 sentences) that is\ngenerated by voluntary conversations. We have performed the comparative\nanalysis by using Naive Bayes and Support Vector Machine as classifiers. Two\ndifferent feature extraction techniques count vector, and TF-IDF is used to\nexperiment on our constructed dataset. We have achieved a maximum 89% accuracy\nand 92% F1-score with our constructed dataset in our experiment.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:21:26 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Mridha", "M. F.", ""], ["Rahman", "Md. Saifur", ""], ["Ohi", "Abu Quwsar", ""]]}, {"id": "2007.10903", "submitter": "Sebastian Dunzer", "authors": "Sebastian Dunzer, Matthias Stierle, Martin Matzner, Stephan Baier", "title": "Conformance checking: A state-of-the-art literature review", "comments": null, "journal-ref": null, "doi": "10.1145/3329007.3329014", "report-no": null, "categories": "cs.SE cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Conformance checking is a set of process mining functions that compare\nprocess instances with a given process model. It identifies deviations between\nthe process instances' actual behaviour (\"as-is\") and its modelled behaviour\n(\"to-be\"). Especially in the context of analyzing compliance in organizations,\nit is currently gaining momentum -- e.g. for auditors. Researchers have\nproposed a variety of conformance checking techniques that are geared towards\ncertain process model notations or specific applications such as process model\nevaluation. This article reviews a set of conformance checking techniques\ndescribed in 37 scholarly publications. It classifies the techniques along the\ndimensions \"modelling language\", \"algorithm type\", \"quality metric\", and\n\"perspective\" using a concept matrix so that the techniques can be better\naccessed by practitioners and researchers. The matrix highlights the dimensions\nwhere extant research concentrates and where blind spots exist. For instance,\nprocess miners use declarative process modelling languages often, but\napplications in conformance checking are rare. Likewise, process mining can\ninvestigate process roles or process metrics such as duration, but conformance\nchecking techniques narrow on analyzing control-flow. Future research may\nconstruct techniques that support these neglected approaches to conformance\nchecking.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 15:48:41 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Dunzer", "Sebastian", ""], ["Stierle", "Matthias", ""], ["Matzner", "Martin", ""], ["Baier", "Stephan", ""]]}, {"id": "2007.11053", "submitter": "Josimar Chire Saire", "authors": "Honorio Apaza Alanoca, Americo A. Rubin de Celis Vidal, and Josimar\n  Edinson Chire Saire", "title": "Curriculum Vitae Recommendation Based on Text Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last years, the development in diverse areas related to computer\nscience and internet, allowed to generate new alternatives for decision making\nin the selection of personnel for state and private companies. In order to\noptimize this selection process, the recommendation systems are the most\nsuitable for working with explicit information related to the likes and\ndislikes of employers or end users, since this information allows to generate\nlists of recommendations based on collaboration or similarity of content.\nTherefore, this research takes as a basis these characteristics contained in\nthe database of curricula and job offers, which correspond to the Peruvian\nambit, which highlights the experience, knowledge and skills of each candidate,\nwhich are described in textual terms or words. This research focuses on the\nproblem: how we can take advantage from the growth of unstructured information\nabout job offers and curriculum vitae on different websites for CV\nrecommendation. So, we use the techniques from Text Mining and Natural Language\nProcessing. Then, as a relevant technique for the present study, we emphasize\nthe technique frequency of the Term - Inverse Frequency of the documents\n(TF-IDF), which allows identifying the most relevant CVs in relation to a job\noffer of website through the average values (TF-IDF). So, the weighted value\ncan be used as a qualification value of the relevant curriculum vitae for the\nrecommendation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 19:29:26 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Alanoca", "Honorio Apaza", ""], ["Vidal", "Americo A. Rubin de Celis", ""], ["Saire", "Josimar Edinson Chire", ""]]}, {"id": "2007.11088", "submitter": "Luyu Gao", "authors": "Luyu Gao, Zhuyun Dai, Jamie Callan", "title": "Understanding BERT Rankers Under Distillation", "comments": null, "journal-ref": null, "doi": "10.1145/3409256.3409838", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep language models such as BERT pre-trained on large corpus have given a\nhuge performance boost to the state-of-the-art information retrieval ranking\nsystems. Knowledge embedded in such models allows them to pick up complex\nmatching signals between passages and queries. However, the high computation\ncost during inference limits their deployment in real-world search scenarios.\nIn this paper, we study if and how the knowledge for search within BERT can be\ntransferred to a smaller ranker through distillation. Our experiments\ndemonstrate that it is crucial to use a proper distillation procedure, which\nproduces up to nine times speedup while preserving the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 20:55:57 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Gao", "Luyu", ""], ["Dai", "Zhuyun", ""], ["Callan", "Jamie", ""]]}, {"id": "2007.11198", "submitter": "Austin Silveria", "authors": "Austin Silveria", "title": "Exploratory Search with Sentence Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploratory search aims to guide users through a corpus rather than\npinpointing exact information. We propose an exploratory search system based on\nhierarchical clusters and document summaries using sentence embeddings. With\nsentence embeddings, we represent documents as the mean of their embedded\nsentences, extract summaries containing sentences close to this document\nrepresentation and extract keyphrases close to the document representation. To\nevaluate our search system, we scrape our personal search history over the past\nyear and report our experience with the system. We then discuss motivating use\ncases of an exploratory search system of this nature and conclude with possible\ndirections of future work.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 04:46:54 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Silveria", "Austin", ""]]}, {"id": "2007.11298", "submitter": "Viola Wenz", "authors": "Arno Kesper, Viola Wenz, Gabriele Taentzer", "title": "Detecting Quality Problems in Research Data: A Model-Driven Approach", "comments": "28 pages. This paper is an extended version of a paper to be\n  published in \"ACM/IEEE 23rd International Conference on Model Driven\n  Engineering Languages and Systems (MODELS '20)\". Added subtitle", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As scientific progress highly depends on the quality of research data, there\nare strict requirements for data quality coming from the scientific community.\nA major challenge in data quality assurance is to localise quality problems\nthat are inherent to data. Due to the dynamic digitalisation in specific\nscientific fields, especially the humanities, different database technologies\nand data formats may be used in rather short terms to gain experiences. We\npresent a model-driven approach to analyse the quality of research data. It\nallows abstracting from the underlying database technology. Based on the\nobservation that many quality problems show anti-patterns, a data engineer\nformulates analysis patterns that are generic concerning the database format\nand technology. A domain expert chooses a pattern that has been adapted to a\nspecific database technology and concretises it for a domain-specific database\nformat. The resulting concrete patterns are used by data analysts to locate\nquality problems in their databases. As proof of concept, we implemented tool\nsupport that realises this approach for XML databases. We evaluated our\napproach concerning expressiveness and performance in the domain of cultural\nheritage based on a qualitative study on quality problems occurring in cultural\nheritage data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 09:32:32 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 06:33:42 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Kesper", "Arno", ""], ["Wenz", "Viola", ""], ["Taentzer", "Gabriele", ""]]}, {"id": "2007.11501", "submitter": "Tiankui Zhang", "authors": "Tiankui Zhang, Yi Wang, Yuanwei Liu, Wenjun Xu, and Arumugam\n  Nallanathan", "title": "Cache-enabling UAV Communications: Network Deployment and Resource\n  Allocation", "comments": "32 pages, 10 figures", "journal-ref": null, "doi": "10.1109/TWC.2020.3011881", "report-no": null, "categories": "eess.SP cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate the content distribution in the hotspot area,\nwhose traffic is offloaded by the combination of the unmanned aerial vehicle\n(UAV) communication and edge caching. In cache-enabling UAV-assisted cellular\nnetworks, the network deployment and resource allocation are vital for quality\nof experience (QoE) of users with content distribution applications. We\nformulate a joint optimization problem of UAV deployment, caching placement and\nuser association for maximizing QoE of users, which is evaluated by mean\nopinion score (MOS). To solve this challenging problem, we decompose the\noptimization problem into three sub-problems. Specifically, we propose a swap\nmatching based UAV deployment algorithm, then obtain the near-optimal caching\nplacement and user association by greedy algorithm and Lagrange dual,\nrespectively. Finally, we propose a low complexity iterative algorithm for the\njoint UAV deployment, caching placement and user association optimization,\nwhich achieves good computational complexity-optimality tradeoff. Simulation\nresults reveal that: i) the MOS of the proposed algorithm approaches that of\nthe exhaustive search method and converges within several iterations; and ii)\ncompared with the benchmark algorithms, the proposed algorithm achieves better\nperformance in terms of MOS, content access delay and backhaul traffic\noffloading.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 15:46:01 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Zhang", "Tiankui", ""], ["Wang", "Yi", ""], ["Liu", "Yuanwei", ""], ["Xu", "Wenjun", ""], ["Nallanathan", "Arumugam", ""]]}, {"id": "2007.11604", "submitter": "Ashkan Ebadi", "authors": "Ashkan Ebadi, Pengcheng Xi, St\\'ephane Tremblay, Bruce Spencer, Raman\n  Pall, Alexander Wong", "title": "Understanding the temporal evolution of COVID-19 research through\n  machine learning and natural language processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of the novel coronavirus disease 2019 (COVID-19), caused by the\nsevere acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has been\ncontinuously affecting human lives and communities around the world in many\nways, from cities under lockdown to new social experiences. Although in most\ncases COVID-19 results in mild illness, it has drawn global attention due to\nthe extremely contagious nature of SARS-CoV-2. Governments and healthcare\nprofessionals, along with people and society as a whole, have taken any\nmeasures to break the chain of transition and flatten the epidemic curve. In\nthis study, we used multiple data sources, i.e., PubMed and ArXiv, and built\nseveral machine learning models to characterize the landscape of current\nCOVID-19 research by identifying the latent topics and analyzing the temporal\nevolution of the extracted research themes, publications similarity, and\nsentiments, within the time-frame of January- May 2020. Our findings confirm\nthe types of research available in PubMed and ArXiv differ significantly, with\nthe former exhibiting greater diversity in terms of COVID-19 related issues and\nthe latter focusing more on intelligent systems/tools to predict/diagnose\nCOVID-19. The special attention of the research community to the high-risk\ngroups and people with complications was also confirmed.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 18:02:39 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Ebadi", "Ashkan", ""], ["Xi", "Pengcheng", ""], ["Tremblay", "St\u00e9phane", ""], ["Spencer", "Bruce", ""], ["Pall", "Raman", ""], ["Wong", "Alexander", ""]]}, {"id": "2007.11659", "submitter": "Gianluca Demartini", "authors": "Edgar Meij, Tara Safavi, Chenyan Xiong, Gianluca Demartini, Miriam\n  Redi, Fatma \\\"Ozcan", "title": "Proceedings of the KG-BIAS Workshop 2020 at AKBC 2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The KG-BIAS 2020 workshop touches on biases and how they surface in knowledge\ngraphs (KGs), biases in the source data that is used to create KGs, methods for\nmeasuring or remediating bias in KGs, but also identifying other biases such as\nhow and which languages are represented in automatically constructed KGs or how\npersonal KGs might incur inherent biases. The goal of this workshop is to\nuncover how various types of biases are introduced into KGs, investigate how to\nmeasure, and propose methods to remediate them.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 22:37:57 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Meij", "Edgar", ""], ["Safavi", "Tara", ""], ["Xiong", "Chenyan", ""], ["Demartini", "Gianluca", ""], ["Redi", "Miriam", ""], ["\u00d6zcan", "Fatma", ""]]}, {"id": "2007.11682", "submitter": "Charles Clarke", "authors": "Charles L. A. Clarke, Alexandra Vtyurina, Mark D. Smucker", "title": "Assessing top-$k$ preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessors make preference judgments faster and more consistently than graded\njudgments. Preference judgments can also recognize distinctions between items\nthat appear equivalent under graded judgments. Unfortunately, preference\njudgments can require more than linear effort to fully order a pool of items,\nand evaluation measures for preference judgments are not as well established as\nthose for graded judgments, such as NDCG. In this paper, we explore the\nassessment process for partial preference judgments, with the aim of\nidentifying and ordering the top items in the pool, rather than fully ordering\nthe entire pool. To measure the performance of a ranker, we compare its output\nto this preferred ordering by applying a rank similarity measure.We demonstrate\nthe practical feasibility of this approach by crowdsourcing partial preferences\nfor the TREC 2019 Conversational Assistance Track, replacing NDCG with a new\nmeasure named \"compatibility\". This new measure has its most striking impact\nwhen comparing modern neural rankers, where it is able to recognize significant\nimprovements in quality that would otherwise be missed by NDCG.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 21:18:36 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 17:52:41 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Clarke", "Charles L. A.", ""], ["Vtyurina", "Alexandra", ""], ["Smucker", "Mark D.", ""]]}, {"id": "2007.11821", "submitter": "Elad Yom-Tov", "authors": "Elad Yom-Tov, Vasileios Lampos, Ingemar J. Cox, Michael Edelstein", "title": "Providing early indication of regional anomalies in COVID19 case counts\n  in England using search engine queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID19 was first reported in England at the end of January 2020, and by\nmid-June over 150,000 cases were reported. We assume that, similarly to\ninfluenza-like illnesses, people who suffer from COVID19 may query for their\nsymptoms prior to accessing the medical system (or in lieu of it). Therefore,\nwe analyzed searches to Bing from users in England, identifying cases where\nunexpected rises in relevant symptom searches occurred at specific areas of the\ncountry. Our analysis shows that searches for \"fever\" and \"cough\" were the most\ncorrelated with future case counts, with searches preceding case counts by\n16-17 days. Unexpected rises in search patterns were predictive of future case\ncounts multiplying by 2.5 or more within a week, reaching an Area Under Curve\n(AUC) of 0.64. Similar rises in mortality were predicted with an AUC of\napproximately 0.61 at a lead time of 3 weeks. Thus, our metric provided Public\nHealth England with an indication which could be used to plan the response to\nCOVID19 and could possibly be utilized to detect regional anomalies of other\npathogens.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 06:47:17 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Yom-Tov", "Elad", ""], ["Lampos", "Vasileios", ""], ["Cox", "Ingemar J.", ""], ["Edelstein", "Michael", ""]]}, {"id": "2007.11893", "submitter": "Maurizio Ferrari Dacrema", "authors": "Maurizio Ferrari Dacrema, Federico Parroni, Paolo Cremonesi, Dietmar\n  Jannach", "title": "Critically Examining the Claimed Value of Convolutions over User-Item\n  Embedding Maps for Recommender Systems", "comments": "Source code available here:\n  https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation", "journal-ref": "The 29th ACM International Conference on Information and Knowledge\n  Management (CIKM '20), October 19--23, 2020, Virtual Event, Ireland", "doi": "10.1145/3340531.3411901", "report-no": null, "categories": "cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, algorithm research in the area of recommender systems has\nshifted from matrix factorization techniques and their latent factor models to\nneural approaches. However, given the proven power of latent factor models,\nsome newer neural approaches incorporate them within more complex network\narchitectures. One specific idea, recently put forward by several researchers,\nis to consider potential correlations between the latent factors, i.e.,\nembeddings, by applying convolutions over the user-item interaction map.\nHowever, contrary to what is claimed in these articles, such interaction maps\ndo not share the properties of images where Convolutional Neural Networks\n(CNNs) are particularly useful. In this work, we show through analytical\nconsiderations and empirical evaluations that the claimed gains reported in the\nliterature cannot be attributed to the ability of CNNs to model embedding\ncorrelations, as argued in the original papers. Moreover, additional\nperformance evaluations show that all of the examined recent CNN-based models\nare outperformed by existing non-neural machine learning techniques or\ntraditional nearest-neighbor approaches. On a more general level, our work\npoints to major methodological issues in recommender systems research.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 10:03:47 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 18:53:28 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Dacrema", "Maurizio Ferrari", ""], ["Parroni", "Federico", ""], ["Cremonesi", "Paolo", ""], ["Jannach", "Dietmar", ""]]}, {"id": "2007.11955", "submitter": "Arindam Pal", "authors": "Rizka Purwanto, Arindam Pal, Alan Blair, Sanjay Jha", "title": "PhishZip: A New Compression-based Algorithm for Detecting Phishing\n  Websites", "comments": "To appear in the proceedings of IEEE Conference on Communications and\n  Network Security (CNS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phishing has grown significantly in the past few years and is predicted to\nfurther increase in the future. The dynamics of phishing introduce challenges\nin implementing a robust phishing detection system and selecting features which\ncan represent phishing despite the change of attack. In this paper, we propose\nPhishZip which is a novel phishing detection approach using a compression\nalgorithm to perform website classification and demonstrate a systematic way to\nconstruct the word dictionaries for the compression models using word\noccurrence likelihood analysis. PhishZip outperforms the use of best-performing\nHTML-based features in past studies, with a true positive rate of 80.04%. We\nalso propose the use of compression ratio as a novel machine learning feature\nwhich significantly improves machine learning based phishing detection over\nprevious studies. Using compression ratios as additional features, the true\npositive rate significantly improves by 30.3% (from 51.47% to 81.77%), while\nthe accuracy increases by 11.84% (from 71.20% to 83.04%).\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 00:32:06 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Purwanto", "Rizka", ""], ["Pal", "Arindam", ""], ["Blair", "Alan", ""], ["Jha", "Sanjay", ""]]}, {"id": "2007.12000", "submitter": "Fei Mi", "authors": "Fei Mi, Xiaoyu Lin, and Boi Faltings", "title": "ADER: Adaptively Distilled Exemplar Replay Towards Continual Learning\n  for Session-based Recommendation", "comments": "Accepted at RecSys 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommendation has received growing attention recently due to\nthe increasing privacy concern. Despite the recent success of neural\nsession-based recommenders, they are typically developed in an offline manner\nusing a static dataset. However, recommendation requires continual adaptation\nto take into account new and obsolete items and users, and requires \"continual\nlearning\" in real-life applications. In this case, the recommender is updated\ncontinually and periodically with new data that arrives in each update cycle,\nand the updated model needs to provide recommendations for user activities\nbefore the next model update. A major challenge for continual learning with\nneural models is catastrophic forgetting, in which a continually trained model\nforgets user preference patterns it has learned before. To deal with this\nchallenge, we propose a method called Adaptively Distilled Exemplar Replay\n(ADER) by periodically replaying previous training samples (i.e., exemplars) to\nthe current model with an adaptive distillation loss. Experiments are conducted\nbased on the state-of-the-art SASRec model using two widely used datasets to\nbenchmark ADER with several well-known continual learning techniques. We\nempirically demonstrate that ADER consistently outperforms other baselines, and\nit even outperforms the method using all historical data at every update cycle.\nThis result reveals that ADER is a promising solution to mitigate the\ncatastrophic forgetting issue towards building more realistic and scalable\nsession-based recommenders.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 13:19:53 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Mi", "Fei", ""], ["Lin", "Xiaoyu", ""], ["Faltings", "Boi", ""]]}, {"id": "2007.12076", "submitter": "Aditya Srivastava", "authors": "Aditya Srivastava, V. Harsha Vardhan", "title": "HCMS at SemEval-2020 Task 9: A Neural Approach to Sentiment Analysis for\n  Code-Mixed Texts", "comments": "6 pages, 2 figures, 4 tables, math equations, to be published in the\n  proceedings of the 14th International Workshop on Semantic Evaluation\n  (SemEval) 2020, Association for Computational Linguistics (ACL). Code for the\n  paper is available at https://github.com/IamAdiSri/hcms-semeval20 . Data and\n  task description is available at\n  https://competitions.codalab.org/competitions/20654", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems involving code-mixed language are often plagued by a lack of\nresources and an absence of materials to perform sophisticated transfer\nlearning with. In this paper we describe our submission to the Sentimix\nHindi-English task involving sentiment classification of code-mixed texts, and\nwith an F1 score of 67.1%, we demonstrate that simple convolution and attention\nmay well produce reasonable results.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 15:39:53 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Srivastava", "Aditya", ""], ["Vardhan", "V. Harsha", ""]]}, {"id": "2007.12135", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Tao Di, Yongfeng Huang, Xing Xie", "title": "FedCTR: Federated Native Ad CTR Prediction with Multi-Platform User\n  Behavior Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Native ad is a popular type of online advertisement which has similar forms\nwith the native content displayed on websites. Native ad CTR prediction is\nuseful for improving user experience and platform revenue. However, it is\nchallenging due to the lack of explicit user intent, and users' behaviors on\nthe platform with native ads may not be sufficient to infer their interest in\nads. Fortunately, user behaviors exist on many online platforms and they can\nprovide complementary information for user interest mining. Thus, leveraging\nmulti-platform user behaviors is useful for native ad CTR prediction. However,\nuser behaviors are highly privacy-sensitive and the behavior data on different\nplatforms cannot be directly aggregated due to user privacy concerns and data\nprotection regulations like GDPR. Existing CTR prediction methods usually\nrequire centralized storage of user behavior data for user modeling and cannot\nbe directly applied to the CTR prediction task with multi-platform user\nbehaviors. In this paper, we propose a federated native ad CTR prediction\nmethod named FedCTR, which can learn user interest representations from their\nbehaviors on multiple platforms in a privacy-preserving way. On each platform a\nlocal user model is used to learn user embeddings from the local user behaviors\non that platform. The local user embeddings from different platforms are\nuploaded to a server for aggregation, and the aggregated user embeddings are\nsent to the ad platform for CTR prediction. Besides, we apply LDP and DP\ntechniques to the local and aggregated user embeddings respectively for better\nprivacy protection. Moreover, we propose a federated framework for model\ntraining with distributed models and user behaviors. Extensive experiments on\nreal-world dataset show that FedCTR can effectively leverage multi-platform\nuser behaviors for native ad CTR prediction in a privacy-preserving manner.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 17:04:54 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Di", "Tao", ""], ["Huang", "Yongfeng", ""], ["Xie", "Xing", ""]]}, {"id": "2007.12144", "submitter": "Oladapo Oyebode", "authors": "Oladapo Oyebode, Chinenye Ndulue, Ashfaq Adib, Dinesh Mulchandani,\n  Banuchitra Suruliraj, Fidelia Anulika Orji, Christine Chambers, Sandra Meier,\n  and Rita Orji", "title": "Health, Psychosocial, and Social issues emanating from COVID-19 pandemic\n  based on Social Media Comments using Natural Language Processing", "comments": null, "journal-ref": "JMIR Medical Informatics. 2021. 9(4):e22734", "doi": "10.2196/22734", "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has caused a global health crisis that affects many\naspects of human lives. In the absence of vaccines and antivirals, several\nbehavioural change and policy initiatives, such as physical distancing, have\nbeen implemented to control the spread of the coronavirus. Social media data\ncan reveal public perceptions toward how governments and health agencies across\nthe globe are handling the pandemic, as well as the impact of the disease on\npeople regardless of their geographic locations in line with various factors\nthat hinder or facilitate the efforts to control the spread of the pandemic\nglobally. This paper aims to investigate the impact of the COVID-19 pandemic on\npeople globally using social media data. We apply natural language processing\n(NLP) and thematic analysis to understand public opinions, experiences, and\nissues with respect to the COVID-19 pandemic using social media data. First, we\ncollect over 47 million COVID-19-related comments from Twitter, Facebook,\nYouTube, and three online discussion forums. Second, we perform data\npreprocessing which involves applying NLP techniques to clean and prepare the\ndata for automated theme extraction. Third, we apply context-aware NLP approach\nto extract meaningful keyphrases or themes from over 1 million randomly\nselected comments, as well as compute sentiment scores for each theme and\nassign sentiment polarity based on the scores using lexicon-based technique.\nFourth, we categorize related themes into broader themes. A total of 34\nnegative themes emerged, out of which 15 are health-related issues,\npsychosocial issues, and social issues related to the COVID-19 pandemic from\nthe public perspective. In addition, 20 positive themes emerged from our\nresults. Finally, we recommend interventions that can help address the negative\nissues based on the positive themes and other remedial ideas rooted in\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 17:19:50 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Oyebode", "Oladapo", ""], ["Ndulue", "Chinenye", ""], ["Adib", "Ashfaq", ""], ["Mulchandani", "Dinesh", ""], ["Suruliraj", "Banuchitra", ""], ["Orji", "Fidelia Anulika", ""], ["Chambers", "Christine", ""], ["Meier", "Sandra", ""], ["Orji", "Rita", ""]]}, {"id": "2007.12161", "submitter": "Morteza Noshad Iranzad", "authors": "Morteza Noshad, Ivana Jankovic, Jonathan H. Chen", "title": "Clinical Recommender System: Predicting Medical Specialty Diagnostic\n  Choices with Neural Network Ensembles", "comments": "Proceedings of 2020 KDD Workshop onApplied Data Science for\n  Healthcare (KDD 2020).ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing demand for key healthcare resources such as clinical expertise\nand facilities has motivated the emergence of artificial intelligence (AI)\nbased decision support systems. We address the problem of predicting clinical\nworkups for specialty referrals. As an alternative for manually-created\nclinical checklists, we propose a data-driven model that recommends the\nnecessary set of diagnostic procedures based on the patients' most recent\nclinical record extracted from the Electronic Health Record (EHR). This has the\npotential to enable health systems expand timely access to initial medical\nspecialty diagnostic workups for patients. The proposed approach is based on an\nensemble of feed-forward neural networks and achieves significantly higher\naccuracy compared to the conventional clinical checklists.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 17:50:15 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Noshad", "Morteza", ""], ["Jankovic", "Ivana", ""], ["Chen", "Jonathan H.", ""]]}, {"id": "2007.12212", "submitter": "Anurag Roy", "authors": "Anurag Roy, Vinay Kumar Verma, Kripabandhu Ghosh, Saptarshi Ghosh", "title": "ZSCRGAN: A GAN-based Expectation Maximization Model for Zero-Shot\n  Retrieval of Images from Textual Descriptions", "comments": "Accepted in CIKM-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing algorithms for cross-modal Information Retrieval are based on a\nsupervised train-test setup, where a model learns to align the mode of the\nquery (e.g., text) to the mode of the documents (e.g., images) from a given\ntraining set. Such a setup assumes that the training set contains an exhaustive\nrepresentation of all possible classes of queries. In reality, a retrieval\nmodel may need to be deployed on previously unseen classes, which implies a\nzero-shot IR setup. In this paper, we propose a novel GAN-based model for\nzero-shot text to image retrieval. When given a textual description as the\nquery, our model can retrieve relevant images in a zero-shot setup. The\nproposed model is trained using an Expectation-Maximization framework.\nExperiments on multiple benchmark datasets show that our proposed model\ncomfortably outperforms several state-of-the-art zero-shot text to image\nretrieval models, as well as zero-shot classification and hashing models\nsuitably used for retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 18:50:03 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 11:57:35 GMT"}, {"version": "v3", "created": "Wed, 23 Sep 2020 11:41:12 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Roy", "Anurag", ""], ["Verma", "Vinay Kumar", ""], ["Ghosh", "Kripabandhu", ""], ["Ghosh", "Saptarshi", ""]]}, {"id": "2007.12230", "submitter": "Himan Abdollahpouri", "authors": "Himan Abdollahpouri, Masoud Mansoury, Robin Burke, Bamshad Mobasher", "title": "Addressing the Multistakeholder Impact of Popularity Bias in\n  Recommendation Through Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popularity bias is a well-known phenomenon in recommender systems: popular\nitems are recommended even more frequently than their popularity would warrant,\namplifying long-tail effects already present in many recommendation domains.\nPrior research has examined various approaches for mitigating popularity bias\nand enhancing the recommendation of long-tail items overall. The effectiveness\nof these approaches, however, has not been assessed in multistakeholder\nenvironments where in addition to the users who receive the recommendations,\nthe utility of the suppliers of the recommended items should also be\nconsidered. In this paper, we propose the concept of popularity calibration\nwhich measures the match between the popularity distribution of items in a\nuser's profile and that of the recommended items. We also develop an algorithm\nthat optimizes this metric. In addition, we demonstrate that existing\nevaluation metrics for popularity bias do not reflect the performance of the\nalgorithms when it is measured from the perspective of different stakeholders.\nUsing music and movie datasets, we empirically show that our approach\noutperforms the existing state-of-the-art approaches in addressing popularity\nbias by calibrating the recommendations to users' preferences. We also show\nthat our proposed algorithm has a secondary effect of improving supplier\nfairness.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 19:51:16 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Abdollahpouri", "Himan", ""], ["Mansoury", "Masoud", ""], ["Burke", "Robin", ""], ["Mobasher", "Bamshad", ""]]}, {"id": "2007.12329", "submitter": "Yujia Zheng", "authors": "Siyi Liu and Yujia Zheng", "title": "Long-tail Session-based Recommendation", "comments": "Accepted at RecSys 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommendation focuses on the prediction of user actions based\non anonymous sessions and is a necessary method in the lack of user historical\ndata. However, none of the existing session-based recommendation methods\nexplicitly takes the long-tail recommendation into consideration, which plays\nan important role in improving the diversity of recommendation and producing\nthe serendipity. As the distribution of items with long-tail is prevalent in\nsession-based recommendation scenarios (e.g., e-commerce, music, and TV program\nrecommendations), more attention should be put on the long-tail session-based\nrecommendation. In this paper, we propose a novel network architecture, namely\nTailNet, to improve long-tail recommendation performance, while maintaining\ncompetitive accuracy performance compared with other methods. We start by\nclassifying items into short-head (popular) and long-tail (niche) items based\non click frequency. Then a novel is proposed and applied in TailNet to\ndetermine user preference between two types of items, so as to softly adjust\nand personalize recommendations. Extensive experiments on two real-world\ndatasets verify the superiority of our method compared with state-of-the-art\nworks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 03:36:35 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 02:00:35 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Liu", "Siyi", ""], ["Zheng", "Yujia", ""]]}, {"id": "2007.12358", "submitter": "Sina Mohseni", "authors": "Sina Mohseni, Fan Yang, Shiva Pentyala, Mengnan Du, Yi Liu, Nic\n  Lupfer, Xia Hu, Shuiwang Ji, Eric Ragan", "title": "Machine Learning Explanations to Prevent Overtrust in Fake News\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combating fake news and misinformation propagation is a challenging task in\nthe post-truth era. News feed and search algorithms could potentially lead to\nunintentional large-scale propagation of false and fabricated information with\nusers being exposed to algorithmically selected false content. Our research\ninvestigates the effects of an Explainable AI assistant embedded in news review\nplatforms for combating the propagation of fake news. We design a news\nreviewing and sharing interface, create a dataset of news stories, and train\nfour interpretable fake news detection algorithms to study the effects of\nalgorithmic transparency on end-users. We present evaluation results and\nanalysis from multiple controlled crowdsourced studies. For a deeper\nunderstanding of Explainable AI systems, we discuss interactions between user\nengagement, mental model, trust, and performance measures in the process of\nexplaining. The study results indicate that explanations helped participants to\nbuild appropriate mental models of the intelligent assistants in different\nconditions and adjust their trust accordingly for model limitations.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 05:42:29 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 03:59:56 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Mohseni", "Sina", ""], ["Yang", "Fan", ""], ["Pentyala", "Shiva", ""], ["Du", "Mengnan", ""], ["Liu", "Yi", ""], ["Lupfer", "Nic", ""], ["Hu", "Xia", ""], ["Ji", "Shuiwang", ""], ["Ragan", "Eric", ""]]}, {"id": "2007.12603", "submitter": "Anup Anand Deshmukh", "authors": "Anup Anand Deshmukh and Udhav Sethi", "title": "IR-BERT: Leveraging BERT for Semantic Search in Background Linking for\n  News Articles", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes our two approaches for the background linking task of\nTREC 2020 News Track. The main objective of this task is to recommend a list of\nrelevant articles that the reader should refer to in order to understand the\ncontext and gain background information of the query article. Our first\napproach focuses on building an effective search query by combining weighted\nkeywords extracted from the query document and uses BM25 for retrieval. The\nsecond approach leverages the capability of SBERT (Nils Reimers et al.) to\nlearn contextual representations of the query in order to perform semantic\nsearch over the corpus. We empirically show that employing a language model\nbenefits our approach in understanding the context as well as the background of\nthe query article. The proposed approaches are evaluated on the TREC 2018\nWashington Post dataset and our best model outperforms the TREC median as well\nas the highest scoring model of 2018 in terms of the nDCG@5 metric. We further\npropose a diversity measure to evaluate the effectiveness of the various\napproaches in retrieving a diverse set of documents. This would potentially\nmotivate researchers to work on introducing diversity in their recommended\nlist. We have open sourced our implementation on Github and plan to submit our\nruns for the background linking task in TREC 2020.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 16:02:14 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Deshmukh", "Anup Anand", ""], ["Sethi", "Udhav", ""]]}, {"id": "2007.12719", "submitter": "Harrie Oosterhuis", "authors": "Harrie Oosterhuis and Maarten de Rijke", "title": "Taking the Counterfactual Online: Efficient and Unbiased Online\n  Evaluation for Ranking", "comments": "ICTIR 2020", "journal-ref": null, "doi": "10.1145/3409256.3409820", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual evaluation can estimate Click-Through-Rate (CTR) differences\nbetween ranking systems based on historical interaction data, while mitigating\nthe effect of position bias and item-selection bias. We introduce the novel\nLogging-Policy Optimization Algorithm (LogOpt), which optimizes the policy for\nlogging data so that the counterfactual estimate has minimal variance. As\nminimizing variance leads to faster convergence, LogOpt increases the\ndata-efficiency of counterfactual estimation. LogOpt turns the counterfactual\napproach - which is indifferent to the logging policy - into an online\napproach, where the algorithm decides what rankings to display. We prove that,\nas an online evaluation method, LogOpt is unbiased w.r.t. position and\nitem-selection bias, unlike existing interleaving methods. Furthermore, we\nperform large-scale experiments by simulating comparisons between thousands of\nrankers. Our results show that while interleaving methods make systematic\nerrors, LogOpt is as efficient as interleaving without being biased.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 18:05:58 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 17:59:33 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Oosterhuis", "Harrie", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2007.12731", "submitter": "Colby Wise", "authors": "Colby Wise, Vassilis N. Ioannidis, Miguel Romero Calvo, Xiang Song,\n  George Price, Ninad Kulkarni, Ryan Brand, Parminder Bhatia, George Karypis", "title": "COVID-19 Knowledge Graph: Accelerating Information Retrieval and\n  Discovery for Scientific Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus disease (COVID-19) has claimed the lives of over 350,000\npeople and infected more than 6 million people worldwide. Several search\nengines have surfaced to provide researchers with additional tools to find and\nretrieve information from the rapidly growing corpora on COVID-19. These\nengines lack extraction and visualization tools necessary to retrieve and\ninterpret complex relations inherent to scientific literature. Moreover,\nbecause these engines mainly rely upon semantic information, their ability to\ncapture complex global relationships across documents is limited, which reduces\nthe quality of similarity-based article recommendations for users. In this\nwork, we present the COVID-19 Knowledge Graph (CKG), a heterogeneous graph for\nextracting and visualizing complex relationships between COVID-19 scientific\narticles. The CKG combines semantic information with document topological\ninformation for the application of similar document retrieval. The CKG is\nconstructed using the latent schema of the data, and then enriched with\nbiomedical entity information extracted from the unstructured text of articles\nusing scalable AWS technologies to form relations in the graph. Finally, we\npropose a document similarity engine that leverages low-dimensional graph\nembeddings from the CKG with semantic embeddings for similar article retrieval.\nAnalysis demonstrates the quality of relationships in the CKG and shows that it\ncan be used to uncover meaningful information in COVID-19 scientific articles.\nThe CKG helps power www.cord19.aws and is publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 18:29:43 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Wise", "Colby", ""], ["Ioannidis", "Vassilis N.", ""], ["Calvo", "Miguel Romero", ""], ["Song", "Xiang", ""], ["Price", "George", ""], ["Kulkarni", "Ninad", ""], ["Brand", "Ryan", ""], ["Bhatia", "Parminder", ""], ["Karypis", "George", ""]]}, {"id": "2007.12761", "submitter": "Temiloluwa Prioleau", "authors": "Temiloluwa Prioleau, Ashutosh Sabharwal, Madhuri M. Vasudevan", "title": "Understanding Reflection Needs for Personal Health Data in Diabetes", "comments": "11 pages, 6 figures, paper to appear in Pervasive Health 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To empower users of wearable medical devices, it is important to enable\nmethods that facilitate reflection on previous care to improve future outcomes.\nIn this work, we conducted a two-phase user-study involving patients,\ncaregivers, and clinicians to understand gaps in current approaches that\nsupport reflection and user needs for new solutions. Our results show that\nusers desire to have specific summarization metrics, solutions that minimize\ncognitive effort, and solutions that enable data integration to support\nmeaningful reflection on diabetes management. In addition, we developed and\nevaluated a visualization called PixelGrid that presents key metrics in a\nmatrix-based plot. Majority of users (84%) found the matrix-based approach to\nbe useful for identifying salient patterns related to certain times and days in\nblood glucose data. Through our evaluation we identified that users desire data\nvisualization solutions with complementary textual descriptors, concise and\nflexible presentation, contextually-fitting content, and informative and\nactionable insights. Directions for future research on tools that automate\npattern discovery, detect abnormalities, and provide recommendations to improve\ncare were also identified.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 20:29:33 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Prioleau", "Temiloluwa", ""], ["Sabharwal", "Ashutosh", ""], ["Vasudevan", "Madhuri M.", ""]]}, {"id": "2007.12865", "submitter": "Tiansheng Yao", "authors": "Tiansheng Yao, Xinyang Yi, Derek Zhiyuan Cheng, Felix Yu, Ting Chen,\n  Aditya Menon, Lichan Hong, Ed H. Chi, Steve Tjoa, Jieqi Kang, Evan Ettinger", "title": "Self-supervised Learning for Large-scale Item Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale recommender models find most relevant items from huge catalogs,\nand they play a critical role in modern search and recommendation systems. To\nmodel the input space with large-vocab categorical features, a typical\nrecommender model learns a joint embedding space through neural networks for\nboth queries and items from user feedback data. However, with millions to\nbillions of items in the corpus, users tend to provide feedback for a very\nsmall set of them, causing a power-law distribution. This makes the feedback\ndata for long-tail items extremely sparse.\n  Inspired by the recent success in self-supervised representation learning\nresearch in both computer vision and natural language understanding, we propose\na multi-task self-supervised learning (SSL) framework for large-scale item\nrecommendations. The framework is designed to tackle the label sparsity problem\nby learning better latent relationship of item features. Specifically, SSL\nimproves item representation learning as well as serving as additional\nregularization to improve generalization. Furthermore, we propose a novel data\naugmentation method that utilizes feature correlations within the proposed\nframework.\n  We evaluate our framework using two real-world datasets with 500M and 1B\ntraining examples respectively. Our results demonstrate the effectiveness of\nSSL regularization and show its superior performance over the state-of-the-art\nregularization techniques. We also have already launched the proposed\ntechniques to a web-scale commercial app-to-app recommendation system, with\nsignificant improvements top-tier business metrics demonstrated in A/B\nexperiments on live traffic. Our online results also verify our hypothesis that\nour framework indeed improves model performance even more on slices that lack\nsupervision.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 06:21:43 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 06:35:35 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 08:10:20 GMT"}, {"version": "v4", "created": "Thu, 25 Feb 2021 02:50:58 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Yao", "Tiansheng", ""], ["Yi", "Xinyang", ""], ["Cheng", "Derek Zhiyuan", ""], ["Yu", "Felix", ""], ["Chen", "Ting", ""], ["Menon", "Aditya", ""], ["Hong", "Lichan", ""], ["Chi", "Ed H.", ""], ["Tjoa", "Steve", ""], ["Kang", "Jieqi", ""], ["Ettinger", "Evan", ""]]}, {"id": "2007.12929", "submitter": "Christian Meurisch", "authors": "Bekir Bayrak, Florian Giger, Christian Meurisch", "title": "Insightful Assistant: AI-compatible Operation Graph Representations for\n  Enhancing Industrial Conversational Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in voice-controlled assistants paved the way into the consumer\nmarket. For professional or industrial use, the capabilities of such assistants\nare too limited or too time-consuming to implement due to the higher complexity\nof data, possible AI-based operations, and requests. In the light of these\ndeficits, this paper presents Insightful Assistant---a pipeline concept based\non a novel operation graph representation resulting from the intents detected.\nUsing a predefined set of semantically annotated (executable) functions, each\nnode of the operation graph is assigned to a function for execution. Besides\nbasic operations, such functions can contain artificial intelligence (AI) based\noperations (e.g., anomaly detection). The result is then visualized to the user\naccording to type and extracted user preferences in an automated way. We\nfurther collected a unique crowd-sourced set of 869 requests, each with four\ndifferent variants expected visualization, for an industrial dataset. The\nevaluation of our proof-of-concept prototype on this dataset shows its\nfeasibility: it achieves an accuracy of up to 95.0% (74.5%) for simple\n(complex) request detection with different variants and a top3-accuracy up to\n95.4% for data-/user-adaptive visualization.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 13:46:58 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Bayrak", "Bekir", ""], ["Giger", "Florian", ""], ["Meurisch", "Christian", ""]]}, {"id": "2007.12986", "submitter": "Praveen Chandar", "authors": "James McInerney, Brian Brost, Praveen Chandar, Rishabh Mehrotra, Ben\n  Carterette", "title": "Counterfactual Evaluation of Slate Recommendations with Sequential\n  Reward Interactions", "comments": null, "journal-ref": null, "doi": "10.1145/3394486.3403229", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users of music streaming, video streaming, news recommendation, and\ne-commerce services often engage with content in a sequential manner. Providing\nand evaluating good sequences of recommendations is therefore a central problem\nfor these services. Prior reweighting-based counterfactual evaluation methods\neither suffer from high variance or make strong independence assumptions about\nrewards. We propose a new counterfactual estimator that allows for sequential\ninteractions in the rewards with lower variance in an asymptotically unbiased\nmanner. Our method uses graphical assumptions about the causal relationships of\nthe slate to reweight the rewards in the logging policy in a way that\napproximates the expected sum of rewards under the target policy. Extensive\nexperiments in simulation and on a live recommender system show that our\napproach outperforms existing methods in terms of bias and data efficiency for\nthe sequential track recommendations problem.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 17:58:01 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 01:34:40 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["McInerney", "James", ""], ["Brost", "Brian", ""], ["Chandar", "Praveen", ""], ["Mehrotra", "Rishabh", ""], ["Carterette", "Ben", ""]]}, {"id": "2007.13019", "submitter": "Masoud Mansoury", "authors": "Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad\n  Mobasher, Robin Burke", "title": "Feedback Loop and Bias Amplification in Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation algorithms are known to suffer from popularity bias; a few\npopular items are recommended frequently while the majority of other items are\nignored. These recommendations are then consumed by the users, their reaction\nwill be logged and added to the system: what is generally known as a feedback\nloop. In this paper, we propose a method for simulating the users interaction\nwith the recommenders in an offline setting and study the impact of feedback\nloop on the popularity bias amplification of several recommendation algorithms.\nWe then show how this bias amplification leads to several other problems such\nas declining the aggregate diversity, shifting the representation of users'\ntaste over time and also homogenization of the users experience. In particular,\nwe show that the impact of feedback loop is generally stronger for the users\nwho belong to the minority group.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 21:59:54 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Mansoury", "Masoud", ""], ["Abdollahpouri", "Himan", ""], ["Pechenizkiy", "Mykola", ""], ["Mobasher", "Bamshad", ""], ["Burke", "Robin", ""]]}, {"id": "2007.13027", "submitter": "Manar Samad", "authors": "Manar D. Samad, Nalin D. Khounviengxay, Megan A. Witherow", "title": "Effect of Text Processing Steps on Twitter Sentiment Classification\n  using Word Embedding", "comments": "14 pages, 3 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Processing of raw text is the crucial first step in text classification and\nsentiment analysis. However, text processing steps are often performed using\noff-the-shelf routines and pre-built word dictionaries without optimizing for\ndomain, application, and context. This paper investigates the effect of seven\ntext processing scenarios on a particular text domain (Twitter) and application\n(sentiment classification). Skip gram-based word embeddings are developed to\ninclude Twitter colloquial words, emojis, and hashtag keywords that are often\nremoved for being unavailable in conventional literature corpora. Our\nexperiments reveal negative effects on sentiment classification of two common\ntext processing steps: 1) stop word removal and 2) averaging of word vectors to\nrepresent individual tweets. New effective steps for 1) including non-ASCII\nemoji characters, 2) measuring word importance from word embedding, 3)\naggregating word vectors into a tweet embedding, and 4) developing linearly\nseparable feature space have been proposed to optimize the sentiment\nclassification pipeline. The best combination of text processing steps yields\nthe highest average area under the curve (AUC) of 88.4 (+/-0.4) in classifying\n14,640 tweets with three sentiment labels. Word selection from context-driven\nword embedding reveals that only the ten most important words in Tweets\ncumulatively yield over 98% of the maximum accuracy. Results demonstrate a\nmeans for data-driven selection of important words in tweet classification as\nopposed to using pre-built word dictionaries. The proposed tweet embedding is\nrobust to and alleviates the need for several text processing steps.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 22:44:00 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Samad", "Manar D.", ""], ["Khounviengxay", "Nalin D.", ""], ["Witherow", "Megan A.", ""]]}, {"id": "2007.13058", "submitter": "Jia Su", "authors": "Jia Su, Yi Guan, Yuge Li, Weile Chen, He Lv, Yageng Yan", "title": "Do recommender systems function in the health domain: a system review", "comments": "32 pages, 1 table, 1 figure, 38 discussed articles", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have fulfilled an important role in everyday life.\nRecommendations such as news by Google, videos by Netflix, goods by e-commerce\nproviders, etc. have heavily changed everyones lifestyle. Health domains\ncontain similar decision-making problems such as what to eat, how to exercise,\nand what is the proper medicine for a patient. Recently, studies focused on\nrecommender systems to solve health problems have attracted attention. In this\npaper, we review aspects of health recommender systems including interests,\nmethods, evaluation, future challenges and trend issues. We find that 1) health\nrecommender systems have their own health concern limitations that cause them\nto focus on less-risky recommendations such as diet recommendation; 2)\ntraditional recommender methods such as content-based and collaborative\nfiltering methods can hardly handle health constraints, but knowledge-based\nmethods function more than ever; 3) evaluating a health recommendation is more\ncomplicated than evaluating a commercial one because multiple dimensions in\naddition to accuracy should be considered. Recommender systems can function\nwell in the health domain after the solution of several key problems. Our work\nis a systematic review of health recommender system studies, we show current\nconditions and future directions. It is believed that this review will help\ndomain researchers and promote health recommender systems to the next step.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 04:58:47 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Su", "Jia", ""], ["Guan", "Yi", ""], ["Li", "Yuge", ""], ["Chen", "Weile", ""], ["Lv", "He", ""], ["Yan", "Yageng", ""]]}, {"id": "2007.13087", "submitter": "Amit Livne", "authors": "Amit Livne, Roy Dor, Eyal Mazuz, Tamar Didi, Bracha Shapira, and Lior\n  Rokach", "title": "Iterative Boosting Deep Neural Networks for Predicting Click-Through\n  Rate", "comments": "16 Pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The click-through rate (CTR) reflects the ratio of clicks on a specific item\nto its total number of views. It has significant impact on websites'\nadvertising revenue. Learning sophisticated models to understand and predict\nuser behavior is essential for maximizing the CTR in recommendation systems.\nRecent works have suggested new methods that replace the expensive and\ntime-consuming feature engineering process with a variety of deep learning (DL)\nclassifiers capable of capturing complicated patterns from raw data; these\nmethods have shown significant improvement on the CTR prediction task. While DL\ntechniques can learn intricate user behavior patterns, it relies on a vast\namount of data and does not perform as well when there is a limited amount of\ndata. We propose XDBoost, a new DL method for capturing complex patterns that\nrequires just a limited amount of raw data. XDBoost is an iterative three-stage\nneural network model influenced by the traditional machine learning boosting\nmechanism. XDBoost's components operate sequentially similar to boosting;\nHowever, unlike conventional boosting, XDBoost does not sum the predictions\ngenerated by its components. Instead, it utilizes these predictions as new\nartificial features and enhances CTR prediction by retraining the model using\nthese features. Comprehensive experiments conducted to illustrate the\neffectiveness of XDBoost on two datasets demonstrated its ability to outperform\nexisting state-of-the-art (SOTA) models for CTR prediction.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 09:41:16 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Livne", "Amit", ""], ["Dor", "Roy", ""], ["Mazuz", "Eyal", ""], ["Didi", "Tamar", ""], ["Shapira", "Bracha", ""], ["Rokach", "Lior", ""]]}, {"id": "2007.13159", "submitter": "Yash Goyal", "authors": "Aayush Surana, Yash Goyal, Manish Shrivastava, Suvi Saarikallio, Vinoo\n  Alluri", "title": "Tag2Risk: Harnessing Social Music Tags for Characterizing Depression\n  Risk", "comments": "Appearing in the proceedings of ISMIR 2020. Aayush Surana and Yash\n  Goyal contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Musical preferences have been considered a mirror of the self. In this age of\nBig Data, online music streaming services allow us to capture ecologically\nvalid music listening behavior and provide a rich source of information to\nidentify several user-specific aspects. Studies have shown musical engagement\nto be an indirect representation of internal states including internalized\nsymptomatology and depression. The current study aims at unearthing patterns\nand trends in the individuals at risk for depression as it manifests in\nnaturally occurring music listening behavior. Mental well-being scores, musical\nengagement measures, and listening histories of Last.fm users (N=541) were\nacquired. Social tags associated with each listener's most popular tracks were\nanalyzed to unearth the mood/emotions and genres associated with the users.\nResults revealed that social tags prevalent in the users at risk for depression\nwere predominantly related to emotions depicting Sadness associated with genre\ntags representing neo-psychedelic-, avant garde-, dream-pop. This study will\nopen up avenues for an MIR-based approach to characterizing and predicting risk\nfor depression which can be helpful in early detection and additionally provide\nbases for designing music recommendations accordingly.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 16:02:10 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Surana", "Aayush", ""], ["Goyal", "Yash", ""], ["Shrivastava", "Manish", ""], ["Saarikallio", "Suvi", ""], ["Alluri", "Vinoo", ""]]}, {"id": "2007.13207", "submitter": "Yikun Xian", "authors": "Yikun Xian, Zuohui Fu, Qiaoying Huang, S. Muthukrishnan, Yongfeng\n  Zhang", "title": "Neural-Symbolic Reasoning over Knowledge Graph for Multi-stage\n  Explainable Recommendation", "comments": "Accepted at AAAI 2020 Workshop on Deep Learning on Graphs:\n  Methodologies and Applications (DLGMA 20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on recommender systems has considered external knowledge graphs\nas valuable sources of information, not only to produce better recommendations\nbut also to provide explanations of why the recommended items were chosen. Pure\nrule-based symbolic methods provide a transparent reasoning process over\nknowledge graph but lack generalization ability to unseen examples, while deep\nlearning models enhance powerful feature representation ability but are hard to\ninterpret. Moreover, direct reasoning over large-scale knowledge graph can be\ncostly due to the huge search space of pathfinding. We approach the problem\nthrough a novel coarse-to-fine neural symbolic reasoning method called NSER. It\nfirst generates a coarse-grained explanation to capture abstract user\nbehavioral pattern, followed by a fined-grained explanation accompanying with\nexplicit reasoning paths and recommendations inferred from knowledge graph. We\nextensively experiment on four real-world datasets and observe substantial\ngains of recommendation performance compared with state-of-the-art methods as\nwell as more diversified explanations in different granularity.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 20:10:37 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Xian", "Yikun", ""], ["Fu", "Zuohui", ""], ["Huang", "Qiaoying", ""], ["Muthukrishnan", "S.", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2007.13237", "submitter": "Zaiqiao Meng", "authors": "Zaiqiao Meng, Richard McCreadie, Craig Macdonald, Iadh Ounis", "title": "Exploring Data Splitting Strategies for the Evaluation of Recommendation\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective methodologies for evaluating recommender systems are critical, so\nthat such systems can be compared in a sound manner. A commonly overlooked\naspect of recommender system evaluation is the selection of the data splitting\nstrategy. In this paper, we both show that there is no standard splitting\nstrategy and that the selection of splitting strategy can have a strong impact\non the ranking of recommender systems. In particular, we perform experiments\ncomparing three common splitting strategies, examining their impact over seven\nstate-of-the-art recommendation models for two datasets. Our results\ndemonstrate that the splitting strategy employed is an important confounding\nvariable that can markedly alter the ranking of state-of-the-art systems,\nmaking much of the currently published literature non-comparable, even when the\nsame dataset and metrics are used.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 22:47:01 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Meng", "Zaiqiao", ""], ["McCreadie", "Richard", ""], ["Macdonald", "Craig", ""], ["Ounis", "Iadh", ""]]}, {"id": "2007.13249", "submitter": "Peixian Chen", "authors": "Peixian Chen, Pingyang Dai, Jianzhuang Liu, Feng Zheng, Qi Tian,\n  Rongrong Ji", "title": "Dual Distribution Alignment Network for Generalizable Person\n  Re-Identification", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain generalization (DG) serves as a promising solution to handle person\nRe-Identification (Re-ID), which trains the model using labels from the source\ndomain alone, and then directly adopts the trained model to the target domain\nwithout model updating. However, existing DG approaches are usually disturbed\nby serious domain variations due to significant dataset variations.\nSubsequently, DG highly relies on designing domain-invariant features, which is\nhowever not well exploited, since most existing approaches directly mix\nmultiple datasets to train DG based models without considering the local\ndataset similarities, i.e., examples that are very similar but from different\ndomains. In this paper, we present a Dual Distribution Alignment Network\n(DDAN), which handles this challenge by mapping images into a domain-invariant\nfeature space by selectively aligning distributions of multiple source domains.\nSuch an alignment is conducted by dual-level constraints, i.e., the domain-wise\nadversarial feature learning and the identity-wise similarity enhancement. We\nevaluate our DDAN on a large-scale Domain Generalization Re-ID (DG Re-ID)\nbenchmark. Quantitative results demonstrate that the proposed DDAN can well\nalign the distributions of various source domains, and significantly\noutperforms all existing domain generalization approaches.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 00:08:07 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Chen", "Peixian", ""], ["Dai", "Pingyang", ""], ["Liu", "Jianzhuang", ""], ["Zheng", "Feng", ""], ["Tian", "Qi", ""], ["Ji", "Rongrong", ""]]}, {"id": "2007.13255", "submitter": "Milad Asgari Mehrabadi", "authors": "Milad Asgari Mehrabadi, Nikil Dutt and Amir M. Rahmani", "title": "The Causality Inference of Public Interest in Restaurants and Bars on\n  COVID-19 Daily Cases in the US: A Google Trends Analysis", "comments": "6 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 coronavirus pandemic has affected virtually every region of the\nglobe. At the time of conducting this study, the number of daily cases in the\nUnited States is more than any other country, and the trend is increasing in\nmost of its states. Google trends provide public interest in various topics\nduring different periods. Analyzing these trends using data mining methods\nmight provide useful insights and observations regarding the COVID-19 outbreak.\nThe objective of this study was to consider the predictive ability of different\nsearch terms (i.e., bars and restaurants) with regards to the increase of daily\ncases in the US. We considered the causation of two different search query\ntrends, namely restaurant and bars, on daily positive cases in top-10\nstates/territories of the United States with the highest and lowest daily new\npositive cases. In addition, to measure the linear relation of different\ntrends, we used Pearson correlation. Our results showed for states/territories\nwith higher numbers of daily cases, the historical trends in search queries\nrelated to bars and restaurants, which mainly happened after re-opening,\nsignificantly affect the daily new cases, on average. California, for example,\nhad most searches for restaurants on June 7th, 2020, which affected the number\nof new cases within two weeks after the peak with the P-value of .004 for\nGranger's causality test. Although a limited number of search queries were\nconsidered, Google search trends for restaurants and bars showed a significant\neffect on daily new cases for regions with higher numbers of daily new cases in\nthe United States. We showed that such influential search trends could be used\nas additional information for prediction tasks in new cases of each region.\nThis prediction can help healthcare leaders manage and control the impact of\nCOVID-19 outbreaks on society and be prepared for the outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 00:29:06 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Mehrabadi", "Milad Asgari", ""], ["Dutt", "Nikil", ""], ["Rahmani", "Amir M.", ""]]}, {"id": "2007.13264", "submitter": "Peixian Chen", "authors": "Pingyang Dai, Peixian Chen, Qiong Wu, Xiaopeng Hong, Qixiang Ye, Qi\n  Tian, Rongrong Ji", "title": "Learning Task-oriented Disentangled Representations for Unsupervised\n  Domain Adaptation", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims to address the domain-shift problem\nbetween a labeled source domain and an unlabeled target domain. Many efforts\nhave been made to address the mismatch between the distributions of training\nand testing data, but unfortunately, they ignore the task-oriented information\nacross domains and are inflexible to perform well in complicated open-set\nscenarios. Many efforts have been made to eliminate the mismatch between the\ndistributions of training and testing data by learning domain-invariant\nrepresentations. However, the learned representations are usually not\ntask-oriented, i.e., being class-discriminative and domain-transferable\nsimultaneously. This drawback limits the flexibility of UDA in complicated\nopen-set tasks where no labels are shared between domains. In this paper, we\nbreak the concept of task-orientation into task-relevance and task-irrelevance,\nand propose a dynamic task-oriented disentangling network (DTDN) to learn\ndisentangled representations in an end-to-end fashion for UDA. The dynamic\ndisentangling network effectively disentangles data representations into two\ncomponents: the task-relevant ones embedding critical information associated\nwith the task across domains, and the task-irrelevant ones with the remaining\nnon-transferable or disturbing information. These two components are\nregularized by a group of task-specific objective functions across domains.\nSuch regularization explicitly encourages disentangling and avoids the use of\ngenerative models or decoders. Experiments in complicated, open-set scenarios\n(retrieval tasks) and empirical benchmarks (classification tasks) demonstrate\nthat the proposed method captures rich disentangled information and achieves\nsuperior performance.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 01:21:18 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Dai", "Pingyang", ""], ["Chen", "Peixian", ""], ["Wu", "Qiong", ""], ["Hong", "Xiaopeng", ""], ["Ye", "Qixiang", ""], ["Tian", "Qi", ""], ["Ji", "Rongrong", ""]]}, {"id": "2007.13273", "submitter": "Xiaomeng Wang", "authors": "Xiaomeng Wang, Yijun Ran and Tao Jia", "title": "Measuring similarity in co-occurrence data using ego-networks", "comments": "9 pages, 6 figures", "journal-ref": "Chaos: An Interdisciplinary Journal of Nonlinear Science 30 (2020)\n  013101", "doi": "10.1063/1.5129036", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The co-occurrence association is widely observed in many empirical data.\nMining the information in co-occurrence data is essential for advancing our\nunderstanding of systems such as social networks, ecosystem, and brain network.\nMeasuring similarity of entities is one of the important tasks, which can\nusually be achieved using a network-based approach. Here we show that\ntraditional methods based on the aggregated network can bring unwanted\nin-directed relationship. To cope with this issue, we propose a similarity\nmeasure based on the ego network of each entity, which effectively considers\nthe change of an entity's centrality from one ego network to another. The index\nproposed is easy to calculate and has a clear physical meaning. Using two\ndifferent data sets, we compare the new index with other existing ones. We find\nthat the new index outperforms the traditional network-based similarity\nmeasures, and it can sometimes surpass the embedding method. In the meanwhile,\nthe measure by the new index is weakly correlated with those by other methods,\nhence providing a different dimension to quantify similarities in co-occurrence\ndata. Altogether, our work makes an extension in the network-based similarity\nmeasure and can be potentially applied in several related tasks.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 02:13:44 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Wang", "Xiaomeng", ""], ["Ran", "Yijun", ""], ["Jia", "Tao", ""]]}, {"id": "2007.13280", "submitter": "Pan Li", "authors": "Pan Li and Alexander Tuzhilin", "title": "Latent Unexpected Recommendations", "comments": "Accepted at ACM TIST", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unexpected recommender system constitutes an important tool to tackle the\nproblem of filter bubbles and user boredom, which aims at providing unexpected\nand satisfying recommendations to target users at the same time. Previous\nunexpected recommendation methods only focus on the straightforward relations\nbetween current recommendations and user expectations by modeling\nunexpectedness in the feature space, thus resulting in the loss of accuracy\nmeasures in order to improve unexpectedness performance. Contrast to these\nprior models, we propose to model unexpectedness in the latent space of user\nand item embeddings, which allows to capture hidden and complex relations\nbetween new recommendations and historic purchases. In addition, we develop a\nnovel Latent Closure (LC) method to construct hybrid utility function and\nprovide unexpected recommendations based on the proposed model. Extensive\nexperiments on three real-world datasets illustrate superiority of our proposed\napproach over the state-of-the-art unexpected recommendation models, which\nleads to significant increase in unexpectedness measure without sacrificing any\naccuracy metric under all experimental settings in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 02:39:30 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Li", "Pan", ""], ["Tuzhilin", "Alexander", ""]]}, {"id": "2007.13287", "submitter": "Zahra Nazari", "authors": "Zahra Nazari, Christophe Charbuillet, Johan Pages, Martin Laurent,\n  Denis Charrier, Briana Vecchione, Ben Carterette", "title": "Recommending Podcasts for Cold-Start Users Based on Music Listening and\n  Taste", "comments": "SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401101", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are increasingly used to predict and serve content that\naligns with user taste, yet the task of matching new users with relevant\ncontent remains a challenge. We consider podcasting to be an emerging medium\nwith rapid growth in adoption, and discuss challenges that arise when applying\ntraditional recommendation approaches to address the cold-start problem. Using\nmusic consumption behavior, we examine two main techniques in inferring Spotify\nusers preferences over more than 200k podcasts. Our results show significant\nimprovements in consumption of up to 50\\% for both offline and online\nexperiments. We provide extensive analysis on model performance and examine the\ndegree to which music data as an input source introduces bias in\nrecommendations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 02:55:23 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Nazari", "Zahra", ""], ["Charbuillet", "Christophe", ""], ["Pages", "Johan", ""], ["Laurent", "Martin", ""], ["Charrier", "Denis", ""], ["Vecchione", "Briana", ""], ["Carterette", "Ben", ""]]}, {"id": "2007.13316", "submitter": "Ye Bi", "authors": "Ye Bi, Liqiang Song, Mengqiu Yao, Zhenyu Wu, Jianming Wang, Jing Xiao", "title": "DCDIR: A Deep Cross-Domain Recommendation System for Cold Start Users in\n  Insurance Domain", "comments": "5 pages, accepted in SIGIR20 Short Paper Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet insurance products are apparently different from traditional\ne-commerce goods for their complexity, low purchasing frequency, etc.So, cold\nstart problem is even worse. In traditional e-commerce field, several\ncross-domain recommendation (CDR) methods have been studied to infer\npreferences of cold start users based on their preferences in other domains.\nHowever, these CDR methods could not be applied into insurance domain directly\ndue to product complexity. In this paper, we propose a Deep Cross Domain\nInsurance Recommendation System (DCDIR) for cold start users. Specifically, we\nfirst learn more effective user and item latent features in both domains. In\ntarget domain, given the complexity of insurance products, we design meta path\nbased method over insurance product knowledge graph. In source domain, we\nemploy GRU to model user dynamic interests. Then we learn a feature mapping\nfunction by multi-layer perceptions. We apply DCDIR on our company datasets,\nand show DCDIR significantly outperforms the state-of-the-art solutions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 06:07:24 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Bi", "Ye", ""], ["Song", "Liqiang", ""], ["Yao", "Mengqiu", ""], ["Wu", "Zhenyu", ""], ["Wang", "Jianming", ""], ["Xiao", "Jing", ""]]}, {"id": "2007.13339", "submitter": "Hamada Nayel", "authors": "Hamada A. Nayel", "title": "NAYEL at SemEval-2020 Task 12: TF/IDF-Based Approach for Automatic\n  Offensive Language Detection in Arabic Tweets", "comments": "Working notes of NAYEL's team submission to task 12 at SemEval-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present the system submitted to \"SemEval-2020 Task 12\". The\nproposed system aims at automatically identify the Offensive Language in Arabic\nTweets. A machine learning based approach has been used to design our system.\nWe implemented a linear classifier with Stochastic Gradient Descent (SGD) as\noptimization algorithm. Our model reported 84.20%, 81.82% f1-score on\ndevelopment set and test set respectively. The best performed system and the\nsystem in the last rank reported 90.17% and 44.51% f1-score on test set\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 07:44:00 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Nayel", "Hamada A.", ""]]}, {"id": "2007.13440", "submitter": "Niall Twomey", "authors": "Niall Twomey, Mikhail Fain, Andrey Ponikar, Nadine Sarraf", "title": "Towards Multi-Language Recipe Personalisation and Recommendation", "comments": "5 tables", "journal-ref": "Fourteenth ACM Conference on Recommender Systems (RecSys 2020)", "doi": "10.1145/3383313.3418478", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-language recipe personalisation and recommendation is an under-explored\nfield of information retrieval in academic and production systems. The existing\ngaps in our current understanding are numerous, even on fundamental questions\nsuch as whether consistent and high-quality recipe recommendation can be\ndelivered across languages. In this paper, we introduce the multi-language\nrecipe recommendation setting and present grounding results that will help to\nestablish the potential and absolute value of future work in this area. Our\nwork draws on several billion events from millions of recipes and users from\nArabic, English, Indonesian, Russian, and Spanish. We represent recipes using a\ncombination of normalised ingredients, standardised skills and image embeddings\nobtained without human intervention. In modelling, we take a classical approach\nbased on optimising an embedded bi-linear user-item metric space towards the\ninteractions that most strongly elicit cooking intent. For users without\ninteraction histories, a bespoke content-based cold-start model that predicts\ncontext and recipe affinity is introduced. We show that our approach to\npersonalisation is stable and easily scales to new languages. A robust\ncross-validation campaign is employed and consistently rejects baseline models\nand representations, strongly favouring those we propose. Our results are\npresented in a language-oriented (as opposed to model-oriented) fashion to\nemphasise the language-based goals of this work. We believe that this is the\nfirst large-scale work that comprehensively considers the value and potential\nof multi-language recipe recommendation and personalisation as well as\ndelivering scalable and reliable models.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 11:26:49 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 10:57:33 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Twomey", "Niall", ""], ["Fain", "Mikhail", ""], ["Ponikar", "Andrey", ""], ["Sarraf", "Nadine", ""]]}, {"id": "2007.13520", "submitter": "Arindam Pal", "authors": "Jagriti Jalal, Mayank Singh, Arindam Pal, Lipika Dey, Animesh\n  Mukherjee", "title": "Identification, Tracking and Impact: Understanding the trade secret of\n  catchphrases", "comments": "To be published in the proceedings of the ACM/IEEE Joint Conference\n  on Digital Libraries (JCDL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the topical evolution in industrial innovation is a challenging\nproblem. With the advancement in the digital repositories in the form of patent\ndocuments, it is becoming increasingly more feasible to understand the\ninnovation secrets -- \"catchphrases\" of organizations. However, searching and\nunderstanding this enormous textual information is a natural bottleneck. In\nthis paper, we propose an unsupervised method for the extraction of\ncatchphrases from the abstracts of patents granted by the U.S. Patent and\nTrademark Office over the years. Our proposed system achieves substantial\nimprovement, both in terms of precision and recall, against state-of-the-art\ntechniques. As a second objective, we conduct an extensive empirical study to\nunderstand the temporal evolution of the catchphrases across various\norganizations. We also show how the overall innovation evolution in the form of\nintroduction of newer catchphrases in an organization's patents correlates with\nthe future citations received by the patents filed by that organization. Our\ncode and data sets will be placed in the public domain soon.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 06:11:25 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Jalal", "Jagriti", ""], ["Singh", "Mayank", ""], ["Pal", "Arindam", ""], ["Dey", "Lipika", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "2007.13534", "submitter": "Longbing Cao", "authors": "Longbing Cao", "title": "Coupling Learning of Complex Interactions", "comments": null, "journal-ref": "Journal of Information Processing and Management, 51(2): 167-186\n  (2015)", "doi": "10.1016/j.ipm.2014.08.007", "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex applications such as big data analytics involve different forms of\ncoupling relationships that reflect interactions between factors related to\ntechnical, business (domain-specific) and environmental (including\nsocio-cultural and economic) aspects. There are diverse forms of couplings\nembedded in poor-structured and ill-structured data. Such couplings are\nubiquitous, implicit and/or explicit, objective and/or subjective,\nheterogeneous and/or homogeneous, presenting complexities to existing learning\nsystems in statistics, mathematics and computer sciences, such as typical\ndependency, association and correlation relationships. Modeling and learning\nsuch couplings thus is fundamental but challenging. This paper discusses the\nconcept of coupling learning, focusing on the involvement of coupling\nrelationships in learning systems. Coupling learning has great potential for\nbuilding a deep understanding of the essence of business problems and handling\nchallenges that have not been addressed well by existing learning theories and\ntools. This argument is verified by several case studies on coupling learning,\nincluding handling coupling in recommender systems, incorporating couplings\ninto coupled clustering, coupling document clustering, coupled recommender\nalgorithms and coupled behavior analysis for groups.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 11:04:25 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Cao", "Longbing", ""]]}, {"id": "2007.13626", "submitter": "Alymzhan Toleu", "authors": "Gulmira Tolegen, Alymzhan Toleu, Orken Mamyrbayev and Rustam\n  Mussabayev", "title": "Neural Named Entity Recognition for Kazakh", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present several neural networks to address the task of named entity\nrecognition for morphologically complex languages (MCL). Kazakh is a\nmorphologically complex language in which each root/stem can produce hundreds\nor thousands of variant word forms. This nature of the language could lead to a\nserious data sparsity problem, which may prevent the deep learning models from\nbeing well trained for under-resourced MCLs. In order to model the MCLs' words\neffectively, we introduce root and entity tag embedding plus tensor layer to\nthe neural networks. The effects of those are significant for improving NER\nmodel performance of MCLs. The proposed models outperform state-of-the-art\nincluding character-based approaches, and can be potentially applied to other\nmorphologically complex languages.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 16:45:22 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Tolegen", "Gulmira", ""], ["Toleu", "Alymzhan", ""], ["Mamyrbayev", "Orken", ""], ["Mussabayev", "Rustam", ""]]}, {"id": "2007.13829", "submitter": "Mayank Kejriwal", "authors": "Ravi Kiran Selvam and Mayank Kejriwal", "title": "On using Product-Specific Schema.org from Web Data Commons: An Empirical\n  Set of Best Practices", "comments": "8 pages, 3 tables, 6 figures, published in Workshop on Knowledge\n  Graphs and E-Commerce at KDD 2020 (non-archival)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schema.org has experienced high growth in recent years. Structured\ndescriptions of products embedded in HTML pages are now not uncommon,\nespecially on e-commerce websites. The Web Data Commons (WDC) project has\nextracted schema.org data at scale from webpages in the Common Crawl and made\nit available as an RDF `knowledge graph' at scale. The portion of this data\nthat specifically describes products offers a golden opportunity for\nresearchers and small companies to leverage it for analytics and downstream\napplications. Yet, because of the broad and expansive scope of this data, it is\nnot evident whether the data is usable in its raw form. In this paper, we do a\ndetailed empirical study on the product-specific schema.org data made available\nby WDC. Rather than simple analysis, the goal of our study is to devise an\nempirically grounded set of best practices for using and consuming WDC\nproduct-specific schema.org data. Our studies reveal six best practices, each\nof which is justified by experimental data and analysis.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 19:46:48 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Selvam", "Ravi Kiran", ""], ["Kejriwal", "Mayank", ""]]}, {"id": "2007.13843", "submitter": "Tyler Tomita", "authors": "Tyler M. Tomita and Joshua T. Vogelstein", "title": "Robust Similarity and Distance Learning via Decision Forests", "comments": "Submitted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical distances such as Euclidean distance often fail to capture the\nappropriate relationships between items, subsequently leading to subpar\ninference and prediction. Many algorithms have been proposed for automated\nlearning of suitable distances, most of which employ linear methods to learn a\nglobal metric over the feature space. While such methods offer nice theoretical\nproperties, interpretability, and computationally efficient means for\nimplementing them, they are limited in expressive capacity. Methods which have\nbeen designed to improve expressiveness sacrifice one or more of the nice\nproperties of the linear methods. To bridge this gap, we propose a highly\nexpressive novel decision forest algorithm for the task of distance learning,\nwhich we call Similarity and Metric Random Forests (SMERF). We show that the\ntree construction procedure in SMERF is a proper generalization of standard\nclassification and regression trees. Thus, the mathematical driving forces of\nSMERF are examined via its direct connection to regression forests, for which\ntheory has been developed. Its ability to approximate arbitrary distances and\nidentify important features is empirically demonstrated on simulated data sets.\nLast, we demonstrate that it accurately predicts links in networks.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 20:17:42 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 15:41:38 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Tomita", "Tyler M.", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "2007.13861", "submitter": "Robert West", "authors": "Robert West", "title": "Calibration of Google Trends Time Series", "comments": "Proceedings of the 29th ACM Conference on Information and Knowledge\n  Management (CIKM), 2020", "journal-ref": null, "doi": "10.1145/3340531.3412075", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Google Trends is a tool that allows researchers to analyze the popularity of\nGoogle search queries across time and space. In a single request, users can\nobtain time series for up to 5 queries on a common scale, normalized to the\nrange from 0 to 100 and rounded to integer precision. Despite the overall value\nof Google Trends, rounding causes major problems, to the extent that entirely\nuninformative, all-zero time series may be returned for unpopular queries when\nrequested together with more popular queries. We address this issue by\nproposing Google Trends Anchor Bank (G-TAB), an efficient solution for the\ncalibration of Google Trends data. Our method expresses the popularity of an\narbitrary number of queries on a common scale without being compromised by\nrounding errors. The method proceeds in two phases. In the offline\npreprocessing phase, an \"anchor bank\" is constructed, a set of queries spanning\nthe full spectrum of popularity, all calibrated against a common reference\nquery by carefully chaining together multiple Google Trends requests. In the\nonline deployment phase, any given search query is calibrated by performing an\nefficient binary search in the anchor bank. Each search step requires one\nGoogle Trends request, but few steps suffice, as we demonstrate in an empirical\nevaluation. We make our code publicly available as an easy-to-use library at\nhttps://github.com/epfl-dlab/GoogleTrendsAnchorBank.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 20:54:33 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 05:58:27 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 16:31:54 GMT"}, {"version": "v4", "created": "Fri, 18 Sep 2020 12:44:40 GMT"}, {"version": "v5", "created": "Thu, 4 Feb 2021 15:03:13 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["West", "Robert", ""]]}, {"id": "2007.14018", "submitter": "Zhuoyi Lin", "authors": "Zhuoyi Lin, Lei Feng, Rui Yin, Chi Xu, and Chee-Keong Kwoh", "title": "GLIMG: Global and Local Item Graphs for Top-N Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based recommendation models work well for top-N recommender systems due\nto their capability to capture the potential relationships between entities.\nHowever, most of the existing methods only construct a single global item graph\nshared by all the users and regrettably ignore the diverse tastes between\ndifferent user groups. Inspired by the success of local models for\nrecommendation, this paper provides the first attempt to investigate multiple\nlocal item graphs along with a global item graph for graph-based recommendation\nmodels. We argue that recommendation on global and local graphs outperforms\nthat on a single global graph or multiple local graphs. Specifically, we\npropose a novel graph-based recommendation model named GLIMG (Global and Local\nIteM Graphs), which simultaneously captures both the global and local user\ntastes. By integrating the global and local graphs into an adapted\nsemi-supervised learning model, users' preferences on items are propagated\nglobally and locally. Extensive experimental results on real-world datasets\nshow that our proposed method consistently outperforms the state-of-the art\ncounterparts on the top-N recommendation task.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 06:50:28 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 06:34:49 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Lin", "Zhuoyi", ""], ["Feng", "Lei", ""], ["Yin", "Rui", ""], ["Xu", "Chi", ""], ["Kwoh", "Chee-Keong", ""]]}, {"id": "2007.14129", "submitter": "Zhuoyi Lin", "authors": "Zhuoyi Lin, Lei Feng, Xingzhi Guo, Rui Yin, Chee Keong Kwoh, Chi Xu", "title": "COMET: Convolutional Dimension Interaction for Deep Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent factor models play a dominant role among recommendation techniques.\nHowever, most of the existing latent factor models assume embedding dimensions\nare independent of each other, and thus regrettably ignore the interaction\ninformation across different embedding dimensions. In this paper, we propose a\nnovel latent factor model called COMET (COnvolutional diMEnsion inTeraction),\nwhich provides the first attempt to model higher-order interaction signals\namong all latent dimensions in an explicit manner. To be specific, COMET stacks\nthe embeddings of historical interactions horizontally, which results in two\n\"embedding maps\" that encode the original dimension information. In this way,\nusers' and items' internal interactions can be exploited by convolutional\nneural networks with kernels of different sizes and a fully-connected\nmulti-layer perceptron. Furthermore, the representations of users and items are\nenriched by the learnt interaction vectors, which can further be used to\nproduce the final prediction. Extensive experiments and ablation studies on\nvarious public implicit feedback datasets clearly demonstrate the effectiveness\nand the rationality of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 11:18:36 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 06:39:13 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Lin", "Zhuoyi", ""], ["Feng", "Lei", ""], ["Guo", "Xingzhi", ""], ["Yin", "Rui", ""], ["Kwoh", "Chee Keong", ""], ["Xu", "Chi", ""]]}, {"id": "2007.14271", "submitter": "Craig Macdonald", "authors": "Craig Macdonald and Nicola Tonellotto", "title": "Declarative Experimentation in Information Retrieval using PyTerrier", "comments": null, "journal-ref": "2020 ACM SIGIR International Conference on the Theory of\n  Information Retrieval (ICTIR '20)", "doi": "10.1145/3409256.3409829", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of deep machine learning platforms such as Tensorflow and Pytorch,\ndeveloped in expressive high-level languages such as Python, have allowed more\nexpressive representations of deep neural network architectures. We argue that\nsuch a powerful formalism is missing in information retrieval (IR), and propose\na framework called PyTerrier that allows advanced retrieval pipelines to be\nexpressed, and evaluated, in a declarative manner close to their conceptual\ndesign. Like the aforementioned frameworks that compile deep learning\nexperiments into primitive GPU operations, our framework targets IR platforms\nas backends in order to execute and evaluate retrieval pipelines. Further, we\ncan automatically optimise the retrieval pipelines to increase their efficiency\nto suite a particular IR platform backend. Our experiments, conducted on TREC\nRobust and ClueWeb09 test collections, demonstrate the efficiency benefits of\nthese optimisations for retrieval pipelines involving both the Anserini and\nTerrier IR platforms.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 14:36:29 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Macdonald", "Craig", ""], ["Tonellotto", "Nicola", ""]]}, {"id": "2007.14560", "submitter": "Vishal Kaushal", "authors": "Vishal Kaushal, Suraj Kothawade, Rishabh Iyer, Ganesh Ramakrishnan", "title": "Realistic Video Summarization through VISIOCITY: A New Benchmark and\n  Evaluation Framework", "comments": "19 pages, 1 figure, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic video summarization is still an unsolved problem due to several\nchallenges. We take steps towards making automatic video summarization more\nrealistic by addressing them. Firstly, the currently available datasets either\nhave very short videos or have few long videos of only a particular type. We\nintroduce a new benchmarking dataset VISIOCITY which comprises of longer videos\nacross six different categories with dense concept annotations capable of\nsupporting different flavors of video summarization and can be used for other\nvision problems. Secondly, for long videos, human reference summaries are\ndifficult to obtain. We present a novel recipe based on pareto optimality to\nautomatically generate multiple reference summaries from indirect ground truth\npresent in VISIOCITY. We show that these summaries are at par with human\nsummaries. Thirdly, we demonstrate that in the presence of multiple ground\ntruth summaries (due to the highly subjective nature of the task), learning\nfrom a single combined ground truth summary using a single loss function is not\na good idea. We propose a simple recipe VISIOCITY-SUM to enhance an existing\nmodel using a combination of losses and demonstrate that it beats the current\nstate of the art techniques when tested on VISIOCITY. We also show that a\nsingle measure to evaluate a summary, as is the current typical practice, falls\nshort. We propose a framework for better quantitative assessment of summary\nquality which is closer to human judgment than a single measure, say F1. We\nreport the performance of a few representative techniques of video\nsummarization on VISIOCITY assessed using various measures and bring out the\nlimitation of the techniques and/or the assessment mechanism in modeling human\njudgment and demonstrate the effectiveness of our evaluation framework in doing\nso.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 02:44:35 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 09:42:26 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Kaushal", "Vishal", ""], ["Kothawade", "Suraj", ""], ["Iyer", "Rishabh", ""], ["Ramakrishnan", "Ganesh", ""]]}, {"id": "2007.14579", "submitter": "T.J. Tsai", "authors": "Daniel Yang and TJ Tsai", "title": "Camera-Based Piano Sheet Music Identification", "comments": "8 pages, 3 figures, 2 tables. Accepted paper at the International\n  Society for Music Information Retrieval Conference (ISMIR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a method for large-scale retrieval of piano sheet music\nimages. Our work differs from previous studies on sheet music retrieval in two\nways. First, we investigate the problem at a much larger scale than previous\nstudies, using all solo piano sheet music images in the entire IMSLP dataset as\na searchable database. Second, we use cell phone images of sheet music as our\ninput queries, which lends itself to a practical, user-facing application. We\nshow that a previously proposed fingerprinting method for sheet music retrieval\nis far too slow for a real-time application, and we diagnose its shortcomings.\nWe propose a novel hashing scheme called dynamic n-gram fingerprinting that\nsignificantly reduces runtime while simultaneously boosting retrieval accuracy.\nIn experiments on IMSLP data, our proposed method achieves a mean reciprocal\nrank of 0.85 and an average runtime of 0.98 seconds per query.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 03:55:27 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Yang", "Daniel", ""], ["Tsai", "TJ", ""]]}, {"id": "2007.14771", "submitter": "Konstantinos Demertzis", "authors": "Vasiliki Demertzi and Konstantinos Demertzis", "title": "A Hybrid Adaptive Educational eLearning Project based on Ontologies\n  Matching and Recommendation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation of teaching interventions in learning needs has received\nconsiderable attention, as the provision of the same educational conditions to\nall students, is pedagogically ineffective. In contrast, more effectively\nconsidered the pedagogical strategies that adapt to the real individual skills\nof the students. An important innovation in this direction is the Adaptive\nEducational Systems (AES) that support automatic modeling study and adjust the\nteaching content on educational needs and students' skills. Effective\nutilization of these educational approaches can be enhanced with Artificial\nIntelligence (AI) technologies in order to the substantive content of the web\nacquires structure and the published information is perceived by the search\nengines. This study proposes a novel Adaptive Educational eLearning System\n(AEeLS) that has the capacity to gather and analyze data from learning\nrepositories and to adapt these to the educational curriculum according to the\nstudent skills and experience. It is a novel hybrid machine learning system\nthat combines a Semi-Supervised Classification method for ontology matching and\na Recommendation Mechanism that uses a hybrid method from neighborhood-based\ncollaborative and content-based filtering techniques, in order to provide a\npersonalized educational environment for each student.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 12:13:19 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 04:32:09 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2020 08:09:41 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Demertzi", "Vasiliki", ""], ["Demertzis", "Konstantinos", ""]]}, {"id": "2007.14856", "submitter": "Donghuo Zeng", "authors": "Donghuo Zeng, Yi Yu, Keizo Oyama", "title": "Unsupervised Generative Adversarial Alignment Representation for Sheet\n  music, Audio and Lyrics", "comments": "5 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.IR cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sheet music, audio, and lyrics are three main modalities during writing a\nsong. In this paper, we propose an unsupervised generative adversarial\nalignment representation (UGAAR) model to learn deep discriminative\nrepresentations shared across three major musical modalities: sheet music,\nlyrics, and audio, where a deep neural network based architecture on three\nbranches is jointly trained. In particular, the proposed model can transfer the\nstrong relationship between audio and sheet music to audio-lyrics and\nsheet-lyrics pairs by learning the correlation in the latent shared subspace.\nWe apply CCA components of audio and sheet music to establish new ground truth.\nThe generative (G) model learns the correlation of two couples of transferred\npairs to generate new audio-sheet pair for a fixed lyrics to challenge the\ndiscriminative (D) model. The discriminative model aims at distinguishing the\ninput which is from the generative model or the ground truth. The two models\nsimultaneously train in an adversarial way to enhance the ability of deep\nalignment representation learning. Our experimental results demonstrate the\nfeasibility of our proposed UGAAR for alignment representation learning among\nsheet music, audio, and lyrics.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 14:18:15 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Zeng", "Donghuo", ""], ["Yu", "Yi", ""], ["Oyama", "Keizo", ""]]}, {"id": "2007.14906", "submitter": "Bingqing Yu", "authors": "Federico Bianchi, Jacopo Tagliabue, Bingqing Yu, Luca Bigon and Ciro\n  Greco", "title": "Fantastic Embeddings and How to Align Them: Zero-Shot Inference in a\n  Multi-Shop Scenario", "comments": "accepted at 2020 SIGIR Workshop On eCommerce", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenge of leveraging multiple embedding spaces\nfor multi-shop personalization, proving that zero-shot inference is possible by\ntransferring shopping intent from one website to another without manual\nintervention. We detail a machine learning pipeline to train and optimize\nembeddings within shops first, and support the quantitative findings with\nadditional qualitative insights. We then turn to the harder task of using\nlearned embeddings across shops: if products from different shops live in the\nsame vector space, user intent - as represented by regions in this space - can\nthen be transferred in a zero-shot fashion across websites. We propose and\nbenchmark unsupervised and supervised methods to \"travel\" between embedding\nspaces, each with its own assumptions on data quantity and quality. We show\nthat zero-shot personalization is indeed possible at scale by testing the\nshared embedding space with two downstream tasks, event prediction and\ntype-ahead suggestions. Finally, we curate a cross-shop anonymized embeddings\ndataset to foster an inclusive discussion of this important business scenario.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 13:46:23 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Bianchi", "Federico", ""], ["Tagliabue", "Jacopo", ""], ["Yu", "Bingqing", ""], ["Bigon", "Luca", ""], ["Greco", "Ciro", ""]]}, {"id": "2007.15084", "submitter": "Diyah Puspitaningrum", "authors": "Diyah Puspitaningrum", "title": "Improving Performance of Relation Extraction Algorithm via Leveled\n  Adversarial PCNN and Database Expansion", "comments": "6 pages", "journal-ref": "2019 7th Int. Conf. CITSM, Jakarta, Indonesia, 2019, pp. 1-6", "doi": "10.1109/CITSM47753.2019.8965423", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This study introduces database expansion using the Minimum Description Length\n(MDL) algorithm to expand the database for better relation extraction.\nDifferent from other previous relation extraction researches, our method\nimproves system performance by expanding data. The goal of database expansion,\ntogether with a robust deep learning classifier, is to diminish wrong labels\ndue to the incomplete or not found nature of relation instances in the relation\ndatabase (e.g., Freebase). The study uses a deep learning method (Piecewise\nConvolutional Neural Network or PCNN) as the base classifier of our proposed\napproach: the leveled adversarial attention neural networks (LATTADV-ATT). In\nthe database expansion process, the semantic entity identification is used to\nenlarge new instances using the most similar itemsets of the most common\npatterns of the data to get its pairs of entities. About the deep learning\nmethod, the use of attention of selective sentences in PCNN can reduce noisy\nsentences. Also, the use of adversarial perturbation training is useful to\nimprove the robustness of system performance. The performance even further is\nimproved using a combination of leveled strategy and database expansion. There\nare two issues: 1) database expansion method: rule generation by allowing step\nsizes on selected strong semantic of most similar itemsets with aims to find\nentity pair for generating instances, 2) a better classifier model for relation\nextraction. Experimental result has shown that the use of the database\nexpansion is beneficial. The MDL database expansion helps improvements in all\nmethods compared to the unexpanded method. The LATTADV-ATT performs as a good\nclassifier with high precision P@100=0.842 (at no expansion). It is even better\nwhile implemented on the expansion data with P@100=0.891 (at expansion factor\nk=7).\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 20:05:18 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Puspitaningrum", "Diyah", ""]]}, {"id": "2007.15091", "submitter": "Diyah Puspitaningrum", "authors": "Diyah Puspitaningrum, Julio Fernando, Edo Afriando, Ferzha Putra\n  Utama, Rina Rahmadini, and Y. Pinata", "title": "Finding Local Experts for Dynamic Recommendations Using Lazy Random Walk", "comments": "6 pages", "journal-ref": "CITSM, Jakarta, Indonesia, 2019, pp. 1-6", "doi": "10.1109/CITSM47753.2019.8965354", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Statistics based privacy-aware recommender systems make suggestions more\npowerful by extracting knowledge from the log of social contacts interactions,\nbut unfortunately, they are static. Moreover, advice from local experts\neffective in finding specific business categories in a particular area. We\npropose a dynamic recommender algorithm based on a lazy random walk that\nrecommends top-rank shopping places to potentially interested visitors. We\nconsider local authority and topical authority. The algorithm tested on\nFourSquare shopping data sets of 5 cities in Indonesia with k-steps of 5,7,9 of\n(lazy) random walks and compared the results with other state-of-the-art\nranking techniques. The results show that it can reach high score precisions\n(0.5, 0.37, and 0.26 respectively on precision at 1, precision at 3, and\nprecision at 5 for k=5). The algorithm also shows scalability concerning\nexecution time. The advantage of dynamicity is the database used to power the\nrecommender system; no need to be very frequently updated to produce a good\nrecommendation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 20:23:41 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Puspitaningrum", "Diyah", ""], ["Fernando", "Julio", ""], ["Afriando", "Edo", ""], ["Utama", "Ferzha Putra", ""], ["Rahmadini", "Rina", ""], ["Pinata", "Y.", ""]]}, {"id": "2007.15103", "submitter": "Aneeshan Sain", "authors": "Aneeshan Sain, Ayan Kumar Bhunia, Yongxin Yang, Tao Xiang, Yi-Zhe Song", "title": "Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image\n  Retrieval", "comments": "Accepted for ORAL presentation in BMVC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch as an image search query is an ideal alternative to text in capturing\nthe fine-grained visual details. Prior successes on fine-grained sketch-based\nimage retrieval (FG-SBIR) have demonstrated the importance of tackling the\nunique traits of sketches as opposed to photos, e.g., temporal vs. static,\nstrokes vs. pixels, and abstract vs. pixel-perfect. In this paper, we study a\nfurther trait of sketches that has been overlooked to date, that is, they are\nhierarchical in terms of the levels of detail -- a person typically sketches up\nto various extents of detail to depict an object. This hierarchical structure\nis often visually distinct. In this paper, we design a novel network that is\ncapable of cultivating sketch-specific hierarchies and exploiting them to match\nsketch with photo at corresponding hierarchical levels. In particular, features\nfrom a sketch and a photo are enriched using cross-modal co-attention, coupled\nwith hierarchical node fusion at every level to form a better embedding space\nto conduct retrieval. Experiments on common benchmarks show our method to\noutperform state-of-the-arts by a significant margin.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 20:50:25 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 17:14:49 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Sain", "Aneeshan", ""], ["Bhunia", "Ayan Kumar", ""], ["Yang", "Yongxin", ""], ["Xiang", "Tao", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2007.15104", "submitter": "Diyah Puspitaningrum", "authors": "Diyah Puspitaningrum", "title": "Social Influences in Recommendation Systems", "comments": "6 pages", "journal-ref": "2019 7th CITSM Int. Conf., Jakarta, Indonesia, 2019, pp. 1-6", "doi": "10.1109/CITSM47753.2019.8965336", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Social networking sites such as Flickr and Facebook allow users to share\ncontent with family, friends, and interest groups. Also, tags can often assign\nto resources. In the previous research using few association rules FAR, we have\nseen that high-quality and efficient association-based tag recommendation is\npossible, but the set-up that we considered was very generic and did not take\nsocial information into account. The proposed method in the previous paper,\nFAR, in particular, exhibited a favorable trade-off between recommendation\nquality and runtime. Unfortunately, recommendation quality is unlikely to be\noptimal because the algorithms are not aware of any social information that may\nbe available. Two proposed approaches take a more social view on tag\nrecommendation regarding the issue: social contact variants and social groups\nof interest. The user data is varied and used as a source of associations. The\nadoption of social contact variants has two approaches. The first social\nvariant is User-centered Knowledge, to contrast Collective Knowledge. It\nimproves tag recommendation by grouping historic tag data according to friend\nrelationships and interests. The second variant is dubbed 'social batched\npersonomy' and attempts to address both quality and scalability issues by\nprocessing queries in batches instead of individually, such as done in a\nconventional personomy approach. For the social group of interest, 'community\nbatched personomy' is proposed to provide better accuracy groups of\nrecommendation systems in contrast also to Collective Knowledge. By taking\nsocial information into account can enhance the performance of recommendation\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 20:52:11 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Puspitaningrum", "Diyah", ""]]}, {"id": "2007.15108", "submitter": "Saeid Sedighi", "authors": "Saeid Sedighi, Kumar Vijay Mishra, M. R. Bhavani Shankar and Bj\\\"orn\n  Ottersten", "title": "Localization with One-Bit Passive Radars in Narrowband\n  Internet-of-Things using Multivariate Polynomial Optimization", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": "10.1109/TSP.2021.3072834", "report-no": null, "categories": "eess.SP cs.IR math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several Internet-of-Things (IoT) applications provide location-based\nservices, wherein it is critical to obtain accurate position estimates by\naggregating information from individual sensors. In the recently proposed\nnarrowband IoT (NB-IoT) standard, which trades off bandwidth to gain wide\ncoverage, the location estimation is compounded by the low sampling rate\nreceivers and limited-capacity links. We address both of these NB-IoT drawbacks\nin the framework of passive sensing devices that receive signals from the\ntarget-of-interest. We consider the limiting case where each node receiver\nemploys one-bit analog-to-digital-converters and propose a novel low-complexity\nnodal delay estimation method using constrained-weighted least squares\nminimization. To support the low-capacity links to the fusion center (FC), the\nrange estimates obtained at individual sensors are then converted to one-bit\ndata. At the FC, we propose target localization with the aggregated one-bit\nrange vector using both optimal and sub-optimal techniques. The computationally\nexpensive former approach is based on Lasserre's method for multivariate\npolynomial optimization while the latter employs our less complex iterative\njoint r\\textit{an}ge-\\textit{tar}get location \\textit{es}timation (ANTARES)\nalgorithm. Our overall one-bit framework not only complements the low NB-IoT\nbandwidth but also supports the design goal of inexpensive NB-IoT location\nsensing. Numerical experiments demonstrate feasibility of the proposed one-bit\napproach with a $0.6$\\% increase in the normalized localization error for the\nsmall set of $20$-$60$ nodes over the full-precision case. When the number of\nnodes is sufficiently large ($>80$), the one-bit methods yield the same\nperformance as the full precision.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 21:03:19 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 11:42:46 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Sedighi", "Saeid", ""], ["Mishra", "Kumar Vijay", ""], ["Shankar", "M. R. Bhavani", ""], ["Ottersten", "Bj\u00f6rn", ""]]}, {"id": "2007.15121", "submitter": "Arjun Roy", "authors": "Arjun Roy, Pavlos Fafalios, Asif Ekbal, Xiaofei Zhu, Stefan Dietze", "title": "Exploiting stance hierarchies for cost-sensitive stance detection of Web\n  documents", "comments": "This is a pre-print version of the Journal paper published in J\n  Intell Inf Syst (2021) (Springer). https://rdcu.be/ckLiC", "journal-ref": null, "doi": "10.1007/s10844-021-00642-z", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fact checking is an essential challenge when combating fake news. Identifying\ndocuments that agree or disagree with a particular statement (claim) is a core\ntask in this process. In this context, stance detection aims at identifying the\nposition (stance) of a document towards a claim. Most approaches address this\ntask through a 4-class classification model where the class distribution is\nhighly imbalanced. Therefore, they are particularly ineffective in detecting\nthe minority classes (for instance, 'disagree'), even though such instances are\ncrucial for tasks such as fact-checking by providing evidence for detecting\nfalse claims. In this paper, we exploit the hierarchical nature of stance\nclasses, which allows us to propose a modular pipeline of cascading binary\nclassifiers, enabling performance tuning on a per step and class basis. We\nimplement our approach through a combination of neural and traditional\nclassification models that highlight the misclassification costs of minority\nclasses. Evaluation results demonstrate state-of-the-art performance of our\napproach and its ability to significantly improve the classification\nperformance of the important 'disagree' class.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 21:40:01 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 17:10:02 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Roy", "Arjun", ""], ["Fafalios", "Pavlos", ""], ["Ekbal", "Asif", ""], ["Zhu", "Xiaofei", ""], ["Dietze", "Stefan", ""]]}, {"id": "2007.15153", "submitter": "Divya Gopinath", "authors": "Divya Gopinath, Monica Agrawal, Luke Murray, Steven Horng, David\n  Karger, David Sontag", "title": "Fast, Structured Clinical Documentation via Contextual Autocomplete", "comments": "Published in Machine Learning for Healthcare 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that uses a learned autocompletion mechanism to\nfacilitate rapid creation of semi-structured clinical documentation. We\ndynamically suggest relevant clinical concepts as a doctor drafts a note by\nleveraging features from both unstructured and structured medical data. By\nconstraining our architecture to shallow neural networks, we are able to make\nthese suggestions in real time. Furthermore, as our algorithm is used to write\na note, we can automatically annotate the documentation with clean labels of\nclinical concepts drawn from medical vocabularies, making notes more structured\nand readable for physicians, patients, and future algorithms. To our knowledge,\nthis system is the only machine learning-based documentation utility for\nclinical notes deployed in a live hospital setting, and it reduces keystroke\nburden of clinical concepts by 67% in real environments.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 23:43:15 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Gopinath", "Divya", ""], ["Agrawal", "Monica", ""], ["Murray", "Luke", ""], ["Horng", "Steven", ""], ["Karger", "David", ""], ["Sontag", "David", ""]]}, {"id": "2007.15209", "submitter": "Amanuel Alambo", "authors": "Amanuel Alambo, Manas Gaur, Krishnaprasad Thirunarayan", "title": "Depressive, Drug Abusive, or Informative: Knowledge-aware Study of News\n  Exposure during COVID-19 Outbreak", "comments": "2020 ACM SIGKDD Workshop on Knowledge-infused Mining and Learning\n  (KiML'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic is having a serious adverse impact on the lives of\npeople across the world. COVID-19 has exacerbated community-wide depression,\nand has led to increased drug abuse brought about by isolation of individuals\nas a result of lockdown. Further, apart from providing informative content to\nthe public, the incessant media coverage of COVID-19 crisis in terms of news\nbroadcasts, published articles and sharing of information on social media have\nhad the undesired snowballing effect on stress levels (further elevating\ndepression and drug use) due to uncertain future. In this position paper, we\npropose a novel framework for assessing the spatio-temporal-thematic\nprogression of depression, drug abuse, and informativeness of the underlying\nnews content across the different states in the United States. Our framework\nemploys an attention-based transfer learning technique to apply knowledge\nlearned on a social media domain to a target domain of media exposure. To\nextract news articles that are related to COVID-19 communications from the\nstreaming news content on the web, we use neural semantic parsing, and\nbackground knowledge bases in a sequence of steps called semantic filtering. We\nachieve promising preliminary results on three variations of Bidirectional\nEncoder Representations from Transformers (BERT) model. We compare our findings\nagainst a report from Mental Health America and the results show that our\nfine-tuned BERT models perform better than vanilla BERT. Our study can benefit\nepidemiologists by offering actionable insights on COVID-19 and its regional\nimpact. Further, our solution can be integrated into end-user applications to\ntailor news for users based on their emotional tone measured on the scale of\ndepressiveness, drug abusiveness, and informativeness.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 03:36:57 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 04:16:47 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2020 00:12:56 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Alambo", "Amanuel", ""], ["Gaur", "Manas", ""], ["Thirunarayan", "Krishnaprasad", ""]]}, {"id": "2007.15211", "submitter": "Victor Dibia", "authors": "Victor Dibia", "title": "NeuralQA: A Usable Library for Question Answering (Contextual Query\n  Expansion + BERT) on Large Datasets", "comments": "Published at Proceedings of the 2020 Conference on Empirical Methods\n  in Natural Language Processing: System Demonstrations. (EMNLP 2020), Demo\n  track. 8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing tools for Question Answering (QA) have challenges that limit their\nuse in practice. They can be complex to set up or integrate with existing\ninfrastructure, do not offer configurable interactive interfaces, and do not\ncover the full set of subtasks that frequently comprise the QA pipeline (query\nexpansion, retrieval, reading, and explanation/sensemaking). To help address\nthese issues, we introduce NeuralQA - a usable library for QA on large\ndatasets. NeuralQA integrates well with existing infrastructure (e.g.,\nElasticSearch instances and reader models trained with the HuggingFace\nTransformers API) and offers helpful defaults for QA subtasks. It introduces\nand implements contextual query expansion (CQE) using a masked language model\n(MLM) as well as relevant snippets (RelSnip) - a method for condensing large\ndocuments into smaller passages that can be speedily processed by a document\nreader model. Finally, it offers a flexible user interface to support workflows\nfor research explorations (e.g., visualization of gradient-based explanations\nto support qualitative inspection of model behaviour) and large scale search\ndeployment. Code and documentation for NeuralQA is available as open source on\nGithub (https://github.com/victordibia/neuralqa}{Github).\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 03:38:30 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 03:06:09 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Dibia", "Victor", ""]]}, {"id": "2007.15236", "submitter": "Andr\\'es Villa", "authors": "Andr\\'es Villa, Vladimir Araujo, Francisca Cattan, Denis Parra", "title": "Interpretable Contextual Team-aware Item Recommendation: Application in\n  Multiplayer Online Battle Arena Games", "comments": null, "journal-ref": null, "doi": "10.1145/3383313.3412211", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The video game industry has adopted recommendation systems to boost users\ninterest with a focus on game sales. Other exciting applications within video\ngames are those that help the player make decisions that would maximize their\nplaying experience, which is a desirable feature in real-time strategy video\ngames such as Multiplayer Online Battle Arena (MOBA) like as DotA and LoL.\nAmong these tasks, the recommendation of items is challenging, given both the\ncontextual nature of the game and how it exposes the dependence on the\nformation of each team. Existing works on this topic do not take advantage of\nall the available contextual match data and dismiss potentially valuable\ninformation. To address this problem we develop TTIR, a contextual recommender\nmodel derived from the Transformer neural architecture that suggests a set of\nitems to every team member, based on the contexts of teams and roles that\ndescribe the match. TTIR outperforms several approaches and provides\ninterpretable recommendations through visualization of attention weights. Our\nevaluation indicates that both the Transformer architecture and the contextual\ninformation are essential to get the best results for this item recommendation\ntask. Furthermore, a preliminary user survey indicates the usefulness of\nattention weights for explaining recommendations as well as ideas for future\nwork. The code and dataset are available at:\nhttps://github.com/ojedaf/IC-TIR-Lol.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 05:17:28 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Villa", "Andr\u00e9s", ""], ["Araujo", "Vladimir", ""], ["Cattan", "Francisca", ""], ["Parra", "Denis", ""]]}, {"id": "2007.15293", "submitter": "Ye Bi", "authors": "Ye Bi, Liqiang Song, Mengqiu Yao, Zhenyu Wu, Jianming Wang, Jing Xiao", "title": "A Heterogeneous Information Network based Cross Domain Insurance\n  Recommendation System for Cold Start Users", "comments": "11 pages, accepted in SIGIR20 Industry Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet is changing the world, adapting to the trend of internet sales will\nbring revenue to traditional insurance companies. Online insurance is still in\nits early stages of development, where cold start problem (prospective\ncustomer) is one of the greatest challenges. In traditional e-commerce field,\nseveral cross-domain recommendation (CDR) methods have been studied to infer\npreferences of cold start users based on their preferences in other domains.\nHowever, these CDR methods could not be applied to insurance domain directly\ndue to the domain specific properties. In this paper, we propose a novel\nframework called a Heterogeneous information network based Cross Domain\nInsurance Recommendation (HCDIR) system for cold start users. Specifically, we\nfirst try to learn more effective user and item latent features in both source\nand target domains. In source domain, we employ gated recurrent unit (GRU) to\nmodule user dynamic interests. In target domain, given the complexity of\ninsurance products and the data sparsity problem, we construct an insurance\nheterogeneous information network (IHIN) based on data from PingAn Jinguanjia,\nthe IHIN connects users, agents, insurance products and insurance product\nproperties together, giving us richer information. Then we employ three-level\n(relational, node, and semantic) attention aggregations to get user and\ninsurance product representations. After obtaining latent features of\noverlapping users, a feature mapping between the two domains is learned by\nmulti-layer perceptron (MLP). We apply HCDIR on Jinguanjia dataset, and show\nHCDIR significantly outperforms the state-of-the-art solutions.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 08:18:57 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Bi", "Ye", ""], ["Song", "Liqiang", ""], ["Yao", "Mengqiu", ""], ["Wu", "Zhenyu", ""], ["Wang", "Jianming", ""], ["Xiao", "Jing", ""]]}, {"id": "2007.15356", "submitter": "Gustavo Penha", "authors": "Gustavo Penha and Claudia Hauff", "title": "What does BERT know about books, movies and music? Probing BERT for\n  Conversational Recommendation", "comments": "Accepted for publication at RecSys'20", "journal-ref": null, "doi": "10.1145/3383313.3412249", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavily pre-trained transformer models such as BERT have recently shown to be\nremarkably powerful at language modelling by achieving impressive results on\nnumerous downstream tasks. It has also been shown that they are able to\nimplicitly store factual knowledge in their parameters after pre-training.\nUnderstanding what the pre-training procedure of LMs actually learns is a\ncrucial step for using and improving them for Conversational Recommender\nSystems (CRS). We first study how much off-the-shelf pre-trained BERT \"knows\"\nabout recommendation items such as books, movies and music. In order to analyze\nthe knowledge stored in BERT's parameters, we use different probes that require\ndifferent types of knowledge to solve, namely content-based and\ncollaborative-based. Content-based knowledge is knowledge that requires the\nmodel to match the titles of items with their content information, such as\ntextual descriptions and genres. In contrast, collaborative-based knowledge\nrequires the model to match items with similar ones, according to community\ninteractions such as ratings. We resort to BERT's Masked Language Modelling\nhead to probe its knowledge about the genre of items, with cloze style prompts.\nIn addition, we employ BERT's Next Sentence Prediction head and\nrepresentations' similarity to compare relevant and non-relevant search and\nrecommendation query-document inputs to explore whether BERT can, without any\nfine-tuning, rank relevant items first. Finally, we study how BERT performs in\na conversational recommendation downstream task. Overall, our analyses and\nexperiments show that: (i) BERT has knowledge stored in its parameters about\nthe content of books, movies and music; (ii) it has more content-based\nknowledge than collaborative-based knowledge; and (iii) fails on conversational\nrecommendation when faced with adversarial data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 10:08:19 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 15:00:17 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Penha", "Gustavo", ""], ["Hauff", "Claudia", ""]]}, {"id": "2007.15409", "submitter": "Amit Livne", "authors": "Amit Livne, Eliad Shem Tov, Adir Solomon, Achiya Elyasaf, Bracha\n  Shapira, and Lior Rokach", "title": "Evolving Context-Aware Recommender Systems With Users in Mind", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A context-aware recommender system (CARS) applies sensing and analysis of\nuser context to provide personalized services. The contextual information can\nbe driven from sensors in order to improve the accuracy of the recommendations.\nYet, generating accurate recommendations is not enough to constitute a useful\nsystem from the users' perspective, since certain contextual information may\ncause different issues, such as draining the user's battery, privacy issues,\nand more. Adding high-dimensional contextual information may increase both the\ndimensionality and sparsity of the model. Previous studies suggest reducing the\namount of contextual information by selecting the most suitable contextual\ninformation using a domain knowledge. Another solution is compressing it into a\ndenser latent space, thus disrupting the ability to explain the recommendation\nitem to the user, and damaging users' trust. In this paper we present an\napproach for selecting low-dimensional subsets of the contextual information\nand incorporating them explicitly within CARS. Specifically, we present a novel\nfeature-selection algorithm, based on genetic algorithms (GA), that outperforms\nSOTA dimensional-reduction CARS algorithms, improves the accuracy and the\nexplainability of the recommendations, and allows for controlling user aspects,\nsuch as privacy and battery consumption. Furthermore, we exploit the top\nsubsets that are generated along the evolutionary process, by learning multiple\ndeep context-aware models and applying a stacking technique on them, thus\nimproving the accuracy while remaining at the explicit space. We evaluated our\napproach on two high-dimensional context-aware datasets driven from\nsmartphones. An empirical analysis of our results validates that our proposed\napproach outperforms SOTA CARS models while improving transparency and\nexplainability to the user.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 12:03:22 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Livne", "Amit", ""], ["Tov", "Eliad Shem", ""], ["Solomon", "Adir", ""], ["Elyasaf", "Achiya", ""], ["Shapira", "Bracha", ""], ["Rokach", "Lior", ""]]}, {"id": "2007.15681", "submitter": "Matej Martinc", "authors": "Matej Martinc, Bla\\v{z} \\v{S}krlj, Sergej Pirkmajer, Nada Lavra\\v{c},\n  Bojan Cestnik, Martin Marzidov\\v{s}ek, Senja Pollak", "title": "COVID-19 therapy target discovery with context-aware literature mining", "comments": "Accepted to the 23rd International Conference on Discovery Science\n  (DS 2020)", "journal-ref": null, "doi": "10.1007/978-3-030-61527-7_8", "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The abundance of literature related to the widespread COVID-19 pandemic is\nbeyond manual inspection of a single expert. Development of systems, capable of\nautomatically processing tens of thousands of scientific publications with the\naim to enrich existing empirical evidence with literature-based associations is\nchallenging and relevant. We propose a system for contextualization of\nempirical expression data by approximating relations between entities, for\nwhich representations were learned from one of the largest COVID-19-related\nliterature corpora. In order to exploit a larger scientific context by transfer\nlearning, we propose a novel embedding generation technique that leverages\nSciBERT language model pretrained on a large multi-domain corpus of scientific\npublications and fine-tuned for domain adaptation on the CORD-19 dataset. The\nconducted manual evaluation by the medical expert and the quantitative\nevaluation based on therapy targets identified in the related work suggest that\nthe proposed method can be successfully employed for COVID-19 therapy target\ndiscovery and that it outperforms the baseline FastText method by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 18:37:36 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 20:19:10 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Martinc", "Matej", ""], ["\u0160krlj", "Bla\u017e", ""], ["Pirkmajer", "Sergej", ""], ["Lavra\u010d", "Nada", ""], ["Cestnik", "Bojan", ""], ["Marzidov\u0161ek", "Martin", ""], ["Pollak", "Senja", ""]]}, {"id": "2007.15683", "submitter": "Mingyang Li", "authors": "Xinru Yang, Haozhi Qi, Mingyang Li, Alexander Hauptmann", "title": "From A Glance to \"Gotcha\": Interactive Facial Image Retrieval with\n  Progressive Relevance Feedback", "comments": null, "journal-ref": "The SIGIR 2020 Workshop on Applied Interactive Information Systems", "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial image retrieval plays a significant role in forensic investigations\nwhere an untrained witness tries to identify a suspect from a massive pool of\nimages. However, due to the difficulties in describing human facial appearances\nverbally and directly, people naturally tend to depict by referring to\nwell-known existing images and comparing specific areas of faces with them and\nit is also challenging to provide complete comparison at each time. Therefore,\nwe propose an end-to-end framework to retrieve facial images with relevance\nfeedback progressively provided by the witness, enabling an exploitation of\nhistory information during multiple rounds and an interactive and iterative\napproach to retrieving the mental image. With no need of any extra annotations,\nour model can be applied at the cost of a little response effort. We experiment\non \\texttt{CelebA} and evaluate the performance by ranking percentile and\nachieve 99\\% under the best setting. Since this topic remains little explored\nto the best of our knowledge, we hope our work can serve as a stepping stone\nfor further research.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 18:46:25 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Yang", "Xinru", ""], ["Qi", "Haozhi", ""], ["Li", "Mingyang", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "2007.15731", "submitter": "Herbert Roitblat", "authors": "Herbert L. Roitblat", "title": "Is there something I'm missing? Topic Modeling in eDiscovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In legal eDiscovery, the parties are required to search through their\nelectronically stored information to find documents that are relevant to a\nspecific case. Negotiations over the scope of these searches are often based on\na fear that something will be missed. This paper continues an argument that\ndiscovery should be based on identifying the facts of a case. If a search\nprocess is less than complete (if it has Recall less than 100%), it may still\nbe complete in presenting all of the relevant available topics. In this study,\nLatent Dirichlet Allocation was used to identify 100 topics from all of the\nknown relevant documents. The documents were then categorized to about 80%\nRecall (i.e., 80% of the relevant documents were found by the categorizer,\ndesignated the hit set and 20% were missed, designated the missed set). Despite\nthe fact that less than all of the relevant documents were identified by the\ncategorizer, the documents that were identified contained all of the topics\nderived from the full set of documents. This same pattern held whether the\ncategorizer was a na\\\"ive Bayes categorizer trained on a random selection of\ndocuments or a Support Vector Machine trained with Continuous Active Learning\n(which focuses evaluation on the most-likely-to-be-relevant documents). No\ntopics were identified in either categorizer's missed set that were not already\nseen in the hit set. Not only is a computer-assisted search process reasonable\n(as required by the Federal Rules of Civil Procedure), it is also complete when\nmeasured by topics.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 20:37:27 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Roitblat", "Herbert L.", ""]]}, {"id": "2007.16000", "submitter": "Dom Huh", "authors": "Dom Huh", "title": "Hierarchical BiGraph Neural Network as Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks emerge as a promising modeling method for applications\ndealing with datasets that are best represented in the graph domain. In\nspecific, developing recommendation systems often require addressing sparse\nstructured data which often lacks the feature richness in either the user\nand/or item side and requires processing within the correct context for optimal\nperformance. These datasets intuitively can be mapped to and represented as\nnetworks or graphs. In this paper, we propose the Hierarchical BiGraph Neural\nNetwork (HBGNN), a hierarchical approach of using GNNs as recommendation\nsystems and structuring the user-item features using a bigraph framework. Our\nexperimental results show competitive performance with current recommendation\nsystem methods and transferability.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 18:01:41 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Huh", "Dom", ""]]}, {"id": "2007.16120", "submitter": "Ivan Palomares", "authors": "Ivan Palomares, Carlos Porcel, Luiz Pizzato, Ido Guy, Enrique\n  Herrera-Viedma", "title": "Reciprocal Recommender Systems: Analysis of State-of-Art Literature,\n  Challenges and Opportunities towards Social Recommendation", "comments": "This paper has been withdrawn by the authors due to an author dispute", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist situations of decision-making under information overload in the\nInternet, where people have an overwhelming number of available options to\nchoose from, e.g. products to buy in an e-commerce site, or restaurants to\nvisit in a large city. Recommender systems arose as a data-driven personalized\ndecision support tool to assist users in these situations: they are able to\nprocess user-related data, filtering and recommending items based on the users\npreferences, needs and/or behaviour. Unlike most conventional recommender\napproaches where items are inanimate entities recommended to the users and\nsuccess is solely determined upon the end users reaction to the\nrecommendation(s) received, in a Reciprocal Recommender System (RRS) users\nbecome the item being recommended to other users. Hence, both the end user and\nthe user being recommended should accept the 'matching' recommendation to yield\na successful RRS performance. The operation of an RRS entails not only\npredicting accurate preference estimates upon user interaction data as\nclassical recommenders do, but also calculating mutual compatibility between\n(pairs of) users, typically by applying fusion processes on unilateral\nuser-to-user preference information. This paper presents a snapshot-style\nanalysis of the extant literature that summarizes the state-of-the-art RRS\nresearch to date, focusing on the algorithms, fusion processes and fundamental\ncharacteristics of RRS, both inherited from conventional user-to-item\nrecommendation models and those inherent to this emerging family of approaches.\nRepresentative RRS models are likewise highlighted. Following this, we discuss\nthe challenges and opportunities for future research on RRSs, with special\nfocus on (i) fusion strategies to account for reciprocity and (ii) emerging\napplication domains related to social recommendation.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 09:48:46 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 16:04:47 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 17:48:38 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Palomares", "Ivan", ""], ["Porcel", "Carlos", ""], ["Pizzato", "Luiz", ""], ["Guy", "Ido", ""], ["Herrera-Viedma", "Enrique", ""]]}, {"id": "2007.16122", "submitter": "Zhe Wang", "authors": "Zhe Wang, Liqin Zhao, Biye Jiang, Guorui Zhou, Xiaoqiang Zhu, Kun Gai", "title": "COLD: Towards the Next Generation of Pre-Ranking System", "comments": "accepted by DLP-KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-stage cascade architecture exists widely in many industrial systems\nsuch as recommender systems and online advertising, which often consists of\nsequential modules including matching, pre-ranking, ranking, etc. For a long\ntime, it is believed pre-ranking is just a simplified version of the ranking\nmodule, considering the larger size of the candidate set to be ranked. Thus,\nefforts are made mostly on simplifying ranking model to handle the explosion of\ncomputing power for online inference. In this paper, we rethink the challenge\nof the pre-ranking system from an algorithm-system co-design view. Instead of\nsaving computing power with restriction of model architecture which causes loss\nof model performance, here we design a new pre-ranking system by joint\noptimization of both the pre-ranking model and the computing power it costs. We\nname it COLD (Computing power cost-aware Online and Lightweight Deep\npre-ranking system). COLD beats SOTA in three folds: (i) an arbitrary deep\nmodel with cross features can be applied in COLD under a constraint of\ncontrollable computing power cost. (ii) computing power cost is explicitly\nreduced by applying optimization tricks for inference acceleration. This\nfurther brings space for COLD to apply more complex deep models to reach better\nperformance. (iii) COLD model works in an online learning and severing manner,\nbringing it excellent ability to handle the challenge of the data distribution\nshift. Meanwhile, the fully online pre-ranking system of COLD provides us with\na flexible infrastructure that supports efficient new model developing and\nonline A/B testing.Since 2019, COLD has been deployed in almost all products\ninvolving the pre-ranking module in the display advertising system in Alibaba,\nbringing significant improvements.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 15:06:43 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 13:13:25 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wang", "Zhe", ""], ["Zhao", "Liqin", ""], ["Jiang", "Biye", ""], ["Zhou", "Guorui", ""], ["Zhu", "Xiaoqiang", ""], ["Gai", "Kun", ""]]}, {"id": "2007.16173", "submitter": "Taher Hekmatfar", "authors": "Taher Hekmatfar, Saman Haratizadeh, Sama Goliaei", "title": "Embedding Ranking-Oriented Recommender System Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based recommender systems (GRSs) analyze the structural information in\nthe graphical representation of data to make better recommendations, especially\nwhen the direct user-item relation data is sparse. Ranking-oriented GRSs that\nform a major class of recommendation systems, mostly use the graphical\nrepresentation of preference (or rank) data for measuring node similarities,\nfrom which they can infer a recommendation list using a neighborhood-based\nmechanism. In this paper, we propose PGRec, a novel graph-based\nranking-oriented recommendation framework. PGRec models the preferences of the\nusers over items, by a novel graph structure called PrefGraph. This graph is\nthen exploited by an improved embedding approach, taking advantage of both\nfactorization and deep learning methods, to extract vectors representing users,\nitems, and preferences. The resulting embedding are then used for predicting\nusers' unknown pairwise preferences from which the final recommendation lists\nare inferred. We have evaluated the performance of the proposed method against\nthe state of the art model-based and neighborhood-based recommendation methods,\nand our experiments show that PGRec outperforms the baseline algorithms up to\n3.2% in terms of NDCG@10 in different MovieLens datasets.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 16:56:54 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Hekmatfar", "Taher", ""], ["Haratizadeh", "Saman", ""], ["Goliaei", "Sama", ""]]}, {"id": "2007.16187", "submitter": "Philippe Esling", "authors": "Philippe Esling, Theis Bazin, Adrien Bitton, Tristan Carsault, Ninon\n  Devis", "title": "Ultra-light deep MIR by trimming lottery tickets", "comments": "8 pages, 2 figures. 21st International Society for Music Information\n  Retrieval Conference 11-15 October 2020, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.MM cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current state-of-the-art results in Music Information Retrieval are largely\ndominated by deep learning approaches. These provide unprecedented accuracy\nacross all tasks. However, the consistently overlooked downside of these models\nis their stunningly massive complexity, which seems concomitantly crucial to\ntheir success. In this paper, we address this issue by proposing a model\npruning method based on the lottery ticket hypothesis. We modify the original\napproach to allow for explicitly removing parameters, through structured\ntrimming of entire units, instead of simply masking individual weights. This\nleads to models which are effectively lighter in terms of size, memory and\nnumber of operations. We show that our proposal can remove up to 90% of the\nmodel parameters without loss of accuracy, leading to ultra-light deep MIR\nmodels. We confirm the surprising result that, at smaller compression ratios\n(removing up to 85% of a network), lighter models consistently outperform their\nheavier counterparts. We exhibit these results on a large array of MIR tasks\nincluding audio classification, pitch recognition, chord extraction, drum\ntranscription and onset estimation. The resulting ultra-light deep learning\nmodels for MIR can run on CPU, and can even fit on embedded devices with\nminimal degradation of accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 17:30:28 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Esling", "Philippe", ""], ["Bazin", "Theis", ""], ["Bitton", "Adrien", ""], ["Carsault", "Tristan", ""], ["Devis", "Ninon", ""]]}, {"id": "2007.16208", "submitter": "Flora D. Salim", "authors": "Kyle K. Qin, Flora D. Salim, Yongli Ren, Wei Shao, Mark Heimann, Danai\n  Koutra", "title": "G-CREWE: Graph CompREssion With Embedding for Network Alignment", "comments": "10 pages, accepted at the 29th ACM International Conference\n  onInformation and Knowledge Management (CIKM 20)", "journal-ref": null, "doi": "10.1145/3340531.3411924", "report-no": null, "categories": "cs.SI cs.DB cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network alignment is useful for multiple applications that require\nincreasingly large graphs to be processed. Existing research approaches this as\nan optimization problem or computes the similarity based on node\nrepresentations. However, the process of aligning every pair of nodes between\nrelatively large networks is time-consuming and resource-intensive. In this\npaper, we propose a framework, called G-CREWE (Graph CompREssion With\nEmbedding) to solve the network alignment problem. G-CREWE uses node embeddings\nto align the networks on two levels of resolution, a fine resolution given by\nthe original network and a coarse resolution given by a compressed version, to\nachieve an efficient and effective network alignment. The framework first\nextracts node features and learns the node embedding via a Graph Convolutional\nNetwork (GCN). Then, node embedding helps to guide the process of graph\ncompression and finally improve the alignment performance. As part of G-CREWE,\nwe also propose a new compression mechanism called MERGE (Minimum dEgRee\nneiGhbors comprEssion) to reduce the size of the input networks while\npreserving the consistency in their topological structure. Experiments on all\nreal networks show that our method is more than twice as fast as the most\ncompetitive existing methods while maintaining high accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 05:30:21 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Qin", "Kyle K.", ""], ["Salim", "Flora D.", ""], ["Ren", "Yongli", ""], ["Shao", "Wei", ""], ["Heimann", "Mark", ""], ["Koutra", "Danai", ""]]}]