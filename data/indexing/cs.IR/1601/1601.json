[{"id": "1601.00599", "submitter": "Matthias Zeppelzauer", "authors": "Matthias Zeppelzauer, Daniel Schopfhauser", "title": "Multimodal Classification of Events in Social Media", "comments": "Preprint of accepted manuscript for the Elsevier Image and Vision\n  Computing Journal (IMAVIS). The paper will be published by IMAVIS under DOI\n  10.1016/j.imavis.2015.12.004", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large amount of social media hosted on platforms like Flickr and Instagram\nis related to social events. The task of social event classification refers to\nthe distinction of event and non-event-related content as well as the\nclassification of event types (e.g. sports events, concerts, etc.). In this\npaper, we provide an extensive study of textual, visual, as well as multimodal\nrepresentations for social event classification. We investigate strengths and\nweaknesses of the modalities and study synergy effects between the modalities.\nExperimental results obtained with our multimodal representation outperform\nstate-of-the-art methods and provide a new baseline for future research.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 18:29:33 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Zeppelzauer", "Matthias", ""], ["Schopfhauser", "Daniel", ""]]}, {"id": "1601.00643", "submitter": "Chandra Yadav Shekhar", "authors": "Chandra Shekhar Yadav, Aditi Sharan", "title": "Hybrid Approach for Single Text Document Summarization using Statistical\n  and Sentiment Features", "comments": "20 page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summarization is a way to represent same information in concise way with\nequal sense. This can be categorized in two type Abstractive and Extractive\ntype. Our work is focused around Extractive summarization. A generic approach\nto extractive summarization is to consider sentence as an entity, score each\nsentence based on some indicative features to ascertain the quality of sentence\nfor inclusion in summary. Sort the sentences on the score and consider top n\nsentences for summarization. Mostly statistical features have been used for\nscoring the sentences. We are proposing a hybrid model for a single text\ndocument summarization. This hybrid model is an extraction based approach,\nwhich is combination of Statistical and semantic technique. The hybrid model\ndepends on the linear combination of statistical measures : sentence position,\nTF-IDF, Aggregate similarity, centroid, and semantic measure. Our idea to\ninclude sentiment analysis for salient sentence extraction is derived from the\nconcept that emotion plays an important role in communication to effectively\nconvey any message hence, it can play a vital role in text document\nsummarization. For comparison we have generated five system summaries Proposed\nWork, MEAD system, Microsoft system, OPINOSIS system, and Human generated\nsummary, and evaluation is done using ROUGE score.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2016 05:55:56 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Yadav", "Chandra Shekhar", ""], ["Sharan", "Aditi", ""]]}, {"id": "1601.00855", "submitter": "Pedro Saleiro", "authors": "Pedro Saleiro, Jorge Teixeira, Carlos Soares, Eug\\'enio Oliveira", "title": "TimeMachine: Entity-centric Search and Visualization of News Archives", "comments": "Advances in Information Retrieval: 38th European Conference on IR\n  Research, ECIR 2016, Padua, Italy, March 20-23, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dynamic web tool that allows interactive search and\nvisualization of large news archives using an entity-centric approach. Users\nare able to search entities using keyword phrases expressing news stories or\nevents and the system retrieves the most relevant entities to the user query\nbased on automatically extracted and indexed entity profiles. From the\ncomputational journalism perspective, TimeMachine allows users to explore media\ncontent through time using automatic identification of entity names, jobs,\nquotations and relations between entities from co-occurrences networks\nextracted from the news articles. TimeMachine demo is available at\nhttp://maquinadotempo.sapo.pt/\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 15:18:10 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Saleiro", "Pedro", ""], ["Teixeira", "Jorge", ""], ["Soares", "Carlos", ""], ["Oliveira", "Eug\u00e9nio", ""]]}, {"id": "1601.01356", "submitter": "Makbule Gulcin Ozsoy", "authors": "Makbule Gulcin Ozsoy", "title": "From Word Embeddings to Item Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network platforms can use the data produced by their users to serve\nthem better. One of the services these platforms provide is recommendation\nservice. Recommendation systems can predict the future preferences of users\nusing their past preferences. In the recommendation systems literature there\nare various techniques, such as neighborhood based methods, machine-learning\nbased methods and matrix-factorization based methods. In this work, a set of\nwell known methods from natural language processing domain, namely Word2Vec, is\napplied to recommendation systems domain. Unlike previous works that use\nWord2Vec for recommendation, this work uses non-textual features, the\ncheck-ins, and it recommends venues to visit/check-in to the target users. For\nthe experiments, a Foursquare check-in dataset is used. The results show that\nuse of continuous vector space representations of items modeled by techniques\nof Word2Vec is promising for making recommendations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 00:09:37 GMT"}, {"version": "v2", "created": "Sun, 6 Mar 2016 16:09:10 GMT"}, {"version": "v3", "created": "Wed, 15 Jun 2016 08:07:36 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Ozsoy", "Makbule Gulcin", ""]]}, {"id": "1601.01611", "submitter": "Kriste Krstovski", "authors": "Kriste Krstovski, David A. Smith and Michael J. Kurtz", "title": "Automatic Construction of Evaluation Sets and Evaluation of Document\n  Similarity Models in Large Scholarly Retrieval Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieval systems for scholarly literature offer the ability for the\nscientific community to search, explore and download scholarly articles across\nvarious scientific disciplines. Mostly used by the experts in the particular\nfield, these systems contain user community logs including information on user\nspecific downloaded articles. In this paper we present a novel approach for\nautomatically evaluating document similarity models in large collections of\nscholarly publications. Unlike typical evaluation settings that use test\ncollections consisting of query documents and human annotated relevance\njudgments, we use download logs to automatically generate pseudo-relevant set\nof similar document pairs. More specifically we show that consecutively\ndownloaded document pairs, extracted from a scholarly information retrieval\n(IR) system, could be utilized as a test collection for evaluating document\nsimilarity models. Another novel aspect of our approach lies in the method that\nwe employ for evaluating the performance of the model by comparing the\ndistribution of consecutively downloaded document pairs and random document\npairs in log space. Across two families of similarity models, that represent\ndocuments in the term vector and topic spaces, we show that our evaluation\napproach achieves very high correlation with traditional performance metrics\nsuch as Mean Average Precision (MAP), while being more efficient to compute.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 17:27:31 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Krstovski", "Kriste", ""], ["Smith", "David A.", ""], ["Kurtz", "Michael J.", ""]]}, {"id": "1601.01892", "submitter": "Kirell Benzi", "authors": "Kirell Benzi, Vassilis Kalofolias, Xavier Bresson, Pierre\n  Vandergheynst", "title": "Song Recommendation with Non-Negative Matrix Factorization and Graph\n  Total Variation", "comments": "Code available at: https://github.com/kikohs/recog", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work formulates a novel song recommender system as a matrix completion\nproblem that benefits from collaborative filtering through Non-negative Matrix\nFactorization (NMF) and content-based filtering via total variation (TV) on\ngraphs. The graphs encode both playlist proximity information and song\nsimilarity, using a rich combination of audio, meta-data and social features.\nAs we demonstrate, our hybrid recommendation system is very versatile and\nincorporates several well-known methods while outperforming them. Particularly,\nwe show on real-world data that our model overcomes w.r.t. two evaluation\nmetrics the recommendation of models solely based on low-rank information,\ngraph-based information or a combination of both.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 14:59:34 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 17:24:33 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Benzi", "Kirell", ""], ["Kalofolias", "Vassilis", ""], ["Bresson", "Xavier", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1601.01917", "submitter": "Sylvain Castagnos", "authors": "Sylvain Castagnos (KIWI), Amaury L 'Huillier (KIWI), Anne Boyer (KIWI)", "title": "Toward a Robust Diversity-Based Model to Detect Changes of Context", "comments": "27th IEEE International Conference on Tools with Artificial\n  Intelligence (ICTAI 2015), Nov 2015, Vietri sul Mare, Italy", "journal-ref": null, "doi": "10.1109/ICTAI.2015.84", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to automatically and quickly understand the user context during a\nsession is a main issue for recommender systems. As a first step toward\nachieving that goal, we propose a model that observes in real time the\ndiversity brought by each item relatively to a short sequence of consultations,\ncorresponding to the recent user history. Our model has a complexity in\nconstant time, and is generic since it can apply to any type of items within an\nonline service (e.g. profiles, products, music tracks) and any application\ndomain (e-commerce, social network, music streaming), as long as we have\npartial item descriptions. The observation of the diversity level over time\nallows us to detect implicit changes. In the long term, we plan to characterize\nthe context, i.e. to find common features among a contiguous sub-sequence of\nitems between two changes of context determined by our model. This will allow\nus to make context-aware and privacy-preserving recommendations, to explain\nthem to users. As this is an ongoing research, the first step consists here in\nstudying the robustness of our model while detecting changes of context. In\norder to do so, we use a music corpus of 100 users and more than 210,000\nconsultations (number of songs played in the global history). We validate the\nrelevancy of our detections by finding connections between changes of context\nand events, such as ends of session. Of course, these events are a subset of\nthe possible changes of context, since there might be several contexts within a\nsession. We altered the quality of our corpus in several manners, so as to test\nthe performances of our model when confronted with sparsity and different types\nof items. The results show that our model is robust and constitutes a promising\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 15:50:03 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Castagnos", "Sylvain", "", "KIWI"], ["'Huillier", "Amaury L", "", "KIWI"], ["Boyer", "Anne", "", "KIWI"]]}, {"id": "1601.02093", "submitter": "Olivier Mor\\`ere", "authors": "Olivier Mor\\`ere, Antoine Veillard, Jie Lin, Julie Petta, Vijay\n  Chandrasekhar, Tomaso Poggio", "title": "Group Invariant Deep Representations for Image Instance Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most image instance retrieval pipelines are based on comparison of vectors\nknown as global image descriptors between a query image and the database\nimages. Due to their success in large scale image classification,\nrepresentations extracted from Convolutional Neural Networks (CNN) are quickly\ngaining ground on Fisher Vectors (FVs) as state-of-the-art global descriptors\nfor image instance retrieval. While CNN-based descriptors are generally\nremarked for good retrieval performance at lower bitrates, they nevertheless\npresent a number of drawbacks including the lack of robustness to common object\ntransformations such as rotations compared with their interest point based FV\ncounterparts.\n  In this paper, we propose a method for computing invariant global descriptors\nfrom CNNs. Our method implements a recently proposed mathematical theory for\ninvariance in a sensory cortex modeled as a feedforward neural network. The\nresulting global descriptors can be made invariant to multiple arbitrary\ntransformation groups while retaining good discriminativeness.\n  Based on a thorough empirical evaluation using several publicly available\ndatasets, we show that our method is able to significantly and consistently\nimprove retrieval results every time a new type of invariance is incorporated.\nWe also show that our method which has few parameters is not prone to\noverfitting: improvements generalize well across datasets with different\nproperties with regard to invariances. Finally, we show that our descriptors\nare able to compare favourably to other state-of-the-art compact descriptors in\nsimilar bitranges, exceeding the highest retrieval results reported in the\nliterature on some datasets. A dedicated dimensionality reduction step\n--quantization or hashing-- may be able to further improve the competitiveness\nof the descriptors.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2016 10:42:35 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 06:43:44 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Mor\u00e8re", "Olivier", ""], ["Veillard", "Antoine", ""], ["Lin", "Jie", ""], ["Petta", "Julie", ""], ["Chandrasekhar", "Vijay", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1601.02300", "submitter": "Marian-Andrei Rizoiu", "authors": "Young-Min Kim, Julien Velcin, St\\'ephane Bonnevay, Marian-Andrei\n  Rizoiu", "title": "Temporal Multinomial Mixture for Instance-Oriented Evolutionary\n  Clustering", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-16354-3_66", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary clustering aims at capturing the temporal evolution of clusters.\nThis issue is particularly important in the context of social media data that\nare naturally temporally driven. In this paper, we propose a new probabilistic\nmodel-based evolutionary clustering technique. The Temporal Multinomial Mixture\n(TMM) is an extension of classical mixture model that optimizes feature\nco-occurrences in the trade-off with temporal smoothness. Our model is\nevaluated for two recent case studies on opinion aggregation over time. We\ncompare four different probabilistic clustering models and we show the\nsuperiority of our proposal in the task of instance-oriented clustering.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 02:06:36 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Kim", "Young-Min", ""], ["Velcin", "Julien", ""], ["Bonnevay", "St\u00e9phane", ""], ["Rizoiu", "Marian-Andrei", ""]]}, {"id": "1601.02327", "submitter": "Guangneng Hu", "authors": "Guang-Neng Hu, Xin-Yu Dai, Yunya Song, Shu-Jian Huang, Jia-Jun Chen", "title": "A Synthetic Approach for Recommendation: Combining Ratings, Social\n  Relations, and Reviews", "comments": "7 pages, 8 figures", "journal-ref": "24th IJCAI,2015,1756-1762", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recommender systems (RSs) provide an effective way of alleviating the\ninformation overload problem by selecting personalized choices. Online social\nnetworks and user-generated content provide diverse sources for recommendation\nbeyond ratings, which present opportunities as well as challenges for\ntraditional RSs. Although social matrix factorization (Social MF) can integrate\nratings with social relations and topic matrix factorization can integrate\nratings with item reviews, both of them ignore some useful information. In this\npaper, we investigate the effective data fusion by combining the two\napproaches, in two steps. First, we extend Social MF to exploit the graph\nstructure of neighbors. Second, we propose a novel framework MR3 to jointly\nmodel these three types of information effectively for rating prediction by\naligning latent factors and hidden topics. We achieve more accurate rating\nprediction on two real-life datasets. Furthermore, we measure the contribution\nof each data source to the proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 05:41:39 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Hu", "Guang-Neng", ""], ["Dai", "Xin-Yu", ""], ["Song", "Yunya", ""], ["Huang", "Shu-Jian", ""], ["Chen", "Jia-Jun", ""]]}, {"id": "1601.02376", "submitter": "Weinan Zhang", "authors": "Weinan Zhang, Tianming Du, Jun Wang", "title": "Deep Learning over Multi-field Categorical Data: A Case Study on User\n  Response Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting user responses, such as click-through rate and conversion rate,\nare critical in many web applications including web search, personalised\nrecommendation, and online advertising. Different from continuous raw features\nthat we usually found in the image and audio domains, the input features in web\nspace are always of multi-field and are mostly discrete and categorical while\ntheir dependencies are little known. Major user response prediction models have\nto either limit themselves to linear models or require manually building up\nhigh-order combination features. The former loses the ability of exploring\nfeature interactions, while the latter results in a heavy computation in the\nlarge feature space. To tackle the issue, we propose two novel models using\ndeep neural networks (DNNs) to automatically learn effective patterns from\ncategorical feature interactions and make predictions of users' ad clicks. To\nget our DNNs efficiently work, we propose to leverage three feature\ntransformation methods, i.e., factorisation machines (FMs), restricted\nBoltzmann machines (RBMs) and denoising auto-encoders (DAEs). This paper\npresents the structure of our models and their efficient training algorithms.\nThe large-scale experiments with real-world data demonstrate that our methods\nwork better than major state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 10:04:40 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Zhang", "Weinan", ""], ["Du", "Tianming", ""], ["Wang", "Jun", ""]]}, {"id": "1601.02377", "submitter": "Weinan Zhang", "authors": "Weinan Zhang, Lingxi Chen, Jun Wang", "title": "Implicit Look-alike Modelling in Display Ads: Transfer Collaborative\n  Filtering to CTR Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  User behaviour targeting is essential in online advertising. Compared with\nsponsored search keyword targeting and contextual advertising page content\ntargeting, user behaviour targeting builds users' interest profiles via\ntracking their online behaviour and then delivers the relevant ads according to\neach user's interest, which leads to higher targeting accuracy and thus more\nimproved advertising performance. The current user profiling methods include\nbuilding keywords and topic tags or mapping users onto a hierarchical taxonomy.\nHowever, to our knowledge, there is no previous work that explicitly\ninvestigates the user online visits similarity and incorporates such similarity\ninto their ad response prediction. In this work, we propose a general framework\nwhich learns the user profiles based on their online browsing behaviour, and\ntransfers the learned knowledge onto prediction of their ad response.\nTechnically, we propose a transfer learning model based on the probabilistic\nlatent factor graphic models, where the users' ad response profiles are\ngenerated from their online browsing profiles. The large-scale experiments\nbased on real-world data demonstrate significant improvement of our solution\nover some strong baselines.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 10:12:17 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Zhang", "Weinan", ""], ["Chen", "Lingxi", ""], ["Wang", "Jun", ""]]}, {"id": "1601.02904", "submitter": "Mahyuddin K. M.  Nasution", "authors": "Mahyuddin K. M. Nasution, Shahrul Azman Mohd. Noah, and Saidah Saad", "title": "Social Network Extraction: Superficial Method and Information Retrieval", "comments": "11 pages, 1 figures", "journal-ref": "Proceeding of International Conference on Informatics for\n  Development (ICID'11), c2-110 - c2-115 (2011)", "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Social network has become one of the themes of government issues, mainly\ndealing with the chaos. The use of web is steadily gaining ground in these\nissues. However, most of the web documents are unstructured and lack of\nsemantic. In this paper we proposed an Information Retrieval driven method for\ndealing with heterogeneity of features in the web. The proposed solution is to\ncompare some approaches have shown the capacity to extract social relation:\nstrength relations and relations based on online academic database.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 15:16:17 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Nasution", "Mahyuddin K. M.", ""], ["Noah", "Shahrul Azman Mohd.", ""], ["Saad", "Saidah", ""]]}, {"id": "1601.03354", "submitter": "Alexey Grigorev", "authors": "Alexey Grigorev", "title": "Identifier Namespaces in Mathematical Notation", "comments": "Master Thesis defended at TU Berlin in Summer 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, we look at the problem of assigning each identifier of a\ndocument to a namespace. At the moment, there does not exist a special dataset\nwhere all identifiers are grouped to namespaces, and therefore we need to\ncreate such a dataset ourselves.\n  To do that, we need to find groups of documents that use identifiers in the\nsame way. This can be done with cluster analysis methods. We argue that\ndocuments can be represented by the identifiers they contain, and this approach\nis similar to representing textual information in the Vector Space Model.\nBecause of this, we can apply traditional document clustering techniques for\nnamespace discovery.\n  Because the problem is new, there is no gold standard dataset, and it is hard\nto evaluate the performance of our method. To overcome it, we first use Java\nsource code as a dataset for our experiments, since it contains the namespace\ninformation. We verify that our method can partially recover namespaces from\nsource code using only information about identifiers.\n  The algorithms are evaluated on the English Wikipedia, and the proposed\nmethod can extract namespaces on a variety of topics. After extraction, the\nnamespaces are organized into a hierarchical structure by using existing\nclassification schemes such as MSC, PACS and ACM. We also apply it to the\nRussian Wikipedia, and the results are consistent across the languages.\n  To our knowledge, the problem of introducing namespaces to mathematics has\nnot been studied before, and prior to our work there has been no dataset where\nidentifiers are grouped into namespaces. Thus, our result is not only a good\nstart, but also a good indicator that automatic namespace discovery is\npossible.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 19:17:00 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Grigorev", "Alexey", ""]]}, {"id": "1601.03541", "submitter": "Harsh Thakkar", "authors": "Saeedeh Shekarpour, Denis Lukovnikov, Ashwini Jaya Kumar, Kemele\n  Endris, Kuldeep Singh, Harsh Thakkar, Christoph Lange", "title": "Question Answering on Linked Data: Challenges and Future Directions", "comments": "Submitted to Question Answering And Activity Analysis in\n  Participatory Sites (Q4APS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Question Answering (QA) systems are becoming the inspiring model for the\nfuture of search engines. While recently, underlying datasets for QA systems\nhave been promoted from unstructured datasets to structured datasets with\nhighly semantic-enriched metadata, but still question answering systems involve\nserious challenges which cause to be far beyond desired expectations. In this\npaper, we raise the challenges for building a Question Answering (QA) system\nespecially with the focus of employing structured data (i.e. knowledge graph).\nThis paper provide an exhaustive insight of the known challenges, so far. Thus,\nit helps researchers to easily spot open rooms for the future research agenda.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 10:21:06 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 13:29:43 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Shekarpour", "Saeedeh", ""], ["Lukovnikov", "Denis", ""], ["Kumar", "Ashwini Jaya", ""], ["Endris", "Kemele", ""], ["Singh", "Kuldeep", ""], ["Thakkar", "Harsh", ""], ["Lange", "Christoph", ""]]}, {"id": "1601.03778", "submitter": "Baichuan Zhang", "authors": "Baichuan Zhang, Sutanay Choudhury, Mohammad Al Hasan, Xia Ning,\n  Khushbu Agarwal, Sumit Purohit, Paola Pesntez Cabrera", "title": "Trust from the past: Bayesian Personalized Ranking based Link Prediction\n  in Knowledge Graphs", "comments": "SDM Workshop on Mining Networks and Graphs (MNG 2016), Miami, FL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction, or predicting the likelihood of a link in a knowledge graph\nbased on its existing state is a key research task. It differs from a\ntraditional link prediction task in that the links in a knowledge graph are\ncategorized into different predicates and the link prediction performance of\ndifferent predicates in a knowledge graph generally varies widely. In this\nwork, we propose a latent feature embedding based link prediction model which\nconsiders the prediction task for each predicate disjointly. To learn the model\nparameters it utilizes a Bayesian personalized ranking based optimization\ntechnique. Experimental results on large-scale knowledge bases such as YAGO2\nshow that our link prediction approach achieves substantially higher\nperformance than several state-of-art approaches. We also show that for a given\npredicate the topological properties of the knowledge graph induced by the\ngiven predicate edges are key indicators of the link prediction performance of\nthat predicate in the knowledge graph.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 23:13:00 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2016 05:12:32 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Zhang", "Baichuan", ""], ["Choudhury", "Sutanay", ""], ["Hasan", "Mohammad Al", ""], ["Ning", "Xia", ""], ["Agarwal", "Khushbu", ""], ["Purohit", "Sumit", ""], ["Cabrera", "Paola Pesntez", ""]]}, {"id": "1601.04075", "submitter": "Igor Podgorny", "authors": "Igor A. Podgorny", "title": "Modification of Question Writing Style Influences Content Popularity in\n  a Social Q&A System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TurboTax AnswerXchange is a social Q&A system supporting users working on\nfederal and state tax returns. Using 2015 data, we demonstrate that content\npopularity (or number of views per AnswerXchange question) can be predicted\nwith reasonable accuracy based on attributes of the question alone. We also\nemploy probabilistic topic analysis and uplift modeling to identify question\nfeatures with the highest impact on popularity. We demonstrate that content\npopularity is driven by behavioral attributes of AnswerXchange users and\ndepends on complex interactions between search ranking algorithms,\npsycholinguistic factors and question writing style. Our findings provide a\nrationale for employing popularity predictions to guide the users into\nformulating better questions and editing the existing ones. For example,\nstarting question title with a question word or adding details to the question\nincrease number of views per question. Similar approach can be applied to\npromoting AnswerXchange content indexed by Google to drive organic traffic to\nTurboTax.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2016 21:01:12 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Podgorny", "Igor A.", ""]]}, {"id": "1601.04605", "submitter": "Marc Sloan Mr", "authors": "Marc Sloan, Jun Wang", "title": "Dynamic Information Retrieval: Theoretical Framework and Application", "comments": "ACM SIGIR International Conference on the Theory of Information\n  Retrieval (ICTIR), 10 pages, 4 figures, 2 algorithms, 3 tables. in\n  Proceedings of the 2015 International Conference on The Theory of Information\n  Retrieval", "journal-ref": null, "doi": "10.1145/2808194.2809457", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theoretical frameworks like the Probability Ranking Principle and its more\nrecent Interactive Information Retrieval variant have guided the development of\nranking and retrieval algorithms for decades, yet they are not capable of\nhelping us model problems in Dynamic Information Retrieval which exhibit the\nfollowing three properties; an observable user signal, retrieval over multiple\nstages and an overall search intent. In this paper a new theoretical framework\nfor retrieval in these scenarios is proposed. We derive a general dynamic\nutility function for optimizing over these types of tasks, that takes into\naccount the utility of each stage and the probability of observing user\nfeedback. We apply our framework to experiments over TREC data in the dynamic\nmulti page search scenario as a practical demonstration of its effectiveness\nand to frame the discussion of its use, its limitations and to compare it\nagainst the existing frameworks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 17:01:34 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Sloan", "Marc", ""], ["Wang", "Jun", ""]]}, {"id": "1601.04615", "submitter": "Marc Sloan Mr", "authors": "Marc Sloan, Hui Yang, Jun Wang", "title": "A Term-Based Methodology for Query Reformulation Understanding", "comments": "Information Retrieval Journal, 23 pages, 10 tables, 6 figures", "journal-ref": "Information Retrieval Journal, April 2015, Volume 18, Issue 2, pp\n  145-165", "doi": "10.1007/s10791-015-9251-5", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key to any research involving session search is the understanding of how a\nuser's queries evolve throughout the session. When a user creates a query\nreformulation, he or she is consciously retaining terms from their original\nquery, removing others and adding new terms. By measuring the similarity\nbetween queries we can make inferences on the user's information need and how\nsuccessful their new query is likely to be. By identifying the origins of added\nterms we can infer the user's motivations and gain an understanding of their\ninteractions.\n  In this paper we present a novel term-based methodology for understanding and\ninterpreting query reformulation actions. We use TREC Session Track data to\ndemonstrate how our technique is able to learn from query logs and we make use\nof click data to test user interaction behavior when reformulating queries. We\nidentify and evaluate a range of term-based query reformulation strategies and\nshow that our methods provide valuable insight into understanding query\nreformulation in session search.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 17:17:41 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 21:58:04 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Sloan", "Marc", ""], ["Yang", "Hui", ""], ["Wang", "Jun", ""]]}, {"id": "1601.04745", "submitter": "Xiaoxue Zhao", "authors": "Xiaoxue Zhao, Jun Wang", "title": "A Theoretical Analysis of Two-Stage Recommendation for Cold-Start\n  Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a theoretical framework for tackling the cold-start\ncollaborative filtering problem, where unknown targets (items or users) keep\ncoming to the system, and there is a limited number of resources (users or\nitems) that can be allocated and related to them. The solution requires a\ntrade-off between exploitation and exploration as with the limited\nrecommendation opportunities, we need to, on one hand, allocate the most\nrelevant resources right away, but, on the other hand, it is also necessary to\nallocate resources that are useful for learning the target's properties in\norder to recommend more relevant ones in the future. In this paper, we study a\nsimple two-stage recommendation combining a sequential and a batch solution\ntogether. We first model the problem with the partially observable Markov\ndecision process (POMDP) and provide an exact solution. Then, through an\nin-depth analysis over the POMDP value iteration solution, we identify that an\nexact solution can be abstracted as selecting resources that are not only\nhighly relevant to the target according to the initial-stage information, but\nalso highly correlated, either positively or negatively, with other potential\nresources for the next stage. With this finding, we propose an approximate\nsolution to ease the intractability of the exact solution. Our initial results\non synthetic data and the Movie Lens 100K dataset confirm the performance gains\nof our theoretical development and analysis.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2016 22:31:06 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Zhao", "Xiaoxue", ""], ["Wang", "Jun", ""]]}, {"id": "1601.04800", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng", "title": "Top-N Recommender System via Matrix Completion", "comments": "AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-N recommender systems have been investigated widely both in industry and\nacademia. However, the recommendation quality is far from satisfactory. In this\npaper, we propose a simple yet promising algorithm. We fill the user-item\nmatrix based on a low-rank assumption and simultaneously keep the original\ninformation. To do that, a nonconvex rank relaxation rather than the nuclear\nnorm is adopted to provide a better rank approximation and an efficient\noptimization strategy is designed. A comprehensive set of experiments on real\ndatasets demonstrates that our method pushes the accuracy of Top-N\nrecommendation to a new level.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 04:48:42 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1601.05194", "submitter": "Kuan-Yu Chen", "authors": "Kuan-Yu Chen, Shih-Hung Liu, Berlin Chen, Hsin-Min Wang", "title": "Improved Spoken Document Summarization with Coverage Modeling Techniques", "comments": "arXiv admin note: text overlap with arXiv:1506.04365", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extractive summarization aims at selecting a set of indicative sentences from\na source document as a summary that can express the major theme of the\ndocument. A general consensus on extractive summarization is that both\nrelevance and coverage are critical issues to address. The existing methods\ndesigned to model coverage can be characterized by either reducing redundancy\nor increasing diversity in the summary. Maximal margin relevance (MMR) is a\nwidely-cited method since it takes both relevance and redundancy into account\nwhen generating a summary for a given document. In addition to MMR, there is\nonly a dearth of research concentrating on reducing redundancy or increasing\ndiversity for the spoken document summarization task, as far as we are aware.\nMotivated by these observations, two major contributions are presented in this\npaper. First, in contrast to MMR, which considers coverage by reducing\nredundancy, we propose two novel coverage-based methods, which directly\nincrease diversity. With the proposed methods, a set of representative\nsentences, which not only are relevant to the given document but also cover\nmost of the important sub-themes of the document, can be selected\nautomatically. Second, we make a step forward to plug in several\ndocument/sentence representation methods into the proposed framework to further\nenhance the summarization performance. A series of empirical evaluations\ndemonstrate the effectiveness of our proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 08:26:07 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Chen", "Kuan-Yu", ""], ["Liu", "Shih-Hung", ""], ["Chen", "Berlin", ""], ["Wang", "Hsin-Min", ""]]}, {"id": "1601.05271", "submitter": "Xianwen Wang", "authors": "Xianwen Wang, Shenmeng Xu and Zhichao Fang", "title": "Tracing Digital Footprints to Academic Articles: An Investigation of\n  PeerJ Publication Referral Data", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.SI physics.bio-ph physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a novel way to explore the patterns of people's\nvisits to academic articles. About 3.4 million links to referral source of\nvisitors of 1432 papers published in the journal of PeerJ are collected and\nanalyzed. We find that at least 57% visits are from external referral sources,\namong which General Search Engine, Social Network, and News & Blog are the top\nthree categories of referrals. Academic Resource, including academic search\nengines and academic publishers' sites, is the fourth largest category of\nreferral sources. In addition, our results show that Google contributes\nsignificantly the most in directing people to scholarly articles. This\nencompasses the usage of Google the search engine, Google Scholar the academic\nsearch engine, and diverse specific country domains of them. Focusing on\nsimilar disciplines to PeerJ's publication scope, NCBI is the academic search\nengine on which people are the most frequently directed to PeerJ. Correlation\nanalysis and regression analysis indicates that papers with more mentions are\nexpected to have more visitors, and Facebook, Twitter and Reddit are the most\ncommonly used social networking tools that refer people to PeerJ.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 13:56:04 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Wang", "Xianwen", ""], ["Xu", "Shenmeng", ""], ["Fang", "Zhichao", ""]]}, {"id": "1601.05274", "submitter": "Lisette Elizabeth Esp\\'in Noboa", "authors": "Lisette Esp\\'in-Noboa, Florian Lemmerich, Philipp Singer and Markus\n  Strohmaier", "title": "Discovering and Characterizing Mobility Patterns in Urban Spaces: A\n  Study of Manhattan Taxi Data", "comments": "Accepted at the Location and the Web (LocWeb) workshop at WWW2016", "journal-ref": null, "doi": "10.1145/2872518.2890468", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, human movement in urban spaces can be traced digitally in many\ncases. It can be observed that movement patterns are not constant, but vary\nacross time and space. In this work,we characterize such spatio-temporal\npatterns with an innovative combination of two separate approaches that have\nbeen utilized for studying human mobility in the past. First, by using\nnon-negative tensor factorization (NTF), we are able to cluster human behavior\nbased on spatio-temporal dimensions. Second, for understanding these clusters,\nwe propose to use HypTrails, a Bayesian approach for expressing and comparing\nhypotheses about human trails. To formalize hypotheses we utilize data that is\npublicly available on the Web, namely Foursquare data and census data provided\nby an open data platform. By applying this combination of approaches to taxi\ndata in Manhattan, we can discover and explain different patterns in human\nmobility that cannot be identified in a collective analysis. As one example, we\ncan find a group of taxi rides that end at locations with a high number of\nparty venues (according to Foursquare) on weekend nights. Overall, our work\ndemonstrates that human mobility is not one-dimensional but rather contains\ndifferent facets both in time and space which we explain by utilizing online\ndata. The findings of this paper argue for a more fine-grained analysis of\nhuman mobility in order to make more informed decisions for e.g., enhancing\nurban structures, tailored traffic control and location-based recommender\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 14:11:25 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 22:42:36 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Esp\u00edn-Noboa", "Lisette", ""], ["Lemmerich", "Florian", ""], ["Singer", "Philipp", ""], ["Strohmaier", "Markus", ""]]}, {"id": "1601.05893", "submitter": "Hans De Sterck", "authors": "Shawn Brunsting, Hans De Sterck, Remco Dolman, Teun van Sprundel", "title": "GeoTextTagger: High-Precision Location Tagging of Textual Documents\n  using a Natural Language Processing Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location tagging, also known as geotagging or geolocation, is the process of\nassigning geographical coordinates to input data. In this paper we present an\nalgorithm for location tagging of textual documents. Our approach makes use of\nprevious work in natural language processing by using a state-of-the-art\npart-of-speech tagger and named entity recognizer to find blocks of text which\nmay refer to locations. A knowledge base (OpenStreatMap) is then used to find a\nlist of possible locations for each block. Finally, one location is chosen for\neach block by assigning distance-based scores to each location and repeatedly\nselecting the location and block with the best score. We tested our geolocation\nalgorithm with Wikipedia articles about topics with a well-defined geographical\nlocation that are geotagged by the articles' authors, where classification\napproaches have achieved median errors as low as 11 km, with attainable\naccuracy limited by the class size. Our approach achieved a 10th percentile\nerror of 490 metres and median error of 54 kilometres on the Wikipedia dataset\nwe used. When considering the five location tags with the greatest scores, 50%\nof articles were assigned at least one tag within 8.5 kilometres of the\narticle's author-assigned true location. We also tested our approach on Twitter\nmessages that are tagged with the location from which the message was sent.\nTwitter texts are challenging because they are short and unstructured and often\ndo not contain words referring to the location they were sent from, but we\nobtain potentially useful results. We explain how we use the Spark framework\nfor data analytics to collect and process our test data. In general,\nclassification-based approaches for location tagging may be reaching their\nupper accuracy limit, but our precision-focused approach has high accuracy for\nsome texts and shows significant potential for improvement overall.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 07:09:54 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Brunsting", "Shawn", ""], ["De Sterck", "Hans", ""], ["Dolman", "Remco", ""], ["van Sprundel", "Teun", ""]]}, {"id": "1601.05952", "submitter": "Carsten Eickhoff", "authors": "Siddharth Sarda, Carsten Eickhoff, Thomas Hofmann", "title": "Semantic Place Descriptors for Classification and Map Discovery", "comments": "13 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban environments develop complex, non-obvious structures that are often\nhard to represent in the form of maps or guides. Finding the right place to go\noften requires intimate familiarity with the location in question and cannot\neasily be deduced by visitors. In this work, we exploit large-scale samples of\nusage information, in the form of mobile phone traces and geo-tagged Twitter\nmessages in order to automatically explore and annotate city maps via kernel\ndensity estimation. Our experiments are based on one year's worth of mobile\nphone activity collected by Nokia's Mobile Data Challenge (MDC). We show that\nusage information can be a strong predictor of semantic place categories,\nallowing us to automatically annotate maps based on the behavior of the local\nuser base.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 10:46:29 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Sarda", "Siddharth", ""], ["Eickhoff", "Carsten", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1601.06439", "submitter": "Amandianeze Nwana", "authors": "Amandianeze O. Nwana and Tsuhan Chen", "title": "Who Ordered This?: Exploiting Implicit User Tag Order Preferences for\n  Personalized Image Tagging", "comments": null, "journal-ref": null, "doi": "10.1109/ICMEW.2016.7574753", "report-no": null, "categories": "cs.IR cs.HC cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What makes a person pick certain tags over others when tagging an image? Does\nthe order that a person presents tags for a given image follow an implicit bias\nthat is personal? Can these biases be used to improve existing automated image\ntagging systems? We show that tag ordering, which has been largely overlooked\nby the image tagging community, is an important cue in understanding user\ntagging behavior and can be used to improve auto-tagging systems. Inspired by\nthe assumption that people order their tags, we propose a new way of measuring\ntag preferences, and also propose a new personalized tagging objective function\nthat explicitly considers a user's preferred tag orderings. We also provide a\n(partially) greedy algorithm that produces good solutions to our new objective\nand under certain conditions produces an optimal solution. We validate our\nmethod on a subset of Flickr images that spans 5000 users, over 5200 tags, and\nover 90,000 images. Our experiments show that exploiting personalized tag\norders improves the average performance of state-of-art approaches both on\nper-image and per-user bases.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 21:03:29 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Nwana", "Amandianeze O.", ""], ["Chen", "Tsuhan", ""]]}, {"id": "1601.06440", "submitter": "Amandianeze Nwana", "authors": "Amandianeze O. Nwana and Tsuhan Chen", "title": "QUOTE: \"Querying\" Users as Oracles in Tag Engines - A Semi-Supervised\n  Learning Approach to Personalized Image Tagging", "comments": null, "journal-ref": null, "doi": "10.1109/ISM.2016.0016", "report-no": null, "categories": "cs.IR cs.LG cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One common trend in image tagging research is to focus on visually relevant\ntags, and this tends to ignore the personal and social aspect of tags,\nespecially on photoblogging websites such as Flickr. Previous work has\ncorrectly identified that many of the tags that users provide on images are not\nvisually relevant (i.e. representative of the salient content in the image) and\nthey go on to treat such tags as noise, ignoring that the users chose to\nprovide those tags over others that could have been more visually relevant.\nAnother common assumption about user generated tags for images is that the\norder of these tags provides no useful information for the prediction of tags\non future images. This assumption also tends to define usefulness in terms of\nwhat is visually relevant to the image. For general tagging or labeling\napplications that focus on providing visual information about image content,\nthese assumptions are reasonable, but when considering personalized image\ntagging applications, these assumptions are at best too rigid, ignoring user\nchoice and preferences.\n  We challenge the aforementioned assumptions, and provide a machine learning\napproach to the problem of personalized image tagging with the following\ncontributions: 1.) We reformulate the personalized image tagging problem as a\nsearch/retrieval ranking problem, 2.) We leverage the order of tags, which does\nnot always reflect visual relevance, provided by the user in the past as a cue\nto their tag preferences, similar to click data, 3.) We propose a technique to\naugment sparse user tag data (semi-supervision), and 4.) We demonstrate the\nefficacy of our method on a subset of Flickr images, showing improvement over\nprevious state-of-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 21:07:16 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Nwana", "Amandianeze O.", ""], ["Chen", "Tsuhan", ""]]}, {"id": "1601.06919", "submitter": "Sebastiano Vigna", "authors": "Paolo Boldi, Andrea Marino, Massimo Santini, Sebastiano Vigna", "title": "BUbiNG: Massive Crawling for the Masses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although web crawlers have been around for twenty years by now, there is\nvirtually no freely available, opensource crawling software that guarantees\nhigh throughput, overcomes the limits of single-machine systems and at the same\ntime scales linearly with the amount of resources available. This paper aims at\nfilling this gap, through the description of BUbiNG, our next-generation web\ncrawler built upon the authors' experience with UbiCrawler [Boldi et al. 2004]\nand on the last ten years of research on the topic. BUbiNG is an opensource\nJava fully distributed crawler; a single BUbiNG agent, using sizeable hardware,\ncan crawl several thousands pages per second respecting strict politeness\nconstraints, both host- and IP-based. Unlike existing open-source distributed\ncrawlers that rely on batch techniques (like MapReduce), BUbiNG job\ndistribution is based on modern high-speed protocols so to achieve very high\nthroughput.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 08:22:37 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Boldi", "Paolo", ""], ["Marino", "Andrea", ""], ["Santini", "Massimo", ""], ["Vigna", "Sebastiano", ""]]}, {"id": "1601.07124", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Elvys Linhares Pontes, Juan-Manuel Torres-Moreno, Andr\\'ea Carneiro\n  Linhares", "title": "LIA-RAG: a system based on graphs and divergence of probabilities\n  applied to Speech-To-Text Summarization", "comments": "7 pages, 2 figures, CCCS Multiling 2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to introduces a new algorithm for automatic speech-to-text\nsummarization based on statistical divergences of probabilities and graphs. The\ninput is a text from speech conversations with noise, and the output a compact\ntext summary. Our results, on the pilot task CCCS Multiling 2015 French corpus\nare very encouraging\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 18:19:00 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Pontes", "Elvys Linhares", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["Linhares", "Andr\u00e9a Carneiro", ""]]}, {"id": "1601.07273", "submitter": "Gangli Liu", "authors": "Gangli Liu, Ling Feng", "title": "A Method to Support Difficult Re-finding Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Re-finding electronic documents from a personal computer is a frequent demand\nto users. In a simple re-finding task, people can use many methods to retrieve\na document, such as navigating directly to the document's folder, searching\nwith a desktop search engine, or checking the Recent Files List. However, when\nencountering a difficult re-finding task, people usually cannot remember the\nattributes used by conventional re-finding methods, such as file path, file\nname, keywords etc., the re-finding would fail. We propose a new method to\nsupport difficult re-finding tasks. When a user is reading a document, we\ncollect all kinds of possible memory pieces of the user about the document,\nsuch as number of pages, number of images, number of math formulas, cumulative\nreading time, reading frequency, printing experiences etc. If the user wants to\nre-find a document later, we use these collected attributes to filter out the\ntarget document. To alleviate the user's cognitive burden, we use a question\nand answer wizard interface and provide recommendations to the answers for the\nuser, the recommendations are generated by analyzing the collected attributes\nof each document and the user's experiences about them.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 06:27:56 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Liu", "Gangli", ""], ["Feng", "Ling", ""]]}, {"id": "1601.07754", "submitter": "Sergey Podlesnyy", "authors": "Anna Podlesnaya, Sergey Podlesnyy", "title": "Deep Learning Based Semantic Video Indexing and Retrieval", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We share the implementation details and testing results for video retrieval\nsystem based exclusively on features extracted by convolutional neural\nnetworks. We show that deep learned features might serve as universal signature\nfor semantic content of video useful in many search and retrieval tasks. We\nfurther show that graph-based storage structure for video index allows to\nefficiently retrieving the content with complicated spatial and temporal search\nqueries.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 13:43:30 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Podlesnaya", "Anna", ""], ["Podlesnyy", "Sergey", ""]]}, {"id": "1601.07885", "submitter": "Hua Sun", "authors": "Hua Sun and Syed A. Jafar", "title": "Blind Interference Alignment for Private Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind interference alignment (BIA) refers to interference alignment schemes\nthat are designed only based on channel coherence pattern knowledge at the\ntransmitters (the \"blind\" transmitters do not know the exact channel values).\nPrivate information retrieval (PIR) refers to the problem where a user\nretrieves one out of K messages from N non-communicating databases (each holds\nall K messages) without revealing anything about the identity of the desired\nmessage index to any individual database. In this paper, we identify an\nintriguing connection between PIR and BIA. Inspired by this connection, we\ncharacterize the information theoretic optimal download cost of PIR, when we\nhave K = 2 messages and the number of databases, N, is arbitrary.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 20:18:05 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Sun", "Hua", ""], ["Jafar", "Syed A.", ""]]}]