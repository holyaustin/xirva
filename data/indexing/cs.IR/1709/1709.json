[{"id": "1709.00149", "submitter": "Clayton Morrison", "authors": "Enrique Noriega-Atala, Marco A. Valenzuela-Escarcega, Clayton T.\n  Morrison, Mihai Surdeanu", "title": "Learning what to read: Focused machine reading", "comments": "6 pages, 1 figure, 1 algorithm, 2 tables, accepted to EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent efforts in bioinformatics have achieved tremendous progress in the\nmachine reading of biomedical literature, and the assembly of the extracted\nbiochemical interactions into large-scale models such as protein signaling\npathways. However, batch machine reading of literature at today's scale (PubMed\nalone indexes over 1 million papers per year) is unfeasible due to both cost\nand processing overhead. In this work, we introduce a focused reading approach\nto guide the machine reading of biomedical literature towards what literature\nshould be read to answer a biomedical query as efficiently as possible. We\nintroduce a family of algorithms for focused reading, including an intuitive,\nstrong baseline, and a second approach which uses a reinforcement learning (RL)\nframework that learns when to explore (widen the search) or exploit (narrow\nit). We demonstrate that the RL approach is capable of answering more queries\nthan the baseline, while being more efficient, i.e., reading fewer documents.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 04:09:42 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Noriega-Atala", "Enrique", ""], ["Valenzuela-Escarcega", "Marco A.", ""], ["Morrison", "Clayton T.", ""], ["Surdeanu", "Mihai", ""]]}, {"id": "1709.00155", "submitter": "Lei Sha", "authors": "Lei Sha, Lili Mou, Tianyu Liu, Pascal Poupart, Sujian Li, Baobao\n  Chang, Zhifang Sui", "title": "Order-Planning Neural Text Generation From Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating texts from structured data (e.g., a table) is important for\nvarious natural language processing tasks such as question answering and dialog\nsystems. In recent studies, researchers use neural language models and\nencoder-decoder frameworks for table-to-text generation. However, these neural\nnetwork-based approaches do not model the order of contents during text\ngeneration. When a human writes a summary based on a given table, he or she\nwould probably consider the content order before wording. In a biography, for\nexample, the nationality of a person is typically mentioned before occupation\nin a biography. In this paper, we propose an order-planning text generation\nmodel to capture the relationship between different fields and use such\nrelationship to make the generated text more fluent and smooth. We conducted\nexperiments on the WikiBio dataset and achieve significantly higher performance\nthan previous methods in terms of BLEU, ROUGE, and NIST scores.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 04:46:10 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Sha", "Lei", ""], ["Mou", "Lili", ""], ["Liu", "Tianyu", ""], ["Poupart", "Pascal", ""], ["Li", "Sujian", ""], ["Chang", "Baobao", ""], ["Sui", "Zhifang", ""]]}, {"id": "1709.00300", "submitter": "Yu Wang", "authors": "Yu Wang, Jixing Xu, Aohan Wu, Mantian Li, Yang He, Jinghe Hu, Weipeng\n  P. Yan", "title": "Telepath: Understanding Users from a Human Vision Perspective in\n  Large-Scale Recommender Systems", "comments": "8 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing an e-commerce recommender system that serves hundreds of millions\nof active users is a daunting challenge. From a human vision perspective,\nthere're two key factors that affect users' behaviors: items' attractiveness\nand their matching degree with users' interests. This paper proposes Telepath,\na vision-based bionic recommender system model, which understands users from\nsuch perspective. Telepath is a combination of a convolutional neural network\n(CNN), a recurrent neural network (RNN) and deep neural networks (DNNs). Its\nCNN subnetwork simulates the human vision system to extract key visual signals\nof items' attractiveness and generate corresponding activations. Its RNN and\nDNN subnetworks simulate cerebral cortex to understand users' interest based on\nthe activations generated from browsed items. In practice, the Telepath model\nhas been launched to JD's recommender system and advertising system. For one of\nthe major item recommendation blocks on the JD app, click-through rate (CTR),\ngross merchandise value (GMV) and orders have increased 1.59%, 8.16% and 8.71%\nrespectively. For several major ads publishers of JD demand-side platform, CTR,\nGMV and return on investment have increased 6.58%, 61.72% and 65.57%\nrespectively by the first launch, and further increased 2.95%, 41.75% and\n41.37% respectively by the second launch.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 13:29:36 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 16:06:57 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Wang", "Yu", ""], ["Xu", "Jixing", ""], ["Wu", "Aohan", ""], ["Li", "Mantian", ""], ["He", "Yang", ""], ["Hu", "Jinghe", ""], ["Yan", "Weipeng P.", ""]]}, {"id": "1709.00389", "submitter": "Jian Tang", "authors": "Jian Tang, Yue Wang, Kai Zheng, Qiaozhu Mei", "title": "End-to-end Learning for Short Text Expansion", "comments": "KDD'2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Effectively making sense of short texts is a critical task for many real\nworld applications such as search engines, social media services, and\nrecommender systems. The task is particularly challenging as a short text\ncontains very sparse information, often too sparse for a machine learning\nalgorithm to pick up useful signals. A common practice for analyzing short text\nis to first expand it with external information, which is usually harvested\nfrom a large collection of longer texts. In literature, short text expansion\nhas been done with all kinds of heuristics. We propose an end-to-end solution\nthat automatically learns how to expand short text to optimize a given learning\ntask. A novel deep memory network is proposed to automatically find relevant\ninformation from a collection of longer documents and reformulate the short\ntext through a gating mechanism. Using short text classification as a\ndemonstrating task, we show that the deep memory network significantly\noutperforms classical text expansion methods with comprehensive experiments on\nreal world data sets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 04:24:06 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Tang", "Jian", ""], ["Wang", "Yue", ""], ["Zheng", "Kai", ""], ["Mei", "Qiaozhu", ""]]}, {"id": "1709.00653", "submitter": "Viet Ha-Thuc", "authors": "Viet Ha-Thuc, Yan Yan, Xianren Wu, Vijay Dialani, Abhishek Gupta,\n  Shakti Sinha", "title": "From Query-By-Keyword to Query-By-Example: LinkedIn Talent Search\n  Approach", "comments": null, "journal-ref": null, "doi": "10.1145/3132847.3132869", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One key challenge in talent search is to translate complex criteria of a\nhiring position into a search query, while it is relatively easy for a searcher\nto list examples of suitable candidates for a given position. To improve search\nefficiency, we propose the next generation of talent search at LinkedIn, also\nreferred to as Search By Ideal Candidates. In this system, a searcher provides\none or several ideal candidates as the input to hire for a given position. The\nsystem then generates a query based on the ideal candidates and uses it to\nretrieve and rank results. Shifting from the traditional Query-By-Keyword to\nthis new Query-By-Example system poses a number of challenges: How to generate\na query that best describes the candidates? When moving to a completely\ndifferent paradigm, how does one leverage previous product logs to learn\nranking models and/or evaluate the new system with no existing usage logs?\nFinally, given the different nature between the two search paradigms, the\nranking features typically used for Query-By-Keyword systems might not be\noptimal for Query-By-Example. This paper describes our approach to solving\nthese challenges. We present experimental results confirming the effectiveness\nof the proposed solution, particularly on query building and search ranking\ntasks. As of writing this paper, the new system has been available to all\nLinkedIn members.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 02:02:08 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Ha-Thuc", "Viet", ""], ["Yan", "Yan", ""], ["Wu", "Xianren", ""], ["Dialani", "Vijay", ""], ["Gupta", "Abhishek", ""], ["Sinha", "Shakti", ""]]}, {"id": "1709.00714", "submitter": "Mitsuo Yoshida", "authors": "Yuki Kondo, Masatsugu Hangyo, Mitsuo Yoshida, Kyoji Umemura", "title": "Home Location Estimation Using Weather Observation Data", "comments": "The 2017 International Conference On Advanced Informatics: Concepts,\n  Theory And Application (ICAICTA2017)", "journal-ref": null, "doi": "10.1109/ICAICTA.2017.8090972", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We can extract useful information from social media data by adding the user's\nhome location. However, since the user's home location is generally not\npublicly available, many researchers have been attempting to develop a more\naccurate home location estimation. In this study, we propose a method to\nestimate a Twitter user's home location by using weather observation data from\nAMeDAS. In our method, we first estimate the weather of the area posted by an\nestimation target user by using the tweet, Next, we check out the estimated\nweather against weather observation data, and narrow down the area posted by\nthe user. Finally, the user's home location is estimated as which areas the\nuser frequently posts from. In our experiments, the results indicate that our\nmethod functions effectively and also demonstrate that accuracy improves under\ncertain conditions.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 13:20:16 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Kondo", "Yuki", ""], ["Hangyo", "Masatsugu", ""], ["Yoshida", "Mitsuo", ""], ["Umemura", "Kyoji", ""]]}, {"id": "1709.00740", "submitter": "Ga\\\"etan Hadjeres", "authors": "Ga\\\"etan Hadjeres and Frank Nielsen", "title": "Deep rank-based transposition-invariant distances on musical sequences", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distances on symbolic musical sequences are needed for a variety of\napplications, from music retrieval to automatic music generation. These musical\nsequences belong to a given corpus (or style) and it is obvious that a good\ndistance on musical sequences should take this information into account; being\nable to define a distance ex nihilo which could be applicable to all music\nstyles seems implausible. A distance could also be invariant under some\ntransformations, such as transpositions, so that it can be used as a distance\nbetween musical motives rather than musical sequences. However, to our\nknowledge, none of the approaches to devise musical distances seem to address\nthese issues. This paper introduces a method to build transposition-invariant\ndistances on symbolic musical sequences which are learned from data. It is a\nhybrid distance which combines learned feature representations of musical\nsequences with a handcrafted rank distance. This distance depends less on the\nmusical encoding of the data than previous methods and gives perceptually good\nresults. We demonstrate its efficiency on the dataset of chorale melodies by\nJ.S. Bach.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 16:43:25 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Hadjeres", "Ga\u00ebtan", ""], ["Nielsen", "Frank", ""]]}, {"id": "1709.00770", "submitter": "Muhammad Mahbubur Rahman", "authors": "Muhammad Mahbubur Rahman, Tim Finin", "title": "Understanding the Logical and Semantic Structure of Large Documents", "comments": "10 pages, 15 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current language understanding approaches focus on small documents, such as\nnewswire articles, blog posts, product reviews and discussion forum entries.\nUnderstanding and extracting information from large documents like legal\nbriefs, proposals, technical manuals and research articles is still a\nchallenging task. We describe a framework that can analyze a large document and\nhelp people to know where a particular information is in that document. We aim\nto automatically identify and classify semantic sections of documents and\nassign consistent and human-understandable labels to similar sections across\ndocuments. A key contribution of our research is modeling the logical and\nsemantic structure of an electronic document. We apply machine learning\ntechniques, including deep learning, in our prototype system. We also make\navailable a dataset of information about a collection of scholarly articles\nfrom the arXiv eprints collection that includes a wide range of metadata for\neach article, including a table of contents, section labels, section\nsummarizations and more. We hope that this dataset will be a useful resource\nfor the machine learning and NLP communities in information retrieval,\ncontent-based question answering and language modeling.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 21:38:56 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Rahman", "Muhammad Mahbubur", ""], ["Finin", "Tim", ""]]}, {"id": "1709.01190", "submitter": "Yiqiu Wang", "authors": "Yiqiu Wang, Anshumali Shrivastava, Jonathan Wang, Junghee Ryu", "title": "FLASH: Randomized Algorithms Accelerated over CPU-GPU for Ultra-High\n  Dimensional Similarity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FLASH (\\textbf{F}ast \\textbf{L}SH \\textbf{A}lgorithm for\n\\textbf{S}imilarity search accelerated with \\textbf{H}PC), a similarity search\nsystem for ultra-high dimensional datasets on a single machine, that does not\nrequire similarity computations and is tailored for high-performance computing\nplatforms. By leveraging a LSH style randomized indexing procedure and\ncombining it with several principled techniques, such as reservoir sampling,\nrecent advances in one-pass minwise hashing, and count based estimations, we\nreduce the computational and parallelization costs of similarity search, while\nretaining sound theoretical guarantees.\n  We evaluate FLASH on several real, high-dimensional datasets from different\ndomains, including text, malicious URL, click-through prediction, social\nnetworks, etc. Our experiments shed new light on the difficulties associated\nwith datasets having several million dimensions. Current state-of-the-art\nimplementations either fail on the presented scale or are orders of magnitude\nslower than FLASH. FLASH is capable of computing an approximate k-NN graph,\nfrom scratch, over the full webspam dataset (1.3 billion nonzeros) in less than\n10 seconds. Computing a full k-NN graph in less than 10 seconds on the webspam\ndataset, using brute-force ($n^2D$), will require at least 20 teraflops. We\nprovide CPU and GPU implementations of FLASH for replicability of our results.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 23:09:19 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 07:09:23 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Wang", "Yiqiu", ""], ["Shrivastava", "Anshumali", ""], ["Wang", "Jonathan", ""], ["Ryu", "Junghee", ""]]}, {"id": "1709.01256", "submitter": "Xiaofeng Zhu", "authors": "Xiaofeng Zhu, Diego Klabjan, Patrick Bless", "title": "Semantic Document Distance Measures and Unsupervised Document Revision\n  Detection", "comments": null, "journal-ref": "IJCNLP 2017", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we model the document revision detection problem as a minimum\ncost branching problem that relies on computing document distances.\nFurthermore, we propose two new document distance measures, word vector-based\nDynamic Time Warping (wDTW) and word vector-based Tree Edit Distance (wTED).\nOur revision detection system is designed for a large scale corpus and\nimplemented in Apache Spark. We demonstrate that our system can more precisely\ndetect revisions than state-of-the-art methods by utilizing the Wikipedia\nrevision dumps https://snap.stanford.edu/data/wiki-meta.html and simulated data\nsets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 06:47:03 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 03:36:40 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Zhu", "Xiaofeng", ""], ["Klabjan", "Diego", ""], ["Bless", "Patrick", ""]]}, {"id": "1709.01532", "submitter": "Wenjie Pei", "authors": "Wenjie Pei, Jie Yang, Zhu Sun, Jie Zhang, Alessandro Bozzon, David\n  M.J. Tax", "title": "Interacting Attention-gated Recurrent Networks for Recommendation", "comments": "Accepted by ACM International Conference on Information and Knowledge\n  Management (CIKM), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the temporal dynamics of user preferences over items is important\nfor recommendation. Existing methods mainly assume that all time steps in\nuser-item interaction history are equally relevant to recommendation, which\nhowever does not apply in real-world scenarios where user-item interactions can\noften happen accidentally. More importantly, they learn user and item dynamics\nseparately, thus failing to capture their joint effects on user-item\ninteractions. To better model user and item dynamics, we present the\nInteracting Attention-gated Recurrent Network (IARN) which adopts the attention\nmodel to measure the relevance of each time step. In particular, we propose a\nnovel attention scheme to learn the attention scores of user and item history\nin an interacting way, thus to account for the dependencies between user and\nitem dynamics in shaping user-item interactions. By doing so, IARN can\nselectively memorize different time steps of a user's history when predicting\nher preferences over different items. Our model can therefore provide\nmeaningful interpretations for recommendation results, which could be further\nenhanced by auxiliary features. Extensive validation on real-world datasets\nshows that IARN consistently outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 18:01:39 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 10:08:44 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Pei", "Wenjie", ""], ["Yang", "Jie", ""], ["Sun", "Zhu", ""], ["Zhang", "Jie", ""], ["Bozzon", "Alessandro", ""], ["Tax", "David M. J.", ""]]}, {"id": "1709.01584", "submitter": "Jill-J\\^enn Vie", "authors": "Jill-J\\^enn Vie, Florian Yger, Ryan Lahfa, Basile Clement, K\\'evin\n  Cocchi, Thomas Chalumeau and Hisashi Kashima", "title": "Using Posters to Recommend Anime and Mangas in a Cold-Start Scenario", "comments": "6 pages, 3 figures, 1 table, accepted at the MANPU 2017 workshop,\n  co-located with ICDAR 2017 in Kyoto on November 10, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Item cold-start is a classical issue in recommender systems that affects\nanime and manga recommendations as well. This problem can be framed as follows:\nhow to predict whether a user will like a manga that received few ratings from\nthe community? Content-based techniques can alleviate this issue but require\nextra information, that is usually expensive to gather. In this paper, we use a\ndeep learning technique, Illustration2Vec, to easily extract tag information\nfrom the manga and anime posters (e.g., sword, or ponytail). We propose BALSE\n(Blended Alternate Least Squares with Explanation), a new model for\ncollaborative filtering, that benefits from this extra information to recommend\nmangas. We show, using real data from an online manga recommender system called\nMangaki, that our model improves substantially the quality of recommendations,\nespecially for less-known manga, and is able to provide an interpretation of\nthe taste of the users.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 16:19:36 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 06:48:31 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Vie", "Jill-J\u00eann", ""], ["Yger", "Florian", ""], ["Lahfa", "Ryan", ""], ["Clement", "Basile", ""], ["Cocchi", "K\u00e9vin", ""], ["Chalumeau", "Thomas", ""], ["Kashima", "Hisashi", ""]]}, {"id": "1709.01687", "submitter": "Shashank Gupta", "authors": "Shashank Gupta, Sachin Pawar, Nitin Ramrakhiyani, Girish Palshikar and\n  Vasudeva Varma", "title": "Semi-Supervised Recurrent Neural Network for Adverse Drug Reaction\n  Mention Extraction", "comments": "Accepted at DTMBIO workshop, CIKM 2017. To appear in BMC\n  Bioinformatics. Pls cite that version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media is an useful platform to share health-related information due to\nits vast reach. This makes it a good candidate for public-health monitoring\ntasks, specifically for pharmacovigilance. We study the problem of extraction\nof Adverse-Drug-Reaction (ADR) mentions from social media, particularly from\ntwitter. Medical information extraction from social media is challenging,\nmainly due to short and highly information nature of text, as compared to more\ntechnical and formal medical reports.\n  Current methods in ADR mention extraction relies on supervised learning\nmethods, which suffers from labeled data scarcity problem. The State-of-the-art\nmethod uses deep neural networks, specifically a class of Recurrent Neural\nNetwork (RNN) which are Long-Short-Term-Memory networks (LSTMs)\n\\cite{hochreiter1997long}. Deep neural networks, due to their large number of\nfree parameters relies heavily on large annotated corpora for learning the end\ntask. But in real-world, it is hard to get large labeled data, mainly due to\nheavy cost associated with manual annotation. Towards this end, we propose a\nnovel semi-supervised learning based RNN model, which can leverage unlabeled\ndata also present in abundance on social media. Through experiments we\ndemonstrate the effectiveness of our method, achieving state-of-the-art\nperformance in ADR mention extraction.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 06:42:22 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Gupta", "Shashank", ""], ["Pawar", "Sachin", ""], ["Ramrakhiyani", "Nitin", ""], ["Palshikar", "Girish", ""], ["Varma", "Vasudeva", ""]]}, {"id": "1709.01709", "submitter": "Dan Li", "authors": "Dan Li, Evangelos Kanoulas", "title": "Active Sampling for Large-scale Information Retrieval Evaluation", "comments": null, "journal-ref": null, "doi": "10.1145/3132847.3133015", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation is crucial in Information Retrieval. The development of models,\ntools and methods has significantly benefited from the availability of reusable\ntest collections formed through a standardized and thoroughly tested\nmethodology, known as the Cranfield paradigm. Constructing these collections\nrequires obtaining relevance judgments for a pool of documents, retrieved by\nsystems participating in an evaluation task; thus involves immense human labor.\nTo alleviate this effort different methods for constructing collections have\nbeen proposed in the literature, falling under two broad categories: (a)\nsampling, and (b) active selection of documents. The former devises a smart\nsampling strategy by choosing only a subset of documents to be assessed and\ninferring evaluation measure on the basis of the obtained sample; the sampling\ndistribution is being fixed at the beginning of the process. The latter\nrecognizes that systems contributing documents to be judged vary in quality,\nand actively selects documents from good systems. The quality of systems is\nmeasured every time a new document is being judged. In this paper we seek to\nsolve the problem of large-scale retrieval evaluation combining the two\napproaches. We devise an active sampling method that avoids the bias of the\nactive selection methods towards good systems, and at the same time reduces the\nvariance of the current sampling approaches by placing a distribution over\nsystems, which varies as judgments become available. We validate the proposed\nmethod using TREC data and demonstrate the advantages of this new method\ncompared to past approaches.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 08:15:42 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Li", "Dan", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "1709.01775", "submitter": "Ekta Vats", "authors": "Ekta Vats and Anders Hast", "title": "On-the-fly Historical Handwritten Text Annotation", "comments": null, "journal-ref": "14th IAPR International Conference on Document Analysis and\n  Recognition (ICDAR), Volume 8, IEEE, Kyoto, Japan, 2017, pp. 10-14", "doi": "10.1109/ICDAR.2017.374", "report-no": null, "categories": "cs.IR cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of information retrieval algorithms depends upon the\navailability of ground truth labels annotated by experts. This is an important\nprerequisite, and difficulties arise when the annotated ground truth labels are\nincorrect or incomplete due to high levels of degradation. To address this\nproblem, this paper presents a simple method to perform on-the-fly annotation\nof degraded historical handwritten text in ancient manuscripts. The proposed\nmethod aims at quick generation of ground truth and correction of inaccurate\nannotations such that the bounding box perfectly encapsulates the word, and\ncontains no added noise from the background or surroundings. This method will\npotentially be of help to historians and researchers in generating and\ncorrecting word labels in a document dynamically. The effectiveness of the\nannotation method is empirically evaluated on an archival manuscript collection\nfrom well-known publicly available datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 11:27:41 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Vats", "Ekta", ""], ["Hast", "Anders", ""]]}, {"id": "1709.01782", "submitter": "Ekta Vats", "authors": "Ekta Vats, Anders Hast and Prashant Singh", "title": "Automatic Document Image Binarization using Bayesian Optimization", "comments": null, "journal-ref": "4th International Workshop on Historical Document Imaging and\n  Processing (HIP2017). ACM, New York, NY, USA, 89-94", "doi": "10.1145/3151509.3151520", "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document image binarization is often a challenging task due to various forms\nof degradation. Although there exist several binarization techniques in\nliterature, the binarized image is typically sensitive to control parameter\nsettings of the employed technique. This paper presents an automatic document\nimage binarization algorithm to segment the text from heavily degraded document\nimages. The proposed technique uses a two band-pass filtering approach for\nbackground noise removal, and Bayesian optimization for automatic\nhyperparameter selection for optimal results. The effectiveness of the proposed\nbinarization technique is empirically demonstrated on the Document Image\nBinarization Competition (DIBCO) and the Handwritten Document Image\nBinarization Competition (H-DIBCO) datasets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 11:47:31 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 08:25:26 GMT"}, {"version": "v3", "created": "Sat, 21 Oct 2017 11:20:24 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Vats", "Ekta", ""], ["Hast", "Anders", ""], ["Singh", "Prashant", ""]]}, {"id": "1709.01784", "submitter": "Xin Ji", "authors": "Xin Ji, Wei Wang, Meihui Zhang, Yang Yang", "title": "Cross-Domain Image Retrieval with Attention Modeling", "comments": "8 pages with an extra reference page", "journal-ref": "2017 ACM Multimedia Conference", "doi": "10.1145/3123266.3123429", "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the proliferation of e-commerce websites and the ubiquitousness of smart\nphones, cross-domain image retrieval using images taken by smart phones as\nqueries to search products on e-commerce websites is emerging as a popular\napplication. One challenge of this task is to locate the attention of both the\nquery and database images. In particular, database images, e.g. of fashion\nproducts, on e-commerce websites are typically displayed with other\naccessories, and the images taken by users contain noisy background and large\nvariations in orientation and lighting. Consequently, their attention is\ndifficult to locate. In this paper, we exploit the rich tag information\navailable on the e-commerce websites to locate the attention of database\nimages. For query images, we use each candidate image in the database as the\ncontext to locate the query attention. Novel deep convolutional neural network\narchitectures, namely TagYNet and CtxYNet, are proposed to learn the attention\nweights and then extract effective representations of the images. Experimental\nresults on public datasets confirm that our approaches have significant\nimprovement over the existing methods in terms of the retrieval accuracy and\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 11:49:46 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Ji", "Xin", ""], ["Wang", "Wei", ""], ["Zhang", "Meihui", ""], ["Yang", "Yang", ""]]}, {"id": "1709.01788", "submitter": "Ekta Vats", "authors": "Anders Hast and Ekta Vats", "title": "Radial Line Fourier Descriptor for Historical Handwritten Text\n  Representation", "comments": "under review", "journal-ref": null, "doi": "10.24132/JWSCG.2018.26.1.4", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic recognition of historical handwritten manuscripts is a daunting\ntask due to paper degradation over time. Recognition-free retrieval or word\nspotting is popularly used for information retrieval and digitization of the\nhistorical handwritten documents. However, the performance of word spotting\nalgorithms depends heavily on feature detection and representation methods.\nAlthough there exist popular feature descriptors such as Scale Invariant\nFeature Transform (SIFT) and Speeded Up Robust Features (SURF), the invariant\nproperties of these descriptors amplify the noise in the degraded document\nimages, rendering them more sensitive to noise and complex characteristics of\nhistorical manuscripts. Therefore, an efficient and relaxed feature descriptor\nis required as handwritten words across different documents are indeed similar,\nbut not identical. This paper introduces a Radial Line Fourier (RLF) descriptor\nfor handwritten word representation, with a short feature vector of 32\ndimensions. A segmentation-free and training-free handwritten word spotting\nmethod is studied herein that relies on the proposed RLF descriptor, takes into\naccount different keypoint representations and uses a simple\npreconditioner-based feature matching algorithm. The effectiveness of the RLF\ndescriptor for segmentation-free handwritten word spotting is empirically\nevaluated on well-known historical handwritten datasets using standard\nevaluation measures.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 11:56:34 GMT"}, {"version": "v2", "created": "Sat, 21 Oct 2017 11:39:39 GMT"}, {"version": "v3", "created": "Mon, 20 Nov 2017 14:30:47 GMT"}, {"version": "v4", "created": "Tue, 20 Mar 2018 15:23:08 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Hast", "Anders", ""], ["Vats", "Ekta", ""]]}, {"id": "1709.01922", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, Gy\\\"orgy Fazekas, Kyunghyun Cho and Mark Sandler", "title": "A Comparison of Audio Signal Preprocessing Methods for Deep Neural\n  Networks on Music Tagging", "comments": "5 pages. EUSIPCO 2018 camera-ready. arXiv:1706.02361 does not have\n  the overlapped part with this submission anymore", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we empirically investigate the effect of audio preprocessing\non music tagging with deep neural networks. We perform comprehensive\nexperiments involving audio preprocessing using different time-frequency\nrepresentations, logarithmic magnitude compression, frequency weighting, and\nscaling. We show that many commonly used input preprocessing techniques are\nredundant except magnitude compression.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 12:44:01 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 13:12:55 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 13:21:38 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "Gy\u00f6rgy", ""], ["Cho", "Kyunghyun", ""], ["Sandler", "Mark", ""]]}, {"id": "1709.01991", "submitter": "Monika Rani", "authors": "Monika Rani, Amit Kumar Dhar and O. P. Vyas", "title": "Semi-Automatic Terminology Ontology Learning Based on Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontologies provide features like a common vocabulary, reusability,\nmachine-readable content, and also allows for semantic search, facilitate agent\ninteraction and ordering & structuring of knowledge for the Semantic Web (Web\n3.0) application. However, the challenge in ontology engineering is automatic\nlearning, i.e., the there is still a lack of fully automatic approach from a\ntext corpus or dataset of various topics to form ontology using machine\nlearning techniques. In this paper, two topic modeling algorithms are explored,\nnamely LSI & SVD and Mr.LDA for learning topic ontology. The objective is to\ndetermine the statistical relationship between document and terms to build a\ntopic ontology and ontology graph with minimum human intervention. Experimental\nanalysis on building a topic ontology and semantic retrieving corresponding\ntopic ontology for the user's query demonstrating the effectiveness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 08:30:48 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Rani", "Monika", ""], ["Dhar", "Amit Kumar", ""], ["Vyas", "O. P.", ""]]}, {"id": "1709.02063", "submitter": "Faez Ahmed", "authors": "Faez Ahmed and Mark Fuge", "title": "Ranking ideas for diversity and quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When selecting ideas or trying to find inspiration, designers often must sift\nthrough hundreds or thousands of ideas. This paper provides an algorithm to\nrank design ideas such that the ranked list simultaneously maximizes the\nquality and diversity of recommended designs. To do so, we first define and\ncompare two diversity measures using Determinantal Point Processes (DPP) and\nadditive sub-modular functions. We show that DPPs are more suitable for items\nexpressed as text and that a greedy algorithm diversifies rankings with both\ntheoretical guarantees and empirical performance on what is otherwise an\nNP-Hard problem. To produce such rankings, this paper contributes a novel way\nto extend quality and diversity metrics from sets to permutations of ranked\nlists.\n  These rank metrics open up the use of multi-objective optimization to\ndescribe trade-offs between diversity and quality in ranked lists. We use such\ntrade-off fronts to help designers select rankings using indifference curves.\nHowever, we also show that rankings on trade-off front share a number of\ntop-ranked items; this means reviewing items (for a given depth like the top\n10) from across the entire diversity-to-quality front incurs only a marginal\nincrease in the number of designs considered. While the proposed techniques are\ngeneral purpose enough to be used across domains, we demonstrate concrete\nperformance on selecting items in an online design community (OpenIDEO), where\nour approach reduces the time required to review diverse, high-quality ideas\nfrom around 25 hours to 90 minutes. This makes evaluation of crowd-generated\nideas tractable for a single designer. Our code is publicly accessible for\nfurther research.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 04:10:42 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Ahmed", "Faez", ""], ["Fuge", "Mark", ""]]}, {"id": "1709.02076", "submitter": "Clayton Morrison", "authors": "Donya Quick, Clayton T. Morrison", "title": "Composition by Conversation", "comments": "6 pages, 8 figures, accepted to ICMC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.IR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most musical programming languages are developed purely for coding virtual\ninstruments or algorithmic compositions. Although there has been some work in\nthe domain of musical query languages for music information retrieval, there\nhas been little attempt to unify the principles of musical programming and\nquery languages with cognitive and natural language processing models that\nwould facilitate the activity of composition by conversation. We present a\nprototype framework, called MusECI, that merges these domains, permitting\nscore-level algorithmic composition in a text editor while also supporting\nconnectivity to existing natural language processing frameworks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 05:39:00 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Quick", "Donya", ""], ["Morrison", "Clayton T.", ""]]}, {"id": "1709.02116", "submitter": "Adam G. Dunn", "authors": "Adam G. Dunn, Enrico Coiera, Florence Bourgeois", "title": "Unreported links between trial registrations and published articles were\n  identified using document similarity measures in a cross-sectional analysis\n  of ClinicalTrials.gov", "comments": "8 pages, 3 figures", "journal-ref": "Journal of Clinical Epidemiology, 2018; 95:94-101", "doi": "10.1016/j.jclinepi.2017.12.007", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: Trial registries can be used to measure reporting biases and\nsupport systematic reviews but 45% of registrations do not provide a link to\nthe article reporting on the trial. We evaluated the use of document similarity\nmethods to identify unreported links between ClinicalTrials.gov and PubMed.\nStudy Design and Setting: We extracted terms and concepts from a dataset of\n72,469 ClinicalTrials.gov registrations and 276,307 PubMed articles, and tested\nmethods for ranking articles across 16,005 reported links and 90\nmanually-identified unreported links. Performance was measured by the median\nrank of matching articles, and the proportion of unreported links that could be\nfound by screening ranked candidate articles in order. Results: The best\nperforming concept-based representation produced a median rank of 3 (IQR 1-21)\nfor reported links and 3 (IQR 1-19) for the manually-identified unreported\nlinks, and term-based representations produced a median rank of 2 (1-20) for\nreported links and 2 (IQR 1-12) in unreported links. The matching article was\nranked first for 40% of registrations, and screening 50 candidate articles per\nregistration identified 86% of the unreported links. Conclusions: Leveraging\nthe growth in the corpus of reported links between ClinicalTrials.gov and\nPubMed, we found that document similarity methods can assist in the\nidentification of unreported links between trial registrations and\ncorresponding articles.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 07:37:10 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 13:29:36 GMT"}, {"version": "v3", "created": "Sun, 7 Jan 2018 06:13:27 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Dunn", "Adam G.", ""], ["Coiera", "Enrico", ""], ["Bourgeois", "Florence", ""]]}, {"id": "1709.02261", "submitter": "Chris Hartgerink", "authors": "Chris Hartgerink and Peter Murray-Rust", "title": "Extracting data from vector figures in scholarly articles", "comments": "4 figures, 1 table", "journal-ref": null, "doi": "10.5281/zenodo.839536", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  It is common for authors to communicate their results in graphical figures,\nbut those data are frequently unavailable for reanalysis. Reconstructing data\npoints from a figure manually requires the author to measure the coordinates\neither on printed pages using a ruler, or from the display screen using a\ncursor. This is time-consuming (often hours) and error-prone, and limited by\nthe precision of the display or ruler. What is often not realised is that the\ndata themselves are held in the PDF document to much higher precision (usually\n0.0-0.01 pixels), if the figure is stored in vector format. We developed alpha\nsoftware to automatically reconstruct data from vector figures and tested it on\nfunnel plots in the meta-analysis literature. Our results indicate that\nreconstructing data from vector based figures is promising, where we correctly\nextracted data for 12 out of 24 funnel plots with extracted data (50%).\nHowever, we observed that vector based figures are relatively sparse (15 out of\n136 papers with funnel plots) and strongly insist publishers to provide more\nvector based data figures in the near future for the benefit of the scholarly\ncommunity.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 14:18:36 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 10:40:16 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 14:01:06 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Hartgerink", "Chris", ""], ["Murray-Rust", "Peter", ""]]}, {"id": "1709.02291", "submitter": "Monika Doerfler", "authors": "Monika Doerfler, Thomas Grill, Roswitha Bammer, Arthur Flexer", "title": "Basic Filters for Convolutional Neural Networks Applied to Music:\n  Training or Design?", "comments": "Completely revised version; 21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When convolutional neural networks are used to tackle learning problems based\non music or, more generally, time series data, raw one-dimensional data are\ncommonly pre-processed to obtain spectrogram or mel-spectrogram coefficients,\nwhich are then used as input to the actual neural network. In this\ncontribution, we investigate, both theoretically and experimentally, the\ninfluence of this pre-processing step on the network's performance and pose the\nquestion, whether replacing it by applying adaptive or learned filters directly\nto the raw data, can improve learning success. The theoretical results show\nthat approximately reproducing mel-spectrogram coefficients by applying\nadaptive filters and subsequent time-averaging is in principle possible. We\nalso conducted extensive experimental work on the task of singing voice\ndetection in music. The results of these experiments show that for\nclassification based on Convolutional Neural Networks the features obtained\nfrom adaptive filter banks followed by time-averaging perform better than the\ncanonical Fourier-transform-based mel-spectrogram coefficients. Alternative\nadaptive approaches with center frequencies or time-averaging lengths learned\nfrom training data perform equally well.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 14:51:37 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 11:25:18 GMT"}, {"version": "v3", "created": "Wed, 19 Sep 2018 14:30:19 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Doerfler", "Monika", ""], ["Grill", "Thomas", ""], ["Bammer", "Roswitha", ""], ["Flexer", "Arthur", ""]]}, {"id": "1709.02858", "submitter": "Aritra Banerjee", "authors": "Aritra Banerjee, Shrey Choudhary", "title": "Advanced Page Rank Algorithm with Semantics, In Links, Out Links and\n  Google Analytics", "comments": "6 pages, 2 figures, Published with International Journal of Computer\n  Trends and Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology(IJCTT) V50\n  (3):137-142, August 2017. ISSN:2231-2803. Published by Seventh Sense Research\n  Group", "doi": "10.14445/22312803/IJCTT-V50P124", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we have modified the existing page ranking mechanism as an\nadvanced Page Rank Algorithm based on Semantics Inlinks Outlinks and Google\nAnalytics. We have used Semantics page ranking to rank pages according to the\nword searched and match it with the metadata of the website and provide a value\nof rank according to the highest priority.We have also used Google analytics to\nstore the number of hits of a website in a particular variable and add the\nrequired percentage amount to the ranking procedure.The proposed algorithm is\nused to find more relevant information according to users query.So this concept\nis very useful to display most valuable pages on the top of the result list on\nthe basis of user browsing behaviour which reduce the search space to a large\nscale.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 13:17:46 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Banerjee", "Aritra", ""], ["Choudhary", "Shrey", ""]]}, {"id": "1709.03061", "submitter": "Douglas Teodoro", "authors": "Douglas Teodoro, Luc Mottin, Julien Gobeill, Arnaud Gaudinat,\n  Th\\'er\\`ese Vachon, Patrick Ruch", "title": "Improving average ranking precision in user searches for biomedical\n  research datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Availability of research datasets is keystone for health and life science\nstudy reproducibility and scientific progress. Due to the heterogeneity and\ncomplexity of these data, a main challenge to be overcome by research data\nmanagement systems is to provide users with the best answers for their search\nqueries. In the context of the 2016 bioCADDIE Dataset Retrieval Challenge, we\ninvestigate a novel ranking pipeline to improve the search of datasets used in\nbiomedical experiments. Our system comprises a query expansion model based on\nword embeddings, a similarity measure algorithm that takes into consideration\nthe relevance of the query terms, and a dataset categorisation method that\nboosts the rank of datasets matching query constraints. The system was\nevaluated using a corpus with 800k datasets and 21 annotated user queries. Our\nsystem provides competitive results when compared to the other challenge\nparticipants. In the official run, it achieved the highest infAP among the\nparticipants, being +22.3% higher than the median infAP of the participant's\nbest submissions. Overall, it is ranked at top 2 if an aggregated metric using\nthe best official measures per participant is considered. The query expansion\nmethod showed positive impact on the system's performance increasing our\nbaseline up to +5.0% and +3.4% for the infAP and infNDCG metrics, respectively.\nOur similarity measure algorithm seems to be robust, in particular compared to\nDivergence From Randomness framework, having smaller performance variations\nunder different training conditions. Finally, the result categorization did not\nhave significant impact on the system's performance. We believe that our\nsolution could be used to enhance biomedical dataset management systems. In\nparticular, the use of data driven query expansion methods could be an\nalternative to the complexity of biomedical terminologies.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 07:35:21 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Teodoro", "Douglas", ""], ["Mottin", "Luc", ""], ["Gobeill", "Julien", ""], ["Gaudinat", "Arnaud", ""], ["Vachon", "Th\u00e9r\u00e8se", ""], ["Ruch", "Patrick", ""]]}, {"id": "1709.03260", "submitter": "Tomohiro Manabe", "authors": "Tomohiro Manabe and Sumio Fujita", "title": "A Short Note on Proximity-based Scoring of Documents with Multiple\n  Fields", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The BM25 ranking function is one of the most well known query relevance\ndocument scoring functions and many variations of it are proposed. The BM25F\nfunction is one of its adaptations designed for modeling documents with\nmultiple fields. The Expanded Span method extends a BM25-like function by\ntaking into considerations of the proximity between term occurrences. In this\nnote, we combine these two variations into one scoring method in view of\nproximity-based scoring of documents with multiple fields.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 06:25:46 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Manabe", "Tomohiro", ""], ["Fujita", "Sumio", ""]]}, {"id": "1709.03496", "submitter": "{\\L}ukasz Kidzi\\'nski", "authors": "{\\L}ukasz Kidzi\\'nski", "title": "SweetRS: Dataset for a recommender systems of sweets", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benchmarking recommender system and matrix completion algorithms could be\ngreatly simplified if the entire matrix was known. We built a \\url{sweetrs.org}\nplatform with $77$ candies and sweets to rank. Over $2000$ users submitted over\n$44000$ grades resulting in a matrix with $28\\%$ coverage. In this report, we\ngive the full description of the environment and we benchmark the\n\\textsc{Soft-Impute} algorithm on the dataset.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 23:19:41 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Kidzi\u0144ski", "\u0141ukasz", ""]]}, {"id": "1709.03621", "submitter": "Tingting Liang", "authors": "Tingting Liang, Lifang He, Chun-Ta Lu, Liang Chen, Philip S. Yu, Jian\n  Wu", "title": "A Broad Learning Approach for Context-Aware Mobile Application\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of mobile apps, the availability of a large number\nof mobile apps in application stores brings challenge to locate appropriate\napps for users. Providing accurate mobile app recommendation for users becomes\nan imperative task. Conventional approaches mainly focus on learning users'\npreferences and app features to predict the user-app ratings. However, most of\nthem did not consider the interactions among the context information of apps.\nTo address this issue, we propose a broad learning approach for\n\\textbf{C}ontext-\\textbf{A}ware app recommendation with \\textbf{T}ensor\n\\textbf{A}nalysis (CATA). Specifically, we utilize a tensor-based framework to\neffectively integrate user's preference, app category information and\nmulti-view features to facilitate the performance of app rating prediction. The\nmultidimensional structure is employed to capture the hidden relationships\nbetween multiple app categories with multi-view features. We develop an\nefficient factorization method which applies Tucker decomposition to learn the\nfull-order interactions within multiple categories and features. Furthermore,\nwe employ a group $\\ell_{1}-$norm regularization to learn the group-wise\nfeature importance of each view with respect to each app category. Experiments\non two real-world mobile app datasets demonstrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 23:11:46 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Liang", "Tingting", ""], ["He", "Lifang", ""], ["Lu", "Chun-Ta", ""], ["Chen", "Liang", ""], ["Yu", "Philip S.", ""], ["Wu", "Jian", ""]]}, {"id": "1709.03742", "submitter": "Christina Lioma Assoc. Prof", "authors": "Christina Lioma", "title": "Dependencies: Formalising Semantic Catenae for Information Retrieval", "comments": "This document is a doktordisputats - a dissertation within the Danish\n  academic system required to obtain the degree of \\textit{Doctor Scientiarum},\n  in form and function equivalent to the French and German Habilitation and the\n  Higher Doctorate of the Commonwealth", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building machines that can understand text like humans is an AI-complete\nproblem. A great deal of research has already gone into this, with astounding\nresults, allowing everyday people to discuss with their telephones, or have\ntheir reading materials analysed and classified by computers. A prerequisite\nfor processing text semantics, common to the above examples, is having some\ncomputational representation of text as an abstract object. Operations on this\nrepresentation practically correspond to making semantic inferences, and by\nextension simulating understanding text. The complexity and granularity of\nsemantic processing that can be realised is constrained by the mathematical and\ncomputational robustness, expressiveness, and rigour of the tools used.\n  This dissertation contributes a series of such tools, diverse in their\nmathematical formulation, but common in their application to model semantic\ninferences when machines process text. These tools are principally expressed in\nnine distinct models that capture aspects of semantic dependence in highly\ninterpretable and non-complex ways. This dissertation further reflects on\npresent and future problems with the current research paradigm in this area,\nand makes recommendations on how to overcome them.\n  The amalgamation of the body of work presented in this dissertation advances\nthe complexity and granularity of semantic inferences that can be made\nautomatically by machines.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 08:54:02 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Lioma", "Christina", ""]]}, {"id": "1709.03968", "submitter": "Nabiha Asghar", "authors": "Nabiha Asghar, Pascal Poupart, Jesse Hoey, Xin Jiang, Lili Mou", "title": "Affective Neural Response Generation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing neural conversational models process natural language primarily on a\nlexico-syntactic level, thereby ignoring one of the most crucial components of\nhuman-to-human dialogue: its affective content. We take a step in this\ndirection by proposing three novel ways to incorporate affective/emotional\naspects into long short term memory (LSTM) encoder-decoder neural conversation\nmodels: (1) affective word embeddings, which are cognitively engineered, (2)\naffect-based objective functions that augment the standard cross-entropy loss,\nand (3) affectively diverse beam search for decoding. Experiments show that\nthese techniques improve the open-domain conversational prowess of\nencoder-decoder networks by enabling them to produce emotionally rich responses\nthat are more interesting and natural.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 17:41:30 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Asghar", "Nabiha", ""], ["Poupart", "Pascal", ""], ["Hoey", "Jesse", ""], ["Jiang", "Xin", ""], ["Mou", "Lili", ""]]}, {"id": "1709.04095", "submitter": "Audrey Durand", "authors": "Audrey Durand, Jean-Alexandre Beaumont, Christian Gagne, Michel Lemay,\n  Sebastien Paquet", "title": "Query Completion Using Bandits for Engines Aggregation", "comments": "Presented at The 3rd Multidisciplinary Conference on Reinforcement\n  Learning and Decision Making (RLDM), June 11-14 (2017), University of\n  Michigan, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assisting users by suggesting completed queries as they type is a common\nfeature of search systems known as query auto-completion. A query\nauto-completion engine may use prior signals and available information (e.g.,\nuser is anonymous, user has a history, user visited the site before the search\nor not, etc.) in order to improve its recommendations. There are many possible\nstrategies for query auto-completion and a challenge is to design one optimal\nengine that considers and uses all available information. When different\nstrategies are used to produce the suggestions, it becomes hard to rank these\nheterogeneous suggestions. An alternative strategy could be to aggregate\nseveral engines in order to enhance the diversity of recommendations by\ncombining the capacity of each engine to digest available information\ndifferently, while keeping the simplicity of each engine. The main objective of\nthis research is therefore to find such mixture of query completion engines\nthat would beat any engine taken alone. We tackle this problem under the\nbandits setting and evaluate four strategies to overcome this challenge.\nExperiments conducted on three real datasets show that a mixture of engines can\noutperform a single engine.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 00:44:09 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Durand", "Audrey", ""], ["Beaumont", "Jean-Alexandre", ""], ["Gagne", "Christian", ""], ["Lemay", "Michel", ""], ["Paquet", "Sebastien", ""]]}, {"id": "1709.04747", "submitter": "Jerome Darmont", "authors": "Ciprian-Octavian Truic\\u{a} (UPB), J\\'er\\^ome Darmont (ERIC)", "title": "T${}^2$K${}^2$: The Twitter Top-K Keywords Benchmark", "comments": null, "journal-ref": "21st European Conference on Advances in Databases and Information\n  Systems (ADBIS 2017), Sep 2017, Nicosie, Cyprus. Springer, Communications in\n  Computer and Information Science, 767, pp.21-28, 2017, New Trends in\n  Databases and Information Systems", "doi": "10.1007/978-3-319-67162-8", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information retrieval from textual data focuses on the construction of\nvocabularies that contain weighted term tuples. Such vocabularies can then be\nexploited by various text analysis algorithms to extract new knowledge, e.g.,\ntop-k keywords, top-k documents, etc. Top-k keywords are casually used for\nvarious purposes, are often computed on-the-fly, and thus must be efficiently\ncomputed. To compare competing weighting schemes and database implementations,\nbenchmarking is customary. To the best of our knowledge, no benchmark currently\naddresses these problems. Hence, in this paper, we present a top-k keywords\nbenchmark, T${}^2$K${}^2$, which features a real tweet dataset and queries with\nvarious complexities and selectivities. T${}^2$K${}^2$ helps evaluate weighting\nschemes and database implementations in terms of computing performance. To\nillustrate T${}^2$K${}^2$'s relevance and genericity, we successfully performed\ntests on the TF-IDF and Okapi BM25 weighting schemes, on one hand, and on\ndifferent relational (Oracle, PostgreSQL) and document-oriented (MongoDB)\ndatabase implementations, on the other hand.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 13:01:51 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Truic\u0103", "Ciprian-Octavian", "", "UPB"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1709.05050", "submitter": "Rohit Reddy Muthyala", "authors": "Rohit Muthyala, Sam Wood, Yi Jin, Yixing Qin, Hua Gao, Amit Rai", "title": "Data-driven Job Search Engine Using Skills and Company Attribute Filters", "comments": "8 pages, 10 figures, ICDM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  According to a report online, more than 200 million unique users search for\njobs online every month. This incredibly large and fast growing demand has\nenticed software giants such as Google and Facebook to enter this space, which\nwas previously dominated by companies such as LinkedIn, Indeed and\nCareerBuilder. Recently, Google released their \"AI-powered Jobs Search Engine\",\n\"Google For Jobs\" while Facebook released \"Facebook Jobs\" within their\nplatform. These current job search engines and platforms allow users to search\nfor jobs based on general narrow filters such as job title, date posted,\nexperience level, company and salary. However, they have severely limited\nfilters relating to skill sets such as C++, Python, and Java and company\nrelated attributes such as employee size, revenue, technographics and\nmicro-industries. These specialized filters can help applicants and companies\nconnect at a very personalized, relevant and deeper level. In this paper we\npresent a framework that provides an end-to-end \"Data-driven Jobs Search\nEngine\". In addition, users can also receive potential contacts of recruiters\nand senior positions for connection and networking opportunities. The high\nlevel implementation of the framework is described as follows: 1) Collect job\npostings data in the United States, 2) Extract meaningful tokens from the\npostings data using ETL pipelines, 3) Normalize the data set to link company\nnames to their specific company websites, 4) Extract and ranking the skill\nsets, 5) Link the company names and websites to their respective company level\nattributes with the EVERSTRING Company API, 6) Run user-specific search queries\non the database to identify relevant job postings and 7) Rank the job search\nresults. This framework offers a highly customizable and highly targeted search\nexperience for end users.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 04:07:00 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Muthyala", "Rohit", ""], ["Wood", "Sam", ""], ["Jin", "Yi", ""], ["Qin", "Yixing", ""], ["Gao", "Hua", ""], ["Rai", "Amit", ""]]}, {"id": "1709.05135", "submitter": "Laming Chen", "authors": "Laming Chen, Guoxin Zhang, Hanning Zhou", "title": "Fast Greedy MAP Inference for Determinantal Point Process to Improve\n  Recommendation Diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The determinantal point process (DPP) is an elegant probabilistic model of\nrepulsion with applications in various machine learning tasks including\nsummarization and search. However, the maximum a posteriori (MAP) inference for\nDPP which plays an important role in many applications is NP-hard, and even the\npopular greedy algorithm can still be too computationally expensive to be used\nin large-scale real-time scenarios. To overcome the computational challenge, in\nthis paper, we propose a novel algorithm to greatly accelerate the greedy MAP\ninference for DPP. In addition, our algorithm also adapts to scenarios where\nthe repulsion is only required among nearby few items in the result sequence.\nWe apply the proposed algorithm to generate relevant and diverse\nrecommendations. Experimental results show that our proposed algorithm is\nsignificantly faster than state-of-the-art competitors, and provides a better\nrelevance-diversity trade-off on several public datasets, which is also\nconfirmed in an online A/B test.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 09:52:06 GMT"}, {"version": "v2", "created": "Sat, 26 May 2018 14:17:04 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Chen", "Laming", ""], ["Zhang", "Guoxin", ""], ["Zhou", "Hanning", ""]]}, {"id": "1709.05168", "submitter": "Evgeny Krivosheev", "authors": "Evgeny Krivosheev, Fabio Casati, Valentina Caforio, Boualem Benatallah", "title": "Crowdsourcing Paper Screening in Systematic Literature Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literature reviews allow scientists to stand on the shoulders of giants,\nshowing promising directions, summarizing progress, and pointing out existing\nchallenges in research. At the same time conducting a systematic literature\nreview is a laborious and consequently expensive process. In the last decade,\nthere have a few studies on crowdsourcing in literature reviews. This paper\nexplores the feasibility of crowdsourcing for facilitating the literature\nreview process in terms of results, time and effort, as well as to identify\nwhich crowdsourcing strategies provide the best results based on the budget\navailable. In particular we focus on the screening phase of the literature\nreview process and we contribute and assess methods for identifying the size of\ntests, labels required per paper, and classification functions as well as\nmethods to split the crowdsourcing process in phases to improve results.\nFinally, we present our findings based on experiments run on Crowdflower.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 11:59:15 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Krivosheev", "Evgeny", ""], ["Casati", "Fabio", ""], ["Caforio", "Valentina", ""], ["Benatallah", "Boualem", ""]]}, {"id": "1709.05193", "submitter": "Stefano Ferretti", "authors": "Stefano Ferretti", "title": "Clustering of Musical Pieces through Complex Networks: an Assessment\n  over Guitar Solos", "comments": "to appear in IEEE Multimedia magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SD cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Musical pieces can be modeled as complex networks. This fosters innovative\nways to categorize music, paving the way towards novel applications in\nmultimedia domains, such as music didactics, multimedia entertainment and\ndigital music generation. Clustering these networks through their main metrics\nallows grouping similar musical tracks. To show the viability of the approach,\nwe provide results on a dataset of guitar solos.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 07:24:08 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Ferretti", "Stefano", ""]]}, {"id": "1709.05278", "submitter": "Daoud Clarke", "authors": "Dion Bailey, Tom Pajak, Daoud Clarke, Carlos Rodriguez", "title": "Algorithms and Architecture for Real-time Recommendations at News UK", "comments": "Accepted for presentation at AI-2017 Thirty-seventh SGAI\n  International Conference on Artificial Intelligence. Cambridge, England 12-14\n  December 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems are recognised as being hugely important in industry,\nand the area is now well understood. At News UK, there is a requirement to be\nable to quickly generate recommendations for users on news items as they are\npublished. However, little has been published about systems that can generate\nrecommendations in response to changes in recommendable items and user\nbehaviour in a very short space of time. In this paper we describe a new\nalgorithm for updating collaborative filtering models incrementally, and\ndemonstrate its effectiveness on clickstream data from The Times. We also\ndescribe the architecture that allows recommendations to be generated on the\nfly, and how we have made each component scalable. The system is currently\nbeing used in production at News UK.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 15:47:23 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Bailey", "Dion", ""], ["Pajak", "Tom", ""], ["Clarke", "Daoud", ""], ["Rodriguez", "Carlos", ""]]}, {"id": "1709.05298", "submitter": "Svitlana Vakulenko", "authors": "Svitlana Vakulenko, Ilya Markov and Maarten de Rijke", "title": "Conversational Exploratory Search via Interactive Storytelling", "comments": "Accepted at ICTIR'17 Workshop on Search-Oriented Conversational AI\n  (SCAI 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational interfaces are likely to become more efficient, intuitive and\nengaging way for human-computer interaction than today's text or touch-based\ninterfaces. Current research efforts concerning conversational interfaces focus\nprimarily on question answering functionality, thereby neglecting support for\nsearch activities beyond targeted information lookup. Users engage in\nexploratory search when they are unfamiliar with the domain of their goal,\nunsure about the ways to achieve their goals, or unsure about their goals in\nthe first place. Exploratory search is often supported by approaches from\ninformation visualization. However, such approaches cannot be directly\ntranslated to the setting of conversational search.\n  In this paper we investigate the affordances of interactive storytelling as a\ntool to enable exploratory search within the framework of a conversational\ninterface. Interactive storytelling provides a way to navigate a document\ncollection in the pace and order a user prefers. In our vision, interactive\nstorytelling is to be coupled with a dialogue-based system that provides verbal\nexplanations and responsive design. We discuss challenges and sketch the\nresearch agenda required to put this vision into life.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 16:42:17 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["Markov", "Ilya", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1709.05700", "submitter": "Amin Jaber", "authors": "Amin Jaber and Fadi A. Zaraket", "title": "Morphology-based Entity and Relational Entity Extraction Framework for\n  Arabic", "comments": null, "journal-ref": "Traitement Automatique des Langues (TAL). 58.3 (2017): 97-121", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rule-based techniques to extract relational entities from documents allow\nusers to specify desired entities with natural language questions, finite state\nautomata, regular expressions and structured query language. They require\nlinguistic and programming expertise and lack support for Arabic morphological\nanalysis. We present a morphology-based entity and relational entity extraction\nframework for Arabic (MERF). MERF requires basic knowledge of linguistic\nfeatures and regular expressions, and provides the ability to interactively\nspecify Arabic morphological and synonymity features, tag types associated with\nregular expressions, and relations and code actions defined over matches of\nsubexpressions. MERF constructs entities and relational entities from matches\nof the specifications. We evaluated MERF with several case studies. The results\nshow that MERF requires shorter development time and effort compared to\nexisting application specific techniques and produces reasonably accurate\nresults within a reasonable overhead in run time.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 17:56:24 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 17:01:12 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Jaber", "Amin", ""], ["Zaraket", "Fadi A.", ""]]}, {"id": "1709.05743", "submitter": "Jan R. Benetka", "authors": "Jan R. Benetka, Krisztian Balog, Kjetil N{\\o}rv{\\aa}g", "title": "Towards Building a Knowledge Base of Monetary Transactions from a News\n  Collection", "comments": "Proceedings of the 17th ACM/IEEE-CS Joint Conference on Digital\n  Libraries (JCDL '17), 2017", "journal-ref": null, "doi": "10.1109/JCDL.2017.7991575", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of extracting structured representations of economic\nevents from a large corpus of news articles, using a combination of natural\nlanguage processing and machine learning techniques. The developed techniques\nallow for semi-automatic population of a financial knowledge base, which, in\nturn, may be used to support a range of data mining and exploration tasks. The\nkey challenge we face in this domain is that the same event is often reported\nmultiple times, with varying correctness of details. We address this challenge\nby first collecting all information pertinent to a given event from the entire\ncorpus, then considering all possible representations of the event, and\nfinally, using a supervised learning method, to rank these representations by\nthe associated confidence scores. A main innovative element of our approach is\nthat it jointly extracts and stores all attributes of the event as a single\nrepresentation (quintuple). Using a purpose-built test set we demonstrate that\nour supervised learning approach can achieve 25% improvement in F1-score over\nbaseline methods that consider the earliest, the latest or the most frequent\nreporting of the event.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 02:09:08 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Benetka", "Jan R.", ""], ["Balog", "Krisztian", ""], ["N\u00f8rv\u00e5g", "Kjetil", ""]]}, {"id": "1709.05749", "submitter": "Jan R. Benetka", "authors": "Jan R. Benetka, Krisztian Balog, Kjetil N{\\o}rv{\\aa}g", "title": "Anticipating Information Needs Based on Check-in Activity", "comments": "Proceedings of the 10th ACM International Conference on Web Search\n  and Data Mining (WSDM '17), 2017", "journal-ref": null, "doi": "10.1145/3018661.3018679", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the development of a smart personal assistant that is\ncapable of anticipating a user's information needs based on a novel type of\ncontext: the person's activity inferred from her check-in records on a\nlocation-based social network. Our main contribution is a method that\ntranslates a check-in activity into an information need, which is in turn\naddressed with an appropriate information card. This task is challenging\nbecause of the large number of possible activities and related information\nneeds, which need to be addressed in a mobile dashboard that is limited in\nsize. Our approach considers each possible activity that might follow after the\nlast (and already finished) activity, and selects the top information cards\nsuch that they maximize the likelihood of satisfying the user's information\nneeds for all possible future scenarios. The proposed models also incorporate\nknowledge about the temporal dynamics of information needs. Using a combination\nof historical check-in data and manual assessments collected via crowdsourcing,\nwe show experimentally the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 02:33:32 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Benetka", "Jan R.", ""], ["Balog", "Krisztian", ""], ["N\u00f8rv\u00e5g", "Kjetil", ""]]}, {"id": "1709.06758", "submitter": "Didi Surian", "authors": "Didi Surian, Adam G. Dunn, Liat Orenstein, Rabia Bashir, Enrico\n  Coiera, Florence T. Bourgeois", "title": "A shared latent space matrix factorisation method for recommending new\n  trial evidence for systematic review updates", "comments": "Journal of Biomedical Informatics Vol. 79, March 2018, p. 32-40", "journal-ref": "J Biomed Inform. Vol. 79, March 2018, p. 32-40", "doi": "10.1016/j.jbi.2018.01.008", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical trial registries can be used to monitor the production of trial\nevidence and signal when systematic reviews become out of date. However, this\nuse has been limited to date due to the extensive manual review required to\nsearch for and screen relevant trial registrations. Our aim was to evaluate a\nnew method that could partially automate the identification of trial\nregistrations that may be relevant for systematic review updates. We identified\n179 systematic reviews of drug interventions for type 2 diabetes, which\nincluded 537 clinical trials that had registrations in ClinicalTrials.gov. We\ntested a matrix factorisation approach that uses a shared latent space to learn\nhow to rank relevant trial registrations for each systematic review, comparing\nthe performance to document similarity to rank relevant trial registrations.\nThe two approaches were tested on a holdout set of the newest trials from the\nset of type 2 diabetes systematic reviews and an unseen set of 141 clinical\ntrial registrations from 17 updated systematic reviews published in the\nCochrane Database of Systematic Reviews. The matrix factorisation approach\noutperformed the document similarity approach with a median rank of 59 and\nrecall@100 of 60.9%, compared to a median rank of 138 and recall@100 of 42.8%\nin the document similarity baseline. In the second set of systematic reviews\nand their updates, the highest performing approach used document similarity and\ngave a median rank of 67 (recall@100 of 62.9%). The proposed method was useful\nfor ranking trial registrations to reduce the manual workload associated with\nfinding relevant trials for systematic review updates. The results suggest that\nthe approach could be used as part of a semi-automated pipeline for monitoring\npotentially new evidence for inclusion in a review update.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 08:10:28 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 03:50:33 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 00:31:19 GMT"}, {"version": "v4", "created": "Tue, 27 Feb 2018 23:43:00 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Surian", "Didi", ""], ["Dunn", "Adam G.", ""], ["Orenstein", "Liat", ""], ["Bashir", "Rabia", ""], ["Coiera", "Enrico", ""], ["Bourgeois", "Florence T.", ""]]}, {"id": "1709.06907", "submitter": "Simon Razniewski", "authors": "Simon Razniewski, Vevake Balaraman, Werner Nutt", "title": "Doctoral Advisor or Medical Condition: Towards Entity-specific Rankings\n  of Knowledge Base Properties [Extended Version]", "comments": "Extended version of an ADMA 2017 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In knowledge bases such as Wikidata, it is possible to assert a large set of\nproperties for entities, ranging from generic ones such as name and place of\nbirth to highly profession-specific or background-specific ones such as\ndoctoral advisor or medical condition. Determining a preference or ranking in\nthis large set is a challenge in tasks such as prioritisation of edits or\nnatural-language generation. Most previous approaches to ranking knowledge base\nproperties are purely data-driven, that is, as we show, mistake frequency for\ninterestingness.\n  In this work, we have developed a human-annotated dataset of 350 preference\njudgments among pairs of knowledge base properties for fixed entities. From\nthis set, we isolate a subset of pairs for which humans show a high level of\nagreement (87.5% on average). We show, however, that baseline and\nstate-of-the-art techniques achieve only 61.3% precision in predicting human\npreferences for this subset.\n  We then analyze what contributes to one property being rated as more\nimportant than another one, and identify that at least three factors play a\nrole, namely (i) general frequency, (ii) applicability to similar entities and\n(iii) semantic similarity between property and entity. We experimentally\nanalyze the contribution of each factor and show that a combination of\ntechniques addressing all the three factors achieves 74% precision on the task.\n  The dataset is available at\nwww.kaggle.com/srazniewski/wikidatapropertyranking.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 14:43:08 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Razniewski", "Simon", ""], ["Balaraman", "Vevake", ""], ["Nutt", "Werner", ""]]}, {"id": "1709.06918", "submitter": "Chao Zhao", "authors": "Chao Zhao, Min Zhao, Yi Guan", "title": "Constructing a Hierarchical User Interest Structure based on User\n  Profiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interests of individual internet users fall into a hierarchical structure\nwhich is useful in regards to building personalized searches and\nrecommendations. Most studies on this subject construct the interest hierarchy\nof a single person from the document perspective. In this study, we constructed\nthe user interest hierarchy via user profiles. We organized 433,397 user\ninterests, referred to here as \"attentions\", into a user attention network\n(UAN) from 200 million user profiles; we then applied the Louvain algorithm to\ndetect hierarchical clusters in these attentions. Finally, a 26-level hierarchy\nwith 34,676 clusters was obtained. We found that these attention clusters were\naggregated according to certain topics as opposed to the hyponymy-relation\nbased conceptual ontologies. The topics can be entities or concepts, and the\nrelations were not restrained by hyponymy. The concept relativity encapsulated\nin the user's interest can be captured by labeling the attention clusters with\ncorresponding concepts.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 15:03:51 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Zhao", "Chao", ""], ["Zhao", "Min", ""], ["Guan", "Yi", ""]]}, {"id": "1709.07434", "submitter": "Preeti Bhargava", "authors": "Guoning Hu, Preeti Bhargava, Saul Fuhrmann, Sarah Ellinger, Nemanja\n  Spasojevic", "title": "Analyzing users' sentiment towards popular consumer industries and\n  brands on Twitter", "comments": "8 pages, 11 figures, 1 table, 2017 IEEE International Conference on\n  Data Mining Workshops (ICDMW 2017), ICDM Sentiment Elicitation from Natural\n  Text for Information Retrieval and Extraction (ICDM SENTIRE) 2017 workshop", "journal-ref": "2017 IEEE International Conference on Data Mining Workshops (ICDMW\n  2017)", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media serves as a unified platform for users to express their thoughts\non subjects ranging from their daily lives to their opinion on consumer brands\nand products. These users wield an enormous influence in shaping the opinions\nof other consumers and influence brand perception, brand loyalty and brand\nadvocacy. In this paper, we analyze the opinion of 19M Twitter users towards 62\npopular industries, encompassing 12,898 enterprise and consumer brands, as well\nas associated subject matter topics, via sentiment analysis of 330M tweets over\na period spanning a month. We find that users tend to be most positive towards\nmanufacturing and most negative towards service industries. In addition, they\ntend to be more positive or negative when interacting with brands than\ngenerally on Twitter. We also find that sentiment towards brands within an\nindustry varies greatly and we demonstrate this using two industries as use\ncases. In addition, we discover that there is no strong correlation between\ntopic sentiments of different industries, demonstrating that topic sentiments\nare highly dependent on the context of the industry that they are mentioned in.\nWe demonstrate the value of such an analysis in order to assess the impact of\nbrands on social media. We hope that this initial study will prove valuable for\nboth researchers and companies in understanding users' perception of\nindustries, brands and associated topics and encourage more research in this\nfield.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 17:50:13 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Hu", "Guoning", ""], ["Bhargava", "Preeti", ""], ["Fuhrmann", "Saul", ""], ["Ellinger", "Sarah", ""], ["Spasojevic", "Nemanja", ""]]}, {"id": "1709.07528", "submitter": "Kenneth Hess", "authors": "Kenneth L. Hess, Hugo D. Paz", "title": "Defining a Lingua Franca to Open the Black Box of a Na\\\"ive Bayes\n  Recommender", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many AI systems have a black box nature that makes it difficult to understand\nhow they make their recommendations. This can be unsettling, as the designer\ncannot be certain how the system will respond to novelty. To penetrate our\nNa\\\"ive Bayes recommender's black box, we first asked, what do we want to know\nfrom our system, and how can it be obtained? The answers led us to recursively\ndefine a common lexicon with the AI, a lingua franca, using the very items that\nthe system ranks to create meta-symbols recognized by the system, and enabling\nus to understand the system's knowledge in plain terms and at different levels\nof abstraction. As one bonus, using its existing knowledge, the lingua franca\ncan enable the system to extend recommendations to related, but entirely new\nareas, ameliorating the cold start problem. We also supplement the lingua\nfranca with techniques for visualizing the system's knowledge state, develop\nmetrics for evaluating the meaningfulness of terms in the lingua franca, and\ngeneralize the requirements for developing a similar lingua franca in other\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 21 Sep 2017 22:06:26 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Hess", "Kenneth L.", ""], ["Paz", "Hugo D.", ""]]}, {"id": "1709.07545", "submitter": "Tian Wang", "authors": "Tian Wang, Kyunghyun Cho", "title": "Attention-based Mixture Density Recurrent Networks for History-based\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of personalized history-based recommendation is to automatically\noutput a distribution over all the items given a sequence of previous purchases\nof a user. In this work, we present a novel approach that uses a recurrent\nnetwork for summarizing the history of purchases, continuous vectors\nrepresenting items for scalability, and a novel attention-based recurrent\nmixture density network, which outputs each component in a mixture\nsequentially, for modelling a multi-modal conditional distribution. We evaluate\nthe proposed approach on two publicly available datasets, MovieLens-20M and\nRecSys15. The experiments show that the proposed approach, which explicitly\nmodels the multi-modal nature of the predictive distribution, is able to\nimprove the performance over various baselines in terms of precision, recall\nand nDCG.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 00:16:35 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Wang", "Tian", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1709.07654", "submitter": "Elias K\\\"arle", "authors": "Elias K\\\"arle and Dieter Fensel", "title": "Annotation based automatic action processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a strong motivational background in search engine optimization the\namount of structured data on the web is growing rapidly. The main search engine\nproviders are promising great increase in visibility through annotation of the\nweb page's content with the vocabulary of schema.org and thus providing it as\nstructured data. But besides the usage by search engines the data can be used\nin various other ways, for example for automatic processing of annotated web\nservices or actions. In this work we present an approach to consume and process\nschema.org annotated data on the web and give an idea how a best practice can\nlook like.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 09:39:08 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 15:45:17 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["K\u00e4rle", "Elias", ""], ["Fensel", "Dieter", ""]]}, {"id": "1709.07782", "submitter": "Anne Chardonnens", "authors": "Anne Chardonnens, Ettore Rizza, Mathias Coeckelbergs, Seth van Hooland", "title": "Mining User Queries with Information Extraction Methods and Linked Data", "comments": "Preprint (17 pages, 3 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Purpose: Advanced usage of Web Analytics tools allows to capture the content\nof user queries. Despite their relevant nature, the manual analysis of large\nvolumes of user queries is problematic. This paper demonstrates the potential\nof using information extraction techniques and Linked Data to gather a better\nunderstanding of the nature of user queries in an automated manner.\n  Design/methodology/approach: The paper presents a large-scale case-study\nconducted at the Royal Library of Belgium consisting of a data set of 83 854\nqueries resulting from 29 812 visits over a 12 month period of the historical\nnewspapers platform BelgicaPress. By making use of information extraction\nmethods, knowledge bases and various authority files, this paper presents the\npossibilities and limits to identify what percentage of end users are looking\nfor person and place names.\n  Findings: Based on a quantitative assessment, our method can successfully\nidentify the majority of person and place names from user queries. Due to the\nspecific character of user queries and the nature of the knowledge bases used,\na limited amount of queries remained too ambiguous to be treated in an\nautomated manner.\n  Originality/value: This paper demonstrates in an empirical manner both the\npossibilities and limits of gaining more insights from user queries extracted\nfrom a Web Analytics tool and analysed with the help of information extraction\ntools and knowledge bases. Methods and tools used are generalisable and can be\nreused by other collection holders.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 14:35:44 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Chardonnens", "Anne", ""], ["Rizza", "Ettore", ""], ["Coeckelbergs", "Mathias", ""], ["van Hooland", "Seth", ""]]}, {"id": "1709.08083", "submitter": "Lingyang Chu", "authors": "Lingyang Chu, Zhefeng Wang, Jian Pei, Yanyan Zhang, Yu Yang, Enhong\n  Chen", "title": "Finding Theme Communities from Database Networks", "comments": null, "journal-ref": "Proceedings of the VLDB Endowment, Vol. 12, No. 10. 2019", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a database network where each vertex is associated with a transaction\ndatabase, we are interested in finding theme communities. Here, a theme\ncommunity is a cohesive subgraph such that a common pattern is frequent in all\ntransaction databases associated with the vertices in the subgraph. Finding all\ntheme communities from a database network enjoys many novel applications.\nHowever, it is challenging since even counting the number of all theme\ncommunities in a database network is #P-hard. Inspired by the observation that\na theme community shrinks when the length of the pattern increases, we\ninvestigate several properties of theme communities and develop TCFI, a\nscalable algorithm that uses these properties to effectively prune the patterns\nthat cannot form any theme community. We also design TC-Tree, a scalable\nalgorithm that decomposes and indexes theme communities efficiently. Retrieving\na ranked list of theme communities from a TC-Tree of hundreds of millions of\ntheme communities takes less than 1 second. Extensive experiments and a case\nstudy demonstrate the effectiveness and scalability of TCFI and TC-Tree in\ndiscovering and querying meaningful theme communities from large database\nnetworks.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 17:35:25 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 17:14:11 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Chu", "Lingyang", ""], ["Wang", "Zhefeng", ""], ["Pei", "Jian", ""], ["Zhang", "Yanyan", ""], ["Yang", "Yu", ""], ["Chen", "Enhong", ""]]}, {"id": "1709.08226", "submitter": "Kazem Qazanfari", "authors": "Kazem Qazanfari, Abdou Youssef, Kai Keane and Joseph Nelson", "title": "A novel recommendation system to match college events and groups to\n  students", "comments": "10 pages, AIAAT 2017, Hawaii, USA", "journal-ref": null, "doi": "10.1088/1757-899X/261/1/012017", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent increase in data online, discovering meaningful opportunities\ncan be time-consuming and complicated for many individuals. To overcome this\ndata overload challenge, we present a novel text-content-based recommender\nsystem as a valuable tool to predict user interests. To that end, we develop a\nspecific procedure to create user models and item feature-vectors, where items\nare described in free text. The user model is generated by soliciting from a\nuser a few keywords and expanding those keywords into a list of weighted\nnear-synonyms. The item feature-vectors are generated from the textual\ndescriptions of the items, using modified tf-idf values of the users' keywords\nand their near-synonyms. Once the users are modeled and the items are\nabstracted into feature vectors, the system returns the maximum-similarity\nitems as recommendations to that user. Our experimental evaluation shows that\nour method of creating the user models and item feature-vectors resulted in\nhigher precision and accuracy in comparison to well-known\nfeature-vector-generating methods like Glove and Word2Vec. It also shows that\nstemming and the use of a modified version of tf-idf increase the accuracy and\nprecision by 2% and 3%, respectively, compared to non-stemming and the standard\ntf-idf definition. Moreover, the evaluation results show that updating the user\nmodel from usage histories improves the precision and accuracy of the system.\nThis recommender system has been developed as part of the Agnes application,\nwhich runs on iOS and Android platforms and is accessible through the Agnes\nwebsite.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 17:40:50 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 15:02:01 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Qazanfari", "Kazem", ""], ["Youssef", "Abdou", ""], ["Keane", "Kai", ""], ["Nelson", "Joseph", ""]]}, {"id": "1709.08267", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Donald E. Brown, Mojtaba Heidarysafa, Kiana Jafari\n  Meimandi, Matthew S. Gerber, Laura E. Barnes", "title": "HDLTex: Hierarchical Deep Learning for Text Classification", "comments": "ICMLA 2017", "journal-ref": null, "doi": "10.1109/ICMLA.2017.0-134", "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continually increasing number of documents produced each year\nnecessitates ever improving information processing methods for searching,\nretrieving, and organizing text. Central to these information processing\nmethods is document classification, which has become an important application\nfor supervised learning. Recently the performance of these traditional\nclassifiers has degraded as the number of documents has increased. This is\nbecause along with this growth in the number of documents has come an increase\nin the number of categories. This paper approaches this problem differently\nfrom current document classification methods that view the problem as\nmulti-class classification. Instead we perform hierarchical classification\nusing an approach we call Hierarchical Deep Learning for Text classification\n(HDLTex). HDLTex employs stacks of deep learning architectures to provide\nspecialized understanding at each level of the document hierarchy.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 21:58:12 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 18:16:31 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Kowsari", "Kamran", ""], ["Brown", "Donald E.", ""], ["Heidarysafa", "Mojtaba", ""], ["Meimandi", "Kiana Jafari", ""], ["Gerber", "Matthew S.", ""], ["Barnes", "Laura E.", ""]]}, {"id": "1709.08880", "submitter": "Noreddine Gherabi", "authors": "Noreddine Gherabi, Abdelhadi Daoui, and Abderrahim Marzouk", "title": "An enhanced method to compute the similarity between concepts of\n  ontology", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-64719-7", "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the use of ontologies in several domains such as semantic web,\ninformation retrieval, artificial intelligence, the concept of similarity\nmeasuring has become a very important domain of research. Therefore, in the\ncurrent paper, we propose our method of similarity measuring which uses the\nDijkstra algorithm to define and compute the shortest path. Then, we use this\none to compute the semantic distance between two concepts defined in the same\nhierarchy of ontology. Afterward, we base on this result to compute the\nsemantic similarity. Finally, we present an experimental comparison between our\nmethod and other methods of similarity measuring.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 08:18:15 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Gherabi", "Noreddine", ""], ["Daoui", "Abdelhadi", ""], ["Marzouk", "Abderrahim", ""]]}, {"id": "1709.08990", "submitter": "Daniel Lemire", "authors": "Daniel Lemire, Nathan Kurz, Christoph Rupp", "title": "Stream VByte: Faster Byte-Oriented Integer Compression", "comments": null, "journal-ref": "Information Processing Letters 130, February 2018, Pages 1-6", "doi": "10.1016/j.ipl.2017.09.011", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Arrays of integers are often compressed in search engines. Though there are\nmany ways to compress integers, we are interested in the popular byte-oriented\ninteger compression techniques (e.g., VByte or Google's Varint-GB). They are\nappealing due to their simplicity and engineering convenience. Amazon's\nvarint-G8IU is one of the fastest byte-oriented compression technique published\nso far. It makes judicious use of the powerful single-instruction-multiple-data\n(SIMD) instructions available in commodity processors. To surpass varint-G8IU,\nwe present Stream VByte, a novel byte-oriented compression technique that\nseparates the control stream from the encoded data. Like varint-G8IU, Stream\nVByte is well suited for SIMD instructions. We show that Stream VByte decoding\ncan be up to twice as fast as varint-G8IU decoding over real data sets. In this\nsense, Stream VByte establishes new speed records for byte-oriented integer\ncompression, at times exceeding the speed of the memcpy function. On a 3.4GHz\nHaswell processor, it decodes more than 4 billion differentially-coded integers\nper second from RAM to L1 cache.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 14:45:20 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 19:53:20 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Lemire", "Daniel", ""], ["Kurz", "Nathan", ""], ["Rupp", "Christoph", ""]]}, {"id": "1709.09214", "submitter": "Monika Rani", "authors": "Monika Rani, Maybin K. Muyeba, and O. P. Vyas", "title": "A Hybrid Approach using Ontology Similarity and Fuzzy Logic for Semantic\n  Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in information retrieval is providing accurate answers\nto a user's question often expressed as uncertainty words. Most answers are\nbased on a Syntactic approach rather than a Semantic analysis of the query. In\nthis paper, our objective is to present a hybrid approach for a Semantic\nquestion answering retrieval system using Ontology Similarity and Fuzzy logic.\nWe use a Fuzzy Co-clustering algorithm to retrieve the collection of documents\nbased on Ontology Similarity. The Fuzzy Scale uses Fuzzy type-1 for documents\nand Fuzzy type-2 for words to prioritize answers. The objective of this work is\nto provide retrieval system with more accurate answers than non-fuzzy Semantic\nOntology approach.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 15:46:53 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 08:14:48 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Rani", "Monika", ""], ["Muyeba", "Maybin K.", ""], ["Vyas", "O. P.", ""]]}, {"id": "1709.09404", "submitter": "Patrice Bellot", "authors": "Wided Bakari (1), Patrice Bellot (1), Mahmoud Neji ((1) LSIS)", "title": "A Preliminary Study for Building an Arabic Corpus of Pair\n  Questions-Texts from the Web: AQA-Webcorp", "comments": null, "journal-ref": "International Journal of Recent Contributions from Engineering,\n  Science \\& IT (iJES), kassel university press GmbH, 2016, 4 (2), pp.38-45", "doi": "10.3991/ijes.v4i2.5345", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of electronic media and the heterogeneity of Arabic data\non the Web, the idea of building a clean corpus for certain applications of\nnatural language processing, including machine translation, information\nretrieval, question answer, become more and more pressing. In this manuscript,\nwe seek to create and develop our own corpus of pair's questions-texts. This\nconstitution then will provide a better base for our experimentation step.\nThus, we try to model this constitution by a method for Arabic insofar as it\nrecovers texts from the web that could prove to be answers to our factual\nquestions. To do this, we had to develop a java script that can extract from a\ngiven query a list of html pages. Then clean these pages to the extent of\nhaving a data base of texts and a corpus of pair's question-texts. In addition,\nwe give preliminary results of our proposal method. Some investigations for the\nconstruction of Arabic corpus are also presented in this document.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 09:20:06 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Bakari", "Wided", "", "LSIS"], ["Bellot", "Patrice", "", "LSIS"], ["Neji", "Mahmoud", ""]]}, {"id": "1709.09450", "submitter": "Mohammad Halawani", "authors": "Mohammad K. Halawani, Rob Forsyth and Phillip Lord", "title": "A Literature Based Approach to Define the Scope of Biomedical\n  Ontologies: A Case Study on a Rehabilitation Therapy Ontology", "comments": "Accepted at the International Conference for Biomedical Ontologies\n  2017(ICBO 2017), 4 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we investigate our early attempts at building an ontology\ndescribing rehabilitation therapies following brain injury. These therapies are\nwide-ranging, involving interventions of many different kinds. As a result,\nthese therapies are hard to describe. As well as restricting actual practice,\nthis is also a major impediment to evidence-based medicine as it is hard to\nmeaningfully compare two treatment plans.\n  Ontology development requires significant effort from both ontologists and\ndomain experts. Knowledge elicited from domain experts forms the scope of the\nontology. The process of knowledge elicitation is expensive, consumes experts'\ntime and might have biases depending on the selection of the experts. Various\nmethodologies and techniques exist for enabling this knowledge elicitation,\nincluding community groups and open development practices. A related problem is\nthat of defining scope. By defining the scope, we can decide whether a concept\n(i.e. term) should be represented in the ontology. This is the opposite of\nknowledge elicitation, in the sense that it defines what should not be in the\nontology. This can be addressed by pre-defining a set of competency questions.\n  These approaches are, however, expensive and time-consuming. Here, we\ndescribe our work toward an alternative approach, bootstrapping the ontology\nfrom an initially small corpus of literature that will define the scope of the\nontology, expanding this to a set covering the domain, then using information\nextraction to define an initial terminology to provide the basis and the\ncompetencies for the ontology. Here, we discuss four approaches to building a\nsuitable corpus that is both sufficiently covering and precise.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 11:11:54 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Halawani", "Mohammad K.", ""], ["Forsyth", "Rob", ""], ["Lord", "Phillip", ""]]}, {"id": "1709.09657", "submitter": "Kunho Kim", "authors": "Kunho Kim, Athar Sefid, C. Lee Giles", "title": "Scaling Author Name Disambiguation with CNF Blocking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An author name disambiguation (AND) algorithm identifies a unique author\nentity record from all similar or same publication records in scholarly or\nsimilar databases. Typically, a clustering method is used that requires\ncalculation of similarities between each possible record pair. However, the\ntotal number of pairs grows quadratically with the size of the author database\nmaking such clustering difficult for millions of records. One remedy for this\nis a blocking function that reduces the number of pairwise similarity\ncalculations. Here, we introduce a new way of learning blocking schemes by\nusing a conjunctive normal form (CNF) in contrast to the disjunctive normal\nform (DNF). We demonstrate on PubMed author records that CNF blocking reduces\nmore pairs while preserving high pairs completeness compared to the previous\nmethods that use a DNF with the computation time significantly reduced. Thus,\nthese concepts in scholarly data can be better represented with CNFs. Moreover,\nwe also show how to ensure that the method produces disjoint blocks so that the\nrest of the AND algorithm can be easily paralleled. Our CNF blocking tested on\nthe entire PubMed database of 80 million author mentions efficiently removes\n82.17% of all author record pairs in 10 minutes.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 17:48:21 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Kim", "Kunho", ""], ["Sefid", "Athar", ""], ["Giles", "C. Lee", ""]]}, {"id": "1709.09836", "submitter": "Bastien Latard", "authors": "Bastien Latard, Jonathan Weber, Germain Forestier, and Michel\n  Hassenforder", "title": "Towards a Semantic Search Engine for Scientific Articles", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-67008-9_54", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Because of the data deluge in scientific publication, finding relevant\ninformation is getting harder and harder for researchers and readers. Building\nan enhanced scientific search engine by taking semantic relations into account\nposes a great challenge. As a starting point, semantic relations between\nkeywords from scientific articles could be extracted in order to classify\narticles. This might help later in the process of browsing and searching for\ncontent in a meaningful scientific way. Indeed, by connecting keywords, the\ncontext of the article can be extracted. This paper aims to provide ideas to\nbuild such a smart search engine and describes the initial contributions\ntowards achieving such an ambitious goal.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 07:56:26 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Latard", "Bastien", ""], ["Weber", "Jonathan", ""], ["Forestier", "Germain", ""], ["Hassenforder", "Michel", ""]]}, {"id": "1709.09973", "submitter": "Diego Monti", "authors": "Iacopo Vagliano, Diego Monti, Ansgar Scherp, Maurizio Morisio", "title": "Content Recommendation through Semantic Annotation of User Reviews and\n  Linked Data - An Extended Technical Report", "comments": "Ninth International Conference on Knowledge Capture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, most recommender systems exploit user-provided ratings to infer\ntheir preferences. However, the growing popularity of social and e-commerce\nwebsites has encouraged users to also share comments and opinions through\ntextual reviews. In this paper, we introduce a new recommendation approach\nwhich exploits the semantic annotation of user reviews to extract useful and\nnon-trivial information about the items to recommend. It also relies on the\nknowledge freely available in the Web of Data, notably in DBpedia and Wikidata,\nto discover other resources connected with the annotated entities. We evaluated\nour approach in three domains, using both DBpedia and Wikidata. The results\nshowed that our solution provides a better ranking than another recommendation\nmethod based on the Web of Data, while it improves in novelty with respect to\ntraditional techniques based on ratings. Additionally, our method achieved a\nbetter performance with Wikidata than DBpedia.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 14:10:32 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 14:29:35 GMT"}, {"version": "v3", "created": "Sat, 28 Oct 2017 11:30:40 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Vagliano", "Iacopo", ""], ["Monti", "Diego", ""], ["Scherp", "Ansgar", ""], ["Morisio", "Maurizio", ""]]}, {"id": "1709.10060", "submitter": "Chanyoung Park", "authors": "Chanyoung Park, Songkuk Kim", "title": "Measuring the Eccentricity of Items", "comments": "Accepted at IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The long-tail phenomenon tells us that there are many items in the tail.\nHowever, not all tail items are the same. Each item acquires different kinds of\nusers. Some items are loved by the general public, while some items are\nconsumed by eccentric fans. In this paper, we propose a novel metric, item\neccentricity, to incorporate this difference between consumers of the items.\nEccentric items are defined as items that are consumed by eccentric users. We\nused this metric to analyze two real-world datasets of music and movies and\nobserved the characteristics of items in terms of eccentricity. The results\nshowed that our defined eccentricity of an item does not change much over time,\nand classified eccentric and noneccentric items present significantly distinct\ncharacteristics. The proposed metric effectively separates the eccentric and\nnoneccentric items mixed in the tail, which could not be done with the previous\nmeasures, which only consider the popularity of items.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 17:02:25 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Park", "Chanyoung", ""], ["Kim", "Songkuk", ""]]}, {"id": "1709.10388", "submitter": "Zhihui Xie", "authors": "Zhihui Xie, Kuang-Chih Lee, Liang Wang", "title": "Optimal Reserve Price for Online Ads Trading Based on Inventory\n  Identification", "comments": "ADKDD'17", "journal-ref": null, "doi": "10.1145/3124749.3124760", "report-no": null, "categories": "cs.GT cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The online ads trading platform plays a crucial role in connecting publishers\nand advertisers and generates tremendous value in facilitating the convenience\nof our lives. It has been evolving into a more and more complicated structure.\nIn this paper, we consider the problem of maximizing the revenue for the seller\nside via utilizing proper reserve price for the auctions in a dynamical way.\n  Predicting the optimal reserve price for each auction in the repeated auction\nmarketplaces is a non-trivial problem. However, we were able to come up with an\nefficient method of improving the seller revenue by mainly focusing on\nadjusting the reserve price for those high-value inventories. Previously, no\ndedicated work has been performed from this perspective. Inspired by Paul and\nMichael, our model first identifies the value of the inventory by predicting\nthe top bid price bucket using a cascade of classifiers. The cascade is\nessential in significantly reducing the false positive rate of a single\nclassifier. Based on the output of the first step, we build another cluster of\nclassifiers to predict the price separations between the top two bids. We\nshowed that although the high-value auctions are only a small portion of all\nthe traffic, successfully identifying them and setting correct reserve price\nwould result in a significant revenue lift. Moreover, our optimization is\ncompatible with all other reserve price models in the system and does not\nimpact their performance. In other words, when combined with other models, the\nenhancement on exchange revenue will be aggregated. Simulations on randomly\nsampled Yahoo ads exchange (YAXR) data showed stable and expected lift after\napplying our model.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 02:22:24 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Xie", "Zhihui", ""], ["Lee", "Kuang-Chih", ""], ["Wang", "Liang", ""]]}]