[{"id": "1304.0104", "submitter": "Diederik Aerts", "authors": "Diederik Aerts, Jan Broekaert, Sandro Sozzo and Tomas Veloz", "title": "Meaning-focused and Quantum-inspired Information Retrieval", "comments": "11 pages", "journal-ref": "Quantum Interaction. Lecture Notes in Computer Science, 8369, pp.\n  71-83, 2014", "doi": "10.1007/978-3-642-54943-4_7", "report-no": null, "categories": "cs.IR cs.CL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, quantum-based methods have promisingly integrated the\ntraditional procedures in information retrieval (IR) and natural language\nprocessing (NLP). Inspired by our research on the identification and\napplication of quantum structures in cognition, more specifically our work on\nthe representation of concepts and their combinations, we put forward a\n'quantum meaning based' framework for structured query retrieval in text\ncorpora and standardized testing corpora. This scheme for IR rests on\nconsidering as basic notions, (i) 'entities of meaning', e.g., concepts and\ntheir combinations and (ii) traces of such entities of meaning, which is how\ndocuments are considered in this approach. The meaning content of these\n'entities of meaning' is reconstructed by solving an 'inverse problem' in the\nquantum formalism, consisting of reconstructing the full states of the entities\nof meaning from their collapsed states identified as traces in relevant\ndocuments. The advantages with respect to traditional approaches, such as\nLatent Semantic Analysis (LSA), are discussed by means of concrete examples.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2013 13:08:58 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Aerts", "Diederik", ""], ["Broekaert", "Jan", ""], ["Sozzo", "Sandro", ""], ["Veloz", "Tomas", ""]]}, {"id": "1304.0419", "submitter": "Mahashweta Das", "authors": "Mahashweta Das, Gautam Das, Vagelis Hristidis", "title": "Top-K Product Design Based on Collaborative Tagging Data", "comments": "The conference version appeared under the title \"Leveraging\n  collaborative tagging for web item design\" in SIGKDD 2011, pages 538-546", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread use and popularity of collaborative content sites (e.g., IMDB,\nAmazon, Yelp, etc.) has created rich resources for users to consult in order to\nmake purchasing decisions on various products such as movies, e-commerce\nproducts, restaurants, etc. Products with desirable tags (e.g., modern,\nreliable, etc.) have higher chances of being selected by prospective customers.\nThis creates an opportunity for product designers to design better products\nthat are likely to attract desirable tags when published. In this paper, we\ninvestigate how to mine collaborative tagging data to decide the attribute\nvalues of new products and to return the top-k products that are likely to\nattract the maximum number of desirable tags when published. Given a training\nset of existing products with their features and user-submitted tags, we first\nbuild a Naive Bayes Classifier for each tag. We show that the problem of is\nNP-complete even if simple Naive Bayes Classifiers are used for tag prediction.\nWe present a suite of algorithms for solving this problem: (a) an exact two\ntier algorithm(based on top-k querying techniques), which performs much better\nthan the naive brute-force algorithm and works well for moderate problem\ninstances, and (b) a set of approximation algorithms for larger problem\ninstances: a novel polynomial-time approximation algorithm with provable error\nbound and a practical hill-climbing heuristic. We conduct detailed experiments\non synthetic and real data crawled from the web to evaluate the efficiency and\nquality of our proposed algorithms, as well as show how product designers can\nbenefit by leveraging collaborative tagging information.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2013 19:09:38 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Das", "Mahashweta", ""], ["Das", "Gautam", ""], ["Hristidis", "Vagelis", ""]]}, {"id": "1304.0954", "submitter": "Marko Horvat", "authors": "Marko Horvat, Anton Grbin, Gordan Gledec", "title": "Labeling and Retrieval of Emotionally-Annotated Images using WordNet", "comments": "16 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:1302.2223", "journal-ref": "International Journal of Knowledge-Based and Intelligent\n  Engineering Systems, Vol. 17, No. 2, pp. 157-166, 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repositories of images with semantic and emotion content descriptions are\nvaluable tools in many areas such as Affective Computing and Human-Computer\nInteraction, but they are also important in the development of multimodal\nsearchable online databases. Ever growing number of image documents available\non the Internet continuously motivates research of better annotation models and\nmore efficient retrieval methods which use mash-up of available data on\nsemantics, scenes, objects, events, context and emotion. Formal knowledge\nrepresentation of such high-level semantics requires rich, explicit, human but\nalso machine-processable information. To achieve these goals we present an\nonline ontology-based image annotation tool WNtags and demonstrate its\nusefulness in knowledge representation and image retrieval using the\nInternational Affective Picture System database. The WNtags uses WordNet as\nimage tagging glossary but considers Suggested Upper Merged Ontology as the\npreferred upper labeling formalism. The retrieval is performed using node\ndistance metrics to establish semantic relatedness between a query and the\ncollaboratively weighted tags describing high-level image semantics, after\nwhich the result is ranked according to the derived importance. We also\nelaborate plans to improve the WNtags to create a collaborative Web-based\nmultimedia repository for research in human emotion and attention.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 13:58:56 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2014 23:27:00 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Horvat", "Marko", ""], ["Grbin", "Anton", ""], ["Gledec", "Gordan", ""]]}, {"id": "1304.1332", "submitter": "Karl Voit", "authors": "Karl Voit", "title": "What really happened on September 15th 2008? Getting The Most from Your\n  Personal Information with Memacs", "comments": "6 pages, 3 figures, 21 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Combining and summarizing meta-data from various kinds of data sources is one\npossible solution to the data fragmentation we are suffering from. Multiple\nprojects have addressed this issue already. This paper presents a new approach\nnamed Memacs. It automatically generates a detailed linked diary of our digital\nartifacts scattered across local files of multiple formats as well as data\nsilos of the internet. Being elegantly simple and open, Memacs uses already\nexisting visualization features of GNU Emacs and Org-mode to provide a\npromising platform for life-logging, Quantified Self movement, and people\nlooking for advanced Personal Information Management (PIM) in general.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 11:43:09 GMT"}], "update_date": "2013-04-05", "authors_parsed": [["Voit", "Karl", ""]]}, {"id": "1304.1567", "submitter": "Roja Bandari", "authors": "Roja Bandari, Hazhir Rahmandad, Vwani P. Roychowdhury", "title": "Blind Men and the Elephant: Detecting Evolving Groups In Social News", "comments": "10 pages, icwsm2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automated and unsupervised methodology for a novel\nsummarization of group behavior based on content preference. We show that graph\ntheoretical community evolution (based on similarity of user preference for\ncontent) is effective in indexing these dynamics. Combined with text analysis\nthat targets automatically-identified representative content for each\ncommunity, our method produces a novel multi-layered representation of evolving\ngroup behavior. We demonstrate this methodology in the context of political\ndiscourse on a social news site with data that spans more than four years and\nfind coexisting political leanings over extended periods and a disruptive\nexternal event that lead to a significant reorganization of existing patterns.\nFinally, where there exists no ground truth, we propose a new evaluation\napproach by using entropy measures as evidence of coherence along the evolution\npath of these groups. This methodology is valuable to designers and managers of\nonline forums in need of granular analytics of user activity, as well as to\nresearchers in social and political sciences who wish to extend their inquiries\nto large-scale data available on the web.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 22:05:20 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2013 19:49:46 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Bandari", "Roja", ""], ["Rahmandad", "Hazhir", ""], ["Roychowdhury", "Vwani P.", ""]]}, {"id": "1304.1677", "submitter": "Sowmya Kamath S", "authors": "Sunil Joy Dommati, Ruchi Agrawal, Ram Mohana Reddy G. and S. Sowmya\n  Kamath", "title": "Bug Classification: Feature Extraction and Comparison of Event Model\n  using Na\\\"ive Bayes Approach", "comments": "5 pages, International Conference on Recent Trends in Computer and\n  Information Engineering (ICRTCIE'2012) April 13-15, 2012 Pattaya,\n  http://psrcentre.org/images/extraimages/412138.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In software industries, individuals at different levels from customer to an\nengineer apply diverse mechanisms to detect to which class a particular bug\nshould be allocated. Sometimes while a simple search in Internet might help, in\nmany other cases a lot of effort is spent in analyzing the bug report to\nclassify the bug. So there is a great need of a structured mining algorithm -\nwhere given a crash log, the existing bug database could be mined to find out\nthe class to which the bug should be allocated. This would involve Mining\npatterns and applying different classification algorithms. This paper focuses\non the feature extraction, noise reduction in data and classification of\nnetwork bugs using probabilistic Na\\\"ive Bayes approach. Different event models\nlike Bernoulli and Multinomial are applied on the extracted features. When new,\nunseen bugs are given as input to the algorithms, the performance comparison of\ndifferent algorithms is done on the basis of accuracy and recall parameters.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2013 11:05:18 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Dommati", "Sunil Joy", ""], ["Agrawal", "Ruchi", ""], ["G.", "Ram Mohana Reddy", ""], ["Kamath", "S. Sowmya", ""]]}, {"id": "1304.1924", "submitter": "Shuguang Han", "authors": "Shuguang Han, Zhen Yue, Daqing He", "title": "Automatic Detection of Search Tactic in Individual Information Seeking:\n  A Hidden Markov Model Approach", "comments": "5 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information seeking process is an important topic in information seeking\nbehavior research. Both qualitative and empirical methods have been adopted in\nanalyzing information seeking processes, with major focus on uncovering the\nlatent search tactics behind user behaviors. Most of the existing works require\ndefining search tactics in advance and coding data manually. Among the few\nworks that can recognize search tactics automatically, they missed making sense\nof those tactics. In this paper, we proposed using an automatic technique, i.e.\nthe Hidden Markov Model (HMM), to explicitly model the search tactics. HMM\nresults show that the identified search tactics of individual information\nseeking behaviors are consistent with Marchioninis Information seeking process\nmodel. With the advantages of showing the connections between search tactics\nand search actions and the transitions among search tactics, we argue that HMM\nis a useful tool to investigate information seeking process, or at least it\nprovides a feasible way to analyze large scale dataset.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 19:13:41 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Han", "Shuguang", ""], ["Yue", "Zhen", ""], ["He", "Daqing", ""]]}, {"id": "1304.1930", "submitter": "Santosh K.C.", "authors": "K.C. Santosh (LORIA), Abdel Bela\\\"id (LORIA)", "title": "Client-Driven Content Extraction Associated with Table", "comments": null, "journal-ref": "Machine Vision Applications (2013)", "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the project is to extract content within table in document images\nbased on learnt patterns. Real-world users i.e., clients first provide a set of\nkey fields within the table which they think are important. These are first\nused to represent the graph where nodes are labelled with semantics including\nother features and edges are attributed with relations. Attributed relational\ngraph (ARG) is then employed to mine similar graphs from a document image. Each\nmined graph will represent an item within the table, and hence a set of such\ngraphs will compose a table. We have validated the concept by using a\nreal-world industrial problem.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 19:24:40 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Santosh", "K. C.", "", "LORIA"], ["Bela\u00efd", "Abdel", "", "LORIA"]]}, {"id": "1304.2133", "submitter": "Conrad Sanderson", "authors": "Yongkang Wong, Conrad Sanderson, Sandra Mau, Brian C. Lovell", "title": "Dynamic Amelioration of Resolution Mismatches for Local Feature Based\n  Identity Inference", "comments": null, "journal-ref": "International Conference on Pattern Recognition (ICPR), pp.\n  1200-1203, 2010", "doi": "10.1109/ICPR.2010.299", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While existing face recognition systems based on local features are robust to\nissues such as misalignment, they can exhibit accuracy degradation when\ncomparing images of differing resolutions. This is common in surveillance\nenvironments where a gallery of high resolution mugshots is compared to low\nresolution CCTV probe images, or where the size of a given image is not a\nreliable indicator of the underlying resolution (eg. poor optics). To alleviate\nthis degradation, we propose a compensation framework which dynamically chooses\nthe most appropriate face recognition system for a given pair of image\nresolutions. This framework applies a novel resolution detection method which\ndoes not rely on the size of the input images, but instead exploits the\nsensitivity of local features to resolution using a probabilistic multi-region\nhistogram approach. Experiments on a resolution-modified version of the\n\"Labeled Faces in the Wild\" dataset show that the proposed resolution detector\nfrontend obtains a 99% average accuracy in selecting the most appropriate face\nrecognition system, resulting in higher overall face discrimination accuracy\n(across several resolutions) compared to the individual baseline face\nrecognition systems.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 08:36:55 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Wong", "Yongkang", ""], ["Sanderson", "Conrad", ""], ["Mau", "Sandra", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1304.2310", "submitter": "Nilanjan  Dey", "authors": "Nilanjan Dey, Prasenjit Maji, Poulami Das, Shouvik Biswas, Achintya\n  Das, Sheli Sinha Chaudhuri", "title": "Embedding of Blink Frequency in Electrooculography Signal using\n  Difference Expansion based Reversible Watermarking Technique", "comments": "6 Pages, 3 Figures, 4 Tables", "journal-ref": "Scientific Bulletin of the Politehnica University of Timisoara -\n  Transactions on Electronics and Communications p-ISSN 1583-3380, vol. 57(71),\n  no. 2, 2012", "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, like other fields, rapid expansion of digitization and\nglobalization has influenced the medical field as well. For progress of\ndiagnostic results most of the reputed hospitals and diagnostic centres all\nover the world have started exchanging medical information. In this proposed\nmethod, the calculated diagnostic parametric values of the original\nElectrooculography (EOG) signal are embedded as a watermark by using Difference\nExpansion (DE) algorithm based reversible watermarking technique. The extracted\nwatermark provides the required parametric values at the recipient end without\nany post computation of the recovered EOG signal. By computing the parametric\nvalues from the recovered signal, the integrity of the extracted watermark can\nbe validated. The time domain features of EOG signal are calculated for the\ngeneration of watermark. In the current work, various features are studied and\ntwo major features related to blink frequency are used to generate the\nwatermark. The high Signal to Noise Ratio (SNR) and the Bit Error Rate (BER)\nclaim the robustness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2013 13:58:40 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Dey", "Nilanjan", ""], ["Maji", "Prasenjit", ""], ["Das", "Poulami", ""], ["Biswas", "Shouvik", ""], ["Das", "Achintya", ""], ["Chaudhuri", "Sheli Sinha", ""]]}, {"id": "1304.2401", "submitter": "Elizabeth Murnane", "authors": "Elizabeth L. Murnane, Bernhard Haslhofer, Carl Lagoze", "title": "RESLVE: Leveraging User Interest to Improve Entity Disambiguation on\n  Short Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the Named Entity Disambiguation (NED) problem for short,\nuser-generated texts on the social Web. In such settings, the lack of\nlinguistic features and sparse lexical context result in a high degree of\nambiguity and sharp performance drops of nearly 50% in the accuracy of\nconventional NED systems. We handle these challenges by developing a model of\nuser-interest with respect to a personal knowledge context; and Wikipedia, a\nparticularly well-established and reliable knowledge base, is used to\ninstantiate the procedure. We conduct systematic evaluations using individuals'\nposts from Twitter, YouTube, and Flickr and demonstrate that our novel\ntechnique is able to achieve substantial performance gains beyond\nstate-of-the-art NED methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 20:00:20 GMT"}], "update_date": "2013-04-10", "authors_parsed": [["Murnane", "Elizabeth L.", ""], ["Haslhofer", "Bernhard", ""], ["Lagoze", "Carl", ""]]}, {"id": "1304.2476", "submitter": "M.M.A. Hashem", "authors": "Rushdi Shams, M.M.A. Hashem, Afrina Hossain, Suraiya Rumana Akter, and\n  Monika Gope", "title": "Corpus-based Web Document Summarization using Statistical and Linguistic\n  Approach", "comments": null, "journal-ref": "Procs. of the IEEE International Conference on Computer and\n  Communication Engineering (ICCCE10), pp. 115-120, Kuala Lumpur, Malaysia, May\n  11-13, (2010)", "doi": "10.1109/ICCCE.2010.5556854", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single document summarization generates summary by extracting the\nrepresentative sentences from the document. In this paper, we presented a novel\ntechnique for summarization of domain-specific text from a single web document\nthat uses statistical and linguistic analysis on the text in a reference corpus\nand the web document. The proposed summarizer uses the combinational function\nof Sentence Weight (SW) and Subject Weight (SuW) to determine the rank of a\nsentence, where SW is the function of number of terms (t_n) and number of words\n(w_n) in a sentence, and term frequency (t_f) in the corpus and SuW is the\nfunction of t_n and w_n in a subject, and t_f in the corpus. 30 percent of the\nranked sentences are considered to be the summary of the web document. We\ngenerated three web document summaries using our technique and compared each of\nthem with the summaries developed manually from 16 different human subjects.\nResults showed that 68 percent of the summaries produced by our approach\nsatisfy the manual summaries.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 07:30:20 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Shams", "Rushdi", ""], ["Hashem", "M. M. A.", ""], ["Hossain", "Afrina", ""], ["Akter", "Suraiya Rumana", ""], ["Gope", "Monika", ""]]}, {"id": "1304.2514", "submitter": "Kamala Sundararaman", "authors": "B. Kamala, J. M. Nandhini", "title": "Automatic Structuring Of Semantic Web Services An Approach", "comments": "ICRTCT, 2013 Coimbatore, International Journal of Computer and\n  Communication Technology, Jan 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontologies have become the effective modeling for various applications and\nsignificantly in the semantic web. The difficulty of extracting information\nfrom the web, which was created mainly for visualising information, has driven\nthe birth of the semantic web, which will contain much more resources than the\nweb and will attach machine-readable semantic information to these resources.\nOntological bootstrapping on a set of predefined sources, such as web services,\nmust address the problem of multiple, largely unrelated concepts. The web\nservices consist of basically two components, Web Services Description Language\n(WSDL) descriptors and free text descriptors. The WSDL descriptor is evaluated\nusing two methods, namely Term Frequency/Inverse Document Frequency (TF/IDF)\nand web context generation. The proposed bootstrapping ontological process\nintegrates TF/IDF and web context generation and applies validation using the\nfree text descriptor service, so that, it offers more accurate definition of\nontologies. This paper uses ranking adaption model which predicts the rank for\na collection of web service documents which leads to the automatic\nconstruction, enrichment and adaptation of ontologies.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 10:23:05 GMT"}], "update_date": "2013-04-10", "authors_parsed": [["Kamala", "B.", ""], ["Nandhini", "J. M.", ""]]}, {"id": "1304.2538", "submitter": "M.M.A. Hashem", "authors": "K.M. Motahar Hossain, Zahir Raihan and M.M.A. Hashem", "title": "On Appropriate Selection of Fuzzy Aggregation Operators in Medical\n  Decision Support System", "comments": null, "journal-ref": "Procs. of the 8th International Conference on Computer &\n  Information Technology (ICCIT 2005), pp. 563-568, Dhaka, Bangladesh, December\n  28-30, (2005)", "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Decision Support System (DSS) contains more than one antecedent and the\ndegrees of strength of the antecedents need to be combined to determine the\noverall strength of the rule consequent. The membership values of the\nlinguistic variables in Fuzzy have to be combined using an aggregation\noperator. But it is not feasible to predefine the form of aggregation operators\nin decision making. Instead, each rule should be found based on the feeling of\nthe experts and on their actual decision pattern over the set of typical\nexamples. Thus this work illustrates how the choice of aggregation operators is\nintended to mimic human decision making and can be selected and adjusted to fit\nempirical data, a series of test cases. Both parametrized and nonparametrized\naggregation operators are adapted to fit empirical data. Moreover, they\nprovided compensatory properties and, therefore, seemed to produce a better\ndecision support system. To solve the problem, a threshold point from the\noutput of the aggregation operators is chosen as the separation point between\ntwo classes. The best achieved accuracy is chosen as the appropriate\naggregation operator. Thus a medical decision can be generated which is very\nclose to a practitioner's guideline.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 11:25:13 GMT"}], "update_date": "2013-04-10", "authors_parsed": [["Hossain", "K. M. Motahar", ""], ["Raihan", "Zahir", ""], ["Hashem", "M. M. A.", ""]]}, {"id": "1304.2681", "submitter": "Daniel Fried", "authors": "Daniel Fried and Stephen G. Kobourov", "title": "Maps of Computer Science", "comments": "10 pages, 8 figures, live version and source code available at\n  mocs.cs.arizona.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a practical approach for visual exploration of research papers.\nSpecifically, we use the titles of papers from the DBLP database to create what\nwe call maps of computer science (MoCS). Words and phrases from the paper\ntitles are the cities in the map, and countries are created based on word and\nphrase similarity, calculated using co-occurrence. With the help of heatmaps,\nwe can visualize the profile of a particular conference or journal over the\nbase map. Similarly, heatmap profiles can be made of individual researchers or\ngroups such as a department. The visualization system also makes it possible to\nchange the data used to generate the base map. For example, a specific journal\nor conference can be used to generate the base map and then the heatmap\noverlays can be used to show the evolution of research topics in the field over\nthe years. As before, individual researchers or research groups profiles can be\nvisualized using heatmap overlays but this time over the journal or conference\nbase map. Finally, research papers or abstracts easily generate visual\nabstracts giving a visual representation of the distribution of topics in the\npaper. We outline a modular and extensible system for term extraction using\nnatural language processing techniques, and show the applicability of methods\nof information retrieval to calculation of term similarity and creation of a\ntopic map. The system is available at mocs.cs.arizona.edu.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 18:03:12 GMT"}], "update_date": "2013-04-10", "authors_parsed": [["Fried", "Daniel", ""], ["Kobourov", "Stephen G.", ""]]}, {"id": "1304.3268", "submitter": "Mustapha Aznag", "authors": "Mustapha Aznag, Mohamed Quafafou, Nicolas Durand and Zahi Jarir", "title": "Web Services Discovery and Recommendation Based on Information\n  Extraction and Symbolic Reputation", "comments": null, "journal-ref": "International Journal on Web Service Computing (IJWSC), Vol.4,\n  No.1, March 2013", "doi": "10.5121/ijwsc.2013.4101", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper shows that the problem of web services representation is crucial\nand analyzes the various factors that influence on it. It presents the\ntraditional representation of web services considering traditional textual\ndescriptions based on the information contained in WSDL files. Unfortunately,\ntextual web services descriptions are dirty and need significant cleaning to\nkeep only useful information. To deal with this problem, we introduce rules\nbased text tagging method, which allows filtering web service description to\nkeep only significant information. A new representation based on such filtered\ndata is then introduced. Many web services have empty descriptions. Also, we\nconsider web services representations based on the WSDL file structure (types,\nattributes, etc.). Alternatively, we introduce a new representation called\nsymbolic reputation, which is computed from relationships between web services.\nThe impact of the use of these representations on web service discovery and\nrecommendation is studied and discussed in the experimentation using real world\nweb services.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 12:21:36 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Aznag", "Mustapha", ""], ["Quafafou", "Mohamed", ""], ["Durand", "Nicolas", ""], ["Jarir", "Zahi", ""]]}, {"id": "1304.3405", "submitter": "Amit Sharma", "authors": "Amit Sharma, Dan Cosley", "title": "Do Social Explanations Work? Studying and Modeling the Effects of Social\n  Explanations in Recommender Systems", "comments": "11 pages, WWW 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems associated with social networks often use social\nexplanations (e.g. \"X, Y and 2 friends like this\") to support the\nrecommendations. We present a study of the effects of these social explanations\nin a music recommendation context. We start with an experiment with 237 users,\nin which we show explanations with varying levels of social information and\nanalyze their effect on users' decisions. We distinguish between two key\ndecisions: the likelihood of checking out the recommended artist, and the\nactual rating of the artist based on listening to several songs. We find that\nwhile the explanations do have some influence on the likelihood, there is\nlittle correlation between the likelihood and actual (listening) rating for the\nsame artist. Based on these insights, we present a generative probabilistic\nmodel that explains the interplay between explanations and background\ninformation on music preferences, and how that leads to a final likelihood\nrating for an artist. Acknowledging the impact of explanations, we discuss a\ngeneral recommendation framework that models external informational elements in\nthe recommendation interface, in addition to inherent preferences of users.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 19:31:39 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Sharma", "Amit", ""], ["Cosley", "Dan", ""]]}, {"id": "1304.3406", "submitter": "Seyed Hamed Alemohammad", "authors": "Seyed Hamed Alemohammad, Dara Entekhabi", "title": "Merging Satellite Measurements of Rainfall Using Multi-scale Imagery\n  Technique", "comments": "6 pages, 10 Figures, WCRP Open Science Conference, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several passive microwave satellites orbit the Earth and measure rainfall.\nThese measurements have the advantage of almost full global coverage when\ncompared to surface rain gauges. However, these satellites have low temporal\nrevisit and missing data over some regions. Image fusion is a useful technique\nto fill in the gaps of one image (one satellite measurement) using another one.\nThe proposed algorithm uses an iterative fusion scheme to integrate information\nfrom two satellite measurements. The algorithm is implemented on two datasets\nfor 7 years of half-hourly data. The results show significant improvements in\nrain detection and rain intensity in the merged measurements.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 19:31:57 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Alemohammad", "Seyed Hamed", ""], ["Entekhabi", "Dara", ""]]}, {"id": "1304.3563", "submitter": "Abdulaziz Alazemi", "authors": "Abdul-Aziz Rashid Al-Azmi", "title": "Data, text and web mining for business intelligence: a survey", "comments": "21 page, journal paper", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.3, No.2, March 2013", "doi": "10.5121/ijdkp.2013.3201", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Information and Communication Technologies revolution brought a digital\nworld with huge amounts of data available. Enterprises use mining technologies\nto search vast amounts of data for vital insight and knowledge. Mining tools\nsuch as data mining, text mining, and web mining are used to find hidden\nknowledge in large databases or the Internet.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 08:04:31 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Al-Azmi", "Abdul-Aziz Rashid", ""]]}, {"id": "1304.3742", "submitter": "Robert West", "authors": "Robert West, Ryen W. White, Eric Horvitz", "title": "From Cookies to Cooks: Insights on Dietary Patterns via Analysis of Web\n  Usage Logs", "comments": "WWW 2013, 11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nutrition is a key factor in people's overall health. Hence, understanding\nthe nature and dynamics of population-wide dietary preferences over time and\nspace can be valuable in public health. To date, studies have leveraged small\nsamples of participants via food intake logs or treatment data. We propose a\ncomplementary source of population data on nutrition obtained via Web logs. Our\nmain contribution is a spatiotemporal analysis of population-wide dietary\npreferences through the lens of logs gathered by a widely distributed\nWeb-browser add-on, using the access volume of recipes that users seek via\nsearch as a proxy for actual food consumption. We discover that variation in\ndietary preferences as expressed via recipe access has two main periodic\ncomponents, one yearly and the other weekly, and that there exist\ncharacteristic regional differences in terms of diet within the United States.\nIn a second study, we identify users who show evidence of having made an acute\ndecision to lose weight. We characterize the shifts in interests that they\nexpress in their search queries and focus on changes in their recipe queries in\nparticular. Last, we correlate nutritional time series obtained from recipe\nqueries with time-aligned data on hospital admissions, aimed at understanding\nhow behavioral data captured in Web logs might be harnessed to identify\npotential relationships between diet and acute health problems. In this\npreliminary study, we focus on patterns of sodium identified in recipes over\ntime and patterns of admission for congestive heart failure, a chronic illness\nthat can be exacerbated by increases in sodium intake.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 22:04:52 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["West", "Robert", ""], ["White", "Ryen W.", ""], ["Horvitz", "Eric", ""]]}, {"id": "1304.3845", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "The Impact of Situation Clustering in Contextual-Bandit Algorithm for\n  Context-Aware Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing approaches in Context-Aware Recommender Systems (CRS) focus on\nrecommending relevant items to users taking into account contextual\ninformation, such as time, location, or social aspects. However, few of them\nhave considered the problem of user's content dynamicity. We introduce in this\npaper an algorithm that tackles the user's content dynamicity by modeling the\nCRS as a contextual bandit algorithm and by including a situation clustering\nalgorithm to improve the precision of the CRS. Within a deliberately designed\noffline simulation framework, we conduct evaluations with real online event log\ndata. The experimental results and detailed analysis reveal several important\ndiscoveries in context aware recommender system.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 20:35:56 GMT"}, {"version": "v2", "created": "Sun, 30 Mar 2014 08:19:52 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1304.4119", "submitter": "Wilko van Hoek", "authors": "Wilko van Hoek and Philipp Mayr", "title": "Assessing Visualization Techniques for the Search Process in Digital\n  Libraries", "comments": "23 pages, 14 figures, pre-print to appear in \"Wissensorganisation mit\n  digitalen Technologien\" (deGruyter)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an overview of several visualization techniques to\nsupport the search process in Digital Libraries (DLs). The search process\ntypically can be separated into three major phases: query formulation and\nrefinement, browsing through result lists and viewing and interacting with\ndocuments and their properties. We discuss a selection of popular visualization\ntechniques that have been developed for the different phases to support the\nuser during the search process. Along prototypes based on the different\ntechniques we show how the approaches have been implemented. Although various\nvisualizations have been developed in prototypical systems very few of these\napproaches have been adapted into today's DLs. We conclude that this is most\nlikely due to the fact that most systems are not evaluated intensely in\nreal-life scenarios with real information seekers and that results of the\ninteresting visualization techniques are often not comparable. We can say that\nmany of the assessed systems did not properly address the information need of\ncur-rent users.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 14:57:27 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["van Hoek", "Wilko", ""], ["Mayr", "Philipp", ""]]}, {"id": "1304.5168", "submitter": "Jialu Liu", "authors": "Jialu Liu", "title": "Image Retrieval based on Bag-of-Words model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article gives a survey for bag-of-words (BoW) or bag-of-features model\nin image retrieval system. In recent years, large-scale image retrieval shows\nsignificant potential in both industry applications and research problems. As\nlocal descriptors like SIFT demonstrate great discriminative power in solving\nvision problems like object recognition, image classification and annotation,\nmore and more state-of-the-art large scale image retrieval systems are trying\nto rely on them. A common way to achieve this is first quantizing local\ndescriptors into visual words, and then applying scalable textual indexing and\nretrieval schemes. We call this model as bag-of-words or bag-of-features model.\nThe goal of this survey is to give an overview of this model and introduce\ndifferent strategies when building the system based on this model.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2013 15:57:34 GMT"}], "update_date": "2013-04-19", "authors_parsed": [["Liu", "Jialu", ""]]}, {"id": "1304.5213", "submitter": "Hany SalahEldeen", "authors": "Hany M. SalahEldeen and Michael L. Nelson", "title": "Carbon Dating The Web: Estimating the Age of Web Resources", "comments": "This work is published at TempWeb03 workshop at WWW 2013 conference\n  in Rio de Janeiro, Brazil", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the course of web research it is often necessary to estimate the creation\ndatetime for web resources (in the general case, this value can only be\nestimated). While it is feasible to manually establish likely datetime values\nfor small numbers of resources, this becomes infeasible if the collection is\nlarge. We present \"carbon date\", a simple web application that estimates the\ncreation date for a URI by polling a number of sources of evidence and\nreturning a machine-readable structure with their respective values. To\nestablish a likely datetime, we poll bitly for the first time someone shortened\nthe URI, topsy for the first time someone tweeted the URI, a Memento aggregator\nfor the first time it appeared in a public web archive, Google's time of last\ncrawl, and the Last-Modified HTTP response header of the resource itself. We\nalso examine the backlinks of the URI as reported by Google and apply the same\ntechniques for the resources that link to the URI. We evaluated our tool on a\ngold-standard data set of 1200 URIs in which the creation date was manually\nverified. We were able to estimate a creation date for 75.90% of the resources,\nwith 32.78% having the correct value. Given the different nature of the URIs,\nthe union of the various methods produces the best results. While the Google\nlast crawl date and topsy account for nearly 66% of the closest answers,\neliminating the web archives or Last-Modified from the results produces the\nlargest overall negative impact on the results. The carbon date application is\navailable for download or use via a webAPI.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2013 18:42:45 GMT"}], "update_date": "2013-04-19", "authors_parsed": [["SalahEldeen", "Hany M.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1304.5457", "submitter": "Joonseok Lee", "authors": "Joonseok Lee, Kisung Lee, Jennifer G. Kim", "title": "Personalized Academic Research Paper Recommendation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A huge number of academic papers are coming out from a lot of conferences and\njournals these days. In these circumstances, most researchers rely on key-based\nsearch or browsing through proceedings of top conferences and journals to find\ntheir related work. To ease this difficulty, we propose a Personalized Academic\nResearch Paper Recommendation System, which recommends related articles, for\neach researcher, that may be interesting to her/him. In this paper, we first\nintroduce our web crawler to retrieve research papers from the web. Then, we\ndefine similarity between two research papers based on the text similarity\nbetween them. Finally, we propose our recommender system developed using\ncollaborative filtering methods. Our evaluation results demonstrate that our\nsystem recommends good quality research papers.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 15:53:53 GMT"}], "update_date": "2013-04-22", "authors_parsed": [["Lee", "Joonseok", ""], ["Lee", "Kisung", ""], ["Kim", "Jennifer G.", ""]]}, {"id": "1304.6023", "submitter": "Gonzalo Navarro", "authors": "Gonzalo Navarro", "title": "Spaces, Trees and Colors: The Algorithmic Landscape of Document\n  Retrieval on Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document retrieval is one of the best established information retrieval\nactivities since the sixties, pervading all search engines. Its aim is to\nobtain, from a collection of text documents, those most relevant to a pattern\nquery. Current technology is mostly oriented to \"natural language\" text\ncollections, where inverted indices are the preferred solution. As successful\nas this paradigm has been, it fails to properly handle some East Asian\nlanguages and other scenarios where the \"natural language\" assumptions do not\nhold. In this survey we cover the recent research in extending the document\nretrieval techniques to a broader class of sequence collections, which has\napplications bioinformatics, data and Web mining, chemoinformatics, software\nengineering, multimedia information retrieval, and many others. We focus on the\nalgorithmic aspects of the techniques, uncovering a rich world of relations\nbetween document retrieval challenges and fundamental problems on trees,\nstrings, range queries, discrete geometry, and others.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 17:12:43 GMT"}, {"version": "v2", "created": "Fri, 10 May 2013 05:52:04 GMT"}, {"version": "v3", "created": "Tue, 14 May 2013 05:04:06 GMT"}, {"version": "v4", "created": "Wed, 26 Jun 2013 23:11:49 GMT"}, {"version": "v5", "created": "Fri, 27 Sep 2013 21:35:29 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Navarro", "Gonzalo", ""]]}, {"id": "1304.6181", "submitter": "GuangGang Geng", "authors": "Guang-Gang Geng, Xiao-Bo Jin, Xin-Chang Zhang, De-Xian Zhang", "title": "Evaluating Web Content Quality via Multi-scale Features", "comments": "4 pages, 1 figures, ecml/pkdd 2010 discovery challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web content quality measurement is crucial to various web content processing\napplications. This paper will explore multi-scale features which may affect the\nquality of a host, and develop automatic statistical methods to evaluate the\nWeb content quality. The extracted properties include statistical content\nfeatures, page and host level link features and TFIDF features. The experiments\non ECML/PKDD 2010 Discovery Challenge data set show that the algorithm is\neffective and feasible for the quality tasks of multiple languages, and the\nmulti-scale features have different identification ability and provide good\ncomplement to each other for most tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 06:42:55 GMT"}], "update_date": "2013-04-24", "authors_parsed": [["Geng", "Guang-Gang", ""], ["Jin", "Xiao-Bo", ""], ["Zhang", "Xin-Chang", ""], ["Zhang", "De-Xian", ""]]}, {"id": "1304.6480", "submitter": "Liwei Wang", "authors": "Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Tie-Yan Liu, Wei Chen", "title": "A Theoretical Analysis of NDCG Type Ranking Measures", "comments": "COLT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in ranking is to design a ranking measure for evaluation of\nranking functions. In this paper we study, from a theoretical perspective, the\nwidely used Normalized Discounted Cumulative Gain (NDCG)-type ranking measures.\nAlthough there are extensive empirical studies of NDCG, little is known about\nits theoretical properties. We first show that, whatever the ranking function\nis, the standard NDCG which adopts a logarithmic discount, converges to 1 as\nthe number of items to rank goes to infinity. On the first sight, this result\nis very surprising. It seems to imply that NDCG cannot differentiate good and\nbad ranking functions, contradicting to the empirical success of NDCG in many\napplications. In order to have a deeper understanding of ranking measures in\ngeneral, we propose a notion referred to as consistent distinguishability. This\nnotion captures the intuition that a ranking measure should have such a\nproperty: For every pair of substantially different ranking functions, the\nranking measure can decide which one is better in a consistent manner on almost\nall datasets. We show that NDCG with logarithmic discount has consistent\ndistinguishability although it converges to the same limit for all ranking\nfunctions. We next characterize the set of all feasible discount functions for\nNDCG according to the concept of consistent distinguishability. Specifically we\nshow that whether NDCG has consistent distinguishability depends on how fast\nthe discount decays, and 1/r is a critical point. We then turn to the cut-off\nversion of NDCG, i.e., NDCG@k. We analyze the distinguishability of NDCG@k for\nvarious choices of k and the discount functions. Experimental results on real\nWeb search datasets agree well with the theory.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2013 04:08:23 GMT"}], "update_date": "2013-04-25", "authors_parsed": [["Wang", "Yining", ""], ["Wang", "Liwei", ""], ["Li", "Yuanzhi", ""], ["He", "Di", ""], ["Liu", "Tie-Yan", ""], ["Chen", "Wei", ""]]}, {"id": "1304.6601", "submitter": "Young-Ho Eom", "authors": "Young-Ho Eom, Klaus M. Frahm, Andr\\'as Bencz\\'ur, Dima L. Shepelyansky", "title": "Time evolution of Wikipedia network ranking", "comments": "10 pages, 11 figures. Accepted for publication in EPJB", "journal-ref": "Eur. Phys. J. B. (2013) 86: 492", "doi": "10.1140/epjb/e2013-40432-5", "report-no": null, "categories": "physics.soc-ph cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the time evolution of ranking and spectral properties of the Google\nmatrix of English Wikipedia hyperlink network during years 2003 - 2011. The\nstatistical properties of ranking of Wikipedia articles via PageRank and\nCheiRank probabilities, as well as the matrix spectrum, are shown to be\nstabilized for 2007 - 2011. A special emphasis is done on ranking of Wikipedia\npersonalities and universities. We show that PageRank selection is dominated by\npoliticians while 2DRank, which combines PageRank and CheiRank, gives more\naccent on personalities of arts. The Wikipedia PageRank of universities\nrecovers 80 percents of top universities of Shanghai ranking during the\nconsidered time period.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2013 14:29:20 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2013 15:27:34 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Eom", "Young-Ho", ""], ["Frahm", "Klaus M.", ""], ["Bencz\u00far", "Andr\u00e1s", ""], ["Shepelyansky", "Dima L.", ""]]}, {"id": "1304.6920", "submitter": "Jo\\~ao Barros", "authors": "Joao Barros, Zeno Toffano, Youssef Meguebli and Bich-Li\\^en Doan", "title": "Contextual Query Using Bell Tests", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tests are essential in Information Retrieval and Data Mining in order to\nevaluate the effectiveness of a query. An automatic measure tool intended to\nexhibit the meaning of words in context has been developed and linked with\nQuantum Theory, particularly entanglement. \"Quantum like\" experiments were\nundertaken on semantic space based on the Hyperspace Analogue Language (HAL)\nmethod. A quantum HAL model was implemented using state vectors issued from the\nHAL matrix and query observables, testing a wide range of windows sizes. The\nBell parameter S, associating measures on two words in a document, was derived\nshowing peaks for specific window sizes. The peaks show maximum quantum\nviolation of the Bell inequalities and are document dependent. This new\ncorrelation measure inspired by Quantum Theory could be promising for measuring\nquery relevance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 14:23:28 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2013 08:46:05 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2013 08:19:45 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Barros", "Joao", ""], ["Toffano", "Zeno", ""], ["Meguebli", "Youssef", ""], ["Doan", "Bich-Li\u00ean", ""]]}, {"id": "1304.7157", "submitter": "Leon Derczynski", "authors": "Leon Derczynski, Richard Shaw, Ben Solway, Jun Wang", "title": "Question Answering Against Very-Large Text Collections", "comments": null, "journal-ref": "Master's theses, 2008, University of Sheffield", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Question answering involves developing methods to extract useful information\nfrom large collections of documents. This is done with specialised search\nengines such as Answer Finder. The aim of Answer Finder is to provide an answer\nto a question rather than a page listing related documents that may contain the\ncorrect answer. So, a question such as \"How tall is the Eiffel Tower\" would\nsimply return \"325m\" or \"1,063ft\". Our task was to build on the current version\nof Answer Finder by improving information retrieval, and also improving the\npre-processing involved in question series analysis.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 13:27:19 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Derczynski", "Leon", ""], ["Shaw", "Richard", ""], ["Solway", "Ben", ""], ["Wang", "Jun", ""]]}, {"id": "1304.7224", "submitter": "Stian Soiland-Reyes", "authors": "Paolo Ciccarese, Stian Soiland-Reyes, Khalid Belhajjame, Alasdair J G\n  Gray, Carole Goble, Tim Clark", "title": "PAV ontology: Provenance, Authoring and Versioning", "comments": "22 pages (incl 5 tables and 19 figures). Submitted to Journal of\n  Biomedical Semantics 2013-04-26 (#1858276535979415). Revised article\n  submitted 2013-08-30. Second revised article submitted 2013-10-06. Accepted\n  2013-10-07. Author proofs sent 2013-10-09 and 2013-10-16. Published\n  2013-11-22. Final version 2013-12-06.\n  http://www.jbiomedsem.com/content/4/1/37", "journal-ref": "Journal of Biomedical Semantics 2013, 4:37", "doi": "10.1186/2041-1480-4-37", "report-no": "University of Manchester eScholar: uk-ac-man-scw:193385", "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Provenance is a critical ingredient for establishing trust of published\nscientific content. This is true whether we are considering a data set, a\ncomputational workflow, a peer-reviewed publication or a simple scientific\nclaim with supportive evidence. Existing vocabularies such as DC Terms and the\nW3C PROV-O are domain-independent and general-purpose and they allow and\nencourage for extensions to cover more specific needs. We identify the specific\nneed for identifying or distinguishing between the various roles assumed by\nagents manipulating digital artifacts, such as author, contributor and curator.\n  We present the Provenance, Authoring and Versioning ontology (PAV): a\nlightweight ontology for capturing just enough descriptions essential for\ntracking the provenance, authoring and versioning of web resources. We argue\nthat such descriptions are essential for digital scientific content. PAV\ndistinguishes between contributors, authors and curators of content and\ncreators of representations in addition to the provenance of originating\nresources that have been accessed, transformed and consumed. We explore five\nprojects (and communities) that have adopted PAV illustrating their usage\nthrough concrete examples. Moreover, we present mappings that show how PAV\nextends the PROV-O ontology to support broader interoperability.\n  The authors strived to keep PAV lightweight and compact by including only\nthose terms that have demonstrated to be pragmatically useful in existing\napplications, and by recommending terms from existing ontologies when\nplausible.\n  We analyze and compare PAV with related approaches, namely Provenance\nVocabulary, DC Terms and BIBFRAME. We identify similarities and analyze their\ndifferences with PAV, outlining strengths and weaknesses of our proposed model.\nWe specify SKOS mappings that align PAV with DC Terms.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 16:48:15 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2013 16:18:14 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2013 15:34:45 GMT"}, {"version": "v4", "created": "Mon, 21 Oct 2013 09:31:12 GMT"}, {"version": "v5", "created": "Fri, 22 Nov 2013 14:57:47 GMT"}, {"version": "v6", "created": "Fri, 6 Dec 2013 23:53:56 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Ciccarese", "Paolo", ""], ["Soiland-Reyes", "Stian", ""], ["Belhajjame", "Khalid", ""], ["Gray", "Alasdair J G", ""], ["Goble", "Carole", ""], ["Clark", "Tim", ""]]}, {"id": "1304.7355", "submitter": "Filip Proborszcz", "authors": "Filip Proborszcz", "title": "Web graph compression with fast access", "comments": "MSc thesis, May 2012, advisors: Szymon Grabowski, Wojciech Bieniecki;\n  65 pages, 16 figures, 6 tables, 5 code snippets, source code available at:\n  http://sourceforge.net/projects/webgraphcompres/files/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years studying the content of the World Wide Web became a very\nimportant yet rather difficult task. There is a need for a compression\ntechnique that would allow a web graph representation to be put into the memory\nwhile maintaining random access time competitive to the time needed to access\nuncompressed web graph on a hard drive.\n  There are already available techniques that accomplish this task, but there\nis still room for improvements and this thesis attempts to prove it. It\nincludes a comparison of two methods contained in state of art of this field\n(BV and k2partitioned) to two already implemented algorithms (rewritten,\nhowever, in C++ programming language to maximize speed and resource management\nefficiency), which are LM and 2D, and introduces the new variant of the latter\none, called 2D stripes.\n  This thesis serves as well as a proof of concept. The final considerations\nshow positive and negative aspects of all presented methods, expose the\nfeasibility of the new variant as well as indicate future direction for\ndevelopment.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2013 10:51:34 GMT"}, {"version": "v2", "created": "Wed, 1 May 2013 10:28:49 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Proborszcz", "Filip", ""]]}]