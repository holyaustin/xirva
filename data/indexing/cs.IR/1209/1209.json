[{"id": "1209.0057", "submitter": "Tao Zhou", "authors": "Zimo Yang, Zi-Ke Zhang, Tao Zhou", "title": "Anchoring Bias in Online Voting", "comments": "5 pages, 4 tables, 5 figures", "journal-ref": "EPL 100 (2012) 68002", "doi": "10.1209/0295-5075/100/68002", "report-no": null, "categories": "physics.data-an cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voting online with explicit ratings could largely reflect people's\npreferences and objects' qualities, but ratings are always irrational, because\nthey may be affected by many unpredictable factors like mood, weather, as well\nas other people's votes. By analyzing two real systems, this paper reveals a\nsystematic bias embedding in the individual decision-making processes, namely\npeople tend to give a low rating after a low rating, as well as a high rating\nfollowing a high rating. This so-called \\emph{anchoring bias} is validated via\nextensive comparisons with null models, and numerically speaking, the extent of\nbias decays with interval voting number in a logarithmic form. Our findings\ncould be applied in the design of recommender systems and considered as\nimportant complementary materials to previous knowledge about anchoring effects\non financial trades, performance judgements, auctions, and so on.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2012 05:20:00 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Yang", "Zimo", ""], ["Zhang", "Zi-Ke", ""], ["Zhou", "Tao", ""]]}, {"id": "1209.0126", "submitter": "Hardik Joshi Mr.", "authors": "Hardik J. Joshi and Pareek Jyoti", "title": "Evaluation of some Information Retrieval models for Gujarati Ad hoc\n  Monolingual Tasks", "comments": "6 pages, Some text in Gujarati Language", "journal-ref": "VNSGU Journal of Science and Technology,3,2,176-181,2012", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper describes the work towards Gujarati Ad hoc Monolingual Retrieval\ntask for widely used Information Retrieval (IR) models. We present an indexing\nbaseline for the Gujarati Language represented by Mean Average Precision (MAP)\nvalues. Our objective is to obtain a relative picture of a better IR model for\nGujarati Language. Results show that Classical IR models like Term Frequency\nInverse Document Frequency (TF_IDF) performs better when compared to few recent\nprobabilistic IR models. The experiments helped to identify the outperforming\nIR models for Gujarati Language.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2012 19:45:01 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Joshi", "Hardik J.", ""], ["Jyoti", "Pareek", ""]]}, {"id": "1209.0249", "submitter": "Mohammed El-Dosuky", "authors": "M. A. El-Dosuky, M. Z. Rashad, T. T. Hamza, A. H. EL-Bassiouny", "title": "Robopinion: Opinion Mining Framework Inspired by Autonomous Robot\n  Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data association methods are used by autonomous robots to find matches\nbetween the current landmarks and the new set of observed features. We seek a\nframework for opinion mining to benefit from advancements in autonomous robot\nnavigation in both research and development\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2012 06:00:04 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["El-Dosuky", "M. A.", ""], ["Rashad", "M. Z.", ""], ["Hamza", "T. T.", ""], ["EL-Bassiouny", "A. H.", ""]]}, {"id": "1209.0911", "submitter": "Junming Huang Junming Huang", "authors": "Junming Huang, Xue-Qi Cheng, Hua-Wei Shen, Xiaoming Sun, Tao Zhou,\n  Xiaolong Jin", "title": "Conquering the rating bound problem in neighborhood-based collaborative\n  filtering: a function recovery approach", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important tool for information filtering in the era of socialized web,\nrecommender systems have witnessed rapid development in the last decade. As\nbenefited from the better interpretability, neighborhood-based collaborative\nfiltering techniques, such as item-based collaborative filtering adopted by\nAmazon, have gained a great success in many practical recommender systems.\nHowever, the neighborhood-based collaborative filtering method suffers from the\nrating bound problem, i.e., the rating on a target item that this method\nestimates is bounded by the observed ratings of its all neighboring items.\nTherefore, it cannot accurately estimate the unobserved rating on a target\nitem, if its ground truth rating is actually higher (lower) than the highest\n(lowest) rating over all items in its neighborhood. In this paper, we address\nthis problem by formalizing rating estimation as a task of recovering a scalar\nrating function. With a linearity assumption, we infer all the ratings by\noptimizing the low-order norm, e.g., the $l_1/2$-norm, of the second derivative\nof the target scalar function, while remaining its observed ratings unchanged.\nExperimental results on three real datasets, namely Douban, Goodreads and\nMovieLens, demonstrate that the proposed approach can well overcome the rating\nbound problem. Particularly, it can significantly improve the accuracy of\nrating estimation by 37% than the conventional neighborhood-based methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 09:55:27 GMT"}], "update_date": "2012-11-07", "authors_parsed": [["Huang", "Junming", ""], ["Cheng", "Xue-Qi", ""], ["Shen", "Hua-Wei", ""], ["Sun", "Xiaoming", ""], ["Zhou", "Tao", ""], ["Jin", "Xiaolong", ""]]}, {"id": "1209.1125", "submitter": "Jamel Slimi", "authors": "Jamel Slimi, Anis Ben Ammar and Adel M. Alimi", "title": "Video Data Visualization System: Semantic Classification And\n  Personalization", "comments": "graphics", "journal-ref": null, "doi": "10.5121/ijcga.2012.2201", "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper an intelligent video data visualization tool, based\non semantic classification, for retrieving and exploring a large scale corpus\nof videos. Our work is based on semantic classification resulting from semantic\nanalysis of video. The obtained classes will be projected in the visualization\nspace. The graph is represented by nodes and edges, the nodes are the keyframes\nof video documents and the edges are the relation between documents and the\nclasses of documents. Finally, we construct the user's profile, based on the\ninteraction with the system, to render the system more adequate to its\nreferences.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 21:28:32 GMT"}], "update_date": "2012-09-07", "authors_parsed": [["Slimi", "Jamel", ""], ["Ammar", "Anis Ben", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1209.1318", "submitter": "Edwin Henneken", "authors": "Michael J. Kurtz and Edwin A. Henneken", "title": "Finding and Recommending Scholarly Articles", "comments": "14 pages, part of the forthcoming MIT book \"Bibliometrics and Beyond:\n  Metrics-Based Evaluation of Scholarly Research\" edited by Blaise Cronin and\n  Cassidy R. Sugimoto", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR astro-ph.IM cs.DL physics.soc-ph", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The rate at which scholarly literature is being produced has been increasing\nat approximately 3.5 percent per year for decades. This means that during a\ntypical 40 year career the amount of new literature produced each year\nincreases by a factor of four. The methods scholars use to discover relevant\nliterature must change. Just like everybody else involved in information\ndiscovery, scholars are confronted with information overload. Two decades ago,\nthis discovery process essentially consisted of paging through abstract books,\ntalking to colleagues and librarians, and browsing journals. A time-consuming\nprocess, which could even be longer if material had to be shipped from\nelsewhere. Now much of this discovery process is mediated by online scholarly\ninformation systems. All these systems are relatively new, and all are still\nchanging. They all share a common goal: to provide their users with access to\nthe literature relevant to their specific needs. To achieve this each system\nresponds to actions by the user by displaying articles which the system judges\nrelevant to the user's current needs. Recently search systems which use\nparticularly sophisticated methodologies to recommend a few specific papers to\nthe user have been called \"recommender systems\". These methods are in line with\nthe current use of the term \"recommender system\" in computer science. We do not\nadopt this definition, rather we view systems like these as components in a\nlarger whole, which is presented by the scholarly information systems\nthemselves. In what follows we view the recommender system as an aspect of the\nentire information system; one which combines the massive memory capacities of\nthe machine with the cognitive abilities of the human user to achieve a\nhuman-machine synergy.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2012 15:37:37 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Kurtz", "Michael J.", ""], ["Henneken", "Edwin A.", ""]]}, {"id": "1209.1481", "submitter": "Tobias Kuhn", "authors": "Tobias Kuhn and Michael Krauthammer", "title": "Image Mining from Gel Diagrams in Biomedical Publications", "comments": null, "journal-ref": "Proceedings of the 5th International Symposium on Semantic Mining\n  in Biomedicine (SMBM 2012)", "doi": null, "report-no": null, "categories": "cs.IR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authors of biomedical publications often use gel images to report\nexperimental results such as protein-protein interactions or protein\nexpressions under different conditions. Gel images offer a way to concisely\ncommunicate such findings, not all of which need to be explicitly discussed in\nthe article text. This fact together with the abundance of gel images and their\nshared common patterns makes them prime candidates for image mining endeavors.\nWe introduce an approach for the detection of gel images, and present an\nautomatic workflow to analyze them. We are able to detect gel segments and\npanels at high accuracy, and present first results for the identification of\ngene names in these images. While we cannot provide a complete solution at this\npoint, we present evidence that this kind of image mining is feasible.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2012 10:24:48 GMT"}], "update_date": "2012-09-10", "authors_parsed": [["Kuhn", "Tobias", ""], ["Krauthammer", "Michael", ""]]}, {"id": "1209.1483", "submitter": "Tobias Kuhn", "authors": "Tobias Kuhn and Michael Krauthammer", "title": "Underspecified Scientific Claims in Nanopublications", "comments": null, "journal-ref": "In Proceedings of the Web of Linked Entities Workshop (WoLE 2012),\n  CEUR Workshop Proceedings, Volume 906, 2012", "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application range of nanopublications --- small entities of scientific\nresults in RDF representation --- could be greatly extended if complete formal\nrepresentations are not mandatory. To that aim, we present an approach to\nrepresent and interlink scientific claims in an underspecified way, based on\nindependent English sentences.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2012 10:29:35 GMT"}], "update_date": "2013-01-21", "authors_parsed": [["Kuhn", "Tobias", ""], ["Krauthammer", "Michael", ""]]}, {"id": "1209.1719", "submitter": "Tiago Simas", "authors": "Tiago Simas and Luis M. Rocha", "title": "Semi-metric networks for recommender systems", "comments": null, "journal-ref": "2012 IEEE/WIC/ACM International Conference on Web Intelligence and\n  Intelligent Agent Technology", "doi": null, "report-no": null, "categories": "cs.IR cond-mat.stat-mech cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted graphs obtained from co-occurrence in user-item relations lead to\nnon-metric topologies. We use this semi-metric behavior to issue\nrecommendations, and discuss its relationship to transitive closure on fuzzy\ngraphs. Finally, we test the performance of this method against other item- and\nuser-based recommender systems on the Movielens benchmark. We show that\nincluding highly semi-metric edges in our recommendation algorithms leads to\nbetter recommendations.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2012 14:02:12 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Simas", "Tiago", ""], ["Rocha", "Luis M.", ""]]}, {"id": "1209.1983", "submitter": "Frank Meyer", "authors": "Frank Meyer, Fran\\c{c}oise Fessant, Fabrice Cl\\'erot, Eric Gaussier", "title": "Toward a New Protocol to Evaluate Recommender Systems", "comments": "6 pages. arXiv admin note: text overlap with arXiv:1203.4487", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an approach to analyze the performance and the\nadded value of automatic recommender systems in an industrial context. We show\nthat recommender systems are multifaceted and can be organized around 4\nstructuring functions: help users to decide, help users to compare, help users\nto discover, help users to explore. A global off line protocol is then proposed\nto evaluate recommender systems. This protocol is based on the definition of\nappropriate evaluation measures for each aforementioned function. The\nevaluation protocol is discussed from the perspective of the usefulness and\ntrust of the recommendation. A new measure called Average Measure of Impact is\nintroduced. This measure evaluates the impact of the personalized\nrecommendation. We experiment with two classical methods, K-Nearest Neighbors\n(KNN) and Matrix Factorization (MF), using the well known dataset: Netflix. A\nsegmentation of both users and items is proposed to finely analyze where the\nalgorithms perform well or badly. We show that the performance is strongly\ndependent on the segments and that there is no clear correlation between the\nRMSE and the quality of the recommendation.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 13:27:23 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Meyer", "Frank", ""], ["Fessant", "Fran\u00e7oise", ""], ["Cl\u00e9rot", "Fabrice", ""], ["Gaussier", "Eric", ""]]}, {"id": "1209.2070", "submitter": "Yi Wang", "authors": "Yi Wang", "title": "Content-based Multi-media Retrieval Technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives a summary of the content-based Image Retrieval and\nContent-based Audio Retrieval, which are two parts of the Content-based\nRetrieval. Content-based Retrieval is the retrieval based on the features of\nthe content. Generally, it is a way to extract features of the media data and\nfind other data with the similar features from the database automatically.\nContent-based Retrieval can not only work on discrete media like texts, but\nalso can be used on continuous media, such as video and audio.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2012 16:00:06 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Wang", "Yi", ""]]}, {"id": "1209.2097", "submitter": "Nadja Kutz", "authors": "Nadja Kutz", "title": "Semantic web applications with regard to math and environment", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following is an outline of possible strategies in using semantic web\ntechniques and math with regard to environmental issues. The article uses\nconcrete examples and applications and provides partially a rather basic\ntreatment of semantic web techniques and math in order to adress a broader\naudience.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2012 10:27:48 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Kutz", "Nadja", ""]]}, {"id": "1209.2137", "submitter": "Daniel Lemire", "authors": "Daniel Lemire and Leonid Boytsov", "title": "Decoding billions of integers per second through vectorization", "comments": "For software, see https://github.com/lemire/FastPFor, For data, see\n  http://boytsov.info/datasets/clueweb09gap/", "journal-ref": "Software: Practice & Experience 45 (1), 2015", "doi": "10.1002/spe.2203", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many important applications -- such as search engines and relational\ndatabase systems -- data is stored in the form of arrays of integers. Encoding\nand, most importantly, decoding of these arrays consumes considerable CPU time.\nTherefore, substantial effort has been made to reduce costs associated with\ncompression and decompression. In particular, researchers have exploited the\nsuperscalar nature of modern processors and SIMD instructions. Nevertheless, we\nintroduce a novel vectorized scheme called SIMD-BP128 that improves over\npreviously proposed vectorized approaches. It is nearly twice as fast as the\npreviously fastest schemes on desktop processors (varint-G8IU and PFOR). At the\nsame time, SIMD-BP128 saves up to 2 bits per integer. For even better\ncompression, we propose another new vectorized scheme (SIMD-FastPFOR) that has\na compression ratio within 10% of a state-of-the-art scheme (Simple-8b) while\nbeing two times faster during decoding.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 20:08:03 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2012 15:01:28 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2012 14:18:36 GMT"}, {"version": "v4", "created": "Tue, 19 Feb 2013 21:16:35 GMT"}, {"version": "v5", "created": "Fri, 19 Apr 2013 02:27:52 GMT"}, {"version": "v6", "created": "Thu, 15 May 2014 15:02:22 GMT"}, {"version": "v7", "created": "Sat, 30 Jan 2021 18:23:38 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Lemire", "Daniel", ""], ["Boytsov", "Leonid", ""]]}, {"id": "1209.2274", "submitter": "Reza Tavoli", "authors": "Reza Tavoli, Fariborz Mahmoudi", "title": "PCA-Based Relevance Feedback in Document Image Retrieval", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 4, No 2, July 2012", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research has been devoted in the past few years to relevance feedback as an\neffective solution to improve performance of information retrieval systems.\nRelevance feedback refers to an interactive process that helps to improve the\nretrieval performance. In this paper we propose the use of relevance feedback\nto improve document image retrieval System (DIRS) performance. This paper\ncompares a variety of strategies for positive and negative feedback. In\naddition, feature subspace is extracted and updated during the feedback process\nusing a Principal Component Analysis (PCA) technique and based on user's\nfeedback. That is, in addition to reducing the dimensionality of feature\nspaces, a proper subspace for each type of features is obtained in the feedback\nprocess to further improve the retrieval accuracy. Experiments show that using\nrelevance Feedback in DIR achieves better performance than common DIR.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 10:02:47 GMT"}], "update_date": "2012-09-12", "authors_parsed": [["Tavoli", "Reza", ""], ["Mahmoudi", "Fariborz", ""]]}, {"id": "1209.2341", "submitter": "Subhabrata Mukherjee", "authors": "A.R. Balamurali, Subhabrata Mukherjee, Akshat Malu, Pushpak\n  Bhattacharyya", "title": "Leveraging Sentiment to Compute Word Similarity", "comments": "The paper is available at\n  http://subhabrata-mukherjee.webs.com/publications.htm", "journal-ref": "In Proceedings of The 6th International Global Wordnet Conference\n  (GWC 2012), Matsue, Japan, January, 9-13, 2012", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we introduce a new WordNet based similarity metric, SenSim,\nwhich incorporates sentiment content (i.e., degree of positive or negative\nsentiment) of the words being compared to measure the similarity between them.\nThe proposed metric is based on the hypothesis that knowing the sentiment is\nbeneficial in measuring the similarity. To verify this hypothesis, we measure\nand compare the annotator agreement for 2 annotation strategies: 1) sentiment\ninformation of a pair of words is considered while annotating and 2) sentiment\ninformation of a pair of words is not considered while annotating.\nInter-annotator correlation scores show that the agreement is better when the\ntwo annotators consider sentiment information while assigning a similarity\nscore to a pair of words. We use this hypothesis to measure the similarity\nbetween a pair of words. Specifically, we represent each word as a vector\ncontaining sentiment scores of all the content words in the WordNet gloss of\nthe sense of that word. These sentiment scores are derived from a sentiment\nlexicon. We then measure the cosine similarity between the two vectors. We\nperform both intrinsic and extrinsic evaluation of SenSim and compare the\nperformance with other widely usedWordNet similarity metrics.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 15:02:20 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2012 14:42:30 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Balamurali", "A. R.", ""], ["Mukherjee", "Subhabrata", ""], ["Malu", "Akshat", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1209.2352", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Pushpak Bhattacharyya", "title": "Feature Specific Sentiment Analysis for Product Reviews", "comments": "The paper is available at\n  http://subhabrata-mukherjee.webs.com/publications.htm", "journal-ref": "COMPUTATIONAL LINGUISTICS AND INTELLIGENT TEXT PROCESSING, Lecture\n  Notes in Computer Science, 2012, Volume 7181/2012, 475-487", "doi": "10.1007/978-3-642-28604-9_39", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we present a novel approach to identify feature specific\nexpressions of opinion in product reviews with different features and mixed\nemotions. The objective is realized by identifying a set of potential features\nin the review and extracting opinion expressions about those features by\nexploiting their associations. Capitalizing on the view that more closely\nassociated words come together to express an opinion about a certain feature,\ndependency parsing is used to identify relations between the opinion\nexpressions. The system learns the set of significant relations to be used by\ndependency parsing and a threshold parameter which allows us to merge closely\nassociated opinion expressions. The data requirement is minimal as this is a\none time learning of the domain independent parameters. The associations are\nrepresented in the form of a graph which is partitioned to finally retrieve the\nopinion expression describing the user specified feature. We show that the\nsystem achieves a high accuracy across all domains and performs at par with\nstate-of-the-art systems despite its data limitations.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 15:39:18 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2012 14:44:36 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1209.2355", "submitter": "L\\'eon Bottou", "authors": "L\\'eon Bottou, Jonas Peters, Joaquin Qui\\~nonero-Candela, Denis X.\n  Charles, D. Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, Ed\n  Snelson", "title": "Counterfactual Reasoning and Learning Systems", "comments": "revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work shows how to leverage causal inference to understand the behavior\nof complex learning systems interacting with their environment and predict the\nconsequences of changes to the system. Such predictions allow both humans and\nalgorithms to select changes that improve both the short-term and long-term\nperformance of such systems. This work is illustrated by experiments carried\nout on the ad placement system associated with the Bing search engine.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 15:47:43 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2012 16:47:55 GMT"}, {"version": "v3", "created": "Fri, 28 Sep 2012 21:36:18 GMT"}, {"version": "v4", "created": "Thu, 10 Jan 2013 03:09:16 GMT"}, {"version": "v5", "created": "Sat, 27 Jul 2013 18:02:46 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Bottou", "L\u00e9on", ""], ["Peters", "Jonas", ""], ["Qui\u00f1onero-Candela", "Joaquin", ""], ["Charles", "Denis X.", ""], ["Chickering", "D. Max", ""], ["Portugaly", "Elon", ""], ["Ray", "Dipankar", ""], ["Simard", "Patrice", ""], ["Snelson", "Ed", ""]]}, {"id": "1209.2433", "submitter": "James Risk", "authors": "James Risk", "title": "Correlations between Google search data and Mortality Rates", "comments": "4 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by correlations recently discovered between Google search data and\nfinancial markets, we show correlations between Google search data mortality\nrates. Words with negative connotations may provide for increased mortality\nrates, while words with positive connotations may provide for decreased\nmortality rates, and so statistical methods were employed to determine to\ninvestigate further.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 20:26:48 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2012 22:38:57 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Risk", "James", ""]]}, {"id": "1209.2493", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Pushpak Bhattacharyya", "title": "WikiSent : Weakly Supervised Sentiment Analysis Through Extractive\n  Summarization With Wikipedia", "comments": "The paper is available at\n  http://subhabrata-mukherjee.webs.com/publications.htm", "journal-ref": "Lecture Notes in Computer Science Volume 7523, 2012, pp 774-793", "doi": "10.1007/978-3-642-33460-3_55", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper describes a weakly supervised system for sentiment analysis in the\nmovie review domain. The objective is to classify a movie review into a\npolarity class, positive or negative, based on those sentences bearing opinion\non the movie alone. The irrelevant text, not directly related to the reviewer\nopinion on the movie, is left out of analysis. Wikipedia incorporates the world\nknowledge of movie-specific features in the system which is used to obtain an\nextractive summary of the review, consisting of the reviewer's opinions about\nthe specific aspects of the movie. This filters out the concepts which are\nirrelevant or objective with respect to the given movie. The proposed system,\nWikiSent, does not require any labeled data for training. The only weak\nsupervision arises out of the usage of resources like WordNet, Part-of-Speech\nTagger and Sentiment Lexicons by virtue of their construction. WikiSent\nachieves a considerable accuracy improvement over the baseline and has a better\nor comparable accuracy to the existing semi-supervised and unsupervised systems\nin the domain, on the same dataset. We also perform a general movie review\ntrend analysis using WikiSent to find the trend in movie-making and the public\nacceptance in terms of movie genre, year of release and polarity.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 04:33:08 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2012 14:44:11 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1209.2495", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Akshat Malu, A.R. Balamurali, Pushpak\n  Bhattacharyya", "title": "TwiSent: A Multistage System for Analyzing Sentiment in Twitter", "comments": "The paper is available at\n  http://subhabrata-mukherjee.webs.com/publications.htm", "journal-ref": "In Proceedings of The 21st ACM Conference on Information and\n  Knowledge Management (CIKM), 2012 as a poster", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we present TwiSent, a sentiment analysis system for Twitter.\nBased on the topic searched, TwiSent collects tweets pertaining to it and\ncategorizes them into the different polarity classes positive, negative and\nobjective. However, analyzing micro-blog posts have many inherent challenges\ncompared to the other text genres. Through TwiSent, we address the problems of\n1) Spams pertaining to sentiment analysis in Twitter, 2) Structural anomalies\nin the text in the form of incorrect spellings, nonstandard abbreviations,\nslangs etc., 3) Entity specificity in the context of the topic searched and 4)\nPragmatics embedded in text. The system performance is evaluated on manually\nannotated gold standard data and on an automatically annotated tweet set based\non hashtags. It is a common practise to show the efficacy of a supervised\nsystem on an automatically annotated dataset. However, we show that such a\nsystem achieves lesser classification accurcy when tested on generic twitter\ndataset. We also show that our system performs much better than an existing\nsystem.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 04:39:37 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2012 14:43:49 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Malu", "Akshat", ""], ["Balamurali", "A. R.", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1209.2868", "submitter": "Georg Groh", "authors": "Georg Groh and Florian Straub and Benjamin Koster", "title": "Spatio-Temporal Small Worlds for Decentralized Information Retrieval in\n  Social Networking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss foundations and options for alternative, agent-based information\nretrieval (IR) approaches in Social Networking, especially Decentralized and\nMobile Social Networking scenarios. In addition to usual semantic contexts,\nthese approaches make use of long-term social and spatio-temporal contexts in\norder to satisfy conscious as well as unconscious information needs according\nto Human IR heuristics. Using a large Twitter dataset, we investigate these\napproaches and especially investigate the question in how far spatio-temporal\ncontexts can act as a conceptual bracket implicating social and semantic\ncohesion, giving rise to the concept of Spatio-Temporal Small Worlds.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 12:11:10 GMT"}], "update_date": "2012-09-14", "authors_parsed": [["Groh", "Georg", ""], ["Straub", "Florian", ""], ["Koster", "Benjamin", ""]]}, {"id": "1209.3026", "submitter": "Hany SalahEldeen", "authors": "Hany M. SalahEldeen and Michael L. Nelson", "title": "Losing My Revolution: How Many Resources Shared on Social Media Have\n  Been Lost?", "comments": "12 pages, Theory and Practice of Digital Libraries (TPDL) 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media content has grown exponentially in the recent years and the role\nof social media has evolved from just narrating life events to actually shaping\nthem. In this paper we explore how many resources shared in social media are\nstill available on the live web or in public web archives. By analyzing six\ndifferent event-centric datasets of resources shared in social media in the\nperiod from June 2009 to March 2012, we found about 11% lost and 20% archived\nafter just a year and an average of 27% lost and 41% archived after two and a\nhalf years. Furthermore, we found a nearly linear relationship between time of\nsharing of the resource and the percentage lost, with a slightly less linear\nrelationship between time of sharing and archiving coverage of the resource.\nFrom this model we conclude that after the first year of publishing, nearly 11%\nof shared resources will be lost and after that we will continue to lose 0.02%\nper day.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 20:08:07 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["SalahEldeen", "Hany M.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1209.3117", "submitter": "Khurram Naim Shamsi", "authors": "Khurram Naim Shamsi, Zafar Iqbal Khan", "title": "Development of an e-learning system incorporating semantic web", "comments": "4 pages, 2 figures, published at International Journal of Research in\n  Computer Science", "journal-ref": "International Journal of Research in Computer Science, 2 (5): pp.\n  11-14, 2012", "doi": "10.7815/ijorcs.25.2012.042", "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-Learning is efficient, task relevant and just-in-time learning grown from\nthe learning requirements of the new and dynamically changing world. The term\nSemantic Web covers the steps to create a new WWW architecture that augments\nthe content with formal semantics enabling better possibilities of navigation\nthrough the cyberspace and its contents. In this paper, we present the Semantic\nWeb-Based model for our e-learning system taking into account the learning\nenvironment at Saudi Arabian universities. The proposed system is mainly based\non ontology-based descriptions of content, context and structure of the\nlearning materials. It further provides flexible and personalized access to\nthese learning materials. The framework has been validated by an interview\nbased qualitative method.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2012 08:01:50 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["Shamsi", "Khurram Naim", ""], ["Khan", "Zafar Iqbal", ""]]}, {"id": "1209.3126", "submitter": "Juan Manuel Torres Moreno", "authors": "Juan-Manuel Torres-Moreno", "title": "Beyond Stemming and Lemmatization: Ultra-stemming to Improve Automatic\n  Text Summarization", "comments": "22 pages, 12 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In Automatic Text Summarization, preprocessing is an important phase to\nreduce the space of textual representation. Classically, stemming and\nlemmatization have been widely used for normalizing words. However, even using\nnormalization on large texts, the curse of dimensionality can disturb the\nperformance of summarizers. This paper describes a new method for normalization\nof words to further reduce the space of representation. We propose to reduce\neach word to its initial letters, as a form of Ultra-stemming. The results show\nthat Ultra-stemming not only preserve the content of summaries produced by this\nrepresentation, but often the performances of the systems can be dramatically\nimproved. Summaries on trilingual corpora were evaluated automatically with\nFresa. Results confirm an increase in the performance, regardless of summarizer\nsystem used.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2012 08:45:26 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["Torres-Moreno", "Juan-Manuel", ""]]}, {"id": "1209.3286", "submitter": "Nikolay Glazyrin", "authors": "Nikolay Glazyrin", "title": "Music Recommendation System for Million Song Dataset Challenge", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a system that took 8th place in Million Song Dataset challenge\nis described. Given full listening history for 1 million of users and half of\nlistening history for 110000 users participatints should predict the missing\nhalf. The system proposed here uses memory-based collaborative filtering\napproach and user-based similarity. MAP@500 score of 0.15037 was achieved.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2012 18:59:03 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2012 18:53:14 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Glazyrin", "Nikolay", ""]]}, {"id": "1209.3590", "submitter": "Ipsita Mohanty", "authors": "Ipsita Mohanty and R. Leela Velusamy", "title": "Information Retrieval From Internet Applications For Digital Forensic", "comments": "15 pages, 9 figures; International Journal of Security, Privacy and\n  Trust Management (IJSPTM), Vol. 1, No 3/4, August 2012", "journal-ref": null, "doi": "10.5121/ijsptm.2012.1302", "report-no": null, "categories": "cs.CR cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced internet technologies providing services like e-mail, social\nnetworking, online banking, online shopping etc., have made day-to-day\nactivities simple and convenient. Increasing dependency on the internet,\nconvenience, and decreasing cost of electronic devices have resulted in\nfrequent use of online services. However, increased indulgence over the\ninternet has also accelerated the pace of digital crimes. The increase in\nnumber and complexity of digital crimes has caught the attention of forensic\ninvestigators. The Digital Investigators are faced with the challenge of\ngathering accurate digital evidence from as many sources as possible. In this\npaper, an attempt was made to recover digital evidence from a system's RAM in\nthe form of information about the most recent browsing session of the user.\nFour different applications were chosen and the experiment was conducted across\ntwo browsers. It was found that crucial information about the target user such\nas, user name, passwords, etc., was recoverable.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2012 08:43:48 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Mohanty", "Ipsita", ""], ["Velusamy", "R. Leela", ""]]}, {"id": "1209.4471", "submitter": "Nikola Milo\\v{s}evi\\'c MSc", "authors": "Nikola Milo\\v{s}evi\\'c", "title": "Stemmer for Serbian language", "comments": "16 pages, 8 figures, code included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In linguistic morphology and information retrieval, stemming is the process\nfor reducing inflected (or sometimes derived) words to their stem, base or root\nform; generally a written word form. In this work is presented suffix stripping\nstemmer for Serbian language, one of the highly inflectional languages.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 09:21:29 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Milo\u0161evi\u0107", "Nikola", ""]]}, {"id": "1209.4479", "submitter": "Benjamin Piwowarski", "authors": "Benjamin Piwowarski and Georges Dupret and Mounia Lalmas", "title": "Beyond Cumulated Gain and Average Precision: Including Willingness and\n  Expectation in the User Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we define a new metric family based on two concepts: The\ndefinition of the stopping criterion and the notion of satisfaction, where the\nformer depends on the willingness and expectation of a user exploring search\nresults. Both concepts have been discussed so far in the IR literature, but we\nargue in this paper that defining a proper single valued metric depends on\nmerging them into a single conceptual framework.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 09:59:53 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Piwowarski", "Benjamin", ""], ["Dupret", "Georges", ""], ["Lalmas", "Mounia", ""]]}, {"id": "1209.4523", "submitter": "Egor Samosvat", "authors": "Damien Lefortier, Liudmila Ostroumova and Egor Samosvat", "title": "Evolution of the Media Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a detailed study of the part of the Web related to media content,\ni.e., the Media Web. Using publicly available data, we analyze the evolution of\nincoming and outgoing links from and to media pages. Based on our observations,\nwe propose a new class of models for the appearance of new media content on the\nWeb where different \\textit{attractiveness} functions of nodes are possible\nincluding ones taken from well-known preferential attachment and fitness\nmodels. We analyze these models theoretically and empirically and show which\nones realistically predict both the incoming degree distribution and the\nso-called \\textit{recency property} of the Media Web, something that existing\nmodels did not do well. Finally we compare these models by estimating the\nlikelihood of the real-world link graph from our data set given each model and\nobtain that models we introduce are significantly more likely than previously\nproposed ones. One of the most surprising results is that in the Media Web the\nprobability for a post to be cited is determined, most likely, by its quality\nrather than by its current popularity.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 13:18:00 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2013 14:38:37 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Lefortier", "Damien", ""], ["Ostroumova", "Liudmila", ""], ["Samosvat", "Egor", ""]]}, {"id": "1209.5448", "submitter": "Tanvir Ahmed", "authors": "Md. Abdullah al Mamun, Md. Hanif, Md. Rakib Uddin, Tanvir Ahmed, Md.\n  Mofizul Islam", "title": "A New Compression Based Index Structure for Efficient Information\n  Retrieval", "comments": "5 pages", "journal-ref": "International Journal of Science and Technology, Volume 2 No.1,\n  pp. 10-14, January 2012", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding desired information from large data set is a difficult problem.\nInformation retrieval is concerned with the structure, analysis, organization,\nstorage, searching, and retrieval of information. Index is the main constituent\nof an IR system. Now a day exponential growth of information makes the index\nstructure large enough affecting the IR system's quality. So compressing the\nIndex structure is our main contribution in this paper. We compressed the\ndocument number in inverted file entries using a new coding technique based on\nrun-length encoding. Our coding mechanism uses a specified code which acts over\nrun-length coding. We experimented and found that our coding mechanism on an\naverage compresses 67.34% percent more than the other techniques.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 22:27:17 GMT"}], "update_date": "2012-09-26", "authors_parsed": [["Mamun", "Md. Abdullah al", ""], ["Hanif", "Md.", ""], ["Uddin", "Md. Rakib", ""], ["Ahmed", "Tanvir", ""], ["Islam", "Md. Mofizul", ""]]}, {"id": "1209.5809", "submitter": "Onur Kucuktunc", "authors": "Onur K\\\"u\\c{c}\\\"uktun\\c{c}, Erik Saule, Kamer Kaya, \\\"Umit V.\n  \\c{C}ataly\\\"urek", "title": "Diversifying Citation Recommendations", "comments": "19 pages, manuscript under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literature search is arguably one of the most important phases of the\nacademic and non-academic research. The increase in the number of published\npapers each year makes manual search inefficient and furthermore insufficient.\nHence, automatized methods such as search engines have been of interest in the\nlast thirty years. Unfortunately, these traditional engines use keyword-based\napproaches to solve the search problem, but these approaches are prone to\nambiguity and synonymy. On the other hand, bibliographic search techniques\nbased only on the citation information are not prone to these problems since\nthey do not consider textual similarity. For many particular research areas and\ntopics, the amount of knowledge to humankind is immense, and obtaining the\ndesired information is as hard as looking for a needle in a haystack.\nFurthermore, sometimes, what we are looking for is a set of documents where\neach one is different than the others, but at the same time, as a whole we want\nthem to cover all the important parts of the literature relevant to our search.\nThis paper targets the problem of result diversification in citation-based\nbibliographic search. It surveys a set of techniques which aim to find a set of\npapers with satisfactory quality and diversity. We enhance these algorithms\nwith a direction-awareness functionality to allow the users to reach either\nold, well-cited, well-known research papers or recent, less-known ones. We also\npropose a set of novel techniques for a better diversification of the results.\nAll the techniques considered are compared by performing a rigorous\nexperimentation. The results show that some of the proposed techniques are very\nsuccessful in practice while performing a search in a bibliographic database.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 01:46:00 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["K\u00fc\u00e7\u00fcktun\u00e7", "Onur", ""], ["Saule", "Erik", ""], ["Kaya", "Kamer", ""], ["\u00c7ataly\u00fcrek", "\u00dcmit V.", ""]]}, {"id": "1209.5818", "submitter": "Bharath Pattabiraman", "authors": "Bharath Pattabiraman, Md. Mostofa Ali Patwary, Assefaw H. Gebremedhin,\n  Wei-keng Liao, and Alok Choudhary", "title": "Fast Algorithms for the Maximum Clique Problem on Massive Sparse Graphs", "comments": "15 pages (including 2-page appendix), 5 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum clique problem is a well known NP-Hard problem with applications\nin data mining, network analysis, informatics, and many other areas. Although\nthere exist several algorithms with acceptable runtimes for certain classes of\ngraphs, many of them are infeasible for massive graphs. We present a new exact\nalgorithm that employs novel pruning techniques to very quickly find maximum\ncliques in large sparse graphs. Extensive experiments on several types of\nsynthetic and real-world graphs show that our new algorithm is up to several\norders of magnitude faster than existing algorithms for most instances. We also\npresent a heuristic variant that runs orders of magnitude faster than the exact\nalgorithm, while providing optimal or near-optimal solutions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 02:23:31 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2012 19:48:59 GMT"}, {"version": "v3", "created": "Sat, 10 Nov 2012 01:21:42 GMT"}, {"version": "v4", "created": "Wed, 14 Nov 2012 17:31:36 GMT"}], "update_date": "2012-11-15", "authors_parsed": [["Pattabiraman", "Bharath", ""], ["Patwary", "Md. Mostofa Ali", ""], ["Gebremedhin", "Assefaw H.", ""], ["Liao", "Wei-keng", ""], ["Choudhary", "Alok", ""]]}, {"id": "1209.5833", "submitter": "Makiko Konoshima", "authors": "Makiko Konoshima and Yui Noma", "title": "Locality-Sensitive Hashing with Margin Based Feature Selection", "comments": "9 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning method with feature selection for Locality-Sensitive\nHashing. Locality-Sensitive Hashing converts feature vectors into bit arrays.\nThese bit arrays can be used to perform similarity searches and personal\nauthentication. The proposed method uses bit arrays longer than those used in\nthe end for similarity and other searches and by learning selects the bits that\nwill be used. We demonstrated this method can effectively perform optimization\nfor cases such as fingerprint images with a large number of labels and\nextremely few data that share the same labels, as well as verifying that it is\nalso effective for natural images, handwritten digits, and speech features.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 05:26:58 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2012 06:21:09 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Konoshima", "Makiko", ""], ["Noma", "Yui", ""]]}, {"id": "1209.6001", "submitter": "Jonathan Shapiro", "authors": "Ruefei He and Jonathan Shapiro", "title": "Bayesian Mixture Models for Frequent Itemset Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In binary-transaction data-mining, traditional frequent itemset mining often\nproduces results which are not straightforward to interpret. To overcome this\nproblem, probability models are often used to produce more compact and\nconclusive results, albeit with some loss of accuracy. Bayesian statistics have\nbeen widely used in the development of probability models in machine learning\nin recent years and these methods have many advantages, including their\nabilities to avoid overfitting. In this paper, we develop two Bayesian mixture\nmodels with the Dirichlet distribution prior and the Dirichlet process (DP)\nprior to improve the previous non-Bayesian mixture model developed for\ntransaction dataset mining. We implement the inference of both mixture models\nusing two methods: a collapsed Gibbs sampling scheme and a variational\napproximation algorithm. Experiments in several benchmark problems have shown\nthat both mixture models achieve better performance than a non-Bayesian mixture\nmodel. The variational algorithm is the faster of the two approaches while the\nGibbs sampling method achieves a more accurate results. The Dirichlet process\nmixture model can automatically grow to a proper complexity for a better\napproximation. Once the model is built, it can be very fast to query and run\nanalysis on (typically 10 times faster than Eclat, as we will show in the\nexperiment section). However, these approaches also show that mixture models\nunderestimate the probabilities of frequent itemsets. Consequently, these\nmodels have a higher sensitivity but a lower specificity.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 16:41:59 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["He", "Ruefei", ""], ["Shapiro", "Jonathan", ""]]}, {"id": "1209.6070", "submitter": "Tanvir Ahmed", "authors": "Khalid Ibnal Asad, Tanvir Ahmed, Md. Saiedur Rahman", "title": "Movie Popularity Classification based on Inherent Movie Attributes using\n  C4.5,PART and Correlation Coefficient", "comments": "6 pages", "journal-ref": "IEEE/OSA/IAPR International Conference on Informatics, Electronics\n  & Vision (ICIEV2012), pp. 747-752, May 2012", "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abundance of movie data across the internet makes it an obvious candidate for\nmachine learning and knowledge discovery. But most researches are directed\ntowards bi-polar classification of movie or generation of a movie\nrecommendation system based on reviews given by viewers on various internet\nsites. Classification of movie popularity based solely on attributes of a movie\ni.e. actor, actress, director rating, language, country and budget etc. has\nbeen less highlighted due to large number of attributes that are associated\nwith each movie and their differences in dimensions. In this paper, we propose\nclassification scheme of pre-release movie popularity based on inherent\nattributes using C4.5 and PART classifier algorithm and define the relation\nbetween attributes of post release movies using correlation coefficient.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 20:30:02 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Asad", "Khalid Ibnal", ""], ["Ahmed", "Tanvir", ""], ["Rahman", "Md. Saiedur", ""]]}, {"id": "1209.6449", "submitter": "Simone Faro", "authors": "Simone Faro and M. Oguzhan K\\\"ulekci", "title": "Fast Packed String Matching for Short Patterns", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for all occurrences of a pattern in a text is a fundamental problem\nin computer science with applications in many other fields, like natural\nlanguage processing, information retrieval and computational biology. In the\nlast two decades a general trend has appeared trying to exploit the power of\nthe word RAM model to speed-up the performances of classical string matching\nalgorithms. In this model an algorithm operates on words of length w, grouping\nblocks of characters, and arithmetic and logic operations on the words take one\nunit of time. In this paper we use specialized word-size packed string matching\ninstructions, based on the Intel streaming SIMD extensions (SSE) technology, to\ndesign very fast string matching algorithms in the case of short patterns. From\nour experimental results it turns out that, despite their quadratic worst case\ntime complexity, the new presented algorithms become the clear winners on the\naverage for short patterns, when compared against the most effective algorithms\nknown in literature.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 08:28:43 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Faro", "Simone", ""], ["K\u00fclekci", "M. Oguzhan", ""]]}, {"id": "1209.6492", "submitter": "Deepak Garg Dr", "authors": "Deepika Sharma and Deepak Garg", "title": "Information Retrieval on the web and its evaluation", "comments": null, "journal-ref": "International Journal of Computer Applications 40(3):26-31, 2012", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet is one of the main sources of information for millions of people.\nOne can find information related to practically all matters on internet.\nMoreover if we want to retrieve information about some particular topic we may\nfind thousands of Web Pages related to that topic. But our main concern is to\nfind relevant Web Pages from among that collection. So in this paper I have\ndiscussed that how information is retrieved from the web and the efforts\nrequired for retrieving this information in terms of system and users efforts.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 11:50:15 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Sharma", "Deepika", ""], ["Garg", "Deepak", ""]]}]