[{"id": "1903.00066", "submitter": "Ting Bai", "authors": "Ting Bai and Pan Du and Wayne Xin Zhao and Ji-Rong Wen and Jian-Yun\n  Nie", "title": "A Long-Short Demands-Aware Model for Next-Item Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending the right products is the central problem in recommender\nsystems, but the right products should also be recommended at the right time to\nmeet the demands of users, so as to maximize their values. Users' demands,\nimplying strong purchase intents, can be the most useful way to promote\nproducts sales if well utilized. Previous recommendation models mainly focused\non user's general interests to find the right products. However, the aspect of\nmeeting users' demands at the right time has been much less explored. To\naddress this problem, we propose a novel Long-Short Demands-aware Model (LSDM),\nin which both user's interests towards items and user's demands over time are\nincorporated. We summarize two aspects: termed as long-time demands (e.g.,\npurchasing the same product repetitively showing a long-time persistent\ninterest) and short-time demands (e.g., co-purchase like buying paintbrushes\nafter pigments). To utilize such long-short demands of users, we create\ndifferent clusters to group the successive product purchases together according\nto different time spans, and use recurrent neural networks to model each\nsequence of clusters at a time scale. The long-short purchase demands with\nmulti-time scales are finally aggregated by joint learning strategies.\nExperimental results on three real-world commerce datasets demonstrate the\neffectiveness of our model for next-item recommendation, showing the usefulness\nof modeling users' long-short purchase demands of items with multi-time scales.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 07:41:56 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Bai", "Ting", ""], ["Du", "Pan", ""], ["Zhao", "Wayne Xin", ""], ["Wen", "Ji-Rong", ""], ["Nie", "Jian-Yun", ""]]}, {"id": "1903.00099", "submitter": "Peng Jiang", "authors": "Peng Jiang and Yingrui Yang (co-first authors), Gann Bierner, Fengjie\n  Alex Li, Ruhan Wang, Azadeh Moghtaderi", "title": "Ranking in Genealogy: Search Results Fusion at Ancestry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Genealogy research is the study of family history using available resources\nsuch as historical records. Ancestry provides its customers with one of the\nworld's largest online genealogical index with billions of records from a wide\nrange of sources, including vital records such as birth and death certificates,\ncensus records, court and probate records among many others. Search at Ancestry\naims to return relevant records from various record types, allowing our\nsubscribers to build their family trees, research their family history, and\nmake meaningful discoveries about their ancestors from diverse perspectives. In\na modern search engine designed for genealogical study, the appropriate ranking\nof search results to provide highly relevant information represents a daunting\nchallenge. In particular, the disparity in historical records makes it\ninherently difficult to score records in an equitable fashion. Herein, we\nprovide an overview of our solutions to overcome such record disparity problems\nin the Ancestry search engine. Specifically, we introduce customized coordinate\nascent (customized CA) to speed up ranking within a specific record type. We\nthen propose stochastic search (SS) that linearly combines ranked results\nfederated across contents from various record types. Furthermore, we propose a\nnovel information retrieval metric, normalized cumulative entropy (NCE), to\nmeasure the diversity of results. We demonstrate the effectiveness of these two\nalgorithms in terms of relevance (by NDCG) and diversity (by NCE) if applicable\nin the offline experiments using real customer data at Ancestry.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 08:02:33 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Jiang", "Peng", "", "co-first authors"], ["Yang", "Yingrui", "", "co-first authors"], ["Bierner", "Gann", ""], ["Li", "Fengjie Alex", ""], ["Wang", "Ruhan", ""], ["Moghtaderi", "Azadeh", ""]]}, {"id": "1903.00103", "submitter": "Xiaorui Wu", "authors": "Xiaorui Wu, Hong Xu, Honglin Zhang, Huaming Chen, Jian Wang", "title": "Saec: Similarity-Aware Embedding Compression in Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Production recommendation systems rely on embedding methods to represent\nvarious features. An impeding challenge in practice is that the large embedding\nmatrix incurs substantial memory footprint in serving as the number of features\ngrows over time. We propose a similarity-aware embedding matrix compression\nmethod called Saec to address this challenge. Saec clusters similar features\nwithin a field to reduce the embedding matrix size. Saec also adopts a fast\nclustering optimization based on feature frequency to drastically improve\nclustering time. We implement and evaluate Saec on Numerous, the production\ndistributed machine learning system in Tencent, with 10-day worth of feature\ndata from QQ mobile browser. Testbed experiments show that Saec reduces the\nnumber of embedding vectors by two orders of magnitude, compresses the\nembedding size by ~27x, and delivers the same AUC and log loss performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 05:00:22 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Wu", "Xiaorui", ""], ["Xu", "Hong", ""], ["Zhang", "Honglin", ""], ["Chen", "Huaming", ""], ["Wang", "Jian", ""]]}, {"id": "1903.00142", "submitter": "Steven Spratley", "authors": "Steven Spratley, Daniel Beck, and Trevor Cohn", "title": "A Unified Neural Architecture for Instrumental Audio Tasks", "comments": "To appear in Proc. ICASSP 2019, May 12-17, Brighton, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within Music Information Retrieval (MIR), prominent tasks -- including\npitch-tracking, source-separation, super-resolution, and synthesis -- typically\ncall for specialised methods, despite their similarities. Conditional\nGenerative Adversarial Networks (cGANs) have been shown to be highly versatile\nin learning general image-to-image translations, but have not yet been adapted\nacross MIR. In this work, we present an end-to-end supervisable architecture to\nperform all aforementioned audio tasks, consisting of a WaveNet synthesiser\nconditioned on the output of a jointly-trained cGAN spectrogram translator. In\ndoing so, we demonstrate the potential of such flexible techniques to unify MIR\ntasks, promote efficient transfer learning, and converge research to the\nimprovement of powerful, general methods. Finally, to the best of our\nknowledge, we present the first application of GANs to guided instrument\nsynthesis.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 03:28:54 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Spratley", "Steven", ""], ["Beck", "Daniel", ""], ["Cohn", "Trevor", ""]]}, {"id": "1903.00252", "submitter": "Ji Liu", "authors": "Ji Liu and Lei Zhang", "title": "Optimal Projection Guided Transfer Hashing for Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, learning to hash has been widely studied for image retrieval thanks\nto the computation and storage efficiency of binary codes. For most existing\nlearning to hash methods, sufficient training images are required and used to\nlearn precise hashing codes. However, in some real-world applications, there\nare not always sufficient training images in the domain of interest. In\naddition, some existing supervised approaches need a amount of labeled data,\nwhich is an expensive process in term of time, label and human expertise. To\nhandle such problems, inspired by transfer learning, we propose a simple yet\neffective unsupervised hashing method named Optimal Projection Guided Transfer\nHashing (GTH) where we borrow the images of other different but related domain\ni.e., source domain to help learn precise hashing codes for the domain of\ninterest i.e., target domain. Besides, we propose to seek for the maximum\nlikelihood estimation (MLE) solution of the hashing functions of target and\nsource domains due to the domain gap. Furthermore,an alternating optimization\nmethod is adopted to obtain the two projections of target and source domains\nsuch that the domain hashing disparity is reduced gradually. Extensive\nexperiments on various benchmark databases verify that our method outperforms\nmany state-of-the-art learning to hash methods. The implementation details are\navailable at https://github.com/liuji93/GTH.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 11:43:31 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Liu", "Ji", ""], ["Zhang", "Lei", ""]]}, {"id": "1903.00562", "submitter": "Shubhra Kanti Karmaker Santu", "authors": "Shubhra Kanti Karmaker Santu, Liangda Li, Yi Chang, ChengXiang Zhai", "title": "JIM: Joint Influence Modeling for Collective Search Behavior", "comments": null, "journal-ref": null, "doi": "10.1145/3269206.3271681", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work has shown that popular trending events are important external\nfactors which pose significant influence on user search behavior and also\nprovided a way to computationally model this influence. However, their problem\nformulation was based on the strong assumption that each event poses its\ninfluence independently. This assumption is unrealistic as there are many\ncorrelated events in the real world which influence each other and thus, would\npose a joint influence on the user search behavior rather than posing influence\nindependently. In this paper, we study this novel problem of Modeling the Joint\nInfluences posed by multiple correlated events on user search behavior. We\npropose a Joint Influence Model based on the Multivariate Hawkes Process which\ncaptures the inter-dependency among multiple events in terms of their influence\nupon user search behavior. We evaluate the proposed Joint Influence Model using\ntwo months query-log data from https://search.yahoo.com/. Experimental results\nshow that the model can indeed capture the temporal dynamics of the joint\ninfluence over time and also achieves superior performance over different\nbaseline methods when applied to solve various interesting prediction problems\nas well as real-word application scenarios, e.g., query auto-completion.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 22:20:47 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Santu", "Shubhra Kanti Karmaker", ""], ["Li", "Liangda", ""], ["Chang", "Yi", ""], ["Zhai", "ChengXiang", ""]]}, {"id": "1903.00719", "submitter": "Lukas Pfannschmidt", "authors": "Lukas Pfannschmidt, Christina G\\\"opfert, Ursula Neumann, Dominik\n  Heider, Barbara Hammer", "title": "FRI -- Feature Relevance Intervals for Interpretable and Interactive\n  Data Exploration", "comments": "Addition of IEEE copyright notice. Accepted for CIBCB 2019\n  (https://cibcb2019.icas.xyz/)", "journal-ref": null, "doi": "10.1109/CIBCB.2019.8791489", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing feature selection methods are insufficient for analytic\npurposes as soon as high dimensional data or redundant sensor signals are dealt\nwith since features can be selected due to spurious effects or correlations\nrather than causal effects. To support the finding of causal features in\nbiomedical experiments, we hereby present FRI, an open source Python library\nthat can be used to identify all-relevant variables in linear classification\nand (ordinal) regression problems. Using the recently proposed feature\nrelevance method, FRI is able to provide the base for further general\nexperimentation or in specific can facilitate the search for alternative\nbiomarkers. It can be used in an interactive context, by providing model\nmanipulation and visualization methods, or in a batch process as a filter\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 15:16:15 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 17:21:03 GMT"}, {"version": "v3", "created": "Fri, 21 Jun 2019 14:41:04 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Pfannschmidt", "Lukas", ""], ["G\u00f6pfert", "Christina", ""], ["Neumann", "Ursula", ""], ["Heider", "Dominik", ""], ["Hammer", "Barbara", ""]]}, {"id": "1903.00780", "submitter": "Alex Beutel", "authors": "Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz\n  Heldt, Zhe Zhao, Lichan Hong, Ed H. Chi, Cristos Goodrow", "title": "Fairness in Recommendation Ranking through Pairwise Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are one of the most pervasive applications of machine\nlearning in industry, with many services using them to match users to products\nor information. As such it is important to ask: what are the possible fairness\nrisks, how can we quantify them, and how should we address them? In this paper\nwe offer a set of novel metrics for evaluating algorithmic fairness concerns in\nrecommender systems. In particular we show how measuring fairness based on\npairwise comparisons from randomized experiments provides a tractable means to\nreason about fairness in rankings from recommender systems. Building on this\nmetric, we offer a new regularizer to encourage improving this metric during\nmodel training and thus improve fairness in the resulting rankings. We apply\nthis pairwise regularization to a large-scale, production recommender system\nand show that we are able to significantly improve the system's pairwise\nfairness.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 22:29:42 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Beutel", "Alex", ""], ["Chen", "Jilin", ""], ["Doshi", "Tulsee", ""], ["Qian", "Hai", ""], ["Wei", "Li", ""], ["Wu", "Yi", ""], ["Heldt", "Lukasz", ""], ["Zhao", "Zhe", ""], ["Hong", "Lichan", ""], ["Chi", "Ed H.", ""], ["Goodrow", "Cristos", ""]]}, {"id": "1903.00905", "submitter": "Anirudha Vishvakarma", "authors": "Anirudha Vishvakarma", "title": "MILDNet: A Lightweight Single Scaled Deep Ranking Architecture", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-scale deep CNN architecture [1, 2, 3] successfully captures both fine\nand coarse level image descriptors for visual similarity task, but they come up\nwith expensive memory overhead and latency. In this paper, we propose a\ncompeting novel CNN architecture, called MILDNet, which merits by being vastly\ncompact (about 3 times). Inspired by the fact that successive CNN layers\nrepresent the image with increasing levels of abstraction, we compressed our\ndeep ranking model to a single CNN by coupling activations from multiple\nintermediate layers along with the last layer. Trained on the famous\nStreet2shop dataset [4], we demonstrate that our approach performs as good as\nthe current state-of-the-art models with only one third of the parameters,\nmodel size, training time and significant reduction in inference time. The\nsignificance of intermediate layers on image retrieval task has also been shown\nto be performing on popular datasets Holidays, Oxford, Paris [5]. So even\nthough our experiments are done on ecommerce domain, it is applicable to other\ndomains as well. We further did an ablation study to validate our hypothesis by\nchecking the impact on adding each intermediate layer. With this we also\npresent two more useful variants of MILDNet, a mobile model (12 times smaller)\nfor on-edge devices and a compactly featured model (512-d feature embeddings)\nfor systems with less RAMs and to reduce the ranking cost. Further we present\nan intuitive way to automatically create a tailored in-house triplet training\ndataset, which is very hard to create manually. This solution too can also be\ndeployed as an all-inclusive visual similarity solution. Finally, we present\nour entire production level architecture which currently powers visual\nsimilarity at Fynd.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 13:26:37 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 02:54:09 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Vishvakarma", "Anirudha", ""]]}, {"id": "1903.01275", "submitter": "Gerhard Wohlgenannt Dr.", "authors": "Gerhard Wohlgenannt and Nikolay Klimov and Dmitry Mouromtsev and\n  Daniil Razdyakonov and Dmitry Pavlov and Yury Emelyanov", "title": "Using Word Embeddings for Visual Data Exploration with Ontodia and\n  Wikidata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the big challenges in Linked Data consumption is to create visual and\nnatural language interfaces to the data usable for non-technical users. Ontodia\nprovides support for diagrammatic data exploration, showcased in this\npublication in combination with the Wikidata dataset. We present improvements\nto the natural language interface regarding exploring and querying Linked Data\nentities. The method uses models of distributional semantics to find and rank\nentity properties related to user input in Ontodia. Various word embedding\ntypes and model settings are evaluated, and the results show that user\nexperience in visual data exploration benefits from the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 14:36:21 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Wohlgenannt", "Gerhard", ""], ["Klimov", "Nikolay", ""], ["Mouromtsev", "Dmitry", ""], ["Razdyakonov", "Daniil", ""], ["Pavlov", "Dmitry", ""], ["Emelyanov", "Yury", ""]]}, {"id": "1903.01306", "submitter": "Ningyu Zhang", "authors": "Ningyu Zhang, Shumin Deng, Zhanlin Sun, Guanying Wang, Xi Chen, Wei\n  Zhang, Huajun Chen", "title": "Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph\n  Convolution Networks", "comments": "To be published in NAACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a distance supervised relation extraction approach for\nlong-tailed, imbalanced data which is prevalent in real-world settings. Here,\nthe challenge is to learn accurate \"few-shot\" models for classes existing at\nthe tail of the class distribution, for which little data is available.\nInspired by the rich semantic correlations between classes at the long tail and\nthose at the head, we take advantage of the knowledge from data-rich classes at\nthe head of the distribution to boost the performance of the data-poor classes\nat the tail. First, we propose to leverage implicit relational knowledge among\nclass labels from knowledge graph embeddings and learn explicit relational\nknowledge using graph convolution networks. Second, we integrate that\nrelational knowledge into relation extraction model by coarse-to-fine\nknowledge-aware attention mechanism. We demonstrate our results for a\nlarge-scale benchmark dataset which show that our approach significantly\noutperforms other baselines, especially for long-tail relations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 15:32:39 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Zhang", "Ningyu", ""], ["Deng", "Shumin", ""], ["Sun", "Zhanlin", ""], ["Wang", "Guanying", ""], ["Chen", "Xi", ""], ["Zhang", "Wei", ""], ["Chen", "Huajun", ""]]}, {"id": "1903.01545", "submitter": "Svebor Karaman", "authors": "Svebor Karaman, Xudong Lin, Xuefeng Hu, Shih-Fu Chang", "title": "Unsupervised Rank-Preserving Hashing for Large-Scale Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an unsupervised hashing method which aims to produce binary codes\nthat preserve the ranking induced by a real-valued representation. Such compact\nhash codes enable the complete elimination of real-valued feature storage and\nallow for significant reduction of the computation complexity and storage cost\nof large-scale image retrieval applications. Specifically, we learn a neural\nnetwork-based model, which transforms the input representation into a binary\nrepresentation. We formalize the training objective of the network in an\nintuitive and effective way, considering each training sample as a query and\naiming to obtain the same retrieval results using the produced hash codes as\nthose obtained with the original features. This training formulation directly\noptimizes the hashing model for the target usage of the hash codes it produces.\nWe further explore the addition of a decoder trained to obtain an approximated\nreconstruction of the original features. At test time, we retrieved the most\npromising database samples with an efficient graph-based search procedure using\nonly our hash codes and perform re-ranking using the reconstructed features,\nthus without needing to access the original features at all. Experiments\nconducted on multiple publicly available large-scale datasets show that our\nmethod consistently outperforms all compared state-of-the-art unsupervised\nhashing methods and that the reconstruction procedure can effectively boost the\nsearch accuracy with a minimal constant additional cost.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 21:24:26 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Karaman", "Svebor", ""], ["Lin", "Xudong", ""], ["Hu", "Xuefeng", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1903.02149", "submitter": "Xie De", "authors": "Chao Li, Cheng Deng, Lei Wang, De Xie, Xianglong Liu", "title": "Coupled CycleGAN: Unsupervised Hashing Network for Cross-Modal Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, hashing has attracted more and more attention owing to its\nsuperior capacity of low storage cost and high query efficiency in large-scale\ncross-modal retrieval. Benefiting from deep leaning, continuously compelling\nresults in cross-modal retrieval community have been achieved. However,\nexisting deep cross-modal hashing methods either rely on amounts of labeled\ninformation or have no ability to learn an accuracy correlation between\ndifferent modalities. In this paper, we proposed Unsupervised coupled Cycle\ngenerative adversarial Hashing networks (UCH), for cross-modal retrieval, where\nouter-cycle network is used to learn powerful common representation, and\ninner-cycle network is explained to generate reliable hash codes. Specifically,\nour proposed UCH seamlessly couples these two networks with generative\nadversarial mechanism, which can be optimized simultaneously to learn\nrepresentation and hash codes. Extensive experiments on three popular benchmark\ndatasets show that the proposed UCH outperforms the state-of-the-art\nunsupervised cross-modal hashing methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 03:09:20 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Li", "Chao", ""], ["Deng", "Cheng", ""], ["Wang", "Lei", ""], ["Xie", "De", ""], ["Liu", "Xianglong", ""]]}, {"id": "1903.02156", "submitter": "Piji Li", "authors": "Piji Li, Zihao Wang, Lidong Bing, Wai Lam", "title": "Persona-Aware Tips Generation", "comments": "Accepted to WWW'2019, 11 pages", "journal-ref": null, "doi": "10.1145/3308558.3313496", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tips, as a compacted and concise form of reviews, were paid less attention by\nresearchers. In this paper, we investigate the task of tips generation by\nconsidering the `persona' information which captures the intrinsic language\nstyle of the users or the different characteristics of the product items. In\norder to exploit the persona information, we propose a framework based on\nadversarial variational auto-encoders (aVAE) for persona modeling from the\nhistorical tips and reviews of users and items. The latent variables from aVAE\nare regarded as persona embeddings. Besides representing persona using the\nlatent embeddings, we design a persona memory for storing the persona related\nwords for users and items. Pointer Network is used to retrieve persona wordings\nfrom the memory when generating tips. Moreover, the persona embeddings are used\nas latent factors by a rating prediction component to predict the sentiment of\na user over an item. Finally, the persona embeddings and the sentiment\ninformation are incorporated into a recurrent neural networks based tips\ngeneration component. Extensive experimental results are reported and discussed\nto elaborate the peculiarities of our framework.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 03:36:29 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 12:42:59 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Li", "Piji", ""], ["Wang", "Zihao", ""], ["Bing", "Lidong", ""], ["Lam", "Wai", ""]]}, {"id": "1903.02861", "submitter": "Milad Moradi", "authors": "Milad Moradi", "title": "Small-world networks for summarization of biomedical articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, many methods have been developed to identify important\nportions of text documents. Summarization tools can utilize these methods to\nextract summaries from large volumes of textual information. However, to\nidentify concepts representing central ideas within a text document and to\nextract the most informative sentences that best convey those concepts still\nremain two crucial tasks in summarization methods. In this paper, we introduce\na graph-based method to address these two challenges in the context of\nbiomedical text summarization. We show that how a summarizer can discover\nmeaningful concepts within a biomedical text document using the Helmholtz\nprinciple. The summarizer considers the meaningful concepts as the main topics\nand constructs a graph based on the topics that the sentences share. The\nsummarizer can produce an informative summary by extracting those sentences\nhaving higher values of the degree. We assess the performance of our method for\nsummarization of biomedical articles using the Recall-Oriented Understudy for\nGisting Evaluation (ROUGE) toolkit. The results show that the degree can be a\nuseful centrality measure to identify important sentences in this type of\ngraph-based modelling. Our method can improve the performance of biomedical\ntext summarization compared to some state-of-the-art and publicly available\nsummarizers. Combining a concept-based modelling strategy and a graph-based\napproach to sentence extraction, our summarizer can produce summaries with the\nhighest scores of informativeness among the comparison methods. This research\nwork can be regarded as a start point to the study of small-world networks in\nsummarization of biomedical texts.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 12:12:17 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Moradi", "Milad", ""]]}, {"id": "1903.02939", "submitter": "Bram Van Den Akker", "authors": "Bram van den Akker, Ilya Markov and Maarten de Rijke", "title": "ViTOR: Learning to Rank Webpages Based on Visual Features", "comments": "In Proceedings of the 2019 World Wide Web Conference (WWW 2019), May\n  2019, San Francisco", "journal-ref": null, "doi": "10.1145/3308558.3313419", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual appearance of a webpage carries valuable information about its\nquality and can be used to improve the performance of learning to rank (LTR).\nWe introduce the Visual learning TO Rank (ViTOR) model that integrates\nstate-of-the-art visual features extraction methods by (i) transfer learning\nfrom a pre-trained image classification model, and (ii) synthetic saliency heat\nmaps generated from webpage snapshots. Since there is currently no public\ndataset for the task of LTR with visual features, we also introduce and release\nthe ViTOR dataset, containing visually rich and diverse webpages. The ViTOR\ndataset consists of visual snapshots, non-visual features and relevance\njudgments for ClueWeb12 webpages and TREC Web Track queries. We experiment with\nthe proposed ViTOR model on the ViTOR dataset and show that it significantly\nimproves the performance of LTR with visual features\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 14:32:41 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Akker", "Bram van den", ""], ["Markov", "Ilya", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1903.03082", "submitter": "Fabio Gonzalez", "authors": "Fabio A. Gonz\\'alez and Juan C. Caicedo", "title": "Quantum Latent Semantic Analysis", "comments": "ICTIR2011 International Conference on the Theory of Information\n  Retrieval", "journal-ref": null, "doi": "10.1007/978-3-642-23318-0_7", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this paper is to explore latent topic analysis (LTA), in the\ncontext of quantum information retrieval. LTA is a valuable technique for\ndocument analysis and representation, which has been extensively used in\ninformation retrieval and machine learning. Different LTA techniques have been\nproposed, some based on geometrical modeling (such as latent semantic analysis,\nLSA) and others based on a strong statistical foundation. However, these two\ndifferent approaches are not usually mixed. Quantum information retrieval has\nthe remarkable virtue of combining both geometry and probability in a common\nprincipled framework. We built on this quantum framework to propose a new LTA\nmethod, which has a clear geometrical motivation but also supports a\nwell-founded probabilistic interpretation. An initial exploratory\nexperimentation was performed on three standard data sets. The results show\nthat the proposed method outperforms LSA on two of the three datasets. These\nresults suggests that the quantum-motivated representation is an alternative\nfor geometrical latent topic modeling worthy of further exploration.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 18:19:55 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Gonz\u00e1lez", "Fabio A.", ""], ["Caicedo", "Juan C.", ""]]}, {"id": "1903.03714", "submitter": "Xiang Ren", "authors": "Weizhi Ma, Min Zhang, Yue Cao, Woojeong, Jin, Chenyang Wang, Yiqun\n  Liu, Shaoping Ma, Xiang Ren", "title": "Jointly Learning Explainable Rules for Recommendation with Knowledge\n  Graph", "comments": "10 pages, plus 1-page references; accepted at The Web Conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainability and effectiveness are two key aspects for building recommender\nsystems. Prior efforts mostly focus on incorporating side information to\nachieve better recommendation performance. However, these methods have some\nweaknesses: (1) prediction of neural network-based embedding methods are hard\nto explain and debug; (2) symbolic, graph-based approaches (e.g., meta\npath-based models) require manual efforts and domain knowledge to define\npatterns and rules, and ignore the item association types (e.g. substitutable\nand complementary). In this paper, we propose a novel joint learning framework\nto integrate \\textit{induction of explainable rules from knowledge graph} with\n\\textit{construction of a rule-guided neural recommendation model}. The\nframework encourages two modules to complement each other in generating\neffective and explainable recommendation: 1) inductive rules, mined from\nitem-centric knowledge graphs, summarize common multi-hop relational patterns\nfor inferring different item associations and provide human-readable\nexplanation for model prediction; 2) recommendation module can be augmented by\ninduced rules and thus have better generalization ability dealing with the\ncold-start issue. Extensive experiments\\footnote{Code and data can be found at:\n\\url{https://github.com/THUIR/RuleRec}} show that our proposed method has\nachieved significant improvements in item recommendation over baselines on\nreal-world datasets. Our model demonstrates robust performance over \"noisy\"\nitem knowledge graphs, generated by linking item names to related entities.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 01:06:04 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Ma", "Weizhi", ""], ["Zhang", "Min", ""], ["Cao", "Yue", ""], ["Woojeong", "", ""], ["Jin", "", ""], ["Wang", "Chenyang", ""], ["Liu", "Yiqun", ""], ["Ma", "Shaoping", ""], ["Ren", "Xiang", ""]]}, {"id": "1903.03762", "submitter": "Senzhang Wang", "authors": "Jianping Cao, Senzhang Wang (Corresponding author), Danyan Wen,\n  Zhaohui Peng, Philip S. Yu and Fei-yue Wang", "title": "Mutual Clustering on Comparative Texts via Heterogeneous Information\n  Networks", "comments": null, "journal-ref": "Knowledge and Information System, 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, many intelligence systems contain the texts from multi-sources,\ne.g., bulletin board system (BBS) posts, tweets and news. These texts can be\n``comparative'' since they may be semantically correlated and thus provide us\nwith different perspectives toward the same topics or events. To better\norganize the multi-sourced texts and obtain more comprehensive knowledge, we\npropose to study the novel problem of Mutual Clustering on Comparative Texts\n(MCCT), which aims to cluster the comparative texts simultaneously and\ncollaboratively. The MCCT problem is difficult to address because 1)\ncomparative texts usually present different data formats and structures and\nthus they are hard to organize, and 2) there lacks an effective method to\nconnect the semantically correlated comparative texts to facilitate clustering\nthem in an unified way. To this aim, in this paper we propose a Heterogeneous\nInformation Network-based Text clustering framework HINT. HINT first models\nmulti-sourced texts (e.g. news and tweets) as heterogeneous information\nnetworks by introducing the shared ``anchor texts'' to connect the comparative\ntexts. Next, two similarity matrices based on HINT as well as a transition\nmatrix for cross-text-source knowledge transfer are constructed. Comparative\ntexts clustering are then conducted by utilizing the constructed matrices.\nFinally, a mutual clustering algorithm is also proposed to further unify the\nseparate clustering results of the comparative texts by introducing a\nclustering consistency constraint. We conduct extensive experimental on three\ntweets-news datasets, and the results demonstrate the effectiveness and\nrobustness of the proposed method in addressing the MCCT problem.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 08:24:15 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Cao", "Jianping", "", "Corresponding author"], ["Wang", "Senzhang", "", "Corresponding author"], ["Wen", "Danyan", ""], ["Peng", "Zhaohui", ""], ["Yu", "Philip S.", ""], ["Wang", "Fei-yue", ""]]}, {"id": "1903.03775", "submitter": "Meriem Manai", "authors": "Meriem Manai", "title": "A New Approach for Topic Detection using Adaptive Neural Networks", "comments": "Master's thesis, in french", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic detection becomes more important due to the increase of information\nelectronically available and the necessity to process and filter it. In this\ncontext our master's thesis work was carried out, where we proposed to present\na new approach to the detection of topics called ClusART. Thus, we proposed a\nthree-phase approach, namely : a first phase during which lexical preprocessing\nwas conducted. A second phase during which the construction and generation of\nvectors representing the documents was carried out. A third phase which is\nitself composed of two steps. In the first step we used the FuzzyART algorithm\nfor the training phase. In the second step we used a classifier using Paragraph\nVector for the test phase. The comparative study of our approach on the 20\nNewsgroups dataset showed that our approach is able to detect almost relevant\ntopics.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 10:15:27 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Manai", "Meriem", ""]]}, {"id": "1903.03846", "submitter": "Dirk Lewandowski", "authors": "Dirk Lewandowski", "title": "The Web is missing an essential part of infrastructure: an Open Web\n  Index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A proposal for building an index of the Web that separates the infrastructure\npart of the search engine - the index - from the services part that will form\nthe basis for myriad search engines and other services utilizing Web data on\ntop of a public infrastructure open to everyone.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 18:33:55 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Lewandowski", "Dirk", ""]]}, {"id": "1903.03941", "submitter": "Suman Kalyan Maity", "authors": "Suman Kalyan Maity, Abhishek Panigrahi, Sayan Ghosh, Arundhati\n  Banerjee, Pawan Goyal and Animesh Mukherjee", "title": "DeepTagRec: A Content-cum-User based Tag Recommendation Framework for\n  Stack Overflow", "comments": "7 pages, 1 figure, 2 tables, In proceedings of ECIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we develop a content-cum-user based deep learning framework\nDeepTagRec to recommend appropriate question tags on Stack Overflow. The\nproposed system learns the content representation from question title and body.\nSubsequently, the learnt representation from heterogeneous relationship between\nuser and tags is fused with the content representation for the final tag\nprediction. On a very large-scale dataset comprising half a million question\nposts, DeepTagRec beats all the baselines; in particular, it significantly\noutperforms the best performing baseline T agCombine achieving an overall gain\nof 60.8% and 36.8% in precision@3 and recall@10 respectively. DeepTagRec also\nachieves 63% and 33.14% maximum improvement in exact-k accuracy and top-k\naccuracy respectively over TagCombine\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 07:05:17 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Maity", "Suman Kalyan", ""], ["Panigrahi", "Abhishek", ""], ["Ghosh", "Sayan", ""], ["Banerjee", "Arundhati", ""], ["Goyal", "Pawan", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "1903.04254", "submitter": "Abhinandan Krishnan", "authors": "Abhinandan Krishnan, Abilash Amarthaluri", "title": "Large Scale Product Categorization using Structured and Unstructured\n  Attributes", "comments": "Submitted to KDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product categorization using text data for eCommerce is a very challenging\nextreme classification problem with several thousands of classes and several\nmillions of products to classify. Even though multi-class text classification\nis a well studied problem both in academia and industry, most approaches either\ndeal with treating product content as a single pile of text, or only consider a\nfew product attributes for modelling purposes. Given the variety of products\nsold on popular eCommerce platforms, it is hard to consider all available\nproduct attributes as part of the modeling exercise, considering that products\npossess their own unique set of attributes based on category. In this paper, we\ncompare hierarchical models to flat models and show that in specific cases,\nflat models perform better. We explore two Deep Learning based models that\nextract features from individual pieces of unstructured data from each product\nand then combine them to create a product signature. We also propose a novel\nidea of using structured attributes and their values together in an\nunstructured fashion along with convolutional filters such that the ordering of\nthe attributes and the differing attributes by product categories no longer\nbecomes a modelling challenge. This approach is also more robust to the\npresence of faulty product attribute names and values and can elegantly\ngeneralize to use both closed list and open list attributes.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 23:41:10 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Krishnan", "Abhinandan", ""], ["Amarthaluri", "Abilash", ""]]}, {"id": "1903.04263", "submitter": "Shubhra-Kanti Karmaker-Santu", "authors": "Shubhra Kanti Karmaker Santu, Parikshit Sondhi, ChengXiang Zhai", "title": "On Application of Learning to Rank for E-Commerce Search", "comments": null, "journal-ref": null, "doi": "10.1145/3077136.3080838", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-Commerce (E-Com) search is an emerging important new application of\ninformation retrieval. Learning to Rank (LETOR) is a general effective strategy\nfor optimizing search engines, and is thus also a key technology for E-Com\nsearch. While the use of LETOR for web search has been well studied, its use\nfor E-Com search has not yet been well explored. In this paper, we discuss the\npractical challenges in applying learning to rank methods to E-Com search,\nincluding the challenges in feature representation, obtaining reliable\nrelevance judgments, and optimally exploiting multiple user feedback signals\nsuch as click rates, add-to-cart ratios, order rates, and revenue. We study\nthese new challenges using experiments on industry data sets and report several\ninteresting findings that can provide guidance on how to optimally apply LETOR\nto E-Com search: First, popularity-based features defined solely on product\nitems are very useful and LETOR methods were able to effectively optimize their\ncombination with relevance-based features. Second, query attribute sparsity\nraises challenges for LETOR, and selecting features to reduce/avoid sparsity is\nbeneficial. Third, while crowdsourcing is often useful for obtaining relevance\njudgments for Web search, it does not work as well for E-Com search due to\ndifficulty in eliciting sufficiently fine grained relevance judgments. Finally,\namong the multiple feedback signals, the order rate is found to be the most\nrobust training objective, followed by click rate, while add-to-cart ratio\nseems least robust, suggesting that an effective practical strategy may be to\ninitially use click rates for training and gradually shift to using order rates\nas they become available.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 22:10:14 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Santu", "Shubhra Kanti Karmaker", ""], ["Sondhi", "Parikshit", ""], ["Zhai", "ChengXiang", ""]]}, {"id": "1903.04276", "submitter": "Leonidas Akritidis Mr", "authors": "Leonidas Akritidis, Athanasios Fevgas, Panayiotis Bozanis, Christos\n  Makris", "title": "A Clustering-Based Combinatorial Approach to Unsupervised Matching of\n  Product Titles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constant growth of the e-commerce industry has rendered the problem of\nproduct retrieval particularly important. As more enterprises move their\nactivities on the Web, the volume and the diversity of the product-related\ninformation increase quickly. These factors make it difficult for the users to\nidentify and compare the features of their desired products. Recent studies\nproved that the standard similarity metrics cannot effectively identify\nidentical products, since similar titles often refer to different products and\nvice-versa. Other studies employed external data sources (search engines) to\nenrich the titles; these solutions are rather impractical mainly because the\nexternal data fetching is slow. In this paper we introduce UPM, an unsupervised\nalgorithm for matching products by their titles. UPM is independent of any\nexternal sources, since it analyzes the titles and extracts combinations of\nwords out of them. These combinations are evaluated according to several\ncriteria, and the most appropriate of them constitutes the cluster where a\nproduct is classified into. UPM is also parameter-free, it avoids product\npairwise comparisons, and includes a post-processing verification stage which\ncorrects the erroneous matches. The experimental evaluation of UPM demonstrated\nits superiority against the state-of-the-art approaches in terms of both\nefficiency and effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 02:22:48 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Akritidis", "Leonidas", ""], ["Fevgas", "Athanasios", ""], ["Bozanis", "Panayiotis", ""], ["Makris", "Christos", ""]]}, {"id": "1903.04360", "submitter": "Yiming Xu", "authors": "Yiming Xu, Dnyanesh Rajpathak, Ian Gibbs, Diego Klabjan", "title": "Automatic Ontology Learning from Domain-Specific Short Unstructured Text\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology learning is a critical task in industry, dealing with identifying\nand extracting concepts captured in text data such that these concepts can be\nused in different tasks, e.g. information retrieval. Ontology learning is\nnon-trivial due to several reasons with limited amount of prior research work\nthat automatically learns a domain specific ontology from data. In our work, we\npropose a two-stage classification system to automatically learn an ontology\nfrom unstructured text data. We first collect candidate concepts, which are\nclassified into concepts and irrelevant collocates by our first classifier. The\nconcepts from the first classifier are further classified by the second\nclassifier into different concept types. The proposed system is deployed as a\nprototype at a company and its performance is validated by using complaint and\nrepair verbatim data collected in automotive industry from different data\nsources.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 18:48:02 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Xu", "Yiming", ""], ["Rajpathak", "Dnyanesh", ""], ["Gibbs", "Ian", ""], ["Klabjan", "Diego", ""]]}, {"id": "1903.04489", "submitter": "Baogui Xin", "authors": "Wei Peng, Baogui Xin", "title": "SPMF: A Social Trust and Preference Segmentation-based Matrix\n  Factorization Recommendation Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional social recommendation algorithm ignores the following fact:\nthe preferences of users with trust relationships are not necessarily similar,\nand the consideration of user preference similarity should be limited to\nspecific areas. A social trust and preference segmentation-based matrix\nfactorization (SPMF) recommendation system is proposed to solve the\nabove-mentioned problems. Experimental results based on the Ciao and Epinions\ndatasets show that the accuracy of the SPMF algorithm is significantly higher\nthan that of some state-of-the-art recommendation algorithms. The proposed SPMF\nalgorithm is a more accurate and effective recommendation algorithm based on\ndistinguishing the difference of trust relations and preference domain, which\ncan support commercial activities such as product marketing.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 14:18:43 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Peng", "Wei", ""], ["Xin", "Baogui", ""]]}, {"id": "1903.04638", "submitter": "Sudeep Das", "authors": "Sudarshan Lamkhede and Sudeep Das", "title": "Challenges in Search on Streaming Services: Netflix Case Study", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss salient challenges of building a search experience for a streaming\nmedia service such as Netflix. We provide an overview of the role of\nrecommendations within the search context to aid content discovery and support\nsearches for unavailable (out-of-catalog) entities. We also stress the\nimportance of keystroke-level instant search experience, and the technical\nchallenges associated with implementing it across different devices and\nlanguages for a global audience.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 22:38:59 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Lamkhede", "Sudarshan", ""], ["Das", "Sudeep", ""]]}, {"id": "1903.04748", "submitter": "Pierrick Bruneau", "authors": "Etienne Brangbour, Pierrick Bruneau, St\\'ephane Marchand-Maillet,\n  Renaud Hostache, Patrick Matgen, Marco Chini, Thomas Tamisier", "title": "Extracting localized information from a Twitter corpus for flood\n  prevention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the collection of a corpus associated to tropical\nstorm Harvey, as well as its analysis from both spatial and topical\nperspectives. From the spatial perspective, our goal here is to get a first\nestimation of the quality and precision of the geographical information\nfeatured in the collected corpus. From a topical perspective, we discuss the\nrepresentation of Twitter posts, and strategies to process an initially\nunlabeled corpus of tweets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 07:01:15 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 07:46:39 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Brangbour", "Etienne", ""], ["Bruneau", "Pierrick", ""], ["Marchand-Maillet", "St\u00e9phane", ""], ["Hostache", "Renaud", ""], ["Matgen", "Patrick", ""], ["Chini", "Marco", ""], ["Tamisier", "Thomas", ""]]}, {"id": "1903.05538", "submitter": "Panayiotis Smeros", "authors": "Panayiotis Smeros, Carlos Castillo, Karl Aberer", "title": "SciLens: Evaluating the Quality of Scientific News Articles Using Social\n  Media and Scientific Literature Indicators", "comments": null, "journal-ref": "Proceedings of the 28th International Conference on World Wide Web\n  (WWW '19), San Francisco, CA, USA, May 13-17, 2019", "doi": "10.1145/3308558.3313657", "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes, develops, and validates SciLens, a method to evaluate\nthe quality of scientific news articles. The starting point for our work are\nstructured methodologies that define a series of quality aspects for manually\nevaluating news. Based on these aspects, we describe a series of indicators of\nnews quality. According to our experiments, these indicators help non-experts\nevaluate more accurately the quality of a scientific news article, compared to\nnon-experts that do not have access to these indicators. Furthermore, SciLens\ncan also be used to produce a completely automated quality score for an\narticle, which agrees more with expert evaluators than manual evaluations done\nby non-experts. One of the main elements of SciLens is the focus on both\ncontent and context of articles, where context is provided by (1) explicit and\nimplicit references on the article to scientific literature, and (2) reactions\nin social media referencing the article. We show that both contextual elements\ncan be valuable sources of information for determining article quality. The\nvalidation of SciLens, done through a combination of expert and non-expert\nannotation, demonstrates its effectiveness for both semi-automatic and\nautomatic quality evaluation of scientific news.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 15:13:57 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 16:23:07 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Smeros", "Panayiotis", ""], ["Castillo", "Carlos", ""], ["Aberer", "Karl", ""]]}, {"id": "1903.05896", "submitter": "Markus Schmid", "authors": "Markus L. Schmid", "title": "Regular Expressions with Backreferences: Polynomial-Time Matching\n  Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular expressions with backreferences (regex, for short), as supported by\nmost modern libraries for regular expression matching, have an NP-complete\nmatching problem. We define a complexity parameter of regex, called active\nvariable degree, such that regex with this parameter bounded by a constant can\nbe matched in polynomial-time. Moreover, we formulate a novel type of\ndeterminism for regex (on an automaton-theoretic level), which yields the class\nof memory-deterministic regex that can be matched in time O(|w|p(|r|)) for a\npolynomial p (where r is the regex and w the word). Natural extensions of these\nconcepts lead to properties of regex that are intractable to check.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 10:22:09 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Schmid", "Markus L.", ""]]}, {"id": "1903.06008", "submitter": "Nicola Montecchio", "authors": "Nicola Montecchio, Pierre Roy, Fran\\c{c}ois Pachet", "title": "The Skipping Behavior of Users of Music Streaming Services and its\n  Relation to Musical Structure", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0239418", "report-no": null, "categories": "cs.IR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The behavior of users of music streaming services is investigated from the\npoint of view of the temporal dimension of individual songs; specifically, the\nmain object of the analysis is the point in time within a song at which users\nstop listening and start streaming another song (\"skip\"). The main contribution\nof this study is the ascertainment of a correlation between the distribution in\ntime of skipping events and the musical structure of songs. It is also shown\nthat such distribution is not only specific to the individual songs, but also\nindependent of the cohort of users and, under stationary conditions, date of\nobservation. Finally, user behavioral data is used to train a predictor of the\nmusical structure of a song solely from its acoustic content; it is shown that\nthe use of such data, available in large quantities to music streaming\nservices, yields significant improvements in accuracy over the customary\nfashion of training this class of algorithms, in which only smaller amounts of\nhand-labeled data are available.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 08:35:49 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Montecchio", "Nicola", ""], ["Roy", "Pierre", ""], ["Pachet", "Fran\u00e7ois", ""]]}, {"id": "1903.06464", "submitter": "Chanwoo Jeong", "authors": "Chanwoo Jeong, Sion Jang, Hyuna Shin, Eunjeong Park, Sungchul Choi", "title": "A Context-Aware Citation Recommendation Model with BERT and Graph\n  Convolutional Networks", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the tremendous growth in the number of scientific papers being\npublished, searching for references while writing a scientific paper is a\ntime-consuming process. A technique that could add a reference citation at the\nappropriate place in a sentence will be beneficial. In this perspective,\ncontext-aware citation recommendation has been researched upon for around two\ndecades. Many researchers have utilized the text data called the context\nsentence, which surrounds the citation tag, and the metadata of the target\npaper to find the appropriate cited research. However, the lack of\nwell-organized benchmarking datasets and no model that can attain high\nperformance has made the research difficult.\n  In this paper, we propose a deep learning based model and well-organized\ndataset for context-aware paper citation recommendation. Our model comprises a\ndocument encoder and a context encoder, which uses Graph Convolutional Networks\n(GCN) layer and Bidirectional Encoder Representations from Transformers (BERT),\nwhich is a pre-trained model of textual data. By modifying the related PeerRead\ndataset, we propose a new dataset called FullTextPeerRead containing context\nsentences to cited references and paper metadata. To the best of our knowledge,\nThis dataset is the first well-organized dataset for context-aware paper\nrecommendation. The results indicate that the proposed model with the proposed\ndatasets can attain state-of-the-art performance and achieve a more than 28%\nimprovement in mean average precision (MAP) and recall@k.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 11:13:22 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Jeong", "Chanwoo", ""], ["Jang", "Sion", ""], ["Shin", "Hyuna", ""], ["Park", "Eunjeong", ""], ["Choi", "Sungchul", ""]]}, {"id": "1903.06902", "submitter": "Liang Pang", "authors": "Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed\n  Zamani, Chen Wu, W. Bruce Croft, Xueqi Cheng", "title": "A Deep Look into Neural Ranking Models for Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking models lie at the heart of research on information retrieval (IR).\nDuring the past decades, different techniques have been proposed for\nconstructing ranking models, from traditional heuristic methods, probabilistic\nmethods, to modern machine learning methods. Recently, with the advance of deep\nlearning technology, we have witnessed a growing body of work in applying\nshallow or deep neural networks to the ranking problem in IR, referred to as\nneural ranking models in this paper. The power of neural ranking models lies in\nthe ability to learn from the raw text inputs for the ranking problem to avoid\nmany limitations of hand-crafted features. Neural networks have sufficient\ncapacity to model complicated tasks, which is needed to handle the complexity\nof relevance estimation in ranking. Since there have been a large variety of\nneural ranking models proposed, we believe it is the right time to summarize\nthe current status, learn from existing methodologies, and gain some insights\nfor future development. In contrast to existing reviews, in this survey, we\nwill take a deep look into the neural ranking models from different dimensions\nto analyze their underlying assumptions, major design principles, and learning\nstrategies. We compare these models through benchmark tasks to obtain a\ncomprehensive empirical understanding of the existing techniques. We will also\ndiscuss what is missing in the current literature and what are the promising\nand desired future directions.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 10:20:09 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 01:20:57 GMT"}, {"version": "v3", "created": "Thu, 27 Jun 2019 15:18:40 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Guo", "Jiafeng", ""], ["Fan", "Yixing", ""], ["Pang", "Liang", ""], ["Yang", "Liu", ""], ["Ai", "Qingyao", ""], ["Zamani", "Hamed", ""], ["Wu", "Chen", ""], ["Croft", "W. Bruce", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1903.07061", "submitter": "Paolo Missier", "authors": "Flavio Primo, Paolo Missier, Alexander Romanovsky, Mickael Figueredo,\n  Nelio Cacho", "title": "A customisable pipeline for continuously harvesting socially-minded\n  Twitter users", "comments": "Procs. ICWE 2019, June 2019, Korea", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On social media platforms and Twitter in particular, specific classes of\nusers such as influencers have been given satisfactory operational definitions\nin terms of network and content metrics.\n  Others, for instance online activists, are not less important but their\ncharacterisation still requires experimenting.\n  We make the hypothesis that such interesting users can be found within\ntemporally and spatially localised contexts, i.e., small but topical fragments\nof the network containing interactions about social events or campaigns with a\nsignificant footprint on Twitter.\n  To explore this hypothesis, we have designed a continuous user profile\ndiscovery pipeline that produces an ever-growing dataset of user profiles by\nharvesting and analysing contexts from the Twitter stream.\n  The profiles dataset includes key network and content-based users metrics,\nenabling experimentation with user-defined score functions that characterise\nspecific classes of online users.\n  The paper describes the design and implementation of the pipeline and its\nempirical evaluation on a case study consisting of healthcare-related campaigns\nin the UK, showing how it supports the operational definitions of online\nactivism, by comparing three experimental ranking functions. The code is\npublicly available.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 11:54:51 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Primo", "Flavio", ""], ["Missier", "Paolo", ""], ["Romanovsky", "Alexander", ""], ["Figueredo", "Mickael", ""], ["Cacho", "Nelio", ""]]}, {"id": "1903.07158", "submitter": "Cheng-Yu Hung", "authors": "Cheng-Yu Hung, Mostafa Kaveh", "title": "Joint Block Low Rank and Sparse Matrix Recovery in Array\n  Self-Calibration Off-Grid DoA Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IR math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter addresses the estimation of directions-of-arrival (DoA) by a\nsensor array using a sparse model in the presence of array calibration errors\nand off-grid directions. The received signal utilizes previously used models\nfor unknown errors in calibration and structured linear representation of the\noff-grid effect. A convex optimization problem is formulated with an objective\nfunction to promote two-layer joint block-sparsity with its second-order cone\nprogramming (SOCP) representation. The performance of the proposed method is\ndemonstrated by numerical simulations and compared with the Cramer-Rao Bound\n(CRB), and several previously proposed methods.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 20:14:24 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 08:26:35 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Hung", "Cheng-Yu", ""], ["Kaveh", "Mostafa", ""]]}, {"id": "1903.07164", "submitter": "Cheng-Yu Hung", "authors": "Cheng-Yu Hung and Mostafa Kaveh", "title": "Linearly Constrained Smoothing Group Sparsity Solvers in Off-grid Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IR math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In compressed sensing, the sensing matrix is assumed perfectly known.\nHowever, there exists perturbation in the sensing matrix in reality due to\nsensor offsets or noise disturbance. Directions-of-arrival (DoA) estimation\nwith off-grid effect satisfies this situation, and can be formulated into a\n(non)convex optimization problem with linear inequalities constraints, which\ncan be solved by the interior point method (using the CVX tools), but at a\nlarge computational cost. In this work, in order to design efficient\nalgorithms, we consider various alternative formulations, such as unconstrained\nformulation, primal-dual formulation, or conic formulation to develop\ngroup-sparsity promoted solvers. First, the consensus alternating direction\nmethod of multipliers (C-ADMM) is applied. Then, iterative algorithms for the\nBPDN formulation is proposed by combining the Nesterov smoothing technique with\naccelerated proximal gradient method, and the convergence analysis of the\nmethod is conducted as well.\n  We also developed a variant of EGT (Excessive Gap Technique)-based\nprimal-dual method to systematically reduce the smoothing parameter\nsequentially. Finally, we propose algorithms for quadratically constrained\nL2-L1 mixed norm minimization problem by using the smoothed dual conic\noptimization (SDCO) and continuation technique. The performance of accuracy and\nconvergence for all the proposed methods are demonstrated in the numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 20:36:39 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 08:41:48 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Hung", "Cheng-Yu", ""], ["Kaveh", "Mostafa", ""]]}, {"id": "1903.07182", "submitter": "Artur Strzelecki", "authors": "Mariia Rizun, Artur Strzelecki", "title": "Knowledge Graph Development for App Store Data Modeling", "comments": "10 pages, 2 figures, presented at ISD2019\n  https://aisel.aisnet.org/isd2014/proceedings2019/CurrentTopics/7/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usage of mobile applications has become a part of our lives today, since\nevery day we use our smartphones for communication, entertainment, business and\neducation. High demand on apps has led to significant growth of supply, yet\nlarge offer has caused complications in users search of the one suitable\napplication. The authors have made an attempt to solve the problem of\nfacilitating the search in app stores. With the help of a website crawling\nsoftware a sample of data was retrieved from one of the well-known mobile app\nstores and divided into 11 groups by types. These groups of data were used to\nconstruct a Knowledge Schema - a graphic model of interconnections of data that\ncharacterize any mobile app in the selected store. Schema creation is the first\nstep in the process of developing a Knowledge Graph that will perform\napplications clustering to facilitate users search in app stores.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 21:47:01 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 13:55:53 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Rizun", "Mariia", ""], ["Strzelecki", "Artur", ""]]}, {"id": "1903.07279", "submitter": "Huan Chen", "authors": "Ji Zhao, Meiyu Yu, Huan Chen, Boning Li, Lingyu Zhang, Qi Song, Li Ma,\n  Hua Chai, Jieping Ye", "title": "POI Semantic Model with a Deep Convolutional Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using the electronic map, POI retrieval is the initial and important\nstep, whose quality directly affects the user experience. Similarity between\nuser query and POI information is the most critical feature in POI retrieval.\nAn accurate similarity calculation is challenging since the mismatch between a\nquery and a retrieval text may exist in the case of a mistyped query or an\nalias inquiry. In this paper, we propose a POI latent semantic model based on\ndeep networks, which can effectively extract query features and POI information\nfeatures for the similarity calculation. Our model describes the semantic\ninformation of complex texts at multiple layers, and achieves multi-field\nmatches by modeling POI's name and detailed address respectively. Our model is\nevaluated by the POI retrieval ranking datasets, including the labeled data of\nrelevance and real-world user click data in POI retrieval. Results show that\nour model significantly outperforms our competitors in POI retrieval ranking\ntasks. The proposed algorithm has become a critical component of an online\nsystem serving millions of people everyday.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 07:14:49 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Zhao", "Ji", ""], ["Yu", "Meiyu", ""], ["Chen", "Huan", ""], ["Li", "Boning", ""], ["Zhang", "Lingyu", ""], ["Song", "Qi", ""], ["Ma", "Li", ""], ["Chai", "Hua", ""], ["Ye", "Jieping", ""]]}, {"id": "1903.07288", "submitter": "Mahidhar Dwarampudi", "authors": "Mahidhar Dwarampudi, N V Subba Reddy", "title": "Effects of padding on LSTMs and CNNs", "comments": "5 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Long Short-Term Memory (LSTM) Networks and Convolutional Neural Networks\n(CNN) have become very common and are used in many fields as they were\neffective in solving many problems where the general neural networks were\ninefficient. They were applied to various problems mostly related to images and\nsequences. Since LSTMs and CNNs take inputs of the same length and dimension,\ninput images and sequences are padded to maximum length while testing and\ntraining. This padding can affect the way the networks function and can make a\ngreat deal when it comes to performance and accuracies. This paper studies this\nand suggests the best way to pad an input sequence. This paper uses a simple\nsentiment analysis task for this purpose. We use the same dataset on both the\nnetworks with various padding to show the difference. This paper also discusses\nsome preprocessing techniques done on the data to ensure effective analysis of\nthe data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 07:52:59 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Dwarampudi", "Mahidhar", ""], ["Reddy", "N V Subba", ""]]}, {"id": "1903.07507", "submitter": "Ishan Jindal", "authors": "Ishan Jindal, Daniel Pressel, Brian Lester, Matthew Nokleby", "title": "An Effective Label Noise Model for DNN Text Classification", "comments": "Accepted at NAACL-HLT 2019 Main Conference Long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because large, human-annotated datasets suffer from labeling errors, it is\ncrucial to be able to train deep neural networks in the presence of label\nnoise. While training image classification models with label noise have\nreceived much attention, training text classification models have not. In this\npaper, we propose an approach to training deep networks that is robust to label\nnoise. This approach introduces a non-linear processing layer (noise model)\nthat models the statistics of the label noise into a convolutional neural\nnetwork (CNN) architecture. The noise model and the CNN weights are learned\njointly from noisy training data, which prevents the model from overfitting to\nerroneous labels. Through extensive experiments on several text classification\ndatasets, we show that this approach enables the CNN to learn better sentence\nrepresentations and is robust even to extreme label noise. We find that proper\ninitialization and regularization of this noise model is critical. Further, by\ncontrast to results focusing on large batch sizes for mitigating label noise\nfor image classification, we find that altering the batch size does not have\nmuch effect on classification performance.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 15:27:50 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Jindal", "Ishan", ""], ["Pressel", "Daniel", ""], ["Lester", "Brian", ""], ["Nokleby", "Matthew", ""]]}, {"id": "1903.07581", "submitter": "Junting Ye", "authors": "Junting Ye, Steven Skiena", "title": "MediaRank: Computational Ranking of Online News Sources", "comments": "9 pages. Demo: www.media-rank.com. Accepted to 2019 ACM SIGKDD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent political climate, the topic of news quality has drawn\nattention both from the public and the academic communities. The growing\ndistrust of traditional news media makes it harder to find a common base of\naccepted truth. In this work, we design and build MediaRank\n(www.media-rank.com), a fully automated system to rank over 50,000 online news\nsources around the world. MediaRank collects and analyzes one million news\nwebpages and two million related tweets everyday. We base our algorithmic\nanalysis on four properties journalists have established to be associated with\nreporting quality: peer reputation, reporting bias / breadth, bottomline\nfinancial pressure, and popularity.\n  Our major contributions of this paper include: (i) Open, interpretable\nquality rankings for over 50,000 of the world's major news sources. Our\nrankings are validated against 35 published news rankings, including French,\nGerman, Russian, and Spanish language sources. MediaRank scores correlate\npositively with 34 of 35 of these expert rankings. (ii) New computational\nmethods for measuring influence and bottomline pressure. To the best of our\nknowledge, we are the first to study the large-scale news reporting citation\ngraph in-depth. We also propose new ways to measure the aggressiveness of\nadvertisements and identify social bots, establishing a connection between both\ntypes of bad behavior. (iii) Analyzing the effect of media source bias and\nsignificance. We prove that news sources cite others despite different\npolitical views in accord with quality measures. However, in four\nEnglish-speaking countries (US, UK, Canada, and Australia), the highest ranking\nsources all disproportionately favor left-wing parties, even when the majority\nof news sources exhibited conservative slants.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 17:23:49 GMT"}, {"version": "v2", "created": "Sun, 12 May 2019 21:40:43 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ye", "Junting", ""], ["Skiena", "Steven", ""]]}, {"id": "1903.07666", "submitter": "Bhaskar Mitra", "authors": "Bhaskar Mitra and Nick Craswell", "title": "An Updated Duet Model for Passage Re-ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose several small modifications to Duet---a deep neural ranking\nmodel---and evaluate the updated model on the MS MARCO passage ranking task. We\nreport significant improvements from the proposed changes based on an ablation\nstudy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 18:44:07 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Mitra", "Bhaskar", ""], ["Craswell", "Nick", ""]]}, {"id": "1903.07826", "submitter": "Yong Liu Stephen", "authors": "Yong Liu, Yinan Zhang, Qiong Wu, Chunyan Miao, Lizhen Cui, Binqiang\n  Zhao, Yin Zhao, Lu Guan", "title": "Diversity-Promoting Deep Reinforcement Learning for Interactive\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive recommendation that models the explicit interactions between\nusers and the recommender system has attracted a lot of research attentions in\nrecent years. Most previous interactive recommendation systems only focus on\noptimizing recommendation accuracy while overlooking other important aspects of\nrecommendation quality, such as the diversity of recommendation results. In\nthis paper, we propose a novel recommendation model, named\n\\underline{D}iversity-promoting \\underline{D}eep \\underline{R}einforcement\n\\underline{L}earning (D$^2$RL), which encourages the diversity of\nrecommendation results in interaction recommendations. More specifically, we\nadopt a Determinantal Point Process (DPP) model to generate diverse, while\nrelevant item recommendations. A personalized DPP kernel matrix is maintained\nfor each user, which is constructed from two parts: a fixed similarity matrix\ncapturing item-item similarity, and the relevance of items dynamically learnt\nthrough an actor-critic reinforcement learning framework. We performed\nextensive offline experiments as well as simulated online experiments with real\nworld datasets to demonstrate the effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 04:38:05 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Liu", "Yong", ""], ["Zhang", "Yinan", ""], ["Wu", "Qiong", ""], ["Miao", "Chunyan", ""], ["Cui", "Lizhen", ""], ["Zhao", "Binqiang", ""], ["Zhao", "Yin", ""], ["Guan", "Lu", ""]]}, {"id": "1903.07860", "submitter": "Guangneng Hu", "authors": "Guangneng Hu", "title": "Personalized Neural Embeddings for Collaborative Filtering with Text", "comments": "NAACL 2019 short papers, oral presentation", "journal-ref": "NAACL 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) is a core technique for recommender systems.\nTraditional CF approaches exploit user-item relations (e.g., clicks, likes, and\nviews) only and hence they suffer from the data sparsity issue. Items are\nusually associated with unstructured text such as article abstracts and product\nreviews. We develop a Personalized Neural Embedding (PNE) framework to exploit\nboth interactions and words seamlessly. We learn such embeddings of users,\nitems, and words jointly, and predict user preferences on items based on these\nlearned representations. PNE estimates the probability that a user will like an\nitem by two terms---behavior factors and semantic factors. On two real-world\ndatasets, PNE shows better performance than four state-of-the-art baselines in\nterms of three metrics. We also show that PNE learns meaningful word embeddings\nby visualization.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 07:05:59 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Hu", "Guangneng", ""]]}, {"id": "1903.08206", "submitter": "Rafael S. Gon\\c{c}alves", "authors": "Rafael S. Gon\\c{c}alves, Maulik R. Kamdar, and Mark A. Musen", "title": "Aligning Biomedical Metadata with Ontologies Using Clustering and\n  Embeddings", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-21348-0_10", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The metadata about scientific experiments published in online repositories\nhave been shown to suffer from a high degree of representational\nheterogeneity---there are often many ways to represent the same type of\ninformation, such as a geographical location via its latitude and longitude. To\nharness the potential that metadata have for discovering scientific data, it is\ncrucial that they be represented in a uniform way that can be queried\neffectively. One step toward uniformly-represented metadata is to normalize the\nmultiple, distinct field names used in metadata (e.g., lat lon, lat and long)\nto describe the same type of value. To that end, we present a new method based\non clustering and embeddings (i.e., vector representations of words) to align\nmetadata field names with ontology terms. We apply our method to biomedical\nmetadata by generating embeddings for terms in biomedical ontologies from the\nBioPortal repository. We carried out a comparative study between our method and\nthe NCBO Annotator, which revealed that our method yields more and\nsubstantially better alignments between metadata and ontology terms.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 18:31:23 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 20:23:54 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Gon\u00e7alves", "Rafael S.", ""], ["Kamdar", "Maulik R.", ""], ["Musen", "Mark A.", ""]]}, {"id": "1903.08305", "submitter": "Juan C. Rold\\'an", "authors": "Juan C. Rold\\'an, Patricia Jim\\'enez, Rafael Corchuelo", "title": "On Extracting Data from Tables that are Encoded using HTML", "comments": null, "journal-ref": null, "doi": "10.1016/j.knosys.2019.105157", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tables are a common means to display data in human-friendly formats. Many\nauthors have worked on proposals to extract those data back since this has many\ninteresting applications. In this article, we summarise and compare many of the\nproposals to extract data from tables that are encoded using HTML and have been\npublished between $2000$ and $2018$. We first present a vocabulary that\nhomogenises the terminology used in this field; next, we use it to summarise\nthe proposals; finally, we compare them side by side. Our analysis highlights\nseveral challenges to which no proposal provides a conclusive solution and a\nfew more that have not been addressed sufficiently; simply put, no proposal\nprovides a complete solution to the problem, which seems to suggest that this\nresearch field shall keep active in the near future. We have also realised that\nthere is no consensus regarding the datasets and the methods used to evaluate\nthe proposals, which hampers comparing the experimental results.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 01:17:25 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 19:42:45 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Rold\u00e1n", "Juan C.", ""], ["Jim\u00e9nez", "Patricia", ""], ["Corchuelo", "Rafael", ""]]}, {"id": "1903.08404", "submitter": "Casper Hansen", "authors": "Casper Hansen, Christian Hansen, Stephen Alstrup, Jakob Grue Simonsen,\n  Christina Lioma", "title": "Neural Check-Worthiness Ranking with Weak Supervision: Finding Sentences\n  for Fact-Checking", "comments": "6 pages", "journal-ref": "In Companion Proceedings of the 2019 World Wide Web Conference", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic fact-checking systems detect misinformation, such as fake news, by\n(i) selecting check-worthy sentences for fact-checking, (ii) gathering related\ninformation to the sentences, and (iii) inferring the factuality of the\nsentences. Most prior research on (i) uses hand-crafted features to select\ncheck-worthy sentences, and does not explicitly account for the recent finding\nthat the top weighted terms in both check-worthy and non-check-worthy sentences\nare actually overlapping [15]. Motivated by this, we present a neural\ncheck-worthiness sentence ranking model that represents each word in a sentence\nby \\textit{both} its embedding (aiming to capture its semantics) and its\nsyntactic dependencies (aiming to capture its role in modifying the semantics\nof other terms in the sentence). Our model is an end-to-end trainable neural\nnetwork for check-worthiness ranking, which is trained on large amounts of\nunlabelled data through weak supervision. Thorough experimental evaluation\nagainst state of the art baselines, with and without weak supervision, shows\nour model to be superior at all times (+13% in MAP and +28% at various\nPrecision cut-offs from the best baseline with statistical significance).\nEmpirical analysis of the use of weak supervision, word embedding pretraining\non domain-specific data, and the use of syntactic dependencies of our model\nreveals that check-worthy sentences contain notably more identical syntactic\ndependencies than non-check-worthy sentences.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 09:40:19 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Hansen", "Casper", ""], ["Hansen", "Christian", ""], ["Alstrup", "Stephen", ""], ["Simonsen", "Jakob Grue", ""], ["Lioma", "Christina", ""]]}, {"id": "1903.08408", "submitter": "Casper Hansen", "authors": "Christian Hansen, Casper Hansen, Stephen Alstrup, Jakob Grue Simonsen,\n  Christina Lioma", "title": "Modelling Sequential Music Track Skips using a Multi-RNN Approach", "comments": "4 pages", "journal-ref": "12th ACM International Conference on Web Search and Data Mining\n  (WSDM) 2019, WSDM Cup", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling sequential music skips provides streaming companies the ability to\nbetter understand the needs of the user base, resulting in a better user\nexperience by reducing the need to manually skip certain music tracks. This\npaper describes the solution of the University of Copenhagen DIKU-IR team in\nthe 'Spotify Sequential Skip Prediction Challenge', where the task was to\npredict the skip behaviour of the second half in a music listening session\nconditioned on the first half. We model this task using a Multi-RNN approach\nconsisting of two distinct stacked recurrent neural networks, where one network\nfocuses on encoding the first half of the session and the other network focuses\non utilizing the encoding to make sequential skip predictions. The encoder\nnetwork is initialized by a learned session-wide music encoding, and both of\nthem utilize a learned track embedding. Our final model consists of a majority\nvoted ensemble of individually trained models, and ranked 2nd out of 45\nparticipating teams in the competition with a mean average accuracy of 0.641\nand an accuracy on the first skip prediction of 0.807. Our code is released at\nhttps://github.com/Varyn/WSDM-challenge-2019-spotify.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 09:47:22 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Hansen", "Christian", ""], ["Hansen", "Casper", ""], ["Alstrup", "Stephen", ""], ["Simonsen", "Jakob Grue", ""], ["Lioma", "Christina", ""]]}, {"id": "1903.08597", "submitter": "Volodymyr Miz", "authors": "Nicolas Aspert, Volodymyr Miz, Benjamin Ricaud, Pierre Vandergheynst", "title": "A Graph-structured Dataset for Wikipedia Research", "comments": null, "journal-ref": null, "doi": "10.1145/3308560.3316757", "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Wikipedia is a rich and invaluable source of information. Its central place\non the Web makes it a particularly interesting object of study for scientists.\nResearchers from different domains used various complex datasets related to\nWikipedia to study language, social behavior, knowledge organization, and\nnetwork theory. While being a scientific treasure, the large size of the\ndataset hinders pre-processing and may be a challenging obstacle for potential\nnew studies. This issue is particularly acute in scientific domains where\nresearchers may not be technically and data processing savvy. On one hand, the\nsize of Wikipedia dumps is large. It makes the parsing and extraction of\nrelevant information cumbersome. On the other hand, the API is straightforward\nto use but restricted to a relatively small number of requests. The middle\nground is at the mesoscopic scale when researchers need a subset of Wikipedia\nranging from thousands to hundreds of thousands of pages but there exists no\nefficient solution at this scale.\n  In this work, we propose an efficient data structure to make requests and\naccess subnetworks of Wikipedia pages and categories. We provide convenient\ntools for accessing and filtering viewership statistics or \"pagecounts\" of\nWikipedia web pages. The dataset organization leverages principles of graph\ndatabases that allows rapid and intuitive access to subgraphs of Wikipedia\narticles and categories. The dataset and deployment guidelines are available on\nthe LTS2 website \\url{https://lts2.epfl.ch/Datasets/Wikipedia/}.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 16:31:29 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Aspert", "Nicolas", ""], ["Miz", "Volodymyr", ""], ["Ricaud", "Benjamin", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1903.08746", "submitter": "Chen Sun", "authors": "Chen Sun, Jean M. Uwabeza Vianney, Dongpu Cao", "title": "Affordance Learning In Direct Perception for Autonomous Driving", "comments": "9 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent development in autonomous driving involves high-level computer vision\nand detailed road scene understanding. Today, most autonomous vehicles are\nusing mediated perception approach for path planning and control, which highly\nrely on high-definition 3D maps and real time sensors. Recent research efforts\naim to substitute the massive HD maps with coarse road attributes. In this\npaper, we follow the direct perception based method to train a deep neural\nnetwork for affordance learning in autonomous driving. Our goal in this work is\nto develop the affordance learning model based on freely available Google\nStreet View panoramas and Open Street Map road vector attributes. Driving scene\nunderstanding can be achieved by learning affordances from the images captured\nby car-mounted cameras. Such scene understanding by learning affordances may be\nuseful for corroborating base maps such as HD maps so that the required data\nstorage space is minimized and available for processing in real time. We\ncompare capability in road attribute identification between human volunteers\nand our model by experimental evaluation. Our results indicate that this method\ncould act as a cheaper way for training data collection in autonomous driving.\nThe cross validation results also indicate the effectiveness of our model.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 21:15:08 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Sun", "Chen", ""], ["Vianney", "Jean M. Uwabeza", ""], ["Cao", "Dongpu", ""]]}, {"id": "1903.08756", "submitter": "Aitor Arronte Alvarez", "authors": "Aitor Arronte-Alvarez, Francisco G\\'omez-Martin", "title": "Distributed Vector Representations of Folksong Motifs", "comments": "MCM 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a distributed vector representation model for learning\nfolksong motifs. A skip-gram version of word2vec with negative sampling is used\nto represent high quality embeddings. Motifs from the Essen Folksong collection\nare compared based on their cosine similarity. A new evaluation method for\ntesting the quality of the embeddings based on a melodic similarity task is\npresented to show how the vector space can represent complex contextual\nfeatures, and how it can be utilized for the study of folksong variation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 21:52:13 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Arronte-Alvarez", "Aitor", ""], ["G\u00f3mez-Martin", "Francisco", ""]]}, {"id": "1903.08816", "submitter": "Haozhen Zhao", "authors": "Christian J. Mahoney, Nathaniel Huber-Fliflet, Katie Jensen, Haozhen\n  Zhao, Robert Neary, Shi Ye", "title": "Empirical Evaluations of Seed Set Selection Strategies for Predictive\n  Coding", "comments": "2018 IEEE International Conference on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training documents have a significant impact on the performance of predictive\nmodels in the legal domain. Yet, there is limited research that explores the\neffectiveness of the training document selection strategy - in particular, the\nstrategy used to select the seed set, or the set of documents an attorney\nreviews first to establish an initial model. Since there is limited research on\nthis important component of predictive coding, the authors of this paper set\nout to identify strategies that consistently perform well. Our research\ndemonstrated that the seed set selection strategy can have a significant impact\non the precision of a predictive model. Enabling attorneys with the results of\nthis study will allow them to initiate the most effective predictive modeling\nprocess to comb through the terabytes of data typically present in modern\nlitigation. This study used documents from four actual legal cases to evaluate\neight different seed set selection strategies. Attorneys can use the results\ncontained within this paper to enhance their approach to predictive coding.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 03:04:30 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Mahoney", "Christian J.", ""], ["Huber-Fliflet", "Nathaniel", ""], ["Jensen", "Katie", ""], ["Zhao", "Haozhen", ""], ["Neary", "Robert", ""], ["Ye", "Shi", ""]]}, {"id": "1903.09238", "submitter": "Ahmed Metwally", "authors": "Ahmed Metwally and Chun-Heng Huang", "title": "Scalable Similarity Joins of Tokenized Strings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work tackles the problem of fuzzy joining of strings that naturally\ntokenize into meaningful substrings, e.g., full names. Tokenized-string joins\nhave several established applications in the context of data integration and\ncleaning. This work is primarily motivated by fraud detection, where attackers\nslightly modify tokenized strings, e.g., names on accounts, to create numerous\nidentities that she can use to defraud service providers, e.g., Google, and\nLinkedIn. To detect such attacks, all the accounts are pair-wise compared, and\nthe resulting similar accounts are considered suspicious and are further\ninvestigated. Comparing the tokenized-string features of a large number of\naccounts requires an intuitive tokenized-string distance that can detect subtle\nedits introduced by an adversary, and a very scalable algorithm. This is not\nachievable by existing distance measure that are unintuitive, hard to tune, and\nwhose join algorithms are serial and hence unscalable. We define a novel\nintuitive distance measure between tokenized strings, Normalized Setwise\nLevenshtein Distance (NSLD). To the best of our knowledge, NSLD is the first\nmetric proposed for comparing tokenized strings. We propose a scalable\ndistributed framework, Tokenized-String Joiner (TSJ), that adopts existing\nscalable string-join algorithms as building blocks to perform NSLD-joins. We\ncarefully engineer optimizations and approximations that dramatically improve\nthe efficiency of TSJ. The effectiveness of the TSJ framework is evident from\nthe evaluation conducted on tens of millions of tokenized-string names from\nGoogle accounts. The superiority of the tokenized-string-specific TSJ framework\nover the general-purpose metric-spaces joining algorithms has been established.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 21:16:28 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Metwally", "Ahmed", ""], ["Huang", "Chun-Heng", ""]]}, {"id": "1903.09374", "submitter": "Dongyang Zhao", "authors": "Dongyang Zhao, Liang Zhang, Bo Zhang, Lizhou Zheng, Yongjun Bao,\n  Weipeng Yan", "title": "Deep Hierarchical Reinforcement Learning Based Recommendations via\n  Multi-goals Abstraction", "comments": "submitted to SIGKDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recommender system is an important form of intelligent application, which\nassists users to alleviate from information redundancy. Among the metrics used\nto evaluate a recommender system, the metric of conversion has become more and\nmore important. The majority of existing recommender systems perform poorly on\nthe metric of conversion due to its extremely sparse feedback signal. To tackle\nthis challenge, we propose a deep hierarchical reinforcement learning based\nrecommendation framework, which consists of two components, i.e., high-level\nagent and low-level agent. The high-level agent catches long-term sparse\nconversion signals, and automatically sets abstract goals for low-level agent,\nwhile the low-level agent follows the abstract goals and interacts with\nreal-time environment. To solve the inherent problem in hierarchical\nreinforcement learning, we propose a novel deep hierarchical reinforcement\nlearning algorithm via multi-goals abstraction (HRL-MG). Our proposed algorithm\ncontains three characteristics: 1) the high-level agent generates multiple\ngoals to guide the low-level agent in different stages, which reduces the\ndifficulty of approaching high-level goals; 2) different goals share the same\nstate encoder parameters, which increases the update frequency of the\nhigh-level agent and thus accelerates the convergence of our proposed\nalgorithm; 3) an appreciate benefit assignment function is designed to allocate\nrewards in each goal so as to coordinate different goals in a consistent\ndirection. We evaluate our proposed algorithm based on a real-world e-commerce\ndataset and validate its effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 06:43:49 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Zhao", "Dongyang", ""], ["Zhang", "Liang", ""], ["Zhang", "Bo", ""], ["Zheng", "Lizhou", ""], ["Bao", "Yongjun", ""], ["Yan", "Weipeng", ""]]}, {"id": "1903.09942", "submitter": "Andrei Damian I", "authors": "Laurentiu Piciu, Andrei Damian, Nicolae Tapus, Andrei\n  Simion-Constantinescu, Bogdan Dumitrescu", "title": "Deep recommender engine based on efficient product embeddings neural\n  pipeline", "comments": "2018 17th RoEduNet Conference: Networking in Education and Research\n  (RoEduNet)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predictive analytics systems are currently one of the most important areas of\nresearch and development within the Artificial Intelligence domain and\nparticularly in Machine Learning. One of the \"holy grails\" of predictive\nanalytics is the research and development of the \"perfect\" recommendation\nsystem. In our paper, we propose an advanced pipeline model for the multi-task\nobjective of determining product complementarity, similarity and sales\nprediction using deep neural models applied to big-data sequential transaction\nsystems. Our highly parallelized hybrid model pipeline consists of both\nunsupervised and supervised models, used for the objectives of generating\nsemantic product embeddings and predicting sales, respectively. Our\nexperimentation and benchmarking processes have been done using pharma industry\nretail real-life transactional Big-Data streams.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 08:11:58 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 13:39:33 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Piciu", "Laurentiu", ""], ["Damian", "Andrei", ""], ["Tapus", "Nicolae", ""], ["Simion-Constantinescu", "Andrei", ""], ["Dumitrescu", "Bogdan", ""]]}, {"id": "1903.10117", "submitter": "Tanmoy Chakraborty", "authors": "Mansi Goel, Ayush Agarwal, Deepak Thukral, Tanmoy Chakraborty", "title": "Fiducia: A Personalized Food Recommender System for Zomato", "comments": "2 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Fiducia, a food review system involving a pipeline which\nprocesses restaurant-related reviews obtained from Zomato (India's largest\nrestaurant search and discovery service). Fiducia is specific to popular cafe\nfood items and manages to identify relevant information pertaining to each item\nseparately in the reviews. It uses a sentiment check on these pieces of text\nand accordingly suggests an appropriate restaurant for the particular item\ndepending on user-item and item-item similarity. Experimental results show that\nthe sentiment analyzer module of Fiducia achieves an accuracy of over 85% and\nour final recommender system achieves an RMSE of about 1.01 beating other\nbaselines.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 03:33:06 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Goel", "Mansi", ""], ["Agarwal", "Ayush", ""], ["Thukral", "Deepak", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "1903.10433", "submitter": "Qitan Wu", "authors": "Qitian Wu, Hengrui Zhang, Xiaofeng Gao, Peng He, Paul Weng, Han Gao,\n  Guihai Chen", "title": "Dual Graph Attention Networks for Deep Latent Representation of\n  Multifaceted Social Effects in Recommender Systems", "comments": "Accepted by WWW2019 as a full paper with oral presentation", "journal-ref": null, "doi": "10.1145/3308558.3313442", "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social recommendation leverages social information to solve data sparsity and\ncold-start problems in traditional collaborative filtering methods. However,\nmost existing models assume that social effects from friend users are static\nand under the forms of constant weights or fixed constraints. To relax this\nstrong assumption, in this paper, we propose dual graph attention networks to\ncollaboratively learn representations for two-fold social effects, where one is\nmodeled by a user-specific attention weight and the other is modeled by a\ndynamic and context-aware attention weight. We also extend the social effects\nin user domain to item domain, so that information from related items can be\nleveraged to further alleviate the data sparsity problem. Furthermore,\nconsidering that different social effects in two domains could interact with\neach other and jointly influence user preferences for items, we propose a new\npolicy-based fusion strategy based on contextual multi-armed bandit to weigh\ninteractions of various social effects. Experiments on one benchmark dataset\nand a commercial dataset verify the efficacy of the key components in our\nmodel. The results show that our model achieves great improvement for\nrecommendation accuracy compared with other state-of-the-art social\nrecommendation methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 16:14:01 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Wu", "Qitian", ""], ["Zhang", "Hengrui", ""], ["Gao", "Xiaofeng", ""], ["He", "Peng", ""], ["Weng", "Paul", ""], ["Gao", "Han", ""], ["Chen", "Guihai", ""]]}, {"id": "1903.10512", "submitter": "Shuo Zhang", "authors": "Jason Shuo Zhang, Mike Gartrell, Richard Han, Qin Lv, and Shivakant\n  Mishra", "title": "GEVR: An Event Venue Recommendation System for Groups of Mobile Users", "comments": "in Proceedings of the ACM on Interactive, Mobile, Wearable and\n  Ubiquitous Technologies (IMWUT), Volume 3 Issue 1, March 2019", "journal-ref": null, "doi": "10.1145/3314421", "report-no": null, "categories": "cs.HC cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present GEVR, the first Group Event Venue Recommendation\nsystem that incorporates mobility via individual location traces and context\ninformation into a \"social-based\" group decision model to provide venue\nrecommendations for groups of mobile users. Our study leverages a real-world\ndataset collected using the OutWithFriendz mobile app for group event planning,\nwhich contains 625 users and over 500 group events. We first develop a novel\n\"social-based\" group location prediction model, which adaptively applies\ndifferent group decision strategies to groups with different social\nrelationship strength to aggregate each group member's location preference, to\npredict where groups will meet. Evaluation results show that our prediction\nmodel not only outperforms commonly used and state-of-the-art group decision\nstrategies with over 80% accuracy for predicting groups' final meeting location\nclusters, but also provides promising qualities in cold-start scenarios. We\nthen integrate our prediction model with the Foursquare Venue Recommendation\nAPI to construct an event venue recommendation framework for groups of mobile\nusers. Evaluation results show that GEVR outperforms the comparative models by\na significant margin.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 18:00:02 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Zhang", "Jason Shuo", ""], ["Gartrell", "Mike", ""], ["Han", "Richard", ""], ["Lv", "Qin", ""], ["Mishra", "Shivakant", ""]]}, {"id": "1903.10663", "submitter": "ByungSoo Ko", "authors": "HeeJae Jun, Byungsoo Ko, Youngjoon Kim, Insik Kim, Jongtack Kim", "title": "Combination of Multiple Global Descriptors for Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies in image retrieval task have shown that ensembling different\nmodels and combining multiple global descriptors lead to performance\nimprovement. However, training different models for the ensemble is not only\ndifficult but also inefficient with respect to time and memory. In this paper,\nwe propose a novel framework that exploits multiple global descriptors to get\nan ensemble effect while it can be trained in an end-to-end manner. The\nproposed framework is flexible and expandable by the global descriptor, CNN\nbackbone, loss, and dataset. Moreover, we investigate the effectiveness of\ncombining multiple global descriptors with quantitative and qualitative\nanalysis. Our extensive experiments show that the combined descriptor\noutperforms a single global descriptor, as it can utilize different types of\nfeature properties. In the benchmark evaluation, the proposed framework\nachieves the state-of-the-art performance on the CARS196, CUB200-2011, In-shop\nClothes, and Stanford Online Products on image retrieval tasks. Our model\nimplementations and pretrained models are publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 03:38:38 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 05:33:04 GMT"}, {"version": "v3", "created": "Sat, 27 Jul 2019 05:28:41 GMT"}, {"version": "v4", "created": "Thu, 23 Apr 2020 06:20:02 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Jun", "HeeJae", ""], ["Ko", "Byungsoo", ""], ["Kim", "Youngjoon", ""], ["Kim", "Insik", ""], ["Kim", "Jongtack", ""]]}, {"id": "1903.10794", "submitter": "Cheng Wang", "authors": "Cheng Wang, Mathias Niepert, Hui Li", "title": "RecSys-DAN: Discriminative Adversarial Networks for Cross-Domain\n  Recommender Systems", "comments": "10 pages, IEEE-TNNLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sparsity and data imbalance are practical and challenging issues in\ncross-domain recommender systems. This paper addresses those problems by\nleveraging the concepts which derive from representation learning, adversarial\nlearning and transfer learning (particularly, domain adaptation). Although\nvarious transfer learning methods have shown promising performance in this\ncontext, our proposed novel method RecSys-DAN focuses on alleviating the\ncross-domain and within-domain data sparsity and data imbalance and learns\ntransferable latent representations for users, items and their interactions.\nDifferent from existing approaches, the proposed method transfers the latent\nrepresentations from a source domain to a target domain in an adversarial way.\nThe mapping functions in the target domain are learned by playing a min-max\ngame with an adversarial loss, aiming to generate domain indistinguishable\nrepresentations for a discriminator. Four neural architectural instances of\nResSys-DAN are proposed and explored. Empirical results on real-world Amazon\ndata show that, even without using labeled data (i.e., ratings) in the target\ndomain, RecSys-DAN achieves competitive performance as compared to the\nstate-of-the-art supervised methods. More importantly, RecSys-DAN is highly\nflexible to both unimodal and multimodal scenarios, and thus it is more robust\nto the cold-start recommendation which is difficult for previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 11:07:00 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 13:14:21 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Wang", "Cheng", ""], ["Niepert", "Mathias", ""], ["Li", "Hui", ""]]}, {"id": "1903.10972", "submitter": "Jimmy Lin", "authors": "Wei Yang, Haotian Zhang, and Jimmy Lin", "title": "Simple Applications of BERT for Ad Hoc Document Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following recent successes in applying BERT to question answering, we explore\nsimple applications to ad hoc document retrieval. This required confronting the\nchallenge posed by documents that are typically longer than the length of input\nBERT was designed to handle. We address this issue by applying inference on\nsentences individually, and then aggregating sentence scores to produce\ndocument scores. Experiments on TREC microblog and newswire test collections\nshow that our approach is simple yet effective, as we report the highest\naverage precision on these datasets by neural approaches that we are aware of.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 15:58:33 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Yang", "Wei", ""], ["Zhang", "Haotian", ""], ["Lin", "Jimmy", ""]]}, {"id": "1903.11272", "submitter": "Tetsuya Sakai", "authors": "Tetsuya Sakai", "title": "Graded Relevance Assessments and Graded Relevance Measures of NTCIR: A\n  Survey of the First Twenty Years", "comments": "31 pages; full length version of a book chapter (Evaluating\n  Information Retrieval and Access Tasks: NTCIR's Legacy of Research Impact)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NTCIR was the first large-scale IR evaluation conference to construct test\ncollections with graded relevance assessments: the NTCIR-1 test collections\nfrom 1998 already featured relevant and partially relevant documents. In this\npaper, I first describe a few graded-relevance measures that originated from\nNTCIR (and a few variants) which are used across different NTCIR tasks. I then\nprovide a survey on the use of graded relevance assessments and of graded\nrelevance measures in the past NTCIR tasks which primarily tackled ranked\nretrieval. My survey shows that the majority of the past tasks fully utilised\ngraded relevance by means of graded evaluation measures, but not all of them;\ninterestingly, even a few relatively recent tasks chose to adhere to binary\nrelevance measures. I conclude this paper by a summary of my survey in table\nform, and a brief discussion on what may lie beyond graded relevance.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 07:21:55 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Sakai", "Tetsuya", ""]]}, {"id": "1903.11279", "submitter": "Huasha Zhao Mr", "authors": "Xiaojing Liu, Feiyu Gao, Qiong Zhang, Huasha Zhao", "title": "Graph Convolution for Multimodal Information Extraction from Visually\n  Rich Documents", "comments": "naacl'19 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visually rich documents (VRDs) are ubiquitous in daily business and life.\nExamples are purchase receipts, insurance policy documents, custom declaration\nforms and so on. In VRDs, visual and layout information is critical for\ndocument understanding, and texts in such documents cannot be serialized into\nthe one-dimensional sequence without losing information. Classic information\nextraction models such as BiLSTM-CRF typically operate on text sequences and do\nnot incorporate visual features. In this paper, we introduce a graph\nconvolution based model to combine textual and visual information presented in\nVRDs. Graph embeddings are trained to summarize the context of a text segment\nin the document, and further combined with text embeddings for entity\nextraction. Extensive experiments have been conducted to show that our method\noutperforms BiLSTM-CRF baselines by significant margins, on two real-world\ndatasets. Additionally, ablation studies are also performed to evaluate the\neffectiveness of each component of our model.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 07:47:12 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Liu", "Xiaojing", ""], ["Gao", "Feiyu", ""], ["Zhang", "Qiong", ""], ["Zhao", "Huasha", ""]]}, {"id": "1903.11461", "submitter": "Kristoffer Nielbo", "authors": "Melvin Wevers, Jianbo Gao, Kristoffer L. Nielbo", "title": "Tracking the Consumption Junction: Temporal Dependencies between\n  Articles and Advertisements in Dutch Newspapers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historians have regularly debated whether advertisements can be used as a\nviable source to study the past. Their main concern centered on the question of\nagency. Were advertisements a reflection of historical events and societal\ndebates, or were ad makers instrumental in shaping society and the ways people\ninteracted with consumer goods? Using techniques from econometrics (Granger\ncausality test) and complexity science (Adaptive Fractal Analysis), this paper\nanalyzes to what extent advertisements shaped or reflected society. We found\nevidence that indicate a fundamental difference between the dynamic behavior of\nword use in articles and advertisements published in a century of Dutch\nnewspapers. Articles exhibit persistent trends that are likely to be reflective\nof communicative memory. Contrary to this, advertisements have a more irregular\nbehavior characterized by short bursts and fast decay, which, in part, mirrors\nthe dynamic through which advertisers introduced terms into public discourse.\nOn the issue of whether advertisements shaped or reflected society, we found\nparticular product types that seemed to be collectively driven by a causality\ngoing from advertisements to articles. Generally, we found support for a\ncomplex interaction pattern dubbed the consumption junction. Finally, we\ndiscovered noteworthy patterns in terms of causality and long-range\ndependencies for specific product groups. All in, this study shows how methods\nfrom econometrics and complexity science can be applied to humanities data to\nimprove our understanding of complex cultural-historical phenomena such as the\nrole of advertising in society.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 14:53:02 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Wevers", "Melvin", ""], ["Gao", "Jianbo", ""], ["Nielbo", "Kristoffer L.", ""]]}, {"id": "1903.11469", "submitter": "Antonio Iovanella", "authors": "Jacopo Arpetti, Antonio Iovanella", "title": "Towards more effective consumer steering via network analysis", "comments": null, "journal-ref": "European Journal of Law and Economics (2019)", "doi": null, "report-no": null, "categories": "cs.SI cs.IR q-fin.GN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Increased data gathering capacity, together with the spread of data analytics\ntechniques, has prompted an unprecedented concentration of information related\nto the individuals' preferences in the hands of a few gatekeepers. In the\npresent paper, we show how platforms' performances still appear astonishing in\nrelation to some unexplored data and networks properties, capable to enhance\nthe platforms' capacity to implement steering practices by means of an\nincreased ability to estimate individuals' preferences. To this end, we rely on\nnetwork science whose analytical tools allow data representations capable of\nhighlighting relationships between subjects and/or items, extracting a great\namount of information. We therefore propose a measure called Network\nInformation Patrimony, considering the amount of information available within\nthe system and we look into how platforms could exploit data stemming from\nconnected profiles within a network, with a view to obtaining competitive\nadvantages. Our measure takes into account the quality of the connections among\nnodes as the one of a hypothetical user in relation to its neighbourhood,\ndetecting how users with a good neighbourhood -- hence of a superior\nconnections set -- obtain better information. We tested our measures on\nAmazons' instances, obtaining evidence which confirm the relevance of\ninformation extracted from nodes' neighbourhood in order to steer targeted\nusers.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 15:02:35 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 20:06:26 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Arpetti", "Jacopo", ""], ["Iovanella", "Antonio", ""]]}, {"id": "1903.11833", "submitter": "Andres Ferraro", "authors": "Andr\\'es Ferraro, Dmitry Bogdanov and Xavier Serra", "title": "Skip prediction using boosting trees based on acoustic features of\n  tracks in sessions", "comments": null, "journal-ref": "WSDM Cup 2019 Workshop on the 12th ACM International Conference on\n  Web Search and Data Mining", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Spotify Sequential Skip Prediction Challenge focuses on predicting if a\ntrack in a session will be skipped by the user or not. In this paper, we\ndescribe our approach to this problem and the final system that was submitted\nto the challenge by our team from the Music Technology Group (MTG) under the\nname \"aferraro\". This system consists in combining the predictions of multiple\nboosting trees models trained with features extracted from the sessions and the\ntracks. The proposed approach achieves good overall performance (MAA of 0.554),\nwith our model ranked 14th out of more than 600 submissions in the final\nleaderboard.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 08:37:10 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Ferraro", "Andr\u00e9s", ""], ["Bogdanov", "Dmitry", ""], ["Serra", "Xavier", ""]]}, {"id": "1903.11983", "submitter": "Adil \\c{C}oban", "authors": "\\.Ilhan Tar{\\i}mer, Adil \\c{C}oban and Arif Emre Kocaman", "title": "Sentiment Analysis on IMDB Movie Comments and Twitter Data by Machine\n  Learning and Vector Space Techniques", "comments": "8 pages, submitted to CIEA2018 (http://iciea.cumhuriyet.edu.tr/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This study's goal is to create a model of sentiment analysis on a 2000 rows\nIMDB movie comments and 3200 Twitter data by using machine learning and vector\nspace techniques; positive or negative preliminary information about the text\nis to provide. In the study, a vector space was created in the KNIME Analytics\nplatform, and a classification study was performed on this vector space by\nDecision Trees, Na\\\"ive Bayes and Support Vector Machines classification\nalgorithms. The conclusions obtained were compared in terms of each algorithms.\nThe classification results for IMDB movie comments are obtained as 94,00%,\n73,20%, and 85,50% by Decision Tree, Naive Bayes and SVM algorithms. The\nclassification results for Twitter data set are presented as 82,76%, 75,44% and\n72,50% by Decision Tree, Naive Bayes SVM algorithms as well. It is seen that\nthe best classification results presented in both data sets are which\ncalculated by SVM algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 09:25:10 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Tar\u0131mer", "\u0130lhan", ""], ["\u00c7oban", "Adil", ""], ["Kocaman", "Arif Emre", ""]]}, {"id": "1903.12090", "submitter": "Fabrizio Sebastiani", "authors": "Alejandro Moreo Fern\\'andez, Andrea Esuli, Fabrizio Sebastiani", "title": "Learning to Weight for Text Classification", "comments": "To appear in IEEE Transactions on Knowledge and Data Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In information retrieval (IR) and related tasks, term weighting approaches\ntypically consider the frequency of the term in the document and in the\ncollection in order to compute a score reflecting the importance of the term\nfor the document. In tasks characterized by the presence of training data (such\nas text classification) it seems logical that the term weighting function\nshould take into account the distribution (as estimated from training data) of\nthe term across the classes of interest. Although `supervised term weighting'\napproaches that use this intuition have been described before, they have failed\nto show consistent improvements. In this article we analyse the possible\nreasons for this failure, and call consolidated assumptions into question.\nFollowing this criticism we propose a novel supervised term weighting approach\nthat, instead of relying on any predefined formula, learns a term weighting\nfunction optimised on the training set of interest; we dub this approach\n\\emph{Learning to Weight} (LTW). The experiments that we run on several\nwell-known benchmarks, and using different learning methods, show that our\nmethod outperforms previous term weighting approaches in text classification.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 16:13:35 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Fern\u00e1ndez", "Alejandro Moreo", ""], ["Esuli", "Andrea", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1903.12099", "submitter": "Laura Sebastia", "authors": "Alan Menk, Laura Sebastia and Rebeca Ferreira", "title": "Recommendation Systems for Tourism Based on Social Networks: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, recommender systems are present in many daily activities such as\nonline shopping, browsing social networks, etc. Given the rising demand for\nreinvigoration of the tourist industry through information technology,\nrecommenders have been included into tourism websites such as Expedia, Booking\nor Tripadvisor, among others. Furthermore, the amount of scientific papers\nrelated to recommender systems for tourism is on solid and continuous growth\nsince 2004. Much of this growth is due to social networks that, besides to\noffer researchers the possibility of using a great mass of available and\nconstantly updated data, they also enable the recommendation systems to become\nmore personalised, effective and natural. This paper reviews and analyses many\nresearch publications focusing on tourism recommender systems that use social\nnetworks in their projects. We detail their main characteristics, like which\nsocial networks are exploited, which data is extracted, the applied\nrecommendation techniques, the methods of evaluation, etc. Through a\ncomprehensive literature review, we aim to collaborate with the future\nrecommender systems, by giving some clear classifications and descriptions of\nthe current tourism recommender systems.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 16:33:18 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Menk", "Alan", ""], ["Sebastia", "Laura", ""], ["Ferreira", "Rebeca", ""]]}, {"id": "1903.12110", "submitter": "Fabrizio Sebastiani", "authors": "Andrea Esuli, Alejandro Moreo, Fabrizio Sebastiani", "title": "Building Automated Survey Coders via Interactive Machine Learning", "comments": "To appear in the International Journal of Market Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software systems trained via machine learning to automatically classify\nopen-ended answers (a.k.a. verbatims) are by now a reality. Still, their\nadoption in the survey coding industry has been less widespread than it might\nhave been. Among the factors that have hindered a more massive takeup of this\ntechnology are the effort involved in manually coding a sufficient amount of\ntraining data, the fact that small studies do not seem to justify this effort,\nand the fact that the process needs to be repeated anew when brand new coding\ntasks arise. In this paper we will argue for an approach to building verbatim\nclassifiers that we will call \"Interactive Learning\", and that addresses all\nthe above problems. We will show that, for the same amount of training effort,\ninteractive learning delivers much better coding accuracy than standard\n\"non-interactive\" learning. This is especially true when the amount of data we\nare willing to manually code is small, which makes this approach attractive\nalso for small-scale studies. Interactive learning also lends itself to reusing\npreviously trained classifiers for dealing with new (albeit related) coding\ntasks. Interactive learning also integrates better in the daily workflow of the\nsurvey specialist, and delivers a better user experience overall.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 16:51:17 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Esuli", "Andrea", ""], ["Moreo", "Alejandro", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1903.12388", "submitter": "Leyang Xue", "authors": "Peng Zhang, Leyang Xue, An Zeng", "title": "Predictability of diffusion-based recommender systems", "comments": null, "journal-ref": null, "doi": "10.1016/j.knosys.2019.104921", "report-no": null, "categories": "physics.soc-ph cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recommendation methods based on network diffusion have been shown to\nperform well in both recommendation accuracy and diversity. Nowdays, numerous\nextensions have been made to further improve the performance of such methods.\nHowever, to what extent can items be predicted by diffusion-based algorithms\nstill lack of understanding. Here, we mainly propose a method to quantify the\npredictability of diffusion-based algorithms. Accordingly, we conduct\nexperiments on Movielens and Netflix data sets. The results show that the\nhigher recommendation accuracy based on diffusion algorithms can still be\nachieved by optimizing the way of resource allocation on a density network. On\na sparse network, the possibility of improving accuracy is relatively low due\nto the fact that the current accuracy of diffusion-based methods is very close\nits predictability. In this case, we find that the predictability can be\nimproved significantly by multi-steps diffusion, especially for users with less\nhistorical information. In contrast to common belief, there are plausible\ncircumstances where the higher predictability of diffusion-based methods do not\ncorrespond to those users with more historical recording. Thus, we proposed the\ndiffusion coverage and item average degree to explain this phenomenon. In\naddition, we demonstrate the recommendation accuracy in real online system is\noverestimated by random partition used in the literature, suggesting the\nrecommendation in real online system may be a harder task.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 08:24:31 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zhang", "Peng", ""], ["Xue", "Leyang", ""], ["Zeng", "An", ""]]}, {"id": "1903.12495", "submitter": "Deepak Thukral", "authors": "Deepak Thukral, Darvesh Punia", "title": "Crowd Sourced Data Analysis: Mapping of Programming Concepts to\n  Syntactical Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since programming concepts do not match their syntactic representations, code\nsearch is a very tedious task. For instance in Java or C, array doesn't match\n[], so using \"array\" as a query, one cannot find what they are looking for.\nOften developers have to search code whether to understand any code, or to\nreuse some part of that code, or just to read it, without natural language\nsearching, developers have to often scroll back and forth or use variable names\nas their queries. In our work, we have used Stackoverflow (SO) question and\nanswers to make a mapping of programming concepts with their respective natural\nlanguage keywords, and then tag these natural language terms to every line of\ncode, which can further we used in searching using natural language keywords.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 15:13:20 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Thukral", "Deepak", ""], ["Punia", "Darvesh", ""]]}, {"id": "1903.12542", "submitter": "Areej Alokaili", "authors": "Areej Alokaili, Nikolaos Aletras and Mark Stevenson", "title": "Re-Ranking Words to Improve Interpretability of Automatically Generated\n  Topics", "comments": "Paper accepted for publication at IWCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topics models, such as LDA, are widely used in Natural Language Processing.\nMaking their output interpretable is an important area of research with\napplications to areas such as the enhancement of exploratory search interfaces\nand the development of interpretable machine learning models. Conventionally,\ntopics are represented by their n most probable words, however, these\nrepresentations are often difficult for humans to interpret. This paper\nexplores the re-ranking of topic words to generate more interpretable topic\nrepresentations. A range of approaches are compared and evaluated in two\nexperiments. The first uses crowdworkers to associate topics represented by\ndifferent word rankings with related documents. The second experiment is an\nautomatic approach based on a document retrieval task applied on multiple\ndomains. Results in both experiments demonstrate that re-ranking words improves\ntopic interpretability and that the most effective re-ranking schemes were\nthose which combine information about the importance of words both within\ntopics and their relative frequency in the entire corpus. In addition, close\ncorrelation between the results of the two evaluation approaches suggests that\nthe automatic method proposed here could be used to evaluate re-ranking methods\nwithout the need for human judgements.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 14:32:02 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Alokaili", "Areej", ""], ["Aletras", "Nikolaos", ""], ["Stevenson", "Mark", ""]]}]