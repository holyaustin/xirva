[{"id": "1306.0054", "submitter": "Ali Seyfi", "authors": "Ali Seyfi", "title": "Analysis and Evaluation of the Link and Content Based Focused\n  Treasure-Crawler", "comments": "13 pages, 12 figures, 7 tables. arXiv admin note: text overlap with\n  arXiv:1305.7265", "journal-ref": null, "doi": "10.1016/j.csi.2015.09.007", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing the Web is becoming a laborious task for search engines as the Web\nexponentially grows in size and distribution. Presently, the most effective\nknown approach to overcome this problem is the use of focused crawlers. A\nfocused crawler applies a proper algorithm in order to detect the pages on the\nWeb that relate to its topic of interest. For this purpose we proposed a custom\nmethod that uses specific HTML elements of a page to predict the topical focus\nof all the pages that have an unvisited link within the current page. These\nrecognized on-topic pages have to be sorted later based on their relevance to\nthe main topic of the crawler for further actual downloads. In the\nTreasure-Crawler, we use a hierarchical structure called the T-Graph which is\nan exemplary guide to assign appropriate priority score to each unvisited link.\nThese URLs will later be downloaded based on this priority. This paper outlines\nthe architectural design and embodies the implementation, test results and\nperformance evaluation of the Treasure-Crawler system. The Treasure-Crawler is\nevaluated in terms of information retrieval criteria such as recall and\nprecision, both with values close to 0.5. Gaining such outcome asserts the\nsignificance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2013 00:37:34 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Seyfi", "Ali", ""]]}, {"id": "1306.0165", "submitter": "Daqiang Zhang", "authors": "Daqiang Zhang, Qin Zou, Haoyi Xiong", "title": "CRUC: Cold-start Recommendations Using Collaborative Filtering in\n  Internet of Things", "comments": "Elsevier ESEP 2011: 9-10 December 2011, Singapore, Elsevier Energy\n  Procedia, http://www.elsevier.com/locate/procedia/, 2011", "journal-ref": null, "doi": "10.1016/j.egypro.2011.11.497", "report-no": null, "categories": "cs.IR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) aims at interconnecting everyday objects\n(including both things and users) and then using this connection information to\nprovide customized user services. However, IoT does not work in its initial\nstages without adequate acquisition of user preferences. This is caused by\ncold-start problem that is a situation where only few users are interconnected.\nTo this end, we propose CRUC scheme - Cold-start Recommendations Using\nCollaborative Filtering in IoT, involving formulation, filtering and prediction\nsteps. Extensive experiments over real cases and simulation have been performed\nto evaluate the performance of CRUC scheme. Experimental results show that CRUC\nefficiently solves the cold-start problem in IoT.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 02:23:44 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Zhang", "Daqiang", ""], ["Zou", "Qin", ""], ["Xiong", "Haoyi", ""]]}, {"id": "1306.0178", "submitter": "Riadh Bouslimi", "authors": "Riadh Bouslimi, Abir Messaoudi, Jalel Akaichi", "title": "Using a bag of Words for Automatic Medical Image Annotation with a\n  Latent Semantic", "comments": "10 pages, 6 figures", "journal-ref": "International Journal of Artificial Intelligence &\n  Applications(IJAIA), Vol 4, No.3 May 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a new approach for the automatic annotation of\nmedical images, using the approach of \"bag-of-words\" to represent the visual\ncontent of the medical image combined with text descriptors based approach\ntf.idf and reduced by latent semantic to extract the co-occurrence between\nterms and visual terms. A medical report is composed of a text describing a\nmedical image. First, we are interested to index the text and extract all\nrelevant terms using a thesaurus containing MeSH medical concepts. In a second\nphase, the medical image is indexed while recovering areas of interest which\nare invariant to change in scale, light and tilt. To annotate a new medical\nimage, we use the approach of \"bagof-words\" to recover the feature vector.\nIndeed, we use the vector space model to retrieve similar medical image from\nthe database training. The calculation of the relevance value of an image to\nthe query image is based on the cosine function. We conclude with an experiment\ncarried out on five types of radiological imaging to evaluate the performance\nof our system of medical annotation. The results showed that our approach works\nbetter with more images from the radiology of the skull.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 06:57:33 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2013 06:27:30 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Bouslimi", "Riadh", ""], ["Messaoudi", "Abir", ""], ["Akaichi", "Jalel", ""]]}, {"id": "1306.0271", "submitter": "Marina Danilevsky", "authors": "Marina Danilevsky, Chi Wang, Nihit Desai, Jingyi Guo, Jiawei Han", "title": "KERT: Automatic Extraction and Ranking of Topical Keyphrases from\n  Content-Representative Document Titles", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce KERT (Keyphrase Extraction and Ranking by Topic), a framework\nfor topical keyphrase generation and ranking. By shifting from the\nunigram-centric traditional methods of unsupervised keyphrase extraction to a\nphrase-centric approach, we are able to directly compare and rank phrases of\ndifferent lengths. We construct a topical keyphrase ranking function which\nimplements the four criteria that represent high quality topical keyphrases\n(coverage, purity, phraseness, and completeness). The effectiveness of our\napproach is demonstrated on two collections of content-representative titles in\nthe domains of Computer Science and Physics.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 01:44:28 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Danilevsky", "Marina", ""], ["Wang", "Chi", ""], ["Desai", "Nihit", ""], ["Guo", "Jingyi", ""], ["Han", "Jiawei", ""]]}, {"id": "1306.1343", "submitter": "Andrea Esuli", "authors": "Andrea Esuli", "title": "The User Feedback on SentiWordNet", "comments": null, "journal-ref": null, "doi": null, "report-no": "/cnr.isti/2013-TR-015", "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the release of SentiWordNet 3.0 the related Web interface has been\nrestyled and improved in order to allow users to submit feedback on the\nSentiWordNet entries, in the form of the suggestion of alternative triplets of\nvalues for an entry. This paper reports on the release of the user feedback\ncollected so far and on the plans for the future.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 08:56:32 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Esuli", "Andrea", ""]]}, {"id": "1306.1478", "submitter": "Benaboud Rohallah Mr", "authors": "Benaboud Rohallah and Maamri Ramdane and Sahnoun Zaidi", "title": "Agents and owl-s based semantic web service discovery with user\n  preference support", "comments": "19 pages, 10 figures", "journal-ref": "International Journal of Web & Semantic Technology (IJWesT), April\n  2013, Volume 4, Number 2, pp 57-75", "doi": "10.5121/ijwest.2013.4206", "report-no": null, "categories": "cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service-oriented computing (SOC) is an interdisciplinary paradigm that\nrevolutionizes the very fabric of distributed software development applications\nthat adopt service-oriented architectures (SOA) can evolve during their\nlifespan and adapt to changing or unpredictable environments more easily. SOA\nis built around the concept of Web Services. Although the Web services\nconstitute a revolution in Word Wide Web, they are always regarded as\nnon-autonomous entities and can be exploited only after their discovery. With\nthe help of software agents, Web services are becoming more efficient and more\ndynamic. The topic of this paper is the development of an agent based approach\nfor Web services discovery and selection in witch, OWL-S is used to describe\nWeb services, QoS and service customer request. We develop an efficient\nsemantic service matching which takes into account concepts properties to match\nconcepts in Web service and service customer request descriptions. Our approach\nis based on an architecture composed of four layers: Web service and Request\ndescription layer, Functional match layer, QoS computing layer and Reputation\ncomputing layer.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 17:25:54 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Rohallah", "Benaboud", ""], ["Ramdane", "Maamri", ""], ["Zaidi", "Sahnoun", ""]]}, {"id": "1306.1743", "submitter": "Philipp Schaer", "authors": "Tamara Heck and Philipp Schaer", "title": "Performing Informetric Analysis on Information Retrieval Test\n  Collections: Preliminary Experiments in the Physics Domain", "comments": "6 pages, 1 figure, 2 tables, accepted for 14th International Society\n  of Scientometrics and Informetrics Conference (ISSI 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of informetric analysis and information retrieval allows a\ntwofold application. (1) While in-formetrics analysis is primarily used to gain\ninsights into a scientific domain, it can be used to build recommen-dation or\nalternative ranking services. They are usually based on methods like\nco-occurrence or citation analyses. (2) Information retrieval and its\ndecades-long tradition of rigorous evaluation using standard document corpora,\npredefined topics and relevance judgements can be used as a test bed for\ninformetric analyses. We show a preliminary experiment on how both domains can\nbe connected using the iSearch test collection, a standard information\nretrieval test collection derived from the open access arXiv.org preprint\nserver. In this paper the aim is to draw a conclusion about the appropriateness\nof iSearch as a test bed for the evaluation of a retrieval or recommendation\nsystem that applies informetric methods to improve retrieval results for the\nuser. Based on an interview study with physicists, bibliographic coupling and\nauthor-co-citation analysis, important authors for ten different research\nquestions are identified. The results show that the analysed corpus includes\nthese authors and their corresponding documents. This study is a first step\ntowards a combination of retrieval evaluations and the evaluation of\ninformetric analyses methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 15:10:15 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Heck", "Tamara", ""], ["Schaer", "Philipp", ""]]}, {"id": "1306.2081", "submitter": "Bo Li", "authors": "Bo Li and Henry Johan", "title": "3D model retrieval using global and local radial distances", "comments": "6", "journal-ref": "The International Workshop on Advanced Image Technology\n  (IWAIT2010), 2010", "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D model retrieval techniques can be classified as histogram-based,\nview-based and graph-based approaches. We propose a hybrid shape descriptor\nwhich combines the global and local radial distance features by utilizing the\nhistogram-based and view-based approaches respectively. We define an\narea-weighted global radial distance with respect to the center of the bounding\nsphere of the model and encode its distribution into a 2D histogram as the\nglobal radial distance shape descriptor. We then uniformly divide the bounding\ncube of a 3D model into a set of small cubes and define their centers as local\ncenters. Then, we compute the local radial distance of a point based on the\nnearest local center. By sparsely sampling a set of views and encoding the\nlocal radial distance feature on the rendered views by color coding, we extract\nthe local radial distance shape descriptor. Based on these two shape\ndescriptors, we develop a hybrid radial distance shape descriptor for 3D model\nretrieval. Experiment results show that our hybrid shape descriptor outperforms\nseveral typical histogram-based and view-based approaches.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 01:38:09 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Li", "Bo", ""], ["Johan", "Henry", ""]]}, {"id": "1306.2499", "submitter": "Mohammed Alaeddine Abderrahim", "authors": "Mohammed Alaeddine Abderrahim, Mohammed El Amine Abderrahim, Mohammed\n  Amine Chikh", "title": "Using Arabic Wordnet for semantic indexation in information retrieval\n  system", "comments": "6 pages,2 figures,7 tables", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 10,\n  Issue 1, No 2, January 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of arabic Information Retrieval Systems (IRS) guided by arabic\nontology and to enable those systems to better respond to user requirements,\nthis paper aims to representing documents and queries by the best concepts\nextracted from Arabic Wordnet. Identified concepts belonging to Arabic WordNet\nsynsets are extracted from documents and queries, and those having a single\nsense are expanded. The expanded query is then used by the IRS to retrieve the\nrelevant documents searched. Our experiments are based primarily on a medium\nsize corpus of arabic text. The results obtained shown us that there are a\nglobal improvement in the performance of the arabic IRS.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 12:24:55 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2013 22:05:37 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Abderrahim", "Mohammed Alaeddine", ""], ["Abderrahim", "Mohammed El Amine", ""], ["Chikh", "Mohammed Amine", ""]]}, {"id": "1306.2597", "submitter": "Tao Qin Dr.", "authors": "Tao Qin and Tie-Yan Liu", "title": "Introducing LETOR 4.0 Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LETOR is a package of benchmark data sets for research on LEarning TO Rank,\nwhich contains standard features, relevance judgments, data partitioning,\nevaluation tools, and several baselines. Version 1.0 was released in April\n2007. Version 2.0 was released in Dec. 2007. Version 3.0 was released in Dec.\n2008. This version, 4.0, was released in July 2009. Very different from\nprevious versions (V3.0 is an update based on V2.0 and V2.0 is an update based\non V1.0), LETOR4.0 is a totally new release. It uses the Gov2 web page\ncollection (~25M pages) and two query sets from Million Query track of TREC\n2007 and TREC 2008. We call the two query sets MQ2007 and MQ2008 for short.\nThere are about 1700 queries in MQ2007 with labeled documents and about 800\nqueries in MQ2008 with labeled documents. If you have any questions or\nsuggestions about the datasets, please kindly email us (letor@microsoft.com).\nOur goal is to make the dataset reliable and useful for the community.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2013 09:58:00 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Qin", "Tao", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1306.2838", "submitter": "Diederik Aerts", "authors": "Diederik Aerts, Jan Broekaert, Sandro Sozzo and Tomas Veloz", "title": "The Quantum Challenge in Concept Theory and Natural Language Processing", "comments": "5 pages", "journal-ref": "Proceedings of the 25th International Conference on System\n  Research, Informatics & Cybernetics (pp. 13-17). Ed.. E. G. Lasker, IIAS,\n  2013", "doi": null, "report-no": null, "categories": "cs.CL cs.IR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mathematical formalism of quantum theory has been successfully used in\nhuman cognition to model decision processes and to deliver representations of\nhuman knowledge. As such, quantum cognition inspired tools have improved\ntechnologies for Natural Language Processing and Information Retrieval. In this\npaper, we overview the quantum cognition approach developed in our Brussels\nteam during the last two decades, specifically our identification of quantum\nstructures in human concepts and language, and the modeling of data from\npsychological and corpus-text-based experiments. We discuss our\nquantum-theoretic framework for concepts and their conjunctions/disjunctions in\na Fock-Hilbert space structure, adequately modeling a large amount of data\ncollected on concept combinations. Inspired by this modeling, we put forward\nelements for a quantum contextual and meaning-based approach to information\ntechnologies in which 'entities of meaning' are inversely reconstructed from\ntexts, which are considered as traces of these entities' states.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 14:35:11 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Aerts", "Diederik", ""], ["Broekaert", "Jan", ""], ["Sozzo", "Sandro", ""], ["Veloz", "Tomas", ""]]}, {"id": "1306.2864", "submitter": "Catarina Moreira", "authors": "Catarina Moreira and Andreas Wichert", "title": "Finding Academic Experts on a MultiSensor Approach using Shannon's\n  Entropy", "comments": null, "journal-ref": "Journal of Expert Systems with Applications, 2013, volume 40,\n  issue 14", "doi": "10.1016/j.eswa.2013.04.001", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expert finding is an information retrieval task concerned with the search for\nthe most knowledgeable people, in some topic, with basis on documents\ndescribing peoples activities. The task involves taking a user query as input\nand returning a list of people sorted by their level of expertise regarding the\nuser query. This paper introduces a novel approach for combining multiple\nestimators of expertise based on a multisensor data fusion framework together\nwith the Dempster-Shafer theory of evidence and Shannon's entropy. More\nspecifically, we defined three sensors which detect heterogeneous information\nderived from the textual contents, from the graph structure of the citation\npatterns for the community of experts, and from profile information about the\nacademic experts. Given the evidences collected, each sensor may define\ndifferent candidates as experts and consequently do not agree in a final\nranking decision. To deal with these conflicts, we applied the Dempster-Shafer\ntheory of evidence combined with Shannon's Entropy formula to fuse this\ninformation and come up with a more accurate and reliable final ranking list.\nExperiments made over two datasets of academic publications from the Computer\nScience domain attest for the adequacy of the proposed approach over the\ntraditional state of the art approaches. We also made experiments against\nrepresentative supervised state of the art algorithms. Results revealed that\nthe proposed method achieved a similar performance when compared to these\nsupervised techniques, confirming the capabilities of the proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 15:35:57 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Moreira", "Catarina", ""], ["Wichert", "Andreas", ""]]}, {"id": "1306.3955", "submitter": "Abderrahim Mohammed El Amine", "authors": "Abderrahim Mohammed El Amine, Benameur Said, Abderrahim Mohammed\n  Alaeddine", "title": "The Number of Terms and Documents for Pseudo-Relevant Feedback for\n  Ad-hoc Information Retrieval", "comments": "7 pages", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 10,\n  Issue 1, No 1, January 2013; pp 661-667; ISSN 1694-0784", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Information Retrieval System (IRS), the Automatic Relevance Feedback (ARF)\nis a query reformulation technique that modifies the initial one without the\nuser intervention. It is applied mainly through the addition of terms coming\nfrom the external resources such as the ontologies and or the results of the\ncurrent research. In this context we are mainly interested in the local\nanalysis technique for the ARF in ad-hoc IRS on Arabic documents. In this\narticle, we have examined the impact of the variation of the two parameters\nimplied in this technique, that is to say, the number of the documents\n{\\guillemotleft}D{\\guillemotright} and the number of terms\n{\\guillemotleft}T{\\guillemotright}, on an Arabic IRS performance. The\nexperimentation, carried out on an Arabic corpus text, enables us to deduce\nthat there are queries which are not easily improvable with the query\nreformulation. In addition, the success of the ARF is due mainly to the\nselection of a sufficient number of documents D and to the extraction of a very\nreduced set of relevant terms T for retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 19:14:12 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Amine", "Abderrahim Mohammed El", ""], ["Said", "Benameur", ""], ["Alaeddine", "Abderrahim Mohammed", ""]]}, {"id": "1306.4193", "submitter": "Zi-Ke Zhang Dr.", "authors": "Jin-Hu Liu, Zi-Ke Zhang, Chengcheng Yang, Lingjiao Chen, Chuang Liu,\n  Xueqi Wang", "title": "Gravity Effects on Information Filtering and Network Evolving", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0091070", "report-no": null, "categories": "physics.soc-ph cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, based on the gravity principle of classical physics, we\npropose a tunable gravity-based model, which considers tag usage pattern to\nweigh both the mass and distance of network nodes. We then apply this model in\nsolving the problems of information filtering and network evolving.\nExperimental results on two real-world data sets, \\emph{Del.icio.us} and\n\\emph{MovieLens}, show that it can not only enhance the algorithmic\nperformance, but can also better characterize the properties of real networks.\nThis work may shed some light on the in-depth understanding of the effect of\ngravity model.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 13:38:31 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2014 08:10:46 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Liu", "Jin-Hu", ""], ["Zhang", "Zi-Ke", ""], ["Yang", "Chengcheng", ""], ["Chen", "Lingjiao", ""], ["Liu", "Chuang", ""], ["Wang", "Xueqi", ""]]}, {"id": "1306.4427", "submitter": "Surekha Mariam Varghese Dr.", "authors": "Nithin K. Anil, Sharath Basil Kurian, Aby Abahai T, Surekha Mariam\n  Varghese", "title": "Multidimensional User Data Model for Web Personalization", "comments": "6 pages, 3 figures -\"Published with International Journal of Computer\n  Applications (IJCA)\"", "journal-ref": "International Journal of Computer Applications, Volume 69, No.12,\n  May 2013", "doi": "10.5120/11896-7955 10.5120/11896-7955 10.5120/11896-7955", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalization is being applied to great extend in many systems. This paper\npresents a multi-dimensional user data model and its application in web search.\nOnline and Offline activities of the user are tracked for creating the user\nmodel. The main phases are identification of relevant documents and the\nrepresentation of relevance and similarity of the documents. The concepts\nKeywords, Topics, URLs and clusters are used in the implementation. The\nalgorithms for profiling, grading and clustering the concepts in the user model\nand algorithm for determining the personalized search results by re-ranking the\nresults in a search bank are presented in this paper. Simple experiments for\nevaluation of the model and their results are described.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 05:25:45 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Anil", "Nithin K.", ""], ["Kurian", "Sharath Basil", ""], ["T", "Aby Abahai", ""], ["Varghese", "Surekha Mariam", ""]]}, {"id": "1306.4606", "submitter": "Luis Marujo", "authors": "Luis Marujo, M\\'arcio Viveiros, Jo\\~ao Paulo da Silva Neto", "title": "Keyphrase Cloud Generation of Broadcast News", "comments": "In Proceeding of Interspeech 2011: 12th Annual Conference of the\n  International Speech Communication Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an enhanced automatic keyphrase extraction method\napplied to Broadcast News. The keyphrase extraction process is used to create a\nconcept level for each news. On top of words resulting from a speech\nrecognition system output and news indexation and it contributes to the\ngeneration of a tag/keyphrase cloud of the top news included in a Multimedia\nMonitoring Solution system for TV and Radio news/programs, running daily, and\nmonitoring 12 TV channels and 4 Radios.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 16:37:23 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Marujo", "Luis", ""], ["Viveiros", "M\u00e1rcio", ""], ["Neto", "Jo\u00e3o Paulo da Silva", ""]]}, {"id": "1306.4608", "submitter": "Luis Marujo", "authors": "Luis Marujo, Miguel Bugalho, Jo\\~ao Paulo da Silva Neto, Anatole\n  Gershman, Jaime Carbonell", "title": "Hourly Traffic Prediction of News Stories", "comments": "In 3rd International Workshop on Context-Aware Recommender Systems\n  held as part of the 5th ACM RecSys Conference 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of predicting news stories popularity from several news sources\nhas become a challenge of great importance for both news producers and readers.\nIn this paper, we investigate methods for automatically predicting the number\nof clicks on a news story during one hour. Our approach is a combination of\nadditive regression and bagging applied over a M5P regression tree using a\nlogarithmic scale (log10). The features included are social-based (social\nnetwork metadata from Facebook), content-based (automatically extracted\nkeyphrases, and stylometric statistics from news titles), and time-based. In\n1st Sapo Data Challenge we obtained 11.99% as mean relative error value which\nput us in the 4th place out of 26 participants.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 16:44:16 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Marujo", "Luis", ""], ["Bugalho", "Miguel", ""], ["Neto", "Jo\u00e3o Paulo da Silva", ""], ["Gershman", "Anatole", ""], ["Carbonell", "Jaime", ""]]}, {"id": "1306.4631", "submitter": "Rachana Parikh", "authors": "Rachana Parikh and Avani R. Vasant", "title": "Table of Content detection using Machine Learning", "comments": "International Journal of Artificial Intelligence and Applications,\n  May-2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Table of content (TOC) detection has drawn attention now a day because it\nplays an important role in digitization of multipage document. Generally book\ndocument is multipage document. So it becomes necessary to detect Table of\nContent page for easy navigation of multipage document and also to make\ninformation retrieval faster for desirable data from the multipage document.\nAll the Table of content pages follow the different layout, different way of\npresenting the contents of the document like chapter, section, subsection etc.\nThis paper introduces a new method to detect Table of content using machine\nlearning technique with different features. With the main aim to detect Table\nof Content pages is to structure the document according to their contents.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 08:08:22 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Parikh", "Rachana", ""], ["Vasant", "Avani R.", ""]]}, {"id": "1306.4633", "submitter": "Mayank Shishodia B.Tech", "authors": "Sumit Goswami and Mayank Singh Shishodia", "title": "A Fuzzy Based Approach to Text Mining and Document Clustering", "comments": "10 pages, 6 tables, 1 figure, review paper, International Journal of\n  Data Mining & Knowledge Management Process (IJDKP) ISSN : 2230 - 9608[Online]\n  ; 2231 - 007X [Print]. Paper can be found at\n  http://airccse.org/journal/ijdkp/current2013.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy logic deals with degrees of truth. In this paper, we have shown how to\napply fuzzy logic in text mining in order to perform document clustering. We\ntook an example of document clustering where the documents had to be clustered\ninto two categories. The method involved cleaning up the text and stemming of\nwords. Then, we chose m number of features which differ significantly in their\nword frequencies (WF), normalized by document length, between documents\nbelonging to these two clusters. The documents to be clustered were represented\nas a collection of m normalized WF values. Fuzzy c-means (FCM) algorithm was\nused to cluster these documents into two clusters. After the FCM execution\nfinished, the documents in the two clusters were analysed for the values of\ntheir respective m features. It was known that documents belonging to a\ndocument type, say X, tend to have higher WF values for some particular\nfeatures. If the documents belonging to a cluster had higher WF values for\nthose same features, then that cluster was said to represent X. By fuzzy logic,\nwe not only get the cluster name, but also the degree to which a document\nbelongs to a cluster.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 07:35:23 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Goswami", "Sumit", ""], ["Shishodia", "Mayank Singh", ""]]}, {"id": "1306.4758", "submitter": "Payal Gulati Ms", "authors": "Payal Gulati, A. K. Sharma", "title": "Analysing Word Importance for Image Annotation", "comments": "4 pages, 3 figures, Published in IJCSI (International Journal of\n  Computer Science Issues) Journal, Volume 10, Issue 1, No 2, January 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image annotation provides several keywords automatically for a given image\nbased on various tags to describe its contents which is useful in Image\nretrieval. Various researchers are working on text based and content based\nimage annotations [7,9]. It is seen, in traditional Image annotation\napproaches, annotation words are treated equally without considering the\nimportance of each word in real world. In context of this, in this work, images\nare annotated with keywords based on their frequency count and word\ncorrelation. Moreover this work proposes an approach to compute importance\nscore of candidate keywords, having same frequency count.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 05:42:58 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Gulati", "Payal", ""], ["Sharma", "A. K.", ""]]}, {"id": "1306.4886", "submitter": "Luis Marujo", "authors": "Luis Marujo, Anatole Gershman, Jaime Carbonell, Robert Frederking,\n  Jo\\~ao P. Neto", "title": "Supervised Topical Key Phrase Extraction of News Stories using\n  Crowdsourcing, Light Filtering and Co-reference Normalization", "comments": "In 8th International Conference on Language Resources and Evaluation\n  (LREC 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and effective automated indexing is critical for search and personalized\nservices. Key phrases that consist of one or more words and represent the main\nconcepts of the document are often used for the purpose of indexing. In this\npaper, we investigate the use of additional semantic features and\npre-processing steps to improve automatic key phrase extraction. These features\ninclude the use of signal words and freebase categories. Some of these features\nlead to significant improvements in the accuracy of the results. We also\nexperimented with 2 forms of document pre-processing that we call light\nfiltering and co-reference normalization. Light filtering removes sentences\nfrom the document, which are judged peripheral to its main content.\nCo-reference normalization unifies several written forms of the same named\nentity into a unique form. We also needed a \"Gold Standard\" - a set of labeled\ndocuments for training and evaluation. While the subjective nature of key\nphrase selection precludes a true \"Gold Standard\", we used Amazon's Mechanical\nTurk service to obtain a useful approximation. Our data indicates that the\nbiggest improvements in performance were due to shallow semantic features, news\ncategories, and rhetorical signals (nDCG 78.47% vs. 68.93%). The inclusion of\ndeeper semantic features such as Freebase sub-categories was not beneficial by\nitself, but in combination with pre-processing, did cause slight improvements\nin the nDCG scores.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 14:22:00 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Marujo", "Luis", ""], ["Gershman", "Anatole", ""], ["Carbonell", "Jaime", ""], ["Frederking", "Robert", ""], ["Neto", "Jo\u00e3o P.", ""]]}, {"id": "1306.4890", "submitter": "Luis Marujo", "authors": "Luis Marujo, Ricardo Ribeiro, David Martins de Matos, Jo\\~ao P. Neto,\n  Anatole Gershman, and Jaime Carbonell", "title": "Key Phrase Extraction of Lightly Filtered Broadcast News", "comments": "In 15th International Conference on Text, Speech and Dialogue (TSD\n  2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the impact of light filtering on automatic key phrase\nextraction (AKE) applied to Broadcast News (BN). Key phrases are words and\nexpressions that best characterize the content of a document. Key phrases are\noften used to index the document or as features in further processing. This\nmakes improvements in AKE accuracy particularly important. We hypothesized that\nfiltering out marginally relevant sentences from a document would improve AKE\naccuracy. Our experiments confirmed this hypothesis. Elimination of as little\nas 10% of the document sentences lead to a 2% improvement in AKE precision and\nrecall. AKE is built over MAUI toolkit that follows a supervised learning\napproach. We trained and tested our AKE method on a gold standard made of 8 BN\nprograms containing 110 manually annotated news stories. The experiments were\nconducted within a Multimedia Monitoring Solution (MMS) system for TV and radio\nnews/programs, running daily, and monitoring 12 TV and 4 radio channels.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 14:35:22 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Marujo", "Luis", ""], ["Ribeiro", "Ricardo", ""], ["de Matos", "David Martins", ""], ["Neto", "Jo\u00e3o P.", ""], ["Gershman", "Anatole", ""], ["Carbonell", "Jaime", ""]]}, {"id": "1306.4908", "submitter": "Luis Marujo", "authors": "Luis Marujo, Wang Ling, Anatole Gershman, Jaime Carbonell, Jo\\~ao P.\n  Neto, David Matos", "title": "Recognition of Named-Event Passages in News Articles", "comments": "In 25th International Conference on Computational Linguistics (COLING\n  2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the concept of Named Entities to Named Events - commonly occurring\nevents such as battles and earthquakes. We propose a method for finding\nspecific passages in news articles that contain information about such events\nand report our preliminary evaluation results. Collecting \"Gold Standard\" data\npresents many problems, both practical and conceptual. We present a method for\nobtaining such data using the Amazon Mechanical Turk service.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 15:22:21 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Marujo", "Luis", ""], ["Ling", "Wang", ""], ["Gershman", "Anatole", ""], ["Carbonell", "Jaime", ""], ["Neto", "Jo\u00e3o P.", ""], ["Matos", "David", ""]]}, {"id": "1306.5170", "submitter": "Wafaa Tawfik abdel-moneim", "authors": "Wafaa Tawfik Abdel-moneim, Mohamed Hashem Abdel-Aziz, and Mohamed\n  Monier Hassan", "title": "Clinical Relationships Extraction Techniques from Patient Narratives", "comments": "15 pages 13 figures 7 tables", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol.10,\n  Issue 1, January 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Clinical E-Science Framework (CLEF) project was used to extract important\ninformation from medical texts by building a system for the purpose of clinical\nresearch, evidence-based healthcare and genotype-meets-phenotype informatics.\nThe system is divided into two parts, one part concerns with the identification\nof relationships between clinically important entities in the text. The full\nparses and domain-specific grammars had been used to apply many approaches to\nextract the relationship. In the second part of the system, statistical machine\nlearning (ML) approaches are applied to extract relationship. A corpus of\noncology narratives that hand annotated with clinical relationships can be used\nto train and test a system that has been designed and implemented by supervised\nmachine learning (ML) approaches. Many features can be extracted from these\ntexts that are used to build a model by the classifier. Multiple supervised\nmachine learning algorithms can be applied for relationship extraction. Effects\nof adding the features, changing the size of the corpus, and changing the type\nof the algorithm on relationship extraction are examined. Keywords: Text\nmining; information extraction; NLP; entities; and relations.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 15:30:09 GMT"}], "update_date": "2013-06-24", "authors_parsed": [["Abdel-moneim", "Wafaa Tawfik", ""], ["Abdel-Aziz", "Mohamed Hashem", ""], ["Hassan", "Mohamed Monier", ""]]}, {"id": "1306.6259", "submitter": "Young-Ho Eom", "authors": "Young-Ho Eom, Dima L. Shepelyansky", "title": "Highlighting Entanglement of Cultures via Ranking of Multilingual\n  Wikipedia Articles", "comments": "Published in PLoS ONE\n  (http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0074554).\n  Supporting information is available on the same webpage", "journal-ref": "PLoS ONE 8(10): e74554 (2013)", "doi": "10.1371/journal.pone.0074554", "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How different cultures evaluate a person? Is an important person in one\nculture is also important in the other culture? We address these questions via\nranking of multilingual Wikipedia articles. With three ranking algorithms based\non network structure of Wikipedia, we assign ranking to all articles in 9\nmultilingual editions of Wikipedia and investigate general ranking structure of\nPageRank, CheiRank and 2DRank. In particular, we focus on articles related to\npersons, identify top 30 persons for each rank among different editions and\nanalyze distinctions of their distributions over activity fields such as\npolitics, art, science, religion, sport for each edition. We find that local\nheroes are dominant but also global heroes exist and create an effective\nnetwork representing entanglement of cultures. The Google matrix analysis of\nnetwork of cultures shows signs of the Zipf law distribution. This approach\nallows to examine diversity and shared characteristics of knowledge\norganization between cultures. The developed computational, data driven\napproach highlights cultural interconnections in a new perspective.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 14:33:39 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2013 13:20:54 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Eom", "Young-Ho", ""], ["Shepelyansky", "Dima L.", ""]]}, {"id": "1306.6370", "submitter": "Tommy Nguyen", "authors": "Tommy Nguyen and Boleslaw K. Szymanski", "title": "Social Ranking Techniques for the Web", "comments": "7 pages, ASONAM 2013", "journal-ref": "Proc. 2013 IEEE/ACM International Conference on Advances in Social\n  Networks Analysis and Mining (ASONAM) Niagara Falls, Canada, August 25-28,\n  2013, 49-55", "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of social media has the potential for changing the\nstructure and organization of the web. In the past, scientists have looked at\nthe web as a large connected component to understand how the topology of\nhyperlinks correlates with the quality of information contained in the page and\nthey proposed techniques to rank information contained in web pages. We argue\nthat information from web pages and network data on social relationships can be\ncombined to create a personalized and socially connected web. In this paper, we\nlook at the web as a composition of two networks, one consisting of information\nin web pages and the other of personal data shared on social media web sites.\nTogether, they allow us to analyze how social media tunnels the flow of\ninformation from person to person and how to use the structure of the social\nnetwork to rank, deliver, and organize information specifically for each\nindividual user. We validate our social ranking concepts through a ranking\nexperiment conducted on web pages that users shared on Google Buzz and Twitter.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 22:17:05 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Nguyen", "Tommy", ""], ["Szymanski", "Boleslaw K.", ""]]}, {"id": "1306.6542", "submitter": "Shuai Yuan", "authors": "Shuai Yuan, Jun Wang, Xiaoxue Zhao", "title": "Real-time Bidding for Online Advertising: Measurement and Analysis", "comments": "Accepted by ADKDD '13 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The real-time bidding (RTB), aka programmatic buying, has recently become the\nfastest growing area in online advertising. Instead of bulking buying and\ninventory-centric buying, RTB mimics stock exchanges and utilises computer\nalgorithms to automatically buy and sell ads in real-time; It uses per\nimpression context and targets the ads to specific people based on data about\nthem, and hence dramatically increases the effectiveness of display\nadvertising. In this paper, we provide an empirical analysis and measurement of\na production ad exchange. Using the data sampled from both demand and supply\nside, we aim to provide first-hand insights into the emerging new impression\nselling infrastructure and its bidding behaviours, and help identifying\nresearch and design issues in such systems. From our study, we observed that\nperiodic patterns occur in various statistics including impressions, clicks,\nbids, and conversion rates (both post-view and post-click), which suggest\ntime-dependent models would be appropriate for capturing the repeated patterns\nin RTB. We also found that despite the claimed second price auction, the first\nprice payment in fact is accounted for 55.4% of total cost due to the\narrangement of the soft floor price. As such, we argue that the setting of soft\nfloor price in the current RTB systems puts advertisers in a less favourable\nposition. Furthermore, our analysis on the conversation rates shows that the\ncurrent bidding strategy is far less optimal, indicating the significant needs\nfor optimisation algorithms incorporating the facts such as the temporal\nbehaviours, the frequency and recency of the ad displays, which have not been\nwell considered in the past.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 15:15:16 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Yuan", "Shuai", ""], ["Wang", "Jun", ""], ["Zhao", "Xiaoxue", ""]]}, {"id": "1306.6755", "submitter": "Kareem Darwish", "authors": "Kareem Darwish", "title": "Arabizi Detection and Conversion to Arabic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Arabizi is Arabic text that is written using Latin characters. Arabizi is\nused to present both Modern Standard Arabic (MSA) or Arabic dialects. It is\ncommonly used in informal settings such as social networking sites and is often\nwith mixed with English. In this paper we address the problems of: identifying\nArabizi in text and converting it to Arabic characters. We used word and\nsequence-level features to identify Arabizi that is mixed with English. We\nachieved an identification accuracy of 98.5%. As for conversion, we used\ntransliteration mining with language modeling to generate equivalent Arabic\ntext. We achieved 88.7% conversion accuracy, with roughly a third of errors\nbeing spelling and morphological variants of the forms in ground truth.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 08:46:11 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Darwish", "Kareem", ""]]}, {"id": "1306.6944", "submitter": "Ulf Sch\\\"oneberg", "authors": "Ulf Sch\\\"oneberg and Wolfram Sperber", "title": "The DeLiVerMATH project - Text analysis in mathematics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A high-quality content analysis is essential for retrieval functionalities\nbut the manual extraction of key phrases and classification is expensive.\nNatural language processing provides a framework to automatize the process.\nHere, a machine-based approach for the content analysis of mathematical texts\nis described. A prototype for key phrase extraction and classification of\nmathematical texts is presented.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 15:48:06 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Sch\u00f6neberg", "Ulf", ""], ["Sperber", "Wolfram", ""]]}]