[{"id": "0907.1005", "submitter": "Julien Cohen", "authors": "Julien Cohen (LINA)", "title": "A class of structured P2P systems supporting browsing", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Browsing is a way of finding documents in a large amount of data which is\ncomplementary to querying and which is particularly suitable for multimedia\ndocuments. Locating particular documents in a very large collection of\nmultimedia documents such as the ones available in peer to peer networks is a\ndifficult task. However, current peer to peer systems do not allow to do this\nby browsing. In this report, we show how one can build a peer to peer system\nsupporting a kind of browsing. In our proposal, one must extend an existing\ndistributed hash table system with a few features : handling partial hash-keys\nand providing appropriate routing mechanisms for these hash-keys. We give such\nan algorithm for the particular case of the Tapestry distributed hash table.\nThis is a work in progress as no proper validation has been done yet.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2009 14:24:27 GMT"}], "update_date": "2009-07-07", "authors_parsed": [["Cohen", "Julien", "", "LINA"]]}, {"id": "0907.1224", "submitter": "Jianguo Liu", "authors": "Jian-Guo Liu, Tao Zhou, Qiang Guo, Bing-Hong Wang, Yi-Cheng Zhang", "title": "Effect of user tastes on personalized recommendation", "comments": "8 pages, 4 figures", "journal-ref": "IJMPC 20(12) 2009 1925-1932", "doi": "10.1142/S0129183109014825", "report-no": null, "categories": "physics.data-an cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, based on a weighted projection of the user-object bipartite\nnetwork, we study the effects of user tastes on the mass-diffusion-based\npersonalized recommendation algorithm, where a user's tastes or interests are\ndefined by the average degree of the objects he has collected. We argue that\nthe initial recommendation power located on the objects should be determined by\nboth of their degree and the users' tastes. By introducing a tunable parameter,\nthe user taste effects on the configuration of initial recommendation power\ndistribution are investigated. The numerical results indicate that the\npresented algorithm could improve the accuracy, measured by the average ranking\nscore, more importantly, we find that when the data is sparse, the algorithm\nshould give more recommendation power to the objects whose degrees are close to\nthe users' tastes, while when the data becomes dense, it should assign more\npower on the objects whose degrees are significantly different from user's\ntastes.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2009 13:56:59 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2009 15:17:35 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Liu", "Jian-Guo", ""], ["Zhou", "Tao", ""], ["Guo", "Qiang", ""], ["Wang", "Bing-Hong", ""], ["Zhang", "Yi-Cheng", ""]]}, {"id": "0907.1228", "submitter": "Jianguo Liu", "authors": "Jian-Guo Liu, Tao Zhou, Zhao-Guo Xuan, Hong-An Che, Bing-Hong Wang,\n  Yi-Cheng Zhang", "title": "Degree correlation effect of bipartite network on personalized\n  recommendation", "comments": "9 pages, 3 figures", "journal-ref": "IJMPC 21(01) 2010 137-147", "doi": "10.1142/S0129183110014999", "report-no": null, "categories": "physics.data-an cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, by introducing a new user similarity index base on the\ndiffusion process, we propose a modified collaborative filtering (MCF)\nalgorithm, which has remarkably higher accuracy than the standard collaborative\nfiltering. In the proposed algorithm, the degree correlation between users and\nobjects is taken into account and embedded into the similarity index by a\ntunable parameter. The numerical simulation on a benchmark data set shows that\nthe algorithmic accuracy of the MCF, measured by the average ranking score, is\nfurther improved by 18.19% in the optimal case. In addition, two significant\ncriteria of algorithmic performance, diversity and popularity, are also taken\ninto account. Numerical results show that the presented algorithm can provide\nmore diverse and less popular recommendations, for example, when the\nrecommendation list contains 10 objects, the diversity, measured by the hamming\ndistance, is improved by 21.90%.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2009 14:20:47 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Liu", "Jian-Guo", ""], ["Zhou", "Tao", ""], ["Xuan", "Zhao-Guo", ""], ["Che", "Hong-An", ""], ["Wang", "Bing-Hong", ""], ["Zhang", "Yi-Cheng", ""]]}, {"id": "0907.1632", "submitter": "Naveen Ashish", "authors": "Naveen Ashish, Sharad Mehrotra, Pouria Pirzadeh", "title": "Incorporating Integrity Constraints in Uncertain Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We develop an approach to incorporate additional knowledge, in the form of\ngeneral purpose integrity constraints (ICs), to reduce uncertainty in\nprobabilistic databases. While incorporating ICs improves data quality (and\nhence quality of answers to a query), it significantly complicates query\nprocessing. To overcome the additional complexity, we develop an approach to\nmap an uncertain relation U with ICs to another uncertain relation U', that\napproximates the set of consistent worlds represented by U. Queries over U can\ninstead be evaluated over U' achieving higher quality (due to reduced\nuncertainty in U') without additional complexity in query processing due to\nICs. We demonstrate the effectiveness and scalability of our approach to large\ndata-sets with complex constraints. We also present experimental results\ndemonstrating the utility of incorporating integrity constraints in uncertain\nrelations, in the context of an information extraction application.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2009 18:45:29 GMT"}], "update_date": "2009-07-10", "authors_parsed": [["Ashish", "Naveen", ""], ["Mehrotra", "Sharad", ""], ["Pirzadeh", "Pouria", ""]]}, {"id": "0907.1728", "submitter": "Tao Zhou", "authors": "Linyuan Lu and Tao Zhou", "title": "Role of Weak Ties in Link Prediction of Complex Networks", "comments": "4 pages, 1 figure and 2 tables. Accepted by CIKM workshop, see\n  http://www.dcs.bbk.ac.uk/~dell/cnikm09/#programme", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plenty of algorithms for link prediction have been proposed and were applied\nto various real networks. Among these works, the weights of links are rarely\ntaken into account. In this paper, we use local similarity indices to estimate\nthe likelihood of the existence of links in weighted networks, including Common\nNeighbor, Adamic-Adar Index, Resource Allocation Index, and their weighted\nversions. In both the unweighted and weighted cases, the resource allocation\nindex performs the best. To our surprise, the weighted indices perform worse,\nwhich reminds us of the well-known Weak Tie Theory. Further extensive\nexperimental study shows that the weak ties play a significant role in the link\nprediction problem, and to emphasize the contribution of weak ties can\nremarkably enhance the predicting accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2009 19:25:20 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2009 17:38:16 GMT"}], "update_date": "2009-08-14", "authors_parsed": [["Lu", "Linyuan", ""], ["Zhou", "Tao", ""]]}, {"id": "0907.1814", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III", "title": "Bayesian Query-Focused Summarization", "comments": null, "journal-ref": "ACL 2006", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BayeSum (for ``Bayesian summarization''), a model for sentence\nextraction in query-focused summarization. BayeSum leverages the common case in\nwhich multiple documents are relevant to a single query. Using these documents\nas reinforcement for query terms, BayeSum is not afflicted by the paucity of\ninformation in short queries. We show that approximate inference in BayeSum is\npossible on large data sets and results in a state-of-the-art summarization\nsystem. Furthermore, we show how BayeSum can be understood as a justified query\nexpansion technique in the language modeling for IR framework.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2009 13:24:55 GMT"}], "update_date": "2009-07-13", "authors_parsed": [["Daum\u00e9", "Hal", "III"]]}, {"id": "0907.2089", "submitter": "Sebastian Maneth", "authors": "A. Arroyuelo, F. Claude, S. Maneth, V. M\\\"akinen, G. Navarro, K.\n  Nguyen, J. Siren, N. V\\\"alim\\\"aki", "title": "Fast In-Memory XPath Search over Compressed Text and Tree Indexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large fraction of an XML document typically consists of text data. The\nXPath query language allows text search via the equal, contains, and\nstarts-with predicates. Such predicates can efficiently be implemented using a\ncompressed self-index of the document's text nodes. Most queries, however,\ncontain some parts of querying the text of the document, plus some parts of\nquerying the tree structure. It is therefore a challenge to choose an\nappropriate evaluation order for a given query, which optimally leverages the\nexecution speeds of the text and tree indexes. Here the SXSI system is\nintroduced; it stores the tree structure of an XML document using a bit array\nof opening and closing brackets, and stores the text nodes of the document\nusing a global compressed self-index. On top of these indexes sits an XPath\nquery engine that is based on tree automata. The engine uses fast counting\nqueries of the text index in order to dynamically determine whether to evaluate\ntop-down or bottom-up with respect to the tree structure. The resulting system\nhas several advantages over existing systems: (1) on pure tree queries (without\ntext search) such as the XPathMark queries, the SXSI system performs on par or\nbetter than the fastest known systems MonetDB and Qizx, (2) on queries that use\ntext search, SXSI outperforms the existing systems by 1--3 orders of magnitude\n(depending on the size of the result set), and (3) with respect to memory\nconsumption, SXSI outperforms all other systems for counting-only queries.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2009 03:19:23 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2011 11:09:37 GMT"}], "update_date": "2011-10-06", "authors_parsed": [["Arroyuelo", "A.", ""], ["Claude", "F.", ""], ["Maneth", "S.", ""], ["M\u00e4kinen", "V.", ""], ["Navarro", "G.", ""], ["Nguyen", "K.", ""], ["Siren", "J.", ""], ["V\u00e4lim\u00e4ki", "N.", ""]]}, {"id": "0907.2209", "submitter": "Andrew Krizhanovsky A", "authors": "A. A. Krizhanovsky, Feiyu Lin", "title": "Related terms search based on WordNet / Wiktionary and its application\n  in Ontology Matching", "comments": "7 pages, 2 tables, 3 figures; In: RCDL 2009. September 17-21,\n  Petrozavodsk, Russia. - pp. 363-369", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  A set of ontology matching algorithms (for finding correspondences between\nconcepts) is based on a thesaurus that provides the source data for the\nsemantic distance calculations. In this wiki era, new resources may spring up\nand improve this kind of semantic search. In the paper a solution of this task\nbased on Russian Wiktionary is compared to WordNet based algorithms. Metrics\nare estimated using the test collection, containing 353 English word pairs with\na relatedness score assigned by human evaluators. The experiment shows that the\nproposed method is capable in principle of calculating a semantic distance\nbetween pair of words in any language presented in Russian Wiktionary. The\ncalculation of Wiktionary based metric had required the development of the\nopen-source Wiktionary parser software.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2009 16:58:22 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2009 16:15:13 GMT"}], "update_date": "2009-10-12", "authors_parsed": [["Krizhanovsky", "A. A.", ""], ["Lin", "Feiyu", ""]]}, {"id": "0907.2268", "submitter": "Martin Klein", "authors": "Martin Klein, Michael L Nelson", "title": "Evaluating Methods to Rediscover Missing Web Pages from the Web\n  Infrastructure", "comments": "10 pages, 11 figures, 5 tables, 40 references, accepted for\n  publication at JCDL 2010 in Brisbane, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing web pages (pages that return the 404 \"Page Not Found\" error) are part\nof the browsing experience. The manual use of search engines to rediscover\nmissing pages can be frustrating and unsuccessful. We compare four automated\nmethods for rediscovering web pages. We extract the page's title, generate the\npage's lexical signature (LS), obtain the page's tags from the bookmarking\nwebsite delicious.com and generate a LS from the page's link neighborhood. We\nuse the output of all methods to query Internet search engines and analyze\ntheir retrieval performance. Our results show that both LSs and titles perform\nfairly well with over 60% URIs returned top ranked from Yahoo!. However, the\ncombination of methods improves the retrieval performance. Considering the\ncomplexity of the LS generation, querying the title first and in case of\ninsufficient results querying the LSs second is the preferable setup. This\ncombination accounts for more than 75% top ranked URIs.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2009 15:47:57 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2010 03:27:17 GMT"}], "update_date": "2010-04-19", "authors_parsed": [["Klein", "Martin", ""], ["Nelson", "Michael L", ""]]}, {"id": "0907.2471", "submitter": "Oktie Hassanzadeh", "authors": "Oktie Hassanzadeh", "title": "Benchmarking Declarative Approximate Selection Predicates", "comments": "75 pages, 7 figures, February 2007, Masters Thesis at University of\n  Toronto", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Declarative data quality has been an active research topic. The fundamental\nprinciple behind a declarative approach to data quality is the use of\ndeclarative statements to realize data quality primitives on top of any\nrelational data source. A primary advantage of such an approach is the ease of\nuse and integration with existing applications. Several similarity predicates\nhave been proposed in the past for common quality primitives (approximate\nselections, joins, etc.) and have been fully expressed using declarative SQL\nstatements. In this thesis, new similarity predicates are proposed along with\ntheir declarative realization, based on notions of probabilistic information\nretrieval. Then, full declarative specifications of previously proposed\nsimilarity predicates in the literature are presented, grouped into classes\naccording to their primary characteristics. Finally, a thorough performance and\naccuracy study comparing a large number of similarity predicates for data\ncleaning operations is performed.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2009 00:40:06 GMT"}], "update_date": "2009-07-16", "authors_parsed": [["Hassanzadeh", "Oktie", ""]]}, {"id": "0907.2868", "submitter": "Thomas Bernecker", "authors": "Thomas Bernecker, Hans-Peter Kriegel, Nikos Mamoulis, Matthias Renz\n  and Andreas Zuefle", "title": "Scalable Probabilistic Similarity Ranking in Uncertain Databases\n  (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a scalable approach for probabilistic top-k similarity\nranking on uncertain vector data. Each uncertain object is represented by a set\nof vector instances that are assumed to be mutually-exclusive. The objective is\nto rank the uncertain data according to their distance to a reference object.\nWe propose a framework that incrementally computes for each object instance and\nranking position, the probability of the object falling at that ranking\nposition. The resulting rank probability distribution can serve as input for\nseveral state-of-the-art probabilistic ranking models. Existing approaches\ncompute this probability distribution by applying a dynamic programming\napproach of quadratic complexity. In this paper we theoretically as well as\nexperimentally show that our framework reduces this to a linear-time complexity\nwhile having the same memory requirements, facilitated by incremental accessing\nof the uncertain vector instances in increasing order of their distance to the\nreference object. Furthermore, we show how the output of our method can be used\nto apply probabilistic top-k ranking for the objects, according to different\nstate-of-the-art definitions. We conduct an experimental evaluation on\nsynthetic and real data, which demonstrates the efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2009 15:25:50 GMT"}], "update_date": "2009-07-17", "authors_parsed": [["Bernecker", "Thomas", ""], ["Kriegel", "Hans-Peter", ""], ["Mamoulis", "Nikos", ""], ["Renz", "Matthias", ""], ["Zuefle", "Andreas", ""]]}, {"id": "0907.3315", "submitter": "Zi-Ke Zhang Mr.", "authors": "Zi-Ke Zhang, Tao Zhou", "title": "Effective Personalized Recommendation in Collaborative Tagging Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, collaborative tagging systems have attracted more and more\nattention and have been widely applied in web systems. Tags provide highly\nabstracted information about personal preferences and item content, and are\ntherefore potential to help in improving better personalized recommendations.\nIn this paper, we propose a tag-based recommendation algorithm considering the\npersonal vocabulary and evaluate it in a real-world dataset: Del.icio.us.\nExperimental results demonstrate that the usage of tag information can\nsignificantly improve the accuracy of personalized recommendations.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2009 18:56:37 GMT"}], "update_date": "2009-07-21", "authors_parsed": [["Zhang", "Zi-Ke", ""], ["Zhou", "Tao", ""]]}, {"id": "0907.3445", "submitter": "Martin Klein", "authors": "Martin Klein, Michael L. Nelson", "title": "Investigating the Change of Web Pages' Titles Over Time", "comments": "6 pages, 8 figures, 1 table, 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inaccessible web pages are part of the browsing experience. The content of\nthese pages however is often not completely lost but rather missing. Lexical\nsignatures (LS) generated from the web pages' textual content have been shown\nto be suitable as search engine queries when trying to discover a (missing) web\npage. Since LSs are expensive to generate, we investigate the potential of web\npages' titles as they are available at a lower cost. We present the results\nfrom studying the change of titles over time. We take titles from copies\nprovided by the Internet Archive of randomly sampled web pages and show the\nfrequency of change as well as the degree of change in terms of the Levenshtein\nscore. We found very low frequencies of change and high Levenshtein scores\nindicating that titles, on average, change little from their original, first\nobserved values (rooted comparison) and even less from the values of their\nprevious observation (sliding).\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2009 16:56:51 GMT"}], "update_date": "2009-07-21", "authors_parsed": [["Klein", "Martin", ""], ["Nelson", "Michael L.", ""]]}, {"id": "0907.3823", "submitter": "Ravindranath Chowdary C Dr.", "authors": "C Ravindranath Chowdary, P Sreenivasa Kumar", "title": "USUM: Update Summary Generation System", "comments": null, "journal-ref": "Advances in Computational Linguistics, Research in Computing\n  Science Vol. 41, 2009, pp. 229-240", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Huge amount of information is present in the World Wide Web and a large\namount is being added to it frequently. A query-specific summary of multiple\ndocuments is very helpful to the user in this context. Currently, few systems\nhave been proposed for query-specific, extractive multi-document summarization.\nIf a summary is available for a set of documents on a given query and if a new\ndocument is added to the corpus, generating an updated summary from the scratch\nis time consuming and many a times it is not practical/possible. In this paper\nwe propose a solution to this problem. This is especially useful in a scenario\nwhere the source documents are not accessible. We cleverly embed the sentences\nof the current summary into the new document and then perform query-specific\nsummary generation on that document. Our experimental results show that the\nperformance of the proposed approach is good in terms of both quality and\nefficiency.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2009 12:00:06 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2009 12:08:31 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Chowdary", "C Ravindranath", ""], ["Kumar", "P Sreenivasa", ""]]}, {"id": "0907.5433", "submitter": "R Doomun", "authors": "Ratnesh Kumar Jain, Dr. R. S. Kasana, Dr. Suresh Jain", "title": "Efficient Web Log Mining using Doubly Linked Tree", "comments": "5 pages, International Journal of Computer Science and Information\n  Security, ISSN 1947 5500, Impact Factor 0.423", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, July 2009, Volume 3. No.1, USA", "doi": null, "report-no": null, "categories": "cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  World Wide Web is a huge data repository and is growing with the explosive\nrate of about 1 million pages a day. As the information available on World Wide\nWeb is growing the usage of the web sites is also growing. Web log records each\naccess of the web page and number of entries in the web logs is increasing\nrapidly. These web logs, when mined properly can provide useful information for\ndecision-making. The designer of the web site, analyst and management\nexecutives are interested in extracting this hidden information from web logs\nfor decision making. Web access pattern, which is the frequently used sequence\nof accesses, is one of the important information that can be mined from the web\nlogs. This information can be used to gather business intelligence to improve\nsales and advertisement, personalization for a user, to analyze system\nperformance and to improve the web site organization. There exist many\ntechniques to mine access patterns from the web logs. This paper describes the\npowerful algorithm that mines the web logs efficiently. Proposed algorithm\nfirstly converts the web access data available in a special doubly linked tree.\nEach access is called an event. This tree keeps the critical mining related\ninformation in very compressed form based on the frequent event count. Proposed\nrecursive algorithm uses this tree to efficiently find all access patterns that\nsatisfy user specified criteria. To prove that our algorithm is efficient from\nthe other GSP (Generalized Sequential Pattern) algorithms we have done\nexperimental studies on sample data.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2009 21:37:46 GMT"}], "update_date": "2009-08-03", "authors_parsed": [["Jain", "Ratnesh Kumar", ""], ["Kasana", "Dr. R. S.", ""], ["Jain", "Dr. Suresh", ""]]}, {"id": "0907.5538", "submitter": "Francesco Carraro", "authors": "Francesco Carraro, Sergio Fonte, Diego Turrini, Maria Cristina De\n  Sanctis, Livia Giacomini", "title": "A preliminary XML-based search system for planetary data", "comments": "10 pages, 5 figures, submitted to Computers and Geosciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR astro-ph.EP cs.DB physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planetary sciences can benefit from several different sources of information,\ni.e. ground-based or near Earth-based observations, space missions and\nlaboratory experiments. The data collected from these sources, however, are\nspread over a number of smaller, separate communities and stored through\ndifferent facilities: this makes it difficult to integrate them. The IDIS\ninitiative, born in the context of the Europlanet project, performed a pilot\nstudy of the viability and the issues to be overcome in order to create an\nintegrated search system for planetary data. As part of the results of such\npilot study, the IDIS Small Bodies and Dust node developed a search system\nbased on a preliminary XML data model. Here we introduce the goals of the IDIS\ninitiative and describe the structure and the working of this search system.\nThe source code of the search system is released under GPL license to allow\npeople interested in participating to the IDIS initiative both as developers\nand as data providers to familiarise with the search environment and to allow\nthe creation of volunteer nodes to be integrated into the existing network.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2009 14:17:34 GMT"}], "update_date": "2009-08-03", "authors_parsed": [["Carraro", "Francesco", ""], ["Fonte", "Sergio", ""], ["Turrini", "Diego", ""], ["De Sanctis", "Maria Cristina", ""], ["Giacomini", "Livia", ""]]}]