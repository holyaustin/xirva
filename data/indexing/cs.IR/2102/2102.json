[{"id": "2102.00141", "submitter": "Abhisek Dash", "authors": "Abhisek Dash, Abhijnan Chakraborty, Saptarshi Ghosh, Animesh\n  Mukherjee, Krishna P. Gummadi", "title": "When the Umpire is also a Player: Bias in Private Label Product\n  Recommendations on E-commerce Marketplaces", "comments": "This work has been accepted for presentation at the ACM Conference on\n  Fairness, Accountability, and Transparency 2021 (ACM FAccT 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic recommendations mediate interactions between millions of\ncustomers and products (in turn, their producers and sellers) on large\ne-commerce marketplaces like Amazon. In recent years, the producers and sellers\nhave raised concerns about the fairness of black-box recommendation algorithms\ndeployed on these marketplaces. Many complaints are centered around\nmarketplaces biasing the algorithms to preferentially favor their own `private\nlabel' products over competitors. These concerns are exacerbated as\nmarketplaces increasingly de-emphasize or replace `organic' recommendations\nwith ad-driven `sponsored' recommendations, which include their own private\nlabels. While these concerns have been covered in popular press and have\nspawned regulatory investigations, to our knowledge, there has not been any\npublic audit of these marketplace algorithms. In this study, we bridge this gap\nby performing an end-to-end systematic audit of related item recommendations on\nAmazon. We propose a network-centric framework to quantify and compare the\nbiases across organic and sponsored related item recommendations. Along a\nnumber of our proposed bias measures, we find that the sponsored\nrecommendations are significantly more biased toward Amazon private label\nproducts compared to organic recommendations. While our findings are primarily\ninteresting to producers and sellers on Amazon, our proposed bias measures are\ngenerally useful for measuring link formation bias in any social or content\nnetworks.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 03:24:38 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 04:24:33 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Dash", "Abhisek", ""], ["Chakraborty", "Abhijnan", ""], ["Ghosh", "Saptarshi", ""], ["Mukherjee", "Animesh", ""], ["Gummadi", "Krishna P.", ""]]}, {"id": "2102.00166", "submitter": "Kaitao Zhang", "authors": "Zhenghao Liu, Kaitao Zhang, Chenyan Xiong, Zhiyuan Liu and Maosong Sun", "title": "OpenMatch: An Open Source Library for Neu-IR Research", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenMatch is a Python-based library that serves for Neural Information\nRetrieval (Neu-IR) research. It provides self-contained neural and traditional\nIR modules, making it easy to build customized and higher-capacity IR systems.\nIn order to develop the advantages of Neu-IR models for users, OpenMatch\nprovides implementations of recent neural IR models, complicated experiment\ninstructions, and advanced few-shot training methods. OpenMatch reproduces\ncorresponding ranking results of previous work on widely-used IR benchmarks,\nliberating users from surplus labor in baseline reimplementation. Our\nOpenMatch-based solutions conduct top-ranked empirical results on various\nranking tasks, such as ad hoc retrieval and conversational retrieval,\nillustrating the convenience of OpenMatch to facilitate building an effective\nIR system. The library, experimental methodologies and results of OpenMatch are\nall publicly available at https://github.com/thunlp/OpenMatch.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 06:47:21 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 03:42:44 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 14:56:54 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Liu", "Zhenghao", ""], ["Zhang", "Kaitao", ""], ["Xiong", "Chenyan", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""]]}, {"id": "2102.00197", "submitter": "Nicola Melluso", "authors": "Nicola Melluso, Andrea Bonaccorsi, Filippo Chiarello, Gualtiero\n  Fantoni", "title": "Rapid detection of fast innovation under the pressure of COVID-19", "comments": "Published in PlOs One in 12/31/2020", "journal-ref": "PLOS ONE (2020) 15(12): e0244175", "doi": "10.1371/journal.pone.0244175", "report-no": null, "categories": "cs.IR econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Covid-19 has rapidly redefined the agenda of technological research and\ndevelopment both for academics and practitioners. If the medical scientific\npublication system has promptly reacted to this new situation, other domains,\nparticularly in new technologies, struggle to map what is happening in their\ncontexts. The pandemic has created the need for a rapid detection of\ntechnological convergence phenomena, but at the same time it has made clear\nthat this task is impossible on the basis of traditional patent and publication\nindicators. This paper presents a novel methodology to perform a rapid\ndetection of the fast technological convergence phenomenon that is occurring\nunder the pressure of the Covid-19 pandemic. The fast detection has been\nperformed thanks to the use of a novel source: the online blogging platform\nMedium. We demonstrate that the hybrid structure of this social journalism\nplatform allows a rapid detection of innovation phenomena, unlike other\ntraditional sources. The technological convergence phenomenon has been modelled\nthrough a network-based approach, analysing the differences of networks\ncomputed during two time periods (pre and post COVID-19). The results led us to\ndiscuss the repurposing of technologies regarding \"Remote Control\", \"Remote\nWorking\", \"Health\" and \"Remote Learning\".\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 09:37:40 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Melluso", "Nicola", ""], ["Bonaccorsi", "Andrea", ""], ["Chiarello", "Filippo", ""], ["Fantoni", "Gualtiero", ""]]}, {"id": "2102.00201", "submitter": "Andres Ferraro", "authors": "Andres Ferraro, Yuntae Kim, Soohyeon Lee, Biho Kim, Namjun Jo, Semi\n  Lim, Suyon Lim, Jungtaek Jang, Sehwan Kim, Xavier Serra, Dmitry Bogdanov", "title": "Melon Playlist Dataset: a public dataset for audio-based playlist\n  generation and music tagging", "comments": "2021 IEEE International Conference on Acoustics, Speech and Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the main limitations in the field of audio signal processing is the\nlack of large public datasets with audio representations and high-quality\nannotations due to restrictions of copyrighted commercial music. We present\nMelon Playlist Dataset, a public dataset of mel-spectrograms for 649,091tracks\nand 148,826 associated playlists annotated by 30,652 different tags. All the\ndata is gathered from Melon, a popular Korean streaming service. The dataset is\nsuitable for music information retrieval tasks, in particular, auto-tagging and\nautomatic playlist continuation. Even though the latter can be addressed by\ncollaborative filtering approaches, audio provides opportunities for research\non track suggestions and building systems resistant to the cold-start problem,\nfor which we provide a baseline. Moreover, the playlists and the annotations\nincluded in the Melon Playlist Dataset make it suitable for metric learning and\nrepresentation learning.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 10:13:10 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ferraro", "Andres", ""], ["Kim", "Yuntae", ""], ["Lee", "Soohyeon", ""], ["Kim", "Biho", ""], ["Jo", "Namjun", ""], ["Lim", "Semi", ""], ["Lim", "Suyon", ""], ["Jang", "Jungtaek", ""], ["Kim", "Sehwan", ""], ["Serra", "Xavier", ""], ["Bogdanov", "Dmitry", ""]]}, {"id": "2102.00230", "submitter": "Raviraj Joshi", "authors": "Pranali Bora, Tulika Awalgaonkar, Himanshu Palve, Raviraj Joshi, Purvi\n  Goel", "title": "ICodeNet -- A Hierarchical Neural Network Approach for Source Code\n  Author Identification", "comments": "Accepted at ICMLC 2021", "journal-ref": null, "doi": "10.1145/3457682.3457709", "report-no": null, "categories": "cs.LG cs.CV cs.IR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the open-source revolution, source codes are now more easily accessible\nthan ever. This has, however, made it easier for malicious users and\ninstitutions to copy the code without giving regards to the license, or credit\nto the original author. Therefore, source code author identification is a\ncritical task with paramount importance. In this paper, we propose ICodeNet - a\nhierarchical neural network that can be used for source code file-level tasks.\nThe ICodeNet processes source code in image format and is employed for the task\nof per file author identification. The ICodeNet consists of an ImageNet trained\nVGG encoder followed by a shallow neural network. The shallow network is based\neither on CNN or LSTM. Different variations of models are evaluated on a source\ncode author classification dataset. We have also compared our image-based\nhierarchical neural network model with simple image-based CNN architecture and\ntext-based CNN and LSTM models to highlight its novelty and efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 14:05:22 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Bora", "Pranali", ""], ["Awalgaonkar", "Tulika", ""], ["Palve", "Himanshu", ""], ["Joshi", "Raviraj", ""], ["Goel", "Purvi", ""]]}, {"id": "2102.00426", "submitter": "Teja Kanchinadam", "authors": "Teja Kanchinadam, Qian You, Keith Westpfahl, James Kim, Siva Gunda,\n  Sebastian Seith, Glenn Fung", "title": "A Simple yet Brisk and Efficient Active Learning Platform for Text\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work, we propose the use of a fully managed machine learning service,\nwhich utilizes active learning to directly build models from unstructured data.\nWith this tool, business users can quickly and easily build machine learning\nmodels and then directly deploy them into a production ready hosted environment\nwithout much involvement from data scientists. Our approach leverages\nstate-of-the-art text representation like OpenAI's GPT2 and a fast\nimplementation of the active learning workflow that relies on a simple\nconstruction of incremental learning using linear models, thus providing a\nbrisk and efficient labeling experience for the users. Experiments on both\npublicly available and real-life insurance datasets empirically show why our\nchoices of simple and fast classification algorithms are ideal for the task at\nhand.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 10:44:04 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Kanchinadam", "Teja", ""], ["You", "Qian", ""], ["Westpfahl", "Keith", ""], ["Kim", "James", ""], ["Gunda", "Siva", ""], ["Seith", "Sebastian", ""], ["Fung", "Glenn", ""]]}, {"id": "2102.00482", "submitter": "Alejandro Bellogin", "authors": "Alejandro Bellog\\'in and Alan Said", "title": "Improving Accountability in Recommender Systems Research Through\n  Reproducibility", "comments": "Submitted in Nov 2020 to the Special Issue on \"Fair, Accountable, and\n  Transparent Recommender Systems\" at User Modeling and User-Adapted\n  Interaction journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducibility is a key requirement for scientific progress. It allows the\nreproduction of the works of others, and, as a consequence, to fully trust the\nreported claims and results. In this work, we argue that, by facilitating\nreproducibility of recommender systems experimentation, we indirectly address\nthe issues of accountability and transparency in recommender systems research\nfrom the perspectives of practitioners, designers, and engineers aiming to\nassess the capabilities of published research works. These issues have become\nincreasingly prevalent in recent literature. Reasons for this include societal\nmovements around intelligent systems and artificial intelligence striving\ntowards fair and objective use of human behavioral data (as in Machine\nLearning, Information Retrieval, or Human-Computer Interaction). Society has\ngrown to expect explanations and transparency standards regarding the\nunderlying algorithms making automated decisions for and around us.\n  This work surveys existing definitions of these concepts, and proposes a\ncoherent terminology for recommender systems research, with the goal to connect\nreproducibility to accountability. We achieve this by introducing several\nguidelines and steps that lead to reproducible and, hence, accountable\nexperimental workflows and research. We additionally analyze several\ninstantiations of recommender system implementations available in the\nliterature, and discuss the extent to which they fit in the introduced\nframework. With this work, we aim to shed light on this important problem, and\nfacilitate progress in the field by increasing the accountability of research.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 16:24:13 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Bellog\u00edn", "Alejandro", ""], ["Said", "Alan", ""]]}, {"id": "2102.00576", "submitter": "Ruolin Wang", "authors": "Ruolin Wang, Zixuan Chen, Mingrui \"Ray\" Zhang, Zhaoheng Li, Zhixiu\n  Liu, Zihan Dang, Chun Yu, Xiang \"Anthony\" Chen", "title": "Revamp: Enhancing Accessible Information Seeking Experience of Online\n  Shopping for Blind or Low Vision Users", "comments": null, "journal-ref": null, "doi": "10.1145/3411764.3445547", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online shopping has become a valuable modern convenience, but blind or low\nvision (BLV) users still face significant challenges using it, because of: 1)\ninadequate image descriptions and 2) the inability to filter large amounts of\ninformation using screen readers. To address those challenges, we propose\nRevamp, a system that leverages customer reviews for interactive information\nretrieval. Revamp is a browser integration that supports review-based\nquestion-answering interactions on a reconstructed product page. From our\ninterview, we identified four main aspects (color, logo, shape, and size) that\nare vital for BLV users to understand the visual appearance of a product. Based\non the findings, we formulated syntactic rules to extract review snippets,\nwhich were used to generate image descriptions and responses to users' queries.\nEvaluations with eight BLV users showed that Revamp 1) provided useful\ndescriptive information for understanding product appearance and 2) helped the\nparticipants locate key information efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 00:53:09 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wang", "Ruolin", ""], ["Chen", "Zixuan", ""], ["Zhang", "Mingrui \"Ray\"", ""], ["Li", "Zhaoheng", ""], ["Liu", "Zhixiu", ""], ["Dang", "Zihan", ""], ["Yu", "Chun", ""], ["Chen", "Xiang \"Anthony\"", ""]]}, {"id": "2102.00627", "submitter": "Lei Li", "authors": "Lei Li, Yongfeng Zhang, Li Chen", "title": "Learning to Explain Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explaining to users why some items are recommended is critical, as it can\nhelp users to make better decisions, increase their satisfaction, and gain\ntheir trust in recommender systems (RS). However, existing explainable RS\nusually consider explanation as a side output of the recommendation model,\nwhich has two problems: (1) it is difficult to evaluate the produced\nexplanations because they are usually model-dependent, and (2) as a result, how\nthe explanations impact the recommendation performance is less investigated.\n  In this paper, explaining recommendations is formulated as a ranking task,\nand learned from data, similar to item ranking for recommendation. This makes\nit possible for standard evaluation of explanations via ranking metrics (e.g.,\nNDCG). Furthermore, we extend traditional item ranking to an item-explanation\njoint-ranking formalization to study if purposely selecting explanations could\nachieve certain learning goals, e.g., improving recommendation performance. A\ngreat challenge, however, is that the sparsity issue in the\nuser-item-explanation data would be inevitably severer than that in traditional\nuser-item interaction data, since not every user-item pair can be associated\nwith all explanations. To mitigate this issue, we propose to perform two sets\nof matrix factorization by considering the ternary relationship as two groups\nof binary relationships. Experiments on three large datasets verify our\nsolution's effectiveness on both item recommendation and explanation ranking.\nTo facilitate the development of explainable RS, we plan to make our code\npublicly available.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 04:29:09 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 09:26:49 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Li", "Lei", ""], ["Zhang", "Yongfeng", ""], ["Chen", "Li", ""]]}, {"id": "2102.00826", "submitter": "Kaibo Cao", "authors": "Kaibo Cao (1), Chunyang Chen (2), Sebastian Baltes (3), Christoph\n  Treude (3), Xiang Chen (4) ((1) Software Institute, Nanjing University,\n  China, (2) Faculty of Information Technology, Monash University, Australia,\n  (3) School of Computer Science, University of Adelaide, Australia, (4) School\n  of Information Science and Technology, Nantong University, China)", "title": "Automated Query Reformulation for Efficient Search based on Query Logs\n  From Stack Overflow", "comments": "13 pages, 6 figures, accepted in ICSE'21: 43rd IEEE/ACM International\n  Conference on Software Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As a popular Q&A site for programming, Stack Overflow is a treasure for\ndevelopers. However, the amount of questions and answers on Stack Overflow make\nit difficult for developers to efficiently locate the information they are\nlooking for. There are two gaps leading to poor search results: the gap between\nthe user's intention and the textual query, and the semantic gap between the\nquery and the post content. Therefore, developers have to constantly\nreformulate their queries by correcting misspelled words, adding limitations to\ncertain programming languages or platforms, etc. As query reformulation is\ntedious for developers, especially for novices, we propose an automated\nsoftware-specific query reformulation approach based on deep learning. With\nquery logs provided by Stack Overflow, we construct a large-scale query\nreformulation corpus, including the original queries and corresponding\nreformulated ones. Our approach trains a Transformer model that can\nautomatically generate candidate reformulated queries when given the user's\noriginal query. The evaluation results show that our approach outperforms five\nstate-of-the-art baselines, and achieves a 5.6% to 33.5% boost in terms of\n$\\mathit{ExactMatch}$ and a 4.8% to 14.4% boost in terms of $\\mathit{GLEU}$.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 13:31:50 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 11:34:08 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Cao", "Kaibo", ""], ["Chen", "Chunyang", ""], ["Baltes", "Sebastian", ""], ["Treude", "Christoph", ""], ["Chen", "Xiang", ""]]}, {"id": "2102.00827", "submitter": "Albert Weichselbraun", "authors": "Albert Weichselbraun, Jakob Steixner, Adrian M.P. Bra\\c{s}oveanu, Arno\n  Scharl, Max G\\\"obel and Lyndon J. B. Nixon", "title": "Automatic Expansion of Domain-Specific Affective Models for Web\n  Intelligence Applications", "comments": "see also https://link.springer.com/article/10.1007/s12559-021-09839-4", "journal-ref": "Cognitive Computation, (2021), 1-18", "doi": "10.1007/s12559-021-09839-4", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentic computing relies on well-defined affective models of different\ncomplexity - polarity to distinguish positive and negative sentiment, for\nexample, or more nuanced models to capture expressions of human emotions. When\nused to measure communication success, even the most granular affective model\ncombined with sophisticated machine learning approaches may not fully capture\nan organisation's strategic positioning goals. Such goals often deviate from\nthe assumptions of standardised affective models. While certain emotions such\nas Joy and Trust typically represent desirable brand associations, specific\ncommunication goals formulated by marketing professionals often go beyond such\nstandard dimensions. For instance, the brand manager of a television show may\nconsider fear or sadness to be desired emotions for its audience. This article\nintroduces expansion techniques for affective models, combining common and\ncommonsense knowledge available in knowledge graphs with language models and\naffective reasoning, improving coverage and consistency as well as supporting\ndomain-specific interpretations of emotions. An extensive evaluation compares\nthe performance of different expansion techniques: (i) a quantitative\nevaluation based on the revisited Hourglass of Emotions model to assess\nperformance on complex models that cover multiple affective categories, using\nmanually compiled gold standard data, and (ii) a qualitative evaluation of a\ndomain-specific affective model for television programme brands. The results of\nthese evaluations demonstrate that the introduced techniques support a variety\nof embeddings and pre-trained models. The paper concludes with a discussion on\napplying this approach to other scenarios where affective model resources are\nscarce.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 13:32:35 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Weichselbraun", "Albert", ""], ["Steixner", "Jakob", ""], ["Bra\u015foveanu", "Adrian M. P.", ""], ["Scharl", "Arno", ""], ["G\u00f6bel", "Max", ""], ["Nixon", "Lyndon J. B.", ""]]}, {"id": "2102.00835", "submitter": "Keping Yu", "authors": "Zhiwei Guo, Keping Yu, Tan Guo, Ali Kashif Bashir, Muhammad Imran,\n  Mohsen Guizani", "title": "Implicit Feedback-based Group Recommender System for Internet of Thing\n  Applications", "comments": "I don't want to share this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the prevalence of Internet of Things (IoT)-based social media\napplications, the distance among people has been greatly shortened. As a\nresult, recommender systems in IoT-based social media need to be developed\noriented to groups of users rather than individual users. However, existing\nmethods were highly dependent on explicit preference feedbacks, ignoring\nscenarios of implicit feedback. To remedy such gap, this paper proposes an\nimplicit feedback-based group recommender system using probabilistic inference\nand non-cooperative game(GREPING) for IoT-based social media. Particularly,\nunknown process variables can be estimated from observable implicit feedbacks\nvia Bayesian posterior probability inference. In addition, the globally optimal\nrecommendation results can be calculated with the aid of non-cooperative game.\nTwo groups of experiments are conducted to assess the GREPING from two aspects:\nefficiency and robustness. Experimental results show obvious promotion and\nconsiderable stability of the GREPING compared to baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 12:33:33 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 16:39:05 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Guo", "Zhiwei", ""], ["Yu", "Keping", ""], ["Guo", "Tan", ""], ["Bashir", "Ali Kashif", ""], ["Imran", "Muhammad", ""], ["Guizani", "Mohsen", ""]]}, {"id": "2102.00976", "submitter": "Yan Wang", "authors": "Yan Wang, Shangde Gao and Wenyu Gao", "title": "Can Predominant Credible Information Suppress Misinformation in Crises?\n  Empirical Studies of Tweets Related to Prevention Measures during COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During COVID-19, misinformation on social media affects the adoption of\nappropriate prevention behaviors. It is urgent to suppress the misinformation\nto prevent negative public health consequences. Although an array of studies\nhas proposed misinformation suppression strategies, few have investigated the\nrole of predominant credible information during crises. None has examined its\neffect quantitatively using longitudinal social media data. Therefore, this\nresearch investigates the temporal correlations between credible information\nand misinformation, and whether predominant credible information can suppress\nmisinformation for two prevention measures (i.e. topics), i.e. wearing masks\nand social distancing using tweets collected from February 15 to June 30, 2020.\nWe trained Support Vector Machine classifiers to retrieve relevant tweets and\nclassify tweets containing credible information and misinformation for each\ntopic. Based on cross-correlation analyses of credible and misinformation time\nseries for both topics, we find that the previously predominant credible\ninformation can lead to the decrease of misinformation (i.e. suppression) with\na time lag. The research findings provide empirical evidence for suppressing\nmisinformation with credible information in complex online environments and\nsuggest practical strategies for future information management during crises\nand emergencies.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 16:59:31 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wang", "Yan", ""], ["Gao", "Shangde", ""], ["Gao", "Wenyu", ""]]}, {"id": "2102.01148", "submitter": "Emanuele Panizzi", "authors": "Marzia Antenore, Jose M. Camacho-Rodriguez, Emanuele Panizzi", "title": "A comparative study of Bot Detection techniques methods with an\n  application related to Covid-19 discourse on Twitter", "comments": "36 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Bot Detection is an essential asset in a period where Online Social\nNetworks(OSN) is a part of our lives. This task becomes more relevant in\ncrises, as the Covid-19 pandemic, where there is an incipient risk of\nproliferation of social bots, producing a possible source of misinformation. In\norder to address this issue, it has been compared different methods to detect\nautomatically social bots on Twitter using Data Selection. The techniques\nutilized to elaborate the bot detection models include the utilization of\nfeatures as the tweets metadata or the Digital Fingerprint of the Twitter\naccounts. In addition, it was analyzed the presence of bots in tweets from\ndifferent periods of the first months of the Covid-19 pandemic, using the bot\ndetection technique which best fits the scope of the task. Moreover, this work\nincludes also analysis over aspects regarding the discourse of bots and humans,\nsuch as sentiment or hashtag utilization.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 20:37:23 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Antenore", "Marzia", ""], ["Camacho-Rodriguez", "Jose M.", ""], ["Panizzi", "Emanuele", ""]]}, {"id": "2102.01156", "submitter": "Despina Christou", "authors": "Despina Christou, Grigorios Tsoumakas", "title": "Improving Distantly-Supervised Relation Extraction through BERT-based\n  Label & Instance Embeddings", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distantly-supervised relation extraction (RE) is an effective method to scale\nRE to large corpora but suffers from noisy labels. Existing approaches try to\nalleviate noise through multi-instance learning and by providing additional\ninformation, but manage to recognize mainly the top frequent relations,\nneglecting those in the long-tail. We propose REDSandT (Relation Extraction\nwith Distant Supervision and Transformers), a novel distantly-supervised\ntransformer-based RE method, that manages to capture a wider set of relations\nthrough highly informative instance and label embeddings for RE, by exploiting\nBERT's pre-trained model, and the relationship between labels and entities,\nrespectively. We guide REDSandT to focus solely on relational tokens by\nfine-tuning BERT on a structured input, including the sub-tree connecting an\nentity pair and the entities' types. Using the extracted informative vectors,\nwe shape label embeddings, which we also use as attention mechanism over\ninstances to further reduce noise. Finally, we represent sentences by\nconcatenating relation and instance embeddings. Experiments in the NYT-10\ndataset show that REDSandT captures a broader set of relations with higher\nconfidence, achieving state-of-the-art AUC (0.424).\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 20:50:24 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Christou", "Despina", ""], ["Tsoumakas", "Grigorios", ""]]}, {"id": "2102.01643", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Kai Wan, Hua Sun, Mingyue Ji, Giuseppe Caire", "title": "A New Design of Cache-aided Multiuser Private Information Retrieval with\n  Uncoded Prefetching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR eess.SP math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the problem of cache-aided multiuser private information retrieval\n(MuPIR), a set of $K_{\\rm u}$ cache-equipped users wish to privately download a\nset of messages from $N$ distributed databases each holding a library of $K$\nmessages. The system works in two phases: {\\it cache placement (prefetching)\nphase} in which the users fill up their cache memory, and {\\it private delivery\nphase} in which the users' demands are revealed and they download an answer\nfrom each database so that the their desired messages can be recovered while\neach individual database learns nothing about the identities of the requested\nmessages. The goal is to design the placement and the private delivery phases\nsuch that the \\emph{load}, which is defined as the total number of downloaded\nbits normalized by the message size, is minimized given any user memory size.\nThis paper considers the MuPIR problem with two messages, arbitrary number of\nusers and databases where uncoded prefetching is assumed, i.e., the users\ndirectly copy some bits from the library as their cached contents. We propose a\nnovel MuPIR scheme inspired by the Maddah-Ali and Niesen (MAN) coded caching\nscheme. The proposed scheme achieves lower load than any existing schemes,\nespecially the product design (PD), and is shown to be optimal within a factor\nof $8$ in general and exactly optimal at very high or low memory regime.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 17:54:42 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 23:33:48 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Zhang", "Xiang", ""], ["Wan", "Kai", ""], ["Sun", "Hua", ""], ["Ji", "Mingyue", ""], ["Caire", "Giuseppe", ""]]}, {"id": "2102.01649", "submitter": "Jinjiang Guo Ph.D.", "authors": "Jinjiang Guo, Jie Li, Dawei Leng and Lurong Pan", "title": "Heterogeneous Graph based Deep Learning for Biomedical Network Link\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-scale biomedical knowledge networks are expanding with emerging\nexperimental technologies that generates multi-scale biomedical big data. Link\nprediction is increasingly used especially in bipartite biomedical networks to\nidentify hidden biological interactions and relationshipts between key entities\nsuch as compounds, targets, gene and diseases. We propose a Graph Neural\nNetworks (GNN) method, namely Graph Pair based Link Prediction model (GPLP),\nfor predicting biomedical network links simply based on their topological\ninteraction information. In GPLP, 1-hop subgraphs extracted from known network\ninteraction matrix is learnt to predict missing links. To evaluate our method,\nthree heterogeneous biomedical networks were used, i.e. Drug-Target Interaction\nnetwork (DTI), Compound-Protein Interaction network (CPI) from NIH Tox21, and\nCompound-Virus Inhibition network (CVI). Our proposed GPLP method significantly\noutperforms over the state-of-the-art baselines. In addition, different network\nincompleteness is analysed with our devised protocol, and we also design an\neffective approach to improve the model robustness towards incomplete networks.\nOur method demonstrates the potential applications in other biomedical\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 07:35:29 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 04:49:10 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 06:56:07 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Guo", "Jinjiang", ""], ["Li", "Jie", ""], ["Leng", "Dawei", ""], ["Pan", "Lurong", ""]]}, {"id": "2102.01662", "submitter": "Anoosheh Heidarzadeh", "authors": "Nahid Esmati, Anoosheh Heidarzadeh, and Alex Sprintson", "title": "Private Linear Transformation: The Individual Privacy Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the single-server Private Linear Transformation (PLT)\nproblem when individual privacy is required. In this problem, there is a user\nthat wishes to obtain $L$ linear combinations of a $D$-subset of messages\nbelonging to a dataset of $K$ messages stored on a single server. The goal is\nto minimize the download cost while keeping the identity of every message\nrequired for the computation individually private. The individual privacy\nrequirement implies that, from the perspective of the server, every message is\nequally likely to belong to the $D$-subset of messages that constitute the\nsupport set of the required linear combinations. We focus on the setting in\nwhich the matrix of coefficients pertaining to the required linear combinations\nis the generator matrix of a Maximum Distance Separable code. We establish\nlower and upper bounds on the capacity of PLT with individual privacy, where\nthe capacity is defined as the supremum of all achievable download rates. We\nshow that our bounds are tight under certain divisibility conditions. In\naddition, we present lower bounds on the capacity of the settings in which the\nuser has a prior side information about a subset of messages.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 18:25:27 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 07:01:59 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Esmati", "Nahid", ""], ["Heidarzadeh", "Anoosheh", ""], ["Sprintson", "Alex", ""]]}, {"id": "2102.01665", "submitter": "Anoosheh Heidarzadeh", "authors": "Nahid Esmati, Anoosheh Heidarzadeh, and Alex Sprintson", "title": "Private Linear Transformation: The Joint Privacy Case", "comments": "arXiv admin note: text overlap with arXiv:2102.01662", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of Private Linear Transformation (PLT). This problem\nincludes a single (or multiple) remote server(s) storing (identical copies of)\n$K$ messages and a user who wants to compute $L$ linear combinations of a\n$D$-subset of these messages by downloading the minimum amount of information\nfrom the server(s) while protecting the privacy of the entire set of $D$\nmessages. This problem generalizes the Private Information Retrieval and\nPrivate Linear Computation problems. In this work, we focus on the\nsingle-server case. For the setting in which the coefficient matrix of the\nrequired $L$ linear combinations generates a Maximum Distance Separable (MDS)\ncode, we characterize the capacity -- defined as the supremum of all achievable\ndownload rates, for all parameters $K, D, L$. In addition, we present lower\nand/or upper bounds on the capacity for the settings with non-MDS coefficient\nmatrices and the settings with a prior side information.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 18:35:51 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 17:10:36 GMT"}], "update_date": "2021-02-07", "authors_parsed": [["Esmati", "Nahid", ""], ["Heidarzadeh", "Anoosheh", ""], ["Sprintson", "Alex", ""]]}, {"id": "2102.01744", "submitter": "Roger Zhe Li", "authors": "Roger Zhe Li, Juli\\'an Urbano, Alan Hanjalic", "title": "Leave No User Behind: Towards Improving the Utility of Recommender\n  Systems for Non-mainstream Users", "comments": "9 pages, 6 figures, accepted to WSDM 2021", "journal-ref": null, "doi": "10.1145/3437963.3441769 10.1145/3437963.3441769 10.1145/3437963.3441769", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a collaborative-filtering recommendation scenario, biases in the data will\nlikely propagate in the learned recommendations. In this paper we focus on the\nso-called mainstream bias: the tendency of a recommender system to provide\nbetter recommendations to users who have a mainstream taste, as opposed to\nnon-mainstream users. We propose NAECF, a conceptually simple but effective\nidea to address this bias. The idea consists of adding an autoencoder (AE)\nlayer when learning user and item representations with text-based Convolutional\nNeural Networks. The AEs, one for the users and one for the items, serve as\nadversaries to the process of minimizing the rating prediction error when\nlearning how to recommend. They enforce that the specific unique properties of\nall users and items are sufficiently well incorporated and preserved in the\nlearned representations. These representations, extracted as the bottlenecks of\nthe corresponding AEs, are expected to be less biased towards mainstream users,\nand to provide more balanced recommendation utility across all users. Our\nexperimental results confirm these expectations, significantly improving the\nrecommendations for non-mainstream users while maintaining the recommendation\nquality for mainstream users. Our results emphasize the importance of deploying\nextensive content-based features, such as online reviews, in order to better\nrepresent users and items to maximize the de-biasing effect.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 20:31:20 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Li", "Roger Zhe", ""], ["Urbano", "Juli\u00e1n", ""], ["Hanjalic", "Alan", ""]]}, {"id": "2102.01868", "submitter": "Yongfeng Zhang", "authors": "Shuyuan Xu, Yingqiang Ge, Yunqi Li, Zuohui Fu, Xu Chen, Yongfeng Zhang", "title": "Causal Collaborative Filtering", "comments": "14 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are important and valuable tools for many personalized\nservices. Collaborative Filtering (CF) algorithms -- among others -- are\nfundamental algorithms driving the underlying mechanism of personalized\nrecommendation. Many of the traditional CF algorithms are designed based on the\nfundamental idea of mining or learning correlative patterns from data for\nmatching, including memory-based methods such as user/item-based CF as well as\nlearning-based methods such as matrix factorization and deep learning models.\nHowever, advancing from correlative learning to causal learning is an important\nproblem, because causal/counterfactual modeling can help us to think outside of\nthe observational data for user modeling and personalization. In this paper, we\npropose Causal Collaborative Filtering (CCF) -- a general framework for\nmodeling causality in collaborative filtering and recommendation. We first\nprovide a unified causal view of CF and mathematically show that many of the\ntraditional CF algorithms are actually special cases of CCF under simplified\ncausal graphs. We then propose a conditional intervention approach for\n$do$-calculus so that we can estimate the causal relations based on\nobservational data. Finally, we further propose a general counterfactual\nconstrained learning framework for estimating the user-item preferences.\nExperiments are conducted on two types of real-world datasets -- traditional\nand randomized trial data -- and results show that our framework can improve\nthe recommendation performance of many CF algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 04:16:11 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 18:31:40 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 16:28:07 GMT"}, {"version": "v4", "created": "Thu, 29 Apr 2021 03:32:42 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Xu", "Shuyuan", ""], ["Ge", "Yingqiang", ""], ["Li", "Yunqi", ""], ["Fu", "Zuohui", ""], ["Chen", "Xu", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2102.01922", "submitter": "Jun Fang", "authors": "Jun Fang", "title": "Session-based Recommendation with Self-Attention Networks", "comments": "12 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Session-based recommendation aims to predict user's next behavior from\ncurrent session and previous anonymous sessions. Capturing long-range\ndependencies between items is a vital challenge in session-based\nrecommendation. A novel approach is proposed for session-based recommendation\nwith self-attention networks (SR-SAN) as a remedy. The self-attention networks\n(SAN) allow SR-SAN capture the global dependencies among all items of a session\nregardless of their distance. In SR-SAN, a single item latent vector is used to\ncapture both current interest and global interest instead of session embedding\nwhich is composed of current interest embedding and global interest embedding.\nSome experiments have been performed on some open benchmark datasets.\nExperimental results show that the proposed method outperforms some\nstate-of-the-arts by comparisons.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 07:50:34 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Fang", "Jun", ""]]}, {"id": "2102.02086", "submitter": "Zahra Ahmadi", "authors": "Patrick Abels, Zahra Ahmadi, Sophie Burkhardt, Benjamin Schiller,\n  Iryna Gurevych, Stefan Kramer", "title": "Focusing Knowledge-based Graph Argument Mining via Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Decision-making usually takes five steps: identifying the problem, collecting\ndata, extracting evidence, identifying pro and con arguments, and making\ndecisions. Focusing on extracting evidence, this paper presents a hybrid model\nthat combines latent Dirichlet allocation and word embeddings to obtain\nexternal knowledge from structured and unstructured data. We study the task of\nsentence-level argument mining, as arguments mostly require some degree of\nworld knowledge to be identified and understood. Given a topic and a sentence,\nthe goal is to classify whether a sentence represents an argument in regard to\nthe topic. We use a topic model to extract topic- and sentence-specific\nevidence from the structured knowledge base Wikidata, building a graph based on\nthe cosine similarity between the entity word vectors of Wikidata and the\nvector of the given sentence. Also, we build a second graph based on\ntopic-specific articles found via Google to tackle the general incompleteness\nof structured knowledge bases. Combining these graphs, we obtain a graph-based\nmodel which, as our evaluation shows, successfully capitalizes on both\nstructured and unstructured data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 14:39:58 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Abels", "Patrick", ""], ["Ahmadi", "Zahra", ""], ["Burkhardt", "Sophie", ""], ["Schiller", "Benjamin", ""], ["Gurevych", "Iryna", ""], ["Kramer", "Stefan", ""]]}, {"id": "2102.02240", "submitter": "Albert Weichselbraun", "authors": "Albert Weichselbraun, Adrian M. P. Brasoveanu, Roger Waldvogel and\n  Fabian Odoni", "title": "Harvest -- An Open Source Toolkit for Extracting Posts and Post Metadata\n  from Web Forums", "comments": "IEEE/WIC/ACM International Joint Conference on Web Intelligence and\n  Intelligent Agent Technology (WI-IAT 2020), Accepted 27 October 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic extraction of forum posts and metadata is a crucial but challenging\ntask since forums do not expose their content in a standardized structure.\nContent extraction methods, therefore, often need customizations such as\nadaptations to page templates and improvements of their extraction code before\nthey can be deployed to new forums. Most of the current solutions are also\nbuilt for the more general case of content extraction from web pages and lack\nkey features important for understanding forum content such as the\nidentification of author metadata and information on the thread structure.\n  This paper, therefore, presents a method that determines the XPath of forum\nposts, eliminating incorrect mergers and splits of the extracted posts that\nwere common in systems from the previous generation. Based on the individual\nposts further metadata such as authors, forum URL and structure are extracted.\nWe also introduce Harvest, a new open source toolkit that implements the\npresented methods and create a gold standard extracted from 52 different Web\nforums for evaluating our approach. A comprehensive evaluation reveals that\nHarvest clearly outperforms competing systems.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 19:20:31 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Weichselbraun", "Albert", ""], ["Brasoveanu", "Adrian M. P.", ""], ["Waldvogel", "Roger", ""], ["Odoni", "Fabian", ""]]}, {"id": "2102.02335", "submitter": "Archita Pathak", "authors": "Archita Pathak, Mohammad Abuzar Shaikh, Rohini Srihari", "title": "Self-Supervised Claim Identification for Automated Fact Checking", "comments": "15 pages, 4 figures, Accepted at ICON 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel, attention-based self-supervised approach to identify\n\"claim-worthy\" sentences in a fake news article, an important first step in\nautomated fact-checking. We leverage \"aboutness\" of headline and content using\nattention mechanism for this task. The identified claims can be used for\ndownstream task of claim verification for which we are releasing a benchmark\ndataset of manually selected compelling articles with veracity labels and\nassociated evidence. This work goes beyond stylistic analysis to identifying\ncontent that influences reader belief. Experiments with three datasets show the\nstrength of our model. Data and code available at\nhttps://github.com/architapathak/Self-Supervised-ClaimIdentification\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 23:37:09 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Pathak", "Archita", ""], ["Shaikh", "Mohammad Abuzar", ""], ["Srihari", "Rohini", ""]]}, {"id": "2102.02478", "submitter": "Faisal Bin Ashraf", "authors": "Md Faisal Ahmed, Zalish Mahmud, Zarin Tasnim Biash, Ahmed Ann Noor\n  Ryen, Arman Hossain, Faisal Bin Ashraf", "title": "Bangla Text Dataset and Exploratory Analysis for Online Harassment\n  Detection", "comments": "3 pages, 5 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Being the seventh most spoken language in the world, the use of the Bangla\nlanguage online has increased in recent times. Hence, it has become very\nimportant to analyze Bangla text data to maintain a safe and harassment-free\nonline place. The data that has been made accessible in this article has been\ngathered and marked from the comments of people in public posts by celebrities,\ngovernment officials, athletes on Facebook. The total amount of collected\ncomments is 44001. The dataset is compiled with the aim of developing the\nability of machines to differentiate whether a comment is a bully expression or\nnot with the help of Natural Language Processing and to what extent it is\nimproper if it is an inappropriate comment. The comments are labeled with\ndifferent categories of harassment. Exploratory analysis from different\nperspectives is also included in this paper to have a detailed overview. Due to\nthe scarcity of data collection of categorized Bengali language comments, this\ndataset can have a significant role for research in detecting bully words,\nidentifying inappropriate comments, detecting different categories of Bengali\nbullies, etc. The dataset is publicly available at\nhttps://data.mendeley.com/datasets/9xjx8twk8p.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 08:35:18 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Ahmed", "Md Faisal", ""], ["Mahmud", "Zalish", ""], ["Biash", "Zarin Tasnim", ""], ["Ryen", "Ahmed Ann Noor", ""], ["Hossain", "Arman", ""], ["Ashraf", "Faisal Bin", ""]]}, {"id": "2102.02511", "submitter": "Matteo Allaix", "authors": "Matteo Allaix, Lukas Holzbaur, Tefjol Pllaha, Camilla Hollanti", "title": "High-Rate Quantum Private Information Retrieval with Weakly Self-Dual\n  Star Product Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR math.IT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical private information retrieval (PIR) setup, a user wants to\nretrieve a file from a database or a distributed storage system (DSS) without\nrevealing the file identity to the servers holding the data. In the quantum PIR\n(QPIR) setting, a user privately retrieves a classical file by receiving\nquantum information from the servers. The QPIR problem has been treated by Song\net al. in the case of replicated servers, both with and without collusion. QPIR\nover $[n,k]$ maximum distance separable (MDS) coded servers was recently\nconsidered by Allaix et al., but the collusion was essentially restricted to\n$t=n-k$ servers in the sense that a smaller $t$ would not improve the retrieval\nrate. In this paper, the QPIR setting is extended to allow for retrieval with\nhigh rate for any number of colluding servers $t$ with $1 \\leq t \\leq n-k$.\nSimilarly to the previous cases, the rates achieved are better than those known\nor conjectured in the classical counterparts, as well as those of the\npreviously proposed coded and colluding QPIR schemes. This is enabled by\nconsidering the stabilizer formalism and weakly self-dual generalized\nReed--Solomon (GRS) star product codes.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 09:44:10 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 16:04:15 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Allaix", "Matteo", ""], ["Holzbaur", "Lukas", ""], ["Pllaha", "Tefjol", ""], ["Hollanti", "Camilla", ""]]}, {"id": "2102.02549", "submitter": "Gongshan He", "authors": "Gongshan He, Dongxing Zhao, Lixin Ding", "title": "Dual-embedding based Neural Collaborative Filtering for Recommender\n  Systems", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among various recommender techniques, collaborative filtering (CF) is the\nmost successful one. And a key problem in CF is how to represent users and\nitems. Previous works usually represent a user (an item) as a vector of latent\nfactors (aka. \\textit{embedding}) and then model the interactions between users\nand items based on the representations. Despite its effectiveness, we argue\nthat it's insufficient to yield satisfactory embeddings for collaborative\nfiltering. Inspired by the idea of SVD++ that represents users based on\nthemselves and their interacted items, we propose a general collaborative\nfiltering framework named DNCF, short for Dual-embedding based Neural\nCollaborative Filtering, to utilize historical interactions to enhance the\nrepresentation. In addition to learning the primitive embedding for a user (an\nitem), we introduce an additional embedding from the perspective of the\ninteracted items (users) to augment the user (item) representation. Extensive\nexperiments on four publicly datasets demonstrated the effectiveness of our\nproposed DNCF framework by comparing its performance with several traditional\nmatrix factorization models and other state-of-the-art deep learning based\nrecommender models.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 11:32:11 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 11:12:04 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["He", "Gongshan", ""], ["Zhao", "Dongxing", ""], ["Ding", "Lixin", ""]]}, {"id": "2102.02636", "submitter": "Hendri Murfi", "authors": "Hendri Murfi, Natasha Rosaline, Nora Hariadi", "title": "Deep Autoencoder-based Fuzzy C-Means for Topic Detection", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Topic detection is a process for determining topics from a collection of\ntextual data. One of the topic detection methods is a clustering-based method,\nwhich assumes that the centroids are topics. The clustering method has the\nadvantage that it can process data with negative representations. Therefore,\nthe clustering method allows a combination with a broader representation\nlearning method. In this paper, we adopt deep learning for topic detection by\nusing a deep autoencoder and fuzzy c-means called deep autoencoder-based fuzzy\nc-means (DFCM). The encoder of the autoencoder performs a lower-dimensional\nrepresentation learning. Fuzzy c-means groups the lower-dimensional\nrepresentation to identify the centroids. The autoencoder's decoder transforms\nback the centroids into the original representation to be interpreted as the\ntopics. Our simulation shows that DFCM improves the coherence score of\neigenspace-based fuzzy c-means (EFCM) and is comparable to the leading standard\nmethods, i.e., nonnegative matrix factorization (NMF) or latent Dirichlet\nallocation (LDA).\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 07:41:52 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Murfi", "Hendri", ""], ["Rosaline", "Natasha", ""], ["Hariadi", "Nora", ""]]}, {"id": "2102.02680", "submitter": "Nguyen Vo", "authors": "Nguyen Vo, Kyumin Lee", "title": "Hierarchical Multi-head Attentive Network for Evidence-aware Fake News\n  Detection", "comments": "EACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The widespread of fake news and misinformation in various domains ranging\nfrom politics, economics to public health has posed an urgent need to\nautomatically fact-check information. A recent trend in fake news detection is\nto utilize evidence from external sources. However, existing evidence-aware\nfake news detection methods focused on either only word-level attention or\nevidence-level attention, which may result in suboptimal performance. In this\npaper, we propose a Hierarchical Multi-head Attentive Network to fact-check\ntextual claims. Our model jointly combines multi-head word-level attention and\nmulti-head document-level attention, which aid explanation in both word-level\nand evidence-level. Experiments on two real-word datasets show that our model\noutperforms seven state-of-the-art baselines. Improvements over baselines are\nfrom 6\\% to 18\\%. Our source code and datasets are released at\n\\texttt{\\url{https://github.com/nguyenvo09/EACL2021}}.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 15:18:44 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Vo", "Nguyen", ""], ["Lee", "Kyumin", ""]]}, {"id": "2102.02922", "submitter": "Osman Din", "authors": "Osman Din", "title": "Towards a Flexible System Architecture for Automated Knowledge Base\n  Construction Frameworks", "comments": null, "journal-ref": "2019 IEEE International Conference on Big Data (Big Data),\n  3066-3071", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although knowledge bases play an important role in many domains (including in\narchives, where they are sometimes used for entity extraction and semantic\nannotation tasks), it is challenging to build knowledge bases by hand. This is\nowing to a number of factors: Knowledge bases must be accurate, up-to-date,\ncomprehensive, and as flexible and as efficient as possible. These requirements\nmean a large undertaking, in the form of extensive work by subject matter\nexperts (such as scientists, programmers, archivists, and other information\nprofessionals). Even when successfully engineered, manually built knowledge\nbases are typically one-off, use-case-specific, non-standardized,\nhard-to-maintain solutions. We present a scalable, flexible, and extensible\narchitecture for knowledge base construction frameworks. As a work in progress,\nwe extend a specific framework to address some of its design limitations. The\ncontributions presented in this short paper can shed a light on the suitability\nof using AKBC frameworks for computational use cases in this domain and provide\nfuture directions for building improved AKBC frameworks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 22:45:05 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Din", "Osman", ""]]}, {"id": "2102.02964", "submitter": "Motohiro Sunouchi", "authors": "Motohiro Sunouchi and Masaharu Yoshioka", "title": "Diversity-Robust Acoustic Feature Signatures Based on Multiscale Fractal\n  Dimension for Similarity Search of Environmental Sounds", "comments": "15 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes new acoustic feature signatures based on the multiscale\nfractal dimension (MFD), which are robust against the diversity of\nenvironmental sounds, for the content-based similarity search. The diversity of\nsound sources and acoustic compositions is a typical feature of environmental\nsounds. Several acoustic features have been proposed for environmental sounds.\nAmong them is the widely-used Mel-Frequency Cepstral Coefficients (MFCCs),\nwhich describes frequency-domain features. However, in addition to these\nfeatures in the frequency domain, environmental sounds have other important\nfeatures in the time domain with various time scales. In our previous paper, we\nproposed enhanced multiscale fractal dimension signature (EMFD) for\nenvironmental sounds. This paper extends EMFD by using the kernel density\nestimation method, which results in better performance of the similarity search\ntasks. Furthermore, it newly proposes another acoustic feature signature based\non MFD, namely very-long-range multiscale fractal dimension signature (MFD-VL).\nThe MFD-VL signature describes several features of the time-varying envelope\nfor long periods of time. The MFD-VL signature has stability and robustness\nagainst background noise and small fluctuations in the parameters of sound\nsources, which are produced in field recordings. We discuss the effectiveness\nof these signatures in the similarity sound search by comparing with acoustic\nfeatures proposed in the DCASE 2018 challenges. Due to the unique\ndescriptiveness of our proposed signatures, we confirmed the signatures are\neffective when they are used with other acoustic features.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 02:37:21 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 05:51:35 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Sunouchi", "Motohiro", ""], ["Yoshioka", "Masaharu", ""]]}, {"id": "2102.02995", "submitter": "Haozhen Zhao", "authors": "Christian J. Mahoney, Katie Jensen, Fusheng Wei, Haozhen Zhao, Han\n  Qin, Shi Ye", "title": "Application of Deep Learning in Recognizing Bates Numbers and\n  Confidentiality Stamping from Images", "comments": "2020 IEEE International Conference on Big Data (Big Data)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In eDiscovery, it is critical to ensure that each page produced in legal\nproceedings conforms with the requirements of court or government agency\nproduction requests. Errors in productions could have severe consequences in a\ncase, putting a party in an adverse position. The volume of pages produced\ncontinues to increase, and tremendous time and effort has been taken to ensure\nquality control of document productions. This has historically been a manual\nand laborious process. This paper demonstrates a novel automated production\nquality control application which leverages deep learning-based image\nrecognition technology to extract Bates Number and Confidentiality Stamping\nfrom legal case production images and validate their correctness. Effectiveness\nof the method is verified with an experiment using a real-world production\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 04:33:58 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Mahoney", "Christian J.", ""], ["Jensen", "Katie", ""], ["Wei", "Fusheng", ""], ["Zhao", "Haozhen", ""], ["Qin", "Han", ""], ["Ye", "Shi", ""]]}, {"id": "2102.03062", "submitter": "Thomas \\\"Ubellacker", "authors": "Jonas Thiergart, Stefan Huber, Thomas \\\"Ubellacker", "title": "Understanding Emails and Drafting Responses -- An Approach Using GPT-3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Providing computer systems with the ability to understand and generate\nnatural language has long been a challenge of engineers. Recent progress in\nnatural language processing (NLP), like the GPT-3 language model released by\nOpenAI, has made both possible to an extent. In this paper, we explore the\npossibility of rationalising email communication using GPT-3. First, we\ndemonstrate the technical feasibility of understanding incoming emails and\ngenerating responses, drawing on literature from the disciplines of software\nengineering as well as data science. Second, we apply knowledge from both\nbusiness studies and, again, software engineering to identify ways to tackle\nchallenges we encountered. Third, we argue for the economic viability of such a\nsolution by analysing costs and market demand. We conclude that applying GPT-3\nto rationalising email communication is feasible both technically and\neconomically.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 08:56:42 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 15:15:38 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 11:11:32 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Thiergart", "Jonas", ""], ["Huber", "Stefan", ""], ["\u00dcbellacker", "Thomas", ""]]}, {"id": "2102.03089", "submitter": "Xi Wang", "authors": "Xi Wang, Iadh Ounis, Craig Macdonald", "title": "Leveraging Review Properties for Effective Recommendation", "comments": "To be published in the International World Wide Web Conference (WWW)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the-art recommendation systems leverage explicit item reviews\nposted by users by considering their usefulness in representing the users'\npreferences and describing the items' attributes. These posted reviews may have\nvarious associated properties, such as their length, their age since they were\nposted, or their item rating. However, it remains unclear how these different\nreview properties contribute to the usefulness of their corresponding reviews\nin addressing the recommendation task. In particular, users show distinct\npreferences when considering different aspects of the reviews (i.e. properties)\nfor making decisions about the items. Hence, it is important to model the\nrelationship between the reviews' properties and the usefulness of reviews\nwhile learning the users' preferences and the items' attributes. Therefore, we\npropose to model the reviews with their associated available properties. We\nintroduce a novel review properties-based recommendation model (RPRM) that\nlearns which review properties are more important than others in capturing the\nusefulness of reviews, thereby enhancing the recommendation results.\nFurthermore, inspired by the users' information adoption framework, we\nintegrate two loss functions and a negative sampling strategy into our proposed\nRPRM model, to ensure that the properties of reviews are correlated with the\nusers' preferences. We examine the effectiveness of RPRM using the well-known\nYelp and Amazon datasets. Our results show that RPRM significantly outperforms\na classical and five state-of-the-art baselines. Moreover, we experimentally\nshow the advantages of using our proposed loss functions and negative sampling\nstrategy, which further enhance the recommendation performances of RPRM.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 10:26:24 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Wang", "Xi", ""], ["Ounis", "Iadh", ""], ["Macdonald", "Craig", ""]]}, {"id": "2102.03135", "submitter": "Jinbo Song", "authors": "Jinbo Song and Chao Chang and Fei Sun and Zhenyang Chen and Guoyong Hu\n  and Peng Jiang", "title": "Graph Attention Collaborative Similarity Embedding for Recommender\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Graph Attention Collaborative Similarity Embedding (GACSE), a new\nrecommendation framework that exploits collaborative information in the\nuser-item bipartite graph for representation learning. Our framework consists\nof two parts: the first part is to learn explicit graph collaborative filtering\ninformation such as user-item association through embedding propagation with\nattention mechanism, and the second part is to learn implicit graph\ncollaborative information such as user-user similarities and item-item\nsimilarities through auxiliary loss. We design a new loss function that\ncombines BPR loss with adaptive margin and similarity loss for the similarities\nlearning. Extensive experiments on three benchmarks show that our model is\nconsistently better than the latest state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 12:26:43 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Song", "Jinbo", ""], ["Chang", "Chao", ""], ["Sun", "Fei", ""], ["Chen", "Zhenyang", ""], ["Hu", "Guoyong", ""], ["Jiang", "Peng", ""]]}, {"id": "2102.03237", "submitter": "Jinseok Kim", "authors": "Jinseok Kim and Jason Owen-Smith", "title": "ORCID-linked labeled data for evaluating author name disambiguation at\n  scale", "comments": "A pre-print of a paper accepted for publication in the journal\n  Scientometrics", "journal-ref": null, "doi": "10.1007/s11192-020-03826-6", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How can we evaluate the performance of a disambiguation method implemented on\nbig bibliographic data? This study suggests that the open researcher profile\nsystem, ORCID, can be used as an authority source to label name instances at\nscale. This study demonstrates the potential by evaluating the disambiguation\nperformances of Author-ity2009 (which algorithmically disambiguates author\nnames in MEDLINE) using 3 million name instances that are automatically labeled\nthrough linkage to 5 million ORCID researcher profiles. Results show that\nalthough ORCID-linked labeled data do not effectively represent the population\nof name instances in Author-ity2009, they do effectively capture the 'high\nprecision over high recall' performances of Author-ity2009. In addition,\nORCID-linked labeled data can provide nuanced details about the\nAuthor-ity2009's performance when name instances are evaluated within and\nacross ethnicity categories. As ORCID continues to be expanded to include more\nresearchers, labeled data via ORCID-linkage can be improved in representing the\npopulation of a whole disambiguated data and updated on a regular basis. This\ncan benefit author name disambiguation researchers and practitioners who need\nlarge-scale labeled data but lack resources for manual labeling or access to\nother authority sources for linkage-based labeling. The ORCID-linked labeled\ndata for Author-tiy2009 are publicly available for validation and reuse.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 15:34:08 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Kim", "Jinseok", ""], ["Owen-Smith", "Jason", ""]]}, {"id": "2102.03250", "submitter": "Jinseok Kim", "authors": "Jinseok Kim and Jenna Kim", "title": "Effect of forename string on author name disambiguation", "comments": "25 pages", "journal-ref": "Journal of the Association for Information Science and Technology,\n  71(7), 839-855 (2020)", "doi": "10.1002/asi.24298", "report-no": null, "categories": "cs.DL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In author name disambiguation, author forenames are used to decide which name\ninstances are disambiguated together and how much they are likely to refer to\nthe same author. Despite such a crucial role of forenames, their effect on the\nperformances of heuristic (string matching) and algorithmic disambiguation is\nnot well understood. This study assesses the contributions of forenames in\nauthor name disambiguation using multiple labeled datasets under varying ratios\nand lengths of full forenames, reflecting real-world scenarios in which an\nauthor is represented by forename variants (synonym) and some authors share the\nsame forenames (homonym). Results show that increasing the ratios of full\nforenames improves substantially the performances of both heuristic and\nmachine-learning-based disambiguation. Performance gains by algorithmic\ndisambiguation are pronounced when many forenames are initialized or homonym is\nprevalent. As the ratios of full forenames increase, however, they become\nmarginal compared to the performances by string matching. Using a small portion\nof forename strings does not reduce much the performances of both heuristic and\nalgorithmic disambiguation compared to using full-length strings. These\nfindings provide practical suggestions such as restoring initialized forenames\ninto a full-string format via record linkage for improved disambiguation\nperformances.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 15:54:11 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Kim", "Jinseok", ""], ["Kim", "Jenna", ""]]}, {"id": "2102.03251", "submitter": "Jinseok Kim", "authors": "Jinseok Kim", "title": "A fast and integrative algorithm for clustering performance evaluation\n  in author name disambiguation", "comments": "20 pages", "journal-ref": "Scientometrics, 120(2), 661-681 (2019)", "doi": "10.1007/s11192-019-03143-7", "report-no": null, "categories": "cs.DL cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Author name disambiguation results are often evaluated by measures such as\nCluster-F, K-metric, Pairwise-F, Splitting & Lumping Error, and B-cubed.\nAlthough these measures have distinctive evaluation schemes, this paper shows\nthat they can be calculated in a single framework by a set of common steps that\ncompare truth and predicted clusters through two hash tables recording\ninformation about name instances with their predicted cluster indices and\nfrequencies of those indices per truth cluster. This integrative calculation\nreduces greatly calculation runtime, which is scalable to a clustering task\ninvolving millions of name instances within a few seconds. During the\nintegration process, B-cubed and K-metric are shown to produce the same\nprecision and recall scores. In this framework, especially, name instance pairs\nfor Pairwise-F are counted using a heuristic, surpassing a state-of-the-art\nalgorithm in speedy calculation. Details of the integrative calculation are\ndescribed with examples and pseudo-code to assist scholars to implement each\nmeasure easily and validate the correctness of implementation. The integrative\ncalculation will help scholars compare similarities and differences of multiple\nmeasures before they select ones that characterize best the clustering\nperformances of their disambiguation methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 15:54:49 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Kim", "Jinseok", ""]]}, {"id": "2102.03265", "submitter": "Alireza Gharahighehi", "authors": "Alireza Gharahighehi and Celine Vens", "title": "Diversification in Session-based News Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommender systems are widely applied in digital platforms such as news\nwebsites to personalize services based on user preferences. In news websites\nmost of users are anonymous and the only available data is sequences of items\nin anonymous sessions. Due to this, typical collaborative filtering methods,\nwhich are highly applied in many applications, are not effective in news\nrecommendations. In this context, session-based recommenders are able to\nrecommend next items given the sequence of previous items in the active\nsession. Neighborhood-based session-based recommenders has been shown to be\nhighly effective compared to more sophisticated approaches. In this study we\npropose scenarios to make these session-based recommender systems\ndiversity-aware and to address the filter bubble phenomenon. The filter bubble\nphenomenon is a common concern in news recommendation systems and it occurs\nwhen the system narrows the information and deprives users of diverse\ninformation. The results of applying the proposed scenarios show that these\ndiversification scenarios improve the diversity measures in these session-based\nrecommender systems based on four news datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 16:12:52 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Gharahighehi", "Alireza", ""], ["Vens", "Celine", ""]]}, {"id": "2102.03272", "submitter": "Jinseok Kim", "authors": "Jinseok Kim, Jinmo Kim, and Jason Owen-Smith", "title": "Generating automatically labeled data for author name disambiguation: An\n  iterative clustering method", "comments": "25 pages", "journal-ref": "Scientometrics, 118(1), 253-280 (2019)", "doi": "10.1007/s11192-018-2968-3", "report-no": null, "categories": "cs.DL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To train algorithms for supervised author name disambiguation, many studies\nhave relied on hand-labeled truth data that are very laborious to generate.\nThis paper shows that labeled training data can be automatically generated\nusing information features such as email address, coauthor names, and cited\nreferences that are available from publication records. For this purpose,\nhigh-precision rules for matching name instances on each feature are decided\nusing an external-authority database. Then, selected name instances in target\nambiguous data go through the process of pairwise matching based on the rules.\nNext, they are merged into clusters by a generic entity resolution algorithm.\nThe clustering procedure is repeated over other features until further merging\nis impossible. Tested on 26,566 instances out of the population of 228K author\nname instances, this iterative clustering produced accurately labeled data with\npairwise F1 = 0.99. The labeled data represented the population data in terms\nof name ethnicity and co-disambiguating name group size distributions. In\naddition, trained on the labeled data, machine learning algorithms\ndisambiguated 24K names in test data with performance of pairwise F1 = 0.90 ~\n0.92. Several challenges are discussed for applying this method to resolving\nauthor name ambiguity in large-scale scholarly data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 16:24:25 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Kim", "Jinseok", ""], ["Kim", "Jinmo", ""], ["Owen-Smith", "Jason", ""]]}, {"id": "2102.03419", "submitter": "Dora Jambor", "authors": "Dora Jambor, Komal Teru, Joelle Pineau, William L. Hamilton", "title": "Exploring the Limits of Few-Shot Link Prediction in Knowledge Graphs", "comments": "code available at\n  https://github.com/dorajam/few-shot-link-prediction-paper", "journal-ref": "European Chapter of the ACL (EACL), 2021", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world knowledge graphs are often characterized by low-frequency\nrelations - a challenge that has prompted an increasing interest in few-shot\nlink prediction methods. These methods perform link prediction for a set of new\nrelations, unseen during training, given only a few example facts of each\nrelation at test time. In this work, we perform a systematic study on a\nspectrum of models derived by generalizing the current state of the art for\nfew-shot link prediction, with the goal of probing the limits of learning in\nthis few-shot setting. We find that a simple zero-shot baseline - which ignores\nany relation-specific information - achieves surprisingly strong performance.\nMoreover, experiments on carefully crafted synthetic datasets show that having\nonly a few examples of a relation fundamentally limits models from using\nfine-grained structural information and only allows for exploiting the\ncoarse-grained positional information of entities. Together, our findings\nchallenge the implicit assumptions and inductive biases of prior work and\nhighlight new directions for research in this area.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 21:04:31 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Jambor", "Dora", ""], ["Teru", "Komal", ""], ["Pineau", "Joelle", ""], ["Hamilton", "William L.", ""]]}, {"id": "2102.03525", "submitter": "Hao Lei", "authors": "Hao Lei and Ying Chen", "title": "Exclusive Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose an Exclusive Topic Modeling (ETM) for unsupervised text\nclassification, which is able to 1) identify the field-specific keywords though\nless frequently appeared and 2) deliver well-structured topics with exclusive\nwords. In particular, a weighted Lasso penalty is imposed to reduce the\ndominance of the frequently appearing yet less relevant words automatically,\nand a pairwise Kullback-Leibler divergence penalty is used to implement topics\nseparation. Simulation studies demonstrate that the ETM detects the\nfield-specific keywords, while LDA fails. When applying to the benchmark NIPS\ndataset, the topic coherence score on average improves by 22% and 10% for the\nmodel with weighted Lasso penalty and pairwise Kullback-Leibler divergence\npenalty, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 07:03:15 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Lei", "Hao", ""], ["Chen", "Ying", ""]]}, {"id": "2102.03577", "submitter": "Zhi Zheng", "authors": "Zhi Zheng, Chao Wang, Tong Xu, Dazhong Shen, Penggang Qin, Baoxing\n  Huai, Tongzhu Liu, Enhong Chen", "title": "Drug Package Recommendation via Interaction-aware Graph Induction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the rapid accumulation of massive electronic\nmedical records (EMRs), which highly support the intelligent medical services\nsuch as drug recommendation. However, prior arts mainly follow the traditional\nrecommendation strategies like collaborative filtering, which usually treat\nindividual drugs as mutually independent, while the latent interactions among\ndrugs, e.g., synergistic or antagonistic effect, have been largely ignored. To\nthat end, in this paper, we target at developing a new paradigm for drug\npackage recommendation with considering the interaction effect within drugs, in\nwhich the interaction effects could be affected by patient conditions.\nSpecifically, we first design a pre-training method based on neural\ncollaborative filtering to get the initial embedding of patients and drugs.\nThen, the drug interaction graph will be initialized based on medical records\nand domain knowledge. Along this line, we propose a new Drug Package\nRecommendation (DPR) framework with two variants, respectively DPR on Weighted\nGraph (DPR-WG) and DPR on Attributed Graph (DPR-AG) to solve the problem, in\nwhich each the interactions will be described as signed weights or attribute\nvectors. In detail, a mask layer is utilized to capture the impact of patient\ncondition, and graph neural networks (GNNs) are leveraged for the final graph\ninduction task to embed the package. Extensive experiments on a real-world data\nset from a first-rate hospital demonstrate the effectiveness of our DPR\nframework compared with several competitive baseline methods, and further\nsupport the heuristic study for the drug package generation task with adequate\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 12:51:00 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Zheng", "Zhi", ""], ["Wang", "Chao", ""], ["Xu", "Tong", ""], ["Shen", "Dazhong", ""], ["Qin", "Penggang", ""], ["Huai", "Baoxing", ""], ["Liu", "Tongzhu", ""], ["Chen", "Enhong", ""]]}, {"id": "2102.03670", "submitter": "Dylan Bates", "authors": "Dylan Bates", "title": "Recommending More Efficient Workflows to Software Developers", "comments": "Paper accepted at SPLASH '14: Conference on Systems, Programming, and\n  Applications: Software for Humanity, Student Research Competition, October\n  2014, Portland, OR., 2 pages", "journal-ref": null, "doi": "10.1145/3251508", "report-no": null, "categories": "cs.SE cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Existing recommendation systems can help developers improve their software\ndevelopment abilities by recommending new programming tools, such as a\nrefactoring tool or a program navigation tool. However, simply recommending\ntools in isolation may not, in and of itself, allow developers to successfully\ncomplete their tasks. In this paper, I introduce a new recommendation system\nthat recommends workflows, or sequences of tools, to developers. By learning\nmore efficient workflows, the system could make software developers more\nefficient.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 21:43:40 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Bates", "Dylan", ""]]}, {"id": "2102.03674", "submitter": "Amy Nesky", "authors": "Amy Nesky and Quentin F. Stout", "title": "Generating Artificial Core Users for Interpretable Condensed Data", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent work has shown that in a dataset of user ratings on items there exists\na group of Core Users who hold most of the information necessary for\nrecommendation. This set of Core Users can be as small as 20 percent of the\nusers. Core Users can be used to make predictions for out-of-sample users\nwithout much additional work. Since Core Users substantially shrink a ratings\ndataset without much loss of information, they can be used to improve\nrecommendation efficiency. We propose a method, combining latent factor models,\nensemble boosting and K-means clustering, to generate a small set of Artificial\nCore Users (ACUs) from real Core User data. Our ACUs have dense rating\ninformation, and improve the recommendation performance of real Core Users\nwhile remaining interpretable.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 21:53:37 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Nesky", "Amy", ""], ["Stout", "Quentin F.", ""]]}, {"id": "2102.03692", "submitter": "Yifan Hu", "authors": "Yifan Hu, Changwei Hu, Thanh Tran, Tejaswi Kasturi, Elizabeth Joseph,\n  Matt Gillingham", "title": "What's in a Name? -- Gender Classification of Names with Character Based\n  Machine Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gender information is no longer a mandatory input when registering for an\naccount at many leading Internet companies. However, prediction of demographic\ninformation such as gender and age remains an important task, especially in\nintervention of unintentional gender/age bias in recommender systems. Therefore\nit is necessary to infer the gender of those users who did not to provide this\ninformation during registration. We consider the problem of predicting the\ngender of registered users based on their declared name. By analyzing the first\nnames of 100M+ users, we found that genders can be very effectively classified\nusing the composition of the name strings. We propose a number of character\nbased machine learning models, and demonstrate that our models are able to\ninfer the gender of users with much higher accuracy than baseline models.\nMoreover, we show that using the last names in addition to the first names\nimproves classification performance further.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 01:01:32 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Hu", "Yifan", ""], ["Hu", "Changwei", ""], ["Tran", "Thanh", ""], ["Kasturi", "Tejaswi", ""], ["Joseph", "Elizabeth", ""], ["Gillingham", "Matt", ""]]}, {"id": "2102.03749", "submitter": "Somil Gupta", "authors": "Somil Gupta and Neeraj Sharma", "title": "Role of Attentive History Selection in Conversational Information\n  Seeking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rise of intelligent assistant systems like Siri and Alexa have led to the\nemergence of Conversational Search, a research track of Information Retrieval\n(IR) that involves interactive and iterative information-seeking user-system\ndialog. Recently released OR-QuAC and TCAsT19 datasets narrow their research\nfocus on the retrieval aspect of conversational search i.e. fetching the\nrelevant documents (passages) from a large collection using the conversational\nsearch history. Currently proposed models for these datasets incorporate\nhistory in retrieval by appending the last N turns to the current question\nbefore encoding. We propose to use another history selection approach that\ndynamically selects and weighs history turns using the attention mechanism for\nquestion embedding. The novelty of our approach lies in experimenting with soft\nattention-based history selection approach in an open-retrieval setting.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 09:06:35 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Gupta", "Somil", ""], ["Sharma", "Neeraj", ""]]}, {"id": "2102.03787", "submitter": "Ruobing Xie", "authors": "Ruobing Xie, Qi Liu, Shukai Liu, Ziwei Zhang, Peng Cui, Bo Zhang, Leyu\n  Lin", "title": "Improving Accuracy and Diversity in Matching of Recommendation with\n  Diversified Preference Network", "comments": "11 pages, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, real-world recommendation systems need to deal with millions of\ncandidates. It is extremely challenging to conduct sophisticated end-to-end\nalgorithms on the entire corpus due to the tremendous computation costs.\nTherefore, conventional recommendation systems usually contain two modules. The\nmatching module focuses on the coverage, which aims to efficiently retrieve\nhundreds of items from large corpora, while the ranking module generates\nspecific ranks for these items. Recommendation diversity is an essential factor\nthat impacts user experience. Most efforts have explored recommendation\ndiversity in ranking, while the matching module should take more responsibility\nfor diversity. In this paper, we propose a novel Heterogeneous graph neural\nnetwork framework for diversified recommendation (GraphDR) in matching to\nimprove both recommendation accuracy and diversity. Specifically, GraphDR\nbuilds a huge heterogeneous preference network to record different types of\nuser preferences, and conduct a field-level heterogeneous graph attention\nnetwork for node aggregation. We also innovatively conduct a\nneighbor-similarity based loss to balance both recommendation accuracy and\ndiversity for the diversified matching task. In experiments, we conduct\nextensive online and offline evaluations on a real-world recommendation system\nwith various accuracy and diversity metrics and achieve significant\nimprovements. We also conduct model analyses and case study for a better\nunderstanding of our model. Moreover, GraphDR has been deployed on a well-known\nrecommendation system, which affects millions of users. The source code will be\nreleased.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 12:14:18 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Xie", "Ruobing", ""], ["Liu", "Qi", ""], ["Liu", "Shukai", ""], ["Zhang", "Ziwei", ""], ["Cui", "Peng", ""], ["Zhang", "Bo", ""], ["Lin", "Leyu", ""]]}, {"id": "2102.03848", "submitter": "arXiv Admin", "authors": "Mohamed A. Hamada and Abdelrahman Abdallah", "title": "Estimate The Efficiency Of Multiprocessor's Cash Memory Work Algorithms", "comments": "This article has been withdrawn by arXiv administrators due to\n  disputed authorship", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IR cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many computer systems for calculating the proper organization of memory are\namong the most critical issues. Using a tier cache memory (along with branching\nprediction) is an effective means of increasing modern multi-core processors'\nperformance. Designing high-performance processors is a complex task and\nrequires preliminary verification and analysis of the model level, usually used\nin analytical and simulation modeling. The refinement of extreme programming is\nan unfortunate challenge. Few experts disagree with the synthesis of access\npoints. This article demonstrates that Internet QoS and 16-bit architectures\nare always incompatible, but it's the same situation for write-back caches. The\nsolution to this problem can be implemented by analyzing simulation models of\ndifferent complexity in combination with the analytical evaluation of\nindividual algorithms. This work is devoted to designing a multi-parameter\nsimulation model of a multi-process for evaluating the performance of cache\nmemory algorithms and the optimality of the structure. Optimization of the\nstructures and algorithms of the cache memory allows you to accelerate the\ninteraction of the memory process and improve the performance of the entire\nsystem.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 17:02:36 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 20:08:21 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Hamada", "Mohamed A.", ""], ["Abdallah", "Abdelrahman", ""]]}, {"id": "2102.04033", "submitter": "Shiyao Wang", "authors": "Shiyao Wang, Qi Liu, Tiezheng Ge, Defu Lian and Zhiqiang Zhang", "title": "A Hybrid Bandit Model with Visual Priors for Creative Ranking in Display\n  Advertising", "comments": "To be published in the International World Wide Web Conference (WWW)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creative plays a great important role in e-commerce for exhibiting products.\nSellers usually create multiple creatives for comprehensive demonstrations,\nthus it is crucial to display the most appealing design to maximize the\nClick-Through Rate~(CTR). For this purpose, modern recommender systems\ndynamically rank creatives when a product is proposed for a user. However, this\ntask suffers more cold-start problem than conventional products recommendation\nIn this paper, we propose a hybrid bandit model with visual priors which first\nmakes predictions with a visual evaluation, and then naturally evolves to focus\non the specialities through the hybrid bandit model. Our contributions are\nthree-fold: 1) We present a visual-aware ranking model (called VAM) that\nincorporates a list-wise ranking loss for ordering the creatives according to\nthe visual appearance. 2) Regarding visual evaluations as a prior, the hybrid\nbandit model (called HBM) is proposed to evolve consistently to make better\nposteriori estimations by taking more observations into consideration for\nonline scenarios. 3) A first large-scale creative dataset, CreativeRanking, is\nconstructed, which contains over 1.7M creatives of 500k products as well as\ntheir real impression and click data. Extensive experiments have also been\nconducted on both our dataset and public Mushroom dataset, demonstrating the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 07:11:20 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 08:24:00 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Wang", "Shiyao", ""], ["Liu", "Qi", ""], ["Ge", "Tiezheng", ""], ["Lian", "Defu", ""], ["Zhang", "Zhiqiang", ""]]}, {"id": "2102.04095", "submitter": "Yingtao Luo", "authors": "Yingtao Luo, Qiang Liu, Zhaocheng Liu", "title": "STAN: Spatio-Temporal Attention Network for Next Location Recommendation", "comments": "Accepted to The Web Conference (WWW2021)", "journal-ref": null, "doi": "10.1145/3442381.3449998", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The next location recommendation is at the core of various location-based\napplications. Current state-of-the-art models have attempted to solve spatial\nsparsity with hierarchical gridding and model temporal relation with explicit\ntime intervals, while some vital questions remain unsolved. Non-adjacent\nlocations and non-consecutive visits provide non-trivial correlations for\nunderstanding a user's behavior but were rarely considered. To aggregate all\nrelevant visits from user trajectory and recall the most plausible candidates\nfrom weighted representations, here we propose a Spatio-Temporal Attention\nNetwork (STAN) for location recommendation. STAN explicitly exploits relative\nspatiotemporal information of all the check-ins with self-attention layers\nalong the trajectory. This improvement allows a point-to-point interaction\nbetween non-adjacent locations and non-consecutive check-ins with explicit\nspatiotemporal effect. STAN uses a bi-layer attention architecture that firstly\naggregates spatiotemporal correlation within user trajectory and then recalls\nthe target with consideration of personalized item frequency (PIF). By\nvisualization, we show that STAN is in line with the above intuition.\nExperimental results unequivocally show that our model outperforms the existing\nstate-of-the-art methods by 9-17%.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 10:04:54 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Luo", "Yingtao", ""], ["Liu", "Qiang", ""], ["Liu", "Zhaocheng", ""]]}, {"id": "2102.04163", "submitter": "Ivan Sekulic", "authors": "Ivan Sekuli\\'c and Mohammad Aliannejadi and Fabio Crestani", "title": "User Engagement Prediction for Clarification in Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clarification is increasingly becoming a vital factor in various topics of\ninformation retrieval, such as conversational search and modern Web search\nengines. Prompting the user for clarification in a search session can be very\nbeneficial to the system as the user's explicit feedback helps the system\nimprove retrieval massively. However, it comes with a very high risk of\nfrustrating the user in case the system fails in asking decent clarifying\nquestions. Therefore, it is of great importance to determine when and how to\nask for clarification. To this aim, in this work, we model search clarification\nprediction as user engagement problem. We assume that the better a\nclarification is, the higher user engagement with it would be. We propose a\nTransformer-based model to tackle the task. The comparison with competitive\nbaselines on large-scale real-life clarification engagement data proves the\neffectiveness of our model. Also, we analyse the effect of all result page\nelements on the performance and find that, among others, the ranked list of the\nsearch engine leads to considerable improvements. Our extensive analysis of\ntask-specific features guides future research.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 12:28:02 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Sekuli\u0107", "Ivan", ""], ["Aliannejadi", "Mohammad", ""], ["Crestani", "Fabio", ""]]}, {"id": "2102.04205", "submitter": "Minghao Wang", "authors": "Minghao Wang, Paolo Mengoni", "title": "How Pandemic Spread in News: Text Analysis Using Topic Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Researches about COVID-19 has increased largely, no matter in the biology\nfield or the others. This research conducted a text analysis using LDA topic\nmodel. We firstly scraped totally 1127 articles and 5563 comments on SCMP\ncovering COVID-19 from Jan 20 to May 19, then we trained the LDA model and\ntuned parameters based on the Cv coherence as the model evaluation method. With\nthe optimal model, dominant topics, representative documents of each topic and\nthe inconsistence between articles and comments are analyzed. 3 possible\nimprovements are discussed at last.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 08:33:45 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 08:37:06 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Wang", "Minghao", ""], ["Mengoni", "Paolo", ""]]}, {"id": "2102.04238", "submitter": "Mohammad R. Rezaei", "authors": "Mohammad R. Rezaei", "title": "Amazon Product Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The number of reviews on Amazon has grown significantly over the years.\nCustomers who made purchases on Amazon provide reviews by rating the product\nfrom 1 to 5 stars and sharing a text summary of their experience and opinion of\nthe product. The ratings of a product are averaged to provide an overall\nproduct rating. We analyzed what ratings score customers give to a specific\nproduct (a music track) in order to build a recommender model for digital music\ntracks on Amazon. We test various traditional models along with our proposed\ndeep neural network (DNN) architecture to predict the reviews rating score. The\nAmazon review dataset contains 200,000 data samples; we train the models on 70%\nof the dataset and test the performance of the models on the remaining 30% of\nthe dataset.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 18:07:16 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Rezaei", "Mohammad R.", ""]]}, {"id": "2102.04274", "submitter": "Behrooz Razeghi", "authors": "Behrooz Razeghi, Sohrab Ferdowsi, Dimche Kostadinov, Flavio. P.\n  Calmon, Slava Voloshynovskiy", "title": "Privacy-Preserving Near Neighbor Search via Sparse Coding with\n  Ambiguation", "comments": "To be presented at 2021 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we propose a framework for privacy-preserving approximate near\nneighbor search via stochastic sparsifying encoding. The core of the framework\nrelies on sparse coding with ambiguation (SCA) mechanism that introduces the\nnotion of inherent shared secrecy based on the support intersection of sparse\ncodes. This approach is `fairness-aware', in the sense that any point in the\nneighborhood has an equiprobable chance to be chosen. Our approach can be\napplied to raw data, latent representation of autoencoders, and aggregated\nlocal descriptors. The proposed method is tested on both synthetic i.i.d data\nand real large-scale image databases.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 15:20:32 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Razeghi", "Behrooz", ""], ["Ferdowsi", "Sohrab", ""], ["Kostadinov", "Dimche", ""], ["Calmon", "Flavio. P.", ""], ["Voloshynovskiy", "Slava", ""]]}, {"id": "2102.04293", "submitter": "Miguel Ramalho", "authors": "Miguel Sozinho Ramalho", "title": "High-level Approaches to Detect Malicious Political Activity on Twitter", "comments": "Master's thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our work represents another step into the detection and prevention of these\never-more present political manipulation efforts. We, therefore, start by\nfocusing on understanding what the state-of-the-art approaches lack -- since\nthe problem remains, this is a fair assumption. We find concerning issues\nwithin the current literature and follow a diverging path. Notably, by placing\nemphasis on using data features that are less susceptible to malicious\nmanipulation and also on looking for high-level approaches that avoid a\ngranularity level that is biased towards easy-to-spot and low impact cases.\n  We designed and implemented a framework -- Twitter Watch -- that performs\nstructured Twitter data collection, applying it to the Portuguese\nTwittersphere. We investigate a data snapshot taken on May 2020, with around 5\nmillion accounts and over 120 million tweets (this value has since increased to\nover 175 million). The analyzed time period stretches from August 2019 to May\n2020, with a focus on the Portuguese elections of October 6th, 2019. However,\nthe Covid-19 pandemic showed itself in our data, and we also delve into how it\naffected typical Twitter behavior.\n  We performed three main approaches: content-oriented, metadata-oriented, and\nnetwork interaction-oriented. We learn that Twitter's suspension patterns are\nnot adequate to the type of political trolling found in the Portuguese\nTwittersphere -- identified by this work and by an independent peer - nor to\nfake news posting accounts. We also surmised that the different types of\nmalicious accounts we independently gathered are very similar both in terms of\ncontent and interaction, through two distinct analysis, and are simultaneously\nvery distinct from regular accounts.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 22:54:44 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ramalho", "Miguel Sozinho", ""]]}, {"id": "2102.04447", "submitter": "John Leung", "authors": "John Kalung Leung and Igor Griva and William G. Kennedy", "title": "Applying the Affective Aware Pseudo Association Method to Enhance the\n  Top-N Recommendations Distribution to Users in Group Emotion Recommender\n  Systems", "comments": "19 pages, 9 tables", "journal-ref": "International Journal on Natural Language Computing (IJNLC) Vol.\n  10, No. 1, February 2021", "doi": "10.5121/ijnlc.2021.10101", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recommender Systems are a subclass of information retrieval systems, or more\nsuccinctly, a class of information filtering systems that seeks to predict how\nclose is the match of the user's preference to a recommended item. A common\napproach for making recommendations for a user group is to extend Personalized\nRecommender Systems' capability. This approach gives the impression that group\nrecommendations are retrofits of the Personalized Recommender Systems.\nMoreover, such an approach not taken the dynamics of group emotion and\nindividual emotion into the consideration in making top_N recommendations.\nRecommending items to a group of two or more users has certainly raised unique\nchallenges in group behaviors that influence group decision-making that\nresearchers only partially understand. This study applies the Affective Aware\nPseudo Association Method in studying group formation and dynamics in group\ndecision-making. The method shows its adaptability to group's moods change when\nmaking recommendations.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 18:59:20 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Leung", "John Kalung", ""], ["Griva", "Igor", ""], ["Kennedy", "William G.", ""]]}, {"id": "2102.04449", "submitter": "Hao Lei", "authors": "Hao Lei and Ying Chen", "title": "Concentrated Document Topic Model", "comments": "arXiv admin note: text overlap with arXiv:2102.03525", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a Concentrated Document Topic Model(CDTM) for unsupervised text\nclassification, which is able to produce a concentrated and sparse document\ntopic distribution. In particular, an exponential entropy penalty is imposed on\nthe document topic distribution. Documents that have diverse topic\ndistributions are penalized more, while those having concentrated topics are\npenalized less. We apply the model to the benchmark NIPS dataset and observe\nmore coherent topics and more concentrated and sparse document-topic\ndistributions than Latent Dirichlet Allocation(LDA).\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 07:12:05 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Lei", "Hao", ""], ["Chen", "Ying", ""]]}, {"id": "2102.04640", "submitter": "Zhuo Li", "authors": "Zhuo Li, Weiqing Min, Jiajun Song, Yaohui Zhu, Liping Kang, Xiaoming\n  Wei, Xiaolin Wei, Shuqiang Jiang", "title": "Rethinking the Optimization of Average Precision: Only Penalizing\n  Negative Instances before Positive Ones is Enough", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimising the approximation of Average Precision (AP) has been widely\nstudied for image retrieval. Such methods consider both negative and positive\ninstances ranking before each positive instance. However, we claim that only\npenalizing negative instances before positive ones is enough, because the loss\nonly comes from them. To this end, we propose a novel loss, namely Penalizing\nNegative instances before Positive ones (PNP), which directly minimizes the\nnumber of negative instances before each positive one. Meanwhile, AP-based\nmethods adopt a sub-optimal gradient assignment strategy. We systematically\ninvestigate different gradient assignment solutions via constructing derivative\nfunctions of the loss, resulting in PNP-I with increasing derivative functions\nand PNP-D with decreasing ones. PNP-I focuses more on the hard positive\ninstances by assigning larger gradients to them and tries to make all relevant\ninstances closer. In contrast, considering such instances may belong to another\ncenter of the corresponding category, PNP-D pays less attention to such\ninstances and keeps them as they were. For most real-world data, one class\nusually contains several local clusters. Thus, PNP-D is more suitable for such\nsituation. Experiments on three standard retrieval datasets show consistent\nresults of the above analysis. Extensive evaluations demonstrate that PNP-D\nachieves the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 04:30:15 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 06:05:32 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Li", "Zhuo", ""], ["Min", "Weiqing", ""], ["Song", "Jiajun", ""], ["Zhu", "Yaohui", ""], ["Kang", "Liping", ""], ["Wei", "Xiaoming", ""], ["Wei", "Xiaolin", ""], ["Jiang", "Shuqiang", ""]]}, {"id": "2102.04656", "submitter": "Kang Zhao", "authors": "Kang Zhao, Pan Pan, Yun Zheng, Yanhao Zhang, Changxu Wang, Yingya\n  Zhang, Yinghui Xu, Rong Jin", "title": "Large-Scale Visual Search with Binary Distributed Graph at Alibaba", "comments": "This paper has been accepted by CIKM2019. Proceedings of the 28th ACM\n  International Conference on Information and Knowledge Management. 2019", "journal-ref": null, "doi": "10.1145/3357384.3357834", "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based approximate nearest neighbor search has attracted more and more\nattentions due to its online search advantages. Numbers of methods studying the\nenhancement of speed and recall have been put forward. However, few of them\nfocus on the efficiency and scale of offline graph-construction. For a deployed\nvisual search system with several billions of online images in total, building\na billion-scale offline graph in hours is essential, which is almost\nunachievable by most existing methods. In this paper, we propose a novel\nalgorithm called Binary Distributed Graph to solve this problem. Specifically,\nwe combine binary codes with graph structure to speedup online and offline\nprocedures, and achieve comparable performance with the ones in real-value\nbased scenarios by recalling more binary candidates. Furthermore, the\ngraph-construction is optimized to completely distributed implementation, which\nsignificantly accelerates the offline process and gets rid of the limitation of\nmemory and disk within a single machine. Experimental comparisons on Alibaba\nCommodity Data Set (more than three billion images) show that the proposed\nmethod outperforms the state-of-the-art with respect to the online/offline\ntrade-off.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 05:51:34 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Zhao", "Kang", ""], ["Pan", "Pan", ""], ["Zheng", "Yun", ""], ["Zhang", "Yanhao", ""], ["Wang", "Changxu", ""], ["Zhang", "Yingya", ""], ["Xu", "Yinghui", ""], ["Jin", "Rong", ""]]}, {"id": "2102.04845", "submitter": "Haozhen Zhao", "authors": "Rishi Chhatwal, Robert Keeling, Peter Gronvall, Nathaniel\n  Huber-Fliflet, Jianping Zhang, Haozhen Zhao", "title": "CNN Application in Detection of Privileged Documents in Legal Document\n  Review", "comments": "2020 IEEE International Conference on Big Data (Big Data)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protecting privileged communications and data from disclosure is paramount\nfor legal teams. Legal advice, such as attorney-client communications or\nlitigation strategy are typically exempt from disclosure in litigations or\nregulatory events and are vital to the attorney-client relationship. To protect\nthis information from disclosure, companies and outside counsel often review\nvast amounts of documents to determine those that contain privileged material.\nThis process is extremely costly and time consuming. As data volumes increase,\nlegal counsel normally employs methods to reduce the number of documents\nrequiring review while balancing the need to ensure the protection of\nprivileged information. Keyword searching is relied upon as a method to target\nprivileged information and reduce document review populations. Keyword searches\nare effective at casting a wide net but often return overly inclusive results -\nmost of which do not contain privileged information. To overcome the weaknesses\nof keyword searching, legal teams increasingly are using machine learning\ntechniques to target privileged information. In these studies, classic text\nclassification techniques are applied to build classification models to\nidentify privileged documents. In this paper, the authors propose a different\nmethod by applying machine learning / convolutional neural network techniques\n(CNN) to identify privileged documents. Our proposed method combines keyword\nsearching with CNN. For each keyword term, a CNN model is created using the\ncontext of the occurrences of the keyword. In addition, a method was proposed\nto select reliable privileged (positive) training keyword occurrences from\nlabeled positive training documents. Extensive experiments were conducted, and\nthe results show that the proposed methods can significantly reduce false\npositives while still capturing most of the true positives.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 14:42:02 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Chhatwal", "Rishi", ""], ["Keeling", "Robert", ""], ["Gronvall", "Peter", ""], ["Huber-Fliflet", "Nathaniel", ""], ["Zhang", "Jianping", ""], ["Zhao", "Haozhen", ""]]}, {"id": "2102.04903", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang", "title": "FeedRec: News Feed Recommendation with Various User Feedbacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalized news recommendation techniques are widely adopted by many online\nnews feed platforms to target user interests. Learning accurate user interest\nmodels is important for news recommendation. Most existing methods for news\nrecommendation rely on implicit feedbacks like click behaviors for inferring\nuser interests and model training. However, click behaviors are implicit\nfeedbacks and usually contain heavy noise. In addition, they cannot help infer\ncomplicated user interest such as dislike. Besides, the feed recommendation\nmodels trained solely on click behaviors cannot optimize other objectives such\nas user engagement. In this paper, we present a news feed recommendation method\nthat can exploit various kinds of user feedbacks to enhance both user interest\nmodeling and recommendation model training. In our method we propose a unified\nuser modeling framework to incorporate various explicit and implicit user\nfeedbacks to infer both positive and negative user interests. In addition, we\npropose a strong-to-weak attention network that uses the representations of\nstronger feedbacks to distill positive and negative user interests from\nimplicit weak feedbacks for accurate user interest modeling. Besides, we\npropose a multi-feedback model training framework by jointly training the model\nin the click, finish and dwell time prediction tasks to learn an\nengagement-aware feed recommendation model. Extensive experiments on real-world\ndataset show that our approach can effectively improve the model performance in\nterms of both news clicks and user engagement.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 16:00:25 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Qi", "Tao", ""], ["Huang", "Yongfeng", ""]]}, {"id": "2102.04925", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Yang Cao, Yongfeng Huang, Xing Xie", "title": "FedGNN: Federated Graph Neural Network for Privacy-Preserving\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph neural network (GNN) is widely used for recommendation to model\nhigh-order interactions between users and items. Existing GNN-based\nrecommendation methods rely on centralized storage of user-item graphs and\ncentralized model learning. However, user data is privacy-sensitive, and the\ncentralized storage of user-item graphs may arouse privacy concerns and risk.\nIn this paper, we propose a federated framework for privacy-preserving\nGNN-based recommendation, which can collectively train GNN models from\ndecentralized user data and meanwhile exploit high-order user-item interaction\ninformation with privacy well protected. In our method, we locally train GNN\nmodel in each user client based on the user-item graph inferred from the local\nuser-item interaction data. Each client uploads the local gradients of GNN to a\nserver for aggregation, which are further sent to user clients for updating\nlocal GNN models. Since local gradients may contain private information, we\napply local differential privacy techniques to the local gradients to protect\nuser privacy. In addition, in order to protect the items that users have\ninteractions with, we propose to incorporate randomly sampled items as pseudo\ninteracted items for anonymity. To incorporate high-order user-item\ninteractions, we propose a user-item graph expansion method that can find\nneighboring users with co-interacted items and exchange their embeddings for\nexpanding the local user-item graphs in a privacy-preserving way. Extensive\nexperiments on six benchmark datasets validate that our approach can achieve\ncompetitive results with existing centralized GNN-based recommendation methods\nand meanwhile effectively protect user privacy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 16:30:53 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 08:27:46 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Cao", "Yang", ""], ["Huang", "Yongfeng", ""], ["Xie", "Xing", ""]]}, {"id": "2102.04974", "submitter": "Michele Garetto", "authors": "Michele Garetto and Emilio Leonardi and Giovanni Neglia", "title": "Content Placement in Networks of Similarity Caches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Similarity caching systems have recently attracted the attention of the\nscientific community, as they can be profitably used in many application\ncontexts, like multimedia retrieval, advertising, object recognition,\nrecommender systems and online content-match applications. In such systems, a\nuser request for an object $o$, which is not in the cache, can be (partially)\nsatisfied by a similar stored object $o$', at the cost of a loss of user\nutility. In this paper we make a first step into the novel area of similarity\ncaching networks, where requests can be forwarded along a path of caches to get\nthe best efficiency-accuracy tradeoff. The offline problem of content placement\ncan be easily shown to be NP-hard, while different polynomial algorithms can be\ndevised to approach the optimal solution in discrete cases. As the content\nspace grows large, we propose a continuous problem formulation whose solution\nexhibits a simple structure in a class of tree topologies. We verify our\nfindings using synthetic and realistic request traces.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 17:48:24 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Garetto", "Michele", ""], ["Leonardi", "Emilio", ""], ["Neglia", "Giovanni", ""]]}, {"id": "2102.05216", "submitter": "Sara Bunian", "authors": "Sara Bunian, Kai Li, Chaima Jemmali, Casper Harteveld, Yun Fu, Magy\n  Seif El-Nasr", "title": "VINS: Visual Search for Mobile User Interface Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for relative mobile user interface (UI) design examples can aid\ninterface designers in gaining inspiration and comparing design alternatives.\nHowever, finding such design examples is challenging, especially as current\nsearch systems rely on only text-based queries and do not consider the UI\nstructure and content into account. This paper introduces VINS, a visual search\nframework, that takes as input a UI image (wireframe, high-fidelity) and\nretrieves visually similar design examples. We first survey interface designers\nto better understand their example finding process. We then develop a\nlarge-scale UI dataset that provides an accurate specification of the\ninterface's view hierarchy (i.e., all the UI components and their specific\nlocation). By utilizing this dataset, we propose an object-detection based\nimage retrieval framework that models the UI context and hierarchical\nstructure. The framework achieves a mean Average Precision of 76.39\\% for the\nUI detection and high performance in querying similar UI designs.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 01:46:33 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Bunian", "Sara", ""], ["Li", "Kai", ""], ["Jemmali", "Chaima", ""], ["Harteveld", "Casper", ""], ["Fu", "Yun", ""], ["El-Nasr", "Magy Seif", ""]]}, {"id": "2102.05238", "submitter": "Shantanu Sharma", "authors": "Peeyush Gupta, Sharad Mehrotra, Shantanu Sharma, Nalini\n  Venkatasubramanian, Guoxi Wang", "title": "Concealer: SGX-based Secure, Volume Hiding, and Verifiable Processing of\n  Spatial Time-Series Datasets", "comments": "A preliminary version of this paper has been accepted in the 24th\n  International Conference on Extending Database Technology (EDBT) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a system, entitled Concealer that allows sharing\ntime-varying spatial data (e.g., as produced by sensors) in encrypted form to\nan untrusted third-party service provider to provide location-based\napplications (involving aggregation queries over selected regions over time\nwindows) to users. Concealer exploits carefully selected encryption techniques\nto use indexes supported by database systems and combines ways to add fake\ntuples in order to realize an efficient system that protects against leakage\nbased on output-size. Thus, the design of Concealer overcomes two limitations\nof existing symmetric searchable encryption (SSE) techniques: (i) it avoids the\nneed of specialized data structures that limit usability/practicality of SSE in\nlarge scale deployments, and (ii) it avoids information leakages based on the\noutput-size, which may leak data distributions. Experimental results validate\nthe efficiency of the proposed algorithms over a spatial time-series dataset\n(collected from a smart space) and TPC-H datasets, each of 136 Million rows,\nthe size of which prior approaches have not scaled to.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 03:28:25 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Gupta", "Peeyush", ""], ["Mehrotra", "Sharad", ""], ["Sharma", "Shantanu", ""], ["Venkatasubramanian", "Nalini", ""], ["Wang", "Guoxi", ""]]}, {"id": "2102.05260", "submitter": "Sm Zobaed", "authors": "Sm Zobaed, Md Enamul Haque, Md Fazle Rabby, and Mohsen Amini Salehi", "title": "SensPick: Sense Picking for Word Sense Disambiguation", "comments": null, "journal-ref": "16th IEEE International Conference on Semantic Computing,\n  ICSC'2021", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Word sense disambiguation (WSD) methods identify the most suitable meaning of\na word with respect to the usage of that word in a specific context. Neural\nnetwork-based WSD approaches rely on a sense-annotated corpus since they do not\nutilize lexical resources. In this study, we utilize both context and related\ngloss information of a target word to model the semantic relationship between\nthe word and the set of glosses. We propose SensPick, a type of stacked\nbidirectional Long Short Term Memory (LSTM) network to perform the WSD task.\nThe experimental evaluation demonstrates that SensPick outperforms traditional\nand state-of-the-art models on most of the benchmark datasets with a relative\nimprovement of 3.5% in F-1 score. While the improvement is not significant,\nincorporating semantic relationships brings SensPick in the leading position\ncompared to others.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 04:52:42 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Zobaed", "Sm", ""], ["Haque", "Md Enamul", ""], ["Rabby", "Md Fazle", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "2102.05374", "submitter": "Pierre Le Bras", "authors": "Tanya Howden, Pierre Le Bras, Thomas S. Methven, Stefano Padilla, Mike\n  J. Chantler", "title": "Enhancing Reading Strategies by Exploring A Theme-based Approach to\n  Literature Surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Searching large digital repositories can be extremely frustrating, as common\nlist-based formats encourage users to adopt a convenience-sampling approach\nthat favours chance discovery and random search, over meaningful exploration.\nWe have designed a methodology that allows users to visually and thematically\nexplore corpora, while developing personalised holistic reading strategies. We\ndescribe the results of a three-phase qualitative study, in which experienced\nresearchers used our interactive visualisation approach to analyse a set of\npublications and select relevant themes and papers. Using in-depth\nsemi-structured interviews and stimulated recall, we found that users: (i)\nselected papers that they otherwise would not have read, (ii) developed a more\ncoherent reading strategy, and (iii) understood the thematic structure and\nrelationships between papers more effectively. Finally, we make six design\nrecommendations to enhance current digital repositories that we have shown\nencourage users to adopt a more holistic and thematic research approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 10:36:45 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Howden", "Tanya", ""], ["Bras", "Pierre Le", ""], ["Methven", "Thomas S.", ""], ["Padilla", "Stefano", ""], ["Chantler", "Mike J.", ""]]}, {"id": "2102.05444", "submitter": "Nicolas Heist", "authors": "Nicolas Heist and Heiko Paulheim", "title": "Information Extraction From Co-Occurring Similar Entities", "comments": "Preprint of a paper accepted for the research track of the Web\n  Conference (WWW'21), April 19-23, 2021, Ljubljana, Slovenia", "journal-ref": null, "doi": "10.1145/3442381.3449836", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge about entities and their interrelations is a crucial factor of\nsuccess for tasks like question answering or text summarization. Publicly\navailable knowledge graphs like Wikidata or DBpedia are, however, far from\nbeing complete. In this paper, we explore how information extracted from\nsimilar entities that co-occur in structures like tables or lists can help to\nincrease the coverage of such knowledge graphs. In contrast to existing\napproaches, we do not focus on relationships within a listing (e.g., between\ntwo entities in a table row) but on the relationship between a listing's\nsubject entities and the context of the listing. To that end, we propose a\ndescriptive rule mining approach that uses distant supervision to derive rules\nfor these relationships based on a listing's context. Extracted from a suitable\ndata corpus, the rules can be used to extend a knowledge graph with novel\nentities and assertions. In our experiments we demonstrate that the approach is\nable to extract up to 3M novel entities and 30M additional assertions from\nlistings in Wikipedia. We find that the extracted information is of high\nquality and thus suitable to extend Wikipedia-based knowledge graphs like\nDBpedia, YAGO, and CaLiGraph. For the case of DBpedia, this would result in an\nincrease of covered entities by roughly 50%.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 14:03:59 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 09:23:03 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 10:17:09 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Heist", "Nicolas", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2102.05571", "submitter": "Nidhi Rastogi", "authors": "Nidhi Rastogi, Sharmishtha Dutta, Ryan Christian, Jared Gridley,\n  Mohammad Zaki, Alex Gittens, Charu Aggarwal", "title": "Predicting malware threat intelligence using KGs", "comments": "14 pages", "journal-ref": null, "doi": "10.13140/RG.2.2.12526.54083", "report-no": null, "categories": "cs.CR cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Large amounts of threat intelligence information about malware attacks are\navailable in disparate, typically unstructured, formats. Knowledge graphs can\ncapture this information and its context using RDF triples represented by\nentities and relations. Sparse or inaccurate threat information, however, leads\nto challenges such as incomplete or erroneous triples. Generic information\nextraction (IE) models used to populate the knowledge graph cannot fully\nguarantee domain-specific context. This paper proposes a system to generate a\nMalware Knowledge Graph called MalKG, the first open-source automated knowledge\ngraph for malware threat intelligence. MalKG dataset (MT40K\\footnote{ Anonymous\nGitHub link: https://github.com/malkg-researcher/MalKG}) contains approximately\n40,000 triples generated from 27,354 unique entities and 34 relations. For\nground truth, we manually curate a knowledge graph called MT3K, with 3,027\ntriples generated from 5,741 unique entities and 22 relations. We demonstrate\nthe intelligence prediction of MalKG using two use cases. Predicting malware\nthreat information using the benchmark model achieves 80.4 for the hits@10\nmetric (predicts the top 10 options for an information class), and 0.75 for the\nMRR (mean reciprocal rank). We also propose an automated, contextual framework\nfor information extraction, both manually and automatically, at the sentence\nlevel from 1,100 malware threat reports and from the common vulnerabilities and\nexposures (CVE) database.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 17:08:09 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 05:03:10 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 19:36:44 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Rastogi", "Nidhi", ""], ["Dutta", "Sharmishtha", ""], ["Christian", "Ryan", ""], ["Gridley", "Jared", ""], ["Zaki", "Mohammad", ""], ["Gittens", "Alex", ""], ["Aggarwal", "Charu", ""]]}, {"id": "2102.05583", "submitter": "Nidhi Rastogi", "authors": "Sharmishtha Dutta, Nidhi Rastogi, Destin Yee, Chuqiao Gu, Qicheng Ma", "title": "Malware Knowledge Graph Generation", "comments": "5 pages", "journal-ref": null, "doi": "10.13140/RG.2.2.27340.95367", "report-no": null, "categories": "cs.CR cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Cyber threat and attack intelligence information are available in\nnon-standard format from heterogeneous sources. Comprehending them and\nutilizing them for threat intelligence extraction requires engaging security\nexperts. Knowledge graphs enable converting this unstructured information from\nheterogeneous sources into a structured representation of data and factual\nknowledge for several downstream tasks such as predicting missing information\nand future threat trends. Existing large-scale knowledge graphs mainly focus on\ngeneral classes of entities and relationships between them. Open-source\nknowledge graphs for the security domain do not exist. To fill this gap, we've\nbuilt \\textsf{TINKER} - a knowledge graph for threat intelligence\n(\\textbf{T}hreat \\textbf{IN}telligence \\textbf{K}nowl\\textbf{E}dge\ng\\textbf{R}aph). \\textsf{TINKER} is generated using RDF triples describing\nentities and relations from tokenized unstructured natural language text from\n83 threat reports published between 2006-2021. We built \\textsf{TINKER} using\nclasses and properties defined by open-source malware ontology and using\nhand-annotated RDF triples. We also discuss ongoing research and challenges\nfaced while creating \\textsf{TINKER}.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 17:26:43 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Dutta", "Sharmishtha", ""], ["Rastogi", "Nidhi", ""], ["Yee", "Destin", ""], ["Gu", "Chuqiao", ""], ["Ma", "Qicheng", ""]]}, {"id": "2102.05691", "submitter": "Jonathan Handler", "authors": "Jonathan A. Handler, Craig F. Feied, Michael T. Gillam", "title": "Novel Techniques to Assess Predictive Systems and Reduce Their Alarm\n  Burden", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of a binary classifier (\"predictor\") depends heavily upon the\ncontext (\"workflow\") in which it operates. Classic measures of predictor\nperformance do not reflect the realized utility of predictors unless certain\nimplied workflow assumptions are met. Failure to meet these implied assumptions\nresults in suboptimal classifier implementations and a mismatch between\npredicted or assessed performance and the actual performance obtained in\nreal-world deployments. The mismatch commonly arises when multiple predictions\ncan be made for the same event, the event is relatively rare, and redundant\ntrue positive predictions for the same event add little value, e.g., a system\nthat makes a prediction each minute, repeatedly issuing interruptive alarms for\na predicted event that may never occur.\n  We explain why classic metrics do not correctly represent the performance of\npredictors in such contexts, and introduce an improved performance assessment\ntechnique (\"u-metrics\") using utility functions to score each prediction.\nU-metrics explicitly account for variability in prediction utility arising from\ntemporal relationships. Compared to traditional performance measures, u-metrics\nmore accurately reflect the real-world benefits and costs of a predictor\noperating in a workflow context. The difference can be significant.\n  We also describe the use of \"snoozing,\" a method whereby predictions are\nsuppressed for a period of time, commonly improving predictor performance by\nreducing false positives while retaining the capture of events. Snoozing is\nespecially useful when predictors generate interruptive alerts, as so often\nhappens in clinical practice. Utility-based performance metrics correctly\npredict and track the performance benefits of snoozing, whereas traditional\nperformance metrics do not.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 19:05:06 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Handler", "Jonathan A.", ""], ["Feied", "Craig F.", ""], ["Gillam", "Michael T.", ""]]}, {"id": "2102.05700", "submitter": "Johannes Knittel", "authors": "Johannes Knittel, Steffen Koch, Thomas Ertl", "title": "ELSKE: Efficient Large-Scale Keyphrase Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrase extraction methods can provide insights into large collections of\ndocuments such as social media posts. Existing methods, however, are less\nsuited for the real-time analysis of streaming data, because they are\ncomputationally too expensive or require restrictive constraints regarding the\nstructure of keyphrases. We propose an efficient approach to extract keyphrases\nfrom large document collections and show that the method also performs\ncompetitively on individual documents.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 19:14:01 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Knittel", "Johannes", ""], ["Koch", "Steffen", ""], ["Ertl", "Thomas", ""]]}, {"id": "2102.05716", "submitter": "Aline Bessa", "authors": "Fernando Chirigati, R\\'emi Rampin, A\\'ecio Santos, Aline Bessa, and\n  Juliana Freire", "title": "Auctus: A Dataset Search Engine for Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning models are increasingly being adopted in many applications.\nThe quality of these models critically depends on the input data on which they\nare trained, and by augmenting their input data with external data, we have the\nopportunity to create better models. However, the massive number of datasets\navailable on the Web makes it challenging to find data suitable for\naugmentation. In this demo, we present our ongoing efforts to develop a dataset\nsearch engine tailored for data augmentation. Our prototype, named Auctus,\nautomatically discovers datasets on the Web and, different from existing\ndataset search engines, infers consistent metadata for indexing and supports\njoin and union search queries. Auctus is already being used in a real\ndeployment environment to improve the performance of ML models. The\ndemonstration will include various real-world data augmentation examples and\nvisitors will be able to interact with the system.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 19:49:10 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Chirigati", "Fernando", ""], ["Rampin", "R\u00e9mi", ""], ["Santos", "A\u00e9cio", ""], ["Bessa", "Aline", ""], ["Freire", "Juliana", ""]]}, {"id": "2102.05719", "submitter": "John Leung", "authors": "John Kalung Leung and Igor Griva and William G. Kennedy", "title": "An Affective Aware Pseudo Association Method to Connect Disjoint Users\n  Across Multiple Datasets -- An Enhanced Validation Method for Text-based\n  Emotion Aware Recommender", "comments": "21 pages, 9 tables. arXiv admin note: substantial text overlap with\n  arXiv:2007.01455", "journal-ref": "International Journal on Natural Language Computing (IJNLC) Vol.\n  9, No. 4, August 2020", "doi": "10.5121/ijnlc.2020.9402", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We derive a method to enhance the evaluation for a text-based Emotion Aware\nRecommender that we have developed. However, we did not implement a suitable\nway to assess the top-N recommendations subjectively. In this study, we\nintroduce an emotion-aware Pseudo Association Method to interconnect disjointed\nusers across different datasets so data files can be combined to form a more\nextensive data file. Users with the same user IDs found in separate data files\nin the same dataset are often the same users. However, users with the same user\nID may not be the same user across different datasets. We advocate an emotion\naware Pseudo Association Method to associate users across different datasets.\nThe approach interconnects users with different user IDs across different\ndatasets through the most similar users' emotion vectors (UVECs). We found the\nmethod improved the evaluation process of assessing the top-N recommendations\nobjectively.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 19:58:34 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Leung", "John Kalung", ""], ["Griva", "Igor", ""], ["Kennedy", "William G.", ""]]}, {"id": "2102.05990", "submitter": "Harrie Oosterhuis", "authors": "Harrie Oosterhuis and Maarten de Rijke", "title": "Robust Generalization and Safe Query-Specialization in Counterfactual\n  Learning to Rank", "comments": "In Proceedings of the Web Conference 2021 (WWW '21), April 19-23,\n  2021, Ljubljana, Slovenia. ACM, New York, NY, USA, 12 pages", "journal-ref": null, "doi": "10.1145/3442381.3450018", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing work in counterfactual Learning to Rank (LTR) has focussed on\noptimizing feature-based models that predict the optimal ranking based on\ndocument features. LTR methods based on bandit algorithms often optimize\ntabular models that memorize the optimal ranking per query. These types of\nmodel have their own advantages and disadvantages. Feature-based models provide\nvery robust performance across many queries, including those previously unseen,\nhowever, the available features often limit the rankings the model can predict.\nIn contrast, tabular models can converge on any possible ranking through\nmemorization. However, memorization is extremely prone to noise, which makes\ntabular models reliable only when large numbers of user interactions are\navailable. Can we develop a robust counterfactual LTR method that pursues\nmemorization-based optimization whenever it is safe to do? We introduce the\nGeneralization and Specialization (GENSPEC) algorithm, a robust feature-based\ncounterfactual LTR method that pursues per-query memorization when it is safe\nto do so. GENSPEC optimizes a single feature-based model for generalization:\nrobust performance across all queries, and many tabular models for\nspecialization: each optimized for high performance on a single query. GENSPEC\nuses novel relative high-confidence bounds to choose which model to deploy per\nquery. By doing so, GENSPEC enjoys the high performance of successfully\nspecialized tabular models with the robustness of a generalized feature-based\nmodel. Our results show that GENSPEC leads to optimal performance on queries\nwith sufficient click data, while having robust behavior on queries with little\nor noisy data.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 13:17:26 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 10:28:16 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Oosterhuis", "Harrie", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2102.05996", "submitter": "Nikola Konstantinov", "authors": "Nikola Konstantinov, Christoph H. Lampert", "title": "Fairness Through Regularization for Learning to Rank", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the abundance of applications of ranking in recent years, addressing\nfairness concerns around automated ranking systems becomes necessary for\nincreasing the trust among end-users. Previous work on fair ranking has mostly\nfocused on application-specific fairness notions, often tailored to online\nadvertising, and it rarely considers learning as part of the process. In this\nwork, we show how to transfer numerous fairness notions from binary\nclassification to a learning to rank setting. Our formalism allows us to design\nmethods for incorporating fairness objectives with provable generalization\nguarantees. An extensive experimental evaluation shows that our method can\nimprove ranking fairness substantially with no or only little loss of model\nquality.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 13:29:08 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 20:22:03 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Konstantinov", "Nikola", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "2102.06008", "submitter": "Arthur Brack", "authors": "Arthur Brack and Anett Hoppe and Pascal Buscherm\\\"ohle and Ralph\n  Ewerth", "title": "Sequential Sentence Classification in Research Papers using Cross-Domain\n  Multi-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of sequential sentence classification enables the semantic\nstructuring of research papers. This can enhance academic search engines to\nsupport researchers in finding and exploring research literature more\neffectively. However, previous work has not investigated the potential of\ntransfer learning with datasets from different scientific domains for this task\nyet. We propose a uniform deep learning architecture and multi-task learning to\nimprove sequential sentence classification in scientific texts across domains\nby exploiting training data from multiple domains. Our contributions can be\nsummarised as follows: (1) We tailor two common transfer learning methods,\nsequential transfer learning and multi-task learning, and evaluate their\nperformance for sequential sentence classification; (2) The presented\nmulti-task model is able to recognise semantically related classes from\ndifferent datasets and thus supports manual comparison and assessment of\ndifferent annotation schemes; (3) The unified approach is capable of handling\ndatasets that contain either only abstracts or full papers without further\nfeature engineering. We demonstrate that models, which are trained on datasets\nfrom different scientific domains, benefit from one another when using the\nproposed multi-task learning architecture. Our approach outperforms the state\nof the art on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 13:54:10 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Brack", "Arthur", ""], ["Hoppe", "Anett", ""], ["Buscherm\u00f6hle", "Pascal", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2102.06021", "submitter": "Arthur Brack", "authors": "Arthur Brack and Anett Hoppe and Markus Stocker and S\\\"oren Auer and\n  Ralph Ewerth", "title": "Analysing the Requirements for an Open Research Knowledge Graph: Use\n  Cases, Quality Requirements and Construction Strategies", "comments": "arXiv admin note: text overlap with arXiv:2005.10334", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current science communication has a number of drawbacks and bottlenecks which\nhave been subject of discussion lately: Among others, the rising number of\npublished articles makes it nearly impossible to get a full overview of the\nstate of the art in a certain field, or reproducibility is hampered by\nfixed-length, document-based publications which normally cannot cover all\ndetails of a research work. Recently, several initiatives have proposed\nknowledge graphs (KG) for organising scientific information as a solution to\nmany of the current issues. The focus of these proposals is, however, usually\nrestricted to very specific use cases. In this paper, we aim to transcend this\nlimited perspective and present a comprehensive analysis of requirements for an\nOpen Research Knowledge Graph (ORKG) by (a) collecting and reviewing daily core\ntasks of a scientist, (b) establishing their consequential requirements for a\nKG-based system, (c) identifying overlaps and specificities, and their coverage\nin current solutions. As a result, we map necessary and desirable requirements\nfor successful KG-based science communication, derive implications, and outline\npossible solutions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 14:09:49 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Brack", "Arthur", ""], ["Hoppe", "Anett", ""], ["Stocker", "Markus", ""], ["Auer", "S\u00f6ren", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2102.06156", "submitter": "Tian Wang", "authors": "Tian Wang, Yuri M. Brovman, Sriganesh Madhvanath", "title": "Personalized Embedding-based e-Commerce Recommendations at eBay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommender systems are an essential component of e-commerce marketplaces,\nhelping consumers navigate massive amounts of inventory and find what they need\nor love. In this paper, we present an approach for generating personalized item\nrecommendations in an e-commerce marketplace by learning to embed items and\nusers in the same vector space. In order to alleviate the considerable\ncold-start problem present in large marketplaces, item and user embeddings are\ncomputed using content features and multi-modal onsite user activity\nrespectively. Data ablation is incorporated into the offline model training\nprocess to improve the robustness of the production system. In offline\nevaluation using a dataset collected from eBay traffic, our approach was able\nto improve the Recall@k metric over the Recently-Viewed-Item (RVI) method. This\napproach to generating personalized recommendations has been launched to serve\nproduction traffic, and the corresponding scalable engineering architecture is\nalso presented. Initial A/B test results show that compared to the current\npersonalized recommendation module in production, the proposed method increases\nthe surface rate by $\\sim$6\\% to generate recommendations for 90\\% of listing\npage impressions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 17:58:51 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Wang", "Tian", ""], ["Brovman", "Yuri M.", ""], ["Madhvanath", "Sriganesh", ""]]}, {"id": "2102.06314", "submitter": "Amila Silva", "authors": "Amila Silva, Ling Luo, Shanika Karunasekera, Christopher Leckie", "title": "Embracing Domain Differences in Fake News: Cross-domain Fake News\n  Detection using Multi-modal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid evolution of social media, fake news has become a significant\nsocial problem, which cannot be addressed in a timely manner using manual\ninvestigation. This has motivated numerous studies on automating fake news\ndetection. Most studies explore supervised training models with different\nmodalities (e.g., text, images, and propagation networks) of news records to\nidentify fake news. However, the performance of such techniques generally drops\nif news records are coming from different domains (e.g., politics,\nentertainment), especially for domains that are unseen or rarely-seen during\ntraining. As motivation, we empirically show that news records from different\ndomains have significantly different word usage and propagation patterns.\nFurthermore, due to the sheer volume of unlabelled news records, it is\nchallenging to select news records for manual labelling so that the\ndomain-coverage of the labelled dataset is maximized. Hence, this work: (1)\nproposes a novel framework that jointly preserves domain-specific and\ncross-domain knowledge in news records to detect fake news from different\ndomains; and (2) introduces an unsupervised technique to select a set of\nunlabelled informative news records for manual labelling, which can be\nultimately used to train a fake news detection model that performs well for\nmany domains while minimizing the labelling cost. Our experiments show that the\nintegration of the proposed fake news model and the selective annotation\napproach achieves state-of-the-art performance for cross-domain news datasets,\nwhile yielding notable improvements for rarely-appearing domains in news\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 23:31:14 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 13:26:43 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 05:33:25 GMT"}, {"version": "v4", "created": "Wed, 24 Feb 2021 07:03:59 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Silva", "Amila", ""], ["Luo", "Ling", ""], ["Karunasekera", "Shanika", ""], ["Leckie", "Christopher", ""]]}, {"id": "2102.06343", "submitter": "Xin Qian", "authors": "Xin Qian, Ryan A. Rossi, Fan Du, Sungchul Kim, Eunyee Koh, Sana Malik,\n  Tak Yeon Lee, Nesreen K. Ahmed", "title": "Personalized Visualization Recommendation", "comments": "37 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visualization recommendation work has focused solely on scoring\nvisualizations based on the underlying dataset and not the actual user and\ntheir past visualization feedback. These systems recommend the same\nvisualizations for every user, despite that the underlying user interests,\nintent, and visualization preferences are likely to be fundamentally different,\nyet vitally important. In this work, we formally introduce the problem of\npersonalized visualization recommendation and present a generic learning\nframework for solving it. In particular, we focus on recommending\nvisualizations personalized for each individual user based on their past\nvisualization interactions (e.g., viewed, clicked, manually created) along with\nthe data from those visualizations. More importantly, the framework can learn\nfrom visualizations relevant to other users, even if the visualizations are\ngenerated from completely different datasets. Experiments demonstrate the\neffectiveness of the approach as it leads to higher quality visualization\nrecommendations tailored to the specific user intent and preferences. To\nsupport research on this new problem, we release our user-centric visualization\ncorpus consisting of 17.4k users exploring 94k datasets with 2.3 million\nattributes and 32k user-generated visualizations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 04:06:34 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Qian", "Xin", ""], ["Rossi", "Ryan A.", ""], ["Du", "Fan", ""], ["Kim", "Sungchul", ""], ["Koh", "Eunyee", ""], ["Malik", "Sana", ""], ["Lee", "Tak Yeon", ""], ["Ahmed", "Nesreen K.", ""]]}, {"id": "2102.06401", "submitter": "Gang Wang", "authors": "Gang Wang, Ziyi Guo, Xiang Li, Dawei Yin, Shuai Ma", "title": "SceneRec: Scene-Based Graph Neural Networks for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collaborative filtering has been largely used to advance modern recommender\nsystems to predict user preference. A key component in collaborative filtering\nis representation learning, which aims to project users and items into a low\ndimensional space to capture collaborative signals. However, the scene\ninformation, which has effectively guided many recommendation tasks, is rarely\nconsidered in existing collaborative filtering methods. To bridge this gap, we\nfocus on scene-based collaborative recommendation and propose a novel\nrepresentation model SceneRec. SceneRec formally defines a scene as a set of\npre-defined item categories that occur simultaneously in real-life situations\nand creatively designs an item-category-scene hierarchical structure to build a\nscene-based graph. In the scene-based graph, we adopt graph neural networks to\nlearn scene-specific representation on each item node, which is further\naggregated with latent representation learned from collaborative interactions\nto make recommendations. We perform extensive experiments on real-world\nE-commerce datasets and the results demonstrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 09:06:12 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Wang", "Gang", ""], ["Guo", "Ziyi", ""], ["Li", "Xiang", ""], ["Yin", "Dawei", ""], ["Ma", "Shuai", ""]]}, {"id": "2102.06429", "submitter": "Yiping Jin", "authors": "Yiping Jin, Vishakha Kadam, Dittaya Wanvarie", "title": "Bootstrapping Large-Scale Fine-Grained Contextual Advertising Classifier\n  from Wikipedia", "comments": "Accepted to TextGraphs-15 Workshop @ NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contextual advertising provides advertisers with the opportunity to target\nthe context which is most relevant to their ads. However, its power cannot be\nfully utilized unless we can target the page content using fine-grained\ncategories, e.g., \"coupe\" vs. \"hatchback\" instead of \"automotive\" vs. \"sport\".\nThe widely used advertising content taxonomy (IAB taxonomy) consists of 23\ncoarse-grained categories and 355 fine-grained categories. With the large\nnumber of categories, it becomes very challenging either to collect training\ndocuments to build a supervised classification model, or to compose\nexpert-written rules in a rule-based classification system. Besides, in\nfine-grained classification, different categories often overlap or co-occur,\nmaking it harder to classify accurately. In this work, we propose wiki2cat, a\nmethod to tackle the problem of large-scaled fine-grained text classification\nby tapping on Wikipedia category graph. The categories in IAB taxonomy are\nfirst mapped to category nodes in the graph. Then the label is propagated\nacross the graph to obtain a list of labeled Wikipedia documents to induce text\nclassifiers. The method is ideal for large-scale classification problems since\nit does not require any manually-labeled document or hand-curated rules or\nkeywords. The proposed method is benchmarked with various learning-based and\nkeyword-based baselines and yields competitive performance on both publicly\navailable datasets and a new dataset containing more than 300 fine-grained\ncategories.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 10:18:25 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 03:28:19 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Jin", "Yiping", ""], ["Kadam", "Vishakha", ""], ["Wanvarie", "Dittaya", ""]]}, {"id": "2102.06543", "submitter": "Matthieu Latapy", "authors": "Fr\\'ed\\'eric Simard and Cl\\'emence Magnien and Matthieu Latapy", "title": "Computing Betweenness Centrality in Link Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Betweeness centrality is one of the most important concepts in graph\nanalysis. It was recently extended to link streams, a graph generalization\nwhere links arrive over time. However, its computation raises non-trivial\nissues, due in particular to the fact that time is considered as continuous. We\nprovide here the first algorithms to compute this generalized betweenness\ncentrality, as well as several companion algorithms that have their own\ninterest. They work in polynomial time and space, we illustrate them on typical\nexamples, and we provide an implementation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 14:12:15 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Simard", "Fr\u00e9d\u00e9ric", ""], ["Magnien", "Cl\u00e9mence", ""], ["Latapy", "Matthieu", ""]]}, {"id": "2102.06634", "submitter": "Alexander Felfernig", "authors": "Alexander Felfernig and Viet-Man Le and Andrei Popescu and Mathias Uta\n  and Thi Ngoc Trang Tran and M\\\"usl\\\"uum Atas", "title": "An Overview of Recommender Systems and Machine Learning in Feature\n  Modeling and Configuration", "comments": "Proceedings of ACM Vamos 2021", "journal-ref": null, "doi": "10.1145/3442391.3442408", "report-no": null, "categories": "cs.IR cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems support decisions in various domains ranging from simple\nitems such as books and movies to more complex items such as financial\nservices, telecommunication equipment, and software systems. In this context,\nrecommendations are determined, for example, on the basis of analyzing the\npreferences of similar users. In contrast to simple items which can be\nenumerated in an item catalog, complex items have to be represented on the\nbasis of variability models (e.g., feature models) since a complete enumeration\nof all possible configurations is infeasible and would trigger significant\nperformance issues. In this paper, we give an overview of a potential new line\nof research which is related to the application of recommender systems and\nmachine learning techniques in feature modeling and configuration. In this\ncontext, we give examples of the application of recommender systems and machine\nlearning and discuss future research issues.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 17:21:36 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Felfernig", "Alexander", ""], ["Le", "Viet-Man", ""], ["Popescu", "Andrei", ""], ["Uta", "Mathias", ""], ["Tran", "Thi Ngoc Trang", ""], ["Atas", "M\u00fcsl\u00fcum", ""]]}, {"id": "2102.06687", "submitter": "Hongliu Cao", "authors": "Hongliu Cao, Eoin Thomas", "title": "Destination similarity based on implicit user interest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the digitization of travel industry, it is more and more important to\nunderstand users from their online behaviors. However, online travel industry\ndata are more challenging to analyze due to extra sparseness, dispersed user\nhistory actions, fast change of user interest and lack of direct or indirect\nfeedbacks. In this work, a new similarity method is proposed to measure the\ndestination similarity in terms of implicit user interest. By comparing the\nproposed method to several other widely used similarity measures in recommender\nsystems, the proposed method achieves a significant improvement on travel data.\nKey words: Destination similarity, Travel industry, Recommender System,\nImplicit user interest\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 18:45:23 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 15:36:36 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Cao", "Hongliu", ""], ["Thomas", "Eoin", ""]]}, {"id": "2102.06732", "submitter": "Jiapeng Wang", "authors": "Jiapeng Wang, Chongyu Liu, Lianwen Jin, Guozhi Tang, Jiaxin Zhang,\n  Shuaitao Zhang, Qianying Wang, Yaqiang Wu, Mingxiang Cai", "title": "Towards Robust Visual Information Extraction in Real World: New Dataset\n  and Novel Solution", "comments": "8 pages, 5 figures, to be published in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual information extraction (VIE) has attracted considerable attention\nrecently owing to its various advanced applications such as document\nunderstanding, automatic marking and intelligent education. Most existing works\ndecoupled this problem into several independent sub-tasks of text spotting\n(text detection and recognition) and information extraction, which completely\nignored the high correlation among them during optimization. In this paper, we\npropose a robust visual information extraction system (VIES) towards real-world\nscenarios, which is a unified end-to-end trainable framework for simultaneous\ntext detection, recognition and information extraction by taking a single\ndocument image as input and outputting the structured information.\nSpecifically, the information extraction branch collects abundant visual and\nsemantic representations from text spotting for multimodal feature fusion and\nconversely, provides higher-level semantic clues to contribute to the\noptimization of text spotting. Moreover, regarding the shortage of public\nbenchmarks, we construct a fully-annotated dataset called EPHOIE\n(https://github.com/HCIILAB/EPHOIE), which is the first Chinese benchmark for\nboth text spotting and visual information extraction. EPHOIE consists of 1,494\nimages of examination paper head with complex layouts and background, including\na total of 15,771 Chinese handwritten or printed text instances. Compared with\nthe state-of-the-art methods, our VIES shows significant superior performance\non the EPHOIE dataset and achieves a 9.01% F-score gain on the widely used\nSROIE dataset under the end-to-end scenario.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 11:05:24 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Wang", "Jiapeng", ""], ["Liu", "Chongyu", ""], ["Jin", "Lianwen", ""], ["Tang", "Guozhi", ""], ["Zhang", "Jiaxin", ""], ["Zhang", "Shuaitao", ""], ["Wang", "Qianying", ""], ["Wu", "Yaqiang", ""], ["Cai", "Mingxiang", ""]]}, {"id": "2102.06762", "submitter": "Nikos Voskarides", "authors": "Nikos Voskarides", "title": "Supporting search engines with knowledge and context", "comments": "PhD thesis of Nikos Voskarides", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines leverage knowledge to improve information access. In order to\neffectively leverage knowledge, search engines should account for context,\ni.e., information about the user and query. In this thesis, we aim to support\nsearch engines in leveraging knowledge while accounting for context. In the\nfirst part of this thesis, we study how to make structured knowledge more\naccessible to the user when the search engine proactively provides such\nknowledge as context to enrich search results. As a first task, we study how to\nretrieve descriptions of knowledge facts from a text corpus. Next, we study how\nto automatically generate knowledge fact descriptions. And finally, we study\nhow to contextualize knowledge facts, that is, to automatically find facts\nrelated to a query fact. In the second part of this thesis, we study how to\nimprove interactive knowledge gathering. We focus on conversational search,\nwhere the user interacts with the search engine to gather knowledge over large\nunstructured knowledge repositories. We focus on multi-turn passage retrieval\nas an instance of conversational search. We propose to model query resolution\nas a term classification task and propose a method to address it. In the final\npart of this thesis, we focus on search engine support for professional writers\nin the news domain. We study how to support such writers create\nevent-narratives by exploring knowledge from a corpus of news articles. We\npropose a dataset construction procedure for this task that relies on existing\nnews articles to simulate incomplete narratives and relevant articles. We study\nthe performance of multiple rankers, lexical and semantic, and provide insights\ninto the characteristics of this task.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 20:28:25 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Voskarides", "Nikos", ""]]}, {"id": "2102.06815", "submitter": "Leonid Boytsov", "authors": "Leonid Boytsov, Zico Kolter", "title": "Exploring Classic and Neural Lexical Translation Models for Information\n  Retrieval: Interpretability, Effectiveness, and Efficiency Benefits", "comments": null, "journal-ref": "ECIR 2021 (The 43rd European Conference on Information Retrieval)", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the utility of the lexical translation model (IBM Model 1) for\nEnglish text retrieval, in particular, its neural variants that are trained\nend-to-end. We use the neural Model1 as an aggregator layer applied to\ncontext-free or contextualized query/document embeddings. This new approach to\ndesign a neural ranking system has benefits for effectiveness, efficiency, and\ninterpretability. Specifically, we show that adding an interpretable neural\nModel 1 layer on top of BERT-based contextualized embeddings (1) does not\ndecrease accuracy and/or efficiency; and (2) may overcome the limitation on the\nmaximum sequence length of existing BERT models. The context-free neural Model\n1 is less effective than a BERT-based ranking model, but it can run efficiently\non a CPU (without expensive index-time precomputation or query-time operations\non large tensors). Using Model 1 we produced best neural and non-neural runs on\nthe MS MARCO document ranking leaderboard in late 2020.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 23:21:55 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 18:43:24 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Boytsov", "Leonid", ""], ["Kolter", "Zico", ""]]}, {"id": "2102.06950", "submitter": "Hui Li", "authors": "Si Chen, Yuqiu Qian, Hui Li, Chen Lin", "title": "Sequential Recommendation in Online Games with Multiple Sequences, Tasks\n  and User Levels", "comments": "Accepted in SSTD'21", "journal-ref": null, "doi": "10.1145/3469830.3470906", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online gaming is growing faster than ever before, with increasing challenges\nof providing better user experience. Recommender systems (RS) for online games\nface unique challenges since they must fulfill players' distinct desires, at\ndifferent user levels, based on their action sequences of various action types.\nAlthough many sequential RS already exist, they are mainly single-sequence,\nsingle-task, and single-user-level. In this paper, we introduce a new\nsequential recommendation model for multiple sequences, multiple tasks, and\nmultiple user levels (abbreviated as M$^3$Rec) in Tencent Games platform, which\ncan fully utilize complex data in online games. We leverage Graph Neural\nNetwork and multi-task learning to design M$^3$Rec in order to model the\ncomplex information in the heterogeneous sequential recommendation scenario of\nTencent Games. We verify the effectiveness of M$^3$Rec on three online games of\nTencent Games platform, in both offline and online evaluations. The results\nshow that M$^3$Rec successfully addresses the challenges of recommendation in\nonline games, and it generates superior recommendations compared with\nstate-of-the-art sequential recommendation approaches.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 16:02:14 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 13:16:16 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chen", "Si", ""], ["Qian", "Yuqiu", ""], ["Li", "Hui", ""], ["Lin", "Chen", ""]]}, {"id": "2102.06964", "submitter": "Kiran Sharma Dr.", "authors": "Parul Khurana and Kiran Sharma", "title": "Impact of h-index on authors ranking: A comparative analysis of Scopus\n  and WoS", "comments": "9 pages, 5 figures, 3 Tables Submitted to the 18th International\n  Conference on Scientometrics and Informetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In academia, the research performance of the faculty members is either\nevaluated by the number of publications or the number of citations. Most of the\ntime h-index is widely used during the hiring process or the faculty\nperformance evaluation. The calculation of the h-index is shown in various\ndatabases; however, there is no recent or systematic evidence about the\ndifferences between them. In this study, we compare the difference in the\nh-index compiled with Scopus and Web of Science (WoS) with the aim of analyzing\nthe ranking of the authors within a university. We analyze the publication\nrecords of 350 authors from Monash University (Australia). We also investigate\nthe discipline wise variation in the authors ranking. 31% of the author's\nprofiles show no variation in the two datasets whereas 55% of the author's\nprofiles show a higher count in Scopus and 9% in WoS. The maximum difference in\nh-index count among Scopus and WoS is 3. On average 12.4% of publications per\nauthor are unique in Scopus and 4.1% in WoS. 53.5% of publications are common\nin both Scopus and WoS. Despite larger unique publications in Scopus, there is\nno difference shown in the Spearman correlation coefficient between WoS and\nScopus citation counts and h-index.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 17:29:52 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Khurana", "Parul", ""], ["Sharma", "Kiran", ""]]}, {"id": "2102.06989", "submitter": "Hao Zheng", "authors": "Hao Zheng, Md Rubel Ahmed, Parijat Mukherjee, Mahesh C. Ketkar, Jin\n  Yang", "title": "Model Synthesis for Communication Traces of System-on-Chip Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concise and abstract models of system-level behaviors are invaluable in\ndesign analysis, testing, and validation. In this paper, we consider the\nproblem of inferring models from communication traces of system-on-chip~(SoC)\ndesigns. The traces capture communications among different blocks of a SoC\ndesign in terms of messages exchanged. The extracted models characterize the\nsystem-level communication protocols governing how blocks exchange messages,\nand coordinate with each other to realize various system functions. In this\npaper, the above problem is formulated as a constraint satisfaction problem,\nwhich is then fed to a SMT solver. The solutions returned by the SMT solver are\nused to extract the models that accept the input traces. In the experiments, we\ndemonstrate the proposed approach with traces collected from a\ntransaction-level simulation model of a multicore SoC design and traces of a\nmore detailed multicore SoC design developed in GEM5 environment.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 19:27:14 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Zheng", "Hao", ""], ["Ahmed", "Md Rubel", ""], ["Mukherjee", "Parijat", ""], ["Ketkar", "Mahesh C.", ""], ["Yang", "Jin", ""]]}, {"id": "2102.07057", "submitter": "Xiang Wang", "authors": "Xiang Wang, Tinglin Huang, Dingxian Wang, Yancheng Yuan, Zhenguang\n  Liu, Xiangnan He, Tat-Seng Chua", "title": "Learning Intents behind Interactions with Knowledge Graph for\n  Recommendation", "comments": "WWW 2021 oral presentation", "journal-ref": null, "doi": "10.1145/3442381.3450133", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graph (KG) plays an increasingly important role in recommender\nsystems. A recent technical trend is to develop end-to-end models founded on\ngraph neural networks (GNNs). However, existing GNN-based models are\ncoarse-grained in relational modeling, failing to (1) identify user-item\nrelation at a fine-grained level of intents, and (2) exploit relation\ndependencies to preserve the semantics of long-range connectivity.\n  In this study, we explore intents behind a user-item interaction by using\nauxiliary item knowledge, and propose a new model, Knowledge Graph-based Intent\nNetwork (KGIN). Technically, we model each intent as an attentive combination\nof KG relations, encouraging the independence of different intents for better\nmodel capability and interpretability. Furthermore, we devise a new information\naggregation scheme for GNN, which recursively integrates the relation sequences\nof long-range connectivity (i.e., relational paths). This scheme allows us to\ndistill useful information about user intents and encode them into the\nrepresentations of users and items. Experimental results on three benchmark\ndatasets show that, KGIN achieves significant improvements over the\nstate-of-the-art methods like KGAT, KGNN-LS, and CKAN. Further analyses show\nthat KGIN offers interpretable explanations for predictions by identifying\ninfluential intents and relational paths. The implementations are available at\nhttps://github.com/huangtinglin/Knowledge_Graph_based_Intent_Network.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 03:21:36 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Wang", "Xiang", ""], ["Huang", "Tinglin", ""], ["Wang", "Dingxian", ""], ["Yuan", "Yancheng", ""], ["Liu", "Zhenguang", ""], ["He", "Xiangnan", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2102.07098", "submitter": "Jiwei Tan", "authors": "Shaowei Yao, Jiwei Tan, Xi Chen, Keping Yang, Rong Xiao, Hongbo Deng\n  and Xiaojun Wan", "title": "Learning a Product Relevance Model from Click-Through Data in E-Commerce", "comments": "Accepted to TheWebConf/WWW 2021", "journal-ref": null, "doi": "10.1145/3442381.3450129", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search engine plays a fundamental role in online e-commerce systems, to\nhelp users find the products they want from the massive product collections.\nRelevance is an essential requirement for e-commerce search, since showing\nproducts that do not match search query intent will degrade user experience.\nWith the existence of vocabulary gap between user language of queries and\nseller language of products, measuring semantic relevance is necessary and\nneural networks are engaged to address this task. However, semantic relevance\nis different from click-through rate prediction in that no direct training\nsignal is available. Most previous attempts learn relevance models from user\nclick-through data that are cheap and abundant. Unfortunately, click behavior\nis noisy and misleading, which is affected by not only relevance but also\nfactors including price, image and attractive titles. Therefore, it is\nchallenging but valuable to learn relevance models from click-through data. In\nthis paper, we propose a new relevance learning framework that concentrates on\nhow to train a relevance model from the weak supervision of click-through data.\nDifferent from previous efforts that treat samples as either relevant or\nirrelevant, we construct more fine-grained samples for training. We propose a\nnovel way to consider samples of different relevance confidence, and come up\nwith a new training objective to learn a robust relevance model with desirable\nscore distribution. The proposed model is evaluated on offline annotated data\nand online A/B testing, and it achieves both promising performance and high\ncomputational efficiency. The model has already been deployed online, serving\nthe search traffic of Taobao for over a year.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 08:05:11 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Yao", "Shaowei", ""], ["Tan", "Jiwei", ""], ["Chen", "Xi", ""], ["Yang", "Keping", ""], ["Xiao", "Rong", ""], ["Deng", "Hongbo", ""], ["Wan", "Xiaojun", ""]]}, {"id": "2102.07134", "submitter": "Marlo H\\\"aring", "authors": "Marlo H\\\"aring and Christoph Stanik and Walid Maalej", "title": "Automatically Matching Bug Reports With Related App Reviews", "comments": "Accepted for publication to the 43rd International Conference on\n  Software Engineering (ICSE21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  App stores allow users to give valuable feedback on apps, and developers to\nfind this feedback and use it for the software evolution. However, finding user\nfeedback that matches existing bug reports in issue trackers is challenging as\nusers and developers often use a different language. In this work, we introduce\nDeepMatcher, an automatic approach using state-of-the-art deep learning methods\nto match problem reports in app reviews to bug reports in issue trackers. We\nevaluated DeepMatcher with four open-source apps quantitatively and\nqualitatively. On average, DeepMatcher achieved a hit ratio of 0.71 and a Mean\nAverage Precision of 0.55. For 91 problem reports, DeepMatcher did not find any\nmatching bug report. When manually analyzing these 91 problem reports and the\nissue trackers of the studied apps, we found that in 47 cases, users actually\ndescribed a problem before developers discovered and documented it in the issue\ntracker. We discuss our findings and different use cases for DeepMatcher.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 11:54:57 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["H\u00e4ring", "Marlo", ""], ["Stanik", "Christoph", ""], ["Maalej", "Walid", ""]]}, {"id": "2102.07142", "submitter": "Zhong Zhao", "authors": "Zhong Zhao, Yanmei Fu, Hanming Liang, Li Ma, Guangyao Zhao, Hongwei\n  Jiang", "title": "Distillation based Multi-task Learning: A Candidate Generation Model for\n  Improving Reading Duration", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In feeds recommendation, the first step is candidate generation. Most of the\ncandidate generation models are based on CTR estimation, which do not consider\nuser's satisfaction with the clicked item. Items with low quality but\nattractive title (i.e., click baits) may be recommended to the user, which\nworsens the user experience. One solution to this problem is to model the click\nand the reading duration simultaneously under the multi-task learning (MTL)\nframework. There are two challenges in the modeling. The first one is how to\ndeal with the zero duration of the negative samples, which does not necessarily\nindicate dislikes. The second one is how to perform multi-task learning in the\ncandidate generation model with double tower structure that can only model one\nsingle task. In this paper, we propose an distillation based multi-task\nlearning (DMTL) approach to tackle these two challenges. We model duration by\nconsidering its dependency of click in the MTL, and then transfer the knowledge\nlearned from the MTL teacher model to the student candidate generation model by\ndistillation. Experiments conducted on dataset gathered from traffic logs of\nTencent Kandian's recommender system show that the proposed approach\noutperforms the competitors significantly in modeling duration, which\ndemonstrates the effectiveness of the proposed candidate generation model.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 12:31:19 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Zhao", "Zhong", ""], ["Fu", "Yanmei", ""], ["Liang", "Hanming", ""], ["Ma", "Li", ""], ["Zhao", "Guangyao", ""], ["Jiang", "Hongwei", ""]]}, {"id": "2102.07279", "submitter": "Keping Bi", "authors": "Keping Bi, Pavel Metrikov, Chunyuan Li, Byungki Byun", "title": "Leveraging User Behavior History for Personalized Email Search", "comments": "In proceedings of the Web Conference 2021", "journal-ref": null, "doi": "10.1145/3442381.3450110", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective email search engine can facilitate users' search tasks and\nimprove their communication efficiency. Users could have varied preferences on\nvarious ranking signals of an email, such as relevance and recency based on\ntheir tasks at hand and even their jobs. Thus a uniform matching pattern is not\noptimal for all users. Instead, an effective email ranker should conduct\npersonalized ranking by taking users' characteristics into account. Existing\nstudies have explored user characteristics from various angles to make email\nsearch results personalized. However, little attention has been given to users'\nsearch history for characterizing users. Although users' historical behaviors\nhave been shown to be beneficial as context in Web search, their effect in\nemail search has not been studied and remains unknown. Given these\nobservations, we propose to leverage user search history as query context to\ncharacterize users and build a context-aware ranking model for email search. In\ncontrast to previous context-dependent ranking techniques that are based on raw\ntexts, we use ranking features in the search history. This frees us from\npotential privacy leakage while giving a better generalization power to unseen\nusers. Accordingly, we propose a context-dependent neural ranking model (CNRM)\nthat encodes the ranking features in users' search history as query context and\nshow that it can significantly outperform the baseline neural model without\nusing the context. We also investigate the benefit of the query context vectors\nobtained from CNRM on the state-of-the-art learning-to-rank model LambdaMart by\nclustering the vectors and incorporating the cluster information. Experimental\nresults show that significantly better results can be achieved on LambdaMart as\nwell, indicating that the query clusters can characterize different users and\neffectively turn the ranking model personalized.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 00:15:46 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 23:02:20 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Bi", "Keping", ""], ["Metrikov", "Pavel", ""], ["Li", "Chunyuan", ""], ["Byun", "Byungki", ""]]}, {"id": "2102.07575", "submitter": "Rahul Ragesh", "authors": "Rahul Ragesh, Sundararajan Sellamanickam, Vijay Lingam, Arun Iyer and\n  Ramakrishna Bairi", "title": "User Embedding based Neighborhood Aggregation Method for Inductive\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning latent features (aka embedding) for users\nand items in a recommendation setting. Given only a user-item interaction\ngraph, the goal is to recommend items for each user. Traditional approaches\nemploy matrix factorization-based collaborative filtering methods. Recent\nmethods using graph convolutional networks (e.g., LightGCN) achieve\nstate-of-the-art performance. They learn both user and item embedding. One\nmajor drawback of most existing methods is that they are not inductive; they do\nnot generalize for users and items unseen during training. Besides, existing\nnetwork models are quite complex, difficult to train and scale. Motivated by\nLightGCN, we propose a graph convolutional network modeling approach for\ncollaborative filtering CF-GCN. We solely learn user embedding and derive item\nembedding using light variant CF-LGCN-U performing neighborhood aggregation,\nmaking it scalable due to reduced model complexity. CF-LGCN-U models naturally\npossess the inductive capability for new items, and we propose a simple\nsolution to generalize for new users. We show how the proposed models are\nrelated to LightGCN. As a by-product, we suggest a simple solution to make\nLightGCN inductive. We perform comprehensive experiments on several benchmark\ndatasets and demonstrate the capabilities of the proposed approach.\nExperimental results show that similar or better generalization performance is\nachievable than the state of the art methods in both transductive and inductive\nsettings.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 14:30:01 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 12:43:13 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Ragesh", "Rahul", ""], ["Sellamanickam", "Sundararajan", ""], ["Lingam", "Vijay", ""], ["Iyer", "Arun", ""], ["Bairi", "Ramakrishna", ""]]}, {"id": "2102.07601", "submitter": "Haiyang Zhang", "authors": "Haiyang Zhang, Ivan Ganchev, Nikola S. Nikolov, Mark Stevenson", "title": "UserReg: A Simple but Strong Model for Rating Prediction", "comments": "Accepted at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Collaborative filtering (CF) has achieved great success in the field of\nrecommender systems. In recent years, many novel CF models, particularly those\nbased on deep learning or graph techniques, have been proposed for a variety of\nrecommendation tasks, such as rating prediction and item ranking. These newly\npublished models usually demonstrate their performance in comparison to\nbaselines or existing models in terms of accuracy improvements. However, others\nhave pointed out that many newly proposed models are not as strong as expected\nand are outperformed by very simple baselines.\n  This paper proposes a simple linear model based on Matrix Factorization (MF),\ncalled UserReg, which regularizes users' latent representations with explicit\nfeedback information for rating prediction. We compare the effectiveness of\nUserReg with three linear CF models that are widely-used as baselines, and with\na set of recently proposed complex models that are based on deep learning or\ngraph techniques. Experimental results show that UserReg achieves overall\nbetter performance than the fine-tuned baselines considered and is highly\ncompetitive when compared with other recently proposed models. We conclude that\nUserReg can be used as a strong baseline for future CF research.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 15:44:29 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Zhang", "Haiyang", ""], ["Ganchev", "Ivan", ""], ["Nikolov", "Nikola S.", ""], ["Stevenson", "Mark", ""]]}, {"id": "2102.07619", "submitter": "Zhang Junlin", "authors": "Zhiqiang Wang, Qingyun She, Junlin Zhang", "title": "MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models\n  by Instance-Guided Mask", "comments": "In Proceedings of DLP-KDD 2021. ACM,Singapore. arXiv admin note: text\n  overlap with arXiv:2006.12753", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Click-Through Rate(CTR) estimation has become one of the most fundamental\ntasks in many real-world applications and it's important for ranking models to\neffectively capture complex high-order features. Shallow feed-forward network\nis widely used in many state-of-the-art DNN models such as FNN, DeepFM and\nxDeepFM to implicitly capture high-order feature interactions. However, some\nresearch has proved that addictive feature interaction, particular feed-forward\nneural networks, is inefficient in capturing common feature interaction. To\nresolve this problem, we introduce specific multiplicative operation into DNN\nranking system by proposing instance-guided mask which performs element-wise\nproduct both on the feature embedding and feed-forward layers guided by input\ninstance. We also turn the feed-forward layer in DNN model into a mixture of\naddictive and multiplicative feature interactions by proposing MaskBlock in\nthis paper. MaskBlock combines the layer normalization, instance-guided mask,\nand feed-forward layer and it is a basic building block to be used to design\nnew ranking model under various configurations. The model consisting of\nMaskBlock is called MaskNet in this paper and two new MaskNet models are\nproposed to show the effectiveness of MaskBlock as basic building block for\ncomposing high performance ranking systems. The experiment results on three\nreal-world datasets demonstrate that our proposed MaskNet models outperform\nstate-of-the-art models such as DeepFM and xDeepFM significantly, which implies\nMaskBlock is an effective basic building unit for composing new high\nperformance ranking systems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 12:27:49 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 08:31:31 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wang", "Zhiqiang", ""], ["She", "Qingyun", ""], ["Zhang", "Junlin", ""]]}, {"id": "2102.07631", "submitter": "Ilya Safro", "authors": "Ilya Tyagin and Ankit Kulshrestha and Justin Sybrandt and Krish Matta\n  and Michael Shtutman and Ilya Safro", "title": "Accelerating COVID-19 research with graph mining and transformer-based\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In 2020, the White House released the, \"Call to Action to the Tech Community\non New Machine Readable COVID-19 Dataset,\" wherein artificial intelligence\nexperts are asked to collect data and develop text mining techniques that can\nhelp the science community answer high-priority scientific questions related to\nCOVID-19. The Allen Institute for AI and collaborators announced the\navailability of a rapidly growing open dataset of publications, the COVID-19\nOpen Research Dataset (CORD-19). As the pace of research accelerates,\nbiomedical scientists struggle to stay current. To expedite their\ninvestigations, scientists leverage hypothesis generation systems, which can\nautomatically inspect published papers to discover novel implicit connections.\nWe present an automated general purpose hypothesis generation systems AGATHA-C\nand AGATHA-GP for COVID-19 research. The systems are based on graph-mining and\nthe transformer model. The systems are massively validated using retrospective\ninformation rediscovery and proactive analysis involving human-in-the-loop\nexpert analysis. Both systems achieve high-quality predictions across domains\n(in some domains up to 0.97% ROC AUC) in fast computational time and are\nreleased to the broad scientific community to accelerate biomedical research.\nIn addition, by performing the domain expert curated study, we show that the\nsystems are able to discover on-going research findings such as the\nrelationship between COVID-19 and oxytocin hormone.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 15:11:36 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Tyagin", "Ilya", ""], ["Kulshrestha", "Ankit", ""], ["Sybrandt", "Justin", ""], ["Matta", "Krish", ""], ["Shtutman", "Michael", ""], ["Safro", "Ilya", ""]]}, {"id": "2102.07640", "submitter": "Jinjiang Guo Ph.D.", "authors": "Yutong Jin, Jie Li, Xinyu Wang, Peiyao Li, Jinjiang Guo, Junfeng Wu,\n  Dawei Leng, Lurong Pan", "title": "Real-time tracking of COVID-19 and coronavirus research updates through\n  text mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The novel coronavirus (SARS-CoV-2) which causes COVID-19 is an ongoing\npandemic. There are ongoing studies with up to hundreds of publications\nuploaded to databases daily. We are exploring the use-case of artificial\nintelligence and natural language processing in order to efficiently sort\nthrough these publications. We demonstrate that clinical trial information,\npreclinical studies, and a general topic model can be used as text mining data\nintelligence tools for scientists all over the world to use as a resource for\ntheir own research. To evaluate our method, several metrics are used to measure\nthe information extraction and clustering results. In addition, we demonstrate\nthat our workflow not only have a use-case for COVID-19, but for other disease\nareas as well. Overall, our system aims to allow scientists to more efficiently\nresearch coronavirus. Our automatically updating modules are available on our\ninformation portal at https://ghddi-ailab.github.io/Targeting2019-nCoV/ for\npublic viewing.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 04:09:42 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Jin", "Yutong", ""], ["Li", "Jie", ""], ["Wang", "Xinyu", ""], ["Li", "Peiyao", ""], ["Guo", "Jinjiang", ""], ["Wu", "Junfeng", ""], ["Leng", "Dawei", ""], ["Pan", "Lurong", ""]]}, {"id": "2102.07645", "submitter": "Hoyeop Lee", "authors": "Hoyeop Lee, Jinbae Im, Chang Ouk Kim, Sehee Chung", "title": "Freudian and Newtonian Recurrent Cell for Sequential Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A sequential recommender system aims to recommend attractive items to users\nbased on behaviour patterns. The predominant sequential recommendation models\nare based on natural language processing models, such as the gated recurrent\nunit, that embed items in some defined space and grasp the user's long-term and\nshort-term preferences based on the item embeddings. However, these approaches\nlack fundamental insight into how such models are related to the user's\ninherent decision-making process. To provide this insight, we propose a novel\nrecurrent cell, namely FaNC, from Freudian and Newtonian perspectives. FaNC\ndivides the user's state into conscious and unconscious states, and the user's\ndecision process is modelled by Freud's two principles: the pleasure principle\nand reality principle. To model the pleasure principle, i.e., free-floating\nuser's instinct, we place the user's unconscious state and item embeddings in\nthe same latent space and subject them to Newton's law of gravitation.\nMoreover, to recommend items to users, we model the reality principle, i.e.,\nbalancing the conscious and unconscious states, via a gating function. Based on\nextensive experiments on various benchmark datasets, this paper provides\ninsight into the characteristics of the proposed model. FaNC initiates a new\ndirection of sequential recommendations at the convergence of psychoanalysis\nand recommender systems.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 12:46:23 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Lee", "Hoyeop", ""], ["Im", "Jinbae", ""], ["Kim", "Chang Ouk", ""], ["Chung", "Sehee", ""]]}, {"id": "2102.07654", "submitter": "Yassine Himeur", "authors": "Yassine Himeur, Abdullah Alsalemi, Ayman Al-Kababji, Faycal Bensaali,\n  Abbes Amira, Christos Sardianos, George Dimitrakopoulos, Iraklis Varlamis", "title": "A survey of recommender systems for energy efficiency in buildings:\n  Principles, challenges and prospects", "comments": "35 pages, 11 figures, 1 table", "journal-ref": "Information Fusion 2021", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have significantly developed in recent years in parallel\nwith the witnessed advancements in both internet of things (IoT) and artificial\nintelligence (AI) technologies. Accordingly, as a consequence of IoT and AI,\nmultiple forms of data are incorporated in these systems, e.g. social,\nimplicit, local and personal information, which can help in improving\nrecommender systems' performance and widen their applicability to traverse\ndifferent disciplines. On the other side, energy efficiency in the building\nsector is becoming a hot research topic, in which recommender systems play a\nmajor role by promoting energy saving behavior and reducing carbon emissions.\nHowever, the deployment of the recommendation frameworks in buildings still\nneeds more investigations to identify the current challenges and issues, where\ntheir solutions are the keys to enable the pervasiveness of research findings,\nand therefore, ensure a large-scale adoption of this technology. Accordingly,\nthis paper presents, to the best of the authors' knowledge, the first timely\nand comprehensive reference for energy-efficiency recommendation systems\nthrough (i) surveying existing recommender systems for energy saving in\nbuildings; (ii) discussing their evolution; (iii) providing an original\ntaxonomy of these systems based on specified criteria, including the nature of\nthe recommender engine, its objective, computing platforms, evaluation metrics\nand incentive measures; and (iv) conducting an in-depth, critical analysis to\nidentify their limitations and unsolved issues. The derived challenges and\nareas of future implementation could effectively guide the energy research\ncommunity to improve the energy-efficiency in buildings and reduce the cost of\ndeveloped recommender systems-based solutions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 16:38:13 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Himeur", "Yassine", ""], ["Alsalemi", "Abdullah", ""], ["Al-Kababji", "Ayman", ""], ["Bensaali", "Faycal", ""], ["Amira", "Abbes", ""], ["Sardianos", "Christos", ""], ["Dimitrakopoulos", "George", ""], ["Varlamis", "Iraklis", ""]]}, {"id": "2102.07662", "submitter": "Bhaskar Mitra", "authors": "Nick Craswell, Bhaskar Mitra, Emine Yilmaz and Daniel Campos", "title": "Overview of the TREC 2020 deep learning track", "comments": "arXiv admin note: substantial text overlap with arXiv:2003.07820", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the second year of the TREC Deep Learning Track, with the goal of\nstudying ad hoc ranking in the large training data regime. We again have a\ndocument retrieval task and a passage retrieval task, each with hundreds of\nthousands of human-labeled training queries. We evaluate using single-shot\nTREC-style evaluation, to give us a picture of which ranking methods work best\nwhen large data is available, with much more comprehensive relevance labeling\non the small number of test queries. This year we have further evidence that\nrankers with BERT-style pretraining outperform other rankers in the large data\nregime.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 16:47:00 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Craswell", "Nick", ""], ["Mitra", "Bhaskar", ""], ["Yilmaz", "Emine", ""], ["Campos", "Daniel", ""]]}, {"id": "2102.07825", "submitter": "Alexander Felfernig", "authors": "Martin Stettinger and Trang Tran and Ingo Pribik and Gerhard Leitner\n  and Alexander Felfernig and Ralph Samer and Muesluem Atas and Manfred Wundara", "title": "KnowledgeCheckR: Intelligent Techniques for Counteracting Forgetting", "comments": "M. Stettinger, T. N. T. Tran, I. Pribik, G. Leitner, A. Felfernig, R.\n  Samer, M. Atas, and M. Wundara. KNOWLEDGECHECKR: Intelligent Techniques for\n  Counteracting Forgetting, The 24th European Conference on Artificial\n  Intelligence (ECAI 2020), pp. 3034-3039, Santiago de Compostela, Spain, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing e-learning environments primarily focus on the aspect of providing\nintuitive learning contents and to recommend learning units in a personalized\nfashion. The major focus of the KnowledgeCheckR environment is to take into\naccount forgetting processes which immediately start after a learning unit has\nbeen completed. In this context, techniques are needed that are able to predict\nwhich learning units are the most relevant ones to be repeated in future\nlearning sessions. In this paper, we provide an overview of the recommendation\napproaches integrated in KnowledgeCheckR. Examples thereof are utility-based\nrecommendation that helps to identify learning contents to be repeated in the\nfuture, collaborative filtering approaches that help to implement session-based\nrecommendation, and content-based recommendation that supports intelligent\nquestion answering. In order to show the applicability of the presented\ntechniques, we provide an overview of the results of empirical studies that\nhave been conducted in real-world scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 20:06:28 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Stettinger", "Martin", ""], ["Tran", "Trang", ""], ["Pribik", "Ingo", ""], ["Leitner", "Gerhard", ""], ["Felfernig", "Alexander", ""], ["Samer", "Ralph", ""], ["Atas", "Muesluem", ""], ["Wundara", "Manfred", ""]]}, {"id": "2102.07831", "submitter": "Przemys\u00c5\u0082aw Pobrotyn", "authors": "Przemys{\\l}aw Pobrotyn and Rados{\\l}aw Bia{\\l}obrzeski", "title": "NeuralNDCG: Direct Optimisation of a Ranking Metric via Differentiable\n  Relaxation of Sorting", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning to Rank (LTR) algorithms are usually evaluated using Information\nRetrieval metrics like Normalised Discounted Cumulative Gain (NDCG) or Mean\nAverage Precision. As these metrics rely on sorting predicted items' scores\n(and thus, on items' ranks), their derivatives are either undefined or zero\neverywhere. This makes them unsuitable for gradient-based optimisation, which\nis the usual method of learning appropriate scoring functions. Commonly used\nLTR loss functions are only loosely related to the evaluation metrics, causing\na mismatch between the optimisation objective and the evaluation criterion. In\nthis paper, we address this mismatch by proposing NeuralNDCG, a novel\ndifferentiable approximation to NDCG. Since NDCG relies on the\nnon-differentiable sorting operator, we obtain NeuralNDCG by relaxing that\noperator using NeuralSort, a differentiable approximation of sorting. As a\nresult, we obtain a new ranking loss function which is an arbitrarily accurate\napproximation to the evaluation metric, thus closing the gap between the\ntraining and the evaluation of LTR models. We introduce two variants of the\nproposed loss function. Finally, the empirical evaluation shows that our\nproposed method outperforms previous work aimed at direct optimisation of NDCG\nand is competitive with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 20:11:42 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 08:50:53 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Pobrotyn", "Przemys\u0142aw", ""], ["Bia\u0142obrzeski", "Rados\u0142aw", ""]]}, {"id": "2102.07919", "submitter": "Haolan Zhan", "authors": "Haolan Zhan, Hainan Zhang, Hongshen Chen, Lei Shen, Yanyan Lan, Zhuoye\n  Ding, Dawei Yin", "title": "User-Inspired Posterior Network for Recommendation Reason Generation", "comments": "SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation reason generation, aiming at showing the selling points of\nproducts for customers, plays a vital role in attracting customers' attention\nas well as improving user experience. A simple and effective way is to extract\nkeywords directly from the knowledge-base of products, i.e., attributes or\ntitle, as the recommendation reason. However, generating recommendation reason\nfrom product knowledge doesn't naturally respond to users' interests.\nFortunately, on some E-commerce websites, there exists more and more\nuser-generated content (user-content for short), i.e., product\nquestion-answering (QA) discussions, which reflect user-cared aspects.\nTherefore, in this paper, we consider generating the recommendation reason by\ntaking into account not only the product attributes but also the\ncustomer-generated product QA discussions. In reality, adequate user-content is\nonly possible for the most popular commodities, whereas large sums of long-tail\nproducts or new products cannot gather a sufficient number of user-content. To\ntackle this problem, we propose a user-inspired multi-source posterior\ntransformer (MSPT), which induces the model reflecting the users' interests\nwith a posterior multiple QA discussions module, and generating recommendation\nreasons containing the product attributes as well as the user-cared aspects.\nExperimental results show that our model is superior to traditional generative\nmodels. Additionally, the analysis also shows that our model can focus more on\nthe user-cared aspects than baselines.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 02:08:52 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Zhan", "Haolan", ""], ["Zhang", "Hainan", ""], ["Chen", "Hongshen", ""], ["Shen", "Lei", ""], ["Lan", "Yanyan", ""], ["Ding", "Zhuoye", ""], ["Yin", "Dawei", ""]]}, {"id": "2102.08113", "submitter": "Alexander Felfernig", "authors": "Alexander Felfernig and Stefan Reiterer and Martin Stettinger and\n  Florian Reinfrank and Michael Jeran and Gerald Ninaus", "title": "Recommender Systems for Configuration Knowledge Engineering", "comments": "A. Felfernig S, Reiterer, M. Stettinger, F. Reinfrank, M. Jeran, and\n  G. Ninaus. Recommender Systems for Configuration Knowledge Engineering,\n  Workshop on Configuration, Vienna, Austria, pp. 51-54, ISBN:\n  979-10-91526-02-9, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knowledge engineering bottleneck is still a major challenge in\nconfigurator projects. In this paper we show how recommender systems can\nsupport knowledge base development and maintenance processes. We discuss a\ncouple of scenarios for the application of recommender systems in knowledge\nengineering and report the results of empirical studies which show the\nimportance of user-centered configuration knowledge organization.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 12:29:54 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Felfernig", "Alexander", ""], ["Reiterer", "Stefan", ""], ["Stettinger", "Martin", ""], ["Reinfrank", "Florian", ""], ["Jeran", "Michael", ""], ["Ninaus", "Gerald", ""]]}, {"id": "2102.08322", "submitter": "Jiahuan Pei", "authors": "Jiahuan Pei, Pengjie Ren, Maarten de Rijke", "title": "A Cooperative Memory Network for Personalized Task-oriented Dialogue\n  Systems with Incomplete User Profiles", "comments": "In Proceedings of the Web Conference 2021 (WWW '21), April 19-23,\n  2021, Ljubljana, Slovenia. ACM, New York, NY, USA, 10 pages", "journal-ref": null, "doi": "10.1145/3442381.3449843", "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is increasing interest in developing personalized Task-oriented\nDialogue Systems (TDSs). Previous work on personalized TDSs often assumes that\ncomplete user profiles are available for most or even all users. This is\nunrealistic because (1) not everyone is willing to expose their profiles due to\nprivacy concerns; and (2) rich user profiles may involve a large number of\nattributes (e.g., gender, age, tastes, . . .). In this paper, we study\npersonalized TDSs without assuming that user profiles are complete. We propose\na Cooperative Memory Network (CoMemNN) that has a novel mechanism to gradually\nenrich user profiles as dialogues progress and to simultaneously improve\nresponse selection based on the enriched profiles. CoMemNN consists of two core\nmodules: User Profile Enrichment (UPE) and Dialogue Response Selection (DRS).\nThe former enriches incomplete user profiles by utilizing collaborative\ninformation from neighbor users as well as current dialogues. The latter uses\nthe enriched profiles to update the current user query so as to encode more\nuseful information, based on which a personalized response to a user request is\nselected.\n  We conduct extensive experiments on the personalized bAbI dialogue benchmark\ndatasets. We find that CoMemNN is able to enrich user profiles effectively,\nwhich results in an improvement of 3.06% in terms of response selection\naccuracy compared to state-of-the-art methods. We also test the robustness of\nCoMemNN against incompleteness of user profiles by randomly discarding\nattribute values from user profiles. Even when discarding 50% of the attribute\nvalues, CoMemNN is able to match the performance of the best performing\nbaseline without discarding user profiles, showing the robustness of CoMemNN.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 18:05:54 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Pei", "Jiahuan", ""], ["Ren", "Pengjie", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2102.08366", "submitter": "Gabriele Pergola", "authors": "Gabriele Pergola, Elena Kochkina, Lin Gui, Maria Liakata, Yulan He", "title": "Boosting Low-Resource Biomedical QA via Entity-Aware Masking Strategies", "comments": "EACL 2021 - Short Paper - European Chapter of the Association for\n  Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical question-answering (QA) has gained increased attention for its\ncapability to provide users with high-quality information from a vast\nscientific literature. Although an increasing number of biomedical QA datasets\nhas been recently made available, those resources are still rather limited and\nexpensive to produce. Transfer learning via pre-trained language models (LMs)\nhas been shown as a promising approach to leverage existing general-purpose\nknowledge. However, finetuning these large models can be costly and time\nconsuming, often yielding limited benefits when adapting to specific themes of\nspecialised domains, such as the COVID-19 literature. To bootstrap further\ntheir domain adaptation, we propose a simple yet unexplored approach, which we\ncall biomedical entity-aware masking (BEM). We encourage masked language models\nto learn entity-centric knowledge based on the pivotal entities characterizing\nthe domain at hand, and employ those entities to drive the LM fine-tuning. The\nresulting strategy is a downstream process applicable to a wide variety of\nmasked LMs, not requiring additional memory or components in the neural\narchitectures. Experimental results show performance on par with\nstate-of-the-art models on several biomedical QA datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 18:51:13 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Pergola", "Gabriele", ""], ["Kochkina", "Elena", ""], ["Gui", "Lin", ""], ["Liakata", "Maria", ""], ["He", "Yulan", ""]]}, {"id": "2102.08465", "submitter": "Igor L. Markov", "authors": "Xiuyan Ni, Shujian Bu, Igor L. Markov", "title": "Prioritizing Original News on Facebook", "comments": "9 pages, 8 figures, 6 tables, 2 algorithm pseudocodes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work outlines how we prioritize original news, a critical indicator of\nnews quality. By examining the landscape and life-cycle of news posts on our\nsocial media platform, we identify challenges of building and deploying an\noriginality score. We pursue an approach based on normalized PageRank values\nand three-step clustering, and refresh the score on an hourly basis to capture\nthe dynamics of online news. We describe a near real-time system architecture,\nevaluate our methodology, and deploy it to production. Our empirical results\nvalidate individual components and show that prioritizing original news\nincreases user engagement with news and improves proprietary cumulative\nmetrics.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 22:00:08 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 06:10:50 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Ni", "Xiuyan", ""], ["Bu", "Shujian", ""], ["Markov", "Igor L.", ""]]}, {"id": "2102.08673", "submitter": "Bell Raj Eapen", "authors": "Bell Raj Eapen, Feroze Kaliyadan and Ashique Karalikkattil T", "title": "DICODerma: A practical approach for metadata management of images in\n  dermatology", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical images are vital for diagnosing and monitoring skin diseases, and\ntheir importance has increased with the growing popularity of machine learning.\nLack of standards has stifled innovation in dermatological imaging, unlike\nother image-intensive specialties such as radiology. We investigate the\nmeta-requirements for utilizing the popular DICOM standard for metadata\nmanagement of images in dermatology. We propose practical design solutions and\nprovide open-source tools to integrate dermatologists' workflow with enterprise\nimaging systems. Using the tool, dermatologists can tag, search, organize and\nconvert clinical images to the DICOM format. We believe that our less\ndisruptive approach will improve the adoption of standards in the specialty.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 10:34:08 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Eapen", "Bell Raj", ""], ["Kaliyadan", "Feroze", ""], ["T", "Ashique Karalikkattil", ""]]}, {"id": "2102.08795", "submitter": "Nikos Voskarides", "authors": "Svitlana Vakulenko, Nikos Voskarides, Zhucheng Tu, Shayne Longpre", "title": "Leveraging Query Resolution and Reading Comprehension for Conversational\n  Passage Retrieval", "comments": "TREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the participation of UvA.ILPS group at the TREC CAsT\n2020 track. Our passage retrieval pipeline consists of (i) an initial retrieval\nmodule that uses BM25, and (ii) a re-ranking module that combines the score of\na BERT ranking model with the score of a machine comprehension model adjusted\nfor passage retrieval. An important challenge in conversational passage\nretrieval is that queries are often under-specified. Thus, we perform query\nresolution, that is, add missing context from the conversation history to the\ncurrent turn query using QuReTeC, a term classification query resolution model.\nWe show that our best automatic and manual runs outperform the corresponding\nmedian runs by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 14:41:57 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["Voskarides", "Nikos", ""], ["Tu", "Zhucheng", ""], ["Longpre", "Shayne", ""]]}, {"id": "2102.09140", "submitter": "Lei Chen", "authors": "Le Wu, Lei Chen, Pengyang Shao, Richang Hong, Xiting Wang and Meng\n  Wang", "title": "Learning Fair Representations for Recommendation: A Graph-based\n  Perspective", "comments": "The paper is accepted by WWW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a key application of artificial intelligence, recommender systems are\namong the most pervasive computer aided systems to help users find potential\nitems of interests. Recently, researchers paid considerable attention to\nfairness issues for artificial intelligence applications. Most of these\napproaches assumed independence of instances, and designed sophisticated models\nto eliminate the sensitive information to facilitate fairness. However,\nrecommender systems differ greatly from these approaches as users and items\nnaturally form a user-item bipartite graph, and are collaboratively correlated\nin the graph structure. In this paper, we propose a novel graph based technique\nfor ensuring fairness of any recommendation models. Here, the fairness\nrequirements refer to not exposing sensitive feature set in the user modeling\nprocess. Specifically, given the original embeddings from any recommendation\nmodels, we learn a composition of filters that transform each user's and each\nitem's original embeddings into a filtered embedding space based on the\nsensitive feature set. For each user, this transformation is achieved under the\nadversarial learning of a user-centric graph, in order to obfuscate each\nsensitive feature between both the filtered user embedding and the sub graph\nstructures of this user. Finally, extensive experimental results clearly show\nthe effectiveness of our proposed model for fair recommendation. We publish the\nsource code at https://github.com/newlei/FairGo.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 03:33:48 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 09:26:27 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 13:11:39 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Wu", "Le", ""], ["Chen", "Lei", ""], ["Shao", "Pengyang", ""], ["Hong", "Richang", ""], ["Wang", "Xiting", ""], ["Wang", "Meng", ""]]}, {"id": "2102.09182", "submitter": "Muneer Ahmad", "authors": "Muneer Ahmad, Dr M Sadik Batcha, S Roselin Jahina", "title": "Testing Lotka's Law and Pattern of Author Productivity in the Scholarly\n  Publications of Artificial Intelligence", "comments": "17 Pages", "journal-ref": "2716, 2019 https://digitalcommons.unl.edu/libphilprac/2716", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial intelligence has changed our day to day life in multitude ways. AI\ntechnology is rearing itself as a driving force to be reckoned with in the\nlargest industries in the world. AI has already engulfed our educational\nsystem, our businesses and our financial establishments. The future is definite\nthat machines with artificial intelligence will soon be captivating over\ntrained manual work that now is mostly cared by humans. Machines can carry out\nhuman-like tasks by new inputs as artificial intelligence makes it possible for\nmachines to learn from experience. AI data from web of science database from\n2008 to 2017 have been mapped to depict the average growth rate, relative\ngrowth rate, contribution made by authors in the view of research productivity,\nauthorship pattern and collaboration of AI literature. The Lotka's law on\nauthorship productivity of AI literature has been tested to confirm the\napplicability of the law to the present data set. A K-S test was applied to\nmeasure the degree of agreement between the distribution of the observed set of\ndata against the inverse general power relationship and the theoretical value\nof {\\alpha} =2. It is found that the inverse square law of Lotka follow as\nsuch.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 06:49:56 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Ahmad", "Muneer", ""], ["Batcha", "Dr M Sadik", ""], ["Jahina", "S Roselin", ""]]}, {"id": "2102.09185", "submitter": "Jaya Lakshmi T", "authors": "T. Jaya Lakshmi and S. Durga Bhavani", "title": "Link Prediction Approach to Recommender Systems", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The problem of recommender system is very popular with myriad available\nsolutions. A novel approach that uses the link prediction problem in social\nnetworks has been proposed in the literature that model the typical user-item\ninformation as a bipartite network in which link prediction would actually mean\nrecommending an item to a user. The standard recommender system methods suffer\nfrom the problems of sparsity and scalability. Since link prediction measures\ninvolve computations pertaining to small neighborhoods in the network, this\napproach would lead to a scalable solution to recommendation. One of the issues\nin this conversion is that link prediction problem is modelled as a binary\nclassification task whereas the problem of recommender systems is solved as a\nregression task in which the rating of the link is to be predicted. We overcome\nthis issue by predicting top k links as recommendations with high ratings\nwithout predicting the actual rating. Our work extends similar approaches in\nthe literature by focusing on exploiting the probabilistic measures for link\nprediction. Moreover, in the proposed approach, prediction measures that\nutilize temporal information available on the links prove to be more effective\nin improving the accuracy of prediction. This approach is evaluated on the\nbenchmark 'Movielens' dataset. We show that the usage of temporal probabilistic\nmeasures helps in improving the quality of recommendations. Temporal\nrandom-walk based measure T_Flow improves recommendation accuracy by 4% and\nTemporal cooccurrence probability measure improves prediction accuracy by 10%\nover item-based collaborative filtering method in terms of AUROC score.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 07:01:45 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Lakshmi", "T. Jaya", ""], ["Bhavani", "S. Durga", ""]]}, {"id": "2102.09211", "submitter": "Jianxun Lian", "authors": "Jianxun Lian, Iyad Batal, Zheng Liu, Akshay Soni, Eun Yong Kang, Yajun\n  Wang, Xing Xie", "title": "Multi-Interest-Aware User Modeling for Large-Scale Sequential\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Precise user modeling is critical for online personalized recommendation\nservices. Generally, users' interests are diverse and are not limited to a\nsingle aspect, which is particularly evident when their behaviors are observed\nfor a longer time. For example, a user may demonstrate interests in cats/dogs,\ndancing and food \\& delights when browsing short videos on Tik Tok; the same\nuser may show interests in real estate and women's wear in her web browsing\nbehaviors. Traditional models tend to encode a user's behaviors into a single\nembedding vector, which do not have enough capacity to effectively capture her\ndiverse interests.\n  This paper proposes a Sequential User Matrix (SUM) to accurately and\nefficiently capture users' diverse interests. SUM models user behavior with a\nmulti-channel network, with each channel representing a different aspect of the\nuser's interests. User states in different channels are updated by an\n\\emph{erase-and-add} paradigm with interest- and instance-level attention. We\nfurther propose a local proximity debuff component and a highway connection\ncomponent to make the model more robust and accurate. SUM can be maintained and\nupdated incrementally, making it feasible to be deployed for large-scale online\nserving. We conduct extensive experiments on two datasets. Results demonstrate\nthat SUM consistently outperforms state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 08:24:14 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 05:01:09 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 08:45:03 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Lian", "Jianxun", ""], ["Batal", "Iyad", ""], ["Liu", "Zheng", ""], ["Soni", "Akshay", ""], ["Kang", "Eun Yong", ""], ["Wang", "Yajun", ""], ["Xie", "Xing", ""]]}, {"id": "2102.09267", "submitter": "Qiaoyu Tan", "authors": "Qiaoyu Tan, Jianwei Zhang, Jiangchao Yao, Ninghao Liu, Jingren Zhou,\n  Hongxia Yang, Xia Hu", "title": "Sparse-Interest Network for Sequential Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent methods in sequential recommendation focus on learning an overall\nembedding vector from a user's behavior sequence for the next-item\nrecommendation. However, from empirical analysis, we discovered that a user's\nbehavior sequence often contains multiple conceptually distinct items, while a\nunified embedding vector is primarily affected by one's most recent frequent\nactions. Thus, it may fail to infer the next preferred item if conceptually\nsimilar items are not dominant in recent interactions. To this end, an\nalternative solution is to represent each user with multiple embedding vectors\nencoding different aspects of the user's intentions. Nevertheless, recent work\non multi-interest embedding usually considers a small number of concepts\ndiscovered via clustering, which may not be comparable to the large pool of\nitem categories in real systems. It is a non-trivial task to effectively model\na large number of diverse conceptual prototypes, as items are often not\nconceptually well clustered in fine granularity. Besides, an individual usually\ninteracts with only a sparse set of concepts. In light of this, we propose a\nnovel \\textbf{S}parse \\textbf{I}nterest \\textbf{NE}twork (SINE) for sequential\nrecommendation. Our sparse-interest module can adaptively infer a sparse set of\nconcepts for each user from the large concept pool and output multiple\nembeddings accordingly. Given multiple interest embeddings, we develop an\ninterest aggregation module to actively predict the user's current intention\nand then use it to explicitly model multiple interests for next-item\nprediction. Empirical results on several public benchmark datasets and one\nlarge-scale industrial dataset demonstrate that SINE can achieve substantial\nimprovement over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 11:03:48 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Tan", "Qiaoyu", ""], ["Zhang", "Jianwei", ""], ["Yao", "Jiangchao", ""], ["Liu", "Ninghao", ""], ["Zhou", "Jingren", ""], ["Yang", "Hongxia", ""], ["Hu", "Xia", ""]]}, {"id": "2102.09268", "submitter": "Shitao Xiao", "authors": "Shitao Xiao, Zheng Liu, Yingxia Shao, Tao Di and Xing Xie", "title": "Training Large-Scale News Recommenders with Pretrained Language Models\n  in the Loop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  News recommendation calls for deep insights of news articles' underlying\nsemantics. Therefore, pretrained language models (PLMs), like BERT and RoBERTa,\nmay substantially contribute to the recommendation quality. However, it's\nextremely challenging to have news recommenders trained together with such big\nmodels: the learning of news recommenders requires intensive news encoding\noperations, whose cost is prohibitive if PLMs are used as the news encoder. In\nthis paper, we propose a novel framework, {SpeedyFeed}, which efficiently\ntrains PLMs-based news recommenders of superior quality. SpeedyFeed is\nhighlighted for its light-weighted encoding pipeline, which gives rise to three\nmajor advantages. Firstly, it makes the intermedia results fully reusable for\nthe training workflow, which removes most of the repetitive but redundant\nencoding operations. Secondly, it improves the data efficiency of the training\nworkflow, where non-informative data can be eliminated from encoding. Thirdly,\nit further saves the cost by leveraging simplified news encoding and compact\nnews representation. Extensive experiments show that SpeedyFeed leads to more\nthan 100$\\times$ acceleration of the training process, which enables big models\nto be trained efficiently and effectively over massive user data. The\nwell-trained PLMs-based model from SpeedyFeed demonstrates highly competitive\nperformance, where it outperforms the state-of-the-art news recommenders with\nsignificant margins. SpeedyFeed is also a model-agnostic framework, which is\npotentially applicable to a wide spectrum of content-based recommender systems;\ntherefore, the whole framework is open-sourced to facilitate the progress in\nrelated areas.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 11:08:38 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 02:15:26 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Xiao", "Shitao", ""], ["Liu", "Zheng", ""], ["Shao", "Yingxia", ""], ["Di", "Tao", ""], ["Xie", "Xing", ""]]}, {"id": "2102.09269", "submitter": "Qiaoyu Tan", "authors": "Qiaoyu Tan, Jianwei Zhang, Ninghao Liu, Xiao Huang, Hongxia Yang,\n  Jingren Zhou, Xia Hu", "title": "Dynamic Memory based Attention Network for Sequential Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequential recommendation has become increasingly essential in various online\nservices. It aims to model the dynamic preferences of users from their\nhistorical interactions and predict their next items. The accumulated user\nbehavior records on real systems could be very long. This rich data brings\nopportunities to track actual interests of users. Prior efforts mainly focus on\nmaking recommendations based on relatively recent behaviors. However, the\noverall sequential data may not be effectively utilized, as early interactions\nmight affect users' current choices. Also, it has become intolerable to scan\nthe entire behavior sequence when performing inference for each user, since\nreal-world system requires short response time. To bridge the gap, we propose a\nnovel long sequential recommendation model, called Dynamic Memory-based\nAttention Network (DMAN). It segments the overall long behavior sequence into a\nseries of sub-sequences, then trains the model and maintains a set of memory\nblocks to preserve long-term interests of users. To improve memory fidelity,\nDMAN dynamically abstracts each user's long-term interest into its own memory\nblocks by minimizing an auxiliary reconstruction loss. Based on the dynamic\nmemory, the user's short-term and long-term interests can be explicitly\nextracted and combined for efficient joint recommendation. Empirical results\nover four benchmark datasets demonstrate the superiority of our model in\ncapturing long-term dependency over various state-of-the-art sequential models.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 11:08:54 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Tan", "Qiaoyu", ""], ["Zhang", "Jianwei", ""], ["Liu", "Ninghao", ""], ["Huang", "Xiao", ""], ["Yang", "Hongxia", ""], ["Zhou", "Jingren", ""], ["Hu", "Xia", ""]]}, {"id": "2102.09283", "submitter": "Jin Li", "authors": "Jin Li, Jie Liu, Shangzhou Li, Yao Xu, Ran Cao, Qi Li, Biye Jiang,\n  Guan Wang, Han Zhu, Kun Gai, Xiaoqiang Zhu", "title": "Truncation-Free Matching System for Display Advertising at Alibaba", "comments": "9 pages, 3 figures. Submitted to KDD 2021 Applied Data Science track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching module plays a critical role in display advertising systems. Without\nquery from user, it is challenging for system to match user traffic and ads\nsuitably. System packs up a group of users with common properties such as the\nsame gender or similar shopping interests into a crowd. Here term crowd can be\nviewed as a tag over users. Then advertisers bid for different crowds and\ndeliver their ads to those targeted users. Matching module in most industrial\ndisplay advertising systems follows a two-stage paradigm. When receiving a user\nrequest, matching system (i) finds the crowds that the user belongs to; (ii)\nretrieves all ads that have targeted those crowds. However, in applications\nsuch as display advertising at Alibaba, with very large volumes of crowds and\nads, both stages of matching have to truncate the long-tailed parts for online\nserving, under limited latency. That's to say, not all ads have the chance to\nparticipate in online matching. This results in sub-optimal result for both\nadvertising performance and platform revenue. In this paper, we study the\ntruncation problem and propose a Truncation Free Matching System (TFMS). The\nbasic idea is to decouple the matching computation from the online pipeline.\nInstead of executing the two-stage matching when user visits, TFMS utilizes a\nnear-line truncation-free matching to pre-calculate and store those top\nvaluable ads for each user. Then the online pipeline just needs to fetch the\npre-stored ads as matching results. In this way, we can jump out of online\nsystem's latency and computation cost limitations, and leverage flexible\ncomputation resource to finish the user-ad matching. TFMS has been deployed in\nour productive system since 2019, bringing (i) more than 50% improvement of\nimpressions for advertisers who encountered truncation before, (ii) 9.4%\nRevenue Per Mile gain, which is significant enough for the business.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 12:01:09 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Li", "Jin", ""], ["Liu", "Jie", ""], ["Li", "Shangzhou", ""], ["Xu", "Yao", ""], ["Cao", "Ran", ""], ["Li", "Qi", ""], ["Jiang", "Biye", ""], ["Wang", "Guan", ""], ["Zhu", "Han", ""], ["Gai", "Kun", ""], ["Zhu", "Xiaoqiang", ""]]}, {"id": "2102.09375", "submitter": "Zhe Ma", "authors": "Zhe Ma, Fenghao Liu, Jianfeng Dong, Xiaoye Qu, Yuan He, Shouling Ji", "title": "Hierarchical Similarity Learning for Language-based Product Image\n  Retrieval", "comments": "Accepted by ICASSP 2021. Code and data will be available at\n  https://github.com/liufh1/hsl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims for the language-based product image retrieval task. The\nmajority of previous works have made significant progress by designing network\nstructure, similarity measurement, and loss function. However, they typically\nperform vision-text matching at certain granularity regardless of the intrinsic\nmultiple granularities of images. In this paper, we focus on the cross-modal\nsimilarity measurement, and propose a novel Hierarchical Similarity Learning\n(HSL) network. HSL first learns multi-level representations of input data by\nstacked encoders, and object-granularity similarity and image-granularity\nsimilarity are computed at each level. All the similarities are combined as the\nfinal hierarchical cross-modal similarity. Experiments on a large-scale product\nretrieval dataset demonstrate the effectiveness of our proposed method. Code\nand data are available at https://github.com/liufh1/hsl.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 14:23:16 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Ma", "Zhe", ""], ["Liu", "Fenghao", ""], ["Dong", "Jianfeng", ""], ["Qu", "Xiaoye", ""], ["He", "Yuan", ""], ["Ji", "Shouling", ""]]}, {"id": "2102.09388", "submitter": "Rishiraj Saha Roy", "authors": "Azin Ghazimatin, Soumajit Pramanik, Rishiraj Saha Roy, Gerhard Weikum", "title": "ELIXIR: Learning from User Feedback on Explanations to Improve\n  Recommender Models", "comments": "WWW 2021, 11 pages", "journal-ref": null, "doi": "10.1145/3442381.3449848", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System-provided explanations for recommendations are an important component\ntowards transparent and trustworthy AI. In state-of-the-art research, this is a\none-way signal, though, to improve user acceptance. In this paper, we turn the\nrole of explanations around and investigate how they can contribute to\nenhancing the quality of the generated recommendations themselves. We devise a\nhuman-in-the-loop framework, called ELIXIR, where user feedback on explanations\nis leveraged for pairwise learning of user preferences. ELIXIR leverages\nfeedback on pairs of recommendations and explanations to learn user-specific\nlatent preference vectors, overcoming sparseness by label propagation with\nitem-similarity-based neighborhoods. Our framework is instantiated using\ngeneralized graph recommendation via Random Walk with Restart. Insightful\nexperiments with a real user study show significant improvements in movie and\nbook recommendations over item-level feedback.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 13:43:49 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 18:26:05 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 18:50:56 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ghazimatin", "Azin", ""], ["Pramanik", "Soumajit", ""], ["Roy", "Rishiraj Saha", ""], ["Weikum", "Gerhard", ""]]}, {"id": "2102.09389", "submitter": "AnChen Li", "authors": "Anchen Li, Bo Yang", "title": "HSR: Hyperbolic Social Recommender", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prevalence of online social media, users' social connections have\nbeen widely studied and utilized to enhance the performance of recommender\nsystems. In this paper, we explore the use of hyperbolic geometry for social\nrecommendation. We present Hyperbolic Social Recommender (HSR), a novel social\nrecommendation framework that utilizes hyperbolic geometry to boost the\nperformance. With the help of hyperbolic spaces, HSR can learn high-quality\nuser and item representations for better modeling user-item interaction and\nuser-user social relations. Via a series of extensive experiments, we show that\nour proposed HSR outperforms its Euclidean counterpart and state-of-the-art\nsocial recommenders in click-through rate prediction and top-K recommendation,\ndemonstrating the effectiveness of social recommendation in the hyperbolic\nspace.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 12:09:46 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Li", "Anchen", ""], ["Yang", "Bo", ""]]}, {"id": "2102.09395", "submitter": "Nikolaos Livathinos", "authors": "Nikolaos Livathinos (1), Cesar Berrospi (1), Maksym Lysak (1), Viktor\n  Kuropiatnyk (1), Ahmed Nassar (1), Andre Carvalho (1), Michele Dolfi (1),\n  Christoph Auer (1), Kasper Dinkla (1), Peter Staar (1) ((1) IBM Research)", "title": "Robust PDF Document Conversion Using Recurrent Neural Networks", "comments": "9 pages, 2 tables, 4 figures, uses aaai21.sty. Accepted at the\n  \"Thirty-Third Annual Conference on Innovative Applications of Artificial\n  Intelligence (IAAI-21)\". Received the \"IAAI-21 Innovative Application Award\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of published PDF documents has increased exponentially in recent\ndecades. There is a growing need to make their rich content discoverable to\ninformation retrieval tools. In this paper, we present a novel approach to\ndocument structure recovery in PDF using recurrent neural networks to process\nthe low-level PDF data representation directly, instead of relying on a visual\nre-interpretation of the rendered PDF page, as has been proposed in previous\nliterature. We demonstrate how a sequence of PDF printing commands can be used\nas input into a neural network and how the network can learn to classify each\nprinting command according to its structural function in the page. This\napproach has three advantages: First, it can distinguish among more\nfine-grained labels (typically 10-20 labels as opposed to 1-5 with visual\nmethods), which results in a more accurate and detailed document structure\nresolution. Second, it can take into account the text flow across pages more\nnaturally compared to visual methods because it can concatenate the printing\ncommands of sequential pages. Last, our proposed method needs less memory and\nit is computationally less expensive than visual methods. This allows us to\ndeploy such models in production environments at a much lower cost. Through\nextensive architectural search in combination with advanced feature\nengineering, we were able to implement a model that yields a weighted average\nF1 score of 97% across 17 distinct structural labels. The best model we\nachieved is currently served in production environments on our Corpus\nConversion Service (CCS), which was presented at KDD18 (arXiv:1806.02284). This\nmodel enhances the capabilities of CCS significantly, as it eliminates the need\nfor human annotated label ground-truth for every unseen document layout. This\nproved particularly useful when applied to a huge corpus of PDF articles\nrelated to COVID-19.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 14:39:54 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Livathinos", "Nikolaos", "", "IBM Research"], ["Berrospi", "Cesar", "", "IBM Research"], ["Lysak", "Maksym", "", "IBM Research"], ["Kuropiatnyk", "Viktor", "", "IBM Research"], ["Nassar", "Ahmed", "", "IBM Research"], ["Carvalho", "Andre", "", "IBM Research"], ["Dolfi", "Michele", "", "IBM Research"], ["Auer", "Christoph", "", "IBM Research"], ["Dinkla", "Kasper", "", "IBM Research"], ["Staar", "Peter", "", "IBM Research"]]}, {"id": "2102.09460", "submitter": "Daheng Wang", "authors": "Daheng Wang, Prashant Shiralkar, Colin Lockard, Binxuan Huang, Xin\n  Luna Dong, Meng Jiang", "title": "TCN: Table Convolutional Network for Web Table Interpretation", "comments": null, "journal-ref": null, "doi": "10.1145/3442381.3450090", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information extraction from semi-structured webpages provides valuable\nlong-tailed facts for augmenting knowledge graph. Relational Web tables are a\ncritical component containing additional entities and attributes of rich and\ndiverse knowledge. However, extracting knowledge from relational tables is\nchallenging because of sparse contextual information. Existing work linearize\ntable cells and heavily rely on modifying deep language models such as BERT\nwhich only captures related cells information in the same table. In this work,\nwe propose a novel relational table representation learning approach\nconsidering both the intra- and inter-table contextual information. On one\nhand, the proposed Table Convolutional Network model employs the attention\nmechanism to adaptively focus on the most informative intra-table cells of the\nsame row or column; and, on the other hand, it aggregates inter-table\ncontextual information from various types of implicit connections between cells\nacross different tables. Specifically, we propose three novel aggregation\nmodules for (i) cells of the same value, (ii) cells of the same schema\nposition, and (iii) cells linked to the same page topic. We further devise a\nsupervised multi-task training objective for jointly predicting column type and\npairwise column relation, as well as a table cell recovery objective for\npre-training. Experiments on real Web table datasets demonstrate our method can\noutperform competitive baselines by +4.8% of F1 for column type prediction and\nby +4.1% of F1 for pairwise column relation prediction.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 02:18:10 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Wang", "Daheng", ""], ["Shiralkar", "Prashant", ""], ["Lockard", "Colin", ""], ["Huang", "Binxuan", ""], ["Dong", "Xin Luna", ""], ["Jiang", "Meng", ""]]}, {"id": "2102.09600", "submitter": "Shafiuddin Rehan Ahmed", "authors": "Shafiuddin Rehan Ahmed and James H. Martin", "title": "Within-Document Event Coreference with BERT-Based Contextualized\n  Representations", "comments": "9 pages, 1 figure, 10 tables, rejected in aaai 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Event coreference continues to be a challenging problem in information\nextraction. With the absence of any external knowledge bases for events,\ncoreference becomes a clustering task that relies on effective representations\nof the context in which event mentions appear. Recent advances in\ncontextualized language representations have proven successful in many tasks,\nhowever, their use in event linking been limited. Here we present a three part\napproach that (1) uses representations derived from a pretrained BERT model to\n(2) train a neural classifier to (3) drive a simple clustering algorithm to\ncreate coreference chains. We achieve state of the art results with this model\non two standard datasets for within-document event coreference task and\nestablish a new standard on a third newer dataset.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 21:12:43 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Ahmed", "Shafiuddin Rehan", ""], ["Martin", "James H.", ""]]}, {"id": "2102.09670", "submitter": "Tao Yang", "authors": "Tao Yang and Qingyao Ai", "title": "Maximizing Marginal Fairness for Dynamic Learning to Rank", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": "10.1145/3442381.3449901", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rankings, especially those in search and recommendation systems, often\ndetermine how people access information and how information is exposed to\npeople. Therefore, how to balance the relevance and fairness of information\nexposure is considered as one of the key problems for modern IR systems. As\nconventional ranking frameworks that myopically sorts documents with their\nrelevance will inevitably introduce unfair result exposure, recent studies on\nranking fairness mostly focus on dynamic ranking paradigms where result\nrankings can be adapted in real-time to support fairness in groups (i.e.,\nraces, genders, etc.). Existing studies on fairness in dynamic learning to\nrank, however, often achieve the overall fairness of document exposure in\nranked lists by significantly sacrificing the performance of result relevance\nand fairness on the top results. To address this problem, we propose a fair and\nunbiased ranking method named Maximal Marginal Fairness (MMF). The algorithm\nintegrates unbiased estimators for both relevance and merit-based fairness\nwhile providing an explicit controller that balances the selection of documents\nto maximize the marginal relevance and fairness in top-k results. Theoretical\nand empirical analysis shows that, with small compromises on long list\nfairness, our method achieves superior efficiency and effectiveness comparing\nto the state-of-the-art algorithms in both relevance and fairness for top-k\nrankings.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 23:34:04 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Yang", "Tao", ""], ["Ai", "Qingyao", ""]]}, {"id": "2102.09681", "submitter": "Vinay Rao", "authors": "Robert Ormandi, Mohammad Saleh, Erin Winter, Vinay Rao", "title": "WebRED: Effective Pretraining And Finetuning For Relation Extraction On\n  The Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Relation extraction is used to populate knowledge bases that are important to\nmany applications. Prior datasets used to train relation extraction models\neither suffer from noisy labels due to distant supervision, are limited to\ncertain domains or are too small to train high-capacity models. This constrains\ndownstream applications of relation extraction. We therefore introduce: WebRED\n(Web Relation Extraction Dataset), a strongly-supervised human annotated\ndataset for extracting relationships from a variety of text found on the World\nWide Web, consisting of ~110K examples. We also describe the methods we used to\ncollect ~200M examples as pre-training data for this task. We show that\ncombining pre-training on a large weakly supervised dataset with fine-tuning on\na small strongly-supervised dataset leads to better relation extraction\nperformance. We provide baselines for this new dataset and present a case for\nthe importance of human annotation in improving the performance of relation\nextraction from text found on the web.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 23:56:12 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Ormandi", "Robert", ""], ["Saleh", "Mohammad", ""], ["Winter", "Erin", ""], ["Rao", "Vinay", ""]]}, {"id": "2102.09900", "submitter": "Muneer Ahmad", "authors": "Muneer Ahmad, Dr. M Sadik Batcha", "title": "Identifying and Mapping the Global Research Output on Coronavirus\n  Disease: A Scientometric Study", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper explores and analyses the trend of world literature on \"Coronavirus\nDisease\" in terms of the output of research publications as indexed in the\nScience Citation Index Expanded (SCI-E) of Web of Science during the period\nfrom 2011 to 2020. The study found that 6071 research records have been\npublished on Coronavirus Disease till March 20, 2020. The various scientometric\ncomponents of the research records published in the study period were studied.\nThe study reveals the various aspects of Coronavirus Disease literature such as\nyear wise distribution, relative growth rate, doubling time of literature,\ngeographical wise, organization wise, language wise, form wise , most prolific\nauthors, and source wise. The highest number of articles was published in the\nyear 2019, while lowest numbers of research article were reported in the year\n2020. Further, the relative growth rate is gradually increases and on the other\nhand doubling time decreases. Most of the research publications are published\nin English language and most of the publications published in the form of\nresearch articles. USA is the highest contributor to the field of Coronavirus\nDisease literature.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 12:41:01 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Ahmad", "Muneer", ""], ["Batcha", "Dr. M Sadik", ""]]}, {"id": "2102.09965", "submitter": "Mahieddine Djoudi", "authors": "Hichem Rahab, Abdelhafid Zitouni, Mahieddine Djoudi (TECHN\\'E - EA\n  6316)", "title": "An Enhanced Corpus for Arabic Newspapers Comments", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.00459", "journal-ref": "International Arab Journal of Information Technology, Colleges of\n  Computing and Information Society (CCIS), 2020, 17 (5), pp.789-798", "doi": "10.34028/iajit/17/5/12", "report-no": null, "categories": "cs.IR cs.CL cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose our enhanced approach to create a dedicated corpus\nfor Algerian Arabic newspapers comments. The developed approach has to enhance\nan existing approach by the enrichment of the available corpus and the\ninclusion of the annotation step by following the Model Annotate Train Test\nEvaluate Revise (MATTER) approach. A corpus is created by collecting comments\nfrom web sites of three well know Algerian newspapers. Three classifiers,\nsupport vector machines, na{\\\"i}ve Bayes, and k-nearest neighbors, were used\nfor classification of comments into positive and negative classes. To identify\nthe influence of the stemming in the obtained results, the classification was\ntested with and without stemming. Obtained results show that stemming does not\nenhance considerably the classification due to the nature of Algerian comments\ntied to Algerian Arabic Dialect. The promising results constitute a motivation\nfor us to improve our approach especially in dealing with non Arabic sentences,\nespecially Dialectal and French ones.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 10:15:44 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Rahab", "Hichem", "", "TECHN\u00c9 - EA\n  6316"], ["Zitouni", "Abdelhafid", "", "TECHN\u00c9 - EA\n  6316"], ["Djoudi", "Mahieddine", "", "TECHN\u00c9 - EA\n  6316"]]}, {"id": "2102.10044", "submitter": "Fan Liu", "authors": "Fan Liu, Zhiyong Cheng, Lei Zhu, Zan Gao, Liqiang Nie", "title": "Interest-aware Message-Passing GCN for Recommendation", "comments": "WWW 2021, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolution Networks (GCNs) manifest great potential in recommendation.\nThis is attributed to their capability on learning good user and item\nembeddings by exploiting the collaborative signals from the high-order\nneighbors. Like other GCN models, the GCN based recommendation models also\nsuffer from the notorious over-smoothing problem - when stacking more layers,\nnode embeddings become more similar and eventually indistinguishable, resulted\nin performance degradation. The recently proposed LightGCN and LR-GCN alleviate\nthis problem to some extent, however, we argue that they overlook an important\nfactor for the over-smoothing problem in recommendation, that is, high-order\nneighboring users with no common interests of a user can be also involved in\nthe user's embedding learning in the graph convolution operation. As a result,\nthe multi-layer graph convolution will make users with dissimilar interests\nhave similar embeddings. In this paper, we propose a novel Interest-aware\nMessage-Passing GCN (IMP-GCN) recommendation model, which performs high-order\ngraph convolution inside subgraphs. The subgraph consists of users with similar\ninterests and their interacted items. To form the subgraphs, we design an\nunsupervised subgraph generation module, which can effectively identify users\nwith common interests by exploiting both user feature and graph structure. To\nthis end, our model can avoid propagating negative information from high-order\nneighbors into embedding learning. Experimental results on three large-scale\nbenchmark datasets show that our model can gain performance improvement by\nstacking more layers and outperform the state-of-the-art GCN-based\nrecommendation models significantly.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 17:24:15 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 04:06:46 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Fan", ""], ["Cheng", "Zhiyong", ""], ["Zhu", "Lei", ""], ["Gao", "Zan", ""], ["Nie", "Liqiang", ""]]}, {"id": "2102.10073", "submitter": "Jimmy Lin", "authors": "Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak\n  Pradeep, and Rodrigo Nogueira", "title": "Pyserini: An Easy-to-Use Python Toolkit to Support Replicable IR\n  Research with Sparse and Dense Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pyserini is an easy-to-use Python toolkit that supports replicable IR\nresearch by providing effective first-stage retrieval in a multi-stage ranking\narchitecture. Our toolkit is self-contained as a standard Python package and\ncomes with queries, relevance judgments, pre-built indexes, and evaluation\nscripts for many commonly used IR test collections. We aim to support, out of\nthe box, the entire research lifecycle of efforts aimed at improving ranking\nwith modern neural approaches. In particular, Pyserini supports sparse\nretrieval (e.g., BM25 scoring using bag-of-words representations), dense\nretrieval (e.g., nearest-neighbor search on transformer-encoded\nrepresentations), as well as hybrid retrieval that integrates both approaches.\nThis paper provides an overview of toolkit features and presents empirical\nresults that illustrate its effectiveness on two popular ranking tasks. We also\ndescribe how our group has built a culture of replicability through shared\nnorms and tools that enable rigorous automated testing.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 18:12:44 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Lin", "Jimmy", ""], ["Ma", "Xueguang", ""], ["Lin", "Sheng-Chieh", ""], ["Yang", "Jheng-Hong", ""], ["Pradeep", "Ronak", ""], ["Nogueira", "Rodrigo", ""]]}, {"id": "2102.10162", "submitter": "Sean Butler Mr", "authors": "Sean Butler", "title": "Clarification of Video Retrieval Query Results by the Automated\n  Insertion of Supporting Shots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Computational Video Editing Systems output video generally follows a\nparticular form, e.g. conversation or music videos, in this way they are domain\nspecific. We describe a recent development in our video annotation and\nsegmentation system to support general computational video editing in which we\nderive a single generic editing strategy from general cinema narrative\nprinciples instead of using a hierarchical film gram-mar. We demonstrate how\nthis single principle coupled with a database of scripts derived from annotated\nvideos leverages the existing video editing knowledge encoded within the\nediting of those sequences in a flexible and generic manner. We discuss the\ncinema theory foundations for this generic editing strategy, review the\nalgorithms used to effect it, and goon by means of examples to show its\nappropriateness in an automated system.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 21:23:50 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Butler", "Sean", ""]]}, {"id": "2102.10246", "submitter": "Thuy Vu", "authors": "Thuy Vu and Alessandro Moschitti", "title": "CDA: a Cost Efficient Content-based Multilingual Web Document Aligner", "comments": null, "journal-ref": "EACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Content-based Document Alignment approach (CDA), an efficient\nmethod to align multilingual web documents based on content in creating\nparallel training data for machine translation (MT) systems operating at the\nindustrial level. CDA works in two steps: (i) projecting documents of a web\ndomain to a shared multilingual space; then (ii) aligning them based on the\nsimilarity of their representations in such space. We leverage lexical\ntranslation models to build vector representations using TF-IDF. CDA achieves\nperformance comparable with state-of-the-art systems in the WMT-16 Bilingual\nDocument Alignment Shared Task benchmark while operating in multilingual space.\nBesides, we created two web-scale datasets to examine the robustness of CDA in\nan industrial setting involving up to 28 languages and millions of documents.\nThe experiments show that CDA is robust, cost-effective, and is significantly\nsuperior in (i) processing large and noisy web data and (ii) scaling to new and\nlow-resourced languages.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 03:37:23 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Vu", "Thuy", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "2102.10315", "submitter": "Lei Li", "authors": "Lei Li, Yongfeng Zhang, Li Chen", "title": "EXTRA: Explanation Ranking Datasets for Explainable Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, research on explainable recommender systems has drawn much\nattention from both academia and industry, resulting in a variety of\nexplainable models. As a consequence, their evaluation approaches vary from\nmodel to model, which makes it quite difficult to compare the explainability of\ndifferent models. To achieve a standard way of evaluating recommendation\nexplanations, we provide three benchmark datasets for EXplanaTion RAnking\n(denoted as EXTRA), on which explainability can be measured by ranking-oriented\nmetrics. Constructing such datasets, however, poses great challenges. First,\nuser-item-explanation triplet interactions are rare in existing recommender\nsystems, so how to find alternatives becomes a challenge. Our solution is to\nidentify nearly identical sentences from user reviews. This idea then leads to\nthe second challenge, i.e., how to efficiently categorize the sentences in a\ndataset into different groups, since it has quadratic runtime complexity to\nestimate the similarity between any two sentences. To mitigate this issue, we\nprovide a more efficient method based on Locality Sensitive Hashing (LSH) that\ncan detect near-duplicates in sub-linear time for a given query. Moreover, we\nmake our code publicly available to allow researchers in the community to\ncreate their own datasets.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 11:27:44 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 08:03:52 GMT"}, {"version": "v3", "created": "Sat, 8 May 2021 03:18:03 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Li", "Lei", ""], ["Zhang", "Yongfeng", ""], ["Chen", "Li", ""]]}, {"id": "2102.10550", "submitter": "Zhenyu Han", "authors": "Zhenyu Han, Fengli Xu, Jinghan Shi, Yu Shang, Haorui Ma, Pan Hui, Yong\n  Li", "title": "Genetic Meta-Structure Search for Recommendation on Heterogeneous\n  Information Network", "comments": "Published in Proceedings of the 29th ACM International Conference on\n  Information and Knowledge Management (CIKM '20)", "journal-ref": null, "doi": "10.1145/3340531.3412015", "report-no": null, "categories": "cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, the heterogeneous information network (HIN) has become an\nimportant methodology for modern recommender systems. To fully leverage its\npower, manually designed network templates, i.e., meta-structures, are\nintroduced to filter out semantic-aware information. The hand-crafted\nmeta-structure rely on intense expert knowledge, which is both laborious and\ndata-dependent. On the other hand, the number of meta-structures grows\nexponentially with its size and the number of node types, which prohibits\nbrute-force search. To address these challenges, we propose Genetic\nMeta-Structure Search (GEMS) to automatically optimize meta-structure designs\nfor recommendation on HINs. Specifically, GEMS adopts a parallel genetic\nalgorithm to search meaningful meta-structures for recommendation, and designs\ndedicated rules and a meta-structure predictor to efficiently explore the\nsearch space. Finally, we propose an attention based multi-view graph\nconvolutional network module to dynamically fuse information from different\nmeta-structures. Extensive experiments on three real-world datasets suggest the\neffectiveness of GEMS, which consistently outperforms all baseline methods in\nHIN recommendation. Compared with simplified GEMS which utilizes hand-crafted\nmeta-paths, GEMS achieves over $6\\%$ performance gain on most evaluation\nmetrics. More importantly, we conduct an in-depth analysis on the identified\nmeta-structures, which sheds light on the HIN based recommender system design.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 08:29:41 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Han", "Zhenyu", ""], ["Xu", "Fengli", ""], ["Shi", "Jinghan", ""], ["Shang", "Yu", ""], ["Ma", "Haorui", ""], ["Hui", "Pan", ""], ["Li", "Yong", ""]]}, {"id": "2102.10560", "submitter": "Yijiang Lian", "authors": "Yijiang Lian, Yubo Liu, Zhicong Ye, Liang Yuan, Yanfeng Zhu, Min Zhao,\n  Jianyi Cheng, Xinwei Feng", "title": "A Concept Knowledge-Driven Keywords Retrieval Framework for Sponsored\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In sponsored search, retrieving synonymous keywords for exact match type is\nimportant for accurately targeted advertising. Data-driven deep learning-based\nmethod has been proposed to tackle this problem. An apparent disadvantage of\nthis method is its poor generalization performance on entity-level long-tail\ninstances, even though they might share similar concept-level patterns with\nfrequent instances. With the help of a large knowledge base, we find that most\ncommercial synonymous query-keyword pairs can be abstracted into meaningful\nconceptual patterns through concept tagging. Based on this fact, we propose a\nnovel knowledge-driven conceptual retrieval framework to mitigate this problem,\nwhich consists of three parts: data conceptualization, matching via conceptual\npatterns and concept-augmented discrimination. Both offline and online\nexperiments show that our method is very effective. This framework has been\nsuccessfully applied to Baidu's sponsored search system, which yields a\nsignificant improvement in revenue.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 08:51:51 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Lian", "Yijiang", ""], ["Liu", "Yubo", ""], ["Ye", "Zhicong", ""], ["Yuan", "Liang", ""], ["Zhu", "Yanfeng", ""], ["Zhao", "Min", ""], ["Cheng", "Jianyi", ""], ["Feng", "Xinwei", ""]]}, {"id": "2102.10745", "submitter": "Zhiyong Cheng", "authors": "Zhiyong Cheng and Shenghan Mei and Yangyang Guo and Lei Zhu and\n  Liqiang Nie", "title": "Factor-level Attentive ICF for Recommendation", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Item-based collaborative filtering (ICF) enjoys the advantages of high\nrecommendation accuracy and ease in online penalization and thus is favored by\nthe industrial recommender systems. ICF recommends items to a target user based\non their similarities to the previously interacted items of the user. Great\nprogresses have been achieved for ICF in recent years by applying advanced\nmachine learning techniques (e.g., deep neural networks) to learn the item\nsimilarity from data. The early methods simply treat all the historical items\nequally and recent ones distinguish the different importance of items for a\nprediction. Despite the progress, we argue that those ICF models neglect the\ndiverse intents of users on adopting items (e.g., watching a movie because of\nthe director, leading actors, or the visual effects). As a result, they fail to\nestimate the item similarity on a finer-grained level to predict the user's\npreference for an item, resulting in sub-optimal recommendation. In this work,\nwe propose a general factor-level attention method for ICF models. The key of\nour method is to distinguish the importance of different factors when computing\nthe item similarity for a prediction. To demonstrate the effectiveness of our\nmethod, we design a light attention neural network to integrate both item-level\nand factor-level attention for neural ICF models. It is model-agnostic and\neasy-to-implement. We apply it to two baseline ICF models and evaluate its\neffectiveness on six public datasets. Extensive experiments show the\nfactor-level attention enhanced models consistently outperform their\ncounterparts, demonstrating the potential of differentiate user intents on the\nfactor-level for ICF recommendation models.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 03:04:27 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Cheng", "Zhiyong", ""], ["Mei", "Shenghan", ""], ["Guo", "Yangyang", ""], ["Zhu", "Lei", ""], ["Nie", "Liqiang", ""]]}, {"id": "2102.10962", "submitter": "David Graus", "authors": "David Graus", "title": "Entities of Interest", "comments": "Ph.D. thesis of David Graus. Published in 2017. ISBN:\n  978-94-6182-800-2. DOI: 11245.1/51be80bb-1cbf-4633-8ff9-e3128e990bfa", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, we continuously - and at times unknowingly - leave\nbehind digital traces, by browsing, sharing, posting, liking, searching,\nwatching, and listening to online content. When aggregated, these digital\ntraces can provide powerful insights into the behavior, preferences,\nactivities, and traits of people. While many have raised privacy concerns\naround the use of aggregated digital traces, it has undisputedly brought us\nmany advances, from the search engines that learn from their users and enable\nour access to unforeseen amounts of data, knowledge, and information, to, e.g.,\nthe discovery of previously unknown adverse drug reactions from search engine\nlogs.\n  Whether in online services, journalism, digital forensics, law, or research,\nwe increasingly set out to exploring large amounts of digital traces to\ndiscover new information. Consider for instance, the Enron scandal, Hillary\nClinton's email controversy, or the Panama papers: cases that revolve around\nanalyzing, searching, investigating, exploring, and turning upside down large\namounts of digital traces to gain new insights, knowledge, and information.\nThis discovery task is at its core about \"finding evidence of activity in the\nreal world.\"\n  This dissertation revolves around discovery in digital traces, and sits at\nthe intersection of Information Retrieval, Natural Language Processing, and\napplied Machine Learning. We propose computational methods that aim to support\nthe exploration and sense-making process of large collections of digital\ntraces. We focus on textual traces, e.g., emails and social media streams, and\naddress two aspects that are central to discovery in digital traces.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 13:07:48 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Graus", "David", ""]]}, {"id": "2102.10966", "submitter": "Mohamad Yaser Jaradeh", "authors": "Mohamad Yaser Jaradeh, Kuldeep Singh, Markus Stocker, Andreas Both,\n  S\\\"oren Auer", "title": "Better Call the Plumber: Orchestrating Dynamic Information Extraction\n  Pipelines", "comments": "Accepted in ICWE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, a large number of Knowledge Graph (KG) information\nextraction approaches were proposed. Albeit effective, these efforts are\ndisjoint, and their collective strengths and weaknesses in effective KG\ninformation extraction (IE) have not been studied in the literature. We propose\nPlumber, the first framework that brings together the research community's\ndisjoint IE efforts. The Plumber architecture comprises 33 reusable components\nfor various KG information extraction subtasks, such as coreference resolution,\nentity linking, and relation extraction. Using these components,Plumber\ndynamically generates suitable information extraction pipelines and offers\noverall 264 distinct pipelines.We study the optimization problem of choosing\nsuitable pipelines based on input sentences. To do so, we train a\ntransformer-based classification model that extracts contextual embeddings from\nthe input and finds an appropriate pipeline. We study the efficacy of Plumber\nfor extracting the KG triples using standard datasets over two KGs: DBpedia,\nand Open Research Knowledge Graph (ORKG). Our results demonstrate the\neffectiveness of Plumber in dynamically generating KG information extraction\npipelines,outperforming all baselines agnostics of the underlying KG.\nFurthermore,we provide an analysis of collective failure cases, study the\nsimilarities and synergies among integrated components, and discuss their\nlimitations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 13:14:02 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Jaradeh", "Mohamad Yaser", ""], ["Singh", "Kuldeep", ""], ["Stocker", "Markus", ""], ["Both", "Andreas", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "2102.10989", "submitter": "Chaojun Xiao", "authors": "Chaojun Xiao, Ruobing Xie, Yuan Yao, Zhiyuan Liu, Maosong Sun, Xu\n  Zhang, and Leyu Lin", "title": "UPRec: User-Aware Pre-training for Recommender Systems", "comments": "This paper has been submitted to IEEE TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing sequential recommendation methods rely on large amounts of training\ndata and usually suffer from the data sparsity problem. To tackle this, the\npre-training mechanism has been widely adopted, which attempts to leverage\nlarge-scale data to perform self-supervised learning and transfer the\npre-trained parameters to downstream tasks. However, previous pre-trained\nmodels for recommendation focus on leverage universal sequence patterns from\nuser behaviour sequences and item information, whereas ignore capturing\npersonalized interests with the heterogeneous user information, which has been\nshown effective in contributing to personalized recommendation. In this paper,\nwe propose a method to enhance pre-trained models with heterogeneous user\ninformation, called User-aware Pre-training for Recommendation (UPRec).\nSpecifically, UPRec leverages the user attributes andstructured social graphs\nto construct self-supervised objectives in the pre-training stage and proposes\ntwo user-aware pre-training tasks. Comprehensive experimental results on\nseveral real-world large-scale recommendation datasets demonstrate that UPRec\ncan effectively integrate user information into pre-trained models and thus\nprovide more appropriate recommendations for users.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 13:50:26 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Xiao", "Chaojun", ""], ["Xie", "Ruobing", ""], ["Yao", "Yuan", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""], ["Zhang", "Xu", ""], ["Lin", "Leyu", ""]]}, {"id": "2102.11127", "submitter": "Xueli Yu", "authors": "Xueli Yu, Weizhi Xu, Zeyu Cui, Shu Wu, Liang Wang", "title": "Graph-based Hierarchical Relevance Matching Signals for Ad-hoc Retrieval", "comments": "To appear at WWW 2021", "journal-ref": null, "doi": "10.1145/3442381.3450115", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ad-hoc retrieval task is to rank related documents given a query and a\ndocument collection. A series of deep learning based approaches have been\nproposed to solve such problem and gained lots of attention. However, we argue\nthat they are inherently based on local word sequences, ignoring the subtle\nlong-distance document-level word relationships. To solve the problem, we\nexplicitly model the document-level word relationship through the graph\nstructure, capturing the subtle information via graph neural networks. In\naddition, due to the complexity and scale of the document collections, it is\nconsiderable to explore the different grain-sized hierarchical matching signals\nat a more general level. Therefore, we propose a Graph-based Hierarchical\nRelevance Matching model (GHRM) for ad-hoc retrieval, by which we can capture\nthe subtle and general hierarchical matching signals simultaneously. We\nvalidate the effects of GHRM over two representative ad-hoc retrieval\nbenchmarks, the comprehensive experiments and results demonstrate its\nsuperiority over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 15:57:08 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Yu", "Xueli", ""], ["Xu", "Weizhi", ""], ["Cui", "Zeyu", ""], ["Wu", "Shu", ""], ["Wang", "Liang", ""]]}, {"id": "2102.11314", "submitter": "Yuval Shahar", "authors": "Erez Shalom, Ayelet Goldstein, Elior Ariel, Moshe Sheinberger, Valerie\n  Jones, Boris Van Schooten, and Yuval Shahar", "title": "Distributed Application of Guideline-Based Decision Support through\n  Mobile Devices: Implementation and Evaluation", "comments": "8 Tables and 16 figures in the main text; two Appendices, one\n  including 1 figure, the other including 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Traditionally Guideline(GL)based Decision Support Systems (DSSs) use a\ncentralized infrastructure to generate recommendations to care providers.\nHowever, managing patients at home is preferable, reducing costs and empowering\npatients. We aimed to design, implement, and demonstrate the feasibility of a\nnew architecture for a distributed DSS that provides patients with\npersonalized, context-sensitive, evidence based guidance through their mobile\ndevice, and increases the robustness of the distributed application of the GL,\nwhile maintaining access to the patient longitudinal record and to an up to\ndate evidence based GL repository. We have designed and implemented a novel\nprojection and callback (PCB) model, in which small portions of the evidence\nbased GL procedural knowledge, adapted to the patient preferences and to their\ncurrent context, are projected from a central DSS server, to a local DSS on the\npatient mobile device that applies that knowledge. When appropriate, as defined\nby a temporal pattern within the projected plan, the local DSS calls back the\ncentral DSS, requesting further assistance, possibly another projection. Thus,\nthe GL specification includes two levels: one for the central DSS, one for the\nlocal DSS. We successfully evaluated the PCB model within the MobiGuide EU\nproject by managing Gestational Diabetes Mellitus patients in Spain, and Atrial\nFibrillation patients in Italy. Significant differences exist between the two\nGL representations, suggesting additional ways to characterize GLs. Mean time\nbetween the central and local interactions was quite different for the two GLs:\n3.95 days for gestational diabetes, 23.80 days for atrial fibrillation. Most\ninteractions, 83%, were due to projections to the mDSS. Others were data\nnotifications, mostly to change context. Robustness was demonstrated through\nsuccessful recovery from multiple local DSS crashes.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 19:20:03 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Shalom", "Erez", ""], ["Goldstein", "Ayelet", ""], ["Ariel", "Elior", ""], ["Sheinberger", "Moshe", ""], ["Jones", "Valerie", ""], ["Van Schooten", "Boris", ""], ["Shahar", "Yuval", ""]]}, {"id": "2102.11345", "submitter": "Alberto Purpura", "authors": "Alberto Purpura, Karolina Buchner, Gianmaria Silvello, Gian Antonio\n  Susto", "title": "Neural Feature Selection for Learning to Rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LEarning TO Rank (LETOR) is a research area in the field of Information\nRetrieval (IR) where machine learning models are employed to rank a set of\nitems. In the past few years, neural LETOR approaches have become a competitive\nalternative to traditional ones like LambdaMART. However, neural architectures\nperformance grew proportionally to their complexity and size. This can be an\nobstacle for their adoption in large-scale search systems where a model size\nimpacts latency and update time. For this reason, we propose an\narchitecture-agnostic approach based on a neural LETOR model to reduce the size\nof its input by up to 60% without affecting the system performance. This\napproach also allows to reduce a LETOR model complexity and, therefore, its\ntraining and inference time up to 50%.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 20:38:41 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Purpura", "Alberto", ""], ["Buchner", "Karolina", ""], ["Silvello", "Gianmaria", ""], ["Susto", "Gian Antonio", ""]]}, {"id": "2102.11472", "submitter": "Tao Zhou", "authors": "Tao Zhou", "title": "Progresses and Challenges in Link Prediction", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction is a paradigmatic problem in network science, which aims at\nestimating the existence likelihoods of nonobserved links, based on known\ntopology. After a brief introduction of the standard problem and metrics of\nlink prediction, this Perspective will summarize representative progresses\nabout local similarity indices, link predictability, network embedding, matrix\ncompletion, ensemble learning and others, mainly extracted from thousands of\nrelated publications in the last decade. Finally, this Perspective will outline\nsome long-standing challenges for future studies.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 03:36:43 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Zhou", "Tao", ""]]}, {"id": "2102.11603", "submitter": "Sourav Garg", "authors": "Sourav Garg and Michael Milford", "title": "SeqNet: Learning Descriptors for Sequence-based Hierarchical Place\n  Recognition", "comments": "Accepted for publication in IEEE RA-L 2021; includes supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Place Recognition (VPR) is the task of matching current visual imagery\nfrom a camera to images stored in a reference map of the environment. While\ninitial VPR systems used simple direct image methods or hand-crafted visual\nfeatures, recent work has focused on learning more powerful visual features and\nfurther improving performance through either some form of sequential matcher /\nfilter or a hierarchical matching process. In both cases the performance of the\ninitial single-image based system is still far from perfect, putting\nsignificant pressure on the sequence matching or (in the case of hierarchical\nsystems) pose refinement stages. In this paper we present a novel hybrid system\nthat creates a high performance initial match hypothesis generator using short\nlearnt sequential descriptors, which enable selective control sequential score\naggregation using single image learnt descriptors. Sequential descriptors are\ngenerated using a temporal convolutional network dubbed SeqNet, encoding short\nimage sequences using 1-D convolutions, which are then matched against the\ncorresponding temporal descriptors from the reference dataset to provide an\nordered list of place match hypotheses. We then perform selective sequential\nscore aggregation using shortlisted single image learnt descriptors from a\nseparate pipeline to produce an overall place match hypothesis. Comprehensive\nexperiments on challenging benchmark datasets demonstrate the proposed method\noutperforming recent state-of-the-art methods using the same amount of\nsequential information. Source code and supplementary material can be found at\nhttps://github.com/oravus/seqNet.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 10:32:10 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 01:52:08 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Garg", "Sourav", ""], ["Milford", "Michael", ""]]}, {"id": "2102.11903", "submitter": "Mohamed Trabelsi", "authors": "Mohamed Trabelsi, Zhiyu Chen, Brian D. Davison, Jeff Heflin", "title": "Neural Ranking Models for Document Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking models are the main components of information retrieval systems.\nSeveral approaches to ranking are based on traditional machine learning\nalgorithms using a set of hand-crafted features. Recently, researchers have\nleveraged deep learning models in information retrieval. These models are\ntrained end-to-end to extract features from the raw data for ranking tasks, so\nthat they overcome the limitations of hand-crafted features. A variety of deep\nlearning models have been proposed, and each model presents a set of neural\nnetwork components to extract features that are used for ranking. In this\npaper, we compare the proposed models in the literature along different\ndimensions in order to understand the major contributions and limitations of\neach model. In our discussion of the literature, we analyze the promising\nneural components, and propose future research directions. We also show the\nanalogy between document retrieval and other retrieval tasks where the items to\nbe ranked are structured documents, answers, images and videos.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 19:30:37 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Trabelsi", "Mohamed", ""], ["Chen", "Zhiyu", ""], ["Davison", "Brian D.", ""], ["Heflin", "Jeff", ""]]}, {"id": "2102.12029", "submitter": "Da Xu", "authors": "Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan", "title": "Theoretical Understandings of Product Embedding for E-commerce Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product embeddings have been heavily investigated in the past few years,\nserving as the cornerstone for a broad range of machine learning applications\nin e-commerce. Despite the empirical success of product embeddings, little is\nknown on how and why they work from the theoretical standpoint. Analogous\nresults from the natural language processing (NLP) often rely on\ndomain-specific properties that are not transferable to the e-commerce setting,\nand the downstream tasks often focus on different aspects of the embeddings. We\ntake an e-commerce-oriented view of the product embeddings and reveal a\ncomplete theoretical view from both the representation learning and the\nlearning theory perspective. We prove that product embeddings trained by the\nwidely-adopted skip-gram negative sampling algorithm and its variants are\nsufficient dimension reduction regarding a critical product relatedness\nmeasure. The generalization performance in the downstream machine learning task\nis controlled by the alignment between the embeddings and the product\nrelatedness measure. Following the theoretical discoveries, we conduct\nexploratory experiments that supports our theoretical insights for the product\nembeddings.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 02:29:15 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Xu", "Da", ""], ["Ruan", "Chuanwei", ""], ["Korpeoglu", "Evren", ""], ["Kumar", "Sushant", ""], ["Achan", "Kannan", ""]]}, {"id": "2102.12057", "submitter": "Yufei Feng", "authors": "Yufei Feng, Yu Gong, Fei Sun, Junfeng Ge, Wenwu Ou", "title": "Revisit Recommender System in the Permutation Prospective", "comments": "Under the review of the KDD2021 Applied Data Science track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems (RS) work effective at alleviating information overload\nand matching user interests in various web-scale applications. Most RS retrieve\nthe user's favorite candidates and then rank them by the rating scores in the\ngreedy manner. In the permutation prospective, however, current RS come to\nreveal the following two limitations: 1) They neglect addressing the\npermutation-variant influence within the recommended results; 2) Permutation\nconsideration extends the latent solution space exponentially, and current RS\nlack the ability to evaluate the permutations. Both drive RS away from the\npermutation-optimal recommended results and better user experience. To\napproximate the permutation-optimal recommended results effectively and\nefficiently, we propose a novel permutation-wise framework PRS in the\nre-ranking stage of RS, which consists of Permutation-Matching (PMatch) and\nPermutation-Ranking (PRank) stages successively. Specifically, the PMatch stage\nis designed to obtain the candidate list set, where we propose the FPSA\nalgorithm to generate multiple candidate lists via the permutation-wise and\ngoal-oriented beam search algorithm. Afterwards, for the candidate list set,\nthe PRank stage provides a unified permutation-wise ranking criterion named LR\nmetric, which is calculated by the rating scores of elaborately designed\npermutation-wise model DPWN. Finally, the list with the highest LR score is\nrecommended to the user. Empirical results show that PRS consistently and\nsignificantly outperforms state-of-the-art methods. Moreover, PRS has achieved\na performance improvement of 11.0% on PV metric and 8.7% on IPV metric after\nthe successful deployment in one popular recommendation scenario of Taobao\napplication.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 04:15:40 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 02:22:07 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Feng", "Yufei", ""], ["Gong", "Yu", ""], ["Sun", "Fei", ""], ["Ge", "Junfeng", ""], ["Ou", "Wenwu", ""]]}, {"id": "2102.12188", "submitter": "Dominik Kowald PhD", "authors": "Dominik Kowald, Peter Muellner, Eva Zangerle, Christine Bauer, Markus\n  Schedl, Elisabeth Lex", "title": "Support the Underground: Characteristics of Beyond-Mainstream Music\n  Listeners", "comments": "Accepted for publication in EPJ Data Science - link to published\n  version will be added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Music recommender systems have become an integral part of music streaming\nservices such as Spotify and Last.fm to assist users navigating the extensive\nmusic collections offered by them. However, while music listeners interested in\nmainstream music are traditionally served well by music recommender systems,\nusers interested in music beyond the mainstream (i.e., non-popular music)\nrarely receive relevant recommendations. In this paper, we study the\ncharacteristics of beyond-mainstream music and music listeners and analyze to\nwhat extent these characteristics impact the quality of music recommendations\nprovided. Therefore, we create a novel dataset consisting of Last.fm listening\nhistories of several thousand beyond-mainstream music listeners, which we\nenrich with additional metadata describing music tracks and music listeners.\nOur analysis of this dataset shows four subgroups within the group of\nbeyond-mainstream music listeners that differ not only with respect to their\npreferred music but also with their demographic characteristics. Furthermore,\nwe evaluate the quality of music recommendations that these subgroups are\nprovided with four different recommendation algorithms where we find\nsignificant differences between the groups. Specifically, our results show a\npositive correlation between a subgroup's openness towards music listened to by\nmembers of other subgroups and recommendation accuracy. We believe that our\nfindings provide valuable insights for developing improved user models and\nrecommendation approaches to better serve beyond-mainstream music listeners.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 10:28:22 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Kowald", "Dominik", ""], ["Muellner", "Peter", ""], ["Zangerle", "Eva", ""], ["Bauer", "Christine", ""], ["Schedl", "Markus", ""], ["Lex", "Elisabeth", ""]]}, {"id": "2102.12327", "submitter": "Alexander Felfernig", "authors": "Alexander Felfernig and Stefan Reiterer and Martin Stettinger and\n  Michael Jeran", "title": "An Overview of Direct Diagnosis and Repair Techniques in the WeeVis\n  Recommendation Environment", "comments": "A. Felfernig, S. Reiterer, M. Stettinger, and M. Jeran. An Overview\n  of Direct Diagnosis and Repair Techniques in the WeeVis Recommendation\n  Environment. In the 25th International Workshop on Principles of Diagnosis,\n  Graz, Austria, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint-based recommenders support users in the identification of items\n(products) fitting their wishes and needs. Example domains are financial\nservices and electronic equipment. In this paper we show how divide-and-conquer\nbased (direct) diagnosis algorithms (no conflict detection is needed) can be\nexploited in constraint-based recommendation scenarios. In this context, we\nprovide an overview of the MediaWiki-based recommendation environment WeeVis.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 15:02:50 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Felfernig", "Alexander", ""], ["Reiterer", "Stefan", ""], ["Stettinger", "Martin", ""], ["Jeran", "Michael", ""]]}, {"id": "2102.12342", "submitter": "Zi-Jing Liu", "authors": "Zijing Liu, Mauricio Barahona", "title": "Similarity measure for sparse time course data based on Gaussian\n  processes", "comments": "10pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a similarity measure for sparsely sampled time course data in the\nform of a log-likelihood ratio of Gaussian processes (GP). The proposed GP\nsimilarity is similar to a Bayes factor and provides enhanced robustness to\nnoise in sparse time series, such as those found in various biological\nsettings, e.g., gene transcriptomics. We show that the GP measure is equivalent\nto the Euclidean distance when the noise variance in the GP is negligible\ncompared to the noise variance of the signal. Our numerical experiments on both\nsynthetic and real data show improved performance of the GP similarity when\nused in conjunction with two distance-based clustering methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 15:23:30 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Liu", "Zijing", ""], ["Barahona", "Mauricio", ""]]}, {"id": "2102.12369", "submitter": "Paul Magron", "authors": "Paul Magron, C\\'edric F\\'evotte", "title": "Neural content-aware collaborative filtering for cold-start music\n  recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art music recommender systems are based on collaborative\nfiltering, which builds upon learning similarities between users and songs from\nthe available listening data. These approaches inherently face the cold-start\nproblem, as they cannot recommend novel songs with no listening history.\nContent-aware recommendation addresses this issue by incorporating content\ninformation about the songs on top of collaborative filtering. However, methods\nfalling in this category rely on a shallow user/item interaction that\noriginates from a matrix factorization framework. In this work, we introduce\nneural content-aware collaborative filtering, a unified framework which\nalleviates these limits, and extends the recently introduced neural\ncollaborative filtering to its content-aware counterpart. We propose a\ngenerative model which leverages deep learning for both extracting content\ninformation from low-level acoustic features and for modeling the interaction\nbetween users and songs embeddings. The deep content feature extractor can\neither directly predict the item embedding, or serve as a regularization prior,\nyielding two variants (strict} and relaxed) of our model. Experimental results\nshow that the proposed method reaches state-of-the-art results for a cold-start\nmusic recommendation task. We notably observe that exploiting deep neural\nnetworks for learning refined user/item interactions outperforms approaches\nusing a more simple interaction model in a content-aware framework.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 15:52:24 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 09:02:00 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Magron", "Paul", ""], ["F\u00e9votte", "C\u00e9dric", ""]]}, {"id": "2102.12413", "submitter": "Alexander Felfernig", "authors": "A. Felfernig and N. Tintarev and T.N.T. Trang and M. Stettinger", "title": "Designing Explanations for Group Recommender Systems", "comments": "Cite as: A. Felfernig, N. Tintarev, T.N.T. Trang, and M. Stettinger.\n  Explanations for Groups. In A. Felfernig, L. Boratto, M. Stettinger, and M.\n  Tkalcic (Eds.), Group Recommender Systems: An Introduction (pp. 105-126).\n  SpringerBriefs in Electrical and Computer Engineering. Springer, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explanations are used in recommender systems for various reasons. Users have\nto be supported in making (high-quality) decisions more quickly. Developers of\nrecommender systems want to convince users to purchase specific items. Users\nshould better understand how the recommender system works and why a specific\nitem has been recommended. Users should also develop a more in-depth\nunderstanding of the item domain. Consequently, explanations are designed in\norder to achieve specific \\emph{goals} such as increasing the transparency of a\nrecommendation or increasing a user's trust in the recommender system. In this\npaper, we provide an overview of existing research related to explanations in\nrecommender systems, and specifically discuss aspects relevant to group\nrecommendation scenarios. In this context, we present different ways of\nexplaining and visualizing recommendations determined on the basis of\npreference aggregation strategies.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 17:05:39 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Felfernig", "A.", ""], ["Tintarev", "N.", ""], ["Trang", "T. N. T.", ""], ["Stettinger", "M.", ""]]}, {"id": "2102.12793", "submitter": "Chen Wu", "authors": "Chen Wu, Ruqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan, and Xueqi\n  Cheng", "title": "Learning to Truncate Ranked Lists for Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranked list truncation is of critical importance in a variety of professional\ninformation retrieval applications such as patent search or legal search. The\ngoal is to dynamically determine the number of returned documents according to\nsome user-defined objectives, in order to reach a balance between the overall\nutility of the results and user efforts. Existing methods formulate this task\nas a sequential decision problem and take some pre-defined loss as a proxy\nobjective, which suffers from the limitation of local decision and non-direct\noptimization. In this work, we propose a global decision based truncation model\nnamed AttnCut, which directly optimizes user-defined objectives for the ranked\nlist truncation. Specifically, we take the successful transformer architecture\nto capture the global dependency within the ranked list for truncation\ndecision, and employ the reward augmented maximum likelihood (RAML) for direct\noptimization. We consider two types of user-defined objectives which are of\npractical usage. One is the widely adopted metric such as F1 which acts as a\nbalanced objective, and the other is the best F1 under some minimal recall\nconstraint which represents a typical objective in professional search.\nEmpirical results over the Robust04 and MQ2007 datasets demonstrate the\neffectiveness of our approach as compared with the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 11:32:17 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 13:33:53 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wu", "Chen", ""], ["Zhang", "Ruqing", ""], ["Guo", "Jiafeng", ""], ["Fan", "Yixing", ""], ["Lan", "Yanyan", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2102.12887", "submitter": "Jimmy Lin", "authors": "Jimmy Lin, Daniel Campos, Nick Craswell, Bhaskar Mitra, and Emine\n  Yilmaz", "title": "Significant Improvements over the State of the Art? A Case Study of the\n  MS MARCO Document Ranking Leaderboard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Leaderboards are a ubiquitous part of modern research in applied machine\nlearning. By design, they sort entries into some linear order, where the\ntop-scoring entry is recognized as the \"state of the art\" (SOTA). Due to the\nrapid progress being made in information retrieval today, particularly with\nneural models, the top entry in a leaderboard is replaced with some regularity.\nThese are touted as improvements in the state of the art. Such pronouncements,\nhowever, are almost never qualified with significance testing. In the context\nof the MS MARCO document ranking leaderboard, we pose a specific question: How\ndo we know if a run is significantly better than the current SOTA? We ask this\nquestion against the backdrop of recent IR debates on scale types: in\nparticular, whether commonly used significance tests are even mathematically\npermissible. Recognizing these potential pitfalls in evaluation methodology,\nour study proposes an evaluation framework that explicitly treats certain\noutcomes as distinct and avoids aggregating them into a single-point metric.\nEmpirical analysis of SOTA runs from the MS MARCO document ranking leaderboard\nreveals insights about how one run can be \"significantly better\" than another\nthat are obscured by the current official evaluation metric (MRR@100).\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 14:42:55 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Lin", "Jimmy", ""], ["Campos", "Daniel", ""], ["Craswell", "Nick", ""], ["Mitra", "Bhaskar", ""], ["Yilmaz", "Emine", ""]]}, {"id": "2102.12970", "submitter": "Mouna Labiadh", "authors": "Mouna Labiadh (SOC, LIRIS, CETHIL), Christian Obrecht (CETHIL),\n  Catarina Ferreira da Silva (ISCTE-IUL), Parisa Ghodous (SOC, LIRIS)", "title": "A microservice-based framework for exploring data selection in\n  cross-building knowledge transfer", "comments": "Service Oriented Computing and Applications, Springer, 2020", "journal-ref": null, "doi": "10.1007/s11761-020-00306-w", "report-no": null, "categories": "cs.LG cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised deep learning has achieved remarkable success in various\napplications. Successful machine learning application however depends on the\navailability of sufficiently large amount of data. In the absence of data from\nthe target domain, representative data collection from multiple sources is\noften needed. However, a model trained on existing multi-source data might\ngeneralize poorly on the unseen target domain. This problem is referred to as\ndomain shift. In this paper, we explore the suitability of multi-source\ntraining data selection to tackle the domain shift challenge in the context of\ndomain generalization. We also propose a microservice-oriented methodology for\nsupporting this solution. We perform our experimental study on the use case of\nbuilding energy consumption prediction. Experimental results suggest that\nminimal building description is capable of improving cross-building\ngeneralization performances when used to select energy consumption data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 10:15:06 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Labiadh", "Mouna", "", "SOC, LIRIS, CETHIL"], ["Obrecht", "Christian", "", "CETHIL"], ["da Silva", "Catarina Ferreira", "", "ISCTE-IUL"], ["Ghodous", "Parisa", "", "SOC, LIRIS"]]}, {"id": "2102.12994", "submitter": "Yang Sun", "authors": "Yang Sun, Junwei Pan, Alex Zhang, Aaron Flores", "title": "$FM^2$: Field-matrixed Factorization Machines for Recommender Systems", "comments": "In Proceedings of the Web Conference 2021 (WWW 2021), April 19-23,\n  2021, Ljubljana, Slovenia. 10 pages", "journal-ref": null, "doi": "10.1145/3442381.3449930", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) prediction plays a critical role in recommender\nsystems and online advertising. The data used in these applications are\nmulti-field categorical data, where each feature belongs to one field. Field\ninformation is proved to be important and there are several works considering\nfields in their models. In this paper, we proposed a novel approach to model\nthe field information effectively and efficiently. The proposed approach is a\ndirect improvement of FwFM, and is named as Field-matrixed Factorization\nMachines (FmFM, or $FM^2$). We also proposed a new explanation of FM and FwFM\nwithin the FmFM framework, and compared it with the FFM. Besides pruning the\ncross terms, our model supports field-specific variable dimensions of embedding\nvectors, which acts as soft pruning. We also proposed an efficient way to\nminimize the dimension while keeping the model performance. The FmFM model can\nalso be optimized further by caching the intermediate vectors, and it only\ntakes thousands of floating-point operations (FLOPs) to make a prediction. Our\nexperiment results show that it can out-perform the FFM, which is more complex.\nThe FmFM model's performance is also comparable to DNN models which require\nmuch more FLOPs in runtime.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 00:03:37 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 17:19:47 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Sun", "Yang", ""], ["Pan", "Junwei", ""], ["Zhang", "Alex", ""], ["Flores", "Aaron", ""]]}, {"id": "2102.12998", "submitter": "JianYu Wang", "authors": "JianYu Wang and Xiao-Lei Zhang", "title": "Deep NMF Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) based topic modeling methods do not\nrely on model- or data-assumptions much. However, they are usually formulated\nas difficult optimization problems, which may suffer from bad local minima and\nhigh computational complexity. In this paper, we propose a deep NMF (DNMF)\ntopic modeling framework to alleviate the aforementioned problems. It first\napplies an unsupervised deep learning method to learn latent hierarchical\nstructures of documents, under the assumption that if we could learn a good\nrepresentation of documents by, e.g. a deep model, then the topic word\ndiscovery problem can be boosted. Then, it takes the output of the deep model\nto constrain a topic-document distribution for the discovery of the\ndiscriminant topic words, which not only improves the efficacy but also reduces\nthe computational complexity over conventional unsupervised NMF methods. We\nconstrain the topic-document distribution in three ways, which takes the\nadvantages of the three major sub-categories of NMF -- basic NMF, structured\nNMF, and constrained NMF respectively. To overcome the weaknesses of deep\nneural networks in unsupervised topic modeling, we adopt a non-neural-network\ndeep model -- multilayer bootstrap network. To our knowledge, this is the first\ntime that a deep NMF model is used for unsupervised topic modeling. We have\ncompared the proposed method with a number of representative references\ncovering major branches of topic modeling on a variety of real-world text\ncorpora. Experimental results illustrate the effectiveness of the proposed\nmethod under various evaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 14:40:22 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Wang", "JianYu", ""], ["Zhang", "Xiao-Lei", ""]]}, {"id": "2102.13302", "submitter": "Shuchang Liu", "authors": "Shuchang Liu, Fei Sun, Yingqiang Ge, Changhua Pei, Yongfeng Zhang", "title": "Variation Control and Evaluation for Generative SlateRecommendations", "comments": "12 pages, 8 figures, to be published in WWW 2021", "journal-ref": null, "doi": "10.1145/3442381.3449864", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slate recommendation generates a list of items as a whole instead of ranking\neach item individually, so as to better model the intra-list positional biases\nand item relations. In order to deal with the enormous combinatorial space of\nslates, recent work considers a generative solution so that a slate\ndistribution can be directly modeled. However, we observe that such approaches\n-- despite their proved effectiveness in computer vision -- suffer from a\ntrade-off dilemma in recommender systems: when focusing on reconstruction, they\neasily over-fit the data and hardly generate satisfactory recommendations; on\nthe other hand, when focusing on satisfying the user interests, they get\ntrapped in a few items and fail to cover the item variation in slates. In this\npaper, we propose to enhance the accuracy-based evaluation with slate variation\nmetrics to estimate the stochastic behavior of generative models. We illustrate\nthat instead of reaching to one of the two undesirable extreme cases in the\ndilemma, a valid generative solution resides in a narrow \"elbow\" region in\nbetween. And we show that item perturbation can enforce slate variation and\nmitigate the over-concentration of generated slates, which expand the \"elbow\"\nperformance to an easy-to-find region. We further propose to separate a pivot\nselection phase from the generation process so that the model can apply\nperturbation before generation. Empirical results show that this simple\nmodification can provide even better variance with the same level of accuracy\ncompared to post-generation perturbation methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 05:04:40 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Liu", "Shuchang", ""], ["Sun", "Fei", ""], ["Ge", "Yingqiang", ""], ["Pei", "Changhua", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2102.13367", "submitter": "Sm Zobaed", "authors": "Sakib M Zobaed, Mohsen Amini Salehi, Rajkumar Buyya", "title": "SAED: Edge-Based Intelligence for Privacy-Preserving Enterprise Search\n  on the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud-based enterprise search services (e.g., AWS Kendra) have been\nentrancing big data owners by offering convenient and real-time search\nsolutions to them. However, the problem is that individuals and organizations\npossessing confidential big data are hesitant to embrace such services due to\nvalid data privacy concerns. In addition, to offer an intelligent search, these\nservices access the user search history that further jeopardizes his/her\nprivacy. To overcome the privacy problem, the main idea of this research is to\nseparate the intelligence aspect of the search from its pattern matching\naspect. According to this idea, the search intelligence is provided by an\non-premises edge tier and the shared cloud tier only serves as an exhaustive\npattern matching search utility. We propose Smartness At Edge (SAED mechanism\nthat offers intelligence in the form of semantic and personalized search at the\nedge tier while maintaining privacy of the search on the cloud tier. At the\nedge tier, SAED uses a knowledge-based lexical database to expand the query and\ncover its semantics. SAED personalizes the search via an RNN model that can\nlearn the user interest. A word embedding model is used to retrieve documents\nbased on their semantic relevance to the search query. SAED is generic and can\nbe plugged into existing enterprise search systems and enable them to offer\nintelligent and privacy-preserving search without enforcing any change on them.\nEvaluation results on two enterprise search systems under real settings and\nverified by human users demonstrate that SAED can improve the relevancy of the\nretrieved results by on average 24% for plain-text and 75% for encrypted\ngeneric datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 09:27:26 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 19:50:26 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 17:26:16 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zobaed", "Sakib M", ""], ["Salehi", "Mohsen Amini", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2102.13392", "submitter": "Dimitri Gominski", "authors": "Dimitri Gominski, Val\\'erie Gouet-Brunet, Liming Chen", "title": "Unifying Remote Sensing Image Retrieval and Classification with Robust\n  Fine-tuning", "comments": "Classification methodology is not correct (using all labels at test\n  time). Further experiments with retrieval results show error in reported\n  results (mean+confidence intervals do not show significant improvement)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Advances in high resolution remote sensing image analysis are currently\nhampered by the difficulty of gathering enough annotated data for training deep\nlearning methods, giving rise to a variety of small datasets and associated\ndataset-specific methods. Moreover, typical tasks such as classification and\nretrieval lack a systematic evaluation on standard benchmarks and training\ndatasets, which make it hard to identify durable and generalizable scientific\ncontributions. We aim at unifying remote sensing image retrieval and\nclassification with a new large-scale training and testing dataset, SF300,\nincluding both vertical and oblique aerial images and made available to the\nresearch community, and an associated fine-tuning method. We additionally\npropose a new adversarial fine-tuning method for global descriptors. We show\nthat our framework systematically achieves a boost of retrieval and\nclassification performance on nine different datasets compared to an ImageNet\npretrained baseline, with currently no other method to compare to.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 11:01:30 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 15:56:56 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Gominski", "Dimitri", ""], ["Gouet-Brunet", "Val\u00e9rie", ""], ["Chen", "Liming", ""]]}, {"id": "2102.13495", "submitter": "Omid Mohammadi Kia", "authors": "Omid Mohammadi Kia, Mahmood Neshati, Mahsa Soudi Alamdari", "title": "Open-domain question classification and completion in conversational\n  information search", "comments": null, "journal-ref": "2020 11th International Conference on Information and Knowledge\n  Technology (IKT)", "doi": "10.1109/IKT51791.2020.9345613", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for new information requires talking to the system. In this\nresearch, an Open-domain Conversational information search system has been\ndeveloped. This system has been implemented using the TREC CAsT 2019 track,\nwhich is one of the first attempts to build a framework in this area. According\nto the user's previous questions, the system firstly completes the question\n(using the first and the previous question in each turn) and then classifies it\n(based on the question words). This system extracts the related answers\naccording to the rules of each question. In this research, a simple yet\neffective method with high performance has been used, which on average,\nextracts 20% more relevant results than the baseline.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 14:06:45 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Kia", "Omid Mohammadi", ""], ["Neshati", "Mahmood", ""], ["Alamdari", "Mahsa Soudi", ""]]}]