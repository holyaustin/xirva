[{"id": "1812.00002", "submitter": "Sen Na", "authors": "Mingyuan Ma, Sen Na, Hongyu Wang, Congzhou Chen, Jin Xu", "title": "The Graph-Based Behavior-Aware Recommendation for Interactive News", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive news recommendation has been launched and attracted much\nattention recently. In this scenario, user's behavior evolves from single click\nbehavior to multiple behaviors including like, comment, share etc. However,\nmost of the existing methods still use single click behavior as the unique\ncriterion of judging user's preferences. Further, although heterogeneous graphs\nhave been applied in different areas, a proper way to construct a heterogeneous\ngraph for interactive news data with an appropriate learning mechanism on it is\nstill desired. To address the above concerns, we propose a graph-based\nbehavior-aware network, which simultaneously considers six different types of\nbehaviors as well as user's demand on the news diversity. We have three main\nsteps. First, we build an interaction behavior graph for multi-level and\nmulti-category data. Second, we apply DeepWalk on the behavior graph to obtain\nentity semantics, then build a graph-based convolutional neural network called\nG-CNN to learn news representations, and an attention-based LSTM to learn\nbehavior sequence representations. Third, we introduce core and coritivity\nfeatures for the behavior graph, which measure the concentration degree of\nuser's interests. These features affect the trade-off between accuracy and\ndiversity of our personalized recommendation system. Taking these features into\naccount, our system finally achieves recommending news to different users at\ntheir different levels of concentration degrees.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 05:13:43 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 10:11:58 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Ma", "Mingyuan", ""], ["Na", "Sen", ""], ["Wang", "Hongyu", ""], ["Chen", "Congzhou", ""], ["Xu", "Jin", ""]]}, {"id": "1812.00073", "submitter": "Rama Kumar Pasumarthi", "authors": "Rama Kumar Pasumarthi, Sebastian Bruch, Xuanhui Wang, Cheng Li,\n  Michael Bendersky, Marc Najork, Jan Pfeifer, Nadav Golbandi, Rohan Anil,\n  Stephan Wolf", "title": "TF-Ranking: Scalable TensorFlow Library for Learning-to-Rank", "comments": "KDD 2019", "journal-ref": null, "doi": "10.1145/3292500.3330677", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-to-Rank deals with maximizing the utility of a list of examples\npresented to the user, with items of higher relevance being prioritized. It has\nseveral practical applications such as large-scale search, recommender systems,\ndocument summarization and question answering. While there is widespread\nsupport for classification and regression based learning, support for\nlearning-to-rank in deep learning has been limited. We propose TensorFlow\nRanking, the first open source library for solving large-scale ranking problems\nin a deep learning framework. It is highly configurable and provides\neasy-to-use APIs to support different scoring mechanisms, loss functions and\nevaluation metrics in the learning-to-rank setting. Our library is developed on\ntop of TensorFlow and can thus fully leverage the advantages of this platform.\nFor example, it is highly scalable, both in training and in inference, and can\nbe used to learn ranking models over massive amounts of user activity data,\nwhich can include heterogeneous dense and sparse features. We empirically\ndemonstrate the effectiveness of our library in learning ranking functions for\nlarge-scale search and recommendation applications in Gmail and Google Drive.\nWe also show that ranking models built using our model scale well for\ndistributed training, without significant impact on metrics. The proposed\nlibrary is available to the open source community, with the hope that it\nfacilitates further academic research and industrial applications in the field\nof learning-to-rank.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 22:11:27 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 17:42:03 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Pasumarthi", "Rama Kumar", ""], ["Bruch", "Sebastian", ""], ["Wang", "Xuanhui", ""], ["Li", "Cheng", ""], ["Bendersky", "Michael", ""], ["Najork", "Marc", ""], ["Pfeifer", "Jan", ""], ["Golbandi", "Nadav", ""], ["Anil", "Rohan", ""], ["Wolf", "Stephan", ""]]}, {"id": "1812.00158", "submitter": "Hiba Ahsan", "authors": "Hiba Ahsan, Rahul Agrawal", "title": "Approximating Categorical Similarity in Sponsored Search Relevance", "comments": "Proceedings of DAPA 2019 WSDM Workshop on Deep Matching in Practical\n  Applications. ACM, New York, USA, 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sponsored Search is a major source of revenue for web search engines. Since\nsponsored search follows a pay-per-click model, showing relevant ads for\nreceiving clicks is crucial. Matching categories of a query and its ad\ncandidates have been explored in modeling relevance of query-ad pairs. The\napproach involves matching cached categories of queries seen in the past to\ncategories of candidate ads. Since queries have a heavy tail distribution, the\napproach has limited coverage. In this work, we propose approximating\ncategorical similarity of a query-ad pairs using neural networks, particularly\nCLSM. Embedding of a query (or document) is generated using its tri-letter\nrepresentation which allows coverage of tail queries. Offline experiments of\nincorporating this feature as opposed to using the categories directly show a\n5.23% improvement in AUC ROC. A/B testing results show an improvement of 8.2%\nin relevance.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 06:26:05 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Ahsan", "Hiba", ""], ["Agrawal", "Rahul", ""]]}, {"id": "1812.00382", "submitter": "Jasper Linmans", "authors": "Jasper Linmans, Bob van de Velde, Evangelos Kanoulas", "title": "Improved and Robust Controversy Detection in General Web Pages Using\n  Semantic Approaches under Large Scale Conditions", "comments": "Presented at the 27th ACM International Conference on Information and\n  Knowledge Management (CIKM 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting controversy in general web pages is a daunting task, but\nincreasingly essential to efficiently moderate discussions and effectively\nfilter problematic content. Unfortunately, controversies occur across many\ntopics and domains, with great changes over time. This paper investigates\nneural classifiers as a more robust methodology for controversy detection in\ngeneral web pages. Current models have often cast controversy detection on\ngeneral web pages as Wikipedia linking, or exact lexical matching tasks. The\ndiverse and changing nature of controversies suggest that semantic approaches\nare better able to detect controversy. We train neural networks that can\ncapture semantic information from texts using weak signal data. By leveraging\nthe semantic properties of word embeddings we robustly improve on existing\ncontroversy detection methods. To evaluate model stability over time and to\nunseen topics, we asses model performance under varying training conditions to\ntest cross-temporal, cross-topic, cross-domain performance and annotator\ncongruence. In doing so, we demonstrate that weak-signal based neural\napproaches are closer to human estimates of controversy and are more robust to\nthe inherent variability of controversies.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 12:41:03 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Linmans", "Jasper", ""], ["van de Velde", "Bob", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "1812.00427", "submitter": "Philipp Mayr", "authors": "Philipp Mayr, Muthu Kumar Chandrasekaran, Kokil Jaidka", "title": "Report on the 3rd Joint Workshop on Bibliometric-enhanced Information\n  Retrieval and Natural Language Processing for Digital Libraries (BIRNDL 2018)", "comments": "6 pages, to appear in SIGIR Forum", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $3^{rd}$ joint BIRNDL workshop was held at the 41st ACM SIGIR Conference\non Research and Development in Information Retrieval (SIGIR 2018) in Ann Arbor,\nUSA. BIRNDL 2018 intended to stimulate IR researchers and digital library\nprofessionals to elaborate on new approaches in natural language processing,\ninformation retrieval, scientometrics, and recommendation techniques that can\nadvance the state-of-the-art in scholarly document understanding, analysis, and\nretrieval at scale. The workshop incorporated three paper sessions and the\n$4^{th}$ edition of the CL-SciSumm Shared Task.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 16:59:39 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Mayr", "Philipp", ""], ["Chandrasekaran", "Muthu Kumar", ""], ["Jaidka", "Kokil", ""]]}, {"id": "1812.00784", "submitter": "Mounir Baammi", "authors": "Mounir Baammi", "title": "Malware static analysis and DDoS capabilities detection", "comments": "60 Pages, 10 Figures, 12 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The present thesis addresses the topic of denial of service capabilities\ndetection at malware binary level, with the aim of designing a framework that\nintegrate results from different binary analysis methods and decide on the DDoS\ncapabilities of the analysed malware. We have implemented a process to extract\nmeaningful data from malware samples, the extracted data was used to find\ncharacteristics and features that can lead to the detection of DDoS\ncapabilities in binaries. Based on the discoveries, a set of rules was\nelaborated to detect those features in binaries. The method is tested on a\ndataset of 815 samples. Another dataset of 525 benign binaries is also used to\ntest false positives rate of the implemented method. The results of our method\nare compared with Virus Total analysis results to assess our detection\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 14:44:53 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Baammi", "Mounir", ""]]}, {"id": "1812.01027", "submitter": "Afshin Sadeghi", "authors": "Afshin Sadeghi, Sarven Capadisli, and Johannes Wilm, Christoph Lange,\n  Philipp Mayr", "title": "Automatically Annotating Articles Towards Opening and Reusing\n  Transparent Peer Reviews", "comments": "submitted to review to \"Publications\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of scientific publications are created in open and\ntransparent peer review models: a submission is published first, and then\nreviewers are invited, or a submission is reviewed in a closed environment but\nthen these reviews are published with the final article, or combinations of\nthese. Reasons for open peer review include giving better credit to reviewers\nand enabling readers to better appraise the quality of a publication. In most\ncases, the full, unstructured text of an open review is published next to the\nfull, unstructured text of the article reviewed. This approach prevents human\nreaders from getting a quick impression of the quality of parts of an article,\nand it does not easily support secondary exploitation, e.g., for scientometrics\non reviews. While document formats have been proposed for publishing structured\narticles including reviews, integrated tool support for entire open peer review\nworkflows resulting in such documents is still scarce. We present AR-Annotator,\nthe Automatic Article and Review Annotator which employs a semantic information\nmodel of an article and its reviews, using semantic markup and unique\nidentifiers for all entities of interest. The fine-grained article structure is\nnot only exposed to authors and reviewers but also preserved in the published\nversion. We publish articles and their reviews in a Linked Data representation\nand thus maximize their reusability by third-party applications. We demonstrate\nthis reusability by running quality-related queries against the structured\nrepresentation of articles and their reviews.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 19:04:59 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Sadeghi", "Afshin", ""], ["Capadisli", "Sarven", ""], ["Wilm", "Johannes", ""], ["Lange", "Christoph", ""], ["Mayr", "Philipp", ""]]}, {"id": "1812.01141", "submitter": "Vibhuti Gupta", "authors": "Vibhuti Gupta and Rattikorn Hewett", "title": "Unleashing the Power of Hashtags in Tweet Analytics with Distributed\n  Framework on Apache Storm", "comments": "IEEE International Conference on Big Data 2018", "journal-ref": "2018, pp. 4554-4558", "doi": "10.1109/BigData.2018.8622302", "report-no": null, "categories": "cs.DC cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter is a popular social network platform where users can interact and\npost texts of up to 280 characters called tweets. Hashtags, hyperlinked words\nin tweets, have increasingly become crucial for tweet retrieval and search.\nUsing hashtags for tweet topic classification is a challenging problem because\nof context dependent among words, slangs, abbreviation and emoticons in a short\ntweet along with evolving use of hashtags. Since Twitter generates millions of\ntweets daily, tweet analytics is a fundamental problem of Big data stream that\noften requires a real-time Distributed processing. This paper proposes a\ndistributed online approach to tweet topic classification with hashtags. Being\nimplemented on Apache Storm, a distributed real time framework, our approach\nincrementally identifies and updates a set of strong predictors in the Na\\\"ive\nBayes model for classifying each incoming tweet instance. Preliminary\nexperiments show promising results with up to 97% accuracy and 37% increase in\nthroughput on eight processors.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 00:35:22 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Gupta", "Vibhuti", ""], ["Hewett", "Rattikorn", ""]]}, {"id": "1812.01190", "submitter": "Wenjin Wu", "authors": "Wenjin Wu, Guojun Liu, Hui Ye, Chenshuang Zhang, Tianshu Wu, Daorui\n  Xiao, Wei Lin, Xiaoyu Zhu", "title": "EENMF: An End-to-End Neural Matching Framework for E-Commerce Sponsored\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  E-commerce sponsored search contributes an important part of revenue for the\ne-commerce company. In consideration of effectiveness and efficiency, a\nlarge-scale sponsored search system commonly adopts a multi-stage architecture.\nWe name these stages as ad retrieval, ad pre-ranking and ad ranking. Ad\nretrieval and ad pre-ranking are collectively referred to as ad matching in\nthis paper. We propose an end-to-end neural matching framework (EENMF) to model\ntwo tasks---vector-based ad retrieval and neural networks based ad pre-ranking.\nUnder the deep matching framework, vector-based ad retrieval harnesses user\nrecent behavior sequence to retrieve relevant ad candidates without the\nconstraint of keyword bidding. Simultaneously, the deep model is employed to\nperform the global pre-ranking of ad candidates from multiple retrieval paths\neffectively and efficiently. Besides, the proposed model tries to optimize the\npointwise cross-entropy loss which is consistent with the objective of predict\nmodels in the ranking stage. We conduct extensive evaluation to validate the\nperformance of the proposed framework. In the real traffic of a large-scale\ne-commerce sponsored search, the proposed approach significantly outperforms\nthe baseline.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 03:10:18 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 05:53:05 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2018 06:18:19 GMT"}, {"version": "v4", "created": "Sun, 9 Dec 2018 06:00:41 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Wu", "Wenjin", ""], ["Liu", "Guojun", ""], ["Ye", "Hui", ""], ["Zhang", "Chenshuang", ""], ["Wu", "Tianshu", ""], ["Xiao", "Daorui", ""], ["Lin", "Wei", ""], ["Zhu", "Xiaoyu", ""]]}, {"id": "1812.01199", "submitter": "Sina Dabiri", "authors": "Sina Dabiri, Kevin Heaslip", "title": "Twitter-based traffic information system based on vector representations\n  for words", "comments": "17 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, researchers have shown an increased interest in harnessing Twitter\ndata for dynamic monitoring of traffic conditions. Bag-of-words representation\nis a common method in literature for tweet modeling and retrieving traffic\ninformation, yet it suffers from the curse of dimensionality and sparsity. To\naddress these issues, our specific objective is to propose a simple and robust\nframework on the top of word embedding for distinguishing traffic-related\ntweets against non-traffic-related ones. In our proposed model, a tweet is\nclassified as traffic-related if semantic similarity between its words and a\nsmall set of traffic keywords exceeds a threshold value. Semantic similarity\nbetween words is captured by means of word-embedding models, which is an\nunsupervised learning tool. The proposed model is as simple as having only one\ntrainable parameter. The model takes advantage of outstanding merits, which are\ndemonstrated through several evaluation steps. The state-of-the-art test\naccuracy for our proposed model is 95.9%.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 03:28:28 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Dabiri", "Sina", ""], ["Heaslip", "Kevin", ""]]}, {"id": "1812.01276", "submitter": "Massimiliano Ruocco", "authors": "Bj{\\o}rnar Vass{\\o}y, Massimiliano Ruocco, Eliezer de Souza da Silva,\n  Erlend Aune", "title": "Time is of the Essence: a Joint Hierarchical RNN and Point Process Model\n  for Time and Item Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years session-based recommendation has emerged as an increasingly\napplicable type of recommendation. As sessions consist of sequences of events,\nthis type of recommendation is a natural fit for Recurrent Neural Networks\n(RNNs). Several additions have been proposed for extending such models in order\nto handle specific problems or data. Two such extensions are 1.) modeling of\ninter-session relations for catching long term dependencies over user sessions,\nand 2.) modeling temporal aspects of user-item interactions. The former allows\nthe session-based recommendation to utilize extended session history and\ninter-session information when providing new recommendations. The latter has\nbeen used to both provide state-of-the-art predictions for when the user will\nreturn to the service and also for improving recommendations. In this work we\ncombine these two extensions in a joint model for the tasks of recommendation\nand return-time prediction. The model consists of a Hierarchical RNN for the\ninter-session and intra-session items recommendation extended with a Point\nProcess model for the time-gaps between the sessions. The experimental results\nindicate that the proposed model improves recommendations significantly on two\ndatasets over a strong baseline, while simultaneously improving return-time\npredictions over a baseline return-time prediction model.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 08:41:02 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Vass\u00f8y", "Bj\u00f8rnar", ""], ["Ruocco", "Massimiliano", ""], ["da Silva", "Eliezer de Souza", ""], ["Aune", "Erlend", ""]]}, {"id": "1812.01404", "submitter": "Zhan Yang", "authors": "Zhan Yang, Osolo Ian Raymond, Wuqing Sun, Jun Long", "title": "Deep Attention-guided Hashing", "comments": "Accepted to IEEE ACCESS", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2891894", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of multimedia data (e.g., image, audio and video etc.)\non the web, learning-based hashing techniques such as Deep Supervised Hashing\n(DSH) have proven to be very efficient for large-scale multimedia search. The\nrecent successes seen in Learning-based hashing methods are largely due to the\nsuccess of deep learning-based hashing methods. However, there are some\nlimitations to previous learning-based hashing methods (e.g., the learned hash\ncodes containing repetitive and highly correlated information). In this paper,\nwe propose a novel learning-based hashing method, named Deep Attention-guided\nHashing (DAgH). DAgH is implemented using two stream frameworks. The core idea\nis to use guided hash codes which are generated by the hashing network of the\nfirst stream framework (called first hashing network) to guide the training of\nthe hashing network of the second stream framework (called second hashing\nnetwork). Specifically, in the first network, it leverages an attention network\nand hashing network to generate the attention-guided hash codes from the\noriginal images. The loss function we propose contains two components: the\nsemantic loss and the attention loss. The attention loss is used to punish the\nattention network to obtain the salient region from pairs of images; in the\nsecond network, these attention-guided hash codes are used to guide the\ntraining of the second hashing network (i.e., these codes are treated as\nsupervised labels to train the second network). By doing this, DAgH can make\nfull use of the most critical information contained in images to guide the\nsecond hashing network in order to learn efficient hash codes in a true\nend-to-end fashion. Results from our experiments demonstrate that DAgH can\ngenerate high quality hash codes and it outperforms current state-of-the-art\nmethods on three benchmark datasets, CIFAR-10, NUS-WIDE, and ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 13:36:35 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 01:48:15 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Yang", "Zhan", ""], ["Raymond", "Osolo Ian", ""], ["Sun", "Wuqing", ""], ["Long", "Jun", ""]]}, {"id": "1812.01504", "submitter": "Bashir Rastegarpanah", "authors": "Bashir Rastegarpanah (1), Krishna P. Gummadi (2), Mark Crovella (1)\n  ((1) Boston University, (2) MPI-SWS)", "title": "Fighting Fire with Fire: Using Antidote Data to Improve Polarization and\n  Fairness of Recommender Systems", "comments": "References to appendices are fixed", "journal-ref": null, "doi": "10.1145/3289600.3291002", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing role of recommender systems in many aspects of society makes\nit essential to consider how such systems may impact social good. Various\nmodifications to recommendation algorithms have been proposed to improve their\nperformance for specific socially relevant measures. However, previous\nproposals are often not easily adapted to different measures, and they\ngenerally require the ability to modify either existing system inputs, the\nsystem's algorithm, or the system's outputs. As an alternative, in this paper\nwe introduce the idea of improving the social desirability of recommender\nsystem outputs by adding more data to the input, an approach we view as\nproviding `antidote' data to the system. We formalize the antidote data\nproblem, and develop optimization-based solutions. We take as our model system\nthe matrix factorization approach to recommendation, and we propose a set of\nmeasures to capture the polarization or fairness of recommendations. We then\nshow how to generate antidote data for each measure, pointing out a number of\ncomputational efficiencies, and discuss the impact on overall system accuracy.\nOur experiments show that a modest budget for antidote data can lead to\nsignificant improvements in the polarization or fairness of recommendations.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 22:15:29 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 23:30:01 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2019 01:13:54 GMT"}, {"version": "v4", "created": "Fri, 25 Jan 2019 21:49:05 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Rastegarpanah", "Bashir", "", "Boston University"], ["Gummadi", "Krishna P.", "", "MPI-SWS"], ["Crovella", "Mark", "", "Boston University"]]}, {"id": "1812.01528", "submitter": "Yue Zhao", "authors": "Yue Zhao, Zain Nasrullah, Maciej K. Hryniewicki, Zheng Li", "title": "LSCP: Locally Selective Combination in Parallel Outlier Ensembles", "comments": "Proceedings of the 2019 SIAM International Conference on Data Mining\n  (SDM)", "journal-ref": null, "doi": "10.1137/1.9781611975673.66", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unsupervised outlier ensembles, the absence of ground truth makes the\ncombination of base outlier detectors a challenging task. Specifically,\nexisting parallel outlier ensembles lack a reliable way of selecting competent\nbase detectors, affecting accuracy and stability, during model combination. In\nthis paper, we propose a framework---called Locally Selective Combination in\nParallel Outlier Ensembles (LSCP)---which addresses the issue by defining a\nlocal region around a test instance using the consensus of its nearest\nneighbors in randomly selected feature subspaces. The top-performing base\ndetectors in this local region are selected and combined as the model's final\noutput. Four variants of the LSCP framework are compared with seven widely used\nparallel frameworks. Experimental results demonstrate that one of these\nvariants, LSCP_AOM, consistently outperforms baselines on the majority of\ntwenty real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 17:02:30 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 02:49:05 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Zhao", "Yue", ""], ["Nasrullah", "Zain", ""], ["Hryniewicki", "Maciej K.", ""], ["Li", "Zheng", ""]]}, {"id": "1812.01567", "submitter": "Neda Sakhaee Ms", "authors": "Neda Sakhaee, Mark C Wilson", "title": "Information Extraction Framework to Build Legislation Network", "comments": "Artif Intell Law (2020)", "journal-ref": null, "doi": "10.1007/s10506-020-09263-3", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns an Information Extraction process for building a dynamic\nLegislation Network from legal documents. Unlike supervised learning approaches\nwhich require additional calculations, the idea here is to apply Information\nExtraction methodologies by identifying distinct expressions in legal text and\nextract quality network information. The study highlights the importance of\ndata accuracy in network analysis and improves approximate string matching\ntechniques for producing reliable network data-sets with more than 98 percent\nprecision and recall. The values, applications, and the complexity of the\ncreated dynamic Legislation Network are also discussed and challenged.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 18:15:33 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Sakhaee", "Neda", ""], ["Wilson", "Mark C", ""]]}, {"id": "1812.01748", "submitter": "Wang-Cheng Kang", "authors": "Wang-Cheng Kang, Eric Kim, Jure Leskovec, Charles Rosenberg, Julian\n  McAuley", "title": "Complete the Look: Scene-based Complementary Product Recommendation", "comments": "Accepted to CVPR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling fashion compatibility is challenging due to its complexity and\nsubjectivity. Existing work focuses on predicting compatibility between product\nimages (e.g. an image containing a t-shirt and an image containing a pair of\njeans). However, these approaches ignore real-world 'scene' images (e.g.\nselfies); such images are hard to deal with due to their complexity, clutter,\nvariations in lighting and pose (etc.) but on the other hand could potentially\nprovide key context (e.g. the user's body type, or the season) for making more\naccurate recommendations. In this work, we propose a new task called 'Complete\nthe Look', which seeks to recommend visually compatible products based on scene\nimages. We design an approach to extract training data for this task, and\npropose a novel way to learn the scene-product compatibility from fashion or\ninterior design images. Our approach measures compatibility both globally and\nlocally via CNNs and attention mechanisms. Extensive experiments show that our\nmethod achieves significant performance gains over alternative systems. Human\nevaluation and qualitative analysis are also conducted to further understand\nmodel behavior. We hope this work could lead to useful applications which link\nlarge corpora of real-world scenes with shoppable products.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 23:30:22 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 21:32:32 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Kang", "Wang-Cheng", ""], ["Kim", "Eric", ""], ["Leskovec", "Jure", ""], ["Rosenberg", "Charles", ""], ["McAuley", "Julian", ""]]}, {"id": "1812.01790", "submitter": "Charith Perera", "authors": "Balkis Abidi, Sadok Ben Yahia, Charith Perera", "title": "Hybrid Microaggregation for Privacy-Preserving Data Mining", "comments": "16", "journal-ref": "Journal of Ambient Intelligence and Humanized Computing, 2018", "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  k-Anonymity by microaggregation is one of the most commonly used\nanonymization techniques. This success is owe to the achievement of a worth of\ninterest tradeoff between information loss and identity disclosure risk.\nHowever, this method may have some drawbacks. On the disclosure limitation\nside, there is a lack of protection against attribute disclosure. On the data\nutility side, dealing with a real datasets is a challenging task to achieve.\nIndeed, the latter are characterized by their large number of attributes and\nthe presence of noisy data, such that outliers or, even, data with missing\nvalues. Generating an anonymous individual data useful for data mining tasks,\nwhile decreasing the influence of noisy data is a compelling task to achieve.\nIn this paper, we introduce a new microaggregation method, called HM-PFSOM,\nbased on fuzzy possibilistic clustering. Our proposed method operates through\nan hybrid manner. This means that the anonymization process is applied per\nblock of similar data. Thus, we can help to decrease the information loss\nduring the anonymization process. The HMPFSOM approach proposes to study the\ndistribution of confidential attributes within each sub-dataset. Then,\naccording to the latter distribution, the privacy parameter k is determined, in\nsuch a way to preserve the diversity of confidential attributes within the\nanonymized microdata. This allows to decrease the disclosure risk of\nconfidential information.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 07:42:33 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Abidi", "Balkis", ""], ["Yahia", "Sadok Ben", ""], ["Perera", "Charith", ""]]}, {"id": "1812.01808", "submitter": "Chia-Wei Chen", "authors": "Chia-Wei Chen, Sheng-Chuan Chou, Lun-Wei Ku", "title": "Enriching Article Recommendation with Phrase Awareness", "comments": "AAAI 2019 Workshop on Recommender Systems Meets NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning methods for recommendation systems are highly\nsophisticated. For article recommendation task, a neural network encoder which\ngenerates a latent representation of the article content would prove useful.\nHowever, using raw text with embedding for models could degrade sentence\nmeanings and deteriorate performance. In this paper, we propose PhrecSys\n(Phrase-based Recommendation System), which injects phrase-level features into\ncontent-based recommendation systems to enhance feature informativeness and\nmodel interpretability. Experiments conducted on six months of real-world data\ndemonstrate that phrase features boost content-based models in predicting both\nuser click and view behavior. Furthermore, the attention mechanism illustrates\nthat phrase awareness benefits the learning of textual focus by putting the\nmodel's attention on meaningful text spans, which leads to interpretable\narticle recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 03:57:35 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 16:30:35 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Chen", "Chia-Wei", ""], ["Chou", "Sheng-Chuan", ""], ["Ku", "Lun-Wei", ""]]}, {"id": "1812.01828", "submitter": "Piyush Mital", "authors": "Piyush Mital, Saurabh Agarwal, Bhargavi Neti, Yashodhara Haribhakta,\n  Vibhavari Kamble, Krishnanjan Bhattacharjee, Debashri Das, Swati Mehta, Ajai\n  Kumar", "title": "Graph based Question Answering System", "comments": "ICACCI 2018 Camera Ready and Accepted. ICACCI'18 is technically\n  co-sponsored by IEEE and IEEE Communications Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's digital age in the dawning era of big data analytics it is not the\ninformation but the linking of information through entities and actions which\ndefines the discourse. Any textual data either available on the Internet off\noff-line (like newspaper data, Wikipedia dump, etc) is basically connect\ninformation which cannot be treated isolated for its wholesome semantics. There\nis a need for an automated retrieval process with proper information extraction\nto structure the data for relevant and fast text analytics. The first big\nchallenge is the conversion of unstructured textual data to structured data.\nUnlike other databases, graph databases handle relationships and connections\nelegantly. Our project aims at developing a graph-based information extraction\nand retrieval system.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 06:31:48 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Mital", "Piyush", ""], ["Agarwal", "Saurabh", ""], ["Neti", "Bhargavi", ""], ["Haribhakta", "Yashodhara", ""], ["Kamble", "Vibhavari", ""], ["Bhattacharjee", "Krishnanjan", ""], ["Das", "Debashri", ""], ["Mehta", "Swati", ""], ["Kumar", "Ajai", ""]]}, {"id": "1812.01879", "submitter": "Ying Shen", "authors": "K. Lei, S. Si, D. Wen, and Y. Shen", "title": "An enhanced computational feature selection method for medical synonym\n  identification via bilingualism and multi-corpus training", "comments": null, "journal-ref": null, "doi": "10.1109/ICBDA.2017.8078771", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical synonym identification has been an important part of medical natural\nlanguage processing (NLP). However, in the field of Chinese medical synonym\nidentification, there are problems like low precision and low recall rate. To\nsolve the problem, in this paper, we propose a method for identifying Chinese\nmedical synonyms. We first selected 13 features including Chinese and English\nfeatures. Then we studied the synonym identification results of each feature\nalone and different combinations of the features. Through the comparison among\nidentification results, we present an optimal combination of features for\nChinese medical synonym identification. Experiments show that our selected\nfeatures have achieved 97.37% precision rate, 96.00% recall rate and 97.33% F1\nscore.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 09:45:52 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Lei", "K.", ""], ["Si", "S.", ""], ["Wen", "D.", ""], ["Shen", "Y.", ""]]}, {"id": "1812.01884", "submitter": "Ying Shen", "authors": "Kai Lei, Kaiqi Yuan, Qiang Zhang, Ying Shen", "title": "MedSim: A Novel Semantic Similarity Measure in Bio-medical Knowledge\n  Graphs", "comments": null, "journal-ref": "International Conference on Knowledge Science, Engineering and\n  Management KSEM 2018: Knowledge Science, Engineering and Management pp\n  479-490", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MedSim, a novel semantic SIMilarity method based on public\nwell-established bio-MEDical knowledge graphs (KGs) and large-scale corpus, to\nstudy the therapeutic substitution of antibiotics. Besides hierarchy and corpus\nof KGs, MedSim further interprets medicine characteristics by constructing\nmulti-dimensional medicine-specific feature vectors. Dataset of 528 antibiotic\npairs scored by doctors is applied for evaluation and MedSim has produced\nstatistically significant improvement over other semantic similarity methods.\nFurthermore, some promising applications of MedSim in drug substitution and\ndrug abuse prevention are presented in case study.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 09:58:54 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Lei", "Kai", ""], ["Yuan", "Kaiqi", ""], ["Zhang", "Qiang", ""], ["Shen", "Ying", ""]]}, {"id": "1812.01889", "submitter": "Ying Shen", "authors": "Kai Lei, Bing Zhang, Yong Liu, Yang Deng, Dongyu Zhang, Ying Shen", "title": "A Knowledge Graph Based Solution for Entity Discovery and Linking in\n  Open-Domain Questions", "comments": null, "journal-ref": "International Conference on Smart Computing and Communication\n  SmartCom 2017: Smart Computing and Communication pp 181-190", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named entity discovery and linking is the fundamental and core component of\nquestion answering. In Question Entity Discovery and Linking (QEDL) problem,\ntraditional methods are challenged because multiple entities in one short\nquestion are difficult to be discovered entirely and the incomplete information\nin short text makes entity linking hard to implement. To overcome these\ndifficulties, we proposed a knowledge graph based solution for QEDL and\ndeveloped a system consists of Question Entity Discovery (QED) module and\nEntity Linking (EL) module. The method of QED module is a tradeoff and ensemble\nof two methods. One is the method based on knowledge graph retrieval, which\ncould extract more entities in questions and guarantee the recall rate, the\nother is the method based on Conditional Random Field (CRF), which improves the\nprecision rate. The EL module is treated as a ranking problem and Learning to\nRank (LTR) method with features such as semantic similarity, text similarity\nand entity popularity is utilized to extract and make full use of the\ninformation in short texts. On the official dataset of a shared QEDL evaluation\ntask, our approach could obtain 64.44% F1 score of QED and 64.86% accuracy of\nEL, which ranks the 2nd place and indicates its practical use for QEDL problem.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 10:10:56 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Lei", "Kai", ""], ["Zhang", "Bing", ""], ["Liu", "Yong", ""], ["Deng", "Yang", ""], ["Zhang", "Dongyu", ""], ["Shen", "Ying", ""]]}, {"id": "1812.02074", "submitter": "Shuo Yang", "authors": "Shuo Yang, Zhenzhe Zheng, Shaojie Tang, Fan Wu, Guihai Chen", "title": "Fine-Grained User Profiling for Personalized Task Matching in Mobile\n  Crowdsensing", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mobile crowdsensing, finding the best match between tasks and users is\ncrucial to ensure both the quality and effectiveness of a crowdsensing system.\nExisting works usually assume a centralized task assignment by the crowdsensing\nplatform, without addressing the need of fine-grained personalized task\nmatching. In this paper, we argue that it is essential to match tasks to users\nbased on a careful characterization of both the users' preference and\nreliability. To that end, we propose a personalized task recommender system for\nmobile crowdsensing, which recommends tasks to users based on a recommendation\nscore that jointly takes each user's preference and reliability into\nconsideration. We first present a hybrid preference metric to characterize\nusers' preference by exploiting their implicit feedback. Then, to profile\nusers' reliability levels, we formalize the problem as a semi-supervised\nlearning model, and propose an efficient block coordinate descent algorithm to\nsolve the problem. For some tasks that lack users' historical information, we\nfurther propose a matrix factorization method to infer the users' reliability\nlevels on those tasks. We conduct extensive experiments to evaluate the\nperformance of our system, and the evaluation results demonstrate that our\nsystem can achieve superior performance to the benchmarks in both user\nprofiling and personalized task recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 14:37:44 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Yang", "Shuo", ""], ["Zheng", "Zhenzhe", ""], ["Tang", "Shaojie", ""], ["Wu", "Fan", ""], ["Chen", "Guihai", ""]]}, {"id": "1812.02091", "submitter": "Kubilay Atasu", "authors": "Kubilay Atasu, Thomas Mittelholzer", "title": "Low-Complexity Data-Parallel Earth Mover's Distance Approximations", "comments": "To appear in ICML 2019:\n  http://proceedings.mlr.press/v97/atasu19a.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Earth Mover's Distance (EMD) is a state-of-the art metric for comparing\ndiscrete probability distributions, but its high distinguishability comes at a\nhigh cost in computational complexity. Even though linear-complexity\napproximation algorithms have been proposed to improve its scalability, these\nalgorithms are either limited to vector spaces with only a few dimensions or\nthey become ineffective when the degree of overlap between the probability\ndistributions is high. We propose novel approximation algorithms that overcome\nboth of these limitations, yet still achieve linear time complexity. All our\nalgorithms are data parallel, and thus, we take advantage of massively parallel\ncomputing engines, such as Graphics Processing Units (GPUs). On the popular\ntext-based 20 Newsgroups dataset, the new algorithms are four orders of\nmagnitude faster than a multi-threaded CPU implementation of Word Mover's\nDistance and match its nearest-neighbors-search accuracy. On MNIST images, the\nnew algorithms are four orders of magnitude faster than a GPU implementation of\nthe Sinkhorn's algorithm while offering a slightly higher\nnearest-neighbors-search accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 16:29:44 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 14:51:02 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Atasu", "Kubilay", ""], ["Mittelholzer", "Thomas", ""]]}, {"id": "1812.02129", "submitter": "Michael Segundo Ortiz", "authors": "Michael Segundo Ortiz, Kazuhiro Seki, Javed Mostafa", "title": "Toward Exploratory Search in Biomedicine: Evaluating Document Clusters\n  by MeSH as a Semantic Anchor", "comments": "This work is currently under consideration for the 17th World\n  Congress of Medical and Health Informatics. Please follow the link for more\n  information - http://www.medinfo-lyon.org/en/about-us/medinfo2019/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current mode of biomedical literature search is severely limited in\neffectively finding information relevant to specialists. A potential approach\nto solving this problem is exploratory search, which allows users to\ninteractively navigate through a vast document collection. As the first step\ntoward exploratory search for specialists in biomedicine, this paper develops a\nmethodology to evaluate quality of document clusters. For this purpose, we\nincorporate human expertise into data set creation and evaluation framework by\nleveraging MeSH terms as semantic anchors. In addition, we investigate the\nbenefit of full-text data for improving cluster quality.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 17:31:47 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Ortiz", "Michael Segundo", ""], ["Seki", "Kazuhiro", ""], ["Mostafa", "Javed", ""]]}, {"id": "1812.02171", "submitter": "Umanga Bista", "authors": "Umanga Bista, Alexander Mathews, Minjeong Shin, Aditya Krishna Menon,\n  Lexing Xie", "title": "Comparative Document Summarisation via Classification", "comments": "Accepted for AAAI 2019", "journal-ref": "Proceedings of the AAAI Conference on Artificial Intelligence.\n  Vol. 33. 2019", "doi": "10.1609/aaai.v33i01.330120", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers extractive summarisation in a comparative setting: given\ntwo or more document groups (e.g., separated by publication time), the goal is\nto select a small number of documents that are representative of each group,\nand also maximally distinguishable from other groups. We formulate a set of new\nobjective functions for this problem that connect recent literature on document\nsummarisation, interpretable machine learning, and data subset selection. In\nparticular, by casting the problem as a binary classification amongst different\ngroups, we derive objectives based on the notion of maximum mean discrepancy,\nas well as a simple yet effective gradient-based optimisation strategy. Our new\nformulation allows scalable evaluations of comparative summarisation as a\nclassification task, both automatically and via crowd-sourcing. To this end, we\nevaluate comparative summarisation methods on a newly curated collection of\ncontroversial news topics over 13 months. We observe that gradient-based\noptimisation outperforms discrete and baseline approaches in 14 out of 24\ndifferent automatic evaluation settings. In crowd-sourced evaluations,\nsummaries from gradient optimisation elicit 7% more accurate classification\nfrom human workers than discrete optimisation. Our result contrasts with recent\nliterature on submodular data subset selection that favours discrete\noptimisation. We posit that our formulation of comparative summarisation will\nprove useful in a diverse range of use cases such as comparing content sources,\nauthors, related topics, or distinct view points.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 04:04:56 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 08:42:03 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Bista", "Umanga", ""], ["Mathews", "Alexander", ""], ["Shin", "Minjeong", ""], ["Menon", "Aditya Krishna", ""], ["Xie", "Lexing", ""]]}, {"id": "1812.02305", "submitter": "Mohammad Tahsin Mostafiz", "authors": "Tahsin Mostafiz, Khalid Ashraf", "title": "Pathology Extraction from Chest X-Ray Radiology Reports: A Performance\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extraction of relevant pathological terms from radiology reports is important\nfor correct image label generation and disease population studies. In this\nletter, we compare the performance of some known application program interface\n(APIs) for the task of thoracic abnormality extraction from radiology reports.\nWe explored several medical domain specific annotation tools like Medical Text\nIndexer(MTI) with Non-MEDLINE and Mesh On Demand(MOD) options and generic\nNatural Language Understanding (NLU) API provided by the IBM cloud. Our results\nshow that although MTI and MOD are intended for extracting medical terms, their\nperformance is worst compared to generic extraction API like IBM NLU. Finally,\nwe trained a DNN-based Named Entity Recognition (NER) model to extract the key\nconcept words from radiology reports. Our model outperforms the medical\nspecific and generic API performance by a large margin. Our results demonstrate\nthe inadequacy of generic APIs for pathology extraction task and establish the\nimportance of domain specific model training for improved results. We hope that\nthese results motivate the research community to release larger de-identified\nradiology reports corpus for building high accuracy machine learning models for\nthe important task of pathology extraction.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 06:56:16 GMT"}], "update_date": "2018-12-08", "authors_parsed": [["Mostafiz", "Tahsin", ""], ["Ashraf", "Khalid", ""]]}, {"id": "1812.02309", "submitter": "Lina Fatima Soualmia", "authors": "Sa\\\"id Abdedda\\\"im, Sylvestre Vimard, Lina Fatima Soualmia", "title": "The MeSH-gram Neural Network Model: Extending Word Embedding Vectors\n  with MeSH Concepts for UMLS Semantic Similarity and Relatedness in the\n  Biomedical Domain", "comments": "6 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eliciting semantic similarity between concepts in the biomedical domain\nremains a challenging task. Recent approaches founded on embedding vectors have\ngained in popularity as they risen to efficiently capture semantic\nrelationships The underlying idea is that two words that have close meaning\ngather similar contexts. In this study, we propose a new neural network model\nnamed MeSH-gram which relies on a straighforward approach that extends the\nskip-gram neural network model by considering MeSH (Medical Subject Headings)\ndescriptors instead words. Trained on publicly available corpus PubMed MEDLINE,\nMeSH-gram is evaluated on reference standards manually annotated for semantic\nsimilarity. MeSH-gram is first compared to skip-gram with vectors of size 300\nand at several windows contexts. A deeper comparison is performed with tewenty\nexisting models. All the obtained results of Spearman's rank correlations\nbetween human scores and computed similarities show that MeSH-gram outperforms\nthe skip-gram model, and is comparable to the best methods but that need more\ncomputation and external resources.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 07:48:27 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Abdedda\u00efm", "Sa\u00efd", ""], ["Vimard", "Sylvestre", ""], ["Soualmia", "Lina Fatima", ""]]}, {"id": "1812.02353", "submitter": "Minmin Chen", "authors": "Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois\n  Belletti, Ed Chi", "title": "Top-K Off-Policy Correction for a REINFORCE Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Industrial recommender systems deal with extremely large action spaces --\nmany millions of items to recommend. Moreover, they need to serve billions of\nusers, who are unique at any point in time, making a complex user state space.\nLuckily, huge quantities of logged implicit feedback (e.g., user clicks, dwell\ntime) are available for learning. Learning from the logged feedback is however\nsubject to biases caused by only observing feedback on recommendations selected\nby the previous versions of the recommender. In this work, we present a general\nrecipe of addressing such biases in a production top-K recommender system at\nYoutube, built with a policy-gradient-based algorithm, i.e. REINFORCE. The\ncontributions of the paper are: (1) scaling REINFORCE to a production\nrecommender system with an action space on the orders of millions; (2) applying\noff-policy correction to address data biases in learning from logged feedback\ncollected from multiple behavior policies; (3) proposing a novel top-K\noff-policy correction to account for our policy recommending multiple items at\na time; (4) showcasing the value of exploration. We demonstrate the efficacy of\nour approaches through a series of simulations and multiple live experiments on\nYoutube.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 05:10:27 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 01:09:30 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Chen", "Minmin", ""], ["Beutel", "Alex", ""], ["Covington", "Paul", ""], ["Jain", "Sagar", ""], ["Belletti", "Francois", ""], ["Chi", "Ed", ""]]}, {"id": "1812.02523", "submitter": "Antonio Fonseca", "authors": "Ant\\'onio Filipe Fonseca", "title": "Representing pictures with emotions", "comments": "V1.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern research in content-based image retrieval systems (CIBR) has become\nprogressively more focused on the richness of human semantics. Several\napproaches may be used to reduced the 'semantic gap' between the high-level\nhuman experience and the low level visual features of pictures. Object\nontology, among others, is one of the methods. In this paper we investigate the\nuse of a codified emotion ontology over global color features of images to\nannotate the images at a high semantic level. In order to speed up the\nannotation process the images are sampled so that each digital image is\nrepresented by a random subset of its content. We test within controlled\nconditions how this random subset may represent the adequate high level\nemotional concept presented in the image. We monitor this information reducing\nprocess with entropy measures, showing that controlled random sampling can\ncapture with significant relevance high level concepts for picture\nrepresentation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 13:50:40 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 13:35:05 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Fonseca", "Ant\u00f3nio Filipe", ""]]}, {"id": "1812.02529", "submitter": "Haifeng Wang", "authors": "Haifeng Wang", "title": "Utilizing Imbalanced Data and Classification Cost Matrix to Predict\n  Movie Preferences", "comments": "12 pages, 4 figures", "journal-ref": "International Journal of Artificial Intelligence and Applications\n  (IJAIA), Vol.9, No.6, November 2018", "doi": "10.5121/ijaia.2018.9601", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a movie genre recommendation system based on\nimbalanced survey data and unequal classification costs for small and\nmedium-sized enterprises (SMEs) who need a data-based and analytical approach\nto stock favored movies and target marketing to young people. The dataset\nmaintains a detailed personal profile as predictors including demographic,\nbehavioral and preferences information for each user as well as imbalanced\ngenre preferences. These predictors do not include the information such as\nactors or directors. The paper applies Gentle boost, Adaboost and Bagged tree\nensembles as well as SVM machine learning algorithms to learn classification\nfrom one thousand observations and predict movie genre preferences with\nadjusted classification costs. The proposed recommendation system also selects\nimportant predictors to avoid overfitting and to shorten training time. This\npaper compares the test error among the above-mentioned algorithms that are\nused to recommend different movie genres. The prediction power is also\nindicated in a comparison of precision and recall with other state-of-the-art\nrecommendation systems. The proposed movie genre recommendation system solves\nproblems such as small dataset, imbalanced response, and unequal classification\ncosts.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 15:28:45 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Wang", "Haifeng", ""]]}, {"id": "1812.02646", "submitter": "Zhaochun Ren", "authors": "Pengjie Ren, Zhumin Chen, Jing Li, Zhaochun Ren, Jun Ma, Maarten de\n  Rijke", "title": "RepeatNet: A Repeat Aware Neural Recommendation Machine for\n  Session-based Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks for session-based recommendation have attracted a\nlot of attention recently because of their promising performance. repeat\nconsumption is a common phenomenon in many recommendation scenarios (e.g.,\ne-commerce, music, and TV program recommendations), where the same item is\nre-consumed repeatedly over time. However, no previous studies have emphasized\nrepeat consumption with neural networks. An effective neural approach is needed\nto decide when to perform repeat recommendation. In this paper, we incorporate\na repeat-explore mechanism into neural networks and propose a new model, called\nRepeatNet, with an encoder-decoder structure. RepeatNet integrates a regular\nneural recommendation approach in the decoder with a new repeat recommendation\nmechanism that can choose items from a user's history and recommends them at\nthe right time. We report on extensive experiments on three benchmark datasets.\nRepeatNet outperforms state-of-the-art baselines on all three datasets in terms\nof MRR and Recall. Furthermore, as the dataset size and the repeat ratio\nincrease, the improvements of RepeatNet over the baselines also increase, which\ndemonstrates its advantage in handling repeat recommendation scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 16:30:33 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Ren", "Pengjie", ""], ["Chen", "Zhumin", ""], ["Li", "Jing", ""], ["Ren", "Zhaochun", ""], ["Ma", "Jun", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1812.02869", "submitter": "Chen Ma", "authors": "Chen Ma, Peng Kang, Bin Wu, Qinglong Wang and Xue Liu", "title": "Gated Attentive-Autoencoder for Content-Aware Recommendation", "comments": "Accepted by the 12th ACM International Conference on Web Search and\n  Data Mining (WSDM 2019), code available: https://github.com/allenjack/GATE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of Internet services and mobile devices provides an\nexcellent opportunity to satisfy the strong demand for the personalized item or\nproduct recommendation. However, with the tremendous increase of users and\nitems, personalized recommender systems still face several challenging\nproblems: (1) the hardness of exploiting sparse implicit feedback; (2) the\ndifficulty of combining heterogeneous data. To cope with these challenges, we\npropose a gated attentive-autoencoder (GATE) model, which is capable of\nlearning fused hidden representations of items' contents and binary ratings,\nthrough a neural gating structure. Based on the fused representations, our\nmodel exploits neighboring relations between items to help infer users'\npreferences. In particular, a word-level and a neighbor-level attention module\nare integrated with the autoencoder. The word-level attention learns the item\nhidden representations from items' word sequences, while favoring informative\nwords by assigning larger attention weights. The neighbor-level attention\nlearns the hidden representation of an item's neighborhood by considering its\nneighbors in a weighted manner. We extensively evaluate our model with several\nstate-of-the-art methods and different validation metrics on four real-world\ndatasets. The experimental results not only demonstrate the effectiveness of\nour model on top-N recommendation but also provide interpretable results\nattributed to the attention modules.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 01:43:45 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Ma", "Chen", ""], ["Kang", "Peng", ""], ["Wu", "Bin", ""], ["Wang", "Qinglong", ""], ["Liu", "Xue", ""]]}, {"id": "1812.02930", "submitter": "Nguyen Khoi Tran", "authors": "Nguyen Khoi Tran, Quan Z. Sheng, M. Ali Babar, Lina Yao, Wei Emma\n  Zhang, Schahram Dustdar", "title": "Internet of Things Search Engine: Concepts, Classification, and Open\n  Issues", "comments": "Accepted for publication in Communications of the ACM", "journal-ref": null, "doi": "10.1145/3284763", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article focuses on the complicated yet still relatively immature area of\nthe Internet of Things Search Engines (IoTSE). It introduces related concepts\nof IoTSE and a model called meta-path to describe and classify IoTSE systems\nbased on their functionality. Based on these concepts, we have organized the\nresearch and development efforts on IoTSE into eight groups and presented the\nrepresentative works in each group. The concepts and ideas presented in this\narticle are generated from an extensive structured study on over 200 works\nspanning over one decade of IoTSE research and development.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 06:58:15 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Tran", "Nguyen Khoi", ""], ["Sheng", "Quan Z.", ""], ["Babar", "M. Ali", ""], ["Yao", "Lina", ""], ["Zhang", "Wei Emma", ""], ["Dustdar", "Schahram", ""]]}, {"id": "1812.02934", "submitter": "Chengsheng Mao", "authors": "Chengsheng Mao, Bin Hu, Lei Chen, Philip Moore and Xiaowei Zhang", "title": "Local Distribution in Neighborhood for Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-nearest-neighbor method performs classification tasks for a query\nsample based on the information contained in its neighborhood. Previous studies\ninto the k-nearest-neighbor algorithm usually achieved the decision value for a\nclass by combining the support of each sample in the neighborhood. They have\ngenerally considered the nearest neighbors separately, and potentially integral\nneighborhood information important for classification was lost, e.g. the\ndistribution information. This article proposes a novel local learning method\nthat organizes the information in the neighborhood through local distribution.\nIn the proposed method, additional distribution information in the neighborhood\nis estimated and then organized; the classification decision is made based on\nmaximum posterior probability which is estimated from the local distribution in\nthe neighborhood. Additionally, based on the local distribution, we generate a\ngeneralized local classification form that can be effectively applied to\nvarious datasets through tuning the parameters. We use both synthetic and real\ndatasets to evaluate the classification performance of the proposed method; the\nexperimental results demonstrate the dimensional scalability, efficiency,\neffectiveness and robustness of the proposed method compared to some other\nstate-of-the-art classifiers. The results indicate that the proposed method is\neffective and promising in a broad range of domains.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 07:50:48 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Mao", "Chengsheng", ""], ["Hu", "Bin", ""], ["Chen", "Lei", ""], ["Moore", "Philip", ""], ["Zhang", "Xiaowei", ""]]}, {"id": "1812.03030", "submitter": "Tanvi Bajpai", "authors": "Arda Antikacioglu, Tanvi Bajpai, R. Ravi", "title": "A new system-wide diversity measure for recommendations with efficient\n  algorithms", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems often operate on item catalogs clustered by genres, and\nuser bases that have natural clusterings into user types by demographic or\npsychographic attributes. Prior work on system-wide diversity has mainly\nfocused on defining intent-aware metrics among such categories and maximizing\nrelevance of the resulting recommendations, but has not combined the notions of\ndiversity from the two point of views of items and users. In this work, (1) we\nintroduce two new system-wide diversity metrics to simultaneously address the\nproblems of diversifying the categories of items that each user sees,\ndiversifying the types of users that each item is shown, and maintaining high\nrecommendation quality. We model this as a subgraph selection problem on the\nbipartite graph of candidate recommendations between users and items. (2) In\nthe case of disjoint item categories and user types, we show that the resulting\nproblems can be solved exactly in polynomial time, by a reduction to a minimum\ncost flow problem. (3) In the case of non-disjoint categories and user types,\nwe prove NP-completeness of the objective and present efficient approximation\nalgorithms using the submodularity of the objective. (4) Finally, we validate\nthe effectiveness of our algorithms on the MovieLens-1m and Netflix datasets,\nand show that algorithms designed for our objective also perform well on sales\ndiversity metrics, and even some intent-aware diversity metrics. Our\nexperimental results justify the validity of our new composite diversity\nmetrics.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 08:28:38 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 04:28:47 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Antikacioglu", "Arda", ""], ["Bajpai", "Tanvi", ""], ["Ravi", "R.", ""]]}, {"id": "1812.03226", "submitter": "Himan Abdollahpouri", "authors": "Himan Abdollahpouri and Steve Essinger", "title": "Towards Effective Exploration/Exploitation in Sequential Music\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music streaming companies collectively serve billions of songs per day.\nRadio-based music services may intersperse audio advertisements among the songs\nas a means to generate revenue, much like traditional FM radio. Regardless of\nthe monetization approach, the recommender system should decide when to play\ncontent that the listener is known to enjoy (exploit) and content that is novel\nto the listener (explore). Recommender systems that rely on this\nexplore/exploit type framework have been deployed in a wide variety of\napplications such as movies, books, music, shopping and more. In this work, we\ninvestigate the impact of different ad/song sequences on listener behavior. In\nparticular, we focus on the impact of exploring new song content for the\nlistener given the previous sequence of ads and songs in the listener's\nsession. Our results show that the prior sequence matters when considering song\nexploration and that this prior sequence has an impact on the listener's\ntendency to interrupt their current session.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 21:55:01 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Abdollahpouri", "Himan", ""], ["Essinger", "Steve", ""]]}, {"id": "1812.03781", "submitter": "Pranav A", "authors": "Pranav A, Nick Sukiennik, Pan Hui", "title": "Inflo: News Categorization and Keyphrase Extraction for Implementation\n  in an Aggregation System", "comments": "Demo paper, links inside the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work herein describes a system for automatic news category and keyphrase\nlabeling, presented in the context of our motivation to improve the speed at\nwhich a user can find relevant and interesting content within an aggregation\nplatform. A set of 12 discrete categories were applied to over 500,000 news\narticles for training a neural network, to be used to facilitate the more\nin-depth task of extracting the most significant keyphrases. The latter was\ndone using three methods: statistical, graphical and numerical, using the\npre-identified category label to improve relevance of extracted phrases. The\nresults are presented in a demo in which the articles are pre-populated via\nNews API, and upon being selected, the category and keyphrase labels will be\ncomputed via the methods explained herein.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 13:36:09 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["A", "Pranav", ""], ["Sukiennik", "Nick", ""], ["Hui", "Pan", ""]]}, {"id": "1812.03835", "submitter": "Haofeng Jia", "authors": "Haofeng Jia and Erik Saule", "title": "Graph Embedding for Citation Recommendation", "comments": "arXiv admin note: text overlap with arXiv:1607.00653,\n  arXiv:1403.6652, arXiv:1405.4053 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As science advances, the academic community has published millions of\nresearch papers. Researchers devote time and effort to search relevant\nmanuscripts when writing a paper or simply to keep up with current research. In\nthis paper, we consider the problem of citation recommendation on graph and\npropose a task-specific neighborhood construction strategy to learn the\ndistributed representations of papers. In addition, given the learned\nrepresentations, we investigate various schemes to rank the candidate papers\nfor citation recommendation. The experimental results show our proposed\nneighborhood construction strategy outperforms the widely-used random walks\nbased sampling strategy on all ranking schemes, and the model based ranking\nscheme outperforms embedding based rankings for both neighborhood construction\nstrategies. We also demonstrated that graph embedding is a robust approach for\ncitation recommendation when hidden ratio changes, while the performance of\nclassic methods drop significantly when the set of seed papers is becoming\nsmall.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 21:33:23 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Jia", "Haofeng", ""], ["Saule", "Erik", ""]]}, {"id": "1812.04109", "submitter": "Junjie Liang", "authors": "Junjie Liang, Jinlong Hu, Shoubin Dong and Vasant Honavar", "title": "Top-N-Rank: A Scalable List-wise Ranking Method for Recommender Systems", "comments": "paper accepted by the 2018 IEEE International Conference on Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Top-N-Rank, a novel family of list-wise Learning-to-Rank models\nfor reliably recommending the N top-ranked items. The proposed models optimize\na variant of the widely used discounted cumulative gain (DCG) objective\nfunction which differs from DCG in two important aspects: (i) It limits the\nevaluation of DCG only on the top N items in the ranked lists, thereby\neliminating the impact of low-ranked items on the learned ranking function; and\n(ii) it incorporates weights that allow the model to leverage multiple types of\nimplicit feedback with differing levels of reliability or trustworthiness.\nBecause the resulting objective function is non-smooth and hence challenging to\noptimize, we consider two smooth approximations of the objective function,\nusing the traditional sigmoid function and the rectified linear unit (ReLU). We\npropose a family of learning-to-rank algorithms (Top-N-Rank) that work with any\nsmooth objective function. Then, a more efficient variant, Top-N-Rank.ReLU, is\nintroduced, which effectively exploits the properties of ReLU function to\nreduce the computational complexity of Top-N-Rank from quadratic to linear in\nthe average number of items rated by users. The results of our experiments\nusing two widely used benchmarks, namely, the MovieLens data set and the Amazon\nVideo Games data set demonstrate that: (i) The `top-N truncation' of the\nobjective function substantially improves the ranking quality of the top N\nrecommendations; (ii) using the ReLU for smoothing the objective function\nyields significant improvement in both ranking quality as well as runtime as\ncompared to using the sigmoid; and (iii) Top-N-Rank.ReLU substantially\noutperforms the well-performing list-wise ranking methods in terms of ranking\nquality.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 21:39:16 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 17:24:46 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Liang", "Junjie", ""], ["Hu", "Jinlong", ""], ["Dong", "Shoubin", ""], ["Honavar", "Vasant", ""]]}, {"id": "1812.04118", "submitter": "Cailey Kerley", "authors": "Cailey I. Kerley, Yuankai Huo, Shikha Chaganti, Shunxing Bao, Mayur B.\n  Patel, Bennett A. Landman", "title": "Montage based 3D Medical Image Retrieval from Traumatic Brain Injury\n  Cohort using Deep Convolutional Neural Network", "comments": "Accepted for SPIE: Medical Imaging 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain imaging analysis on clinically acquired computed tomography (CT) is\nessential for the diagnosis, risk prediction of progression, and treatment of\nthe structural phenotypes of traumatic brain injury (TBI). However, in real\nclinical imaging scenarios, entire body CT images (e.g., neck, abdomen, chest,\npelvis) are typically captured along with whole brain CT scans. For instance,\nin a typical sample of clinical TBI imaging cohort, only ~15% of CT scans\nactually contain whole brain CT images suitable for volumetric brain analyses;\nthe remaining are partial brain or non-brain images. Therefore, a manual image\nretrieval process is typically required to isolate the whole brain CT scans\nfrom the entire cohort. However, the manual image retrieval is time and\nresource consuming and even more difficult for the larger cohorts. To alleviate\nthe manual efforts, in this paper we propose an automated 3D medical image\nretrieval pipeline, called deep montage-based image retrieval (dMIR), which\nperforms classification on 2D montage images via a deep convolutional neural\nnetwork. The novelty of the proposed method for image processing is to\ncharacterize the medical image retrieval task based on the montage images. In a\ncohort of 2000 clinically acquired TBI scans, 794 scans were used as training\ndata, 206 scans were used as validation data, and the remaining 1000 scans were\nused as testing data. The proposed achieved accuracy=1.0, recall=1.0,\nprecision=1.0, f1=1.0 for validation data, while achieved accuracy=0.988,\nrecall=0.962, precision=0.962, f1=0.962 for testing data. Thus, the proposed\ndMIR is able to perform accurate CT whole brain image retrieval from\nlarge-scale clinical cohorts.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 21:58:01 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Kerley", "Cailey I.", ""], ["Huo", "Yuankai", ""], ["Chaganti", "Shikha", ""], ["Bao", "Shunxing", ""], ["Patel", "Mayur B.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1812.04265", "submitter": "Anne Dirkson", "authors": "Alex Brandsen, Anne Dirkson, Wessel Kraaij, Wout Lamers, Suzan\n  Verberne, Hugo de Vos, Gineke Wiggers", "title": "Proceedings of the 17th Dutch-Belgian Information Retrieval Workshop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the papers presented at DIR 2018: 17th Dutch-Belgian\nInformation Retrieval Workshop (DIR) held on November 23, 2018 in Leiden. DIR\naims to serve as an international platform (with a special focus on the\nNetherlands and Belgium) for exchange and discussions on research &\napplications in the field of information retrieval and related fields.\n  The committee accepted 4 short papers presenting novel work, 3 demo\nproposals, and 8 compressed contributions (summaries of papers recently\npublished in international journals and conferences). Each submission was\nreviewed by at least 3 programme committee members.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 08:23:01 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Brandsen", "Alex", ""], ["Dirkson", "Anne", ""], ["Kraaij", "Wessel", ""], ["Lamers", "Wout", ""], ["Verberne", "Suzan", ""], ["de Vos", "Hugo", ""], ["Wiggers", "Gineke", ""]]}, {"id": "1812.04407", "submitter": "Xiaoting Zhao", "authors": "Xiaoting Zhao, Raphael Louca, Diane Hu, Liangjie Hong", "title": "Learning Item-Interaction Embeddings for User Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry-scale recommendation systems have become a cornerstone of the\ne-commerce shopping experience. For Etsy, an online marketplace with over 50\nmillion handmade and vintage items, users come to rely on personalized\nrecommendations to surface relevant items from its massive inventory. One\nhallmark of Etsy's shopping experience is the multitude of ways in which a user\ncan interact with an item they are interested in: they can view it, favorite\nit, add it to a collection, add it to cart, purchase it, etc. We hypothesize\nthat the different ways in which a user interacts with an item indicates\ndifferent kinds of intent. Consequently, a user's recommendations should be\nbased not only on the item from their past activity, but also the way in which\nthey interacted with that item. In this paper, we propose a novel method for\nlearning interaction-based item embeddings that encode the co-occurrence\npatterns of not only the item itself, but also the interaction type. The\nlearned embeddings give us a convenient way of approximating the likelihood\nthat one item-interaction pair would co-occur with another by way of a simple\ninner product. Because of its computational efficiency, our model lends itself\nnaturally as a candidate set selection method, and we evaluate it as such in an\nindustry-scale recommendation system that serves live traffic on Etsy.com. Our\nexperiments reveal that taking interaction type into account shows promising\nresults in improving the accuracy of modeling user shopping behavior.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 14:06:13 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Zhao", "Xiaoting", ""], ["Louca", "Raphael", ""], ["Hu", "Diane", ""], ["Hong", "Liangjie", ""]]}, {"id": "1812.04412", "submitter": "Chang Li", "authors": "Chang Li, Ilya Markov, Maarten de Rijke and Masrour Zoghi", "title": "MergeDTS: A Method for Effective Large-Scale Online Ranker Evaluation", "comments": "Accepted at TOIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online ranker evaluation is one of the key challenges in information\nretrieval. While the preferences of rankers can be inferred by interleaving\nmethods, the problem of how to effectively choose the ranker pair that\ngenerates the interleaved list without degrading the user experience too much\nis still challenging. On the one hand, if two rankers have not been compared\nenough, the inferred preference can be noisy and inaccurate. On the other, if\ntwo rankers are compared too many times, the interleaving process inevitably\nhurts the user experience too much. This dilemma is known as the exploration\nversus exploitation tradeoff. It is captured by the $K$-armed dueling bandit\nproblem, which is a variant of the $K$-armed bandit problem, where the feedback\ncomes in the form of pairwise preferences. Today's deployed search systems can\nevaluate a large number of rankers concurrently, and scaling effectively in the\npresence of numerous rankers is a critical aspect of $K$-armed dueling bandit\nproblems.\n  In this paper, we focus on solving the large-scale online ranker evaluation\nproblem under the so-called Condorcet assumption, where there exists an optimal\nranker that is preferred to all other rankers. We propose Merge Double Thompson\nSampling (MergeDTS), which first utilizes a divide-and-conquer strategy that\nlocalizes the comparisons carried out by the algorithm to small batches of\nrankers, and then employs Thompson Sampling (TS) to reduce the comparisons\nbetween suboptimal rankers inside these small batches. The effectiveness\n(regret) and efficiency (time complexity) of MergeDTS are extensively evaluated\nusing examples from the domain of online evaluation for web search. Our main\nfinding is that for large-scale Condorcet ranker evaluation problems, MergeDTS\noutperforms the state-of-the-art dueling bandit algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 14:07:53 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 11:23:25 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Li", "Chang", ""], ["Markov", "Ilya", ""], ["de Rijke", "Maarten", ""], ["Zoghi", "Masrour", ""]]}, {"id": "1812.04900", "submitter": "Mirela Danubianu Mrs", "authors": "Mirela Danubianu, Stefan Gheorghe Pentiuc, Iolanda Tobolcea and\n  Tiberiu Socaciu", "title": "Model of a Data Mining System for Personalized Therapy of Speech\n  Disorders", "comments": "6 pages, 3 figures, Journal of applied computer science and\n  mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Lately, the children with speech disorder have more and more become object of\nspecialists attention and investment in speech disorder therapy are increasing\nThe development and use of information technology in order to assist and follow\nspeech disorder therapy allowed researchers to collect a considerable volume of\ndata. The aim of this paper is to present a data mining system designed to be\nassociated with TERAPERS system in order to provide information based on which\none could improve the process of personalized therapy of speech disorders.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 11:18:19 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Danubianu", "Mirela", ""], ["Pentiuc", "Stefan Gheorghe", ""], ["Tobolcea", "Iolanda", ""], ["Socaciu", "Tiberiu", ""]]}, {"id": "1812.04910", "submitter": "Chang Li", "authors": "Chang Li, Artem Grotov, Ilya Markov, and Maarten de Rijke", "title": "Online Learning to Rank with List-level Feedback for Image Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning to rank (OLTR) via implicit feedback has been extensively\nstudied for document retrieval in cases where the feedback is available at the\nlevel of individual items. To learn from item-level feedback, the current\nalgorithms require certain assumptions about user behavior. In this paper, we\nstudy a more general setup: OLTR with list-level feedback, where the feedback\nis provided only at the level of an entire ranked list. We propose two methods\nthat allow online learning to rank in this setup. The first method, PGLearn,\nuses a ranking model to generate policies and optimizes it online using policy\ngradients. The second method, RegLearn, learns to combine individual document\nrelevance scores by directly predicting the observed list-level feedback\nthrough regression. We evaluate the proposed methods on the image filtering\ntask, in which deep neural networks (DNNs) are used to rank images in response\nto a set of standing queries. We show that PGLearn does not perform well in\nOLTR with list-level feedback. RegLearn, instead, shows good performance in\nboth online and offline metrics.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 11:58:58 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 12:38:04 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Li", "Chang", ""], ["Grotov", "Artem", ""], ["Markov", "Ilya", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1812.05001", "submitter": "John McCrae", "authors": "Narumol Prangnawarat, John P. McCrae and Conor Hayes", "title": "Temporal Analysis of Entity Relatedness and its Evolution using\n  Wikipedia and DBpedia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many researchers have made use of the Wikipedia network for relatedness and\nsimilarity tasks. However, most approaches use only the most recent information\nand not historical changes in the network. We provide an analysis of entity\nrelatedness using temporal graph-based approaches over different versions of\nthe Wikipedia article link network and DBpedia, which is an open-source\nknowledge base extracted from Wikipedia. We consider creating the Wikipedia\narticle link network as both a union and intersection of edges over multiple\ntime points and present a novel variation of the Jaccard index to weight edges\nbased on their transience. We evaluate our results against the KORE dataset,\nwhich was created in 2010, and show that using the 2010 Wikipedia article link\nnetwork produces the strongest result, suggesting that semantic similarity is\ntime sensitive. We then show that integrating multiple time frames in our\nmethods can give a better overall similarity demonstrating that temporal\nevolution can have an important effect on entity relatedness.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 16:11:31 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Prangnawarat", "Narumol", ""], ["McCrae", "John P.", ""], ["Hayes", "Conor", ""]]}, {"id": "1812.05161", "submitter": "Aman Agarwal", "authors": "Aman Agarwal, Ivan Zaitsev, Xuanhui Wang, Cheng Li, Marc Najork and\n  Thorsten Joachims", "title": "Estimating Position Bias without Intrusive Interventions", "comments": null, "journal-ref": null, "doi": "10.1145/3289600.3291017", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presentation bias is one of the key challenges when learning from implicit\nfeedback in search engines, as it confounds the relevance signal. While it was\nrecently shown how counterfactual learning-to-rank (LTR) approaches\n\\cite{Joachims/etal/17a} can provably overcome presentation bias when\nobservation propensities are known, it remains to show how to effectively\nestimate these propensities. In this paper, we propose the first method for\nproducing consistent propensity estimates without manual relevance judgments,\ndisruptive interventions, or restrictive relevance modeling assumptions. First,\nwe show how to harvest a specific type of intervention data from historic\nfeedback logs of multiple different ranking functions, and show that this data\nis sufficient for consistent propensity estimation in the position-based model.\nSecond, we propose a new extremum estimator that makes effective use of this\ndata. In an empirical evaluation, we find that the new estimator provides\nsuperior propensity estimates in two real-world systems -- Arxiv Full-text\nSearch and Google Drive Search. Beyond these two points, we find that the\nmethod is robust to a wide range of settings in simulation studies.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 21:26:43 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Agarwal", "Aman", ""], ["Zaitsev", "Ivan", ""], ["Wang", "Xuanhui", ""], ["Li", "Cheng", ""], ["Najork", "Marc", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1812.05168", "submitter": "Luiz Capretz Dr.", "authors": "Tamer Mohamed Abdellatif, Luiz Fernando Capretz", "title": "Searching for Relevant Lessons Learned Using Hybrid Information\n  Retrieval Classifiers: A Case Study in Software Engineering", "comments": null, "journal-ref": "International Workshop on Professional Search (ACM SIGIR - ProfS\n  2018), pp. 12-17, Ann Arbor, Michigan, USA, July 2018", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lessons learned (LL) repository is one of the most valuable sources of\nknowledge for a software organization. It can provide distinctive guidance\nregarding previous working solutions for historical software management\nproblems, or former success stories to be followed. However, the unstructured\nformat of the LL repository makes it difficult to search using general queries,\nwhich are manually inputted by project managers (PMs). For this reason, this\nrepository may often be overlooked despite the valuable information it\nprovides. Since the LL repository targets PMs, the search method should be\ndomain specific rather than generic as in the case of general web searching. In\nprevious work, we provided an automatic information retrieval based LL\nclassifier solution. In our solution, we relied on existing project management\nartifacts in constructing the search query on-the-fly. In this paper, we extend\nour previous work by examining the impact of the hybridization of multiple LL\nclassifiers, from our previous study, on performance. We employ two of the\nhybridization techniques from the literature to construct the hybrid\nclassifiers. An industrial dataset of 212 LL records is used for validation.\nThe results show the superiority of the hybrid classifier over the top\nachieving individual classifier, which reached 25%.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 21:45:33 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Abdellatif", "Tamer Mohamed", ""], ["Capretz", "Luiz Fernando", ""]]}, {"id": "1812.05366", "submitter": "Longxuan Ma", "authors": "Longxuan Ma, Pengfei Wang and Lei Zhang", "title": "Dynamic Feature Generation Network for Answer Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting appropriate features to represent a corpus is an important task\nfor textual mining. Previous attention based work usually enhance feature at\nthe lexical level, which lacks the exploration of feature augmentation at the\nsentence level. In this paper, we exploit a Dynamic Feature Generation Network\n(DFGN) to solve this problem. Specifically, DFGN generates features based on a\nvariety of attention mechanisms and attaches features to sentence\nrepresentation. Then a thresholder is designed to filter the mined features\nautomatically. DFGN extracts the most significant characteristics from datasets\nto keep its practicability and robustness. Experimental results on multiple\nwell-known answer selection datasets show that our proposed approach\nsignificantly outperforms state-of-the-art baselines. We give a detailed\nanalysis of the experiments to illustrate why DFGN provides excellent retrieval\nand interpretative ability.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 11:23:18 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Ma", "Longxuan", ""], ["Wang", "Pengfei", ""], ["Zhang", "Lei", ""]]}, {"id": "1812.05465", "submitter": "Alejandro Baldominos", "authors": "Alejandro Baldominos and David Quintana", "title": "Data-Driven Interaction Review of an Ed-Tech Application", "comments": "14 pages, 2 figures", "journal-ref": "Sensors 2019, 19(8), 1910", "doi": "10.3390/s19081910", "report-no": null, "categories": "cs.CY cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smile and Learn is an Ed-Tech company that runs a smart library with more\nthat 100 applications, games and interactive stories, aimed at children aged\ntwo to 10 and their families. The platform gathers thousands of data points\nfrom the interaction with the system to subsequently offer reports and\nrecommendations. Given the complexity of navigating all the content, the\nlibrary implements a recommender system. The purpose of this paper is to\nevaluate two aspects of such system focused on children: the influence of the\norder of recommendations on user exploratory behavior, and the impact of the\nchoice of the recommendation algorithm on engagement. The assessment, based on\ndata collected between 15 October 2018 and 1 December 2018, required the\nanalysis of the number of clicks performed on the recommendations depending on\ntheir ordering, and an A/B/C testing where two standard recommendation\nalgorithmswere comparedwith a randomrecommendation that served as baseline. The\nresults suggest a direct connection between the order of the recommendation and\nthe interest raised, and the superiority of recommendations based on popularity\nagainst other alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 14:55:57 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 13:43:41 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 15:14:47 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Baldominos", "Alejandro", ""], ["Quintana", "David", ""]]}, {"id": "1812.05731", "submitter": "Keping Bi", "authors": "Keping Bi, Qingyao Ai and W. Bruce Croft", "title": "Revisiting Iterative Relevance Feedback for Document and Passage\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As more and more search traffic comes from mobile phones, intelligent\nassistants, and smart-home devices, new challenges (e.g., limited presentation\nspace) and opportunities come up in information retrieval. Previously, an\neffective technique, relevance feedback (RF), has rarely been used in real\nsearch scenarios due to the overhead of collecting users' relevance judgments.\nHowever, since users tend to interact more with the search results shown on the\nnew interfaces, it becomes feasible to obtain users' assessments on a few\nresults during each interaction. This makes iterative relevance feedback (IRF)\ntechniques look promising today. IRF has not been studied systematically in the\nnew search scenarios and its effectiveness is mostly unknown. In this paper, we\nre-visit IRF and extend it with RF models proposed in recent years. We conduct\nextensive experiments to analyze and compare IRF with the standard top-k RF\nframework on document and passage retrieval. Experimental results show that IRF\nis at least as effective as the standard top-k RF framework for documents and\nmuch more effective for passages. This indicates that IRF for passage retrieval\nhas huge potential.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 23:32:47 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 19:40:39 GMT"}, {"version": "v3", "created": "Sun, 9 Jun 2019 22:18:36 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Bi", "Keping", ""], ["Ai", "Qingyao", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1812.05945", "submitter": "Daniel Omeiza A", "authors": "Daniel Omeiza, Kayode Sakariyah Adewole, Daniel Nkemelu", "title": "EEG-based Communication with a Predictive Text Algorithm", "comments": "Conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several changes occur in the brain in response to voluntary and involuntary\nactivities performed by a person. The ability to retrieve data from the brain\nwithin a time space provides a basis for in-depth analyses that offer insight\non what changes occur in the brain during its decision-making processes. In\nthis work, we present the technical description and software implementation of\nan electroencephalographic (EEG) based communication system. We read EEG data\nin real-time with which we compute the likelihood that a voluntary eye blink\nhas been made by a person and use the decision to trigger buttons on a user\ninterface in order to produce text. Relevant texts are suggested using a\nmodification of the T9 algorithm. Our results indicate that EEG-based\ntechnology can be effectively applied in facilitating speech for people with\nsevere speech and muscular disabilities, providing a foundation for future work\nin the area.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 14:08:35 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 19:55:29 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 14:58:02 GMT"}, {"version": "v4", "created": "Sat, 27 Jun 2020 18:04:30 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Omeiza", "Daniel", ""], ["Adewole", "Kayode Sakariyah", ""], ["Nkemelu", "Daniel", ""]]}, {"id": "1812.06081", "submitter": "Sendong Zhao", "authors": "Sendong Zhao, Ting Liu, Sicheng Zhao, Fei Wang", "title": "A Neural Multi-Task Learning Framework to Jointly Model Medical Named\n  Entity Recognition and Normalization", "comments": "AAAI-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art studies have demonstrated the superiority of joint modelling\nover pipeline implementation for medical named entity recognition and\nnormalization due to the mutual benefits between the two processes. To exploit\nthese benefits in a more sophisticated way, we propose a novel deep neural\nmulti-task learning framework with explicit feedback strategies to jointly\nmodel recognition and normalization. On one hand, our method benefits from the\ngeneral representations of both tasks provided by multi-task learning. On the\nother hand, our method successfully converts hierarchical tasks into a parallel\nmulti-task setting while maintaining the mutual supports between tasks. Both of\nthese aspects improve the model performance. Experimental results demonstrate\nthat our method performs significantly better than state-of-the-art approaches\non two publicly available medical literature datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 18:59:41 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Zhao", "Sendong", ""], ["Liu", "Ting", ""], ["Zhao", "Sicheng", ""], ["Wang", "Fei", ""]]}, {"id": "1812.06082", "submitter": "Fl\\'avio Martins", "authors": "Fl\\'avio Martins, Jo\\~ao Magalh\\~aes, Jamie Callan", "title": "Modeling Temporal Evidence from External Collections", "comments": "To appear in WSDM 2019", "journal-ref": null, "doi": "10.1145/3289600.3290966", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Newsworthy events are broadcast through multiple mediums and prompt the\ncrowds to produce comments on social media. In this paper, we propose to\nleverage on this behavioral dynamics to estimate the most relevant time periods\nfor an event (i.e., query). Recent advances have shown how to improve the\nestimation of the temporal relevance of such topics. In this approach, we build\non two major novelties. First, we mine temporal evidences from hundreds of\nexternal sources into topic-based external collections to improve the\nrobustness of the detection of relevant time periods. Second, we propose a\nformal retrieval model that generalizes the use of the temporal dimension\nacross different aspects of the retrieval process. In particular, we show that\ntemporal evidence of external collections can be used to (i) infer a topic's\ntemporal relevance, (ii) select the query expansion terms, and (iii) re-rank\nthe final results for improved precision. Experiments with TREC Microblog\ncollections show that the proposed time-aware retrieval model makes an\neffective and extensive use of the temporal dimension to improve search results\nover the most recent temporal models. Interestingly, we observe a strong\ncorrelation between precision and the temporal distribution of retrieved and\nrelevant documents.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 02:15:26 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Martins", "Fl\u00e1vio", ""], ["Magalh\u00e3es", "Jo\u00e3o", ""], ["Callan", "Jamie", ""]]}, {"id": "1812.06229", "submitter": "Linh Nguyen", "authors": "Linh Nguyen and Tsukasa Ishigaki", "title": "Domain-to-Domain Translation Model for Recommender System", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently multi-domain recommender systems have received much attention from\nresearchers because they can solve cold-start problem as well as support for\ncross-selling. However, when applying into multi-domain items, although\nalgorithms specifically addressing a single domain have many difficulties in\ncapturing the specific characteristics of each domain, multi-domain algorithms\nhave less opportunity to obtain similar features among domains. Because both\nsimilarities and differences exist among domains, multi-domain models must\ncapture both to achieve good performance. Other studies of multi-domain systems\nmerely transfer knowledge from the source domain to the target domain, so the\nsource domain usually comes from external factors such as the search query or\nsocial network, which is sometimes impossible to obtain. To handle the two\nproblems, we propose a model that can extract both homogeneous and divergent\nfeatures among domains and extract data in a domain can support for other\ndomain equally: a so-called Domain-to-Domain Translation Model (D2D-TM). It is\nbased on generative adversarial networks (GANs), Variational Autoencoders\n(VAEs), and Cycle-Consistency (CC) for weight-sharing. We use the user\ninteraction history of each domain as input and extract latent features through\na VAE-GAN-CC network. Experiments underscore the effectiveness of the proposed\nsystem over state-of-the-art methods by a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 03:42:27 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Nguyen", "Linh", ""], ["Ishigaki", "Tsukasa", ""]]}, {"id": "1812.06443", "submitter": "Bhaskar Gautam", "authors": "Bhaskar Gautam, Annappa Basava, Abhishek Singh and Amit Agrawal", "title": "\"When and Where?\": Behavior Dominant Location Forecasting with\n  Micro-blog Streams", "comments": "Accepted as a full paper in the 2nd International Workshop on Social\n  Computing co-located with ICDM, 2018 Singapore", "journal-ref": null, "doi": "10.1109/ICDMW.2018.00169", "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of smartphones and wearable devices has increased the\navailability of large amounts of geospatial streams to provide significant\nautomated discovery of knowledge in pervasive environments, but most prominent\ninformation related to altering interests have not yet adequately capitalized.\nIn this paper, we provide a novel algorithm to exploit the dynamic fluctuations\nin user's point-of-interest while forecasting the future place of visit with\nfine granularity. Our proposed algorithm is based on the dynamic formation of\ncollective personality communities using different languages, opinions,\ngeographical and temporal distributions for finding out optimized equivalent\ncontent. We performed extensive empirical experiments involving, real-time\nstreams derived from 0.6 million stream tuples of micro-blog comprising 1945\nsocial person fusion with graph algorithm and feed-forward neural network model\nas a predictive classification model. Lastly, The framework achieves 62.10%\nmean average precision on 1,20,000 embeddings on unlabeled users and\nsurprisingly 85.92% increment on the state-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 11:00:37 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Gautam", "Bhaskar", ""], ["Basava", "Annappa", ""], ["Singh", "Abhishek", ""], ["Agrawal", "Amit", ""]]}, {"id": "1812.06610", "submitter": "Tianyu Li", "authors": "Tianyu Li, Yukun Ma, Jiu Xu, Bjorn Stenger, Chen Liu, Yu Hirate", "title": "Deep Heterogeneous Autoencoders for Collaborative Filtering", "comments": "Proceedings of the IEEE International Conference on Data Mining, pp.\n  1164-1169, Singapore, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper leverages heterogeneous auxiliary information to address the data\nsparsity problem of recommender systems. We propose a model that learns a\nshared feature space from heterogeneous data, such as item descriptions,\nproduct tags and online purchase history, to obtain better predictions. Our\nmodel consists of autoencoders, not only for numerical and categorical data,\nbut also for sequential data, which enables capturing user tastes, item\ncharacteristics and the recent dynamics of user preference. We learn the\nautoencoder architecture for each data source independently in order to better\nmodel their statistical properties. Our evaluation on two MovieLens datasets\nand an e-commerce dataset shows that mean average precision and recall improve\nover state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 04:11:18 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Li", "Tianyu", ""], ["Ma", "Yukun", ""], ["Xu", "Jiu", ""], ["Stenger", "Bjorn", ""], ["Liu", "Chen", ""], ["Hirate", "Yu", ""]]}, {"id": "1812.06974", "submitter": "Jieli Zhou", "authors": "Jieli Zhou, Yuntao Zhou, Yi Xu", "title": "Analogy Search Engine: Finding Analogies in Cross-Domain Research Papers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the rapid proliferation of research publications in the\nfield of Artificial Intelligence, it is becoming increasingly difficult for\nresearchers to effectively keep up with all the latest research in one's own\ndomains. However, history has shown that scientific breakthroughs often come\nfrom collaborations of researchers from different domains. Traditional search\nalgorithms like Lexical search, which look for literal matches or synonyms and\nvariants of the query words, are not effective for discovering cross-domain\nresearch papers and meeting the needs of researchers in this age of information\noverflow. In this paper, we developed and tested an innovative semantic search\nengine, Analogy Search Engine (ASE), for 2000 AI research paper abstracts\nacross domains like Language Technologies, Robotics, Machine Learning,\nComputational Biology, Human Computer Interactions, etc. ASE combines recent\ntheories and methods from Computational Analogy and Natural Language Processing\nto go beyond keyword-based lexical search and discover the deeper analogical\nrelationships among research paper abstracts. We experimentally show that ASE\nis capable of finding more interesting and useful research papers than baseline\nelasticsearch. Furthermore, we believe that the methods used in ASE go beyond\nacademic paper and will benefit many other document search tasks.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 16:15:13 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Zhou", "Jieli", ""], ["Zhou", "Yuntao", ""], ["Xu", "Yi", ""]]}, {"id": "1812.07081", "submitter": "Mohammad Aliannejadi", "authors": "Mohammad Aliannejadi, Morgan Harvey, Luca Costa, Matthew Pointon,\n  Fabio Crestani", "title": "Understanding Mobile Search Task Relevance and User Behaviour in Context", "comments": "To appear in CHIIR 2019 in Glasgow, UK", "journal-ref": null, "doi": "10.1145/3295750.3298923", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improvements in mobile technologies have led to a dramatic change in how and\nwhen people access and use information, and is having a profound impact on how\nusers address their daily information needs. Smart phones are rapidly becoming\nour main method of accessing information and are frequently used to perform\n`on-the-go' search tasks. As research into information retrieval continues to\nevolve, evaluating search behaviour in context is relatively new. Previous\nresearch has studied the effects of context through either self-reported diary\nstudies or quantitative log analysis; however, neither approach is able to\naccurately capture context of use at the time of searching. In this study, we\naim to gain a better understanding of task relevance and search behaviour via a\ntask-based user study (n=31) employing a bespoke Android app. The app allowed\nus to accurately capture the user's context when completing tasks at different\ntimes of the day over the period of a week. Through analysis of the collected\ndata, we gain a better understanding of how using smart phones on the go\nimpacts search behaviour, search performance and task relevance and whether or\nnot the actual context is an important factor.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 22:37:34 GMT"}, {"version": "v2", "created": "Sun, 13 Jan 2019 17:45:37 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Aliannejadi", "Mohammad", ""], ["Harvey", "Morgan", ""], ["Costa", "Luca", ""], ["Pointon", "Matthew", ""], ["Crestani", "Fabio", ""]]}, {"id": "1812.07127", "submitter": "Xiangyu Zhao", "authors": "Xiangyu Zhao and Long Xia and Jiliang Tang and Dawei Yin", "title": "Deep reinforcement learning for search, recommendation, and online\n  advertising: a survey", "comments": null, "journal-ref": null, "doi": "10.1145/3320496.3320500", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search, recommendation, and online advertising are the three most important\ninformation-providing mechanisms on the web. These information seeking\ntechniques, satisfying users' information needs by suggesting users\npersonalized objects (information or services) at the appropriate time and\nplace, play a crucial role in mitigating the information overload problem. With\nrecent great advances in deep reinforcement learning (DRL), there have been\nincreasing interests in developing DRL based information seeking techniques.\nThese DRL based techniques have two key advantages -- (1) they are able to\ncontinuously update information seeking strategies according to users'\nreal-time feedback, and (2) they can maximize the expected cumulative long-term\nreward from users where reward has different definitions according to\ninformation seeking applications such as click-through rate, revenue, user\nsatisfaction and engagement. In this paper, we give an overview of deep\nreinforcement learning for search, recommendation, and online advertising from\nmethodologies to applications, review representative algorithms, and discuss\nsome appealing research directions.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 01:32:48 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 16:31:19 GMT"}, {"version": "v3", "created": "Fri, 15 Feb 2019 18:40:48 GMT"}, {"version": "v4", "created": "Tue, 30 Apr 2019 20:04:49 GMT"}, {"version": "v5", "created": "Fri, 17 Jan 2020 03:18:39 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Zhao", "Xiangyu", ""], ["Xia", "Long", ""], ["Tang", "Jiliang", ""], ["Yin", "Dawei", ""]]}, {"id": "1812.07617", "submitter": "Raymond Li", "authors": "Raymond Li, Samira Kahou, Hannes Schulz, Vincent Michalski, Laurent\n  Charlin, Chris Pal", "title": "Towards Deep Conversational Recommendations", "comments": "17 pages, 5 figures, Accepted at 32nd Conference on Neural\n  Information Processing Systems (NeurIPS 2018), Montr\\'eal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been growing interest in using neural networks and deep learning\ntechniques to create dialogue systems. Conversational recommendation is an\ninteresting setting for the scientific exploration of dialogue with natural\nlanguage as the associated discourse involves goal-driven dialogue that often\ntransforms naturally into more free-form chat. This paper provides two\ncontributions. First, until now there has been no publicly available\nlarge-scale dataset consisting of real-world dialogues centered around\nrecommendations. To address this issue and to facilitate our exploration here,\nwe have collected ReDial, a dataset consisting of over 10,000 conversations\ncentered around the theme of providing movie recommendations. We make this data\navailable to the community for further research. Second, we use this dataset to\nexplore multiple facets of conversational recommendations. In particular we\nexplore new neural architectures, mechanisms, and methods suitable for\ncomposing conversational recommendation systems. Our dataset allows us to\nsystematically probe model sub-components addressing different parts of the\noverall problem domain ranging from: sentiment analysis and cold-start\nrecommendation generation to detailed aspects of how natural language is used\nin this setting in the real world. We combine such sub-components into a\nfull-blown dialogue system and examine its behavior.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 19:34:32 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 18:54:59 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Li", "Raymond", ""], ["Kahou", "Samira", ""], ["Schulz", "Hannes", ""], ["Michalski", "Vincent", ""], ["Charlin", "Laurent", ""], ["Pal", "Chris", ""]]}, {"id": "1812.07640", "submitter": "Alexander Veretennikov Borisovich", "authors": "Alexander B. Veretennikov", "title": "Proximity Full-Text Search by Means of Additional Indexes with\n  Multi-component Keys: In Pursuit of Optimal Performance", "comments": "Revised paper of \"Veretennikov A.B. Proximity full-text search with a\n  response time guarantee by means of additional indexes with multi-component\n  keys\", Selected Papers of the XX International Conference on Data Analytics\n  and Management in Data Intensive Domains (DAMDID/RCDL 2018), Moscow, Russia,\n  October 9-12, 2018, http://ceur-ws.org/Vol-2277,\n  http://ceur-ws.org/Vol-2277/paper23.pdf", "journal-ref": "Data Analytics and Management in Data Intensive Domains.\n  DAMDID/RCDL 2018. Communications in Computer and Information Science, vol\n  1003. Springer, Cham", "doi": "10.1007/978-3-030-23584-0_7", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Full-text search engines are important tools for information retrieval. In a\nproximity full-text search, a document is relevant if it contains query terms\nnear each other, especially if the query terms are frequently occurring words.\nFor each word in a text, we use additional indexes to store information about\nnearby words that are at distances from the given word of less than or equal to\nthe MaxDistance parameter. We showed that additional indexes with\nthree-component keys can be used to improve the average query execution time by\nup to 94.7 times if the queries consist of high-frequency occurring words. In\nthis paper, we present a new search algorithm with even more performance gains.\nWe consider several strategies for selecting multi-component key indexes for a\nspecific query and compare these strategies with the optimal strategy. We also\npresent the results of search experiments, which show that three-component key\nindexes enable much faster searches in comparison with two-component key\nindexes.\n  This is a pre-print of a contribution \"Veretennikov A.B. (2019) Proximity\nFull-Text Search by Means of Additional Indexes with Multi-component Keys: In\nPursuit of Optimal Performance.\" published in \"Manolopoulos Y., Stupnikov S.\n(eds) Data Analytics and Management in Data Intensive Domains. DAMDID/RCDL\n2018. Communications in Computer and Information Science, vol 1003\" published\nby Springer, Cham. This book constitutes the refereed proceedings of the 20th\nInternational Conference on Data Analytics and Management in Data Intensive\nDomains, DAMDID/RCDL 2018, held in Moscow, Russia, in October 2018. The 9\nrevised full papers presented together with three invited papers were carefully\nreviewed and selected from 54 submissions. The final authenticated version is\navailable online at https://doi.org/10.1007/978-3-030-23584-0_7.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 20:57:15 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 18:33:56 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Veretennikov", "Alexander B.", ""]]}, {"id": "1812.07805", "submitter": "Zheng Chen", "authors": "Zheng Chen, Yong Zhang, Yue Shang, Xiaohua Hu", "title": "Unifying Topic, Sentiment & Preference in an HDP-Based Rating Regression\n  Model for Online Reviews", "comments": null, "journal-ref": "Asian Conference on Machine Learning. 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new HDP based online review rating regression model\nnamed Topic-Sentiment-Preference Regression Analysis (TSPRA). TSPRA combines\ntopics (i.e. product aspects), word sentiment and user preference as regression\nfactors, and is able to perform topic clustering, review rating prediction,\nsentiment analysis and what we invent as \"critical aspect\" analysis altogether\nin one framework. TSPRA extends sentiment approaches by integrating the key\nconcept \"user preference\" in collaborative filtering (CF) models into\nconsideration, while it is distinct from current CF models by decoupling \"user\npreference\" and \"sentiment\" as independent factors. Our experiments conducted\non 22 Amazon datasets show overwhelming better performance in rating\npredication against a state-of-art model FLAME (2015) in terms of error,\nPearson's Correlation and number of inverted pairs. For sentiment analysis, we\ncompare the derived word sentiments against a public sentiment resource\nSenticNet3 and our sentiment estimations clearly make more sense in the context\nof online reviews. Last, as a result of the de-correlation of \"user preference\"\nfrom \"sentiment\", TSPRA is able to evaluate a new concept \"critical aspects\",\ndefined as the product aspects seriously concerned by users but negatively\ncommented in reviews. Improvement to such \"critical aspects\" could be most\neffective to enhance user experience.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 08:33:31 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Chen", "Zheng", ""], ["Zhang", "Yong", ""], ["Shang", "Yue", ""], ["Hu", "Xiaohua", ""]]}, {"id": "1812.08046", "submitter": "Maral Dadvar", "authors": "Maral Dadvar, Kai Eckert", "title": "Cyberbullying Detection in Social Networks Using Deep Learning Based\n  Models; A Reproducibility Study", "comments": "13 Pages, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyberbullying is a disturbing online misbehaviour with troubling\nconsequences. It appears in different forms, and in most of the social\nnetworks, it is in textual format. Automatic detection of such incidents\nrequires intelligent systems. Most of the existing studies have approached this\nproblem with conventional machine learning models and the majority of the\ndeveloped models in these studies are adaptable to a single social network at a\ntime. In recent studies, deep learning based models have found their way in the\ndetection of cyberbullying incidents, claiming that they can overcome the\nlimitations of the conventional models, and improve the detection performance.\nIn this paper, we investigate the findings of a recent literature in this\nregard. We successfully reproduced the findings of this literature and\nvalidated their findings using the same datasets, namely Wikipedia, Twitter,\nand Formspring, used by the authors. Then we expanded our work by applying the\ndeveloped methods on a new YouTube dataset (~54k posts by ~4k users) and\ninvestigated the performance of the models in new social media platforms. We\nalso transferred and evaluated the performance of the models trained on one\nplatform to another platform. Our findings show that the deep learning based\nmodels outperform the machine learning models previously applied to the same\nYouTube dataset. We believe that the deep learning based models can also\nbenefit from integrating other sources of information and looking into the\nimpact of profile information of the users in social networks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 16:02:08 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Dadvar", "Maral", ""], ["Eckert", "Kai", ""]]}, {"id": "1812.08092", "submitter": "Martin Gerlach", "authors": "Martin Gerlach, Francesc Font-Clos", "title": "A standardized Project Gutenberg corpus for statistical analysis of\n  natural language and quantitative linguistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Project Gutenberg (PG) as a text corpus has been extremely popular\nin statistical analysis of language for more than 25 years. However, in\ncontrast to other major linguistic datasets of similar importance, no\nconsensual full version of PG exists to date. In fact, most PG studies so far\neither consider only a small number of manually selected books, leading to\npotential biased subsets, or employ vastly different pre-processing strategies\n(often specified in insufficient details), raising concerns regarding the\nreproducibility of published results. In order to address these shortcomings,\nhere we present the Standardized Project Gutenberg Corpus (SPGC), an open\nscience approach to a curated version of the complete PG data containing more\nthan 50,000 books and more than $3 \\times 10^9$ word-tokens. Using different\nsources of annotated metadata, we not only provide a broad characterization of\nthe content of PG, but also show different examples highlighting the potential\nof SPGC for investigating language variability across time, subjects, and\nauthors. We publish our methodology in detail, the code to download and process\nthe data, as well as the obtained corpus itself on 3 different levels of\ngranularity (raw text, timeseries of word tokens, and counts of words). In this\nway, we provide a reproducible, pre-processed, full-size version of Project\nGutenberg as a new scientific resource for corpus linguistics, natural language\nprocessing, and information retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 17:10:14 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Gerlach", "Martin", ""], ["Font-Clos", "Francesc", ""]]}, {"id": "1812.08254", "submitter": "Babak Loni", "authors": "Babak Loni, Martha Larson, Alan Hanjalic", "title": "Factorization Machines for Data with Implicit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose FM-Pair, an adaptation of Factorization Machines\nwith a pairwise loss function, making them effective for datasets with implicit\nfeedback. The optimization model in FM-Pair is based on the BPR (Bayesian\nPersonalized Ranking) criterion, which is a well-established pairwise\noptimization model. FM-Pair retains the advantages of FMs on generality,\nexpressiveness and performance and yet it can be used for datasets with\nimplicit feedback. We also propose how to apply FM-Pair effectively on two\ncollaborative filtering problems, namely, context-aware recommendation and\ncross-domain collaborative filtering. By performing experiments on different\ndatasets with explicit or implicit feedback we empirically show that in most of\nthe tested datasets, FM-Pair beats state-of-the-art learning-to-rank methods\nsuch as BPR-MF (BPR with Matrix Factorization model). We also show that FM-Pair\nis significantly more effective for ranking, compared to the standard FMs\nmodel. Moreover, we show that FM-Pair can utilize context or cross-domain\ninformation effectively as the accuracy of recommendations would always improve\nwith the right auxiliary features. Finally we show that FM-Pair has a linear\ntime complexity and scales linearly by exploiting additional features.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 21:32:01 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Loni", "Babak", ""], ["Larson", "Martha", ""], ["Hanjalic", "Alan", ""]]}, {"id": "1812.08304", "submitter": "Hamed Jelodar", "authors": "Hamed Jelodar, Yongli Wang, Mahdi Rabbani, Ru-xin Zhao, Seyedvalyallah\n  Ayobi, Peng Hu and Isma Masood", "title": "Recommendation System based on Semantic Scholar Mining and Topic\n  modeling: A behavioral analysis of researchers from six conferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems have an important place to help online users in the\ninternet society. Recommendation Systems in computer science are of very\npractical use these days in various aspects of the Internet portals, such as\nsocial networks, and library websites. There are several approaches to\nimplement recommendation systems, Latent Dirichlet Allocation (LDA) is one the\npopular techniques in Topic Modeling. Recently, researchers have proposed many\napproaches based on Recommendation Systems and LDA. According to importance of\nthe subject, in this paper we discover the trends of the topics and find\nrelationship between LDA topics and Scholar-Context-documents. In fact, We\napply probabilistic topic modeling based on Gibbs sampling algorithms for a\nsemantic mining from six conference publications in computer science from DBLP\ndataset. According to our experimental results, our semantic framework can be\neffective to help organizations to better organize these conferences and cover\nfuture research topics.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 01:07:04 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Jelodar", "Hamed", ""], ["Wang", "Yongli", ""], ["Rabbani", "Mahdi", ""], ["Zhao", "Ru-xin", ""], ["Ayobi", "Seyedvalyallah", ""], ["Hu", "Peng", ""], ["Masood", "Isma", ""]]}, {"id": "1812.08330", "submitter": "Supun Abeysinghe", "authors": "Supun Abeysinghe, Isura Manchanayake, Chamod Samarajeewa, Prabod\n  Rathnayaka, Malaka J. Walpola, Rashmika Nawaratne, Tharindu Bandaragoda,\n  Damminda Alahakoon", "title": "Enhancing Decision Making Capacity in Tourism Domain Using Social Media\n  Analytics", "comments": "To Appear in Proceedings of International Conference on Advances in\n  ICT for Emerging Regions, Colombo, LK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media has gained an immense popularity over the last decade. People\ntend to express opinions about their daily encounters on social media freely.\nThese daily encounters include the places they traveled, hotels or restaurants\nthey have tried and aspects related to tourism in general. Since people usually\nexpress their true experiences on social media, the expressed opinions contain\nvaluable information that can be used to generate business value and aid in\ndecision-making processes. Due to the large volume of data, it is not a\nfeasible task to manually go through each and every item and extract the\ninformation. Hence, we propose a social media analytics platform which has the\ncapability to identify discussion pathways and aspects with their corresponding\nsentiment and deeper emotions using machine learning techniques and a\nvisualization tool which shows the extracted insights in a comprehensible and\nconcise manner. Identified topic pathways and aspects will give a decision\nmaker some insight into what are the most discussed topics about the entity\nwhereas associated sentiments and emotions will help to identify the feedback.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 17:19:50 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Abeysinghe", "Supun", ""], ["Manchanayake", "Isura", ""], ["Samarajeewa", "Chamod", ""], ["Rathnayaka", "Prabod", ""], ["Walpola", "Malaka J.", ""], ["Nawaratne", "Rashmika", ""], ["Bandaragoda", "Tharindu", ""], ["Alahakoon", "Damminda", ""]]}, {"id": "1812.08585", "submitter": "Malte Bonart", "authors": "Malte Bonart and Philipp Schaer", "title": "Intertemporal Connections Between Query Suggestions and Search Engine\n  Results for Politics Related Queries", "comments": "Submitted and accepted for the dataset challenge at the European\n  Symposium Series on Societal Challenges in Computational Social Science\n  (EuroCSS) 2018. For the associated data set, see\n  http://doi.org/10.5281/zenodo.1494858. Updated version contains\n  acknowledgments section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short paper deals with the combination and comparison of two data\nsources: Search engine results and query suggestions for 16 terms related to\npolitical candidates and parties. The data was collected before the federal\nelection in Germany in September 2017 for a period of two months. The rank\nbiased overlap (RBO) statistic is used to measure the similarity of the\ntop-weighted rankings. For each search term and for both the search results and\nquery auto-completions we study the stability of the rankings over time.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 14:16:35 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 12:19:38 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Bonart", "Malte", ""], ["Schaer", "Philipp", ""]]}, {"id": "1812.08870", "submitter": "Keping Bi", "authors": "Keping Bi, Qingyao Ai and W. Bruce Croft", "title": "Iterative Relevance Feedback for Answer Passage Retrieval with\n  Passage-level Semantic Match", "comments": null, "journal-ref": "41st European Conference on IR Research, ECIR 2019", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relevance feedback techniques assume that users provide relevance judgments\nfor the top k (usually 10) documents and then re-rank using a new query model\nbased on those judgments. Even though this is effective, there has been little\nresearch recently on this topic because requiring users to provide substantial\nfeedback on a result list is impractical in a typical web search scenario. In\nnew environments such as voice-based search with smart home devices, however,\nfeedback about result quality can potentially be obtained during users'\ninteractions with the system. Since there are severe limitations on the length\nand number of results that can be presented in a single interaction in this\nenvironment, the focus should move from browsing result lists to iterative\nretrieval and from retrieving documents to retrieving answers. In this paper,\nwe study iterative relevance feedback techniques with a focus on retrieving\nanswer passages. We first show that iterative feedback is more effective than\nthe top-k approach for answer retrieval. Then we propose an iterative feedback\nmodel based on passage-level semantic match and show that it can produce\nsignificant improvements compared to both word-based iterative feedback models\nand those based on term-level semantic similarity.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 22:27:25 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Bi", "Keping", ""], ["Ai", "Qingyao", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1812.09221", "submitter": "Milad Alshomary", "authors": "Milad Alshomary, Michael V\\\"olske, Tristan Licht, Henning Wachsmuth,\n  Benno Stein, Matthias Hagen, Martin Potthast", "title": "Wikipedia Text Reuse: Within and Without", "comments": "accepted at ECIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study text reuse related to Wikipedia at scale by compiling the first\ncorpus of text reuse cases within Wikipedia as well as without (i.e., reuse of\nWikipedia text in a sample of the Common Crawl). To discover reuse beyond\nverbatim copy and paste, we employ state-of-the-art text reuse detection\ntechnology, scaling it for the first time to process the entire Wikipedia as\npart of a distributed retrieval pipeline. We further report on a pilot analysis\nof the 100 million reuse cases inside, and the 1.6 million reuse cases outside\nWikipedia that we discovered. Text reuse inside Wikipedia gives rise to new\ntasks such as article template induction, fixing quality flaws due to\ninconsistencies arising from asynchronous editing of reused passages, or\ncomplementing Wikipedia's ontology. Text reuse outside Wikipedia yields a\ntangible metric for the emerging field of quantifying Wikipedia's influence on\nthe web. To foster future research into these tasks, and for reproducibility's\nsake, the Wikipedia text reuse corpus and the retrieval pipeline are made\nfreely available.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 16:05:04 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Alshomary", "Milad", ""], ["V\u00f6lske", "Michael", ""], ["Licht", "Tristan", ""], ["Wachsmuth", "Henning", ""], ["Stein", "Benno", ""], ["Hagen", "Matthias", ""], ["Potthast", "Martin", ""]]}, {"id": "1812.09233", "submitter": "Shantanu Sharma", "authors": "Sharad Mehrotra, Shantanu Sharma, Jeffrey D. Ullman, and Anurag Mishra", "title": "Partitioned Data Security on Outsourced Sensitive and Non-sensitive Data", "comments": "Accepted in IEEE International Conference on Data Engineering (ICDE),\n  2019. arXiv admin note: text overlap with arXiv:1812.01741", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite extensive research on cryptography, secure and efficient query\nprocessing over outsourced data remains an open challenge. This paper continues\nalong the emerging trend in secure data processing that recognizes that the\nentire dataset may not be sensitive, and hence, non-sensitivity of data can be\nexploited to overcome limitations of existing encryption-based approaches. We\npropose a new secure approach, entitled query binning (QB) that allows\nnon-sensitive parts of the data to be outsourced in clear-text while\nguaranteeing that no information is leaked by the joint processing of\nnon-sensitive data (in clear-text) and sensitive data (in encrypted form). QB\nmaps a query to a set of queries over the sensitive and non-sensitive data in a\nway that no leakage will occur due to the joint processing over sensitive and\nnon-sensitive data. Interestingly, in addition to improve performance, we show\nthat QB actually strengthens the security of the underlying cryptographic\ntechnique by preventing size, frequency-count, and workload-skew attacks.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 03:06:17 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Mehrotra", "Sharad", ""], ["Sharma", "Shantanu", ""], ["Ullman", "Jeffrey D.", ""], ["Mishra", "Anurag", ""]]}, {"id": "1812.09338", "submitter": "Grigor Aslanyan", "authors": "Grigor Aslanyan, Utkarsh Porwal", "title": "Position Bias Estimation for Unbiased Learning-to-Rank in eCommerce\n  Search", "comments": "10 pages, 3 figures", "journal-ref": "SPIRE 2019. Lecture Notes in Computer Science, vol 11811.\n  Springer, Cham", "doi": "10.1007/978-3-030-32686-9_4", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Unbiased Learning-to-Rank framework has been recently proposed as a\ngeneral approach to systematically remove biases, such as position bias, from\nlearning-to-rank models. The method takes two steps - estimating click\npropensities and using them to train unbiased models. Most common methods\nproposed in the literature for estimating propensities involve some degree of\nintervention in the live search engine. An alternative approach proposed\nrecently uses an Expectation Maximization (EM) algorithm to estimate\npropensities by using ranking features for estimating relevances. In this work\nwe propose a novel method to directly estimate propensities which does not use\nany intervention in live search or rely on modeling relevance. Rather, we take\nadvantage of the fact that the same query-document pair may naturally change\nranks over time. This typically occurs for eCommerce search because of change\nof popularity of items over time, existence of time dependent ranking features,\nor addition or removal of items to the index (an item getting sold or a new\nitem being listed). However, our method is general and can be applied to any\nsearch engine for which the rank of the same document may naturally change over\ntime for the same query. We derive a simple likelihood function that depends on\npropensities only, and by maximizing the likelihood we are able to get\nestimates of the propensities. We apply this method to eBay search data to\nestimate click propensities for web and mobile search and compare these with\nestimates using the EM method. We also use simulated data to show that the\nmethod gives reliable estimates of the \"true\" simulated propensities. Finally,\nwe train an unbiased learning-to-rank model for eBay search using the estimated\npropensities and show that it outperforms both baselines - one without position\nbias correction and one with position bias correction using the EM method.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 19:05:15 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 22:13:30 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 17:53:19 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Aslanyan", "Grigor", ""], ["Porwal", "Utkarsh", ""]]}, {"id": "1812.09380", "submitter": "Radin Hamidi Rad", "authors": "Maliheh Goliforoushani, Radin Hamidi Rad, Maryam Amir Haeri", "title": "A Fuzzy Community-Based Recommender System Using PageRank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems are widely used by different user service providers\nspecially those who have interactions with the large community of users. This\npaper introduces a recommender system based on community detection. The\nrecommendation is provided using the local and global similarities between\nusers. The local information is obtained from communities, and the global ones\nare based on the ratings. Here, a new fuzzy community detection using the\npersonalized PageRank metaphor is introduced. The fuzzy membership values of\nthe users to the communities are utilized to define a similarity measure. The\nmethod is evaluated by using two well-known datasets: MovieLens and FilmTrust.\nThe results show that our method outperforms recent recommender systems.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 21:35:44 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Goliforoushani", "Maliheh", ""], ["Rad", "Radin Hamidi", ""], ["Haeri", "Maryam Amir", ""]]}, {"id": "1812.09541", "submitter": "Soumyabrata Dev", "authors": "Murhaf Hossari, Soumyabrata Dev, John D. Kelleher", "title": "TEST: A Terminology Extraction System for Technology Related Terms", "comments": "Published in 11th International Conference on Computer and Automation\n  Engineering (ICCAE 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking developments in the highly dynamic data-technology landscape are\nvital to keeping up with novel technologies and tools, in the various areas of\nArtificial Intelligence (AI). However, It is difficult to keep track of all the\nrelevant technology keywords. In this paper, we propose a novel system that\naddresses this problem. This tool is used to automatically detect the existence\nof new technologies and tools in text, and extract terms used to describe these\nnew technologies. The extracted new terms can be logged as new AI technologies\nas they are found on-the-fly in the web. It can be subsequently classified into\nthe relevant semantic labels and AI domains. Our proposed tool is based on a\ntwo-stage cascading model -- the first stage classifies if the sentence\ncontains a technology term or not; and the second stage identifies the\ntechnology keyword in the sentence. We obtain a competitive accuracy for both\ntasks of sentence classification and text identification.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 15:07:32 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 14:23:12 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Hossari", "Murhaf", ""], ["Dev", "Soumyabrata", ""], ["Kelleher", "John D.", ""]]}, {"id": "1812.09693", "submitter": "Diganta Misra", "authors": "Diganta Misra and Vanshika Arora", "title": "Image Processing on IOPA Radiographs: A comprehensive case study on\n  Apical Periodontitis", "comments": "15 pages, 42 figures and Submitted at ICIAP 2019: 21st International\n  Conference on Image Analysis and Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advancements in Image Processing Techniques and development\nof new robust computer vision algorithms, new areas of research within Medical\nDiagnosis and Biomedical Engineering are picking up pace. This paper provides a\ncomprehensive in-depth case study of Image Processing, Feature Extraction and\nAnalysis of Apical Periodontitis diagnostic cases in IOPA (Intra Oral\nPeri-Apical) Radiographs, a common case in oral diagnostic pipeline. This paper\nprovides a detailed analytical approach towards improving the diagnostic\nprocedure with improved and faster results with higher accuracy targeting to\neliminate True Negative and False Positive cases.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 11:38:35 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 14:02:33 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Misra", "Diganta", ""], ["Arora", "Vanshika", ""]]}, {"id": "1812.09702", "submitter": "Sparsha Mishra", "authors": "Diganta Misra, Sparsha Mishra and Bhargav Appasani", "title": "Advanced Image Processing for Astronomical Images", "comments": "7 pages, 13 figures, accepted at IEEE International Conference on\n  Electrical, Communication, Electronics, Instrumentation and Computing\n  (ICECEIC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image Processing in Astronomy is a major field of research and involves a lot\nof techniques pertaining to improve analyzing the properties of the celestial\nobjects or obtaining preliminary inference from the image data. In this paper,\nwe provide a comprehensive case study of advanced image processing techniques\napplied to Astronomical Galaxy Images for improved analysis, accurate\ninferences and faster analysis.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 13:18:26 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Misra", "Diganta", ""], ["Mishra", "Sparsha", ""], ["Appasani", "Bhargav", ""]]}, {"id": "1812.10021", "submitter": "Xun Yang", "authors": "Xun Yang, Yunshan Ma, Lizi Liao, Meng Wang, Tat-Seng Chua", "title": "TransNFCM: Translation-Based Neural Fashion Compatibility Modeling", "comments": "Accepted in AAAI 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying mix-and-match relationships between fashion items is an urgent\ntask in a fashion e-commerce recommender system. It will significantly enhance\nuser experience and satisfaction. However, due to the challenges of inferring\nthe rich yet complicated set of compatibility patterns in a large e-commerce\ncorpus of fashion items, this task is still underexplored. Inspired by the\nrecent advances in multi-relational knowledge representation learning and deep\nneural networks, this paper proposes a novel Translation-based Neural Fashion\nCompatibility Modeling (TransNFCM) framework, which jointly optimizes fashion\nitem embeddings and category-specific complementary relations in a unified\nspace via an end-to-end learning manner. TransNFCM places items in a unified\nembedding space where a category-specific relation (category-comp-category) is\nmodeled as a vector translation operating on the embeddings of compatible items\nfrom the corresponding categories. By this way, we not only capture the\nspecific notion of compatibility conditioned on a specific pair of\ncomplementary categories, but also preserve the global notion of compatibility.\nWe also design a deep fashion item encoder which exploits the complementary\ncharacteristic of visual and textual features to represent the fashion\nproducts. To the best of our knowledge, this is the first work that uses\ncategory-specific complementary relations to model the category-aware\ncompatibility between items in a translation-based embedding space. Extensive\nexperiments demonstrate the effectiveness of TransNFCM over the\nstate-of-the-arts on two real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 03:55:14 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Yang", "Xun", ""], ["Ma", "Yunshan", ""], ["Liao", "Lizi", ""], ["Wang", "Meng", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1812.10119", "submitter": "Salah Zaiem", "authors": "Salah Zaiem and Fatiha Sadat", "title": "Sequence to Sequence Learning for Query Expansion", "comments": "8 pages, 2 figures, AAAI-19 Student Abstract and Poster Program", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using sequence to sequence algorithms for query expansion has not been\nexplored yet in Information Retrieval literature nor in Question-Answering's.\nWe tried to fill this gap in the literature with a custom Query Expansion\nengine trained and tested on open datasets. Starting from open datasets, we\nbuilt a Query Expansion training set using sentence-embeddings-based Keyword\nExtraction. We therefore assessed the ability of the Sequence to Sequence\nneural networks to capture expanding relations in the words embeddings' space.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 15:24:04 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Zaiem", "Salah", ""], ["Sadat", "Fatiha", ""]]}, {"id": "1812.10233", "submitter": "Yangbin Chen", "authors": "Yangbin Chen, Tom Ko, Lifeng Shang, Xiao Chen, Xin Jiang, Qing Li", "title": "An Investigation of Few-Shot Learning in Spoken Term Classification", "comments": "Accepted by INTERSPEECH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the feasibility of applying few-shot learning\nalgorithms to a speech task. We formulate a user-defined scenario of spoken\nterm classification as a few-shot learning problem. In most few-shot learning\nstudies, it is assumed that all the N classes are new in a N-way problem. We\nsuggest that this assumption can be relaxed and define a N+M-way problem where\nN and M are the number of new classes and fixed classes respectively. We\npropose a modification to the Model-Agnostic Meta-Learning (MAML) algorithm to\nsolve the problem. Experiments on the Google Speech Commands dataset show that\nour approach outperforms the conventional supervised learning approach and the\noriginal MAML.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 05:43:23 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 01:18:29 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 04:03:40 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Chen", "Yangbin", ""], ["Ko", "Tom", ""], ["Shang", "Lifeng", ""], ["Chen", "Xiao", ""], ["Jiang", "Xin", ""], ["Li", "Qing", ""]]}, {"id": "1812.10315", "submitter": "Amit Kirschenbaum", "authors": "Milan Dojchinovski and Julio Hernandez and Markus Ackermann and Amit\n  Kirschenbaum and Sebastian Hellmann", "title": "DBpedia NIF: Open, Large-Scale and Multilingual Knowledge Extraction\n  Corpus", "comments": "15 pages, 1 figure, 4 tables, 1 listing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, the DBpedia community has put significant amount of\neffort on developing technical infrastructure and methods for efficient\nextraction of structured information from Wikipedia. These efforts have been\nprimarily focused on harvesting, refinement and publishing semi-structured\ninformation found in Wikipedia articles, such as information from infoboxes,\ncategorization information, images, wikilinks and citations. Nevertheless,\nstill vast amount of valuable information is contained in the unstructured\nWikipedia article texts. In this paper, we present DBpedia NIF - a large-scale\nand multilingual knowledge extraction corpus. The aim of the dataset is\ntwo-fold: to dramatically broaden and deepen the amount of structured\ninformation in DBpedia, and to provide large-scale and multilingual language\nresource for development of various NLP and IR task. The dataset provides the\ncontent of all articles for 128 Wikipedia languages. We describe the dataset\ncreation process and the NLP Interchange Format (NIF) used to model the\ncontent, links and the structure the information of the Wikipedia articles. The\ndataset has been further enriched with about 25% more links and selected\npartitions published as Linked Data. Finally, we describe the maintenance and\nsustainability plans, and selected use cases of the dataset from the TextExt\nknowledge extraction challenge.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 13:50:50 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Dojchinovski", "Milan", ""], ["Hernandez", "Julio", ""], ["Ackermann", "Markus", ""], ["Kirschenbaum", "Amit", ""], ["Hellmann", "Sebastian", ""]]}, {"id": "1812.10387", "submitter": "Pavlos Fafalios", "authors": "Renato Stoffalette Jo\\~ao, Pavlos Fafalios, Stefan Dietze", "title": "Same but Different: Distant Supervision for Predicting and Understanding\n  Entity Linking Difficulty", "comments": "Preprint of paper accepted for publication in the 34th ACM/SIGAPP\n  Symposium On Applied Computing (SAC 2019)", "journal-ref": null, "doi": "10.1145/3297280.3297381", "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity Linking (EL) is the task of automatically identifying entity mentions\nin a piece of text and resolving them to a corresponding entity in a reference\nknowledge base like Wikipedia. There is a large number of EL tools available\nfor different types of documents and domains, yet EL remains a challenging task\nwhere the lack of precision on particularly ambiguous mentions often spoils the\nusefulness of automated disambiguation results in real applications. A priori\napproximations of the difficulty to link a particular entity mention can\nfacilitate flagging of critical cases as part of semi-automated EL systems,\nwhile detecting latent factors that affect the EL performance, like\ncorpus-specific features, can provide insights on how to improve a system based\non the special characteristics of the underlying corpus. In this paper, we\nfirst introduce a consensus-based method to generate difficulty labels for\nentity mentions on arbitrary corpora. The difficulty labels are then exploited\nas training data for a supervised classification task able to predict the EL\ndifficulty of entity mentions using a variety of features. Experiments over a\ncorpus of news articles show that EL difficulty can be estimated with high\naccuracy, revealing also latent features that affect EL performance. Finally,\nevaluation results demonstrate the effectiveness of the proposed method to\ninform semi-automated EL pipelines.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 12:48:40 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Jo\u00e3o", "Renato Stoffalette", ""], ["Fafalios", "Pavlos", ""], ["Dietze", "Stefan", ""]]}, {"id": "1812.10546", "submitter": "Yuri Brovman", "authors": "Daniel A. Galron, Yuri M. Brovman, Jin Chung, Michal Wieja, Paul Wang", "title": "Deep Item-based Collaborative Filtering for Sparse Implicit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are ubiquitous in the domain of e-commerce, used to\nimprove the user experience and to market inventory, thereby increasing revenue\nfor the site. Techniques such as item-based collaborative filtering are used to\nmodel users' behavioral interactions with items and make recommendations from\nitems that have similar behavioral patterns. However, there are challenges when\napplying these techniques on extremely sparse and volatile datasets. On some\ne-commerce sites, such as eBay, the volatile inventory and minimal structured\ninformation about items make it very difficult to aggregate user interactions\nwith an item. In this work, we describe a novel deep learning-based method to\naddress the challenges. We propose an objective function that optimizes a\nsimilarity measure between binary implicit feedback vectors between two items.\nWe demonstrate formally and empirically that a model trained to optimize this\nfunction estimates the log of the cosine similarity between the feedback\nvectors. We also propose a neural network architecture optimized on this\nobjective. We present the results of experiments comparing the output of the\nneural network with traditional item-based collaborative filtering models on an\nimplicit-feedback dataset, as well as results of experiments comparing\ndifferent neural network architectures on user purchase behavior on eBay.\nFinally, we discuss the results of an A/B test that show marked improvement of\nthe proposed technique over eBay's existing collaborative filtering recommender\nsystem.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 21:40:17 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Galron", "Daniel A.", ""], ["Brovman", "Yuri M.", ""], ["Chung", "Jin", ""], ["Wieja", "Michal", ""], ["Wang", "Paul", ""]]}, {"id": "1812.10613", "submitter": "Xinshi Chen", "authors": "Xinshi Chen, Shuang Li, Hui Li, Shaohua Jiang, Yuan Qi, Le Song", "title": "Generative Adversarial User Model for Reinforcement Learning Based\n  Recommendation System", "comments": null, "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning, PMLR 97:1052-1061, 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are great interests as well as many challenges in applying\nreinforcement learning (RL) to recommendation systems. In this setting, an\nonline user is the environment; neither the reward function nor the environment\ndynamics are clearly defined, making the application of RL challenging. In this\npaper, we propose a novel model-based reinforcement learning framework for\nrecommendation systems, where we develop a generative adversarial network to\nimitate user behavior dynamics and learn her reward function. Using this user\nmodel as the simulation environment, we develop a novel Cascading DQN algorithm\nto obtain a combinatorial recommendation policy which can handle a large number\nof candidate items efficiently. In our experiments with real data, we show this\ngenerative adversarial user model can better explain user behavior than\nalternatives, and the RL policy based on this model can lead to a better\nlong-term reward for the user and higher click rate for the system.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 03:44:07 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 06:42:35 GMT"}, {"version": "v3", "created": "Wed, 1 Jan 2020 03:17:02 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Chen", "Xinshi", ""], ["Li", "Shuang", ""], ["Li", "Hui", ""], ["Jiang", "Shaohua", ""], ["Qi", "Yuan", ""], ["Song", "Le", ""]]}, {"id": "1812.10720", "submitter": "Svitlana Vakulenko", "authors": "Svitlana Vakulenko, Kate Revoredo, Claudio Di Ciccio and Maarten de\n  Rijke", "title": "QRFA: A Data-Driven Model of Information-Seeking Dialogues", "comments": "Advances in Information Retrieval. Proceedings of the 41st European\n  Conference on Information Retrieval (ECIR '19), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the structure of interaction processes helps us to improve\ninformation-seeking dialogue systems. Analyzing an interaction process boils\ndown to discovering patterns in sequences of alternating utterances exchanged\nbetween a user and an agent. Process mining techniques have been successfully\napplied to analyze structured event logs, discovering the underlying process\nmodels or evaluating whether the observed behavior is in conformance with the\nknown process. In this paper, we apply process mining techniques to discover\npatterns in conversational transcripts and extract a new model of\ninformation-seeking dialogues, QRFA, for Query, Request, Feedback, Answer. Our\nresults are grounded in an empirical evaluation across multiple conversational\ndatasets from different domains, which was never attempted before. We show that\nthe QRFA model better reflects conversation flows observed in real\ninformation-seeking conversations than models proposed previously. Moreover,\nQRFA allows us to identify malfunctioning in dialogue system transcripts as\ndeviations from the expected conversation flow described by the model via\nconformance analysis.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 13:36:41 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["Revoredo", "Kate", ""], ["Di Ciccio", "Claudio", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1812.10814", "submitter": "Jan Kowollik", "authors": "Jan Kowollik and Ahmet Aker", "title": "Uni-DUE Student Team: Tackling fact checking through decomposable\n  attention neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our system for the FEVER Challenge. The task of this\nchallenge is to verify claims by extracting information from Wikipedia. Our\nsystem has two parts. In the first part it performs a search for candidate\nsentences by treating the claims as query. In the second part it filters out\nnoise from these candidates and uses the remaining ones to decide whether they\nsupport or refute or entail not enough information to verify the claim. We show\nthat this system achieves a FEVER score of 0.3927 on the FEVER shared task\ndevelopment data set which is a 25.5% improvement over the baseline score.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 20:09:51 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Kowollik", "Jan", ""], ["Aker", "Ahmet", ""]]}, {"id": "1812.10847", "submitter": "Martin Potthast", "authors": "Martin Potthast, Tim Gollub, Matthias Hagen, Benno Stein", "title": "The Clickbait Challenge 2017: Towards a Regression Model for Clickbait\n  Strength", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clickbait has grown to become a nuisance to social media users and social\nmedia operators alike. Malicious content publishers misuse social media to\nmanipulate as many users as possible to visit their websites using clickbait\nmessages. Machine learning technology may help to handle this problem, giving\nrise to automatic clickbait detection. To accelerate progress in this\ndirection, we organized the Clickbait Challenge 2017, a shared task inviting\nthe submission of clickbait detectors for a comparative evaluation. A total of\n13 detectors have been submitted, achieving significant improvements over the\nprevious state of the art in terms of detection performance. Also, many of the\nsubmitted approaches have been published open source, rendering them\nreproducible, and a good starting point for newcomers. While the 2017 challenge\nhas passed, we maintain the evaluation system and answer to new registrations\nin support of the ongoing research on better clickbait detectors.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 23:42:06 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Potthast", "Martin", ""], ["Gollub", "Tim", ""], ["Hagen", "Matthias", ""], ["Stein", "Benno", ""]]}, {"id": "1812.10937", "submitter": "Bracha Shapira", "authors": "Shahar Admati, Lior Rokach, Bracha Shapira", "title": "Wikibook-Bot - Automatic Generation of a Wikipedia Book", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Wikipedia book (known as Wikibook) is a collection of Wikipedia articles on\na particular theme that is organized as a book. We propose Wikibook-Bot, a\nmachine-learning based technique for automatically generating high quality\nWikibooks based on a concept provided by the user. In order to create the\nWikibook we apply machine learning algorithms to the different steps of the\nproposed technique. Firs, we need to decide whether an article belongs to a\nspecific Wikibook - a classification task. Then, we need to divide the chosen\narticles into chapters - a clustering task - and finally, we deal with the\nordering task which includes two subtasks: order articles within each chapter\nand order the chapters themselves. We propose a set of structural, text-based\nand unique Wikipedia features, and we show that by using these features, a\nmachine learning classifier can successfully address the above challenges. The\npredictive performance of the proposed method is evaluated by comparing the\nauto-generated books to existing 407 Wikibooks which were manually generated by\nhumans. For all the tasks we were able to obtain high and statistically\nsignificant results when comparing the Wikibook-bot books to books that were\nmanually generated by Wikipedia contributors\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 09:49:56 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Admati", "Shahar", ""], ["Rokach", "Lior", ""], ["Shapira", "Bracha", ""]]}, {"id": "1812.11252", "submitter": "Haofeng Jia", "authors": "Haofeng Jia and Erik Saule", "title": "Towards Finding Non-obvious Papers: An Analysis of Citation Recommender\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As science advances, the academic community has published millions of\nresearch papers. Researchers devote time and effort to search relevant\nmanuscripts when writing a paper or simply to keep up with current research. In\nthis paper, we consider the problem of citation recommendation by extending a\nset of known-to-be-relevant references. Our analysis shows the degrees of cited\npapers in the subgraph induced by the citations of a paper, called projection\ngraph, follow a power law distribution. Existing popular methods are only good\nat finding the long tail papers, the ones that are highly connected to others.\nIn other words, the majority of cited papers are loosely connected in the\nprojection graph but they are not going to be found by existing methods. To\naddress this problem, we propose to combine author, venue and keyword\ninformation to interpret the citation behavior behind those loosely connected\npapers. Results show that different methods are finding cited papers with\nwidely different properties. We suggest multiple recommended lists by different\nalgorithms could satisfy various users for a real citation recommendation\nsystem. Moreover, we also explore the fast local approximation for combined\nmethods in order to improve the efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 00:31:43 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Jia", "Haofeng", ""], ["Saule", "Erik", ""]]}, {"id": "1812.11275", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen and Karin Verspoor", "title": "End-to-end neural relation extraction using deep biaffine attention", "comments": "Proceedings of the 41st European Conference on Information Retrieval\n  (ECIR 2019), to appear", "journal-ref": null, "doi": "10.1007/978-3-030-15712-8_47", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural network model for joint extraction of named entities and\nrelations between them, without any hand-crafted features. The key contribution\nof our model is to extend a BiLSTM-CRF-based entity recognition model with a\ndeep biaffine attention layer to model second-order interactions between latent\nfeatures for relation classification, specifically attending to the role of an\nentity in a directional relationship. On the benchmark \"relation and entity\nrecognition\" dataset CoNLL04, experimental results show that our model\noutperforms previous models, producing new state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 03:32:09 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Nguyen", "Dat Quoc", ""], ["Verspoor", "Karin", ""]]}, {"id": "1812.11321", "submitter": "Ningyu Zhang", "authors": "Ningyu Zhang, Shumin Deng, Zhanlin Sun, Xi Chen, Wei Zhang, Huajun\n  Chen", "title": "Attention-Based Capsule Networks with Dynamic Routing for Relation\n  Extraction", "comments": "To be published in EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A capsule is a group of neurons, whose activity vector represents the\ninstantiation parameters of a specific type of entity. In this paper, we\nexplore the capsule networks used for relation extraction in a multi-instance\nmulti-label learning framework and propose a novel neural approach based on\ncapsule networks with attention mechanisms. We evaluate our method with\ndifferent benchmarks, and it is demonstrated that our method improves the\nprecision of the predicted relations. Particularly, we show that capsule\nnetworks improve multiple entity pairs relation extraction.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 09:34:23 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Zhang", "Ningyu", ""], ["Deng", "Shumin", ""], ["Sun", "Zhanlin", ""], ["Chen", "Xi", ""], ["Zhang", "Wei", ""], ["Chen", "Huajun", ""]]}, {"id": "1812.11422", "submitter": "Bibek Paudel", "authors": "Bibek Paudel, Sandro Luck, Abraham Bernstein", "title": "Loss Aversion in Recommender Systems: Utilizing Negative User Preference\n  to Improve Recommendation Quality", "comments": "The First International Workshop on Context-Aware Recommendation\n  Systems with Big Data Analytics (CARS-BDA), co-organized with the 12th ACM\n  International Conference on Web Search and Data Mining, 2019, Melbourne,\n  Australia", "journal-ref": "CARS-BDA, at the 12th ACM International Conference on Web Search\n  and Data Mining (WSDM), 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Negative user preference is an important context that is not sufficiently\nutilized by many existing recommender systems. This context is especially\nuseful in scenarios where the cost of negative items is high for the users. In\nthis work, we describe a new recommender algorithm that explicitly models\nnegative user preferences in order to recommend more positive items at the top\nof recommendation-lists. We build upon existing machine-learning model to\nincorporate the contextual information provided by negative user preference.\nWith experimental evaluations on two openly available datasets, we show that\nour method is able to improve recommendation quality: by improving accuracy and\nat the same time reducing the number of negative items at the top of\nrecommendation-lists. Our work demonstrates the value of the contextual\ninformation provided by negative feedback, and can also be extended to signed\nsocial networks and link prediction in other networks.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 18:49:13 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Paudel", "Bibek", ""], ["Luck", "Sandro", ""], ["Bernstein", "Abraham", ""]]}, {"id": "1812.11561", "submitter": "Chen Qu", "authors": "Chen Qu, Feng Ji, Minghui Qiu, Liu Yang, Zhiyu Min, Haiqing Chen, Jun\n  Huang and W. Bruce Croft", "title": "Learning to Selectively Transfer: Reinforced Transfer Learning for Deep\n  Text Matching", "comments": "Accepted to WSDM 2019", "journal-ref": null, "doi": "10.1145/3289600.3290978", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep text matching approaches have been widely studied for many applications\nincluding question answering and information retrieval systems. To deal with a\ndomain that has insufficient labeled data, these approaches can be used in a\nTransfer Learning (TL) setting to leverage labeled data from a resource-rich\nsource domain. To achieve better performance, source domain data selection is\nessential in this process to prevent the \"negative transfer\" problem. However,\nthe emerging deep transfer models do not fit well with most existing data\nselection methods, because the data selection policy and the transfer learning\nmodel are not jointly trained, leading to sub-optimal training efficiency.\n  In this paper, we propose a novel reinforced data selector to select\nhigh-quality source domain data to help the TL model. Specifically, the data\nselector \"acts\" on the source domain data to find a subset for optimization of\nthe TL model, and the performance of the TL model can provide \"rewards\" in turn\nto update the selector. We build the reinforced data selector based on the\nactor-critic framework and integrate it to a DNN based transfer learning model,\nresulting in a Reinforced Transfer Learning (RTL) method. We perform a thorough\nexperimental evaluation on two major tasks for text matching, namely,\nparaphrase identification and natural language inference. Experimental results\nshow the proposed RTL can significantly improve the performance of the TL\nmodel. We further investigate different settings of states, rewards, and policy\noptimization methods to examine the robustness of our method. Last, we conduct\na case study on the selected data and find our method is able to select source\ndomain data whose Wasserstein distance is close to the target domain data. This\nis reasonable and intuitive as such source domain data can provide more\ntransferability power to the model.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 15:39:57 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Qu", "Chen", ""], ["Ji", "Feng", ""], ["Qiu", "Minghui", ""], ["Yang", "Liu", ""], ["Min", "Zhiyu", ""], ["Chen", "Haiqing", ""], ["Huang", "Jun", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1812.11709", "submitter": "Zhuoren Jiang", "authors": "Zhuoren Jiang, Yue Yin, Liangcai Gao, Yao Lu, Xiaozhong Liu", "title": "Cross-language Citation Recommendation via Hierarchical Representation\n  Learning on Heterogeneous Graph", "comments": "The 41st International ACM SIGIR Conference on Research & Development\n  in Information Retrieval (SIGIR 2018), 635--644", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the volume of scholarly publications has increased at a frenetic pace,\naccessing and consuming the useful candidate papers, in very large digital\nlibraries, is becoming an essential and challenging task for scholars.\nUnfortunately, because of language barrier, some scientists (especially the\njunior ones or graduate students who do not master other languages) cannot\nefficiently locate the publications hosted in a foreign language repository. In\nthis study, we propose a novel solution, cross-language citation recommendation\nvia Hierarchical Representation Learning on Heterogeneous Graph (HRLHG), to\naddress this new problem. HRLHG can learn a representation function by mapping\nthe publications, from multilingual repositories, to a low-dimensional joint\nembedding space from various kinds of vertexes and relations on a heterogeneous\ngraph. By leveraging both global (task specific) plus local (task independent)\ninformation as well as a novel supervised hierarchical random walk algorithm,\nthe proposed method can optimize the publication representations by maximizing\nthe likelihood of locating the important cross-language neighborhoods on the\ngraph. Experiment results show that the proposed method can not only outperform\nstate-of-the-art baseline models, but also improve the interpretability of the\nrepresentation model for cross-language citation recommendation task.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 07:09:59 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Jiang", "Zhuoren", ""], ["Yin", "Yue", ""], ["Gao", "Liangcai", ""], ["Lu", "Yao", ""], ["Liu", "Xiaozhong", ""]]}, {"id": "1812.11740", "submitter": "Yiren Liu", "authors": "Jionghao Lin, Yiren Liu", "title": "A Neural Network Based Explainable Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation system could help the companies to persuade users to visit or\nconsume at a particular place, which was based on many traditional methods such\nas the set of collaborative filtering algorithms. Most research discusses the\nmodel design or feature engineering methods to minimize the root mean square\nerror (RMSE) of rating prediction, but lacks exploring the ways to generate the\nreasons for recommendations. This paper proposed an integrated neural network\nbased model which integrates rating scores prediction and explainable words\ngeneration. Based on the experimental results, this model presented lower RMSE\ncompared with traditional methods, and generate the explanation of\nrecommendation to convince customers to visit the recommended place.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 09:51:37 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Lin", "Jionghao", ""], ["Liu", "Yiren", ""]]}, {"id": "1812.11786", "submitter": "Zhuoren Jiang", "authors": "Zhuoren Jiang, Liangcai Gao, Ke Yuan, Zheng Gao, Zhi Tang, Xiaozhong\n  Liu", "title": "Mathematics Content Understanding for Cyberlearning via Formula\n  Evolution Map", "comments": "The 27th ACM International Conference on Information and Knowledge\n  Management (CIKM2018) 37--46", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the scientific digital library is growing at a rapid pace,\nscholars/students often find reading Science, Technology, Engineering, and\nMathematics (STEM) literature daunting, especially for the\nmath-content/formula. In this paper, we propose a novel problem, ``mathematics\ncontent understanding'', for cyberlearning and cyberreading. To address this\nproblem, we create a Formula Evolution Map (FEM) offline and implement a novel\nonline learning/reading environment, PDF Reader with Math-Assistant (PRMA),\nwhich incorporates innovative math-scaffolding methods. The proposed\nalgorithm/system can auto-characterize student emerging math-information need\nwhile reading a paper and enable students to readily explore the formula\nevolution trajectory in FEM. Based on a math-information need, PRMA utilizes\ninnovative joint embedding, formula evolution mining, and heterogeneous graph\nmining algorithms to recommend high quality Open Educational Resources (OERs),\ne.g., video, Wikipedia page, or slides, to help students better understand the\nmath-content in the paper. Evaluation and exit surveys show that the PRMA\nsystem and the proposed formula understanding algorithm can effectively assist\nmaster and PhD students better understand the complex math-content in the class\nreadings.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 13:20:50 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Jiang", "Zhuoren", ""], ["Gao", "Liangcai", ""], ["Yuan", "Ke", ""], ["Gao", "Zheng", ""], ["Tang", "Zhi", ""], ["Liu", "Xiaozhong", ""]]}]