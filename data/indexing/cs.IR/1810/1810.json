[{"id": "1810.00272", "submitter": "Farzad Eskandanian", "authors": "Farzad Eskandanian, Bamshad Mobasher", "title": "Detecting Changes in User Preferences using Hidden Markov Models for\n  Sequential Recommendation Tasks", "comments": "7 pages, 4 figures, RecSysKTL, Workshop on Intelligent Recommender\n  Systems by Knowledge Transfer and Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems help users find relevant items of interest based on the\npast preferences of those users. In many domains, however, the tastes and\npreferences of users change over time due to a variety of factors and\nrecommender systems should capture these dynamics in user preferences in order\nto remain tuned to the most current interests of users. In this work we present\na recommendation framework based on Hidden Markov Models (HMM) which takes into\naccount the dynamics of user preferences. We propose a HMM-based approach to\nchange point detection in the sequence of user interactions which reflect\nsignificant changes in preference according to the sequential behavior of all\nthe users in the data. The proposed framework leverages the identified change\npoints to generate recommendations in two ways. In one approach change points\nare used to create a sequence-aware non-negative matrix factorization model to\ngenerate recommendations that are aligned with the current tastes of user. In\nthe second approach the HMM is used directly to generate recommendations taking\ninto account the identified change points. These models are evaluated in terms\nof accuracy of change point detection and also the effectiveness of\nrecommendations using a real music streaming dataset.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 22:33:25 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Eskandanian", "Farzad", ""], ["Mobasher", "Bamshad", ""]]}, {"id": "1810.00647", "submitter": "I\\~naki San Vicente Roncal", "authors": "I\\~naki San Vicente, Xabier Saralegi, Rodrigo Agerri", "title": "Real Time Monitoring of Social Media and Digital Press", "comments": "Preprint submission, 35 pages (22 + references and Appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Talaia is a platform for monitoring social media and digital press. A\nconfigurable crawler gathers content with respect to user defined domains or\ntopics. Crawled data is processed by means of the EliXa Sentiment Analysis\nsystem. A Django powered interface provides data visualization for a user-based\nanalysis of the data. This paper presents the architecture of the system and\ndescribes in detail its different components. To prove the validity of the\napproach, two real use cases are accounted for: one in the cultural domain and\none in the political domain. Evaluation for the sentiment analysis task in both\nscenarios is also provided, showing the capacity for domain adaptation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 16:00:20 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 16:56:26 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Vicente", "I\u00f1aki San", ""], ["Saralegi", "Xabier", ""], ["Agerri", "Rodrigo", ""]]}, {"id": "1810.00679", "submitter": "Rasool Fakoor", "authors": "Rasool Fakoor, Amanjit Kainth, Siamak Shakeri, Christopher Winestock,\n  Abdel-rahman Mohamed, Ruhi Sarikaya", "title": "Direct optimization of F-measure for retrieval-based personal question\n  answering", "comments": "accepted at SLT2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in spoken language technologies and the introduction of many\ncustomer facing products, have given rise to a wide customer reliance on smart\npersonal assistants for many of their daily tasks. In this paper, we present a\nsystem to reduce users' cognitive load by extending personal assistants with\nlong-term personal memory where users can store and retrieve by voice,\narbitrary pieces of information. The problem is framed as a neural retrieval\nbased question answering system where answers are selected from previously\nstored user memories. We propose to directly optimize the end-to-end retrieval\nperformance, measured by the F1-score, using reinforcement learning, leading to\nbetter performance on our experimental test set(s).\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 00:51:24 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Fakoor", "Rasool", ""], ["Kainth", "Amanjit", ""], ["Shakeri", "Siamak", ""], ["Winestock", "Christopher", ""], ["Mohamed", "Abdel-rahman", ""], ["Sarikaya", "Ruhi", ""]]}, {"id": "1810.00751", "submitter": "Zahra Vahidi Ferdousi", "authors": "Zahra Vahidi Ferdousi, Dario Colazzo, Elsa Negre", "title": "CBPF: leveraging context and content information for better\n  recommendations", "comments": "15 pages, 4 figures, this is the long version of the paper submitted\n  to the conference ADMA'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recommender systems help users to find their appropriate items among large\nvolumes of information. Different types of recommender systems have been\nproposed. Among these, context-aware recommender systems aim at personalizing\nas much as possible the recommendations based on the context situation in which\nthe user is. In this paper we present an approach integrating contextual\ninformation into the recommendation process by modeling either item-based or\nuser-based influence of the context on ratings, using the Pearson Correlation\nCoefficient. The proposed solution aims at taking advantage of content and\ncontextual information in the recommendation process. We evaluate and show\neffectiveness of our approach on three different contextual datasets and\nanalyze the performances of the variants of our approach based on the\ncharacteristics of these datasets, especially the sparsity level of the input\ndata and amount of available information.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 15:09:42 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Ferdousi", "Zahra Vahidi", ""], ["Colazzo", "Dario", ""], ["Negre", "Elsa", ""]]}, {"id": "1810.01468", "submitter": "Gaurav Singh", "authors": "Gaurav Singh, James Thomas, Iain J. Marshall, John Shawe-Taylor and\n  Byron C. Wallace", "title": "Structured Multi-Label Biomedical Text Tagging via Attentive Neural Tree\n  Decoding", "comments": "Accepted for Publication in EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model for tagging unstructured texts with an arbitrary number of\nterms drawn from a tree-structured vocabulary (i.e., an ontology). We treat\nthis as a special case of sequence-to-sequence learning in which the decoder\nbegins at the root node of an ontological tree and recursively elects to expand\nchild nodes as a function of the input text, the current node, and the latent\ndecoder state. In our experiments the proposed method outperforms\nstate-of-the-art approaches on the important task of automatically assigning\nMeSH terms to biomedical abstracts.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 19:32:12 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Singh", "Gaurav", ""], ["Thomas", "James", ""], ["Marshall", "Iain J.", ""], ["Shawe-Taylor", "John", ""], ["Wallace", "Byron C.", ""]]}, {"id": "1810.01477", "submitter": "Houssam Nassif", "authors": "Choon Hui Teo, Houssam Nassif, Daniel Hill, Sriram Srinavasan,\n  Mitchell Goodman, Vijai Mohan, SVN Vishwanathan", "title": "Adaptive, Personalized Diversity for Visual Discovery", "comments": "Best Paper Award", "journal-ref": "Adaptive, Personalized Diversity for Visual Discovery. Teo CH,\n  Nassif H, Hill D, Srinavasan S, Goodman M, Mohan V, and Vishwanathan SVN. ACM\n  Conference on Recommender Systems (RecSys'16), Boston, pp. 35-38, 2016", "doi": "10.1145/2959100.2959171", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search queries are appropriate when users have explicit intent, but they\nperform poorly when the intent is difficult to express or if the user is simply\nlooking to be inspired. Visual browsing systems allow e-commerce platforms to\naddress these scenarios while offering the user an engaging shopping\nexperience. Here we explore extensions in the direction of adaptive\npersonalization and item diversification within Stream, a new form of visual\nbrowsing and discovery by Amazon. Our system presents the user with a diverse\nset of interesting items while adapting to user interactions. Our solution\nconsists of three components (1) a Bayesian regression model for scoring the\nrelevance of items while leveraging uncertainty, (2) a submodular\ndiversification framework that re-ranks the top scoring items based on\ncategory, and (3) personalized category preferences learned from the user's\nbehavior. When tested on live traffic, our algorithms show a strong lift in\nclick-through-rate and session duration.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 19:51:46 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Teo", "Choon Hui", ""], ["Nassif", "Houssam", ""], ["Hill", "Daniel", ""], ["Srinavasan", "Sriram", ""], ["Goodman", "Mitchell", ""], ["Mohan", "Vijai", ""], ["Vishwanathan", "SVN", ""]]}, {"id": "1810.01482", "submitter": "Houssam Nassif", "authors": "Houssam Nassif, Kemal Oral Cansizlar, Mitchell Goodman, and SVN\n  Vishwanathan", "title": "Diversifying Music Recommendations", "comments": "Machine Learning for Music Discovery Workshop at the 33rd\n  International Conference on Machine Learning (ICML'16), New York, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We compare submodular and Jaccard methods to diversify Amazon Music\nrecommendations. Submodularity significantly improves recommendation quality\nand user engagement. Unlike the Jaccard method, our submodular approach\nincorporates item relevance score within its optimization function, and\nproduces a relevant and uniformly diverse set.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 20:02:20 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Nassif", "Houssam", ""], ["Cansizlar", "Kemal Oral", ""], ["Goodman", "Mitchell", ""], ["Vishwanathan", "SVN", ""]]}, {"id": "1810.01520", "submitter": "Hamed Zamani", "authors": "Hamed Zamani, Markus Schedl, Paul Lamere, Ching-Wei Chen", "title": "An Analysis of Approaches Taken in the ACM RecSys Challenge 2018 for\n  Automatic Music Playlist Continuation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ACM Recommender Systems Challenge 2018 focused on the task of automatic\nmusic playlist continuation, which is a form of the more general task of\nsequential recommendation. Given a playlist of arbitrary length with some\nadditional meta-data, the task was to recommend up to 500 tracks that fit the\ntarget characteristics of the original playlist. For the RecSys Challenge,\nSpotify released a dataset of one million user-generated playlists.\nParticipants could compete in two tracks, i.e., main and creative tracks.\nParticipants in the main track were only allowed to use the provided training\nset, however, in the creative track, the use of external public sources was\npermitted. In total, 113 teams submitted 1,228 runs to the main track; 33 teams\nsubmitted 239 runs to the creative track. The highest performing team in the\nmain track achieved an R-precision of 0.2241, an NDCG of 0.3946, and an average\nnumber of recommended songs clicks of 1.784. In the creative track, an\nR-precision of 0.2233, an NDCG of 0.3939, and a click rate of 1.785 was\nobtained by the best team. This article provides an overview of the challenge,\nincluding motivation, task definition, dataset description, and evaluation. We\nfurther report and analyze the results obtained by the top performing teams in\neach track and explore the approaches taken by the winners. We finally\nsummarize our key findings, discuss generalizability of approaches and results\nto domains other than music, and list the open avenues and possible future\ndirections in the area of automatic playlist continuation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 21:19:19 GMT"}, {"version": "v2", "created": "Sat, 31 Aug 2019 22:13:33 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zamani", "Hamed", ""], ["Schedl", "Markus", ""], ["Lamere", "Paul", ""], ["Chen", "Ching-Wei", ""]]}, {"id": "1810.01765", "submitter": "Ramy Baly", "authors": "Ramy Baly (1), Georgi Karadzhov (3), Dimitar Alexandrov (3), James\n  Glass (1), Preslav Nakov (2) ((1) MIT Computer Science and Artificial\n  Intelligence Laboratory, (2) Qatar Computing Research Institute, HBKU, Qatar,\n  (3) Sofia University, Bulgaria)", "title": "Predicting Factuality of Reporting and Bias of News Media Sources", "comments": "Fact-checking, political ideology, news media, EMNLP-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study on predicting the factuality of reporting and bias of news\nmedia. While previous work has focused on studying the veracity of claims or\ndocuments, here we are interested in characterizing entire news media. These\nare under-studied but arguably important research problems, both in their own\nright and as a prior for fact-checking systems. We experiment with a large list\nof news websites and with a rich set of features derived from (i) a sample of\narticles from the target news medium, (ii) its Wikipedia page, (iii) its\nTwitter account, (iv) the structure of its URL, and (v) information about the\nWeb traffic it attracts. The experimental results show sizable performance\ngains over the baselines, and confirm the importance of each feature type.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 03:27:04 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Baly", "Ramy", ""], ["Karadzhov", "Georgi", ""], ["Alexandrov", "Dimitar", ""], ["Glass", "James", ""], ["Nakov", "Preslav", ""]]}, {"id": "1810.01807", "submitter": "Romain Hennequin", "authors": "Jimena Royo-Letelier, Romain Hennequin, Viet-Anh Tran, Manuel\n  Moussallam", "title": "Disambiguating Music Artists at Scale with Audio Metric Learning", "comments": "published in ISMIR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG cs.SD stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of disambiguating large scale catalogs through the\ndefinition of an unknown artist clustering task. We explore the use of metric\nlearning techniques to learn artist embeddings directly from audio, and using a\ndedicated homonym artists dataset, we compare our method with a recent approach\nthat learn similar embeddings using artist classifiers. While both systems have\nthe ability to disambiguate unknown artists relying exclusively on audio, we\nshow that our system is more suitable in the case when enough audio data is\navailable for each artist in the train dataset. We also propose a new negative\nsampling method for metric learning that takes advantage of side information\nsuch as music genre during the learning phase and shows promising results for\nthe artist clustering task.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 15:49:43 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Royo-Letelier", "Jimena", ""], ["Hennequin", "Romain", ""], ["Tran", "Viet-Anh", ""], ["Moussallam", "Manuel", ""]]}, {"id": "1810.02019", "submitter": "Ofer Meshi", "authors": "Irwan Bello, Sayali Kulkarni, Sagar Jain, Craig Boutilier, Ed Chi,\n  Elad Eban, Xiyang Luo, Alan Mackey, Ofer Meshi", "title": "Seq2Slate: Re-ranking and Slate Optimization with RNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking is a central task in machine learning and information retrieval. In\nthis task, it is especially important to present the user with a slate of items\nthat is appealing as a whole. This in turn requires taking into account\ninteractions between items, since intuitively, placing an item on the slate\naffects the decision of which other items should be placed alongside it. In\nthis work, we propose a sequence-to-sequence model for ranking called\nseq2slate. At each step, the model predicts the next `best' item to place on\nthe slate given the items already selected. The sequential nature of the model\nallows complex dependencies between the items to be captured directly in a\nflexible and scalable way. We show how to learn the model end-to-end from weak\nsupervision in the form of easily obtained click-through data. We further\ndemonstrate the usefulness of our approach in experiments on standard ranking\nbenchmarks as well as in a real-world recommendation system.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 01:35:14 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 17:38:40 GMT"}, {"version": "v3", "created": "Tue, 19 Mar 2019 18:36:25 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Bello", "Irwan", ""], ["Kulkarni", "Sayali", ""], ["Jain", "Sagar", ""], ["Boutilier", "Craig", ""], ["Chi", "Ed", ""], ["Eban", "Elad", ""], ["Luo", "Xiyang", ""], ["Mackey", "Alan", ""], ["Meshi", "Ofer", ""]]}, {"id": "1810.02518", "submitter": "arXiv Admin", "authors": "Rahul Makhijani", "title": "Social Choice Random Utility Models of Intransitive Pairwise Comparisons", "comments": "This article has been withdrawn by arXiv administrators due to an\n  irreconcilable authorship dispute", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing need for discrete choice models that account for the\ncomplex nature of human choices, escaping traditional behavioral assumptions\nsuch as the transitivity of pairwise preferences. Recently, several parametric\nmodels of intransitive comparisons have been proposed, but in all cases the\nmaximum likelihood problem is non-concave, making inference difficult. In this\nwork we generalize this trend, showing that there cannot exist any parametric\nmodel with a concave log-likelihood function that can exhibit intransitive\npreferences. Given this result, we motivate a new model for analyzing\nintransitivity in pairwise comparisons, taking inspiration from the Condorcet\nmethod (majority vote) in social choice theory. The Majority Vote model we\nanalyze is defined as a voting process over independent Random Utility Models\n(RUMs). We infer a multidimensional embedding of each object or player, in\ncontrast to the traditional one-dimensional embedding used by models such as\nthe Thurstone or Bradley-Terry-Luce (BTL) models. We show that a\nthree-dimensional majority vote model is capable of modeling arbitrarily strong\nand long intransitive cycles, and can also represent arbitrary pairwise\ncomparison probabilities on any triplet. We provide experimental results that\nsubstantiate our claims regarding the effectiveness of our model in capturing\nintransitivity for various pairwise choice tasks such as predicting choices in\nrecommendation systems, winners in online video games, and elections.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 05:26:29 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 18:51:21 GMT"}, {"version": "v3", "created": "Thu, 11 Oct 2018 15:39:17 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Makhijani", "Rahul", ""]]}, {"id": "1810.02579", "submitter": "Qijun Zhu", "authors": "Qijun Zhu, Dandan Li, Dik Lun Lee", "title": "C-DLSI: An Extended LSI Tailored for Federated Text Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the web expands in data volume and in geographical distribution,\ncentralized search methods become inefficient, leading to increasing interest\nin cooperative information retrieval, e.g., federated text retrieval (FTR).\nDifferent from existing centralized information retrieval (IR) methods, in\nwhich search is done on a logically centralized document collection, FTR is\ncomposed of a number of peers, each of which is a complete search engine by\nitself. To process a query, FTR requires firstly the identification of\npromising peers that host the relevant documents and secondly the retrieval of\nthe most relevant documents from the selected peers. Most of the existing\nmethods only apply traditional IR techniques that treat each text collection as\na single large document and utilize term matching to rank the collections. In\nthis paper, we formalize the problem and identify the properties of FTR, and\nanalyze the feasibility of extending LSI with clustering to adapt to FTR, based\non which a novel approach called Cluster-based Distributed Latent Semantic\nIndexing (C-DLSI) is proposed. C-DLSI distinguishes the topics of a peer with\nclustering, captures the local LSI spaces within the clusters, and consider the\nrelations among these LSI spaces, thus providing more precise characterization\nof the peer. Accordingly, novel descriptors of the peers and a compatible local\ntext retrieval are proposed. The experimental results show that C-DLSI\noutperforms existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 09:14:50 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Zhu", "Qijun", ""], ["Li", "Dandan", ""], ["Lee", "Dik Lun", ""]]}, {"id": "1810.02717", "submitter": "Shaoyang Ning", "authors": "Shaoyang Ning, Xi Qu, Victor Cai, Nathan Sanders", "title": "Clust-LDA: Joint Model for Text Mining and Author Group Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media corpora pose unique challenges and opportunities, including\ntypically short document lengths and rich meta-data such as author\ncharacteristics and relationships. This creates great potential for systematic\nanalysis of the enormous body of the users and thus provides implications for\nindustrial strategies such as targeted marketing. Here we propose a novel and\nstatistically principled method, clust-LDA, which incorporates authorship\nstructure into the topical modeling, thus accomplishing the task of the topical\ninferences across documents on the basis of authorship and, simultaneously, the\nidentification of groupings between authors. We develop an inference procedure\nfor clust-LDA and demonstrate its performance on simulated data, showing that\nclust-LDA out-performs the \"vanilla\" LDA on the topic identification task where\nauthors exhibit distinctive topical preference. We also showcase the empirical\nperformance of clust-LDA based on a real-world social media dataset from\nReddit.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 14:33:40 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Ning", "Shaoyang", ""], ["Qu", "Xi", ""], ["Cai", "Victor", ""], ["Sanders", "Nathan", ""]]}, {"id": "1810.02802", "submitter": "Ni Lao", "authors": "Gengchen Mai, Krzysztof Janowicz, Cheng He, Sumang Liu, Ni Lao", "title": "POIReviewQA: A Semantically Enriched POI Retrieval and Question\n  Answering Dataset", "comments": null, "journal-ref": "12th Workshop on Geographic Information Retrieval (GIR 2018)", "doi": "10.1145/3281354.3281359", "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many services that perform information retrieval for Points of Interest (POI)\nutilize a Lucene-based setup with spatial filtering. While this type of system\nis easy to implement it does not make use of semantics but relies on direct\nword matches between a query and reviews leading to a loss in both precision\nand recall. To study the challenging task of semantically enriching POIs from\nunstructured data in order to support open-domain search and question answering\n(QA), we introduce a new dataset POIReviewQA. It consists of 20k questions\n(e.g.\"is this restaurant dog friendly?\") for 1022 Yelp business types. For each\nquestion we sampled 10 reviews, and annotated each sentence in the reviews\nwhether it answers the question and what the corresponding answer is. To test a\nsystem's ability to understand the text we adopt an information retrieval\nevaluation by ranking all the review sentences for a question based on the\nlikelihood that they answer this question. We build a Lucene-based baseline\nmodel, which achieves 77.0% AUC and 48.8% MAP. A sentence embedding-based model\nachieves 79.2% AUC and 41.8% MAP, indicating that the dataset presents a\nchallenging problem for future research by the GIR community. The result\ntechnology can help exploit the thematic content of web documents and social\nmedia for characterisation of locations.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 17:37:37 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Mai", "Gengchen", ""], ["Janowicz", "Krzysztof", ""], ["He", "Cheng", ""], ["Liu", "Sumang", ""], ["Lao", "Ni", ""]]}, {"id": "1810.02832", "submitter": "Huaizheng Zhang", "authors": "Yong Luo, Huaizheng Zhang, Yongjie Wang, Yonggang We, Xinwen Zhang", "title": "ResumeNet: A Learning-based Framework for Automatic Resume Quality\n  Assessment", "comments": "ICDM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recruitment of appropriate people for certain positions is critical for any\ncompanies or organizations. Manually screening to select appropriate candidates\nfrom large amounts of resumes can be exhausted and time-consuming. However,\nthere is no public tool that can be directly used for automatic resume quality\nassessment (RQA). This motivates us to develop a method for automatic RQA.\nSince there is also no public dataset for model training and evaluation, we\nbuild a dataset for RQA by collecting around 10K resumes, which are provided by\na private resume management company. By investigating the dataset, we identify\nsome factors or features that could be useful to discriminate good resumes from\nbad ones, e.g., the consistency between different parts of a resume. Then a\nneural-network model is designed to predict the quality of each resume, where\nsome text processing techniques are incorporated. To deal with the label\ndeficiency issue in the dataset, we propose several variants of the model by\neither utilizing the pair/triplet-based loss, or introducing some\nsemi-supervised learning technique to make use of the abundant unlabeled data.\nBoth the presented baseline model and its variants are general and easy to\nimplement. Various popular criteria including the receiver operating\ncharacteristic (ROC) curve, F-measure and ranking-based average precision (AP)\nare adopted for model evaluation. We compare the different variants with our\nbaseline model. Since there is no public algorithm for RQA, we further compare\nour results with those obtained from a website that can score a resume.\nExperimental results in terms of different criteria demonstrate the\neffectiveness of the proposed method. We foresee that our approach would\ntransform the way of future human resources management.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 18:02:19 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Luo", "Yong", ""], ["Zhang", "Huaizheng", ""], ["Wang", "Yongjie", ""], ["We", "Yonggang", ""], ["Zhang", "Xinwen", ""]]}, {"id": "1810.02907", "submitter": "Cameron VandenBerg", "authors": "Cameron VandenBerg and Jamie Callan", "title": "Sifaka: Text Mining Above a Search API", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text mining and analytics software has become popular, but little attention\nhas been paid to the software architectures of such systems. Often they are\nbuilt from scratch using special-purpose software and data structures, which\nincreases their cost and complexity. This demo paper describes Sifaka, a new\nopen-source text mining application constructed above a standard search engine\nindex using existing application programmer interface (API) calls. Indexing\nintegrates popular annotation software libraries to augment the full-text index\nwith noun phrase and named-entities; n-grams are also provided. Sifaka enables\na person to quickly explore and analyze large text collections using search,\nfrequency analysis, and co-occurrence analysis; and import existing document\nlabels or interactively construct document sets that are positive or negative\nexamples of new concepts, perform feature selection, and export feature vectors\ncompatible with popular machine learning software. Sifaka demonstrates that\nsearch engines are good platforms for text mining applications while also\nmaking common IR text mining capabilities accessible to researchers in\ndisciplines where programming skills are less common.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 23:24:19 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["VandenBerg", "Cameron", ""], ["Callan", "Jamie", ""]]}, {"id": "1810.02938", "submitter": "Yi Tay", "authors": "Yi Tay, Luu Anh Tuan, Siu Cheung Hui", "title": "Co-Stack Residual Affinity Networks with Multi-level Attention\n  Refinement for Matching Text Sequences", "comments": "EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a matching function between two text sequences is a long standing\nproblem in NLP research. This task enables many potential applications such as\nquestion answering and paraphrase identification. This paper proposes Co-Stack\nResidual Affinity Networks (CSRAN), a new and universal neural architecture for\nthis problem. CSRAN is a deep architecture, involving stacked (multi-layered)\nrecurrent encoders. Stacked/Deep architectures are traditionally difficult to\ntrain, due to the inherent weaknesses such as difficulty with feature\npropagation and vanishing gradients. CSRAN incorporates two novel components to\ntake advantage of the stacked architecture. Firstly, it introduces a new\nbidirectional alignment mechanism that learns affinity weights by fusing\nsequence pairs across stacked hierarchies. Secondly, it leverages a multi-level\nattention refinement component between stacked recurrent layers. The key\nintuition is that, by leveraging information across all network hierarchies, we\ncan not only improve gradient flow but also improve overall performance. We\nconduct extensive experiments on six well-studied text sequence matching\ndatasets, achieving state-of-the-art performance on all.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 05:25:24 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Tay", "Yi", ""], ["Tuan", "Luu Anh", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1810.03067", "submitter": "Keith Harrigian", "authors": "Keith Harrigian", "title": "Geocoding Without Geotags: A Text-based Approach for reddit", "comments": "Accepted to the EMNLP Workshop on Noisy User-generated Text (W-NUT).\n  Brussels, Belgium. November 1, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we introduce the first geolocation inference approach for\nreddit, a social media platform where user pseudonymity has thus far made\nsupervised demographic inference difficult to implement and validate. In\nparticular, we design a text-based heuristic schema to generate ground truth\nlocation labels for reddit users in the absence of explicitly geotagged data.\nAfter evaluating the accuracy of our labeling procedure, we train and test\nseveral geolocation inference models across our reddit data set and three\nbenchmark Twitter geolocation data sets. Ultimately, we show that geolocation\nmodels trained and applied on the same domain substantially outperform models\nattempting to transfer training data across domains, even more so on reddit\nwhere platform-specific interest-group metadata can be used to improve\ninferences.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 01:46:27 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Harrigian", "Keith", ""]]}, {"id": "1810.03099", "submitter": "Hamid Mohammadi", "authors": "Hamid Mohammadi, Amin Nikoukaran", "title": "Multi-reference Cosine: A New Approach to Text Similarity Measurement in\n  Large Collections", "comments": "8 pages, 18 figures, 1 table, 3 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of an efficient and scalable document similarity detection\nsystem is undeniable nowadays. Search engines need batch text similarity\nmeasures to detect duplicated and near-duplicated web pages in their indexes in\norder to prevent indexing a web page multiple times. Furthermore, in the\nscoring phase, search engines need similarity measures to detect duplicated\ncontents on web pages so as to increase the quality of their results. In this\npaper, a new approach to batch text similarity detection is proposed by\ncombining some ideas from dimensionality reduction techniques and information\ngain theory. The new approach is focused on search engines need to detect\nduplicated and near-duplicated web pages. The new approach is evaluated on the\nNEWS20 dataset and the results show that the new approach is faster than the\ncosine text similarity algorithm in terms of speed and performance. On top of\nthat, It is faster and more accurate than the other rival method, Simhash\nsimilarity algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 08:04:18 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Mohammadi", "Hamid", ""], ["Nikoukaran", "Amin", ""]]}, {"id": "1810.03102", "submitter": "Hamid Mohammadi", "authors": "Hamid Mohammadi, Seyed Hossein Khasteh", "title": "A Fast Text Similarity Measure for Large Document Collections using\n  Multi-reference Cosine and Genetic Algorithm", "comments": "8 pages, 2 figures, 5 tables, 4 equation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the important factors that make a search engine fast and accurate is a\nconcise and duplicate free index. In order to remove duplicate and\nnear-duplicate documents from the index, a search engine needs a swift and\nreliable duplicate and near-duplicate text document detection system.\nTraditional approaches to this problem, such as brute force comparisons or\nsimple hash-based algorithms are not suitable as they are not scalable and are\nnot capable of detecting near-duplicate documents effectively. In this paper, a\nnew signature-based approach to text similarity detection is introduced which\nis fast, scalable, reliable and needs less storage space. The proposed method\nis examined on popular text document data-sets such as CiteseerX, Enron, Gold\nSet of Near-duplicate News Articles and etc. The results are promising and\ncomparable with the best cutting-edge algorithms, considering the accuracy and\nperformance. The proposed method is based on the idea of using reference texts\nto generate signatures for text documents. The novelty of this paper is the use\nof genetic algorithms to generate better reference texts.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 08:17:07 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 13:08:22 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 18:11:47 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Mohammadi", "Hamid", ""], ["Khasteh", "Seyed Hossein", ""]]}, {"id": "1810.03235", "submitter": "Pedro Saleiro", "authors": "Pedro Saleiro, Natasa Milic-Frayling, Eduarda Mendes Rodrigues, Carlos\n  Soares", "title": "Entity-Relationship Search over the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity-Relationship (E-R) Search is a complex case of Entity Search where the\ngoal is to search for multiple unknown entities and relationships connecting\nthem. We assume that a E-R query can be decomposed as a sequence of sub-queries\neach containing keywords related to a specific entity or relationship. We adopt\na probabilistic formulation of the E-R search problem. When creating specific\nrepresentations for entities (e.g. context terms) and for pairs of entities\n(i.e. relationships) it is possible to create a graph of probabilistic\ndependencies between sub-queries and entity plus relationship representations.\nTo the best of our knowledge this represents the first probabilistic model of\nE-R search. We propose and develop a novel supervised Early Fusion-based model\nfor E-R search, the Entity-Relationship Dependence Model (ERDM). It uses Markov\nRandom Field to model term dependencies of E-R sub-queries and\nentity/relationship documents. We performed experiments with more than 800M\nentities and relationships extractions from ClueWeb-09-B with FACC1 entity\nlinking. We obtained promising results using 3 different query collections\ncomprising 469 E-R queries, with results showing that it is possible to perform\nE-R search without using fix and pre-defined entity and relationship types,\nenabling a wide range of queries to be addressed.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 00:35:55 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Saleiro", "Pedro", ""], ["Milic-Frayling", "Natasa", ""], ["Rodrigues", "Eduarda Mendes", ""], ["Soares", "Carlos", ""]]}, {"id": "1810.03430", "submitter": "Mohd Zeeshan Ansari", "authors": "Mohd Zeeshan Ansari, Tanvir Ahmad and Md Arshad Ali", "title": "Cross Script Hindi English NER Corpus from Wikipedia", "comments": "International Conference on Intelligent Data Communication\n  Technologies and Internet of Things (ICICI-2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The text generated on social media platforms is essentially a mixed lingual\ntext. The mixing of language in any form produces considerable amount of\ndifficulty in language processing systems. Moreover, the advancements in\nlanguage processing research depends upon the availability of standard corpora.\nThe development of mixed lingual Indian Named Entity Recognition (NER) systems\nare facing obstacles due to unavailability of the standard evaluation corpora.\nSuch corpora may be of mixed lingual nature in which text is written using\nmultiple languages predominantly using a single script only. The motivation of\nour work is to emphasize the automatic generation such kind of corpora in order\nto encourage mixed lingual Indian NER. The paper presents the preparation of a\nCross Script Hindi-English Corpora from Wikipedia category pages. The corpora\nis successfully annotated using standard CoNLL-2003 categories of PER, LOC,\nORG, and MISC. Its evaluation is carried out on a variety of machine learning\nalgorithms and favorable results are achieved.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 13:25:05 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Ansari", "Mohd Zeeshan", ""], ["Ahmad", "Tanvir", ""], ["Ali", "Md Arshad", ""]]}, {"id": "1810.03519", "submitter": "Fl\\'avio Martins", "authors": "Fl\\'avio Martins, Jo\\~ao Magalh\\~aes, Jamie Callan", "title": "A Vertical PRF Architecture for Microblog Search", "comments": "To appear in ICTIR 2018", "journal-ref": null, "doi": "10.1145/3234944.3234960", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In microblog retrieval, query expansion can be essential to obtain good\nsearch results due to the short size of queries and posts. Since information in\nmicroblogs is highly dynamic, an up-to-date index coupled with pseudo-relevance\nfeedback (PRF) with an external corpus has a higher chance of retrieving more\nrelevant documents and improving ranking. In this paper, we focus on the\nresearch question:how can we reduce the query expansion computational cost\nwhile maintaining the same retrieval precision as standard PRF? Therefore, we\npropose to accelerate the query expansion step of pseudo-relevance feedback.\nThe hypothesis is that using an expansion corpus organized into verticals for\nexpanding the query, will lead to a more efficient query expansion process and\nimproved retrieval effectiveness. Thus, the proposed query expansion method\nuses a distributed search architecture and resource selection algorithms to\nprovide an efficient query expansion process. Experiments on the TREC Microblog\ndatasets show that the proposed approach can match or outperform standard PRF\nin MAP and NDCG@30, with a computational cost that is three orders of magnitude\nlower.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 15:07:42 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Martins", "Fl\u00e1vio", ""], ["Magalh\u00e3es", "Jo\u00e3o", ""], ["Callan", "Jamie", ""]]}, {"id": "1810.03918", "submitter": "Lokesh Kumar Sharma", "authors": "Lokesh Kumar Sharma and Namita Mittal", "title": "Answer Extraction in Question Answering using Structure Features and\n  Dependency Principles", "comments": "12 Pages, 11 Figures, 6 Tables, 4 Algorithms and IEEE Format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering (QA) research is a significant and challenging task in\nNatural Language Processing. QA aims to extract an exact answer from a relevant\ntext snippet or a document. The motivation behind QA research is the need of\nuser who is using state-of-the-art search engines. The user expects an exact\nanswer rather than a list of documents that probably contain the answer. In\nthis paper, for a successful answer extraction from relevant documents several\nefficient features and relations are required to extract. The features include\nvarious lexical, syntactic, semantic and structural features. The proposed\nstructural features are extracted from the dependency features of the question\nand supported document. Experimental results show that structural features\nimprove the accuracy of answer extraction when combined with the basic features\nand designed using dependency principles. Proposed structural features use new\ndesign principles which extract the long-distance relations. This addition is a\npossible reason behind the improvement in overall answer extraction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 11:25:32 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Sharma", "Lokesh Kumar", ""], ["Mittal", "Namita", ""]]}, {"id": "1810.03947", "submitter": "Pankaj Gupta", "authors": "Pankaj Gupta and Yatin Chaudhary and Florian Buettner and Hinrich\n  Sch\\\"utze", "title": "textTOvec: Deep Contextualized Neural Autoregressive Topic Models of\n  Language with Distributed Compositional Prior", "comments": "Published in #ICLR2019 International Conference on Learning\n  Representations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address two challenges of probabilistic topic modelling in order to better\nestimate the probability of a word in a given context, i.e., P(word|context):\n(1) No Language Structure in Context: Probabilistic topic models ignore word\norder by summarizing a given context as a \"bag-of-word\" and consequently the\nsemantics of words in the context is lost. The LSTM-LM learns a vector-space\nrepresentation of each word by accounting for word order in local collocation\npatterns and models complex characteristics of language (e.g., syntax and\nsemantics), while the TM simultaneously learns a latent representation from the\nentire document and discovers the underlying thematic structure. We unite two\ncomplementary paradigms of learning the meaning of word occurrences by\ncombining a TM (e.g., DocNADE) and a LM in a unified probabilistic framework,\nnamed as ctx-DocNADE. (2) Limited Context and/or Smaller training corpus of\ndocuments: In settings with a small number of word occurrences (i.e., lack of\ncontext) in short text or data sparsity in a corpus of few documents, the\napplication of TMs is challenging. We address this challenge by incorporating\nexternal knowledge into neural autoregressive topic models via a language\nmodelling approach: we use word embeddings as input of a LSTM-LM with the aim\nto improve the word-topic mapping on a smaller and/or short-text corpus. The\nproposed DocNADE extension is named as ctx-DocNADEe.\n  We present novel neural autoregressive topic model variants coupled with\nneural LMs and embeddings priors that consistently outperform state-of-the-art\ngenerative TMs in terms of generalization (perplexity), interpretability (topic\ncoherence) and applicability (retrieval and classification) over 6 long-text\nand 8 short-text datasets from diverse domains.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 13:04:25 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 11:29:00 GMT"}, {"version": "v3", "created": "Sun, 25 Nov 2018 11:40:26 GMT"}, {"version": "v4", "created": "Sat, 23 Feb 2019 14:14:05 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Gupta", "Pankaj", ""], ["Chaudhary", "Yatin", ""], ["Buettner", "Florian", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1810.03990", "submitter": "Lianlin Li Dr", "authors": "Lianlin Li, Long Gang Wang, Fernando L. Teixeira, Che Liu, Arye\n  Nehora, and Tie Jun Cui", "title": "DeepNIS: Deep Neural Network for Nonlinear Electromagnetic Inverse\n  Scattering", "comments": null, "journal-ref": null, "doi": "10.1109/TAP.2018.2885437", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear electromagnetic (EM) inverse scattering is a quantitative and\nsuper-resolution imaging technique, in which more realistic interactions\nbetween the internal structure of scene and EM wavefield are taken into account\nin the imaging procedure, in contrast to conventional tomography. However, it\nposes important challenges arising from its intrinsic strong nonlinearity,\nill-posedness, and expensive computation costs. To tackle these difficulties,\nwe, for the first time to our best knowledge, exploit a connection between the\ndeep neural network (DNN) architecture and the iterative method of nonlinear EM\ninverse scattering. This enables the development of a novel DNN-based\nmethodology for nonlinear EM inverse problems (termed here DeepNIS). The\nproposed DeepNIS consists of a cascade of multi-layer complexvalued residual\nconvolutional neural network (CNN) modules. We numerically and experimentally\ndemonstrate that the DeepNIS outperforms remarkably conventional nonlinear\ninverse scattering methods in terms of both the image quality and computational\ntime. We show that DeepNIS can learn a general model approximating the\nunderlying EM inverse scattering system. It is expected that the DeepNIS will\nserve as powerful tool in treating highly nonlinear EM inverse scattering\nproblems over different frequency bands, involving large-scale and\nhigh-contrast objects, which are extremely hard and impractical to solve using\nconventional inverse scattering methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 08:02:54 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Li", "Lianlin", ""], ["Wang", "Long Gang", ""], ["Teixeira", "Fernando L.", ""], ["Liu", "Che", ""], ["Nehora", "Arye", ""], ["Cui", "Tie Jun", ""]]}, {"id": "1810.04040", "submitter": "Chen Zhu", "authors": "Chen Zhu, Hengshu Zhu, Hui Xiong, Chao Ma, Fang Xie, Pengliang Ding,\n  Pan Li", "title": "Person-Job Fit: Adapting the Right Talent for the Right Job with Joint\n  Representation Learning", "comments": "16 pages, 5 figures", "journal-ref": "ACM Transactions on Management Information Systems (2018)", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person-Job Fit is the process of matching the right talent for the right job\nby identifying talent competencies that are required for the job. While many\nqualitative efforts have been made in related fields, it still lacks of\nquantitative ways of measuring talent competencies as well as the job's talent\nrequirements. To this end, in this paper, we propose a novel end-to-end\ndata-driven model based on Convolutional Neural Network (CNN), namely\nPerson-Job Fit Neural Network (PJFNN), for matching a talent qualification to\nthe requirements of a job. To be specific, PJFNN is a bipartite neural network\nwhich can effectively learn the joint representation of Person-Job fitness from\nhistorical job applications. In particular, due to the design of a hierarchical\nrepresentation structure, PJFNN can not only estimate whether a candidate fits\na job, but also identify which specific requirement items in the job posting\nare satisfied by the candidate by measuring the distances between corresponding\nlatent representations. Finally, the extensive experiments on a large-scale\nreal-world dataset clearly validate the performance of PJFNN in terms of\nPerson-Job Fit prediction. Also, we provide effective data visualization to\nshow some job and talent benchmark insights obtained by PJFNN.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 04:13:39 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Zhu", "Chen", ""], ["Zhu", "Hengshu", ""], ["Xiong", "Hui", ""], ["Ma", "Chao", ""], ["Xie", "Fang", ""], ["Ding", "Pengliang", ""], ["Li", "Pan", ""]]}, {"id": "1810.04111", "submitter": "Gon\\c{c}alo Marcelino", "authors": "Gon\\c{c}alo Marcelino, Ricardo Pinto, Jo\\~ao Magalh\\~aes", "title": "Ranking News-Quality Multimedia", "comments": "To appear in ICMR'18", "journal-ref": null, "doi": "10.1145/3206025.3206053", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News editors need to find the photos that best illustrate a news piece and\nfulfill news-media quality standards, while being pressed to also find the most\nrecent photos of live events. Recently, it became common to use social-media\ncontent in the context of news media for its unique value in terms of immediacy\nand quality. Consequently, the amount of images to be considered and filtered\nthrough is now too much to be handled by a person. To aid the news editor in\nthis process, we propose a framework designed to deliver high-quality,\nnews-press type photos to the user. The framework, composed of two parts, is\nbased on a ranking algorithm tuned to rank professional media highly and a\nvisual SPAM detection module designed to filter-out low-quality media. The core\nranking algorithm is leveraged by aesthetic, social and deep-learning semantic\nfeatures. Evaluation showed that the proposed framework is effective at finding\nhigh-quality photos (true-positive rate) achieving a retrieval MAP of 64.5% and\na classification precision of 70%.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 16:38:28 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Marcelino", "Gon\u00e7alo", ""], ["Pinto", "Ricardo", ""], ["Magalh\u00e3es", "Jo\u00e3o", ""]]}, {"id": "1810.04167", "submitter": "Carlos Lorenzetti", "authors": "Carlos M. Lorenzetti", "title": "Caracterizaci\\'on Formal y An\\'alisis Emp\\'irico de Mecanismos\n  Incrementales de B\\'usqueda basados en Contexto", "comments": "in Spanish", "journal-ref": "Lorenzetti C.M., Caracterizaci\\'on Formal y An\\'alisis Emp\\'irico\n  de Mecanismos Incrementales de B\\'usqueda basados en Contexto, 2011, PhD\n  Thesis, Universidad Nacional del Sur, Argentina", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Web has become a potentially infinite information resource, turning into\nan essential tool for many daily activities. This resulted in an increase in\nthe amount of information available in users' contexts that is not taken into\naccount by current information retrieval systems. This thesis proposes a\nsemisupervised information retrieval technique that helps users to recover\ncontext relevant information. The objective of the proposed technique is to\nreduce the vocabulary gap existing between the knowledge a user has about a\nspecific topic and the relevant documents available in the Web. This thesis\npresents a method for learning novel terms associated with a thematic context.\nThis is achieved by identifying those terms that are good descriptors and good\ndiscriminators of the user's current thematic context. In order to evaluate the\nproposed method, a theoretical framework for the evaluation of search\nmechanisms was developed. This served as a guide for the implementation of an\nevaluation framework that allowed to compare the techniques proposed in this\nthesis with other techniques existing in the literature. The experimental\nevidence indicates that the methods proposed in this thesis present significant\nimprovements over previously published techniques. In addition, the evaluation\nframework was equipped with novel evaluation metrics that favor the exploration\nof novel material and incorporates a semantic relationship metric between\ndocuments. The algorithms developed in this thesis evolve high quality queries,\nwhich have the capability of retrieving results that are relevant to the user\ncontext. These results have a positive impact on the way users interact with\navailable resources.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 12:36:17 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Lorenzetti", "Carlos M.", ""]]}, {"id": "1810.04401", "submitter": "Luca Rossetto M.Sc.", "authors": "Luca Rossetto, Heiko Schuldt, George Awad, Asad A. Butt", "title": "V3C - a Research Video Collection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread use of smartphones as recording devices and the massive\ngrowth in bandwidth, the number and volume of video collections has increased\nsignificantly in the last years. This poses novel challenges to the management\nof these large-scale video data and especially to the analysis of and retrieval\nfrom such video collections. At the same time, existing video datasets used for\nresearch and experimentation are either not large enough to represent current\ncollections or do not reflect the properties of video commonly found on the\nInternet in terms of content, length, or resolution. In this paper, we\nintroduce the Vimeo Creative Commons Collection, in short V3C, a collection of\n28'450 videos (with overall length of about 3'800 hours) published under\ncreative commons license on Vimeo. V3C comes with a shot segmentation for each\nvideo, together with the resulting keyframes in original as well as reduced\nresolution and additional metadata. It is intended to be used from 2019 at the\nInternational large-scale TREC Video Retrieval Evaluation campaign (TRECVid).\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 07:50:58 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 19:33:04 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Rossetto", "Luca", ""], ["Schuldt", "Heiko", ""], ["Awad", "George", ""], ["Butt", "Asad A.", ""]]}, {"id": "1810.04502", "submitter": "Diptesh Kanojia", "authors": "Diptesh Kanojia, Nikhil Wani, Pushpak Bhattacharyya", "title": "Is your Statement Purposeless? Predicting Computer Science Graduation\n  Admission Acceptance based on Statement Of Purpose", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a quantitative, data-driven machine learning approach to mitigate\nthe problem of unpredictability of Computer Science Graduate School Admissions.\nIn this paper, we discuss the possibility of a system which may help\nprospective applicants evaluate their Statement of Purpose (SOP) based on our\nsystem output. We, then, identify feature sets which can be used to train a\npredictive model. We train a model over fifty manually verified SOPs for which\nit uses an SVM classifier and achieves the highest accuracy of 92% with 10-fold\ncross-validation. We also perform experiments to establish that Word Embedding\nbased features and Document Similarity-based features outperform other\nidentified feature combinations. We plan to deploy our application as a web\nservice and release it as a FOSS service.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 05:07:51 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Kanojia", "Diptesh", ""], ["Wani", "Nikhil", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1810.04604", "submitter": "Vuong M. Ngo", "authors": "Sven Helmer and Vuong M. Ngo", "title": "A Similarity Measure for Weaving Patterns in Textiles", "comments": "10 papes, will be published in SIGIR 2015", "journal-ref": "SIGIR 2015", "doi": null, "report-no": null, "categories": "cs.DB cs.CV cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for measuring the similarity between weaving\npatterns that can provide similarity-based search functionality for textile\narchives. We represent textile structures using hypergraphs and extract\nmultisets of k-neighborhoods from these graphs. The resulting multisets are\nthen compared using Jaccard coefficients, Hamming distances, and cosine\nmeasures. We evaluate the different variants of our similarity measure\nexperimentally, showing that it can be implemented efficiently and illustrating\nits quality using it to cluster and query a data set containing more than a\nthousand textile samples.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 15:50:03 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Helmer", "Sven", ""], ["Ngo", "Vuong M.", ""]]}, {"id": "1810.04606", "submitter": "Sara Hosseinzadeh Kassani", "authors": "Sara Hosseinzadeh Kassani, Peyman Hosseinzadeh Kassani", "title": "Building an Ontology for the Domain of Plant Science using Prot\\'eg\\'e", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the rapid development of technology, large amounts of heterogeneous\ndata generated every day. Biological data is also growing in terms of the\nquantity and quality of data considerably. Despite the attempts for building a\nuniform platform to handle data management in Plant Science, researchers are\nfacing the challenge of not only accessing and integrating data stored in\nheterogeneous data sources but also representing the implicit and explicit\ndomain knowledge based on the available plant genomic and phenomic data.\nOntologies provide a framework for describing the structures and vocabularies\nto support the semantics of information and facilitate automated reasoning and\nknowledge discovery. In this paper, we focus on building an ontology for\nArabidopsis Thaliana in Plant Science domain. The aim of this study is to\nprovide a conceptual model of Arabidopsis Thaliana as a reference plant for\nbotany and other plant sciences, including concepts and their relationships.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 16:02:55 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 04:08:03 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Kassani", "Sara Hosseinzadeh", ""], ["Kassani", "Peyman Hosseinzadeh", ""]]}, {"id": "1810.04652", "submitter": "Huy Nguyen", "authors": "Eric Dodds, Huy Nguyen, Simao Herdade, Jack Culpepper, Andrew Kae,\n  Pierre Garrigues", "title": "Learning Embeddings for Product Visual Search with Triplet Loss and\n  Online Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose learning an embedding function for content-based\nimage retrieval within the e-commerce domain using the triplet loss and an\nonline sampling method that constructs triplets from within a minibatch. We\ncompare our method to several strong baselines as well as recent works on the\nDeepFashion and Stanford Online Product datasets. Our approach significantly\noutperforms the state-of-the-art on the DeepFashion dataset. With a\nmodification to favor sampling minibatches from a single product category, the\nsame approach demonstrates competitive results when compared to the\nstate-of-the-art for the Stanford Online Products dataset.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 17:19:08 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Dodds", "Eric", ""], ["Nguyen", "Huy", ""], ["Herdade", "Simao", ""], ["Culpepper", "Jack", ""], ["Kae", "Andrew", ""], ["Garrigues", "Pierre", ""]]}, {"id": "1810.04793", "submitter": "Kamran Kowsari", "authors": "Jinghe Zhang, Kamran Kowsari, James H. Harrison, Jennifer M. Lobo,\n  Laura E. Barnes", "title": "Patient2Vec: A Personalized Interpretable Deep Representation of the\n  Longitudinal Electronic Health Record", "comments": "Accepted by IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2018.2875677", "report-no": null, "categories": "q-bio.QM cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide implementation of electronic health record (EHR) systems facilitates\nthe collection of large-scale health data from real clinical settings. Despite\nthe significant increase in adoption of EHR systems, this data remains largely\nunexplored, but presents a rich data source for knowledge discovery from\npatient health histories in tasks such as understanding disease correlations\nand predicting health outcomes. However, the heterogeneity, sparsity, noise,\nand bias in this data present many complex challenges. This complexity makes it\ndifficult to translate potentially relevant information into machine learning\nalgorithms. In this paper, we propose a computational framework, Patient2Vec,\nto learn an interpretable deep representation of longitudinal EHR data which is\npersonalized for each patient. To evaluate this approach, we apply it to the\nprediction of future hospitalizations using real EHR data and compare its\npredictive performance with baseline methods. Patient2Vec produces a vector\nspace with meaningful structure and it achieves an AUC around 0.799\noutperforming baseline methods. In the end, the learned feature importance can\nbe visualized and interpreted at both the individual and population levels to\nbring clinical insights.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 16:41:05 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 15:13:16 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2018 13:38:34 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Zhang", "Jinghe", ""], ["Kowsari", "Kamran", ""], ["Harrison", "James H.", ""], ["Lobo", "Jennifer M.", ""], ["Barnes", "Laura E.", ""]]}, {"id": "1810.04956", "submitter": "Diego Monti", "authors": "Diego Monti, Enrico Palumbo, Giuseppe Rizzo, Maurizio Morisio", "title": "Sequeval: A Framework to Assess and Benchmark Sequence-based Recommender\n  Systems", "comments": "REVEAL 2018 Workshop on Offline Evaluation for Recommender Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present sequeval, a software tool capable of performing the\noffline evaluation of a recommender system designed to suggest a sequence of\nitems. A sequence-based recommender is trained considering the sequences\nalready available in the system and its purpose is to generate a personalized\nsequence starting from an initial seed. This tool automatically evaluates the\nsequence-based recommender considering a comprehensive set of eight different\nmetrics adapted to the sequential scenario. sequeval has been developed\nfollowing the best practices of software extensibility. For this reason, it is\npossible to easily integrate and evaluate novel recommendation techniques.\nsequeval is publicly available as an open source tool and it aims to become a\nfocal point for the community to assess sequence-based recommender systems.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 11:24:53 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 09:43:04 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Monti", "Diego", ""], ["Palumbo", "Enrico", ""], ["Rizzo", "Giuseppe", ""], ["Morisio", "Maurizio", ""]]}, {"id": "1810.04957", "submitter": "Diego Monti", "authors": "Diego Monti, Giuseppe Rizzo, Maurizio Morisio", "title": "A Distributed and Accountable Approach to Offline Recommender Systems\n  Evaluation", "comments": "REVEAL 2018 Workshop on Offline Evaluation for Recommender Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different software tools have been developed with the purpose of performing\noffline evaluations of recommender systems. However, the results obtained with\nthese tools may be not directly comparable because of subtle differences in the\nexperimental protocols and metrics. Furthermore, it is difficult to analyze in\nthe same experimental conditions several algorithms without disclosing their\nimplementation details. For these reasons, we introduce RecLab, an open source\nsoftware for evaluating recommender systems in a distributed fashion. By\nrelying on consolidated web protocols, we created RESTful APIs for training and\nquerying recommenders remotely. In this way, it is possible to easily integrate\ninto the same toolkit algorithms realized with different technologies. In\ndetails, the experimenter can perform an evaluation by simply visiting a web\ninterface provided by RecLab. The framework will then interact with all the\nselected recommenders and it will compute and display a comprehensive set of\nmeasures, each representing a different metric. The results of all experiments\nare permanently stored and publicly available in order to support\naccountability and comparative analyses.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 11:31:09 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Monti", "Diego", ""], ["Rizzo", "Giuseppe", ""], ["Morisio", "Maurizio", ""]]}, {"id": "1810.05032", "submitter": "Xiaoyan Gao", "authors": "Xiaoyan Gao, Fuli Feng, Xiangnan He, Heyan Huang, Xinyu Guan, Chong\n  Feng, Zhaoyan Ming, Tat-Seng Chua", "title": "Hierarchical Attention Network for Visually-aware Food Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food recommender systems play an important role in assisting users to\nidentify the desired food to eat. Deciding what food to eat is a complex and\nmulti-faceted process, which is influenced by many factors such as the\ningredients, appearance of the recipe, the user's personal preference on food,\nand various contexts like what had been eaten in the past meals. In this work,\nwe formulate the food recommendation problem as predicting user preference on\nrecipes based on three key factors that determine a user's choice on food,\nnamely, 1) the user's (and other users') history; 2) the ingredients of a\nrecipe; and 3) the descriptive image of a recipe. To address this challenging\nproblem, we develop a dedicated neural network based solution Hierarchical\nAttention based Food Recommendation (HAFR) which is capable of: 1) capturing\nthe collaborative filtering effect like what similar users tend to eat; 2)\ninferring a user's preference at the ingredient level; and 3) learning user\npreference from the recipe's visual images. To evaluate our proposed method, we\nconstruct a large-scale dataset consisting of millions of ratings from\nAllRecipes.com. Extensive experiments show that our method outperforms several\ncompeting recommender solutions like Factorization Machine and Visual Bayesian\nPersonalized Ranking with an average improvement of 12%, offering promising\nresults in predicting user preference for food. Codes and dataset will be\nreleased upon acceptance.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 14:02:43 GMT"}, {"version": "v2", "created": "Fri, 12 Oct 2018 01:41:24 GMT"}, {"version": "v3", "created": "Sun, 6 Jan 2019 10:20:50 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Gao", "Xiaoyan", ""], ["Feng", "Fuli", ""], ["He", "Xiangnan", ""], ["Huang", "Heyan", ""], ["Guan", "Xinyu", ""], ["Feng", "Chong", ""], ["Ming", "Zhaoyan", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1810.05102", "submitter": "Pankaj Gupta", "authors": "Pankaj Gupta and Subburam Rajaram and Hinrich Sch\\\"utze and Bernt\n  Andrassy and Thomas Runkler", "title": "Neural Relation Extraction Within and Across Sentence Boundaries", "comments": "AAAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past work in relation extraction mostly focuses on binary relation between\nentity pairs within single sentence. Recently, the NLP community has gained\ninterest in relation extraction in entity pairs spanning multiple sentences. In\nthis paper, we propose a novel architecture for this task: inter-sentential\ndependency-based neural networks (iDepNN). iDepNN models the shortest and\naugmented dependency paths via recurrent and recursive neural networks to\nextract relationships within (intra-) and across (inter-) sentence boundaries.\nCompared to SVM and neural network baselines, iDepNN is more robust to false\npositives in relationships spanning sentences.\n  We evaluate our models on four datasets from newswire (MUC6) and medical\n(BioNLP shared task) domains that achieve state-of-the-art performance and show\na better balance in precision and recall for inter-sentential relationships. We\nperform better than 11 teams participating in the BioNLP shared task 2016 and\nachieve a gain of 5.2% (0.587 vs 0.558) in F1 over the winning team. We also\nrelease the crosssentence annotations for MUC6.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 16:07:20 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 16:56:53 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Gupta", "Pankaj", ""], ["Rajaram", "Subburam", ""], ["Sch\u00fctze", "Hinrich", ""], ["Andrassy", "Bernt", ""], ["Runkler", "Thomas", ""]]}, {"id": "1810.05187", "submitter": "Faiz Ali Shah", "authors": "Faiz Ali Shah, Kairit Sirts, Dietmar Pfahl", "title": "The Impact of Annotation Guidelines and Annotated Data on Extracting App\n  Features from App Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotation guidelines used to guide the annotation of training and evaluation\ndatasets can have a considerable impact on the quality of machine learning\nmodels. In this study, we explore the effects of annotation guidelines on the\nquality of app feature extraction models. As a main result, we propose several\nchanges to the existing annotation guidelines with a goal of making the\nextracted app features more useful and informative to the app developers. We\ntest the proposed changes via simulating the application of the new annotation\nguidelines and then evaluating the performance of the supervised machine\nlearning models trained on datasets annotated with initial and simulated\nguidelines. While the overall performance of automatic app feature extraction\nremains the same as compared to the model trained on the dataset with initial\nannotations, the features extracted by the model trained on the dataset with\nsimulated new annotations are less noisy and more informative to the app\ndevelopers. Secondly, we are interested in what kind of annotated training data\nis necessary for training an automatic app feature extraction model. In\nparticular, we explore whether the training set should contain annotated app\nreviews from those apps/app categories on which the model is subsequently\nplanned to be applied, or is it sufficient to have annotated app reviews from\nany app available for training, even when these apps are from very different\ncategories compared to the test app. Our experiments show that having annotated\ntraining reviews from the test app is not necessary although including them\ninto training set helps to improve recall. Furthermore, we test whether\naugmenting the training set with annotated product reviews helps to improve the\nperformance of app feature extraction. We find that the models trained on\naugmented training set lead to improved recall but at the cost of the drop in\nprecision.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 18:07:14 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Shah", "Faiz Ali", ""], ["Sirts", "Kairit", ""], ["Pfahl", "Dietmar", ""]]}, {"id": "1810.05252", "submitter": "Michael Bendersky", "authors": "Aman Agarwal, Xuanhui Wang, Cheng Li, Michael Bendersky, Marc Najork", "title": "Offline Comparison of Ranking Functions using Randomized Data", "comments": "Published at REVEAL Workshop in ACM Recommender Systems (RecSys)\n  (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking functions return ranked lists of items, and users often interact with\nthese items. How to evaluate ranking functions using historical interaction\nlogs, also known as off-policy evaluation, is an important but challenging\nproblem. The commonly used Inverse Propensity Scores (IPS) approaches work\nbetter for the single item case, but suffer from extremely low data efficiency\nfor the ranked list case. In this paper, we study how to improve the data\nefficiency of IPS approaches in the offline comparison setting. We propose two\napproaches Trunc-match and Rand-interleaving for offline comparison using\nuniformly randomized data. We show that these methods can improve the data\nefficiency and also the comparison sensitivity based on one of the largest\nemail search engines.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 21:19:05 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Agarwal", "Aman", ""], ["Wang", "Xuanhui", ""], ["Li", "Cheng", ""], ["Bendersky", "Michael", ""], ["Najork", "Marc", ""]]}, {"id": "1810.05376", "submitter": "Teng Xiao", "authors": "Teng Xiao and Shangsong Liang and Hong Shen and Zaiqiao Meng", "title": "Neural Variational Hybrid Collaborative Filtering", "comments": "7 pages, fix some typos (Eq.7)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Filtering (CF) is one of the most used methods for Recommender\nSystem. Because of the Bayesian nature and nonlinearity, deep generative\nmodels, e.g. Variational Autoencoder (VAE), have been applied into CF task, and\nhave achieved great performance. However, most VAE-based methods suffer from\nmatrix sparsity and consider the prior of users' latent factors to be the same,\nwhich leads to poor latent representations of users and items. Additionally,\nmost existing methods model latent factors of users only and but not items,\nwhich makes them not be able to recommend items to a new user. To tackle these\nproblems, we propose a Neural Variational Hybrid Collaborative Filtering,\nNVHCF. Specifically, we consider both the generative processes of users and\nitems, and the prior of latent factors of users and items to be side\ninformationspecific, which enables our model to alleviate matrix sparsity and\nlearn better latent representations of users and items. For inference purpose,\nwe derived a Stochastic Gradient Variational Bayes (SGVB) algorithm to\nanalytically approximate the intractable distributions of latent factors of\nusers and items. Experiments conducted on two large datasets have showed our\nmethods significantly outperform the state-of-the-art CF methods, including the\nVAE-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 06:48:35 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 02:31:53 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 16:03:28 GMT"}, {"version": "v4", "created": "Sat, 3 Nov 2018 05:35:27 GMT"}, {"version": "v5", "created": "Tue, 4 Dec 2018 07:18:48 GMT"}, {"version": "v6", "created": "Mon, 25 Feb 2019 05:46:57 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Xiao", "Teng", ""], ["Liang", "Shangsong", ""], ["Shen", "Hong", ""], ["Meng", "Zaiqiao", ""]]}, {"id": "1810.05414", "submitter": "Jie Zou", "authors": "Jie Zou, Dan Li and Evangelos Kanoulas", "title": "Technology Assisted Reviews: Finding the Last Few Relevant Documents by\n  Asking Yes/No Questions to Reviewers", "comments": "This paper is accepted by SIGIR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of a technology-assisted review is to achieve high recall with low\nhuman effort. Continuous active learning algorithms have demonstrated good\nperformance in locating the majority of relevant documents in a collection,\nhowever their performance is reaching a plateau when 80\\%-90\\% of them has been\nfound. Finding the last few relevant documents typically requires exhaustively\nreviewing the collection. In this paper, we propose a novel method to identify\nthese last few, but significant, documents efficiently. Our method makes the\nhypothesis that entities carry vital information in documents, and that\nreviewers can answer questions about the presence or absence of an entity in\nthe missing relevance documents. Based on this we devise a sequential Bayesian\nsearch method that selects the optimal sequence of questions to ask. The\nexperimental results show that our proposed method can greatly improve\nperformance requiring less reviewing effort.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 08:59:04 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Zou", "Jie", ""], ["Li", "Dan", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "1810.05436", "submitter": "Hosein Azarbonyad", "authors": "Hosein Azarbonyad, Mostafa Dehghani, Tom Kenter, Maarten Marx, Jaap\n  Kamps, and Maarten de Rijke", "title": "HiTR: Hierarchical Topic Model Re-estimation for Measuring Topical\n  Diversity of Documents", "comments": "IEEE Transactions on Knowledge and Data Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A high degree of topical diversity is often considered to be an important\ncharacteristic of interesting text documents. A recent proposal for measuring\ntopical diversity identifies three distributions for assessing the diversity of\ndocuments: distributions of words within documents, words within topics, and\ntopics within documents. Topic models play a central role in this approach and,\nhence, their quality is crucial to the efficacy of measuring topical diversity.\nThe quality of topic models is affected by two causes: generality and impurity\nof topics. General topics only include common information of a background\ncorpus and are assigned to most of the documents. Impure topics contain words\nthat are not related to the topic. Impurity lowers the interpretability of\ntopic models. Impure topics are likely to get assigned to documents\nerroneously. We propose a hierarchical re-estimation process aimed at removing\ngenerality and impurity. Our approach has three re-estimation components: (1)\ndocument re-estimation, which removes general words from the documents; (2)\ntopic re-estimation, which re-estimates the distribution over words of each\ntopic; and (3) topic assignment re-estimation, which re-estimates for each\ndocument its distributions over topics. For measuring topical diversity of text\ndocuments, our HiTR approach improves over the state-of-the-art measured on\nPubMed dataset.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 10:02:23 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Azarbonyad", "Hosein", ""], ["Dehghani", "Mostafa", ""], ["Kenter", "Tom", ""], ["Marx", "Maarten", ""], ["Kamps", "Jaap", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1810.05642", "submitter": "Robert Krajewski", "authors": "Robert Krajewski, Julian Bock, Laurent Kloeker and Lutz Eckstein", "title": "The highD Dataset: A Drone Dataset of Naturalistic Vehicle Trajectories\n  on German Highways for Validation of Highly Automated Driving Systems", "comments": "IEEE International Conference on Intelligent Transportation Systems\n  (ITSC) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scenario-based testing for the safety validation of highly automated vehicles\nis a promising approach that is being examined in research and industry. This\napproach heavily relies on data from real-world scenarios to derive the\nnecessary scenario information for testing. Measurement data should be\ncollected at a reasonable effort, contain naturalistic behavior of road users\nand include all data relevant for a description of the identified scenarios in\nsufficient quality. However, the current measurement methods fail to meet at\nleast one of the requirements. Thus, we propose a novel method to measure data\nfrom an aerial perspective for scenario-based validation fulfilling the\nmentioned requirements. Furthermore, we provide a large-scale naturalistic\nvehicle trajectory dataset from German highways called highD. We evaluate the\ndata in terms of quantity, variety and contained scenarios. Our dataset\nconsists of 16.5 hours of measurements from six locations with 110 000\nvehicles, a total driven distance of 45 000 km and 5600 recorded complete lane\nchanges. The highD dataset is available online at: http://www.highD-dataset.com\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 22:47:33 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Krajewski", "Robert", ""], ["Bock", "Julian", ""], ["Kloeker", "Laurent", ""], ["Eckstein", "Lutz", ""]]}, {"id": "1810.05784", "submitter": "Brendan Whitaker", "authors": "Luann Jung, Brendan Whitaker, Kyle Chard, Aaron Elmore", "title": "Measuring Swampiness: Quantifying Chaos in Large Heterogeneous Data\n  Repositories", "comments": "In Proceedings of ACM Student Poster Competition (SC'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As scientific data repositories and filesystems grow in size and complexity,\nthey become increasingly disorganized. The coupling of massive quantities of\ndata with poor organization makes it challenging for scientists to locate and\nutilize relevant data, thus slowing the process of analyzing data of interest.\nTo address these issues, we explore an automated clustering approach for\nquantifying the organization of data repositories. Our parallel pipeline\nprocesses heterogeneous filetypes (e.g., text and tabular data), automatically\nclusters files based on content and metadata similarities, and computes a novel\n\"cleanliness\" score from the resulting clustering. We demonstrate the\ngeneration and accuracy of our cleanliness measure using both synthetic and\nreal datasets, and conclude that it is more consistent than other potential\ncleanliness measures.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 01:59:49 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Jung", "Luann", ""], ["Whitaker", "Brendan", ""], ["Chard", "Kyle", ""], ["Elmore", "Aaron", ""]]}, {"id": "1810.06185", "submitter": "Ming-Hsiang Tsou", "authors": "Ming-Hsiang Tsou and Daniel Lusher", "title": "Mapping Web Pages by Internet Protocol (IP) addresses: Analyzing Spatial\n  and Temporal Characteristics of Web Search Engine Results", "comments": "Proceedings of the International Symposium on Cartography in Internet\n  and Ubiquitous Environments 2015, 17th - 19th March, The University of Tokyo,\n  Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.NI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet Protocol (IP) addresses are frequently used as a method of locating\nweb users by researchers in several different fields. However, there are\ncompeting reports concerning the accuracy of those locations, and little\nresearch has been done in manually comparing the IP geolocation databases and\nweb page geographic information. This paper categorized web page from the Yahoo\nsearch engine into twelve categories, ranging from 'Blog' and 'News' to\n'Education' and 'Governmental'. Then we manually compared the mailing or street\naddress of the web page's content creator with the geolocation results by the\ngiven IP address. We introduced a cartographic design method by creating kernel\ndensity maps for visualizing the information landscape of web pages associated\nwith specific keywords.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 05:23:00 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Tsou", "Ming-Hsiang", ""], ["Lusher", "Daniel", ""]]}, {"id": "1810.06306", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen, Richard Billingsley, Lan Du, Mark Johnson", "title": "Improving Topic Models with Latent Feature Word Representations", "comments": "The published version is available at:\n  https://transacl.org/ojs/index.php/tacl/article/view/582 ; The source code is\n  available at: https://github.com/datquocnguyen/LFTM", "journal-ref": "Transactions of the Association for Computational Linguistics,\n  vol. 3, pp. 299-313, 2015", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Probabilistic topic models are widely used to discover latent topics in\ndocument collections, while latent feature vector representations of words have\nbeen used to obtain high performance in many NLP tasks. In this paper, we\nextend two different Dirichlet multinomial topic models by incorporating latent\nfeature vector representations of words trained on very large corpora to\nimprove the word-topic mapping learnt on a smaller corpus. Experimental results\nshow that by using information from the external corpora, our new models\nproduce significant improvements on topic coherence, document clustering and\ndocument classification tasks, especially on datasets with few or short\ndocuments.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 12:34:05 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Nguyen", "Dat Quoc", ""], ["Billingsley", "Richard", ""], ["Du", "Lan", ""], ["Johnson", "Mark", ""]]}, {"id": "1810.06313", "submitter": "Linqi Song", "authors": "Linqi Song, Christina Fragouli, Devavrat Shah", "title": "Regret vs. Bandwidth Trade-off for Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider recommendation systems that need to operate under wireless\nbandwidth constraints, measured as number of broadcast transmissions, and\ndemonstrate a (tight for some instances) tradeoff between regret and bandwidth\nfor two scenarios: the case of multi-armed bandit with context, and the case\nwhere there is a latent structure in the message space that we can exploit to\nreduce the learning phase.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 12:41:25 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Song", "Linqi", ""], ["Fragouli", "Christina", ""], ["Shah", "Devavrat", ""]]}, {"id": "1810.06665", "submitter": "Ahmed Elnaggar", "authors": "Ahmed Elnaggar, Bernhard Waltl, Ingo Glaser, J\\\"org Landthaler, Elena\n  Scepankova and Florian Matthes", "title": "Stop Illegal Comments: A Multi-Task Deep Learning Approach", "comments": "10 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods are often difficult to apply in the legal domain due to\nthe large amount of labeled data required by deep learning methods. A recent\nnew trend in the deep learning community is the application of multi-task\nmodels that enable single deep neural networks to perform more than one task at\nthe same time, for example classification and translation tasks. These powerful\nnovel models are capable of transferring knowledge among different tasks or\ntraining sets and therefore could open up the legal domain for many deep\nlearning applications. In this paper, we investigate the transfer learning\ncapabilities of such a multi-task model on a classification task on the\npublicly available Kaggle toxic comment dataset for classifying illegal\ncomments and we can report promising results.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 20:22:44 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Elnaggar", "Ahmed", ""], ["Waltl", "Bernhard", ""], ["Glaser", "Ingo", ""], ["Landthaler", "J\u00f6rg", ""], ["Scepankova", "Elena", ""], ["Matthes", "Florian", ""]]}, {"id": "1810.06818", "submitter": "Xiaoshi Zhong", "authors": "Xiaoshi Zhong and Erik Cambria and Jagath C. Rajapakse", "title": "Named Entity Analysis and Extraction with Uncommon Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most previous research treats named entity extraction and classification as\nan end-to-end task. We argue that the two sub-tasks should be addressed\nseparately. Entity extraction lies at the level of syntactic analysis while\nentity classification lies at the level of semantic analysis. According to Noam\nChomsky's \"Syntactic Structures,\" pp. 93-94 (Chomsky 1957), syntax is not\nappealed to semantics and semantics does not affect syntax. We analyze two\nbenchmark datasets for the characteristics of named entities, finding that\nuncommon words can distinguish named entities from common text; where uncommon\nwords are the words that hardly appear in common text and they are mainly the\nproper nouns. Experiments validate that lexical and syntactic features achieve\nstate-of-the-art performance on entity extraction and that semantic features do\nnot further improve the extraction performance, in both of our model and the\nstate-of-the-art baselines. With Chomsky's view, we also explain the failure of\njoint syntactic and semantic parsings in other works.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 05:40:03 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 11:57:12 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Zhong", "Xiaoshi", ""], ["Cambria", "Erik", ""], ["Rajapakse", "Jagath C.", ""]]}, {"id": "1810.07237", "submitter": "Hyunji Chung", "authors": "Hyunji Chung", "title": "A Retrieval Framework and Implementation for Electronic Documents with\n  Similar Layouts", "comments": "21 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the number of digital documents requiring investigation increases, it has\nbecome more important to identify relevant documents to a given case. There\nhave been continual demands for finding relevant files in order to overcome\nthis kind of issues. Regarding finding similar files, there can be a situation\nwhere there is no available metadata such as timestamp, file size, title,\nsubject, template, author, etc. In this situation, investigators will focus on\nsearching document files having specific keywords related to a given case.\nAlthough the traditional keyword search with elaborate regular expressions is\nuseful for digital forensics, there is a possibility that closely related\ndocuments are missing because they have totally different body contents. In\nthis paper, we introduce a recent actual case on handling large amounts of\ndocument files. This case suggests that similar layout search will be useful\nfor more efficient digital investigations if it can be utilized appropriately\nfor supplementing results of the traditional keyword search. Until now,\nresearch involving electronic-document similarity has mainly focused on byte\nstreams, format structures and body contents. However, there has been little\nresearch on the similarity of visual layouts from the viewpoint of digital\nforensics. In order to narrow this gap, this study demonstrates a novel\nframework for retrieving electronic document files having similar layouts, and\nimplements a tool for finding similar Microsoft OOXML files using\nuser-controlled layout queries based on the framework.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 19:10:50 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Chung", "Hyunji", ""]]}, {"id": "1810.07355", "submitter": "Masajiro Iwasaki", "authors": "Masajiro Iwasaki and Daisuke Miyazaki", "title": "Optimization of Indexing Based on k-Nearest Neighbor Graph for Proximity\n  Search in High-dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for high-dimensional vector data with high accuracy is an\ninevitable search technology for various types of data. Graph-based indexes are\nknown to reduce the query time for high-dimensional data. To further improve\nthe query time by using graphs, we focused on the indegrees and outdegrees of\ngraphs. While a sufficient number of incoming edges (indegrees) are\nindispensable for increasing search accuracy, an excessive number of outgoing\nedges (outdegrees) should be suppressed so as to not increase the query time.\nTherefore, we propose three degree-adjustment methods: static degree adjustment\nof not only outdegrees but also indegrees, dynamic degree adjustment with which\noutdegrees are determined by the search accuracy users require, and path\nadjustment to remove edges that have alternative search paths to reduce\noutdegrees. We also show how to obtain optimal degree-adjustment parameters and\nthat our methods outperformed previous methods for image and textual data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 02:22:34 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Iwasaki", "Masajiro", ""], ["Miyazaki", "Daisuke", ""]]}, {"id": "1810.07382", "submitter": "Kamran Kowsari", "authors": "Mojtaba Heidarysafa, Kamran Kowsari, Laura E. Barnes and Donald E.\n  Brown", "title": "Analysis of Railway Accidents' Narratives Using Deep Learning", "comments": "accepted in IEEE International Conference on Machine Learning and\n  Applications (IEEE ICMLA)", "journal-ref": null, "doi": "10.1109/ICMLA.2018.00235", "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic understanding of domain specific texts in order to extract useful\nrelationships for later use is a non-trivial task. One such relationship would\nbe between railroad accidents' causes and their correspondent descriptions in\nreports. From 2001 to 2016 rail accidents in the U.S. cost more than $4.6B.\nRailroads involved in accidents are required to submit an accident report to\nthe Federal Railroad Administration (FRA). These reports contain a variety of\nfixed field entries including primary cause of the accidents (a coded variable\nwith 389 values) as well as a narrative field which is a short text description\nof the accident. Although these narratives provide more information than a\nfixed field entry, the terminologies used in these reports are not easy to\nunderstand by a non-expert reader. Therefore, providing an assisting method to\nfill in the primary cause from such domain specific texts(narratives) would\nhelp to label the accidents with more accuracy. Another important question for\ntransportation safety is whether the reported accident cause is consistent with\nnarrative description. To address these questions, we applied deep learning\nmethods together with powerful word embeddings such as Word2Vec and GloVe to\nclassify accident cause values for the primary cause field using the text in\nthe narratives. The results show that such approaches can both accurately\nclassify accident causes based on report narratives and find important\ninconsistencies in accident reporting.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 04:30:02 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 22:08:21 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 16:16:48 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Heidarysafa", "Mojtaba", ""], ["Kowsari", "Kamran", ""], ["Barnes", "Laura E.", ""], ["Brown", "Donald E.", ""]]}, {"id": "1810.07513", "submitter": "Ahmed Elnaggar", "authors": "Ahmed Elnaggar, Christoph Gebendorfer, Ingo Glaser and Florian Matthes", "title": "Multi-Task Deep Learning for Legal Document Translation, Summarization\n  and Multi-Label Classification", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digitalization of the legal domain has been ongoing for a couple of\nyears. In that process, the application of different machine learning (ML)\ntechniques is crucial. Tasks such as the classification of legal documents or\ncontract clauses as well as the translation of those are highly relevant. On\nthe other side, digitized documents are barely accessible in this field,\nparticularly in Germany. Today, deep learning (DL) is one of the hot topics\nwith many publications and various applications. Sometimes it provides results\noutperforming the human level. Hence this technique may be feasible for the\nlegal domain as well. However, DL requires thousands of samples to provide\ndecent results. A potential solution to this problem is multi-task DL to enable\ntransfer learning. This approach may be able to overcome the data scarcity\nproblem in the legal domain, specifically for the German language. We applied\nthe state of the art multi-task model on three tasks: translation,\nsummarization, and multi-label classification. The experiments were conducted\non legal document corpora utilizing several task combinations as well as\nvarious model parameters. The goal was to find the optimal configuration for\nthe tasks at hand within the legal domain. The multi-task DL approach\noutperformed the state of the art results in all three tasks. This opens a new\ndirection to integrate DL technology more efficiently in the legal domain.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 08:54:50 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Elnaggar", "Ahmed", ""], ["Gebendorfer", "Christoph", ""], ["Glaser", "Ingo", ""], ["Matthes", "Florian", ""]]}, {"id": "1810.07767", "submitter": "Suryanto Nugroho", "authors": "Suryanto Nugroho, Prihandoko", "title": "Architecture of Text Mining Application in Analyzing Public Sentiments\n  of West Java Governor Election using Naive Bayes Classification", "comments": "5 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The selection of West Java governor is one event that seizes the attention of\nthe public is no exception to social media users. Public opinion on a\nprospective regional leader can help predict electability and tendency of\nvoters. Data that can be used by the opinion mining process can be obtained\nfrom Twitter. Because the data is very varied form and very unstructured, it\nmust be managed and uninformed using data pre-processing techniques into\nsemi-structured data. This semi-structured information is followed by a\nclassification stage to categorize the opinion into negative or positive\nopinions. The research methodology uses a literature study where the research\nwill examine previous research on a similar topic. The purpose of this study is\nto find the right architecture to develop it into the application of twitter\nopinion mining to know public sentiments toward the election of the governor of\nwest java. The result of this research is that Twitter opinion mining is part\nof text mining where opinions in Twitter if they want to be classified, must go\nthrough the preprocessing text stage first. The preprocessing step required\nfrom twitter data is cleansing, case folding, POS Tagging and stemming. The\nresulting text mining architecture is an architecture that can be used for text\nmining research with different topics.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 07:14:10 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Nugroho", "Suryanto", ""], ["Prihandoko", "", ""]]}, {"id": "1810.08047", "submitter": "Sepanta Zeighami", "authors": "Sepanta Zeighami and Raymong Chi-Wing Wong", "title": "Finding Average Regret Ratio Minimizing Set in Database", "comments": "Submitted to ICDE '19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting a certain number of data points (or records) from a database which\n\"best\" satisfy users' expectations is a very prevalent problem with many\napplications. One application is a hotel booking website showing a certain\nnumber of hotels on a single page. However, this problem is very challenging\nsince the selected points should \"collectively\" satisfy the expectation of all\nusers. Showing a certain number of data points to a single user could decrease\nthe satisfaction of a user because the user may not be able to see his/her\nfavorite point which could be found in the original database. In this paper, we\nwould like to find a set of k points such that on average, the satisfaction\n(ratio) of a user is maximized. This problem takes into account the probability\ndistribution of the users and considers the satisfaction (ratio) of all users,\nwhich is more reasonable in practice, compared with the existing studies that\nonly consider the worst-case satisfaction (ratio) of the users, which may not\nreflect the whole population and is not useful in some applications. Motivated\nby this, in this paper, we propose algorithms for this problem. Finally, we\nconducted experiments to show the effectiveness and the efficiency of the\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 13:42:04 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Zeighami", "Sepanta", ""], ["Wong", "Raymong Chi-Wing", ""]]}, {"id": "1810.08189", "submitter": "Cheng Kang Hsieh", "authors": "Cheng-Kang Hsieh, Miguel Campo, Abhinav Taliyan, Matt Nickens,\n  Mitkumar Pandya, JJ Espinoza", "title": "Convolutional Collaborative Filter Network for Video Based\n  Recommendation Systems", "comments": "8 pages, 3 figures, 1 table include ablation study. arguments /\n  results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This analysis explores the temporal sequencing of objects in a movie trailer.\nTemporal sequencing of objects in a movie trailer (e.g., a long shot of an\nobject vs intermittent short shots) can convey information about the type of\nmovie, plot of the movie, role of the main characters, and the filmmakers\ncinematographic choices. When combined with historical customer data,\nsequencing analysis can be used to improve predictions of customer behavior.\nE.g., a customer buys tickets to a new movie and maybe the customer has seen\nmovies in the past that contained similar sequences. To explore object\nsequencing in movie trailers, we propose a video convolutional network to\ncapture actions and scenes that are predictive of customers' preferences. The\nmodel learns the specific nature of sequences for different types of objects\n(e.g., cars vs faces), and the role of sequences in predicting customer future\nbehavior. We show how such a temporal-aware model outperforms simple feature\npooling methods proposed in our previous works and, importantly, demonstrate\nthe additional model explain-ability allowed by such a model.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 17:57:58 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 20:43:16 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Hsieh", "Cheng-Kang", ""], ["Campo", "Miguel", ""], ["Taliyan", "Abhinav", ""], ["Nickens", "Matt", ""], ["Pandya", "Mitkumar", ""], ["Espinoza", "JJ", ""]]}, {"id": "1810.08223", "submitter": "Muhammad Asiful Islam", "authors": "Muhammad Asiful Islam, Ramakrishnan Srikant, Sugato Basu", "title": "Micro-Browsing Models for Search Snippets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) is a key signal of relevance for search engine\nresults, both organic and sponsored. CTR of a result has two core components:\n(a) the probability of examination of a result by a user, and (b) the perceived\nrelevance of the result given that it has been examined by the user. There has\nbeen considerable work on user browsing models, to model and analyze both the\nexamination and the relevance components of CTR. In this paper, we propose a\nnovel formulation: a micro-browsing model for how users read result snippets.\nThe snippet text of a result often plays a critical role in the perceived\nrelevance of the result. We study how particular words within a line of snippet\ncan influence user behavior. We validate this new micro-browsing user model by\nconsidering the problem of predicting which snippet will yield higher CTR, and\nshow that classification accuracy is dramatically higher with our\nmicro-browsing user model. The key insight in this paper is that varying\nrelatively few words within a snippet, and even their location within a\nsnippet, can have a significant influence on the clickthrough of a snippet.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 18:13:28 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Islam", "Muhammad Asiful", ""], ["Srikant", "Ramakrishnan", ""], ["Basu", "Sugato", ""]]}, {"id": "1810.08382", "submitter": "Vahid Ranjbar", "authors": "Vahid Ranjbar, Mostafa Salehi, Pegah Jandaghi, Mahdi Jalili", "title": "QANet: Tensor Decomposition Approach for Query-based Anomaly Detection\n  in Heterogeneous Information Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex networks have now become integral parts of modern information\ninfrastructures. This paper proposes a user-centric method for detecting\nanomalies in heterogeneous information networks, in which nodes and/or edges\nmight be from different types. In the proposed anomaly detection method, users\ninteract directly with the system and anomalous entities can be detected\nthrough queries. Our approach is based on tensor decomposition and clustering\nmethods. We also propose a network generation model to construct synthetic\nheterogeneous information network to test the performance of the proposed\nmethod. The proposed anomaly detection method is compared with state-of-the-art\nmethods in both synthetic and real-world networks. Experimental results show\nthat the proposed tensor-based method considerably outperforms the existing\nanomaly detection methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 07:54:54 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Ranjbar", "Vahid", ""], ["Salehi", "Mostafa", ""], ["Jandaghi", "Pegah", ""], ["Jalili", "Mahdi", ""]]}, {"id": "1810.08747", "submitter": "Arun Kumar", "authors": "Arun Kumar, Karan Aggarwal, Paul Schrater", "title": "Temporal Proximity induces Attributes Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users consume their favorite content in temporal proximity of consumption\nbundles according to their preferences and tastes. Thus, the underlying\nattributes of items implicitly match user preferences, however, current\nrecommender systems largely ignore this fundamental driver in identifying\nmatching items. In this work, we introduce a novel temporal proximity filtering\nmethod to enable items-matching. First, we demonstrate that proximity\npreferences exist. Second, we present an induced similarity metric in temporal\nproximity driven by user tastes and third, we show that this induced similarity\ncan be used to learn items pairwise similarity in attribute space. The proposed\nmodel does not rely on any knowledge outside users' consumption bundles and\nprovide a novel way to devise user preferences and tastes driven novel items\nrecommender.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 03:35:48 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Kumar", "Arun", ""], ["Aggarwal", "Karan", ""], ["Schrater", "Paul", ""]]}, {"id": "1810.08765", "submitter": "Wen-Hao Chen", "authors": "Wen-Hao Chen, Chin-Chi Hsu, Yi-An Lai, Vincent Liu, Mi-Yen Yeh,\n  Shou-De Lin", "title": "Attribute-aware Collaborative Filtering: Survey and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute-aware CF models aims at rating prediction given not only the\nhistorical rating from users to items, but also the information associated with\nusers (e.g. age), items (e.g. price), or even ratings (e.g. rating time). This\npaper surveys works in the past decade developing attribute-aware CF systems,\nand discovered that mathematically they can be classified into four different\ncategories. We provide the readers not only the high level mathematical\ninterpretation of the existing works in this area but also the mathematical\ninsight for each category of models. Finally we provide in-depth experiment\nresults comparing the effectiveness of the major works in each category.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 07:29:52 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Chen", "Wen-Hao", ""], ["Hsu", "Chin-Chi", ""], ["Lai", "Yi-An", ""], ["Liu", "Vincent", ""], ["Yeh", "Mi-Yen", ""], ["Lin", "Shou-De", ""]]}, {"id": "1810.09068", "submitter": "Salvador Medina", "authors": "Salvador Medina, Zhuyun Dai, Yingkai Gao", "title": "Where is this? Video geolocation based on neural network features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a method that geolocates videos within a delimited\nwidespread area based solely on the frames visual content. Our proposed method\ntackles video-geolocation through traditional image retrieval techniques\nconsidering Google Street View as the reference point. To achieve this goal we\nuse the deep learning features obtained from NetVLAD to represent images, since\nthrough this feature vectors the similarity is their L2 norm. In this paper, we\npropose a family of voting-based methods to aggregate frame-wise geolocation\nresults which boost the video geolocation result. The best aggregation found\nthrough our experiments considers both NetVLAD and SIFT similarity, as well as\nthe geolocation density of the most similar results. To test our proposed\nmethod, we gathered a new video dataset from Pittsburgh Downtown area to\nbenefit and stimulate more work in this area. Our system achieved a precision\nof 90% while geolocating videos within a range of 150 meters or two blocks away\nfrom the original position.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 03:27:43 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 00:51:00 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Medina", "Salvador", ""], ["Dai", "Zhuyun", ""], ["Gao", "Yingkai", ""]]}, {"id": "1810.09079", "submitter": "Tianyi Lin", "authors": "Tianyi Lin, Zhiyue Hu and Xin Guo", "title": "Sparsemax and Relaxed Wasserstein for Topic Sparsity", "comments": "10 Pages. To appear in WSDM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic sparsity refers to the observation that individual documents usually\nfocus on several salient topics instead of covering a wide variety of topics,\nand a real topic adopts a narrow range of terms instead of a wide coverage of\nthe vocabulary. Understanding this topic sparsity is especially important for\nanalyzing user-generated web content and social media, which are featured in\nthe form of extremely short posts and discussions. As topic sparsity of\nindividual documents in online social media increases, so does the difficulty\nof analyzing the online text sources using traditional methods.\n  In this paper, we propose two novel neural models by providing sparse\nposterior distributions over topics based on the Gaussian sparsemax\nconstruction, enabling efficient training by stochastic backpropagation. We\nconstruct an inference network conditioned on the input data and infer the\nvariational distribution with the relaxed Wasserstein (RW) divergence. Unlike\nexisting works based on Gaussian softmax construction and Kullback-Leibler (KL)\ndivergence, our approaches can identify latent topic sparsity with training\nstability, predictive performance, and topic coherence. Experiments on\ndifferent genres of large text corpora have demonstrated the effectiveness of\nour models as they outperform both probabilistic and neural methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 04:23:44 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 10:22:18 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Lin", "Tianyi", ""], ["Hu", "Zhiyue", ""], ["Guo", "Xin", ""]]}, {"id": "1810.09147", "submitter": "Abhisek Dash", "authors": "Abhisek Dash, Anurag Shandilya, Arindam Biswas, Kripabandhu Ghosh,\n  Saptarshi Ghosh, Abhijnan Chakraborty", "title": "Summarizing User-generated Textual Content: Motivation and Methods for\n  Fairness in Algorithmic Summaries", "comments": "This work has been accepted for presentation at the ACM Conference on\n  Computer-Supported Cooperative Work and Social Computing (CSCW) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the amount of user-generated textual content grows rapidly, text\nsummarization algorithms are increasingly being used to provide users a quick\noverview of the information content. Traditionally, summarization algorithms\nhave been evaluated only based on how well they match human-written summaries\n(e.g. as measured by ROUGE scores). In this work, we propose to evaluate\nsummarization algorithms from a completely new perspective that is important\nwhen the user-generated data to be summarized comes from different socially\nsalient user groups, e.g. men or women, Caucasians or African-Americans, or\ndifferent political groups (Republicans or Democrats). In such cases, we check\nwhether the generated summaries fairly represent these different social groups.\nSpecifically, considering that an extractive summarization algorithm selects a\nsubset of the textual units (e.g. microblogs) in the original data for\ninclusion in the summary, we investigate whether this selection is fair or not.\nOur experiments over real-world microblog datasets show that existing\nsummarization algorithms often represent the socially salient user-groups very\ndifferently compared to their distributions in the original data. More\nimportantly, some groups are frequently under-represented in the generated\nsummaries, and hence get far less exposure than what they would have obtained\nin the original data. To reduce such adverse impacts, we propose novel\nfairness-preserving summarization algorithms which produce high-quality\nsummaries while ensuring fairness among various groups. To our knowledge, this\nis the first attempt to produce fair text summarization, and is likely to open\nup an interesting research direction.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 09:22:28 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 08:54:43 GMT"}, {"version": "v3", "created": "Fri, 9 Nov 2018 10:21:02 GMT"}, {"version": "v4", "created": "Mon, 6 May 2019 19:56:04 GMT"}, {"version": "v5", "created": "Mon, 2 Sep 2019 11:52:28 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Dash", "Abhisek", ""], ["Shandilya", "Anurag", ""], ["Biswas", "Arindam", ""], ["Ghosh", "Kripabandhu", ""], ["Ghosh", "Saptarshi", ""], ["Chakraborty", "Abhijnan", ""]]}, {"id": "1810.09177", "submitter": "Hao Ren", "authors": "Hao Ren, Hong Lu", "title": "Compositional coding capsule network with k-means routing for text\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification is a challenging problem which aims to identify the\ncategory of texts. Recently, Capsule Networks (CapsNets) are proposed for image\nclassification. It has been shown that CapsNets have several advantages over\nConvolutional Neural Networks (CNNs), while, their validity in the domain of\ntext has less been explored. An effective method named deep compositional code\nlearning has been proposed lately. This method can save many parameters about\nword embeddings without any significant sacrifices in performance. In this\npaper, we introduce the Compositional Coding (CC) mechanism between capsules,\nand we propose a new routing algorithm, which is based on k-means clustering\ntheory. Experiments conducted on eight challenging text classification datasets\nshow the proposed method achieves competitive accuracy compared to the\nstate-of-the-art approach with significantly fewer parameters.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 11:04:27 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 07:34:04 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 14:29:24 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Ren", "Hao", ""], ["Lu", "Hong", ""]]}, {"id": "1810.09305", "submitter": "Mahnaz Koupaee", "authors": "Mahnaz Koupaee, William Yang Wang", "title": "WikiHow: A Large Scale Text Summarization Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models have recently gained the state of the art\nperformance in summarization. However, not too many large-scale high-quality\ndatasets are available and almost all the available ones are mainly news\narticles with specific writing style. Moreover, abstractive human-style systems\ninvolving description of the content at a deeper level require data with higher\nlevels of abstraction. In this paper, we present WikiHow, a dataset of more\nthan 230,000 article and summary pairs extracted and constructed from an online\nknowledge base written by different human authors. The articles span a wide\nrange of topics and therefore represent high diversity styles. We evaluate the\nperformance of the existing methods on WikiHow to present its challenges and\nset some baselines to further improve it.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 05:29:41 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Koupaee", "Mahnaz", ""], ["Wang", "William Yang", ""]]}, {"id": "1810.09401", "submitter": "Hamid Dadkhahi", "authors": "Hamid Dadkhahi and Sahand Negahban", "title": "Alternating Linear Bandits for Online Matrix-Factorization\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of online collaborative filtering in the online\nsetting, where items are recommended to the users over time. At each time step,\nthe user (selected by the environment) consumes an item (selected by the agent)\nand provides a rating of the selected item. In this paper, we propose a novel\nalgorithm for online matrix factorization recommendation that combines linear\nbandits and alternating least squares. In this formulation, the bandit feedback\nis equal to the difference between the ratings of the best and selected items.\nWe evaluate the performance of the proposed algorithm over time using both\ncumulative regret and average cumulative NDCG. Simulation results over three\nsynthetic datasets as well as three real-world datasets for online\ncollaborative filtering indicate the superior performance of the proposed\nalgorithm over two state-of-the-art online algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 16:52:57 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Dadkhahi", "Hamid", ""], ["Negahban", "Sahand", ""]]}, {"id": "1810.09580", "submitter": "Alvaro Henrique Chaim Correia", "authors": "Alvaro Henrique Chaim Correia, Jorge Luiz Moreira Silva, Thiago de\n  Castro Martins, Fabio Gagliardi Cozman", "title": "A Fully Attention-Based Information Retriever", "comments": "Accepted for presentation at the International Joint Conference on\n  Neural Networks (IJCNN) 2018", "journal-ref": "A. H. C. Correia, J. L. M. Silva, T. d. C. Martins and F. G.\n  Cozman, \"A Fully Attention-Based Information Retriever,\" 2018 International\n  Joint Conference on Neural Networks (IJCNN), Rio de Janeiro, Brazil, 2018,\n  pp. 2799-2806", "doi": "10.1109/IJCNN.2018.8489656", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks are now the state-of-the-art in natural language\nprocessing because they can build rich contextual representations and process\ntexts of arbitrary length. However, recent developments on attention mechanisms\nhave equipped feedforward networks with similar capabilities, hence enabling\nfaster computations due to the increase in the number of operations that can be\nparallelized. We explore this new type of architecture in the domain of\nquestion-answering and propose a novel approach that we call Fully Attention\nBased Information Retriever (FABIR). We show that FABIR achieves competitive\nresults in the Stanford Question Answering Dataset (SQuAD) while having fewer\nparameters and being faster at both learning and inference than rival methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 22:10:46 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Correia", "Alvaro Henrique Chaim", ""], ["Silva", "Jorge Luiz Moreira", ""], ["Martins", "Thiago de Castro", ""], ["Cozman", "Fabio Gagliardi", ""]]}, {"id": "1810.09591", "submitter": "Malay Haldar", "authors": "Malay Haldar, Mustafa Abdool, Prashant Ramanathan, Tao Xu, Shulin\n  Yang, Huizhong Duan, Qing Zhang, Nick Barrow-Williams, Bradley C. Turnbull,\n  Brendan M. Collins and Thomas Legrand", "title": "Applying Deep Learning To Airbnb Search", "comments": "8 pages", "journal-ref": null, "doi": "10.1145/3292500.3330658", "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application to search ranking is one of the biggest machine learning\nsuccess stories at Airbnb. Much of the initial gains were driven by a gradient\nboosted decision tree model. The gains, however, plateaued over time. This\npaper discusses the work done in applying neural networks in an attempt to\nbreak out of that plateau. We present our perspective not with the intention of\npushing the frontier of new modeling techniques. Instead, ours is a story of\nthe elements we found useful in applying neural networks to a real life\nproduct. Deep learning was steep learning for us. To other teams embarking on\nsimilar journeys, we hope an account of our struggles and triumphs will provide\nsome useful pointers. Bon voyage!\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 23:11:01 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 18:28:03 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Haldar", "Malay", ""], ["Abdool", "Mustafa", ""], ["Ramanathan", "Prashant", ""], ["Xu", "Tao", ""], ["Yang", "Shulin", ""], ["Duan", "Huizhong", ""], ["Zhang", "Qing", ""], ["Barrow-Williams", "Nick", ""], ["Turnbull", "Bradley C.", ""], ["Collins", "Brendan M.", ""], ["Legrand", "Thomas", ""]]}, {"id": "1810.10004", "submitter": "Pavlos Fafalios", "authors": "Nilamadhaba Mohapatra, Vasileios Iosifidis, Asif Ekbal, Stefan Dietze,\n  Pavlos Fafalios", "title": "Time-Aware and Corpus-Specific Entity Relatedness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity relatedness has emerged as an important feature in a plethora of\napplications such as information retrieval, entity recommendation and entity\nlinking. Given an entity, for instance a person or an organization, entity\nrelatedness measures can be exploited for generating a list of highly-related\nentities. However, the relation of an entity to some other entity depends on\nseveral factors, with time and context being two of the most important ones\n(where, in our case, context is determined by a particular corpus). For\nexample, the entities related to the International Monetary Fund are different\nnow compared to some years ago, while these entities also may highly differ in\nthe context of a USA news portal compared to a Greek news portal. In this\npaper, we propose a simple but flexible model for entity relatedness which\nconsiders time and entity aware word embeddings by exploiting the underlying\ncorpus. The proposed model does not require external knowledge and is language\nindependent, which makes it widely useful in a variety of applications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 11:17:45 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Mohapatra", "Nilamadhaba", ""], ["Iosifidis", "Vasileios", ""], ["Ekbal", "Asif", ""], ["Dietze", "Stefan", ""], ["Fafalios", "Pavlos", ""]]}, {"id": "1810.10038", "submitter": "Halima Nefzi", "authors": "Halima Nefzi", "title": "A new approach of contextual recommendation based on the method of\n  Hierarchical Analysis of Processes", "comments": "Master's thesis. in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are able to estimate the user's interest for resource\ngiven from some relative information to others similar users and to propriety\nof the resource. In this Memory, we introduced a new contextual recommendation\napproach based on the AHP Process Hierarchical Analysis method. This work\nconsisted in making a bibliographic study on the works having proposed systems\nof recommendation based on the context of the users in the field of films. The\ngoal is to design and develop a new approach to recommending movies based on\nuser context. And we relied on methods of multi-criteria decision making (MCDM)\nand more precisely the method of Hierarchical Process Analysis (AHP) for\ncontext integration in the recommendation process.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 18:35:38 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Nefzi", "Halima", ""]]}, {"id": "1810.10176", "submitter": "Tolgahan Cakaloglu Ph.D.c", "authors": "Tolgahan Cakaloglu, Christian Szegedy, Xiaowei Xu", "title": "Text Embeddings for Retrieval From a Large Knowledge Base", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text embedding representing natural language documents in a semantic vector\nspace can be used for document retrieval using nearest neighbor lookup. In\norder to study the feasibility of neural models specialized for retrieval in a\nsemantically meaningful way, we suggest the use of the Stanford Question\nAnswering Dataset (SQuAD) in an open-domain question answering context, where\nthe first task is to find paragraphs useful for answering a given question.\nFirst, we compare the quality of various text-embedding methods on the\nperformance of retrieval and give an extensive empirical comparison on the\nperformance of various non-augmented base embedding with, and without IDF\nweighting. Our main results are that by training deep residual neural models,\nspecifically for retrieval purposes, can yield significant gains when it is\nused to augment existing embeddings. We also establish that deeper models are\nsuperior to this task. The best base baseline embeddings augmented by our\nlearned neural approach improves the top-1 paragraph recall of the system by\n14%.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 03:57:11 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 17:19:08 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Cakaloglu", "Tolgahan", ""], ["Szegedy", "Christian", ""], ["Xu", "Xiaowei", ""]]}, {"id": "1810.10226", "submitter": "Hanbing Zhan", "authors": "Zhou Zhao, Hanbing Zhan, Lingtao Meng, Jun Xiao, Jun Yu, Min Yang, Fei\n  Wu, Deng Cai", "title": "Textually Guided Ranking Network for Attentional Image Retweet Modeling", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retweet prediction is a challenging problem in social media sites (SMS). In\nthis paper, we study the problem of image retweet prediction in social media,\nwhich predicts the image sharing behavior that the user reposts the image\ntweets from their followees. Unlike previous studies, we learn user preference\nranking model from their past retweeted image tweets in SMS. We first propose\nheterogeneous image retweet modeling network (IRM) that exploits users' past\nretweeted image tweets with associated contexts, their following relations in\nSMS and preference of their followees. We then develop a novel attentional\nmulti-faceted ranking network learning framework with textually guided\nmulti-modal neural networks for the proposed heterogenous IRM network to learn\nthe joint image tweet representations and user preference representations for\nprediction task. The extensive experiments on a large-scale dataset from\nTwitter site shows that our method achieves better performance than other\nstate-of-the-art solutions to the problem.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 07:43:20 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Zhao", "Zhou", ""], ["Zhan", "Hanbing", ""], ["Meng", "Lingtao", ""], ["Xiao", "Jun", ""], ["Yu", "Jun", ""], ["Yang", "Min", ""], ["Wu", "Fei", ""], ["Cai", "Deng", ""]]}, {"id": "1810.10251", "submitter": "Jaspreet Singh", "authors": "Jaspreet Singh, Wolfgang Nejdl, Avishek Anand", "title": "History by Diversity: Helping Historians search News Archives", "comments": null, "journal-ref": null, "doi": "10.1145/2854946.2854959", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal corpora like newspaper archives are of immense value to\nhistorical research, and time as an important factor for historians strongly\ninfluences their search behaviour in these archives. While searching for\narticles published over time, a key preference is to retrieve documents which\ncover the important aspects from important points in time which is different\nfrom standard search behavior. To support this search strategy, we introduce\nthe notion of a Historical Query Intent to explicitly model a historian's\nsearch task and define an aspect-time diversification problem over news\narchives.\n  We present a novel algorithm, HistDiv, that explicitly models the aspects and\nimportant time windows based on a historian's information seeking behavior. By\nincorporating temporal priors based on publication times and temporal\nexpressions, we diversify both on the aspect and temporal dimensions. We test\nour methods by constructing a test collection based on The New York Times\n  Collection with a workload of 30 queries of historical intent assessed\nmanually. We find that HistDiv outperforms all competitors in subtopic recall\nwith a slight loss in precision. We also present results of a qualitative user\nstudy to determine wether this drop in precision is detrimental to user\nexperience. Our results show that users still preferred HistDiv's ranking.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 09:01:23 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Singh", "Jaspreet", ""], ["Nejdl", "Wolfgang", ""], ["Anand", "Avishek", ""]]}, {"id": "1810.10252", "submitter": "Jaspreet Singh", "authors": "Jaspreet Singh, Johannes Hoffart, Avishek Anand", "title": "Discovering Entities with Just a Little Help from You", "comments": null, "journal-ref": null, "doi": "10.1145/2983323.2983798", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linking entities like people, organizations, books, music groups and their\nsongs in text to knowledge bases (KBs) is a fundamental task for many\ndownstream search and mining applications. Achieving high disambiguation\naccuracy crucially depends on a rich and holistic representation of the\nentities in the KB. For popular entities, such a representation can be easily\nmined from Wikipedia, and many current entity disambiguation and linking\nmethods make use of this fact. However, Wikipedia does not contain long-tail\nentities that only few people are interested in, and also at times lags behind\nuntil newly emerging entities are added. For such entities, mining a suitable\nrepresentation in a fully automated fashion is very difficult, resulting in\npoor linking accuracy.\n  What can automatically be mined, though, is a high-quality representation\ngiven the context of a new entity occurring in any text. Due to the lack of\nknowledge about the entity, no method can retrieve these occurrences\nautomatically with high precision, resulting in a chicken-egg problem. To\naddress this, our approach automatically generates candidate occurrences of\nentities, prompting the user for feedback to decide if the occurrence refers to\nthe actual entity in question. This feedback gradually improves the knowledge\nand allows our methods to provide better candidate suggestions to keep the user\nengaged. We propose novel human-in-the-loop retrieval methods for generating\ncandidates based on gradient interleaving of diversification and textual\nrelevance approaches.\n  We conducted extensive experiments on the FACC dataset, showing that our\napproaches convincingly outperform carefully selected baselines in both\nintrinsic and extrinsic measures while keeping users engaged.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 09:01:36 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Singh", "Jaspreet", ""], ["Hoffart", "Johannes", ""], ["Anand", "Avishek", ""]]}, {"id": "1810.10253", "submitter": "Jaspreet Singh", "authors": "Jaspreet Singh, Avishek Anand", "title": "Designing Search Tasks for Archive Search", "comments": null, "journal-ref": null, "doi": "10.1145/3020165.3022153", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal corpora like legal, corporate and newspaper archives are of\nimmense value to a variety of users, and time as an important factor strongly\ninfluences their search behavior in these archives. While many systems have\nbeen developed to support users temporal information needs, questions remain\nover how users utilize these advances to satisfy their needs. Analyzing their\nsearch behavior will provide us with novel insights into search strategy, guide\nbetter interface and system design and highlight new problems for further\nresearch. In this paper we propose a set of search tasks, with varying\ncomplexity, that IIR researchers can utilize to study user search behavior in\narchives. We discuss how we created and refined these tasks as the result of a\npilot study using a temporal search engine. We not only propose task\ndescriptions but also pre and post-task evaluation mechanisms that can be\nemployed for a large-scale study (crowdsourcing). Our initial findings show the\nviability of such tasks for investigating search behavior in archives.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 09:01:50 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Singh", "Jaspreet", ""], ["Anand", "Avishek", ""]]}, {"id": "1810.10307", "submitter": "Jinjin Chi", "authors": "Jinjin Chi, Jihong Ouyang, Changchun Li, Xueyang Dong, Ximing Li,\n  Xinhua Wang", "title": "Topic representation: finding more representative words in topic models", "comments": "The paper has been submitted to Pattern Recognition Letters and is\n  being reviewed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The top word list, i.e., the top-M words with highest marginal probability in\na given topic, is the standard topic representation in topic models. Most of\nrecent automatical topic labeling algorithms and popular topic quality metrics\nare based on it. However, we find, empirically, words in this type of top word\nlist are not always representative. The objective of this paper is to find more\nrepresentative top word lists for topics. To achieve this, we rerank the words\nin a given topic by further considering marginal probability on words over\nevery other topic. The reranking list of top-M words is used to be a novel\ntopic representation for topic models. We investigate three reranking\nmethodologies, using (1) standard deviation weight, (2) standard deviation\nweight with topic size and (3) Chi Square \\c{hi}2statistic selection.\nExperimental results on real world collections indicate that our\nrepresentations can extract more representative words for topics, agreeing with\nhuman judgements.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 04:33:49 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Chi", "Jinjin", ""], ["Ouyang", "Jihong", ""], ["Li", "Changchun", ""], ["Dong", "Xueyang", ""], ["Li", "Ximing", ""], ["Wang", "Xinhua", ""]]}, {"id": "1810.10308", "submitter": "Pavlos Fafalios", "authors": "Pavlos Fafalios, Vasileios Iosifidis, Eirini Ntoutsi, Stefan Dietze", "title": "TweetsKB: A Public and Large-Scale RDF Corpus of Annotated Tweets", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-93417-4_12", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Publicly available social media archives facilitate research in a variety of\nfields, such as data science, sociology or the digital humanities, where\nTwitter has emerged as one of the most prominent sources. However, obtaining,\narchiving and annotating large amounts of tweets is costly. In this paper, we\ndescribe TweetsKB, a publicly available corpus of currently more than 1.5\nbillion tweets, spanning almost 5 years (Jan'13-Nov'17). Metadata information\nabout the tweets as well as extracted entities, hashtags, user mentions and\nsentiment information are exposed using established RDF/S vocabularies. Next to\na description of the extraction and annotation process, we present use cases to\nillustrate scenarios for entity-centric information exploration, data\nintegration and knowledge discovery facilitated by TweetsKB.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 11:54:23 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Fafalios", "Pavlos", ""], ["Iosifidis", "Vasileios", ""], ["Ntoutsi", "Eirini", ""], ["Dietze", "Stefan", ""]]}, {"id": "1810.10324", "submitter": "Christopher Tralie", "authors": "Christopher J. Tralie, Paul Bendich, John Harer", "title": "Multi-scale Geometric Summaries for Similarity-based Sensor Fusion", "comments": "9 pages, 13 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address fusion of heterogeneous sensor data using\nwavelet-based summaries of fused self-similarity information from each sensor.\nThe technique we develop is quite general, does not require domain specific\nknowledge or physical models, and requires no training. Nonetheless, it can\nperform surprisingly well at the general task of differentiating classes of\ntime-ordered behavior sequences which are sensed by more than one modality. As\na demonstration of our capabilities in the audio to video context, we focus on\nthe differentiation of speech sequences.\n  Data from two or more modalities first are represented using self-similarity\nmatrices(SSMs) corresponding to time-ordered point clouds in feature spaces of\neach of these data sources; we note that these feature spaces can be of\nentirely different scale and dimensionality.\n  A fused similarity template is then derived from the modality-specific SSMs\nusing a technique called similarity network fusion (SNF). We investigate\npipelines using SNF as both an upstream (feature-level) and a downstream\n(ranking-level) fusion technique. Multiscale geometric features of this\ntemplate are then extracted using a recently-developed technique called the\nscattering transform, and these features are then used to differentiate speech\nsequences. This method outperforms unsupervised techniques which operate\ndirectly on the raw data, and it also outperforms stovepiped methods which\noperate on SSMs separately derived from the distinct modalities. The benefits\nof this method become even more apparent as the simulated peak signal to noise\nratio decreases.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 18:52:07 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 01:27:01 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Tralie", "Christopher J.", ""], ["Bendich", "Paul", ""], ["Harer", "John", ""]]}, {"id": "1810.10419", "submitter": "Archit Sakhadeo", "authors": "Archit Sakhadeo and Nisheeth Srivastava", "title": "Effective extractive summarization using frequency-filtered entity\n  relationship graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word frequency-based methods for extractive summarization are easy to\nimplement and yield reasonable results across languages. However, they have\nsignificant limitations - they ignore the role of context, they offer uneven\ncoverage of topics in a document, and sometimes are disjointed and hard to\nread. We use a simple premise from linguistic typology - that English sentences\nare complete descriptors of potential interactions between entities, usually in\nthe order subject-verb-object - to address a subset of these difficulties. We\nhave developed a hybrid model of extractive summarization that combines\nword-frequency based keyword identification with information from automatically\ngenerated entity relationship graphs to select sentences for summaries.\nComparative evaluation with word-frequency and topic word-based methods shows\nthat the proposed method is competitive by conventional ROUGE standards, and\nyields moderately more informative summaries on average, as assessed by a large\npanel (N=94) of human raters.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 14:30:39 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Sakhadeo", "Archit", ""], ["Srivastava", "Nisheeth", ""]]}, {"id": "1810.10455", "submitter": "Pavlos Fafalios", "authors": "Pavlos Fafalios, Helge Holzmann, Vaibhav Kasturia, Wolfgang Nejdl", "title": "Building and Querying Semantic Layers for Web Archives (Extended\n  Version)", "comments": "This is a preprint of an article accepted for publication in the\n  International Journal on Digital Libraries (2018)", "journal-ref": "International Journal on Digital Libraries, ISSN: 1432-5012\n  (Print) 1432-1300 (Online), 2018", "doi": "10.1007/s00799-018-0251-0", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web archiving is the process of collecting portions of the Web to ensure that\nthe information is preserved for future exploitation. However, despite the\nincreasing number of web archives worldwide, the absence of efficient and\nmeaningful exploration methods still remains a major hurdle in the way of\nturning them into a usable and useful information source. In this paper, we\nfocus on this problem and propose an RDF/S model and a distributed framework\nfor building semantic profiles (\"layers\") that describe semantic information\nabout the contents of web archives. A semantic layer allows describing metadata\ninformation about the archived documents, annotating them with useful semantic\ninformation (like entities, concepts and events), and publishing all this data\non the Web as Linked Data. Such structured repositories offer advanced query\nand integration capabilities, and make web archives directly exploitable by\nother systems and tools. To demonstrate their query capabilities, we build and\nquery semantic layers for three different types of web archives. An\nexperimental evaluation showed that a semantic layer can answer information\nneeds that existing keyword-based systems are not able to sufficiently satisfy.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 15:32:42 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Fafalios", "Pavlos", ""], ["Holzmann", "Helge", ""], ["Kasturia", "Vaibhav", ""], ["Nejdl", "Wolfgang", ""]]}, {"id": "1810.10759", "submitter": "Dirk Oliver Theis", "authors": "Tore Vincent Carstens and Dirk Oliver Theis", "title": "Note on (active-)QRAM-style data access as a quantum circuit", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe how an active (i.e., requring $2^n$ parallel control operations)\nQRAM-like effect $$\\sum_{y=0}^{N-1} |y\\rangle\\langle y| \\otimes\nU^y_{\\text{result},\\text{memory}_y}$$ can be realized, as a quantum circuit of\ndepth $O(n+\\sqrt m)$ (where $m$ is the size of the result register) plus the\nmaximum over all~$z$ of the circuit depths of controlled-$U^z$ operations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 08:21:20 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Carstens", "Tore Vincent", ""], ["Theis", "Dirk Oliver", ""]]}, {"id": "1810.10769", "submitter": "Jaspreet Singh", "authors": "Jaspreet Singh, Wolfgang Nejdl, Avishek Anand", "title": "Expedition: A Time-Aware Exploratory Search System Designed for Scholars", "comments": null, "journal-ref": null, "doi": "10.1145/2911451.2911465", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archives are an important source of study for various scholars. Digitization\nand the web have made archives more accessible and led to the development of\nseveral time-aware exploratory search systems. However these systems have been\ndesigned for more general users rather than scholars. Scholars have more\ncomplex information needs in comparison to general users. They also require\nsupport for corpus creation during their exploration process. In this paper we\npresent Expedition - a time-aware exploratory search system that addresses the\nrequirements and information needs of scholars. Expedition possesses a suite of\nad-hoc and diversity based retrieval models to address complex information\nneeds; a newspaper-style user interface to allow for larger textual previews\nand comparisons; entity filters to more naturally refine a result list and an\ninteractive annotated timeline which can be used to better identify periods of\nimportance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 08:39:47 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Singh", "Jaspreet", ""], ["Nejdl", "Wolfgang", ""], ["Anand", "Avishek", ""]]}, {"id": "1810.10814", "submitter": "Matthias Miller", "authors": "Matthias Miller, Johannes H\\\"au{\\ss}ler, Matthias Kraus, Daniel Keim,\n  and Mennatallah El-Assady", "title": "Analyzing Visual Mappings of Traditional and Alternative Music Notation", "comments": "5 pages including references, 3rd Workshop on Visualization for the\n  Digital Humanities, Vis4DH, IEEE Vis 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we postulate that combining the domains of information\nvisualization and music studies paves the ground for a more structured analysis\nof the design space of music notation, enabling the creation of alternative\nmusic notations that are tailored to different users and their tasks. Hence, we\ndiscuss the instantiation of a design and visualization pipeline for music\nnotation that follows a structured approach, based on the fundamental concepts\nof information and data visualization. This enables practitioners and\nresearchers of digital humanities and information visualization, alike, to\nconceptualize, create, and analyze novel music notation methods. Based on the\nanalysis of relevant stakeholders and their usage of music notation as a mean\nof communication, we identify a set of relevant features typically encoded in\ndifferent annotations and encodings, as used by interpreters, performers, and\nreaders of music. We analyze the visual mappings of musical dimensions for\nvarying notation methods to highlight gaps and frequent usages of encodings,\nvisual channels, and Gestalt laws. This detailed analysis leads us to the\nconclusion that such an under-researched area in information visualization\nholds the potential for fundamental research. This paper discusses possible\nresearch opportunities, open challenges, and arguments that can be pursued in\nthe process of analyzing, improving, or rethinking existing music notation\nsystems and techniques.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 10:12:23 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Miller", "Matthias", ""], ["H\u00e4u\u00dfler", "Johannes", ""], ["Kraus", "Matthias", ""], ["Keim", "Daniel", ""], ["El-Assady", "Mennatallah", ""]]}, {"id": "1810.11017", "submitter": "Pavlos Fafalios", "authors": "Pavlos Fafalios, Vasileios Iosifidis, Kostas Stefanidis, Eirini\n  Ntoutsi", "title": "Tracking the History and Evolution of Entities: Entity-centric Temporal\n  Analysis of Large Social Media Archives", "comments": "This is a preprint of an article accepted for publication in the\n  International Journal on Digital Libraries (2018)", "journal-ref": null, "doi": "10.1007/s00799-018-0257-7", "report-no": null, "categories": "cs.SI cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How did the popularity of the Greek Prime Minister evolve in 2015? How did\nthe predominant sentiment about him vary during that period? Were there any\ncontroversial sub-periods? What other entities were related to him during these\nperiods? To answer these questions, one needs to analyze archived documents and\ndata about the query entities, such as old news articles or social media\narchives. In particular, user-generated content posted in social networks, like\nTwitter and Facebook, can be seen as a comprehensive documentation of our\nsociety, and thus meaningful analysis methods over such archived data are of\nimmense value for sociologists, historians and other interested parties who\nwant to study the history and evolution of entities and events. To this end, in\nthis paper we propose an entity-centric approach to analyze social media\narchives and we define measures that allow studying how entities were reflected\nin social media in different time periods and under different aspects, like\npopularity, attitude, controversiality, and connectedness with other entities.\nA case study using a large Twitter archive of four years illustrates the\ninsights that can be gained by such an entity-centric and multi-aspect\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 15:35:44 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Fafalios", "Pavlos", ""], ["Iosifidis", "Vasileios", ""], ["Stefanidis", "Kostas", ""], ["Ntoutsi", "Eirini", ""]]}, {"id": "1810.11048", "submitter": "Pavlos Fafalios", "authors": "Pavlos Fafalios, Vaibhav Kasturia, Wolfgang Nejdl", "title": "Ranking Archived Documents for Structured Queries on Semantic Layers", "comments": null, "journal-ref": null, "doi": "10.1145/3197026.3197049", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archived collections of documents (like newspaper and web archives) serve as\nimportant information sources in a variety of disciplines, including Digital\nHumanities, Historical Science, and Journalism. However, the absence of\nefficient and meaningful exploration methods still remains a major hurdle in\nthe way of turning them into usable sources of information. A semantic layer is\nan RDF graph that describes metadata and semantic information about a\ncollection of archived documents, which in turn can be queried through a\nsemantic query language (SPARQL). This allows running advanced queries by\ncombining metadata of the documents (like publication date) and content-based\nsemantic information (like entities mentioned in the documents). However, the\nresults returned by such structured queries can be numerous and moreover they\nall equally match the query. In this paper, we deal with this problem and\nformalize the task of \"ranking archived documents for structured queries on\nsemantic layers\". Then, we propose two ranking models for the problem at hand\nwhich jointly consider: i) the relativeness of documents to entities, ii) the\ntimeliness of documents, and iii) the temporal relations among the entities.\nThe experimental results on a new evaluation dataset show the effectiveness of\nthe proposed models and allow us to understand their limitations\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 12:42:46 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Fafalios", "Pavlos", ""], ["Kasturia", "Vaibhav", ""], ["Nejdl", "Wolfgang", ""]]}, {"id": "1810.11049", "submitter": "Pavlos Fafalios", "authors": "Pavlos Fafalios, Vaibhav Kasturia, Wolfgang Nejdl", "title": "Towards a Ranking Model for Semantic Layers over Digital Archives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archived collections of documents (like newspaper archives) serve as\nimportant information sources for historians, journalists, sociologists and\nother interested parties. Semantic Layers over such digital archives allow\ndescribing and publishing metadata and semantic information about the archived\ndocuments in a standard format (RDF), which in turn can be queried through a\nstructured query language (e.g., SPARQL). This enables to run advanced queries\nby combining metadata of the documents (like publication date) and\ncontent-based semantic information (like entities mentioned in the documents).\nHowever, the results returned by structured queries can be numerous and also\nthey all equally match the query. Thus, there is the need to rank these results\nin order to promote the most important ones. In this paper, we focus on this\nproblem and propose a ranking model that considers and combines: i) the\nrelativeness of documents to entities, ii) the timeliness of documents, and\niii) the relations among the entities.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 09:42:16 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Fafalios", "Pavlos", ""], ["Kasturia", "Vaibhav", ""], ["Nejdl", "Wolfgang", ""]]}, {"id": "1810.11303", "submitter": "Dimitris Gkoumas", "authors": "Dimitris Gkoumas, Sagar Uprety, and Dawei Song", "title": "Investigating non-classical correlations between decision fused\n  multi-modal documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation has been widely used to facilitate various information retrieval\nmethods such as query expansion, relevance feedback, document clustering, and\nmulti-modal fusion. Especially, correlation and independence are important\nissues when fusing different modalities that influence a multi-modal\ninformation retrieval process. The basic idea of correlation is that an\nobservable can help predict or enhance another observable. In quantum\nmechanics, quantum correlation, called entanglement, is a sort of correlation\nbetween the observables measured in atomic-size particles when these particles\nare not necessarily collected in ensembles. In this paper, we examine a\nmultimodal fusion scenario that might be similar to that encountered in physics\nby firstly measuring two observables (i.e., text-based relevance and\nimage-based relevance) of a multi-modal document without counting on an\nensemble of multi-modal documents already labeled in terms of these two\nvariables. Then, we investigate the existence of non-classical correlations\nbetween pairs of multi-modal documents. Despite there are some basic\ndifferences between entanglement and classical correlation encountered in the\nmacroscopic world, we investigate the existence of this kind of non-classical\ncorrelation through the Bell inequality violation. Here, we experimentally test\nseveral novel association methods in a small-scale experiment. However, in the\ncurrent experiment we did not find any violation of the Bell inequality.\nFinally, we present a series of interesting discussions, which may provide\ntheoretical and empirical insights and inspirations for future development of\nthis direction.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 13:00:37 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Gkoumas", "Dimitris", ""], ["Uprety", "Sagar", ""], ["Song", "Dawei", ""]]}, {"id": "1810.11305", "submitter": "Chaoran Huang", "authors": "Chaoran Huang, Lina Yao, Xianzhi Wang, Boualem Benatallah and Xiang\n  Zhang", "title": "Software Expert Discovery via Knowledge Domain Embeddings in a\n  Collaborative Network", "comments": "Accepted by Pattern Recognition Letters, Oct 2018", "journal-ref": null, "doi": "10.1016/j.patrec.2018.10.030", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Community Question Answering (CQA) websites can be claimed as the most major\nvenues for knowledge sharing, and the most effective way of exchanging\nknowledge at present. Considering that massive amount of users are\nparticipating online and generating huge amount data, management of knowledge\nhere systematically can be challenging. Expert recommendation is one of the\nmajor challenges, as it highlights users in CQA with potential expertise, which\nmay help match unresolved questions with existing high quality answers while at\nthe same time may help external services like human resource systems as another\nreference to evaluate their candidates. In this paper, we in this work we\npropose to exploring experts in CQA websites. We take advantage of recent\ndistributed word representation technology to help summarize text chunks, and\nin a semantic view exploiting the relationships between natural language\nphrases to extract latent knowledge domains. By domains, the users' expertise\nis determined on their historical performance, and a rank can be compute to\ngiven recommendation accordingly. In particular, Stack Overflow is chosen as\nour dataset to test and evaluate our work, where inclusive experiment shows our\ncompetence.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 13:07:44 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Huang", "Chaoran", ""], ["Yao", "Lina", ""], ["Wang", "Xianzhi", ""], ["Benatallah", "Boualem", ""], ["Zhang", "Xiang", ""]]}, {"id": "1810.11414", "submitter": "Durmus Sahin", "authors": "Durmus Ozkan Sahin, Oguz Emre Kural, Erdal Kilic, Armagan Karabina", "title": "A Text Classification Application: Poet Detection from Poetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread use of the internet, the size of the text data increases\nday by day. Poems can be given as an example of the growing text. In this\nstudy, we aim to classify poetry according to poet. Firstly, data set\nconsisting of three different poetry of poets written in English have been\nconstructed. Then, text categorization techniques are implemented on it.\nChi-Square technique are used for feature selection. In addition, five\ndifferent classification algorithms are tried. These algorithms are Sequential\nminimal optimization, Naive Bayes, C4.5 decision tree, Random Forest and\nk-nearest neighbors. Although each classifier showed very different results,\nover the 70% classification success rate was taken by sequential minimal\noptimization technique.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 17:44:57 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Sahin", "Durmus Ozkan", ""], ["Kural", "Oguz Emre", ""], ["Kilic", "Erdal", ""], ["Karabina", "Armagan", ""]]}, {"id": "1810.11498", "submitter": "Bhaskar Gautam", "authors": "Bhaskar Gautam and Annappa Basava", "title": "Automatic Identification and Ranking of Emergency Aids in Social Media\n  Macro Community", "comments": "Working notes of IRMiDis Track {FIRE} 2017 - Forum for Information\n  Retrieval Evaluation, Bangalore, India, December 8-10, 2017,\n  http://ceur-ws.org/Vol-2036", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Online social microblogging platforms including Twitter are increasingly used\nfor aiding relief operations during disaster events. During most of the\ncalamities that can be natural disasters or even armed attacks,\nnon-governmental organizations look for critical information about resources to\nsupport effected people. Despite the recent advancement of natural language\nprocessing with deep neural networks, retrieval and ranking of short text\nbecomes a challenging task because a lot of conversational and sympathy content\nmerged with the critical information. In this paper, we address the problem of\ncategorical information retrieval and ranking of most relevance information\nwhile considering the presence of short-text and multilingual languages that\narise during such events. Our proposed model is based on the formation of\nembedding vector with the help of textual and statistical preprocessing, and\nfinally, entire training 2,100,000 vectors were normalized using feed-forward\nneural network for need and availability tweets. Another important contribution\nof this paper lies in novel weighted Ranking Key algorithm based on top five\ngeneral terms to rank the classified tweets in most relevance with\nclassification. Lastly, we test our model on Nepal Earthquake dataset (contains\nshort text and multilingual language tweets) and achieved 6.81% of mean average\nprecision on 5,250,000 unlabeled embedding vectors of disaster relief tweets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 18:45:12 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Gautam", "Bhaskar", ""], ["Basava", "Annappa", ""]]}, {"id": "1810.11903", "submitter": "Oscar Karnalim", "authors": "Oscar Karnalim, Lisan Sulistiani", "title": "Dynamic Thresholding Mechanisms for IR-Based Filtering in Efficient\n  Source Code Plagiarism Detection", "comments": "The 2018 International Conference on Advanced Computer Science and\n  Information Systems (ICACSIS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve time inefficiency issue, only potential pairs are compared in\nstring-matching-based source code plagiarism detection; wherein potentiality is\ndefined through a fast-yet-order-insensitive similarity measurement (adapted\nfrom Information Retrieval) and only pairs which similarity degrees are higher\nor equal to a particular threshold is selected. Defining such threshold is not\na trivial task considering the threshold should lead to high efficiency\nimprovement and low effectiveness reduction (if it is unavoidable). This paper\nproposes two thresholding mechanisms---namely range-based and pair-count-based\nmechanism---that dynamically tune the threshold based on the distribution of\nresulted similarity degrees. According to our evaluation, both mechanisms are\nmore practical to be used than manual threshold assignment since they are more\nproportional to efficiency improvement and effectiveness reduction.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 23:29:46 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Karnalim", "Oscar", ""], ["Sulistiani", "Lisan", ""]]}, {"id": "1810.11921", "submitter": "Weiping Song", "authors": "Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming\n  Zhang, Jian Tang", "title": "AutoInt: Automatic Feature Interaction Learning via Self-Attentive\n  Neural Networks", "comments": "Accepted at CIKM2019", "journal-ref": null, "doi": "10.1145/3357384.3357925", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) prediction, which aims to predict the probability of\na user clicking on an ad or an item, is critical to many online applications\nsuch as online advertising and recommender systems. The problem is very\nchallenging since (1) the input features (e.g., the user id, user age, item id,\nitem category) are usually sparse and high-dimensional, and (2) an effective\nprediction relies on high-order combinatorial features (\\textit{a.k.a.} cross\nfeatures), which are very time-consuming to hand-craft by domain experts and\nare impossible to be enumerated. Therefore, there have been efforts in finding\nlow-dimensional representations of the sparse and high-dimensional raw features\nand their meaningful combinations. In this paper, we propose an effective and\nefficient method called the \\emph{AutoInt} to automatically learn the\nhigh-order feature interactions of input features. Our proposed algorithm is\nvery general, which can be applied to both numerical and categorical input\nfeatures. Specifically, we map both the numerical and categorical features into\nthe same low-dimensional space. Afterwards, a multi-head self-attentive neural\nnetwork with residual connections is proposed to explicitly model the feature\ninteractions in the low-dimensional space. With different layers of the\nmulti-head self-attentive neural networks, different orders of feature\ncombinations of input features can be modeled. The whole model can be\nefficiently fit on large-scale raw data in an end-to-end fashion. Experimental\nresults on four real-world datasets show that our proposed approach not only\noutperforms existing state-of-the-art approaches for prediction but also offers\ngood explainability. Code is available at:\n\\url{https://github.com/DeepGraphLearning/RecommenderSystems}.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 01:56:25 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 19:51:41 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Song", "Weiping", ""], ["Shi", "Chence", ""], ["Xiao", "Zhiping", ""], ["Duan", "Zhijian", ""], ["Xu", "Yewen", ""], ["Zhang", "Ming", ""], ["Tang", "Jian", ""]]}, {"id": "1810.12027", "submitter": "Feng Liu", "authors": "Feng Liu, Ruiming Tang, Xutao Li, Weinan Zhang, Yunming Ye, Haokun\n  Chen, Huifeng Guo, Yuzhou Zhang", "title": "Deep Reinforcement Learning based Recommendation with Explicit User-Item\n  Interactions Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation is crucial in both academia and industry, and various\ntechniques are proposed such as content-based collaborative filtering, matrix\nfactorization, logistic regression, factorization machines, neural networks and\nmulti-armed bandits. However, most of the previous studies suffer from two\nlimitations: (1) considering the recommendation as a static procedure and\nignoring the dynamic interactive nature between users and the recommender\nsystems, (2) focusing on the immediate feedback of recommended items and\nneglecting the long-term rewards. To address the two limitations, in this paper\nwe propose a novel recommendation framework based on deep reinforcement\nlearning, called DRR. The DRR framework treats recommendation as a sequential\ndecision making procedure and adopts an \"Actor-Critic\" reinforcement learning\nscheme to model the interactions between the users and recommender systems,\nwhich can consider both the dynamic adaptation and long-term rewards.\nFurthermore, a state representation module is incorporated into DRR, which can\nexplicitly capture the interactions between items and users. Three\ninstantiation structures are developed. Extensive experiments on four\nreal-world datasets are conducted under both the offline and online evaluation\nsettings. The experimental results demonstrate the proposed DRR method indeed\noutperforms the state-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 09:41:52 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 01:08:48 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 06:41:48 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Liu", "Feng", ""], ["Tang", "Ruiming", ""], ["Li", "Xutao", ""], ["Zhang", "Weinan", ""], ["Ye", "Yunming", ""], ["Chen", "Haokun", ""], ["Guo", "Huifeng", ""], ["Zhang", "Yuzhou", ""]]}, {"id": "1810.12085", "submitter": "Emily Alsentzer", "authors": "Emily Alsentzer and Anne Kim", "title": "Extractive Summarization of EHR Discharge Notes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patient summarization is essential for clinicians to provide coordinated care\nand practice effective communication. Automated summarization has the potential\nto save time, standardize notes, aid clinical decision making, and reduce\nmedical errors. Here we provide an upper bound on extractive summarization of\ndischarge notes and develop an LSTM model to sequentially label topics of\nhistory of present illness notes. We achieve an F1 score of 0.876, which\nindicates that this model can be employed to create a dataset for evaluation of\nextractive summarization methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 16:36:27 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Alsentzer", "Emily", ""], ["Kim", "Anne", ""]]}, {"id": "1810.12091", "submitter": "Shelan Jeawak", "authors": "Shelan S. Jeawak, Christopher B. Jones, and Steven Schockaert", "title": "Embedding Geographic Locations for Modelling the Natural Environment\n  using Flickr Tags and Structured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-data from photo-sharing websites such as Flickr can be used to obtain\nrich bag-of-words descriptions of geographic locations, which have proven\nvaluable, among others, for modelling and predicting ecological features. One\nimportant insight from previous work is that the descriptions obtained from\nFlickr tend to be complementary to the structured information that is available\nfrom traditional scientific resources. To better integrate these two diverse\nsources of information, in this paper we consider a method for learning vector\nspace embeddings of geographic locations. We show experimentally that this\nmethod improves on existing approaches, especially in cases where structured\ninformation is available.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 12:22:34 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Jeawak", "Shelan S.", ""], ["Jones", "Christopher B.", ""], ["Schockaert", "Steven", ""]]}, {"id": "1810.12118", "submitter": "Jiamou Liu", "authors": "Helen Jiahe Zhao and Jiamou Liu", "title": "Finding Answers from the Word of God: Domain Adaptation for Neural\n  Networks in Biblical Question Answering", "comments": "The paper has been accepted at IJCNN 2018", "journal-ref": null, "doi": "10.1109/IJCNN.2018.8489756", "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering (QA) has significantly benefitted from deep learning\ntechniques in recent years. However, domain-specific QA remains a challenge due\nto the significant amount of data required to train a neural network. This\npaper studies the answer sentence selection task in the Bible domain and answer\nquestions by selecting relevant verses from the Bible. For this purpose, we\ncreate a new dataset BibleQA based on bible trivia questions and propose three\nneural network models for our task. We pre-train our models on a large-scale QA\ndataset, SQuAD, and investigate the effect of transferring weights on model\naccuracy. Furthermore, we also measure the model accuracies with different\nanswer context lengths and different Bible translations. We affirm that\ntransfer learning has a noticeable improvement in the model accuracy. We\nachieve relatively good results with shorter context lengths, whereas longer\ncontext lengths decreased model accuracy. We also find that using a more modern\nBible translation in the dataset has a positive effect on the task.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 12:34:21 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Zhao", "Helen Jiahe", ""], ["Liu", "Jiamou", ""]]}, {"id": "1810.12123", "submitter": "Pengfei Xu", "authors": "Pengfei Xu, Jiaheng Lu", "title": "Efficient Taxonomic Similarity Joins with Adaptive Overlap Constraint", "comments": null, "journal-ref": null, "doi": "10.1145/3269206.3269236", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A similarity join aims to find all similar pairs between two collections of\nrecords. Established approaches usually deal with synthetic differences like\ntypos and abbreviations, but neglect the semantic relations between words. Such\nrelations, however, are helpful for obtaining high-quality joining results. In\nthis paper, we leverage the taxonomy knowledge (i.e., a set of IS-A\nhierarchical relations) to define a similarity measure which finds\nsemantic-similar records from two datasets. Based on this measure, we develop a\nsimilarity join algorithm with prefix filtering framework to prune away\nirrelevant pairs effectively. Our technical contribution here is an algorithm\nthat judiciously selects critical parameters in a prefix filter to maximise its\nfiltering power, supported by an estimation technique and Monte Carlo\nsimulation process. Empirical experiments show that our proposed methods\nexhibit high efficiency and scalability, outperforming the state-of-art by a\nlarge margin.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 13:42:10 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Xu", "Pengfei", ""], ["Lu", "Jiaheng", ""]]}, {"id": "1810.12423", "submitter": "Laura Koesten", "authors": "Laura Koesten, Elena Simperl, Emilia Kacprzak, Tom Blount, Jeni\n  Tennison", "title": "Everything you always wanted to know about a dataset: studies in data\n  summarisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summarising data as text helps people make sense of it. It also improves data\ndiscovery, as search algorithms can match this text against keyword queries. In\nthis paper, we explore the characteristics of text summaries of data in order\nto understand how meaningful summaries look like. We present two complementary\nstudies: a data-search diary study with 69 students, which offers insight into\nthe information needs of people searching for data; and a summarisation study,\nwith a lab and a crowdsourcing component with overall 80 data-literate\nparticipants, which produced summaries for 25 datasets. In each study we\ncarried out a qualitative analysis to identify key themes and commonly\nmentioned dataset attributes, which people consider when searching and making\nsense of data. The results helped us design a template to create more\nmeaningful textual representations of data, alongside guidelines for improving\ndata-search experience overall.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 23:26:38 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Koesten", "Laura", ""], ["Simperl", "Elena", ""], ["Kacprzak", "Emilia", ""], ["Blount", "Tom", ""], ["Tennison", "Jeni", ""]]}, {"id": "1810.12627", "submitter": "Hans-J\\\"urgen Profitlich", "authors": "Daniel Sonntag, Hans-J\\\"urgen Profitlich", "title": "An architecture of open-source tools to combine textual information\n  extraction, faceted search and information visualisation", "comments": "Preprint submitted to Artificial Intelligence in Medicine", "journal-ref": null, "doi": "10.1016/j.artmed.2018.08.003", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents our steps to integrate complex and partly unstructured\nmedical data into a clinical research database with subsequent decision\nsupport. Our main application is an integrated faceted search tool, accompanied\nby the visualisation of results of automatic information extraction from\ntextual documents. We describe the details of our technical architecture\n(open-source tools), to be replicated at other universities, research\ninstitutes, or hospitals. Our exemplary use cases are nephrology and\nmammography. The software was first developed in the nephrology domain and then\nadapted to the mammography use case. We report on these case studies,\nillustrating how the application can be used by a clinician and which questions\ncan be answered. We show that our architecture and the employed software\nmodules are suitable for both areas of application with a limited amount of\nadaptations. For example, in nephrology we try to answer questions about the\ntemporal characteristics of event sequences to gain significant insight from\nthe data for cohort selection. We present a versatile time-line tool that\nenables the user to explore relations between a multitude of diagnosis and\nlaboratory values.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 10:20:42 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Sonntag", "Daniel", ""], ["Profitlich", "Hans-J\u00fcrgen", ""]]}, {"id": "1810.12770", "submitter": "Supriyo Mandal", "authors": "Supriyo Mandal and Abyayananda Maiti", "title": "Explicit Feedbacks Meet with Implicit Feedbacks : A Combined Approach\n  for Recommendation System", "comments": "12 pages. Accepted in Complex Networks, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems recommend items more accurately by analyzing users'\npotential interest on different brands' items. In conjunction with users'\nrating similarity, the presence of users' implicit feedbacks like clicking\nitems, viewing items specifications, watching videos etc. have been proved to\nbe helpful for learning users' embedding, that helps better rating prediction\nof users. Most existing recommender systems focus on modeling of ratings and\nimplicit feedbacks ignoring users' explicit feedbacks. Explicit feedbacks can\nbe used to validate the reliability of the particular users and can be used to\nlearn about the users' characteristic. Users' characteristic mean what type of\nreviewers they are. In this paper, we explore three different models for\nrecommendation with more accuracy focusing on users' explicit feedbacks and\nimplicit feedbacks. First one is RHC-PMF that predicts users' rating more\naccurately based on user's three explicit feedbacks (rating, helpfulness score\nand centrality) and second one is RV-PMF, where user's implicit feedback (view\nrelationship) is considered. Last one is RHCV-PMF, where both type of feedbacks\nare considered. In this model users' explicit feedbacks' similarity indicate\nthe similarity of their reliability and characteristic and implicit feedback's\nsimilarity indicates their preference similarity. Extensive experiments on real\nworld dataset, i.e. Amazon.com online review dataset shows that our models\nperform better compare to base-line models in term of users' rating prediction.\nRHCV-PMF model also performs better rating prediction compare to baseline\nmodels for cold start users and cold start items.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 06:03:29 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Mandal", "Supriyo", ""], ["Maiti", "Abyayananda", ""]]}, {"id": "1810.12897", "submitter": "Sumit Bhatia", "authors": "Sumit Bhatia and Deepak P", "title": "Topic-Specific Sentiment Analysis Can Help Identify Political Ideology", "comments": "Presented at EMNLP Workshop on Computational Approaches to\n  Subjectivity, Sentiment & Social Media Analysis, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ideological leanings of an individual can often be gauged by the sentiment\none expresses about different issues. We propose a simple framework that\nrepresents a political ideology as a distribution of sentiment polarities\ntowards a set of topics. This representation can then be used to detect\nideological leanings of documents (speeches, news articles, etc.) based on the\nsentiments expressed towards different topics. Experiments performed using a\nwidely used dataset show the promise of our proposed approach that achieves\ncomparable performance to other methods despite being much simpler and more\ninterpretable.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 17:49:46 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Bhatia", "Sumit", ""], ["P", "Deepak", ""]]}, {"id": "1810.12936", "submitter": "Kai Hui", "authors": "Canjia Li, Yingfei Sun, Ben He, Le Wang, Kai Hui, Andrew Yates, Le\n  Sun, Jungang Xu", "title": "NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc\n  Information Retrieval", "comments": "Full paper in EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-relevance feedback (PRF) is commonly used to boost the performance of\ntraditional information retrieval (IR) models by using top-ranked documents to\nidentify and weight new query terms, thereby reducing the effect of\nquery-document vocabulary mismatches. While neural retrieval models have\nrecently demonstrated strong results for ad-hoc retrieval, combining them with\nPRF is not straightforward due to incompatibilities between existing PRF\napproaches and neural architectures. To bridge this gap, we propose an\nend-to-end neural PRF framework that can be used with existing neural IR models\nby embedding different neural models as building blocks. Extensive experiments\non two standard test collections confirm the effectiveness of the proposed NPRF\nframework in improving the performance of two state-of-the-art neural IR\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 18:03:12 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Li", "Canjia", ""], ["Sun", "Yingfei", ""], ["He", "Ben", ""], ["Wang", "Le", ""], ["Hui", "Kai", ""], ["Yates", "Andrew", ""], ["Sun", "Le", ""], ["Xu", "Jungang", ""]]}, {"id": "1810.13103", "submitter": "Zhongdao Wang", "authors": "Zhongdao Wang, Liang Zheng, Shengjin Wang", "title": "Query Adaptive Late Fusion for Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature fusion is a commonly used strategy in image retrieval tasks, which\naggregates the matching responses of multiple visual features. Feasible sets of\nfeatures can be either descriptors (SIFT, HSV) for an entire image or the same\ndescriptor for different local parts (face, body). Ideally, the to-be-fused\nheterogeneous features are pre-assumed to be discriminative and complementary\nto each other. However, the effectiveness of different features varies\ndramatically according to different queries. That is to say, for some queries,\na feature may be neither discriminative nor complementary to existing ones,\nwhile for other queries, the feature suffices. As a result, it is important to\nestimate the effectiveness of features in a query-adaptive manner. To this end,\nthis article proposes a new late fusion scheme at the score level. We base our\nmethod on the observation that the sorted score curves contain patterns that\ndescribe their effectiveness. For example, an \"L\"-shaped curve indicates that\nthe feature is discriminative while a gradually descending curve suggests a bad\nfeature. As such, this paper introduces a query-adaptive late fusion pipeline.\nIn the hand-crafted version, it can be an unsupervised approach to tasks like\nparticular object retrieval. In the learning version, it can also be applied to\nsupervised tasks like person recognition and pedestrian retrieval, based on a\ntrainable neural module. Extensive experiments are conducted on two object\nretrieval datasets and one person recognition dataset. We show that our method\nis able to highlight the good features and suppress the bad ones, is resilient\nto distractor features, and achieves very competitive retrieval accuracy\ncompared with the state of the art. In an additional person re-identification\ndataset, the application scope and limitation of the proposed method are\nstudied.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 04:51:16 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Wang", "Zhongdao", ""], ["Zheng", "Liang", ""], ["Wang", "Shengjin", ""]]}, {"id": "1810.13144", "submitter": "Agus Sulistya", "authors": "Agus Sulistya, Gede Artha Azriadi Prana, Abhishek Sharma, David Lo,\n  Christoph Treude", "title": "SIEVE: Helping Developers Sift Wheat from Chaff via Cross-Platform\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software developers have benefited from various sources of knowledge such as\nforums, question-and-answer sites, and social media platforms to help them in\nvarious tasks. Extracting software-related knowledge from different platforms\ninvolves many challenges. In this paper, we propose an approach to improve the\neffectiveness of knowledge extraction tasks by performing cross-platform\nanalysis. Our approach is based on transfer representation learning and word\nembeddings, leveraging information extracted from a source platform which\ncontains rich domain-related content. The information extracted is then used to\nsolve tasks in another platform (considered as target platform) with less\ndomain-related contents. We first build a word embeddings model as a\nrepresentation learned from the source platform, and use the model to improve\nthe performance of knowledge extraction tasks in the target platform. We\nexperiment with Software Engineering Stack Exchange and Stack Overflow as\nsource platforms, and two different target platforms, i.e., Twitter and\nYouTube. Our experiments show that our approach improves performance of\nexisting work for the tasks of identifying software-related tweets and helpful\nYouTube comments.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 08:04:06 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Sulistya", "Agus", ""], ["Prana", "Gede Artha Azriadi", ""], ["Sharma", "Abhishek", ""], ["Lo", "David", ""], ["Treude", "Christoph", ""]]}]