[{"id": "2003.00097", "submitter": "Xiangyu Zhao", "authors": "Xiangyu Zhao, Xudong Zheng, Xiwang Yang, Xiaobing Liu, Jiliang Tang", "title": "Jointly Learning to Recommend and Advertise", "comments": "Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery\n  and Data Mining (KDD '20), August 23--27, 2020, Virtual Event, CA, USA", "journal-ref": null, "doi": "10.1145/3394486.3403384", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online recommendation and advertising are two major income channels for\nonline recommendation platforms (e.g. e-commerce and news feed site). However,\nmost platforms optimize recommending and advertising strategies by different\nteams separately via different techniques, which may lead to suboptimal overall\nperformances. To this end, in this paper, we propose a novel two-level\nreinforcement learning framework to jointly optimize the recommending and\nadvertising strategies, where the first level generates a list of\nrecommendations to optimize user experience in the long run; then the second\nlevel inserts ads into the recommendation list that can balance the immediate\nadvertising revenue from advertisers and the negative influence of ads on\nlong-term user experience. To be specific, the first level tackles high\ncombinatorial action space problem that selects a subset items from the large\nitem space; while the second level determines three internally related tasks,\ni.e., (i) whether to insert an ad, and if yes, (ii) the optimal ad and (iii)\nthe optimal location to insert. The experimental results based on real-world\ndata demonstrate the effectiveness of the proposed framework. We have released\nthe implementation code to ease reproductivity.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 22:32:05 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 00:22:54 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Zhao", "Xiangyu", ""], ["Zheng", "Xudong", ""], ["Yang", "Xiwang", ""], ["Liu", "Xiaobing", ""], ["Tang", "Jiliang", ""]]}, {"id": "2003.00107", "submitter": "John Kastner", "authors": "John Kastner, Hanan Samet, Hong Wei", "title": "NewsStand CoronaViz: A Map Query Interface for Spatio-Temporal and\n  Spatio-Textual Monitoring of Disease Spread", "comments": "For associated demonstration website see https://coronaviz.umiacs.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid continuing spread of COVID-19, it is clearly important to be\nable to track the progress of the virus over time in order to be better\nprepared to anticipate its emergence and spread in new regions as well as\ndeclines in its presence in regions thereby leading to or justifying\n\"reopening\" decisions. There are many applications and web sites that monitor\nofficially released numbers of cases which are likely to be the most accurate\nmethods for tracking the progress of the virus; however, they will not\nnecessarily paint a complete picture. To begin filling any gaps in official\nreports, we have developed the NewsStand CoronaViz web application\n(https://coronaviz.umiacs.io) that can run on desktops and mobile devices that\nallows users to explore the geographic spread in discussions about the virus\nthrough analysis of keyword prevalence in geotagged news articles and tweets in\nrelation to the real spread of the virus as measured by confirmed case numbers\nreported by the appropriate authorities. NewsStand CoronaViz users have access\nto dynamic variants of the disease-related variables corresponding to the\nnumbers of confirmed cases, active cases, deaths, and recoveries (where they\nare provided) via a map query interface. It has the ability to step forward and\nbackward in time using both a variety of temporal window sizes (day, week,\nmonth, or combinations thereof) in addition to user-defined varying spatial\nwindow sizes specified by direct manipulation actions (e.g., pan, zoom, and\nhover) as well as textually (e.g., by the name of the containing country, state\nor province, or county as well as textually-specified spatially-adjacent\ncombinations thereof), and finally by the amount of spatio-temporally-varying\nnews and tweet volume involving COVID-19.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 23:11:02 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 20:54:29 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 15:28:43 GMT"}, {"version": "v4", "created": "Mon, 4 May 2020 15:42:56 GMT"}, {"version": "v5", "created": "Sat, 23 May 2020 17:17:24 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Kastner", "John", ""], ["Samet", "Hanan", ""], ["Wei", "Hong", ""]]}, {"id": "2003.00129", "submitter": "Duc P. Truong", "authors": "Duc P. Truong, Erik Skau, Vladimir I. Valtchinov, Boian S. Alexandrov", "title": "Determination of Latent Dimensionality in International Trade Flow", "comments": null, "journal-ref": null, "doi": "10.1088/2632-2153/aba9ee", "report-no": null, "categories": "cs.LG cs.IR econ.GN q-fin.EC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, high-dimensional data is ubiquitous in data science, which\nnecessitates the development of techniques to decompose and interpret such\nmultidimensional (aka tensor) datasets. Finding a low dimensional\nrepresentation of the data, that is, its inherent structure, is one of the\napproaches that can serve to understand the dynamics of low dimensional latent\nfeatures hidden in the data. Nonnegative RESCAL is one such technique,\nparticularly well suited to analyze self-relational data, such as dynamic\nnetworks found in international trade flows. Nonnegative RESCAL computes a low\ndimensional tensor representation by finding the latent space containing\nmultiple modalities. Estimating the dimensionality of this latent space is\ncrucial for extracting meaningful latent features. Here, to determine the\ndimensionality of the latent space with nonnegative RESCAL, we propose a latent\ndimension determination method which is based on clustering of the solutions of\nmultiple realizations of nonnegative RESCAL decompositions. We demonstrate the\nperformance of our model selection method on synthetic data and then we apply\nour method to decompose a network of international trade flows data from\nInternational Monetary Fund and validate the resulting features against\nempirical facts from economic literature.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 00:06:01 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Truong", "Duc P.", ""], ["Skau", "Erik", ""], ["Valtchinov", "Vladimir I.", ""], ["Alexandrov", "Boian S.", ""]]}, {"id": "2003.00134", "submitter": "Saurav Manchanda", "authors": "Khoa D. Doan and Saurav Manchanda and Sarkhan Badirli and Chandan K.\n  Reddy", "title": "Image Hashing by Minimizing Discrete Component-wise Wasserstein Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image hashing is one of the fundamental problems that demand both efficient\nand effective solutions for various practical scenarios. Adversarial\nautoencoders are shown to be able to implicitly learn a robust,\nlocality-preserving hash function that generates balanced and high-quality hash\ncodes. However, the existing adversarial hashing methods are inefficient to be\nemployed for large-scale image retrieval applications. Specifically, they\nrequire an exponential number of samples to be able to generate optimal hash\ncodes and a significantly high computational cost to train. In this paper, we\nshow that the high sample-complexity requirement often results in sub-optimal\nretrieval performance of the adversarial hashing methods. To address this\nchallenge, we propose a new adversarial-autoencoder hashing approach that has a\nmuch lower sample requirement and computational cost. Specifically, by\nexploiting the desired properties of the hash function in the low-dimensional,\ndiscrete space, our method efficiently estimates a better variant of\nWasserstein distance by averaging a set of easy-to-compute one-dimensional\nWasserstein distances. The resulting hashing approach has an order-of-magnitude\nbetter sample complexity, thus better generalization property, compared to the\nother adversarial hashing methods. In addition, the computational cost is\nsignificantly reduced using our approach. We conduct experiments on several\nreal-world datasets and show that the proposed method outperforms the competing\nhashing methods, achieving up to 10% improvement over the current\nstate-of-the-art image hashing methods. The code accompanying this paper is\navailable on Github (https://github.com/khoadoan/adversarial-hashing).\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 00:22:53 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 20:45:15 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 01:33:29 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Doan", "Khoa D.", ""], ["Manchanda", "Saurav", ""], ["Badirli", "Sarkhan", ""], ["Reddy", "Chandan K.", ""]]}, {"id": "2003.00559", "submitter": "Sai Ravela", "authors": "Kshitij Bakliwal and Sai Ravela", "title": "The Sloop System for Individual Animal Identification with Deep Learning", "comments": "To appear in WACV 2020 Workshop on Deep Learning for\n  Re-Identification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MIT Sloop system indexes and retrieves photographs from databases of\nnon-stationary animal population distributions. To do this, it adaptively\nrepresents and matches generic visual feature representations using sparse\nrelevance feedback from experts and crowds. Here, we describe the Sloop system\nand its application, then compare its approach to a standard deep learning\nformulation. We then show that priming with amplitude and deformation features\nrequires very shallow networks to produce superior recognition results. Results\nsuggest that relevance feedback, which enables Sloop's high-recall performance\nmay also be essential for deep learning approaches to individual identification\nto deliver comparable results.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 19:08:06 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Bakliwal", "Kshitij", ""], ["Ravela", "Sai", ""]]}, {"id": "2003.00602", "submitter": "Monica Ribero", "authors": "M\\'onica Ribero, Jette Henderson, Sinead Williamson, Haris Vikalo", "title": "Federating Recommendations Using Differentially Private Prototypes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods allow us to make recommendations to users in\napplications across fields including entertainment, dating, and commerce, by\nexploiting similarities in users' interaction patterns. However, in domains\nthat demand protection of personally sensitive data, such as medicine or\nbanking, how can we learn such a model without accessing the sensitive data,\nand without inadvertently leaking private information? We propose a new\nfederated approach to learning global and local private models for\nrecommendation without collecting raw data, user statistics, or information\nabout personal preferences. Our method produces a set of prototypes that allows\nus to infer global behavioral patterns, while providing differential privacy\nguarantees for users in any database of the system. By requiring only two\nrounds of communication, we both reduce the communication costs and avoid the\nexcessive privacy loss associated with iterative procedures. We test our\nframework on synthetic data as well as real federated medical data and\nMovielens ratings data. We show local adaptation of the global model allows our\nmethod to outperform centralized matrix-factorization-based recommender system\nmodels, both in terms of accuracy of matrix reconstruction and in terms of\nrelevance of the recommendations, while maintaining provable privacy\nguarantees. We also show that our method is more robust and is characterized by\nsmaller variance than individual models learned by independent entities.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 22:21:31 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Ribero", "M\u00f3nica", ""], ["Henderson", "Jette", ""], ["Williamson", "Sinead", ""], ["Vikalo", "Haris", ""]]}, {"id": "2003.00708", "submitter": "Gaurav Verma", "authors": "Gaurav Verma, Vishwa Vinay, Sahil Bansal, Shashank Oberoi, Makkunda\n  Sharma, Prakhar Gupta", "title": "Using Image Captions and Multitask Learning for Recommending Query\n  Reformulations", "comments": "Accepted as a full paper at ECIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive search sessions often contain multiple queries, where the user\nsubmits a reformulated version of the previous query in response to the\noriginal results. We aim to enhance the query recommendation experience for a\ncommercial image search engine. Our proposed methodology incorporates current\nstate-of-the-art practices from relevant literature -- the use of\ngeneration-based sequence-to-sequence models that capture session context, and\na multitask architecture that simultaneously optimizes the ranking of results.\nWe extend this setup by driving the learning of such a model with captions of\nclicked images as the target, instead of using the subsequent query within the\nsession. Since these captions tend to be linguistically richer, the\nreformulation mechanism can be seen as assistance to construct more descriptive\nqueries. In addition, via the use of a pairwise loss for the secondary ranking\ntask, we show that the generated reformulations are more diverse.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 08:22:46 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Verma", "Gaurav", ""], ["Vinay", "Vishwa", ""], ["Bansal", "Sahil", ""], ["Oberoi", "Shashank", ""], ["Sharma", "Makkunda", ""], ["Gupta", "Prakhar", ""]]}, {"id": "2003.00807", "submitter": "Jay Kumar", "authors": "Jay Kumar", "title": "Fake Review Detection Using Behavioral and Contextual Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User reviews reflect significant value of product in the world of e-market.\nMany firms or product providers hire spammers for misleading new customers by\nposting spam reviews. There are three types of fake reviews, untruthful\nreviews, brand reviews and non-reviews. All three types mislead the new\ncustomers. A multinomial organization \"Yelp\" is separating fake reviews from\nnon-fake reviews since last decade. However, there are many e-commerce sites\nwhich do not filter fake and non-fake reviews separately. Automatic fake review\ndetection is focused by researcher for last ten years. Many approaches and\nfeature set are proposed for improving classification model of fake review\ndetection. There are two types of dataset commonly used in this research area:\npsuedo fake and real life reviews. Literature reports low performance of\nclassification model real life dataset if compared with pseudo fake reviews.\nAfter investigation behavioral and contextual features are proved important for\nfake review detection Our research has exploited important behavioral feature\nof reviewer named as \"reviewer deviation\". Our study comprises of investigating\nreviewer deviation with other contextual and behavioral features. We\nempirically proved importance of selected feature set for classification model\nto identify fake reviews. We ranked features in selected feature set where\nreviewer deviation achieved ninth rank. To assess the viability of selected\nfeature set we scaled dataset and concluded that scaling dataset can improve\nrecall as well as accuracy. Our selected feature set contains a contextual\nfeature which capture text similarity between reviews of a reviewer. We\nexperimented on NNC, LTC and BM25 term weighting schemes for calculating text\nsimilarity of reviews. We report that BM25 outperformed other term weighting\nscheme.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 10:36:40 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Kumar", "Jay", ""]]}, {"id": "2003.00810", "submitter": "Bharath K P", "authors": "Anirudh Itagi, Ritam Sil, Saurav Mohapatra, Subham Rout, Bharath K P,\n  Karthik R, Rajesh Kumar Muthu", "title": "Medicine Strip Identification using 2-D Cepstral Feature Extraction and\n  Multiclass Classification Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Misclassification of medicine is perilous to the health of a patient, more so\nif the said patient is visually impaired or simply did not recognize the color,\nshape or type of medicine strip. This paper proposes a method for\nidentification of medicine strips by 2-D cepstral analysis of their images\nfollowed by performing classification that has been done using the K-Nearest\nNeighbor (KNN), Support Vector Machine (SVM) and Logistic Regression (LR)\nClassifiers. The 2-D cepstral features extracted are extremely distinct to a\nmedicine strip and consequently make identifying them exceptionally accurate.\nThis paper also proposes the Color Gradient and Pill shape Feature (CGPF)\nextraction procedure and discusses the Binary Robust Invariant Scalable\nKeypoints (BRISK) algorithm as well. The mentioned algorithms were implemented\nand their identification results have been compared.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 09:45:01 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Itagi", "Anirudh", ""], ["Sil", "Ritam", ""], ["Mohapatra", "Saurav", ""], ["Rout", "Subham", ""], ["P", "Bharath K", ""], ["R", "Karthik", ""], ["Muthu", "Rajesh Kumar", ""]]}, {"id": "2003.00911", "submitter": "Fuzhen Zhuang", "authors": "Qingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong\n  and Qing He", "title": "A Survey on Knowledge Graph-Based Recommender Systems", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve the information explosion problem and enhance user experience in\nvarious online applications, recommender systems have been developed to model\nusers preferences. Although numerous efforts have been made toward more\npersonalized recommendations, recommender systems still suffer from several\nchallenges, such as data sparsity and cold start. In recent years, generating\nrecommendations with the knowledge graph as side information has attracted\nconsiderable interest. Such an approach can not only alleviate the\nabovementioned issues for a more accurate recommendation, but also provide\nexplanations for recommended items. In this paper, we conduct a systematical\nsurvey of knowledge graph-based recommender systems. We collect recently\npublished papers in this field and summarize them from two perspectives. On the\none hand, we investigate the proposed algorithms by focusing on how the papers\nutilize the knowledge graph for accurate and explainable recommendation. On the\nother hand, we introduce datasets used in these works. Finally, we propose\nseveral potential research directions in this field.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 02:26:30 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Guo", "Qingyu", ""], ["Zhuang", "Fuzhen", ""], ["Qin", "Chuan", ""], ["Zhu", "Hengshu", ""], ["Xie", "Xing", ""], ["Xiong", "Hui", ""], ["He", "Qing", ""]]}, {"id": "2003.01006", "submitter": "Jennifer D'Souza", "authors": "Jennifer D'Souza, Anett Hoppe, Arthur Brack, Mohamad Yaser Jaradeh,\n  S\\\"oren Auer, Ralph Ewerth", "title": "The STEM-ECR Dataset: Grounding Scientific Entity References in STEM\n  Scholarly Content to Authoritative Encyclopedic and Lexicographic Sources", "comments": "Published in LREC 2020. Publication URL\n  https://www.aclweb.org/anthology/2020.lrec-1.268/; Dataset DOI\n  https://doi.org/10.25835/0017546", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the STEM (Science, Technology, Engineering, and Medicine)\nDataset for Scientific Entity Extraction, Classification, and Resolution,\nversion 1.0 (STEM-ECR v1.0). The STEM-ECR v1.0 dataset has been developed to\nprovide a benchmark for the evaluation of scientific entity extraction,\nclassification, and resolution tasks in a domain-independent fashion. It\ncomprises abstracts in 10 STEM disciplines that were found to be the most\nprolific ones on a major publishing platform. We describe the creation of such\na multidisciplinary corpus and highlight the obtained findings in terms of the\nfollowing features: 1) a generic conceptual formalism for scientific entities\nin a multidisciplinary scientific context; 2) the feasibility of the\ndomain-independent human annotation of scientific entities under such a generic\nformalism; 3) a performance benchmark obtainable for automatic extraction of\nmultidisciplinary scientific entities using BERT-based neural models; 4) a\ndelineated 3-step entity resolution procedure for human annotation of the\nscientific entities via encyclopedic entity linking and lexicographic word\nsense disambiguation; and 5) human evaluations of Babelfy returned encyclopedic\nlinks and lexicographic senses for our entities. Our findings cumulatively\nindicate that human annotation and automatic learning of multidisciplinary\nscientific concepts as well as their semantic disambiguation in a wide-ranging\nsetting as STEM is reasonable.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 16:35:17 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 11:38:40 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 08:58:48 GMT"}, {"version": "v4", "created": "Tue, 28 Jul 2020 09:45:52 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["D'Souza", "Jennifer", ""], ["Hoppe", "Anett", ""], ["Brack", "Arthur", ""], ["Jaradeh", "Mohamad Yaser", ""], ["Auer", "S\u00f6ren", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2003.01271", "submitter": "Andrey Kormilitzin", "authors": "Andrey Kormilitzin, Nemanja Vaci, Qiang Liu, Alejo Nevado-Holgado", "title": "Med7: a transferable clinical natural language processing model for\n  electronic health records", "comments": "16 pages, 1 figure, 15 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of clinical natural language processing has been advanced\nsignificantly since the introduction of deep learning models. The\nself-supervised representation learning and the transfer learning paradigm\nbecame the methods of choice in many natural language processing application,\nin particular in the settings with the dearth of high quality manually\nannotated data. Electronic health record systems are ubiquitous and the\nmajority of patients' data are now being collected electronically and in\nparticular in the form of free text. Identification of medical concepts and\ninformation extraction is a challenging task, yet important ingredient for\nparsing unstructured data into structured and tabulated format for downstream\nanalytical tasks. In this work we introduced a named-entity recognition model\nfor clinical natural language processing. The model is trained to recognise\nseven categories: drug names, route, frequency, dosage, strength, form,\nduration. The model was first self-supervisedly pre-trained by predicting the\nnext word, using a collection of 2 million free-text patients' records from\nMIMIC-III corpora and then fine-tuned on the named-entity recognition task. The\nmodel achieved a lenient (strict) micro-averaged F1 score of 0.957 (0.893)\nacross all seven categories. Additionally, we evaluated the transferability of\nthe developed model using the data from the Intensive Care Unit in the US to\nsecondary care mental health records (CRIS) in the UK. A direct application of\nthe trained NER model to CRIS data resulted in reduced performance of F1=0.762,\nhowever after fine-tuning on a small sample from CRIS, the model achieved a\nreasonable performance of F1=0.944. This demonstrated that despite a close\nsimilarity between the data sets and the NER tasks, it is essential to\nfine-tune on the target domain data in order to achieve more accurate results.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 00:55:43 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 13:16:15 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Kormilitzin", "Andrey", ""], ["Vaci", "Nemanja", ""], ["Liu", "Qiang", ""], ["Nevado-Holgado", "Alejo", ""]]}, {"id": "2003.01792", "submitter": "Yaotian Wang", "authors": "Yaotian Wang, Xiaohang Sun and Jason W. Fleischer", "title": "When deep denoising meets iterative phase retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering a signal from its Fourier intensity underlies many important\napplications, including lensless imaging and imaging through scattering media.\nConventional algorithms for retrieving the phase suffer when noise is present\nbut display global convergence when given clean data. Neural networks have been\nused to improve algorithm robustness, but efforts to date are sensitive to\ninitial conditions and give inconsistent performance. Here, we combine\niterative methods from phase retrieval with image statistics from deep\ndenoisers, via regularization-by-denoising. The resulting methods inherit the\nadvantages of each approach and outperform other noise-robust phase retrieval\nalgorithms. Our work paves the way for hybrid imaging methods that integrate\nmachine-learned constraints in conventional algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 21:00:45 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Wang", "Yaotian", ""], ["Sun", "Xiaohang", ""], ["Fleischer", "Jason W.", ""]]}, {"id": "2003.01892", "submitter": "Jiawei Chen", "authors": "Jiawei Chen, Can Wang, Sheng Zhou, Qihao Shi, Jingbang Chen, Yan Feng,\n  Chun Chen", "title": "Fast Adaptively Weighted Matrix Factorization for Recommendation with\n  Implicit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation from implicit feedback is a highly challenging task due to the\nlack of the reliable observed negative data. A popular and effective approach\nfor implicit recommendation is to treat unobserved data as negative but\ndownweight their confidence. Naturally, how to assign confidence weights and\nhow to handle the large number of the unobserved data are two key problems for\nimplicit recommendation models. However, existing methods either pursuit fast\nlearning by manually assigning simple confidence weights, which lacks\nflexibility and may create empirical bias in evaluating user's preference; or\nadaptively infer personalized confidence weights but suffer from low\nefficiency. To achieve both adaptive weights assignment and efficient model\nlearning, we propose a fast adaptively weighted matrix factorization (FAWMF)\nbased on variational auto-encoder. The personalized data confidence weights are\nadaptively assigned with a parameterized neural network (function) and the\nnetwork can be inferred from the data. Further, to support fast and stable\nlearning of FAWMF, a new specific batch-based learning algorithm fBGD has been\ndeveloped, which trains on all feedback data but its complexity is linear to\nthe number of observed data. Extensive experiments on real-world datasets\ndemonstrate the superiority of the proposed FAWMF and its learning algorithm\nfBGD.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 04:50:44 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Chen", "Jiawei", ""], ["Wang", "Can", ""], ["Zhou", "Sheng", ""], ["Shi", "Qihao", ""], ["Chen", "Jingbang", ""], ["Feng", "Yan", ""], ["Chen", "Chun", ""]]}, {"id": "2003.01917", "submitter": "Qiaoyu Tan", "authors": "Qiaoyu Tan, Ninghao Liu, Xing Zhao, Hongxia Yang, Jingren Zhou, Xia Hu", "title": "Learning to Hash with Graph Neural Networks for Recommender Systems", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph representation learning has attracted much attention in supporting high\nquality candidate search at scale. Despite its effectiveness in learning\nembedding vectors for objects in the user-item interaction network, the\ncomputational costs to infer users' preferences in continuous embedding space\nare tremendous. In this work, we investigate the problem of hashing with graph\nneural networks (GNNs) for high quality retrieval, and propose a simple yet\neffective discrete representation learning framework to jointly learn\ncontinuous and discrete codes. Specifically, a deep hashing with GNNs (HashGNN)\nis presented, which consists of two components, a GNN encoder for learning node\nrepresentations, and a hash layer for encoding representations to hash codes.\nThe whole architecture is trained end-to-end by jointly optimizing two losses,\ni.e., reconstruction loss from reconstructing observed links, and ranking loss\nfrom preserving the relative ordering of hash codes. A novel discrete\noptimization strategy based on straight through estimator (STE) with guidance\nis proposed. The principal idea is to avoid gradient magnification in\nback-propagation of STE with continuous embedding guidance, in which we begin\nfrom learning an easier network that mimic the continuous embedding and let it\nevolve during the training until it finally goes back to STE. Comprehensive\nexperiments over three publicly available and one real-world Alibaba company\ndatasets demonstrate that our model not only can achieve comparable performance\ncompared with its continuous counterpart but also runs multiple times faster\nduring inference.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 06:59:56 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Tan", "Qiaoyu", ""], ["Liu", "Ninghao", ""], ["Zhao", "Xing", ""], ["Yang", "Hongxia", ""], ["Zhou", "Jingren", ""], ["Hu", "Xia", ""]]}, {"id": "2003.02474", "submitter": "Junwei Zhang", "authors": "Min Gao, Junwei Zhang, Junliang Yu, Jundong Li, Junhao Wen and Qingyu\n  Xiong", "title": "Recommender Systems Based on Generative Adversarial Networks: A\n  Problem-Driven Perspective", "comments": "24 pages, 12 figures, 8 tables, the journal of Information Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems (RSs) now play a very important role in the online lives\nof people as they serve as personalized filters for users to find relevant\nitems from an array of options. Owing to their effectiveness, RSs have been\nwidely employed in consumer-oriented e-commerce platforms. However, despite\ntheir empirical successes, these systems still suffer from two limitations:\ndata noise and data sparsity. In recent years, generative adversarial networks\n(GANs) have garnered increased interest in many fields, owing to their strong\ncapacity to learn complex real data distributions; their abilities to enhance\nRSs by tackling the challenges these systems exhibit have also been\ndemonstrated in numerous studies. In general, two lines of research have been\nconducted, and their common ideas can be summarized as follows: (1) for the\ndata noise issue, adversarial perturbations and adversarial sampling-based\ntraining often serve as a solution; (2) for the data sparsity issue, data\naugmentation--implemented by capturing the distribution of real data under the\nminimax framework--is the primary coping strategy. To gain a comprehensive\nunderstanding of these research efforts, we review the corresponding studies\nand models, organizing them from a problem-driven perspective. More\nspecifically, we propose a taxonomy of these models, along with their detailed\ndescriptions and advantages. Finally, we elaborate on several open issues and\ncurrent trends in GAN-based RSs.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 08:05:38 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 14:17:20 GMT"}, {"version": "v3", "created": "Sun, 20 Sep 2020 09:21:41 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Gao", "Min", ""], ["Zhang", "Junwei", ""], ["Yu", "Junliang", ""], ["Li", "Jundong", ""], ["Wen", "Junhao", ""], ["Xiong", "Qingyu", ""]]}, {"id": "2003.02498", "submitter": "Palakorn Achananuparp", "authors": "Helena H. Lee, Ke Shu, Palakorn Achananuparp, Philips Kokoh Prasetyo,\n  Yue Liu, Ee-Peng Lim, Lav R. Varshney", "title": "RecipeGPT: Generative Pre-training Based Cooking Recipe Generation and\n  Evaluation System", "comments": "Accepted to WWW 2020. Demo track paper", "journal-ref": null, "doi": "10.1145/3366424.3383536", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interests in the automatic generation of cooking recipes have been growing\nsteadily over the past few years thanks to a large amount of online cooking\nrecipes. We present RecipeGPT, a novel online recipe generation and evaluation\nsystem. The system provides two modes of text generations: (1) instruction\ngeneration from given recipe title and ingredients; and (2) ingredient\ngeneration from recipe title and cooking instructions. Its back-end text\ngeneration module comprises a generative pre-trained language model GPT-2\nfine-tuned on a large cooking recipe dataset. Moreover, the recipe evaluation\nmodule allows the users to conveniently inspect the quality of the generated\nrecipe contents and store the results for future reference. RecipeGPT can be\naccessed online at https://recipegpt.org/.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 09:25:30 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Lee", "Helena H.", ""], ["Shu", "Ke", ""], ["Achananuparp", "Palakorn", ""], ["Prasetyo", "Philips Kokoh", ""], ["Liu", "Yue", ""], ["Lim", "Ee-Peng", ""], ["Varshney", "Lav R.", ""]]}, {"id": "2003.02546", "submitter": "ByungSoo Ko", "authors": "Byungsoo Ko, Geonmo Gu", "title": "Embedding Expansion: Augmentation in Embedding Space for Deep Metric\n  Learning", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the distance metric between pairs of samples has been studied for\nimage retrieval and clustering. With the remarkable success of pair-based\nmetric learning losses, recent works have proposed the use of generated\nsynthetic points on metric learning losses for augmentation and generalization.\nHowever, these methods require additional generative networks along with the\nmain network, which can lead to a larger model size, slower training speed, and\nharder optimization. Meanwhile, post-processing techniques, such as query\nexpansion and database augmentation, have proposed the combination of feature\npoints to obtain additional semantic information. In this paper, inspired by\nquery expansion and database augmentation, we propose an augmentation method in\nan embedding space for pair-based metric learning losses, called embedding\nexpansion. The proposed method generates synthetic points containing augmented\ninformation by a combination of feature points and performs hard negative pair\nmining to learn with the most informative feature representations. Because of\nits simplicity and flexibility, it can be used for existing metric learning\nlosses without affecting model size, training speed, or optimization\ndifficulty. Finally, the combination of embedding expansion and representative\nmetric learning losses outperforms the state-of-the-art losses and previous\nsample generation methods in both image retrieval and clustering tasks. The\nimplementation is publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 11:43:17 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 08:45:38 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 06:13:11 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Ko", "Byungsoo", ""], ["Gu", "Geonmo", ""]]}, {"id": "2003.02576", "submitter": "Antoine Amarilli", "authors": "Antoine Amarilli, Pierre Bourhis, Stefan Mengel, Matthias Niewerth", "title": "Constant-Delay Enumeration for Nondeterministic Document Spanners", "comments": "29 pages. Extended version of arXiv:1807.09320. Integrates all\n  corrections following reviewer feedback. Outside of some minor formatting\n  differences and tweaks, this paper is the same as the paper to appear in the\n  ACM TODS journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the information extraction framework known as document spanners,\nand study the problem of efficiently computing the results of the extraction\nfrom an input document, where the extraction task is described as a sequential\nvariable-set automaton (VA). We pose this problem in the setting of enumeration\nalgorithms, where we can first run a preprocessing phase and must then produce\nthe results with a small delay between any two consecutive results. Our goal is\nto have an algorithm which is tractable in combined complexity, i.e., in the\nsizes of the input document and the VA; while ensuring the best possible data\ncomplexity bounds in the input document size, i.e., constant delay in the\ndocument size. Several recent works at PODS'18 proposed such algorithms but\nwith linear delay in the document size or with an exponential dependency in\nsize of the (generally nondeterministic) input VA. In particular, Florenzano et\nal. suggest that our desired runtime guarantees cannot be met for general\nsequential VAs. We refute this and show that, given a nondeterministic\nsequential VA and an input document, we can enumerate the mappings of the VA on\nthe document with the following bounds: the preprocessing is linear in the\ndocument size and polynomial in the size of the VA, and the delay is\nindependent of the document and polynomial in the size of the VA. The resulting\nalgorithm thus achieves tractability in combined complexity and the best\npossible data complexity bounds. Moreover, it is rather easy to describe, in\nparticular for the restricted case of so-called extended VAs. Finally, we\nevaluate our algorithm empirically using a prototype implementation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 12:49:56 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 07:48:10 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 13:51:51 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Amarilli", "Antoine", ""], ["Bourhis", "Pierre", ""], ["Mengel", "Stefan", ""], ["Niewerth", "Matthias", ""]]}, {"id": "2003.02615", "submitter": "Imad Afyouni", "authors": "Faizan Ur Rehman, Imad Afyouni, Ahmed Lbath, Saleh Basalamah", "title": "Hadath: From Social Media Mapping to Multi-Resolution Event-Enriched\n  Maps", "comments": "Proceedings of 16th International Conference on Computer Systems and\n  Applications (AICCSA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Publicly available data is increasing rapidly, and will continue to grow with\nthe advancement of technologies in sensors, smartphones and the Internet of\nThings. Data from multiple sources can improve coverage and provide more\nrelevant knowledge about surrounding events and points of Interest. The\nstrength of one source of data can compensate for the shortcomings of another\nsource by providing supplementary information. Maps are also getting popular\nday-by-day and people are using it to achieve their daily task smoothly and\nefficiently. Starting from paper maps hundred years ago, multiple type of maps\nare available with point of interest, real-time traffic update or displaying\nmicro-blogs from social media. In this paper, we introduce Hadath, a system\nthat displays multi-resolution live events of interest from a variety of\navailable data sources. The system has been designed to be able to handle\nmultiple type of inputs by encapsulating incoming unstructured data into\ngeneric data packets. System extracts local events of interest from generic\ndata packets and identify their spatio-temporal scope to display such events on\na map, so that as a user changes the zoom level, only events of appropriate\nscope are displayed. This allows us to show live events in correspondence to\nthe scale of view - when viewing at a city scale, we see events of higher\nsignificance, while zooming in to a neighbourhood, events of a more local\ninterest are highlighted. The final output creates a unique and dynamic map\nbrowsing experience. Finally, to validate our proposed system, we conducted\nexperiments on social media data.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 13:53:59 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Rehman", "Faizan Ur", ""], ["Afyouni", "Imad", ""], ["Lbath", "Ahmed", ""], ["Basalamah", "Saleh", ""]]}, {"id": "2003.03064", "submitter": "Minh-Tien Nguyen", "authors": "Minh-Tien Nguyen, Viet-Anh Phan, Le Thai Linh, Nguyen Hong Son, Le\n  Tien Dung, Miku Hirano and Hajime Hotta", "title": "Transfer Learning for Information Extraction with Limited Data", "comments": "14 pages, 5 figures, PACLING conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a practical approach to fine-grained information\nextraction. Through plenty of experiences of authors in practically applying\ninformation extraction to business process automation, there can be found a\ncouple of fundamental technical challenges: (i) the availability of labeled\ndata is usually limited and (ii) highly detailed classification is required.\nThe main idea of our proposal is to leverage the concept of transfer learning,\nwhich is to reuse the pre-trained model of deep neural networks, with a\ncombination of common statistical classifiers to determine the class of each\nextracted term. To do that, we first exploit BERT to deal with the limitation\nof training data in real scenarios, then stack BERT with Convolutional Neural\nNetworks to learn hidden representation for classification. To validate our\napproach, we applied our model to an actual case of document processing, which\nis a process of competitive bids for government projects in Japan. We used 100\ndocuments for training and testing and confirmed that the model enables to\nextract fine-grained named entities with a detailed level of information\npreciseness specialized in the targeted business process, such as a department\nname of application receivers.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 08:08:20 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 13:56:57 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Nguyen", "Minh-Tien", ""], ["Phan", "Viet-Anh", ""], ["Linh", "Le Thai", ""], ["Son", "Nguyen Hong", ""], ["Dung", "Le Tien", ""], ["Hirano", "Miku", ""], ["Hotta", "Hajime", ""]]}, {"id": "2003.03155", "submitter": "Shrestha Ghosh", "authors": "Shrestha Ghosh, Simon Razniewski, Gerhard Weikum", "title": "Uncovering Hidden Semantics of Set Information in Knowledge Bases", "comments": "This work is under review in the Journal of Web Semantics, Special\n  Issue on Language Technology and Knowledge Graphs. This is a revision draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Bases (KBs) contain a wealth of structured information about\nentities and predicates. This paper focuses on set-valued predicates, i.e., the\nrelationship between an entity and a set of entities. In KBs, this information\nis often represented in two formats: (i) via counting predicates such as\nnumberOfChildren and staffSize, that store aggregated integers, and (ii) via\nenumerating predicates such as parentOf and worksFor, that store individual set\nmemberships. Both formats are typically complementary: unlike enumerating\npredicates, counting predicates do not give away individuals, but are more\nlikely informative towards the true set size, thus this coexistence could\nenable interesting applications in question answering and KB curation.\n  In this paper we aim at uncovering this hidden knowledge. We proceed in two\nsteps. (i) We identify set-valued predicates from a given KB predicates via\nstatistical and embedding-based features. (ii) We link counting predicates and\nenumerating predicates by a combination of co-occurrence, correlation and\ntextual relatedness metrics. We analyze the prevalence of count information in\nfour prominent knowledge bases, and show that our linking method achieves up to\n0.55 F1 score in set predicate identification versus 0.40 F1 score of a random\nselection, and normalized discounted gains of up to 0.84 at position 1 and 0.75\nat position 3 in relevant predicate alignments. Our predicate alignments are\nshowcased in a demonstration system available at\nhttps://counqer.mpi-inf.mpg.de/spo.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 12:26:00 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 10:20:33 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Ghosh", "Shrestha", ""], ["Razniewski", "Simon", ""], ["Weikum", "Gerhard", ""]]}, {"id": "2003.03264", "submitter": "Erion \\c{C}ano", "authors": "Erion \\c{C}ano and Maurizio Morisio", "title": "Quality of Word Embeddings on Sentiment Analysis Tasks", "comments": "6 pages, 4 figures, 2 tables. Published in proceedings of NLDB 2017,\n  the 22nd Conference of Natural Language Processing and Information Systems,\n  Liege, Belgium", "journal-ref": null, "doi": "10.1007/978-3-319-59569-6_42", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings or distributed representations of words are being used in\nvarious applications like machine translation, sentiment analysis, topic\nidentification etc. Quality of word embeddings and performance of their\napplications depends on several factors like training method, corpus size and\nrelevance etc. In this study we compare performance of a dozen of pretrained\nword embedding models on lyrics sentiment analysis and movie review polarity\ntasks. According to our results, Twitter Tweets is the best on lyrics sentiment\nanalysis, whereas Google News and Common Crawl are the top performers on movie\npolarity analysis. Glove trained models slightly outrun those trained with\nSkipgram. Also, factors like topic relevance and size of corpus significantly\nimpact the quality of the models. When medium or large-sized text sets are\navailable, obtaining word embeddings from same training dataset is usually the\nbest choice.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 15:03:08 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["\u00c7ano", "Erion", ""], ["Morisio", "Maurizio", ""]]}, {"id": "2003.03318", "submitter": "Marc Faddoul", "authors": "Marc Faddoul, Guillaume Chaslot and Hany Farid", "title": "A Longitudinal Analysis of YouTube's Promotion of Conspiracy Videos", "comments": "8 pages, 3 figures. This paper was first released on March 2nd, 2020\n  along with a coverage from the New York Times available at\n  https://www.nytimes.com/interactive/2020/03/02/technology/youtube-conspiracy-theory.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conspiracy theories have flourished on social media, raising concerns that\nsuch content is fueling the spread of disinformation, supporting extremist\nideologies, and in some cases, leading to violence. Under increased scrutiny\nand pressure from legislators and the public, YouTube announced efforts to\nchange their recommendation algorithms so that the most egregious conspiracy\nvideos are demoted and demonetized. To verify this claim, we have developed a\nclassifier for automatically determining if a video is conspiratorial (e.g.,\nthe moon landing was faked, the pyramids of Giza were built by aliens, end of\nthe world prophecies, etc.). We coupled this classifier with an emulation of\nYouTube's watch-next algorithm on more than a thousand popular informational\nchannels to obtain a year-long picture of the videos actively promoted by\nYouTube. We also obtained trends of the so-called filter-bubble effect for\nconspiracy theories.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 17:31:30 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Faddoul", "Marc", ""], ["Chaslot", "Guillaume", ""], ["Farid", "Hany", ""]]}, {"id": "2003.03531", "submitter": "Ali Choumane", "authors": "Ali Choumane, Zein Al Abidin Ibrahim", "title": "Friend Recommendation based on Hashtags Analysis", "comments": null, "journal-ref": "Journal of Computer Science: Theory and Application, vol. 4, no.\n  3., p. 63-69", "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks include millions of users constantly looking for new\nrelationships for personal or professional purposes. Social network sites\nrecommend friends based on relationship features and content information. A\nsignificant part of information shared every day is spread in Hashtags. None of\nthe existing content-based recommender systems uses the semantic of hashtags\nwhile suggesting new friends. Currently, hashtags are considered as strings\nwithout looking at their meanings. Social network sites group together people\nsharing exactly the same hashtags and never semantically close ones. We think\nthat hashtags encapsulate some people interests. In this paper, we propose a\nframework showing how a recommender system can benefit from hashtags to enrich\nusers' profiles. This framework consists of three main components: (1)\nconstructing user's profile based on shared hashtags, (2) matching method that\ncomputes semantic similarity between profiles, (3) grouping semantically close\nusers using clustering technics. The proposed framework has been tested on a\nTwitter dataset from the Stanford Large Network Dataset Collection consisting\nof 81306 profiles.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 08:00:58 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Choumane", "Ali", ""], ["Ibrahim", "Zein Al Abidin", ""]]}, {"id": "2003.03734", "submitter": "Gong Cheng", "authors": "Qingxia Liu, Gong Cheng, Kalpa Gunaratna, Yuzhong Qu", "title": "ESBM: An Entity Summarization BenchMark", "comments": "16 pages, accepted to the Resource Track of ESWC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Entity summarization is the problem of computing an optimal compact summary\nfor an entity by selecting a size-constrained subset of triples from RDF data.\nEntity summarization supports a multiplicity of applications and has led to\nfruitful research. However, there is a lack of evaluation efforts that cover\nthe broad spectrum of existing systems. One reason is a lack of benchmarks for\nevaluation. Some benchmarks are no longer available, while others are small and\nhave limitations. In this paper, we create an Entity Summarization BenchMark\n(ESBM) which overcomes the limitations of existing benchmarks and meets\nstandard desiderata for a benchmark. Using this largest available benchmark for\nevaluating general-purpose entity summarizers, we perform the most extensive\nexperiment to date where 9~existing systems are compared. Considering that all\nof these systems are unsupervised, we also implement and evaluate a supervised\nlearning based system for reference.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 07:12:20 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Liu", "Qingxia", ""], ["Cheng", "Gong", ""], ["Gunaratna", "Kalpa", ""], ["Qu", "Yuzhong", ""]]}, {"id": "2003.03736", "submitter": "Gong Cheng", "authors": "Qingxia Liu, Gong Cheng, Yuzhong Qu", "title": "DeepLENS: Deep Learning for Entity Summarization", "comments": "6 pages, submitted to DL4KG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Entity summarization has been a prominent task over knowledge graphs. While\nexisting methods are mainly unsupervised, we present DeepLENS, a simple yet\neffective deep learning model where we exploit textual semantics for encoding\ntriples and we score each candidate triple based on its interdependence on\nother triples. DeepLENS significantly outperformed existing methods on a public\nbenchmark.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 07:15:48 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Liu", "Qingxia", ""], ["Cheng", "Gong", ""], ["Qu", "Yuzhong", ""]]}, {"id": "2003.03741", "submitter": "Triet Le", "authors": "Triet H. M. Le, David Hin, Roland Croft, M. Ali Babar", "title": "PUMiner: Mining Security Posts from Developer Question and Answer\n  Websites with PU Learning", "comments": "Accepted for publication at the 17th Mining Software Repositories\n  2020 conference", "journal-ref": null, "doi": "10.1145/3379597.3387443", "report-no": null, "categories": "cs.SE cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security is an increasing concern in software development. Developer Question\nand Answer (Q&A) websites provide a large amount of security discussion.\nExisting studies have used human-defined rules to mine security discussions,\nbut these works still miss many posts, which may lead to an incomplete analysis\nof the security practices reported on Q&A websites. Traditional supervised\nMachine Learning methods can automate the mining process; however, the required\nnegative (non-security) class is too expensive to obtain. We propose a novel\nlearning framework, PUMiner, to automatically mine security posts from Q&A\nwebsites. PUMiner builds a context-aware embedding model to extract features of\nthe posts, and then develops a two-stage PU model to identify security content\nusing the labelled Positive and Unlabelled posts. We evaluate PUMiner on more\nthan 17.2 million posts on Stack Overflow and 52,611 posts on Security\nStackExchange. We show that PUMiner is effective with the validation\nperformance of at least 0.85 across all model configurations. Moreover,\nMatthews Correlation Coefficient (MCC) of PUMiner is 0.906, 0.534 and 0.084\npoints higher than one-class SVM, positive-similarity filtering, and one-stage\nPU models on unseen testing posts, respectively. PUMiner also performs well\nwith an MCC of 0.745 for scenarios where string matching totally fails. Even\nwhen the ratio of the labelled positive posts to the unlabelled ones is only\n1:100, PUMiner still achieves a strong MCC of 0.65, which is 160% better than\nfully-supervised learning. Using PUMiner, we provide the largest and up-to-date\nsecurity content on Q&A websites for practitioners and researchers.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 08:11:10 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Le", "Triet H. M.", ""], ["Hin", "David", ""], ["Croft", "Roland", ""], ["Babar", "M. Ali", ""]]}, {"id": "2003.03975", "submitter": "Yu Zheng", "authors": "Yu Zheng, Chen Gao, Xiangnan He, Yong Li, Depeng Jin", "title": "Price-aware Recommendation with Graph Convolutional Networks", "comments": "12 pages, 6 figures, to appear in ICDE2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, much research effort on recommendation has been devoted to\nmining user behaviors, i.e., collaborative filtering, along with the general\ninformation which describes users or items, e.g., textual attributes,\ncategorical demographics, product images, and so on. Price, an important factor\nin marketing --- which determines whether a user will make the final purchase\ndecision on an item --- surprisingly, has received relatively little scrutiny.\n  In this work, we aim at developing an effective method to predict user\npurchase intention with the focus on the price factor in recommender systems.\nThe main difficulties are two-fold: 1) the preference and sensitivity of a user\non item price are unknown, which are only implicitly reflected in the items\nthat the user has purchased, and 2) how the item price affects a user's\nintention depends largely on the product category, that is, the perception and\naffordability of a user on item price could vary significantly across\ncategories. Towards the first difficulty, we propose to model the transitive\nrelationship between user-to-item and item-to-price, taking the inspiration\nfrom the recently developed Graph Convolution Networks (GCN). The key idea is\nto propagate the influence of price on users with items as the bridge, so as to\nmake the learned user representations be price-aware. For the second\ndifficulty, we further integrate item categories into the propagation progress\nand model the possible pairwise interactions for predicting user-item\ninteractions. We conduct extensive experiments on two real-world datasets,\ndemonstrating the effectiveness of our GCN-based method in learning the\nprice-aware preference of users. Further analysis reveals that modeling the\nprice awareness is particularly useful for predicting user preference on items\nof unexplored categories.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 08:57:29 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Zheng", "Yu", ""], ["Gao", "Chen", ""], ["He", "Xiangnan", ""], ["Li", "Yong", ""], ["Jin", "Depeng", ""]]}, {"id": "2003.04094", "submitter": "Jacek Dabrowski", "authors": "Mikolaj Wieczorek (1), Andrzej Michalowski (1), Anna Wroblewska (1 and\n  2), Jacek Dabrowski (1) ((1) Synerise, (2) Warsaw University of Technology)", "title": "A Strong Baseline for Fashion Retrieval with Person Re-Identification\n  Models", "comments": "33 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion retrieval is the challenging task of finding an exact match for\nfashion items contained within an image. Difficulties arise from the\nfine-grained nature of clothing items, very large intra-class and inter-class\nvariance. Additionally, query and source images for the task usually come from\ndifferent domains - street photos and catalogue photos respectively. Due to\nthese differences, a significant gap in quality, lighting, contrast, background\nclutter and item presentation exists between domains. As a result, fashion\nretrieval is an active field of research both in academia and the industry.\n  Inspired by recent advancements in Person Re-Identification research, we\nadapt leading ReID models to be used in fashion retrieval tasks. We introduce a\nsimple baseline model for fashion retrieval, significantly outperforming\nprevious state-of-the-art results despite a much simpler architecture. We\nconduct in-depth experiments on Street2Shop and DeepFashion datasets and\nvalidate our results. Finally, we propose a cross-domain (cross-dataset)\nevaluation method to test the robustness of fashion retrieval models.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 12:50:15 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Wieczorek", "Mikolaj", "", "Synerise"], ["Michalowski", "Andrzej", "", "Synerise"], ["Wroblewska", "Anna", "", "1 and\n  2"], ["Dabrowski", "Jacek", "", "Synerise"]]}, {"id": "2003.04315", "submitter": "Benjamin Lee", "authors": "Benjamin Charles Germain Lee, Kyle Lo, Doug Downey, Daniel S. Weld", "title": "Explanation-Based Tuning of Opaque Machine Learners with Application to\n  Paper Recommendation", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in human-centered AI has shown the benefits of machine-learning\nsystems that can explain their predictions. Methods that allow users to tune a\nmodel in response to the explanations are similarly useful. While both\ncapabilities are well-developed for transparent learning models (e.g., linear\nmodels and GA2Ms), and recent techniques (e.g., LIME and SHAP) can generate\nexplanations for opaque models, no method currently exists for tuning of opaque\nmodels in response to explanations. This paper introduces LIMEADE, a general\nframework for tuning an arbitrary machine learning model based on an\nexplanation of the model's prediction. We apply our framework to Semantic\nSanity, a neural recommender system for scientific papers, and report on a\ndetailed user study, showing that our framework leads to significantly higher\nperceived user control, trust, and satisfaction.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 18:00:00 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Lee", "Benjamin Charles Germain", ""], ["Lo", "Kyle", ""], ["Downey", "Doug", ""], ["Weld", "Daniel S.", ""]]}, {"id": "2003.04628", "submitter": "Ygor Gallina", "authors": "Ygor Gallina, Florian Boudin, B\\'eatrice Daille", "title": "Large-Scale Evaluation of Keyphrase Extraction Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Keyphrase extraction models are usually evaluated under different, not\ndirectly comparable, experimental setups. As a result, it remains unclear how\nwell proposed models actually perform, and how they compare to each other. In\nthis work, we address this issue by presenting a systematic large-scale\nanalysis of state-of-the-art keyphrase extraction models involving multiple\nbenchmark datasets from various sources and domains. Our main results reveal\nthat state-of-the-art models are in fact still challenged by simple baselines\non some datasets. We also present new insights about the impact of using\nauthor- or reader-assigned keyphrases as a proxy for gold standard, and give\nrecommendations for strong baselines and reliable benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 10:46:12 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Gallina", "Ygor", ""], ["Boudin", "Florian", ""], ["Daille", "B\u00e9atrice", ""]]}, {"id": "2003.04888", "submitter": "Xin Liu", "authors": "Xin Liu, Yongbin Sun, Ziwei Liu, and Dahua Lin", "title": "Learning Diverse Fashion Collocation by Neural Graph Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion recommendation systems are highly desired by customers to find\nvisually-collocated fashion items, such as clothes, shoes, bags, etc. While\nexisting methods demonstrate promising results, they remain lacking in\nflexibility and diversity, e.g. assuming a fixed number of items or favoring\nsafe but boring recommendations. In this paper, we propose a novel fashion\ncollocation framework, Neural Graph Filtering, that models a flexible set of\nfashion items via a graph neural network. Specifically, we consider the visual\nembeddings of each garment as a node in the graph, and describe the\ninter-garment relationship as the edge between nodes. By applying symmetric\noperations on the edge vectors, this framework allows varying numbers of\ninputs/outputs and is invariant to their ordering. We further include a style\nclassifier augmented with focal loss to enable the collocation of significantly\ndiverse styles, which are inherently imbalanced in the training set. To\nfacilitate a comprehensive study on diverse fashion collocation, we reorganize\nAmazon Fashion dataset with carefully designed evaluation protocols. We\nevaluate the proposed approach on three popular benchmarks, the Polyvore\ndataset, the Polyvore-D dataset, and our reorganized Amazon Fashion dataset.\nExtensive experimental results show that our approach significantly outperforms\nthe state-of-the-art methods with over 10% improvements on the standard AUC\nmetric on the established tasks. More importantly, 82.5% of the users prefer\nour diverse-style recommendations over other alternatives in a real-world\nperception study.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 16:17:08 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Liu", "Xin", ""], ["Sun", "Yongbin", ""], ["Liu", "Ziwei", ""], ["Lin", "Dahua", ""]]}, {"id": "2003.05019", "submitter": "V\\'it Novotn\\'y", "authors": "V\\'it Novotn\\'y, Eniafe Festus Ayetiran, Michal \\v{S}tef\\'anik, and\n  Petr Sojka", "title": "Text classification with word embedding regularization and soft\n  similarity measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the seminal work of Mikolov et al., word embeddings have become the\npreferred word representations for many natural language processing tasks.\nDocument similarity measures extracted from word embeddings, such as the soft\ncosine measure (SCM) and the Word Mover's Distance (WMD), were reported to\nachieve state-of-the-art performance on semantic text similarity and text\nclassification.\n  Despite the strong performance of the WMD on text classification and semantic\ntext similarity, its super-cubic average time complexity is impractical. The\nSCM has quadratic worst-case time complexity, but its performance on text\nclassification has never been compared with the WMD. Recently, two word\nembedding regularization techniques were shown to reduce storage and memory\ncosts, and to improve training speed, document processing speed, and task\nperformance on word analogy, word similarity, and semantic text similarity.\nHowever, the effect of these techniques on text classification has not yet been\nstudied.\n  In our work, we investigate the individual and joint effect of the two word\nembedding regularization techniques on the document processing speed and the\ntask performance of the SCM and the WMD on text classification. For evaluation,\nwe use the $k$NN classifier and six standard datasets: BBCSPORT, TWITTER,\nOHSUMED, REUTERS-21578, AMAZON, and 20NEWS.\n  We show 39% average $k$NN test error reduction with regularized word\nembeddings compared to non-regularized word embeddings. We describe a practical\nprocedure for deriving such regularized embeddings through Cholesky\nfactorization. We also show that the SCM with regularized word embeddings\nsignificantly outperforms the WMD on text classification and is over 10,000\ntimes faster.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 22:07:34 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Novotn\u00fd", "V\u00edt", ""], ["Ayetiran", "Eniafe Festus", ""], ["\u0160tef\u00e1nik", "Michal", ""], ["Sojka", "Petr", ""]]}, {"id": "2003.05146", "submitter": "Nicolas Heist", "authors": "Nicolas Heist and Heiko Paulheim", "title": "Entity Extraction from Wikipedia List Pages", "comments": "Preprint of a full paper at European Semantic Web Conference 2020\n  (ESWC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When it comes to factual knowledge about a wide range of domains, Wikipedia\nis often the prime source of information on the web. DBpedia and YAGO, as large\ncross-domain knowledge graphs, encode a subset of that knowledge by creating an\nentity for each page in Wikipedia, and connecting them through edges. It is\nwell known, however, that Wikipedia-based knowledge graphs are far from\ncomplete. Especially, as Wikipedia's policies permit pages about subjects only\nif they have a certain popularity, such graphs tend to lack information about\nless well-known entities. Information about these entities is oftentimes\navailable in the encyclopedia, but not represented as an individual page. In\nthis paper, we present a two-phased approach for the extraction of entities\nfrom Wikipedia's list pages, which have proven to serve as a valuable source of\ninformation. In the first phase, we build a large taxonomy from categories and\nlist pages with DBpedia as a backbone. With distant supervision, we extract\ntraining data for the identification of new entities in list pages that we use\nin the second phase to train a classification model. With this approach we\nextract over 700k new entities and extend DBpedia with 7.5M new type statements\nand 3.8M new facts of high precision.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 07:48:46 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Heist", "Nicolas", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2003.05377", "submitter": "Raul Lima", "authors": "Raul de Ara\\'ujo Lima, R\\^omulo C\\'esar Costa de Sousa, Simone Diniz\n  Junqueira Barbosa, H\\'elio Cort\\^es Vieira Lopes", "title": "Brazilian Lyrics-Based Music Genre Classification Using a BLSTM Network", "comments": "7 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organize songs, albums, and artists in groups with shared similarity could be\ndone with the help of genre labels. In this paper, we present a novel approach\nfor automatic classifying musical genre in Brazilian music using only the song\nlyrics. This kind of classification remains a challenge in the field of Natural\nLanguage Processing. We construct a dataset of 138,368 Brazilian song lyrics\ndistributed in 14 genres. We apply SVM, Random Forest and a Bidirectional Long\nShort-Term Memory (BLSTM) network combined with different word embeddings\ntechniques to address this classification task. Our experiments show that the\nBLSTM method outperforms the other models with an F1-score average of $0.48$.\nSome genres like \"gospel\", \"funk-carioca\" and \"sertanejo\", which obtained 0.89,\n0.70 and 0.69 of F1-score, respectively, can be defined as the most distinct\nand easy to classify in the Brazilian musical genres context.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 05:39:21 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Lima", "Raul de Ara\u00fajo", ""], ["de Sousa", "R\u00f4mulo C\u00e9sar Costa", ""], ["Barbosa", "Simone Diniz Junqueira", ""], ["Lopes", "H\u00e9lio Cort\u00eas Vieira", ""]]}, {"id": "2003.05591", "submitter": "Mohammad Heydari", "authors": "Mohammad Heydari and Babak Teimourpour", "title": "Analysis of ResearchGate, A Community Detection Approach", "comments": "8 pages, 9 Figures, International Conference of Web Research, ICWR,\n  Tehran, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are living in the data age. Communications over scientific networks\ncreates new opportunities for researchers who aim to discover the hidden\npattern in these huge repositories. This study utilizes network science to\ncreate collaboration network of Iranian Scientific Institutions. A\nmodularity-based approach applied to find network communities. To reach a big\npicture of science production flow, analysis of the collaboration network is\ncrucial. Our results demonstrated that geographic location closeness and ethnic\nattributes has important roles in academic collaboration network establishment.\nBesides, it shows that famous scientific centers in the capital city of Iran,\nTehran has strong influence on the production flow of scientific activities.\nThese academic papers are mostly viewed and downloaded from the United State of\nAmerica, China, India, and Iran. The motivation of this research is that by\ndiscovering hidden communities in the network and finding the structure of\nintuitions communications, we can identify each scientific center research\npotential separately and clear mutual scientific fields. Therefore, an\nefficient strategic program can be designed, developed and tested to keep\nscientific centers in progress way and navigate their research goals into a\nstraight useful roadmap to identify and fill the unknown gaps.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 03:15:23 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 11:04:01 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Heydari", "Mohammad", ""], ["Teimourpour", "Babak", ""]]}, {"id": "2003.05731", "submitter": "Yue Zhao", "authors": "Yue Zhao, Xiyang Hu, Cheng Cheng, Cong Wang, Changlin Wan, Wen Wang,\n  Jianing Yang, Haoping Bai, Zheng Li, Cao Xiao, Yunlong Wang, Zhi Qiao, Jimeng\n  Sun, Leman Akoglu", "title": "SUOD: Accelerating Large-Scale Unsupervised Heterogeneous Outlier\n  Detection", "comments": "Proceedings of the 4th Conference on Machine Learning and Systems\n  (MLSys). The code is available at see http://github.com/yzhao062/SUOD. arXiv\n  admin note: text overlap with arXiv:2002.03222", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection (OD) is a key machine learning (ML) task for identifying\nabnormal objects from general samples with numerous high-stake applications\nincluding fraud detection and intrusion detection. Due to the lack of ground\ntruth labels, practitioners often have to build a large number of unsupervised,\nheterogeneous models (i.e., different algorithms with varying hyperparameters)\nfor further combination and analysis, rather than relying on a single model.\nHow to accelerate the training and scoring on new-coming samples by\noutlyingness (referred as prediction throughout the paper) with a large number\nof unsupervised, heterogeneous OD models? In this study, we propose a modular\nacceleration system, called SUOD, to address it. The proposed system focuses on\nthree complementary acceleration aspects (data reduction for high-dimensional\ndata, approximation for costly models, and taskload imbalance optimization for\ndistributed environment), while maintaining performance accuracy. Extensive\nexperiments on more than 20 benchmark datasets demonstrate SUOD's effectiveness\nin heterogeneous OD acceleration, along with a real-world deployment case on\nfraudulent claim analysis at IQVIA, a leading healthcare firm. We open-source\nSUOD for reproducibility and accessibility.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 00:22:50 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 21:57:38 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 14:38:38 GMT"}, {"version": "v4", "created": "Fri, 5 Mar 2021 01:55:27 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhao", "Yue", ""], ["Hu", "Xiyang", ""], ["Cheng", "Cheng", ""], ["Wang", "Cong", ""], ["Wan", "Changlin", ""], ["Wang", "Wen", ""], ["Yang", "Jianing", ""], ["Bai", "Haoping", ""], ["Li", "Zheng", ""], ["Xiao", "Cao", ""], ["Wang", "Yunlong", ""], ["Qiao", "Zhi", ""], ["Sun", "Jimeng", ""], ["Akoglu", "Leman", ""]]}, {"id": "2003.05753", "submitter": "Xiang Wang", "authors": "Xiang Wang, Yaokun Xu, Xiangnan He, Yixin Cao, Meng Wang, Tat-Seng\n  Chua", "title": "Reinforced Negative Sampling over Knowledge Graph for Recommendation", "comments": "WWW 2020 oral presentation", "journal-ref": null, "doi": "10.1145/3366423.3380098", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Properly handling missing data is a fundamental challenge in recommendation.\nMost present works perform negative sampling from unobserved data to supply the\ntraining of recommender models with negative signals. Nevertheless, existing\nnegative sampling strategies, either static or adaptive ones, are insufficient\nto yield high-quality negative samples --- both informative to model training\nand reflective of user real needs. In this work, we hypothesize that item\nknowledge graph (KG), which provides rich relations among items and KG\nentities, could be useful to infer informative and factual negative samples.\nTowards this end, we develop a new negative sampling model, Knowledge Graph\nPolicy Network (KGPolicy), which works as a reinforcement learning agent to\nexplore high-quality negatives. Specifically, by conducting our designed\nexploration operations, it navigates from the target positive interaction,\nadaptively receives knowledge-aware negative signals, and ultimately yields a\npotential negative item to train the recommender. We tested on a matrix\nfactorization (MF) model equipped with KGPolicy, and it achieves significant\nimprovements over both state-of-the-art sampling methods like DNS and IRGAN,\nand KG-enhanced recommender models like KGAT. Further analyses from different\nangles provide insights of knowledge-aware sampling. We release the codes and\ndatasets at https://github.com/xiangwang1223/kgpolicy.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 12:44:30 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Wang", "Xiang", ""], ["Xu", "Yaokun", ""], ["He", "Xiangnan", ""], ["Cao", "Yixin", ""], ["Wang", "Meng", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2003.05917", "submitter": "Niklas K\\\"uhl", "authors": "Niklas K\\\"uhl, Jan Scheurenbrand, Gerhard Satzger", "title": "Needmining: Identifying micro blog data containing customer needs", "comments": "European Conference on Information Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The design of new products and services starts with the identification of\nneeds of potential customers or users. Many existing methods like observations,\nsurveys, and experiments draw upon specific efforts to elicit unsatisfied needs\nfrom individuals. At the same time, a huge amount of user-generated content in\nmicro blogs is freely accessible at no cost. While this information is already\nanalyzed to monitor sentiments towards existing offerings, it has not yet been\ntapped for the elicitation of needs. In this paper, we lay an important\nfoundation for this endeavor: we propose a Machine Learning approach to\nidentify those posts that do express needs. Our evaluation of tweets in the\ne-mobility domain demonstrates that the small share of relevant tweets can be\nidentified with remarkable precision or recall results. Applied to huge data\nsets, the developed method should enable scalable need elicitation support for\ninnovation managers - across thousands of users, and thus augment the service\ndesign tool set available to him.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 17:31:51 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["K\u00fchl", "Niklas", ""], ["Scheurenbrand", "Jan", ""], ["Satzger", "Gerhard", ""]]}, {"id": "2003.06199", "submitter": "Takashi Okumura", "authors": "Ikki Ohmukai, Yasunori Yamamoto, Maori Ito, Takashi Okumura", "title": "Tracing patients' PLOD with mobile phones: Mitigation of epidemic risks\n  through patients' locational open data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the cases when public health authorities confirm a patient with highly\ncontagious disease, they release the summaries about patient locations and\ntravel information. However, due to privacy concerns, these releases do not\ninclude the detailed data and typically comprise the information only about\ncommercial facilities and public transportation used by the patients. We\naddressed this problem and proposed to release the patient location data as\nopen data represented in a structured form of the information described in\npress releases. Therefore, residents would be able to use these data for\nautomated estimation of the potential risks of contacts combined with the\nlocation information stored in their mobile phones. This paper proposes the\ndesign of the open data based on Resource Description Framework (RDF), and\nperforms a preliminary evaluation of the first draft of the specification\nfollowed by a discussion on possible future directions.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 11:00:26 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Ohmukai", "Ikki", ""], ["Yamamoto", "Yasunori", ""], ["Ito", "Maori", ""], ["Okumura", "Takashi", ""]]}, {"id": "2003.06205", "submitter": "Eva Blanco", "authors": "E. Blanco-Mallo, B. Remeseiro, V. Bol\\'on-Canedo, A. Alonso-Betanzos", "title": "On the effectiveness of convolutional autoencoders on image-based\n  personalized recommender systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems (RS) are increasingly present in our daily lives,\nespecially since the advent of Big Data, which allows for storing all kinds of\ninformation about users' preferences. Personalized RS are successfully applied\nin platforms such as Netflix, Amazon or YouTube. However, they are missing in\ngastronomic platforms such as TripAdvisor, where moreover we can find millions\nof images tagged with users' tastes. This paper explores the potential of using\nthose images as sources of information for modeling users' tastes and proposes\nan image-based classification system to obtain personalized recommendations,\nusing a convolutional autoencoder as feature extractor. The proposed\narchitecture will be applied to TripAdvisor data, using users' reviews that can\nbe defined as a triad composed by a user, a restaurant, and an image of it\ntaken by the user. Since the dataset is highly unbalanced, the use of data\naugmentation on the minority class is also considered in the experimentation.\nResults on data from three cities of different sizes (Santiago de Compostela,\nBarcelona and New York) demonstrate the effectiveness of using a convolutional\nautoencoder as feature extractor, instead of the standard deep features\ncomputed with convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 11:19:02 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Blanco-Mallo", "E.", ""], ["Remeseiro", "B.", ""], ["Bol\u00f3n-Canedo", "V.", ""], ["Alonso-Betanzos", "A.", ""]]}, {"id": "2003.06461", "submitter": "Jessie Smith", "authors": "Jessie Smith, Nasim Sonboli, Casey Fiesler, Robin Burke", "title": "Exploring User Opinions of Fairness in Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Algorithmic fairness for artificial intelligence has become increasingly\nrelevant as these systems become more pervasive in society. One realm of AI,\nrecommender systems, presents unique challenges for fairness due to trade offs\nbetween optimizing accuracy for users and fairness to providers. But what is\nfair in the context of recommendation--particularly when there are multiple\nstakeholders? In an initial exploration of this problem, we ask users what\ntheir ideas of fair treatment in recommendation might be, and why. We analyze\nwhat might cause discrepancies or changes between user's opinions towards\nfairness to eventually help inform the design of fairer and more transparent\nrecommendation algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 19:44:26 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 20:19:42 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Smith", "Jessie", ""], ["Sonboli", "Nasim", ""], ["Fiesler", "Casey", ""], ["Burke", "Robin", ""]]}, {"id": "2003.06499", "submitter": "Hadi Abdi Khojasteh", "authors": "Hadi Abdi Khojasteh, Ebrahim Ansari, Mahdi Bohlouli", "title": "LSCP: Enhanced Large Scale Colloquial Persian Language Understanding", "comments": "6 pages, 2 figures, 3 tables, Accepted at the 12th International\n  Conference on Language Resources and Evaluation (LREC 2020)", "journal-ref": "https://www.aclweb.org/anthology/2020.lrec-1.776/", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language recognition has been significantly advanced in recent years by means\nof modern machine learning methods such as deep learning and benchmarks with\nrich annotations. However, research is still limited in low-resource formal\nlanguages. This consists of a significant gap in describing the colloquial\nlanguage especially for low-resourced ones such as Persian. In order to target\nthis gap for low resource languages, we propose a \"Large Scale Colloquial\nPersian Dataset\" (LSCP). LSCP is hierarchically organized in a semantic\ntaxonomy that focuses on multi-task informal Persian language understanding as\na comprehensive problem. This encompasses the recognition of multiple semantic\naspects in the human-level sentences, which naturally captures from the\nreal-world sentences. We believe that further investigations and processing, as\nwell as the application of novel algorithms and methods, can strengthen\nenriching computerized understanding and processing of low resource languages.\nThe proposed corpus consists of 120M sentences resulted from 27M tweets\nannotated with parsing tree, part-of-speech tags, sentiment polarity and\ntranslation in five different languages.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 22:24:14 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Khojasteh", "Hadi Abdi", ""], ["Ansari", "Ebrahim", ""], ["Bohlouli", "Mahdi", ""]]}, {"id": "2003.06561", "submitter": "Gengchen Mai", "authors": "Gengchen Mai, Krzysztof Janowicz, Sathya Prasad, Meilin Shi, Ling Cai,\n  Rui Zhu, Blake Regalia, Ni Lao", "title": "Semantically-Enriched Search Engine for Geoportals: A Case Study with\n  ArcGIS Online", "comments": "18 pages; Accepted to AGILE 2020 as a full paper GitHub Code\n  Repository: https://github.com/gengchenmai/arcgis-online-search-engine", "journal-ref": "AGILE 2020, Jun. 16 - 19, 2020, Chania, Crete, Greece", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many geoportals such as ArcGIS Online are established with the goal of\nimproving geospatial data reusability and achieving intelligent knowledge\ndiscovery. However, according to previous research, most of the existing\ngeoportals adopt Lucene-based techniques to achieve their core search\nfunctionality, which has a limited ability to capture the user's search\nintentions. To better understand a user's search intention, query expansion can\nbe used to enrich the user's query by adding semantically similar terms. In the\ncontext of geoportals and geographic information retrieval, we advocate the\nidea of semantically enriching a user's query from both geospatial and thematic\nperspectives. In the geospatial aspect, we propose to enrich a query by using\nboth place partonomy and distance decay. In terms of the thematic aspect,\nconcept expansion and embedding-based document similarity are used to infer the\nimplicit information hidden in a user's query. This semantic query expansion 1\n2 G. Mai et al. framework is implemented as a semantically-enriched search\nengine using ArcGIS Online as a case study. A benchmark dataset is constructed\nto evaluate the proposed framework. Our evaluation results show that the\nproposed semantic query expansion framework is very effective in capturing a\nuser's search intention and significantly outperforms a well-established\nbaseline-Lucene's practical scoring function-with more than 3.0 increments in\nDCG@K (K=3,5,10).\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 06:16:30 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Mai", "Gengchen", ""], ["Janowicz", "Krzysztof", ""], ["Prasad", "Sathya", ""], ["Shi", "Meilin", ""], ["Cai", "Ling", ""], ["Zhu", "Rui", ""], ["Regalia", "Blake", ""], ["Lao", "Ni", ""]]}, {"id": "2003.06713", "submitter": "Rodrigo Nogueira", "authors": "Rodrigo Nogueira, Zhiying Jiang, Jimmy Lin", "title": "Document Ranking with a Pretrained Sequence-to-Sequence Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a novel adaptation of a pretrained sequence-to-sequence\nmodel to the task of document ranking. Our approach is fundamentally different\nfrom a commonly-adopted classification-based formulation of ranking, based on\nencoder-only pretrained transformer architectures such as BERT. We show how a\nsequence-to-sequence model can be trained to generate relevance labels as\n\"target words\", and how the underlying logits of these target words can be\ninterpreted as relevance probabilities for ranking. On the popular MS MARCO\npassage ranking task, experimental results show that our approach is at least\non par with previous classification-based models and can surpass them with\nlarger, more-recent models. On the test collection from the TREC 2004 Robust\nTrack, we demonstrate a zero-shot transfer-based approach that outperforms\nprevious state-of-the-art models requiring in-dataset cross-validation.\nFurthermore, we find that our approach significantly outperforms an\nencoder-only model in a data-poor regime (i.e., with few training examples). We\ninvestigate this observation further by varying target words to probe the\nmodel's use of latent knowledge.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 22:29:50 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Nogueira", "Rodrigo", ""], ["Jiang", "Zhiying", ""], ["Lin", "Jimmy", ""]]}, {"id": "2003.06858", "submitter": "Nguyen Thi Thanh Thuy", "authors": "Nguyen Thi Thanh Thuy, Ngo Xuan Bach, Tu Minh Phuong", "title": "Leveraging Foreign Language Labeled Data for Aspect-Based Opinion Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aspect-based opinion mining is the task of identifying sentiment at the\naspect level in opinionated text, which consists of two subtasks: aspect\ncategory extraction and sentiment polarity classification. While aspect\ncategory extraction aims to detect and categorize opinion targets such as\nproduct features, sentiment polarity classification assigns a sentiment label,\ni.e. positive, negative, or neutral, to each identified aspect. Supervised\nlearning methods have been shown to deliver better accuracy for this task but\nthey require labeled data, which is costly to obtain, especially for\nresource-poor languages like Vietnamese. To address this problem, we present a\nsupervised aspect-based opinion mining method that utilizes labeled data from a\nforeign language (English in this case), which is translated to Vietnamese by\nan automated translation tool (Google Translate). Because aspects and opinions\nin different languages may be expressed by different words, we propose using\nword embeddings, in addition to other features, to reduce the vocabulary\ndifference between the original and translated texts, thus improving the\neffectiveness of aspect category extraction and sentiment polarity\nclassification processes. We also introduce an annotated corpus of aspect\ncategories and sentiment polarities extracted from restaurant reviews in\nVietnamese, and conduct a series of experiments on the corpus. Experimental\nresults demonstrate the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 15:53:53 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Thuy", "Nguyen Thi Thanh", ""], ["Bach", "Ngo Xuan", ""], ["Phuong", "Tu Minh", ""]]}, {"id": "2003.07027", "submitter": "Meng Chen", "authors": "Meng Chen, Xiaoyi Jia, Elizabeth Gorbonos, Chnh T. Hong, Xiaohui Yu,\n  Yang Liu", "title": "Eating Healthier: Exploring Nutrition Information for Healthier Recipe\n  Recommendation", "comments": null, "journal-ref": "Information Processing & Management, 102051, 2019", "doi": "10.1016/j.ipm.2019.05.012", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the booming of personalized recipe sharing networks (e.g., Yummly), a\ndeluge of recipes from different cuisines could be obtained easily. In this\npaper, we aim to solve a problem which many home-cooks encounter when searching\nfor recipes online. Namely, finding recipes which best fit a handy set of\ningredients while at the same time follow healthy eating guidelines. This task\nis especially difficult since the lions share of online recipes have been shown\nto be unhealthy. In this paper we propose a novel framework named NutRec, which\nmodels the interactions between ingredients and their proportions within\nrecipes for the purpose of offering healthy recommendation. Specifically,\nNutRec consists of three main components: 1) using an embedding-based\ningredient predictor to predict the relevant ingredients with user-defined\ninitial ingredients, 2) predicting the amounts of the relevant ingredients with\na multi-layer perceptron-based network, 3) creating a healthy pseudo-recipe\nwith a list of ingredients and their amounts according to the nutritional\ninformation and recommending the top similar recipes with the pseudo-recipe. We\nconduct the experiments on two recipe datasets, including Allrecipes with\n36,429 recipes and Yummly with 89,413 recipes, respectively. The empirical\nresults support the framework's intuition and showcase its ability to retrieve\nhealthier recipes.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 04:59:32 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Chen", "Meng", ""], ["Jia", "Xiaoyi", ""], ["Gorbonos", "Elizabeth", ""], ["Hong", "Chnh T.", ""], ["Yu", "Xiaohui", ""], ["Liu", "Yang", ""]]}, {"id": "2003.07051", "submitter": "Parisa Abolfath Beygi Dezfouli", "authors": "Parisa Abolfath Beygi Dezfouli, Saeedeh Momtazi, Mehdi Dehghan", "title": "Deep Neural Review Text Interaction for Recommendation Systems", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users' reviews contain valuable information which are not taken into account\nin most recommender systems. According to the latest studies in this field,\nusing review texts could not only improve the performance of recommendation,\nbut it can also alleviate the impact of data sparsity and help to tackle the\ncold start problem. In this paper, we present a neural recommender model which\nrecommends items by leveraging user reviews. In order to predict user rating\nfor each item, our proposed model, named MatchPyramid Recommender System\n(MPRS), represents each user and item with their corresponding review texts.\nThus, the problem of recommendation is viewed as a text matching problem such\nthat the matching score obtained from matching user and item texts could be\nconsidered as a good representative of their joint extent of similarity. To\nsolve the text matching problem, inspired by MatchPyramid (Pang, 2016), we\nemployed an interaction-based approach according to which a matching matrix is\nconstructed given a pair of input texts. The matching matrix, which has the\nproperty of hierarchical matching patterns, is then fed into a Convolutional\nNeural Network (CNN) to compute the matching score for the given user-item\npair. Our experiments on the small data categories of Amazon review dataset\nshow that our proposed model gains from 1.76% to 21.72% relative improvement\ncompared to DeepCoNN model, and from 0.83% to 3.15% relative improvement\ncompared to TransNets model. Also, on two large categories, namely AZ-CSJ and\nAZ-Mov, our model achieves relative improvements of 8.08% and 7.56% compared to\nthe DeepCoNN model, and relative improvements of 1.74% and 0.86% compared to\nthe TransNets model, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 07:16:33 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Dezfouli", "Parisa Abolfath Beygi", ""], ["Momtazi", "Saeedeh", ""], ["Dehghan", "Mehdi", ""]]}, {"id": "2003.07122", "submitter": "Siyuan Chen", "authors": "Siyuan Chen, Jiahai Wang, Xin Du, Yanqing Hu", "title": "A Novel Framework with Information Fusion and Neighborhood Enhancement\n  for User Identity Linkage", "comments": "8 pages, 7 figures, accepted by ECAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User identity linkage across social networks is an essential problem for\ncross-network data mining. Since network structure, profile and content\ninformation describe different aspects of users, it is critical to learn\neffective user representations that integrate heterogeneous information. This\npaper proposes a novel framework with INformation FUsion and Neighborhood\nEnhancement (INFUNE) for user identity linkage. The information fusion\ncomponent adopts a group of encoders and decoders to fuse heterogeneous\ninformation and generate discriminative node embeddings for preliminary\nmatching. Then, these embeddings are fed to the neighborhood enhancement\ncomponent, a novel graph neural network, to produce adaptive neighborhood\nembeddings that reflect the overlapping degree of neighborhoods of varying\ncandidate user pairs. The importance of node embeddings and neighborhood\nembeddings are weighted for final prediction. The proposed method is evaluated\non real-world social network data. The experimental results show that INFUNE\nsignificantly outperforms existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 11:23:59 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Chen", "Siyuan", ""], ["Wang", "Jiahai", ""], ["Du", "Xin", ""], ["Hu", "Yanqing", ""]]}, {"id": "2003.07158", "submitter": "Jianbin Lin Lin", "authors": "Jianbin Lin, Daixin Wang, Lu Guan, Yin Zhao, Binqiang Zhao, Jun Zhou,\n  Xiaolong Li, and Yuan Qi", "title": "RNE: A Scalable Network Embedding for Billion-scale Recommendation", "comments": "PAKDD 2019", "journal-ref": null, "doi": "10.1007/978-3-030-16145-3_34", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays designing a real recommendation system has been a critical problem\nfor both academic and industry. However, due to the huge number of users and\nitems, the diversity and dynamic property of the user interest, how to design a\nscalable recommendation system, which is able to efficiently produce effective\nand diverse recommendation results on billion-scale scenarios, is still a\nchallenging and open problem for existing methods. In this paper, given the\nuser-item interaction graph, we propose RNE, a data-efficient\nRecommendation-based Network Embedding method, to give personalized and diverse\nitems to users. Specifically, we propose a diversity- and dynamics-aware\nneighbor sampling method for network embedding. On the one hand, the method is\nable to preserve the local structure between the users and items while modeling\nthe diversity and dynamic property of the user interest to boost the\nrecommendation quality. On the other hand the sampling method can reduce the\ncomplexity of the whole method theoretically to make it possible for\nbillion-scale recommendation. We also implement the designed algorithm in a\ndistributed way to further improves its scalability. Experimentally, we deploy\nRNE on a recommendation scenario of Taobao, the largest E-commerce platform in\nChina, and train it on a billion-scale user-item graph. As is shown on several\nonline metrics on A/B testing, RNE is able to achieve both high-quality and\ndiverse results compared with CF-based methods. We also conduct the offline\nexperiments on Pinterest dataset comparing with several state-of-the-art\nrecommendation methods and network embedding methods. The results demonstrate\nthat our method is able to produce a good result while runs much faster than\nthe baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 07:08:57 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 06:47:20 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Lin", "Jianbin", ""], ["Wang", "Daixin", ""], ["Guan", "Lu", ""], ["Zhao", "Yin", ""], ["Zhao", "Binqiang", ""], ["Zhou", "Jun", ""], ["Li", "Xiaolong", ""], ["Qi", "Yuan", ""]]}, {"id": "2003.07160", "submitter": "Bingqing Yu", "authors": "Bingqing Yu, Jacopo Tagliabue, Ciro Greco and Federico Bianchi", "title": "\"An Image is Worth a Thousand Features\": Scalable Product\n  Representations for In-Session Type-Ahead Personalization", "comments": null, "journal-ref": null, "doi": "10.1145/3366424.3386198", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of personalizing query completion in a digital\ncommerce setting, in which the bounce rate is typically high and recurring\nusers are rare. We focus on in-session personalization and improve a standard\nnoisy channel model by injecting dense vectors computed from product images at\nquery time. We argue that image-based personalization displays several\nadvantages over alternative proposals (from data availability to business\nscalability), and provide quantitative evidence and qualitative support on the\neffectiveness of the proposed methods. Finally, we show how a shared vector\nspace between similar shops can be used to improve the experience of users\nbrowsing across sites, opening up the possibility of applying zero-shot\nunsupervised personalization to increase conversions. This will prove to be\nparticularly relevant to retail groups that manage multiple brands and/or\nwebsites and to multi-tenant SaaS providers that serve multiple clients in the\nsame space.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 18:41:56 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Yu", "Bingqing", ""], ["Tagliabue", "Jacopo", ""], ["Greco", "Ciro", ""], ["Bianchi", "Federico", ""]]}, {"id": "2003.07162", "submitter": "Jiwei Tan", "authors": "Xiang Li, Chao Wang, Jiwei Tan, Xiaoyi Zeng, Dan Ou, Bo Zheng", "title": "Adversarial Multimodal Representation Learning for Click-Through Rate\n  Prediction", "comments": "Accepted to WWW 2020, 10 pages", "journal-ref": null, "doi": "10.1145/3366423.3380163", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For better user experience and business effectiveness, Click-Through Rate\n(CTR) prediction has been one of the most important tasks in E-commerce.\nAlthough extensive CTR prediction models have been proposed, learning good\nrepresentation of items from multimodal features is still less investigated,\nconsidering an item in E-commerce usually contains multiple heterogeneous\nmodalities. Previous works either concatenate the multiple modality features,\nthat is equivalent to giving a fixed importance weight to each modality; or\nlearn dynamic weights of different modalities for different items through\ntechnique like attention mechanism. However, a problem is that there usually\nexists common redundant information across multiple modalities. The dynamic\nweights of different modalities computed by using the redundant information may\nnot correctly reflect the different importance of each modality. To address\nthis, we explore the complementarity and redundancy of modalities by\nconsidering modality-specific and modality-invariant features differently. We\npropose a novel Multimodal Adversarial Representation Network (MARN) for the\nCTR prediction task. A multimodal attention network first calculates the\nweights of multiple modalities for each item according to its modality-specific\nfeatures. Then a multimodal adversarial network learns modality-invariant\nrepresentations where a double-discriminators strategy is introduced. Finally,\nwe achieve the multimodal item representations by combining both\nmodality-specific and modality-invariant representations. We conduct extensive\nexperiments on both public and industrial datasets, and the proposed method\nconsistently achieves remarkable improvements to the state-of-the-art methods.\nMoreover, the approach has been deployed in an operational E-commerce system\nand online A/B testing further demonstrates the effectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 15:50:23 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Li", "Xiang", ""], ["Wang", "Chao", ""], ["Tan", "Jiwei", ""], ["Zeng", "Xiaoyi", ""], ["Ou", "Dan", ""], ["Zheng", "Bo", ""]]}, {"id": "2003.07193", "submitter": "Gustavo Paiva Guedes", "authors": "Flavio Carvalho and Gustavo Paiva Guedes", "title": "TF-IDFC-RF: A Novel Supervised Term Weighting Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment Analysis is a branch of Affective Computing usually considered a\nbinary classification task. In this line of reasoning, Sentiment Analysis can\nbe applied in several contexts to classify the attitude expressed in text\nsamples, for example, movie reviews, sarcasm, among others. A common approach\nto represent text samples is the use of the Vector Space Model to compute\nnumerical feature vectors consisting of the weight of terms. The most popular\nterm weighting scheme is TF-IDF (Term Frequency - Inverse Document Frequency).\nIt is an Unsupervised Weighting Scheme (UWS) since it does not consider the\nclass information in the weighting of terms. Apart from that, there are\nSupervised Weighting Schemes (SWS), which consider the class information on\nterm weighting calculation. Several SWS have been recently proposed,\ndemonstrating better results than TF-IDF. In this scenario, this work presents\na comparative study on different term weighting schemes and proposes a novel\nsupervised term weighting scheme, named as TF-IDFC-RF (Term Frequency - Inverse\nDocument Frequency in Class - Relevance Frequency). The effectiveness of\nTF-IDFC-RF is validated with SVM (Support Vector Machine) and NB (Naive Bayes)\nclassifiers on four commonly used Sentiment Analysis datasets. TF-IDFC-RF shows\npromising results, outperforming all other weighting schemes on two datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 21:31:46 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 03:23:15 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Carvalho", "Flavio", ""], ["Guedes", "Gustavo Paiva", ""]]}, {"id": "2003.07424", "submitter": "Lenz Furrer", "authors": "Lenz Furrer (1 and 3), Joseph Cornelius (1), Fabio Rinaldi (1, 2, and\n  3) ((1) University of Zurich, Switzerland, (2) Dalle Molle Institute for\n  Artificial Intelligence Research (IDSIA), Switzerland, (3) Swiss Institute of\n  Bioinformatics, Switzerland)", "title": "Parallel sequence tagging for concept recognition", "comments": "14 pages, 5 figures; style changed, sections reordered, minor\n  additions suggested by reviewers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Named Entity Recognition (NER) and Normalisation (NEN) are core\ncomponents of any text-mining system for biomedical texts. In a traditional\nconcept-recognition pipeline, these tasks are combined in a serial way, which\nis inherently prone to error propagation from NER to NEN. We propose a parallel\narchitecture, where both NER and NEN are modeled as a sequence-labeling task,\noperating directly on the source text. We examine different harmonisation\nstrategies for merging the predictions of the two classifiers into a single\noutput sequence. Results: We test our approach on the recent Version 4 of the\nCRAFT corpus. In all 20 annotation sets of the concept-annotation task, our\nsystem outperforms the pipeline system reported as a baseline in the CRAFT\nshared task 2019. Conclusions: Our analysis shows that the strengths of the two\nclassifiers can be combined in a fruitful way. However, prediction\nharmonisation requires individual calibration on a development set for each\nannotation set. This allows achieving a good trade-off between established\nknowledge (training set) and novel information (unseen concepts). Availability\nand Implementation: Source code freely available for download at\nhttps://github.com/OntoGene/craft-st. Supplementary data are available at arXiv\nonline.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 19:41:07 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 07:54:17 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Furrer", "Lenz", "", "1 and 3"], ["Cornelius", "Joseph", "", "1, 2, and\n  3"], ["Rinaldi", "Fabio", "", "1, 2, and\n  3"]]}, {"id": "2003.07461", "submitter": "Antonia Saravanou", "authors": "Antonia Saravanou and Giorgio Stefanoni and Edgar Meij", "title": "Identifying Notable News Stories", "comments": "Proceedings of The 42nd European Conference on Information Retrieval\n  2020 (ECIR '20), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The volume of news content has increased significantly in recent years and\nsystems to process and deliver this information in an automated fashion at\nscale are becoming increasingly prevalent. One critical component that is\nrequired in such systems is a method to automatically determine how notable a\ncertain news story is, in order to prioritize these stories during delivery.\nOne way to do so is to compare each story in a stream of news stories to a\nnotable event. In other words, the problem of detecting notable news can be\ndefined as a ranking task; given a trusted source of notable events and a\nstream of candidate news stories, we aim to answer the question: \"Which of the\ncandidate news stories is most similar to the notable one?\". We employ\ndifferent combinations of features and learning to rank (LTR) models and gather\nrelevance labels using crowdsourcing. In our approach, we use structured\nrepresentations of candidate news stories (triples) and we link them to\ncorresponding entities. Our evaluation shows that the features in our proposed\nmethod outperform standard ranking methods, and that the trained model\ngeneralizes well to unseen news stories.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 22:57:50 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Saravanou", "Antonia", ""], ["Stefanoni", "Giorgio", ""], ["Meij", "Edgar", ""]]}, {"id": "2003.07820", "submitter": "Bhaskar Mitra", "authors": "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Ellen M.\n  Voorhees", "title": "Overview of the TREC 2019 deep learning track", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Deep Learning Track is a new track for TREC 2019, with the goal of\nstudying ad hoc ranking in a large data regime. It is the first track with\nlarge human-labeled training sets, introducing two sets corresponding to two\ntasks, each with rigorous TREC-style blind evaluation and reusable test sets.\nThe document retrieval task has a corpus of 3.2 million documents with 367\nthousand training queries, for which we generate a reusable test set of 43\nqueries. The passage retrieval task has a corpus of 8.8 million passages with\n503 thousand training queries, for which we generate a reusable test set of 43\nqueries. This year 15 groups submitted a total of 75 runs, using various\ncombinations of deep learning, transfer learning and traditional IR ranking\nmethods. Deep learning runs significantly outperformed traditional IR runs.\nPossible explanations for this result are that we introduced large training\ndata and we included deep models trained on such data in our judging pools,\nwhereas some past studies did not have such training data or pooling.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 17:12:36 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 16:56:56 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Craswell", "Nick", ""], ["Mitra", "Bhaskar", ""], ["Yilmaz", "Emine", ""], ["Campos", "Daniel", ""], ["Voorhees", "Ellen M.", ""]]}, {"id": "2003.08052", "submitter": "Yuan Shen", "authors": "Yuan Shen, Shanduojiao Jiang, Muhammad Rizky Wellyanto, and Ranjitha\n  Kumar", "title": "Can AI decrypt fashion jargon for you?", "comments": "5 pages, 6 figures, Accepted at workshop paper for AI4HCI at CHI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When people talk about fashion, they care about the underlying meaning of\nfashion concepts,e.g., style.For example, people ask questions like what\nfeatures make this dress smart.However, the product descriptions in today\nfashion websites are full of domain specific and low level words. It is not\nclear to people how exactly those low level descriptions can contribute to a\nstyle or any high level fashion concept. In this paper, we proposed a data\ndriven solution to address this concept understanding issues by leveraging a\nlarge number of existing product data on fashion sites. We first collected and\ncategorized 1546 fashion keywords into 5 different fashion categories. Then, we\ncollected a new fashion product dataset with 853,056 products in total.\nFinally, we trained a deep learning model that can explicitly predict and\nexplain high level fashion concepts in a product image with its low level and\ndomain specific fashion features.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 05:32:04 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Shen", "Yuan", ""], ["Jiang", "Shanduojiao", ""], ["Wellyanto", "Muhammad Rizky", ""], ["Kumar", "Ranjitha", ""]]}, {"id": "2003.08276", "submitter": "Jimmy Lin", "authors": "Jimmy Lin, Joel Mackenzie, Chris Kamphuis, Craig Macdonald, Antonio\n  Mallia, Micha{\\l} Siedlaczek, Andrew Trotman, and Arjen de Vries", "title": "Supporting Interoperability Between Open-Source Search Engines with the\n  Common Index File Format", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a natural tension between encouraging a diverse ecosystem of\nopen-source search engines and supporting fair, replicable comparisons across\nthose systems. To balance these two goals, we examine two approaches to\nproviding interoperability between the inverted indexes of several systems. The\nfirst takes advantage of internal abstractions around index structures and\nbuilding wrappers that allow one system to directly read the indexes of\nanother. The second involves sharing indexes across systems via a data exchange\nspecification that we have developed, called the Common Index File Format\n(CIFF). We demonstrate the first approach with the Java systems Anserini and\nTerrier, and the second approach with Anserini, JASSv2, OldDog, PISA, and\nTerrier. Together, these systems provide a wide range of implementations and\nfeatures, with different research goals. Overall, we recommend CIFF as a\nlow-effort approach to support independent innovation while enabling the types\nof fair evaluations that are critical for driving the field forward.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 15:33:46 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Lin", "Jimmy", ""], ["Mackenzie", "Joel", ""], ["Kamphuis", "Chris", ""], ["Macdonald", "Craig", ""], ["Mallia", "Antonio", ""], ["Siedlaczek", "Micha\u0142", ""], ["Trotman", "Andrew", ""], ["de Vries", "Arjen", ""]]}, {"id": "2003.08343", "submitter": "Islam Elnabarawy", "authors": "Islam Elnabarawy, Wei Jiang, Donald C. Wunsch II", "title": "Survey of Privacy-Preserving Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering recommendation systems provide recommendations to\nusers based on their own past preferences, as well as those of other users who\nshare similar interests. The use of recommendation systems has grown widely in\nrecent years, helping people choose which movies to watch, books to read, and\nitems to buy. However, users are often concerned about their privacy when using\nsuch systems, and many users are reluctant to provide accurate information to\nmost online services. Privacy-preserving collaborative filtering recommendation\nsystems aim to provide users with accurate recommendations while maintaining\ncertain guarantees about the privacy of their data. This survey examines the\nrecent literature in privacy-preserving collaborative filtering, providing a\nbroad perspective of the field and classifying the key contributions in the\nliterature using two different criteria: the type of vulnerability they address\nand the type of approach they use to solve it.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 17:14:50 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Elnabarawy", "Islam", ""], ["Jiang", "Wei", ""], ["Wunsch", "Donald C.", "II"]]}, {"id": "2003.08553", "submitter": "Anshuman Suri", "authors": "Parag Agrawal, Tulasi Menon, Aya Kamel, Michel Naim, Chaikesh\n  Chouragade, Gurvinder Singh, Rohan Kulkarni, Anshuman Suri, Sahithi Katakam,\n  Vineet Pratik, Prakul Bansal, Simerpreet Kaur, Neha Rajput, Anand Duggal,\n  Achraf Chalabi, Prashant Choudhari, Reddy Satti, Niranjan Nayak", "title": "QnAMaker: Data to Bot in 2 Minutes", "comments": "Published at The Web Conference 2020 in the demo track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having a bot for seamless conversations is a much-desired feature that\nproducts and services today seek for their websites and mobile apps. These bots\nhelp reduce traffic received by human support significantly by handling\nfrequent and directly answerable known questions. Many such services have huge\nreference documents such as FAQ pages, which makes it hard for users to browse\nthrough this data. A conversation layer over such raw data can lower traffic to\nhuman support by a great margin. We demonstrate QnAMaker, a service that\ncreates a conversational layer over semi-structured data such as FAQ pages,\nproduct manuals, and support documents. QnAMaker is the popular choice for\nExtraction and Question-Answering as a service and is used by over 15,000 bots\nin production. It is also used by search interfaces and not just bots.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 03:32:03 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Agrawal", "Parag", ""], ["Menon", "Tulasi", ""], ["Kamel", "Aya", ""], ["Naim", "Michel", ""], ["Chouragade", "Chaikesh", ""], ["Singh", "Gurvinder", ""], ["Kulkarni", "Rohan", ""], ["Suri", "Anshuman", ""], ["Katakam", "Sahithi", ""], ["Pratik", "Vineet", ""], ["Bansal", "Prakul", ""], ["Kaur", "Simerpreet", ""], ["Rajput", "Neha", ""], ["Duggal", "Anand", ""], ["Chalabi", "Achraf", ""], ["Choudhari", "Prashant", ""], ["Satti", "Reddy", ""], ["Nayak", "Niranjan", ""]]}, {"id": "2003.08741", "submitter": "Shuo Jiang", "authors": "Shuo Jiang, Jianxi Luo, Guillermo Ruiz Pava, Jie Hu, Christopher L.\n  Magee", "title": "A Convolutional Neural Network-based Patent Image Retrieval Method for\n  Design Ideation", "comments": "11 pages, 11 figures", "journal-ref": "ASME 2020 International Design Engineering Technical Conferences\n  and Computers and Information in Engineering Conference", "doi": "10.1115/DETC2020-22048", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The patent database is often used in searches of inspirational stimuli for\ninnovative design opportunities because of its large size, extensive variety\nand rich design information in patent documents. However, most patent mining\nresearch only focuses on textual information and ignores visual information.\nHerein, we propose a convolutional neural network (CNN)-based patent image\nretrieval method. The core of this approach is a novel neural network\narchitecture named Dual-VGG that is aimed to accomplish two tasks: visual\nmaterial type prediction and international patent classification (IPC) class\nlabel prediction. In turn, the trained neural network provides the deep\nfeatures in the image embedding vectors that can be utilized for patent image\nretrieval and visual mapping. The accuracy of both training tasks and patent\nimage embedding space are evaluated to show the performance of our model. This\napproach is also illustrated in a case study of robot arm design retrieval.\nCompared to traditional keyword-based searching and Google image searching, the\nproposed method discovers more useful visual information for engineering\ndesign.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 13:32:08 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 20:31:24 GMT"}, {"version": "v3", "created": "Tue, 19 May 2020 22:58:48 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Jiang", "Shuo", ""], ["Luo", "Jianxi", ""], ["Pava", "Guillermo Ruiz", ""], ["Hu", "Jie", ""], ["Magee", "Christopher L.", ""]]}, {"id": "2003.08763", "submitter": "A. Ben Hamza", "authors": "David Pickup, Xianfang Sun, Paul L Rosin, Ralph R Martin, Z Cheng,\n  Zhouhui Lian, Masaki Aono, A Ben Hamza, A Bronstein, M Bronstein, S Bu,\n  Umberto Castellani, S Cheng, Valeria Garro, Andrea Giachetti, Afzal Godil,\n  Luca Isaia, J Han, Henry Johan, L Lai, Bo Li, C Li, Haisheng Li, Roee Litman,\n  X Liu, Z Liu, Yijuan Lu, L Sun, G Tam, Atsushi Tatsuma, J Ye", "title": "Shape retrieval of non-rigid 3d human models", "comments": "International Journal of Computer Vision, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D models of humans are commonly used within computer graphics and vision,\nand so the ability to distinguish between body shapes is an important shape\nretrieval problem. We extend our recent paper which provided a benchmark for\ntesting non-rigid 3D shape retrieval algorithms on 3D human models. This\nbenchmark provided a far stricter challenge than previous shape benchmarks. We\nhave added 145 new models for use as a separate training set, in order to\nstandardise the training data used and provide a fairer comparison. We have\nalso included experiments with the FAUST dataset of human scans. All\nparticipants of the previous benchmark study have taken part in the new tests\nreported here, many providing updated results using the new data. In addition,\nfurther participants have also taken part, and we provide extra analysis of the\nretrieval results. A total of 25 different shape retrieval methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 20:03:16 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Pickup", "David", ""], ["Sun", "Xianfang", ""], ["Rosin", "Paul L", ""], ["Martin", "Ralph R", ""], ["Cheng", "Z", ""], ["Lian", "Zhouhui", ""], ["Aono", "Masaki", ""], ["Hamza", "A Ben", ""], ["Bronstein", "A", ""], ["Bronstein", "M", ""], ["Bu", "S", ""], ["Castellani", "Umberto", ""], ["Cheng", "S", ""], ["Garro", "Valeria", ""], ["Giachetti", "Andrea", ""], ["Godil", "Afzal", ""], ["Isaia", "Luca", ""], ["Han", "J", ""], ["Johan", "Henry", ""], ["Lai", "L", ""], ["Li", "Bo", ""], ["Li", "C", ""], ["Li", "Haisheng", ""], ["Litman", "Roee", ""], ["Liu", "X", ""], ["Liu", "Z", ""], ["Lu", "Yijuan", ""], ["Sun", "L", ""], ["Tam", "G", ""], ["Tatsuma", "Atsushi", ""], ["Ye", "J", ""]]}, {"id": "2003.08769", "submitter": "Nitish Nag", "authors": "Nitish Nag, Bindu Rajanna, Ramesh Jain", "title": "Personalized Taste and Cuisine Preference Modeling via Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the exponential growth in the usage of social media to share live\nupdates about life, taking pictures has become an unavoidable phenomenon.\nIndividuals unknowingly create a unique knowledge base with these images. The\nfood images, in particular, are of interest as they contain a plethora of\ninformation. From the image metadata and using computer vision tools, we can\nextract distinct insights for each user to build a personal profile. Using the\nunderlying connection between cuisines and their inherent tastes, we attempt to\ndevelop such a profile for an individual based solely on the images of his\nfood. Our study provides insights about an individual's inclination towards\nparticular cuisines. Interpreting these insights can lead to the development of\na more precise recommendation system. Such a system would avoid the generic\napproach in favor of a personalized recommendation system.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 01:07:56 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Nag", "Nitish", ""], ["Rajanna", "Bindu", ""], ["Jain", "Ramesh", ""]]}, {"id": "2003.09086", "submitter": "Fan Liu", "authors": "Fan Liu, Zhiyong Cheng, Lei Zhu, Chenghao Liu, Liqiang Nie", "title": "A^2-GCN: An Attribute-aware Attentive GCN Model for Recommendation", "comments": "This paper has been submitted to IEEE TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As important side information, attributes have been widely exploited in the\nexisting recommender system for better performance. In the real-world\nscenarios, it is common that some attributes of items/users are missing (e.g.,\nsome movies miss the genre data). Prior studies usually use a default value\n(i.e., \"other\") to represent the missing attribute, resulting in sub-optimal\nperformance. To address this problem, in this paper, we present an\nattribute-aware attentive graph convolution network (A${^2}$-GCN). In\nparticular, we first construct a graph, whereby users, items, and attributes\nare three types of nodes and their associations are edges. Thereafter, we\nleverage the graph convolution network to characterize the complicated\ninteractions among <users, items, attributes>. To learn the node\nrepresentation, we turn to the message-passing strategy to aggregate the\nmessage passed from the other directly linked types of nodes (e.g., a user or\nan attribute). To this end, we are capable of incorporating associate\nattributes to strengthen the user and item representations, and thus naturally\nsolve the attribute missing problem. Considering the fact that for different\nusers, the attributes of an item have different influence on their preference\nfor this item, we design a novel attention mechanism to filter the message\npassed from an item to a target user by considering the attribute information.\nExtensive experiments have been conducted on several publicly accessible\ndatasets to justify our model. Results show that our model outperforms several\nstate-of-the-art methods and demonstrate the effectiveness of our attention\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 03:12:32 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Liu", "Fan", ""], ["Cheng", "Zhiyong", ""], ["Zhu", "Lei", ""], ["Liu", "Chenghao", ""], ["Nie", "Liqiang", ""]]}, {"id": "2003.09417", "submitter": "Moritz Schubotz", "authors": "Moritz Schubotz and Andr\\'e Greiner-Petter and Norman Meuschke and\n  Olaf Teschke and Bela Gipp", "title": "Mathematical Formulae in Wikimedia Projects 2020", "comments": "Submitted to JCDL 2020: Proceedings of the ACM/ IEEE Joint Conference\n  on Digital Libraries in 2020 (JCDL '20), August 1-5, 2020, Virtual Event,\n  China", "journal-ref": null, "doi": "10.1145/3383583.3398557", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This poster summarizes our contributions to Wikimedia's processing pipeline\nfor mathematical formulae. We describe how we have supported the transition\nfrom rendering formulae as course-grained PNG images in 2001 to providing\nmodern semantically enriched language-independent MathML formulae in 2020.\nAdditionally, we describe our plans to improve the accessibility and\ndiscoverability of mathematical knowledge in Wikimedia projects further.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 17:56:26 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 19:25:19 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Schubotz", "Moritz", ""], ["Greiner-Petter", "Andr\u00e9", ""], ["Meuschke", "Norman", ""], ["Teschke", "Olaf", ""], ["Gipp", "Bela", ""]]}, {"id": "2003.09557", "submitter": "Siqi Wu", "authors": "Siqi Wu, Marian-Andrei Rizoiu, Lexing Xie", "title": "Variation across Scales: Measurement Fidelity under Twitter Data\n  Sampling", "comments": "ICWSM 2020, the code and datasets are publicly available at\n  https://github.com/avalanchesiqi/twitter-sampling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive understanding of data quality is the cornerstone of\nmeasurement studies in social media research. This paper presents in-depth\nmeasurements on the effects of Twitter data sampling across different\ntimescales and different subjects (entities, networks, and cascades). By\nconstructing complete tweet streams, we show that Twitter rate limit message is\nan accurate indicator for the volume of missing tweets. Sampling also differs\nsignificantly across timescales. While the hourly sampling rate is influenced\nby the diurnal rhythm in different time zones, the millisecond level sampling\nis heavily affected by the implementation choices. For Twitter entities such as\nusers, we find the Bernoulli process with a uniform rate approximates the\nempirical distributions well. It also allows us to estimate the true ranking\nwith the observed sample data. For networks on Twitter, their structures are\naltered significantly and some components are more likely to be preserved. For\nretweet cascades, we observe changes in distributions of tweet inter-arrival\ntime and user influence, which will affect models that rely on these features.\nThis work calls attention to noises and potential biases in social data, and\nprovides a few tools to measure Twitter sampling effects.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 02:27:27 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 11:50:00 GMT"}, {"version": "v3", "created": "Sun, 5 Apr 2020 14:07:17 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wu", "Siqi", ""], ["Rizoiu", "Marian-Andrei", ""], ["Xie", "Lexing", ""]]}, {"id": "2003.09592", "submitter": "Tao Qi", "authors": "Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang, Xing Xie", "title": "Privacy-Preserving News Recommendation Model Learning", "comments": "The paper is to appear in \"Findings of ACL: EMNLP 2020\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News recommendation aims to display news articles to users based on their\npersonal interest. Existing news recommendation methods rely on centralized\nstorage of user behavior data for model training, which may lead to privacy\nconcerns and risks due to the privacy-sensitive nature of user behaviors. In\nthis paper, we propose a privacy-preserving method for news recommendation\nmodel training based on federated learning, where the user behavior data is\nlocally stored on user devices. Our method can leverage the useful information\nin the behaviors of massive number users to train accurate news recommendation\nmodels and meanwhile remove the need of centralized storage of them. More\nspecifically, on each user device we keep a local copy of the news\nrecommendation model, and compute gradients of the local model based on the\nuser behaviors in this device. The local gradients from a group of randomly\nselected users are uploaded to server, which are further aggregated to update\nthe global model in the server. Since the model gradients may contain some\nimplicit private information, we apply local differential privacy (LDP) to them\nbefore uploading for better privacy protection. The updated global model is\nthen distributed to each user device for local model update. We repeat this\nprocess for multiple rounds. Extensive experiments on a real-world dataset show\nthe effectiveness of our method in news recommendation model training with\nprivacy protection.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 06:52:27 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 06:29:13 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 05:55:08 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Qi", "Tao", ""], ["Wu", "Fangzhao", ""], ["Wu", "Chuhan", ""], ["Huang", "Yongfeng", ""], ["Xie", "Xing", ""]]}, {"id": "2003.09881", "submitter": "Malte Ostendorff", "authors": "Malte Ostendorff, Terry Ruas, Moritz Schubotz, Georg Rehm, Bela Gipp", "title": "Pairwise Multi-Class Document Classification for Semantic Relations\n  between Wikipedia Articles", "comments": "Accepted at ACM/IEEE Joint Conference on Digital Libraries (JCDL\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many digital libraries recommend literature to their users considering the\nsimilarity between a query document and their repository. However, they often\nfail to distinguish what is the relationship that makes two documents alike. In\nthis paper, we model the problem of finding the relationship between two\ndocuments as a pairwise document classification task. To find the semantic\nrelation between documents, we apply a series of techniques, such as GloVe,\nParagraph-Vectors, BERT, and XLNet under different configurations (e.g.,\nsequence length, vector concatenation scheme), including a Siamese architecture\nfor the Transformer-based systems. We perform our experiments on a newly\nproposed dataset of 32,168 Wikipedia article pairs and Wikidata properties that\ndefine the semantic document relations. Our results show vanilla BERT as the\nbest performing system with an F1-score of 0.93, which we manually examine to\nbetter understand its applicability to other domains. Our findings suggest that\nclassifying semantic relations between documents is a solvable task and\nmotivates the development of recommender systems based on the evaluated\ntechniques. The discussions in this paper serve as first steps in the\nexploration of documents through SPARQL-like queries such that one could find\ndocuments that are similar in one aspect but dissimilar in another.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 12:52:56 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Ostendorff", "Malte", ""], ["Ruas", "Terry", ""], ["Schubotz", "Moritz", ""], ["Rehm", "Georg", ""], ["Gipp", "Bela", ""]]}, {"id": "2003.09986", "submitter": "Yao Qiang", "authors": "Yao Qiang, Xin Li, Dongxiao Zhu", "title": "Toward Tag-free Aspect Based Sentiment Analysis: A Multiple Attention\n  Network Approach", "comments": "to appear in the proceedings of IJCNN'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing aspect based sentiment analysis (ABSA) approaches leverage various\nneural network models to extract the aspect sentiments via learning\naspect-specific feature representations. However, these approaches heavily rely\non manual tagging of user reviews according to the predefined aspects as the\ninput, a laborious and time-consuming process. Moreover, the underlying methods\ndo not explain how and why the opposing aspect level polarities in a user\nreview lead to the overall polarity. In this paper, we tackle these two\nproblems by designing and implementing a new Multiple-Attention Network (MAN)\napproach for more powerful ABSA without the need for aspect tags using two new\ntag-free data sets crawled directly from TripAdvisor\n({https://www.tripadvisor.com}). With the Self- and Position-Aware attention\nmechanism, MAN is capable of extracting both aspect level and overall\nsentiments from the text reviews using the aspect level and overall customer\nratings, and it can also detect the vital aspect(s) leading to the overall\nsentiment polarity among different aspects via a new aspect ranking scheme. We\ncarry out extensive experiments to demonstrate the strong performance of MAN\ncompared to other state-of-the-art ABSA approaches and the explainability of\nour approach by visualizing and interpreting attention weights in case studies.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 20:18:20 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Qiang", "Yao", ""], ["Li", "Xin", ""], ["Zhu", "Dongxiao", ""]]}, {"id": "2003.09989", "submitter": "Alexander Nwala", "authors": "Alexander C. Nwala, Michele C. Weigle, Michael L. Nelson", "title": "365 Dots in 2019: Quantifying Attention of News Sources", "comments": "This is an extended version of the paper accepted at Computation +\n  Journalism Symposium 2020, which has been postponed because of COVID-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate the overlap of topics of online news articles from a variety\nof sources. To do this, we provide a platform for studying the news by\nmeasuring this overlap and scoring news stories according to the degree of\nattention in near-real time. This can enable multiple studies, including\nidentifying topics that receive the most attention from news organizations and\nidentifying slow news days versus major news days. Our application, StoryGraph,\nperiodically (10-minute intervals) extracts the first five news articles from\nthe RSS feeds of 17 US news media organizations across the partisanship\nspectrum (left, center, and right). From these articles, StoryGraph extracts\nnamed entities (PEOPLE, LOCATIONS, ORGANIZATIONS, etc.) and then represents\neach news article with its set of extracted named entities. Finally, StoryGraph\ngenerates a news similarity graph where the nodes represent news articles, and\nan edge between a pair of nodes represents a high degree of similarity between\nthe nodes (similar news stories). Each news story within the news similarity\ngraph is assigned an attention score which quantifies the amount of attention\nthe topics in the news story receive collectively from the news media\norganizations. The StoryGraph service has been running since August 2017, and\nusing this method, we determined that the top news story of 2018 was the\n\"Kavanaugh hearings\" with attention score of 25.85 on September 27, 2018.\nSimilarly, the top news story for 2019 so far (2019-12-12) is \"AG William\nBarr's release of his principal conclusions of the Mueller Report,\" with an\nattention score of 22.93 on March 24, 2019.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 20:32:47 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Nwala", "Alexander C.", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "2003.10149", "submitter": "Jiaying Peng", "authors": "Yang Liu, Liang Chen, Xiangnan He, Jiaying Peng, Zibin Zheng, Jie Tang", "title": "Modelling High-Order Social Relations for Item Recommendation", "comments": "11 pages, 8 figurs, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of online social network makes it compulsory to study how\nsocial relations affect user choice. However, most existing methods leverage\nonly first-order social relations, that is, the direct neighbors that are\nconnected to the target user. The high-order social relations, e.g., the\nfriends of friends, which very informative to reveal user preference, have been\nlargely ignored. In this work, we focus on modeling the indirect influence from\nthe high-order neighbors in social networks to improve the performance of item\nrecommendation. Distinct from mainstream social recommenders that regularize\nthe model learning with social relations, we instead propose to directly factor\nsocial relations in the predictive model, aiming at learning better user\nembeddings to improve recommendation. To address the challenge that high-order\nneighbors increase dramatically with the order size, we propose to recursively\n\"propagate\" embeddings along the social network, effectively injecting the\ninfluence of high-order neighbors into user representation. We conduct\nexperiments on two real datasets of Yelp and Douban to verify our High-Order\nSocial Recommender (HOSR) model. Empirical results show that our HOSR\nsignificantly outperforms recent graph regularization-based recommenders NSCR\nand IF-BPR+, and graph convolutional network-based social influence prediction\nmodel DeepInf, achieving new state-of-the-arts of the task.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 09:35:41 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Liu", "Yang", ""], ["Chen", "Liang", ""], ["He", "Xiangnan", ""], ["Peng", "Jiaying", ""], ["Zheng", "Zibin", ""], ["Tang", "Jie", ""]]}, {"id": "2003.10412", "submitter": "Caleb Belth", "authors": "Caleb Belth, Xinyi Zheng, Jilles Vreeken, Danai Koutra", "title": "What is Normal, What is Strange, and What is Missing in a Knowledge\n  Graph: Unified Characterization via Inductive Summarization", "comments": "10 pages, plus 2 pages of references. 5 figures. Accepted at The Web\n  Conference 2020", "journal-ref": null, "doi": "10.1145/3366423.3380189", "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge graphs (KGs) store highly heterogeneous information about the world\nin the structure of a graph, and are useful for tasks such as question\nanswering and reasoning. However, they often contain errors and are missing\ninformation. Vibrant research in KG refinement has worked to resolve these\nissues, tailoring techniques to either detect specific types of errors or\ncomplete a KG.\n  In this work, we introduce a unified solution to KG characterization by\nformulating the problem as unsupervised KG summarization with a set of\ninductive, soft rules, which describe what is normal in a KG, and thus can be\nused to identify what is abnormal, whether it be strange or missing. Unlike\nfirst-order logic rules, our rules are labeled, rooted graphs, i.e., patterns\nthat describe the expected neighborhood around a (seen or unseen) node, based\non its type, and information in the KG. Stepping away from the traditional\nsupport/confidence-based rule mining techniques, we propose KGist, Knowledge\nGraph Inductive SummarizaTion, which learns a summary of inductive rules that\nbest compress the KG according to the Minimum Description Length principle---a\nformulation that we are the first to use in the context of KG rule mining. We\napply our rules to three large KGs (NELL, DBpedia, and Yago), and tasks such as\ncompression, various types of error detection, and identification of incomplete\ninformation. We show that KGist outperforms task-specific, supervised and\nunsupervised baselines in error detection and incompleteness identification,\n(identifying the location of up to 93% of missing entities---over 10% more than\nbaselines), while also being efficient for large knowledge graphs.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 17:38:31 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Belth", "Caleb", ""], ["Zheng", "Xinyi", ""], ["Vreeken", "Jilles", ""], ["Koutra", "Danai", ""]]}, {"id": "2003.10414", "submitter": "Venkatesh Shenoy Kadandale", "authors": "Venkatesh S. Kadandale, Juan F. Montesinos, Gloria Haro, Emilia\n  G\\'omez", "title": "Multi-channel U-Net for Music Source Separation", "comments": "The paper has been accepted at IEEE MMSP2020. Project Page:\n  https://vskadandale.github.io/multi-channel-unet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fairly straightforward approach for music source separation is to train\nindependent models, wherein each model is dedicated for estimating only a\nspecific source. Training a single model to estimate multiple sources generally\ndoes not perform as well as the independent dedicated models. However,\nConditioned U-Net (C-U-Net) uses a control mechanism to train a single model\nfor multi-source separation and attempts to achieve a performance comparable to\nthat of the dedicated models. We propose a multi-channel U-Net (M-U-Net)\ntrained using a weighted multi-task loss as an alternative to the C-U-Net. We\ninvestigate two weighting strategies for our multi-task loss: 1) Dynamic\nWeighted Average (DWA), and 2) Energy Based Weighting (EBW). DWA determines the\nweights by tracking the rate of change of loss of each task during training.\nEBW aims to neutralize the effect of the training bias arising from the\ndifference in energy levels of each of the sources in a mixture. Our methods\nprovide three-fold advantages compared to C-UNet: 1) Fewer effective training\niterations per epoch, 2) Fewer trainable network parameters (no control\nparameters), and 3) Faster processing at inference. Our methods achieve\nperformance comparable to that of C-U-Net and the dedicated U-Nets at a much\nlower training cost.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 17:42:35 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 10:14:44 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2020 13:37:58 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Kadandale", "Venkatesh S.", ""], ["Montesinos", "Juan F.", ""], ["Haro", "Gloria", ""], ["G\u00f3mez", "Emilia", ""]]}, {"id": "2003.10421", "submitter": "Eric M\\\"uller-Budack", "authors": "Eric M\\\"uller-Budack, Jonas Theiner, Sebastian Diering, Maximilian\n  Idahl, Ralph Ewerth", "title": "Multimodal Analytics for Real-world News using Measures of Cross-modal\n  Entity Consistency", "comments": "Accepted for publication in: International Conference on Multimedia\n  Retrieval (ICMR), Dublin, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web has become a popular source for gathering information and\nnews. Multimodal information, e.g., enriching text with photos, is typically\nused to convey the news more effectively or to attract attention. Photo content\ncan range from decorative, depict additional important information, or can even\ncontain misleading information. Therefore, automatic approaches to quantify\ncross-modal consistency of entity representation can support human assessors to\nevaluate the overall multimodal message, for instance, with regard to bias or\nsentiment. In some cases such measures could give hints to detect fake news,\nwhich is an increasingly important topic in today's society. In this paper, we\nintroduce a novel task of cross-modal consistency verification in real-world\nnews and present a multimodal approach to quantify the entity coherence between\nimage and text. Named entity linking is applied to extract persons, locations,\nand events from news texts. Several measures are suggested to calculate\ncross-modal similarity for these entities using state of the art approaches. In\ncontrast to previous work, our system automatically gathers example data from\nthe Web and is applicable to real-world news. Results on two novel datasets\nthat cover different languages, topics, and domains demonstrate the feasibility\nof our approach. Datasets and code are publicly available to foster research\ntowards this new direction.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 17:49:06 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 09:22:53 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["M\u00fcller-Budack", "Eric", ""], ["Theiner", "Jonas", ""], ["Diering", "Sebastian", ""], ["Idahl", "Maximilian", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2003.10534", "submitter": "Somalee Datta", "authors": "Somalee Datta, Jose Posada, Garrick Olson, Wencheng Li, Ciaran\n  O'Reilly, Deepa Balraj, Joseph Mesterhazy, Joseph Pallas, Priyamvada Desai,\n  Nigam Shah", "title": "A new paradigm for accelerating clinical data science at Stanford\n  Medicine", "comments": "Total of 44 pages. Main has total of 18 pages including references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stanford Medicine is building a new data platform for our academic research\ncommunity to do better clinical data science. Hospitals have a large amount of\npatient data and researchers have demonstrated the ability to reuse that data\nand AI approaches to derive novel insights, support patient care, and improve\ncare quality. However, the traditional data warehouse and Honest Broker\napproaches that are in current use, are not scalable. We are establishing a new\nsecure Big Data platform that aims to reduce time to access and analyze data.\nIn this platform, data is anonymized to preserve patient data privacy and made\navailable preparatory to Institutional Review Board (IRB) submission.\nFurthermore, the data is standardized such that analysis done at Stanford can\nbe replicated elsewhere using the same analytical code and clinical concepts.\nFinally, the analytics data warehouse integrates with a secure data science\ncomputational facility to support large scale data analytics. The ecosystem is\ndesigned to bring the modern data science community to highly sensitive\nclinical data in a secure and collaborative big data analytics environment with\na goal to enable bigger, better and faster science.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 16:21:42 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Datta", "Somalee", ""], ["Posada", "Jose", ""], ["Olson", "Garrick", ""], ["Li", "Wencheng", ""], ["O'Reilly", "Ciaran", ""], ["Balraj", "Deepa", ""], ["Mesterhazy", "Joseph", ""], ["Pallas", "Joseph", ""], ["Desai", "Priyamvada", ""], ["Shah", "Nigam", ""]]}, {"id": "2003.10699", "submitter": "Dominik Kowald PhD", "authors": "Dominik Kowald, Elisabeth Lex, Markus Schedl", "title": "Utilizing Human Memory Processes to Model Genre Preferences for\n  Personalized Music Recommendations", "comments": "Dominik Kowald and Elisabeth Lex contributed equally to this work", "journal-ref": "HUMANIZE Workshop @ IUI'2020", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a psychology-inspired approach to model and\npredict the music genre preferences of different groups of users by utilizing\nhuman memory processes. These processes describe how humans access information\nunits in their memory by considering the factors of (i) past usage frequency,\n(ii) past usage recency, and (iii) the current context. Using a publicly\navailable dataset of more than a billion music listening records shared on the\nmusic streaming platform Last.fm, we find that our approach provides\nsignificantly better prediction accuracy results than various baseline\nalgorithms for all evaluated user groups, i.e., (i) low-mainstream music\nlisteners, (ii) medium-mainstream music listeners, and (iii) high-mainstream\nmusic listeners. Furthermore, our approach is based on a simple psychological\nmodel, which contributes to the transparency and explainability of the\ncalculated predictions.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 07:40:33 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Kowald", "Dominik", ""], ["Lex", "Elisabeth", ""], ["Schedl", "Markus", ""]]}, {"id": "2003.10715", "submitter": "David Schindler", "authors": "David Schindler, Benjamin Zapilko, Frank Kr\\\"uger", "title": "Investigating Software Usage in the Social Sciences: A Knowledge Graph\n  Approach", "comments": "16 pages, 4 figures, preprint of a full paper at Extended Semantic\n  Web Conference (ESWC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge about the software used in scientific investigations is necessary\nfor different reasons, including provenance of the results, measuring software\nimpact to attribute developers, and bibliometric software citation analysis in\ngeneral. Additionally, providing information about whether and how the software\nand the source code are available allows an assessment about the state and role\nof open source software in science in general. While such analyses can be done\nmanually, large scale analyses require the application of automated methods of\ninformation extraction and linking. In this paper, we present SoftwareKG - a\nknowledge graph that contains information about software mentions from more\nthan 51,000 scientific articles from the social sciences. A silver standard\ncorpus, created by a distant and weak supervision approach, and a gold standard\ncorpus, created by manual annotation, were used to train an LSTM based neural\nnetwork to identify software mentions in scientific articles. The model\nachieves a recognition rate of .82 F-score in exact matches. As a result, we\nidentified more than 133,000 software mentions. For entity disambiguation, we\nused the public domain knowledge base DBpedia. Furthermore, we linked the\nentities of the knowledge graph to other knowledge bases such as the Microsoft\nAcademic Knowledge Graph, the Software Ontology, and Wikidata. Finally, we\nillustrate, how SoftwareKG can be used to assess the role of software in the\nsocial sciences.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 08:38:36 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Schindler", "David", ""], ["Zapilko", "Benjamin", ""], ["Kr\u00fcger", "Frank", ""]]}, {"id": "2003.10719", "submitter": "Yang Xu", "authors": "Yang Xu, Lei Zhu, Zhiyong Cheng, Jingjing Li, Jiande Sun", "title": "Multi-Feature Discrete Collaborative Filtering for Fast Cold-start\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing is an effective technique to address the large-scale recommendation\nproblem, due to its high computation and storage efficiency on calculating the\nuser preferences on items. However, existing hashing-based recommendation\nmethods still suffer from two important problems: 1) Their recommendation\nprocess mainly relies on the user-item interactions and single specific content\nfeature. When the interaction history or the content feature is unavailable\n(the cold-start problem), their performance will be seriously deteriorated. 2)\nExisting methods learn the hash codes with relaxed optimization or adopt\ndiscrete coordinate descent to directly solve binary hash codes, which results\nin significant quantization loss or consumes considerable computation time. In\nthis paper, we propose a fast cold-start recommendation method, called\nMulti-Feature Discrete Collaborative Filtering (MFDCF), to solve these\nproblems. Specifically, a low-rank self-weighted multi-feature fusion module is\ndesigned to adaptively project the multiple content features into binary yet\ninformative hash codes by fully exploiting their complementarity. Additionally,\nwe develop a fast discrete optimization algorithm to directly compute the\nbinary hash codes with simple operations. Experiments on two public\nrecommendation datasets demonstrate that MFDCF outperforms the\nstate-of-the-arts on various aspects.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 08:55:15 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Xu", "Yang", ""], ["Zhu", "Lei", ""], ["Cheng", "Zhiyong", ""], ["Li", "Jingjing", ""], ["Sun", "Jiande", ""]]}, {"id": "2003.10998", "submitter": "Artur Strzelecki", "authors": "Artur Strzelecki", "title": "The Second Worldwide Wave of Interest in Coronavirus since the COVID-19\n  Outbreaks in South Korea, Italy and Iran: A Google Trends Study", "comments": "4 pages, 1 figure, 2 tables", "journal-ref": "Brain, Behavior, and Immunity 88 (2020) 950-951", "doi": "10.1016/j.bbi.2020.04.042", "report-no": null, "categories": "cs.CY cs.IR econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent emergence of a new coronavirus, COVID-19, has gained extensive\ncoverage in public media and global news. As of 24 March 2020, the virus has\ncaused viral pneumonia in tens of thousands of people in Wuhan, China, and\nthousands of cases in 184 other countries and territories. This study explores\nthe potential use of Google Trends (GT) to monitor worldwide interest in this\nCOVID-19 epidemic. GT was chosen as a source of reverse engineering data, given\nthe interest in the topic. Current data on COVID-19 is retrieved from (GT)\nusing one main search topic: Coronavirus. Geographical settings for GT are\nworldwide, China, South Korea, Italy and Iran. The reported period is 15\nJanuary 2020 to 24 March 2020. The results show that the highest worldwide peak\nin the first wave of demand for information was on 31 January 2020. After the\nfirst peak, the number of new cases reported daily rose for 6 days. A second\nwave started on 21 February 2020 after the outbreaks were reported in Italy,\nwith the highest peak on 16 March 2020. The second wave is six times as big as\nthe first wave. The number of new cases reported daily is rising day by day.\nThis short communication gives a brief introduction to how the demand for\ninformation on coronavirus epidemic is reported through GT.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 17:33:59 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 07:23:37 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 13:41:20 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Strzelecki", "Artur", ""]]}, {"id": "2003.11159", "submitter": "Josimar Chire Saire", "authors": "Josimar E. Chire Saire and Roberto C. Navarro", "title": "What is the people posting about symptoms related to Coronavirus in\n  Bogota, Colombia?", "comments": "The paper has 3 pages, two columns. If the paper does not fit\n  Information Networks field, you may suggest one", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last months, there is an increasing alarm about a new mutation of\ncoronavirus, covid-19 coined by World Health Organization(WHO) with an impact\nin many areas: economy, health, politics and others. This situation was\ndeclared a pandemic by WHO, because of the fast expansion over many countries.\nAt the same time, people is using Social Networks to express what they think,\nfeel or experiment, so this people are Social Sensors and helps to analyze what\nis happening in their city. The objective of this paper is analyze the\npublications of Colombian people living in Bogota with a radius of 50 km using\nText Mining techniques from symptomatology approach. The results support the\nunderstanding of the spread in Colombia related to symptoms of covid19.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 00:07:50 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Saire", "Josimar E. Chire", ""], ["Navarro", "Roberto C.", ""]]}, {"id": "2003.11205", "submitter": "Charilaos Kanatsoulis", "authors": "Mikael S{\\o}rensen, Charilaos I. Kanatsoulis, and Nicholas D.\n  Sidiropoulos", "title": "Generalized Canonical Correlation Analysis: A Subspace Intersection\n  Approach", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2021.3061218", "report-no": null, "categories": "cs.LG cs.IR eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Canonical Correlation Analysis (GCCA) is an important tool that\nfinds numerous applications in data mining, machine learning, and artificial\nintelligence. It aims at finding `common' random variables that are strongly\ncorrelated across multiple feature representations (views) of the same set of\nentities. CCA and to a lesser extent GCCA have been studied from the\nstatistical and algorithmic points of view, but not as much from the standpoint\nof linear algebra. This paper offers a fresh algebraic perspective of GCCA\nbased on a (bi-)linear generative model that naturally captures its essence. It\nis shown that from a linear algebra point of view, GCCA is tantamount to\nsubspace intersection; and conditions under which the common subspace of the\ndifferent views is identifiable are provided. A novel GCCA algorithm is\nproposed based on subspace intersection, which scales up to handle large GCCA\ntasks. Synthetic as well as real data experiments are provided to showcase the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 04:04:25 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["S\u00f8rensen", "Mikael", ""], ["Kanatsoulis", "Charilaos I.", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "2003.11235", "submitter": "Guilin Li", "authors": "Bin Liu, Chenxu Zhu, Guilin Li, Weinan Zhang, Jincai Lai, Ruiming\n  Tang, Xiuqiang He, Zhenguo Li, Yong Yu", "title": "AutoFIS: Automatic Feature Interaction Selection in Factorization Models\n  for Click-Through Rate Prediction", "comments": "KDD 2020 ADS track oral accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning feature interactions is crucial for click-through rate (CTR)\nprediction in recommender systems. In most existing deep learning models,\nfeature interactions are either manually designed or simply enumerated.\nHowever, enumerating all feature interactions brings large memory and\ncomputation cost. Even worse, useless interactions may introduce noise and\ncomplicate the training process. In this work, we propose a two-stage algorithm\ncalled Automatic Feature Interaction Selection (AutoFIS). AutoFIS can\nautomatically identify important feature interactions for factorization models\nwith computational cost just equivalent to training the target model to\nconvergence. In the \\emph{search stage}, instead of searching over a discrete\nset of candidate feature interactions, we relax the choices to be continuous by\nintroducing the architecture parameters. By implementing a regularized\noptimizer over the architecture parameters, the model can automatically\nidentify and remove the redundant feature interactions during the training\nprocess of the model. In the \\emph{re-train stage}, we keep the architecture\nparameters serving as an attention unit to further boost the performance.\nOffline experiments on three large-scale datasets (two public benchmarks, one\nprivate) demonstrate that AutoFIS can significantly improve various FM based\nmodels. AutoFIS has been deployed onto the training platform of Huawei App\nStore recommendation service, where a 10-day online A/B test demonstrated that\nAutoFIS improved the DeepFM model by 20.3\\% and 20.1\\% in terms of CTR and CVR\nrespectively.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 06:53:54 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 02:04:20 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 14:19:47 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Liu", "Bin", ""], ["Zhu", "Chenxu", ""], ["Li", "Guilin", ""], ["Zhang", "Weinan", ""], ["Lai", "Jincai", ""], ["Tang", "Ruiming", ""], ["He", "Xiuqiang", ""], ["Li", "Zhenguo", ""], ["Yu", "Yong", ""]]}, {"id": "2003.11314", "submitter": "Noemi Mauro", "authors": "Noemi Mauro, Liliana Ardissono", "title": "Session-based Suggestion of Topics for Geographic Exploratory Search", "comments": null, "journal-ref": "Proceedings of the 23rd International Conference on Intelligent\n  User Interfaces (IUI 2018)", "doi": "10.1145/3172944.3172957", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploratory information search can challenge users in the formulation of\nefficacious search queries. Moreover, complex information spaces, such as those\nmanaged by Geographical Information Systems, can disorient people, making it\ndifficult to find relevant data. In order to address these issues, we developed\na session-based suggestion model that proposes concepts as a \"you might also be\ninterested in\" function, by taking the user's previous queries into account.\nOur model can be applied to incrementally generate suggestions in interactive\nsearch. It can be used for query expansion, and in general to guide users in\nthe exploration of possibly complex spaces of data categories. Our model is\nbased on a concept co-occurrence graph that describes how frequently concepts\nare searched together in search sessions. Starting from an ontological domain\nrepresentation, we generated the graph by analyzing the query log of a major\nsearch engine. Moreover, we identified clusters of ontology concepts which\nfrequently co-occur in the sessions of the log via community detection on the\ngraph. The evaluation of our model provided satisfactory accuracy results.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 10:46:03 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Mauro", "Noemi", ""], ["Ardissono", "Liliana", ""]]}, {"id": "2003.11445", "submitter": "Noemi Mauro", "authors": "Noemi Mauro, Liliana Ardissono and Zhongli Filippo Hu", "title": "Multi-faceted Trust-based Collaborative Filtering", "comments": null, "journal-ref": "Proceedings of the 27th ACM Conference on User Modeling,\n  Adaptation and Personalization (UMAP 2019)", "doi": "10.1145/3320435.3320441", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many collaborative recommender systems leverage social correlation theories\nto improve suggestion performance. However, they focus on explicit relations\nbetween users and they leave out other types of information that can contribute\nto determine users' global reputation; e.g., public recognition of reviewers'\nquality. We are interested in understanding if and when these additional types\nof feedback improve Top-N recommendation. For this purpose, we propose a\nmulti-faceted trust model to integrate local trust, represented by social\nlinks, with various types of global trust evidence provided by social networks.\nWe aim at identifying general classes of data in order to make our model\napplicable to different case studies. Then, we test the model by applying it to\na variant of User-to-User Collaborative filtering (U2UCF) which supports the\nfusion of rating similarity, local trust derived from social relations, and\nmulti-faceted reputation for rating prediction. We test our model on two\ndatasets: the Yelp one publishes generic friend relations between users but\nprovides different types of trust feedback, including user profile\nendorsements. The LibraryThing dataset offers fewer types of feedback but it\nprovides more selective friend relations aimed at content sharing. The results\nof our experiments show that, on the Yelp dataset, our model outperforms both\nU2UCF and state-of-the-art trust-based recommenders that only use rating\nsimilarity and social relations. Differently, in the LibraryThing dataset, the\ncombination of social relations and rating similarity achieves the best\nresults. The lesson we learn is that multi-faceted trust can be a valuable type\nof information for recommendation. However, before using it in an application\ndomain, an analysis of the type and amount of available trust evidence has to\nbe done to assess its real impact on recommendation performance.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 15:27:06 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Mauro", "Noemi", ""], ["Ardissono", "Liliana", ""], ["Hu", "Zhongli Filippo", ""]]}, {"id": "2003.11459", "submitter": "Kunwoo Park", "authors": "Kunwoo Park, Taegyun Kim, Seunghyun Yoon, Meeyoung Cha, and Kyomin\n  Jung", "title": "BaitWatcher: A lightweight web interface for the detection of\n  incongruent news headlines", "comments": "24 pages (single column), 7 figures. This research article is\n  published as a book chapter of \\textit{Fake News, Disinformation, and\n  Misinformation in Social Media-Emerging Research Challenges and\n  Opportunities}. Springer, 2020. arXiv admin note: text overlap with\n  arXiv:1811.07066", "journal-ref": null, "doi": "10.1007/978-3-030-42699-6", "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In digital environments where substantial amounts of information are shared\nonline, news headlines play essential roles in the selection and diffusion of\nnews articles. Some news articles attract audience attention by showing\nexaggerated or misleading headlines. This study addresses the \\textit{headline\nincongruity} problem, in which a news headline makes claims that are either\nunrelated or opposite to the contents of the corresponding article. We present\n\\textit{BaitWatcher}, which is a lightweight web interface that guides readers\nin estimating the likelihood of incongruence in news articles before clicking\non the headlines. BaitWatcher utilizes a hierarchical recurrent encoder that\nefficiently learns complex textual representations of a news headline and its\nassociated body text. For training the model, we construct a million scale\ndataset of news articles, which we also release for broader research use. Based\non the results of a focus group interview, we discuss the importance of\ndeveloping an interpretable AI agent for the design of a better interface for\nmitigating the effects of online misinformation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 23:43:02 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Park", "Kunwoo", ""], ["Kim", "Taegyun", ""], ["Yoon", "Seunghyun", ""], ["Cha", "Meeyoung", ""], ["Jung", "Kyomin", ""]]}, {"id": "2003.11634", "submitter": "Himan Abdollahpouri", "authors": "Himan Abdollahpouri, Robin Burke, Masoud Mansoury", "title": "Unfair Exposure of Artists in Music Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness in machine learning has been studied by many researchers. In\nparticular, fairness in recommender systems has been investigated to ensure the\nrecommendations meet certain criteria with respect to certain sensitive\nfeatures such as race, gender etc. However, often recommender systems are\nmulti-stakeholder environments in which the fairness towards all stakeholders\nshould be taken care of. It is well-known that the recommendation algorithms\nsuffer from popularity bias; few popular items are over-recommended which leads\nto the majority of other items not getting proportionate attention. This bias\nhas been investigated from the perspective of the users and how it makes the\nfinal recommendations skewed towards popular items in general. In this paper,\nhowever, we investigate the impact of popularity bias in recommendation\nalgorithms on the provider of the items (i.e. the entities who are behind the\nrecommended items). Using a music dataset for our experiments, we show that,\ndue to some biases in the algorithms, different groups of artists with varying\ndegrees of popularity are systematically and consistently treated differently\nthan others.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 21:03:19 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Abdollahpouri", "Himan", ""], ["Burke", "Robin", ""], ["Mansoury", "Masoud", ""]]}, {"id": "2003.11650", "submitter": "Asia Biega", "authors": "Asia J. Biega, Fernando Diaz, Michael D. Ekstrand, Sebastian Kohlmeier", "title": "Overview of the TREC 2019 Fair Ranking Track", "comments": "Published in The Twenty-Eighth Text REtrieval Conference Proceedings\n  (TREC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the TREC Fair Ranking track was to develop a benchmark for\nevaluating retrieval systems in terms of fairness to different content\nproviders in addition to classic notions of relevance. As part of the\nbenchmark, we defined standardized fairness metrics with evaluation protocols\nand released a dataset for the fair ranking problem. The 2019 task focused on\nreranking academic paper abstracts given a query. The objective was to fairly\nrepresent relevant authors from several groups that were unknown at the system\nsubmission time. Thus, the track emphasized the development of systems which\nhave robust performance across a variety of group definitions. Participants\nwere provided with querylog data (queries, documents, and relevance) from\nSemantic Scholar. This paper presents an overview of the track, including the\ntask definition, descriptions of the data and the annotation process, as well\nas a comparison of the performance of submitted systems.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 21:34:58 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Biega", "Asia J.", ""], ["Diaz", "Fernando", ""], ["Ekstrand", "Michael D.", ""], ["Kohlmeier", "Sebastian", ""]]}, {"id": "2003.11949", "submitter": "Julian Risch", "authors": "Julian Risch, Ralf Krestel", "title": "Top Comment or Flop Comment? Predicting and Explaining User Engagement\n  in Online News Discussions", "comments": "Accepted at the International Conference on Web and Social Media\n  (ICWSM 2020); 11 pages; code and data are available at\n  https://hpi.de/naumann/projects/repeatability/text-mining.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comment sections below online news articles enjoy growing popularity among\nreaders. However, the overwhelming number of comments makes it infeasible for\nthe average news consumer to read all of them and hinders engaging discussions.\nMost platforms display comments in chronological order, which neglects that\nsome of them are more relevant to users and are better conversation starters.\nIn this paper, we systematically analyze user engagement in the form of the\nupvotes and replies that a comment receives. Based on comment texts, we train a\nmodel to distinguish comments that have either a high or low chance of\nreceiving many upvotes and replies. Our evaluation on user comments from\nTheGuardian.com compares recurrent and convolutional neural network models, and\na traditional feature-based classifier. Further, we investigate what makes some\ncomments more engaging than others. To this end, we identify engagement\ntriggers and arrange them in a taxonomy. Explanation methods for neural\nnetworks reveal which input words have the strongest influence on our model's\npredictions. In addition, we evaluate on a dataset of product reviews, which\nexhibit similar properties as user comments, such as featuring upvotes for\nhelpfulness.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 14:48:25 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Risch", "Julian", ""], ["Krestel", "Ralf", ""]]}, {"id": "2003.12218", "submitter": "Xuan Wang", "authors": "Xuan Wang, Xiangchen Song, Bangzheng Li, Yingjun Guan, Jiawei Han", "title": "Comprehensive Named Entity Recognition on CORD-19 with Distant or Weak\n  Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We created this CORD-NER dataset with comprehensive named entity recognition\n(NER) on the COVID-19 Open Research Dataset Challenge (CORD-19) corpus\n(2020-03-13). This CORD-NER dataset covers 75 fine-grained entity types: In\naddition to the common biomedical entity types (e.g., genes, chemicals and\ndiseases), it covers many new entity types related explicitly to the COVID-19\nstudies (e.g., coronaviruses, viral proteins, evolution, materials, substrates\nand immune responses), which may benefit research on COVID-19 related virus,\nspreading mechanisms, and potential vaccines. CORD-NER annotation is a\ncombination of four sources with different NER methods. The quality of CORD-NER\nannotation surpasses SciSpacy (over 10% higher on the F1 score based on a\nsample set of documents), a fully supervised BioNER tool. Moreover, CORD-NER\nsupports incrementally adding new documents as well as adding new entity types\nwhen needed by adding dozens of seeds as the input examples. We will constantly\nupdate CORD-NER based on the incremental updates of the CORD-19 corpus and the\nimprovement of our system.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 03:35:46 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 01:42:54 GMT"}, {"version": "v3", "created": "Wed, 8 Apr 2020 01:19:09 GMT"}, {"version": "v4", "created": "Fri, 10 Apr 2020 04:07:24 GMT"}, {"version": "v5", "created": "Wed, 15 Apr 2020 19:37:16 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Wang", "Xuan", ""], ["Song", "Xiangchen", ""], ["Li", "Bangzheng", ""], ["Guan", "Yingjun", ""], ["Han", "Jiawei", ""]]}, {"id": "2003.12265", "submitter": "Alexander Schindler", "authors": "Alexander Schindler, Sergiu Gordea, Peter Knees", "title": "Unsupervised Cross-Modal Audio Representation Learning from Unstructured\n  Multilingual Text", "comments": "This is the long version of our SAC2020 poster presentation", "journal-ref": "In Proceedings of the 35th ACM/SIGAPP Symposium On Applied\n  Computing (SAC2020), March 30-April 3, 2020, Brno, Czech Republic", "doi": null, "report-no": null, "categories": "cs.MM cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to unsupervised audio representation learning. Based\non a triplet neural network architecture, we harnesses semantically related\ncross-modal information to estimate audio track-relatedness. By applying Latent\nSemantic Indexing (LSI) we embed corresponding textual information into a\nlatent vector space from which we derive track relatedness for online triplet\nselection. This LSI topic modelling facilitates fine-grained selection of\nsimilar and dissimilar audio-track pairs to learn the audio representation\nusing a Convolution Recurrent Neural Network (CRNN). By this we directly\nproject the semantic context of the unstructured text modality onto the learned\nrepresentation space of the audio modality without deriving structured\nground-truth annotations from it. We evaluate our approach on the Europeana\nSounds collection and show how to improve search in digital audio libraries by\nharnessing the multilingual meta-data provided by numerous European digital\nlibraries. We show that our approach is invariant to the variety of annotation\nstyles as well as to the different languages of this collection. The learned\nrepresentations perform comparable to the baseline of handcrafted features,\nrespectively exceeding this baseline in similarity retrieval precision at\nhigher cut-offs with only 15\\% of the baseline's feature vector length.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 07:37:15 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Schindler", "Alexander", ""], ["Gordea", "Sergiu", ""], ["Knees", "Peter", ""]]}, {"id": "2003.12611", "submitter": "Angelo Salatino", "authors": "Angelo A. Salatino, Francesco Osborne, Enrico Motta", "title": "Ontology Extraction and Usage in the Scholarly Knowledge Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ontologies of research areas have been proven to be useful in many\napplication for analysing and making sense of scholarly data. In this chapter,\nwe present the Computer Science Ontology (CSO), which is the largest ontology\nof research areas in the field of Computer Science, and discuss a number of\napplications that build on CSO, to support high-level tasks, such as topic\nclassification, metadata extraction, and recommendation of books.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 19:35:47 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 15:18:52 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Salatino", "Angelo A.", ""], ["Osborne", "Francesco", ""], ["Motta", "Enrico", ""]]}, {"id": "2003.12773", "submitter": "Venky Soundararajan", "authors": "AJ Venkatakrishnan, Arjun Puranik, Akash Anand, David Zemmour, Xiang\n  Yao, Xiaoying Wu, Ramakrishna Chilaka, Dariusz K. Murakowski, Kristopher\n  Standish, Bharathwaj Raghunathan, Tyler Wagner, Enrique Garcia-Rivera, Hugo\n  Solomon, Abhinav Garg, Rakesh Barve, Anuli Anyanwu-Ofili, Najat Khan, Venky\n  Soundararajan", "title": "Knowledge synthesis from 100 million biomedical documents augments the\n  deep expression profiling of coronavirus receptors", "comments": "5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.IR q-bio.BM q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic demands assimilation of all available biomedical\nknowledge to decode its mechanisms of pathogenicity and transmission. Despite\nthe recent renaissance in unsupervised neural networks for decoding\nunstructured natural languages, a platform for the real-time synthesis of the\nexponentially growing biomedical literature and its comprehensive triangulation\nwith deep omic insights is not available. Here, we present the nferX platform\nfor dynamic inference from over 45 quadrillion possible conceptual associations\nextracted from unstructured biomedical text, and their triangulation with\nSingle Cell RNA-sequencing based insights from over 25 tissues. Using this\nplatform, we identify intersections between the pathologic manifestations of\nCOVID-19 and the comprehensive expression profile of the SARS-CoV-2 receptor\nACE2. We find that tongue keratinocytes and olfactory epithelial cells are\nlikely under-appreciated targets of SARS-CoV-2 infection, correlating with\nreported loss of sense of taste and smell as early indicators of COVID-19\ninfection, including in otherwise asymptomatic patients. Airway club cells,\nciliated cells and type II pneumocytes in the lung, and enterocytes of the gut\nalso express ACE2. This study demonstrates how a holistic data science platform\ncan leverage unprecedented quantities of structured and unstructured publicly\navailable data to accelerate the generation of impactful biological insights\nand hypotheses.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 11:48:32 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Venkatakrishnan", "AJ", ""], ["Puranik", "Arjun", ""], ["Anand", "Akash", ""], ["Zemmour", "David", ""], ["Yao", "Xiang", ""], ["Wu", "Xiaoying", ""], ["Chilaka", "Ramakrishna", ""], ["Murakowski", "Dariusz K.", ""], ["Standish", "Kristopher", ""], ["Raghunathan", "Bharathwaj", ""], ["Wagner", "Tyler", ""], ["Garcia-Rivera", "Enrique", ""], ["Solomon", "Hugo", ""], ["Garg", "Abhinav", ""], ["Barve", "Rakesh", ""], ["Anyanwu-Ofili", "Anuli", ""], ["Khan", "Najat", ""], ["Soundararajan", "Venky", ""]]}, {"id": "2003.13016", "submitter": "Georg Rehm", "authors": "Elena Leitner and Georg Rehm and Juli\\'an Moreno-Schneider", "title": "A Dataset of German Legal Documents for Named Entity Recognition", "comments": "Proceedings of the 12th Language Resources and Evaluation Conference\n  (LREC 2020). To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a dataset developed for Named Entity Recognition in German\nfederal court decisions. It consists of approx. 67,000 sentences with over 2\nmillion tokens. The resource contains 54,000 manually annotated entities,\nmapped to 19 fine-grained semantic classes: person, judge, lawyer, country,\ncity, street, landscape, organization, company, institution, court, brand, law,\nordinance, European legal norm, regulation, contract, court decision, and legal\nliterature. The legal documents were, furthermore, automatically annotated with\nmore than 35,000 TimeML-based time expressions. The dataset, which is available\nunder a CC-BY 4.0 license in the CoNNL-2002 format, was developed for training\nan NER service for German legal documents in the EU project Lynx.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 13:20:43 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Leitner", "Elena", ""], ["Rehm", "Georg", ""], ["Moreno-Schneider", "Juli\u00e1n", ""]]}, {"id": "2003.13230", "submitter": "Xusheng Luo", "authors": "Xusheng Luo, Luxin Liu, Yonghua Yang, Le Bo, Yuanpeng Cao, Jinhang Wu,\n  Qiang Li, Keping Yang and Kenny Q. Zhu", "title": "AliCoCo: Alibaba E-commerce Cognitive Concept Net", "comments": "15 pages. Accepted by SIGMOD 2020 Industry Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the ultimate goals of e-commerce platforms is to satisfy various\nshopping needs for their customers. Much efforts are devoted to creating\ntaxonomies or ontologies in e-commerce towards this goal. However, user needs\nin e-commerce are still not well defined, and none of the existing ontologies\nhas the enough depth and breadth for universal user needs understanding. The\nsemantic gap in-between prevents shopping experience from being more\nintelligent. In this paper, we propose to construct a large-scale e-commerce\ncognitive concept net named \"AliCoCo\", which is practiced in Alibaba, the\nlargest Chinese e-commerce platform in the world. We formally define user needs\nin e-commerce, then conceptualize them as nodes in the net. We present details\non how AliCoCo is constructed semi-automatically and its successful, ongoing\nand potential applications in e-commerce.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 05:42:03 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Luo", "Xusheng", ""], ["Liu", "Luxin", ""], ["Yang", "Yonghua", ""], ["Bo", "Le", ""], ["Cao", "Yuanpeng", ""], ["Wu", "Jinhang", ""], ["Li", "Qiang", ""], ["Yang", "Keping", ""], ["Zhu", "Kenny Q.", ""]]}, {"id": "2003.13345", "submitter": "Tomislav Duricic", "authors": "Tomislav Duricic, Hussain Hussain, Emanuel Lacic, Dominik Kowald,\n  Denis Helic, Elisabeth Lex", "title": "Empirical Comparison of Graph Embeddings for Trust-Based Collaborative\n  Filtering", "comments": "10 pages, Accepted as a full paper on the 25th International\n  Symposium on Methodologies for Intelligent Systems (ISMIS'20)", "journal-ref": "Lecture Notes in Computer Science, vol 12117. Springer, Cham. 2020", "doi": "10.1007/978-3-030-59491-6_17", "report-no": null, "categories": "cs.SI cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the utility of graph embeddings to generate latent\nuser representations for trust-based collaborative filtering. In a cold-start\nsetting, on three publicly available datasets, we evaluate approaches from four\nmethod families: (i) factorization-based, (ii) random walk-based, (iii) deep\nlearning-based, and (iv) the Large-scale Information Network Embedding (LINE)\napproach. We find that across the four families, random-walk-based approaches\nconsistently achieve the best accuracy. Besides, they result in highly novel\nand diverse recommendations. Furthermore, our results show that the use of\ngraph embeddings in trust-based collaborative filtering significantly improves\nuser coverage.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 11:22:44 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 15:30:58 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Duricic", "Tomislav", ""], ["Hussain", "Hussain", ""], ["Lacic", "Emanuel", ""], ["Kowald", "Dominik", ""], ["Helic", "Denis", ""], ["Lex", "Elisabeth", ""]]}, {"id": "2003.13474", "submitter": "Noemi Mauro", "authors": "Noemi Mauro, Liliana Ardissono", "title": "Extending a Tag-based Collaborative Recommender with Co-occurring\n  Information Interests", "comments": null, "journal-ref": "Proceedings of the 27th ACM Conference on User Modeling,\n  Adaptation and Personalization (UMAP 2019)", "doi": "10.1145/3320435.3320458", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Filtering is largely applied to personalize item recommendation\nbut its performance is affected by the sparsity of rating data. In order to\naddress this issue, recent systems have been developed to improve\nrecommendation by extracting latent factors from the rating matrices, or by\nexploiting trust relations established among users in social networks. In this\nwork, we are interested in evaluating whether other sources of preference\ninformation than ratings and social ties can be used to improve recommendation\nperformance. Specifically, we aim at testing whether the integration of\nfrequently co-occurring interests in information search logs can improve\nrecommendation performance in User-to-User Collaborative Filtering (U2UCF). For\nthis purpose, we propose the Extended Category-based Collaborative Filtering\n(ECCF) recommender, which enriches category-based user profiles derived from\nthe analysis of rating behavior with data categories that are frequently\nsearched together by people in search sessions. We test our model using a big\nrating dataset and a log of a largely used search engine to extract the\nco-occurrence of interests. The experiments show that ECCF outperforms U2UCF\nand category-based collaborative recommendation in accuracy, MRR, diversity of\nrecommendations and user coverage. Moreover, it outperforms the SVD++ Matrix\nFactorization algorithm in accuracy and diversity of recommendation lists.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 13:36:09 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Mauro", "Noemi", ""], ["Ardissono", "Liliana", ""]]}, {"id": "2003.13481", "submitter": "Noemi Mauro", "authors": "Noemi Mauro, Liliana Ardissono and Adriano Savoca", "title": "Concept-aware Geographic Information Retrieval", "comments": null, "journal-ref": "Proceedings of the International Conference on Web Intelligence\n  (WI 2017)", "doi": "10.1145/3106426.3106498", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textual queries are largely employed in information retrieval to let users\nspecify search goals in a natural way. However, differences in user and system\nterminologies can challenge the identification of the user's information needs,\nand thus the generation of relevant results. We argue that the explicit\nmanagement of ontological knowledge, and of the meaning of concepts (by\nintegrating linguistic and encyclopedic knowledge in the system ontology), can\nimprove the analysis of search queries, because it enables a flexible\nidentification of the topics the user is searching for, regardless of the\nadopted vocabulary. This paper proposes an information retrieval support model\nbased on semantic concept identification. Starting from the recognition of the\nontology concepts that the search query refers to, this model exploits the\nqualifiers specified in the query to select information items on the basis of\npossibly fine-grained features. Moreover, it supports query expansion and\nreformulation by suggesting the exploration of semantically similar concepts,\nas well as of concepts related to those referred in the query through thematic\nrelations. A test on a data-set collected using the OnToMap Participatory GIS\nhas shown that this approach provides accurate results.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 13:47:41 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Mauro", "Noemi", ""], ["Ardissono", "Liliana", ""], ["Savoca", "Adriano", ""]]}, {"id": "2003.13624", "submitter": "Jeffrey Dalton", "authors": "Jeffrey Dalton, Chenyan Xiong, Jamie Callan", "title": "TREC CAsT 2019: The Conversational Assistance Track Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Conversational Assistance Track (CAsT) is a new track for TREC 2019 to\nfacilitate Conversational Information Seeking (CIS) research and to create a\nlarge-scale reusable test collection for conversational search systems. The\ndocument corpus is 38,426,252 passages from the TREC Complex Answer Retrieval\n(CAR) and Microsoft MAchine Reading COmprehension (MARCO) datasets. Eighty\ninformation seeking dialogues (30 train, 50 test) are an average of 9 to 10\nquestions long. Relevance assessments are provided for 30 training topics and\n20 test topics. This year 21 groups submitted a total of 65 runs using varying\nmethods for conversational query understanding and ranking. Methods include\ntraditional retrieval based methods, feature based learning-to-rank, neural\nmodels, and knowledge enhanced methods. A common theme through the runs is the\nuse of BERT-based neural reranking methods. Leading methods also employed\ndocument expansion, conversational query expansion, and generative language\nmodels for conversational query rewriting (GPT-2). The results show a gap\nbetween automatic systems and those using the manually resolved utterances,\nwith a 35% relative improvement of manual rewrites over the best automatic\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 16:58:04 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Dalton", "Jeffrey", ""], ["Xiong", "Chenyan", ""], ["Callan", "Jamie", ""]]}, {"id": "2003.13684", "submitter": "Hai Dong", "authors": "Tooba Aamir, Hai Dong, and Athman Bouguettaya", "title": "Social-Sensor Composition for Tapestry Scenes", "comments": "15 pages. IEEE Transactions on Services Computing", "journal-ref": null, "doi": "10.1109/TSC.2020.2974741", "report-no": null, "categories": "cs.MM cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extensive use of social media platforms and overwhelming amounts of\nimagery data creates unique opportunities for sensing, gathering and sharing\ninformation about events. One of its potential applications is to leverage\ncrowdsourced social media images to create a tapestry scene for scene analysis\nof designated locations and time intervals. The existing attempts however\nignore the temporal-semantic relevance and spatio-temporal evolution of the\nimages and direction-oriented scene reconstruction. We propose a novel\nsocial-sensor cloud (SocSen) service composition approach to form tapestry\nscenes for scene analysis. The novelty lies in utilising images and image\nmeta-information to bypass expensive traditional image processing techniques to\nreconstruct scenes. Metadata, such as geolocation, time and angle of view of an\nimage are modelled as non-functional attributes of a SocSen service. Our major\ncontribution lies on proposing a context and direction-aware spatio-temporal\nclustering and recommendation approach for selecting a set of temporally and\nsemantically similar services to compose the best available SocSen services.\nAnalytical results based on real datasets are presented to demonstrate the\nperformance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 00:07:34 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Aamir", "Tooba", ""], ["Dong", "Hai", ""], ["Bouguettaya", "Athman", ""]]}, {"id": "2003.13894", "submitter": "Juan Banda", "authors": "Ramya Tekumalla and Juan M. Banda", "title": "Social Media Mining Toolkit (SMMT)", "comments": "3 figures, 8 pages double spaced, Under review in Genomics &\n  Informatics journal", "journal-ref": "Genomics & Informatics 2020; 18(2): e16", "doi": "10.5808/GI.2020.18.2.e16", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a dramatic increase in the popularity of utilizing social\nmedia data for research purposes within the biomedical community. In PubMed\nalone, there have been nearly 2,500 publication entries since 2014 that deal\nwith analyzing social media data from Twitter and Reddit. However, the vast\nmajority of those works do not share their code or data for replicating their\nstudies. With minimal exceptions, the few that do, place the burden on the\nresearcher to figure out how to fetch the data, how to best format their data,\nand how to create automatic and manual annotations on the acquired data. In\norder to address this pressing issue, we introduce the Social Media Mining\nToolkit (SMMT), a suite of tools aimed to encapsulate the cumbersome details of\nacquiring, preprocessing, annotating and standardizing social media data. The\npurpose of our toolkit is for researchers to focus on answering research\nquestions, and not the technical aspects of using social media data. By using a\nstandard toolkit, researchers will be able to acquire, use, and release data in\na consistent way that is transparent for everybody using the toolkit, hence,\nsimplifying research reproducibility and accessibility in the social media\ndomain.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 01:16:14 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Tekumalla", "Ramya", ""], ["Banda", "Juan M.", ""]]}, {"id": "2003.13900", "submitter": "Juan Banda", "authors": "Ramya Tekumalla and Juan M. Banda", "title": "A large-scale Twitter dataset for drug safety applications mined from\n  publicly existing resources", "comments": "8 tables, 2 figures, 7 pages, accepted after peer review as a\n  workshop paper in ACM Conference on Health, Inference, and Learning (CHIL)\n  2020 https://www.chilconference.org/agenda/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increase in popularity of deep learning models for natural language\nprocessing (NLP) tasks, in the field of Pharmacovigilance, more specifically\nfor the identification of Adverse Drug Reactions (ADRs), there is an inherent\nneed for large-scale social-media datasets aimed at such tasks. With most\nresearchers allocating large amounts of time to crawl Twitter or buying\nexpensive pre-curated datasets, then manually annotating by humans, these\napproaches do not scale well as more and more data keeps flowing in Twitter. In\nthis work we re-purpose a publicly available archived dataset of more than 9.4\nbillion Tweets with the objective of creating a very large dataset of drug\nusage-related tweets. Using existing manually curated datasets from the\nliterature, we then validate our filtered tweets for relevance using machine\nlearning methods, with the end result of a publicly available dataset of\n1,181,993 million tweets for public use. We provide all code and detailed\nprocedure on how to extract this dataset and the selected tweet ids for\nresearchers to use.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 01:30:24 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Tekumalla", "Ramya", ""], ["Banda", "Juan M.", ""]]}, {"id": "2003.13968", "submitter": "Yuliang Li", "authors": "Aaron Feng, Shuwei Chen, Yuliang Li, Hiroshi Matsuda, Hidekazu Tamaki,\n  Wang-Chiew Tan", "title": "Towards Productionizing Subjective Search Systems", "comments": "In Submission to VLDB 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing e-commerce search engines typically support search only over\nobjective attributes, such as price and locations, leaving the more desirable\nsubjective attributes, such as romantic vibe and worklife balance unsearchable.\nWe found that this is also the case for Recruit Group, which operates a wide\nrange of online booking and search services, including jobs, travel, housing,\nbridal, dining, beauty, and where each service is among the biggest in Japan,\nif not internationally. We present our progress towards productionizing a\nrecent subjective search prototype (OpineDB) developed by Megagon Labs for\nRecruit Group. Several components within OpineDB are enhanced to satisfy\nproduction demands, including adding a BERT language model pre-trained on\nmassive hospitality domain review corpora. We also found that the challenges of\nproductionizing the system are beyond enhancing the components. In particular,\nan important requirement in production-quality systems is to instrument a\nproper way of measuring the search quality, which is extremely tricky when the\nsearch results are subjective. This led to the creation of a high-quality\nbenchmark dataset from scratch, involving over 600 queries by user interviews\nand a collection of more than 120,000 query-entity relevancy labels. Also, we\nfound that the existing search algorithms do not meet the search quality\nstandard required by production systems. Consequently, we enhanced the ranking\nmodel by fine-tuning several search algorithms and combining them under a\nlearning-to-rank framework. The model achieves 5%-10% overall precision\nimprovement and 90+% precision on more than half of the benchmark testing\nqueries making these queries ready for AB-testing. While some enhancements can\nbe immediately applied to other verticals, our experience reveals that\nbenchmarking and fine-tuning ranking algorithms are specific to each domain and\ncannot be avoided.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 06:17:49 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Feng", "Aaron", ""], ["Chen", "Shuwei", ""], ["Li", "Yuliang", ""], ["Matsuda", "Hiroshi", ""], ["Tamaki", "Hidekazu", ""], ["Tan", "Wang-Chiew", ""]]}, {"id": "2003.14159", "submitter": "Bj\\\"orn Friedrich", "authors": "Bj\\\"orn Friedrich, J\\\"urgen Bauer, Andreas Hein", "title": "Detecting impending malnutrition of elderly people in domestic smart\n  home environments", "comments": "Insufficient quality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proper nutrition is very important for the well-being and independence of\nelderly people. A significant loss of body weight or a decrease of the Body\nMass Index respectively is an indicator for malnutrition. A continuous\nmonitoring of the BMI enables doctors and nutritionists to intervene on\nimpending malnutrition. However, continuous monitoring of the BMI by\nprofessionals is not applicable and self-monitoring not reliable. In this\narticle a method for monitoring the trend of the BMI based on ambient sensors\nis introduced. The ambient sensors are used to measure the time a person spends\nfor preparing meals at home. When the trend of the average time for 4 weeks\nchanges, so does the trend of the BMI for those 4 weeks. Both values show a\nvery strong correlation. Thus, the average time for preparing a meal is a\nsuitable indicator for doctors and nutritionists to examine the patient\nfurther, become aware of an impending malnutrition, and intervene at an early\nstage of malnutrition. The method has been tested on a real-world dataset\ncollected during a 10-month field study with 20 participants of an age of about\n85 years.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 12:53:31 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 10:06:01 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Friedrich", "Bj\u00f6rn", ""], ["Bauer", "J\u00fcrgen", ""], ["Hein", "Andreas", ""]]}, {"id": "2003.14257", "submitter": "Artur Sokolovsky", "authors": "A. Sokolovsky, T. Gross, J. Bacardit", "title": "Is it feasible to detect FLOSS version release events from textual\n  messages? A case study on Stack Overflow", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0246464", "report-no": null, "categories": "cs.SE cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic Detection and Tracking (TDT) is a very active research question within\nthe area of text mining, generally applied to news feeds and Twitter datasets,\nwhere topics and events are detected. The notion of \"event\" is broad, but\ntypically it applies to occurrences that can be detected from a single post or\na message. Little attention has been drawn to what we call \"micro-events\",\nwhich, due to their nature, cannot be detected from a single piece of textual\ninformation. The study investigates the feasibility of micro-event detection on\ntextual data using a sample of messages from the Stack Overflow Q&A platform\nand Free/Libre Open Source Software (FLOSS) version releases from Libraries.io\ndataset. We build pipelines for detection of micro-events using three different\nestimators whose parameters are optimized using a grid search approach. We\nconsider two feature spaces: LDA topic modeling with sentiment analysis, and\nhSBM topics with sentiment analysis. The feature spaces are optimized using the\nrecursive feature elimination with cross validation (RFECV) strategy.\n  In our experiments we investigate whether there is a characteristic change in\nthe topics distribution or sentiment features before or after micro-events take\nplace and we thoroughly evaluate the capacity of each variant of our analysis\npipeline to detect micro-events. Additionally, we perform a detailed\nstatistical analysis of the models, including influential cases, variance\ninflation factors, validation of the linearity assumption, pseudo R squared\nmeasures and no-information rate. Finally, in order to study limits of\nmicro-event detection, we design a method for generating micro-event synthetic\ndatasets with similar properties to the real-world data, and use them to\nidentify the micro-event detectability threshold for each of the evaluated\nclassifiers.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 16:55:38 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 10:50:58 GMT"}, {"version": "v3", "created": "Sat, 19 Dec 2020 12:49:35 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Sokolovsky", "A.", ""], ["Gross", "T.", ""], ["Bacardit", "J.", ""]]}, {"id": "2003.14292", "submitter": "Suyu Ge", "authors": "Suyu Ge and Chuhan Wu and Fangzhao Wu and Tao Qi and Yongfeng Huang", "title": "Graph Enhanced Representation Learning for News Recommendation", "comments": null, "journal-ref": null, "doi": "10.1145/3366423.3380050", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the explosion of online news, personalized news recommendation becomes\nincreasingly important for online news platforms to help their users find\ninteresting information. Existing news recommendation methods achieve\npersonalization by building accurate news representations from news content and\nuser representations from their direct interactions with news (e.g., click),\nwhile ignoring the high-order relatedness between users and news. Here we\npropose a news recommendation method which can enhance the representation\nlearning of users and news by modeling their relatedness in a graph setting. In\nour method, users and news are both viewed as nodes in a bipartite graph\nconstructed from historical user click behaviors. For news representations, a\ntransformer architecture is first exploited to build news semantic\nrepresentations. Then we combine it with the information from neighbor news in\nthe graph via a graph attention network. For user representations, we not only\nrepresent users from their historically clicked news, but also attentively\nincorporate the representations of their neighbor users in the graph. Improved\nperformances on a large-scale real-world dataset validate the effectiveness of\nour proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 15:27:31 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Ge", "Suyu", ""], ["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Qi", "Tao", ""], ["Huang", "Yongfeng", ""]]}]