[{"id": "1205.0044", "submitter": "Ankur Moitra", "authors": "Ankur Moitra", "title": "A Singly-Exponential Time Algorithm for Computing Nonnegative Rank", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, we give an algorithm for deciding if the nonnegative rank of a matrix\n$M$ of dimension $m \\times n$ is at most $r$ which runs in time\n$(nm)^{O(r^2)}$. This is the first exact algorithm that runs in time\nsingly-exponential in $r$. This algorithm (and earlier algorithms) are built on\nmethods for finding a solution to a system of polynomial inequalities (if one\nexists). Notably, the best algorithms for this task run in time exponential in\nthe number of variables but polynomial in all of the other parameters (the\nnumber of inequalities and the maximum degree).\n  Hence these algorithms motivate natural algebraic questions whose solution\nhave immediate {\\em algorithmic} implications: How many variables do we need to\nrepresent the decision problem, does $M$ have nonnegative rank at most $r$? A\nnaive formulation uses $nr + mr$ variables and yields an algorithm that is\nexponential in $n$ and $m$ even for constant $r$. (Arora, Ge, Kannan, Moitra,\nSTOC 2012) recently reduced the number of variables to $2r^2 2^r$, and here we\nexponentially reduce the number of variables to $2r^2$ and this yields our main\nalgorithm. In fact, the algorithm that we obtain is nearly-optimal (under the\nExponential Time Hypothesis) since an algorithm that runs in time $(nm)^{o(r)}$\nwould yield a subexponential algorithm for 3-SAT .\n  Our main result is based on establishing a normal form for nonnegative matrix\nfactorization - which in turn allows us to exploit algebraic dependence among a\nlarge collection of linear transformations with variable entries. Additionally,\nwe also demonstrate that nonnegative rank cannot be certified by even a very\nlarge submatrix of $M$, and this property also follows from the intuition\ngained from viewing nonnegative rank through the lens of systems of polynomial\ninequalities.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 22:26:51 GMT"}], "update_date": "2012-05-02", "authors_parsed": [["Moitra", "Ankur", ""]]}, {"id": "1205.0312", "submitter": "Weimao Ke Weimao Ke", "authors": "Weimao Ke", "title": "Least Information Modeling for Information Retrieval", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a Least Information theory (LIT) to quantify meaning of\ninformation in probability distribution changes, from which a new information\nretrieval model was developed. We observed several important characteristics of\nthe proposed theory and derived two quantities in the IR context for document\nrepresentation. Given probability distributions in a collection as prior\nknowledge, LI Binary (LIB) quantifies least information due to the binary\noccurrence of a term in a document whereas LI Frequency (LIF) measures least\ninformation based on the probability of drawing a term from a bag of words.\nThree fusion methods were also developed to combine LIB and LIF quantities for\nterm weighting and document ranking. Experiments on four benchmark TREC\ncollections for ad hoc retrieval showed that LIT-based methods demonstrated\nvery strong performances compared to classic TF*IDF and BM25, especially for\nverbose queries and hard search topics. The least information theory offers a\nnew approach to measuring semantic quantities of information and provides\nvaluable insight into the development of new IR models.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2012 03:02:21 GMT"}], "update_date": "2012-05-03", "authors_parsed": [["Ke", "Weimao", ""]]}, {"id": "1205.0591", "submitter": "Xuanhui Wang", "authors": "Deepak Agarwal, Bee-Chung Chen, Xuanhui Wang", "title": "Multi-Faceted Ranking of News Articles using Post-Read Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized article recommendation is important to improve user engagement\non news sites. Existing work quantifies engagement primarily through click\nrates. We argue that quality of recommendations can be improved by\nincorporating different types of \"post-read\" engagement signals like sharing,\ncommenting, printing and e-mailing article links. More specifically, we propose\na multi-faceted ranking problem for recommending news articles where each facet\ncorresponds to a ranking problem to maximize actions of a post-read action\ntype. The key technical challenge is to estimate the rates of post-read action\ntypes by mitigating the impact of enormous data sparsity, we do so through\nseveral variations of factor models. To exploit correlations among post-read\naction types we also introduce a novel variant called locally augmented tensor\n(LAT) model. Through data obtained from a major news site in the US, we show\nthat factor models significantly outperform a few baseline IR models and the\nLAT model significantly outperforms several other variations of factor models.\nOur findings show that it is possible to incorporate post-read signals that are\ncommonly available on online news sites to improve quality of recommendations.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2012 00:59:39 GMT"}], "update_date": "2012-05-04", "authors_parsed": [["Agarwal", "Deepak", ""], ["Chen", "Bee-Chung", ""], ["Wang", "Xuanhui", ""]]}, {"id": "1205.0917", "submitter": "Omri Mohamed Nazih", "authors": "Radhouane Boughamoura, Lobna Hlaoua and Mohamed Nazih Omri", "title": "VIQI: A New Approach for Visual Interpretation of Deep Web Query\n  Interfaces", "comments": "8th NCM: 2012 International Conference on Networked Computing and\n  Advanced Information Management", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Web databases contain more than 90% of pertinent information of the Web.\nDespite their importance, users don't profit of this treasury. Many deep web\nservices are offering competitive services in term of prices, quality of\nservice, and facilities. As the number of services is growing rapidly, users\nhave difficulty to ask many web services in the same time. In this paper, we\nimagine a system where users have the possibility to formulate one query using\none query interface and then the system translates query to the rest of query\ninterfaces. However, interfaces are created by designers in order to be\ninterpreted visually by users, machines can not interpret query from a given\ninterface. We propose a new approach which emulates capacity of interpretation\nof users and extracts query from deep web query interfaces. Our approach has\nproved good performances on two standard datasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2012 11:01:42 GMT"}], "update_date": "2012-05-07", "authors_parsed": [["Boughamoura", "Radhouane", ""], ["Hlaoua", "Lobna", ""], ["Omri", "Mohamed Nazih", ""]]}, {"id": "1205.0919", "submitter": "Omri Mohamed Nazih", "authors": "Radhouane Boughammoura Lobna Hlaoua and Mohamed Nazih Omri", "title": "ViQIE: A New Approach for Visual Query Interpretation and Extraction", "comments": "ICITES 2012 - 2nd International Conference on Information Technology\n  and e-Services", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web services are accessed via query interfaces which hide databases\ncontaining thousands of relevant information. User's side, distant database is\na black box which accepts query and returns results, there is no way to access\ndatabase schema which reflect data and query meanings. Hence, web services are\nvery autonomous. Users view this autonomy as a major drawback because they need\noften to combine query capabilities of many web services at the same time. In\nthis work, we will present a new approach which allows users to benefit of\nquery capabilities of many web services while respecting autonomy of each\nservice. This solution is a new contribution in Information Retrieval research\naxe and has proven good performances on two standard datasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2012 11:08:31 GMT"}], "update_date": "2012-05-07", "authors_parsed": [["Hlaoua", "Radhouane Boughammoura Lobna", ""], ["Omri", "Mohamed Nazih", ""]]}, {"id": "1205.1143", "submitter": "Onur Kucuktunc", "authors": "Onur K\\\"u\\c{c}\\\"uktun\\c{c}, Erik Saule, Kamer Kaya, \\\"Umit V.\n  \\c{C}ataly\\\"urek", "title": "Recommendation on Academic Networks using Direction Aware Citation\n  Analysis", "comments": "10 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature search has always been an important part of an academic\nresearch. It greatly helps to improve the quality of the research process and\noutput, and increase the efficiency of the researchers in terms of their novel\ncontribution to science. As the number of published papers increases every\nyear, a manual search becomes more exhaustive even with the help of today's\nsearch engines since they are not specialized for this task. In academics, two\nrelevant papers do not always have to share keywords, cite one another, or even\nbe in the same field. Although a well-known paper is usually an easy pray in\nsuch a hunt, relevant papers using a different terminology, especially recent\nones, are not obvious to the eye.\n  In this work, we propose paper recommendation algorithms by using the\ncitation information among papers. The proposed algorithms are direction aware\nin the sense that they can be tuned to find either recent or traditional\npapers. The algorithms require a set of papers as input and recommend a set of\nrelated ones. If the user wants to give negative or positive feedback on the\nsuggested paper set, the recommendation is refined. The search process can be\neasily guided in that sense by relevance feedback. We show that this slight\nguidance helps the user to reach a desired paper in a more efficient way. We\nadapt our models and algorithms also for the venue and reviewer recommendation\ntasks. Accuracy of the models and algorithms is thoroughly evaluated by\ncomparison with multiple baselines and algorithms from the literature in terms\nof several objectives specific to citation, venue, and reviewer recommendation\ntasks. All of these algorithms are implemented within a publicly available\nweb-service framework (http://theadvisor.osu.edu/) which currently uses the\ndata from DBLP and CiteSeer to construct the proposed citation graph.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2012 14:48:54 GMT"}], "update_date": "2012-05-08", "authors_parsed": [["K\u00fc\u00e7\u00fcktun\u00e7", "Onur", ""], ["Saule", "Erik", ""], ["Kaya", "Kamer", ""], ["\u00c7ataly\u00fcrek", "\u00dcmit V.", ""]]}, {"id": "1205.1505", "submitter": "Lucas Lacasa", "authors": "Lucas Lacasa, Jacopo Tagliabue, and Andrew Berdahl", "title": "Crossover phenomenon in the performance of an Internet search engine", "comments": "Working paper, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore the ability of the Google search engine to find\nresults for random N-letter strings. These random strings, dense over the set\nof possible N-letter words, address the existence of typos, acronyms, and other\nwords without semantic meaning. Interestingly, we find that the probability of\nfinding such strings sharply drops from one to zero at Nc = 6. The behavior of\nsuch order parameter suggests the presence of a transition-like phenomenon in\nthe geometry of the search space. Furthermore, we define a susceptibility-like\nparameter which reaches a maximum in the neighborhood, suggesting the presence\nof criticality. We finally speculate on the possible connections to Ramsey\ntheory.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2012 09:39:55 GMT"}], "update_date": "2012-05-09", "authors_parsed": [["Lacasa", "Lucas", ""], ["Tagliabue", "Jacopo", ""], ["Berdahl", "Andrew", ""]]}, {"id": "1205.1602", "submitter": "Izzat Alsmadi", "authors": "Abdulrahman Al Molijy, Ismail Hmeidi, and Izzat Alsmadi", "title": "Indexing of Arabic documents automatically based on lexical analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuous information explosion through the Internet and all information\nsources makes it necessary to perform all information processing activities\nautomatically in quick and reliable manners. In this paper, we proposed and\nimplemented a method to automatically create and Index for books written in\nArabic language. The process depends largely on text summarization and\nabstraction processes to collect main topics and statements in the book. The\nprocess is developed in terms of accuracy and performance and results showed\nthat this process can effectively replace the effort of manually indexing books\nand document, a process that can be very useful in all information processing\nand retrieval applications.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2012 06:52:15 GMT"}], "update_date": "2012-05-09", "authors_parsed": [["Molijy", "Abdulrahman Al", ""], ["Hmeidi", "Ismail", ""], ["Alsmadi", "Izzat", ""]]}, {"id": "1205.1638", "submitter": "Aji S", "authors": "Aji S, Ramachandra Kaimal", "title": "Document summarization using positive pointwise mutual information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The degree of success in document summarization processes depends on the\nperformance of the method used in identifying significant sentences in the\ndocuments. The collection of unique words characterizes the major signature of\nthe document, and forms the basis for Term-Sentence-Matrix (TSM). The Positive\nPointwise Mutual Information, which works well for measuring semantic\nsimilarity in the Term-Sentence-Matrix, is used in our method to assign weights\nfor each entry in the Term-Sentence-Matrix. The Sentence-Rank-Matrix generated\nfrom this weighted TSM, is then used to extract a summary from the document.\nOur experiments show that such a method would outperform most of the existing\nmethods in producing summaries from large documents.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2012 09:19:10 GMT"}], "update_date": "2012-05-09", "authors_parsed": [["S", "Aji", ""], ["Kaimal", "Ramachandra", ""]]}, {"id": "1205.1639", "submitter": "Sajilal Divakaran", "authors": "Sajilal Divakaran", "title": "Spectral Analysis of Projection Histogram for Enhancing Close matching\n  character Recognition in Malayalam", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success rates of Optical Character Recognition (OCR) systems for printed\nMalayalam documents is quite impressive with the state of the art accuracy\nlevels in the range of 85-95% for various. However for real applications,\nfurther enhancement of this accuracy levels are required. One of the bottle\nnecks in further enhancement of the accuracy is identified as close-matching\ncharacters. In this paper, we delineate the close matching characters in\nMalayalam and report the development of a specialised classifier for these\nclose-matching characters. The output of a state of the art of OCR is taken and\ncharacters falling into the close-matching character set is further fed into\nthis specialised classifier for enhancing the accuracy. The classifier is based\non support vector machine algorithm and uses feature vectors derived out of\nspectral coefficients of projection histogram signals of close-matching\ncharacters.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2012 09:25:14 GMT"}], "update_date": "2012-05-09", "authors_parsed": [["Divakaran", "Sajilal", ""]]}, {"id": "1205.1779", "submitter": "Ricardo Pires", "authors": "Ricardo Pires", "title": "A Common Evaluation Setting for Just.Ask, Open Ephyra and Aranea QA\n  systems", "comments": "Technical Report elaborated by Ricardo Pires for the course Natural\n  Language Processing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering (QA) is not a new research field in Natural Language\nProcessing (NLP). However in recent years, QA has been a subject of growing\nstudy. Nowadays, most of the QA systems have a similar pipelined architecture\nand each system use a set of unique techniques to accomplish its state of the\nart results. However, many things are not clear in the QA processing. It is not\nclear the extend of the impact of tasks performed in earlier stages in\nfollowing stages of the pipelining process. It is not clear, if techniques used\nin a QA system can be used in another QA system to improve its results. And\nfinally, it is not clear in what setting should be these systems tested in\norder to properly analyze their results.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2012 19:19:32 GMT"}], "update_date": "2012-05-09", "authors_parsed": [["Pires", "Ricardo", ""]]}, {"id": "1205.2467", "submitter": "Mark Thamm Mark Thamm", "authors": "Peter Mutschke and Mark Thamm", "title": "Linking Social Networking Sites to Scholarly Information Portals by\n  ScholarLib", "comments": "5 pages, ACM Web Science 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Social Networks usually provide no or limited way to access scholarly\ninformation provided by Digital Libraries (DLs) in order to share and discuss\nscholarly content with other online community members. The paper addresses the\npotentials of Social Networking sites (SNSs) for science and proposes initial\nuse cases as well as a basic bi-directional model called ScholarLib for linking\nSNSs to scholarly DLs. The major aim of ScholarLib is to make scholarly\ninformation provided by DLs accessible at SNSs, and vice versa, to enhance\nretrieval quality at DL side by social information provided by SNSs.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2012 09:41:32 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Mutschke", "Peter", ""], ["Thamm", "Mark", ""]]}, {"id": "1205.2611", "submitter": "Tran The Truyen", "authors": "Tran The Truyen, Dinh Q. Phung, Svetha Venkatesh", "title": "Ordinal Boltzmann Machines for Collaborative Filtering", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-548-556", "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is an effective recommendation technique wherein the\npreference of an individual can potentially be predicted based on preferences\nof other members. Early algorithms often relied on the strong locality in the\npreference data, that is, it is enough to predict preference of a user on a\nparticular item based on a small subset of other users with similar tastes or\nof other items with similar properties. More recently, dimensionality reduction\ntechniques have proved to be equally competitive, and these are based on the\nco-occurrence patterns rather than locality. This paper explores and extends a\nprobabilistic model known as Boltzmann Machine for collaborative filtering\ntasks. It seamlessly integrates both the similarity and co-occurrence in a\nprincipled manner. In particular, we study parameterisation options to deal\nwith the ordinal nature of the preferences, and propose a joint modelling of\nboth the user-based and item-based processes. Experiments on moderate and\nlarge-scale movie recommendation show that our framework rivals existing\nwell-known methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 18:35:35 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Truyen", "Tran The", ""], ["Phung", "Dinh Q.", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1205.2618", "submitter": "Steffen Rendle", "authors": "Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars\n  Schmidt-Thieme", "title": "BPR: Bayesian Personalized Ranking from Implicit Feedback", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-452-461", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item recommendation is the task of predicting a personalized ranking on a set\nof items (e.g. websites, movies, products). In this paper, we investigate the\nmost common scenario with implicit feedback (e.g. clicks, purchases). There are\nmany methods for item recommendation from implicit feedback like matrix\nfactorization (MF) or adaptive knearest-neighbor (kNN). Even though these\nmethods are designed for the item prediction task of personalized ranking, none\nof them is directly optimized for ranking. In this paper we present a generic\noptimization criterion BPR-Opt for personalized ranking that is the maximum\nposterior estimator derived from a Bayesian analysis of the problem. We also\nprovide a generic learning algorithm for optimizing models with respect to\nBPR-Opt. The learning method is based on stochastic gradient descent with\nbootstrap sampling. We show how to apply our method to two state-of-the-art\nrecommender models: matrix factorization and adaptive kNN. Our experiments\nindicate that for the task of personalized ranking our optimization method\noutperforms the standard learning techniques for MF and kNN. The results show\nthe importance of optimizing models for the right criterion.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 18:25:09 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Rendle", "Steffen", ""], ["Freudenthaler", "Christoph", ""], ["Gantner", "Zeno", ""], ["Schmidt-Thieme", "Lars", ""]]}, {"id": "1205.2657", "submitter": "Jordan Boyd-Graber", "authors": "Jordan Boyd-Graber, David Blei", "title": "Multilingual Topic Models for Unaligned Text", "comments": "Appears in Proceedings of the Twenty-Fifth Conference on Uncertainty\n  in Artificial Intelligence (UAI2009)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2009-PG-75-82", "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the multilingual topic model for unaligned text (MuTo), a\nprobabilistic model of text that is designed to analyze corpora composed of\ndocuments in two languages. From these documents, MuTo uses stochastic EM to\nsimultaneously discover both a matching between the languages and multilingual\nlatent topics. We demonstrate that MuTo is able to find shared topics on\nreal-world multilingual corpora, successfully pairing related documents across\nlanguages. MuTo provides a new framework for creating multilingual topic models\nwithout needing carefully curated parallel corpora and allows applications\nbuilt using the topic model formalism to be applied to a much wider class of\ncorpora.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 14:53:11 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Boyd-Graber", "Jordan", ""], ["Blei", "David", ""]]}, {"id": "1205.2822", "submitter": "Zi-Ke Zhang Dr.", "authors": "Tian Qiu, Zi-Ke Zhang, Guang Chen", "title": "Promotional effect on cold start problem and diversity in a data\n  characteristic based recommendation method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pure methods generally perform excellently in either recommendation accuracy\nor diversity, whereas hybrid methods generally outperform pure cases in both\nrecommendation accuracy and diversity, but encounter the dilemma of optimal\nhybridization parameter selection for different recommendation focuses. In this\narticle, based on a user-item bipartite network, we propose a data\ncharacteristic based algorithm, by relating the hybridization parameter to the\ndata characteristic. Different from previous hybrid methods, the present\nalgorithm adaptively assign the optimal parameter specifically for each\nindividual items according to the correlation between the algorithm and the\nitem degrees. Compared with a highly accurate pure method, and a hybrid method\nwhich is outstanding in both the recommendation accuracy and the diversity, our\nmethod shows a remarkably promotional effect on the long-standing challenging\nproblem of the cold start, as well as the recommendation diversity, while\nsimultaneously keeps a high overall recommendation accuracy. Even compared with\nan improved hybrid method which is highly efficient on the cold start problem,\nthe proposed method not only further improves the recommendation accuracy of\nthe cold items, but also enhances the recommendation diversity. Our work might\nprovide a promising way to better solving the personal recommendation from the\nperspective of relating algorithms with dataset properties.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2012 02:47:08 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2012 15:43:06 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2012 20:17:06 GMT"}], "update_date": "2012-07-26", "authors_parsed": [["Qiu", "Tian", ""], ["Zhang", "Zi-Ke", ""], ["Chen", "Guang", ""]]}, {"id": "1205.2891", "submitter": "Abdul Nabi shaik", "authors": "Sk. AbdulNabi, P. Premchand", "title": "Effective performance of information retrieval on web by using web\n  crawling", "comments": "International Journal of Web & Semantic Technology (IJWesT) Vol.3,\n  No.2, April 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  World Wide Web consists of more than 50 billion pages online. It is highly\ndynamic i.e. the web continuously introduces new capabilities and attracts many\npeople. Due to this explosion in size, the effective information retrieval\nsystem or search engine can be used to access the information. In this paper we\nhave proposed the EPOW (Effective Performance of WebCrawler) architecture. It\nis a software agent whose main objective is to minimize the overload of a user\nlocating needed information. We have designed the web crawler by considering\nthe parallelization policy. Since our EPOW crawler has a highly optimized\nsystem it can download a large number of pages per second while being robust\nagainst crashes. We have also proposed to use the data structure concepts for\nimplementation of scheduler & circular Queue to improve the performance of our\nweb crawler.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2012 17:56:52 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["AbdulNabi", "Sk.", ""], ["Premchand", "P.", ""]]}, {"id": "1205.2930", "submitter": "Deng Cai", "authors": "Yue Lin and Deng Cai and Cheng Li", "title": "Density Sensitive Hashing", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbors search is a fundamental problem in various research fields\nlike machine learning, data mining and pattern recognition. Recently,\nhashing-based approaches, e.g., Locality Sensitive Hashing (LSH), are proved to\nbe effective for scalable high dimensional nearest neighbors search. Many\nhashing algorithms found their theoretic root in random projection. Since these\nalgorithms generate the hash tables (projections) randomly, a large number of\nhash tables (i.e., long codewords) are required in order to achieve both high\nprecision and recall. To address this limitation, we propose a novel hashing\nalgorithm called {\\em Density Sensitive Hashing} (DSH) in this paper. DSH can\nbe regarded as an extension of LSH. By exploring the geometric structure of the\ndata, DSH avoids the purely random projections selection and uses those\nprojective functions which best agree with the distribution of the data.\nExtensive experimental results on real-world data sets have shown that the\nproposed method achieves better performance compared to the state-of-the-art\nhashing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2012 02:27:52 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Lin", "Yue", ""], ["Cai", "Deng", ""], ["Li", "Cheng", ""]]}, {"id": "1205.2958", "submitter": "Ping Li", "authors": "Ping Li and Anshumali Shrivastava and Arnd Christian Konig", "title": "b-Bit Minwise Hashing in Practice: Large-Scale Batch and Online Learning\n  and Using GPUs for Fast Preprocessing with Simple Hash Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study several critical issues which must be tackled before\none can apply b-bit minwise hashing to the volumes of data often used\nindustrial applications, especially in the context of search.\n  1. (b-bit) Minwise hashing requires an expensive preprocessing step that\ncomputes k (e.g., 500) minimal values after applying the corresponding\npermutations for each data vector. We developed a parallelization scheme using\nGPUs and observed that the preprocessing time can be reduced by a factor of\n20-80 and becomes substantially smaller than the data loading time.\n  2. One major advantage of b-bit minwise hashing is that it can substantially\nreduce the amount of memory required for batch learning. However, as online\nalgorithms become increasingly popular for large-scale learning in the context\nof search, it is not clear if b-bit minwise yields significant improvements for\nthem. This paper demonstrates that $b$-bit minwise hashing provides an\neffective data size/dimension reduction scheme and hence it can dramatically\nreduce the data loading time for each epoch of the online training process.\nThis is significant because online learning often requires many (e.g., 10 to\n100) epochs to reach a sufficient accuracy.\n  3. Another critical issue is that for very large data sets it becomes\nimpossible to store a (fully) random permutation matrix, due to its space\nrequirements. Our paper is the first study to demonstrate that $b$-bit minwise\nhashing implemented using simple hash functions, e.g., the 2-universal (2U) and\n4-universal (4U) hash families, can produce very similar learning results as\nusing fully random permutations. Experiments on datasets of up to 200GB are\npresented.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2012 08:28:10 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Li", "Ping", ""], ["Shrivastava", "Anshumali", ""], ["Konig", "Arnd Christian", ""]]}, {"id": "1205.3031", "submitter": "Dmitry Lande", "authors": "D. V. Lande, Ya. A. Kalinovskiy, Yu. E. Boyarinova", "title": "The model of information retrieval based on the theory of hypercomplex\n  numerical systems", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper provided a description of a new model of information retrieval,\nwhich is an extension of vector-space model and is based on the principles of\nthe theory of hypercomplex numerical systems. The model allows to some extent\nrealize the idea of fuzzy search and allows you to apply in practice the model\nof information retrieval practical developments in the field of hypercomplex\nnumerical systems.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2012 14:01:56 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Lande", "D. V.", ""], ["Kalinovskiy", "Ya. A.", ""], ["Boyarinova", "Yu. E.", ""]]}, {"id": "1205.3193", "submitter": "Joonseok Lee", "authors": "Joonseok Lee, Mingxuan Sun, Guy Lebanon", "title": "A Comparative Study of Collaborative Filtering Algorithms", "comments": "27 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is a rapidly advancing research area. Every year\nseveral new techniques are proposed and yet it is not clear which of the\ntechniques work best and under what conditions. In this paper we conduct a\nstudy comparing several collaborative filtering techniques -- both classic and\nrecent state-of-the-art -- in a variety of experimental contexts. Specifically,\nwe report conclusions controlling for number of items, number of users,\nsparsity level, performance criteria, and computational complexity. Our\nconclusions identify what algorithms work well and in what conditions, and\ncontribute to both industrial deployment collaborative filtering algorithms and\nto the research community.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2012 21:08:05 GMT"}], "update_date": "2012-05-16", "authors_parsed": [["Lee", "Joonseok", ""], ["Sun", "Mingxuan", ""], ["Lebanon", "Guy", ""]]}, {"id": "1205.4138", "submitter": "Daniel Hienert", "authors": "Daniel Hienert and Francesco Luciano", "title": "Extraction of Historical Events from Wikipedia", "comments": "To be published in Proceedings of Knowledge Discovery and Data Mining\n  Meets Linked Open Data (Know@LOD) Workshop at ESWC 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DBpedia project extracts structured information from Wikipedia and makes\nit available on the web. Information is gathered mainly with the help of\ninfoboxes that contain structured information of the Wikipedia article. A lot\nof information is only contained in the article body and is not yet included in\nDBpedia. In this paper we focus on the extraction of historical events from\nWikipedia articles that are available for about 2,500 years for different\nlanguages. We have extracted about 121,000 events with more than 325,000 links\nto DBpedia entities and provide access to this data via a Web API, SPARQL\nendpoint, Linked Data Interface and in a timeline application.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2012 11:25:31 GMT"}], "update_date": "2012-05-21", "authors_parsed": [["Hienert", "Daniel", ""], ["Luciano", "Francesco", ""]]}, {"id": "1205.4213", "submitter": "Pannagadatta Shivaswamy", "authors": "Pannaga Shivaswamy and Thorsten Joachims", "title": "Online Structured Prediction via Coactive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Coactive Learning as a model of interaction between a learning\nsystem and a human user, where both have the common goal of providing results\nof maximum utility to the user. At each step, the system (e.g. search engine)\nreceives a context (e.g. query) and predicts an object (e.g. ranking). The user\nresponds by correcting the system if necessary, providing a slightly improved\n-- but not necessarily optimal -- object as feedback. We argue that such\nfeedback can often be inferred from observable user behavior, for example, from\nclicks in web-search. Evaluating predictions by their cardinal utility to the\nuser, we propose efficient learning algorithms that have ${\\cal\nO}(\\frac{1}{\\sqrt{T}})$ average regret, even though the learning algorithm\nnever observes cardinal utility values as in conventional online learning. We\ndemonstrate the applicability of our model and learning algorithms on a movie\nrecommendation task, as well as ranking for web-search.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2012 18:19:13 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2012 16:25:02 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Shivaswamy", "Pannaga", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1205.5569", "submitter": "Jagadeesh Gorla", "authors": "Jagadeesh Gorla, Stephen Robertson, Jun Wang and Tamas Jambor", "title": "A Theory of Information Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a theory for information matching. It is motivated\nby the observation that retrieval is about the relevance matching between two\nsets of properties (features), namely, the information need representation and\ninformation item representation. However, many probabilistic retrieval models\nrely on fixing one representation and optimizing the other (e.g. fixing the\nsingle information need and tuning the document) but not both. Therefore, it is\ndifficult to use the available related information on both the document and the\nquery at the same time in calculating the probability of relevance. In this\npaper, we address the problem by hypothesizing the relevance as a logical\nrelationship between the two sets of properties; the relationship is defined on\ntwo separate mappings between these properties. By using the hypothesis we\ndevelop a unified probabilistic relevance model which is capable of using all\nthe available information. We validate the proposed theory by formulating and\ndeveloping probabilistic relevance ranking functions for both ad-hoc text\nretrieval and collaborative filtering. Our derivation in text retrieval\nillustrates the use of the theory in the situation where no relevance\ninformation is available. In collaborative filtering, we show that the\nresulting recommender model unifies the user and item information into a\nrelevance ranking function without applying any dimensionality reduction\ntechniques or computing explicit similarity between two different users (or\nitems), in contrast to the state-of-the-art recommender models.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2012 20:56:16 GMT"}, {"version": "v2", "created": "Mon, 28 May 2012 01:02:35 GMT"}, {"version": "v3", "created": "Fri, 1 Jun 2012 18:59:09 GMT"}], "update_date": "2012-06-04", "authors_parsed": [["Gorla", "Jagadeesh", ""], ["Robertson", "Stephen", ""], ["Wang", "Jun", ""], ["Jambor", "Tamas", ""]]}, {"id": "1205.5632", "submitter": "Rom\\`an R. Zapatrin", "authors": "Roman Zapatrin", "title": "Quantum contextuality in classical information retrieval", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document ranking based on probabilistic evaluations of relevance is known to\nexhibit non-classical correlations, which may be explained by admitting a\ncomplex structure of the event space, namely, by assuming the events to emerge\nfrom multiple sample spaces. The structure of event space formed by overlapping\nsample spaces is known in quantum mechanics, they may exhibit some\ncounter-intuitive features, called quantum contextuality. In this Note I\nobserve that from the structural point of view quantum contextuality looks\nsimilar to personalization of information retrieval scenarios. Along these\nlines, Knowledge Revision is treated as operationalistic measurement and a way\nto quantify the rate of personalization of Information Retrieval scenarios is\nsuggested.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2012 08:43:06 GMT"}], "update_date": "2012-05-28", "authors_parsed": [["Zapatrin", "Roman", ""]]}, {"id": "1205.5651", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a, \\'Alvaro Corral, Mari\\'an Bogu\\~n\\'a, Mart\\'in Haro,\n  Josep Lluis Arcos", "title": "Measuring the evolution of contemporary western popular music", "comments": "Supplementary materials not included. Please see the journal\n  reference or contact the authors", "journal-ref": "Scientific Reports 2, 521 (2012)", "doi": "10.1038/srep00521", "report-no": null, "categories": "cs.SD cs.IR cs.MM physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular music is a key cultural expression that has captured listeners'\nattention for ages. Many of the structural regularities underlying musical\ndiscourse are yet to be discovered and, accordingly, their historical evolution\nremains formally unknown. Here we unveil a number of patterns and metrics\ncharacterizing the generic usage of primary musical facets such as pitch,\ntimbre, and loudness in contemporary western popular music. Many of these\npatterns and metrics have been consistently stable for a period of more than\nfifty years, thus pointing towards a great degree of conventionalism.\nNonetheless, we prove important changes or trends related to the restriction of\npitch transitions, the homogenization of the timbral palette, and the growing\nloudness levels. This suggests that our perception of the new would be rooted\non these changing characteristics. Hence, an old tune could perfectly sound\nnovel and fashionable, provided that it consisted of common harmonic\nprogressions, changed the instrumentation, and increased the average loudness.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2012 09:54:24 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Corral", "\u00c1lvaro", ""], ["Bogu\u00f1\u00e1", "Mari\u00e1n", ""], ["Haro", "Mart\u00edn", ""], ["Arcos", "Josep Lluis", ""]]}, {"id": "1205.5923", "submitter": "Noreddine Gherabi", "authors": "Noreddine Gherabi and Mohamed Bahaj", "title": "Conversion database of the shapes into XML data for shape matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to the matching of 2D shapes using XML language and\ndynamic programming. Given a 2D shape, we extract its contour and which is\nrepresented by set of points. The contour is divided into curves using corner\ndetection. After, each curve is described by local and global features; these\nfeatures are coded in a string of symbols and stored in a XML file. Finally,\nusing the dynamic programming, we find the optimal alignment between sequences\nof symbols. Results are presented and compared with existing methods using\nMATLAB for KIMIA-25 database and MPEG7 databases.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2012 22:45:50 GMT"}], "update_date": "2012-05-29", "authors_parsed": [["Gherabi", "Noreddine", ""], ["Bahaj", "Mohamed", ""]]}, {"id": "1205.6343", "submitter": "Klaus Frahm", "authors": "K. M. Frahm, A. D. Chepelianskii and D. L. Shepelyansky", "title": "PageRank of integers", "comments": "Research at http://www.quantware.ups-tlse.fr/, 22 pages, 14 figures", "journal-ref": "J. Phys. A: Math. Theor. 45, 405101(2012)", "doi": "10.1088/1751-8113/45/40/405101", "report-no": null, "categories": "cs.IR cond-mat.stat-mech math.NT nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build up a directed network tracing links from a given integer to its\ndivisors and analyze the properties of the Google matrix of this network. The\nPageRank vector of this matrix is computed numerically and it is shown that its\nprobability is inversely proportional to the PageRank index thus being similar\nto the Zipf law and the dependence established for the World Wide Web. The\nspectrum of the Google matrix of integers is characterized by a large gap and a\nrelatively small number of nonzero eigenvalues. A simple semi-analytical\nexpression for the PageRank of integers is derived that allows to find this\nvector for matrices of billion size. This network provides a new PageRank order\nof integers.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2012 12:18:44 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Frahm", "K. M.", ""], ["Chepelianskii", "A. D.", ""], ["Shepelyansky", "D. L.", ""]]}, {"id": "1205.6396", "submitter": "Murphy Choy", "authors": "Murphy Choy", "title": "Effective Listings of Function Stop words for Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many words in documents recur very frequently but are essentially meaningless\nas they are used to join words together in a sentence. It is commonly\nunderstood that stop words do not contribute to the context or content of\ntextual documents. Due to their high frequency of occurrence, their presence in\ntext mining presents an obstacle to the understanding of the content in the\ndocuments. To eliminate the bias effects, most text mining software or\napproaches make use of stop words list to identify and remove those words.\nHowever, the development of such top words list is difficult and inconsistent\nbetween textual sources. This problem is further aggravated by sources such as\nTwitter which are highly repetitive or similar in nature. In this paper, we\nwill be examining the original work using term frequency, inverse document\nfrequency and term adjacency for developing a stop words list for the Twitter\ndata source. We propose a new technique using combinatorial values as an\nalternative measure to effectively list out stop words.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2012 15:37:46 GMT"}], "update_date": "2012-05-30", "authors_parsed": [["Choy", "Murphy", ""]]}, {"id": "1205.6855", "submitter": "Jimmy Lin", "authors": "Jimmy Lin and Gilad Mishne", "title": "A Study of \"Churn\" in Tweets and Real-Time Search Queries (Extended\n  Version)", "comments": "This is an extended version of a similarly-titled paper at the 6th\n  International AAAI Conference on Weblogs and Social Media (ICWSM 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The real-time nature of Twitter means that term distributions in tweets and\nin search queries change rapidly: the most frequent terms in one hour may look\nvery different from those in the next. Informally, we call this phenomenon\n\"churn\". Our interest in analyzing churn stems from the perspective of\nreal-time search. Nearly all ranking functions, machine-learned or otherwise,\ndepend on term statistics such as term frequency, document frequency, as well\nas query frequencies. In the real-time context, how do we compute these\nstatistics, considering that the underlying distributions change rapidly? In\nthis paper, we present an analysis of tweet and query churn on Twitter, as a\nfirst step to answering this question. Analyses reveal interesting insights on\nthe temporal dynamics of term distributions on Twitter and hold implications\nfor the design of search systems.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 23:39:28 GMT"}], "update_date": "2012-06-01", "authors_parsed": [["Lin", "Jimmy", ""], ["Mishne", "Gilad", ""]]}]