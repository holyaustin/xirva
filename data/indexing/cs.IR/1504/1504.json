[{"id": "1504.00191", "submitter": "Sanjay Sahay", "authors": "Rajendra Kumar Roul, Shubham Rohan Asthana, Sanjay Kumar Sahay", "title": "Automated Document Indexing via Intelligent Hierarchical Clustering: A\n  Novel Approach", "comments": "6 Pages, 3 Figures. IEEE Xplore, ICHPCA-2014", "journal-ref": null, "doi": "10.1109/ICHPCA.2014.7045347", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rising quantity of textual data available in electronic format, the\nneed to organize it become a highly challenging task. In the present paper, we\nexplore a document organization framework that exploits an intelligent\nhierarchical clustering algorithm to generate an index over a set of documents.\nThe framework has been designed to be scalable and accurate even with large\ncorpora. The advantage of the proposed algorithm lies in the need for minimal\ninputs, with much of the hierarchy attributes being decided in an automated\nmanner using statistical methods. The use of topic modeling in a pre-processing\nstage ensures robustness to a range of variations in the input data. For\nexperimental work 20-Newsgroups dataset has been used. The F- measure of the\nproposed approach has been compared with the traditional K-Means and K-Medoids\nclustering algorithms. Test results demonstrate the applicability, efficiency\nand effectiveness of our proposed approach. After extensive experimentation, we\nconclude that the framework shows promise for further research and specialized\ncommercial applications.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 12:08:36 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Roul", "Rajendra Kumar", ""], ["Asthana", "Shubham Rohan", ""], ["Sahay", "Sanjay Kumar", ""]]}, {"id": "1504.00305", "submitter": "Vladimir Ivanov", "authors": "V. K. Ivanov, B. V. Palyukh", "title": "Study the effectiveness of genetic algorithm for documentary subject\n  search", "comments": "7 pages, in Russian", "journal-ref": "OSTIS-2015 1 (2015) 471-476", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents results of experimental studies the effectiveness of\nthe genetic algorithm that was applied to effective queries creation and\nrelevant document selection. Studies were carried out to the comparative\nanalysis of the semantic relevance and quality ranking of the documents found\non the Internet in various ways. Analysis of the results shows that the\ngreatest effect of presented technology is achieved by finding new documents\nfor skilled users in the initial stages of the study of the topic.\nAdditionally, the number of unique and relevant results is significantly\nincreased.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 17:24:52 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Ivanov", "V. K.", ""], ["Palyukh", "B. V.", ""]]}, {"id": "1504.00416", "submitter": "Junyu Xuan", "authors": "Junyu Xuan, Jie Lu, Xiangfeng Luo, Guangquan Zhang", "title": "Nonnegative Multi-level Network Factorization for Latent Factor Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) aims to factorize a matrix into two\noptimized nonnegative matrices and has been widely used for unsupervised\nlearning tasks such as product recommendation based on a rating matrix.\nHowever, although networks between nodes with the same nature exist, standard\nNMF overlooks them, e.g., the social network between users. This problem leads\nto comparatively low recommendation accuracy because these networks are also\nreflections of the nature of the nodes, such as the preferences of users in a\nsocial network. Also, social networks, as complex networks, have many different\nstructures. Each structure is a composition of links between nodes and reflects\nthe nature of nodes, so retaining the different network structures will lead to\ndifferences in recommendation performance. To investigate the impact of these\nnetwork structures on the factorization, this paper proposes four multi-level\nnetwork factorization algorithms based on the standard NMF, which integrates\nthe vertical network (e.g., rating matrix) with the structures of horizontal\nnetwork (e.g., user social network). These algorithms are carefully designed\nwith corresponding convergence proofs to retain four desired network\nstructures. Experiments on synthetic data show that the proposed algorithms are\nable to preserve the desired network structures as designed. Experiments on\nreal-world data show that considering the horizontal networks improves the\naccuracy of document clustering and recommendation with standard NMF, and\nvarious structures show their differences in performance on these two tasks.\nThese results can be directly used in document clustering and recommendation\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 23:46:07 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Xuan", "Junyu", ""], ["Lu", "Jie", ""], ["Luo", "Xiangfeng", ""], ["Zhang", "Guangquan", ""]]}, {"id": "1504.00657", "submitter": "Geoffrey Fairchild", "authors": "Geoffrey Fairchild (1 and 3), Lalindra De Silva (2), Sara Y. Del Valle\n  (1), Alberto M. Segre (3) ((1) Los Alamos National Laboratory, Los Alamos,\n  NM, USA, (2) The University of Utah, Salt Lake City, UT, USA, (3) The\n  University of Iowa, Iowa City, IA, USA)", "title": "Eliciting Disease Data from Wikipedia Articles", "comments": "9 pages, 3 figures, 4 tables, accepted to 2015 ICWSM Wikipedia\n  workshop; v2 includes author formatting fixes and a few sentences removed to\n  make it 8 pages (although arXiv renders it as 9); v3 uses embedded type 1\n  fonts in the figures and title-cases the title (required by AAAI); v4 fixes\n  typo in abstract", "journal-ref": null, "doi": null, "report-no": "LA-UR-15-22528", "categories": "cs.IR cs.CL cs.SI q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional disease surveillance systems suffer from several disadvantages,\nincluding reporting lags and antiquated technology, that have caused a movement\ntowards internet-based disease surveillance systems. Internet systems are\nparticularly attractive for disease outbreaks because they can provide data in\nnear real-time and can be verified by individuals around the globe. However,\nmost existing systems have focused on disease monitoring and do not provide a\ndata repository for policy makers or researchers. In order to fill this gap, we\nanalyzed Wikipedia article content.\n  We demonstrate how a named-entity recognizer can be trained to tag case\ncounts, death counts, and hospitalization counts in the article narrative that\nachieves an F1 score of 0.753. We also show, using the 2014 West African Ebola\nvirus disease epidemic article as a case study, that there are detailed time\nseries data that are consistently updated that closely align with ground truth\ndata.\n  We argue that Wikipedia can be used to create the first community-driven\nopen-source emerging disease detection, monitoring, and repository system.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 19:34:01 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2015 17:53:32 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2015 00:42:23 GMT"}, {"version": "v4", "created": "Mon, 24 Aug 2015 22:14:55 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Fairchild", "Geoffrey", "", "1 and 3"], ["De Silva", "Lalindra", ""], ["Del Valle", "Sara Y.", ""], ["Segre", "Alberto M.", ""]]}, {"id": "1504.01183", "submitter": "Monica Jha Miss", "authors": "Monica Jha", "title": "Document Clustering using K-Medoids", "comments": "5 pages", "journal-ref": "International Journal on Advanced Computer Theory and Engineering\n  (IJACTE), ISSN (Print): 2319-2526, Volume-4, Issue-1, 2015", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People are always in search of matters for which they are prone to use\ninternet, but again it has huge assemblage of data due to which it becomes\ndifficult for the reader to get the most accurate data. To make it easier for\npeople to gather accurate data, similar information has to be clustered at one\nplace. There are many algorithms used for clustering of relevant information in\none platform. In this paper, K-Medoids clustering algorithm has been employed\nfor formation of clusters which is further used for document summarization.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 01:19:25 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Jha", "Monica", ""]]}, {"id": "1504.01433", "submitter": "Joan Hurtado", "authors": "Joan Hurtado", "title": "Automated System for Improving RSS Feeds Data Quality", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the majority of RSS feeds provide incomplete information about\ntheir news items. The lack of information leads to engagement loss in users. We\npresent a new automated system for improving the RSS feeds' data quality. RSS\nfeeds provide a list of the latest news items ordered by date. Therefore, it\nmakes it easy for a web crawler to precisely locate the item and extract its\nraw content. Then it identifies where the main content is located and extracts:\nmain text corpus, relevant keywords, bigrams, best image and predicts the\ncategory of the item. The output of the system is an enhanced RSS feed. The\nproposed system showed an average item data quality improvement from 39.98% to\n95.62%.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 22:55:38 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Hurtado", "Joan", ""]]}, {"id": "1504.01760", "submitter": "Jeon-Hyung Kang", "authors": "Jeon-Hyung Kang and Kristina Lerman", "title": "User Effort and Network Structure Mediate Access to Information in\n  Networks", "comments": "The 9TH International AAAI Conference on Web and Social Media", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals' access to information in a social network depends on its\ndistributed and where in the network individuals position themselves. However,\nindividuals have limited capacity to manage their social connections and\nprocess information. In this work, we study how this limited capacity and\nnetwork structure interact to affect the diversity of information social media\nusers receive. Previous studies of the role of networks in information access\nwere limited in their ability to measure the diversity of information. We\naddress this problem by learning the topics of interest to social media users\nby observing messages they share online with their followers. We present a\nprobabilistic model that incorporates human cognitive constraints in a\ngenerative model of information sharing. We then use the topics learned by the\nmodel to measure the diversity of information users receive from their social\nmedia contacts. We confirm that users in structurally diverse network\npositions, which bridge otherwise disconnected regions of the follower graph,\nare exposed to more diverse information. In addition, we identify user effort\nas an important variable that mediates access to diverse information in social\nmedia. Users who invest more effort into their activity on the site not only\nplace themselves in more structurally diverse positions within the network than\nthe less engaged users, but they also receive more diverse information when\nlocated in similar network positions. These findings indicate that the\nrelationship between network structure and access to information in networks is\nmore nuanced than previously thought.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 21:30:51 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Kang", "Jeon-Hyung", ""], ["Lerman", "Kristina", ""]]}, {"id": "1504.02356", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Eva Mohedano, Amaia Salvador, Sergi Porta, Xavier Gir\\'o-i-Nieto,\n  Graham Healy, Kevin McGuinness, Noel O'Connor and Alan F. Smeaton", "title": "Exploring EEG for Object Detection and Retrieval", "comments": "This preprint is the full version of a short paper accepted in the\n  ACM International Conference on Multimedia Retrieval (ICMR) 2015 (Shanghai,\n  China)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the potential for using Brain Computer Interfaces (BCI)\nas a relevance feedback mechanism in content-based image retrieval. We\ninvestigate if it is possible to capture useful EEG signals to detect if\nrelevant objects are present in a dataset of realistic and complex images. We\nperform several experiments using a rapid serial visual presentation (RSVP) of\nimages at different rates (5Hz and 10Hz) on 8 users with different degrees of\nfamiliarization with BCI and the dataset. We then use the feedback from the BCI\nand mouse-based interfaces to retrieve localized objects in a subset of TRECVid\nimages. We show that it is indeed possible to detect such objects in complex\nimages and, also, that users with previous knowledge on the dataset or\nexperience with the RSVP outperform others. When the users have limited time to\nannotate the images (100 seconds in our experiments) both interfaces are\ncomparable in performance. Comparing our best users in a retrieval task, we\nfound that EEG-based relevance feedback outperforms mouse-based feedback. The\nrealistic and complex image dataset differentiates our work from previous\nstudies on EEG for image retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 15:43:52 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Mohedano", "Eva", ""], ["Salvador", "Amaia", ""], ["Porta", "Sergi", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""], ["Healy", "Graham", ""], ["McGuinness", "Kevin", ""], ["O'Connor", "Noel", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "1504.02362", "submitter": "Vladimir Ivanov", "authors": "V. K. Ivanov, B. V. Palyukh, A. N. Sotnikov", "title": "Approaches to the Intelligent Subject Search", "comments": "8 pages", "journal-ref": "FedCSIS'2014 3 (2014) 13-20", "doi": "10.15439/2014F70 10.15439/978-83-60810-57-6", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents main results of the pilot study of approaches to the\nsubject information search based on automated semantic processing of mass\nscientific and technical data. The authors focus on technology of building and\nqualification of search queries with the following filtering and ranking of\nsearch data. Software architecture, specific features of subject search and\nresearch results application are considered.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 16:22:32 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Ivanov", "V. K.", ""], ["Palyukh", "B. V.", ""], ["Sotnikov", "A. N.", ""]]}, {"id": "1504.02382", "submitter": "Shahab Basiri", "authors": "Shahab Basiri, Esa Ollila and Visa Koivunen", "title": "Robust, scalable and fast bootstrap method for analyzing large scale\n  data", "comments": "This paper is submitted for publication in IEEE Transactions On\n  Signal Processing, 8 pages, 8 figures", "journal-ref": null, "doi": "10.1109/TSP.2015.2498121", "report-no": null, "categories": "stat.ME cs.IR cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of performing statistical inference for\nlarge scale data sets i.e., Big Data. The volume and dimensionality of the data\nmay be so high that it cannot be processed or stored in a single computing\nnode. We propose a scalable, statistically robust and computationally efficient\nbootstrap method, compatible with distributed processing and storage systems.\nBootstrap resamples are constructed with smaller number of distinct data points\non multiple disjoint subsets of data, similarly to the bag of little bootstrap\nmethod (BLB) [1]. Then significant savings in computation is achieved by\navoiding the re-computation of the estimator for each bootstrap sample.\nInstead, a computationally efficient fixed-point estimation equation is\nanalytically solved via a smart approximation following the Fast and Robust\nBootstrap method (FRB) [2]. Our proposed bootstrap method facilitates the use\nof highly robust statistical methods in analyzing large scale data sets. The\nfavorable statistical properties of the method are established analytically.\nNumerical examples demonstrate scalability, low complexity and robust\nstatistical performance of the method in analyzing large data sets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 16:48:28 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2015 20:01:28 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Basiri", "Shahab", ""], ["Ollila", "Esa", ""], ["Koivunen", "Visa", ""]]}, {"id": "1504.03068", "submitter": "Ahmad Kamal", "authors": "Ahmad Kamal", "title": "Review Mining for Feature Based Opinion Summarization and Visualization", "comments": "6 pages, 5 figures, 2 tables", "journal-ref": "International Journal of Computer Applications, 119(17), 2015, pp.\n  6-13", "doi": "10.5120/21157-4183", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application and usage of opinion mining, especially for business\nintelligence, product recommendation, targeted marketing etc. have fascinated\nmany research attentions around the globe. Various research efforts attempted\nto mine opinions from customer reviews at different levels of granularity,\nincluding word-, sentence-, and document-level. However, development of a fully\nautomatic opinion mining and sentiment analysis system is still elusive. Though\nthe development of opinion mining and sentiment analysis systems are getting\nmomentum, most of them attempt to perform document-level sentiment analysis,\nclassifying a review document as positive, negative, or neutral. Such\ndocument-level opinion mining approaches fail to provide insight about users\nsentiment on individual features of a product or service. Therefore, it seems\nto be a great help for both customers and manufacturers, if the reviews could\nbe processed at a finer-grained level and presented in a summarized form\nthrough some visual means, highlighting individual features of a product and\nusers sentiment expressed over them. In this paper, the design of a unified\nopinion mining and sentiment analysis framework is presented at the\nintersection of both machine learning and natural language processing\napproaches. Also, design of a novel feature-level review summarization scheme\nis proposed to visualize mined features, opinions and their polarity values in\na comprehendible way.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 05:53:59 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 03:02:18 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Kamal", "Ahmad", ""]]}, {"id": "1504.03713", "submitter": "Subhashini Krishnasamy", "authors": "Subhashini Krishnasamy, Rajat Sen, Sewoong Oh, Sanjay Shakkottai", "title": "Detecting Sponsored Recommendations", "comments": "Shorter version to appear in Sigmetrics, June 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a vast number of items, web-pages, and news to choose from, online\nservices and the customers both benefit tremendously from personalized\nrecommender systems. Such systems however provide great opportunities for\ntargeted advertisements, by displaying ads alongside genuine recommendations.\nWe consider a biased recommendation system where such ads are displayed without\nany tags (disguised as genuine recommendations), rendering them\nindistinguishable to a single user. We ask whether it is possible for a small\nsubset of collaborating users to detect such a bias. We propose an algorithm\nthat can detect such a bias through statistical analysis on the collaborating\nusers' feedback. The algorithm requires only binary information indicating\nwhether a user was satisfied with each of the recommended item or not. This\nmakes the algorithm widely appealing to real world issues such as\nidentification of search engine bias and pharmaceutical lobbying. We prove that\nthe proposed algorithm detects the bias with high probability for a broad class\nof recommendation systems when sufficient number of users provide feedback on\nsufficient number of recommendations. We provide extensive simulations with\nreal data sets and practical recommender systems, which confirm the trade offs\nin the theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 20:38:30 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Krishnasamy", "Subhashini", ""], ["Sen", "Rajat", ""], ["Oh", "Sewoong", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "1504.04208", "submitter": "Andrea Scharnhorst", "authors": "Rob Koopman, Shenghui Wang, Andrea Scharnhorst", "title": "Contextualization of topics - browsing through terms, authors, journals\n  and cluster allocations", "comments": "proceedings of the ISSI 2015 conference (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper builds on an innovative Information Retrieval tool, Ariadne. The\ntool has been developed as an interactive network visualization and browsing\ntool for large-scale bibliographic databases. It basically allows to gain\ninsights into a topic by contextualizing a search query (Koopman et al., 2015).\nIn this paper, we apply the Ariadne tool to a far smaller dataset of 111,616\ndocuments in astronomy and astrophysics. Labeled as the Berlin dataset, this\ndata have been used by several research teams to apply and later compare\ndifferent clustering algorithms. The quest for this team effort is how to\ndelineate topics. This paper contributes to this challenge in two different\nways. First, we produce one of the different cluster solution and second, we\nuse Ariadne (the method behind it, and the interface - called LittleAriadne) to\ndisplay cluster solutions of the different group members. By providing a tool\nthat allows the visual inspection of the similarity of article clusters\nproduced by different algorithms, we present a complementary approach to other\npossible means of comparison. More particular, we discuss how we can - with\nLittleAriadne - browse through the network of topical terms, authors, journals\nand cluster solutions in the Berlin dataset and compare cluster solutions as\nwell as see their context.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 12:38:10 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Koopman", "Rob", ""], ["Wang", "Shenghui", ""], ["Scharnhorst", "Andrea", ""]]}, {"id": "1504.04216", "submitter": "Vladimir Ivanov", "authors": "V. K. Ivanov, P. I. Meskin", "title": "Genetic algorithm implementation for effective document subject search", "comments": "in Russian", "journal-ref": "Programmnye produkty i sistemy 4 (2014) 118-126", "doi": "10.15827/0236-235X.108.118-126", "report-no": null, "categories": "cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the software implementation of genetic algorithm for\nidentifying and selecting most relevant results received during sequentially\nexecuted subject search operations. Simulated evolutionary process generates\nsustainable and effective population of search queries, forms search pattern of\ndocuments or semantic core, creates relevant sets of required documents, allows\nautomatic classification of search results. The paper discusses the features of\nsubject search, justifies the use of a genetic algorithm, describes arguments\nof the fitness function and describes basic steps and parameters of the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 13:05:19 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Ivanov", "V. K.", ""], ["Meskin", "P. I.", ""]]}, {"id": "1504.04317", "submitter": "Robert Bridges", "authors": "Corinne L. Jones, Robert A. Bridges, Kelly Huffer, John Goodall", "title": "Towards a relation extraction framework for cyber-security concepts", "comments": "4 pages in Cyber & Information Security Research Conference 2015, ACM", "journal-ref": null, "doi": "10.1145/2746266.2746277", "report-no": null, "categories": "cs.IR cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to assist security analysts in obtaining information pertaining to\ntheir network, such as novel vulnerabilities, exploits, or patches, information\nretrieval methods tailored to the security domain are needed. As labeled text\ndata is scarce and expensive, we follow developments in semi-supervised Natural\nLanguage Processing and implement a bootstrapping algorithm for extracting\nsecurity entities and their relationships from text. The algorithm requires\nlittle input data, specifically, a few relations or patterns (heuristics for\nidentifying relations), and incorporates an active learning component which\nqueries the user on the most important decisions to prevent drifting from the\ndesired relations. Preliminary testing on a small corpus shows promising\nresults, obtaining precision of .82.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 17:26:24 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Jones", "Corinne L.", ""], ["Bridges", "Robert A.", ""], ["Huffer", "Kelly", ""], ["Goodall", "John", ""]]}, {"id": "1504.04558", "submitter": "Quanzeng You", "authors": "Quanzeng You, Sumit Bhatia, Jiebo Luo", "title": "A Picture Tells a Thousand Words -- About You! User Interest Profiling\n  from User Generated Visual Content", "comments": "7 pages, 6 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference of online social network users' attributes and interests has been\nan active research topic. Accurate identification of users' attributes and\ninterests is crucial for improving the performance of personalization and\nrecommender systems. Most of the existing works have focused on textual content\ngenerated by the users and have successfully used it for predicting users'\ninterests and other identifying attributes. However, little attention has been\npaid to user generated visual content (images) that is becoming increasingly\npopular and pervasive in recent times. We posit that images posted by users on\nonline social networks are a reflection of topics they are interested in and\npropose an approach to infer user attributes from images posted by them. We\nanalyze the content of individual images and then aggregate the image-level\nknowledge to infer user-level interest distribution. We employ image-level\nsimilarity to propagate the label information between images, as well as\nutilize the image category information derived from the user created\norganization structure to further propagate the category-level knowledge for\nall images. A real life social network dataset created from Pinterest is used\nfor evaluation and the experimental results demonstrate the effectiveness of\nour proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 16:28:35 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["You", "Quanzeng", ""], ["Bhatia", "Sumit", ""], ["Luo", "Jiebo", ""]]}, {"id": "1504.04596", "submitter": "Yadong Zhu", "authors": "Yadong Zhu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng", "title": "Structural Learning of Diverse Ranking", "comments": "Discriminant Function, Diversity Feature, Learning Framework", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relevance and diversity are both crucial criteria for an effective search\nsystem. In this paper, we propose a unified learning framework for\nsimultaneously optimizing both relevance and diversity. Specifically, the\nproblem is formalized as a structural learning framework optimizing\nDiversity-Correlated Evaluation Measures (DCEM), such as ERR-IA, a-NDCG and\nNRBP. Within this framework, the discriminant function is defined to be a\nbi-criteria objective maximizing the sum of the relevance scores and\ndissimilarities (or diversity) among the documents. Relevance and diversity\nfeatures are utilized to define the relevance scores and dissimilarities,\nrespectively. Compared with traditional methods, the advantages of our approach\nlie in that: (1) Directly optimizing DCEM as the loss function is more\nfundamental for the task; (2) Our framework does not rely on explicit diversity\ninformation such as subtopics, thus is more adaptive to real application; (3)\nThe representation of diversity as the feature-based scoring function is more\nflexible to incorporate rich diversity-based features into the learning\nframework. Extensive experiments on the public TREC datasets show that our\napproach significantly outperforms state-of-the-art diversification approaches,\nwhich validate the above advantages.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2015 18:23:16 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2015 09:59:00 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Zhu", "Yadong", ""], ["Lan", "Yanyan", ""], ["Guo", "Jiafeng", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1504.04712", "submitter": "Arkaitz Zubiaga", "authors": "Arkaitz Zubiaga, Maria Liakata, Rob Procter, Kalina Bontcheva, Peter\n  Tolmie", "title": "Towards Detecting Rumours in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of false rumours during emergencies can jeopardise the well-being\nof citizens as they are monitoring the stream of news from social media to stay\nabreast of the latest updates. In this paper, we describe the methodology we\nhave developed within the PHEME project for the collection and sampling of\nconversational threads, as well as the tool we have developed to facilitate the\nannotation of these threads so as to identify rumourous ones. We describe the\nannotation task conducted on threads collected during the 2014 Ferguson unrest\nand we present and analyse our findings. Our results show that we can collect\neffectively social media rumours and identify multiple rumours associated with\na range of stories that would have been hard to identify by relying on existing\ntechniques that need manual input of rumour-specific keywords.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 11:50:54 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Zubiaga", "Arkaitz", ""], ["Liakata", "Maria", ""], ["Procter", "Rob", ""], ["Bontcheva", "Kalina", ""], ["Tolmie", "Peter", ""]]}, {"id": "1504.04730", "submitter": "Sourav Bhattacharya", "authors": "Sourav Bhattacharya and Otto Huhta and N. Asokan", "title": "LookAhead: Augmenting Crowdsourced Website Reputation Systems With\n  Predictive Modeling", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsafe websites consist of malicious as well as inappropriate sites, such as\nthose hosting questionable or offensive content. Website reputation systems are\nintended to help ordinary users steer away from these unsafe sites. However,\nthe process of assigning safety ratings for websites typically involves humans.\nConsequently it is time consuming, costly and not scalable. This has resulted\nin two major problems: (i) a significant proportion of the web space remains\nunrated and (ii) there is an unacceptable time lag before new websites are\nrated.\n  In this paper, we show that by leveraging structural and content-based\nproperties of websites, it is possible to reliably and efficiently predict\ntheir safety ratings, thereby mitigating both problems. We demonstrate the\neffectiveness of our approach using four datasets of up to 90,000 websites. We\nuse ratings from Web of Trust (WOT), a popular crowdsourced web reputation\nsystem, as ground truth. We propose a novel ensemble classification technique\nthat makes opportunistic use of available structural and content properties of\nwebpages to predict their eventual ratings in two dimensions used by WOT:\ntrustworthiness and child safety. Ours is the first classification system to\npredict such subjective ratings and the same approach works equally well in\nidentifying malicious websites. Across all datasets, our classification\nperforms well with average F$_1$-score in the 74--90\\% range.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 15:13:13 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Bhattacharya", "Sourav", ""], ["Huhta", "Otto", ""], ["Asokan", "N.", ""]]}, {"id": "1504.04818", "submitter": "Mingsheng Long", "authors": "Mingsheng Long, Yue Cao, Jianmin Wang, Philip S. Yu", "title": "Composite Correlation Quantization for Efficient Multimodal Retrieval", "comments": null, "journal-ref": null, "doi": "10.1145/2911451.2911493", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient similarity retrieval from large-scale multimodal database is\npervasive in modern search engines and social networks. To support queries\nacross content modalities, the system should enable cross-modal correlation and\ncomputation-efficient indexing. While hashing methods have shown great\npotential in achieving this goal, current attempts generally fail to learn\nisomorphic hash codes in a seamless scheme, that is, they embed multiple\nmodalities in a continuous isomorphic space and separately threshold embeddings\ninto binary codes, which incurs substantial loss of retrieval accuracy. In this\npaper, we approach seamless multimodal hashing by proposing a novel Composite\nCorrelation Quantization (CCQ) model. Specifically, CCQ jointly finds\ncorrelation-maximal mappings that transform different modalities into\nisomorphic latent space, and learns composite quantizers that convert the\nisomorphic latent features into compact binary codes. An optimization framework\nis devised to preserve both intra-modal similarity and inter-modal correlation\nthrough minimizing both reconstruction and quantization errors, which can be\ntrained from both paired and partially paired data in linear time. A\ncomprehensive set of experiments clearly show the superior effectiveness and\nefficiency of CCQ against the state of the art hashing methods for both\nunimodal and cross-modal retrieval.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2015 10:16:50 GMT"}, {"version": "v2", "created": "Sun, 22 May 2016 08:46:41 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Long", "Mingsheng", ""], ["Cao", "Yue", ""], ["Wang", "Jianmin", ""], ["Yu", "Philip S.", ""]]}, {"id": "1504.04945", "submitter": "Yadong Zhu", "authors": "Yadong Zhu, Yanyan Lan, Jiafeng Guo, Xueqi Cheng", "title": "Topic-focused Dynamic Information Filtering in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the quick development of online social media such as twitter or sina\nweibo in china, many users usually track hot topics to satisfy their desired\ninformation need. For a hot topic, new opinions or ideas will be continuously\nproduced in the form of online data stream. In this scenario, how to\neffectively filter and display information for a certain topic dynamically,\nwill be a critical problem. We call the problem as Topic-focused Dynamic\nInformation Filtering (denoted as TDIF for short) in social media. In this\npaper, we start open discussions on such application problems. We first analyze\nthe properties of the TDIF problem, which usually contains several typical\nrequirements: relevance, diversity, recency and confidence. Recency means that\nusers want to follow the recent opinions or news. Additionally, the confidence\nof information must be taken into consideration. How to balance these factors\nproperly in online data stream is very important and challenging. We propose a\ndynamic preservation strategy on the basis of an existing feature-based utility\nfunction, to solve the TDIF problem. Additionally, we propose new dynamic\ndiversity measures, to get a more reasonable evaluation for such application\nproblems. Extensive exploratory experiments have been conducted on TREC public\ntwitter dataset, and the experimental results validate the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 06:12:06 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Zhu", "Yadong", ""], ["Lan", "Yanyan", ""], ["Guo", "Jiafeng", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1504.05469", "submitter": "Yury Kashnitsky", "authors": "Yury Kashnitsky", "title": "Visual analytics in FCA-based clustering", "comments": "11 pages, 3 figures, 2 algorithms, 3rd International Conference on\n  Analysis of Images, Social Networks and Texts (AIST'2014). in Supplementary\n  Proceedings of the 3rd International Conference on Analysis of Images, Social\n  Networks and Texts (AIST 2014), Vol. 1197, CEUR-WS.org, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual analytics is a subdomain of data analysis which combines both human\nand machine analytical abilities and is applied mostly in decision-making and\ndata mining tasks. Triclustering, based on Formal Concept Analysis (FCA), was\ndeveloped to detect groups of objects with similar properties under similar\nconditions. It is used in Social Network Analysis (SNA) and is a basis for\ncertain types of recommender systems. The problem of triclustering algorithms\nis that they do not always produce meaningful clusters. This article describes\na specific triclustering algorithm and a prototype of a visual analytics\nplatform for working with obtained clusters. This tool is designed as a testing\nframeworkis and is intended to help an analyst to grasp the results of\ntriclustering and recommender algorithms, and to make decisions on\nmeaningfulness of certain triclusters and recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 15:28:23 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Kashnitsky", "Yury", ""]]}, {"id": "1504.05473", "submitter": "Yury Kashnitsky", "authors": "Yury Kashnitsky, Dmitry I. Ignatov", "title": "Can FCA-based Recommender System Suggest a Proper Classifier?", "comments": "10 pages, 1 figure, 4 tables, ECAI 2014, workshop \"What FCA can do\n  for \"Artifficial Intelligence\"", "journal-ref": "CEUR Workshop Proceedings, 1257, pp. 17-26 (2014)", "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper briefly introduces multiple classifier systems and describes a new\nalgorithm, which improves classification accuracy by means of recommendation of\na proper algorithm to an object classification. This recommendation is done\nassuming that a classifier is likely to predict the label of the object\ncorrectly if it has correctly classified its neighbors. The process of\nassigning a classifier to each object is based on Formal Concept Analysis. We\nexplain the idea of the algorithm with a toy example and describe our first\nexperiments with real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2015 15:38:23 GMT"}], "update_date": "2015-04-22", "authors_parsed": [["Kashnitsky", "Yury", ""], ["Ignatov", "Dmitry I.", ""]]}, {"id": "1504.06077", "submitter": "Nicolas Turenne", "authors": "Nicolas Turenne, Mathieu Andro, Roselyne Corbi\\`ere, Tien T. Phan", "title": "Open Data Platform for Knowledge Access in Plant Health Domain : VESPA\n  Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important data are locked in ancient literature. It would be uneconomic to\nproduce these data again and today or to extract them without the help of text\nmining technologies. Vespa is a text mining project whose aim is to extract\ndata on pest and crops interactions, to model and predict attacks on crops, and\nto reduce the use of pesticides. A few attempts proposed an agricultural\ninformation access. Another originality of our work is to parse documents with\na dependency of the document architecture.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 08:27:29 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Turenne", "Nicolas", ""], ["Andro", "Mathieu", ""], ["Corbi\u00e8re", "Roselyne", ""], ["Phan", "Tien T.", ""]]}, {"id": "1504.06165", "submitter": "Nitish Gupta", "authors": "Nitish Gupta, Sameer Singh", "title": "Collectively Embedding Multi-Relational Data for Predicting User\n  Preferences", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization has found incredible success and widespread application\nas a collaborative filtering based approach to recommendations. Unfortunately,\nincorporating additional sources of evidence, especially ones that are\nincomplete and noisy, is quite difficult to achieve in such models, however, is\noften crucial for obtaining further gains in accuracy. For example, additional\ninformation about businesses from reviews, categories, and attributes should be\nleveraged for predicting user preferences, even though this information is\noften inaccurate and partially-observed. Instead of creating customized methods\nthat are specific to each type of evidences, in this paper we present a generic\napproach to factorization of relational data that collectively models all the\nrelations in the database. By learning a set of embeddings that are shared\nacross all the relations, the model is able to incorporate observed information\nfrom all the relations, while also predicting all the relations of interest.\nOur evaluation on multiple Amazon and Yelp datasets demonstrates effective\nutilization of additional information for held-out preference prediction, but\nfurther, we present accurate models even for the cold-starting businesses and\nproducts for which we do not observe any ratings or reviews. We also illustrate\nthe capability of the model in imputing missing information and jointly\nvisualizing words, categories, and attribute factors.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 13:07:24 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Gupta", "Nitish", ""], ["Singh", "Sameer", ""]]}, {"id": "1504.06837", "submitter": "Marc Claesen", "authors": "Marc Claesen, Jesse Davis, Frank De Smet, Bart De Moor", "title": "Assessing binary classifiers using only positive and unlabeled data", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the performance of a learned model is a crucial part of machine\nlearning. However, in some domains only positive and unlabeled examples are\navailable, which prohibits the use of most standard evaluation metrics. We\npropose an approach to estimate any metric based on contingency tables,\nincluding ROC and PR curves, using only positive and unlabeled data. Estimating\nthese performance metrics is essentially reduced to estimating the fraction of\n(latent) positives in the unlabeled set, assuming known positives are a random\nsample of all positives. We provide theoretical bounds on the quality of our\nestimates, illustrate the importance of estimating the fraction of positives in\nthe unlabeled set and demonstrate empirically that we are able to reliably\nestimate ROC and PR curves on real data.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 14:59:12 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2015 13:18:43 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Claesen", "Marc", ""], ["Davis", "Jesse", ""], ["De Smet", "Frank", ""], ["De Moor", "Bart", ""]]}, {"id": "1504.06867", "submitter": "Rafal Scherer", "authors": "Rafal Grycuk, Marcin Gabryel, Rafal Scherer, Sviatoslav Voloshynovskiy", "title": "Multi-layer Architecture For Storing Visual Data Based on WCF and\n  Microsoft SQL Server Database", "comments": "Accepted for the 14th International Conference on Artificial\n  Intelligence and Soft Computing, ICAISC, June 14-18, 2015, Zakopane, Poland", "journal-ref": null, "doi": "10.1007/978-3-319-19324-3_64", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we present a novel architecture for storing visual data.\nEffective storing, browsing and searching collections of images is one of the\nmost important challenges of computer science. The design of architecture for\nstoring such data requires a set of tools and frameworks such as SQL database\nmanagement systems and service-oriented frameworks. The proposed solution is\nbased on a multi-layer architecture, which allows to replace any component\nwithout recompilation of other components. The approach contains five\ncomponents, i.e. Model, Base Engine, Concrete Engine, CBIR service and\nPresentation. They were based on two well-known design patterns: Dependency\nInjection and Inverse of Control. For experimental purposes we implemented the\nSURF local interest point detector as a feature extractor and $K$-means\nclustering as indexer. The presented architecture is intended for content-based\nretrieval systems simulation purposes as well as for real-world CBIR tasks.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 19:11:37 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Grycuk", "Rafal", ""], ["Gabryel", "Marcin", ""], ["Scherer", "Rafal", ""], ["Voloshynovskiy", "Sviatoslav", ""]]}, {"id": "1504.06868", "submitter": "Gordon Cormack", "authors": "Gordon V. Cormack and Maura R. Grossman", "title": "Autonomy and Reliability of Continuous Active Learning for\n  Technology-Assisted Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We enhance the autonomy of the continuous active learning method shown by\nCormack and Grossman (SIGIR 2014) to be effective for technology-assisted\nreview, in which documents from a collection are retrieved and reviewed, using\nrelevance feedback, until substantially all of the relevant documents have been\nreviewed. Autonomy is enhanced through the elimination of topic-specific and\ndataset-specific tuning parameters, so that the sole input required by the user\nis, at the outset, a short query, topic description, or single relevant\ndocument; and, throughout the review, ongoing relevance assessments of the\nretrieved documents. We show that our enhancements consistently yield superior\nresults to Cormack and Grossman's version of continuous active learning, and\nother methods, not only on average, but on the vast majority of topics from\nfour separate sets of tasks: the legal datasets examined by Cormack and\nGrossman, the Reuters RCV1-v2 subject categories, the TREC 6 AdHoc task, and\nthe construction of the TREC 2002 filtering test collection.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 19:19:01 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Cormack", "Gordon V.", ""], ["Grossman", "Maura R.", ""]]}, {"id": "1504.06936", "submitter": "Alejandro Metke Jimenez", "authors": "Alejandro Metke-Jimenez, Sarvnaz Karimi", "title": "Concept Extraction to Identify Adverse Drug Reactions in Medical Forums:\n  A Comparison of Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media is becoming an increasingly important source of information to\ncomplement traditional pharmacovigilance methods. In order to identify signals\nof potential adverse drug reactions, it is necessary to first identify medical\nconcepts in the social media text. Most of the existing studies use\ndictionary-based methods which are not evaluated independently from the overall\nsignal detection task.\n  We compare different approaches to automatically identify and normalise\nmedical concepts in consumer reviews in medical forums. Specifically, we\nimplement several dictionary-based methods popular in the relevant literature,\nas well as a method we suggest based on a state-of-the-art machine learning\nmethod for entity recognition. MetaMap, a popular biomedical concept extraction\ntool, is used as a baseline. Our evaluations were performed in a controlled\nsetting on a common corpus which is a collection of medical forum posts\nannotated with concepts and linked to controlled vocabularies such as MedDRA\nand SNOMED CT.\n  To our knowledge, our study is the first to systematically examine the effect\nof popular concept extraction methods in the area of signal detection for\nadverse reactions. We show that the choice of algorithm or controlled\nvocabulary has a significant impact on concept extraction, which will impact\nthe overall signal detection process. We also show that our proposed machine\nlearning approach significantly outperforms all the other methods in\nidentification of both adverse reactions and drugs, even when trained with a\nrelatively small set of annotated text.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 05:56:13 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Metke-Jimenez", "Alejandro", ""], ["Karimi", "Sarvnaz", ""]]}, {"id": "1504.06961", "submitter": "Daniel Hienert", "authors": "Daniel Hienert, Wilko van Hoek, Alina Weber and Dagmar Kern", "title": "WHOSE - A Tool for Whole-Session Analysis in IIR", "comments": "In Advances in Information Retrieval: 37th European Conference on IR\n  Research, ECIR 2015, Vienna, Austria, March 29 - April 2, 2015. Proceedings,\n  Lecture Notes in Computer Science 9022, 172-183", "journal-ref": null, "doi": "10.1007/978-3-319-16354-3_18", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in Interactive Information Retrieval (IIR)\nevaluation is the development and application of re-usable tools that allow\nresearchers to analyze search behavior of real users in different environments\nand different domains, but with comparable results. Furthermore, IIR recently\nfocuses more on the analysis of whole sessions, which includes all user\ninteractions that are carried out within a session but also across several\nsessions by the same user. Some frameworks have already been proposed for the\nevaluation of controlled experiments in IIR, but yet no framework is available\nfor interactive evaluation of search behavior from real-world information\nretrieval (IR) systems with real users. In this paper we present a framework\nfor whole-session evaluation that can also utilize these uncontrolled data\nsets. The logging component can easily be integrated into real-world IR systems\nfor generating and analyzing new log data. Furthermore, due to a supplementary\nmapping it is also possible to analyze existing log data. For every IR system\ndifferent actions and filters can be defined. This allows system operators and\nresearchers to use the framework for the analysis of user search behavior in\ntheir IR systems and to compare it with others. Using a graphical user\ninterface they have the possibility to interactively explore the data set from\na broad overview down to individual sessions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 08:06:21 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Hienert", "Daniel", ""], ["van Hoek", "Wilko", ""], ["Weber", "Alina", ""], ["Kern", "Dagmar", ""]]}, {"id": "1504.07004", "submitter": "Moitreya Chatterjee", "authors": "Moitreya Chatterjee and Anton Leuski", "title": "An Active Learning Based Approach For Effective Video Annotation And\n  Retrieval", "comments": "5 pages, 3 figures, Compressed version published at ACM ICMR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional multimedia annotation/retrieval systems such as Normalized\nContinuous Relevance Model (NormCRM) [16] require a fully labeled training data\nfor a good performance. Active Learning, by determining an order for labeling\nthe training data, allows for a good performance even before the training data\nis fully annotated. In this work we propose an active learning algorithm, which\ncombines a novel measure of sample uncertainty with a novel clustering-based\napproach for determining sample density and diversity and integrate it with\nNormCRM. The clusters are also iteratively refined to ensure both feature and\nlabel-level agreement among samples. We show that our approach outperforms\nmultiple baselines both on a recent, open character animation dataset and on\nthe popular TRECVID corpus at both the tasks of annotation and text-based\nretrieval of videos.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 09:44:30 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Chatterjee", "Moitreya", ""], ["Leuski", "Anton", ""]]}, {"id": "1504.07071", "submitter": "Daniel Hienert", "authors": "Daniel Hienert, Dennis Wegener, Siegfried Schomisch", "title": "Exploring semantically-related concepts from Wikipedia: the case of SeRE", "comments": "In Classification & visualization : interfaces to knowledge ;\n  proceedings of the International UDC Seminar 24 - 25 October 2013, The Hague,\n  The Netherlands, edited by Aida Slavic, Almila Akdag Salah, and Sylvie\n  Davies, 153-165. W\\\"urzburg: Ergon-Verl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our web application SeRE designed to explore\nsemantically related concepts. Wikipedia and DBpedia are rich data sources to\nextract related entities for a given topic, like in- and out-links, broader and\nnarrower terms, categorisation information etc. We use the Wikipedia full text\nbody to compute the semantic relatedness for extracted terms, which results in\na list of entities that are most relevant for a topic. For any given query, the\nuser interface of SeRE visualizes these related concepts, ordered by semantic\nrelatedness; with snippets from Wikipedia articles that explain the connection\nbetween those two entities. In a user study we examine how SeRE can be used to\nfind important entities and their relationships for a given topic and to answer\nthe question of how the classification system can be used for filtering.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 13:08:36 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Hienert", "Daniel", ""], ["Wegener", "Dennis", ""], ["Schomisch", "Siegfried", ""]]}, {"id": "1504.07295", "submitter": "Matt Taddy", "authors": "Matt Taddy", "title": "Document Classification by Inversion of Distributed Language\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been many recent advances in the structure and measurement of\ndistributed language models: those that map from words to a vector-space that\nis rich in information about word choice and composition. This vector-space is\nthe distributed language representation. The goal of this note is to point out\nthat any distributed representation can be turned into a classifier through\ninversion via Bayes rule. The approach is simple and modular, in that it will\nwork with any language representation whose training can be formulated as\noptimizing a probability model. In our application to 2 million sentences from\nYelp reviews, we also find that it performs as well as or better than complex\npurpose-built algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 22:32:40 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 19:46:35 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2015 15:27:20 GMT"}], "update_date": "2015-07-27", "authors_parsed": [["Taddy", "Matt", ""]]}, {"id": "1504.07350", "submitter": "Heri Ramampiaro", "authors": "Massimiliano Ruocco and Heri Ramampiaro", "title": "Geo-Temporal Distribution of Tag Terms for Event-Related Image Retrieval", "comments": null, "journal-ref": "Information Processing & Management Journal (IPM), 51(1), pp.\n  92-110. 2015", "doi": "10.1016/j.ipm.2014.09.001", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Media sharing applications, such as Flickr and Panoramio, contain a large\namount of pictures related to real life events. For this reason, the\ndevelopment of effective methods to retrieve these pictures is important, but\nstill a challenging task. Recognizing this importance, and to improve the\nretrieval effectiveness of tag-based event retrieval systems, we propose a new\nmethod to extract a set of geographical tag features from raw geo-spatial\nprofiles of user tags. The main idea is to use these features to select the\nbest expansion terms in a machine learning-based query expansion approach.\nSpecifically, we apply rigorous statistical exploratory analysis of spatial\npoint patterns to extract the geo-spatial features. We use the features both to\nsummarize the spatial characteristics of the spatial distribution of a single\nterm, and to determine the similarity between the spatial profiles of two terms\n-- i.e., term-to-term spatial similarity. To further improve our approach, we\ninvestigate the effect of combining our geo-spatial features with temporal\nfeatures on choosing the expansion terms. To evaluate our method, we perform\nseveral experiments, including well-known feature analyses. Such analyses show\nhow much our proposed geo-spatial features contribute to improve the overall\nretrieval performance. The results from our experiments demonstrate the\neffectiveness and viability of our method.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 05:38:34 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Ruocco", "Massimiliano", ""], ["Ramampiaro", "Heri", ""]]}, {"id": "1504.07361", "submitter": "Nazanin Dehghani", "authors": "Nazanin Dehghani and Masoud Asadpour", "title": "Graph-based Method for Summarized Storyline Generation in Twitter", "comments": "19 pages, 11 figures This paper has been withdrawn by the author\n  because the method improved through some significant modifications and it\n  will be submitted to another journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter has become a leading source of real-time world-wide information and a\ngreat medium for exploring emerging events, breaking news and general topics\nwhich most matter to a broad audience. On the other hand, the explosive rate of\nincoming information in Twitter leads users to experience information overload.\nWhereas, a significant fraction of tweets are about news events, summarizing\nthe storyline of events can be helpful for users to easily access to the\nrelevant and key information hidden among tweets and thereby draw high level\nconclusions. Storytelling is the task of providing chronological summaries of\nsignificant sub-events development and sketching the relationship between\nsub-events. In this paper, we propose a novel framework to generate a\nsummarized storyline of news events from social point of view. Utilizing the\nconcepts in graph-theory, we identify sub-events, summarize the evolution of\nsub-events and generate a coherent storyline of them. Our approach models a\nstoryline as a directed tree of social salient sub-events evolving over time.\nTo overcome the enormous number of redundant tweets, we keep distilled\ninformation in super-tweets. Experiments performed on a large scale data set\nfrom tweets sent during the Iranian Presidential Election (#IranElection) and\nthe results demonstrate the efficiency and effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 07:04:06 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 13:33:09 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Dehghani", "Nazanin", ""], ["Asadpour", "Masoud", ""]]}, {"id": "1504.07389", "submitter": "Marc Claesen", "authors": "Marc Claesen, Frank De Smet, Pieter Gillard, Chantal Mathieu, Bart De\n  Moor", "title": "Building Classifiers to Predict the Start of Glucose-Lowering\n  Pharmacotherapy Using Belgian Health Expenditure Data", "comments": "23 pages, 5 figures, submitted to JMLR special issue on Learning from\n  Electronic Health Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis is important for type 2 diabetes (T2D) to improve patient\nprognosis, prevent complications and reduce long-term treatment costs. We\npresent a novel risk profiling approach based exclusively on health expenditure\ndata that is available to Belgian mutual health insurers. We used expenditure\ndata related to drug purchases and medical provisions to construct models that\npredict whether a patient will start glucose-lowering pharmacotherapy in the\ncoming years, based on that patient's recent medical expenditure history. The\ndesign and implementation of the modeling strategy are discussed in detail and\nseveral learning methods are benchmarked for our application. Our best\nperforming model obtains between 74.9% and 76.8% area under the ROC curve,\nwhich is comparable to state-of-the-art risk prediction approaches for T2D\nbased on questionnaires. In contrast to other methods, our approach can be\nimplemented on a population-wide scale at virtually no extra operational cost.\nPossibly, our approach can be further improved by additional information about\nsome risk factors of T2D that is unavailable in health expenditure data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 09:27:03 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Claesen", "Marc", ""], ["De Smet", "Frank", ""], ["Gillard", "Pieter", ""], ["Mathieu", "Chantal", ""], ["De Moor", "Bart", ""]]}, {"id": "1504.07766", "submitter": "Gianna Maria Del Corso", "authors": "Gianna M. Del Corso and Francesco Romani", "title": "A multi-class approach for ranking graph nodes: models and experiments\n  with incomplete data", "comments": null, "journal-ref": "Information Sciences 2016, vol 329 pages 619-637", "doi": "10.1016/j.ins.2015.09.046", "report-no": null, "categories": "math.NA cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After the phenomenal success of the PageRank algorithm, many researchers have\nextended the PageRank approach to ranking graphs with richer structures beside\nthe simple linkage structure. In some scenarios we have to deal with\nmulti-parameters data where each node has additional features and there are\nrelationships between such features.\n  This paper stems from the need of a systematic approach when dealing with\nmulti-parameter data. We propose models and ranking algorithms which can be\nused with little adjustments for a large variety of networks (bibliographic\ndata, patent data, twitter and social data, healthcare data). In this paper we\nfocus on several aspects which have not been addressed in the literature: (1)\nwe propose different models for ranking multi-parameters data and a class of\nnumerical algorithms for efficiently computing the ranking score of such\nmodels, (2) by analyzing the stability and convergence properties of the\nnumerical schemes we tune a fast and stable technique for the ranking problem,\n(3) we consider the issue of the robustness of our models when data are\nincomplete. The comparison of the rank on the incomplete data with the rank on\nthe full structure shows that our models compute consistent rankings whose\ncorrelation is up to 60% when just 10% of the links of the attributes are\nmaintained suggesting the suitability of our model also when the data are\nincomplete.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 08:37:20 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Del Corso", "Gianna M.", ""], ["Romani", "Francesco", ""]]}, {"id": "1504.07874", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Jennifer Roldan-Carlos, Mathias Lux, Xavier Gir\\'o-i-Nieto, Pia\n  Mu\\~noz and Nektarios Anagnostopoulos", "title": "Visual Information Retrieval in Endoscopic Video Archives", "comments": "Paper accepted at the IEEE/ACM 13th International Workshop on\n  Content-Based Multimedia Indexing (CBMI) in Prague (Czech Republic) between\n  10 and 12 June 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In endoscopic procedures, surgeons work with live video streams from the\ninside of their subjects. A main source for documentation of procedures are\nstill frames from the video, identified and taken during the surgery. However,\nwith growing demands and technical means, the streams are saved to storage\nservers and the surgeons need to retrieve parts of the videos on demand. In\nthis submission we present a demo application allowing for video retrieval\nbased on visual features and late fusion, which allows surgeons to re-find\nshots taken during the procedure.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 14:35:35 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Roldan-Carlos", "Jennifer", ""], ["Lux", "Mathias", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""], ["Mu\u00f1oz", "Pia", ""], ["Anagnostopoulos", "Nektarios", ""]]}, {"id": "1504.08175", "submitter": "Jo\\~ao Vinagre", "authors": "Jo\\~ao Vinagre, Al\\'ipio M\\'ario Jorge, Jo\\~ao Gama", "title": "Evaluation of recommender systems in streaming environments", "comments": "Workshop on 'Recommender Systems Evaluation: Dimensions and Design'\n  (REDD 2014), held in conjunction with RecSys 2014. October 10, 2014, Silicon\n  Valley, United States", "journal-ref": null, "doi": "10.13140/2.1.4381.5367", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Evaluation of recommender systems is typically done with finite datasets.\nThis means that conventional evaluation methodologies are only applicable in\noffline experiments, where data and models are stationary. However, in real\nworld systems, user feedback is continuously generated, at unpredictable rates.\nGiven this setting, one important issue is how to evaluate algorithms in such a\nstreaming data environment. In this paper we propose a prequential evaluation\nprotocol for recommender systems, suitable for streaming data environments, but\nalso applicable in stationary settings. Using this protocol we are able to\nmonitor the evolution of algorithms' accuracy over time. Furthermore, we are\nable to perform reliable comparative assessments of algorithms by computing\nsignificance tests over a sliding window. We argue that besides being suitable\nfor streaming data, prequential evaluation allows the detection of phenomena\nthat would otherwise remain unnoticed in the evaluation of both offline and\nonline recommender systems.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 11:41:49 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Vinagre", "Jo\u00e3o", ""], ["Jorge", "Al\u00edpio M\u00e1rio", ""], ["Gama", "Jo\u00e3o", ""]]}]