[{"id": "1702.00171", "submitter": "Nut Limsopatham", "authors": "Nut Limsopatham, Craig Macdonald, Iadh Ounis", "title": "Inferring Conceptual Relationships When Ranking Patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching patients based on the relevance of their medical records is\nchallenging because of the inherent implicit knowledge within the patients'\nmedical records and queries. Such knowledge is known to the medical\npractitioners but may be hidden from a search system. For example, when\nsearching for the patients with a heart disease, medical practitioners commonly\nknow that patients who are taking the amiodarone medicine are relevant, since\nthis drug is used to combat heart disease. In this article, we argue that\nleveraging such implicit knowledge improves the retrieval effectiveness, since\nit provides new evidence to infer the relevance of patients' medical records\ntowards a query. Specifically, built upon existing conceptual representation\nfor both medical records and queries, we proposed a novel expansion of queries\nthat infers additional conceptual relationships from domain-specific resources\nas well as by extracting informative concepts from the top-ranked patients'\nmedical records. We evaluate the retrieval effectiveness of our proposed\napproach in the context of the TREC 2011 and 2012 Medical Records track. Our\nresults show the effectiveness of our approach to model the implicit knowledge\nin patient search, whereby the retrieval performance is significantly improved\nover both an effective conceptual representation baseline and an existing\nsemantic query expansion baseline. In addition, we provide an analysis of the\ntypes of queries that the proposed approach is likely to be effective.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 09:21:20 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Limsopatham", "Nut", ""], ["Macdonald", "Craig", ""], ["Ounis", "Iadh", ""]]}, {"id": "1702.00187", "submitter": "Fr\\'ed\\'eric Rayar", "authors": "Fr\\'ed\\'eric Rayar", "title": "ImageNet MPEG-7 Visual Descriptors - Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ImageNet is a large scale and publicly available image database. It currently\noffers more than 14 millions of images, organised according to the WordNet\nhierarchy. One of the main objective of the creators is to provide to the\nresearch community a relevant database for visual recognition applications such\nas object recognition, image classification or object localisation. However,\nonly a few visual descriptors of the images are available to be used by the\nresearchers. Only SIFT-based features have been extracted from a subset of the\ncollection. This technical report presents the extraction of some MPEG-7 visual\ndescriptors from the ImageNet database. These descriptors are made publicly\navailable in an effort towards open research.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 10:15:13 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Rayar", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1702.00619", "submitter": "Tarcisio Souza Costa", "authors": "Tarcisio Souza and Elena Demidova and Thomas Risse and Helge Holzmann\n  and Gerhard Gossen and Julian Szymanski", "title": "Semantic URL Analytics to Support Efficient Annotation of Large Scale\n  Web Archives", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-27932-9_14", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term Web archives comprise Web documents gathered over longer time\nperiods and can easily reach hundreds of terabytes in size. Semantic\nannotations such as named entities can facilitate intelligent access to the Web\narchive data. However, the annotation of the entire archive content on this\nscale is often infeasible. The most efficient way to access the documents\nwithin Web archives is provided through their URLs, which are typically stored\nin dedicated index files.The URLs of the archived Web documents can contain\nsemantic information and can offer an efficient way to obtain initial semantic\nannotations for the archived documents. In this paper, we analyse the\napplicability of semantic analysis techniques such as named entity extraction\nto the URLs in a Web archive. We evaluate the precision of the named entity\nextraction from the URLs in the Popular German Web dataset and analyse the\nproportion of the archived URLs from 1,444 popular domains in the time interval\nfrom 2000 to 2012 to which these techniques are applicable. Our results\ndemonstrate that named entity recognition can be successfully applied to a\nlarge number of URLs in our Web archive and provide a good starting point to\nefficiently annotate large scale collections of Web documents.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 11:09:53 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Souza", "Tarcisio", ""], ["Demidova", "Elena", ""], ["Risse", "Thomas", ""], ["Holzmann", "Helge", ""], ["Gossen", "Gerhard", ""], ["Szymanski", "Julian", ""]]}, {"id": "1702.00855", "submitter": "Enno Shioji", "authors": "Enno Shioji, Masayuki Arai", "title": "Neural Feature Embedding for User Response Prediction in Real-Time\n  Bidding (RTB)", "comments": null, "journal-ref": "Proc. of the Workshop on Social Media for Personalization and\n  Search (2017) 8-13", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of ad-targeting, predicting user responses is essential for many\napplications such as Real-Time Bidding (RTB). Many of the features available in\nthis domain are sparse categorical features. This presents a challenge\nespecially when the user responses to be predicted are rare, because each\nfeature will only have very few positive examples. Recently, neural embedding\ntechniques such as word2vec which learn distributed representations of words\nusing occurrence statistics in the corpus have been shown to be effective in\nmany Natural Language Processing tasks. In this paper, we use real-world data\nset to show that a similar technique can be used to learn distributed\nrepresentations of features from users' web history, and that such\nrepresentations can be used to improve the accuracy of commonly used models for\npredicting rare user responses.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 22:32:29 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 10:37:24 GMT"}, {"version": "v3", "created": "Wed, 26 Apr 2017 17:01:40 GMT"}, {"version": "v4", "created": "Tue, 9 May 2017 11:21:42 GMT"}, {"version": "v5", "created": "Wed, 17 May 2017 07:05:42 GMT"}, {"version": "v6", "created": "Thu, 18 May 2017 17:35:36 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Shioji", "Enno", ""], ["Arai", "Masayuki", ""]]}, {"id": "1702.00860", "submitter": "Jaimie Murdock", "authors": "Colin Allen and Hongliang Luo and Jaimie Murdock and Jianghuai Pu and\n  Xiaohong Wang and Yanjie Zhai and Kun Zhao", "title": "Topic Modeling the H\\`an di\\u{a}n Ancient Classics", "comments": "24 pages; 14 pages supplemental", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.DL cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ancient Chinese texts present an area of enormous challenge and opportunity\nfor humanities scholars interested in exploiting computational methods to\nassist in the development of new insights and interpretations of culturally\nsignificant materials. In this paper we describe a collaborative effort between\nIndiana University and Xi'an Jiaotong University to support exploration and\ninterpretation of a digital corpus of over 18,000 ancient Chinese documents,\nwhich we refer to as the \"Handian\" ancient classics corpus (H\\`an di\\u{a}n\ng\\u{u} j\\'i, i.e, the \"Han canon\" or \"Chinese classics\"). It contains classics\nof ancient Chinese philosophy, documents of historical and biographical\nsignificance, and literary works. We begin by describing the Digital Humanities\ncontext of this joint project, and the advances in humanities computing that\nmade this project feasible. We describe the corpus and introduce our\napplication of probabilistic topic modeling to this corpus, with attention to\nthe particular challenges posed by modeling ancient Chinese documents. We give\na specific example of how the software we have developed can be used to aid\ndiscovery and interpretation of themes in the corpus. We outline more advanced\nforms of computer-aided interpretation that are also made possible by the\nprogramming interface provided by our system, and the general implications of\nthese methods for understanding the nature of meaning in these texts.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 22:51:04 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Allen", "Colin", ""], ["Luo", "Hongliang", ""], ["Murdock", "Jaimie", ""], ["Pu", "Jianghuai", ""], ["Wang", "Xiaohong", ""], ["Zhai", "Yanjie", ""], ["Zhao", "Kun", ""]]}, {"id": "1702.00921", "submitter": "C Ravindranath Chowdary", "authors": "Shubham Varma, Neyshith Sameer and C. Ravindranath Chowdary", "title": "ReLiC: Entity Profiling by using Random Forest and Trustworthiness of a\n  Source - Technical Report", "comments": null, "journal-ref": null, "doi": "10.1007/s12046-019-1178-x", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The digital revolution has brought most of the world on the world wide web.\nThe data available on WWW has increased many folds in the past decade. Social\nnetworks, online clubs and organisations have come into existence. Information\nis extracted from these venues about a real world entity like a person,\norganisation, event, etc. However, this information may change over time, and\nthere is a need for the sources to be up-to-date. Therefore, it is desirable to\nhave a model to extract relevant data items from different sources and merge\nthem to build a complete profile of an entity (entity profiling). Further, this\nmodel should be able to handle incorrect or obsolete data items. In this paper,\nwe propose a novel method for completing a profile. We have developed a two\nphase method-1) The first phase (resolution phase) links records to the\nqueries. We have proposed and observed that the use of random forest for entity\nresolution increases the performance of the system as this has resulted in more\nrecords getting linked to the correct entity. Also, we used trustworthiness of\na source as a feature to the random forest. 2) The second phase selects the\nappropriate values from records to complete a profile based on our proposed\nselection criteria. We have used various metrics for measuring the performance\nof the resolution phase as well as for the overall ReLiC framework. It is\nestablished through our results that the use of biased sources has\nsignificantly improved the performance of the ReLiC framework. Experimental\nresults show that our proposed system, ReLiC outperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 07:04:13 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Varma", "Shubham", ""], ["Sameer", "Neyshith", ""], ["Chowdary", "C. Ravindranath", ""]]}, {"id": "1702.01032", "submitter": "Surendra Sedhai", "authors": "Surendra Sedhai, Aixin Sun", "title": "Semi-Supervised Spam Detection in Twitter Stream", "comments": "9", "journal-ref": null, "doi": "10.1109/TCSS.2017.2773581", "report-no": null, "categories": "cs.IR cs.CR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing techniques for spam detection on Twitter aim to identify and\nblock users who post spam tweets. In this paper, we propose a Semi-Supervised\nSpam Detection (S3D) framework for spam detection at tweet-level. The proposed\nframework consists of two main modules: spam detection module operating in\nreal-time mode, and model update module operating in batch mode. The spam\ndetection module consists of four light-weight detectors: (i) blacklisted\ndomain detector to label tweets containing blacklisted URLs, (ii)\nnear-duplicate detector to label tweets that are near-duplicates of confidently\npre-labeled tweets, (iii) reliable ham detector to label tweets that are posted\nby trusted users and that do not contain spammy words, and (iv)\nmulti-classifier based detector labels the remaining tweets. The information\nrequired by the detection module are updated in batch mode based on the tweets\nthat are labeled in the previous time window. Experiments on a large scale\ndataset show that the framework adaptively learns patterns of new spam\nactivities and maintain good accuracy for spam detection in a tweet stream.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 04:40:00 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Sedhai", "Surendra", ""], ["Sun", "Aixin", ""]]}, {"id": "1702.01076", "submitter": "Helge Holzmann", "authors": "Helge Holzmann, Avishek Anand", "title": "Tempas: Temporal Archive Search Based on Tags", "comments": "WWW 2016, Montreal, Quebec, Canada", "journal-ref": null, "doi": "10.1145/2872518.2890555", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited search and access patterns over Web archives have been well\ndocumented. One of the key reasons is the lack of understanding of the user\naccess patterns over such collections, which in turn is attributed to the lack\nof effective search interfaces. Current search interfaces for Web archives are\n(a) either purely navigational or (b) have sub-optimal search experience due to\nineffective retrieval models or query modeling. We identify that external\nlongitudinal resources, such as social bookmarking data, are crucial sources to\nidentify important and popular websites in the past. To this extent we present\nTempas, a tag-based temporal search engine for Web archives.\n  Websites are posted at specific times of interest on several external\nplatforms, such as bookmarking sites like Delicious. Attached tags not only act\nas relevant descriptors useful for retrieval, but also encode the time of\nrelevance. With Tempas we tackle the challenge of temporally searching a Web\narchive by indexing tags and time. We allow temporal selections for search\nterms, rank documents based on their popularity and also provide meaningful\nquery recommendations by exploiting tag-tag and tag-document co-occurrence\nstatistics in arbitrary time windows. Finally, Tempas operates as a fairly\nnon-invasive indexing framework. By not dealing with contents from the actual\nWeb archive it constitutes an attractive and low-overhead approach for quick\naccess into Web archives.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 16:47:46 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Holzmann", "Helge", ""], ["Anand", "Avishek", ""]]}, {"id": "1702.01090", "submitter": "Jaimie Murdock", "authors": "Jaimie Murdock and Colin Allen and Katy B\\\"orner and Robert Light and\n  Simon McAlister and Andrew Ravenscroft and Robert Rose and Doori Rose and Jun\n  Otsuka and David Bourget and John Lawrence and Chris Reed", "title": "Multi-level computational methods for interdisciplinary research in the\n  HathiTrust Digital Library", "comments": "revised, 29 pages, 3 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0184188", "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show how faceted search using a combination of traditional classification\nsystems and mixed-membership topic models can go beyond keyword search to\ninform resource discovery, hypothesis formulation, and argument extraction for\ninterdisciplinary research. Our test domain is the history and philosophy of\nscientific work on animal mind and cognition. The methods can be generalized to\nother research areas and ultimately support a system for semi-automatic\nidentification of argument structures. We provide a case study for the\napplication of the methods to the problem of identifying and extracting\narguments about anthropomorphism during a critical period in the development of\ncomparative psychology. We show how a combination of classification systems and\nmixed-membership models trained over large digital libraries can inform\nresource discovery in this domain. Through a novel approach of \"drill-down\"\ntopic modeling---simultaneously reducing both the size of the corpus and the\nunit of analysis---we are able to reduce a large collection of fulltext volumes\nto a much smaller set of pages within six focal volumes containing arguments of\ninterest to historians and philosophers of comparative psychology. The volumes\nidentified in this way did not appear among the first ten results of the\nkeyword search in the HathiTrust digital library and the pages bear the kind of\n\"close reading\" needed to generate original interpretations that is the heart\nof scholarly work in the humanities. Zooming back out, we provide a way to\nplace the books onto a map of science originally constructed from very\ndifferent data and for different purposes. The multilevel approach advances\nunderstanding of the intellectual and societal contexts in which writings are\ninterpreted.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 17:36:19 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 00:22:59 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Murdock", "Jaimie", ""], ["Allen", "Colin", ""], ["B\u00f6rner", "Katy", ""], ["Light", "Robert", ""], ["McAlister", "Simon", ""], ["Ravenscroft", "Andrew", ""], ["Rose", "Robert", ""], ["Rose", "Doori", ""], ["Otsuka", "Jun", ""], ["Bourget", "David", ""], ["Lawrence", "John", ""], ["Reed", "Chris", ""]]}, {"id": "1702.01159", "submitter": "Helge Holzmann", "authors": "Helge Holzmann, Wolfgang Nejdl, Avishek Anand", "title": "On the Applicability of Delicious for Temporal Search on Web Archives", "comments": "SIGIR 2016, Pisa, Italy", "journal-ref": null, "doi": "10.1145/2911451.2914724", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web archives are large longitudinal collections that store webpages from the\npast, which might be missing on the current live Web. Consequently, temporal\nsearch over such collections is essential for finding prominent missing\nwebpages and tasks like historical analysis. However, this has been challenging\ndue to the lack of popularity information and proper ground truth to evaluate\ntemporal retrieval models. In this paper we investigate the applicability of\nexternal longitudinal resources to identify important and popular websites in\nthe past and analyze the social bookmarking service Delicious for this purpose.\n  The timestamped bookmarks on Delicious provide explicit cues about popular\ntime periods in the past along with relevant descriptors. These are valuable to\nidentify important documents in the past for a given temporal query. Focusing\npurely on recall, we analyzed more than 12,000 queries and find that using\nDelicious yields average recall values from 46% up to 100%, when limiting\nourselves to the best represented queries in the considered dataset. This\nconstitutes an attractive and low-overhead approach for quick access into Web\narchives by not dealing with the actual contents.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 21:06:47 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Holzmann", "Helge", ""], ["Nejdl", "Wolfgang", ""], ["Anand", "Avishek", ""]]}, {"id": "1702.01516", "submitter": "Yifan Chen", "authors": "Yifan Chen and Xiang Zhao", "title": "Leveraging High-Dimensional Side Information for Top-N Recommendation", "comments": "the idea is changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-$N$ recommender systems typically utilize side information to address the\nproblem of data sparsity. As nowadays side information is growing towards high\ndimensionality, the performances of existing methods deteriorate in terms of\nboth effectiveness and efficiency, which imposes a severe technical challenge.\nIn order to take advantage of high-dimensional side information, we propose in\nthis paper an embedded feature selection method to facilitate top-$N$\nrecommendation. In particular, we propose to learn feature weights of side\ninformation, where zero-valued features are naturally filtered out. We also\nintroduce non-negativity and sparsity to the feature weights, to facilitate\nfeature selection and encourage low-rank structure. Two optimization problems\nare accordingly put forward, respectively, where the feature selection is\ntightly or loosely coupled with the learning procedure. Augmented Lagrange\nMultiplier and Alternating Direction Method are applied to efficiently solve\nthe problems. Experiment results demonstrate the superior recommendation\nquality of the proposed algorithm to that of the state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 07:23:47 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 13:22:38 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Chen", "Yifan", ""], ["Zhao", "Xiang", ""]]}, {"id": "1702.01520", "submitter": "Shaohua Li", "authors": "Shaohua Li, Tat-Seng Chua", "title": "Document Visualization using Topic Clouds", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally a document is visualized by a word cloud. Recently, distributed\nrepresentation methods for documents have been developed, which map a document\nto a set of topic embeddings. Visualizing such a representation is useful to\npresent the semantics of a document in higher granularity; it is also\nchallenging, as there are multiple topics, each containing multiple words. We\npropose to visualize a set of topics using Topic Cloud, which is a pie chart\nconsisting of topic slices, where each slice contains important words in this\ntopic. To make important topics/words visually prominent, the sizes of topic\nslices and word fonts are proportional to their importance in the document. A\ntopic cloud can help the user quickly evaluate the quality of derived document\nrepresentations. For NLP practitioners, It can be used to qualitatively compare\nthe topic quality of different document representation algorithms, or to\ninspect how model parameters impact the derived representations.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 07:38:03 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Li", "Shaohua", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1702.01713", "submitter": "Nikolaos Polatidis Mr", "authors": "Nikolaos Polatidis, Christos K. Georgiadis", "title": "A dynamic multi-level collaborative filtering method for improved\n  recommendations", "comments": null, "journal-ref": "Computer Standards & Interfaces, 51, 14-21 (2017)", "doi": "10.1016/j.csi.2016.10.014", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most used approaches for providing recommendations in various\nonline environments such as e-commerce is collaborative filtering. Although,\nthis is a simple method for recommending items or services, accuracy and\nquality problems still exist. Thus, we propose a dynamic multi-level\ncollaborative filtering method that improves the quality of the\nrecommendations. The proposed method is based on positive and negative\nadjustments and can be used in different domains that utilize collaborative\nfiltering to increase the quality of the user experience. Furthermore, the\neffectiveness of the proposed method is shown by providing an extensive\nexperimental evaluation based on three real datasets and by comparisons to\nalternative methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 17:19:07 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Polatidis", "Nikolaos", ""], ["Georgiadis", "Christos K.", ""]]}, {"id": "1702.01717", "submitter": "Zeeshan Malik Khawar", "authors": "Zeeshan Khawar Malik, Mo Kobrosli and Peter Maas", "title": "Search Intelligence: Deep Learning For Dominant Category Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks, and specifically fully-connected convolutional neural\nnetworks are achieving remarkable results across a wide variety of domains.\nThey have been trained to achieve state-of-the-art performance when applied to\nproblems such as speech recognition, image classification, natural language\nprocessing and bioinformatics. Most of these deep learning models when applied\nto classification employ the softmax activation function for prediction and aim\nto minimize cross-entropy loss. In this paper, we have proposed a supervised\nmodel for dominant category prediction to improve search recall across all eBay\nclassifieds platforms. The dominant category label for each query in the last\n90 days is first calculated by summing the total number of collaborative clicks\namong all categories. The category having the highest number of collaborative\nclicks for the given query will be considered its dominant category. Second,\neach query is transformed to a numeric vector by mapping each unique word in\nthe query document to a unique integer value; all padded to equal length based\non the maximum document length within the pre-defined vocabulary size. A\nfully-connected deep convolutional neural network (CNN) is then applied for\nclassification. The proposed model achieves very high classification accuracy\ncompared to other state-of-the-art machine learning techniques.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 17:27:12 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Malik", "Zeeshan Khawar", ""], ["Kobrosli", "Mo", ""], ["Maas", "Peter", ""]]}, {"id": "1702.01739", "submitter": "Karim Banawan", "authors": "Karim Banawan, Sennur Ulukus", "title": "Multi-Message Private Information Retrieval: Capacity Results and\n  Near-Optimal Schemes", "comments": "Submitted to IEEE Transactions on Information Theory, February 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multi-message private information retrieval (MPIR)\nfrom $N$ non-communicating replicated databases. In MPIR, the user is\ninterested in retrieving $P$ messages out of $M$ stored messages without\nleaking the identity of the retrieved messages. The information-theoretic sum\ncapacity of MPIR $C_s^P$ is the maximum number of desired message symbols that\ncan be retrieved privately per downloaded symbol. For the case $P \\geq\n\\frac{M}{2}$, we determine the exact sum capacity of MPIR as\n$C_s^P=\\frac{1}{1+\\frac{M-P}{PN}}$. The achievable scheme in this case is based\non downloading MDS-coded mixtures of all messages. For $P \\leq \\frac{M}{2}$, we\ndevelop lower and upper bounds for all $M,P,N$. These bounds match if the total\nnumber of messages $M$ is an integer multiple of the number of desired messages\n$P$, i.e., $\\frac{M}{P} \\in \\mathbb{N}$. In this case,\n$C_s^P=\\frac{1-\\frac{1}{N}}{1-(\\frac{1}{N})^{M/P}}$. The achievable scheme in\nthis case generalizes the single-message capacity achieving scheme to have\nunbalanced number of stages per round of download. For all the remaining cases,\nthe difference between the lower and upper bound is at most $0.0082$, which\noccurs for $M=5$, $P=2$, $N=2$. Our results indicate that joint retrieval of\ndesired messages is more efficient than successive use of single-message\nretrieval schemes.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 18:50:02 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Banawan", "Karim", ""], ["Ulukus", "Sennur", ""]]}, {"id": "1702.01925", "submitter": "Ibrahim Abu El-khair Dr. <", "authors": "Ibrahim Abu El-Khair", "title": "Effects of Stop Words Elimination for Arabic Information Retrieval: A\n  Comparative Study", "comments": null, "journal-ref": "Abu El-Khair, Ibrahim. (2006). Effects of stop words elimination\n  for Arabic information retrieval: a comparative study. International Journal\n  of Computing & Information Sciences 4 (3), 119-133", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of three stop words lists for Arabic Information\nRetrieval---General Stoplist, Corpus-Based Stoplist, Combined Stoplist ---were\ninvestigated in this study. Three popular weighting schemes were examined: the\ninverse document frequency weight, probabilistic weighting, and statistical\nlanguage modelling. The Idea is to combine the statistical approaches with\nlinguistic approaches to reach an optimal performance, and compare their effect\non retrieval. The LDC (Linguistic Data Consortium) Arabic Newswire data set was\nused with the Lemur Toolkit. The Best Match weighting scheme used in the Okapi\nretrieval system had the best overall performance of the three weighting\nalgorithms used in the study, stoplists improved retrieval effectiveness\nespecially when used with the BM25 weight. The overall performance of a general\nstoplist was better than the other two lists.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 08:49:58 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["El-Khair", "Ibrahim Abu", ""]]}, {"id": "1702.01978", "submitter": "Navid Rekabsaz", "authors": "Navid Rekabsaz, Mihai Lupu, Artem Baklanov, Allan Hanbury, Alexander\n  Duer, Linda Anderson", "title": "Volatility Prediction using Financial Disclosures Sentiments with Word\n  Embedding-based IR Models", "comments": null, "journal-ref": null, "doi": "10.18653/v1/P17-1157", "report-no": null, "categories": "cs.IR cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volatility prediction--an essential concept in financial markets--has\nrecently been addressed using sentiment analysis methods. We investigate the\nsentiment of annual disclosures of companies in stock markets to forecast\nvolatility. We specifically explore the use of recent Information Retrieval\n(IR) term weighting models that are effectively extended by related terms using\nword embeddings. In parallel to textual information, factual market data have\nbeen widely used as the mainstream approach to forecast market risk. We\ntherefore study different fusion methods to combine text and market data\nresources. Our word embedding-based approach significantly outperforms\nstate-of-the-art methods. In addition, we investigate the characteristics of\nthe reports of the companies in different financial sectors.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 12:18:13 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 14:01:29 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Rekabsaz", "Navid", ""], ["Lupu", "Mihai", ""], ["Baklanov", "Artem", ""], ["Hanbury", "Allan", ""], ["Duer", "Alexander", ""], ["Anderson", "Linda", ""]]}, {"id": "1702.02107", "submitter": "Hui Guan", "authors": "Hui Guan, Thanos Gentimis, Hamid Krim, James Keiser", "title": "First Study on Data Readiness Level", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.GL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the idea of Data Readiness Level (DRL) to measure the relative\nrichness of data to answer specific questions often encountered by data\nscientists. We first approach the problem in its full generality explaining its\ndesired mathematical properties and applications and then we propose and study\ntwo DRL metrics. Specifically, we define DRL as a function of at least four\nproperties of data: Noisiness, Believability, Relevance, and Coherence. The\ninformation-theoretic based metrics, Cosine Similarity and Document Disparity,\nare proposed as indicators of Relevance and Coherence for a piece of data. The\nproposed metrics are validated through a text-based experiment using Twitter\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 15:23:41 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Guan", "Hui", ""], ["Gentimis", "Thanos", ""], ["Krim", "Hamid", ""], ["Keiser", "James", ""]]}, {"id": "1702.02287", "submitter": "Baichuan Zhang", "authors": "Baichuan Zhang, Mohammad Al Hasan", "title": "Name Disambiguation in Anonymized Graphs using Network Embedding", "comments": "The 26th ACM International Conference on Information and Knowledge\n  Management (CIKM 2017) research track full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world, our DNA is unique but many people share names. This phenomenon\noften causes erroneous aggregation of documents of multiple persons who are\nnamesake of one another. Such mistakes deteriorate the performance of document\nretrieval, web search, and more seriously, cause improper attribution of credit\nor blame in digital forensic. To resolve this issue, the name disambiguation\ntask is designed which aims to partition the documents associated with a name\nreference such that each partition contains documents pertaining to a unique\nreal-life person. Existing solutions to this task substantially rely on feature\nengineering, such as biographical feature extraction, or construction of\nauxiliary features from Wikipedia. However, for many scenarios, such features\nmay be costly to obtain or unavailable due to the risk of privacy violation. In\nthis work, we propose a novel name disambiguation method. Our proposed method\nis non-intrusive of privacy because instead of using attributes pertaining to a\nreal-life person, our method leverages only relational data in the form of\nanonymized graphs. In the methodological aspect, the proposed method uses a\nnovel representation learning model to embed each document in a low dimensional\nvector space where name disambiguation can be solved by a hierarchical\nagglomerative clustering algorithm. Our experimental results demonstrate that\nthe proposed method is significantly better than the existing name\ndisambiguation methods working in a similar setting.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 04:54:09 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 00:40:44 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 14:29:03 GMT"}, {"version": "v4", "created": "Sat, 9 Sep 2017 23:05:04 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Zhang", "Baichuan", ""], ["Hasan", "Mohammad Al", ""]]}, {"id": "1702.02737", "submitter": "Xuan-Son Vu", "authors": "Xuan-Son Vu, Seong-Bae Park", "title": "Mining User/Movie Preferred Features Based on Reviews for Video\n  Recommendation System", "comments": "The 2nd Workshop on Future Researches of Computer Science and\n  Engineering, Kyungpook National University, pp. 21-24, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an approach for mining user preferences and\nrecommendation based on reviews. There have been various studies worked on\nrecommendation problem. However, most of the studies beyond one aspect user\ngenerated- content such as user ratings, user feedback and so on to state user\npreferences. There is a prob- lem in one aspect mining is lacking for stating\nuser preferences. As a demonstration, in collaborative filter recommendation,\nwe try to figure out the preference trend of crowded users, then use that trend\nto predict current user preference. Therefore, there is a gap between real user\npreferences and the trend of the crowded people. Additionally, user preferences\ncan be addressed from mining user reviews since user often comment about\nvarious aspects of products. To solve this problem, we mainly focus on mining\nproduct aspects and user aspects inside user reviews to directly state user\npreferences. We also take into account Social Network Analysis for cold-start\nitem problem. With cold-start user problem, collaborative filter algorithm is\nemployed in our work. The framework is general enough to be applied to\ndifferent recommendation domains. Theoretically, our method would achieve a\nsignificant enhancement.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 08:03:16 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Vu", "Xuan-Son", ""], ["Park", "Seong-Bae", ""]]}, {"id": "1702.02817", "submitter": "Immanuel Bayer", "authors": "Immanuel Bayer, Uwe Nagel, Steffen Rendle", "title": "Graph Based Relational Features for Collective Classification", "comments": "Pacific-Asia Conference on Knowledge Discovery and Data Mining", "journal-ref": null, "doi": "10.1007/978-3-319-18032-8_35", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical Relational Learning (SRL) methods have shown that classification\naccuracy can be improved by integrating relations between samples. Techniques\nsuch as iterative classification or relaxation labeling achieve this by\npropagating information between related samples during the inference process.\nWhen only a few samples are labeled and connections between samples are sparse,\ncollective inference methods have shown large improvements over standard\nfeature-based ML methods. However, in contrast to feature based ML, collective\ninference methods require complex inference procedures and often depend on the\nstrong assumption of label consistency among related samples. In this paper, we\nintroduce new relational features for standard ML methods by extracting\ninformation from direct and indirect relations. We show empirically on three\nstandard benchmark datasets that our relational features yield results\ncomparable to collective inference methods. Finally we show that our proposal\noutperforms these methods when additional information is available.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 12:58:23 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Bayer", "Immanuel", ""], ["Nagel", "Uwe", ""], ["Rendle", "Steffen", ""]]}, {"id": "1702.03222", "submitter": "Pranjul Yadav", "authors": "Pranjul Yadav, Michael Steinbach, Vipin Kumar, Gyorgy Simon", "title": "Mining Electronic Health Records: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuously increasing cost of the US healthcare system has received\nsignificant attention. Central to the ideas aimed at curbing this trend is the\nuse of technology, in the form of the mandate to implement electronic health\nrecords (EHRs). EHRs consist of patient information such as demographics,\nmedications, laboratory test results, diagnosis codes and procedures. Mining\nEHRs could lead to improvement in patient health management as EHRs contain\ndetailed information related to disease prognosis for large patient\npopulations. In this manuscript, we provide a structured and comprehensive\noverview of data mining techniques for modeling EHR data. We first provide a\ndetailed understanding of the major application areas to which EHR mining has\nbeen applied and then discuss the nature of EHR data and its accompanying\nchallenges. Next, we describe major approaches used for EHR mining, the metrics\nassociated with EHRs, and the various study designs. With this foundation, we\nthen provide a systematic and methodological organization of existing data\nmining techniques used to model EHRs and discuss ideas for future research. We\nconclude this survey with a comprehensive summary of clinical data mining\napplications of EHR data, as illustrated in the online supplement.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 17:33:48 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 17:08:13 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Yadav", "Pranjul", ""], ["Steinbach", "Michael", ""], ["Kumar", "Vipin", ""], ["Simon", "Gyorgy", ""]]}, {"id": "1702.03859", "submitter": "Samuel L. Smith", "authors": "Samuel L. Smith, David H. P. Turban, Steven Hamblin and Nils Y.\n  Hammerla", "title": "Offline bilingual word vectors, orthogonal transformations and the\n  inverted softmax", "comments": "Accepted to conference track at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usually bilingual word vectors are trained \"online\". Mikolov et al. showed\nthey can also be found \"offline\", whereby two pre-trained embeddings are\naligned with a linear transformation, using dictionaries compiled from expert\nknowledge. In this work, we prove that the linear transformation between two\nspaces should be orthogonal. This transformation can be obtained using the\nsingular value decomposition. We introduce a novel \"inverted softmax\" for\nidentifying translation pairs, with which we improve the precision @1 of\nMikolov's original mapping from 34% to 43%, when translating a test set\ncomposed of both common and rare English words into Italian. Orthogonal\ntransformations are more robust to noise, enabling us to learn the\ntransformation without expert bilingual signal by constructing a\n\"pseudo-dictionary\" from the identical character strings which appear in both\nlanguages, achieving 40% precision on the same test set. Finally, we extend our\nmethod to retrieve the true translations of English sentences from a corpus of\n200k Italian sentences with a precision @1 of 68%.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 16:31:06 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Smith", "Samuel L.", ""], ["Turban", "David H. P.", ""], ["Hamblin", "Steven", ""], ["Hammerla", "Nils Y.", ""]]}, {"id": "1702.04815", "submitter": "Konstantinos Bougiatiotis", "authors": "Konstantinos Bougiatiotis and Theodore Giannakopoulos", "title": "Multimodal Content Representation and Similarity Ranking of Movies", "comments": "Preliminary work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine the existence of correlation between movie\nsimilarity and low level features from respective movie content. In particular,\nwe demonstrate the extraction of multi-modal representation models of movies\nbased on subtitles, audio and metadata mining. We emphasize our research in\ntopic modeling of movies based on their subtitles. In order to demonstrate the\nproposed content representation approach, we have built a small dataset of 160\nwidely known movies. We assert movie similarities, as propagated by the\nsingular modalities and fusion models, in the form of recommendation rankings.\nWe showcase a novel topic model browser for movies that allows for exploration\nof the different aspects of similarities between movies and an information\nretrieval system for movie similarity based on multi-modal content.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 23:31:44 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 13:34:21 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Bougiatiotis", "Konstantinos", ""], ["Giannakopoulos", "Theodore", ""]]}, {"id": "1702.05042", "submitter": "Bhaskar Mitra", "authors": "Bhaskar Mitra, Fernando Diaz and Nick Craswell", "title": "Luandri: a Clean Lua Interface to the Indri Search Engine", "comments": "Under review for SIGIR'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the information retrieval (IR) community has witnessed the\nfirst successful applications of deep neural network models to short-text\nmatching and ad-hoc retrieval. It is exciting to see the research on deep\nneural networks and IR converge on these tasks of shared interest. However, the\ntwo communities have less in common when it comes to the choice of programming\nlanguages. Indri, an indexing framework popularly used by the IR community, is\nwritten in C++, while Torch, a popular machine learning library for deep\nlearning, is written in the light-weight scripting language Lua. To bridge this\ngap, we introduce Luandri (pronounced \"laundry\"), a simple interface for\nexposing the search capabilities of Indri to Torch models implemented in Lua.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 16:30:06 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Mitra", "Bhaskar", ""], ["Diaz", "Fernando", ""], ["Craswell", "Nick", ""]]}, {"id": "1702.05181", "submitter": "Akshay Soni", "authors": "Akshay Soni and Yashar Mehdad", "title": "RIPML: A Restricted Isometry Property based Approach to Multilabel\n  Learning", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multilabel learning problem with large number of labels, features, and\ndata-points has generated a tremendous interest recently. A recurring theme of\nthese problems is that only a few labels are active in any given datapoint as\ncompared to the total number of labels. However, only a small number of\nexisting work take direct advantage of this inherent extreme sparsity in the\nlabel space. By the virtue of Restricted Isometry Property (RIP), satisfied by\nmany random ensembles, we propose a novel procedure for multilabel learning\nknown as RIPML. During the training phase, in RIPML, labels are projected onto\na random low-dimensional subspace followed by solving a least-square problem in\nthis subspace. Inference is done by a k-nearest neighbor (kNN) based approach.\nWe demonstrate the effectiveness of RIPML by conducting extensive simulations\nand comparing results with the state-of-the-art linear dimensionality reduction\nbased approaches.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 23:08:50 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Soni", "Akshay", ""], ["Mehdad", "Yashar", ""]]}, {"id": "1702.05446", "submitter": "Arda Antikacioglu", "authors": "Arda Antikacioglu, R Ravi", "title": "Network Flow Based Post Processing for Sales Diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is a broad and powerful framework for building\nrecommendation systems that has seen widespread adoption. Over the past decade,\nthe propensity of such systems for favoring popular products and thus creating\necho chambers have been observed. This has given rise to an active area of\nresearch that seeks to diversify recommendations generated by such algorithms.\n  We address the problem of increasing diversity in recommendation systems that\nare based on collaborative filtering that use past ratings to predicting a\nrating quality for potential recommendations. Following our earlier work, we\nformulate recommendation system design as a subgraph selection problem from a\ncandidate super-graph of potential recommendations where both diversity and\nrating quality are explicitly optimized: (1) On the modeling side, we define a\nnew flexible notion of diversity that allows a system designer to prescribe the\nnumber of recommendations each item should receive, and smoothly penalizes\ndeviations from this distribution. (2) On the algorithmic side, we show that\nminimum-cost network flow methods yield fast algorithms in theory and practice\nfor designing recommendation subgraphs that optimize this notion of diversity.\n(3) On the empirical side, we show the effectiveness of our new model and\nmethod to increase diversity while maintaining high rating quality in standard\nrating data sets from Netflix and MovieLens.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 17:18:58 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Antikacioglu", "Arda", ""], ["Ravi", "R", ""]]}, {"id": "1702.06151", "submitter": "Tal Yarkoni", "authors": "Quinten McNamara, Alejandro de la Vega, and Tal Yarkoni", "title": "Developing a comprehensive framework for multimodal feature extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction is a critical component of many applied data science\nworkflows. In recent years, rapid advances in artificial intelligence and\nmachine learning have led to an explosion of feature extraction tools and\nservices that allow data scientists to cheaply and effectively annotate their\ndata along a vast array of dimensions---ranging from detecting faces in images\nto analyzing the sentiment expressed in coherent text. Unfortunately, the\nproliferation of powerful feature extraction services has been mirrored by a\ncorresponding expansion in the number of distinct interfaces to feature\nextraction services. In a world where nearly every new service has its own API,\ndocumentation, and/or client library, data scientists who need to combine\ndiverse features obtained from multiple sources are often forced to write and\nmaintain ever more elaborate feature extraction pipelines. To address this\nchallenge, we introduce a new open-source framework for comprehensive\nmultimodal feature extraction. Pliers is an open-source Python package that\nsupports standardized annotation of diverse data types (video, images, audio,\nand text), and is expressly with both ease-of-use and extensibility in mind.\nUsers can apply a wide range of pre-existing feature extraction tools to their\ndata in just a few lines of Python code, and can also easily add their own\ncustom extractors by writing modular classes. A graph-based API enables rapid\ndevelopment of complex feature extraction pipelines that output results in a\nsingle, standardized format. We describe the package's architecture, detail its\nmajor advantages over previous feature extraction toolboxes, and use a sample\napplication to a large functional MRI dataset to illustrate how pliers can\nsignificantly reduce the time and effort required to construct sophisticated\nfeature extraction workflows while increasing code clarity and maintainability.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 19:22:21 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["McNamara", "Quinten", ""], ["de la Vega", "Alejandro", ""], ["Yarkoni", "Tal", ""]]}, {"id": "1702.06157", "submitter": "Xue Jiang", "authors": "Xue Jiang, H. C. So, X. Liu", "title": "Robust Phase Retrieval via ADMM with Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An outlier-resistance phase retrieval algorithm based on alternating\ndirection method of multipliers (ADMM) is devised in this letter. Instead of\nthe widely used least squares criterion that is only optimal for Gaussian noise\nenvironment, we adopt the least absolute deviation criterion to enhance the\nrobustness against outliers. Considering both intensity- and amplitude-based\nobservation models, the framework of ADMM is developed to solve the resulting\nnon-differentiable optimization problems. It is demonstrated that the core\nsubproblem of ADMM is the proximity operator of the L1-norm, which can be\ncomputed efficiently by soft-thresholding in each iteration. Simulation results\nare provided to validate the accuracy and efficiency of the proposed approach\ncompared to the existing schemes.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 02:57:16 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Jiang", "Xue", ""], ["So", "H. C.", ""], ["Liu", "X.", ""]]}, {"id": "1702.06176", "submitter": "Ilya Safro", "authors": "Justin Sybrandt, Michael Shtutman, Ilya Safro", "title": "MOLIERE: Automatic Biomedical Hypothesis Generation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.SI q-bio.QM stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis generation is becoming a crucial time-saving technique which\nallows biomedical researchers to quickly discover implicit connections between\nimportant concepts. Typically, these systems operate on domain-specific\nfractions of public medical data. MOLIERE, in contrast, utilizes information\nfrom over 24.5 million documents. At the heart of our approach lies a\nmulti-modal and multi-relational network of biomedical objects extracted from\nseveral heterogeneous datasets from the National Center for Biotechnology\nInformation (NCBI). These objects include but are not limited to scientific\npapers, keywords, genes, proteins, diseases, and diagnoses. We model hypotheses\nusing Latent Dirichlet Allocation applied on abstracts found near shortest\npaths discovered within this network, and demonstrate the effectiveness of\nMOLIERE by performing hypothesis generation on historical data. Our network,\nimplementation, and resulting data are all publicly available for the broad\nscientific community.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 21:09:44 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 22:29:28 GMT"}, {"version": "v3", "created": "Wed, 31 May 2017 19:34:47 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Sybrandt", "Justin", ""], ["Shtutman", "Michael", ""], ["Safro", "Ilya", ""]]}, {"id": "1702.06216", "submitter": "Michael Bloodgood", "authors": "Alan Mishler, Kevin Wonus, Wendy Chambers and Michael Bloodgood", "title": "Filtering Tweets for Social Unrest", "comments": "7 pages, 8 figures, 3 tables; published in Proceedings of the 2017\n  IEEE 11th International Conference on Semantic Computing (ICSC), San Diego,\n  CA, USA, pages 17-23, January 2017", "journal-ref": "In Proceedings of the 2017 IEEE 11th International Conference on\n  Semantic Computing (ICSC), pages 17-23, San Diego, CA, USA, January 2017.\n  IEEE", "doi": "10.1109/ICSC.2017.75", "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the events of the Arab Spring, there has been increased interest in\nusing social media to anticipate social unrest. While efforts have been made\ntoward automated unrest prediction, we focus on filtering the vast volume of\ntweets to identify tweets relevant to unrest, which can be provided to\ndownstream users for further analysis. We train a supervised classifier that is\nable to label Arabic language tweets as relevant to unrest with high\nreliability. We examine the relationship between training data size and\nperformance and investigate ways to optimize the model building process while\nminimizing cost. We also explore how confidence thresholds can be set to\nachieve desired levels of performance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 23:48:39 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 22:37:35 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Mishler", "Alan", ""], ["Wonus", "Kevin", ""], ["Chambers", "Wendy", ""], ["Bloodgood", "Michael", ""]]}, {"id": "1702.06247", "submitter": "Han Xiao Almighty", "authors": "Han Xiao, Lian Meng", "title": "SAR: Semantic Analysis for Recommendation", "comments": "Submitting to IJCAI-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation system is a common demand in daily life and matrix completion\nis a widely adopted technique for this task. However, most matrix completion\nmethods lack semantic interpretation and usually result in weak-semantic\nrecommendations. To this end, this paper proposes a $S$emantic $A$nalysis\napproach for $R$ecommendation systems $(SAR)$, which applies a two-level\nhierarchical generative process that assigns semantic properties and categories\nfor user and item. $SAR$ learns semantic representations of users/items merely\nfrom user ratings on items, which offers a new path to recommendation by\nsemantic matching with the learned representations. Extensive experiments\ndemonstrate $SAR$ outperforms other state-of-the-art baselines substantially.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 03:09:10 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 02:20:47 GMT"}, {"version": "v3", "created": "Sat, 2 Dec 2017 11:29:58 GMT"}, {"version": "v4", "created": "Sat, 16 Dec 2017 11:18:33 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Xiao", "Han", ""], ["Meng", "Lian", ""]]}, {"id": "1702.06383", "submitter": "Biswa Sengupta", "authors": "Y Qian and E Vazquez and B Sengupta", "title": "Differential Geometric Retrieval of Deep Features", "comments": "5th ICDM Workshop on High Dimensional Data Mining (HDM 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing images to recommend items from an image-inventory is a subject of\ncontinued interest. Added with the scalability of deep-learning architectures\nthe once `manual' job of hand-crafting features have been largely alleviated,\nand images can be compared according to features generated from a deep\nconvolutional neural network. In this paper, we compare distance metrics (and\ndivergences) to rank features generated from a neural network, for\ncontent-based image retrieval. Specifically, after modelling individual images\nusing approximations of mixture models or sparse covariance estimators, we\nresort to their information-theoretic and Riemann geometric comparisons. We\nshow that using approximations of mixture models enable us to compute a\ndistance measure based on the Wasserstein metric that requires less effort than\nother computationally intensive optimal transport plans; finally, an affine\ninvariant metric is used to compare the optimal transport metric to its Riemann\ngeometric counterpart -- we conclude that although expensive, retrieval metric\nbased on Wasserstein geometry is more suitable than information theoretic\ncomparison of images. In short, we combine GPU scalability in learning deep\nfeature vectors with statistically efficient metrics that we foresee being\nutilised in a commercial setting.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 13:55:47 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 09:01:35 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Qian", "Y", ""], ["Vazquez", "E", ""], ["Sengupta", "B", ""]]}, {"id": "1702.06467", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Carlos-Emiliano Gonz\\'alez-Gallardo, Juan-Manuel Torres-Moreno,\n  Azucena Montes Rend\\'on and Gerardo Sierra", "title": "Efficient Social Network Multilingual Classification using Character,\n  POS n-grams and Dynamic Normalization", "comments": "8 pages, 6 figures, Conference paper", "journal-ref": "Proceedings of the 8th International Joint Conference on Knowledge\n  Discovery, Knowledge Engineering and Knowledge Management, Vol 1: KDIR,\n  307-314, 2016, Porto, Portugal", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a dynamic normalization process applied to social\nnetwork multilingual documents (Facebook and Twitter) to improve the\nperformance of the Author profiling task for short texts. After the\nnormalization process, $n$-grams of characters and n-grams of POS tags are\nobtained to extract all the possible stylistic information encoded in the\ndocuments (emoticons, character flooding, capital letters, references to other\nusers, hyperlinks, hashtags, etc.). Experiments with SVM showed up to 90% of\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 16:26:54 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Gonz\u00e1lez-Gallardo", "Carlos-Emiliano", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["Rend\u00f3n", "Azucena Montes", ""], ["Sierra", "Gerardo", ""]]}, {"id": "1702.06478", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Xavier Bost, Ilaria Brunetti, Luis Adri\\'an Cabrera-Diego,\n  Jean-Val\\`ere Cossu, Andr\\'ea Linhares, Mohamed Morchid, Juan-Manuel\n  Torres-Moreno, Marc El-B\\`eze, Richard Dufour", "title": "Syst\\`emes du LIA \\`a DEFT'13", "comments": "12 pages, 3 tables, (Paper in French)", "journal-ref": "Proceedings of the Ninth DEFT Workshop, DEFT2013, Les\n  Sables-d'Olonne, France, 21st June 2013", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2013 D\\'efi de Fouille de Textes (DEFT) campaign is interested in two\ntypes of language analysis tasks, the document classification and the\ninformation extraction in the specialized domain of cuisine recipes. We present\nthe systems that the LIA has used in DEFT 2013. Our systems show interesting\nresults, even though the complexity of the proposed tasks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 17:14:56 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Bost", "Xavier", ""], ["Brunetti", "Ilaria", ""], ["Cabrera-Diego", "Luis Adri\u00e1n", ""], ["Cossu", "Jean-Val\u00e8re", ""], ["Linhares", "Andr\u00e9a", ""], ["Morchid", "Mohamed", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["El-B\u00e8ze", "Marc", ""], ["Dufour", "Richard", ""]]}, {"id": "1702.06510", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Luis Adri\\'an Cabrera-Diego, St\\'ephane Huet, Bassam Jabaian,\n  Alejandro Molina, Juan-Manuel Torres-Moreno, Marc El-B\\`eze, Barth\\'el\\'emy\n  Durette", "title": "Algorithmes de classification et d'optimisation: participation du\n  LIA/ADOC \\'a DEFT'14", "comments": "8 pages, 3 tables, Conference paper (in French)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This year, the DEFT campaign (D\\'efi Fouilles de Textes) incorporates a task\nwhich aims at identifying the session in which articles of previous TALN\nconferences were presented. We describe the three statistical systems developed\nat LIA/ADOC for this task. A fusion of these systems enables us to obtain\ninteresting results (micro-precision score of 0.76 measured on the test corpus)\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 18:24:52 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Cabrera-Diego", "Luis Adri\u00e1n", ""], ["Huet", "St\u00e9phane", ""], ["Jabaian", "Bassam", ""], ["Molina", "Alejandro", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["El-B\u00e8ze", "Marc", ""], ["Durette", "Barth\u00e9l\u00e9my", ""]]}, {"id": "1702.06663", "submitter": "Saurav Ghosh", "authors": "Saurav Ghosh, Prithwish Chakraborty, Bryan L. Lewis, Maimuna S.\n  Majumder, Emily Cohn, John S. Brownstein, Madhav V. Marathe, Naren\n  Ramakrishnan", "title": "Guided Deep List: Automating the Generation of Epidemiological Line\n  Lists from Open Sources", "comments": "This paper has been submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time monitoring and responses to emerging public health threats rely on\nthe availability of timely surveillance data. During the early stages of an\nepidemic, the ready availability of line lists with detailed tabular\ninformation about laboratory-confirmed cases can assist epidemiologists in\nmaking reliable inferences and forecasts. Such inferences are crucial to\nunderstand the epidemiology of a specific disease early enough to stop or\ncontrol the outbreak. However, construction of such line lists requires\nconsiderable human supervision and therefore, difficult to generate in\nreal-time. In this paper, we motivate Guided Deep List, the first tool for\nbuilding automated line lists (in near real-time) from open source reports of\nemerging disease outbreaks. Specifically, we focus on deriving epidemiological\ncharacteristics of an emerging disease and the affected population from reports\nof illness. Guided Deep List uses distributed vector representations (ala\nword2vec) to discover a set of indicators for each line list feature. This\ndiscovery of indicators is followed by the use of dependency parsing based\ntechniques for final extraction in tabular form. We evaluate the performance of\nGuided Deep List against a human annotated line list provided by HealthMap\ncorresponding to MERS outbreaks in Saudi Arabia. We demonstrate that Guided\nDeep List extracts line list features with increased accuracy compared to a\nbaseline method. We further show how these automatically extracted line list\nfeatures can be used for making epidemiological inferences, such as inferring\ndemographics and symptoms-to-hospitalization period of affected individuals.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 03:14:36 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Ghosh", "Saurav", ""], ["Chakraborty", "Prithwish", ""], ["Lewis", "Bryan L.", ""], ["Majumder", "Maimuna S.", ""], ["Cohn", "Emily", ""], ["Brownstein", "John S.", ""], ["Marathe", "Madhav V.", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1702.06777", "submitter": "David Sanchez", "authors": "Gonzalo Donoso, David Sanchez", "title": "Dialectometric analysis of language variation in Twitter", "comments": "10 pages, 7 figures, 1 table. Accepted to VarDial 2017", "journal-ref": "Proceedings of the Fourth Workshop on NLP for Similar Languages,\n  Varieties and Dialects (VarDial), pp. 16-25, 2017", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, microblogging platforms such as Twitter have given\nrise to a deluge of textual data that can be used for the analysis of informal\ncommunication between millions of individuals. In this work, we propose an\ninformation-theoretic approach to geographic language variation using a corpus\nbased on Twitter. We test our models with tens of concepts and their associated\nkeywords detected in Spanish tweets geolocated in Spain. We employ\ndialectometric measures (cosine similarity and Jensen-Shannon divergence) to\nquantify the linguistic distance on the lexical level between cells created in\na uniform grid over the map. This can be done for a single concept or in the\ngeneral case taking into account an average of the considered variants. The\nlatter permits an analysis of the dialects that naturally emerge from the data.\nInterestingly, our results reveal the existence of two dialect macrovarieties.\nThe first group includes a region-specific speech spoken in small towns and\nrural areas whereas the second cluster encompasses cities that tend to use a\nmore uniform variety. Since the results obtained with the two different metrics\nqualitatively agree, our work suggests that social media corpora can be\nefficiently used for dialectometric analyses.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 12:42:06 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Donoso", "Gonzalo", ""], ["Sanchez", "David", ""]]}, {"id": "1702.06875", "submitter": "Arman Cohan", "authors": "Arman Cohan, Sydney Young, Andrew Yates, Nazli Goharian", "title": "Triaging Content Severity in Online Mental Health Forums", "comments": "Accepted for publication in Journal of the Association for\n  Information Science and Technology (2017)", "journal-ref": null, "doi": "10.1002/asi.23865", "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental health forums are online communities where people express their issues\nand seek help from moderators and other users. In such forums, there are often\nposts with severe content indicating that the user is in acute distress and\nthere is a risk of attempted self-harm. Moderators need to respond to these\nsevere posts in a timely manner to prevent potential self-harm. However, the\nlarge volume of daily posted content makes it difficult for the moderators to\nlocate and respond to these critical posts. We present a framework for triaging\nuser content into four severity categories which are defined based on\nindications of self-harm ideation. Our models are based on a feature-rich\nclassification framework which includes lexical, psycholinguistic, contextual\nand topic modeling features. Our approaches improve the state of the art in\ntriaging the content severity in mental health forums by large margins (up to\n17% improvement over the F-1 scores). Using the proposed model, we analyze the\nmental state of users and we show that overall, long-term users of the forum\ndemonstrate a decreased severity of risk over time. Our analysis on the\ninteraction of the moderators with the users further indicates that without an\nautomatic way to identify critical content, it is indeed challenging for the\nmoderators to provide timely response to the users in need.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 16:14:12 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Cohan", "Arman", ""], ["Young", "Sydney", ""], ["Yates", "Andrew", ""], ["Goharian", "Nazli", ""]]}, {"id": "1702.06934", "submitter": "Olegs Verhodubs", "authors": "Olegs Verhodubs", "title": "Realization of Ontology Web Search Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the realization of the Ontology Web Search Engine. The\nOntology Web Search Engine is realizable as independent project and as a part\nof other projects. The main purpose of this paper is to present the Ontology\nWeb Search Engine realization details as the part of the Semantic Web Expert\nSystem and to present the results of the Ontology Web Search Engine\nfunctioning. It is expected that the Semantic Web Expert System will be able to\nprocess ontologies from the Web, generate rules from these ontologies and\ndevelop its knowledge base.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 18:35:43 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Verhodubs", "Olegs", ""]]}, {"id": "1702.07083", "submitter": "Jianfei Chen", "authors": "Jianfei Chen, Jun Zhu, Jie Lu, Shixia Liu", "title": "Scalable Inference for Nested Chinese Restaurant Process Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested Chinese Restaurant Process (nCRP) topic models are powerful\nnonparametric Bayesian methods to extract a topic hierarchy from a given text\ncorpus, where the hierarchical structure is automatically determined by the\ndata. Hierarchical Latent Dirichlet Allocation (hLDA) is a popular instance of\nnCRP topic models. However, hLDA has only been evaluated at small scale,\nbecause the existing collapsed Gibbs sampling and instantiated weight\nvariational inference algorithms either are not scalable or sacrifice inference\nquality with mean-field assumptions. Moreover, an efficient distributed\nimplementation of the data structures, such as dynamically growing count\nmatrices and trees, is challenging.\n  In this paper, we propose a novel partially collapsed Gibbs sampling (PCGS)\nalgorithm, which combines the advantages of collapsed and instantiated weight\nalgorithms to achieve good scalability as well as high model quality. An\ninitialization strategy is presented to further improve the model quality.\nFinally, we propose an efficient distributed implementation of PCGS through\nvectorization, pre-processing, and a careful design of the concurrent data\nstructures and communication strategy.\n  Empirical studies show that our algorithm is 111 times more efficient than\nthe previous open-source implementation for hLDA, with comparable or even\nbetter model quality. Our distributed implementation can extract 1,722 topics\nfrom a 131-million-document corpus with 28 billion tokens, which is 4-5 orders\nof magnitude larger than the previous largest corpus, with 50 machines in 7\nhours.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 03:34:07 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Chen", "Jianfei", ""], ["Zhu", "Jun", ""], ["Lu", "Jie", ""], ["Liu", "Shixia", ""]]}, {"id": "1702.07092", "submitter": "Arman Cohan", "authors": "Arman Cohan, Allan Fong, Nazli Goharian, and Raj Ratwani", "title": "A Neural Attention Model for Categorizing Patient Safety Events", "comments": "ECIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical errors are leading causes of death in the US and as such, prevention\nof these errors is paramount to promoting health care. Patient Safety Event\nreports are narratives describing potential adverse events to the patients and\nare important in identifying and preventing medical errors. We present a neural\nnetwork architecture for identifying the type of safety events which is the\nfirst step in understanding these narratives. Our proposed model is based on a\nsoft neural attention model to improve the effectiveness of encoding long\nsequences. Empirical results on two large-scale real-world datasets of patient\nsafety reports demonstrate the effectiveness of our method with significant\nimprovements over existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 04:27:49 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Cohan", "Arman", ""], ["Fong", "Allan", ""], ["Goharian", "Nazli", ""], ["Ratwani", "Raj", ""]]}, {"id": "1702.07186", "submitter": "Derek Greene", "authors": "Mark Belford and Brian Mac Namee and Derek Greene", "title": "Stability of Topic Modeling via Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models can provide us with an insight into the underlying latent\nstructure of a large corpus of documents. A range of methods have been proposed\nin the literature, including probabilistic topic models and techniques based on\nmatrix factorization. However, in both cases, standard implementations rely on\nstochastic elements in their initialization phase, which can potentially lead\nto different results being generated on the same corpus when using the same\nparameter values. This corresponds to the concept of \"instability\" which has\npreviously been studied in the context of $k$-means clustering. In many\napplications of topic modeling, this problem of instability is not considered\nand topic models are treated as being definitive, even though the results may\nchange considerably if the initialization process is altered. In this paper we\ndemonstrate the inherent instability of popular topic modeling approaches,\nusing a number of new measures to assess stability. To address this issue in\nthe context of matrix factorization for topic modeling, we propose the use of\nensemble learning strategies. Based on experiments performed on annotated text\ncorpora, we show that a K-Fold ensemble strategy, combining both ensembles and\nstructured initialization, can significantly reduce instability, while\nsimultaneously yielding more accurate topic models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 12:00:10 GMT"}, {"version": "v2", "created": "Sat, 9 Sep 2017 17:06:18 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Belford", "Mark", ""], ["Mac Namee", "Brian", ""], ["Greene", "Derek", ""]]}, {"id": "1702.07326", "submitter": "Christina Lioma Assoc. Prof", "authors": "Niels Dalum Hansen and K{\\aa}re M{\\o}lbak and Ingemar J. Cox and\n  Christina Lioma", "title": "Time-Series Adaptive Estimation of Vaccination Uptake Using Web Search\n  Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating vaccination uptake is an integral part of ensuring public health.\nIt was recently shown that vaccination uptake can be estimated automatically\nfrom web data, instead of slowly collected clinical records or population\nsurveys. All prior work in this area assumes that features of vaccination\nuptake collected from the web are temporally regular. We present the first ever\nmethod to remove this assumption from vaccination uptake estimation: our method\ndynamically adapts to temporal fluctuations in time series web data used to\nestimate vaccination uptake. We show our method to outperform the state of the\nart compared to competitive baselines that use not only web data but also\ncurated clinical data. This performance improvement is more pronounced for\nvaccines whose uptake has been irregular due to negative media attention (HPV-1\nand HPV-2), problems in vaccine supply (DiTeKiPol), and targeted at children of\n12 years old (whose vaccination is more irregular compared to younger\nchildren).\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 18:24:58 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Hansen", "Niels Dalum", ""], ["M\u00f8lbak", "K\u00e5re", ""], ["Cox", "Ingemar J.", ""], ["Lioma", "Christina", ""]]}, {"id": "1702.07680", "submitter": "Cem Sahin", "authors": "Cem Safak Sahin, Rajmonda S. Caceres, Brandon Oselio, William M.\n  Campbell", "title": "Consistent Alignment of Word Embedding Models", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding models offer continuous vector representations that can\ncapture rich contextual semantics based on their word co-occurrence patterns.\nWhile these word vectors can provide very effective features used in many NLP\ntasks such as clustering similar words and inferring learning relationships,\nmany challenges and open research questions remain. In this paper, we propose a\nsolution that aligns variations of the same model (or different models) in a\njoint low-dimensional latent space leveraging carefully generated synthetic\ndata points. This generative process is inspired by the observation that a\nvariety of linguistic relationships is captured by simple linear operations in\nembedded space. We demonstrate that our approach can lead to substantial\nimprovements in recovering embeddings of local neighborhoods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 17:40:28 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Sahin", "Cem Safak", ""], ["Caceres", "Rajmonda S.", ""], ["Oselio", "Brandon", ""], ["Campbell", "William M.", ""]]}, {"id": "1702.07745", "submitter": "Rupinder Paul Khandpur", "authors": "Rupinder Paul Khandpur, Taoran Ji, Steve Jan, Gang Wang, Chang-Tien Lu\n  and Naren Ramakrishnan", "title": "Crowdsourcing Cybersecurity: Cyber Attack Detection using Social Media", "comments": "13 single column pages, 5 figures, submitted to KDD 2017", "journal-ref": null, "doi": "10.1145/3132847.3132866", "report-no": null, "categories": "cs.CR cs.HC cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media is often viewed as a sensor into various societal events such as\ndisease outbreaks, protests, and elections. We describe the use of social media\nas a crowdsourced sensor to gain insight into ongoing cyber-attacks. Our\napproach detects a broad range of cyber-attacks (e.g., distributed denial of\nservice (DDOS) attacks, data breaches, and account hijacking) in an\nunsupervised manner using just a limited fixed set of seed event triggers. A\nnew query expansion strategy based on convolutional kernels and dependency\nparses helps model reporting structure and aids in identifying key event\ncharacteristics. Through a large-scale analysis over Twitter, we demonstrate\nthat our approach consistently identifies and encodes events, outperforming\nexisting methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 20:17:00 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Khandpur", "Rupinder Paul", ""], ["Ji", "Taoran", ""], ["Jan", "Steve", ""], ["Wang", "Gang", ""], ["Lu", "Chang-Tien", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1702.07969", "submitter": "David Liu", "authors": "David C. Liu, Stephanie Rogers, Raymond Shiau, Dmitry Kislyuk, Kevin\n  C. Ma, Zhigang Zhong, Jenny Liu, Yushi Jing", "title": "Related Pins at Pinterest: The Evolution of a Real-World Recommender\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Related Pins is the Web-scale recommender system that powers over 40% of user\nengagement on Pinterest. This paper is a longitudinal study of three years of\nits development, exploring the evolution of the system and its components from\nprototypes to present state. Each component was originally built with many\nconstraints on engineering effort and computational resources, so we\nprioritized the simplest and highest-leverage solutions. We show how organic\ngrowth led to a complex system and how we managed this complexity. Many\nchallenges arose while building this system, such as avoiding feedback loops,\nevaluating performance, activating content, and eliminating legacy heuristics.\nFinally, we offer suggestions for tackling these challenges when engineering\nWeb-scale recommender systems.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 01:50:27 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Liu", "David C.", ""], ["Rogers", "Stephanie", ""], ["Shiau", "Raymond", ""], ["Kislyuk", "Dmitry", ""], ["Ma", "Kevin C.", ""], ["Zhong", "Zhigang", ""], ["Liu", "Jenny", ""], ["Jing", "Yushi", ""]]}, {"id": "1702.08070", "submitter": "William Rowe", "authors": "William Rowe, Paul D. Dobson, Bede Constantinides, and Mark Platt", "title": "PubTree: A Hierarchical Search Tool for the MEDLINE Database", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keeping track of the ever-increasing body of scientific literature is an\nescalating challenge. We present PubTree a hierarchical search tool that\nefficiently searches the PubMed/MEDLINE dataset based upon a decision tree\nconstructed using >26 million abstracts. The tool is implemented as a webpage,\nwhere users are asked a series of eighteen questions to locate pertinent\narticles. The implementation of this hierarchical search tool highlights issues\nendemic with document retrieval. However, the construction of this tree\nindicates that with future developments hierarchical search could become an\neffective tool (or adjunct) in the mining of biological literature.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 19:09:59 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Rowe", "William", ""], ["Dobson", "Paul D.", ""], ["Constantinides", "Bede", ""], ["Platt", "Mark", ""]]}, {"id": "1702.08199", "submitter": "Shenghui Wang", "authors": "Rob Koopman, Shenghui Wang", "title": "Mutual Information based labelling and comparing clusters", "comments": "Special Issue of Scientometrics: Same data - different results?\n  Towards a comparative approach to the identification of thematic structures\n  in science", "journal-ref": null, "doi": "10.1007/s11192-017-2305-2", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After a clustering solution is generated automatically, labelling these\nclusters becomes important to help understanding the results. In this paper, we\npropose to use a Mutual Information based method to label clusters of journal\narticles. Topical terms which have the highest Normalised Mutual Information\n(NMI) with a certain cluster are selected to be the labels of the cluster.\nDiscussion of the labelling technique with a domain expert was used as a check\nthat the labels are discriminating not only lexical-wise but also semantically.\nBased on a common set of topical terms, we also propose to generate lexical\nfingerprints as a representation of individual clusters. Eventually, we\nvisualise and compare these fingerprints of different clusters from either one\nclustering solution or different ones.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 09:23:46 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Koopman", "Rob", ""], ["Wang", "Shenghui", ""]]}, {"id": "1702.08734", "submitter": "Matthijs Douze", "authors": "Jeff Johnson and Matthijs Douze and Herv\\'e J\\'egou", "title": "Billion-scale similarity search with GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search finds application in specialized database systems handling\ncomplex data such as images or videos, which are typically represented by\nhigh-dimensional features and require specific indexing structures. This paper\ntackles the problem of better utilizing GPUs for this task. While GPUs excel at\ndata-parallel tasks, prior approaches are bottlenecked by algorithms that\nexpose less parallelism, such as k-min selection, or make poor use of the\nmemory hierarchy.\n  We propose a design for k-selection that operates at up to 55% of theoretical\npeak performance, enabling a nearest neighbor implementation that is 8.5x\nfaster than prior GPU state of the art. We apply it in different similarity\nsearch scenarios, by proposing optimized design for brute-force, approximate\nand compressed-domain search based on product quantization. In all these\nsetups, we outperform the state of the art by large margins. Our implementation\nenables the construction of a high accuracy k-NN graph on 95 million images\nfrom the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion\nvectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced\nour approach for the sake of comparison and reproducibility.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 10:42:31 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Johnson", "Jeff", ""], ["Douze", "Matthijs", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}]