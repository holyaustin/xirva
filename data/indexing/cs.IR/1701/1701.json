[{"id": "1701.00185", "submitter": "Jiaming Xu", "authors": "Jiaming Xu, Bo Xu, Peng Wang, Suncong Zheng, Guanhua Tian, Jun Zhao,\n  Bo Xu", "title": "Self-Taught Convolutional Neural Networks for Short Text Clustering", "comments": "33 pages, accepted for publication in Neural Networks", "journal-ref": null, "doi": "10.1016/j.neunet.2016.12.008", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short text clustering is a challenging problem due to its sparseness of text\nrepresentation. Here we propose a flexible Self-Taught Convolutional neural\nnetwork framework for Short Text Clustering (dubbed STC^2), which can flexibly\nand successfully incorporate more useful semantic features and learn non-biased\ndeep text representation in an unsupervised manner. In our framework, the\noriginal raw text features are firstly embedded into compact binary codes by\nusing one existing unsupervised dimensionality reduction methods. Then, word\nembeddings are explored and fed into convolutional neural networks to learn\ndeep feature representations, meanwhile the output units are used to fit the\npre-trained binary codes in the training process. Finally, we get the optimal\nclusters by employing K-means to cluster the learned representations. Extensive\nexperimental results demonstrate that the proposed framework is effective,\nflexible and outperform several popular clustering methods when tested on three\npublic short text datasets.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 01:57:59 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Xu", "Jiaming", ""], ["Xu", "Bo", ""], ["Wang", "Peng", ""], ["Zheng", "Suncong", ""], ["Tian", "Guanhua", ""], ["Zhao", "Jun", ""], ["Xu", "Bo", ""]]}, {"id": "1701.00199", "submitter": "Aidong Lu", "authors": "Kodzo Wegba, Aidong Lu, Yuemeng Li, and Wencheng Wang", "title": "Interactive Movie Recommendation Through Latent Semantic Analysis and\n  Storytelling", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation has become one of the most important components of online\nservices for improving sale records, however visualization work for online\nrecommendation is still very limited. This paper presents an interactive\nrecommendation approach with the following two components. First, rating\nrecords are the most widely used data for online recommendation, but they are\noften processed in high-dimensional spaces that can not be easily understood or\ninteracted with. We propose a Latent Semantic Model (LSM) that captures the\nstatistical features of semantic concepts on 2D domains and abstracts user\npreferences for personal recommendation. Second, we propose an interactive\nrecommendation approach through a storytelling mechanism for promoting the\ncommunication between the user and the recommendation system. Our approach\nemphasizes interactivity, explicit user input, and semantic information convey;\nthus it can be used by general users without any knowledge of recommendation or\nvisualization algorithms. We validate our model with data statistics and\ndemonstrate our approach with case studies from the MovieLens100K dataset. Our\napproaches of latent semantic analysis and interactive recommendation can also\nbe extended to other network-based visualization applications, including\nvarious online recommendation systems.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 04:52:37 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Wegba", "Kodzo", ""], ["Lu", "Aidong", ""], ["Li", "Yuemeng", ""], ["Wang", "Wencheng", ""]]}, {"id": "1701.00289", "submitter": "Mariano Beguerisse-D\\'iaz", "authors": "David J.P. O'Sullivan and Guillermo Gardu\\~no-Hern\\'andez and James P.\n  Gleeson and Mariano Beguerisse-D\\'iaz", "title": "Integrating sentiment and social structure to determine preference\n  alignments: The Irish Marriage Referendum", "comments": "16 pages, 12 figures", "journal-ref": "R. Soc. open sci., 4, 170154 (2017)", "doi": "10.1098/rsos.170154", "report-no": null, "categories": "cs.SI cs.CL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the relationship between social structure and sentiment through\nthe analysis of a large collection of tweets about the Irish Marriage\nReferendum of 2015. We obtain the sentiment of every tweet with the hashtags\n#marref and #marriageref that was posted in the days leading to the referendum,\nand construct networks to aggregate sentiment and use it to study the\ninteractions among users. Our results show that the sentiment of mention tweets\nposted by users is correlated with the sentiment of received mentions, and\nthere are significantly more connections between users with similar sentiment\nscores than among users with opposite scores in the mention and follower\nnetworks. We combine the community structure of the two networks with the\nactivity level of the users and sentiment scores to find groups of users who\nsupport voting `yes' or `no' in the referendum. There were numerous\nconversations between users on opposing sides of the debate in the absence of\nfollower connections, which suggests that there were efforts by some users to\nestablish dialogue and debate across ideological divisions. Our analysis shows\nthat social structure can be integrated successfully with sentiment to analyse\nand understand the disposition of social media users. These results have\npotential applications in the integration of data and meta-data to study\nopinion dynamics, public opinion modelling, and polling.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 20:54:52 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2017 16:14:00 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["O'Sullivan", "David J. P.", ""], ["Gardu\u00f1o-Hern\u00e1ndez", "Guillermo", ""], ["Gleeson", "James P.", ""], ["Beguerisse-D\u00edaz", "Mariano", ""]]}, {"id": "1701.00324", "submitter": "Walid Shalaby PhD", "authors": "Walid Shalaby, Wlodek Zadrozny", "title": "Patent Retrieval: A Literature Review", "comments": null, "journal-ref": null, "doi": "10.1007/s10115-018-1322-7", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever increasing number of filed patent applications every year, the\nneed for effective and efficient systems for managing such tremendous amounts\nof data becomes inevitably important. Patent Retrieval (PR) is considered the\npillar of almost all patent analysis tasks. PR is a subfield of Information\nRetrieval (IR) which is concerned with developing techniques and methods that\neffectively and efficiently retrieve relevant patent documents in response to a\ngiven search request. In this paper we present a comprehensive review on PR\nmethods and approaches. It is clear that, recent successes and maturity in IR\napplications such as Web search cannot be transferred directly to PR without\ndeliberate domain adaptation and customization. Furthermore, state-of-the-art\nperformance in automatic PR is still around average in terms of recall. These\nobservations motivate the need for interactive search tools which provide\ncognitive assistance to patent professionals with minimal effort. These tools\nmust also be developed in hand with patent professionals considering their\npractices and expectations. We additionally touch on related tasks to PR such\nas patent valuation, litigation, licensing, and highlight potential\nopportunities and open directions for computational scientists in these\ndomains.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 06:32:38 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 00:34:09 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Shalaby", "Walid", ""], ["Zadrozny", "Wlodek", ""]]}, {"id": "1701.00595", "submitter": "Saeid Hosseini", "authors": "Saeid Hosseini, Hongzhi Yin, Xiaofang Zhou, Shazia Sadiq", "title": "Leveraging Multi-aspect Time-related Influence in Location\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point-Of-Interest (POI) recommendation aims to mine a user's visiting history\nand find her/his potentially preferred places. Although location recommendation\nmethods have been studied and improved pervasively, the challenges w.r.t\nemploying various influences including temporal aspect still remain. Inspired\nby the fact that time includes numerous granular slots (e.g. minute, hour, day,\nweek and etc.), in this paper, we define a new problem to perform\nrecommendation through exploiting all diversified temporal factors. In\nparticular, we argue that most existing methods only focus on a limited number\nof time-related features and neglect others. Furthermore, considering a\nspecific granularity (e.g. time of a day) in recommendation cannot always apply\nto each user or each dataset. To address the challenges, we propose a\nprobabilistic generative model, named after Multi-aspect Time-related Influence\n(MATI) to promote POI recommendation. We also develop a novel optimization\nalgorithm based on Expectation Maximization (EM). Our MATI model firstly\ndetects a user's temporal multivariate orientation using her check-in log in\nLocation-based Social Networks(LBSNs). It then performs recommendation using\ntemporal correlations between the user and proposed locations. Our method is\nadaptable to various types of recommendation systems and can work efficiently\nin multiple time-scales. Extensive experimental results on two large-scale LBSN\ndatasets verify the effectiveness of our method over other competitors.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 06:50:50 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Hosseini", "Saeid", ""], ["Yin", "Hongzhi", ""], ["Zhou", "Xiaofang", ""], ["Sadiq", "Shazia", ""]]}, {"id": "1701.00694", "submitter": "Ming Yan", "authors": "Xiaolin Huang and Yan Xia and Lei Shi and Yixing Huang and Ming Yan\n  and Joachim Hornegger and Andreas Maier", "title": "Mixed one-bit compressive sensing with applications to overexposure\n  correction for CT reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a measurement falls outside the quantization or measurable range, it\nbecomes saturated and cannot be used in classical reconstruction methods. For\nexample, in C-arm angiography systems, which provide projection radiography,\nfluoroscopy, digital subtraction angiography, and are widely used for medical\ndiagnoses and interventions, the limited dynamic range of C-arm flat detectors\nleads to overexposure in some projections during an acquisition, such as\nimaging relatively thin body parts (e.g., the knee). Aiming at overexposure\ncorrection for computed tomography (CT) reconstruction, we in this paper\npropose a mixed one-bit compressive sensing (M1bit-CS) to acquire information\nfrom both regular and saturated measurements. This method is inspired by the\nrecent progress on one-bit compressive sensing, which deals with only sign\nobservations. Its successful applications imply that information carried by\nsaturated measurements is useful to improve recovery quality. For the proposed\nM1bit-CS model, alternating direction methods of multipliers is developed and\nan iterative saturation detection scheme is established. Then we evaluate\nM1bit-CS on one-dimensional signal recovery tasks. In some experiments, the\nperformance of the proposed algorithms on mixed measurements is almost the same\nas recovery on unsaturated ones with the same amount of measurements. Finally,\nwe apply the proposed method to overexposure correction for CT reconstruction\non a phantom and a simulated clinical image. The results are promising, as the\ntypical streaking artifacts and capping artifacts introduced by saturated\nprojection data are effectively reduced, yielding significant error reduction\ncompared with existing algorithms based on extrapolation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 14:35:33 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Huang", "Xiaolin", ""], ["Xia", "Yan", ""], ["Shi", "Lei", ""], ["Huang", "Yixing", ""], ["Yan", "Ming", ""], ["Hornegger", "Joachim", ""], ["Maier", "Andreas", ""]]}, {"id": "1701.00749", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel and Evangelos Kanoulas and Maarten de Rijke", "title": "Pyndri: a Python Interface to the Indri Search Engine", "comments": "ECIR2017. Proceedings of the 39th European Conference on Information\n  Retrieval. 2017. The final publication will be available at Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce pyndri, a Python interface to the Indri search engine. Pyndri\nallows to access Indri indexes from Python at two levels: (1) dictionary and\ntokenized document collection, (2) evaluating queries on the index. We hope\nthat with the release of pyndri, we will stimulate reproducible, open and\nfast-paced IR research.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 17:17:34 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Van Gysel", "Christophe", ""], ["Kanoulas", "Evangelos", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1701.00991", "submitter": "Robert J\\\"aschke", "authors": "Christoph Hube, Frank Fischer, Robert J\\\"aschke, Gerhard Lauer, Mads\n  Rosendahl Thomsen", "title": "World Literature According to Wikipedia: Introduction to a DBpedia-Based\n  Framework", "comments": "33 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Among the manifold takes on world literature, it is our goal to contribute to\nthe discussion from a digital point of view by analyzing the representation of\nworld literature in Wikipedia with its millions of articles in hundreds of\nlanguages. As a preliminary, we introduce and compare three different\napproaches to identify writers on Wikipedia using data from DBpedia, a\ncommunity project with the goal of extracting and providing structured\ninformation from Wikipedia. Equipped with our basic set of writers, we analyze\nhow they are represented throughout the 15 biggest Wikipedia language versions.\nWe combine intrinsic measures (mostly examining the connectedness of articles)\nwith extrinsic ones (analyzing how often articles are frequented by readers)\nand develop methods to evaluate our results. The better part of our findings\nseems to convey a rather conservative, old-fashioned version of world\nliterature, but a version derived from reproducible facts revealing an implicit\nliterary canon based on the editing and reading behavior of millions of people.\nWhile still having to solve some known issues, the introduced methods will help\nus build an observatory of world literature to further investigate its\nrepresentativeness and biases.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 13:06:18 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Hube", "Christoph", ""], ["Fischer", "Frank", ""], ["J\u00e4schke", "Robert", ""], ["Lauer", "Gerhard", ""], ["Thomsen", "Mads Rosendahl", ""]]}, {"id": "1701.01231", "submitter": "Max Ren", "authors": "Max Yi Ren and Clayton Scott", "title": "Adaptive Questionnaires for Direct Identification of Optimal Product\n  Design", "comments": "submitted to Journal of Mechanical Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying the most profitable product design\nfrom a finite set of candidates under unknown consumer preference. A standard\napproach to this problem follows a two-step strategy: First, estimate the\npreference of the consumer population, represented as a point in part-worth\nspace, using an adaptive discrete-choice questionnaire. Second, integrate the\nestimated part-worth vector with engineering feasibility and cost models to\ndetermine the optimal design. In this work, we (1) demonstrate that accurate\npreference estimation is neither necessary nor sufficient for identifying the\noptimal design, (2) introduce a novel adaptive questionnaire that leverages\nknowledge about engineering feasibility and manufacturing costs to directly\ndetermine the optimal design, and (3) interpret product design in terms of a\nnonlinear segmentation of part-worth space, and use this interpretation to\nilluminate the intrinsic difficulty of optimal design in the presence of noisy\nquestionnaire responses. We establish the superiority of the proposed approach\nusing a well-documented optimal product design task. This study demonstrates\nhow the identification of optimal product design can be accelerated by\nintegrating marketing and manufacturing knowledge into the adaptive\nquestionnaire.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 07:25:23 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Ren", "Max Yi", ""], ["Scott", "Clayton", ""]]}, {"id": "1701.01250", "submitter": "Jun Wang", "authors": "Jun Wang and Qiang Tang", "title": "A Probabilistic View of Neighborhood-based Recommendation Methods", "comments": "accepted by: ICDM 2016 - IEEE International Conference on Data Mining\n  series (ICDM) workshop CLOUDMINE, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic graphic model is an elegant framework to compactly present\ncomplex real-world observations by modeling uncertainty and logical flow\n(conditionally independent factors). In this paper, we present a probabilistic\nframework of neighborhood-based recommendation methods (PNBM) in which\nsimilarity is regarded as an unobserved factor. Thus, PNBM leads the estimation\nof user preference to maximizing a posterior over similarity. We further\nintroduce a novel multi-layer similarity descriptor which models and learns the\njoint influence of various features under PNBM, and name the new framework\nMPNBM. Empirical results on real-world datasets show that MPNBM allows very\naccurate estimation of user preferences.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 08:53:02 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Wang", "Jun", ""], ["Tang", "Qiang", ""]]}, {"id": "1701.01276", "submitter": "Dominik Kowald", "authors": "Dominik Kowald, Subhash Pujari, Elisabeth Lex", "title": "Temporal Effects on Hashtag Reuse in Twitter: A Cognitive-Inspired\n  Hashtag Recommendation Approach", "comments": "Accepted at WWW 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashtags have become a powerful tool in social platforms such as Twitter to\ncategorize and search for content, and to spread short messages across members\nof the social network. In this paper, we study temporal hashtag usage practices\nin Twitter with the aim of designing a cognitive-inspired hashtag\nrecommendation algorithm we call BLLi,s. Our main idea is to incorporate the\neffect of time on (i) individual hashtag reuse (i.e., reusing own hashtags),\nand (ii) social hashtag reuse (i.e., reusing hashtags, which has been\npreviously used by a followee) into a predictive model. For this, we turn to\nthe Base-Level Learning (BLL) equation from the cognitive architecture ACT-R,\nwhich accounts for the time-dependent decay of item exposure in human memory.\nWe validate BLLi,s using two crawled Twitter datasets in two evaluation\nscenarios: firstly, only temporal usage patterns of past hashtag assignments\nare utilized and secondly, these patterns are combined with a content-based\nanalysis of the current tweet. In both scenarios, we find not only that\ntemporal effects play an important role for both individual and social hashtag\nreuse but also that BLLi,s provides significantly better prediction accuracy\nand ranking results than current state-of-the-art hashtag recommendation\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 11:07:16 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Kowald", "Dominik", ""], ["Pujari", "Subhash", ""], ["Lex", "Elisabeth", ""]]}, {"id": "1701.01325", "submitter": "Ramakrishnan Kannan", "authors": "Ramakrishnan Kannan, Hyenkyun Woo, Charu C. Aggarwal, Haesun Park", "title": "Outlier Detection for Text Data : An Extended Version", "comments": "Accepted at 2017 SIAM Data Mining Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of outlier detection is extremely challenging in many domains\nsuch as text, in which the attribute values are typically non-negative, and\nmost values are zero. In such cases, it often becomes difficult to separate the\noutliers from the natural variations in the patterns in the underlying data. In\nthis paper, we present a matrix factorization method, which is naturally able\nto distinguish the anomalies with the use of low rank approximations of the\nunderlying data. Our iterative algorithm TONMF is based on block coordinate\ndescent (BCD) framework. We define blocks over the term-document matrix such\nthat the function becomes solvable. Given most recently updated values of other\nmatrix blocks, we always update one block at a time to its optimal. Our\napproach has significant advantages over traditional methods for text outlier\ndetection. Finally, we present experimental results illustrating the\neffectiveness of our method over competing methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 14:14:52 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Kannan", "Ramakrishnan", ""], ["Woo", "Hyenkyun", ""], ["Aggarwal", "Charu C.", ""], ["Park", "Haesun", ""]]}, {"id": "1701.01417", "submitter": "Pranav A", "authors": "Pranav Agrawal", "title": "Exploration of Proximity Heuristics in Length Normalization", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking functions used in information retrieval are primarily used in the\nsearch engines and they are often adopted for various language processing\napplications. However, features used in the construction of ranking functions\nshould be analyzed before applying it on a data set. This paper gives\nguidelines on construction of generalized ranking functions with\napplication-dependent features. The paper prescribes a specific case of a\ngeneralized function for recommendation system using feature engineering\nguidelines on the given data set. The behavior of both generalized and specific\nfunctions are studied and implemented on the unstructured textual data. The\nproximity feature based ranking function has outperformed by 52% from regular\nBM25.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 18:36:26 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Agrawal", "Pranav", ""]]}, {"id": "1701.01737", "submitter": "Dominik Wurzer Dominik Wurzer", "authors": "Dominik Wurzer, Yumeng Qin", "title": "Spotting Information biases in Chinese and Western Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Newswire and Social Media are the major sources of information in our time.\nWhile the topical demographic of Western Media was subjects of studies in the\npast, less is known about Chinese Media. In this paper, we apply event\ndetection and tracking technology to examine the information overlap and\ndifferences between Chinese and Western - Traditional Media and Social Media.\nOur experiments reveal a biased interest of China towards the West, which\nbecomes particularly apparent when comparing the interest in celebrities.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 19:04:34 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Wurzer", "Dominik", ""], ["Qin", "Yumeng", ""]]}, {"id": "1701.02021", "submitter": "Mehdi Elahi", "authors": "Roberto Pagano, Massimo Quadrana, Mehdi Elahi, Paolo Cremonesi", "title": "Toward Active Learning in Cross-domain Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges in Recommender Systems (RSs) is the New User\nproblem which happens when the system has to generate personalised\nrecommendations for a new user whom the system has no information about. Active\nLearning tries to solve this problem by acquiring user preference data with the\nmaximum quality, and with the minimum acquisition cost. Although there are\nvariety of works in active learning for RSs research area, almost all of them\nhave focused only on the single-domain recommendation scenario. However,\nseveral real-world RSs operate in the cross-domain scenario, where the system\ngenerates recommendations in the target domain by exploiting user preferences\nin both the target and auxiliary domains. In such a scenario, the performance\nof active learning strategies can be significantly influenced and typical\nactive learning strategies may fail to perform properly. In this paper, we\naddress this limitation, by evaluating active learning strategies in a novel\nevaluation framework, explicitly suited for the cross-domain recommendation\nscenario. We show that having access to the preferences of the users in the\nauxiliary domain may have a huge impact on the performance of active learning\nstrategies w.r.t. the classical, single-domain scenario.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 21:47:45 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Pagano", "Roberto", ""], ["Quadrana", "Massimo", ""], ["Elahi", "Mehdi", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "1701.02050", "submitter": "Thanh Vu", "authors": "Thanh Vu and Alistair Willis and Udo Kruschwitz and Dawei Song", "title": "Personalised Query Suggestion for Intranet Search with Temporal User\n  Profiling", "comments": "4 pages, 2 figures, the 2017 ACM SIGIR Conference on Human\n  Information Interaction & Retrieval (CHIIR)", "journal-ref": null, "doi": "10.1145/3020165.3022129", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown the usefulness of using collective user interaction\ndata (e.g., query logs) to recommend query modification suggestions for\nIntranet search. However, most of the query suggestion approaches for Intranet\nsearch follow an \"one size fits all\" strategy, whereby different users who\nsubmit an identical query would get the same query suggestion list. This is\nproblematic, as even with the same query, different users may have different\ntopics of interest, which may change over time in response to the user's\ninteraction with the system. We address the problem by proposing a personalised\nquery suggestion framework for Intranet search. For each search session, we\nconstruct two temporal user profiles: a click user profile using the user's\nclicked documents and a query user profile using the user's submitted queries.\nWe then use the two profiles to re-rank the non-personalised query suggestion\nlist returned by a state-of-the-art query suggestion method for Intranet\nsearch. Experimental results on a large-scale query logs collection show that\nour personalised framework significantly improves the quality of suggested\nqueries.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 01:41:13 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Vu", "Thanh", ""], ["Willis", "Alistair", ""], ["Kruschwitz", "Udo", ""], ["Song", "Dawei", ""]]}, {"id": "1701.02120", "submitter": "Jun Wang", "authors": "Jun Wang and Qiang Tang", "title": "Differentially Private Neighborhood-based Recommender Systems", "comments": "Accepted by IFIP SEC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy issues of recommender systems have become a hot topic for the society\nas such systems are appearing in every corner of our life. In contrast to the\nfact that many secure multi-party computation protocols have been proposed to\nprevent information leakage in the process of recommendation computation, very\nlittle has been done to restrict the information leakage from the\nrecommendation results. In this paper, we apply the differential privacy\nconcept to neighborhood-based recommendation methods (NBMs) under a\nprobabilistic framework. We first present a solution, by directly calibrating\nLaplace noise into the training process, to differential-privately find the\nmaximum a posteriori parameters similarity. Then we connect differential\nprivacy to NBMs by exploiting a recent observation that sampling from the\nscaled posterior distribution of a Bayesian model results in provably\ndifferentially private systems. Our experiments show that both solutions allow\npromising accuracy with a modest privacy budget, and the second solution yields\nbetter accuracy if the sampling asymptotically converges. We also compare our\nsolutions to the recent differentially private matrix factorization (MF)\nrecommender systems, and show that our solutions achieve better accuracy when\nthe privacy budget is reasonably small. This is an interesting result because\nMF systems often offer better accuracy when differential privacy is not\napplied.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 10:11:08 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 10:33:48 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Wang", "Jun", ""], ["Tang", "Qiang", ""]]}, {"id": "1701.02163", "submitter": "Valentina Franzoni", "authors": "Valentina Franzoni", "title": "Just an Update on PMING Distance for Web-based Semantic Similarity in\n  Artificial Intelligence and Data Mining", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.20531.22560", "report-no": null, "categories": "cs.AI cs.CL cs.IR math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main problems that emerges in the classic approach to semantics is\nthe difficulty in acquisition and maintenance of ontologies and semantic\nannotations. On the other hand, the Internet explosion and the massive\ndiffusion of mobile smart devices lead to the creation of a worldwide system,\nwhich information is daily checked and fueled by the contribution of millions\nof users who interacts in a collaborative way. Search engines, continually\nexploring the Web, are a natural source of information on which to base a\nmodern approach to semantic annotation. A promising idea is that it is possible\nto generalize the semantic similarity, under the assumption that semantically\nsimilar terms behave similarly, and define collaborative proximity measures\nbased on the indexing information returned by search engines. The PMING\nDistance is a proximity measure used in data mining and information retrieval,\nwhich collaborative information express the degree of relationship between two\nterms, using only the number of documents returned as result for a query on a\nsearch engine. In this work, the PMINIG Distance is updated, providing a novel\nformal algebraic definition, which corrects previous works. The novel point of\nview underlines the features of the PMING to be a locally normalized linear\ncombination of the Pointwise Mutual Information and Normalized Google Distance.\nThe analyzed measure dynamically reflects the collaborative change made on the\nweb resources.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 13:02:35 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Franzoni", "Valentina", ""]]}, {"id": "1701.02617", "submitter": "Amit Awekar", "authors": "Anasua Mitra, Amit Awekar", "title": "On Low Overlap Among Search Results of Academic Search Engines", "comments": "2 pages, submitted to ACM WWW Conference 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Number of published scholarly articles is growing exponentially. To tackle\nthis information overload, researchers are increasingly depending on niche\nacademic search engines. Recent works have shown that two major general web\nsearch engines: Google and Bing, have high level of agreement in their top\nsearch results. In contrast, we show that various academic search engines have\nlow degree of agreement among themselves. We performed experiments using 2500\nqueries over four academic search engines. We observe that overlap in search\nresult sets of any pair of academic search engines is significantly low and in\nmost of the cases the search result sets are mutually exclusive. We also\ndiscuss implications of this low overlap.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 14:46:17 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Mitra", "Anasua", ""], ["Awekar", "Amit", ""]]}, {"id": "1701.03051", "submitter": "Venkata Naveen Reddy Chedeti", "authors": "Tapan Sahni, Chinmay Chandak, Naveen Reddy Chedeti, Manish Singh", "title": "Efficient Twitter Sentiment Classification using Subjective Distant\n  Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As microblogging services like Twitter are becoming more and more influential\nin today's globalised world, its facets like sentiment analysis are being\nextensively studied. We are no longer constrained by our own opinion. Others\nopinions and sentiments play a huge role in shaping our perspective. In this\npaper, we build on previous works on Twitter sentiment analysis using Distant\nSupervision. The existing approach requires huge computation resource for\nanalysing large number of tweets. In this paper, we propose techniques to speed\nup the computation process for sentiment analysis. We use tweet subjectivity to\nselect the right training samples. We also introduce the concept of EFWS\n(Effective Word Score) of a tweet that is derived from polarity scores of\nfrequently used words, which is an additional heuristic that can be used to\nspeed up the sentiment classification with standard machine learning\nalgorithms. We performed our experiments using 1.6 million tweets. Experimental\nevaluations show that our proposed technique is more efficient and has higher\naccuracy compared to previously proposed methods. We achieve overall accuracies\nof around 80% (EFWS heuristic gives an accuracy around 85%) on a training\ndataset of 100K tweets, which is half the size of the dataset used for the\nbaseline model. The accuracy of our proposed model is 2-3% higher than the\nbaseline model, and the model effectively trains at twice the speed of the\nbaseline model.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 16:39:04 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Sahni", "Tapan", ""], ["Chandak", "Chinmay", ""], ["Chedeti", "Naveen Reddy", ""], ["Singh", "Manish", ""]]}, {"id": "1701.03079", "submitter": "Lili Mou", "authors": "Chongyang Tao, Lili Mou, Dongyan Zhao, Rui Yan", "title": "RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain\n  Dialog Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain human-computer conversation has been attracting increasing\nattention over the past few years. However, there does not exist a standard\nautomatic evaluation metric for open-domain dialog systems; researchers usually\nresort to human annotation for model evaluation, which is time- and\nlabor-intensive. In this paper, we propose RUBER, a Referenced metric and\nUnreferenced metric Blended Evaluation Routine, which evaluates a reply by\ntaking into consideration both a groundtruth reply and a query (previous\nuser-issued utterance). Our metric is learnable, but its training does not\nrequire labels of human satisfaction. Hence, RUBER is flexible and extensible\nto different datasets and languages. Experiments on both retrieval and\ngenerative dialog systems show that RUBER has a high correlation with human\nannotation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 17:43:57 GMT"}, {"version": "v2", "created": "Sun, 16 Jul 2017 16:43:08 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Tao", "Chongyang", ""], ["Mou", "Lili", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "1701.03227", "submitter": "Angela Fan", "authors": "Angela Fan, Finale Doshi-Velez, Luke Miratrix", "title": "Prior matters: simple and general methods for evaluating and improving\n  topic quality in topic modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation (LDA) models trained without stopword removal\noften produce topics with high posterior probabilities on uninformative words,\nobscuring the underlying corpus content. Even when canonical stopwords are\nmanually removed, uninformative words common in that corpus will still dominate\nthe most probable words in a topic. In this work, we first show how the\nstandard topic quality measures of coherence and pointwise mutual information\nact counter-intuitively in the presence of common but irrelevant words, making\nit difficult to even quantitatively identify situations in which topics may be\ndominated by stopwords. We propose an additional topic quality metric that\ntargets the stopword problem, and show that it, unlike the standard measures,\ncorrectly correlates with human judgements of quality. We also propose a\nsimple-to-implement strategy for generating topics that are evaluated to be of\nmuch higher quality by both human assessment and our new metric. This approach,\na collection of informative priors easily introduced into most LDA-style\ninference methods, automatically promotes terms with domain relevance and\ndemotes domain-specific stop words. We demonstrate this approach's\neffectiveness in three very different domains: Department of Labor accident\nreports, online health forum posts, and NIPS abstracts. Overall we find that\ncurrent practices thought to solve this problem do not do so adequately, and\nthat our proposal offers a substantial improvement for those interested in\ninterpreting their topics as objects in their own right.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 04:26:00 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 23:17:12 GMT"}, {"version": "v3", "created": "Sat, 14 Oct 2017 18:25:03 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Fan", "Angela", ""], ["Doshi-Velez", "Finale", ""], ["Miratrix", "Luke", ""]]}, {"id": "1701.03277", "submitter": "Miroslav Shaltev", "authors": "Miroslav Shaltev, Jan-Hendrik Zab, Philipp Kemkes, Stefan Siersdorfer,\n  Sergej Zerr", "title": "Cobwebs from the Past and Present: Extracting Large Social Networks\n  using Internet Archive Data", "comments": "5 pages, 5 figures, SIGIR '16, July 17-21, 2016, Pisa, Italy", "journal-ref": "Proceedings of the 39th International ACM SIGIR conference on\n  Research and Development in Information Retrieval, Pisa, Italy, July 17 - 21,\n  2016", "doi": "10.1145/2911451.2911467", "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social graph construction from various sources has been of interest to\nresearchers due to its application potential and the broad range of technical\nchallenges involved. The World Wide Web provides a huge amount of continuously\nupdated data and information on a wide range of topics created by a variety of\ncontent providers, and makes the study of extracted people networks and their\ntemporal evolution valuable for social as well as computer scientists. In this\npaper we present SocGraph - an extraction and exploration system for social\nrelations from the content of around 2 billion web pages collected by the\nInternet Archive over the 17 years time period between 1996 and 2013. We\ndescribe methods for constructing large social graphs from extracted relations\nand introduce an interface to study their temporal evolution.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 09:40:09 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Shaltev", "Miroslav", ""], ["Zab", "Jan-Hendrik", ""], ["Kemkes", "Philipp", ""], ["Siersdorfer", "Stefan", ""], ["Zerr", "Sergej", ""]]}, {"id": "1701.03492", "submitter": "Emrah Budur", "authors": "Emrah Budur", "title": "Scalable, Trie-based Approximate Entity Extraction for Real-Time\n  Financial Transaction Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial institutions have to screen their transactions to ensure that they\nare not affiliated with terrorism entities. Developing appropriate solutions to\ndetect such affiliations precisely while avoiding any kind of interruption to\nlarge amount of legitimate transactions is essential. In this paper, we present\nbuilding blocks of a scalable solution that may help financial institutions to\nbuild their own software to extract terrorism entities out of both structured\nand unstructured financial messages in real time and with approximate\nsimilarity matching approach.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 20:14:52 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Budur", "Emrah", ""]]}, {"id": "1701.03855", "submitter": "Oluwaseun Ajao", "authors": "Oluwaseun Ajao, Deepak P and Jun Hong", "title": "Location Inference from Tweets using Grid-based Classification", "comments": "Location Inference from Tweets using Grid-based Classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of social media and its growing association with the sharing of\nideas and propagation of messages remains vital in everyday communication.\nTwitter is one effective platform for the dissemination of news and stories\nabout recent events happening around the world. It has a continually growing\ndatabase currently adopted by over 300 million users. In this paper we propose\na novel grid-based approach employing supervised Multinomial Naive Bayes while\nextracting geographic entities from relevant user descriptions metadata which\ngives a spatial indication of the user location. To the best of our knowledge\nour approach is the first to make location inference from tweets using\ngeo-enriched grid-based classification. Our approach performs better than\nexisting baselines achieving more than 57% accuracy at city-level granularity.\nIn addition we present a novel framework for content-based estimation of user\nlocations by specifying levels of granularity required in pre-defined location\ngrids.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 00:44:49 GMT"}], "update_date": "2017-02-12", "authors_parsed": [["Ajao", "Oluwaseun", ""], ["P", "Deepak", ""], ["Hong", "Jun", ""]]}, {"id": "1701.03937", "submitter": "Tuan Tran", "authors": "Tuan Tran, Tu Ngoc Nguyen", "title": "Hedera: Scalable Indexing and Exploring Entities in Wikipedia Revision\n  History", "comments": "Pubished via CEUR-WS.org/Vol-1272", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of work in semantic web relying on Wikipedia as the main source of\nknowledge often work on static snapshots of the dataset. The full history of\nWikipedia revisions, while contains much more useful information, is still\ndifficult to access due to its exceptional volume. To enable further research\non this collection, we developed a tool, named Hedera, that efficiently\nextracts semantic information from Wikipedia revision history datasets. Hedera\nexploits Map-Reduce paradigm to achieve rapid extraction, it is able to handle\none entire Wikipedia articles revision history within a day in a medium-scale\ncluster, and supports flexible data structures for various kinds of semantic\nweb study.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 15:47:06 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Tran", "Tuan", ""], ["Nguyen", "Tu Ngoc", ""]]}, {"id": "1701.03939", "submitter": "Tuan Tran", "authors": "Tuan Tran, Nam Khanh Tran, Teka Hadgu Asmelash, Robert J\\\"aschke", "title": "Semantic Annotation for Microblog Topics Using Wikipedia Temporal\n  Information", "comments": "Published via ACL to EMNLP 2025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trending topics in microblogs such as Twitter are valuable resources to\nunderstand social aspects of real-world events. To enable deep analyses of such\ntrends, semantic annotation is an effective approach; yet the problem of\nannotating microblog trending topics is largely unexplored by the research\ncommunity. In this work, we tackle the problem of mapping trending Twitter\ntopics to entities from Wikipedia. We propose a novel model that complements\ntraditional text-based approaches by rewarding entities that exhibit a high\ntemporal correlation with topics during their burst time period. By exploiting\ntemporal information from the Wikipedia edit history and page view logs, we\nhave improved the annotation performance by 17-28\\%, as compared to the\ncompetitive baselines.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 16:11:50 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Tran", "Tuan", ""], ["Tran", "Nam Khanh", ""], ["Asmelash", "Teka Hadgu", ""], ["J\u00e4schke", "Robert", ""]]}, {"id": "1701.03942", "submitter": "Tuan Tran", "authors": "Khoi Duy Vo, Tuan Tran, Tu Ngoc Nguyen, Xiaofei Zhu, Wolfgang Nejdl", "title": "Can We Find Documents in Web Archives without Knowing their Contents?", "comments": "Published via ACM to Websci 2015", "journal-ref": null, "doi": "10.1145/2908131.2908165", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances of preservation technologies have led to an increasing number\nof Web archive systems and collections. These collections are valuable to\nexplore the past of the Web, but their value can only be uncovered with\neffective access and exploration mechanisms. Ideal search and rank- ing methods\nmust be robust to the high redundancy and the temporal noise of contents, as\nwell as scalable to the huge amount of data archived. Despite several attempts\nin Web archive search, facilitating access to Web archive still remains a\nchallenging problem.\n  In this work, we conduct a first analysis on different ranking strategies\nthat exploit evidences from metadata instead of the full content of documents.\nWe perform a first study to compare the usefulness of non-content evidences to\nWeb archive search, where the evidences are mined from the metadata of file\nheaders, links and URL strings only. Based on these findings, we propose a\nsimple yet surprisingly effective learning model that combines multiple\nevidences to distinguish \"good\" from \"bad\" search results. We conduct empirical\nexperiments quantitatively as well as qualitatively to confirm the validity of\nour proposed method, as a first step towards better ranking in Web archives\ntaking meta- data into account.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 16:23:09 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Vo", "Khoi Duy", ""], ["Tran", "Tuan", ""], ["Nguyen", "Tu Ngoc", ""], ["Zhu", "Xiaofei", ""], ["Nejdl", "Wolfgang", ""]]}, {"id": "1701.03947", "submitter": "Tuan Tran", "authors": "Tuan Tran, Claudia Nieder\\'ee, Nattiya Kanhabua, Ujwal Gadiraju,\n  Avishek Anand", "title": "Balancing Novelty and Salience: Adaptive Learning to Rank Entities for\n  Timeline Summarization of High-impact Events", "comments": "Published via ACM to CIKM 2015", "journal-ref": null, "doi": "10.1145/2806416.2806486", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-running, high-impact events such as the Boston Marathon bombing often\ndevelop through many stages and involve a large number of entities in their\nunfolding. Timeline summarization of an event by key sentences eases story\ndigestion, but does not distinguish between what a user remembers and what she\nmight want to re-check. In this work, we present a novel approach for timeline\nsummarization of high-impact events, which uses entities instead of sentences\nfor summarizing the event at each individual point in time. Such entity\nsummaries can serve as both (1) important memory cues in a retrospective event\nconsideration and (2) pointers for personalized event exploration. In order to\nautomatically create such summaries, it is crucial to identify the \"right\"\nentities for inclusion. We propose to learn a ranking function for entities,\nwith a dynamically adapted trade-off between the in-document salience of\nentities and the informativeness of entities across documents, i.e., the level\nof new information associated with an entity for a time point under\nconsideration. Furthermore, for capturing collective attention for an entity we\nuse an innovative soft labeling approach based on Wikipedia. Our experiments on\na real large news datasets confirm the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 16:47:51 GMT"}], "update_date": "2017-02-12", "authors_parsed": [["Tran", "Tuan", ""], ["Nieder\u00e9e", "Claudia", ""], ["Kanhabua", "Nattiya", ""], ["Gadiraju", "Ujwal", ""], ["Anand", "Avishek", ""]]}, {"id": "1701.04039", "submitter": "David Graus", "authors": "David Graus, Daan Odijk, Maarten de Rijke", "title": "The Birth of Collective Memories: Analyzing Emerging Entities in Text\n  Streams", "comments": "To appear in JASIST", "journal-ref": null, "doi": "10.1002/asi.24004", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how collective memories are formed online. We do so by tracking\nentities that emerge in public discourse, that is, in online text streams such\nas social media and news streams, before they are incorporated into Wikipedia,\nwhich, we argue, can be viewed as an online place for collective memory. By\ntracking how entities emerge in public discourse, i.e., the temporal patterns\nbetween their first mention in online text streams and subsequent incorporation\ninto collective memory, we gain insights into how the collective remembrance\nprocess happens online. Specifically, we analyze nearly 80,000 entities as they\nemerge in online text streams before they are incorporated into Wikipedia. The\nonline text streams we use for our analysis comprise of social media and news\nstreams, and span over 579 million documents in a timespan of 18 months. We\ndiscover two main emergence patterns: entities that emerge in a \"bursty\"\nfashion, i.e., that appear in public discourse without a precedent, blast into\nactivity and transition into collective memory. Other entities display a\n\"delayed\" pattern, where they appear in public discourse, experience a period\nof inactivity, and then resurface before transitioning into our cultural\ncollective memory.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 13:34:43 GMT"}, {"version": "v2", "created": "Fri, 8 Dec 2017 16:19:18 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Graus", "David", ""], ["Odijk", "Daan", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1701.04273", "submitter": "Hosein Azarbonyad", "authors": "Hosein Azarbonyad and Mostafa Dehghani and Tom Kenter and Maarten Marx\n  and Jaap Kamps and Maarten de Rijke", "title": "Hierarchical Re-estimation of Topic Models for Measuring Topical\n  Diversity", "comments": "Proceedings of the 39th European Conference on Information Retrieval\n  (ECIR2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A high degree of topical diversity is often considered to be an important\ncharacteristic of interesting text documents. A recent proposal for measuring\ntopical diversity identifies three elements for assessing diversity: words,\ntopics, and documents as collections of words. Topic models play a central role\nin this approach. Using standard topic models for measuring diversity of\ndocuments is suboptimal due to generality and impurity. General topics only\ninclude common information from a background corpus and are assigned to most of\nthe documents in the collection. Impure topics contain words that are not\nrelated to the topic; impurity lowers the interpretability of topic models and\nimpure topics are likely to get assigned to documents erroneously. We propose a\nhierarchical re-estimation approach for topic models to combat generality and\nimpurity; the proposed approach operates at three levels: words, topics, and\ndocuments. Our re-estimation approach for measuring documents' topical\ndiversity outperforms the state of the art on PubMed dataset which is commonly\nused for diversity experiments.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 12:59:47 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Azarbonyad", "Hosein", ""], ["Dehghani", "Mostafa", ""], ["Kenter", "Tom", ""], ["Marx", "Maarten", ""], ["Kamps", "Jaap", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1701.04292", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Piotr Borkowski and Krzysztof Ciesielski and Mieczys{\\l}aw A.\n  K{\\l}opotek", "title": "Semantic classifier approach to document classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new document classification method, bridging\ndiscrepancies (so-called semantic gap) between the training set and the\napplication sets of textual data. We demonstrate its superiority over classical\ntext classification approaches, including traditional classifier ensembles. The\nmethod consists in combining a document categorization technique with a single\nclassifier or a classifier ensemble (SEMCOM algorithm - Committee with Semantic\nCategorizer).\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 14:02:19 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Borkowski", "Piotr", ""], ["Ciesielski", "Krzysztof", ""], ["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1701.04313", "submitter": "Kartik Audhkhasi", "authors": "Kartik Audhkhasi, Andrew Rosenberg, Abhinav Sethy, Bhuvana\n  Ramabhadran, Brian Kingsbury", "title": "End-to-End ASR-free Keyword Search from Speech", "comments": "Published in the IEEE 2017 International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP 2017), scheduled for 5-9 March 2017 in\n  New Orleans, Louisiana, USA", "journal-ref": null, "doi": "10.1109/JSTSP.2017.2759726", "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end (E2E) systems have achieved competitive results compared to\nconventional hybrid hidden Markov model (HMM)-deep neural network based\nautomatic speech recognition (ASR) systems. Such E2E systems are attractive due\nto the lack of dependence on alignments between input acoustic and output\ngrapheme or HMM state sequence during training. This paper explores the design\nof an ASR-free end-to-end system for text query-based keyword search (KWS) from\nspeech trained with minimal supervision. Our E2E KWS system consists of three\nsub-systems. The first sub-system is a recurrent neural network (RNN)-based\nacoustic auto-encoder trained to reconstruct the audio through a\nfinite-dimensional representation. The second sub-system is a character-level\nRNN language model using embeddings learned from a convolutional neural\nnetwork. Since the acoustic and text query embeddings occupy different\nrepresentation spaces, they are input to a third feed-forward neural network\nthat predicts whether the query occurs in the acoustic utterance or not. This\nE2E ASR-free KWS system performs respectably despite lacking a conventional ASR\nsystem and trains much faster.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 15:05:39 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Audhkhasi", "Kartik", ""], ["Rosenberg", "Andrew", ""], ["Sethy", "Abhinav", ""], ["Ramabhadran", "Bhuvana", ""], ["Kingsbury", "Brian", ""]]}, {"id": "1701.04600", "submitter": "Amit Awekar", "authors": "Siddhesh Khandelwal, Amit Awekar", "title": "Faster K-Means Cluster Estimation", "comments": "6 pages, Accepted at ECIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable work on improving popular clustering algorithm\n`K-means' in terms of mean squared error (MSE) and speed, both. However, most\nof the k-means variants tend to compute distance of each data point to each\ncluster centroid for every iteration. We propose a fast heuristic to overcome\nthis bottleneck with only marginal increase in MSE. We observe that across all\niterations of K-means, a data point changes its membership only among a small\nsubset of clusters. Our heuristic predicts such clusters for each data point by\nlooking at nearby clusters after the first iteration of k-means. We augment\nwell known variants of k-means with our heuristic to demonstrate effectiveness\nof our heuristic. For various synthetic and real-world datasets, our heuristic\nachieves speed-up of up-to 3 times when compared to efficient variants of\nk-means.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 10:00:51 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Khandelwal", "Siddhesh", ""], ["Awekar", "Amit", ""]]}, {"id": "1701.04783", "submitter": "Lei Zheng", "authors": "Lei Zheng, Vahid Noroozi, Philip S. Yu", "title": "Joint Deep Modeling of Users and Items Using Reviews for Recommendation", "comments": "WSDM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A large amount of information exists in reviews written by users. This source\nof information has been ignored by most of the current recommender systems\nwhile it can potentially alleviate the sparsity problem and improve the quality\nof recommendations. In this paper, we present a deep model to learn item\nproperties and user behaviors jointly from review text. The proposed model,\nnamed Deep Cooperative Neural Networks (DeepCoNN), consists of two parallel\nneural networks coupled in the last layers. One of the networks focuses on\nlearning user behaviors exploiting reviews written by the user, and the other\none learns item properties from the reviews written for the item. A shared\nlayer is introduced on the top to couple these two networks together. The\nshared layer enables latent factors learned for users and items to interact\nwith each other in a manner similar to factorization machine techniques.\nExperimental results demonstrate that DeepCoNN significantly outperforms all\nbaseline recommender systems on a variety of datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 17:46:04 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Zheng", "Lei", ""], ["Noroozi", "Vahid", ""], ["Yu", "Philip S.", ""]]}, {"id": "1701.04931", "submitter": "Swati Agarwal", "authors": "Swati Agarwal and Ashish Sureka", "title": "Characterizing Linguistic Attributes for Automatic Classification of\n  Intent Based Racist/Radicalized Posts on Tumblr Micro-Blogging Website", "comments": "This paper is an extended and detailed version of our (same authors')\n  short paper published in EISIC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research shows that many like-minded people use popular microblogging\nwebsites for posting hateful speech against various religions and race.\nAutomatic identification of racist and hate promoting posts is required for\nbuilding social media intelligence and security informatics based solutions.\nHowever, just keyword spotting based techniques cannot be used to accurately\nidentify the intent of a post. In this paper, we address the challenge of the\npresence of ambiguity in such posts by identifying the intent of author. We\nconduct our study on Tumblr microblogging website and develop a cascaded\nensemble learning classifier for identifying the posts having racist or\nradicalized intent. We train our model by identifying various semantic,\nsentiment and linguistic features from free-form text. Our experimental results\nshows that the proposed approach is effective and the emotion tone, social\ntendencies, language cues and personality traits of a narrative are\ndiscriminatory features for identifying the racist intent behind a post.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 03:17:20 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Agarwal", "Swati", ""], ["Sureka", "Ashish", ""]]}, {"id": "1701.04934", "submitter": "Swati Agarwal", "authors": "Swati Agarwal and Ashish Sureka", "title": "Investigating the Application of Common-Sense Knowledge-Base for\n  Identifying Term Obfuscation in Adversarial Communication", "comments": "This paper is an extended and detailed version of our (same authors)\n  previous paper (regular paper) published at COMSNETS2015", "journal-ref": "S. Agarwal and A. Sureka, \"Using common-sense knowledge-base for\n  detecting word obfuscation in adversarial communication,\" 2015 7th\n  International Conference on Communication Systems and Networks (COMSNETS),\n  Bangalore, 2015, pp. 1-6", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Word obfuscation or substitution means replacing one word with another word\nin a sentence to conceal the textual content or communication. Word obfuscation\nis used in adversarial communication by terrorist or criminals for conveying\ntheir messages without getting red-flagged by security and intelligence\nagencies intercepting or scanning messages (such as emails and telephone\nconversations). ConceptNet is a freely available semantic network represented\nas a directed graph consisting of nodes as concepts and edges as assertions of\ncommon sense about these concepts. We present a solution approach exploiting\nvast amount of semantic knowledge in ConceptNet for addressing the technically\nchallenging problem of word substitution in adversarial communication. We frame\nthe given problem as a textual reasoning and context inference task and utilize\nConceptNet's natural-language-processing tool-kit for determining word\nsubstitution. We use ConceptNet to compute the conceptual similarity between\nany two given terms and define a Mean Average Conceptual Similarity (MACS)\nmetric to identify out-of-context terms. The test-bed to evaluate our proposed\napproach consists of Enron email dataset (having over 600000 emails generated\nby 158 employees of Enron Corporation) and Brown corpus (totaling about a\nmillion words drawn from a wide variety of sources). We implement word\nsubstitution techniques used by previous researches to generate a test dataset.\nWe conduct a series of experiments consisting of word substitution methods used\nin the past to evaluate our approach. Experimental results reveal that the\nproposed approach is effective.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 03:36:33 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Agarwal", "Swati", ""], ["Sureka", "Ashish", ""]]}, {"id": "1701.05149", "submitter": "G\\\"urkan Alpaslan", "authors": "G\\\"urkan Alpaslan", "title": "Comparison of the Efficiency of Different Algorithms on Recommendation\n  System Design: a Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By the growing trend of online shopping and e-commerce websites,\nrecommendation systems have gained more importance in recent years in order to\nincrease the sales ratios of companies. Different algorithms on recommendation\nsystems are used and every one produce different results. Every algorithm on\nthis area have positive and negative attributes. The purpose of the research is\nto test the different algorithms for choosing the best one according as\nstructure of dataset and aims of developers. For this purpose, threshold and\nk-means based collaborative filtering and content-based filtering algorithms\nare utilized on the dataset contains 100*73421 matrix length. What are the\ndifferences and effects of these different algorithms on the same dataset? What\nare the challenges of the algorithms? What criteria are more important in order\nto evaluate a recommendation systems? In the study, we answer these crucial\nproblems with the case study.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 17:58:38 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Alpaslan", "G\u00fcrkan", ""]]}, {"id": "1701.05228", "submitter": "Konstantina Christakopoulou", "authors": "Konstantina Christakopoulou, Jaya Kawale, Arindam Banerjee", "title": "Recommendation under Capacity Constraints", "comments": "Extended methods section and experimental section to include bayesian\n  personalized ranking objective as well", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the common scenario where every candidate item\nfor recommendation is characterized by a maximum capacity, i.e., number of\nseats in a Point-of-Interest (POI) or size of an item's inventory. Despite the\nprevalence of the task of recommending items under capacity constraints in a\nvariety of settings, to the best of our knowledge, none of the known\nrecommender methods is designed to respect capacity constraints. To close this\ngap, we extend three state-of-the art latent factor recommendation approaches:\nprobabilistic matrix factorization (PMF), geographical matrix factorization\n(GeoMF), and bayesian personalized ranking (BPR), to optimize for both\nrecommendation accuracy and expected item usage that respects the capacity\nconstraints. We introduce the useful concepts of user propensity to listen and\nitem capacity. Our experimental results in real-world datasets, both for the\ndomain of item recommendation and POI recommendation, highlight the benefit of\nour method for the setting of recommendation under capacity constraints.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 20:45:57 GMT"}, {"version": "v2", "created": "Sun, 12 Mar 2017 23:33:18 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Christakopoulou", "Konstantina", ""], ["Kawale", "Jaya", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1701.05311", "submitter": "Valentina Franzoni", "authors": "Valentina Franzoni, Yuanxi Li, Clement H.C.Leung and Alfredo Milani", "title": "Semantic Evolutionary Concept Distances for Effective Information\n  Retrieval in Query Expansion", "comments": "author's copy of publication in NLCS ICCSA 2013 proceedings:\n  Collective Evolutionary Concept Distance Based Query Expansion for Effective\n  Web Document Retrieval", "journal-ref": "Chapter Computational Science and Its Applications, ICCSA 2013,\n  Volume 7974 of the series Lecture Notes in Computer Science, pp 657-672", "doi": "10.1007/978-3-642-39649-6_47", "report-no": null, "categories": "cs.IR cs.AI cs.CL math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work several semantic approaches to concept-based query expansion and\nreranking schemes are studied and compared with different ontology-based\nexpansion methods in web document search and retrieval. In particular, we focus\non concept-based query expansion schemes, where, in order to effectively\nincrease the precision of web document retrieval and to decrease the users\nbrowsing time, the main goal is to quickly provide users with the most suitable\nquery expansion. Two key tasks for query expansion in web document retrieval\nare to find the expansion candidates, as the closest concepts in web document\ndomain, and to rank the expanded queries properly. The approach we propose aims\nat improving the expansion phase for better web document retrieval and\nprecision. The basic idea is to measure the distance between candidate concepts\nusing the PMING distance, a collaborative semantic proximity measure, i.e. a\nmeasure which can be computed by using statistical results from web search\nengine. Experiments show that the proposed technique can provide users with\nmore satisfying expansion results and improve the quality of web document\nretrieval.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 06:38:33 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Franzoni", "Valentina", ""], ["Li", "Yuanxi", ""], ["Leung", "Clement H. C.", ""], ["Milani", "Alfredo", ""]]}, {"id": "1701.05596", "submitter": "Roger Schaer", "authors": "Dimitrios Markonis, Roger Schaer, Alba Garc\\'ia Seco de Herrera,\n  Henning M\\\"uller", "title": "The Parallel Distributed Image Search Engine (ParaDISE)", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval is a complex task that differs according to the context and\nthe user requirements in any specific field, for example in a medical\nenvironment. Search by text is often not possible or optimal and retrieval by\nthe visual content does not always succeed in modelling high-level concepts\nthat a user is looking for. Modern image retrieval techniques consist of\nmultiple steps and aim to retrieve information from large--scale datasets and\nnot only based on global image appearance but local features and if possible in\na connection between visual features and text or semantics. This paper presents\nthe Parallel Distributed Image Search Engine (ParaDISE), an image retrieval\nsystem that combines visual search with text--based retrieval and that is\navailable as open source and free of charge. The main design concepts of\nParaDISE are flexibility, expandability, scalability and interoperability.\nThese concepts constitute the system, able to be used both in real-world\napplications and as an image retrieval research platform. Apart from the\narchitecture and the implementation of the system, two use cases are described,\nan application of ParaDISE in retrieval of images from the medical literature\nand a visual feature evaluation for medical image retrieval. Future steps\ninclude the creation of an open source community that will contribute and\nexpand this platform based on the existing parts.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 20:51:56 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Markonis", "Dimitrios", ""], ["Schaer", "Roger", ""], ["de Herrera", "Alba Garc\u00eda Seco", ""], ["M\u00fcller", "Henning", ""]]}, {"id": "1701.06078", "submitter": "Sungkyun Chang", "authors": "Sungkyun Chang, Kyogu Lee", "title": "Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive\n  Patterns in Vowel Acoustics", "comments": "13 pages", "journal-ref": "IEEE Access, Vol. 5, (2017) 16635-16648", "doi": "10.1109/ACCESS.2017.2738558", "report-no": null, "categories": "cs.SD cs.AI cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the previous approaches to lyrics-to-audio alignment used a\npre-developed automatic speech recognition (ASR) system that innately suffered\nfrom several difficulties to adapt the speech model to individual singers. A\nsignificant aspect missing in previous works is the self-learnability of\nrepetitive vowel patterns in the singing voice, where the vowel part used is\nmore consistent than the consonant part. Based on this, our system first learns\na discriminative subspace of vowel sequences, based on weighted symmetric\nnon-negative matrix factorization (WS-NMF), by taking the self-similarity of a\nstandard acoustic feature as an input. Then, we make use of canonical time\nwarping (CTW), derived from a recent computer vision technique, to find an\noptimal spatiotemporal transformation between the text and the acoustic\nsequences. Experiments with Korean and English data sets showed that deploying\nthis method after a pre-developed, unsupervised, singing source separation\nachieved more promising results than other state-of-the-art unsupervised\napproaches and an existing ASR-based system.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 20:15:08 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 16:25:15 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Chang", "Sungkyun", ""], ["Lee", "Kyogu", ""]]}, {"id": "1701.07058", "submitter": "Panagiotis Papadopoulos", "authors": "Panagiotis Papadopoulos, Nicolas Kourtellis, Pablo Rodriguez\n  Rodriguez, Nikolaos Laoutaris", "title": "If you are not paying for it, you are the product: How much do\n  advertisers pay to reach you?", "comments": "In Proceedings of IMC '17, London, United Kingdom, November 1-3,\n  2017, 15 pages", "journal-ref": null, "doi": "10.1145/3131365.3131397", "report-no": null, "categories": "cs.GT cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online advertising is progressively moving towards a programmatic model in\nwhich ads are matched to actual interests of individuals collected as they\nbrowse the web. Letting the huge debate around privacy aside, a very important\nquestion in this area, for which little is known, is: How much do advertisers\npay to reach an individual? In this study, we develop a first of its kind\nmethodology for computing exactly that -- the price paid for a web user by the\nad ecosystem -- and we do that in real time. Our approach is based on tapping\non the Real Time Bidding (RTB) protocol to collect cleartext and encrypted\nprices for winning bids paid by advertisers in order to place targeted ads. Our\nmain technical contribution is a method for tallying winning bids even when\nthey are encrypted. We achieve this by training a model using as ground truth\nprices obtained by running our own \"probe\" ad-campaigns. We design our\nmethodology through a browser extension and a back-end server that provides it\nwith fresh models for encrypted bids. We validate our methodology using a one\nyear long trace of 1600 mobile users and demonstrate that it can estimate a\nuser's advertising worth with more than 82% accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 19:47:14 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 12:53:42 GMT"}, {"version": "v3", "created": "Sun, 24 Sep 2017 20:57:10 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Papadopoulos", "Panagiotis", ""], ["Kourtellis", "Nicolas", ""], ["Rodriguez", "Pablo Rodriguez", ""], ["Laoutaris", "Nikolaos", ""]]}, {"id": "1701.07083", "submitter": "Tim Althoff", "authors": "Tim Althoff and Eric Horvitz and Ryen W. White and Jamie Zeitzer", "title": "Harnessing the Web for Population-Scale Physiological Sensing: A Case\n  Study of Sleep and Performance", "comments": "Published in Proceedings of WWW 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.IR q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human cognitive performance is critical to productivity, learning, and\naccident avoidance. Cognitive performance varies throughout each day and is in\npart driven by intrinsic, near 24-hour circadian rhythms. Prior research on the\nimpact of sleep and circadian rhythms on cognitive performance has typically\nbeen restricted to small-scale laboratory-based studies that do not capture the\nvariability of real-world conditions, such as environmental factors,\nmotivation, and sleep patterns in real-world settings. Given these limitations,\nleading sleep researchers have called for larger in situ monitoring of sleep\nand performance. We present the largest study to date on the impact of\nobjectively measured real-world sleep on performance enabled through a\nreframing of everyday interactions with a web search engine as a series of\nperformance tasks. Our analysis includes 3 million nights of sleep and 75\nmillion interaction tasks. We measure cognitive performance through the speed\nof keystroke and click interactions on a web search engine and correlate them\nto wearable device-defined sleep measures over time. We demonstrate that\nreal-world performance varies throughout the day and is influenced by both\ncircadian rhythms, chronotype (morning/evening preference), and prior sleep\nduration and timing. We develop a statistical model that operationalizes a\nlarge body of work on sleep and performance and demonstrates that our estimates\nof circadian rhythms, homeostatic sleep drive, and sleep inertia align with\nexpectations from laboratory-based sleep studies. Further, we quantify the\nimpact of insufficient sleep on real-world performance and show that two\nconsecutive nights with less than six hours of sleep are associated with\ndecreases in performance which last for a period of six days. This work\ndemonstrates the feasibility of using online interactions for large-scale\nphysiological sensing.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 19:49:43 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 00:07:28 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Althoff", "Tim", ""], ["Horvitz", "Eric", ""], ["White", "Ryen W.", ""], ["Zeitzer", "Jamie", ""]]}, {"id": "1701.07675", "submitter": "Sohrab Ferdowsi", "authors": "Sohrab Ferdowsi, Slava Voloshynovskiy, Dimche Kostadinov, Taras\n  Holotyak", "title": "Sparse Ternary Codes for similarity search have higher coding gain than\n  dense binary codes", "comments": "Accepted at 2017 IEEE International Symposium on Information Theory\n  (ISIT'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of Approximate Nearest Neighbor (ANN) search\nin pattern recognition where feature vectors in a database are encoded as\ncompact codes in order to speed-up the similarity search in large-scale\ndatabases. Considering the ANN problem from an information-theoretic\nperspective, we interpret it as an encoding, which maps the original feature\nvectors to a less entropic sparse representation while requiring them to be as\ninformative as possible. We then define the coding gain for ANN search using\ninformation-theoretic measures. We next show that the classical approach to\nthis problem, which consists of binarization of the projected vectors is\nsub-optimal. Instead, a properly designed ternary encoding achieves higher\ncoding gains and lower complexity.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 12:41:58 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 09:58:45 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Ferdowsi", "Sohrab", ""], ["Voloshynovskiy", "Slava", ""], ["Kostadinov", "Dimche", ""], ["Holotyak", "Taras", ""]]}, {"id": "1701.07795", "submitter": "Aaron Jaech", "authors": "Aaron Jaech and Hetunandan Kamisetty and Eric Ringger and Charlie\n  Clarke", "title": "Match-Tensor: a Deep Relevance Model for Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of Deep Neural Networks for ranking in search engines may\nobviate the need for the extensive feature engineering common to current\nlearning-to-rank methods. However, we show that combining simple relevance\nmatching features like BM25 with existing Deep Neural Net models often\nsubstantially improves the accuracy of these models, indicating that they do\nnot capture essential local relevance matching signals. We describe a novel\ndeep Recurrent Neural Net-based model that we call Match-Tensor. The\narchitecture of the Match-Tensor model simultaneously accounts for both local\nrelevance matching and global topicality signals allowing for a rich interplay\nbetween them when computing the relevance of a document to a query. On a large\nheld-out test set consisting of social media documents, we demonstrate not only\nthat Match-Tensor outperforms BM25 and other classes of DNNs but also that it\nlargely subsumes signals present in these models.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 17:59:38 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Jaech", "Aaron", ""], ["Kamisetty", "Hetunandan", ""], ["Ringger", "Eric", ""], ["Clarke", "Charlie", ""]]}, {"id": "1701.07807", "submitter": "Hua Sun", "authors": "Hua Sun and Syed A. Jafar", "title": "Private Information Retrieval from MDS Coded Data with Colluding\n  Servers: Settling a Conjecture by Freij-Hollanti et al.", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A $(K, N, T, K_c)$ instance of the MDS-TPIR problem is comprised of $K$\nmessages and $N$ distributed servers. Each message is separately encoded\nthrough a $(K_c, N)$ MDS storage code. A user wishes to retrieve one message,\nas efficiently as possible, while revealing no information about the desired\nmessage index to any colluding set of up to $T$ servers. The fundamental limit\non the efficiency of retrieval, i.e., the capacity of MDS-TPIR is known only at\nthe extremes where either $T$ or $K_c$ belongs to $\\{1,N\\}$. The focus of this\nwork is a recent conjecture by Freij-Hollanti, Gnilke, Hollanti and Karpuk\nwhich offers a general capacity expression for MDS-TPIR. We prove that the\nconjecture is false by presenting as a counterexample a PIR scheme for the\nsetting $(K, N, T, K_c) = (2,4,2,2)$, which achieves the rate $3/5$, exceeding\nthe conjectured capacity, $4/7$. Insights from the counterexample lead us to\ncapacity characterizations for various instances of MDS-TPIR including all\ncases with $(K, N, T, K_c) = (2,N,T,N-1)$, where $N$ and $T$ can be arbitrary.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 18:35:53 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 17:13:14 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Sun", "Hua", ""], ["Jafar", "Syed A.", ""]]}, {"id": "1701.07810", "submitter": "Mucahid Kutlu", "authors": "Mucahid Kutlu, Tamer Elsayed, Matthew Lease", "title": "Intelligent Topic Selection for Low-Cost Information Retrieval\n  Evaluation: A New Perspective on Deep vs. Shallow Judging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While test collections provide the cornerstone for Cranfield-based evaluation\nof information retrieval (IR) systems, it has become practically infeasible to\nrely on traditional pooling techniques to construct test collections at the\nscale of today's massive document collections. In this paper, we propose a new\nintelligent topic selection method which reduces the number of search topics\nneeded for reliable IR evaluation. To rigorously assess our method, we\nintegrate previously disparate lines of research on intelligent topic selection\nand deep vs. shallow judging. While prior work on intelligent topic selection\nhas never been evaluated against shallow judging baselines, prior work on deep\nvs. shallow judging has largely argued for shallowed judging, but assuming\nrandom topic selection. We argue that for evaluating any topic selection\nmethod, ultimately one must ask whether it is actually useful to select topics,\nor should one simply perform shallow judging over many topics? In seeking a\nrigorous answer to this over-arching question, we conduct a comprehensive\ninvestigation over a set of relevant factors never previously studied together\n1) topic selection method 2) the effect of topic familiarity on human judging\nspeed and 3) how different topic generation processes impact (i) budget\nutilization and (ii) the resultant quality of judgments. Experiments on NIST\nTREC Robust 2003 and Robust 2004 test collections show that not only can we\nreliably evaluate IR systems with fewer topics, but also that 1) when topics\nare intelligently selected, deep judging is often more cost-effective than\nshallow judging in evaluation reliability and 2) topic familiarity and topic\ngeneration costs greatly impact the evaluation cost vs. reliability trade-off.\nOur findings challenge conventional wisdom in showing that deep judging is\noften preferable to shallow judging when topics are selected intelligently.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 18:44:37 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 18:21:14 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2017 12:29:41 GMT"}, {"version": "v4", "created": "Tue, 19 Sep 2017 13:14:36 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Kutlu", "Mucahid", ""], ["Elsayed", "Tamer", ""], ["Lease", "Matthew", ""]]}, {"id": "1701.07955", "submitter": "Saiful Islam Md", "authors": "Syed Mehedi Hasan Nirob, Md. Kazi Nayeem and Md. Saiful Islam", "title": "Statistical Analysis on Bangla Newspaper Data to Extract Trending Topic\n  and Visualize Its Change Over Time", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trending topic of newspapers is an indicator to understand the situation of a\ncountry and also a way to evaluate the particular newspaper. This paper\nrepresents a model describing few techniques to select trending topics from\nBangla Newspaper. Topics that are discussed more frequently than other in\nBangla newspaper will be marked and how a very famous topic loses its\nimportance with the change of time and another topic takes its place will be\ndemonstrated. Data from two popular Bangla Newspaper with date and time were\ncollected. Statistical analysis was performed after on these data after\npreprocessing. Popular and most used keywords were extracted from the stream of\nBangla keyword with this analysis. This model can also cluster category wise\nnews trend or a list of news trend in daily or weekly basis with enough data. A\npattern can be found on their news trend too. Comparison among past news trend\nof Bangla newspapers will give a visualization of the situation of Bangladesh.\nThis visualization will be helpful to predict future trending topics of Bangla\nNewspaper.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 06:30:21 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Nirob", "Syed Mehedi Hasan", ""], ["Nayeem", "Md. Kazi", ""], ["Islam", "Md. Saiful", ""]]}, {"id": "1701.08229", "submitter": "Danielle Mowery PhD", "authors": "Danielle Mowery and Craig Bryan and Mike Conway", "title": "Feature Studies to Inform the Classification of Depressive Symptoms from\n  Twitter Data for Population Health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The utility of Twitter data as a medium to support population-level mental\nhealth monitoring is not well understood. In an effort to better understand the\npredictive power of supervised machine learning classifiers and the influence\nof feature sets for efficiently classifying depression-related tweets on a\nlarge-scale, we conducted two feature study experiments. In the first\nexperiment, we assessed the contribution of feature groups such as lexical\ninformation (e.g., unigrams) and emotions (e.g., strongly negative) using a\nfeature ablation study. In the second experiment, we determined the percentile\nof top ranked features that produced the optimal classification performance by\napplying a three-step feature elimination approach. In the first experiment, we\nobserved that lexical features are critical for identifying depressive\nsymptoms, specifically for depressed mood (-35 points) and for disturbed sleep\n(-43 points). In the second experiment, we observed that the optimal F1-score\nperformance of top ranked features in percentiles variably ranged across\nclasses e.g., fatigue or loss of energy (5th percentile, 288 features) to\ndepressed mood (55th percentile, 3,168 features) suggesting there is no\nconsistent count of features for predicting depressive-related tweets. We\nconclude that simple lexical features and reduced feature sets can produce\ncomparable results to larger feature sets.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 00:32:40 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Mowery", "Danielle", ""], ["Bryan", "Craig", ""], ["Conway", "Mike", ""]]}, {"id": "1701.08256", "submitter": "Philipp Kemkes", "authors": "Nattiya Kanhabua, Philipp Kemkes, Wolfgang Nejdl, Tu Ngoc Nguyen,\n  Felipe Reis, Nam Khanh Tran", "title": "How to Search the Internet Archive Without Indexing It", "comments": null, "journal-ref": "20th International Conference on Theory and Practice of Digital\n  Libraries, TPDL 2016, Proceedings, pp 147-160", "doi": "10.1007/978-3-319-43997-6_12", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant parts of cultural heritage are produced on the web during the\nlast decades. While easy accessibility to the current web is a good baseline,\noptimal access to the past web faces several challenges. This includes dealing\nwith large-scale web archive collections and lacking of usage logs that contain\nimplicit human feedback most relevant for today's web search. In this paper, we\npropose an entity-oriented search system to support retrieval and analytics on\nthe Internet Archive. We use Bing to retrieve a ranked list of results from the\ncurrent web. In addition, we link retrieved results to the WayBack Machine;\nthus allowing keyword search on the Internet Archive without processing and\nindexing its raw archived content. Our search system complements existing web\narchive search tools through a user-friendly interface, which comes close to\nthe functionalities of modern web search engines (e.g., keyword search, query\nauto-completion and related query suggestion), and provides a great benefit of\ntaking user feedback on the current web into account also for web archive\nsearch. Through extensive experiments, we conduct quantitative and qualitative\nanalyses in order to provide insights that enable further research on and\npractical applications of web archives.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 05:46:46 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Kanhabua", "Nattiya", ""], ["Kemkes", "Philipp", ""], ["Nejdl", "Wolfgang", ""], ["Nguyen", "Tu Ngoc", ""], ["Reis", "Felipe", ""], ["Tran", "Nam Khanh", ""]]}, {"id": "1701.08285", "submitter": "Philipp Kemkes", "authors": "Stefan Siersdorfer, Philipp Kemkes, Hanno Ackermann, Sergej Zerr", "title": "Who With Whom And How?: Extracting Large Social Networks Using Search\n  Engines", "comments": null, "journal-ref": "CIKM 2015 Proceedings of the 24th ACM International on Conference\n  on Information and Knowledge Management Pages 1491-1500", "doi": "10.1145/2806416.2806582", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network analysis is leveraged in a variety of applications such as\nidentifying influential entities, detecting communities with special interests,\nand determining the flow of information and innovations. However, existing\napproaches for extracting social networks from unstructured Web content do not\nscale well and are only feasible for small graphs. In this paper, we introduce\nnovel methodologies for query-based search engine mining, enabling efficient\nextraction of social networks from large amounts of Web data. To this end, we\nuse patterns in phrase queries for retrieving entity connections, and employ a\nbootstrapping approach for iteratively expanding the pattern set. Our\nexperimental evaluation in different domains demonstrates that our algorithms\nprovide high quality results and allow for scalable and efficient construction\nof social graphs.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 12:32:03 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Siersdorfer", "Stefan", ""], ["Kemkes", "Philipp", ""], ["Ackermann", "Hanno", ""], ["Zerr", "Sergej", ""]]}, {"id": "1701.08511", "submitter": "Diego Valsesia", "authors": "Diego Valsesia, Enrico Magli", "title": "Binary adaptive embeddings from order statistics of random projections", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2016.2639036", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use some of the largest order statistics of the random projections of a\nreference signal to construct a binary embedding that is adapted to signals\ncorrelated with such signal. The embedding is characterized from the analytical\nstandpoint and shown to provide improved performance on tasks such as\nclassification in a reduced-dimensionality space.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 08:37:25 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Valsesia", "Diego", ""], ["Magli", "Enrico", ""]]}, {"id": "1701.08744", "submitter": "Junaid Effendi", "authors": "Muhammad Junaid Effendi and Syed Abbas Ali", "title": "Click Through Rate Prediction for Contextual Advertisment Using Linear\n  Regression", "comments": "8 pages, 13 Figures, 11 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This research presents an innovative and unique way of solving the\nadvertisement prediction problem which is considered as a learning problem over\nthe past several years. Online advertising is a multi-billion-dollar industry\nand is growing every year with a rapid pace. The goal of this research is to\nenhance click through rate of the contextual advertisements using Linear\nRegression. In order to address this problem, a new technique propose in this\npaper to predict the CTR which will increase the overall revenue of the system\nby serving the advertisements more suitable to the viewers with the help of\nfeature extraction and displaying the advertisements based on context of the\npublishers. The important steps include the data collection, feature\nextraction, CTR prediction and advertisement serving. The statistical results\nobtained from the dynamically used technique show an efficient outcome by\nfitting the data close to perfection for the LR technique using optimized\nfeature selection.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 18:32:59 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Effendi", "Muhammad Junaid", ""], ["Ali", "Syed Abbas", ""]]}, {"id": "1701.08888", "submitter": "Guangneng Hu", "authors": "Guang-Neng Hu, Xin-Yu Dai", "title": "Integrating Reviews into Personalized Ranking for Cold Start\n  Recommendation", "comments": "TextBPR", "journal-ref": "PAKDD 2017", "doi": "10.1007/978-3-319-57529-2_55", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item recommendation task predicts a personalized ranking over a set of items\nfor each individual user. One paradigm is the rating-based methods that\nconcentrate on explicit feedbacks and hence face the difficulties in collecting\nthem. Meanwhile, the ranking-based methods are presented with rated items and\nthen rank the rated above the unrated. This paradigm takes advantage of widely\navailable implicit feedback. It, however, usually ignores a kind of important\ninformation: item reviews. Item reviews not only justify the preferences of\nusers, but also help alleviate the cold-start problem that fails the\ncollaborative filtering. In this paper, we propose two novel and simple models\nto integrate item reviews into Bayesian personalized ranking. In each model, we\nmake use of text features extracted from item reviews using word embeddings. On\ntop of text features we uncover the review dimensions that explain the\nvariation in users' feedback and these review factors represent a prior\npreference of users. Experiments on six real-world data sets show the benefits\nof leveraging item reviews on ranking prediction. We also conduct analyses to\nunderstand the proposed models.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 02:13:57 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 08:50:40 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Hu", "Guang-Neng", ""], ["Dai", "Xin-Yu", ""]]}, {"id": "1701.09039", "submitter": "Bryan Perozzi", "authors": "Aria Rezaei, Bryan Perozzi, Leman Akoglu", "title": "Ties That Bind - Characterizing Classes by Attributes and Social Ties", "comments": "WWW'17 Web Science, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of attributed subgraphs known to be from different classes, how\ncan we discover their differences? There are many cases where collections of\nsubgraphs may be contrasted against each other. For example, they may be\nassigned ground truth labels (spam/not-spam), or it may be desired to directly\ncompare the biological networks of different species or compound networks of\ndifferent chemicals.\n  In this work we introduce the problem of characterizing the differences\nbetween attributed subgraphs that belong to different classes. We define this\ncharacterization problem as one of partitioning the attributes into as many\ngroups as the number of classes, while maximizing the total attributed quality\nscore of all the given subgraphs.\n  We show that our attribute-to-class assignment problem is NP-hard and an\noptimal $(1 - 1/e)$-approximation algorithm exists. We also propose two\ndifferent faster heuristics that are linear-time in the number of attributes\nand subgraphs. Unlike previous work where only attributes were taken into\naccount for characterization, here we exploit both attributes and social ties\n(i.e. graph structure).\n  Through extensive experiments, we compare our proposed algorithms, show\nfindings that agree with human intuition on datasets from Amazon co-purchases,\nCongressional bill sponsorships, and DBLP co-authorships. We also show that our\napproach of characterizing subgraphs is better suited for sense-making than\ndiscriminating classification approaches.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 14:01:04 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Rezaei", "Aria", ""], ["Perozzi", "Bryan", ""], ["Akoglu", "Leman", ""]]}, {"id": "1701.09049", "submitter": "Amit Awekar", "authors": "Panthadeep Bhattacharjee and Amit Awekar", "title": "Batch Incremental Shared Nearest Neighbor Density Based Clustering\n  Algorithm for Dynamic Datasets", "comments": "6 pages, Accepted at ECIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental data mining algorithms process frequent updates to dynamic\ndatasets efficiently by avoiding redundant computation. Existing incremental\nextension to shared nearest neighbor density based clustering (SNND) algorithm\ncannot handle deletions to dataset and handles insertions only one point at a\ntime. We present an incremental algorithm to overcome both these bottlenecks by\nefficiently identifying affected parts of clusters while processing updates to\ndataset in batch mode. We show effectiveness of our algorithm by performing\nexperiments on large synthetic as well as real world datasets. Our algorithm is\nup to four orders of magnitude faster than SNND and requires up to 60% extra\nmemory than SNND while providing output identical to SNND.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 14:19:18 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Bhattacharjee", "Panthadeep", ""], ["Awekar", "Amit", ""]]}, {"id": "1701.09138", "submitter": "Maged Eljazzar", "authors": "Maged M. Eljazzar, Afnan Hassan, and Amira A. AlSharkawy", "title": "Towards A Time Based Video Search Engine for Al Quran Interpretation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of Internet Muslim-users is remarkably increasing from all over\nthe world countries. There are a lot of structured, and well-documented text\nresources for the Quran interpretation, Tafsir, over the Internet with several\nlanguages. Nevertheless, when searching for the meaning of specific words, many\nusers prefer watching short videos rather than reading a script or a book. This\npaper introduces the solution for the challenge of partitioning the common\nTafsir videos into short videos according to the search query and sharing these\nresult videos on the social networks. Furthermore, we provide the ability of\nuser commenting on a specific time-based frame on the video or a specific verse\nin a particular subject. It would be very valuable to apply the current\ntechnologies on Holy Quran and Tafsir to easy the query for verses,\nunderstanding of its meaning, and sharing it on the different social media.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 15:17:55 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Eljazzar", "Maged M.", ""], ["Hassan", "Afnan", ""], ["AlSharkawy", "Amira A.", ""]]}]