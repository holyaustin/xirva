[{"id": "1908.00071", "submitter": "Yashar Deldjoo", "authors": "Jens Adamczak, Gerard-Paul Leyson, Peter Knees, Yashar Deldjoo,\n  Farshad Bakhshandegan Moghaddam, Julia Neidhardt, Wolfgang W\\\"orndl, Philipp\n  Monreal", "title": "Session-Based Hotel Recommendations: Challenges and Future Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the year 2019, the Recommender Systems Challenge deals with a real-world\ntask from the area of e-tourism for the first time, namely the recommendation\nof hotels in booking sessions. In this context, this article aims at\nidentifying and investigating what we believe are important domain-specific\nchallenges recommendation systems research in hotel search is facing, from both\nacademic and industry perspectives. We focus on three main challenges, namely\ndealing with (1) multiple stakeholders and value-awareness in recommendations,\n(2) sparsity of user data and the extensive cold-start problem, and (3) dynamic\ninput data and computational requirements. To this end, we review the state of\nthe art toward solving these challenges and discuss shortcomings. We detail\npossible future directions and visions we contemplate for the further evolution\nof the field. This article should, therefore, serve two purposes: giving the\ninterested reader an overview of current challenges in the field and inspiring\nnew approaches for the ACM Recommender Systems Challenge 2019 and beyond.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 20:12:02 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Adamczak", "Jens", ""], ["Leyson", "Gerard-Paul", ""], ["Knees", "Peter", ""], ["Deldjoo", "Yashar", ""], ["Moghaddam", "Farshad Bakhshandegan", ""], ["Neidhardt", "Julia", ""], ["W\u00f6rndl", "Wolfgang", ""], ["Monreal", "Philipp", ""]]}, {"id": "1908.00148", "submitter": "Mansura A. Khan", "authors": "Mansura A. Khan, Ellen Rushe, Barry Smyth and David Coyle", "title": "Personalized, Health-Aware Recipe Recommendation: An Ensemble Topic\n  Modeling Based Approach", "comments": "This is a pre-print version of the accepted full-paper in\n  HealthRecsys2019 workshop (https://healthrecsys.github.io/2019/). The final\n  version of the article would be published in the workshop preceding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food choices are personal and complex and have a significant impact on our\nlong-term health and quality of life. By helping users to make informed and\nsatisfying decisions, Recommender Systems (RS) have the potential to support\nusers in making healthier food choices. Intelligent users-modeling is a key\nchallenge in achieving this potential. This paper investigates Ensemble Topic\nModelling (EnsTM) based Feature Identification techniques for efficient\nuser-modeling and recipe recommendation. It builds on findings in EnsTM to\npropose a reduced data representation format and a smart user-modeling strategy\nthat makes capturing user-preference fast, efficient and interactive. This\napproach enables personalization, even in a cold-start scenario. This paper\nproposes two different EnsTM based and one Hybrid EnsTM based recommenders. We\ncompared all three EnsTM based variations through a user study with 48\nparticipants, using a large-scale,real-world corpus of 230,876 recipes, and\ncompare against a conventional Content Based (CB) approach. EnsTM based\nrecommenders performed significantly better than the CB approach. Besides\nacknowledging multi-domain contents such as taste, demographics and cost, our\nproposed approach also considers user's nutritional preference and assists them\nfinding recipes under diverse nutritional categories. Furthermore, it provides\nexcellent coverage and enables implicit understanding of user's food practices.\nSubsequent analysis also exposed correlation between certain features and a\nhealthier lifestyle.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 23:51:51 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Khan", "Mansura A.", ""], ["Rushe", "Ellen", ""], ["Smyth", "Barry", ""], ["Coyle", "David", ""]]}, {"id": "1908.00277", "submitter": "Zhaosong Huang", "authors": "Zhaosong Huang, Ye Zhao, Wei Chen, Shengjie Gao, Kejie Yu, Weixia Xu,\n  Mingjie Tang, Minfeng Zhu, and Mingliang Xu", "title": "A Natural-language-based Visual Query Approach of Uncertain Human\n  Trajectories", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934671", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual querying is essential for interactively exploring massive trajectory\ndata. However, the data uncertainty imposes profound challenges to fulfill\nadvanced analytics requirements. On the one hand, many underlying data does not\ncontain accurate geographic coordinates, e.g., positions of a mobile phone only\nrefer to the regions (i.e., mobile cell stations) in which it resides, instead\nof accurate GPS coordinates. On the other hand, domain experts and general\nusers prefer a natural way, such as using a natural language sentence, to\naccess and analyze massive movement data. In this paper, we propose a visual\nanalytics approach that can extract spatial-temporal constraints from a textual\nsentence and support an effective query method over uncertain mobile trajectory\ndata. It is built up on encoding massive, spatially uncertain trajectories by\nthe semantic information of the POIs and regions covered by them, and then\nstoring the trajectory documents in text database with an effective indexing\nscheme. The visual interface facilitates query condition specification,\nsituation-aware visualization, and semantic exploration of large trajectory\ndata. Usage scenarios on real-world human mobility datasets demonstrate the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 09:00:59 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 01:50:00 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Huang", "Zhaosong", ""], ["Zhao", "Ye", ""], ["Chen", "Wei", ""], ["Gao", "Shengjie", ""], ["Yu", "Kejie", ""], ["Xu", "Weixia", ""], ["Tang", "Mingjie", ""], ["Zhu", "Minfeng", ""], ["Xu", "Mingliang", ""]]}, {"id": "1908.00413", "submitter": "Hoyeop Lee", "authors": "Hoyeop Lee, Jinbae Im, Seongwon Jang, Hyunsouk Cho, Sehee Chung", "title": "MeLU: Meta-Learned User Preference Estimator for Cold-Start\n  Recommendation", "comments": "Accepted as a full paper at KDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes a recommender system to alleviate the cold-start problem\nthat can estimate user preferences based on only a small number of items. To\nidentify a user's preference in the cold state, existing recommender systems,\nsuch as Netflix, initially provide items to a user; we call those items\nevidence candidates. Recommendations are then made based on the items selected\nby the user. Previous recommendation studies have two limitations: (1) the\nusers who consumed a few items have poor recommendations and (2) inadequate\nevidence candidates are used to identify user preferences. We propose a\nmeta-learning-based recommender system called MeLU to overcome these two\nlimitations. From meta-learning, which can rapidly adopt new task with a few\nexamples, MeLU can estimate new user's preferences with a few consumed items.\nIn addition, we provide an evidence candidate selection strategy that\ndetermines distinguishing items for customized preference estimation. We\nvalidate MeLU with two benchmark datasets, and the proposed model reduces at\nleast 5.92% mean absolute error than two comparative models on the datasets. We\nalso conduct a user study experiment to verify the evidence selection strategy.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 07:43:00 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Lee", "Hoyeop", ""], ["Im", "Jinbae", ""], ["Jang", "Seongwon", ""], ["Cho", "Hyunsouk", ""], ["Chung", "Sehee", ""]]}, {"id": "1908.00419", "submitter": "Derek Bridge", "authors": "Derek Bridge and Mesut Kaya and Pablo Castells", "title": "Sudden Death: A New Way to Compare Recommendation Diversification", "comments": "4 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes problems with the current way we compare the diversity\nof different recommendation lists in offline experiments. We illustrate the\nproblems with a case study. We propose the Sudden Death score as a new and\nbetter way of making these comparisons.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 09:07:13 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Bridge", "Derek", ""], ["Kaya", "Mesut", ""], ["Castells", "Pablo", ""]]}, {"id": "1908.00469", "submitter": "Rishiraj Saha Roy", "authors": "Xiaolu Lu, Soumajit Pramanik, Rishiraj Saha Roy, Abdalghani Abujabal,\n  Yafang Wang, Gerhard Weikum", "title": "Answering Complex Questions by Joining Multi-Document Evidence with\n  Quasi Knowledge Graphs", "comments": "SIGIR 2019 Long Paper, 10 pages", "journal-ref": null, "doi": "10.1145/3331184.3331252", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct answering of questions that involve multiple entities and relations is\na challenge for text-based QA. This problem is most pronounced when answers can\nbe found only by joining evidence from multiple documents. Curated knowledge\ngraphs (KGs) may yield good answers, but are limited by their inherent\nincompleteness and potential staleness. This paper presents QUEST, a method\nthat can answer complex questions directly from textual sources on-the-fly, by\ncomputing similarity joins over partial results from different documents. Our\nmethod is completely unsupervised, avoiding training-data bottlenecks and being\nable to cope with rapidly evolving ad hoc topics and formulation style in user\nquestions. QUEST builds a noisy quasi KG with node and edge weights, consisting\nof dynamically retrieved entity names and relational phrases. It augments this\ngraph with types and semantic alignments, and computes the best answers by an\nalgorithm for Group Steiner Trees. We evaluate QUEST on benchmarks of complex\nquestions, and show that it substantially outperforms state-of-the-art\nbaselines.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 15:57:42 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 19:04:48 GMT"}, {"version": "v3", "created": "Wed, 13 May 2020 18:50:21 GMT"}, {"version": "v4", "created": "Sat, 28 Nov 2020 22:17:47 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lu", "Xiaolu", ""], ["Pramanik", "Soumajit", ""], ["Roy", "Rishiraj Saha", ""], ["Abujabal", "Abdalghani", ""], ["Wang", "Yafang", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1908.00475", "submitter": "Mennatallah El-Assady", "authors": "Mennatallah El-Assady, Rebecca Kehlbeck, Christopher Collins, Daniel\n  Keim, Oliver Deussen", "title": "Semantic Concept Spaces: Guided Topic Model Refinement using\n  Word-Embedding Projections", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2019", "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework that allows users to incorporate the semantics of\ntheir domain knowledge for topic model refinement while remaining\nmodel-agnostic. Our approach enables users to (1) understand the semantic space\nof the model, (2) identify regions of potential conflicts and problems, and (3)\nreadjust the semantic relation of concepts based on their understanding,\ndirectly influencing the topic modeling. These tasks are supported by an\ninteractive visual analytics workspace that uses word-embedding projections to\ndefine concept regions which can then be refined. The user-refined concepts are\nindependent of a particular document collection and can be transferred to\nrelated corpora. All user interactions within the concept space directly affect\nthe semantic relations of the underlying vector space model, which, in turn,\nchange the topic modeling. In addition to direct manipulation, our system\nguides the users' decision-making process through recommended interactions that\npoint out potential improvements. This targeted refinement aims at minimizing\nthe feedback required for an efficient human-in-the-loop process. We confirm\nthe improvements achieved through our approach in two user studies that show\ntopic model quality improvements through our visual knowledge externalization\nand learning process.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 16:02:04 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["El-Assady", "Mennatallah", ""], ["Kehlbeck", "Rebecca", ""], ["Collins", "Christopher", ""], ["Keim", "Daniel", ""], ["Deussen", "Oliver", ""]]}, {"id": "1908.00648", "submitter": "Amine Trabelsi", "authors": "Amine Trabelsi and Osmar R. Zaiane", "title": "Contrastive Reasons Detection and Clustering from Online Polarized\n  Debate", "comments": "Best paper award in CICLing 2019: International Conference on\n  Computational Linguistics and Intelligent Text Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work tackles the problem of unsupervised modeling and extraction of the\nmain contrastive sentential reasons conveyed by divergent viewpoints on\npolarized issues. It proposes a pipeline approach centered around the detection\nand clustering of phrases, assimilated to argument facets using a novel Phrase\nAuthor Interaction Topic-Viewpoint model. The evaluation is based on the\ninformativeness, the relevance and the clustering accuracy of extracted\nreasons. The pipeline approach shows a significant improvement over\nstate-of-the-art methods in contrastive summarization on online debate\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 22:42:36 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Trabelsi", "Amine", ""], ["Zaiane", "Osmar R.", ""]]}, {"id": "1908.00814", "submitter": "Pengcheng Lin", "authors": "Wan-Lei Zhao, Hui Wang, Peng-Cheng Lin, and Chong-Wah Ngo", "title": "On the Merge of k-NN Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  k-nearest neighbor graph is a fundamental data structure in many disciplines\nsuch as information retrieval, data-mining, pattern recognition, and machine\nlearning, etc. In the literature, considerable research has been focusing on\nhow to efficiently build an approximate k-nearest neighbor graph (k-NN graph)\nfor a fixed dataset. Unfortunately, a closely related issue of how to merge two\nexisting k-NN graphs has been overlooked. In this paper, we address the issue\nof k-NN graph merging in two different scenarios. In the first scenario, a\nsymmetric merge algorithm is proposed to combine two approximate k-NN graphs.\nThe algorithm facilitates large-scale processing by the efficient merging of\nk-NN graphs that are produced in parallel. In the second scenario, a joint\nmerge algorithm is proposed to expand an existing k-NN graph with a raw\ndataset. The algorithm enables the incremental construction of a hierarchical\napproximate k-NN graph. Superior performance is attained when leveraging the\nhierarchy for NN search of various data types, dimensionality, and distance\nmeasures.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 11:46:42 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 08:16:48 GMT"}, {"version": "v3", "created": "Tue, 27 Aug 2019 01:45:23 GMT"}, {"version": "v4", "created": "Fri, 13 Mar 2020 06:37:05 GMT"}, {"version": "v5", "created": "Wed, 15 Apr 2020 04:07:35 GMT"}, {"version": "v6", "created": "Thu, 29 Jul 2021 11:07:40 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Zhao", "Wan-Lei", ""], ["Wang", "Hui", ""], ["Lin", "Peng-Cheng", ""], ["Ngo", "Chong-Wah", ""]]}, {"id": "1908.00831", "submitter": "Masoud Mansoury", "authors": "Masoud Mansoury, Bamshad Mobasher, Robin Burke, Mykola Pechenizkiy", "title": "Bias Disparity in Collaborative Recommendation: Algorithmic Evaluation\n  and Comparison", "comments": "Workshop on Recommendation in Multi-Stakeholder Environments (RMSE)\n  at ACM RecSys 2019, Copenhagen, Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on fairness in machine learning has been recently extended to\nrecommender systems. One of the factors that may impact fairness is bias\ndisparity, the degree to which a group's preferences on various item categories\nfail to be reflected in the recommendations they receive. In some cases biases\nin the original data may be amplified or reversed by the underlying\nrecommendation algorithm. In this paper, we explore how different\nrecommendation algorithms reflect the tradeoff between ranking quality and bias\ndisparity. Our experiments include neighborhood-based, model-based, and\ntrust-aware recommendation algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 13:00:27 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Mansoury", "Masoud", ""], ["Mobasher", "Bamshad", ""], ["Burke", "Robin", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "1908.00977", "submitter": "Elisabeth Lex", "authors": "Elisabeth Lex, Dominik Kowald", "title": "The Impact of Time on Hashtag Reuse in Twitter: A Cognitive-Inspired\n  Hashtag Recommendation Approach", "comments": "49. GI-Jahrestagung INFORMATIK 2019, Best of Data Science Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In our work [KPL17], we study temporal usage patterns of Twitter hashtags,\nand we use the Base-Level Learning (BLL) equation from the cognitive\narchitecture ACT-R [An04] to model how a person reuses her own, individual\nhashtags as well as hashtags from her social network. The BLL equation accounts\nfor the time-dependent decay of item exposure in human memory. According to\nBLL, the usefulness of a piece of information (e.g., a hashtag) is defined by\nhow frequently and how recently it was used in the past, following a\ntime-dependent decay that is best modeled with a power-law distribution. We\nused the BLL equation in our previous work to recommend tags in social\nbookmarking systems [KL16]. Here [KPL17], we adopt the BLL equation to model\ntemporal reuse patterns of individual (i.e., reusing own hashtags) and social\nhashtags (i.e., reusing hashtags, which has been previously used by a followee)\nand to build a cognitive-inspired hashtag recommendation algorithm. We\ndemonstrate the efficacy of our approach in two empirical social networks\ncrawled from Twitter, i.e., CompSci and Random (for details about the datasets,\nsee [KPL17]). Our results show that our approach can outperform current\nstate-of-the-art hashtag recommendation approaches.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 11:40:05 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Lex", "Elisabeth", ""], ["Kowald", "Dominik", ""]]}, {"id": "1908.01031", "submitter": "Adam Gudy\\'s", "authors": "Adam Gudy\\'s, Marek Sikora, {\\L}ukasz Wr\\'obel", "title": "RuleKit: A Comprehensive Suite for Rule-Based Learning", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": "10.1016/j.knosys.2020.105480", "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rule-based models are often used for data analysis as they combine\ninterpretability with predictive power. We present RuleKit, a versatile tool\nfor rule learning. Based on a sequential covering induction algorithm, it is\nsuitable for classification, regression, and survival problems. The presence of\na user-guided induction facilitates verifying hypotheses concerning data\ndependencies which are expected or of interest. The powerful and flexible\nexperimental environment allows straightforward investigation of different\ninduction schemes. The analysis can be performed in batch mode, through\nRapidMiner plug-in, or R package. A documented Java API is also provided for\nconvenience. The software is publicly available at GitHub under GNU AGPL-3.0\nlicense.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 19:53:46 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Gudy\u015b", "Adam", ""], ["Sikora", "Marek", ""], ["Wr\u00f3bel", "\u0141ukasz", ""]]}, {"id": "1908.01061", "submitter": "Martin Strohmeier", "authors": "Martin Strohmeier, Matthew Smith, Vincent Lenders, Ivan Martinovic", "title": "Classi-Fly: Inferring Aircraft Categories from Open Data using Machine\n  Learning", "comments": "10 pages, 6 figures, 8 tables, 40 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, air traffic communication data has become easy to access,\nenabling novel research in many fields. Exploiting this new data source, a wide\nrange of applications have emerged, from weather forecasting to stock market\nprediction, or the collection of information about military and government\nmovements. Typically these applications require knowledge about the metadata of\nthe aircraft, specifically its operator and the aircraft category.\n  armasuisse Science + Technology, the R\\&D agency for the Swiss Armed Forces,\nhas been developing Classi-Fly, a novel approach to obtain metadata about\naircraft based on their movement patterns. We validate Classi-Fly using several\nhundred thousand flights collected through open source means, in conjunction\nwith ground truth from publicly available aircraft registries containing more\nthan two million aircraft. Classi-Fly obtains the correct aircraft category\nwith an accuracy of over 88%, demonstrating that it can improve the meta data\nnecessary for applications working with air traffic communication. Finally, we\nshow that it is feasible to automatically detect specific flights such as\npolice and surveillance missions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 17:31:25 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 15:08:01 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Strohmeier", "Martin", ""], ["Smith", "Matthew", ""], ["Lenders", "Vincent", ""], ["Martinovic", "Ivan", ""]]}, {"id": "1908.01099", "submitter": "Yixin Su", "authors": "Yixin Su, Sarah Monazam Erfani, Rui Zhang", "title": "MMF: Attribute Interpretable Collaborative Filtering", "comments": "8 pages, IJCNN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is one of the most popular techniques in designing\nrecommendation systems, and its most representative model, matrix\nfactorization, has been wildly used by researchers and the industry. However,\nthis model suffers from the lack of interpretability and the item cold-start\nproblem, which limit its reliability and practicability. In this paper, we\npropose an interpretable recommendation model called Multi-Matrix Factorization\n(MMF), which addresses these two limitations and achieves the state-of-the-art\nprediction accuracy by exploiting common attributes that are present in\ndifferent items. In the model, predicted item ratings are regarded as weighted\naggregations of attribute ratings generated by the inner product of the user\nlatent vectors and the attribute latent vectors. MMF provides more fine grained\nanalyses than matrix factorization in the following ways: attribute ratings\nwith weights allow the understanding of how much each attribute contributes to\nthe recommendation and hence provide interpretability; the common attributes\ncan act as a link between existing and new items, which solves the item\ncold-start problem when no rating exists on an item. We evaluate the\ninterpretability of MMF comprehensively, and conduct extensive experiments on\nreal datasets to show that MMF outperforms state-of-the-art baselines in terms\nof accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 01:10:41 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Su", "Yixin", ""], ["Erfani", "Sarah Monazam", ""], ["Zhang", "Rui", ""]]}, {"id": "1908.01304", "submitter": "Shaojie Qu", "authors": "Shaojie Qu, Kan Li, Zheyi Fan, Sisi Wu, Xinyi Liu and Zhiguo Huang", "title": "Behavior Pattern and Compiled Information Based Performance Prediction\n  in MOOCs", "comments": "5 pages, 1 figures, 6 tables accepted by FIE2019. In proceeding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of MOOCs massive open online courses, increasingly more\nsubjects can be studied online. Researchers currently show growing interest in\nthe field of MOOCs, including dropout prediction, cheating detection and\nachievement prediction. Previous studies on achievement prediction mainly\nfocused on students' video and forum behaviors, and few researchers have\nconsidered how well students perform their assignments. In this paper, we\nchoose a C programming course as the experimental subject, which involved 1528\nstudents. This paper mainly focuses on the students' accomplishment behaviors\nin programming assignments and compiled information from programming\nassignments. In this paper, feature sequences are extracted from the logs\naccording to submission times, submission order and plagiarism. The\nexperimental results show that the students who did not pass the exam had\nobvious sequence patterns but that the students who passed the test did not\nhave an obvious sequence pattern. Then, we extract 23 features from the\ncompiled information of students' programming assignments and select the most\ndistinguishing features to predict the students' performances. The experimental\nresults show that we can obtain an accuracy rate of 0.7049 for predicting\nstudents' performances.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 09:40:06 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Qu", "Shaojie", ""], ["Li", "Kan", ""], ["Fan", "Zheyi", ""], ["Wu", "Sisi", ""], ["Liu", "Xinyi", ""], ["Huang", "Zhiguo", ""]]}, {"id": "1908.01351", "submitter": "Atri Mandal", "authors": "Atri Mandal, Shivali Agarwal, Nikhil Malhotra, Giriprasad Sridhara,\n  Anupama Ray, Daivik Swarup", "title": "Improving IT Support by Enhancing Incident Management Process with\n  Multi-modal Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IT support services industry is going through a major transformation with AI\nbecoming commonplace. There has been a lot of effort in the direction of\nautomation at every human touchpoint in the IT support processes. Incident\nmanagement is one such process which has been a beacon process for AI based\nautomation. The vision is to automate the process from the time an\nincident/ticket arrives till it is resolved and closed. While text is the\nprimary mode of communicating the incidents, there has been a growing trend of\nusing alternate modalities like image to communicate the problem. A large\nfraction of IT support tickets today contain attached image data in the form of\nscreenshots, log messages, invoices and so on. These attachments help in better\nexplanation of the problem which aids in faster resolution. Anybody who aspires\nto provide AI based IT support, it is essential to build systems which can\nhandle multi-modal content. In this paper we present how incident management in\nIT support domain can be made much more effective using multi-modal analysis.\nThe information extracted from different modalities are correlated to enrich\nthe information in the ticket and used for better ticket routing and\nresolution. We evaluate our system using about 25000 real tickets containing\nattachments from selected problem areas. Our results demonstrate significant\nimprovements in both routing and resolution with the use of multi-modal ticket\nanalysis compared to only text based analysis.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 14:27:11 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Mandal", "Atri", ""], ["Agarwal", "Shivali", ""], ["Malhotra", "Nikhil", ""], ["Sridhara", "Giriprasad", ""], ["Ray", "Anupama", ""], ["Swarup", "Daivik", ""]]}, {"id": "1908.01505", "submitter": "Hiroki Tanioka Dr", "authors": "Hiroki Tanioka", "title": "A Fast Content-Based Image Retrieval Method Using Deep Visual Features", "comments": "accepted in ICDAR-WML: The 2nd International Workshop on Machine\n  Learning 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and scalable Content-Based Image Retrieval using visual features is\nrequired for document analysis, Medical image analysis, etc. in the present\nage. Convolutional Neural Network (CNN) activations as features achieved their\noutstanding performance in this area. Deep Convolutional representations using\nthe softmax function in the output layer are also ones among visual features.\nHowever, almost all the image retrieval systems hold their index of visual\nfeatures on main memory in order to high responsiveness, limiting their\napplicability for big data applications. In this paper, we propose a fast\ncalculation method of cosine similarity with L2 norm indexed in advance on\nElasticsearch. We evaluate our approach with ImageNet Dataset and VGG-16\npre-trained model. The evaluation results show the effectiveness and efficiency\nof our proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 08:09:36 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Tanioka", "Hiroki", ""]]}, {"id": "1908.01519", "submitter": "Momchil Hardalov", "authors": "Momchil Hardalov, Ivan Koychev, Preslav Nakov", "title": "Beyond English-Only Reading Comprehension: Experiments in Zero-Shot\n  Multilingual Transfer for Bulgarian", "comments": "Accepted at RANLP 2019 (13 pages, 2 figures, 6 tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, reading comprehension models achieved near-human performance on\nlarge-scale datasets such as SQuAD, CoQA, MS Macro, RACE, etc. This is largely\ndue to the release of pre-trained contextualized representations such as BERT\nand ELMo, which can be fine-tuned for the target task. Despite those advances\nand the creation of more challenging datasets, most of the work is still done\nfor English. Here, we study the effectiveness of multilingual BERT fine-tuned\non large-scale English datasets for reading comprehension (e.g., for RACE), and\nwe apply it to Bulgarian multiple-choice reading comprehension. We propose a\nnew dataset containing 2,221 questions from matriculation exams for twelfth\ngrade in various subjects -history, biology, geography and philosophy-, and 412\nadditional questions from online quizzes in history. While the quiz authors\ngave no relevant context, we incorporate knowledge from Wikipedia, retrieving\ndocuments matching the combination of question + each answer option. Moreover,\nwe experiment with different indexing and pre-training strategies. The\nevaluation results show accuracy of 42.23%, which is well above the baseline of\n24.89%.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 08:45:20 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 09:33:46 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Hardalov", "Momchil", ""], ["Koychev", "Ivan", ""], ["Nakov", "Preslav", ""]]}, {"id": "1908.01587", "submitter": "Amir Mosavi Prof", "authors": "Muhammad Zubair Asghar, Fazli Subhan, Muhammad Imran, Fazal Masud\n  Kundi, Shahboddin Shamshirband, Amir Mosavi, Peter Csiba, Annamaria R.\n  Varkonyi-Koczy", "title": "Performance Evaluation of Supervised Machine Learning Techniques for\n  Efficient Detection of Emotions from Online Content", "comments": "30 pages, 13 tables, 1 figure", "journal-ref": null, "doi": "10.20944/preprints201908.0019.v1", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotion detection from the text is an important and challenging problem in\ntext analytics. The opinion-mining experts are focusing on the development of\nemotion detection applications as they have received considerable attention of\nonline community including users and business organization for collecting and\ninterpreting public emotions. However, most of the existing works on emotion\ndetection used less efficient machine learning classifiers with limited\ndatasets, resulting in performance degradation. To overcome this issue, this\nwork aims at the evaluation of the performance of different machine learning\nclassifiers on a benchmark emotion dataset. The experimental results show the\nperformance of different machine learning classifiers in terms of different\nevaluation metrics like precision, recall ad f-measure. Finally, a classifier\nwith the best performance is recommended for the emotion classification.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 16:48:22 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Asghar", "Muhammad Zubair", ""], ["Subhan", "Fazli", ""], ["Imran", "Muhammad", ""], ["Kundi", "Fazal Masud", ""], ["Shamshirband", "Shahboddin", ""], ["Mosavi", "Amir", ""], ["Csiba", "Peter", ""], ["Varkonyi-Koczy", "Annamaria R.", ""]]}, {"id": "1908.01798", "submitter": "Dar\\'io Garigliotti", "authors": "Dar\\'io Garigliotti and Dyaa Albakour and Miguel Martinez and\n  Krisztian Balog", "title": "Unsupervised Context Retrieval for Long-tail Entities", "comments": "Proceedings of the 2019 ACM International Conference on Theory of\n  Information Retrieval (ICTIR' 19)", "journal-ref": null, "doi": "10.1145/3341981.3344244", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring entities in media streams often relies on rich entity\nrepresentations, like structured information available in a knowledge base\n(KB). For long-tail entities, such monitoring is highly challenging, due to\ntheir limited, if not entirely missing, representation in the reference KB. In\nthis paper, we address the problem of retrieving textual contexts for\nmonitoring long-tail entities. We propose an unsupervised method to overcome\nthe limited representation of long-tail entities by leveraging established\nentities and their contexts as support information. Evaluation on a\npurpose-built test collection shows the suitability of our approach and its\nrobustness for out-of-KB entities.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 18:28:09 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Garigliotti", "Dar\u00edo", ""], ["Albakour", "Dyaa", ""], ["Martinez", "Miguel", ""], ["Balog", "Krisztian", ""]]}, {"id": "1908.01815", "submitter": "Taha Shangipour Ataei", "authors": "Taha Shangipour Ataei, Kamyar Darvishi, Soroush Javdan, Behrouz\n  Minaei-Bidgoli, Sauleh Eetemadi", "title": "Pars-ABSA: an Aspect-based Sentiment Analysis dataset for Persian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increased availability of online reviews, sentiment analysis had\nbeen witnessed a booming interest from the researchers. Sentiment analysis is a\ncomputational treatment of sentiment used to extract and understand the\nopinions of authors. While many systems were built to predict the sentiment of\na document or a sentence, many others provide the necessary detail on various\naspects of the entity (i.e. aspect-based sentiment analysis). Most of the\navailable data resources were tailored to English and the other popular\nEuropean languages. Although Persian is a language with more than 110 million\nspeakers, to the best of our knowledge, there is a lack of public dataset on\naspect-based sentiment analysis for Persian. This paper provides a manually\nannotated Persian dataset, Pars-ABSA, which is verified by 3 native Persian\nspeakers. The dataset consists of 5,114 positive, 3,061 negative and 1,827\nneutral data samples from 5,602 unique reviews. Moreover, as a baseline, this\npaper reports the performance of some state-of-the-art aspect-based sentiment\nanalysis methods with a focus on deep learning, on Pars-ABSA. The obtained\nresults are impressive compared to similar English state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 16:19:07 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 08:09:31 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 14:35:42 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Ataei", "Taha Shangipour", ""], ["Darvishi", "Kamyar", ""], ["Javdan", "Soroush", ""], ["Minaei-Bidgoli", "Behrouz", ""], ["Eetemadi", "Sauleh", ""]]}, {"id": "1908.01837", "submitter": "Chenwei Zhang", "authors": "Chenwei Zhang", "title": "Structured Knowledge Discovery from Massive Text Corpus", "comments": "PhD Thesis, University of Illinois at Chicago, July 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, with the booming development of the Internet, people benefit from\nits convenience due to its open and sharing nature. A large volume of natural\nlanguage texts is being generated by users in various forms, such as search\nqueries, documents, and social media posts. As the unstructured text corpus is\nusually noisy and messy, it becomes imperative to correctly identify and\naccurately annotate structured information in order to obtain meaningful\ninsights or better understand unstructured texts. On the other hand, the\nexisting structured information, which embodies our knowledge such as entity or\nconcept relations, often suffers from incompleteness or quality-related issues.\nGiven a gigantic collection of texts which offers rich semantic information, it\nis also important to harness the massiveness of the unannotated text corpus to\nexpand and refine existing structured knowledge with fewer annotation efforts.\n  In this dissertation, I will introduce principles, models, and algorithms for\neffective structured knowledge discovery from the massive text corpus. We are\ngenerally interested in obtaining insights and better understanding\nunstructured texts with the help of structured annotations or by\nstructure-aware modeling. Also, given the existing structured knowledge, we are\ninterested in expanding its scale and improving its quality harnessing the\nmassiveness of the text corpus. In particular, four problems are studied in\nthis dissertation: Structured Intent Detection for Natural Language\nUnderstanding, Structure-aware Natural Language Modeling, Generative Structured\nKnowledge Expansion, and Synonym Refinement on Structured Knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 20:45:41 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Zhang", "Chenwei", ""]]}, {"id": "1908.01839", "submitter": "Ping Wang", "authors": "Ping Wang, Tian Shi, Chandan K. Reddy", "title": "Text-to-SQL Generation for Question Answering on Electronic Medical\n  Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic medical records (EMR) contain comprehensive patient information\nand are typically stored in a relational database with multiple tables.\nEffective and efficient patient information retrieval from EMR data is a\nchallenging task for medical experts. Question-to-SQL generation methods tackle\nthis problem by first predicting the SQL query for a given question about a\ndatabase, and then, executing the query on the database. However, most of the\nexisting approaches have not been adapted to the healthcare domain due to a\nlack of healthcare Question-to-SQL dataset for learning models specific to this\ndomain. In addition, wide use of the abbreviation of terminologies and possible\ntypos in questions introduce additional challenges for accurately generating\nthe corresponding SQL queries. In this paper, we tackle these challenges by\ndeveloping a deep learning based TRanslate-Edit Model for Question-to-SQL\n(TREQS) generation, which adapts the widely used sequence-to-sequence model to\ndirectly generate the SQL query for a given question, and further performs the\nrequired edits using an attentive-copying mechanism and task-specific look-up\ntables. Based on the widely used publicly available electronic medical\ndatabase, we create a new large-scale Question-SQL pair dataset, named\nMIMICSQL, in order to perform the Question-to-SQL generation task in healthcare\ndomain. An extensive set of experiments are conducted to evaluate the\nperformance of our proposed model on MIMICSQL. Both quantitative and\nqualitative experimental results indicate the flexibility and efficiency of our\nproposed method in predicting condition values and its robustness to random\nquestions with abbreviations and typos.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 21:04:05 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 04:20:44 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Wang", "Ping", ""], ["Shi", "Tian", ""], ["Reddy", "Chandan K.", ""]]}, {"id": "1908.01868", "submitter": "Omar Alonso", "authors": "Omar Alonso, Vasileios Kandylas and Serge-Eric Tremblay", "title": "Local versus Global Strategies in Social Query Expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Link sharing in social media can be seen as a collaboratively retrieved set\nof documents for a query or topic expressed by a hashtag. Temporal information\nplays an important role for identifying the correct context for which such\nannotations are valid for retrieval purposes. We investigate how social data as\ntemporal context can be used for query expansion and compare global versus\nlocal strategies for computing such contextual information for a set of\nhashtags.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 21:31:10 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Alonso", "Omar", ""], ["Kandylas", "Vasileios", ""], ["Tremblay", "Serge-Eric", ""]]}, {"id": "1908.02425", "submitter": "John Brandt", "authors": "John Brandt", "title": "Text mining policy: Classifying forest and landscape restoration policy\n  agenda with neural information retrieval", "comments": "In FEED 19 Workshop at KDD 2019. Anchorage, AK, USA, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dozens of countries have committed to restoring the ecological functionality\nof 350 million hectares of land by 2030. In order to achieve such wide-scale\nimplementation of restoration, the values and priorities of multi-sectoral\nstakeholders must be aligned and integrated with national level commitments and\nother development agenda. Although misalignment across scales of policy and\nbetween stakeholders are well known barriers to implementing restoration,\nfast-paced policy making in multi-stakeholder environments complicates the\nmonitoring and analysis of governance and policy. In this work, we assess the\npotential of machine learning to identify restoration policy agenda across\ndiverse policy documents. An unsupervised neural information retrieval\narchitecture is introduced that leverages transfer learning and word embeddings\nto create high-dimensional representations of paragraphs. Policy agenda labels\nare recast as information retrieval queries in order to classify policies with\na cosine similarity threshold between paragraphs and query embeddings. This\napproach achieves a 0.83 F1-score measured across 14 policy agenda in 31 policy\ndocuments in Malawi, Kenya, and Rwanda, indicating that automated text mining\ncan provide reliable, generalizable, and efficient analyses of restoration\npolicy.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 02:58:24 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Brandt", "John", ""]]}, {"id": "1908.02451", "submitter": "Manish Patel", "authors": "Manish Patel", "title": "TinySearch -- Semantics based Search Engine using Bert Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing search engines use keyword matching or tf-idf based matching to map\nthe query to the web-documents and rank them. They also consider other factors\nsuch as page rank, hubs-and-authority scores, knowledge graphs to make the\nresults more meaningful. However, the existing search engines fail to capture\nthe meaning of query when it becomes large and complex. BERT, introduced by\nGoogle in 2018, provides embeddings for words as well as sentences. In this\npaper, I have developed a semantics-oriented search engine using neural\nnetworks and BERT embeddings that can search for query and rank the documents\nin the order of the most meaningful to least meaningful. The results shows\nimprovement over one existing search engine for complex queries for given set\nof documents.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 06:02:17 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Patel", "Manish", ""]]}, {"id": "1908.02569", "submitter": "Kyungmin Kim", "authors": "Kyung-Min Kim, Donghyun Kwak, Hanock Kwak, Young-Jin Park, Sangkwon\n  Sim, Jae-Han Cho, Minkyu Kim, Jihun Kwon, Nako Sung, and Jung-Woo Ha", "title": "Tripartite Heterogeneous Graph Propagation for Large-scale Social\n  Recommendation", "comments": "6 pages, accepted for RecSys 2019 LBR Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have been emerging as a promising method for\nrelational representation including recommender systems. However, various\nchallenging issues of social graphs hinder the practical usage of GNNs for\nsocial recommendation, such as their complex noisy connections and high\nheterogeneity. The oversmoothing of GNNs is an obstacle of GNN-based social\nrecommendation as well. Here we propose a new graph embedding method\nHeterogeneous Graph Propagation (HGP) to tackle these issues. HGP uses a\ngroup-user-item tripartite graph as input to reduce the number of edges and the\ncomplexity of paths in a social graph. To solve the oversmoothing issue, HGP\nembeds nodes under a personalized PageRank based propagation scheme, separately\nfor group-user graph and user-item graph. Node embeddings from each graph are\nintegrated using an attention mechanism. We evaluate our HGP on a large-scale\nreal-world dataset consisting of 1,645,279 nodes and 4,711,208 edges. The\nexperimental results show that HGP outperforms several baselines in terms of\nAUC and F1-score metrics.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 08:27:07 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Kim", "Kyung-Min", ""], ["Kwak", "Donghyun", ""], ["Kwak", "Hanock", ""], ["Park", "Young-Jin", ""], ["Sim", "Sangkwon", ""], ["Cho", "Jae-Han", ""], ["Kim", "Minkyu", ""], ["Kwon", "Jihun", ""], ["Sung", "Nako", ""], ["Ha", "Jung-Woo", ""]]}, {"id": "1908.02571", "submitter": "Afshin Sadeghi", "authors": "Afshin Sadeghi and Jens Lehmann", "title": "Linking Physicians to Medical Research Results via Knowledge Graph\n  Embeddings and Twitter", "comments": "AI for Good, Data Science for Social Good, Machine learning for\n  Social Good, Twitter Data, Knowledge Graph Embeddings, Medical Research", "journal-ref": "ECML SOGOOD 2019", "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Informing professionals about the latest research results in their field is a\nparticularly important task in the field of health care, since any development\nin this field directly improves the health status of the patients. Meanwhile,\nsocial media is an infrastructure that allows public instant sharing of\ninformation, thus it has recently become popular in medical applications. In\nthis study, we apply Multi Distance Knowledge Graph Embeddings (MDE) to link\nphysicians and surgeons to the latest medical breakthroughs that are shared as\nthe research results on Twitter. Our study shows that using this method\nphysicians can be informed about the new findings in their field given that\nthey have an account dedicated to their profession.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 10:15:40 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 14:37:36 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2020 14:25:56 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Sadeghi", "Afshin", ""], ["Lehmann", "Jens", ""]]}, {"id": "1908.02786", "submitter": "V\\'itor Louren\\c{c}o", "authors": "V\\'itor N. Louren\\c{c}o, Gabriela G. Silva, Leandro A. F. Fernandes", "title": "Hierarchy-of-Visual-Words: a Learning-based Approach for Trademark Image\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present the Hierarchy-of-Visual-Words (HoVW), a novel\ntrademark image retrieval (TIR) method that decomposes images into simpler\ngeometric shapes and defines a descriptor for binary trademark image\nrepresentation by encoding the hierarchical arrangement of component shapes.\nThe proposed hierarchical organization of visual data stores each component\nshape as a visual word. It is capable of representing the geometry of\nindividual elements and the topology of the trademark image, making the\ndescriptor robust against linear as well as to some level of nonlinear\ntransformation. Experiments show that HoVW outperforms previous TIR methods on\nthe MPEG-7 CE-1 and MPEG-7 CE-2 image databases.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 18:19:43 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Louren\u00e7o", "V\u00edtor N.", ""], ["Silva", "Gabriela G.", ""], ["Fernandes", "Leandro A. F.", ""]]}, {"id": "1908.02804", "submitter": "Trevor Bostic", "authors": "Trevor Bostic, Jeff Stanley, John Higgins, Rachael L.\n  Bradley-Montgomery, Justin F. Brunelle, Daniel Chudnov", "title": "Exploring the Intersections of Web Science and Accessibility", "comments": "10 pages, Latex", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The web is the prominent way information is exchanged in the 21st century.\nHowever, ensuring web-based information is accessible is complicated,\nparticularly with web applications that rely on JavaScript and other\ntechnologies to deliver and build representations; representations are often\nthe HTML, images, or other code a server delivers for a web resource. Static\nrepresentations are becoming rarer and assessing the accessibility of web-based\ninformation to ensure it is available to all users is increasingly difficult\ngiven the dynamic nature of representations.\n  In this work, we survey three ongoing research threads that can inform web\naccessibility solutions: assessing web accessibility, modeling web user\nactivity, and web application crawling. Current web accessibility research is\ncontinually focused on increasing the percentage of automatically testable\nstandards, but still relies heavily upon manual testing for complex interactive\napplications. Along-side web accessibility research, there are mechanisms\ndeveloped by researchers that replicate user interactions with web pages based\non usage patterns. Crawling web applications is a broad research domain;\nexposing content in web applications is difficult because of incompatibilities\nin web crawlers and the technologies used to create the applications. We\ndescribe research on crawling the deep web by exercising user forms. We close\nwith a thought exercise regarding the convergence of these three threads and\nthe future of automated, web-based accessibility evaluation and assurance\nthrough a use case in web archiving. These research efforts provide insight\ninto how users interact with websites, how to automate and simulate user\ninteractions, how to record the results of user interactions, and how to\nanalyze, evaluate, and map resulting website content to determine its relative\naccessibility.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 19:15:58 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Bostic", "Trevor", ""], ["Stanley", "Jeff", ""], ["Higgins", "John", ""], ["Bradley-Montgomery", "Rachael L.", ""], ["Brunelle", "Justin F.", ""], ["Chudnov", "Daniel", ""]]}, {"id": "1908.02819", "submitter": "Michele Weigle", "authors": "Lulwah M. Alkwai, Michael L. Nelson, Michele C. Weigle", "title": "Making Recommendations from Web Archives for \"Lost\" Web Pages", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When a user requests a web page from a web archive, the user will typically\neither get an HTTP 200 if the page is available, or an HTTP 404 if the web page\nhas not been archived. This is because web archives are typically accessed by\nURI lookup, and the response is binary: the archive either has the page or it\ndoes not, and the user will not know of other archived web pages that exist and\nare potentially similar to the requested web page. In this paper, we propose\naugmenting these binary responses with a model for selecting and ranking\nrecommended web pages in a Web archive. This is to enhance both HTTP 404\nresponses and HTTP 200 responses by surfacing web pages in the archive that the\nuser may not know existed. First, we check if the URI is already classified in\nDMOZ or Wikipedia. If the requested URI is not found, we use ML to classify the\nURI using DMOZ as our ontology and collect candidate URIs to recommended to the\nuser. Next, we filter the candidates based on if they are present in the\narchive. Finally, we rank candidates based on several features, such as\narchival quality, web page popularity, temporal similarity, and URI similarity.\nWe calculated the F1 score for different methods of classifying the requested\nweb page at the first level. We found that using all-grams from the URI after\nremoving numerals and the TLD produced the best result with F1=0.59. For\nsecond-level classification, the micro-average F1=0.30. We found that 44.89% of\nthe correctly classified URIs contained at least one word that exists in a\ndictionary and 50.07% of the correctly classified URIs contained long strings\nin the domain. In comparison with the URIs from our Wayback access logs, only\n5.39% of those URIs contained only words from a dictionary, and 26.74%\ncontained at least one word from a dictionary. These percentages are low and\nmay affect the ability for the requested URI to be correctly classified.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 20:11:30 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Alkwai", "Lulwah M.", ""], ["Nelson", "Michael L.", ""], ["Weigle", "Michele C.", ""]]}, {"id": "1908.02938", "submitter": "Yue Yin", "authors": "Yue Yin, Chenyan Xiong, Cheng Luo, Zhiyuan Liu", "title": "Neural Document Expansion with User Feedback", "comments": "The 2019 ACM SIGIR International Conference on the Theory of\n  Information Retrieval", "journal-ref": null, "doi": "10.1145/3341981.3344213", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a neural document expansion approach (NeuDEF) that\nenriches document representations for neural ranking models. NeuDEF harvests\nexpansion terms from queries which lead to clicks on the document and weights\nthese expansion terms with learned attention. It is plugged into a standard\nneural ranker and learned end-to-end. Experiments on a commercial search log\ndemonstrate that NeuDEF significantly improves the accuracy of state-of-the-art\nneural rankers and expansion methods on queries with different frequencies.\nFurther studies show the contribution of click queries and learned expansion\nweights, as well as the influence of document popularity of NeuDEF's\neffectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 05:45:57 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Yin", "Yue", ""], ["Xiong", "Chenyan", ""], ["Luo", "Cheng", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "1908.03141", "submitter": "Xinting Huang", "authors": "Xinting Huang, Jianzhong Qi, Yu Sun, Rui Zhang, Hai-Tao Zheng", "title": "CARL: Aggregated Search with Context-Aware Module Embedding Learning", "comments": "IJCNN2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregated search aims to construct search result pages (SERPs) from\nblue-links and heterogeneous modules (such as news, images, and videos).\nExisting studies have largely ignored the correlations between blue-links and\nheterogeneous modules when selecting the heterogeneous modules to be presented.\nWe observe that the top ranked blue-links, which we refer to as the\n\\emph{context}, can provide important information about query intent and helps\nidentify the relevant heterogeneous modules. For example, informative terms\nlike \"streamed\" and \"recorded\" in the context imply that a video module may\nbetter satisfy the query. To model and utilize the context information for\naggregated search, we propose a model with context attention and representation\nlearning (CARL). Our model applies a recurrent neural network with an attention\nmechanism to encode the context, and incorporates the encoded context\ninformation into module embeddings. The context-aware module embeddings\ntogether with the ranking policy are jointly optimized under the Markov\ndecision process (MDP) formulation. To achieve a more effective joint learning,\nwe further propose an optimization function with self-supervision loss to\nprovide auxiliary supervision signals. Experimental results based on two public\ndatasets demonstrate the superiority of CARL over multiple baseline approaches,\nand confirm the effectiveness of the proposed optimization function in boosting\nthe joint learning process.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 01:44:36 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Huang", "Xinting", ""], ["Qi", "Jianzhong", ""], ["Sun", "Yu", ""], ["Zhang", "Rui", ""], ["Zheng", "Hai-Tao", ""]]}, {"id": "1908.03142", "submitter": "Chen Ma", "authors": "Chen Ma", "title": "The Hitchhiker's Guide to LDA", "comments": "148 pages, in Chinese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation (LDA) model is a famous model in the topic model\nfield, it has been studied for years due to its extensive application value in\nindustry and academia. However, the mathematical derivation of LDA model is\nchallenging and difficult, which makes it difficult for the beginners to learn.\nTo help the beginners in learning LDA, this book analyzes the mathematical\nderivation of LDA in detail, and it also introduces all the knowledge\nbackground to make it easy for beginners to understand. Thus, this book\ncontains the author's unique insights. It should be noted that this book is\nwritten in Chinese.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 03:59:19 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 12:41:30 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Ma", "Chen", ""]]}, {"id": "1908.03313", "submitter": "Neelmadhav Gantayat", "authors": "Prateeti Mohapatra, Neelamadhav Gantayat, Gargi B. Dasgupta", "title": "Using Semantic Role Knowledge for Relevance Ranking of Key Phrases in\n  Documents: An Unsupervised Approach", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the integration of sentence position and\nsemantic role of words in a PageRank system to build a key phrase ranking\nmethod. We present the evaluation results of our approach on three scientific\narticles. We show that semantic role information, when integrated with a\nPageRank system, can become a new lexical feature. Our approach had an overall\nimprovement on all the data sets over the state-of-art baseline approaches.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 04:44:12 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Mohapatra", "Prateeti", ""], ["Gantayat", "Neelamadhav", ""], ["Dasgupta", "Gargi B.", ""]]}, {"id": "1908.03361", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Kai Schr\\\"oter, Moritz M\\\"unch, Bin Yang, Andrea Unger,\n  Doris Dransch, Joachim Denzler", "title": "Enhancing Flood Impact Analysis using Interactive Retrieval of Social\n  Media Images", "comments": null, "journal-ref": "Archives of Data Science, Series A, 5.1, 2018", "doi": "10.5445/KSP/1000087327/06", "report-no": null, "categories": "cs.IR cs.CV cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The analysis of natural disasters such as floods in a timely manner often\nsuffers from limited data due to a coarse distribution of sensors or sensor\nfailures. This limitation could be alleviated by leveraging information\ncontained in images of the event posted on social media platforms, so-called\n\"Volunteered Geographic Information (VGI)\". To save the analyst from the need\nto inspect all images posted online manually, we propose to use content-based\nimage retrieval with the possibility of relevance feedback for retrieving only\nrelevant images of the event to be analyzed. To evaluate this approach, we\nintroduce a new dataset of 3,710 flood images, annotated by domain experts\nregarding their relevance with respect to three tasks (determining the flooded\narea, inundation depth, water pollution). We compare several image features and\nrelevance feedback methods on that dataset, mixed with 97,085 distractor\nimages, and are able to improve the precision among the top 100 retrieval\nresults from 55% with the baseline retrieval to 87% after 5 rounds of feedback.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 08:29:57 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["Schr\u00f6ter", "Kai", ""], ["M\u00fcnch", "Moritz", ""], ["Yang", "Bin", ""], ["Unger", "Andrea", ""], ["Dransch", "Doris", ""], ["Denzler", "Joachim", ""]]}, {"id": "1908.03451", "submitter": "Wenmian Yang", "authors": "Wenmian Yang, Weijia Jia, Wenyuan Gao, Xiaojie Zhou, Yutao Luo", "title": "Interactive Variance Attention based Online Spoiler Detection for\n  Time-Sync Comments", "comments": "Accepted by CIKM 2019", "journal-ref": null, "doi": "10.1145/3357384.3357872", "report-no": null, "categories": "cs.IR cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, time-sync comment (TSC), a new form of interactive comments, has\nbecome increasingly popular in Chinese video websites. By posting TSCs, people\ncan easily express their feelings and exchange their opinions with others when\nwatching online videos. However, some spoilers appear among the TSCs. These\nspoilers reveal crucial plots in videos that ruin people's surprise when they\nfirst watch the video. In this paper, we proposed a novel Similarity-Based\nNetwork with Interactive Variance Attention (SBN-IVA) to classify comments as\nspoilers or not. In this framework, we firstly extract textual features of TSCs\nthrough the word-level attentive encoder. We design Similarity-Based Network\n(SBN) to acquire neighbor and keyframe similarity according to semantic\nsimilarity and timestamps of TSCs. Then, we implement Interactive Variance\nAttention (IVA) to eliminate the impact of noise comments. Finally, we obtain\nthe likelihood of spoiler based on the difference between the neighbor and\nkeyframe similarity. Experiments show SBN-IVA is on average 11.2\\% higher than\nthe state-of-the-art method on F1-score in baselines.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 13:24:21 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 06:22:08 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Yang", "Wenmian", ""], ["Jia", "Weijia", ""], ["Gao", "Wenyuan", ""], ["Zhou", "Xiaojie", ""], ["Luo", "Yutao", ""]]}, {"id": "1908.03475", "submitter": "Tajul Rosli Razak Mr", "authors": "Mohammad Hafiz Ismail, Tajul Rosli Razak, Muhamad Arif Hashim, Alif\n  Faisal Ibrahim", "title": "A Simple Recommender Engine for Matching Final-Year Project Student with\n  Supervisor", "comments": null, "journal-ref": "Colloquium in Computer & Mathematical Sciences Education (2015)", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a simple recommender engine, which can match final year\nproject student based on their interests with potential supervisors. The\nrecommender engine is constructed based on Euclidean distance algorithm. The\ninitial input data for the recommender system is obtained by distributing\nquestionnaire to final year students and recording their response in CSV\nformat. The recommender engine is implemented using Java class and application,\nand result of the initial tests has shown promises that the project is feasible\nto be pursued as it has the potential of solving the problem of final year\nstudents in finding their potential supervisors.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 08:42:03 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Ismail", "Mohammad Hafiz", ""], ["Razak", "Tajul Rosli", ""], ["Hashim", "Muhamad Arif", ""], ["Ibrahim", "Alif Faisal", ""]]}, {"id": "1908.03548", "submitter": "Zongcheng Ji", "authors": "Zongcheng Ji, Qiang Wei, Hua Xu", "title": "BERT-based Ranking for Biomedical Entity Normalization", "comments": "9 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing high-performance entity normalization algorithms that can\nalleviate the term variation problem is of great interest to the biomedical\ncommunity. Although deep learning-based methods have been successfully applied\nto biomedical entity normalization, they often depend on traditional\ncontext-independent word embeddings. Bidirectional Encoder Representations from\nTransformers (BERT), BERT for Biomedical Text Mining (BioBERT) and BERT for\nClinical Text Mining (ClinicalBERT) were recently introduced to pre-train\ncontextualized word representation models using bidirectional Transformers,\nadvancing the state-of-the-art for many natural language processing tasks. In\nthis study, we proposed an entity normalization architecture by fine-tuning the\npre-trained BERT / BioBERT / ClinicalBERT models and conducted extensive\nexperiments to evaluate the effectiveness of the pre-trained models for\nbiomedical entity normalization using three different types of datasets. Our\nexperimental results show that the best fine-tuned models consistently\noutperformed previous methods and advanced the state-of-the-art for biomedical\nentity normalization, with up to 1.17% increase in accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 17:19:43 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Ji", "Zongcheng", ""], ["Wei", "Qiang", ""], ["Xu", "Hua", ""]]}, {"id": "1908.03608", "submitter": "Jorge Calvo-Zaragoza", "authors": "Jorge Calvo-Zaragoza, Jan Haji\\v{c} Jr., Alexander Pacha", "title": "Understanding Optical Music Recognition", "comments": null, "journal-ref": "ACM Comput. Surv. 53, 4 (2020) Article 77", "doi": "10.1145/3397499", "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For over 50 years, researchers have been trying to teach computers to read\nmusic notation, referred to as Optical Music Recognition (OMR). However, this\nfield is still difficult to access for new researchers, especially those\nwithout a significant musical background: few introductory materials are\navailable, and furthermore the field has struggled with defining itself and\nbuilding a shared terminology. In this tutorial, we address these shortcomings\nby (1) providing a robust definition of OMR and its relationship to related\nfields, (2) analyzing how OMR inverts the music encoding process to recover the\nmusical notation and the musical semantics from documents, (3) proposing a\ntaxonomy of OMR, with most notably a novel taxonomy of applications.\nAdditionally, we discuss how deep learning affects modern OMR research, as\nopposed to the traditional pipeline. Based on this work, the reader should be\nable to attain a basic understanding of OMR: its objectives, its inherent\nstructure, its relationship to other fields, the state of the art, and the\nresearch opportunities it affords.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 08:37:16 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 09:38:29 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 08:59:52 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Calvo-Zaragoza", "Jorge", ""], ["Haji\u010d", "Jan", "Jr."], ["Pacha", "Alexander", ""]]}, {"id": "1908.03650", "submitter": "Rishiraj Saha Roy", "authors": "Zhen Jia, Abdalghani Abujabal, Rishiraj Saha Roy, Jannik Stroetgen,\n  Gerhard Weikum", "title": "TEQUILA: Temporal Question Answering over Knowledge Bases", "comments": "CIKM 2018 Short Paper", "journal-ref": "CIKM 2018", "doi": "10.1145/3269206.3269247", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering over knowledge bases (KB-QA) poses challenges in handling\ncomplex questions that need to be decomposed into sub-questions. An important\ncase, addressed here, is that of temporal questions, where cues for temporal\nrelations need to be discovered and handled. We present TEQUILA, an enabler\nmethod for temporal QA that can run on top of any KB-QA engine. TEQUILA has\nfour stages. It detects if a question has temporal intent. It decomposes and\nrewrites the question into non-temporal sub-questions and temporal constraints.\nAnswers to sub-questions are then retrieved from the underlying KB-QA engine.\nFinally, TEQUILA uses constraint reasoning on temporal intervals to compute\nfinal answers to the full question. Comparisons against state-of-the-art\nbaselines show the viability of our method.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 22:41:20 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 15:35:36 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 15:32:19 GMT"}, {"version": "v4", "created": "Mon, 25 Jan 2021 09:20:05 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Jia", "Zhen", ""], ["Abujabal", "Abdalghani", ""], ["Roy", "Rishiraj Saha", ""], ["Stroetgen", "Jannik", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1908.03737", "submitter": "Donghuo Zeng", "authors": "Donghuo Zeng, Yi Yu, Keizo Oyama", "title": "Deep Triplet Neural Networks with Cluster-CCA for Audio-Visual\n  Cross-modal Retrieval", "comments": "21 pages,11 figures", "journal-ref": "ACM TOMM 2020", "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval aims to retrieve data in one modality by a query in\nanother modality, which has been a very interesting research issue in the field\nof multimedia, information retrieval, and computer vision, and database. Most\nexisting works focus on cross-modal retrieval between text-image, text-video,\nand lyrics-audio.Little research addresses cross-modal retrieval between audio\nand video due to limited audio-video paired datasets and semantic information.\nThe main challenge of audio-visual cross-modal retrieval task focuses on\nlearning joint embeddings from a shared subspace for computing the similarity\nacross different modalities, where generating new representations is to\nmaximize the correlation between audio and visual modalities space. In this\nwork, we propose a novel deep triplet neural network with cluster canonical\ncorrelation analysis(TNN-C-CCA), which is an end-to-end supervised learning\narchitecture with audio branch and video branch.We not only consider the\nmatching pairs in the common space but also compute the mismatching pairs when\nmaximizing the correlation. In particular, two significant contributions are\nmade: i) a better representation by constructing deep triplet neural network\nwith triplet loss for optimal projections can be generated to maximize\ncorrelation in the shared subspace. ii) positive examples and negative examples\nare used in the learning stage to improve the capability of embedding learning\nbetween audio and video. Our experiment is run over 5-fold cross-validation,\nwhere average performance is applied to demonstrate the performance of\naudio-video cross-modal retrieval. The experimental results achieved on two\ndifferent audio-visual datasets show the proposed learning architecture with\ntwo branches outperforms existing six CCA-based methods and four\nstate-of-the-art based cross-modal retrieval methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 12:03:48 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 13:06:50 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 17:07:43 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zeng", "Donghuo", ""], ["Yu", "Yi", ""], ["Oyama", "Keizo", ""]]}, {"id": "1908.03738", "submitter": "Donghuo Zeng", "authors": "Haoting Liang, Donghuo Zeng, Yi Yu, Keizo Oyama", "title": "Personalized Music Recommendation with Triplet Network", "comments": "1 figure; 1 table", "journal-ref": "DEIM 2019", "doi": null, "report-no": "SU-4240-720", "categories": "cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since many online music services emerged in recent years so that effective\nmusic recommendation systems are desirable. Some common problems in\nrecommendation system like feature representations, distance measure and cold\nstart problems are also challenges for music recommendation. In this paper, I\nproposed a triplet neural network, exploiting both positive and negative\nsamples to learn the representation and distance measure between users and\nitems, to solve the recommendation task.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 12:03:55 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Liang", "Haoting", ""], ["Zeng", "Donghuo", ""], ["Yu", "Yi", ""], ["Oyama", "Keizo", ""]]}, {"id": "1908.03744", "submitter": "Donghuo Zeng", "authors": "Donghuo Zeng, Yi Yu, Keizo Oyama", "title": "Audio-Visual Embedding for Cross-Modal MusicVideo Retrieval through\n  Supervised Deep CCA", "comments": "8 pages, 9 figures. Accepted by ISM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has successfully shown excellent performance in learning joint\nrepresentations between different data modalities. Unfortunately, little\nresearch focuses on cross-modal correlation learning where temporal structures\nof different data modalities, such as audio and video, should be taken into\naccount. Music video retrieval by given musical audio is a natural way to\nsearch and interact with music contents. In this work, we study cross-modal\nmusic video retrieval in terms of emotion similarity. Particularly, audio of an\narbitrary length is used to retrieve a longer or full-length music video. To\nthis end, we propose a novel audio-visual embedding algorithm by Supervised\nDeep CanonicalCorrelation Analysis (S-DCCA) that projects audio and video into\na shared space to bridge the semantic gap between audio and video. This also\npreserves the similarity between audio and visual contents from different\nvideos with the same class label and the temporal structure. The contribution\nof our approach is mainly manifested in the two aspects: i) We propose to\nselect top k audio chunks by attention-based Long Short-Term Memory\n(LSTM)model, which can represent good audio summarization with local\nproperties. ii) We propose an end-to-end deep model for cross-modal\naudio-visual learning where S-DCCA is trained to learn the semantic correlation\nbetween audio and visual modalities. Due to the lack of music video dataset, we\nconstruct 10K music video dataset from YouTube 8M dataset. Some promising\nresults such as MAP and precision-recall show that our proposed model can be\napplied to music video retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 12:29:05 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zeng", "Donghuo", ""], ["Yu", "Yi", ""], ["Oyama", "Keizo", ""]]}, {"id": "1908.03825", "submitter": "Manojkumar Rangasamy Kannadasan", "authors": "Saratchandra Indrakanti, Svetlana Strunjas, Shubhangi Tandon,\n  Manojkumar Rangasamy Kannadasan", "title": "Influence of Neighborhood on the Preference of an Item in eCommerce\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surfacing a ranked list of items for a search query to help buyers discover\ninventory and make purchase decisions is a critical problem in eCommerce\nsearch. Typically, items are independently predicted with a probability of sale\nwith respect to a given search query. But in a dynamic marketplace like eBay,\neven for a single product, there are various different factors distinguishing\none item from another which can influence the purchase decision for the user.\nUsers have to make a purchase decision by considering all of these options.\nMajority of the existing learning to rank algorithms model the relative\nrelevance between labeled items only at the loss functions like pairwise or\nlist-wise losses. But they are limited to point-wise scoring functions where\nitems are ranked independently based on the features of the item itself. In\nthis paper, we study the influence of an item's neighborhood to its purchase\ndecision. Here, we consider the neighborhood as the items ranked above and\nbelow the current item in search results. By adding delta features comparing\nitems within a neighborhood and learning a ranking model, we are able to\nexperimentally show that the new ranker with delta features outperforms our\nbaseline ranker in terms of Mean Reciprocal Rank (MRR). The ranking models with\nproposed delta features result in $3-5\\%$ improvement in MRR over the baseline\nmodel. We also study impact of different sizes for neighborhood. Experimental\nresults show that neighborhood size $3$ perform the best based on MRR with an\nimprovement of $4-5\\%$ over the baseline model.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 23:21:56 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 17:15:31 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Indrakanti", "Saratchandra", ""], ["Strunjas", "Svetlana", ""], ["Tandon", "Shubhangi", ""], ["Kannadasan", "Manojkumar Rangasamy", ""]]}, {"id": "1908.03846", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Jinsong Su, Jiebo Luo", "title": "Exploiting Temporal Relationships in Video Moment Localization with\n  Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of video moment localization with natural language,\ni.e. localizing a video segment described by a natural language sentence. While\nmost prior work focuses on grounding the query as a whole, temporal\ndependencies and reasoning between events within the text are not fully\nconsidered. In this paper, we propose a novel Temporal Compositional Modular\nNetwork (TCMN) where a tree attention network first automatically decomposes a\nsentence into three descriptions with respect to the main event, context event\nand temporal signal. Two modules are then utilized to measure the visual\nsimilarity and location similarity between each segment and the decomposed\ndescriptions. Moreover, since the main event and context event may rely on\ndifferent modalities (RGB or optical flow), we use late fusion to form an\nensemble of four models, where each model is independently trained by one\ncombination of the visual input. Experiments show that our model outperforms\nthe state-of-the-art methods on the TEMPO dataset.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 03:59:18 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zhang", "Songyang", ""], ["Su", "Jinsong", ""], ["Luo", "Jiebo", ""]]}, {"id": "1908.03957", "submitter": "Georgios Katsimpras", "authors": "Frosso Papanastasiou, Georgios Katsimpras and Georgios Paliouras", "title": "Tensor Factorization with Label Information for Fake News Detection", "comments": "Presented at the Workshop on Reducing Online Misinformation Exposure\n  ROME 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The buzz over the so-called \"fake news\" has created concerns about a\ndegenerated media environment and led to the need for technological solutions.\nAs the detection of fake news is increasingly considered a technological\nproblem, it has attracted considerable research. Most of these studies\nprimarily focus on utilizing information extracted from textual news content.\nIn contrast, we focus on detecting fake news solely based on structural\ninformation of social networks. We suggest that the underlying network\nconnections of users that share fake news are discriminative enough to support\nthe detection of fake news. Thereupon, we model each post as a network of\nfriendship interactions and represent a collection of posts as a\nmultidimensional tensor. Taking into account the available labeled data, we\npropose a tensor factorization method which associates the class labels of data\nsamples with their latent representations. Specifically, we combine a\nclassification error term with the standard factorization in a unified\noptimization process. Results on real-world datasets demonstrate that our\nproposed method is competitive against state-of-the-art methods by implementing\nan arguably simpler approach.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 19:51:48 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Papanastasiou", "Frosso", ""], ["Katsimpras", "Georgios", ""], ["Paliouras", "Georgios", ""]]}, {"id": "1908.04017", "submitter": "Emanuel Laci\\'c", "authors": "Dominik Kowald, Matthias Traub, Dieter Theiler, Heimo Gursch, Emanuel\n  Lacic, Stefanie Lindstaedt, Roman Kern, Elisabeth Lex", "title": "Using the Open Meta Kaggle Dataset to Evaluate Tripartite\n  Recommendations in Data Markets", "comments": "REVEAL workshop @ RecSys'2019, Kopenhagen, Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work addresses the problem of providing and evaluating recommendations\nin data markets. Since most of the research in recommender systems is focused\non the bipartite relationship between users and items (e.g., movies), we extend\nthis view to the tripartite relationship between users, datasets and services,\nwhich is present in data markets. Between these entities, we identify four use\ncases for recommendations: (i) recommendation of datasets for users, (ii)\nrecommendation of services for users, (iii) recommendation of services for\ndatasets, and (iv) recommendation of datasets for services. Using the open Meta\nKaggle dataset, we evaluate the recommendation accuracy of a popularity-based\nas well as a collaborative filtering-based algorithm for these four use cases\nand find that the recommendation accuracy strongly depends on the given use\ncase. The presented work contributes to the tripartite recommendation problem\nin general and to the under-researched portfolio of evaluating recommender\nsystems for data markets in particular.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 06:15:44 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 09:21:36 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Kowald", "Dominik", ""], ["Traub", "Matthias", ""], ["Theiler", "Dieter", ""], ["Gursch", "Heimo", ""], ["Lacic", "Emanuel", ""], ["Lindstaedt", "Stefanie", ""], ["Kern", "Roman", ""], ["Lex", "Elisabeth", ""]]}, {"id": "1908.04032", "submitter": "Yanru Qu", "authors": "Yanru Qu, Ting Bai, Weinan Zhang, Jianyun Nie, Jian Tang", "title": "An End-to-End Neighborhood-based Interaction Model for\n  Knowledge-enhanced Recommendation", "comments": "9 pages, accepted by DLP-KDD'19, best paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies graph-based recommendation, where an interaction graph is\nconstructed from historical records and is lever-aged to alleviate data\nsparsity and cold start problems. We reveal an early summarization problem in\nexisting graph-based models, and propose Neighborhood Interaction (NI) model to\ncapture each neighbor pair (between user-side and item-side) distinctively. NI\nmodel is more expressive and can capture more complicated structural patterns\nbehind user-item interactions. To further enrich node connectivity and utilize\nhigh-order structural information, we incorporate extra knowledge graphs (KGs)\nand adopt graph neural networks (GNNs) in NI, called Knowledge-enhanced\nNeighborhoodInteraction (KNI). Compared with the state-of-the-art\nrecommendation methods,e.g., feature-based, meta path-based, and KG-based\nmodels, our KNI achieves superior performance in click-through rate prediction\n(1.1%-8.4% absolute AUC improvements) and out-performs by a wide margin in\ntop-N recommendation on 4 real-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 07:19:11 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 04:35:15 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Qu", "Yanru", ""], ["Bai", "Ting", ""], ["Zhang", "Weinan", ""], ["Nie", "Jianyun", ""], ["Tang", "Jian", ""]]}, {"id": "1908.04042", "submitter": "Dominik Kowald PhD", "authors": "Emanuel Lacic, Dominik Kowald, Dieter Theiler, Matthias Traub, Lucky\n  Kuffer, Stefanie Lindstaedt, Elisabeth Lex", "title": "Evaluating Tag Recommendations for E-Book Annotation Using a Semantic\n  Similarity Metric", "comments": "REVEAL Workshop @ RecSys'2019, Kopenhagen, Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present our work to support publishers and editors in\nfinding descriptive tags for e-books through tag recommendations. We propose a\nhybrid tag recommendation system for e-books, which leverages search query\nterms from Amazon users and e-book metadata, which is assigned by publishers\nand editors. Our idea is to mimic the vocabulary of users in Amazon, who search\nfor and review e-books, and to combine these search terms with editor tags in a\nhybrid tag recommendation approach. In total, we evaluate 19 tag recommendation\nalgorithms on the review content of Amazon users, which reflects the readers'\nvocabulary. Our results show that we can improve the performance of tag\nrecommender systems for e-books both concerning tag recommendation accuracy,\ndiversity as well as a novel semantic similarity metric, which we also propose\nin this paper.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 08:04:42 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Lacic", "Emanuel", ""], ["Kowald", "Dominik", ""], ["Theiler", "Dieter", ""], ["Traub", "Matthias", ""], ["Kuffer", "Lucky", ""], ["Lindstaedt", "Stefanie", ""], ["Lex", "Elisabeth", ""]]}, {"id": "1908.04045", "submitter": "Yunshan Ma", "authors": "Yunshan Ma, Lizi Liao, Tat-Seng Chua", "title": "Automatic Fashion Knowledge Extraction from Social Media", "comments": "2 pages, 4 figures, ACMMM 2019 Demo", "journal-ref": null, "doi": "10.1145/3343031.3350607", "report-no": null, "categories": "cs.IR cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion knowledge plays a pivotal role in helping people in their dressing.\nIn this paper, we present a novel system to automatically harvest fashion\nknowledge from social media. It unifies three tasks of occasion, person and\nclothing discovery from multiple modalities of images, texts and metadata. A\ncontextualized fashion concept learning model is applied to leverage the rich\ncontextual information for improving the fashion concept learning performance.\nAt the same time, to counter the label noise within training data, we employ a\nweak label modeling method to further boost the performance. We build a website\nto demonstrate the quality of fashion knowledge extracted by our system.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 08:15:27 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Ma", "Yunshan", ""], ["Liao", "Lizi", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1908.04200", "submitter": "Roman Vainshtein", "authors": "Roman Vainshtein, Gilad Katz, Bracha Shapira, Lior Rokach", "title": "Assessing the Quality of Scientific Papers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multitude of factors are responsible for the overall quality of scientific\npapers, including readability, linguistic quality, fluency,semantic complexity,\nand of course domain-specific technical factors. These factors vary from one\nfield of study to another. In this paper, we propose a measure and method for\nassessing the overall quality of the scientific papers in a particular field of\nstudy. We evaluate our method in the computer science domain, but it can be\napplied to other technical and scientific fields.Our method is based on the\ncorpus linguistics technique. This technique enables the extraction of required\ninformation and knowledge associated with a specific domain. For this purpose,\nwe have created a large corpus, consisting of papers from very high impact\nconferences. First, we analyze this corpus in order to extract rich\ndomain-specific terminology and knowledge. Then we use the acquired knowledge\nto estimate the quality of scientific papers by applying our proposed measure.\nWe examine our measure on high and low scientific impact test corpora. Our\nresults show a significant difference in the measure scores of the high and low\nimpact test corpora. Second, we develop a classifier based on our proposed\nmeasure and compare it to the baseline classifier. Our results show that the\nclassifier based on our measure over-performed the baseline classifier. Based\non the presented results the proposed measure and the technique can be used for\nautomated assessment of scientific papers.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 15:32:10 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Vainshtein", "Roman", ""], ["Katz", "Gilad", ""], ["Shapira", "Bracha", ""], ["Rokach", "Lior", ""]]}, {"id": "1908.04364", "submitter": "Nitish Kulkarni", "authors": "Mansi Gupta, Nitish Kulkarni, Raghuveer Chanda, Anirudha Rayasam and\n  Zachary C Lipton", "title": "AmazonQA: A Review-Based Question Answering Task", "comments": "8 pages, 7 figures; IJCAI-19; first three authors contribute equally.\n  Data and code available at https://github.com/amazonqa/amazonqa", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Every day, thousands of customers post questions on Amazon product pages.\nAfter some time, if they are fortunate, a knowledgeable customer might answer\ntheir question. Observing that many questions can be answered based upon the\navailable product reviews, we propose the task of review-based QA. Given a\ncorpus of reviews and a question, the QA system synthesizes an answer. To this\nend, we introduce a new dataset and propose a method that combines information\nretrieval techniques for selecting relevant reviews (given a question) and\n\"reading comprehension\" models for synthesizing an answer (given a question and\nreview). Our dataset consists of 923k questions, 3.6M answers and 14M reviews\nacross 156k products. Building on the well-known Amazon dataset, we collect\nadditional annotations, marking each question as either answerable or\nunanswerable based on the available reviews. A deployed system could first\nclassify a question as answerable and then attempt to generate an answer.\nNotably, unlike many popular QA datasets, here, the questions, passages, and\nanswers are all extracted from real human interactions. We evaluate numerous\nmodels for answer generation and propose strong baselines, demonstrating the\nchallenging nature of this new task.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 20:18:50 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 23:34:19 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Gupta", "Mansi", ""], ["Kulkarni", "Nitish", ""], ["Chanda", "Raghuveer", ""], ["Rayasam", "Anirudha", ""], ["Lipton", "Zachary C", ""]]}, {"id": "1908.04464", "submitter": "Selasi Kwashie", "authors": "Jixue Liu, Selasi Kwashie, Jiuyong Li, Lin Liu and Michael Bewong", "title": "Linking Graph Entities with Multiplicity and Provenance", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity linking and resolution is a fundamental database problem with\napplications in data integration, data cleansing, information retrieval,\nknowledge fusion, and knowledge-base population. It is the task of accurately\nidentifying multiple, differing, and possibly contradicting representations of\nthe same real-world entity in data. In this work, we propose an entity linking\nand resolution system capable of linking entities across different databases\nand mentioned-entities extracted from text data. Our entity linking/resolution\nsolution, called Certus, uses a graph model to represent the profiles of\nentities. The graph model is versatile, thus, it is capable of handling\nmultiple values for an attribute or a relationship, as well as the provenance\ndescriptions of the values. Provenance descriptions of a value provide the\nsettings of the value, such as validity periods, sources, security\nrequirements, etc. This paper presents the architecture for the entity linking\nsystem, the logical, physical, and indexing models used in the system, and the\ngeneral linking process. Furthermore, we demonstrate the performance of update\noperations of the physical storage models when the system is implemented in two\nstate-of-the-art database management systems, HBase and Postgres.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 02:28:37 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 05:20:51 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Liu", "Jixue", ""], ["Kwashie", "Selasi", ""], ["Li", "Jiuyong", ""], ["Liu", "Lin", ""], ["Bewong", "Michael", ""]]}, {"id": "1908.04472", "submitter": "Peng Qi", "authors": "Peng Qi, Juan Cao, Tianyun Yang, Junbo Guo, and Jintao Li", "title": "Exploiting Multi-domain Visual Information for Fake News Detection", "comments": "10 pages, 9 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of social media promotes the proliferation of fake\nnews. With the development of multimedia technology, fake news attempts to\nutilize multimedia contents with images or videos to attract and mislead\nreaders for rapid dissemination, which makes visual contents an important part\nof fake news. Fake-news images, images attached in fake news posts,include not\nonly fake images which are maliciously tampered but also real images which are\nwrongly used to represent irrelevant events. Hence, how to fully exploit the\ninherent characteristics of fake-news images is an important but challenging\nproblem for fake news detection. In the real world, fake-news images may have\nsignificantly different characteristics from real-news images at both physical\nand semantic levels, which can be clearly reflected in the frequency and pixel\ndomain, respectively. Therefore, we propose a novel framework Multi-domain\nVisual Neural Network (MVNN) to fuse the visual information of frequency and\npixel domains for detecting fake news. Specifically, we design a CNN-based\nnetwork to automatically capture the complex patterns of fake-news images in\nthe frequency domain; and utilize a multi-branch CNN-RNN model to extract\nvisual features from different semantic levels in the pixel domain. An\nattention mechanism is utilized to fuse the feature representations of\nfrequency and pixel domains dynamically. Extensive experiments conducted on a\nreal-world dataset demonstrate that MVNN outperforms existing methods with at\nleast 9.2% in accuracy, and can help improve the performance of multimodal fake\nnews detection by over 5.2%.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 03:19:46 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Qi", "Peng", ""], ["Cao", "Juan", ""], ["Yang", "Tianyun", ""], ["Guo", "Junbo", ""], ["Li", "Jintao", ""]]}, {"id": "1908.04629", "submitter": "Tiago Machado", "authors": "Tiago Machado, Daniel Gopstein, Oded Nov, Angela Wang, Andy Nealen,\n  Julian Togelius", "title": "Evaluation of a Recommender System for Assisting Novice Game Designers", "comments": "The 15th AAAI Conference on Artificial Intelligence and Interactive\n  Digital Entertainment (AIIDE 19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game development is a complex task involving multiple disciplines and\ntechnologies. Developers and researchers alike have suggested that AI-driven\ngame design assistants may improve developer workflow. We present a recommender\nsystem for assisting humans in game design as well as a rigorous human subjects\nstudy to validate it. The AI-driven game design assistance system suggests game\nmechanics to designers based on characteristics of the game being developed. We\nbelieve this method can bring creative insights and increase users'\nproductivity. We conducted quantitative studies that showed the recommender\nsystem increases users' levels of accuracy and computational affect, and\ndecreases their levels of workload.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 13:22:51 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Machado", "Tiago", ""], ["Gopstein", "Daniel", ""], ["Nov", "Oded", ""], ["Wang", "Angela", ""], ["Nealen", "Andy", ""], ["Togelius", "Julian", ""]]}, {"id": "1908.04729", "submitter": "Zewen Chi", "authors": "Zewen Chi, Heyan Huang, Heng-Da Xu, Houjin Yu, Wanxuan Yin and\n  Xian-Ling Mao", "title": "Complicated Table Structure Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of table structure recognition aims to recognize the internal\nstructure of a table, which is a key step to make machines understand tables.\nCurrently, there are lots of studies on this task for different file formats\nsuch as ASCII text and HTML. It also attracts lots of attention to recognize\nthe table structures in PDF files. However, it is hard for the existing methods\nto accurately recognize the structure of complicated tables in PDF files. The\ncomplicated tables contain spanning cells which occupy at least two columns or\nrows. To address the issue, we propose a novel graph neural network for\nrecognizing the table structure in PDF files, named GraphTSR. Specifically, it\ntakes table cells as input, and then recognizes the table structures by\npredicting relations among cells. Moreover, to evaluate the task better, we\nconstruct a large-scale table structure recognition dataset from scientific\npapers, named SciTSR, which contains 15,000 tables from PDF files and their\ncorresponding structure labels. Extensive experiments demonstrate that our\nproposed model is highly effective for complicated tables and outperforms\nstate-of-the-art baselines over a benchmark dataset and our new constructed\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 16:47:08 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 16:24:53 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Chi", "Zewen", ""], ["Huang", "Heyan", ""], ["Xu", "Heng-Da", ""], ["Yu", "Houjin", ""], ["Yin", "Wanxuan", ""], ["Mao", "Xian-Ling", ""]]}, {"id": "1908.04979", "submitter": "Guoli Song", "authors": "Guoli Song, Shuhui Wang, Qingming Huang, Qi Tian", "title": "Harmonized Multimodal Learning with Gaussian Process Latent Variable\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal learning aims to discover the relationship between multiple\nmodalities. It has become an important research topic due to extensive\nmultimodal applications such as cross-modal retrieval. This paper attempts to\naddress the modality heterogeneity problem based on Gaussian process latent\nvariable models (GPLVMs) to represent multimodal data in a common space.\nPrevious multimodal GPLVM extensions generally adopt individual learning\nschemes on latent representations and kernel hyperparameters, which ignore\ntheir intrinsic relationship. To exploit strong complementarity among different\nmodalities and GPLVM components, we develop a novel learning scheme called\nHarmonization, where latent model parameters are jointly learned from each\nother. Beyond the correlation fitting or intra-modal structure preservation\nparadigms widely used in existing studies, the harmonization is derived in a\nmodel-driven manner to encourage the agreement between modality-specific GP\nkernels and the similarity of latent representations. We present a range of\nmultimodal learning models by incorporating the harmonization mechanism into\nseveral representative GPLVM-based approaches. Experimental results on four\nbenchmark datasets show that the proposed models outperform the strong\nbaselines for cross-modal retrieval tasks, and that the harmonized multimodal\nlearning method is superior in discovering semantically consistent latent\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 06:40:28 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Song", "Guoli", ""], ["Wang", "Shuhui", ""], ["Huang", "Qingming", ""], ["Tian", "Qi", ""]]}, {"id": "1908.05098", "submitter": "Mohamad Yaser Jaradeh", "authors": "Kuldeep Singh, Mohamad Yaser Jaradeh, Saeedeh Shekarpour, Akash\n  Kulkarni, Arun Sethupat Radhakrishna, Ioanna Lytra, Maria-Esther Vidal, Jens\n  Lehmann", "title": "Towards Optimisation of Collaborative Question Answering over Knowledge\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Question Answering (CQA) frameworks for knowledge graphs aim at\nintegrating existing question answering (QA) components for implementing\nsequences of QA tasks (i.e. QA pipelines). The research community has paid\nsubstantial attention to CQAs since they support reusability and scalability of\nthe available components in addition to the flexibility of pipelines. CQA\nframeworks attempt to build such pipelines automatically by solving two\noptimisation problems: 1) local collective performance of QA components per QA\ntask and 2) global performance of QA pipelines. In spite offering several\nadvantages over monolithic QA systems, the effectiveness and efficiency of CQA\nframeworks in answering questions is limited. In this paper, we tackle the\nproblem of local optimisation of CQA frameworks and propose a three fold\napproach, which applies feature selection techniques with supervised machine\nlearning approaches in order to identify the best performing components\nefficiently. We have empirically evaluated our approach over existing\nbenchmarks and compared to existing automatic CQA frameworks. The observed\nresults provide evidence that our approach answers a higher number of questions\nthan the state of the art while reducing: i) the number of used features by 50%\nand ii) the number of components used by 76%.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 12:42:48 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Singh", "Kuldeep", ""], ["Jaradeh", "Mohamad Yaser", ""], ["Shekarpour", "Saeedeh", ""], ["Kulkarni", "Akash", ""], ["Radhakrishna", "Arun Sethupat", ""], ["Lytra", "Ioanna", ""], ["Vidal", "Maria-Esther", ""], ["Lehmann", "Jens", ""]]}, {"id": "1908.05391", "submitter": "Qibin Chen", "authors": "Qibin Chen, Junyang Lin, Yichang Zhang, Ming Ding, Yukuo Cen, Hongxia\n  Yang, Jie Tang", "title": "Towards Knowledge-Based Recommender Dialog System", "comments": "To appear in EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel end-to-end framework called KBRD, which\nstands for Knowledge-Based Recommender Dialog System. It integrates the\nrecommender system and the dialog generation system. The dialog system can\nenhance the performance of the recommendation system by introducing\nknowledge-grounded information about users' preferences, and the recommender\nsystem can improve that of the dialog generation system by providing\nrecommendation-aware vocabulary bias. Experimental results demonstrate that our\nproposed model has significant advantages over the baselines in both the\nevaluation of dialog generation and recommendation. A series of analyses show\nthat the two systems can bring mutual benefits to each other, and the\nintroduced knowledge contributes to both their performances.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 01:49:19 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 04:38:02 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Chen", "Qibin", ""], ["Lin", "Junyang", ""], ["Zhang", "Yichang", ""], ["Ding", "Ming", ""], ["Cen", "Yukuo", ""], ["Yang", "Hongxia", ""], ["Tang", "Jie", ""]]}, {"id": "1908.05435", "submitter": "Liwei Wu", "authors": "Liwei Wu, Shuqing Li, Cho-Jui Hsieh, James Sharpnack", "title": "Temporal Collaborative Ranking Via Personalized Transformer", "comments": "plan to submit for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collaborative ranking problem has been an important open research\nquestion as most recommendation problems can be naturally formulated as ranking\nproblems. While much of collaborative ranking methodology assumes static\nranking data, the importance of temporal information to improving ranking\nperformance is increasingly apparent. Recent advances in deep learning,\nespecially the discovery of various attention mechanisms and newer\narchitectures in addition to widely used RNN and CNN in natural language\nprocessing, have allowed us to make better use of the temporal ordering of\nitems that each user has engaged with. In particular, the SASRec model,\ninspired by the popular Transformer model in natural languages processing, has\nachieved state-of-art results in the temporal collaborative ranking problem and\nenjoyed more than 10x speed-up when compared to earlier CNN/RNN-based methods.\nHowever, SASRec is inherently an un-personalized model and does not include\npersonalized user embeddings. To overcome this limitation, we propose a\nPersonalized Transformer (SSE-PT) model, outperforming SASRec by almost 5% in\nterms of NDCG@10 on 5 real-world datasets. Furthermore, after examining some\nrandom users' engagement history and corresponding attention heat maps used\nduring the inference stage, we find our model is not only more interpretable\nbut also able to focus on recent engagement patterns for each user. Moreover,\nour SSE-PT model with a slight modification, which we call SSE-PT++, can handle\nextremely long sequences and outperform SASRec in ranking results with\ncomparable training speed, striking a balance between performance and speed\nrequirements. Code and data are open sourced at\nhttps://github.com/wuliwei9278/SSE-PT.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 06:35:04 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Wu", "Liwei", ""], ["Li", "Shuqing", ""], ["Hsieh", "Cho-Jui", ""], ["Sharpnack", "James", ""]]}, {"id": "1908.05541", "submitter": "Felix Hamann", "authors": "Felix Hamann, Nadja Kurz, Adrian Ulges", "title": "Hamming Sentence Embeddings for Information Retrieval", "comments": "4 Pages, 9 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In retrieval applications, binary hashes are known to offer significant\nimprovements in terms of both memory and speed. We investigate the compression\nof sentence embeddings using a neural encoder-decoder architecture, which is\ntrained by minimizing reconstruction error. Instead of employing the original\nreal-valued embeddings, we use latent representations in Hamming space produced\nby the encoder for similarity calculations.\n  In quantitative experiments on several benchmarks for semantic similarity\ntasks, we show that our compressed hamming embeddings yield a comparable\nperformance to uncompressed embeddings (Sent2Vec, InferSent, Glove-BoW), at\ncompression ratios of up to 256:1. We further demonstrate that our model\nstrongly decorrelates input features, and that the compressor generalizes well\nwhen pre-trained on Wikipedia sentences. We publish the source code on Github\nand all experimental results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 13:51:12 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Hamann", "Felix", ""], ["Kurz", "Nadja", ""], ["Ulges", "Adrian", ""]]}, {"id": "1908.05544", "submitter": "Tobias Eichinger", "authors": "Tobias Eichinger, Felix Beierle, Robin Papke, Lucas Rebscher, Hong\n  Chinh Tran, Magdalena Trzeciak", "title": "On Gossip-based Information Dissemination in Pervasive Recommender\n  Systems", "comments": null, "journal-ref": null, "doi": "10.1145/3298689.3347067", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pervasive computing systems employ distributed and embedded devices in order\nto raise, communicate, and process data in an anytime-anywhere fashion.\nCertainly, its most prominent device is the smartphone due to its wide\nproliferation, growing computation power, and wireless networking capabilities.\nIn this context, we revisit the implementation of digitalized word-of-mouth\nthat suggests exchanging item preferences between smartphones offline and\ndirectly in immediate proximity. Collaboratively and decentrally collecting\ndata in this way has two benefits. First, it allows to attach for instance\nlocation-sensitive context information in order to enrich collected item\npreferences. %enhance on-device recommendations. Second, model building does\nnot require network connectivity. Despite the benefits, the approach naturally\nraises data privacy and data scarcity issues. In order to address both, we\npropose Propagate and Filter, a method that translates the traditional approach\nof finding similar peers and exchanging item preferences among each other from\nthe field of decentralized to that of pervasive recommender systems.\nAdditionally, we present preliminary results on a prototype mobile application\nthat implements the proposed device-to-device information exchange. Average\nad-hoc connection delays of 25.9 seconds and reliable connection success rates\nwithin 6 meters underpin the approach's technical feasibility.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 13:53:23 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Eichinger", "Tobias", ""], ["Beierle", "Felix", ""], ["Papke", "Robin", ""], ["Rebscher", "Lucas", ""], ["Tran", "Hong Chinh", ""], ["Trzeciak", "Magdalena", ""]]}, {"id": "1908.05596", "submitter": "Timothy Miller", "authors": "Dianbo Liu, Dmitriy Dligach, Timothy Miller", "title": "Two-stage Federated Phenotyping and Patient Representation Learning", "comments": "9 pages; Proceedings of the 18th BioNLP Workshop and Shared Task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large percentage of medical information is in unstructured text format in\nelectronic medical record systems. Manual extraction of information from\nclinical notes is extremely time consuming. Natural language processing has\nbeen widely used in recent years for automatic information extraction from\nmedical texts. However, algorithms trained on data from a single healthcare\nprovider are not generalizable and error-prone due to the heterogeneity and\nuniqueness of medical documents. We develop a two-stage federated natural\nlanguage processing method that enables utilization of clinical notes from\ndifferent hospitals or clinics without moving the data, and demonstrate its\nperformance using obesity and comorbities phenotyping as medical task. This\napproach not only improves the quality of a specific clinical task but also\nfacilitates knowledge progression in the whole healthcare system, which is an\nessential part of learning health system. To the best of our knowledge, this is\nthe first application of federated machine learning in clinical NLP.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 14:06:11 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Liu", "Dianbo", ""], ["Dligach", "Dmitriy", ""], ["Miller", "Timothy", ""]]}, {"id": "1908.05601", "submitter": "Mengnan Du", "authors": "Mengnan Du, Ninghao Liu, Fan Yang, Xia Hu", "title": "Learning Credible Deep Neural Networks with Rationale Regularization", "comments": "ICDM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent explainability related studies have shown that state-of-the-art DNNs\ndo not always adopt correct evidences to make decisions. It not only hampers\ntheir generalization but also makes them less likely to be trusted by\nend-users. In pursuit of developing more credible DNNs, in this paper we\npropose CREX, which encourages DNN models to focus more on evidences that\nactually matter for the task at hand, and to avoid overfitting to\ndata-dependent bias and artifacts. Specifically, CREX regularizes the training\nprocess of DNNs with rationales, i.e., a subset of features highlighted by\ndomain experts as justifications for predictions, to enforce DNNs to generate\nlocal explanations that conform with expert rationales. Even when rationales\nare not available, CREX still could be useful by requiring the generated\nexplanations to be sparse. Experimental results on two text classification\ndatasets demonstrate the increased credibility of DNNs trained with CREX.\nComprehensive analysis further shows that while CREX does not always improve\nprediction accuracy on the held-out test set, it significantly increases DNN\naccuracy on new and previously unseen data beyond test set, highlighting the\nadvantage of the increased credibility.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 12:57:26 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Du", "Mengnan", ""], ["Liu", "Ninghao", ""], ["Yang", "Fan", ""], ["Hu", "Xia", ""]]}, {"id": "1908.05602", "submitter": "Heikki Arponen Dr", "authors": "Heikki Arponen, Tom E Bishop", "title": "SHREWD: Semantic Hierarchy-based Relational Embeddings for\n  Weakly-supervised Deep Hashing", "comments": "4 pages, Published in ICLR LLD Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using class labels to represent class similarity is a typical approach to\ntraining deep hashing systems for retrieval; samples from the same or different\nclasses take binary 1 or 0 similarity values. This similarity does not model\nthe full rich knowledge of semantic relations that may be present between data\npoints. In this work we build upon the idea of using semantic hierarchies to\nform distance metrics between all available sample labels; for example cat to\ndog has a smaller distance than cat to guitar. We combine this type of semantic\ndistance into a loss function to promote similar distances between the deep\nneural network embeddings. We also introduce an empirical Kullback-Leibler\ndivergence loss term to promote binarization and uniformity of the embeddings.\nWe test the resulting SHREWD method and demonstrate improvements in\nhierarchical retrieval scores using compact, binary hash codes instead of real\nvalued ones, and show that in a weakly supervised hashing setting we are able\nto learn competitively without explicitly relying on class labels, but instead\non similarities between labels.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 08:24:40 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Arponen", "Heikki", ""], ["Bishop", "Tom E", ""]]}, {"id": "1908.05604", "submitter": "Ye Liu", "authors": "Ye Liu, Chenwei Zhang, Xiaohui Yan, Yi Chang, Philip S. Yu", "title": "Generative Question Refinement with Deep Reinforcement Learning in\n  Retrieval-based QA System", "comments": "CIKM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world question-answering (QA) systems, ill-formed questions, such as\nwrong words, ill word order, and noisy expressions, are common and may prevent\nthe QA systems from understanding and answering them accurately. In order to\neliminate the effect of ill-formed questions, we approach the question\nrefinement task and propose a unified model, QREFINE, to refine the ill-formed\nquestions to well-formed question. The basic idea is to learn a Seq2Seq model\nto generate a new question from the original one. To improve the quality and\nretrieval performance of the generated questions, we make two major\nimprovements: 1) To better encode the semantics of ill-formed questions, we\nenrich the representation of questions with character embedding and the recent\nproposed contextual word embedding such as BERT, besides the traditional\ncontext-free word embeddings; 2) To make it capable to generate desired\nquestions, we train the model with deep reinforcement learning techniques that\nconsiders an appropriate wording of the generation as an immediate reward and\nthe correlation between generated question and answer as time-delayed long-term\nrewards. Experimental results on real-world datasets show that the proposed\nQREFINE method can generate refined questions with more readability but fewer\nmistakes than the original questions provided by users. Moreover, the refined\nquestions also significantly improve the accuracy of answer retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 05:06:42 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 04:03:05 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 04:38:34 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Liu", "Ye", ""], ["Zhang", "Chenwei", ""], ["Yan", "Xiaohui", ""], ["Chang", "Yi", ""], ["Yu", "Philip S.", ""]]}, {"id": "1908.05608", "submitter": "Mostafa Khalaji", "authors": "Mostafa Khalaji and Nilufar Mohammadnejad", "title": "FCNHSMRA_HRS: Improve the performance of the movie hybrid recommender\n  system using resource allocation approach", "comments": "Accepted in 4th International Conference on Researchers in Science &\n  Engineering & International Congress on Civil, Architecture and Urbanism in\n  Asia, Kasem Bundit University, Bangkok, Thailand", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are systems that are capable of offering the most\nsuitable services and products to users. Through specific methods and\ntechniques, the recommender systems try to identify the most appropriate items,\nsuch as types of information and goods and propose the closest to the user's\ntastes. Collaborative filtering offering active user suggestions based on the\nrating of a set of users is one of the simplest and most comprehensible and\nsuccessful models for finding people in the same tastes in the recommender\nsystems. In this model, with increasing number of users and movie, the system\nis subject to scalability. On the other hand, it is important to improve the\nperformance of the system when there is little information available on the\nratings. In this paper, a movie hybrid recommender system based on FNHSM_HRS\nstructure using resource allocation approach called FCNHSMRA_HRS is presented.\nThe FNHSM_HRS structure was based on the heuristic similarity measure (NHSM),\nalong with fuzzy clustering. Using the fuzzy clustering method in the proposed\nsystem improves the scalability problem and increases the accuracy of system\nsuggestions. The proposed systems is based on collaborative filtering and, by\nusing the heuristic similarity measure and applying the resource allocation\napproach, improves the performance, accuracy and precision of the system. The\nexperimental results using MAE, Accuracy, Precision and Recall metrics based on\nMovieLens dataset show that the performance of the system is improved and the\naccuracy of recommendations in comparison of FNHSM_HRS and collaborative\nfiltering methods that use other similarity measures for finding similarity, is\nincreased\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 13:33:42 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Khalaji", "Mostafa", ""], ["Mohammadnejad", "Nilufar", ""]]}, {"id": "1908.05609", "submitter": "Mostafa Khalaji", "authors": "Mostafa Khalaji and Nilufar Mohammadnejad", "title": "CUPCF: Combining Users Preferences in Collaborative Filtering for Better\n  Recommendation", "comments": "Accepted in SN Applied sciences journal", "journal-ref": null, "doi": "10.1007/s42452-019-1071-6", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to make the best decision between the opinions and tastes of your friends\nand acquaintances? Therefore, recommender systems are used to solve such\nissues. The common algorithms use a similarity measure to predict active users'\ntastes over a particular item. According to the cold start and data sparsity\nproblems, these systems cannot predict and suggest particular items to users.\nIn this paper, we introduce a new recommender system is able to find user\npreferences and based on it, provides the recommendations. Our proposed system\ncalled CUPCF is a combination of two similarity measures in collaborative\nfiltering to solve the data sparsity problem and poor prediction (high\nprediction error rate) problems for better recommendation. The experimental\nresults based on MovieLens dataset show that, combined with the preferences of\nthe user's nearest neighbor, the proposed system error rate compared to a\nnumber of state-of-the-art recommendation methods improved. Furthermore, the\nresults indicate the efficiency of CUPCF. The maximum improved error rate of\nthe system is 15.5% and the maximum values of Accuracy, Precision and Recall of\nCUPCF are 0.91402, 0.91436 and 0.9974 respectively.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 13:32:50 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Khalaji", "Mostafa", ""], ["Mohammadnejad", "Nilufar", ""]]}, {"id": "1908.05611", "submitter": "Chang-You Tai", "authors": "Chang-You Tai, Meng-Ru Wu, Yun-Wei Chu, Shao-Yu Chu", "title": "GraphSW: a training protocol based on stage-wise training for GNN-based\n  Recommender Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, researchers utilize Knowledge Graph (KG) as side information in\nrecommendation system to address cold start and sparsity issue and improve the\nrecommendation performance. Existing KG-aware recommendation model use the\nfeature of neighboring entities and structural information to update the\nembedding of currently located entity. Although the fruitful information is\nbeneficial to the following task, the cost of exploring the entire graph is\nmassive and impractical. In order to reduce the computational cost and maintain\nthe pattern of extracting features, KG-aware recommendation model usually\nutilize fixed-size and random set of neighbors rather than complete information\nin KG. Nonetheless, there are two critical issues in these approaches: First of\nall, fixed-size and randomly selected neighbors restrict the view of graph. In\naddition, as the order of graph feature increases, the growth of parameter\ndimensionality of the model may lead the training process hard to converge. To\nsolve the aforementioned limitations, we propose GraphSW, a strategy based on\nstage-wise training framework which would only access to a subset of the\nentities in KG in every stage. During the following stages, the learned\nembedding from previous stages is provided to the network in the next stage and\nthe model can learn the information gradually from the KG. We apply stage-wise\ntraining on two SOTA recommendation models, RippleNet and Knowledge Graph\nConvolutional Networks (KGCN). Moreover, we evaluate the performance on six\nreal world datasets, Last.FM 2011, Book-Crossing,movie, LFM-1b 2015,\nAmazon-book and Yelp 2018. The result of our experiments shows that proposed\nstrategy can help both models to collect more information from the KG and\nimprove the performance. Furthermore, it is observed that GraphSW can assist\nKGCN to converge effectively in high-order graph feature.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 05:50:50 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 15:31:49 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Tai", "Chang-You", ""], ["Wu", "Meng-Ru", ""], ["Chu", "Yun-Wei", ""], ["Chu", "Shao-Yu", ""]]}, {"id": "1908.05762", "submitter": "Hamed Shahbazi", "authors": "Hamed Shahbazi, Xiaoli Z. Fern, Reza Ghaeini, Rasha Obeidat and Prasad\n  Tadepalli", "title": "Entity-aware ELMo: Learning Contextual Entity Representation for Entity\n  Disambiguation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new local entity disambiguation system. The key to our system is\na novel approach for learning entity representations. In our approach we learn\nan entity aware extension of Embedding for Language Model (ELMo) which we call\nEntity-ELMo (E-ELMo). Given a paragraph containing one or more named entity\nmentions, each mention is first defined as a function of the entire paragraph\n(including other mentions), then they predict the referent entities. Utilizing\nE-ELMo for local entity disambiguation, we outperform all of the\nstate-of-the-art local and global models on the popular benchmarks by improving\nabout 0.5\\% on micro average accuracy for AIDA test-b with Yago candidate set.\nThe evaluation setup of the training data and candidate set are the same as our\nbaselines for fair comparison.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 03:51:25 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 16:49:24 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Shahbazi", "Hamed", ""], ["Fern", "Xiaoli Z.", ""], ["Ghaeini", "Reza", ""], ["Obeidat", "Rasha", ""], ["Tadepalli", "Prasad", ""]]}, {"id": "1908.05780", "submitter": "Venet Osmani", "authors": "Seyedmostafa Sheikhalishahi, Riccardo Miotto, Joel T Dudley, Alberto\n  Lavelli, Fabio Rinaldi, Venet Osmani", "title": "Natural Language Processing of Clinical Notes on Chronic Diseases:\n  Systematic Review", "comments": "Supplementary material detailing articles reviewed, classification of\n  diseases and associated algorithms, can be found at:\n  http://venetosmani.com/research/publications.html", "journal-ref": "JMIR Medical Informatics 2019;7(2):e12239, PMID:31066697", "doi": "10.2196/12239", "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Of the 2652 articles considered, 106 met the inclusion criteria. Review of\nthe included papers resulted in identification of 43 chronic diseases, which\nwere then further classified into 10 disease categories using ICD-10. The\nmajority of studies focused on diseases of the circulatory system (n=38) while\nendocrine and metabolic diseases were fewest (n=14). This was due to the\nstructure of clinical records related to metabolic diseases, which typically\ncontain much more structured data, compared with medical records for diseases\nof the circulatory system, which focus more on unstructured data and\nconsequently have seen a stronger focus of NLP. The review has shown that there\nis a significant increase in the use of machine learning methods compared to\nrule-based approaches; however, deep learning methods remain emergent (n=3).\nConsequently, the majority of works focus on classification of disease\nphenotype with only a handful of papers addressing extraction of comorbidities\nfrom the free text or integration of clinical notes with structured data. There\nis a notable use of relatively simple methods, such as shallow classifiers (or\ncombination with rule-based methods), due to the interpretability of\npredictions, which still represents a significant issue for more complex\nmethods. Finally, scarcity of publicly available data may also have contributed\nto insufficient development of more advanced methods, such as extraction of\nword embeddings from clinical notes. Further efforts are still required to\nimprove (1) progression of clinical NLP methods from extraction toward\nunderstanding; (2) recognition of relations among entities rather than entities\nin isolation; (3) temporal extraction to understand past, current, and future\nclinical events; (4) exploitation of alternative sources of clinical knowledge;\nand (5) availability of large-scale, de-identified clinical corpora.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 21:54:06 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Sheikhalishahi", "Seyedmostafa", ""], ["Miotto", "Riccardo", ""], ["Dudley", "Joel T", ""], ["Lavelli", "Alberto", ""], ["Rinaldi", "Fabio", ""], ["Osmani", "Venet", ""]]}, {"id": "1908.05928", "submitter": "Liang Zhang", "authors": "Guannan Liu, Liang Zhang, Junjie Wu, Xiao Fang", "title": "Recommendation with Attribute-aware Product Networks: A Representation\n  Learning Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prosperity of business intelligence, recommender systems have\nevolved into a new stage that we not only care about what to recommend, but why\nit is recommended. Explainability of recommendations thus emerges as a focal\npoint of research and becomes extremely desired in e-commerce. Existent studies\nalong this line often exploit item attributes and correlations from different\nperspectives, but they yet lack an effective way to combine both types of\ninformation for deep learning of personalized interests. In light of this, we\npropose a novel graph structure, \\emph{attribute network}, based on both items'\nco-purchase network and important attributes. A novel neural model called\n\\emph{eRAN} is then proposed to generate recommendations from attribute\nnetworks with explainability and cold-start capability. Specifically, eRAN\nfirst maps items connected in attribute networks to low-dimensional embedding\nvectors through a deep autoencoder, and then an attention mechanism is applied\nto model the attractions of attributes to users, from which personalized item\nrepresentation can be derived. Moreover, a pairwise ranking loss is constructed\ninto eRAN to improve recommendations, with the assumption that item pairs\nco-purchased by a user should be more similar than those non-paired with\nnegative sampling in personalized view. Experiments on real-world datasets\ndemonstrate the effectiveness of our method compared with some state-of-the-art\ncompetitors. In particular, eRAN shows its unique abilities in recommending\ncold-start items with higher accuracy, as well as in understanding user\npreferences underlying complicated co-purchasing behaviors.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 11:07:46 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 06:01:44 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Liu", "Guannan", ""], ["Zhang", "Liang", ""], ["Wu", "Junjie", ""], ["Fang", "Xiao", ""]]}, {"id": "1908.06082", "submitter": "Prathusha Kameswara Sarma", "authors": "Prathusha K Sarma, Yingyu Liang, William A Sethares", "title": "Shallow Domain Adaptive Embeddings for Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a way to improve the performance of existing algorithms\nfor text classification in domains with strong language semantics. We propose a\ndomain adaptation layer learns weights to combine a generic and a domain\nspecific (DS) word embedding into a domain adapted (DA) embedding. The DA word\nembeddings are then used as inputs to a generic encoder + classifier framework\nto perform a downstream task such as classification. This adaptation layer is\nparticularly suited to datasets that are modest in size, and which are,\ntherefore, not ideal candidates for (re)training a deep neural network\narchitecture. Results on binary and multi-class classification tasks using\npopular encoder architectures, including current state-of-the-art methods (with\nand without the shallow adaptation layer) show the effectiveness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 20:25:13 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Sarma", "Prathusha K", ""], ["Liang", "Yingyu", ""], ["Sethares", "William A", ""]]}, {"id": "1908.06121", "submitter": "Rishav Chakravarti", "authors": "Rishav Chakravarti, Cezar Pendus, Andrzej Sakrajda, Anthony Ferritto,\n  Lin Pan, Michael Glass, Vittorio Castelli, J. William Murdock, Radu Florian,\n  Salim Roukos, Avirup Sil", "title": "CFO: A Framework for Building Production NLP Systems", "comments": "http://ibm.biz/cfo_framework", "journal-ref": "Proceedings of the 2019 Conference on Empirical Methods in Natural\n  Language Processing and the 9th International Joint Conference on Natural\n  Language Processing (EMNLP-IJCNLP): System Demonstrations", "doi": "10.18653/v1/D19-3006", "report-no": "D19-3006", "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel orchestration framework, called CFO\n(COMPUTATION FLOW ORCHESTRATOR), for building, experimenting with, and\ndeploying interactive NLP (Natural Language Processing) and IR (Information\nRetrieval) systems to production environments. We then demonstrate a question\nanswering system built using this framework which incorporates state-of-the-art\nBERT based MRC (Machine Reading Comprehension) with IR components to enable\nend-to-end answer retrieval. Results from the demo system are shown to be high\nquality in both academic and industry domain specific settings. Finally, we\ndiscuss best practices when (pre-)training BERT based MRC models for production\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 18:19:59 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 15:01:20 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 20:24:05 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Chakravarti", "Rishav", ""], ["Pendus", "Cezar", ""], ["Sakrajda", "Andrzej", ""], ["Ferritto", "Anthony", ""], ["Pan", "Lin", ""], ["Glass", "Michael", ""], ["Castelli", "Vittorio", ""], ["Murdock", "J. William", ""], ["Florian", "Radu", ""], ["Roukos", "Salim", ""], ["Sil", "Avirup", ""]]}, {"id": "1908.06132", "submitter": "Rodrigo Nogueira", "authors": "Rodrigo Nogueira", "title": "Learning Representations and Agents for Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A goal shared by artificial intelligence and information retrieval is to\ncreate an oracle, that is, a machine that can answer our questions, no matter\nhow difficult they are. A more limited, but still instrumental, version of this\noracle is a question-answering system, in which an open-ended question is given\nto the machine, and an answer is produced based on the knowledge it has access\nto. Such systems already exist and are increasingly capable of answering\ncomplicated questions. This progress can be partially attributed to the recent\nsuccess of machine learning and to the efficient methods for storing and\nretrieving information, most notably through web search engines. One can\nimagine that this general-purpose question-answering system can be built as a\nbillion-parameters neural network trained end-to-end with a large number of\npairs of questions and answers. We argue, however, that although this approach\nhas been very successful for tasks such as machine translation, storing the\nworld's knowledge as parameters of a learning machine can be very hard. A more\nefficient way is to train an artificial agent on how to use an external\nretrieval system to collect relevant information. This agent can leverage the\neffort that has been put into designing and running efficient storage and\nretrieval systems by learning how to best utilize them to accomplish a task.\n...\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 19:07:07 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Nogueira", "Rodrigo", ""]]}, {"id": "1908.06158", "submitter": "Meisam Hejazi Nia", "authors": "Meisam Hejazinia, Kyler Eastman, Shuqin Ye, Abbas Amirabadi, Ravi\n  Divvela", "title": "Accelerated learning from recommender systems using multi-armed bandit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems are a vital component of many online marketplaces,\nwhere there are often millions of items to potentially present to users who\nhave a wide variety of wants or needs. Evaluating recommender system algorithms\nis a hard task, given all the inherent bias in the data, and successful\ncompanies must be able to rapidly iterate on their solution to maintain their\ncompetitive advantage. The gold standard for evaluating recommendation\nalgorithms has been the A/B test since it is an unbiased way to estimate how\nwell one or more algorithms compare in the real world. However, there are a\nnumber of issues with A/B testing that make it impractical to be the sole\nmethod of testing, including long lead time, and high cost of exploration. We\nargue that multi armed bandit (MAB) testing as a solution to these issues. We\nshowcase how we implemented a MAB solution as an extra step between offline and\nonline A/B testing in a production system. We present the result of our\nexperiment and compare all the offline, MAB, and online A/B tests metrics for\nour use case.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 20:44:01 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Hejazinia", "Meisam", ""], ["Eastman", "Kyler", ""], ["Ye", "Shuqin", ""], ["Amirabadi", "Abbas", ""], ["Divvela", "Ravi", ""]]}, {"id": "1908.06169", "submitter": "Dimitrios Rafailidis Dr", "authors": "Dimitrios Rafailidis", "title": "Cross-Domain Collaborative Filtering via Translation-based Learning", "comments": "arXiv admin note: text overlap with arXiv:1907.01645", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of social media platforms and e-commerce sites,\nseveral cross-domain collaborative filtering strategies have been recently\nintroduced to transfer the knowledge of user preferences across domains. The\nmain challenge of cross-domain recommendation is to weigh and learn users'\ndifferent behaviors in multiple domains. In this paper, we propose a\nCross-Domain collaborative filtering model following a Translation-based\nstrategy, namely CDT. In our model, we learn the embedding space with\ntranslation vectors and capture high-order feature interactions in users'\nmultiple preferences across domains. In doing so, we efficiently compute the\ntransitivity between feature latent embeddings, that is if feature pairs have\nhigh interaction weights in the latent space, then feature embeddings with no\nobserved interactions across the domains will be closely related as well. We\nformulate our objective function as a ranking problem in factorization machines\nand learn the model's parameters via gradient descent. In addition, to better\ncapture the non-linearity in user preferences across domains we extend the\nproposed CDT model by using a deep learning strategy, namely DeepCDT. Our\nexperiments on six publicly available cross-domain tasks demonstrate the\neffectiveness of the proposed models, outperforming other state-of-the-art\ncross-domain strategies.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 15:23:35 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Rafailidis", "Dimitrios", ""]]}, {"id": "1908.06318", "submitter": "Magnus Lie Hetland", "authors": "Magnus Lie Hetland", "title": "Comparison-Based Indexing From First Principles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basic assumptions about comparison-based indexing are laid down and a general\ndesign space is derived from these. An index structure spanning this design\nspace (the sprawl) is described, along with an associated family of\npartitioning predicates, or regions (the ambits), as well as algorithms for\nsearch and, to some extent, construction. The sprawl of ambits forms a\nunification and generalization of current indexing methods, and a jumping-off\npoint for future designs.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 16:54:40 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Hetland", "Magnus Lie", ""]]}, {"id": "1908.06583", "submitter": "Sapumal Ahangama", "authors": "Sapumal Ahangama and Danny Chiang-Choon Poo", "title": "Latent User Linking for Collaborative Cross Domain Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread adoption of information systems, recommender systems are\nwidely used for better user experience. Collaborative filtering is a popular\napproach in implementing recommender systems. Yet, collaborative filtering\nmethods are highly dependent on user feedback, which is often highly sparse and\nhard to obtain. However, such issues could be alleviated if knowledge from a\nmuch denser and a related secondary domain could be used to enhance the\nrecommendation accuracy in the sparse target domain. In this publication, we\npropose a deep learning method for cross-domain recommender systems through the\nlinking of cross-domain user latent representations as a form of knowledge\ntransfer across domains. We assume that cross-domain similarities of user\ntastes and behaviors are clearly observable in the low dimensional user latent\nrepresentations. These user similarities are used to link the domains. As a\nresult, we propose a Variational Autoencoder based network model for\ncross-domain linking with added contextualization to handle sparse data and for\nbetter transfer of cross-domain knowledge. We further extend the model to be\nmore suitable in cold start scenarios and to utilize auxiliary user information\nfor additional gains in recommendation accuracy. The effectiveness of the\nproposed model was empirically evaluated using multiple datasets. The\nexperiments proved that the proposed model outperforms the state of the art\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 04:06:44 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Ahangama", "Sapumal", ""], ["Poo", "Danny Chiang-Choon", ""]]}, {"id": "1908.06708", "submitter": "Yashar Deldjoo", "authors": "Yashar Deldjoo, Vito Walter Anelli, Hamed Zamani, Alejandro Bellogin,\n  Tommaso Di Noia", "title": "Recommender Systems Fairness Evaluation via Generalized Cross Entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness in recommender systems has been considered with respect to sensitive\nattributes of users (e.g., gender, race) or items (e.g., revenue in a\nmultistakeholder setting). Regardless, the concept has been commonly\ninterpreted as some form of equality -- i.e., the degree to which the system is\nmeeting the information needs of all its users in an equal sense. In this\npaper, we argue that fairness in recommender systems does not necessarily imply\nequality, but instead it should consider a distribution of resources based on\nmerits and needs.\n  We present a probabilistic framework based on generalized cross entropy to\nevaluate fairness of recommender systems under this perspective, where we show\nthat the proposed framework is flexible and explanatory by allowing to\nincorporate domain knowledge (through an ideal fair distribution) that can help\nto understand which item or user aspects a recommendation algorithm is over- or\nunder-representing. Results on two real-world datasets show the merits of the\nproposed evaluation framework both in terms of user and item fairness.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 11:36:54 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Deldjoo", "Yashar", ""], ["Anelli", "Vito Walter", ""], ["Zamani", "Hamed", ""], ["Bellogin", "Alejandro", ""], ["Di Noia", "Tommaso", ""]]}, {"id": "1908.06780", "submitter": "Yosi Mass", "authors": "Yosi Mass, Haggai Roitman, Shai Erera, Or Rivlin, Bar Weiner and David\n  Konopnicki", "title": "A Study of BERT for Non-Factoid Question-Answering under Passage Length\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the use of BERT for non-factoid question-answering, focusing on the\npassage re-ranking task under varying passage lengths. To this end, we explore\nthe fine-tuning of BERT in different learning-to-rank setups, comprising both\npoint-wise and pair-wise methods, resulting in substantial improvements over\nthe state-of-the-art. We then analyze the effectiveness of BERT for different\npassage lengths and suggest how to cope with large passages.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 13:14:02 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Mass", "Yosi", ""], ["Roitman", "Haggai", ""], ["Erera", "Shai", ""], ["Rivlin", "Or", ""], ["Weiner", "Bar", ""], ["Konopnicki", "David", ""]]}, {"id": "1908.06887", "submitter": "Stanislav Morozov", "authors": "Stanislav Morozov, Artem Babenko", "title": "Relevance Proximity Graphs for Fast Relevance Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In plenty of machine learning applications, the most relevant items for a\nparticular query should be efficiently extracted, while the relevance function\nis based on a highly-nonlinear model, e.g., DNNs or GBDTs. Due to the high\ncomputational complexity of such models, exhaustive search is infeasible even\nfor medium-scale problems. To address this issue, we introduce Relevance\nProximity Graphs (RPG): an efficient non-exhaustive approach that provides a\nhigh-quality approximate solution for maximal relevance retrieval. Namely, we\nextend the recent similarity graphs framework to the setting, when there is no\nsimilarity measure defined on item pairs, which is a common practical use-case.\nBy design, our approach directly maximizes off-the-shelf relevance functions\nand does not require any proxy auxiliary models. Via extensive experiments, we\nshow that the developed method provides excellent retrieval accuracy while\nrequiring only a few model computations, outperforming indirect models. We\nopen-source our implementation as well as two large-scale datasets to support\nfurther research on relevance retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 15:44:24 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 16:12:20 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2019 15:51:55 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Morozov", "Stanislav", ""], ["Babenko", "Artem", ""]]}, {"id": "1908.06917", "submitter": "Svitlana Vakulenko", "authors": "Svitlana Vakulenko, Javier David Fernandez Garcia, Axel Polleres,\n  Maarten de Rijke, Michael Cochez", "title": "Message Passing for Complex Question Answering over Knowledge Graphs", "comments": "Accepted in CIKM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering over knowledge graphs (KGQA) has evolved from simple\nsingle-fact questions to complex questions that require graph traversal and\naggregation. We propose a novel approach for complex KGQA that uses\nunsupervised message passing, which propagates confidence scores obtained by\nparsing an input question and matching terms in the knowledge graph to a set of\npossible answers. First, we identify entity, relationship, and class names\nmentioned in a natural language question, and map these to their counterparts\nin the graph. Then, the confidence scores of these mappings propagate through\nthe graph structure to locate the answer entities. Finally, these are\naggregated depending on the identified question type. This approach can be\nefficiently implemented as a series of sparse matrix multiplications mimicking\njoins over small local subgraphs. Our evaluation results show that the proposed\napproach outperforms the state-of-the-art on the LC-QuAD benchmark. Moreover,\nwe show that the performance of the approach depends only on the quality of the\nquestion interpretation results, i.e., given a correct relevance score\ndistribution, our approach always produces a correct answer ranking. Our error\nanalysis reveals correct answers missing from the benchmark dataset and\ninconsistencies in the DBpedia knowledge graph. Finally, we provide a\ncomprehensive evaluation of the proposed approach accompanied with an ablation\nstudy and an error analysis, which showcase the pitfalls for each of the\nquestion answering components in more detail.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 16:31:29 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["Garcia", "Javier David Fernandez", ""], ["Polleres", "Axel", ""], ["de Rijke", "Maarten", ""], ["Cochez", "Michael", ""]]}, {"id": "1908.06967", "submitter": "Yingyuan Xiao", "authors": "Wanqiao Yuan, Yingyuan Xiao, Xu Jiao, Wenguang Zheng, Zihao Ling", "title": "Detection of Shilling Attack Based on T-distribution on the Dynamic Time\n  Intervals in Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of information technology and the Internet,\nrecommendation systems have become an important means to solve the problem of\ninformation overload. However, recommendation system is greatly fragile as it\nrelies heavily on behavior data of users, which makes it very easy for a host\nof malicious merchants to inject shilling attacks in order to manipulate the\nrecommendation results. Some papers on shilling attack have proposed the\ndetection methods, whether based on false user profiles or abnormal items, but\ntheir detection rate, false alarm rate, universality, and time overhead need to\nbe further improved. In this paper, we propose a new item anomaly detection\nmethod, through T-distribution technology based on Dynamic Time Intervals.\nFirst of all, based on the characteristics of shilling attack quickness\n(Attackers inject a large number of fake profiles in a short period in order to\nsave costs), we use dynamic time interval method to divide the rating history\nof item into multiple time windows. Then, we use the T-distribution to detect\nthe exception windows. By conducting extensive experiments on a dataset that\naccords with real-life situations and comparing it to currently outstanding\nmethods, our proposed approach has a higher detection rate, lower false alarm\nrate and smaller time overhead to the different attack models and filler sizes.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 05:10:12 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Yuan", "Wanqiao", ""], ["Xiao", "Yingyuan", ""], ["Jiao", "Xu", ""], ["Zheng", "Wenguang", ""], ["Ling", "Zihao", ""]]}, {"id": "1908.06968", "submitter": "Yingyuan Xiao", "authors": "Xin Liu, Yingyuan Xiao, Xu Jiao, Wenguang Zheng, Zihao Ling", "title": "A Novel Kalman Filter Based Shilling Attack Detection Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering has been widely used in recommendation systems to\nrecommend items that users might like. However, collaborative filtering based\nrecommendation systems are vulnerable to shilling attacks. Malicious users tend\nto increase or decrease the recommended frequency of target items by injecting\nfake profiles. In this paper, we propose a Kalman filter-based attack detection\nmodel, which statistically analyzes the difference between the actual rating\nand the predicted rating calculated by this model to find the potential\nabnormal time period. The Kalman filter filters out suspicious ratings based on\nthe abnormal time period and identifies suspicious users based on the source of\nthese ratings. The experimental results show that our method performs much\nbetter detection performance for the shilling attack than the traditional\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 12:12:45 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Liu", "Xin", ""], ["Xiao", "Yingyuan", ""], ["Jiao", "Xu", ""], ["Zheng", "Wenguang", ""], ["Ling", "Zihao", ""]]}, {"id": "1908.07018", "submitter": "Ayush Maheshwari", "authors": "Ayush Maheshwari, Hrishikesh Patel, Nandan Rathod, Ritesh Kumar,\n  Ganesh Ramakrishnan and Pushpak Bhattacharyya", "title": "Tale of tails using rule augmented sequence labeling for event\n  extraction", "comments": "9 pages, 4 figures, 6 tables", "journal-ref": "StarAI Workshop at AAAI 2020", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of event extraction is a relatively difficult task for low\nresource languages due to the non-availability of sufficient annotated data.\nMoreover, the task becomes complex for tail (rarely occurring) labels wherein\nextremely less data is available. In this paper, we present a new dataset\n(InDEE-2019) in the disaster domain for multiple Indic languages, collected\nfrom news websites. Using this dataset, we evaluate several rule-based\nmechanisms to augment deep learning based models. We formulate our problem of\nevent extraction as a sequence labeling task and perform extensive experiments\nto study and understand the effectiveness of different approaches. We further\nshow that tail labels can be easily incorporated by creating new rules without\nthe requirement of large annotated data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 18:43:06 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 06:10:02 GMT"}, {"version": "v3", "created": "Fri, 31 Jan 2020 07:36:09 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Maheshwari", "Ayush", ""], ["Patel", "Hrishikesh", ""], ["Rathod", "Nandan", ""], ["Kumar", "Ritesh", ""], ["Ramakrishnan", "Ganesh", ""], ["Bhattacharyya", "Pushpak", ""]]}, {"id": "1908.07069", "submitter": "Zulfat Miftahutdinov", "authors": "Sergey Nikolenko and Elena Tutubalina and Zulfat Miftahutdinov and\n  Eugene Beloded", "title": "CommentsRadar: Dive into Unique Data on All Comments on the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an entity-centric search engineCommentsRadarthatpairs entity\nqueries with articles and user opinions covering a widerange of topics from top\ncommented sites. The engine aggregatesarticles and comments for these articles,\nextracts named entities,links them together and with knowledge base entries,\nperformssentiment analysis, and aggregates the results, aiming to mine\nfortemporal trends and other insights. In this work, we present thegeneral\nengine, discuss the models used for all steps of this pipeline,and introduce\nseveral case studies that discover important insightsfrom online commenting\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 13:01:30 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Nikolenko", "Sergey", ""], ["Tutubalina", "Elena", ""], ["Miftahutdinov", "Zulfat", ""], ["Beloded", "Eugene", ""]]}, {"id": "1908.07087", "submitter": "Hamed Nilforoshan", "authors": "Hamed Nilforoshan, Neil Shah", "title": "SliceNDice: Mining Suspicious Multi-attribute Entity Groups with\n  Multi-view Graphs", "comments": "Published in Proceedings of 2019 IEEE 6th International Conference on\n  Data Science and Advanced Analytics (DSAA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the reach of web platforms, bad actors have considerable incentives to\nmanipulate and defraud users at the expense of platform integrity. This has\nspurred research in numerous suspicious behavior detection tasks, including\ndetection of sybil accounts, false information, and payment scams/fraud. In\nthis paper, we draw the insight that many such initiatives can be tackled in a\ncommon framework by posing a detection task which seeks to find groups of\nentities which share too many properties with one another across multiple\nattributes (sybil accounts created at the same time and location, propaganda\nspreaders broadcasting articles with the same rhetoric and with similar\nreshares, etc.) Our work makes four core contributions: Firstly, we posit a\nnovel formulation of this task as a multi-view graph mining problem, in which\ndistinct views reflect distinct attribute similarities across entities, and\ncontextual similarity and attribute importance are respected. Secondly, we\npropose a novel suspiciousness metric for scoring entity groups given the\nabnormality of their synchronicity across multiple views, which obeys intuitive\ndesiderata that existing metrics do not. Finally, we propose the SliceNDice\nalgorithm which enables efficient extraction of highly suspicious entity\ngroups, and demonstrate its practicality in production, in terms of strong\ndetection performance and discoveries on Snapchat's large advertiser ecosystem\n(89% precision and numerous discoveries of real fraud rings), marked\noutperformance of baselines (over 97% precision/recall in simulated settings)\nand linear scalability.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 22:09:10 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 16:51:08 GMT"}, {"version": "v3", "created": "Sat, 16 May 2020 22:15:18 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Nilforoshan", "Hamed", ""], ["Shah", "Neil", ""]]}, {"id": "1908.07123", "submitter": "Siqi Wu", "authors": "Siqi Wu, Marian-Andrei Rizoiu, Lexing Xie", "title": "Estimating Attention Flow in Online Video Networks", "comments": "CSCW 2019, the code and datasets are publicly available at\n  https://github.com/avalanchesiqi/networked-popularity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online videos have shown tremendous increase in Internet traffic. Most video\nhosting sites implement recommender systems, which connect the videos into a\ndirected network and conceptually act as a source of pathways for users to\nnavigate. At present, little is known about how human attention is allocated\nover such large-scale networks, and about the impacts of the recommender\nsystems. In this paper, we first construct the Vevo network -- a YouTube video\nnetwork with 60,740 music videos interconnected by the recommendation links,\nand we collect their associated viewing dynamics. This results in a total of\n310 million views every day over a period of 9 weeks. Next, we present\nlarge-scale measurements that connect the structure of the recommendation\nnetwork and the video attention dynamics. We use the bow-tie structure to\ncharacterize the Vevo network and we find that its core component (23.1% of the\nvideos), which occupies most of the attention (82.6% of the views), is made out\nof videos that are mainly recommended among themselves. This is indicative of\nthe links between video recommendation and the inequality of attention\nallocation. Finally, we address the task of estimating the attention flow in\nthe video recommendation network. We propose a model that accounts for the\nnetwork effects for predicting video popularity, and we show it consistently\noutperforms the baselines. This model also identifies a group of artists\ngaining attention because of the recommendation network. Altogether, our\nobservations and our models provide a new set of tools to better understand the\nimpacts of recommender systems on collective social attention.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 01:37:26 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 08:47:27 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2020 04:55:02 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Wu", "Siqi", ""], ["Rizoiu", "Marian-Andrei", ""], ["Xie", "Lexing", ""]]}, {"id": "1908.07162", "submitter": "Yu Meng", "authors": "Yu Meng, Jiaxin Huang, Guangyuan Wang, Zihan Wang, Chao Zhang, Yu\n  Zhang, Jiawei Han", "title": "Discriminative Topic Mining via Category-Name Guided Text Embedding", "comments": "WWW 2020. (Code: https://github.com/yumeng5/CatE)", "journal-ref": null, "doi": "10.1145/3366423.3380278", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining a set of meaningful and distinctive topics automatically from massive\ntext corpora has broad applications. Existing topic models, however, typically\nwork in a purely unsupervised way, which often generate topics that do not fit\nusers' particular needs and yield suboptimal performance on downstream tasks.\nWe propose a new task, discriminative topic mining, which leverages a set of\nuser-provided category names to mine discriminative topics from text corpora.\nThis new task not only helps a user understand clearly and distinctively the\ntopics he/she is most interested in, but also benefits directly keyword-driven\nclassification tasks. We develop CatE, a novel category-name guided text\nembedding method for discriminative topic mining, which effectively leverages\nminimal user guidance to learn a discriminative embedding space and discover\ncategory representative terms in an iterative manner. We conduct a\ncomprehensive set of experiments to show that CatE mines high-quality set of\ntopics guided by category names only, and benefits a variety of downstream\napplications including weakly-supervised classification and lexical entailment\ndirection identification.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 04:32:30 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 21:34:49 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Meng", "Yu", ""], ["Huang", "Jiaxin", ""], ["Wang", "Guangyuan", ""], ["Wang", "Zihan", ""], ["Zhang", "Chao", ""], ["Zhang", "Yu", ""], ["Han", "Jiawei", ""]]}, {"id": "1908.07281", "submitter": "Sameh K. Mohamed", "authors": "Sameh K. Mohamed", "title": "Unsupervised Hierarchical Grouping of Knowledge Graph Entities", "comments": "10 pages - LASCAR@ESWC'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Knowledge graphs have attracted lots of attention in academic and industrial\nenvironments. Despite their usefulness, popular knowledge graphs suffer from\nincompleteness of information, especially in their type assertions. This has\nencouraged research in the automatic discovery of entity types. In this\ncontext, multiple works were developed to utilize logical inference on\nontologies and statistical machine learning methods to learn type assertion in\nknowledge graphs. However, these approaches suffer from limited performance on\nnoisy data, limited scalability and the dependence on labeled training samples.\nIn this work, we propose a new unsupervised approach that learns to categorize\nentities into a hierarchy of named groups. We show that our approach is able to\neffectively learn entity groups using a scalable procedure in noisy and sparse\ndatasets. We experiment our approach on a set of popular knowledge graph\nbenchmarking datasets, and we publish a collection of the outcome group\nhierarchies.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 11:40:16 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Mohamed", "Sameh K.", ""]]}, {"id": "1908.07371", "submitter": "Zitao Liu", "authors": "Zitao Liu, Zhexuan Xu, Yan Yan", "title": "Hierarchical Bayesian Personalized Recommendation: A Case Study and\n  Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Items in modern recommender systems are often organized in hierarchical\nstructures. These hierarchical structures and the data within them provide\nvaluable information for building personalized recommendation systems. In this\npaper, we propose a general hierarchical Bayesian learning framework, i.e.,\n\\emph{HBayes}, to learn both the structures and associated latent factors.\nFurthermore, we develop a variational inference algorithm that is able to learn\nmodel parameters with fast empirical convergence rate. The proposed HBayes is\nevaluated on two real-world datasets from different domains. The results\ndemonstrate the benefits of our approach on item recommendation tasks, and show\nthat it can outperform the state-of-the-art models in terms of precision,\nrecall, and normalized discounted cumulative gain. To encourage the\nreproducible results, we make our code public on a git repo:\n\\url{https://tinyurl.com/ycruhk4t}.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 14:10:17 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Liu", "Zitao", ""], ["Xu", "Zhexuan", ""], ["Yan", "Yan", ""]]}, {"id": "1908.07389", "submitter": "Ning Wang", "authors": "Jie Li, Haifeng Liu, Chuanghua Gui, Jianyu Chen, Zhenyun Ni, Ning Wang", "title": "The Design and Implementation of a Real Time Visual Search System on JD\n  E-commerce Platform", "comments": "7 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design and implementation of a visual search system for real\ntime image retrieval on JD.com, the world's third largest and China's largest\ne-commerce site. We demonstrate that our system can support real time visual\nsearch with hundreds of billions of product images at sub-second timescales and\nhandle frequent image updates through distributed hierarchical architecture and\nefficient indexing methods. We hope that sharing our practice with our real\nproduction system will inspire the middleware community's interest and\nappreciation for building practical large scale systems for emerging\napplications, such as ecommerce visual search.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 03:39:41 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Li", "Jie", ""], ["Liu", "Haifeng", ""], ["Gui", "Chuanghua", ""], ["Chen", "Jianyu", ""], ["Ni", "Zhenyun", ""], ["Wang", "Ning", ""]]}, {"id": "1908.07409", "submitter": "Ritwik Bhaduri", "authors": "Ritwik Bhaduri, Soham Bonnerjee, Subhrajyoty Roy", "title": "Onset detection: A new approach to QBH system", "comments": "30 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query by Humming (QBH) is a system to provide a user with the song(s) which\nthe user hums to the system. Current QBH method requires the extraction of\nonset and pitch information in order to track similarity with various versions\nof different songs. However, we here focus on detecting precise onsets only and\nuse them to build a QBH system which is better than existing methods in terms\nof speed and memory and empirically in terms of accuracy. We also provide\nstatistical analogy for onset detection functions and provide a measure of\nerror in our algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 14:44:32 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 18:40:49 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Bhaduri", "Ritwik", ""], ["Bonnerjee", "Soham", ""], ["Roy", "Subhrajyoty", ""]]}, {"id": "1908.07410", "submitter": "Giorgos Kordopatis-Zilos Mr.", "authors": "Giorgos Kordopatis-Zilos, Symeon Papadopoulos, Ioannis Patras, Ioannis\n  Kompatsiaris", "title": "ViSiL: Fine-grained Spatio-Temporal Video Similarity Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce ViSiL, a Video Similarity Learning architecture\nthat considers fine-grained Spatio-Temporal relations between pairs of videos\n-- such relations are typically lost in previous video retrieval approaches\nthat embed the whole frame or even the whole video into a vector descriptor\nbefore the similarity estimation. By contrast, our Convolutional Neural Network\n(CNN)-based approach is trained to calculate video-to-video similarity from\nrefined frame-to-frame similarity matrices, so as to consider both intra- and\ninter-frame relations. In the proposed method, pairwise frame similarity is\nestimated by applying Tensor Dot (TD) followed by Chamfer Similarity (CS) on\nregional CNN frame features - this avoids feature aggregation before the\nsimilarity calculation between frames. Subsequently, the similarity matrix\nbetween all video frames is fed to a four-layer CNN, and then summarized using\nChamfer Similarity (CS) into a video-to-video similarity score -- this avoids\nfeature aggregation before the similarity calculation between videos and\ncaptures the temporal similarity patterns between matching frame sequences. We\ntrain the proposed network using a triplet loss scheme and evaluate it on five\npublic benchmark datasets on four different video retrieval problems where we\ndemonstrate large improvements in comparison to the state of the art. The\nimplementation of ViSiL is publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 15:06:24 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Kordopatis-Zilos", "Giorgos", ""], ["Papadopoulos", "Symeon", ""], ["Patras", "Ioannis", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "1908.07590", "submitter": "Songwei Ge", "authors": "Songwei Ge, Curtis Xuan, Ruihua Song, Chao Zou, Wei Liu, Jin Zhou", "title": "From Text to Sound: A Preliminary Study on Retrieving Sound Effects to\n  Radio Stories", "comments": "In the Proceedings of the 42nd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sound effects play an essential role in producing high-quality radio stories\nbut require enormous labor cost to add. In this paper, we address the problem\nof automatically adding sound effects to radio stories with a retrieval-based\nmodel. However, directly implementing a tag-based retrieval model leads to high\nfalse positives due to the ambiguity of story contents. To solve this problem,\nwe introduce a retrieval-based framework hybridized with a semantic inference\nmodel which helps to achieve robust retrieval results. Our model relies on\nfine-designed features extracted from the context of candidate triggers. We\ncollect two story dubbing datasets through crowdsourcing to analyze the setting\nof adding sound effects and to train and test our proposed methods. We further\ndiscuss the importance of each feature and introduce several heuristic rules\nfor the trade-off between precision and recall. Together with the\ntext-to-speech technology, our results reveal a promising automatic pipeline on\nproducing high-quality radio stories.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 20:10:05 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Ge", "Songwei", ""], ["Xuan", "Curtis", ""], ["Song", "Ruihua", ""], ["Zou", "Chao", ""], ["Liu", "Wei", ""], ["Zhou", "Jin", ""]]}, {"id": "1908.07600", "submitter": "Songwei Ge", "authors": "Songwei Ge, Zhicheng Dou, Zhengbao Jiang, Jian-Yun Nie, and Ji-Rong\n  Wen", "title": "Personalizing Search Results Using Hierarchical RNN with Query-aware\n  Attention", "comments": "In the Proceedings of the 27th ACM International Conference on\n  Information and Knowledge Management (CIKM 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search results personalization has become an effective way to improve the\nquality of search engines. Previous studies extracted information such as past\nclicks, user topical interests, query click entropy and so on to tailor the\noriginal ranking. However, few studies have taken into account the sequential\ninformation underlying previous queries and sessions. Intuitively, the order of\nissued queries is important in inferring the real user interests. And more\nrecent sessions should provide more reliable personal signals than older\nsessions. In addition, the previous search history and user behaviors should\ninfluence the personalization of the current query depending on their\nrelatedness. To implement these intuitions, in this paper we employ a\nhierarchical recurrent neural network to exploit such sequential information\nand automatically generate user profile from historical data. We propose a\nquery-aware attention model to generate a dynamic user profile based on the\ninput query. Significant improvement is observed in the experiment with data\nfrom a commercial search engine when compared with several traditional\npersonalization models. Our analysis reveals that the attention model is able\nto attribute higher weights to more related past sessions after fine training.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 20:35:41 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Ge", "Songwei", ""], ["Dou", "Zhicheng", ""], ["Jiang", "Zhengbao", ""], ["Nie", "Jian-Yun", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "1908.07673", "submitter": "Donghuo Zeng", "authors": "Donghuo Zeng", "title": "Learning Joint Embedding for Cross-Modal Retrieval", "comments": "3 pages, 1 figure, Submitted to ICDM2019 Ph.D. Forum session", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cross-modal retrieval process is to use a query in one modality to obtain\nrelevant data in another modality. The challenging issue of cross-modal\nretrieval lies in bridging the heterogeneous gap for similarity computation,\nwhich has been broadly discussed in image-text, audio-text, and video-text\ncross-modal multimedia data mining and retrieval. However, the gap in temporal\nstructures of different data modalities is not well addressed due to the lack\nof alignment relationship between temporal cross-modal structures. Our research\nfocuses on learning the correlation between different modalities for the task\nof cross-modal retrieval. We have proposed an architecture: Supervised-Deep\nCanonical Correlation Analysis (S-DCCA), for cross-modal retrieval. In this\nforum paper, we will talk about how to exploit triplet neural networks (TNN) to\nenhance the correlation learning for cross-modal retrieval. The experimental\nresult shows the proposed TNN-based supervised correlation learning\narchitecture can get the best result when the data representation extracted by\nsupervised learning.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 02:04:18 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Zeng", "Donghuo", ""]]}, {"id": "1908.07738", "submitter": "Fan Liu", "authors": "Fan Liu, Zhiyong Cheng, Changchang Sun, Yinglong Wang, Liqiang Nie,\n  Mohan Kankanhalli", "title": "User Diverse Preference Modeling by Multimodal Attentive Metric Learning", "comments": "Accepted by ACM Multimedia 2019 as a full paper", "journal-ref": null, "doi": "10.1145/3343031.3350953", "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing recommender systems represent a user's preference with a\nfeature vector, which is assumed to be fixed when predicting this user's\npreferences for different items. However, the same vector cannot accurately\ncapture a user's varying preferences on all items, especially when considering\nthe diverse characteristics of various items. To tackle this problem, in this\npaper, we propose a novel Multimodal Attentive Metric Learning (MAML) method to\nmodel user diverse preferences for various items. In particular, for each\nuser-item pair, we propose an attention neural network, which exploits the\nitem's multimodal features to estimate the user's special attention to\ndifferent aspects of this item. The obtained attention is then integrated into\na metric-based learning method to predict the user preference on this item. The\nadvantage of metric learning is that it can naturally overcome the problem of\ndot product similarity, which is adopted by matrix factorization (MF) based\nrecommendation models but does not satisfy the triangle inequality property. In\naddition, it is worth mentioning that the attention mechanism cannot only help\nmodel user's diverse preferences towards different items, but also overcome the\ngeometrically restrictive problem caused by collaborative metric learning.\nExtensive experiments on large-scale real-world datasets show that our model\ncan substantially outperform the state-of-the-art baselines, demonstrating the\npotential of modeling user diverse preference for recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 07:52:39 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Liu", "Fan", ""], ["Cheng", "Zhiyong", ""], ["Sun", "Changchang", ""], ["Wang", "Yinglong", ""], ["Nie", "Liqiang", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1908.07749", "submitter": "Binh Nguyen-Thai", "authors": "ThaiBinh Nguyen, Atsuhiro Takasu", "title": "Boosting the Rating Prediction with Click Data and Textual Contents", "comments": "arXiv admin note: substantial text overlap with arXiv:1705.02085", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization (MF) is one of the most efficient methods for rating\npredictions. MF learns user and item representations by factorizing the\nuser-item rating matrix. Further, textual contents are integrated to\nconventional MF to address the cold-start problem. However, the textual\ncontents do not reflect all aspects of the items. In this paper, we propose a\nmodel that leverages the information hidden in the item co-click (i.e., items\nthat are often clicked together by a user) into learning item representations.\nWe develop TCMF (Textual Co Matrix Factorization) that learns the user and item\nrepresentations jointly from the user-item matrix, textual contents and item\nco-click matrix built from click data. Item co-click information captures the\nrelationships between items which are not captured via textual contents. The\nexperiments on two real-world datasets MovieTweetings, and Bookcrossing)\ndemonstrate that our method outperforms competing methods in terms of rating\nprediction. Further, we show that the proposed model can learn effective item\nrepresentations by comparing with state-of-the-art methods in classification\ntask which uses the item representations as input vectors.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 08:33:21 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 01:48:58 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Nguyen", "ThaiBinh", ""], ["Takasu", "Atsuhiro", ""]]}, {"id": "1908.07817", "submitter": "Zhengxuan Wu", "authors": "Zhengxuan Wu and Yueyi Jiang", "title": "Disentangling Latent Emotions of Word Embeddings on Complex Emotional\n  Narratives", "comments": "9 pages, submitted and accepted by NLP conference 2019", "journal-ref": "NLPCC2019", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding models such as GloVe are widely used in natural language\nprocessing (NLP) research to convert words into vectors. Here, we provide a\npreliminary guide to probe latent emotions in text through GloVe word vectors.\nFirst, we trained a neural network model to predict continuous emotion valence\nratings by taking linguistic inputs from Stanford Emotional Narratives Dataset\n(SEND). After interpreting the weights in the model, we found that only a few\ndimensions of the word vectors contributed to expressing emotions in text, and\nwords were clustered on the basis of their emotional polarities. Furthermore,\nwe performed a linear transformation that projected high dimensional embedded\nvectors into an emotion space. Based on NRC Emotion Lexicon (EmoLex), we\nvisualized the entanglement of emotions in the lexicon by using both projected\nand raw GloVe word vectors. We showed that, in the proposed emotion space, we\nwere able to better disentangle emotions than using raw GloVe vectors alone. In\naddition, we found that the sum vectors of different pairs of emotion words\nsuccessfully captured expressed human feelings in the EmoLex. For example, the\nsum of two embedded word vectors expressing Joy and Trust which express Love\nshared high similarity (similarity score .62) with the embedded vector\nexpressing Optimism. On the contrary, this sum vector was dissimilar\n(similarity score -.19) with the the embedded vector expressing Remorse. In\nthis paper, we argue that through the proposed emotion space, arithmetic of\nemotions is preserved in the word vectors. The affective representation\nuncovered in emotion vector space could shed some light on how to help machines\nto disentangle emotion expressed in word embeddings.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 11:05:45 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Wu", "Zhengxuan", ""], ["Jiang", "Yueyi", ""]]}, {"id": "1908.07818", "submitter": "Shibamouli Lahiri", "authors": "Shibamouli Lahiri", "title": "Replication of the Keyword Extraction part of the paper \"'Without the\n  Clutter of Unimportant Words': Descriptive Keyphrases for Text Visualization\"", "comments": "36 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Keyword Extraction\" refers to the task of automatically identifying the most\nrelevant and informative phrases in natural language text. As we are deluged\nwith large amounts of text data in many different forms and content - emails,\nblogs, tweets, Facebook posts, academic papers, news articles - the task of\n\"making sense\" of all this text by somehow summarizing them into a coherent\nstructure assumes paramount importance. Keyword extraction - a well-established\nproblem in Natural Language Processing - can help us here. In this report, we\nconstruct and test three different hypotheses (all related to the task of\nkeyword extraction) that take us one step closer to understanding how to\nmeaningfully identify and extract \"descriptive\" keyphrases. The work reported\nhere was done as part of replicating the study by Chuang et al. [3].\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 04:09:12 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Lahiri", "Shibamouli", ""]]}, {"id": "1908.07968", "submitter": "Felice Antonio Merra", "authors": "Yashar Deldjoo, Tommaso Di Noia and Felice Antonio Merra", "title": "Assessing the Impact of a User-Item Collaborative Attack on Class of\n  Users", "comments": "5 pages, RecSys2019, The 1st Workshop on the Impact of Recommender\n  Systems with ACM RecSys 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collaborative Filtering (CF) models lie at the core of most recommendation\nsystems due to their state-of-the-art accuracy. They are commonly adopted in\ne-commerce and online services for their impact on sales volume and/or\ndiversity, and their impact on companies' outcome. However, CF models are only\nas good as the interaction data they work with. As these models rely on outside\nsources of information, counterfeit data such as user ratings or reviews can be\ninjected by attackers to manipulate the underlying data and alter the impact of\nresulting recommendations, thus implementing a so-called shilling attack. While\nprevious works have focused on evaluating shilling attack strategies from a\nglobal perspective paying particular attention to the effect of the size of\nattacks and attacker's knowledge, in this work we explore the effectiveness of\nshilling attacks under novel aspects. First, we investigate the effect of\nattack strategies crafted on a target user in order to push the recommendation\nof a low-ranking item to a higher position, referred to as user-item attack.\nSecond, we evaluate the effectiveness of attacks in altering the impact of\ndifferent CF models by contemplating the class of the target user, from the\nperspective of the richness of her profile (i.e., cold v.s. warm user).\nFinally, similar to previous work we contemplate the size of attack (i.e., the\namount of fake profiles injected) in examining their success. The results of\nexperiments on two widely used datasets in business and movie domains, namely\nYelp and MovieLens, suggest that warm and cold users exhibit contrasting\nbehaviors in datasets with different characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 16:20:54 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Deldjoo", "Yashar", ""], ["Di Noia", "Tommaso", ""], ["Merra", "Felice Antonio", ""]]}, {"id": "1908.08037", "submitter": "Shalin Shah", "authors": "Shalin Shah, Venkataramana Kini", "title": "Hebbian Graph Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning has recently been successfully used to create vector\nrepresentations of entities in language learning, recommender systems and in\nsimilarity learning. Graph embeddings exploit the locality structure of a graph\nand generate embeddings for nodes which could be words in a language, products\nof a retail website; and the nodes are connected based on a context window. In\nthis paper, we consider graph embeddings with an error-free associative\nlearning update rule, which models the embedding vector of node as a non-convex\nGaussian mixture of the embeddings of the nodes in its immediate vicinity with\nsome constant variance that is reduced as iterations progress. It is very easy\nto parallelize our algorithm without any form of shared memory, which makes it\npossible to use it on very large graphs with a much higher dimensionality of\nthe embeddings. We study the efficacy of proposed method on several benchmark\ndata sets and favorably compare with state of the art methods. Further,\nproposed method is applied to generate relevant recommendations for a large\nretailer.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 02:45:43 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 20:55:07 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 23:28:36 GMT"}, {"version": "v4", "created": "Thu, 20 Feb 2020 21:25:36 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Shah", "Shalin", ""], ["Kini", "Venkataramana", ""]]}, {"id": "1908.08147", "submitter": "Nagendra Kumar", "authors": "Nagendra Kumar, Rakshita Nagalla, Tanya Marwah, Manish Singh", "title": "Sentiment Dynamics in Social Media News Channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media is currently one of the most important means of news\ncommunication. Since people are consuming a large fraction of their daily news\nthrough social media, most of the traditional news channels are using social\nmedia to catch the attention of users. Each news channel has its own strategies\nto attract more users. In this paper, we analyze how the news channels use\nsentiment to garner users' attention in social media. We compare the sentiment\nof social media news posts of television, radio and print media, to show the\ndifferences in the ways these channels cover the news. We also analyze users'\nreactions and opinion sentiment on news posts with different sentiments. We\nperform our experiments on a dataset extracted from Facebook Pages of five\npopular news channels. Our dataset contains 0.15 million news posts and 1.13\nbillion users reactions. The results of our experiments show that the sentiment\nof user opinion has a strong correlation with the sentiment of the news post\nand the type of information source. Our study also illustrates the differences\namong the social media news channels of different types of news sources.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 23:56:02 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Kumar", "Nagendra", ""], ["Nagalla", "Rakshita", ""], ["Marwah", "Tanya", ""], ["Singh", "Manish", ""]]}, {"id": "1908.08284", "submitter": "Jose Antonio Sanchez", "authors": "Jos\\'e Antonio S\\'anchez Rodr\\'iguez, Jui-Chieh Wu, Mustafa\n  Khandwawala", "title": "Two-Stage Session-based Recommendations with Candidate Rank Embeddings", "comments": "Accepted in the Fashion RECSYS workshop recsysXfashion'19, September\n  20, 2019, Copenhagen, Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in Session-based recommender systems have gained attention\ndue to their potential of providing real-time personalized recommendations with\nhigh recall, especially when compared to traditional methods like matrix\nfactorization and item-based collaborative filtering. Nowadays, two of the most\nrecent methods are Short-Term Attention/Memory Priority Model for Session-based\nRecommendation (STAMP) and Neural Attentive Session-based Recommendation\n(NARM). However, when these two methods were applied in the similar-item\nrecommendation dataset of Zalando (Fashion-Similar), they did not work\nout-of-the-box compared to a simple Collaborative-Filtering approach. Aiming\nfor improving the similar-item recommendation, we propose to concentrate\nefforts on enhancing the rank of the few most relevant items from the original\nrecommendations, by employing the information of the session of the user\nencoded by an attention network. The efficacy of this strategy was confirmed\nwhen using a novel Candidate Rank Embedding that encodes the global ranking\ninformation of each candidate in the re-ranking process. Experimental results\nin Fashion-Similar show significant improvements over the baseline on Recall\nand MRR at 20, as well as improvements in Click Through Rate based on an online\ntest. Additionally, it is important to point out from the evaluation that was\nperformed the potential of this method on the next click prediction problem\nbecause when applied to STAMP and NARM, it improves the Recall and MRR at 20 on\ntwo publicly available real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 09:48:34 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Rodr\u00edguez", "Jos\u00e9 Antonio S\u00e1nchez", ""], ["Wu", "Jui-Chieh", ""], ["Khandwawala", "Mustafa", ""]]}, {"id": "1908.08298", "submitter": "Nagendra Kumar", "authors": "Nagendra Kumar, Yash Chandarana, K. Anand, and Manish Singh", "title": "Using Social Media for Word-of-Mouth Marketing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays online social networks are used extensively for personal and\ncommercial purposes. This widespread popularity makes them an ideal platform\nfor advertisements. Social media can be used for both direct and word-of-mouth\n(WoM) marketing. Although WoM marketing is considered more effective and it\nrequires less advertisement cost, it is currently being under-utilized. To do\nWoM marketing, we need to identify a set of people who can use their\nauthoritative position in social network to promote a given product. In this\npaper, we show how to do WoM marketing in Facebook group, which is a question\nanswer type of social network. We also present concept of reinforced WoM\nmarketing, where multiple authorities can together promote a product to\nincrease the effectiveness of marketing. We perform our experiments on Facebook\ngroup dataset consisting of 0.3 million messages and 10 million user reactions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 10:37:46 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Kumar", "Nagendra", ""], ["Chandarana", "Yash", ""], ["Anand", "K.", ""], ["Singh", "Manish", ""]]}, {"id": "1908.08327", "submitter": "Humberto Corona Pampin", "authors": "Jui-Chieh Wu, Jos\\'e Antonio S\\'anchez Rodr\\'iguez, Humberto Jes\\'us\n  Corona Pamp\\'in", "title": "Session-based Complementary Fashion Recommendations", "comments": "Workshop on Recommender Systems in Fashion, 13th ACM Conference on\n  Recommender Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In modern fashion e-commerce platforms, where customers can browse thousands\nto millions of products, recommender systems are useful tools to navigate and\nnarrow down the vast assortment. In this scenario, complementary\nrecommendations serve the user need to find items that can be worn together. In\nthis paper, we present a personalized, session-based complementary item\nrecommendation algorithm, ZSF-c, tailored for the fashion usecase. We propose a\nsampling strategy adopted to build the training set, which is useful when\nexisting user interaction data cannot be directly used due to poor quality or\navailability. Our proposed approach shows significant improvements in terms of\naccuracy compared to the collaborative filtering approach, serving\ncomplementary item recommendations to our customers at the time of the\nexperiments CF-c. The results show an offline relative uplift of +8.2% in\nOrders Recall@5, as well as a significant +3.24% increase in the number of\npurchased products measured in an online A/B test carried out in a fashion\ne-commerce platform with 28 million active customers.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 11:46:43 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Wu", "Jui-Chieh", ""], ["Rodr\u00edguez", "Jos\u00e9 Antonio S\u00e1nchez", ""], ["Pamp\u00edn", "Humberto Jes\u00fas Corona", ""]]}, {"id": "1908.08328", "submitter": "Dietmar Jannach", "authors": "Dietmar Jannach and Michael Jugovac", "title": "Measuring the Business Value of Recommender Systems", "comments": null, "journal-ref": null, "doi": "10.1145/3370082", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender Systems are nowadays successfully used by all major web sites\n(from e-commerce to social media) to filter content and make suggestions in a\npersonalized way. Academic research largely focuses on the value of\nrecommenders for consumers, e.g., in terms of reduced information overload. To\nwhat extent and in which ways recommender systems create business value is,\nhowever, much less clear, and the literature on the topic is scattered. In this\nresearch commentary, we review existing publications on field tests of\nrecommender systems and report which business-related performance measures were\nused in such real-world deployments. We summarize common challenges of\nmeasuring the business value in practice and critically discuss the value of\nalgorithmic improvements and offline experiments as commonly done in academic\nenvironments. Overall, our review indicates that various open questions remain\nboth regarding the realistic quantification of the business effects of\nrecommenders and the performance assessment of recommendation algorithms in\nacademia.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 11:52:09 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 12:05:24 GMT"}, {"version": "v3", "created": "Tue, 17 Dec 2019 17:37:30 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Jannach", "Dietmar", ""], ["Jugovac", "Michael", ""]]}, {"id": "1908.08478", "submitter": "Roland Molontay", "authors": "Roland Molontay and Marcell Nagy", "title": "Two Decades of Network Science as seen through the co-authorship network\n  of network scientists", "comments": "To appear in International Conference on Advances in Social Networks\n  Analysis and Mining (ASONAM '19), Vancouver, BC, Canada", "journal-ref": null, "doi": "10.1145/3341161.3343685", "report-no": null, "categories": "cs.SI cs.DL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex networks have attracted a great deal of research interest in the last\ntwo decades since Watts & Strogatz, Barab\\'asi & Albert and Girvan & Newman\npublished their highly-cited seminal papers on small-world networks, on\nscale-free networks and on the community structure of complex networks,\nrespectively. These fundamental papers initiated a new era of research\nestablishing an interdisciplinary field called network science. Due to the\nmultidisciplinary nature of the field, a diverse but not divided network\nscience community has emerged in the past 20 years. This paper honors the\ncontributions of network science by exploring the evolution of this community\nas seen through the growing co-authorship network of network scientists (here\nthe notion refers to a scholar with at least one paper citing at least one of\nthe three aforementioned milestone papers). After investigating various\ncharacteristics of 29,528 network science papers, we construct the\nco-authorship network of 52,406 network scientists and we analyze its topology\nand dynamics. We shed light on the collaboration patterns of the last 20 years\nof network science by investigating numerous structural properties of the\nco-authorship network and by using enhanced data visualization techniques. We\nalso identify the most central authors, the largest communities, investigate\nthe spatiotemporal changes, and compare the properties of the network to\nscientometric indicators.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 16:17:44 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 14:15:03 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Molontay", "Roland", ""], ["Nagy", "Marcell", ""]]}, {"id": "1908.08564", "submitter": "Saurav Manchanda", "authors": "Saurav Manchanda, Mohit Sharma and George Karypis", "title": "Intent term selection and refinement in e-commerce queries", "comments": "Extended version of paper \"Intent term weighing in e-commerce\n  queries\" to appear in CIKM'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In e-commerce, a user tends to search for the desired product by issuing a\nquery to the search engine and examining the retrieved results. If the search\nengine was successful in correctly understanding the user's query, it will\nreturn results that correspond to the products whose attributes match the terms\nin the query that are representative of the query's product intent. However,\nthe search engine may fail to retrieve results that satisfy the query's product\nintent and thus degrading user experience due to different issues in query\nprocessing: (i) when multiple terms are present in a query it may fail to\ndetermine the relevant terms that are representative of the query's product\nintent, and (ii) it may suffer from vocabulary gap between the terms in the\nquery and the product's description, i.e., terms used in the query are\nsemantically similar but different from the terms in the product description.\nHence, identifying the terms that describe the query's product intent and\npredicting additional terms that describe the query's product intent better\nthan the existing query terms to the search engine is an essential task in\ne-commerce search. In this paper, we leverage the historical query\nreformulation logs of a major e-commerce retailer to develop distant-supervised\napproaches to solve both these problems. Our approaches exploit the fact that\nthe significance of a term is dependent upon the context (other terms in the\nneighborhood) in which it is used in order to learn the importance of the term\ntowards the query's product intent. We show that identifying and emphasizing\nthe terms that define the query's product intent leads to a 3% improvement in\nranking. Moreover, for the tasks of identifying the important terms in a query\nand for predicting the additional terms that represent product intent,\nexperiments illustrate that our approaches outperform the non-contextual\nbaselines.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 19:04:24 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Manchanda", "Saurav", ""], ["Sharma", "Mohit", ""], ["Karypis", "George", ""]]}, {"id": "1908.08581", "submitter": "Carsten Eickhoff", "authors": "Carsten Eickhoff and Floran Gmehlin and Anu V. Patel and Jocelyn\n  Boullier and Hamish Fraser", "title": "DC3 -- A Diagnostic Case Challenge Collection for Clinical Decision\n  Support", "comments": null, "journal-ref": "The 2019 ACM SIGIR International Conference on the Theory of\n  Information Retrieval (ICTIR '19)", "doi": "10.1145/3341981.3344239", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical care, obtaining a correct diagnosis is the first step towards\nsuccessful treatment and, ultimately, recovery. Depending on the complexity of\nthe case, the diagnostic phase can be lengthy and ridden with errors and\ndelays. Such errors have a high likelihood to cause patients severe harm or\neven lead to their death and are estimated to cost the U.S. healthcare system\nseveral hundred billion dollars each year.\n  To avoid diagnostic errors, physicians increasingly rely on diagnostic\ndecision support systems drawing from heuristics, historic cases, textbooks,\nclinical guidelines and scholarly biomedical literature. The evaluation of such\nsystems, however, is often conducted in an ad-hoc fashion, using\nnon-transparent methodology, and proprietary data.\n  This paper presents DC3, a collection of 31 extremely difficult diagnostic\ncase challenges, manually compiled and solved by clinical experts. For each\ncase, we present a number of temporally ordered physician-generated\nobservations alongside the eventually confirmed true diagnosis. We additionally\nprovide inferred dense relevance judgments for these cases among the PubMed\ncollection of 27 million scholarly biomedical articles.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 20:20:22 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Eickhoff", "Carsten", ""], ["Gmehlin", "Floran", ""], ["Patel", "Anu V.", ""], ["Boullier", "Jocelyn", ""], ["Fraser", "Hamish", ""]]}, {"id": "1908.08609", "submitter": "Kai Middlebrook", "authors": "Kai Middlebrook, Kian Sheik", "title": "Song Hit Prediction: Predicting Billboard Hits Using Spotify Data", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we attempt to solve the Hit Song Science problem, which aims to\npredict which songs will become chart-topping hits. We constructed a dataset\nwith approximately 1.8 million hit and non-hit songs and extracted their audio\nfeatures using the Spotify Web API. We test four models on our dataset. Our\nbest model was random forest, which was able to predict Billboard song success\nwith 88% accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 22:10:08 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 18:39:45 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Middlebrook", "Kai", ""], ["Sheik", "Kian", ""]]}, {"id": "1908.08622", "submitter": "Nagendra Kumar", "authors": "Nagendra Kumar, Gopi Ande, J. Shirish Kumar, Manish Singh", "title": "Toward Maximizing the Visibility of Content in Social Media Brand Pages:\n  A Temporal Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large amount of content is generated everyday in social media. One of the\nmain goals of content creators is to spread their information to a large\naudience. There are many factors that affect information spread, such as\nposting time, location, type of information, number of social connections, etc.\nIn this paper, we look at the problem of finding the best posting time(s) to\nget high content visibility. The posting time is derived taking other factors\ninto account, such as location, type of information, etc. In this paper, we do\nour analysis over Facebook pages. We propose six posting schedules that can be\nused for individual pages or group of pages with similar audience reaction\nprofile. We perform our experiment on a Facebook pages dataset containing 0.3\nmillion posts, 10 million audience reactions. Our best posting schedule can\nlead to seven times more number of audience reactions compared to the average\nnumber of audience reactions that users would get without following any\noptimized posting schedule. We also present some interesting audience reaction\npatterns that we obtained through daily, weekly and monthly audience reaction\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 23:45:23 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Kumar", "Nagendra", ""], ["Ande", "Gopi", ""], ["Kumar", "J. Shirish", ""], ["Singh", "Manish", ""]]}, {"id": "1908.08656", "submitter": "Ninh Pham", "authors": "Stephan S. Lorenzen, Ninh Pham", "title": "Revisiting Wedge Sampling for Budgeted Maximum Inner Product Search", "comments": "ECML-PKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-k maximum inner product search (MIPS) is a central task in many machine\nlearning applications. This paper extends top-k MIPS with a budgeted setting,\nthat asks for the best approximate top-k MIPS given a limit of B computational\noperations. We investigate recent advanced sampling algorithms, including wedge\nand diamond sampling to solve it. Though the design of these sampling schemes\nnaturally supports budgeted top-k MIPS, they suffer from the linear cost from\nscanning all data points to retrieve top-k results and the performance\ndegradation for handling negative inputs.\n  This paper makes two main contributions. First, we show that diamond sampling\nis essentially a combination between wedge sampling and basic sampling for\ntop-k MIPS. Our theoretical analysis and empirical evaluation show that wedge\nis competitive (often superior) to diamond on approximating top-k MIPS\nregarding both efficiency and accuracy. Second, we propose a series of\nalgorithmic engineering techniques to deploy wedge sampling on budgeted top-k\nMIPS. Our novel deterministic wedge-based algorithm runs significantly faster\nthan the state-of-the-art methods for budgeted and exact top-k MIPS while\nmaintaining the top-5 precision at least 80% on standard recommender system\ndata sets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 04:05:25 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 08:54:28 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Lorenzen", "Stephan S.", ""], ["Pham", "Ninh", ""]]}, {"id": "1908.08690", "submitter": "Yoshifumi Seki", "authors": "Yoshifumi Seki, Mitsuo Yoshida", "title": "Analysis of User Dwell Time by Category in News Application", "comments": "4 pages, 3 figures, WI 2018 Workshop : The International Workshop on\n  Web Personalization, Recommender Systems, and Social Media (WPRSM2018)", "journal-ref": null, "doi": "10.1109/WI.2018.000-3", "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dwell time indicates how long a user looked at a page, and this is used\nespecially in fields where ratings from users such as search engines,\nrecommender systems, and advertisements are important. Despite the importance\nof this index, however, its characteristics are not well known. In this paper,\nwe analyze the dwell time of news pages according to category in smartphone\napplication. Our aim is to clarify the characteristics of dwell time and the\nrelation between length of news page and dwell time, for each category. The\nresults indicated different dwell time trends for each category. For example,\nthe social category had fewer news pages with shorter dwell time than peaks,\ncompared to other categories, and there were a few news pages with remarkably\nshort dwell time. We also found a large difference by category in the\ncorrelation value between dwell time and length of news page. Specifically,\npolitical news had the highest correlation value and technology news had the\nlowest. In addition, we found that a user tends to get sufficient information\nabout the news content from the news title in short dwell times.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 06:51:57 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Seki", "Yoshifumi", ""], ["Yoshida", "Mitsuo", ""]]}, {"id": "1908.08788", "submitter": "Ningyu Zhang", "authors": "Shumin Deng, Ningyu Zhang, Zhanlin Sun, Jiaoyan Chen, Huajun Chen", "title": "When Low Resource NLP Meets Unsupervised Language Model:\n  Meta-pretraining Then Meta-learning for Few-shot Text Classification", "comments": "AAAI student abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification tends to be difficult when data are deficient or when it\nis required to adapt to unseen classes. In such challenging scenarios, recent\nstudies have often used meta-learning to simulate the few-shot task, thus\nnegating implicit common linguistic features across tasks. This paper addresses\nsuch problems using meta-learning and unsupervised language models. Our\napproach is based on the insight that having a good generalization from a few\nexamples relies on both a generic model initialization and an effective\nstrategy for adapting this model to newly arising tasks. We show that our\napproach is not only simple but also produces a state-of-the-art performance on\na well-studied sentiment classification dataset. It can thus be further\nsuggested that pretraining could be a promising solution for few-shot learning\nof many other NLP tasks. The code and the dataset to replicate the experiments\nare made available at https://github.com/zxlzr/FewShotNLP.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 17:23:29 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 01:57:41 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Deng", "Shumin", ""], ["Zhang", "Ningyu", ""], ["Sun", "Zhanlin", ""], ["Chen", "Jiaoyan", ""], ["Chen", "Huajun", ""]]}, {"id": "1908.08810", "submitter": "Jannik Fischbach", "authors": "Jannik Fischbach, Maximilian Junker, Andreas Vogelsang, Dietmar\n  Freudenstein", "title": "Automated Generation of Test Models from Semi-Structured Requirements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  [Context:] Model-based testing is an instrument for automated generation of\ntest cases. It requires identifying requirements in documents, understanding\nthem syntactically and semantically, and then translating them into a test\nmodel. One light-weight language for these test models are Cause-Effect-Graphs\n(CEG) that can be used to derive test cases. [Problem:] The creation of test\nmodels is laborious and we lack an automated solution that covers the entire\nprocess from requirement detection to test model creation. In addition, the\nmajority of requirements is expressed in natural language (NL), which is hard\nto translate to test models automatically. [Principal Idea:] We build on the\nfact that not all NL requirements are equally unstructured. We found that 14 %\nof the lines in requirements documents of our industry partner contain\n\"pseudo-code\"-like descriptions of business rules. We apply Machine Learning to\nidentify such semi-structured requirements descriptions and propose a\nrule-based approach for their translation into CEGs. [Contribution:] We make\nthree contributions: (1) an algorithm for the automatic detection of\nsemi-structured requirements descriptions in documents, (2) an algorithm for\nthe automatic translation of the identified requirements into a CEG and (3) a\nstudy demonstrating that our proposed solution leads to 86 % time savings for\ntest model creation without loss of quality.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 13:02:20 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Fischbach", "Jannik", ""], ["Junker", "Maximilian", ""], ["Vogelsang", "Andreas", ""], ["Freudenstein", "Dietmar", ""]]}, {"id": "1908.08987", "submitter": "Qun Liu", "authors": "Qun Liu, Edward Collier, Supratik Mukhopadhyay", "title": "PCGAN-CHAR: Progressively Trained Classifier Generative Adversarial\n  Networks for Classification of Noisy Handwritten Bangla Characters", "comments": "Paper was accepted at the 21st International Conference on\n  Asia-Pacific Digital Libraries (ICADL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the sparsity of features, noise has proven to be a great inhibitor in\nthe classification of handwritten characters. To combat this, most techniques\nperform denoising of the data before classification. In this paper, we\nconsolidate the approach by training an all-in-one model that is able to\nclassify even noisy characters. For classification, we progressively train a\nclassifier generative adversarial network on the characters from low to high\nresolution. We show that by learning the features at each resolution\nindependently a trained model is able to accurately classify characters even in\nthe presence of noise. We experimentally demonstrate the effectiveness of our\napproach by classifying noisy versions of MNIST, handwritten Bangla Numeral,\nand Basic Character datasets.\n", "versions": [{"version": "v1", "created": "Sun, 11 Aug 2019 08:01:58 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Liu", "Qun", ""], ["Collier", "Edward", ""], ["Mukhopadhyay", "Supratik", ""]]}, {"id": "1908.08998", "submitter": "Wanling Gao", "authors": "Wanling Gao, Fei Tang, Lei Wang, Jianfeng Zhan, Chunxin Lan, Chunjie\n  Luo, Yunyou Huang, Chen Zheng, Jiahui Dai, Zheng Cao, Daoyi Zheng, Haoning\n  Tang, Kunlin Zhan, Biao Wang, Defei Kong, Tong Wu, Minghe Yu, Chongkang Tan,\n  Huan Li, Xinhui Tian, Yatao Li, Junchao Shao, Zhenyu Wang, Xiaoyu Wang, and\n  Hainan Ye", "title": "AIBench: An Industry Standard Internet Service AI Benchmark Suite", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's Internet Services are undergoing fundamental changes and shifting to\nan intelligent computing era where AI is widely employed to augment services.\nIn this context, many innovative AI algorithms, systems, and architectures are\nproposed, and thus the importance of benchmarking and evaluating them rises.\nHowever, modern Internet services adopt a microservice-based architecture and\nconsist of various modules. The diversity of these modules and complexity of\nexecution paths, the massive scale and complex hierarchy of datacenter\ninfrastructure, the confidential issues of data sets and workloads pose great\nchallenges to benchmarking. In this paper, we present the first\nindustry-standard Internet service AI benchmark suite---AIBench with seventeen\nindustry partners, including several top Internet service providers. AIBench\nprovides a highly extensible, configurable, and flexible benchmark framework\nthat contains loosely coupled modules. We identify sixteen prominent AI problem\ndomains like learning to rank, each of which forms an AI component benchmark,\nfrom three most important Internet service domains: search engine, social\nnetwork, and e-commerce, which is by far the most comprehensive AI benchmarking\neffort. On the basis of the AIBench framework, abstracting the real-world data\nsets and workloads from one of the top e-commerce providers, we design and\nimplement the first end-to-end Internet service AI benchmark, which contains\nthe primary modules in the critical paths of an industry scale application and\nis scalable to deploy on different cluster scales. The specifications, source\ncode, and performance numbers are publicly available from the benchmark council\nweb site http://www.benchcouncil.org/AIBench/index.html.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 10:15:39 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 14:39:47 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Gao", "Wanling", ""], ["Tang", "Fei", ""], ["Wang", "Lei", ""], ["Zhan", "Jianfeng", ""], ["Lan", "Chunxin", ""], ["Luo", "Chunjie", ""], ["Huang", "Yunyou", ""], ["Zheng", "Chen", ""], ["Dai", "Jiahui", ""], ["Cao", "Zheng", ""], ["Zheng", "Daoyi", ""], ["Tang", "Haoning", ""], ["Zhan", "Kunlin", ""], ["Wang", "Biao", ""], ["Kong", "Defei", ""], ["Wu", "Tong", ""], ["Yu", "Minghe", ""], ["Tan", "Chongkang", ""], ["Li", "Huan", ""], ["Tian", "Xinhui", ""], ["Li", "Yatao", ""], ["Shao", "Junchao", ""], ["Wang", "Zhenyu", ""], ["Wang", "Xiaoyu", ""], ["Ye", "Hainan", ""]]}, {"id": "1908.09104", "submitter": "Yujie Lin", "authors": "Yujie Lin, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Jun Ma, Maarten de\n  Rijke", "title": "Improving Outfit Recommendation with Co-supervision of Fashion\n  Generation", "comments": null, "journal-ref": null, "doi": "10.1145/3308558.3313614", "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of fashion recommendation includes two main challenges: visual\nunderstanding and visual matching. Visual understanding aims to extract\neffective visual features. Visual matching aims to model a human notion of\ncompatibility to compute a match between fashion items. Most previous studies\nrely on recommendation loss alone to guide visual understanding and matching.\nAlthough the features captured by these methods describe basic characteristics\n(e.g., color, texture, shape) of the input items, they are not directly related\nto the visual signals of the output items (to be recommended). This is\nproblematic because the aesthetic characteristics (e.g., style, design), based\non which we can directly infer the output items, are lacking. Features are\nlearned under the recommendation loss alone, where the supervision signal is\nsimply whether the given two items are matched or not. To address this problem,\nwe propose a neural co-supervision learning framework, called the FAshion\nRecommendation Machine (FARM). FARM improves visual understanding by\nincorporating the supervision of generation loss, which we hypothesize to be\nable to better encode aesthetic information. FARM enhances visual matching by\nintroducing a novel layer-to-layer matching mechanism to fuse aesthetic\ninformation more effectively, and meanwhile avoiding paying too much attention\nto the generation quality and ignoring the recommendation performance.\nExtensive experiments on two publicly available datasets show that FARM\noutperforms state-of-the-art models on outfit recommendation, in terms of AUC\nand MRR. Detailed analyses of generated and recommended items demonstrate that\nFARM can encode better features and generate high quality images as references\nto improve recommendation performance.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 07:37:57 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Lin", "Yujie", ""], ["Ren", "Pengjie", ""], ["Chen", "Zhumin", ""], ["Ren", "Zhaochun", ""], ["Ma", "Jun", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1908.09119", "submitter": "Varun Pandya", "authors": "Varun Pandya", "title": "Automatic Text Summarization of Legal Cases: A Hybrid Approach", "comments": "Part of 5th International Conference on Natural Language Processing\n  (NATP 2019) Proceedings", "journal-ref": null, "doi": "10.5121/csit.2019.91004", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual Summarization of large bodies of text involves a lot of human effort\nand time, especially in the legal domain. Lawyers spend a lot of time preparing\nlegal briefs of their clients' case files. Automatic Text summarization is a\nconstantly evolving field of Natural Language Processing(NLP), which is a\nsubdiscipline of the Artificial Intelligence Field. In this paper a hybrid\nmethod for automatic text summarization of legal cases using k-means clustering\ntechnique and tf-idf(term frequency-inverse document frequency) word vectorizer\nis proposed. The summary generated by the proposed method is compared using\nROGUE evaluation parameters with the case summary as prepared by the lawyer for\nappeal in court. Further, suggestions for improving the proposed method are\nalso presented.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 10:05:40 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Pandya", "Varun", ""]]}, {"id": "1908.09205", "submitter": "Paul Kantor", "authors": "Vladimir Menkov, Paul Kantor", "title": "Ontology alignment: A Content-Based Bayesian Approach", "comments": "29 pp. 2 figure. Research Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many legacy databases, and related stores of information that are\nmaintained by distinct organizations, and there are other organizations that\nwould like to be able to access and use those disparate sources. Among the\nexamples of current interest are such things as emergency room records, of\ninterest in tracking and interdicting illicit drugs, or social media public\nposts that indicate preparation and intention for a mass shooting incident. In\nmost cases, this information is discovered too late to be useful. While\nagencies responsible for coordination are aware of the potential value of\ncontemporaneous access to new data, the costs of establishing a connection are\nprohibitive. The problem grown even worse with the proliferation of\n``hash-tagging,'' which permits new labels and ontological relations to spring\nup overnight. While research interest has waned, the need for powerful and\ninexpensive tools enabling prompt access to multiple sources has grown ever\nmore pressing. This paper describes techniques for computing alignment matrix\ncoefficients, which relate the fields or content of one database to those of\nanother, using the Bayesian Ontology Alignment tool (BOA). Particular attention\nis given to formulas that have an easy-to-understand meaning when all cells of\nthe data sources containing values from some small set. These formulas can be\nexpressed in terms of probability estimates. The estimates themselves are given\nby a ``black box'' polytomous logistic regression model (PLRM), and thus can be\neasily generalized to the case of any arbitrary probability-generating model.\nThe specific PLRM model used in this example is the BOXER Bayesian Extensible\nOnline Regression model.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 20:54:27 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Menkov", "Vladimir", ""], ["Kantor", "Paul", ""]]}, {"id": "1908.09246", "submitter": "Rui Wang", "authors": "Rui Wang and Deyu Zhou and Yulan He", "title": "Open Event Extraction from Online Text using a Generative Adversarial\n  Network", "comments": "Accepted by EMNLP-IJCNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To extract the structured representations of open-domain events, Bayesian\ngraphical models have made some progress. However, these approaches typically\nassume that all words in a document are generated from a single event. While\nthis may be true for short text such as tweets, such an assumption does not\ngenerally hold for long text such as news articles. Moreover, Bayesian\ngraphical models often rely on Gibbs sampling for parameter inference which may\ntake long time to converge. To address these limitations, we propose an event\nextraction model based on Generative Adversarial Nets, called\nAdversarial-neural Event Model (AEM). AEM models an event with a Dirichlet\nprior and uses a generator network to capture the patterns underlying latent\nevents. A discriminator is used to distinguish documents reconstructed from the\nlatent events and the original documents. A byproduct of the discriminator is\nthat the features generated by the learned discriminator network allow the\nvisualization of the extracted events. Our model has been evaluated on two\nTwitter datasets and a news article dataset. Experimental results show that our\nmodel outperforms the baseline approaches on all the datasets, with more\nsignificant improvements observed on the news article dataset where an increase\nof 15\\% is observed in F-measure.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 03:17:38 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Wang", "Rui", ""], ["Zhou", "Deyu", ""], ["He", "Yulan", ""]]}, {"id": "1908.09354", "submitter": "Kun Cao", "authors": "Kun Cao, James Fairbanks", "title": "Unsupervised Construction of Knowledge Graphs From Text and Code", "comments": "25th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,\n  15th International Workshop On Mining and Learning with Graphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scientific literature is a rich source of information for data mining\nwith conceptual knowledge graphs; the open science movement has enriched this\nliterature with complementary source code that implements scientific models. To\nexploit this new resource, we construct a knowledge graph using unsupervised\nlearning methods to identify conceptual entities. We associate source code\nentities to these natural language concepts using word embedding and clustering\ntechniques. Practical naming conventions for methods and functions tend to\nreflect the concept(s) they implement. We take advantage of this specificity by\npresenting a novel process for joint clustering text concepts that combines\nword-embeddings, nonlinear dimensionality reduction, and clustering techniques\nto assist in understanding, organizing, and comparing software in the open\nscience ecosystem. With our pipeline, we aim to assist scientists in building\non existing models in their discipline when making novel models for new\nphenomena. By combining source code and conceptual information, our knowledge\ngraph enhances corpus-wide understanding of scientific literature.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 16:10:31 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Cao", "Kun", ""], ["Fairbanks", "James", ""]]}, {"id": "1908.09454", "submitter": "Bhaskarjyoti Das", "authors": "Vishwas Sathish, Tanya Mehrotra, Simran Dhinwa, Bhaskarjyoti Das", "title": "Graph Embedding Based Hybrid Social Recommendation System", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item recommendation tasks are a widely studied topic. Recent developments in\ndeep learning and spectral methods paved a path towards efficient graph\nembedding techniques. But little research has been done on applying these graph\nembedding to social graphs for recommendation tasks. This paper focuses at\nperformance of various embedding methods applied on social graphs for the task\nof item recommendation. Additionally, a hybrid model is proposed wherein chosen\nembedding models are combined together to give a collective output. We put\nforward the hypothesis that such a hybrid model would perform better than\nindividual embedding for recommendation task. With recommendation using\nindividual embedding as a baseline, performance for hybrid model for the same\ntask is evaluated and compared. Standard metrics are used for qualitative\ncomparison. It is found that the proposed hybrid model outperforms the\nbaseline.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 03:34:48 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Sathish", "Vishwas", ""], ["Mehrotra", "Tanya", ""], ["Dhinwa", "Simran", ""], ["Das", "Bhaskarjyoti", ""]]}, {"id": "1908.09456", "submitter": "Chen Qu", "authors": "Chen Qu, Liu Yang, Minghui Qiu, Yongfeng Zhang, Cen Chen, W. Bruce\n  Croft and Mohit Iyyer", "title": "Attentive History Selection for Conversational Question Answering", "comments": "Accepted to CIKM 2019", "journal-ref": null, "doi": "10.1145/3357384.3357905", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational question answering (ConvQA) is a simplified but concrete\nsetting of conversational search. One of its major challenges is to leverage\nthe conversation history to understand and answer the current question. In this\nwork, we propose a novel solution for ConvQA that involves three aspects.\nFirst, we propose a positional history answer embedding method to encode\nconversation history with position information using BERT in a natural way.\nBERT is a powerful technique for text representation. Second, we design a\nhistory attention mechanism (HAM) to conduct a \"soft selection\" for\nconversation histories. This method attends to history turns with different\nweights based on how helpful they are on answering the current question. Third,\nin addition to handling conversation history, we take advantage of multi-task\nlearning (MTL) to do answer prediction along with another essential\nconversation task (dialog act prediction) using a uniform model architecture.\nMTL is able to learn more expressive and generic representations to improve the\nperformance of ConvQA. We demonstrate the effectiveness of our model with\nextensive experimental evaluations on QuAC, a large-scale ConvQA dataset. We\nshow that position information plays an important role in conversation history\nmodeling. We also visualize the history attention and provide new insights into\nconversation history understanding.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 03:39:51 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Qu", "Chen", ""], ["Yang", "Liu", ""], ["Qiu", "Minghui", ""], ["Zhang", "Yongfeng", ""], ["Chen", "Cen", ""], ["Croft", "W. Bruce", ""], ["Iyyer", "Mohit", ""]]}, {"id": "1908.09485", "submitter": "Jong Seon Kim", "authors": "Jong Seon Kim, Jong Wook Kim, Yon Dohn Chung", "title": "Successive Point-of-Interest Recommendation with Local Differential\n  Privacy", "comments": "This paper has been accepted to IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3076809", "report-no": null, "categories": "cs.IR cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A point-of-interest (POI) recommendation system performs an important role in\nlocation-based services because it can help people to explore new locations and\npromote advertisers to launch advertisements at appropriate locations. The\nexisting POI recommendation systems require raw check-in history of users,\nwhich might cause location privacy violations. Although there have been several\nmatrix factorization (MF) based privacy-preserving recommendation systems, they\ncan only focus on user-POI relationships without considering the human\nmovements in check-in history. To tackle this problem, we design a successive\nPOI recommendation framework with local differential privacy, named SPIREL.\nSPIREL uses two types of information derived from the check-in history as input\nfor the factorization: a transition pattern between two POIs and the visit\ncounts of POIs. We propose a novel objective function for learning the user-POI\nand POI-POI relationships simultaneously. We further integrate local\ndifferential privacy mechanisms in our proposed framework to prevent potential\nlocation privacy breaches. Experiments using four public datasets demonstrate\nthat SPIREL achieves better POI recommendation quality while accomplishing\nstronger privacy protection.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 06:02:30 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 00:13:54 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kim", "Jong Seon", ""], ["Kim", "Jong Wook", ""], ["Chung", "Yon Dohn", ""]]}, {"id": "1908.09520", "submitter": "Zhixian Yang", "authors": "Zhixian Yang, Yuanning Gao, Xiaofeng Gao, Guihai Chen", "title": "NETR-Tree: An Eifficient Framework for Social-Based Time-Aware Spatial\n  Keyword Query", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of social media and the development of geo-positioning\ntechnology stimulate the growth of location-based social networks (LBSNs). With\na large volume of data containing locations, texts, check-in information, and\nsocial relationships, spatial keyword queries in LBSNs have become increasingly\ncomplex. In this paper, we identify and solve the Social-based Time-aware\nSpatial Keyword Query (STSKQ) that returns the top-k objects by taking\ngeo-spatial score, keywords similarity, visiting time score, and social\nrelationship effect into consideration. To tackle STSKQ, we propose a two-layer\nhybrid index structure called Network Embedding Time-aware R-tree (NETR-tree).\nIn user layer, we exploit network embedding strategy to measure relationship\neffect in users' relationship network. In location layer, we build a Time-aware\nR-tree (TR-tree), which considers spatial objects' spatio-temporal check-in\ninformation. On the basis of NETR-tree, a corresponding query processing\nalgorithm is presented. Finally, extensive experiments on real-data collected\nfrom two different real-life LBSNs demonstrate the effectiveness and efficiency\nof the proposed methods, compared with existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 08:26:16 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Yang", "Zhixian", ""], ["Gao", "Yuanning", ""], ["Gao", "Xiaofeng", ""], ["Chen", "Guihai", ""]]}, {"id": "1908.09541", "submitter": "Juan Jose Prieto-Gutierrez", "authors": "Juan Jose Prieto-Gutierrez and Francisco Segado-Boj", "title": "Annals of Library and Information Studies. A bibliometric analysis of\n  the journal and a comparison with the top library and information studies\n  journals in Asia and worldwide (2011_2017)", "comments": "7 Tables, 3 Figures, 16 pages", "journal-ref": null, "doi": "10.1080/0361526X.2019.1637387", "report-no": null, "categories": "cs.DL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a thorough bibliometric analysis of research published in\nAnnals of Library and Information Studies (ALIS), an India-based journal, for\nthe period 2011_2017. Specifically, it compares this journal's trends with\nthose of other library and information science (LIS) journals from the same\ngeographical area (India, and Asia as a whole) and with the 10 highest-rated\nLIS journals worldwide. The source of the data used was the multidisciplinary\ndatabase Scopus. To perform this comparison, ALIS' production was analyzed in\norder to identify authorship patterns; for example, authors' countries of\nresidence, co-authorship trends, and collaboration networks. Research topics\nwere identified through keyword analysis, while performance was measured by\nexamining the number of citations articles received. This study provides\nsubstantial information. The research lines detected through examining the\nkeywords in ALIS articles were determined to be similar to those for the top\nLIS journals in both Asia and worldwide. Specifically, ALIS authors are\nfocusing on metrics, bibliometrics, and social networking, which follows global\ntrends. Notably, however, collaboration among Asia-based journals was found to\nbe lower than that in the top-indexed journals in the LIS field. The results\nobtained present a roadmap for expanding the research in this field.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 09:08:39 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Prieto-Gutierrez", "Juan Jose", ""], ["Segado-Boj", "Francisco", ""]]}, {"id": "1908.09701", "submitter": "Huan Zhao", "authors": "Huan Zhao, Yingqi Zhou, Yangqiu Song, Dik Lun Lee", "title": "Motif Enhanced Recommendation over Heterogeneous Information Network", "comments": "CIKM 2019 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous Information Networks (HIN) has been widely used in recommender\nsystems (RSs). In previous HIN-based RSs, meta-path is used to compute the\nsimilarity between users and items. However, existing meta-path based methods\nonly consider first-order relations, ignoring higher-order relations among the\nnodes of \\textit{same} type, captured by \\textit{motifs}. In this paper, we\npropose to use motifs to capture higher-order relations among nodes of same\ntype in a HIN and develop the motif-enhanced meta-path (MEMP) to combine\nmotif-based higher-order relations with edge-based first-order relations. With\nMEMP-based similarities between users and items, we design a recommending model\nMoHINRec, and experimental results on two real-world datasets, Epinions and\nCiaoDVD, demonstrate its superiority over existing HIN-based RS methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 14:21:14 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Zhao", "Huan", ""], ["Zhou", "Yingqi", ""], ["Song", "Yangqiu", ""], ["Lee", "Dik Lun", ""]]}, {"id": "1908.09785", "submitter": "Preslav Nakov", "authors": "Yoan Dinkov, Ivan Koychev, Preslav Nakov", "title": "Detecting Toxicity in News Articles: Application to Bulgarian", "comments": "Fact-checking, source reliability, political ideology, news media,\n  Bulgarian, RANLP-2019. arXiv admin note: text overlap with arXiv:1810.01765", "journal-ref": "RANLP-2019", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online media aim for reaching ever bigger audience and for attracting ever\nlonger attention span. This competition creates an environment that rewards\nsensational, fake, and toxic news. To help limit their spread and impact, we\npropose and develop a news toxicity detector that can recognize various types\nof toxic content. While previous research primarily focused on English, here we\ntarget Bulgarian. We created a new dataset by crawling a website that for five\nyears has been collecting Bulgarian news articles that were manually\ncategorized into eight toxicity groups. Then we trained a multi-class\nclassifier with nine categories: eight toxic and one non-toxic. We experimented\nwith different representations based on ElMo, BERT, and XLM, as well as with a\nvariety of domain-specific features. Due to the small size of our dataset, we\ncreated a separate model for each feature type, and we ultimately combined\nthese models into a meta-classifier. The evaluation results show an accuracy of\n59.0% and a macro-F1 score of 39.7%, which represent sizable improvements over\nthe majority-class baseline (Acc=30.3%, macro-F1=5.2%).\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 16:37:03 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Dinkov", "Yoan", ""], ["Koychev", "Ivan", ""], ["Nakov", "Preslav", ""]]}, {"id": "1908.09876", "submitter": "Jacson Barbosa", "authors": "Jacson Rodrigues Barbosa, Ricardo Marcondes Marcacini, Ricardo Britto,\n  Frederico Soares, Solange Rezende, Auri M. R. Vincenzi, Marcio E. Delamaro", "title": "BULNER: BUg Localization with word embeddings and NEtwork Regularization", "comments": "VII Workshop on Software Visualization, Evolution and Maintenance\n  (VEM '19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bug localization (BL) from the bug report is the strategic activity of the\nsoftware maintaining process. Because BL is a costly and tedious activity, BL\ntechniques information retrieval-based and machine learning-based could aid\nsoftware engineers. We propose a method for BUg Localization with word\nembeddings and Network Regularization (BULNER). The preliminary results suggest\nthat BULNER has better performance than two state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 18:52:30 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Barbosa", "Jacson Rodrigues", ""], ["Marcacini", "Ricardo Marcondes", ""], ["Britto", "Ricardo", ""], ["Soares", "Frederico", ""], ["Rezende", "Solange", ""], ["Vincenzi", "Auri M. R.", ""], ["Delamaro", "Marcio E.", ""]]}, {"id": "1908.09928", "submitter": "Mansi Ranjit Mane", "authors": "Mansi Ranjit Mane, Stephen Guo, Kannan Achan", "title": "Complementary-Similarity Learning using Quadruplet Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel learning framework to answer questions such as \"if a user\nis purchasing a shirt, what other items will (s)he need with the shirt?\" Our\nframework learns distributed representations for items from available textual\ndata, with the learned representations representing items in a latent space\nexpressing functional complementarity as well similarity. In particular, our\nframework places functionally similar items close together in the latent space,\nwhile also placing complementary items closer than non-complementary items, but\nfarther away than similar items. In this study, we introduce a new dataset of\nsimilar, complementary, and negative items derived from the Amazon co-purchase\ndataset. For evaluation purposes, we focus our approach on clothing and fashion\nverticals. As per our knowledge, this is the first attempt to learn similar and\ncomplementary relationships simultaneously through just textual title metadata.\nOur framework is applicable across a broad set of items in the product catalog\nand can generate quality complementary item recommendations at scale.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 21:29:19 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 21:42:27 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Mane", "Mansi Ranjit", ""], ["Guo", "Stephen", ""], ["Achan", "Kannan", ""]]}, {"id": "1908.09951", "submitter": "Bilal Ghanem", "authors": "Bilal Ghanem, Paolo Rosso, Francisco Rangel", "title": "An Emotional Analysis of False Information in Social Media and News\n  Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fake news is risky since it has been created to manipulate the readers'\nopinions and beliefs. In this work, we compared the language of false news to\nthe real one of real news from an emotional perspective, considering a set of\nfalse information types (propaganda, hoax, clickbait, and satire) from social\nmedia and online news articles sources. Our experiments showed that false\ninformation has different emotional patterns in each of its types, and emotions\nplay a key role in deceiving the reader. Based on that, we proposed a LSTM\nneural network model that is emotionally-infused to detect false news.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 22:49:35 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Ghanem", "Bilal", ""], ["Rosso", "Paolo", ""], ["Rangel", "Francisco", ""]]}, {"id": "1908.09972", "submitter": "An Yan", "authors": "An Yan, Shuo Cheng, Wang-Cheng Kang, Mengting Wan, Julian McAuley", "title": "CosRec: 2D Convolutional Neural Networks for Sequential Recommendation", "comments": "To appear in CIKM-2019, code at https://github.com/zzxslp/CosRec", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential patterns play an important role in building modern recommender\nsystems. To this end, several recommender systems have been built on top of\nMarkov Chains and Recurrent Models (among others). Although these sequential\nmodels have proven successful at a range of tasks, they still struggle to\nuncover complex relationships nested in user purchase histories. In this paper,\nwe argue that modeling pairwise relationships directly leads to an efficient\nrepresentation of sequential features and captures complex item correlations.\nSpecifically, we propose a 2D convolutional network for sequential\nrecommendation (CosRec). It encodes a sequence of items into a three-way\ntensor; learns local features using 2D convolutional filters; and aggregates\nhigh-order interactions in a feedforward manner. Quantitative results on two\npublic datasets show that our method outperforms both conventional methods and\nrecent sequence-based approaches, achieving state-of-the-art performance on\nvarious evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 01:08:18 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Yan", "An", ""], ["Cheng", "Shuo", ""], ["Kang", "Wang-Cheng", ""], ["Wan", "Mengting", ""], ["McAuley", "Julian", ""]]}, {"id": "1908.09980", "submitter": "Chang Liu", "authors": "Eddie S.J. Du, Chang Liu, David H. Wayne", "title": "Automated Fashion Size Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately predict the fit of fashion items and recommend the\ncorrect size is key to reducing merchandise returns in e-commerce. A critical\nprerequisite of fit prediction is size normalization, the mapping of product\nsizes across brands to a common space in which sizes can be compared. At\npresent, size normalization is usually a time-consuming manual process. We\npropose a method to automate size normalization through the use of salesdata.\nThe size mappings generated from our automated approaches are comparable to\nhuman-generated mappings.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 01:43:43 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Du", "Eddie S. J.", ""], ["Liu", "Chang", ""], ["Wayne", "David H.", ""]]}, {"id": "1908.09987", "submitter": "Yinwei Wei", "authors": "Yinwei Wei, Zhiyong Cheng, Xuzheng Yu, Zhou Zhao, Lei Zhu, Liqiang Nie", "title": "Personalized Hashtag Recommendation for Micro-videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized hashtag recommendation methods aim to suggest users hashtags to\nannotate, categorize, and describe their posts. The hashtags, that a user\nprovides to a post (e.g., a micro-video), are the ones which in her mind can\nwell describe the post content where she is interested in. It means that we\nshould consider both users' preferences on the post contents and their personal\nunderstanding on the hashtags. Most existing methods rely on modeling either\nthe interactions between hashtags and posts or the interactions between users\nand hashtags for hashtag recommendation. These methods have not well explored\nthe complicated interactions among users, hashtags, and micro-videos. In this\npaper, towards the personalized micro-video hashtag recommendation, we propose\na Graph Convolution Network based Personalized Hashtag Recommendation (GCN-PHR)\nmodel, which leverages recently advanced GCN techniques to model the complicate\ninteractions among <users, hashtags, micro-videos> and learn their\nrepresentations. In our model, the users, hashtags, and micro-videos are three\ntypes of nodes in a graph and they are linked based on their direct\nassociations. In particular, the message-passing strategy is used to learn the\nrepresentation of a node (e.g., user) by aggregating the message passed from\nthe directly linked other types of nodes (e.g., hashtag and micro-video).\nBecause a user is often only interested in certain parts of a micro-video and a\nhashtag is typically used to describe the part (of a micro-video) that the user\nis interested in, we leverage the attention mechanism to filter the message\npassed from micro-videos to users and hashtags, which can significantly improve\nthe representation capability. Extensive experiments have been conducted on two\nreal-world micro-video datasets and demonstrate that our model outperforms the\nstate-of-the-art approaches by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 02:14:52 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Wei", "Yinwei", ""], ["Cheng", "Zhiyong", ""], ["Yu", "Xuzheng", ""], ["Zhao", "Zhou", ""], ["Zhu", "Lei", ""], ["Nie", "Liqiang", ""]]}, {"id": "1908.10033", "submitter": "Shantanu Sharma", "authors": "Nisha Panwar, Shantanu Sharma, Guoxi Wang, Sharad Mehrotra, Nalini\n  Venkatasubramanian, Mamadou H. Diallo, Ardalan Amiri Sani", "title": "IoT Notary: Sensor Data Attestation in Smart Environment", "comments": "Accepted in IEEE International Symposium on Network Computing and\n  Applications (NCA), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary IoT environments, such as smart buildings, require end-users to\ntrust data-capturing rules published by the systems. There are several reasons\nwhy such a trust is misplaced --- IoT systems may violate the rules\ndeliberately or IoT devices may transfer user data to a malicious third-party\ndue to cyberattacks, leading to the loss of individuals' privacy or service\nintegrity. To address such concerns, we propose IoT Notary, a framework to\nensure trust in IoT systems and applications. IoT Notary provides secure log\nsealing on live sensor data to produce a verifiable `proof-of-integrity,' based\non which a verifier can attest that captured sensor data adheres to the\npublished data-capturing rules. IoT Notary is an integral part of TIPPERS, a\nsmart space system that has been deployed at UCI to provide various real-time\nlocation-based services in the campus. IoT Notary imposes nominal overheads for\nverification, thereby users can verify their data of one day in less than two\nseconds.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 05:10:04 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Panwar", "Nisha", ""], ["Sharma", "Shantanu", ""], ["Wang", "Guoxi", ""], ["Mehrotra", "Sharad", ""], ["Venkatasubramanian", "Nalini", ""], ["Diallo", "Mamadou H.", ""], ["Sani", "Ardalan Amiri", ""]]}, {"id": "1908.10128", "submitter": "Erik Bryhn Myklebust", "authors": "Erik Bryhn Myklebust and Ernesto Jimenez-Ruiz and Jiaoyan Chen and\n  Raoul Wolf and Knut Erik Tollefsen", "title": "TERA: the Toxicological Effect and Risk Assessment Knowledge Graph", "comments": "Submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ecological risk assessment requires large amounts of chemical effect data\nfrom laboratory experiments. Due to experimental effort and animal welfare\nconcerns it is desired to extrapolate data from existing sources. To cover the\nrequired chemical effect data several data sources need to be integrated to\nenable their interoperability. In this paper we introduce the Toxicological\nEffect and Risk Assessment (TERA) knowledge graph, which aims at providing such\nintegrated view, and the data preparation and steps followed to construct this\nknowledge graph.\n  We also present the applications of TERA for chemical effect prediction and\nthe potential applications within the Semantic Web community.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 11:05:40 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 12:28:25 GMT"}, {"version": "v3", "created": "Sat, 7 Sep 2019 17:35:47 GMT"}, {"version": "v4", "created": "Wed, 11 Sep 2019 10:53:35 GMT"}, {"version": "v5", "created": "Thu, 12 Dec 2019 06:22:06 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Myklebust", "Erik Bryhn", ""], ["Jimenez-Ruiz", "Ernesto", ""], ["Chen", "Jiaoyan", ""], ["Wolf", "Raoul", ""], ["Tollefsen", "Knut Erik", ""]]}, {"id": "1908.10139", "submitter": "Sreekanth Vempati", "authors": "Sreekanth Vempati, Korah T Malayil, Sruthi V, Sandeep R", "title": "Enabling Hyper-Personalisation: Automated Ad Creative Generation and\n  Ranking for Fashion e-Commerce", "comments": "Workshop on Recommender Systems in Fashion, 13th ACM Conference on\n  Recommender Systems, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Homepage is the first touch point in the customer's journey and is one of the\nprominent channels of revenue for many e-commerce companies. A user's attention\nis mostly captured by homepage banner images (also called Ads/Creatives). The\nset of banners shown and their design, influence the customer's interest and\nplays a key role in optimizing the click through rates of the banners.\nPresently, massive and repetitive effort is put in, to manually create\naesthetically pleasing banner images. Due to the large amount of time and\neffort involved in this process, only a small set of banners are made live at\nany point. This reduces the number of banners created as well as the degree of\npersonalization that can be achieved. This paper thus presents a method to\ngenerate creatives automatically on a large scale in a short duration. The\navailability of diverse banners generated helps in improving personalization as\nthey can cater to the taste of larger audience. The focus of our paper is on\ngenerating wide variety of homepage banners that can be made as an input for\nuser level personalization engine. Following are the main contributions of this\npaper: 1) We introduce and explain the need for large scale banner generation\nfor e-commerce 2) We present on how we utilize existing deep learning based\ndetectors which can automatically annotate the required objects/tags from the\nimage. 3) We also propose a Genetic Algorithm based method to generate an\noptimal banner layout for the given image content, input components and other\ndesign constraints. 4) Further, to aid the process of picking the right set of\nbanners, we designed a ranking method and evaluated multiple models. All our\nexperiments have been performed on data from Myntra (http://www.myntra.com),\none of the top fashion e-commerce players in India.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 11:28:37 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Vempati", "Sreekanth", ""], ["Malayil", "Korah T", ""], ["V", "Sruthi", ""], ["R", "Sandeep", ""]]}, {"id": "1908.10149", "submitter": "Michael Barz", "authors": "Michael Barz and Daniel Sonntag", "title": "Incremental Improvement of a Question Answering System by Re-ranking\n  Answer Candidates using Machine Learning", "comments": "Accepted for oral presentation at tenth International Workshop on\n  Spoken Dialogue Systems Technology (IWSDS) 2019", "journal-ref": null, "doi": "10.1007/978-981-15-9323-9_34", "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implement a method for re-ranking top-10 results of a state-of-the-art\nquestion answering (QA) system. The goal of our re-ranking approach is to\nimprove the answer selection given the user question and the top-10 candidates.\nWe focus on improving deployed QA systems that do not allow re-training or\nre-training comes at a high cost. Our re-ranking approach learns a similarity\nfunction using n-gram based features using the query, the answer and the\ninitial system confidence as input. Our contributions are: (1) we generate a QA\ntraining corpus starting from 877 answers from the customer care domain of\nT-Mobile Austria, (2) we implement a state-of-the-art QA pipeline using neural\nsentence embeddings that encode queries in the same space than the answer\nindex, and (3) we evaluate the QA pipeline and our re-ranking approach using a\nseparately provided test set. The test set can be considered to be available\nafter deployment of the system, e.g., based on feedback of users. Our results\nshow that the system performance, in terms of top-n accuracy and the mean\nreciprocal rank, benefits from re-ranking using gradient boosted regression\ntrees. On average, the mean reciprocal rank improves by 9.15%.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 11:54:23 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Barz", "Michael", ""], ["Sonntag", "Daniel", ""]]}, {"id": "1908.10171", "submitter": "Wanyu Chen", "authors": "Wanyu Chen, Pengjie Ren, Fei Cai, Maarten de Rijke", "title": "Improving End-to-End Sequential Recommendations with Intent-aware\n  Diversification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Recommendation (SRs) that capture users' dynamic intents by\nmodeling user sequential behaviors can recommend closely accurate products to\nusers. Previous work on SRs is mostly focused on optimizing the recommendation\naccuracy, often ignoring the recommendation diversity, even though it is an\nimportant criterion for evaluating the recommendation performance. Most\nexisting methods for improving the diversity of recommendations are not ideally\napplicable for SRs because they assume that user intents are static and rely on\npost-processing the list of recommendations to promote diversity. We consider\nboth recommendation accuracy and diversity for SRs by proposing an end-to-end\nneural model, called Intent-aware Diversified Sequential Recommendation (IDSR).\nSpecifically, we introduce an Implicit Intent Mining module (IIM) into SRs to\ncapture different user intents reflected in user behavior sequences. Then, we\ndesign an Intent-aware Diversity Promoting (IDP) loss to supervise the learning\nof the IIM module and force the model to take recommendation diversity into\nconsideration during training. Extensive experiments on two benchmark datasets\nshow that IDSR significantly outperforms state-of-the-art methods in terms of\nrecommendation diversity while yielding comparable or superior recommendation\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 12:57:25 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Chen", "Wanyu", ""], ["Ren", "Pengjie", ""], ["Cai", "Fei", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1908.10180", "submitter": "Qizhi Zhang", "authors": "Qizhi Zhang and Yi Lin and Kangle Wu and Yongliang Li and Anxiang Zeng", "title": "Matrix embedding method in match for session-based recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session based model is widely used in recommend system. It use the user click\nsequence as input of a Recurrent Neural Network (RNN), and get the output of\nthe RNN network as the vector embedding of the session, and use the inner\nproduct of the vector embedding of session and the vector embedding of the next\nitem as the score that is the metric of the interest to the next item. This\nmethod can be used for the \"match\" stage for the recommendation system whose\nitem number is very big by using some index method like KD-Tree or Ball-Tree\nand etc.. But this method repudiate the variousness of the interest of user in\na session. We generated the model to modify the vector embedding of session to\na symmetric matrix embedding, that is equivalent to a quadratic form on the\nvector space of items. The score is builded as the value of the vector\nembedding of next item under the quadratic form. The eigenvectors of the\nsymmetric matrix embedding corresponding to the positive eigenvalues are\nconjectured to represent the interests of user in the session. This method can\nbe used for the \"match\" stage also. The experiments show that this method is\nbetter than the method of vector embedding.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 13:18:51 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Zhang", "Qizhi", ""], ["Lin", "Yi", ""], ["Wu", "Kangle", ""], ["Li", "Yongliang", ""], ["Zeng", "Anxiang", ""]]}, {"id": "1908.10193", "submitter": "Hiteshwar Azad", "authors": "Hiteshwar Kumar Azad, Akshay Deepak", "title": "A novel model for query expansion using pseudo-relevant web knowledge", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of information retrieval, query expansion (QE) has long been\nused as a technique to deal with the fundamental issue of word mismatch between\na user's query and the target information. In the context of the relationship\nbetween the query and expanded terms, existing weighting techniques often fail\nto appropriately capture the term-term relationship and term to the whole query\nrelationship, resulting in low retrieval effectiveness. Our proposed QE\napproach addresses this by proposing three weighting models based on (1)\ntf-itf, (2) k-nearest neighbor (kNN) based cosine similarity, and (3)\ncorrelation score. Further, to extract the initial set of expanded terms, we\nuse pseudo-relevant web knowledge consisting of the top N web pages returned by\nthe three popular search engines namely, Google, Bing, and DuckDuckGo, in\nresponse to the original query. Among the three weighting models, tf-itf scores\neach of the individual terms obtained from the web content, kNN-based cosine\nsimilarity scores the expansion terms to obtain the term-term relationship, and\ncorrelation score weighs the selected expansion terms with respect to the whole\nquery. The proposed model, called web knowledge based query expansion (WKQE),\nachieves an improvement of 25.89% on the MAP score and 30.83% on the GMAP score\nover the unexpanded queries on the FIRE dataset. A comparative analysis of the\nWKQE techniques with other related approaches clearly shows significant\nimprovement in the retrieval performance. We have also analyzed the effect of\nvarying the number of pseudo-relevant documents and expansion terms on the\nretrieval effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 13:38:35 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Azad", "Hiteshwar Kumar", ""], ["Deepak", "Akshay", ""]]}, {"id": "1908.10322", "submitter": "Dokook Choe", "authors": "Dokook Choe, Rami Al-Rfou, Mandy Guo, Heeyoung Lee, Noah Constant", "title": "Bridging the Gap for Tokenizer-Free Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purely character-based language models (LMs) have been lagging in quality on\nlarge scale datasets, and current state-of-the-art LMs rely on word\ntokenization. It has been assumed that injecting the prior knowledge of a\ntokenizer into the model is essential to achieving competitive results. In this\npaper, we show that contrary to this conventional wisdom, tokenizer-free LMs\nwith sufficient capacity can achieve competitive performance on a large scale\ndataset. We train a vanilla transformer network with 40 self-attention layers\non the One Billion Word (lm1b) benchmark and achieve a new state of the art for\ntokenizer-free LMs, pushing these models to be on par with their word-based\ncounterparts.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 16:53:59 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Choe", "Dokook", ""], ["Al-Rfou", "Rami", ""], ["Guo", "Mandy", ""], ["Lee", "Heeyoung", ""], ["Constant", "Noah", ""]]}, {"id": "1908.10383", "submitter": "Yuning Mao", "authors": "Yuning Mao, Liyuan Liu, Qi Zhu, Xiang Ren, Jiawei Han", "title": "Facet-Aware Evaluation for Extractive Summarization", "comments": "ACL 2020, Long Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonly adopted metrics for extractive summarization focus on lexical\noverlap at the token level. In this paper, we present a facet-aware evaluation\nsetup for better assessment of the information coverage in extracted summaries.\nSpecifically, we treat each sentence in the reference summary as a\n\\textit{facet}, identify the sentences in the document that express the\nsemantics of each facet as \\textit{support sentences} of the facet, and\nautomatically evaluate extractive summarization methods by comparing the\nindices of extracted sentences and support sentences of all the facets in the\nreference summary. To facilitate this new evaluation setup, we construct an\nextractive version of the CNN/Daily Mail dataset and perform a thorough\nquantitative investigation, through which we demonstrate that facet-aware\nevaluation manifests better correlation with human judgment than ROUGE, enables\nfine-grained evaluation as well as comparative analysis, and reveals valuable\ninsights of state-of-the-art summarization methods. Data can be found at\nhttps://github.com/morningmoni/FAR.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 18:03:12 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 04:12:38 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Mao", "Yuning", ""], ["Liu", "Liyuan", ""], ["Zhu", "Qi", ""], ["Ren", "Xiang", ""], ["Han", "Jiawei", ""]]}, {"id": "1908.10408", "submitter": "Vikas Garg", "authors": "Vikas K. Garg and Inderjit S. Dhillon and Hsiang-Fu Yu", "title": "Multiresolution Transformer Networks: Recurrence is Not Essential for\n  Modeling Hierarchical Structure", "comments": "Initial version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The architecture of Transformer is based entirely on self-attention, and has\nbeen shown to outperform models that employ recurrence on sequence transduction\ntasks such as machine translation. The superior performance of Transformer has\nbeen attributed to propagating signals over shorter distances, between\npositions in the input and the output, compared to the recurrent architectures.\nWe establish connections between the dynamics in Transformer and recurrent\nnetworks to argue that several factors including gradient flow along an\nensemble of multiple weakly dependent paths play a paramount role in the\nsuccess of Transformer. We then leverage the dynamics to introduce {\\em\nMultiresolution Transformer Networks} as the first architecture that exploits\nhierarchical structure in data via self-attention. Our models significantly\noutperform state-of-the-art recurrent and hierarchical recurrent models on two\nreal-world datasets for query suggestion, namely, \\aol and \\amazon. In\nparticular, on AOL data, our model registers at least 20\\% improvement on each\nprecision score, and over 25\\% improvement on the BLEU score with respect to\nthe best performing recurrent model. We thus provide strong evidence that\nrecurrence is not essential for modeling hierarchical structure.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 18:51:50 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Garg", "Vikas K.", ""], ["Dhillon", "Inderjit S.", ""], ["Yu", "Hsiang-Fu", ""]]}, {"id": "1908.10410", "submitter": "Daniel Probst", "authors": "Daniel Probst and Jean-Louis Reymond", "title": "Visualization of Very Large High-Dimensional Data Sets as Minimum\n  Spanning Trees", "comments": "33 pages, 14 figures, 1 table, supplementary information included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chemical sciences are producing an unprecedented amount of large,\nhigh-dimensional data sets containing chemical structures and associated\nproperties. However, there are currently no algorithms to visualize such data\nwhile preserving both global and local features with a sufficient level of\ndetail to allow for human inspection and interpretation. Here, we propose a\nsolution to this problem with a new data visualization method, TMAP, capable of\nrepresenting data sets of up to millions of data points and arbitrary high\ndimensionality as a two-dimensional tree (http://tmap.gdb.tools).\nVisualizations based on TMAP are better suited than t-SNE or UMAP for the\nexploration and interpretation of large data sets due to their tree-like\nnature, increased local and global neighborhood and structure preservation, and\nthe transparency of the methods the algorithm is based on. We apply TMAP to the\nmost used chemistry data sets including databases of molecules such as ChEMBL,\nFDB17, the Natural Products Atlas, DSSTox, as well as to the MoleculeNet\nbenchmark collection of data sets. We also show its broad applicability with\nfurther examples from biology, particle physics, and literature.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 15:14:19 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 10:43:40 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 14:32:02 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Probst", "Daniel", ""], ["Reymond", "Jean-Louis", ""]]}, {"id": "1908.10419", "submitter": "Yuning Mao", "authors": "Yuning Mao, Jingjing Tian, Jiawei Han, Xiang Ren", "title": "Hierarchical Text Classification with Reinforced Label Assignment", "comments": "EMNLP 2019", "journal-ref": null, "doi": "10.18653/v1/D19-1042", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While existing hierarchical text classification (HTC) methods attempt to\ncapture label hierarchies for model training, they either make local decisions\nregarding each label or completely ignore the hierarchy information during\ninference. To solve the mismatch between training and inference as well as\nmodeling label dependencies in a more principled way, we formulate HTC as a\nMarkov decision process and propose to learn a Label Assignment Policy via deep\nreinforcement learning to determine where to place an object and when to stop\nthe assignment process. The proposed method, HiLAP, explores the hierarchy\nduring both training and inference time in a consistent manner and makes\ninter-dependent decisions. As a general framework, HiLAP can incorporate\ndifferent neural encoders as base models for end-to-end training. Experiments\non five public datasets and four base models show that HiLAP yields an average\nimprovement of 33.4% in Macro-F1 over flat classifiers and outperforms\nstate-of-the-art HTC methods by a large margin. Data and code can be found at\nhttps://github.com/morningmoni/HiLAP.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 19:15:26 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Mao", "Yuning", ""], ["Tian", "Jingjing", ""], ["Han", "Jiawei", ""], ["Ren", "Xiang", ""]]}, {"id": "1908.10554", "submitter": "Zhenghao Liu PhD.", "authors": "Zhenghao Liu, Chenyan Xiong, Maosong Sun, Zhiyuan Liu", "title": "Explore Entity Embedding Effectiveness in Entity Retrieval", "comments": "12 pages, 2 figures", "journal-ref": "CCL 2019", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores entity embedding effectiveness in ad-hoc entity\nretrieval, which introduces distributed representation of entities into entity\nretrieval. The knowledge graph contains lots of knowledge and models entity\nsemantic relations with the well-formed structural representation. Entity\nembedding learns lots of semantic information from the knowledge graph and\nrepresents entities with a low-dimensional representation, which provides an\nopportunity to establish interactions between query related entities and\ncandidate entities for entity retrieval. Our experiments demonstrate the\neffectiveness of entity embedding based model, which achieves more than 5\\%\nimprovement than the previous state-of-the-art learning to rank based entity\nretrieval model. Our further analysis reveals that the entity semantic match\nfeature effective, especially for the scenario which needs more semantic\nunderstanding.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 05:27:19 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Liu", "Zhenghao", ""], ["Xiong", "Chenyan", ""], ["Sun", "Maosong", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "1908.10598", "submitter": "Giulio Ermanno Pibiri", "authors": "Giulio Ermanno Pibiri and Rossano Venturini", "title": "Techniques for Inverted Index Compression", "comments": "Accepted by ACM Computing Surveys (CSUR), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data structure at the core of large-scale search engines is the inverted\nindex, which is essentially a collection of sorted integer sequences called\ninverted lists. Because of the many documents indexed by such engines and\nstringent performance requirements imposed by the heavy load of queries, the\ninverted index stores billions of integers that must be searched efficiently.\nIn this scenario, index compression is essential because it leads to a better\nexploitation of the computer memory hierarchy for faster query processing and,\nat the same time, allows reducing the number of storage machines. The aim of\nthis article is twofold: first, surveying the encoding algorithms suitable for\ninverted index compression and, second, characterizing the performance of the\ninverted index through experimentation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 08:29:14 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 13:32:17 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Pibiri", "Giulio Ermanno", ""], ["Venturini", "Rossano", ""]]}, {"id": "1908.10642", "submitter": "Diogo Goncalves", "authors": "Diogo Goncalves, Liweu Liu, Ana Magalh\\~aes", "title": "How big can style be? Addressing high dimensionality for recommending\n  with style", "comments": "Accepted as a poster in the Fashion RecSys'19 workshop\n  recsysXfashion'19, September 20, 2019, Copenhagen, Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using embeddings as representations of products is quite commonplace in\nrecommender systems, either by extracting the semantic embeddings of text\ndescriptions, user sessions, collaborative relationships, or product images. In\nthis paper, we present an approach to extract style embeddings for using in\nfashion recommender systems, with a special focus on style information such as\ntextures, prints, material, etc. The main issue of using such a type of\nembeddings is its high dimensionality. So, we propose feature reduction\nsolutions alongside the investigation of its influence in the overall task of\nrecommending products of the same style based on their main image. The feature\nreduction we propose allows for reducing the embedding vector from 600k\nfeatures to 512, leading to a memory reduction of 99.91\\% without critically\ncompromising the quality of the recommendations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 11:10:11 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Goncalves", "Diogo", ""], ["Liu", "Liweu", ""], ["Magalh\u00e3es", "Ana", ""]]}, {"id": "1908.10678", "submitter": "Yanshan Wang", "authors": "Krishna B. Soundararajan, Sunyang Fu, Luke A. Carlson, Rebecca A.\n  Smith, David S. Knopman, Hongfang Liu, Yanshan Wang", "title": "How Good is Artificial Intelligence at Automatically Answering Consumer\n  Questions Related to Alzheimer's Disease?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's Disease (AD) is the most common type of dementia, comprising\n60-80% of cases. There were an estimated 5.8 million Americans living with\nAlzheimer's dementia in 2019, and this number will almost double every 20\nyears. The total lifetime cost of care for someone with dementia is estimated\nto be $350,174 in 2018, 70% of which is associated with family-provided care.\nMost family caregivers face emotional, financial and physical difficulties. As\na medium to relieve this burden, online communities in social media websites\nsuch as Twitter, Reddit, and Yahoo! Answers provide potential venues for\ncaregivers to search relevant questions and answers, or post questions and seek\nanswers from other members. However, there are often a limited number of\nrelevant questions and responses to search from, and posted questions are\nrarely answered immediately. Due to recent advancement in Artificial\nIntelligence (AI), particularly Natural Language Processing (NLP), we propose\nto utilize AI to automatically generate answers to AD-related consumer\nquestions posted by caregivers and evaluate how good AI is at answering those\nquestions. To the best of our knowledge, this is the first study in the\nliterature applying and evaluating AI models designed to automatically answer\nconsumer questions related to AD.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 19:08:56 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 15:57:49 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Soundararajan", "Krishna B.", ""], ["Fu", "Sunyang", ""], ["Carlson", "Luke A.", ""], ["Smith", "Rebecca A.", ""], ["Knopman", "David S.", ""], ["Liu", "Hongfang", ""], ["Wang", "Yanshan", ""]]}, {"id": "1908.10679", "submitter": "Zhou Qin", "authors": "Ao Li, Zhou Qin, Runshi Liu, Yiqun Yang, Dong Li", "title": "Spam Review Detection with Graph Convolutional Networks", "comments": "Accepted at CIKM 2019", "journal-ref": null, "doi": "10.1145/3357384.3357820", "report-no": null, "categories": "cs.IR cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customers make a lot of reviews on online shopping websites every day, e.g.,\nAmazon and Taobao. Reviews affect the buying decisions of customers, meanwhile,\nattract lots of spammers aiming at misleading buyers. Xianyu, the largest\nsecond-hand goods app in China, suffering from spam reviews. The anti-spam\nsystem of Xianyu faces two major challenges: scalability of the data and\nadversarial actions taken by spammers. In this paper, we present our technical\nsolutions to address these challenges. We propose a large-scale anti-spam\nmethod based on graph convolutional networks (GCN) for detecting spam\nadvertisements at Xianyu, named GCN-based Anti-Spam (GAS) model. In this model,\na heterogeneous graph and a homogeneous graph are integrated to capture the\nlocal context and global context of a comment. Offline experiments show that\nthe proposed method is superior to our baseline model in which the information\nof reviews, features of users and items being reviewed are utilized.\nFurthermore, we deploy our system to process million-scale data daily at\nXianyu. The online performance also demonstrates the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 12:44:16 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Li", "Ao", ""], ["Qin", "Zhou", ""], ["Liu", "Runshi", ""], ["Yang", "Yiqun", ""], ["Li", "Dong", ""]]}, {"id": "1908.10725", "submitter": "Michael P. J. Camilleri Mr", "authors": "Michael P. J. Camilleri, Adrian Muscat, Victor Buttigieg, and Maria\n  Attard", "title": "VJA\\'G\\'G -- A Thick-Client Smart-Phone Journey Detection Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe $Vja\\dot{g}\\dot{g}$, a battery-aware journey\ndetection algorithm that executes on the mobile device. The algorithm can be\nembedded in the client app of the transport service provider or in a general\npurpose mobility data collector. The thick client setup allows the\ncustomer/participant to select which journeys are transferred to the server,\nkeeping customers in control of their personal data and encouraging user\nuptake. The algorithm is tested in the field and optimised for both accuracy in\nregistering complete journeys and battery power consumption. Typically the\nalgorithm can run for a full day without the need of recharging and more than\n88% of journeys are correctly detected from origin to destination, whilst 12%\nwould be missing part of the journey.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 13:53:26 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 18:19:12 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Camilleri", "Michael P. J.", ""], ["Muscat", "Adrian", ""], ["Buttigieg", "Victor", ""], ["Attard", "Maria", ""]]}, {"id": "1908.10784", "submitter": "Camille Roth", "authors": "Telmo Menezes and Camille Roth", "title": "Semantic Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaches to Natural language processing (NLP) may be classified along a\ndouble dichotomy open/opaque - strict/adaptive. The former axis relates to the\npossibility of inspecting the underlying processing rules, the latter to the\nuse of fixed or adaptive rules. We argue that many techniques fall into either\nthe open-strict or opaque-adaptive categories. Our contribution takes steps in\nthe open-adaptive direction, which we suggest is likely to provide key\ninstruments for interdisciplinary research. The central idea of our approach is\nthe Semantic Hypergraph (SH), a novel knowledge representation model that is\nintrinsically recursive and accommodates the natural hierarchical richness of\nnatural language. The SH model is hybrid in two senses. First, it attempts to\ncombine the strengths of ML and symbolic approaches. Second, it is a formal\nlanguage representation that reduces but tolerates ambiguity and structural\nvariability. We will see that SH enables simple yet powerful methods of pattern\ndetection, and features a good compromise for intelligibility both for humans\nand machines. It also provides a semantically deep starting point (in terms of\nexplicit meaning) for further algorithms to operate and collaborate on. We show\nhow modern NLP ML-based building blocks can be used in combination with a\nrandom forest classifier and a simple search tree to parse NL to SH, and that\nthis parser can achieve high precision in a diversity of text categories. We\ndefine a pattern language representable in SH itself, and a process to discover\nknowledge inference rules. We then illustrate the efficiency of the SH\nframework in a variety of tasks, including conjunction decomposition, open\ninformation extraction, concept taxonomy inference and co-reference resolution,\nand an applied example of claim and conflict analysis in a news corpus.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 15:39:02 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 14:26:53 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Menezes", "Telmo", ""], ["Roth", "Camille", ""]]}, {"id": "1908.11055", "submitter": "Yashar Deldjoo", "authors": "Luca Luciano Costanzo, Yashar Deldjoo, Maurizio Ferrari Dacrema,\n  Markus Schedl, Paolo Cremonesi", "title": "Towards Evaluating User Profiling Methods Based on Explicit Ratings on\n  Item Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to improve the accuracy of recommendations, many recommender systems\nnowadays use side information beyond the user rating matrix, such as item\ncontent. These systems build user profiles as estimates of users' interest on\ncontent (e.g., movie genre, director or cast) and then evaluate the performance\nof the recommender system as a whole e.g., by their ability to recommend\nrelevant and novel items to the target user. The user profile modelling stage,\nwhich is a key stage in content-driven RS is barely properly evaluated due to\nthe lack of publicly available datasets that contain user preferences on\ncontent features of items.\n  To raise awareness of this fact, we investigate differences between explicit\nuser preferences and implicit user profiles. We create a dataset of explicit\npreferences towards content features of movies, which we release publicly. We\nthen compare the collected explicit user feature preferences and implicit user\nprofiles built via state-of-the-art user profiling models. Our results show a\nmaximum average pairwise cosine similarity of 58.07\\% between the explicit\nfeature preferences and the implicit user profiles modelled by the best\ninvestigated profiling method and considering movies' genres only. For actors\nand directors, this maximum similarity is only 9.13\\% and 17.24\\%,\nrespectively. This low similarity between explicit and implicit preference\nmodels encourages a more in-depth study to investigate and improve this\nimportant user profile modelling step, which will eventually translate into\nbetter recommendations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 05:14:23 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Costanzo", "Luca Luciano", ""], ["Deldjoo", "Yashar", ""], ["Dacrema", "Maurizio Ferrari", ""], ["Schedl", "Markus", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "1908.11078", "submitter": "Wei Dong", "authors": "Wei Dong, Qinliang Su, Dinghan Shen and Changyou Chen", "title": "Document Hashing with Mixture-Prior Generative Models", "comments": "10 pages, 8 figures, to appear at EMNLP-IJCNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing is promising for large-scale information retrieval tasks thanks to\nthe efficiency of distance evaluation between binary codes. Generative hashing\nis often used to generate hashing codes in an unsupervised way. However,\nexisting generative hashing methods only considered the use of simple priors,\nlike Gaussian and Bernoulli priors, which limits these methods to further\nimprove their performance. In this paper, two mixture-prior generative models\nare proposed, under the objective to produce high-quality hashing codes for\ndocuments. Specifically, a Gaussian mixture prior is first imposed onto the\nvariational auto-encoder (VAE), followed by a separate step to cast the\ncontinuous latent representation of VAE into binary code. To avoid the\nperformance loss caused by the separate casting, a model using a Bernoulli\nmixture prior is further developed, in which an end-to-end training is admitted\nby resorting to the straight-through (ST) discrete gradient estimator.\nExperimental results on several benchmark datasets demonstrate that the\nproposed methods, especially the one using Bernoulli mixture priors,\nconsistently outperform existing ones by a substantial margin.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 07:29:28 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Dong", "Wei", ""], ["Su", "Qinliang", ""], ["Shen", "Dinghan", ""], ["Chen", "Changyou", ""]]}, {"id": "1908.11146", "submitter": "Gong Cheng", "authors": "Jinchi Chen, Xiaxia Wang, Gong Cheng, Evgeny Kharlamov, Yuzhong Qu", "title": "Towards More Usable Dataset Search: From Query Characterization to\n  Snippet Generation", "comments": "4 pages, The 28th ACM International Conference on Information and\n  Knowledge Management (CIKM 2019)", "journal-ref": null, "doi": "10.1145/3357384.3358096", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reusing published datasets on the Web is of great interest to researchers and\ndevelopers. Their data needs may be met by submitting queries to a dataset\nsearch engine to retrieve relevant datasets. In this ongoing work towards\ndeveloping a more usable dataset search engine, we characterize real data needs\nby annotating the semantics of 1,947 queries using a novel fine-grained scheme,\nto provide implications for enhancing dataset search. Based on the findings, we\npresent a query-centered framework for dataset search, and explore the\nimplementation of snippet generation and evaluate it with a preliminary user\nstudy.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 10:48:26 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Chen", "Jinchi", ""], ["Wang", "Xiaxia", ""], ["Cheng", "Gong", ""], ["Kharlamov", "Evgeny", ""], ["Qu", "Yuzhong", ""]]}, {"id": "1908.11216", "submitter": "Alexandre Garcia", "authors": "Alexandre Garcia, Pierre Colombo, Slim Essid, Florence d'Alch\\'e-Buc,\n  Chlo\\'e Clavel", "title": "From the Token to the Review: A Hierarchical Multimodal approach to\n  Opinion Mining", "comments": "Accepted to 2019 Conference on Empirical Methods in Natural Language\n  Processing (EMNLP) and 9th International Joint Conference on Natural Language\n  Processing (IJCNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of predicting fine grained user opinion based on spontaneous spoken\nlanguage is a key problem arising in the development of Computational Agents as\nwell as in the development of social network based opinion miners.\nUnfortunately, gathering reliable data on which a model can be trained is\nnotoriously difficult and existing works rely only on coarsely labeled\nopinions. In this work we aim at bridging the gap separating fine grained\nopinion models already developed for written language and coarse grained models\ndeveloped for spontaneous multimodal opinion mining. We take advantage of the\nimplicit hierarchical structure of opinions to build a joint fine and coarse\ngrained opinion model that exploits different views of the opinion expression.\nThe resulting model shares some properties with attention-based models and is\nshown to provide competitive results on a recently released multimodal fine\ngrained annotated corpus.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 13:34:50 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 11:07:23 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2019 08:29:00 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Garcia", "Alexandre", ""], ["Colombo", "Pierre", ""], ["Essid", "Slim", ""], ["d'Alch\u00e9-Buc", "Florence", ""], ["Clavel", "Chlo\u00e9", ""]]}, {"id": "1908.11302", "submitter": "Denis Newman-Griffis", "authors": "Denis Newman-Griffis, Eric Fosler-Lussier", "title": "HARE: a Flexible Highlighting Annotator for Ranking and Exploration", "comments": "EMNLP 2019 Systems Demonstration. Online version including\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration and analysis of potential data sources is a significant challenge\nin the application of NLP techniques to novel information domains. We describe\nHARE, a system for highlighting relevant information in document collections to\nsupport ranking and triage, which provides tools for post-processing and\nqualitative analysis for model development and tuning. We apply HARE to the use\ncase of narrative descriptions of mobility information in clinical data, and\ndemonstrate its utility in comparing candidate embedding features. We provide a\nweb-based interface for annotation visualization and document ranking, with a\nmodular backend to support interoperability with existing annotation tools. Our\nsystem is available online at https://github.com/OSU-slatelab/HARE.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 15:36:40 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Newman-Griffis", "Denis", ""], ["Fosler-Lussier", "Eric", ""]]}, {"id": "1908.11322", "submitter": "Qingyao Ai", "authors": "Qingyao Ai, Daniel N. Hill, S. V. N. Vishwanathan, W. Bruce Croft", "title": "A Zero Attention Model for Personalized Product Search", "comments": null, "journal-ref": null, "doi": "10.1145/3357384.3357980", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product search is one of the most popular methods for people to discover and\npurchase products on e-commerce websites. Because personal preferences often\nhave an important influence on the purchase decision of each customer, it is\nintuitive that personalization should be beneficial for product search engines.\nWhile synthetic experiments from previous studies show that purchase histories\nare useful for identifying the individual intent of each product search\nsession, the effect of personalization on product search in practice, however,\nremains mostly unknown. In this paper, we formulate the problem of personalized\nproduct search and conduct large-scale experiments with search logs sampled\nfrom a commercial e-commerce search engine. Results from our preliminary\nanalysis show that the potential of personalization depends on query\ncharacteristics, interactions between queries, and user purchase histories.\nBased on these observations, we propose a Zero Attention Model for product\nsearch that automatically determines when and how to personalize a user-query\npair via a novel attention mechanism. Empirical results on commercial product\nsearch logs show that the proposed model not only significantly outperforms\nstate-of-the-art personalized product retrieval models, but also provides\nimportant information on the potential of personalization in each product\nsearch session.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 16:12:49 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Ai", "Qingyao", ""], ["Hill", "Daniel N.", ""], ["Vishwanathan", "S. V. N.", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1908.11588", "submitter": "Chang Liu", "authors": "Chang Liu, Yi Dong, Han Yu, Zhiqi Shen, Zhanning Gao, Pan Wang,\n  Changgong Zhang, Peiran Ren, Xuansong Xie, Lizhen Cui, Chunyan Miao", "title": "Generating Persuasive Visual Storylines for Promotional Videos", "comments": "10 pages, accepted by The 28th ACM International Conference on\n  Information and Knowledge Management (CIKM)", "journal-ref": null, "doi": "10.1145/3357384.3357906", "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video contents have become a critical tool for promoting products in\nE-commerce. However, the lack of automatic promotional video generation\nsolutions makes large-scale video-based promotion campaigns infeasible. The\nfirst step of automatically producing promotional videos is to generate visual\nstorylines, which is to select the building block footage and place them in an\nappropriate order. This task is related to the subjective viewing experience.\nIt is hitherto performed by human experts and thus, hard to scale. To address\nthis problem, we propose WundtBackpack, an algorithmic approach to generate\nstorylines based on available visual materials, which can be video clips or\nimages. It consists of two main parts, 1) the Learnable Wundt Curve to evaluate\nthe perceived persuasiveness based on the stimulus intensity of a sequence of\nvisual materials, which only requires a small volume of data to train; and 2) a\nclustering-based backpacking algorithm to generate persuasive sequences of\nvisual materials while considering video length constraints. In this way, the\nproposed approach provides a dynamic structure to empower artificial\nintelligence (AI) to organize video footage in order to construct a sequence of\nvisual stimuli with persuasive power. Extensive real-world experiments show\nthat our approach achieves close to 10% higher perceived persuasiveness scores\nby human testers, and 12.5% higher expected revenue compared to the best\nperforming state-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 08:13:22 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Liu", "Chang", ""], ["Dong", "Yi", ""], ["Yu", "Han", ""], ["Shen", "Zhiqi", ""], ["Gao", "Zhanning", ""], ["Wang", "Pan", ""], ["Zhang", "Changgong", ""], ["Ren", "Peiran", ""], ["Xie", "Xuansong", ""], ["Cui", "Lizhen", ""], ["Miao", "Chunyan", ""]]}, {"id": "1908.11722", "submitter": "Preslav Nakov", "authors": "Dimitrina Zlatkova, Preslav Nakov, Ivan Koychev", "title": "Fact-Checking Meets Fauxtography: Verifying Claims About Images", "comments": "Claims about Images; Fauxtography; Fact-Checking; Veracity; Fake News", "journal-ref": "EMNLP-2019", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent explosion of false claims in social media and on the Web in\ngeneral has given rise to a lot of manual fact-checking initiatives.\nUnfortunately, the number of claims that need to be fact-checked is several\norders of magnitude larger than what humans can handle manually. Thus, there\nhas been a lot of research aiming at automating the process. Interestingly,\nprevious work has largely ignored the growing number of claims about images.\nThis is despite the fact that visual imagery is more influential than text and\nnaturally appears alongside fake news. Here we aim at bridging this gap. In\nparticular, we create a new dataset for this problem, and we explore a variety\nof features modeling the claim, the image, and the relationship between the\nclaim and the image. The evaluation results show sizable improvements over the\nbaseline. We release our dataset, hoping to enable further research on\nfact-checking claims about images.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 13:12:21 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Zlatkova", "Dimitrina", ""], ["Nakov", "Preslav", ""], ["Koychev", "Ivan", ""]]}, {"id": "1908.11733", "submitter": "Jie Zou", "authors": "Jie Zou and Evangelos Kanoulas", "title": "Learning to Ask: Question-based Sequential Bayesian Product Search", "comments": "This paper is accepted by CIKM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product search is generally recognized as the first and foremost stage of\nonline shopping and thus significant for users and retailers of e-commerce.\nMost of the traditional retrieval methods use some similarity functions to\nmatch the user's query and the document that describes a product, either\ndirectly or in a latent vector space. However, user queries are often too\ngeneral to capture the minute details of the specific product that a user is\nlooking for. In this paper, we propose a novel interactive method to\neffectively locate the best matching product. The method is based on the\nassumption that there is a set of candidate questions for each product to be\nasked. In this work, we instantiate this candidate set by making the hypothesis\nthat products can be discriminated by the entities that appear in the documents\nassociated with them. We propose a Question-based Sequential Bayesian Product\nSearch method, QSBPS, which directly queries users on the expected presence of\nentities in the relevant product documents. The method learns the product\nrelevance as well as the reward of the potential questions to be asked to the\nuser by being trained on the search history and purchase behavior of a specific\nuser together with that of other users. The experimental results show that the\nproposed method can greatly improve the performance of product search compared\nto the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 13:39:56 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Zou", "Jie", ""], ["Kanoulas", "Evangelos", ""]]}]