[{"id": "1307.0087", "submitter": "Fabrizio M.A. Lolli", "authors": "Fabrizio M.A. Lolli", "title": "Semantics and pragmatics in actual software applications and in web\n  search engines: exploring innovations", "comments": null, "journal-ref": "International Symposium on Language and Communication: Exploring\n  Novelties, vol.3, 955-962. IICS, 2013, Turkey", "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While new ways to use the Semantic Web are developed every week, which allow\nthe user to find information on web more accurately - for example in search\nengines - some sophisticated pragmatic tools are becoming more important - for\nexample in web interfaces known as Social Intelligence, or in the most famous\nSiri by Apple. The work aims to analyze whether and where we can identify the\nboundary between semantics and pragmatics in the software used by analyzed\nsystems. examining how the linguistic disciplines are fundamental in their\nprogress. Is it possible to assume that the tools of social intelligence have a\npragmatic approach to the questions of the user, or it is just a use of a very\nrich vocabulary, with the use of semantic tools?\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2013 10:40:59 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Lolli", "Fabrizio M. A.", ""]]}, {"id": "1307.0261", "submitter": "Bhavana Dalvi", "authors": "Bhavana Dalvi, William W. Cohen, and Jamie Callan", "title": "WebSets: Extracting Sets of Entities from the Web Using Unsupervised\n  Information Extraction", "comments": "10 pages; International Conference on Web Search and Data Mining 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a open-domain information extraction method for extracting\nconcept-instance pairs from an HTML corpus. Most earlier approaches to this\nproblem rely on combining clusters of distributionally similar terms and\nconcept-instance pairs obtained with Hearst patterns. In contrast, our method\nrelies on a novel approach for clustering terms found in HTML tables, and then\nassigning concept names to these clusters using Hearst patterns. The method can\nbe efficiently applied to a large corpus, and experimental results on several\ndatasets show that our method can accurately extract large numbers of\nconcept-instance pairs.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 02:49:08 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Dalvi", "Bhavana", ""], ["Cohen", "William W.", ""], ["Callan", "Jamie", ""]]}, {"id": "1307.0317", "submitter": "Jaka \\v{S}peh", "authors": "Jaka \\v{S}peh, Andrej Muhi\\v{c}, Jan Rupnik", "title": "Algorithms of the LDA model [REPORT]", "comments": "5 pages, 4 figures, report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We review three algorithms for Latent Dirichlet Allocation (LDA). Two of them\nare variational inference algorithms: Variational Bayesian inference and Online\nVariational Bayesian inference and one is Markov Chain Monte Carlo (MCMC)\nalgorithm -- Collapsed Gibbs sampling. We compare their time complexity and\nperformance. We find that online variational Bayesian inference is the fastest\nalgorithm and still returns reasonably good results.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 10:03:58 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["\u0160peh", "Jaka", ""], ["Muhi\u010d", "Andrej", ""], ["Rupnik", "Jan", ""]]}, {"id": "1307.0320", "submitter": "Wanling Gao", "authors": "Wanling Gao, Yuqing Zhu, Zhen Jia, Chunjie Luo, Lei Wang, Zhiguo Li,\n  Jianfeng Zhan, Yong Qi, Yongqiang He, Shiming Gong, Xiaona Li, Shujie Zhang,\n  and Bizhu Qiu", "title": "BigDataBench: a Big Data Benchmark Suite from Web Search Engines", "comments": "7 pages, 5 figures, The Third Workshop on Architectures and Systems\n  for Big Data(ASBD 2013) in conjunction with The 40th International Symposium\n  on Computer Architecture, May 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our joint research efforts on big data benchmarking with\nseveral industrial partners. Considering the complexity, diversity, workload\nchurns, and rapid evolution of big data systems, we take an incremental\napproach in big data benchmarking. For the first step, we pay attention to\nsearch engines, which are the most important domain in Internet services in\nterms of the number of page views and daily visitors. However, search engine\nservice providers treat data, applications, and web access logs as business\nconfidentiality, which prevents us from building benchmarks. To overcome those\ndifficulties, with several industry partners, we widely investigated the open\nsource solutions in search engines, and obtained the permission of using\nanonymous Web access logs. Moreover, with two years' great efforts, we created\na sematic search engine named ProfSearch (available from\nhttp://prof.ict.ac.cn). These efforts pave the path for our big data benchmark\nsuite from search engines---BigDataBench, which is released on the web page\n(http://prof.ict.ac.cn/BigDataBench). We report our detailed analysis of search\nengine workloads, and present our benchmarking methodology. An innovative data\ngeneration methodology and tool are proposed to generate scalable volumes of\nbig data from a small seed of real data, preserving semantics and locality of\ndata. Also, we preliminarily report two case studies using BigDataBench for\nboth system and architecture researches.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2013 10:27:48 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Gao", "Wanling", ""], ["Zhu", "Yuqing", ""], ["Jia", "Zhen", ""], ["Luo", "Chunjie", ""], ["Wang", "Lei", ""], ["Li", "Zhiguo", ""], ["Zhan", "Jianfeng", ""], ["Qi", "Yong", ""], ["He", "Yongqiang", ""], ["Gong", "Shiming", ""], ["Li", "Xiaona", ""], ["Zhang", "Shujie", ""], ["Qiu", "Bizhu", ""]]}, {"id": "1307.0846", "submitter": "Evgeni Tsivtsivadze", "authors": "Evgeni Tsivtsivadze and Tom Heskes", "title": "Semi-supervised Ranking Pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel sparse preference learning/ranking algorithm. Our\nalgorithm approximates the true utility function by a weighted sum of basis\nfunctions using the squared loss on pairs of data points, and is a\ngeneralization of the kernel matching pursuit method. It can operate both in a\nsupervised and a semi-supervised setting and allows efficient search for\nmultiple, near-optimal solutions. Furthermore, we describe the extension of the\nalgorithm suitable for combined ranking and regression tasks. In our\nexperiments we demonstrate that the proposed algorithm outperforms several\nstate-of-the-art learning methods when taking into account unlabeled data and\nperforms comparably in a supervised learning scenario, while providing sparser\nsolutions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 20:51:40 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Tsivtsivadze", "Evgeni", ""], ["Heskes", "Tom", ""]]}, {"id": "1307.1024", "submitter": "Abdelhakim Herrouz", "authors": "Abdelhakim Herrouz, Chabane Khentout and Mahieddine Djoudi", "title": "Overview of Web Content Mining Tools", "comments": "06 pages", "journal-ref": "The International Journal of Engineering And Science (IJES),\n  Vol.2, Issue 6, June 2013, pp. 106-110, 2013", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the Web has become one of the most widespread platforms for\ninformation change and retrieval. As it becomes easier to publish documents, as\nthe number of users, and thus publishers, increases and as the number of\ndocuments grows, searching for information is turning into a cumbersome and\ntime-consuming operation. Due to heterogeneity and unstructured nature of the\ndata available on the WWW, Web mining uses various data mining techniques to\ndiscover useful knowledge from Web hyperlinks, page content and usage log. The\nmain uses of web content mining are to gather, categorize, organize and provide\nthe best possible information available on the Web to the user requesting the\ninformation. The mining tools are imperative to scanning the many HTML\ndocuments, images, and text. Then, the result is used by the search engines. In\nthis paper, we first introduce the concepts related to web mining; we then\npresent an overview of different Web Content Mining tools. We conclude by\npresenting a comparative table of these tools based on some pertinent criteria.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2013 19:57:29 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Herrouz", "Abdelhakim", ""], ["Khentout", "Chabane", ""], ["Djoudi", "Mahieddine", ""]]}, {"id": "1307.1179", "submitter": "Andrew Trotman", "authors": "Andrew Trotman, Jinglan Zhang", "title": "Future Web Growth and its Consequences for Web Search Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction: Before embarking on the design of any computer system it is\nfirst necessary to assess the magnitude of the problem. In the case of a web\nsearch engine this assessment amounts to determining the current size of the\nweb, the growth rate of the web, and the quantity of computing resource\nnecessary to search it, and projecting the historical growth of this into the\nfuture. Method: The over 20 year history of the web makes it possible to make\nshort-term projections on future growth. The longer history of hard disk drives\n(and smart phone memory card) makes it possible to make short-term hardware\nprojections. Analysis: Historical data on Internet uptake and hardware growth\nis extrapolated. Results: It is predicted that within a decade the storage\ncapacity of a single hard drive will exceed the size of the index of the web at\nthat time. Within another decade it will be possible to store the entire\nsearchable text on the same hard drive. Within another decade the entire\nsearchable web (including images) will also fit. Conclusion: This result raises\nquestions about the future architecture of search engines. Several new models\nare proposed. In one model the user's computer is an active part of the\ndistributed search architecture. They search a pre-loaded snapshot (back-file)\nof the web on their local device which frees up the online data centre for\nsearching just the difference between the snapshot and the current time.\nAdvantageously this also makes it possible to search when the user is\ndisconnected from the Internet. In another model all changes to all files are\nbroadcast to all users (forming a star-like network) and no data centre is\nneeded.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 00:38:41 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Trotman", "Andrew", ""], ["Zhang", "Jinglan", ""]]}, {"id": "1307.1289", "submitter": "Miguel Angel Veganzones", "authors": "Miguel Angel Veganzones (GIPSA), Mihai Datcu (DLR), Manuel Gra\\~na\n  (GIC)", "title": "Further results on dissimilarity spaces for hyperspectral images RF-CBIR", "comments": "In Pattern Recognition Letters (2013)", "journal-ref": "Pattern Recognition Letters 34, 14 (2013) 1659-1668", "doi": "10.1016/j.patrec.2013.05.025", "report-no": "veganzones_PRL2013", "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-Based Image Retrieval (CBIR) systems are powerful search tools in\nimage databases that have been little applied to hyperspectral images.\nRelevance feedback (RF) is an iterative process that uses machine learning\ntechniques and user's feedback to improve the CBIR systems performance. We\npursued to expand previous research in hyperspectral CBIR systems built on\ndissimilarity functions defined either on spectral and spatial features\nextracted by spectral unmixing techniques, or on dictionaries extracted by\ndictionary-based compressors. These dissimilarity functions were not suitable\nfor direct application in common machine learning techniques. We propose to use\na RF general approach based on dissimilarity spaces which is more appropriate\nfor the application of machine learning algorithms to the hyperspectral\nRF-CBIR. We validate the proposed RF method for hyperspectral CBIR systems over\na real hyperspectral dataset.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2013 11:58:04 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Veganzones", "Miguel Angel", "", "GIPSA"], ["Datcu", "Mihai", "", "DLR"], ["Gra\u00f1a", "Manuel", "", "GIC"]]}, {"id": "1307.1543", "submitter": "Christian von der Weth", "authors": "Christian von der Weth and Manfred Hauswirth", "title": "Finding Information Through Integrated Ad-Hoc Socializing in the Virtual\n  and Physical World", "comments": null, "journal-ref": null, "doi": "10.1109/WI-IAT.2013.6", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the services of sophisticated search engines like Google, there are a\nnumber of interesting information sources which are useful but largely\ninaccessible to current Web users. These information sources are often ad-hoc,\nlocation-specific and only useful for users over short periods of time, or\nrelate to tacit knowledge of users or implicit knowledge in crowds. The\nsolution presented in this paper addresses these problems by introducing an\nintegrated concept of \"location\" and \"presence\" across the physical and virtual\nworlds enabling ad-hoc socializing of users interested in, or looking for\nsimilar information. While the definition of presence in the physical world is\nstraightforward - through a spatial location and vicinity at a certain point in\ntime - their definitions in the virtual world are neither obvious nor trivial.\nBased on a detailed analysis we provide an integrated spatial model spanning\nboth worlds which enables us to define presence of users in a unified way. This\nintegrated model allows us to enable ad-hoc socializing of users browsing the\nWeb with users in the physical world specific to their joint information needs\nand allows us to unlock the untapped information sources mentioned above. We\ndescribe a proof-of-concept implementation of our model and provide an\nempirical analysis based on real-world experiments.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 08:15:28 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["von der Weth", "Christian", ""], ["Hauswirth", "Manfred", ""]]}, {"id": "1307.1561", "submitter": "Vimina E R", "authors": "E. R. Vimina, K. Poulose Jacob", "title": "A Sub-block Based Image Retrieval Using Modified Integrated Region\n  Matching", "comments": "7 pages", "journal-ref": "International Journal of Computer Science Issues, Vol.10, Issue\n  1,No 2, January 2013, pp. 686-692", "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a content based image retrieval (CBIR) system using the\nlocal colour and texture features of selected image sub-blocks and global\ncolour and shape features of the image. The image sub-blocks are roughly\nidentified by segmenting the image into partitions of different configuration,\nfinding the edge density in each partition using edge thresholding followed by\nmorphological dilation. The colour and texture features of the identified\nregions are computed from the histograms of the quantized HSV colour space and\nGray Level Co- occurrence Matrix (GLCM) respectively. The colour and texture\nfeature vectors is computed for each region. The shape features are computed\nfrom the Edge Histogram Descriptor (EHD). A modified Integrated Region Matching\n(IRM) algorithm is used for finding the minimum distance between the sub-blocks\nof the query and target image. Experimental results show that the proposed\nmethod provides better retrieving result than retrieval using some of the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 09:29:46 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Vimina", "E. R.", ""], ["Jacob", "K. Poulose", ""]]}, {"id": "1307.1718", "submitter": "Kyle Williams", "authors": "Pucktada Treeratpituk, Madian Khabsa, C. Lee Giles", "title": "Graph-based Approach to Automatic Taxonomy Generation (GraBTax)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel graph-based approach for constructing concept hierarchy\nfrom a large text corpus. Our algorithm, GraBTax, incorporates both statistical\nco-occurrences and lexical similarity in optimizing the structure of the\ntaxonomy. To automatically generate topic-dependent taxonomies from a large\ntext corpus, GraBTax first extracts topical terms and their relationships from\nthe corpus. The algorithm then constructs a weighted graph representing topics\nand their associations. A graph partitioning algorithm is then used to\nrecursively partition the topic graph into a taxonomy. For evaluation, we apply\nGraBTax to articles, primarily computer science, in the CiteSeerX digital\nlibrary and search engine. The quality of the resulting concept hierarchy is\nassessed by both human judges and comparison with Wikipedia categories.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2013 21:05:20 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2014 20:50:22 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Treeratpituk", "Pucktada", ""], ["Khabsa", "Madian", ""], ["Giles", "C. Lee", ""]]}, {"id": "1307.2015", "submitter": "Christos Tryfonopoulos Dr.", "authors": "Lefteris Zervakis, Christos Tryfonopoulos, Antonios\n  Papadakis-Pesaresi, Manolis Koubarakis, Spiros Skiadopoulos", "title": "Full-text Support for Publish/Subscribe Ontology Systems", "comments": "ESWC 2012 Demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We envision a publish/subscribe ontology system that is able to index\nmillions of user subscriptions and filter them against ontology data that\narrive in a streaming fashion. In this work, we propose a SPARQL extension\nappropriate for a publish/subscribe setting; our extension builds on the\nnatural semantic graph matching of the language and supports the creation of\nfull-text subscriptions. Subsequently, we propose a main-memory subscription\nindexing algorithm which performs both semantic and full-text matching at low\ncomplexity and minimal filtering time. Thus, when ontology data are published\nmatching subscriptions are identified and notifications are forwarded to users.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2013 10:12:01 GMT"}], "update_date": "2013-07-09", "authors_parsed": [["Zervakis", "Lefteris", ""], ["Tryfonopoulos", "Christos", ""], ["Papadakis-Pesaresi", "Antonios", ""], ["Koubarakis", "Manolis", ""], ["Skiadopoulos", "Spiros", ""]]}, {"id": "1307.2669", "submitter": "Haoyang (Hubert) Duan", "authors": "Hubert Haoyang Duan, Vladimir Pestov, and Varun Singla", "title": "Text Categorization via Similarity Search: An Efficient and Effective\n  Novel Algorithm", "comments": "12 pages, 5 tables, accepted for the 6th International Conference on\n  Similarity Search and Applications (SISAP 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a supervised learning algorithm for text categorization which has\nbrought the team of authors the 2nd place in the text categorization division\nof the 2012 Cybersecurity Data Mining Competition (CDMC'2012) and a 3rd prize\noverall. The algorithm is quite different from existing approaches in that it\nis based on similarity search in the metric space of measure distributions on\nthe dictionary. At the preprocessing stage, given a labeled learning sample of\ntexts, we associate to every class label (document category) a point in the\nspace of question. Unlike it is usual in clustering, this point is not a\ncentroid of the category but rather an outlier, a uniform measure distribution\non a selection of domain-specific words. At the execution stage, an unlabeled\ntext is assigned a text category as defined by the closest labeled neighbour to\nthe point representing the frequency distribution of the words in the text. The\nalgorithm is both effective and efficient, as further confirmed by experiments\non the Reuters 21578 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2013 04:41:19 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Duan", "Hubert Haoyang", ""], ["Pestov", "Vladimir", ""], ["Singla", "Varun", ""]]}, {"id": "1307.2982", "submitter": "Mohammad Norouzi", "authors": "Mohammad Norouzi, Ali Punjani, David J. Fleet", "title": "Fast Exact Search in Hamming Space with Multi-Index Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in representing image data and feature descriptors\nusing compact binary codes for fast near neighbor search. Although binary codes\nare motivated by their use as direct indices (addresses) into a hash table,\ncodes longer than 32 bits are not being used as such, as it was thought to be\nineffective. We introduce a rigorous way to build multiple hash tables on\nbinary code substrings that enables exact k-nearest neighbor search in Hamming\nspace. The approach is storage efficient and straightforward to implement.\nTheoretical analysis shows that the algorithm exhibits sub-linear run-time\nbehavior for uniformly distributed codes. Empirical results show dramatic\nspeedups over a linear scan baseline for datasets of up to one billion codes of\n64, 128, or 256 bits.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 05:52:21 GMT"}, {"version": "v2", "created": "Sun, 15 Dec 2013 02:36:21 GMT"}, {"version": "v3", "created": "Fri, 25 Apr 2014 01:31:55 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Norouzi", "Mohammad", ""], ["Punjani", "Ali", ""], ["Fleet", "David J.", ""]]}, {"id": "1307.3284", "submitter": "Shuai Yuan", "authors": "Shuai Yuan, Jun Wang", "title": "Sequential Selection of Correlated Ads by POMDPs", "comments": null, "journal-ref": "Proceedings of the ACM CIKM '12. 515-524", "doi": "10.1145/2396761.2396828", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online advertising has become a key source of revenue for both web search\nengines and online publishers. For them, the ability of allocating right ads to\nright webpages is critical because any mismatched ads would not only harm web\nusers' satisfactions but also lower the ad income. In this paper, we study how\nonline publishers could optimally select ads to maximize their ad incomes over\ntime. The conventional offline, content-based matching between webpages and ads\nis a fine start but cannot solve the problem completely because good matching\ndoes not necessarily lead to good payoff. Moreover, with the limited display\nimpressions, we need to balance the need of selecting ads to learn true ad\npayoffs (exploration) with that of allocating ads to generate high immediate\npayoffs based on the current belief (exploitation). In this paper, we address\nthe problem by employing Partially observable Markov decision processes\n(POMDPs) and discuss how to utilize the correlation of ads to improve the\nefficiency of the exploration and increase ad incomes in a long run. Our\nmathematical derivation shows that the belief states of correlated ads can be\nnaturally updated using a formula similar to collaborative filtering. To test\nour model, a real world ad dataset from a major search engine is collected and\ncategorized. Experimenting over the data, we provide an analyse of the effect\nof the underlying parameters, and demonstrate that our algorithms significantly\noutperform other strong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2013 22:20:32 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Yuan", "Shuai", ""], ["Wang", "Jun", ""]]}, {"id": "1307.3336", "submitter": "Arti Buche Ms", "authors": "Arti Buche, Dr. M. B. Chandak and Akshay Zadgaonkar", "title": "Opinion Mining and Analysis: A survey", "comments": "10 pages", "journal-ref": "IJNLC Vol. 2, No.3, June 2013", "doi": "10.5121/ijnlc.2013.2304", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The current research is focusing on the area of Opinion Mining also called as\nsentiment analysis due to sheer volume of opinion rich web resources such as\ndiscussion forums, review sites and blogs are available in digital form. One\nimportant problem in sentiment analysis of product reviews is to produce\nsummary of opinions based on product features. We have surveyed and analyzed in\nthis paper, various techniques that have been developed for the key tasks of\nopinion mining. We have provided an overall picture of what is involved in\ndeveloping a software system for opinion mining on the basis of our survey and\nanalysis.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 06:20:36 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Buche", "Arti", ""], ["Chandak", "Dr. M. B.", ""], ["Zadgaonkar", "Akshay", ""]]}, {"id": "1307.3573", "submitter": "Shuai Yuan", "authors": "Shuai Yuan, Jun Wang, Maurice van der Meer", "title": "Adaptive Keywords Extraction with Contextual Bandits for Advertising on\n  Parked Domains", "comments": "To appear in the proceedings of the IATP '13 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain name registrars and URL shortener service providers place\nadvertisements on the parked domains (Internet domain names which are not in\nservice) in order to generate profits. As the web contents have been removed,\nit is critical to make sure the displayed ads are directly related to the\nintents of the visitors who have been directed to the parked domains. Because\nof the missing contents in these domains, it is non-trivial to generate the\nkeywords to describe the previous contents and therefore the users intents. In\nthis paper we discuss the adaptive keywords extraction problem and introduce an\nalgorithm based on the BM25F term weighting and linear multi-armed bandits. We\nbuilt a prototype over a production domain registration system and evaluated it\nusing crowdsourcing in multiple iterations. The prototype is compared with\nother popular methods and is shown to be more effective.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2013 20:32:42 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Yuan", "Shuai", ""], ["Wang", "Jun", ""], ["van der Meer", "Maurice", ""]]}, {"id": "1307.3673", "submitter": "Omar Alonso", "authors": "Alexandros Ntoulas, Omar Alonso, Vasilis Kandylas", "title": "A Data Management Approach for Dataset Selection Using Human Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the number of applications that use machine learning algorithms increases,\nthe need for labeled data useful for training such algorithms intensifies.\n  Getting labels typically involves employing humans to do the annotation,\nwhich directly translates to training and working costs. Crowdsourcing\nplatforms have made labeling cheaper and faster, but they still involve\nsignificant costs, especially for the cases where the potential set of\ncandidate data to be labeled is large. In this paper we describe a methodology\nand a prototype system aiming at addressing this challenge for Web-scale\nproblems in an industrial setting. We discuss ideas on how to efficiently\nselect the data to use for training of machine learning algorithms in an\nattempt to reduce cost. We show results achieving good performance with reduced\ncost by carefully selecting which instances to label. Our proposed algorithm is\npresented as part of a framework for managing and generating training datasets,\nwhich includes, among other components, a human computation element.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2013 19:29:33 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Ntoulas", "Alexandros", ""], ["Alonso", "Omar", ""], ["Kandylas", "Vasilis", ""]]}, {"id": "1307.3855", "submitter": "Yue Shi", "authors": "Yue Shi, Alexandros Karatzoglou, Linas Baltrunas, Martha Larson, Alan\n  Hanjalic", "title": "GAPfm: Optimal Top-N Recommendations for Graded Relevance Domains", "comments": "Manuscript under review. A short version of this manuscript has been\n  accepted at CIKM 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are frequently used in domains in which users express\ntheir preferences in the form of graded judgments, such as ratings. If accurate\ntop-N recommendation lists are to be produced for such graded relevance\ndomains, it is critical to generate a ranked list of recommended items directly\nrather than predicting ratings. Current techniques choose one of two\nsub-optimal approaches: either they optimize for a binary metric such as\nAverage Precision, which discards information on relevance grades, or they\noptimize for Normalized Discounted Cumulative Gain (NDCG), which ignores the\ndependence of an item's contribution on the relevance of more highly ranked\nitems.\n  In this paper, we address the shortcomings of existing approaches by\nproposing the Graded Average Precision factor model (GAPfm), a latent factor\nmodel that is particularly suited to the problem of top-N recommendation in\ndomains with graded relevance data. The model optimizes for Graded Average\nPrecision, a metric that has been proposed recently for assessing the quality\nof ranked results list for graded relevance. GAPfm learns a latent factor model\nby directly optimizing a smoothed approximation of GAP. GAPfm's advantages are\ntwofold: it maintains full information about graded relevance and also\naddresses the limitations of models that optimize NDCG. Experimental results\nshow that GAPfm achieves substantial improvements on the top-N recommendation\ntask, compared to several state-of-the-art approaches. In order to ensure that\nGAPfm is able to scale to very large data sets, we propose a fast learning\nalgorithm that uses an adaptive item selection strategy. A final experiment\nshows that GAPfm is useful not only for generating recommendation lists, but\nalso for ranking a given list of rated items.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 08:55:11 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Shi", "Yue", ""], ["Karatzoglou", "Alexandros", ""], ["Baltrunas", "Linas", ""], ["Larson", "Martha", ""], ["Hanjalic", "Alan", ""]]}, {"id": "1307.4063", "submitter": "Hany SalahEldeen", "authors": "Hany M. SalahEldeen and Michael L. Nelson", "title": "Reading the Correct History? Modeling Temporal Intention in Resource\n  Sharing", "comments": "JCDL 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The web is trapped in the \"perpetual now\", and when users traverse from page\nto page, they are seeing the state of the web resource (i.e., the page) as it\nexists at the time of the click and not necessarily at the time when the link\nwas made. Thus, a temporal discrepancy can arise between the resource at the\ntime the page author created a link to it and the time when a reader follows\nthe link. This is especially important in the context of social media: the ease\nof sharing links in a tweet or Facebook post allows many people to author web\ncontent, but the space constraints combined with poor awareness by authors\noften prevents sufficient context from being generated to determine the intent\nof the post. If the links are clicked as soon as they are shared, the temporal\ndistance between sharing and clicking is so small that there is little to no\ndifference in content. However, not all clicks occur immediately, and a delay\nof days or even hours can result in reading something other than what the\nauthor intended. We introduce the concept of a user's temporal intention upon\npublishing a link in social media. We investigate the features that could be\nextracted from the post, the linked resource, and the patterns of social\ndissemination to model this user intention. Finally, we analyze the historical\nintegrity of the shared resources in social media across time. In other words,\nhow much is the knowledge of the author's intent beneficial in maintaining the\nconsistency of the story being told through social posts and in enriching the\narchived content coverage and depth of vulnerable resources?\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2013 19:28:10 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["SalahEldeen", "Hany M.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1307.4518", "submitter": "Zeyu Zhang", "authors": "Jian Li and Zeyu Zhang", "title": "Ranking with Diverse Intents and Correlated Contents", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following document ranking problem: We have a collection of\ndocuments, each containing some topics (e.g. sports, politics, economics). We\nalso have a set of users with diverse interests. Assume that user u is\ninterested in a subset I_u of topics. Each user u is also associated with a\npositive integer K_u, which indicates that u can be satisfied by any K_u topics\nin I_u. Each document s contains information for a subset C_s of topics. The\nobjective is to pick one document at a time such that the average satisfying\ntime is minimized, where a user's satisfying time is the first time that at\nleast K_u topics in I_u are covered in the documents selected so far. Our main\nresult is an O({\\rho})-approximation algorithm for the problem, where {\\rho} is\nthe algorithmic integrality gap of the linear programming relaxation of the set\ncover instance defined by the documents and topics. This result generalizes the\nconstant approximations for generalized min-sum set cover and ranking with\nunrelated intents and the logarithmic approximation for the problem of ranking\nwith submodular valuations (when the submodular function is the coverage\nfunction), and can be seen as an interpolation between these results. We\nfurther extend our model to the case when each user may interest in more than\none sets of topics and when the user's valuation function is XOS, and obtain\nsimilar results for these models.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2013 06:53:29 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2013 03:10:34 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Li", "Jian", ""], ["Zhang", "Zeyu", ""]]}, {"id": "1307.4879", "submitter": "Carlos Castillo", "authors": "Carlos Castillo, Gianmarco De Francisci Morales, Marcelo Mendoza,\n  Nasir Khan", "title": "Says who? Automatic Text-Based Content Analysis of Television News", "comments": "In the 2013 workshop on Mining Unstructured Big Data Using Natural\n  Language Processing, co-located with CIKM 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We perform an automatic analysis of television news programs, based on the\nclosed captions that accompany them. Specifically, we collect all the news\nbroadcasted in over 140 television channels in the US during a period of six\nmonths. We start by segmenting, processing, and annotating the closed captions\nautomatically. Next, we focus on the analysis of their linguistic style and on\nmentions of people using NLP methods. We present a series of key insights about\nnews providers, people in the news, and we discuss the biases that can be\nuncovered by automatic means. These insights are contrasted by looking at the\ndata from multiple points of view, including qualitative assessment.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 09:37:45 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2013 08:12:30 GMT"}], "update_date": "2013-08-30", "authors_parsed": [["Castillo", "Carlos", ""], ["Morales", "Gianmarco De Francisci", ""], ["Mendoza", "Marcelo", ""], ["Khan", "Nasir", ""]]}, {"id": "1307.4980", "submitter": "Bowei Chen", "authors": "Bowei Chen and Jun Wang and Ingemar J. Cox and Mohan S. Kankanhalli", "title": "Multi-keyword multi-click advertisement option contracts for sponsored\n  search", "comments": "Chen, Bowei and Wang, Jun and Cox, Ingemar J. and Kankanhalli, Mohan\n  S. (2015) Multi-keyword multi-click advertisement option contracts for\n  sponsored search. ACM Transactions on Intelligent Systems and Technology, 7\n  (1). pp. 1-29. ISSN: 2157-6904", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sponsored search, advertisement (abbreviated ad) slots are usually sold by\na search engine to an advertiser through an auction mechanism in which\nadvertisers bid on keywords. In theory, auction mechanisms have many desirable\neconomic properties. However, keyword auctions have a number of limitations\nincluding: the uncertainty in payment prices for advertisers; the volatility in\nthe search engine's revenue; and the weak loyalty between advertiser and search\nengine. In this paper we propose a special ad option that alleviates these\nproblems. In our proposal, an advertiser can purchase an option from a search\nengine in advance by paying an upfront fee, known as the option price. He then\nhas the right, but no obligation, to purchase among the pre-specified set of\nkeywords at the fixed cost-per-clicks (CPCs) for a specified number of clicks\nin a specified period of time. The proposed option is closely related to a\nspecial exotic option in finance that contains multiple underlying assets\n(multi-keyword) and is also multi-exercisable (multi-click). This novel\nstructure has many benefits: advertisers can have reduced uncertainty in\nadvertising; the search engine can improve the advertisers' loyalty as well as\nobtain a stable and increased expected revenue over time. Since the proposed ad\noption can be implemented in conjunction with the existing keyword auctions,\nthe option price and corresponding fixed CPCs must be set such that there is no\narbitrage between the two markets. Option pricing methods are discussed and our\nexperimental results validate the development. Compared to keyword auctions, a\nsearch engine can have an increased expected revenue by selling an ad option.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2013 15:27:12 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2013 17:42:59 GMT"}, {"version": "v3", "created": "Sun, 19 Jan 2014 20:48:33 GMT"}, {"version": "v4", "created": "Tue, 21 Jan 2014 08:27:34 GMT"}, {"version": "v5", "created": "Sat, 19 Apr 2014 14:04:01 GMT"}, {"version": "v6", "created": "Mon, 16 Mar 2015 22:52:01 GMT"}, {"version": "v7", "created": "Wed, 9 Dec 2015 23:23:46 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Chen", "Bowei", ""], ["Wang", "Jun", ""], ["Cox", "Ingemar J.", ""], ["Kankanhalli", "Mohan S.", ""]]}, {"id": "1307.5336", "submitter": "Pekka Malo", "authors": "Pekka Malo, Ankur Sinha, Pyry Takala, Pekka Korhonen, Jyrki Wallenius", "title": "Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts", "comments": "To be published in Journal of the American Society for Information\n  Science and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of robo-readers to analyze news texts is an emerging technology trend\nin computational finance. In recent research, a substantial effort has been\ninvested to develop sophisticated financial polarity-lexicons that can be used\nto investigate how financial sentiments relate to future company performance.\nHowever, based on experience from other fields, where sentiment analysis is\ncommonly applied, it is well-known that the overall semantic orientation of a\nsentence may differ from the prior polarity of individual words. The objective\nof this article is to investigate how semantic orientations can be better\ndetected in financial and economic news by accommodating the overall\nphrase-structure information and domain-specific use of language. Our three\nmain contributions are: (1) establishment of a human-annotated finance\nphrase-bank, which can be used as benchmark for training and evaluating\nalternative models; (2) presentation of a technique to enhance financial\nlexicons with attributes that help to identify expected direction of events\nthat affect overall sentiment; (3) development of a linearized phrase-structure\nmodel for detecting contextual semantic orientations in financial and economic\nnews texts. The relevance of the newly added lexicon features and the benefit\nof using the proposed learning-algorithm are demonstrated in a comparative\nstudy against previously used general sentiment models as well as the popular\nword frequency models used in recent financial studies. The proposed framework\nis parsimonious and avoids the explosion in feature-space caused by the use of\nconventional n-gram features.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2013 20:49:06 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2013 19:56:38 GMT"}], "update_date": "2013-07-24", "authors_parsed": [["Malo", "Pekka", ""], ["Sinha", "Ankur", ""], ["Takala", "Pyry", ""], ["Korhonen", "Pekka", ""], ["Wallenius", "Jyrki", ""]]}, {"id": "1307.6080", "submitter": "Liudmila Ostroumova", "authors": "Damien Lefortier, Liudmila Ostroumova, Egor Samosvat, Pavel Serdyukov", "title": "Timely crawling of high-quality ephemeral new content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, more and more people use the Web as their primary source of\nup-to-date information. In this context, fast crawling and indexing of newly\ncreated Web pages has become crucial for search engines, especially because\nuser traffic to a significant fraction of these new pages (like news, blog and\nforum posts) grows really quickly right after they appear, but lasts only for\nseveral days.\n  In this paper, we study the problem of timely finding and crawling of such\nephemeral new pages (in terms of user interest). Traditional crawling policies\ndo not give any particular priority to such pages and may thus crawl them not\nquickly enough, and even crawl already obsolete content. We thus propose a new\nmetric, well thought out for this task, which takes into account the decrease\nof user interest for ephemeral pages over time.\n  We show that most ephemeral new pages can be found at a relatively small set\nof content sources and present a procedure for finding such a set. Our idea is\nto periodically recrawl content sources and crawl newly created pages linked\nfrom them, focusing on high-quality (in terms of user interest) content. One of\nthe main difficulties here is to divide resources between these two activities\nin an efficient way. We find the adaptive balance between crawls and recrawls\nby maximizing the proposed metric. Further, we incorporate search engine click\nlogs to give our crawler an insight about the current user demands. Efficiency\nof our approach is finally demonstrated experimentally on real-world data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2013 13:52:14 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2013 12:00:56 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Lefortier", "Damien", ""], ["Ostroumova", "Liudmila", ""], ["Samosvat", "Egor", ""], ["Serdyukov", "Pavel", ""]]}, {"id": "1307.6422", "submitter": "Mauro Gaio", "authors": "Van Tien Nguyen (LIUPPA), Christian Sallaberry (LIUPPA), Mauro Gaio\n  (LIUPPA)", "title": "Mesure de la similarit\\'e entre termes et labels de concepts\n  ontologiques", "comments": null, "journal-ref": "CORIA 2013, Neufch\\^atel : Suisse (2013)", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this paper a method for measuring the similarity between\nontological concepts and terms. Our metric can take into account not only the\ncommon words of two strings to compare but also other features such as the\nposition of the words in these strings, or the number of deletion, insertion or\nreplacement of words required for the construction of one of the two strings\nfrom each other. The proposed method was then used to determine the ontological\nconcepts which are equivalent to the terms that qualify toponymes. It aims to\nfind the topographical type of the toponyme.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2013 13:59:48 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Nguyen", "Van Tien", "", "LIUPPA"], ["Sallaberry", "Christian", "", "LIUPPA"], ["Gaio", "Mauro", "", "LIUPPA"]]}, {"id": "1307.6789", "submitter": "Gonzalo Navarro", "authors": "Gonzalo Navarro and Yakov Nekrich", "title": "Optimal Top-k Document Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathcal{D}$ be a collection of $D$ documents, which are strings over an\nalphabet of size $\\sigma$, of total length $n$. We describe a data structure\nthat uses linear space and and reports $k$ most relevant documents that contain\na query pattern $P$, which is a string of length $p$, in time $O(p/\\log_\\sigma\nn+k)$, which is optimal in the RAM model in the general case where $\\lg D =\n\\Theta(\\log n)$, and involves a novel RAM-optimal suffix tree search. Our\nconstruction supports an ample set of important relevance measures... [clip]\n  When $\\lg D = o(\\log n)$, we show how to reduce the space of the data\nstructure from $O(n\\log n)$ to $O(n(\\log\\sigma+\\log D+\\log\\log n))$ bits...\n[clip]\n  We also consider the dynamic scenario, where documents can be inserted and\ndeleted from the collection. We obtain linear space and query time\n$O(p(\\log\\log n)^2/\\log_\\sigma n+\\log n + k\\log\\log k)$, whereas insertions and\ndeletions require $O(\\log^{1+\\epsilon} n)$ time per symbol, for any constant\n$\\epsilon>0$.\n  Finally, we consider an extended static scenario where an extra parameter\n$par(P,d)$ is defined, and the query must retrieve only documents $d$ such that\n$par(P,d)\\in [\\tau_1,\\tau_2]$, where this range is specified at query time. We\nsolve these queries using linear space and $O(p/\\log_\\sigma n +\n\\log^{1+\\epsilon} n + k\\log^\\epsilon n)$ time, for any constant $\\epsilon>0$.\n  Our technique is to translate these top-$k$ problems into multidimensional\ngeometric search problems. As an additional bonus, we describe some\nimprovements to those problems.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2013 15:26:38 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2013 23:59:56 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Navarro", "Gonzalo", ""], ["Nekrich", "Yakov", ""]]}, {"id": "1307.6937", "submitter": "Rosy Madaan", "authors": "Renu Mudgal, Rosy Madaan, A.K.Sharma, Ashutosh Dixit", "title": "A Novel Architecture For Question Classification Based Indexing Scheme\n  For Efficient Question Answering", "comments": "International Journal of Computer Engineering and Applications,\n  April-June 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering system can be seen as the next step in information\nretrieval, allowing users to pose question in natural language and receive\ncompact answers. For the Question answering system to be successful, research\nhas shown that the correct classification of question with respect to the\nexpected answer type is requisite. We propose a novel architecture for question\nclassification and searching in the index, maintained on the basis of expected\nanswer types, for efficient question answering. The system uses the criteria\nfor Answer Relevance Score for finding the relevance of each answer returned by\nthe system. On analysis of the proposed system, it has been found that the\nsystem has shown promising results than the existing systems based on question\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2013 06:57:37 GMT"}], "update_date": "2013-07-29", "authors_parsed": [["Mudgal", "Renu", ""], ["Madaan", "Rosy", ""], ["Sharma", "A. K.", ""], ["Dixit", "Ashutosh", ""]]}, {"id": "1307.7291", "submitter": "Denzil Correa", "authors": "Denzil Correa, Ashish Sureka", "title": "Fit or Unfit : Analysis and Prediction of 'Closed Questions' on Stack\n  Overflow", "comments": "13 pages, 14 figures, 10 tables, version 1.0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Stack Overflow is widely regarded as the most popular Community driven\nQuestion Answering (CQA) website for programmers. Questions posted on Stack\nOverflow which are not related to programming topics, are marked as 'closed' by\nexperienced users and community moderators. A question can be 'closed' for five\nreasons - duplicate, off-topic, subjective, not a real question and too\nlocalized. In this work, we present the first study of 'closed' questions in\nStack Overflow. We download 4 years of publicly available data which contains\n3.4 Million questions. We first analyze and characterize the complete set of\n0.1 Million 'closed' questions. Next, we use a machine learning framework and\nbuild a predictive model to identify a 'closed' question at the time of\nquestion creation.\n  One of our key findings is that despite being marked as 'closed', subjective\nquestions contain high information value and are very popular with the users.\nWe observe an increasing trend in the percentage of closed questions over time\nand find that this increase is positively correlated to the number of newly\nregistered users. In addition, we also see a decrease in community\nparticipation to mark a 'closed' question which has led to an increase in\nmoderation job time. We also find that questions closed with the Duplicate and\nOff Topic labels are relatively more prone to reputation gaming. For the\n'closed' question prediction task, we make use of multiple genres of feature\nsets based on - user profile, community process, textual style and question\ncontent. We use a state-of-art machine learning classifier based on an ensemble\nlearning technique and achieve an overall accuracy of 73%. To the best of our\nknowledge, this is the first experimental study to analyze and predict 'closed'\nquestions on Stack Overflow.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2013 18:21:37 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Correa", "Denzil", ""], ["Sureka", "Ashish", ""]]}, {"id": "1307.7332", "submitter": "Aditya Kurve", "authors": "Aditya Kurve, David J Miller, George Kesidis", "title": "Multicategory Crowdsourcing Accounting for Plurality in Worker Skill and\n  Intention, Task Difficulty, and Task Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing allows to instantly recruit workers on the web to annotate\nimage, web page, or document databases. However, worker unreliability prevents\ntaking a workers responses at face value. Thus, responses from multiple workers\nare typically aggregated to more reliably infer ground-truth answers. We study\ntwo approaches for crowd aggregation on multicategory answer spaces stochastic\nmodeling based and deterministic objective function based. Our stochastic model\nfor answer generation plausibly captures the interplay between worker skills,\nintentions, and task difficulties and allows us to model a broad range of\nworker types. Our deterministic objective based approach does not assume a\nmodel for worker response generation. Instead, it aims to maximize the average\naggregate confidence of weighted plurality crowd decision making. In both\napproaches, we explicitly model the skill and intention of individual workers,\nwhich is exploited for improved crowd aggregation. Our methods are applicable\nin both unsupervised and semisupervised settings, and also when the batch of\ntasks is heterogeneous. As observed experimentally, the proposed methods can\ndefeat tyranny of the masses, they are especially advantageous when there is a\nminority of skilled workers amongst a large crowd of unskilled and malicious\nworkers.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2013 04:03:27 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Kurve", "Aditya", ""], ["Miller", "David J", ""], ["Kesidis", "George", ""]]}, {"id": "1307.7973", "submitter": "Antoine Bordes", "authors": "Jason Weston, Antoine Bordes, Oksana Yakhnenko, Nicolas Usunier", "title": "Connecting Language and Knowledge Bases with Embedding Models for\n  Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach for relation extraction from free text\nwhich is trained to jointly use information from the text and from existing\nknowledge. Our model is based on two scoring functions that operate by learning\nlow-dimensional embeddings of words and of entities and relationships from a\nknowledge base. We empirically show on New York Times articles aligned with\nFreebase relations that our approach is able to efficiently use the extra\ninformation provided by a large subset of Freebase data (4M entities, 23k\nrelationships) to improve over existing methods that rely on text features\nalone.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 13:37:09 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Weston", "Jason", ""], ["Bordes", "Antoine", ""], ["Yakhnenko", "Oksana", ""], ["Usunier", "Nicolas", ""]]}, {"id": "1307.7974", "submitter": "Jingdong Wang", "authors": "Jingdong Wang, Jiazhen Zhou, Hao Xu, Tao Mei, Xian-Sheng Hua, and\n  Shipeng Li", "title": "Image Tag Refinement by Regularized Latent Dirichlet Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Tagging is nowadays the most prevalent and practical way to make images\nsearchable. However, in reality many manually-assigned tags are irrelevant to\nimage content and hence are not reliable for applications. A lot of recent\nefforts have been conducted to refine image tags. In this paper, we propose to\ndo tag refinement from the angle of topic modeling and present a novel\ngraphical model, regularized Latent Dirichlet Allocation (rLDA). In the\nproposed approach, tag similarity and tag relevance are jointly estimated in an\niterative manner, so that they can benefit from each other, and the multi-wise\nrelationships among tags are explored. Moreover, both the statistics of tags\nand visual affinities of images in the corpus are explored to help topic\nmodeling. We also analyze the superiority of our approach from the deep\nstructure perspective. The experiments on tag ranking and image retrieval\ndemonstrate the advantages of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2013 08:12:03 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Wang", "Jingdong", ""], ["Zhou", "Jiazhen", ""], ["Xu", "Hao", ""], ["Mei", "Tao", ""], ["Hua", "Xian-Sheng", ""], ["Li", "Shipeng", ""]]}, {"id": "1307.8057", "submitter": "Rushdi Shams Mr", "authors": "Rushdi Shams and Robert E. Mercer", "title": "Extracting Connected Concepts from Biomedical Texts using Fog Index", "comments": "12th Conference of the Pacific Association for Computational\n  Linguistics (PACLING 2011), Kuala Lumpur, Malaysia, July 19-21, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish Fog Index (FI) as a text filter to locate the\nsentences in texts that contain connected biomedical concepts of interest. To\ndo so, we have used 24 random papers each containing four pairs of connected\nconcepts. For each pair, we categorize sentences based on whether they contain\nboth, any or none of the concepts. We then use FI to measure difficulty of the\nsentences of each category and find that sentences containing both of the\nconcepts have low readability. We rank sentences of a text according to their\nFI and select 30 percent of the most difficult sentences. We use an association\nmatrix to track the most frequent pairs of concepts in them. This matrix\nreports that the first filter produces some pairs that hold almost no\nconnections. To remove these unwanted pairs, we use the Equally Weighted\nHarmonic Mean of their Positive Predictive Value (PPV) and Sensitivity as a\nsecond filter. Experimental results demonstrate the effectiveness of our\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 17:27:29 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Shams", "Rushdi", ""], ["Mercer", "Robert E.", ""]]}, {"id": "1307.8060", "submitter": "Rushdi Shams Mr", "authors": "Rushdi Shams", "title": "Extracting Information-rich Part of Texts using Text Denoising", "comments": "26th Canadian Conference on Artificial Intelligence (CAI-2013),\n  Regina, Canada, May 29-31, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to report on a novel text reduction technique,\ncalled Text Denoising, that highlights information-rich content when processing\na large volume of text data, especially from the biomedical domain. The core\nfeature of the technique, the text readability index, embodies the hypothesis\nthat complex text is more information-rich than the rest. When applied on tasks\nlike biomedical relation bearing text extraction, keyphrase indexing and\nextracting sentences describing protein interactions, it is evident that the\nreduced set of text produced by text denoising is more information-rich than\nthe rest.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 17:36:53 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Shams", "Rushdi", ""]]}, {"id": "1307.8083", "submitter": "Guanfeng Liang", "authors": "Guanfeng Liang and Ulas C. Kozat", "title": "TOFEC: Achieving Optimal Throughput-Delay Trade-off of Cloud Storage\n  Using Erasure Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our paper presents solutions using erasure coding, parallel connections to\nstorage cloud and limited chunking (i.e., dividing the object into a few\nsmaller segments) together to significantly improve the delay performance of\nuploading and downloading data in and out of cloud storage.\n  TOFEC is a strategy that helps front-end proxy adapt to level of workload by\ntreating scalable cloud storage (e.g. Amazon S3) as a shared resource requiring\nadmission control. Under light workloads, TOFEC creates more smaller chunks and\nuses more parallel connections per file, minimizing service delay. Under heavy\nworkloads, TOFEC automatically reduces the level of chunking (fewer chunks with\nincreased size) and uses fewer parallel connections to reduce overhead,\nresulting in higher throughput and preventing queueing delay. Our trace-driven\nsimulation results show that TOFEC's adaptation mechanism converges to an\nappropriate code that provides the optimal delay-throughput trade-off without\nreducing system capacity. Compared to a non-adaptive strategy optimized for\nthroughput, TOFEC delivers 2.5x lower latency under light workloads; compared\nto a non-adaptive strategy optimized for latency, TOFEC can scale to support\nover 3x as many requests.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2013 18:49:33 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Liang", "Guanfeng", ""], ["Kozat", "Ulas C.", ""]]}, {"id": "1307.8225", "submitter": "Rosy Madaan", "authors": "Deepti Kapri, Rosy Madaan, A. K Sharma, Ashutosh Dixit", "title": "A Novel Architecture for Relevant Blog Page Identifcation", "comments": "13 Pages. International Journal of Computer Engineering and\n  Applications, June 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blogs are undoubtedly the richest source of information available in\ncyberspace. Blogs can be of various natures i.e. personal blogs which contain\nposts on mixed issues or blogs can be domain specific which contains posts on\nparticular topics, this is the reason, they offer wide variety of relevant\ninformation which is often focused. A general search engine gives back a huge\ncollection of web pages which may or may not give correct answers, as web is\nthe repository of information of all kinds and a user has to go through various\ndocuments before he gets what he was originally looking for, which is a very\ntime consuming process. So, the search can be made more focused and accurate if\nit is limited to blogosphere instead of web pages. The reason being that the\nblogs are more focused in terms of information. So, User will only get related\nblogs in response to his query. These results will be then ranked according to\nour proposed method and are finally presented in front of user in descending\norder\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 05:40:59 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Kapri", "Deepti", ""], ["Madaan", "Rosy", ""], ["Sharma", "A. K", ""], ["Dixit", "Ashutosh", ""]]}]