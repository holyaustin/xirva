[{"id": "1910.00054", "submitter": "Giannis Karamanolakis", "authors": "Giannis Karamanolakis, Daniel Hsu, Luis Gravano", "title": "Weakly Supervised Attention Networks for Fine-Grained Opinion Mining and\n  Public Health", "comments": "Accepted for the 5th Workshop on Noisy User-generated Text (W-NUT\n  2019), held in conjunction with EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many review classification applications, a fine-grained analysis of the\nreviews is desirable, because different segments (e.g., sentences) of a review\nmay focus on different aspects of the entity in question. However, training\nsupervised models for segment-level classification requires segment labels,\nwhich may be more difficult or expensive to obtain than review labels. In this\npaper, we employ Multiple Instance Learning (MIL) and use only weak supervision\nin the form of a single label per review. First, we show that when\ninappropriate MIL aggregation functions are used, then MIL-based networks are\noutperformed by simpler baselines. Second, we propose a new aggregation\nfunction based on the sigmoid attention mechanism and show that our proposed\nmodel outperforms the state-of-the-art models for segment-level sentiment\nclassification (by up to 9.8% in F1). Finally, we highlight the importance of\nfine-grained predictions in an important public-health application: finding\nactionable reports of foodborne illness. We show that our model achieves 48.6%\nhigher recall compared to previous models, thus increasing the chance of\nidentifying previously unknown foodborne outbreaks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 18:40:59 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Karamanolakis", "Giannis", ""], ["Hsu", "Daniel", ""], ["Gravano", "Luis", ""]]}, {"id": "1910.00290", "submitter": "Marco Valentino", "authors": "Mokanarangan Thayaparan, Marco Valentino, Viktor Schlegel, Andre\n  Freitas", "title": "Identifying Supporting Facts for Multi-hop Question Answering with\n  Document Graph Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in reading comprehension have resulted in models that surpass\nhuman performance when the answer is contained in a single, continuous passage\nof text. However, complex Question Answering (QA) typically requires multi-hop\nreasoning - i.e. the integration of supporting facts from different sources, to\ninfer the correct answer. This paper proposes Document Graph Network (DGN), a\nmessage passing architecture for the identification of supporting facts over a\ngraph-structured representation of text. The evaluation on HotpotQA shows that\nDGN obtains competitive results when compared to a reading comprehension\nbaseline operating on raw text, confirming the relevance of structured\nrepresentations for supporting multi-hop reasoning.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 10:26:08 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Thayaparan", "Mokanarangan", ""], ["Valentino", "Marco", ""], ["Schlegel", "Viktor", ""], ["Freitas", "Andre", ""]]}, {"id": "1910.00314", "submitter": "Yatin Chaudhary", "authors": "Yatin Chaudhary, Pankaj Gupta, Hinrich Sch\\\"utze", "title": "BioNLP-OST 2019 RDoC Tasks: Multi-grain Neural Relevance Ranking Using\n  Topics and Attention Based Query-Document-Sentence Interactions", "comments": "EMNLP2019, 10 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our system details and results of participation in the\nRDoC Tasks of BioNLP-OST 2019. Research Domain Criteria (RDoC) construct is a\nmulti-dimensional and broad framework to describe mental health disorders by\ncombining knowledge from genomics to behaviour. Non-availability of RDoC\nlabelled dataset and tedious labelling process hinders the use of RDoC\nframework to reach its full potential in Biomedical research community and\nHealthcare industry. Therefore, Task-1 aims at retrieval and ranking of PubMed\nabstracts relevant to a given RDoC construct and Task-2 aims at extraction of\nthe most relevant sentence from a given PubMed abstract. We investigate (1)\nattention based supervised neural topic model and SVM for retrieval and ranking\nof PubMed abstracts and, further utilize BM25 and other relevance measures for\nre-ranking, (2) supervised and unsupervised sentence ranking models utilizing\nmulti-view representations comprising of query-aware attention-based sentence\nrepresentation (QAR), bag-of-words (BoW) and TF-IDF. Our best systems achieved\n1st rank and scored 0.86 mean average precision (mAP) and 0.58 macro average\naccuracy (MAA) in Task-1 and Task-2 respectively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 11:47:36 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 08:06:02 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Chaudhary", "Yatin", ""], ["Gupta", "Pankaj", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "1910.00341", "submitter": "Myunghun Jung", "authors": "Myunghun Jung, Hyungjun Lim, Jahyun Goo, Youngmoon Jung, and Hoirin\n  Kim", "title": "Additional Shared Decoder on Siamese Multi-view Encoders for Learning\n  Acoustic Word Embeddings", "comments": "Accepted at 2019 IEEE Automatic Speech Recognition and Understanding\n  Workshop (ASRU 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.IR cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic word embeddings --- fixed-dimensional vector representations of\narbitrary-length words --- have attracted increasing interest in\nquery-by-example spoken term detection. Recently, on the fact that the\northography of text labels partly reflects the phonetic similarity between the\nwords' pronunciation, a multi-view approach has been introduced that jointly\nlearns acoustic and text embeddings. It showed that it is possible to learn\ndiscriminative embeddings by designing the objective which takes text labels as\nwell as word segments. In this paper, we propose a network architecture that\nexpands the multi-view approach by combining the Siamese multi-view encoders\nwith a shared decoder network to maximize the effect of the relationship\nbetween acoustic and text embeddings in embedding space. Discriminatively\ntrained with multi-view triplet loss and decoding loss, our proposed approach\nachieves better performance on acoustic word discrimination task with the WSJ\ndataset, resulting in 11.1% relative improvement in average precision. We also\npresent experimental results on cross-view word discrimination and word level\nspeech recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 12:36:18 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Jung", "Myunghun", ""], ["Lim", "Hyungjun", ""], ["Goo", "Jahyun", ""], ["Jung", "Youngmoon", ""], ["Kim", "Hoirin", ""]]}, {"id": "1910.00352", "submitter": "Moksh Jain", "authors": "Moksh Jain and Sowmya Kamath S", "title": "Proximal Policy Optimization for Improved Convergence in IRGAN", "comments": "5 pages. Smooth Games Optimization and Machine Learning Workshop\n  (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  IRGAN is an information retrieval (IR) modeling approach that uses a\ntheoretical minimax game between a generative and a discriminative model to\niteratively optimize both of them, hence unifying the generative and\ndiscriminative approaches. Despite significant performance improvements in\nseveral information retrieval tasks, IRGAN training is an unstable process, and\nthe solution varies largely with the random parameter initialization. In this\nwork, we present an improved training objective based on proximal policy\noptimization objective and Gumbel-Softmax based sampling for the generator. We\nalso propose a modified training algorithm which takes a single gradient update\non both the generator as well as discriminator for each iteration step. We\npresent empirical evidence of the improved convergence of the proposed model\nover the original IRGAN and a comparison on three different IR tasks on\nbenchmark datasets is also discussed, emphasizing the proposed model's superior\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 12:56:32 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Jain", "Moksh", ""], ["S", "Sowmya Kamath", ""]]}, {"id": "1910.00421", "submitter": "Khanh Nguyen", "authors": "Khanh Nguyen and Hal Daum\\'e III", "title": "Global Voices: Crossing Borders in Automatic News Summarization", "comments": "NewSum workshop at EMNLP 2019, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct Global Voices, a multilingual dataset for evaluating\ncross-lingual summarization methods. We extract social-network descriptions of\nGlobal Voices news articles to cheaply collect evaluation data for into-English\nand from-English summarization in 15 languages. Especially, for the\ninto-English summarization task, we crowd-source a high-quality evaluation\ndataset based on guidelines that emphasize accuracy, coverage, and\nunderstandability. To ensure the quality of this dataset, we collect human\nratings to filter out bad summaries, and conduct a survey on humans, which\nshows that the remaining summaries are preferred over the social-network\nsummaries. We study the effect of translation quality in cross-lingual\nsummarization, comparing a translate-then-summarize approach with several\nbaselines. Our results highlight the limitations of the ROUGE metric that are\noverlooked in monolingual summarization. Our dataset is available for download\nat https://forms.gle/gpkJDT6RJWHM1Ztz9 .\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 14:19:40 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 02:13:40 GMT"}, {"version": "v3", "created": "Sun, 10 Nov 2019 15:19:30 GMT"}, {"version": "v4", "created": "Mon, 15 Jun 2020 16:00:33 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Nguyen", "Khanh", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "1910.00896", "submitter": "Suzan Verberne", "authors": "Benjamin van der Burgh and Suzan Verberne", "title": "The merits of Universal Language Model Fine-tuning for Small Datasets --\n  a case with Dutch book reviews", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluated the effectiveness of using language models, that were\npre-trained in one domain, as the basis for a classification model in another\ndomain: Dutch book reviews. Pre-trained language models have opened up new\npossibilities for classification tasks with limited labelled data, because\nrepresentation can be learned in an unsupervised fashion. In our experiments we\nhave studied the effects of training set size (100-1600 items) on the\nprediction accuracy of a ULMFiT classifier, based on a language models that we\npre-trained on the Dutch Wikipedia. We also compared ULMFiT to Support Vector\nMachines, which is traditionally considered suitable for small collections. We\nfound that ULMFiT outperforms SVM for all training set sizes and that\nsatisfactory results (~90%) can be achieved using training sets that can be\nmanually annotated within a few hours. We deliver both our new benchmark\ncollection of Dutch book reviews for sentiment classification as well as the\npre-trained Dutch language model to the community.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 12:02:46 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["van der Burgh", "Benjamin", ""], ["Verberne", "Suzan", ""]]}, {"id": "1910.02028", "submitter": "Preslav Nakov", "authors": "Yifan Zhang, Giovanni Da San Martino, Alberto Barr\\'on-Cede\\~no,\n  Salvatore Romeo, Jisun An, Haewoon Kwak, Todor Staykovski, Israa Jaradat,\n  Georgi Karadzhov, Ramy Baly, Kareem Darwish, James Glass, Preslav Nakov", "title": "Tanbih: Get To Know What You Are Reading", "comments": null, "journal-ref": "EMNLP-2019", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Tanbih, a news aggregator with intelligent analysis tools to\nhelp readers understanding what's behind a news story. Our system displays news\ngrouped into events and generates media profiles that show the general\nfactuality of reporting, the degree of propagandistic content,\nhyper-partisanship, leading political ideology, general frame of reporting, and\nstance with respect to various claims and topics of a news outlet. In addition,\nwe automatically analyse each article to detect whether it is propagandistic\nand to determine its stance with respect to a number of controversial topics.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 16:43:49 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Zhang", "Yifan", ""], ["Martino", "Giovanni Da San", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["Romeo", "Salvatore", ""], ["An", "Jisun", ""], ["Kwak", "Haewoon", ""], ["Staykovski", "Todor", ""], ["Jaradat", "Israa", ""], ["Karadzhov", "Georgi", ""], ["Baly", "Ramy", ""], ["Darwish", "Kareem", ""], ["Glass", "James", ""], ["Nakov", "Preslav", ""]]}, {"id": "1910.02049", "submitter": "Rui Guo", "authors": "Rui Guo, Dorien Herremans, Thor Magnusson", "title": "Midi Miner -- A Python library for tonal tension and track\n  classification", "comments": "2 pages. ISMIR - Late Breaking Demo, Delft, The Netherlands. November\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a Python library, called Midi Miner, that can calculate tonal\ntension and classify different tracks. MIDI (Music Instrument Digital\nInterface) is a hardware and software standard for communicating musical events\nbetween digital music devices. It is often used for tasks such as music\nrepresentation, communication between devices, and even music generation [5].\nTension is an essential element of the music listening experience, which can\ncome from a number of musical features including timbre, loudness and harmony\n[3]. Midi Miner provides a Python implementation for the tonal tension model\nbased on the spiral array [1] as presented by Herremans and Chew [4]. Midi\nMiner also performs key estimation and includes a track classifier that can\ndisentangle melody, bass, and harmony tracks. Even though tracks are often\nseparated in MIDI files, the musical function of each track is not always\nclear. The track classifier keeps the identified tracks and discards messy\ntracks, which can enable further analysis and training tasks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 08:09:55 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 08:35:20 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Guo", "Rui", ""], ["Herremans", "Dorien", ""], ["Magnusson", "Thor", ""]]}, {"id": "1910.02202", "submitter": "Nguyen Vo", "authors": "Nguyen Vo, Kyumin Lee", "title": "Learning from Fact-checkers: Analysis and Generation of Fact-checking\n  Language", "comments": "SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In fighting against fake news, many fact-checking systems comprised of\nhuman-based fact-checking sites (e.g., snopes.com and politifact.com) and\nautomatic detection systems have been developed in recent years. However,\nonline users still keep sharing fake news even when it has been debunked. It\nmeans that early fake news detection may be insufficient and we need another\ncomplementary approach to mitigate the spread of misinformation. In this paper,\nwe introduce a novel application of text generation for combating fake news. In\nparticular, we (1) leverage online users named \\emph{fact-checkers}, who cite\nfact-checking sites as credible evidences to fact-check information in public\ndiscourse; (2) analyze linguistic characteristics of fact-checking tweets; and\n(3) propose and build a deep learning framework to generate responses with\nfact-checking intention to increase the fact-checkers' engagement in\nfact-checking activities. Our analysis reveals that the fact-checkers tend to\nrefute misinformation and use formal language (e.g. few swear words and\nInternet slangs). Our framework successfully generates relevant responses, and\noutperforms competing models by achieving up to 30\\% improvements. Our\nqualitative study also confirms that the superiority of our generated responses\ncompared with responses generated from the existing models.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 03:23:45 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Vo", "Nguyen", ""], ["Lee", "Kyumin", ""]]}, {"id": "1910.02332", "submitter": "Mhd Wesam Al-Nabki", "authors": "Mhd Wesam Al-Nabki, Eduardo Fidalgo, Enrique Alegre, and Deisy Chaves", "title": "Content-Based Features to Rank Influential Hidden Services of the Tor\n  Darknet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unevenness importance of criminal activities in the onion domains of the\nTor Darknet and the different levels of their appeal to the end-user make them\ntangled to measure their influence. To this end, this paper presents a novel\ncontent-based ranking framework to detect the most influential onion domains.\nOur approach comprises a modeling unit that represents an onion domain using\nforty features extracted from five different resources: user-visible text, HTML\nmarkup, Named Entities, network topology, and visual content. And also, a\nranking unit that, using the Learning-to-Rank (LtR) approach, automatically\nlearns a ranking function by integrating the previously obtained features.\nUsing a case-study based on drugs-related onion domains, we obtained the\nfollowing results. (1) Among the explored LtR schemes, the listwise approach\noutperforms the benchmarked methods with an NDCG of 0.95 for the top-10 ranked\ndomains. (2) We proved quantitatively that our framework surpasses the\nlink-based ranking techniques. Also, (3) with the selected feature, we observed\nthat the textual content, composed by text, NER, and HTML features, is the most\nbalanced approach, in terms of efficiency and score obtained. The proposed\nframework might support Law Enforcement Agencies in detecting the most\ninfluential domains related to possible suspicious activities.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 21:39:20 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Al-Nabki", "Mhd Wesam", ""], ["Fidalgo", "Eduardo", ""], ["Alegre", "Enrique", ""], ["Chaves", "Deisy", ""]]}, {"id": "1910.02358", "submitter": "Kyung-Wha Park", "authors": "Kyung-Wha Park, JungHoon Lee, Sunyoung Kwon, Jung-Woo Ha, Kyung-Min\n  Kim, Byoung-Tak Zhang", "title": "Which Ads to Show? Advertisement Image Assessment with Auxiliary\n  Information via Multi-step Modality Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing aesthetic preference is a fundamental task related to human\ncognition. It can also contribute to various practical applications such as\nimage creation for online advertisements. Despite crucial influences of image\nquality, auxiliary information of ad images such as tags and target subjects\ncan also determine image preference. Existing studies mainly focus on images\nand thus are less useful for advertisement scenarios where rich auxiliary data\nare available. Here we propose a modality fusion-based neural network that\nevaluates the aesthetic preference of images with auxiliary information. Our\nmethod fully utilizes auxiliary data by introducing multi-step modality fusion\nusing both conditional batch normalization-based low-level and attention-based\nhigh-level fusion mechanisms, inspired by the findings from statistical\nanalyses on real advertisement data. Our approach achieved state-of-the-art\nperformance on the AVA dataset, a widely used dataset for aesthetic assessment.\nBesides, the proposed method is evaluated on large-scale real-world\nadvertisement image data with rich auxiliary attributes, providing promising\npreference prediction results. Through extensive experiments, we investigate\nhow image and auxiliary information together influence click-through rate.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 03:17:38 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Park", "Kyung-Wha", ""], ["Lee", "JungHoon", ""], ["Kwon", "Sunyoung", ""], ["Ha", "Jung-Woo", ""], ["Kim", "Kyung-Min", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1910.02448", "submitter": "Pengjie Ren", "authors": "Wenchao Sun and Muyang Ma and Pengjie Ren and Yujie Lin and Zhumin\n  Chen and Zhaochun Ren and Jun Ma and Maarten de Rijke", "title": "Parallel Split-Join Networks for Shared-account Cross-domain Sequential\n  Recommendations", "comments": "Submitted to TKDE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential recommendation is a task in which one models and uses sequential\ninformation about user behavior for recommendation purposes. We study\nsequential recommendation in a particularly challenging context, in which\nmultiple individual users share asingle account (i.e., they have a shared\naccount) and in which user behavior is available in multiple domains (i.e.,\nrecommendations are cross-domain). These two characteristics bring new\nchallenges on top of those of the traditional sequential recommendation task.\nFirst, we need to identify the behavior associated with different users and\ndifferent user roles under the same account in order to recommend the right\nitem to the right user role at the right time. Second, we need to identify\nbehavior in one domain that might be helpful to improve recommendations in\nother domains. In this work, we study shared account cross-domain sequential\nrecommendation and propose Parallel Split-Join Network (PSJNet), a parallel\nmodeling network to address the two challenges above. We present two variants\nof PSJNet, PSJNet-I and PSJNet-II. PSJNet-I is a \"split-by-join\" framework that\nsplits the mixed representations to get role-specific representations and joins\nthem to obtain cross-domain representations at each timestamp simultaneously.\nPSJNet-II is a \"split-and-join\" framework that first splits role-specific\nrepresentations at each timestamp, and then the representations from all\ntimestamps and all roles are joined to obtain cross-domain representations. We\nuse two datasets to assess the effectiveness of PSJNet. Our experimental\nresults demonstrate that PSJNet outperforms state-of-the-art sequential\nrecommendation baselines in terms of MRR and Recall.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 13:35:31 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 10:25:32 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2019 09:48:25 GMT"}, {"version": "v4", "created": "Tue, 13 Apr 2021 02:15:41 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Sun", "Wenchao", ""], ["Ma", "Muyang", ""], ["Ren", "Pengjie", ""], ["Lin", "Yujie", ""], ["Chen", "Zhumin", ""], ["Ren", "Zhaochun", ""], ["Ma", "Jun", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1910.02517", "submitter": "Preslav Nakov", "authors": "Giovanni Da San Martino, Seunghak Yu, Alberto Barr\\'on-Cede\\~no,\n  Rostislav Petrov, Preslav Nakov", "title": "Fine-Grained Analysis of Propaganda in News Articles", "comments": null, "journal-ref": "EMNLP-2019", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propaganda aims at influencing people's mindset with the purpose of advancing\na specific agenda. Previous work has addressed propaganda detection at the\ndocument level, typically labelling all articles from a propagandistic news\noutlet as propaganda. Such noisy gold labels inevitably affect the quality of\nany learning system trained on them. A further issue with most existing systems\nis the lack of explainability. To overcome these limitations, we propose a\nnovel task: performing fine-grained analysis of texts by detecting all\nfragments that contain propaganda techniques as well as their type. In\nparticular, we create a corpus of news articles manually annotated at the\nfragment level with eighteen propaganda techniques and we propose a suitable\nevaluation measure. We further design a novel multi-granularity neural network,\nand we show that it outperforms several strong BERT-based baselines.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 20:26:12 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Martino", "Giovanni Da San", ""], ["Yu", "Seunghak", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["Petrov", "Rostislav", ""], ["Nakov", "Preslav", ""]]}, {"id": "1910.02574", "submitter": "Tong Wu", "authors": "Tong Wu, Yunlong Wang, Yue Wang, Emily Zhao, Yilian Yuan, Zhi Yang", "title": "Representation Learning of EHR Data via Graph-Based Medical Entity\n  Embedding", "comments": "5 pages, 2 figures, NeurIPS 2019 Graph Representation Learning\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic representation learning of key entities in electronic health record\n(EHR) data is a critical step for healthcare informatics that turns\nheterogeneous medical records into structured and actionable information. Here\nwe propose ME2Vec, an algorithmic framework for learning low-dimensional\nvectors of the most common entities in EHR: medical services, doctors, and\npatients. ME2Vec leverages diverse graph embedding techniques to cater for the\nunique characteristic of each medical entity. Using real-world clinical data,\nwe demonstrate the efficacy of ME2Vec over competitive baselines on disease\ndiagnosis prediction.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 02:01:32 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Wu", "Tong", ""], ["Wang", "Yunlong", ""], ["Wang", "Yue", ""], ["Zhao", "Emily", ""], ["Yuan", "Yilian", ""], ["Yang", "Zhi", ""]]}, {"id": "1910.02611", "submitter": "Gaurav Gupta", "authors": "Gaurav Gupta, Minghao Yan, Benjamin Coleman, R. A. Leo Elworth, Tharun\n  Medini, Todd Treangen, Anshumali Shrivastava", "title": "RAMBO: Repeated And Merged BloOm Filter for Ultra-fast Multiple Set\n  Membership Testing (MSMT) on Large-Scale Data", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Set Membership Testing (MSMT) is a well-known problem in a variety\nof search and query applications. Given a dataset of K different sets and a\nquery q, it aims to find all of the sets containing the query. Trivially, an\nMSMT instance can be reduced to K membership testing instances, each with the\nsame q, leading to O(K) query time with a simple array of Bloom Filters. We\npropose a data-structure called RAMBO (Repeated And Merged BloOm Filter) that\nachieves O(\\sqrt{K} log K) query time in expectation with an additional\nworst-case memory cost factor of O(log K) beyond the array of Bloom Filters.\nDue to this, RAMBO is a very fast and accurate data-structure. Apart from being\nembarrassingly parallel, supporting cheap updates for streaming inputs, zero\nfalse-negative rate, and low false-positive rate, RAMBO beats the\nstate-of-the-art approaches for genome indexing methods: COBS (Compact\nbit-sliced signature index), Sequence Bloom Trees (a Bloofi based\nimplementation), HowDeSBT, SSBT, and document indexing methods like BitFunnel.\nThe proposed data-structure is simply a count-min sketch type arrangement of a\nmembership testing utility (Bloom Filter in our case). It indexes k-grams and\nprovides an approximate membership testing based search utility. The simplicity\nof the algorithm and embarrassingly parallel architecture allows us to index a\n170 TB genome dataset in a mere 14 hours on a cluster of 100 nodes while\ncompeting methods require weeks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 05:15:27 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 20:30:47 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Gupta", "Gaurav", ""], ["Yan", "Minghao", ""], ["Coleman", "Benjamin", ""], ["Elworth", "R. A. Leo", ""], ["Medini", "Tharun", ""], ["Treangen", "Todd", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1910.02932", "submitter": "Kashif Ahmad Dr", "authors": "Kashif Ahmad, Konstantin Pogorelov, Mohib Ullah, Michael Riegler,\n  Nicola Conci, Johannes Langguth, Ala Al-Fuqaha", "title": "Multi-Modal Machine Learning for Flood Detection in News, Social Media\n  and Satellite Sequences", "comments": null, "journal-ref": "MediaEval 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our methods for the MediaEval 2019 Mul-timedia\nSatellite Task, which is aiming to extract complementaryinformation associated\nwith adverse events from Social Media andsatellites. For the first challenge,\nwe propose a framework jointly uti-lizing colour, object and scene-level\ninformation to predict whetherthe topic of an article containing an image is a\nflood event or not.Visual features are combined using early and late fusion\ntechniquesachieving an average F1-score of82.63,82.40,81.40and76.77. Forthe\nmulti-modal flood level estimation, we rely on both visualand textual\ninformation achieving an average F1-score of58.48and46.03, respectively.\nFinally, for the flooding detection in time-based satellite image sequences we\nused a combination of classicalcomputer-vision and machine learning approaches\nachieving anaverage F1-score of58.82%\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 17:49:59 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Ahmad", "Kashif", ""], ["Pogorelov", "Konstantin", ""], ["Ullah", "Mohib", ""], ["Riegler", "Michael", ""], ["Conci", "Nicola", ""], ["Langguth", "Johannes", ""], ["Al-Fuqaha", "Ala", ""]]}, {"id": "1910.02993", "submitter": "Daniel Y. Fu", "authors": "Daniel Y. Fu, Will Crichton, James Hong, Xinwei Yao, Haotian Zhang,\n  Anh Truong, Avanika Narayan, Maneesh Agrawala, Christopher R\\'e, Kayvon\n  Fatahalian", "title": "Rekall: Specifying Video Events using Compositions of Spatiotemporal\n  Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world video analysis applications require the ability to identify\ndomain-specific events in video, such as interviews and commercials in TV news\nbroadcasts, or action sequences in film. Unfortunately, pre-trained models to\ndetect all the events of interest in video may not exist, and training new\nmodels from scratch can be costly and labor-intensive. In this paper, we\nexplore the utility of specifying new events in video in a more traditional\nmanner: by writing queries that compose outputs of existing, pre-trained\nmodels. To write these queries, we have developed Rekall, a library that\nexposes a data model and programming model for compositional video event\nspecification. Rekall represents video annotations from different sources\n(object detectors, transcripts, etc.) as spatiotemporal labels associated with\ncontinuous volumes of spacetime in a video, and provides operators for\ncomposing labels into queries that model new video events. We demonstrate the\nuse of Rekall in analyzing video from cable TV news broadcasts, films,\nstatic-camera vehicular video streams, and commercial autonomous vehicle logs.\nIn these efforts, domain experts were able to quickly (in a few hours to a day)\nauthor queries that enabled the accurate detection of new events (on par with,\nand in some cases much more accurate than, learned approaches) and to rapidly\nretrieve video clips for human-in-the-loop tasks such as video content curation\nand training data curation. Finally, in a user study, novice users of Rekall\nwere able to author queries to retrieve new events in video given just one hour\nof query development time.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 18:18:37 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Fu", "Daniel Y.", ""], ["Crichton", "Will", ""], ["Hong", "James", ""], ["Yao", "Xinwei", ""], ["Zhang", "Haotian", ""], ["Truong", "Anh", ""], ["Narayan", "Avanika", ""], ["Agrawala", "Maneesh", ""], ["R\u00e9", "Christopher", ""], ["Fatahalian", "Kayvon", ""]]}, {"id": "1910.03040", "submitter": "Oznur Alkan", "authors": "Oznur Alkan, Massimiliano Mattetti, Elizabeth M. Daly, Adi Botea, Inge\n  Vejsbjerg", "title": "IRF: Interactive Recommendation through Dialogue", "comments": "2 pages, 1 figure, ACM RecSys Conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent research focuses beyond recommendation accuracy, towards human factors\nthat influence the acceptance of recommendations, such as user satisfaction,\ntrust, transparency and sense of control.We present a generic interactive\nrecommender framework that can add interaction functionalities to\nnon-interactive recommender systems.We take advantage of dialogue systems to\ninteract with the user and we design a middleware layer to provide the\ninteraction functions, such as providing explanations for the recommendations,\nmanaging users preferences learnt from dialogue, preference elicitation and\nrefining recommendations based on learnt preferences.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 19:35:27 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Alkan", "Oznur", ""], ["Mattetti", "Massimiliano", ""], ["Daly", "Elizabeth M.", ""], ["Botea", "Adi", ""], ["Vejsbjerg", "Inge", ""]]}, {"id": "1910.03089", "submitter": "Vedant Bhatia", "authors": "Vedant Bhatia, Prateek Rawat, Ajit Kumar, Rajiv Ratn Shah", "title": "End-to-End Resume Parsing and Finding Candidates for a Job Description\n  using BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-increasing number of applications to job positions presents a\nchallenge for employers to find suitable candidates manually. We present an\nend-to-end solution for ranking candidates based on their suitability to a job\ndescription. We accomplish this in two stages. First, we build a resume parser\nwhich extracts complete information from candidate resumes. This parser is made\navailable to the public in the form of a web application. Second, we use BERT\nsentence pair classification to perform ranking based on their suitability to\nthe job description. To approximate the job description, we use the description\nof past job experiences by a candidate as mentioned in his resume. Our dataset\ncomprises resumes in LinkedIn format and general non-LinkedIn formats. We parse\nthe LinkedIn resumes with 100\\% accuracy and establish a strong baseline of\n73\\% accuracy for candidate suitability.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 19:12:41 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 17:04:55 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Bhatia", "Vedant", ""], ["Rawat", "Prateek", ""], ["Kumar", "Ajit", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "1910.03090", "submitter": "Fatih Cagatay Akyon", "authors": "Fatih Cagatay Akyon, Esat Kalfaoglu", "title": "Instagram Fake and Automated Account Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fake engagement is one of the significant problems in Online Social Networks\n(OSNs) which is used to increase the popularity of an account in an inorganic\nmanner. The detection of fake engagement is crucial because it leads to loss of\nmoney for businesses, wrong audience targeting in advertising, wrong product\npredictions systems, and unhealthy social network environment. This study is\nrelated with the detection of fake and automated accounts which leads to fake\nengagement on Instagram. Prior to this work, there were no publicly available\ndataset for fake and automated accounts. For this purpose, two datasets have\nbeen published for the detection of fake and automated accounts. For the\ndetection of these accounts, machine learning algorithms like Naive Bayes,\nLogistic Regression, Support Vector Machines and Neural Networks are applied.\nAdditionally, for the detection of automated accounts, cost sensitive genetic\nalgorithm is proposed to handle the unnatural bias in the dataset. To deal with\nthe unevenness problem in the fake dataset, Smote-nc algorithm is implemented.\nFor the automated and fake account detection datasets, 86% and 96%\nclassification accuracies are obtained, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 12:51:01 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 10:08:35 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Akyon", "Fatih Cagatay", ""], ["Kalfaoglu", "Esat", ""]]}, {"id": "1910.03118", "submitter": "Mohamed Nadjib Mami", "authors": "Mohamed Nadjib Mami, Damien Graux, Harsh Thakkar, Simon Scerri,\n  S\\\"oren Auer, Jens Lehmann", "title": "The Query Translation Landscape: a Survey", "comments": "25 pages, 5 tables, 92 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Whereas the availability of data has seen a manyfold increase in past years,\nits value can be only shown if the data variety is effectively tackled ---one\nof the prominent Big Data challenges. The lack of data interoperability limits\nthe potential of its collective use for novel applications. Achieving\ninteroperability through the full transformation and integration of diverse\ndata structures remains an ideal that is hard, if not impossible, to achieve.\nInstead, methods that can simultaneously interpret different types of data\navailable in different data structures and formats have been explored. On the\nother hand, many query languages have been designed to enable users to interact\nwith the data, from relational, to object-oriented, to hierarchical, to the\nmultitude emerging NoSQL languages. Therefore, the interoperability issue could\nbe solved not by enforcing physical data transformation, but by looking at\ntechniques that are able to query heterogeneous sources using one uniform\nlanguage. Both industry and research communities have been keen to develop such\ntechniques, which require the translation of a chosen 'universal' query\nlanguage to the various data model specific query languages that make the\nunderlying data accessible. In this article, we survey more than forty query\ntranslation methods and tools for popular query languages, and classify them\naccording to eight criteria. In particular, we study which query language is a\nmost suitable candidate for that 'universal' query language. Further, the\nresults enable us to discover the weakly addressed and unexplored translation\npaths, to discover gaps and to learn lessons that can benefit future research\nin the area.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 22:37:33 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Mami", "Mohamed Nadjib", ""], ["Graux", "Damien", ""], ["Thakkar", "Harsh", ""], ["Scerri", "Simon", ""], ["Auer", "S\u00f6ren", ""], ["Lehmann", "Jens", ""]]}, {"id": "1910.03206", "submitter": "Ashiqur KhudaBukhsh Ashiqur Rahman KhudaBukhsh", "authors": "Shriphani Palakodety, Ashiqur R. KhudaBukhsh, Jaime G. Carbonell", "title": "Voice for the Voiceless: Active Sampling to Detect Comments Supporting\n  the Rohingyas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Rohingya refugee crisis is one of the biggest humanitarian crises of\nmodern times with more than 600,000 Rohingyas rendered homeless according to\nthe United Nations High Commissioner for Refugees. While it has received\nsustained press attention globally, no comprehensive research has been\nperformed on social media pertaining to this large evolving crisis. In this\nwork, we construct a substantial corpus of YouTube video comments (263,482\ncomments from 113,250 users in 5,153 relevant videos) with an aim to analyze\nthe possible role of AI in helping a marginalized community. Using a novel\ncombination of multiple Active Learning strategies and a novel active sampling\nstrategy based on nearest-neighbors in the comment-embedding space, we\nconstruct a classifier that can detect comments defending the Rohingyas among\nlarger numbers of disparaging and neutral ones. We advocate that beyond the\nburgeoning field of hate-speech detection, automatic detection of\n\\emph{help-speech} can lend voice to the voiceless people and make the internet\nsafer for marginalized communities.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 04:17:33 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 19:33:00 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Palakodety", "Shriphani", ""], ["KhudaBukhsh", "Ashiqur R.", ""], ["Carbonell", "Jaime G.", ""]]}, {"id": "1910.03262", "submitter": "Rishiraj Saha Roy", "authors": "Philipp Christmann, Rishiraj Saha Roy, Abdalghani Abujabal, Jyotsna\n  Singh, Gerhard Weikum", "title": "Look before you Hop: Conversational Question Answering over Knowledge\n  Graphs Using Judicious Context Expansion", "comments": "CIKM 2019 Long Paper, 10 pages", "journal-ref": "CIKM 2019", "doi": "10.1145/3357384.3358016", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fact-centric information needs are rarely one-shot; users typically ask\nfollow-up questions to explore a topic. In such a conversational setting, the\nuser's inputs are often incomplete, with entities or predicates left out, and\nungrammatical phrases. This poses a huge challenge to question answering (QA)\nsystems that typically rely on cues in full-fledged interrogative sentences. As\na solution, we develop CONVEX: an unsupervised method that can answer\nincomplete questions over a knowledge graph (KG) by maintaining conversation\ncontext using entities and predicates seen so far and automatically inferring\nmissing or ambiguous pieces for follow-up questions. The core of our method is\na graph exploration algorithm that judiciously expands a frontier to find\ncandidate answers for the current question. To evaluate CONVEX, we release\nConvQuestions, a crowdsourced benchmark with 11,200 distinct conversations from\nfive different domains. We show that CONVEX: (i) adds conversational support to\nany stand-alone QA system, and (ii) outperforms state-of-the-art baselines and\nquestion completion strategies.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 07:57:48 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 13:03:19 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 15:42:33 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Christmann", "Philipp", ""], ["Roy", "Rishiraj Saha", ""], ["Abujabal", "Abdalghani", ""], ["Singh", "Jyotsna", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1910.03291", "submitter": "Alireza Mohammadshahi", "authors": "Alireza Mohammadshahi, Remi Lebret, Karl Aberer", "title": "Aligning Multilingual Word Embeddings for Cross-Modal Retrieval Task", "comments": null, "journal-ref": null, "doi": "10.18653/v1/D19-6605", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new approach to learn multimodal multilingual\nembeddings for matching images and their relevant captions in two languages. We\ncombine two existing objective functions to make images and captions close in a\njoint embedding space while adapting the alignment of word embeddings between\nexisting languages in our model. We show that our approach enables better\ngeneralization, achieving state-of-the-art performance in text-to-image and\nimage-to-text retrieval task, and caption-caption similarity task. Two\nmultimodal multilingual datasets are used for evaluation: Multi30k with German\nand English captions and Microsoft-COCO with English and Japanese captions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 09:13:39 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Mohammadshahi", "Alireza", ""], ["Lebret", "Remi", ""], ["Aberer", "Karl", ""]]}, {"id": "1910.03295", "submitter": "Xusheng Luo", "authors": "Xusheng Luo, Yonghua Yang, Kenny Q. Zhu, Yu Gong and Keping Yang", "title": "Conceptualize and Infer User Needs in E-commerce", "comments": "9 pages, 6 figures. Accepted by CIKM 2019 Applied Research Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding latent user needs beneath shopping behaviors is critical to\ne-commercial applications. Without a proper definition of user needs in\ne-commerce, most industry solutions are not driven directly by user needs at\ncurrent stage, which prevents them from further improving user satisfaction.\nRepresenting implicit user needs explicitly as nodes like \"outdoor barbecue\" or\n\"keep warm for kids\" in a knowledge graph, provides new imagination for various\ne- commerce applications. Backed by such an e-commerce knowledge graph, we\npropose a supervised learning algorithm to conceptualize user needs from their\ntransaction history as \"concept\" nodes in the graph and infer those concepts\nfor each user through a deep attentive model. Offline experiments demonstrate\nthe effectiveness and stability of our model, and online industry strength\ntests show substantial advantages of such user needs understanding.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 09:29:56 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Luo", "Xusheng", ""], ["Yang", "Yonghua", ""], ["Zhu", "Kenny Q.", ""], ["Gong", "Yu", ""], ["Yang", "Keping", ""]]}, {"id": "1910.03497", "submitter": "Seyed Amjad Seyedi", "authors": "Seyed Amjad Seyedi, S.Siamak Ghodsi, Fardin Akhlaghian, Mahdi Jalili,\n  Parham Moradi", "title": "Self-Paced Multi-Label Learning with Diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The major challenge of learning from multi-label data has arisen from the\noverwhelming size of label space which makes this problem NP-hard. This problem\ncan be alleviated by gradually involving easy to hard tags into the learning\nprocess. Besides, the utilization of a diversity maintenance approach avoids\noverfitting on a subset of easy labels. In this paper, we propose a self-paced\nmulti-label learning with diversity (SPMLD) which aims to cover diverse labels\nwith respect to its learning pace. In addition, the proposed framework is\napplied to an efficient correlation-based multi-label method. The non-convex\nobjective function is optimized by an extension of the block coordinate descent\nalgorithm. Empirical evaluations on real-world datasets with different\ndimensions of features and labels imply the effectiveness of the proposed\npredictive model.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:59:57 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Seyedi", "Seyed Amjad", ""], ["Ghodsi", "S. Siamak", ""], ["Akhlaghian", "Fardin", ""], ["Jalili", "Mahdi", ""], ["Moradi", "Parham", ""]]}, {"id": "1910.03498", "submitter": "Dominique Mercier", "authors": "Dominique Mercier, Akansha Bhardwaj, Andreas Dengel, Sheraz Ahmed", "title": "SentiCite: An Approach for Publication Sentiment Analysis", "comments": "Preprint, 8 pages, 2 figures, 10th International Conference on Agents\n  and Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth in the number of scientific publications, year after\nyear, it is becoming increasingly difficult to identify quality authoritative\nwork on a single topic. Though there is an availability of scientometric\nmeasures which promise to offer a solution to this problem, these measures are\nmostly quantitative and rely, for instance, only on the number of times an\narticle is cited. With this approach, it becomes irrelevant if an article is\ncited 10 times in a positive, negative or neutral way. In this context, it is\nquite important to study the qualitative aspect of a citation to understand its\nsignificance. This paper presents a novel system for sentiment analysis of\ncitations in scientific documents (SentiCite) and is also capable of detecting\nnature of citations by targeting the motivation behind a citation, e.g.,\nreference to a dataset, reading reference. Furthermore, the paper also presents\ntwo datasets (SentiCiteDB and IntentCiteDB) containing about 2,600 citations\nwith their ground truth for sentiment and nature of citation. SentiCite along\nwith other state-of-the-art methods for sentiment analysis are evaluated on the\npresented datasets. Evaluation results reveal that SentiCite outperforms\nstate-of-the-art methods for sentiment analysis in scientific publications by\nachieving a F1-measure of 0.71.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 06:49:52 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Mercier", "Dominique", ""], ["Bhardwaj", "Akansha", ""], ["Dengel", "Andreas", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "1910.03534", "submitter": "Leonid Boytsov", "authors": "Leonid Boytsov, Eric Nyberg", "title": "Accurate and Fast Retrieval for Complex Non-metric Data via Neighborhood\n  Graphs", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-32047-8_12", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that a graph-based search algorithm-relying on the\nconstruction of an approximate neighborhood graph-can directly work with\nchallenging non-metric and/or non-symmetric distances without resorting to\nmetric-space mapping and/or distance symmetrization, which, in turn, lead to\nsubstantial performance degradation. Although the straightforward metrization\nand symmetrization is usually ineffective, we find that constructing an index\nusing a modified, e.g., symmetrized, distance can improve performance. This\nobservation paves a way to a new line of research of designing index-specific\ngraph-construction distance functions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 16:45:20 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Boytsov", "Leonid", ""], ["Nyberg", "Eric", ""]]}, {"id": "1910.03539", "submitter": "Leonid Boytsov", "authors": "Leonid Boytsov, Eric Nyberg", "title": "Pruning Algorithms for Low-Dimensional Non-metric k-NN Search: A Case\n  Study", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-32047-8_7", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on low-dimensional non-metric search, where tree-based approaches\npermit efficient and accurate retrieval while having short indexing time. These\nmethods rely on space partitioning and require a pruning rule to avoid visiting\nunpromising parts. We consider two known data-driven approaches to extend these\nrules to non-metric spaces: TriGen and a piece-wise linear approximation of the\npruning rule. We propose and evaluate two adaptations of TriGen to\nnon-symmetric similarities (TriGen does not support non-symmetric distances).\nWe also evaluate a hybrid of TriGen and the piece-wise linear approximation\npruning. We find that this hybrid approach is often more effective than either\nof the pruning rules. We make our software publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 16:50:50 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Boytsov", "Leonid", ""], ["Nyberg", "Eric", ""]]}, {"id": "1910.03638", "submitter": "Mahesh Chandra Mukkamala", "authors": "Mahesh Chandra Mukkamala, Felix Westerkamp, Emanuel Laude, Daniel\n  Cremers, Peter Ochs", "title": "Bregman Proximal Framework for Deep Linear Neural Networks", "comments": "34 pages, 54 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical assumption for the analysis of first order optimization methods is\nthe Lipschitz continuity of the gradient of the objective function. However,\nfor many practical applications this assumption is violated, including loss\nfunctions in deep learning. To overcome this issue, certain extensions based on\ngeneralized proximity measures known as Bregman distances were introduced. This\ninitiated the development of the Bregman proximal gradient (BPG) algorithm and\nan inertial variant (momentum based) CoCaIn BPG, which however rely on problem\ndependent Bregman distances. In this paper, we develop Bregman distances for\nusing BPG methods to train Deep Linear Neural Networks. The main implications\nof our results are strong convergence guarantees for these algorithms. We also\npropose several strategies for their efficient implementation, for example,\nclosed form updates and a closed form expression for the inertial parameter of\nCoCaIn BPG. Moreover, the BPG method requires neither diminishing step sizes\nnor line search, unlike its corresponding Euclidean version. We numerically\nillustrate the competitiveness of the proposed methods compared to existing\nstate of the art schemes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 18:45:34 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Mukkamala", "Mahesh Chandra", ""], ["Westerkamp", "Felix", ""], ["Laude", "Emanuel", ""], ["Cremers", "Daniel", ""], ["Ochs", "Peter", ""]]}, {"id": "1910.03846", "submitter": "Qiang Tang", "authors": "Qiang Tang", "title": "Privacy-preserving and yet Robust Collaborative Filtering Recommender as\n  a Service", "comments": "19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering recommenders provide effective personalization\nservices at the cost of sacrificing the privacy of their end users. Due to the\nincreasing concerns from the society and stricter privacy regulations, it is an\nurgent research challenge to design privacy-preserving and yet robust\nrecommenders which offer recommendation services to privacy-aware users. Our\nanalysis shows that existing solutions fall short in several aspects, including\nlacking attention to the precise output to end users and ignoring the\ncorrelated robustness issues. In this paper, we provide a general system\nstructure for latent factor based collaborative filtering recommenders by\nformulating them into model training and prediction computing stages, and also\ndescribe a new security model. Aiming at pragmatic solutions, we first show how\nto construct privacy-preserving and yet robust model training stage based on\nexisting solutions. Then, we propose two cryptographic protocols to realize a\nprivacy-preserving prediction computing stage, depending on whether or not an\nextra proxy is involved. Different from standard Top-k recommendations, we\nalternatively let the end user retrieve the unrated items whose predictions are\nabove a threshold, as a result of our privacy by design strategy. Experimental\nresults show that our new protocols are quite efficient.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 08:43:46 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Tang", "Qiang", ""]]}, {"id": "1910.03940", "submitter": "Mamdouh Farouk", "authors": "Mamdouh Farouk", "title": "Measuring Sentences Similarity: A Survey", "comments": "11 pages, 2 figures, journal", "journal-ref": "Indian Journal of Science and Technology, Vol 12(25), July 2019", "doi": "10.17485/ijst/2019/v12i25/143977", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This study is to review the approaches used for measuring sentences\nsimilarity. Measuring similarity between natural language sentences is a\ncrucial task for many Natural Language Processing applications such as text\nclassification, information retrieval, question answering, and plagiarism\ndetection. This survey classifies approaches of calculating sentences\nsimilarity based on the adopted methodology into three categories. Word-to-word\nbased, structure based, and vector-based are the most widely used approaches to\nfind sentences similarity. Each approach measures relatedness between short\ntexts based on a specific perspective. In addition, datasets that are mostly\nused as benchmarks for evaluating techniques in this field are introduced to\nprovide a complete view on this issue. The approaches that combine more than\none perspective give better results. Moreover, structure based similarity that\nmeasures similarity between sentences structures needs more investigation.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 09:21:21 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Farouk", "Mamdouh", ""]]}, {"id": "1910.03943", "submitter": "Ali Sadeghian", "authors": "Ali Sadeghian, Shervin Minaee, Ioannis Partalas, Xinxin Li, Daisy Zhe\n  Wang, Brooke Cowan", "title": "Hotel2vec: Learning Attribute-Aware Hotel Embeddings with\n  Self-Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural network architecture for learning vector representations\nof hotels. Unlike previous works, which typically only use user click\ninformation for learning item embeddings, we propose a framework that combines\nseveral sources of data, including user clicks, hotel attributes (e.g.,\nproperty type, star rating, average user rating), amenity information (e.g.,\nthe hotel has free Wi-Fi or free breakfast), and geographic information. During\nmodel training, a joint embedding is learned from all of the above information.\nWe show that including structured attributes about hotels enables us to make\nbetter predictions in a downstream task than when we rely exclusively on click\ndata. We train our embedding model on more than 40 million user click sessions\nfrom a leading online travel platform and learn embeddings for more than one\nmillion hotels. Our final learned embeddings integrate distinct sub-embeddings\nfor user clicks, hotel attributes, and geographic information, providing an\ninterpretable representation that can be used flexibly depending on the\napplication. We show empirically that our model generates high-quality\nrepresentations that boost the performance of a hotel recommendation system in\naddition to other applications. An important advantage of the proposed neural\nmodel is that it addresses the cold-start problem for hotels with insufficient\nhistorical click information by incorporating additional hotel attributes which\nare available for all hotels.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 23:47:55 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Sadeghian", "Ali", ""], ["Minaee", "Shervin", ""], ["Partalas", "Ioannis", ""], ["Li", "Xinxin", ""], ["Wang", "Daisy Zhe", ""], ["Cowan", "Brooke", ""]]}, {"id": "1910.04176", "submitter": "Varun Kumar", "authors": "Varun Kumar, Hadrien Glaude, Cyprien de Lichy, William Campbell", "title": "A Closer Look At Feature Space Data Augmentation For Few-Shot Intent\n  Classification", "comments": "Accepted at Deep Learning for low-resource NLP workshop @ EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  New conversation topics and functionalities are constantly being added to\nconversational AI agents like Amazon Alexa and Apple Siri. As data collection\nand annotation is not scalable and is often costly, only a handful of examples\nfor the new functionalities are available, which results in poor generalization\nperformance. We formulate it as a Few-Shot Integration (FSI) problem where a\nfew examples are used to introduce a new intent. In this paper, we study six\nfeature space data augmentation methods to improve classification performance\nin FSI setting in combination with both supervised and unsupervised\nrepresentation learning methods such as BERT. Through realistic experiments on\ntwo public conversational datasets, SNIPS, and the Facebook Dialog corpus, we\nshow that data augmentation in feature space provides an effective way to\nimprove intent classification performance in few-shot setting beyond\ntraditional transfer learning approaches. In particular, we show that (a)\nupsampling in latent space is a competitive baseline for feature space\naugmentation (b) adding the difference between two examples to a new example is\na simple yet effective data augmentation method.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 18:00:04 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Kumar", "Varun", ""], ["Glaude", "Hadrien", ""], ["de Lichy", "Cyprien", ""], ["Campbell", "William", ""]]}, {"id": "1910.04658", "submitter": "Rameshwar Pratap", "authors": "Rameshwar Pratap, Debajyoti Bera, Karthik Revanuru", "title": "Efficient Sketching Algorithm for Sparse Binary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancement of the WWW, IOT, social network, e-commerce, etc. have\ngenerated a large volume of data. These datasets are mostly represented by high\ndimensional and sparse datasets. Many fundamental subroutines of common data\nanalytic tasks such as clustering, classification, ranking, nearest neighbour\nsearch, etc. scale poorly with the dimension of the dataset. In this work, we\naddress this problem and propose a sketching (alternatively, dimensionality\nreduction) algorithm -- $\\binsketch$ (Binary Data Sketch) -- for sparse binary\ndatasets. $\\binsketch$ preserves the binary version of the dataset after\nsketching and maintains estimates for multiple similarity measures such as\nJaccard, Cosine, Inner-Product similarities, and Hamming distance, on the same\nsketch. We present a theoretical analysis of our algorithm and complement it\nwith extensive experimentation on several real-world datasets. We compare the\nperformance of our algorithm with the state-of-the-art algorithms on the task\nof mean-square-error and ranking. Our proposed algorithm offers a comparable\naccuracy while suggesting a significant speedup in the dimensionality reduction\ntime, with respect to the other candidate algorithms. Our proposal is simple,\neasy to implement, and therefore can be adopted in practice.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 15:43:11 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Pratap", "Rameshwar", ""], ["Bera", "Debajyoti", ""], ["Revanuru", "Karthik", ""]]}, {"id": "1910.04792", "submitter": "Shruti Jadon", "authors": "Shruti Jadon, Mahmood Jasim", "title": "Unsupervised video summarization framework using keyframe extraction and\n  video skimming", "comments": "5 pages, 3 figures. Technical Report", "journal-ref": "2020 IEEE 5th International Conference on Computing Communication\n  and Automation (ICCCA)", "doi": "10.1109/ICCCA49541.2020.9250764", "report-no": "140 - 145", "categories": "cs.IR cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video is one of the robust sources of information and the consumption of\nonline and offline videos has reached an unprecedented level in the last few\nyears. A fundamental challenge of extracting information from videos is a\nviewer has to go through the complete video to understand the context, as\nopposed to an image where the viewer can extract information from a single\nframe. Apart from context understanding, it almost impossible to create a\nuniversal summarized video for everyone, as everyone has their own bias of\nkeyframe, e.g; In a soccer game, a coach person might consider those frames\nwhich consist of information on player placement, techniques, etc; however, a\nperson with less knowledge about a soccer game, will focus more on frames which\nconsist of goals and score-board. Therefore, if we were to tackle problem video\nsummarization through a supervised learning path, it will require extensive\npersonalized labeling of data. In this paper, we attempt to solve video\nsummarization through unsupervised learning by employing traditional\nvision-based algorithmic methodologies for accurate feature extraction from\nvideo frames. We have also proposed a deep learning-based feature extraction\nfollowed by multiple clustering methods to find an effective way of summarizing\na video by interesting key-frame extraction. We have compared the performance\nof these approaches on the SumMe dataset and showcased that using deep\nlearning-based feature extraction has been proven to perform better in case of\ndynamic viewpoint videos.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 18:14:48 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 18:27:25 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Jadon", "Shruti", ""], ["Jasim", "Mahmood", ""]]}, {"id": "1910.04865", "submitter": "Adewale Akinfaderin", "authors": "Adewale Akinfaderin and Olamilekan Wahab", "title": "NASS-AI: Towards Digitization of Parliamentary Bills using Document\n  Level Embedding and Bidirectional Long Short-Term Memory", "comments": "Presented at NeurIPS 2019 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been several reports in the Nigerian and International media about\nthe Senators and House of Representative Members of the Nigerian National\nAssembly (NASS) being the highest paid in the world. Despite this high-level of\nparliamentary compensation and a lack of oversight, most of the legislative\nduties like bills introduced and vote proceedings are shrouded in mystery\nwithout an open and annotated corpus. In this paper, we present results from\nongoing research on the categorization of bills introduced in the Nigerian\nparliament since the fourth republic (1999 - 2018). For this task, we employed\na multi-step approach which involves extracting text from scanned and embedded\npdfs with low to medium quality using Optical Character Recognition (OCR) tools\nand labeling them into eight categories. We investigate the performance of\ndocument level embedding for feature representation of the extracted texts\nbefore using a Bidirectional Long Short-Term Memory (Bi-LSTM) for our\nclassifier. The performance was further compared with other feature\nrepresentation and machine learning techniques. We believe that these results\nare well-positioned to have a substantial impact on the quest to meet the basic\nopen data charter principles.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 00:39:02 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Akinfaderin", "Adewale", ""], ["Wahab", "Olamilekan", ""]]}, {"id": "1910.04927", "submitter": "Gong Cheng", "authors": "Tianshuo Zhou, Ziyang Li, Gong Cheng, Jun Wang, Yu'Ang Wei", "title": "GREASE: A Generative Model for Relevance Search over Knowledge Graphs", "comments": "9 pages, accepted to WSDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relevance search is to find top-ranked entities in a knowledge graph (KG)\nthat are relevant to a query entity. Relevance is ambiguous, particularly over\na schema-rich KG like DBpedia which supports a wide range of different\nsemantics of relevance based on numerous types of relations and attributes. As\nusers may lack the expertise to formalize the desired semantics, supervised\nmethods have emerged to learn the hidden user-defined relevance from\nuser-provided examples. Along this line, in this paper we propose a novel\ngenerative model over KGs for relevance search, named GREASE. The model applies\nto meta-path based relevance where a meta-path characterizes a particular type\nof semantics of relating the query entity to answer entities. It is also\nextended to support properties that constrain answer entities. Extensive\nexperiments on two large-scale KGs demonstrate that GREASE has advanced the\nstate of the art in effectiveness, expressiveness, and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 01:13:51 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Zhou", "Tianshuo", ""], ["Li", "Ziyang", ""], ["Cheng", "Gong", ""], ["Wang", "Jun", ""], ["Wei", "Yu'Ang", ""]]}, {"id": "1910.04964", "submitter": "Xin Wang", "authors": "Wenwu Zhu, Xin Wang, Hongzhi Li", "title": "Multi-modal Deep Analysis for Multimedia", "comments": "25 pages, 39 figures, IEEE Transactions on Circuits and Systems for\n  Video Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2019.2940647", "report-no": null, "categories": "cs.MM cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of Internet and multimedia services in the past\ndecade, a huge amount of user-generated and service provider-generated\nmultimedia data become available. These data are heterogeneous and multi-modal\nin nature, imposing great challenges for processing and analyzing them.\nMulti-modal data consist of a mixture of various types of data from different\nmodalities such as texts, images, videos, audios etc. In this article, we\npresent a deep and comprehensive overview for multi-modal analysis in\nmultimedia. We introduce two scientific research problems, data-driven\ncorrelational representation and knowledge-guided fusion for multimedia\nanalysis. To address the two scientific problems, we investigate them from the\nfollowing aspects: 1) multi-modal correlational representation: multi-modal\nfusion of data across different modalities, and 2) multi-modal data and\nknowledge fusion: multi-modal fusion of data with domain knowledge. More\nspecifically, on data-driven correlational representation, we highlight three\nimportant categories of methods, such as multi-modal deep representation,\nmulti-modal transfer learning, and multi-modal hashing. On knowledge-guided\nfusion, we discuss the approaches for fusing knowledge with data and four\nexemplar applications that require various kinds of domain knowledge, including\nmulti-modal visual question answering, multi-modal video summarization,\nmulti-modal visual pattern mining and multi-modal recommendation. Finally, we\nbring forward our insights and future research directions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 04:21:36 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 08:42:13 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Zhu", "Wenwu", ""], ["Wang", "Xin", ""], ["Li", "Hongzhi", ""]]}, {"id": "1910.05059", "submitter": "Erion \\c{C}ano", "authors": "Erion \\c{C}ano and Ond\\v{r}ej Bojar", "title": "Keyphrase Generation: A Multi-Aspect Survey", "comments": "10 pages, 5 tables. Published in proceedings of FRUCT 2019, the 25th\n  Conference of the Open Innovations Association FRUCT, Helsinki, Finland", "journal-ref": null, "doi": "10.23919/FRUCT48121.2019.8981519", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extractive keyphrase generation research has been around since the nineties,\nbut the more advanced abstractive approach based on the encoder-decoder\nframework and sequence-to-sequence learning has been explored only recently. In\nfact, more than a dozen of abstractive methods have been proposed in the last\nthree years, producing meaningful keyphrases and achieving state-of-the-art\nscores. In this survey, we examine various aspects of the extractive keyphrase\ngeneration methods and focus mostly on the more recent abstractive methods that\nare based on neural networks. We pay particular attention to the mechanisms\nthat have driven the perfection of the later. A huge collection of scientific\narticle metadata and the corresponding keyphrases is created and released for\nthe research community. We also present various keyphrase generation and text\nsummarization research patterns and trends of the last two decades.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 10:03:46 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["\u00c7ano", "Erion", ""], ["Bojar", "Ond\u0159ej", ""]]}, {"id": "1910.05189", "submitter": "Pan Li", "authors": "Pan Li, Alexander Tuzhilin", "title": "DDTCDR: Deep Dual Transfer Cross Domain Recommendation", "comments": "Accepted to WSDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross domain recommender systems have been increasingly valuable for helping\nconsumers identify the most satisfying items from different categories.\nHowever, previously proposed cross-domain models did not take into account\nbidirectional latent relations between users and items. In addition, they do\nnot explicitly model information of user and item features, while utilizing\nonly user ratings information for recommendations. To address these concerns,\nin this paper we propose a novel approach to cross-domain recommendations based\non the mechanism of dual learning that transfers information between two\nrelated domains in an iterative manner until the learning process stabilizes.\nWe develop a novel latent orthogonal mapping to extract user preferences over\nmultiple domains while preserving relations between users across different\nlatent spaces. Combining with autoencoder approach to extract the latent\nessence of feature information, we propose Deep Dual Transfer Cross Domain\nRecommendation (DDTCDR) model to provide recommendations in respective domains.\nWe test the proposed method on a large dataset containing three domains of\nmovies, book and music items and demonstrate that it consistently and\nsignificantly outperforms several state-of-the-art baselines and also classical\ntransfer learning approaches.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 13:51:20 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Li", "Pan", ""], ["Tuzhilin", "Alexander", ""]]}, {"id": "1910.05242", "submitter": "Zeman Shao", "authors": "Zeman Shao, Runyu Mao and Fengqing Zhu", "title": "Semi-Automatic Crowdsourcing Tool for Online Food Image Collection and\n  Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing dietary intake accurately remains an open and challenging research\nproblem. In recent years, image-based approaches have been developed to\nautomatically estimate food intake by capturing eat occasions with mobile\ndevices and wearable cameras. To build a reliable machine-learning models that\ncan automatically map pixels to calories, successful image-based systems need\nlarge collections of food images with high quality groundtruth labels to\nimprove the learned models. In this paper, we introduce a semi-automatic system\nfor online food image collection and annotation. Our system consists of a web\ncrawler, an automatic food detection method and a web-based crowdsoucing tool.\nThe web crawler is used to download large sets of online food images based on\nthe given food labels. Since not all retrieved images contain foods, we\nintroduce an automatic food detection method to remove irrelevant images. We\ndesigned a web-based crowdsourcing tool to assist the crowd or human annotators\nto locate and label all the foods in the images. The proposed semi-automatic\nonline food image collection system can be used to build large food image\ndatasets with groundtruth labels efficiently from scratch.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 15:20:48 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 18:22:32 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Shao", "Zeman", ""], ["Mao", "Runyu", ""], ["Zhu", "Fengqing", ""]]}, {"id": "1910.05243", "submitter": "Md. Mirajul Islam", "authors": "Sharmin Akther Purabi, Rayhan Rashed, Md. Mirajul Islam, Md. Nahiyan\n  Uddin, Mahmuda Naznin, and A. B. M. Alim Al Islam", "title": "As You Are, So Shall You Move Your Head: A System-Level Analysis between\n  Head Movements and Corresponding Traits and Emotions", "comments": "9 pages, 7 figures, NSysS 2019", "journal-ref": null, "doi": "10.1145/3362966.3362985", "report-no": null, "categories": "cs.HC cs.IR cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying physical traits and emotions based on system-sensed physical\nactivities is a challenging problem in the realm of human-computer interaction.\nOur work contributes in this context by investigating an underlying connection\nbetween head movements and corresponding traits and emotions. To do so, we\nutilize a head movement measuring device called eSense, which gives\nacceleration and rotation of a head. Here, first, we conduct a thorough study\nover head movement data collected from 46 persons using eSense while inducing\nfive different emotional states over them in isolation. Our analysis reveals\nseveral new head movement based findings, which in turn, leads us to a novel\nunified solution for identifying different human traits and emotions through\nexploiting machine learning techniques over head movement data. Our analysis\nconfirms that the proposed solution can result in high accuracy over the\ncollected data. Accordingly, we develop an integrated unified solution for\nreal-time emotion and trait identification using head movement data leveraging\noutcomes of our analysis.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 15:22:37 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Purabi", "Sharmin Akther", ""], ["Rashed", "Rayhan", ""], ["Islam", "Md. Mirajul", ""], ["Uddin", "Md. Nahiyan", ""], ["Naznin", "Mahmuda", ""], ["Islam", "A. B. M. Alim Al", ""]]}, {"id": "1910.05428", "submitter": "Asif Imran", "authors": "Asif Imran, Tevfik Kosar", "title": "Design Smell Analysis for Developing and Established Open Source Java\n  Software", "comments": "none", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software design smells are design attributes which violate the fundamental\ndesign principles. Design smells are a key cause of design debt. Although the\nactivities of design smell identification and measurement are predominantly\nconsidered in current literature, those which identify and communicate which\ndesign smells occur more frequently in newly developing software and which ones\nare more dominant in established software have been studied to a limited\nextent. This research describes a mechanism for identifying the design smells\nthat are more prevalent in developing and established software respectively. A\ntool is provided which is used for design smell detection by analyzing large\nvolumes of source code. More specifically, 164,609 Lines of Code (LoC) and\n5,712 class files of six developing and 244,930 LoC and 12,048 class files of\nfive established open-source Java software are analyzed. Obtained results show\nthat out of the 4,020 occurrences of smells that were made for nine preselected\ntypes of design smells, 1,643 design smells were detected for developing\nsoftware, which mainly consisted of four specific types of smells. For\nestablished software, 2,397 design smells were observed which predominantly\nconsisted of four other types of smells. The remaining design smell was equally\nprevalent in both developing and established software. Desirable precision\nvalues ranging from 72.9% to 84.1% were obtained for the tool.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 22:11:31 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Imran", "Asif", ""], ["Kosar", "Tevfik", ""]]}, {"id": "1910.05552", "submitter": "Zekun Li", "authors": "Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, Liang Wang", "title": "Fi-GNN: Modeling Feature Interactions via Graph Neural Networks for CTR\n  Prediction", "comments": "10 pages, accepted by the 2019 Conference on Information and\n  Knowledge Management (CIKM-2019)", "journal-ref": null, "doi": "10.1145/3357384.3357951", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) prediction is an essential task in web applications\nsuch as online advertising and recommender systems, whose features are usually\nin multi-field form. The key of this task is to model feature interactions\namong different feature fields. Recently proposed deep learning based models\nfollow a general paradigm: raw sparse input multi-filed features are first\nmapped into dense field embedding vectors, and then simply concatenated\ntogether to feed into deep neural networks (DNN) or other specifically designed\nnetworks to learn high-order feature interactions. However, the simple\n\\emph{unstructured combination} of feature fields will inevitably limit the\ncapability to model sophisticated interactions among different fields in a\nsufficiently flexible and explicit fashion.\n  In this work, we propose to represent the multi-field features in a graph\nstructure intuitively, where each node corresponds to a feature field and\ndifferent fields can interact through edges. The task of modeling feature\ninteractions can be thus converted to modeling node interactions on the\ncorresponding graph. To this end, we design a novel model Feature Interaction\nGraph Neural Networks (Fi-GNN). Taking advantage of the strong representative\npower of graphs, our proposed model can not only model sophisticated feature\ninteractions in a flexible and explicit fashion, but also provide good model\nexplanations for CTR prediction. Experimental results on two real-world\ndatasets show its superiority over the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 11:33:05 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 05:51:11 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Li", "Zekun", ""], ["Cui", "Zeyu", ""], ["Wu", "Shu", ""], ["Zhang", "Xiaoyu", ""], ["Wang", "Liang", ""]]}, {"id": "1910.05755", "submitter": "Himan Abdollahpouri", "authors": "Himan Abdollahpouri, Masoud Mansoury, Robin Burke and Bamshad Mobasher", "title": "The Impact of Popularity Bias on Fairness and Calibration in\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been a growing interest in fairness-aware recommender\nsystems, including fairness in providing consistent performance across\ndifferent users or groups of users. A recommender system could be considered\nunfair if the recommendations do not fairly represent the tastes of a certain\ngroup of users while other groups receive recommendations that are consistent\nwith their preferences. In this paper, we use a metric called miscalibration\nfor measuring how a recommendation algorithm is responsive to users' true\npreferences and we consider how various algorithms may result in different\ndegrees of miscalibration. A well-known type of bias in recommendation is\npopularity bias where few popular items are over-represented in\nrecommendations, while the majority of other items do not get significant\nexposure. We conjecture that popularity bias is one important factor leading to\nmiscalibration in recommendation. Our experimental results using two real-world\ndatasets show that there is a strong correlation between how different user\ngroups are affected by algorithmic popularity bias and their level of interest\nin popular items. Moreover, we show algorithms with greater popularity bias\namplification tend to have greater miscalibration.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 14:08:50 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 01:00:32 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 00:48:59 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Abdollahpouri", "Himan", ""], ["Mansoury", "Masoud", ""], ["Burke", "Robin", ""], ["Mobasher", "Bamshad", ""]]}, {"id": "1910.06109", "submitter": "Asif Imran", "authors": "Asif Imran, Tevfik Kosar", "title": "Software Sustainability: A Systematic Literature Review and\n  Comprehensive Analysis", "comments": "none", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CY cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software Engineering is a constantly evolving subject area that faces new\nchallenges every day as it tries to automate newer business processes. One of\nthe key challenges to the success of a software solution is attaining\nsustainability. The inability of numerous software to sustain for the desired\ntime-length is caused by limited consideration given towards sustainability\nduring the stages of software development. This review aims to present a\ndetailed and inclusive study covering both the technical and non-technical\nchallenges and approaches of software sustainability. A systematic and\ncomprehensive literature review was conducted based on 107 relevant studies\nthat were selected using the Evidence-Based Software Engineering (EBSE)\ntechnique. The study showed that sustainability can be achieved by conducting\nspecific activities at the technical and non-technical levels. The technical\nlevel consists of software design, coding, and user experience attributes. The\nnon-technical level consists of documentation, sustainability manifestos,\ntraining of software engineers, funding software projects, and leadership\nskills of project managers to achieve sustainability. This paper groups the\nexisting research efforts based on the above aspects. Next, how those aspects\naffect open and closed source software is tabulated. Based on the findings of\nthis review, it is seen that both technical and non-technical sustainability\naspects are equally important, taking one into contention and ignoring the\nother will threaten the sustenance of software products.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 00:54:13 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Imran", "Asif", ""], ["Kosar", "Tevfik", ""]]}, {"id": "1910.06169", "submitter": "Giorgio Vinciguerra", "authors": "Paolo Ferragina and Giorgio Vinciguerra", "title": "The PGM-index: a multicriteria, compressed and learned approach to data\n  indexing", "comments": "We remark to the reader that this paper is an extended and improved\n  version of our previous paper titled \"Superseding traditional indexes by\n  orchestrating learning and geometry\" (arXiv:1903.00507)", "journal-ref": "PVLDB, 13(8): 1162-1175, 2020", "doi": "10.14778/3389133.3389135", "report-no": null, "categories": "cs.DS cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent introduction of learned indexes has shaken the foundations of the\ndecades-old field of indexing data structures. Combining, or even replacing,\nclassic design elements such as B-tree nodes with machine learning models has\nproven to give outstanding improvements in the space footprint and time\nefficiency of data systems. However, these novel approaches are based on\nheuristics, thus they lack any guarantees both in their time and space\nrequirements. We propose the Piecewise Geometric Model index (shortly,\nPGM-index), which achieves guaranteed I/O-optimality in query operations,\nlearns an optimal number of linear models, and its peculiar recursive\nconstruction makes it a purely learned data structure, rather than a hybrid of\ntraditional and learned indexes (such as RMI and FITing-tree). We show that the\nPGM-index improves the space of the FITing-tree by 63.3% and of the B-tree by\nmore than four orders of magnitude, while achieving their same or even better\nquery time efficiency. We complement this result by proposing three variants of\nthe PGM-index. First, we design a compressed PGM-index that further reduces its\nspace footprint by exploiting the repetitiveness at the level of the learned\nlinear models it is composed of. Second, we design a PGM-index that adapts\nitself to the distribution of the queries, thus resulting in the first known\ndistribution-aware learned index to date. Finally, given its flexibility in the\noffered space-time trade-offs, we propose the multicriteria PGM-index that\nefficiently auto-tune itself in a few seconds over hundreds of millions of keys\nto the possibly evolving space-time constraints imposed by the application of\nuse.\n  We remark to the reader that this paper is an extended and improved version\nof our previous paper titled \"Superseding traditional indexes by orchestrating\nlearning and geometry\" (arXiv:1903.00507).\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 14:25:25 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Ferragina", "Paolo", ""], ["Vinciguerra", "Giorgio", ""]]}, {"id": "1910.06213", "submitter": "Felipe Gonz\\'alez", "authors": "Felipe Gonz\\'alez, Yihan Yu, Andrea Figueroa, Claudia L\\'opez, Cecilia\n  Aragon", "title": "Global Reactions to the Cambridge Analytica Scandal: An Inter-Language\n  Social Media Study", "comments": "2019 World Wide Web Conference (WWW '19 Companion)", "journal-ref": null, "doi": "10.1145/3308560.3316456", "report-no": null, "categories": "cs.IR cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Currently, there is a limited understanding of how data privacy concerns vary\nacross the world. The Cambridge Analytica scandal triggered a wide-ranging\ndiscussion on social media about user data collection and use practices. We\nconducted an inter-language study of this online conversation to compare how\npeople speaking different languages react to data privacy breaches. We\ncollected tweets about the scandal written in Spanish and English between April\nand July 2018. We used the Meaning Extraction Method in both datasets to\nidentify their main topics. They reveal a similar emphasis on Zuckerberg's\nhearing in the US Congress and the scandal's impact on political issues.\nHowever, our analysis also shows that while English speakers tend to attribute\nresponsibilities to companies, Spanish speakers are more likely to connect them\nto people. These findings show the potential of inter-language comparisons of\nsocial media data to deepen the understanding of cultural differences in data\nprivacy perspectives.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 15:38:00 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Gonz\u00e1lez", "Felipe", ""], ["Yu", "Yihan", ""], ["Figueroa", "Andrea", ""], ["L\u00f3pez", "Claudia", ""], ["Aragon", "Cecilia", ""]]}, {"id": "1910.06669", "submitter": "Noreen Jamil", "authors": "Bushra Ramzan, Imran Sarwar Bajwa, Noreen Jamil, Farhaan Mirza", "title": "An Intelligent Data Analysis for Hotel Recommendation Systems using\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper presents an intelligent approach to handle heterogeneous and\nlarge-sized data using machine learning to generate true recommendations for\nthe future customers. The Collaborative Filtering (CF) approach is one of the\nmost popular techniques of the RS to generate recommendations. We have proposed\na novel CF recommendation approach in which opinion based sentiment analysis is\nused to achieve hotel feature matrix by polarity identification. Our approach\ncombines lexical analysis, syntax analysis and semantic analysis to understand\nsentiment towards hotel features and the profiling of guest type (solo, family,\ncouple etc). The proposed system recommends hotels based on the hotel features\nand guest type as additional information for personalized recommendation. The\ndeveloped system not only has the ability to handle heterogeneous data using\nbig data Hadoop platform but it also recommend hotel class based on guest type\nusing fuzzy rules. Different experiments are performed over the real world\ndataset obtained from two hotel websites. Moreover, the values of precision and\nrecall and F-measure have been calculated and results are discussed in terms of\nimproved accuracy and response time, significantly better than the traditional\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 11:57:50 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Ramzan", "Bushra", ""], ["Bajwa", "Imran Sarwar", ""], ["Jamil", "Noreen", ""], ["Mirza", "Farhaan", ""]]}, {"id": "1910.06859", "submitter": "Hrishikesh Kulkarni", "authors": "Hrishikesh Kulkarni, P Joshi, P Chande", "title": "Computational Psychology to Embed Emotions into News or Advertisements\n  to Increase Reader Affinity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Readers take decisions about going through the complete news based on many\nfactors. The emotional impact of the news title on reader is one of the most\nimportant factors. Cognitive ergonomics tries to strike the balance between\nwork, product and environment with human needs and capabilities. The utmost\nneed to integrate emotions in the news as well as advertisements cannot be\ndenied. The idea is that news or advertisement should be able to engage the\nreader on emotional and behavioral platform. While achieving this objective\nthere is need to learn about reader behavior and use computational psychology\nwhile presenting as well as writing news or advertisements. This paper based on\nMachine Learning, tries to map behavior of the reader with the\nnews/advertisements and also provide inputs for affective value for building\npersonalized news or advertisements presentations. The affective value of the\nnews is determined and news artifacts are mapped to reader. The algorithm\nsuggests the most suitable news for readers while understanding emotional\ntraits required for personalization. This work can be used to improve reader\nsatisfaction through embedding emotions in the reading material and\nprioritizing news presentations. It can be used to map personal reading\nmaterial range, personalized programs and ranking programs, advertisements with\nreference to individuals.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 16:12:21 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Kulkarni", "Hrishikesh", ""], ["Joshi", "P", ""], ["Chande", "P", ""]]}, {"id": "1910.06954", "submitter": "Marius C\\u{a}t\\u{a}lin Iordan", "authors": "Marius C\\u{a}t\\u{a}lin Iordan, Tyler Giallanza, Cameron T. Ellis,\n  Nicole M. Beckage, Jonathan D. Cohen", "title": "Context Matters: Recovering Human Semantic Structure from Machine\n  Learning Analysis of Large-Scale Text Corpora", "comments": "Main Text: 35 pages, 5 figures; Supplemental: 21 pages, 11 figures, 6\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying machine learning algorithms to large-scale, text-based corpora\n(embeddings) presents a unique opportunity to investigate at scale how human\nsemantic knowledge is organized and how people use it to judge fundamental\nrelationships, such as similarity between concepts. However, efforts to date\nhave shown a substantial discrepancy between algorithm predictions and\nempirical judgments. Here, we introduce a novel approach of generating\nembeddings motivated by the psychological theory that semantic context plays a\ncritical role in human judgments. Specifically, we train state-of-the-art\nmachine learning algorithms using contextually-constrained text corpora and\nshow that this greatly improves predictions of similarity judgments and feature\nratings. By improving the correspondence between representations derived using\nembeddings generated by machine learning methods and empirical measurements of\nhuman judgments, the approach we describe helps advance the use of large-scale\ntext corpora to understand the structure of human semantic representations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 17:51:01 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 23:49:12 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 20:41:41 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Iordan", "Marius C\u0103t\u0103lin", ""], ["Giallanza", "Tyler", ""], ["Ellis", "Cameron T.", ""], ["Beckage", "Nicole M.", ""], ["Cohen", "Jonathan D.", ""]]}, {"id": "1910.07099", "submitter": "Jing Zhang", "authors": "Hong Wen and Jing Zhang and Yuan Wang and Fuyu Lv and Wentian Bao and\n  Quan Lin and Keping Yang", "title": "Entire Space Multi-Task Modeling via Post-Click Behavior Decomposition\n  for Conversion Rate Prediction", "comments": "10page, 7 figures. Accepted by SIGIR 2020. The source code will be\n  released at https://github.com/chaimi2013/ESM2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender system, as an essential part of modern e-commerce, consists of\ntwo fundamental modules, namely Click-Through Rate (CTR) and Conversion Rate\n(CVR) prediction. While CVR has a direct impact on the purchasing volume, its\nprediction is well-known challenging due to the Sample Selection Bias (SSB) and\nData Sparsity (DS) issues. Although existing methods, typically built on the\nuser sequential behavior path ``impression$\\to$click$\\to$purchase'', is\neffective for dealing with SSB issue, they still struggle to address the DS\nissue due to rare purchase training samples. Observing that users always take\nseveral purchase-related actions after clicking, we propose a novel idea of\npost-click behavior decomposition. Specifically, disjoint purchase-related\nDeterministic Action (DAction) and Other Action (OAction) are inserted between\nclick and purchase in parallel, forming a novel user sequential behavior graph\n``impression$\\to$click$\\to$D(O)Action$\\to$purchase''. Defining model on this\ngraph enables to leverage all the impression samples over the entire space and\nextra abundant supervised signals from D(O)Action, which will effectively\naddress the SSB and DS issues together. To this end, we devise a novel deep\nrecommendation model named Elaborated Entire Space Supervised Multi-task Model\n($ESM^{2}$). According to the conditional probability rule defined on the\ngraph, it employs multi-task learning to predict some decomposed sub-targets in\nparallel and compose them sequentially to formulate the final CVR. Extensive\nexperiments on both offline and online environments demonstrate the superiority\nof $ESM^{2}$ over state-of-the-art models. The source code and dataset will be\nreleased.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 23:15:42 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 00:44:54 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Wen", "Hong", ""], ["Zhang", "Jing", ""], ["Wang", "Yuan", ""], ["Lv", "Fuyu", ""], ["Bao", "Wentian", ""], ["Lin", "Quan", ""], ["Yang", "Keping", ""]]}, {"id": "1910.07130", "submitter": "Junhao Wang Junior", "authors": "Junhao Wang, Sacha Levy, Ren Wang, Aayushi Kulshrestha, Reihaneh\n  Rabbany", "title": "SCG: Spotting Coordinated Groups in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent events have led to a burgeoning awareness on the misuse of social\nmedia sites to affect political events, sway public opinion, and confuse the\nvoters. Such serious, hostile mass manipulation has motivated a large body of\nworks on bots/troll detection and fake news detection, which mostly focus on\nclassifying at the user level based on the content generated by the users. In\nthis study, we jointly analyze the connections among the users, as well as the\ncontent generated by them to Spot Coordinated Groups (SCG), sets of users that\nare likely to be organized towards impacting the general discourse. Given their\ntiny size (relative to the whole data), detecting these groups is\ncomputationally hard. Our proposed method detects these tiny-clusters\neffectively and efficiently. We deploy our SCG method to summarize and explain\nthe coordinated groups on Twitter around the 2019 Canadian Federal Elections,\nby analyzing over 60 thousand user accounts with 3.4 million followership\nconnections, and 1.3 million unique hashtags in the content of their tweets.\nThe users in the detected coordinated groups are over 4x more likely to get\nsuspended, whereas the hashtags which characterize their creed are linked to\nmisinformation campaigns.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 02:05:26 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 06:23:36 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2019 04:27:22 GMT"}, {"version": "v4", "created": "Tue, 29 Oct 2019 09:42:31 GMT"}, {"version": "v5", "created": "Wed, 2 Sep 2020 02:01:28 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Wang", "Junhao", ""], ["Levy", "Sacha", ""], ["Wang", "Ren", ""], ["Kulshrestha", "Aayushi", ""], ["Rabbany", "Reihaneh", ""]]}, {"id": "1910.07233", "submitter": "Varsha Pathak", "authors": "Varsha Pathak, Manish Joshi", "title": "Rule based Approach for Word Normalization by resolving Transcription\n  Ambiguity in Transliterated Search Queries", "comments": "11 pages, 2 figures, 2 tables, Unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Query term matching with document term matching is the basic function of any\nbest effort Information Retrieval models like Vector Space Model. In our\nproblem of SMS based Information Systems we expect common people to participate\nin information search. Our system allows mobile users to formulate their\nqueries in their own words, own transliteration style and spelling formation.\nTo achieve this flexibility we have resolved the term level ambiguity due to\ninherent transcription noise in user query terms. We have developed a rule\nbased approach to select most relevantly close standard term for each noisy\nterm in the user query. We have used four different versions of the rule based\nalgorithm with variation in the rule set. We have formulated this rule set\nincluding the basic Levenshtein minimum edit distance algorithm for term\nmatching. This paper presents the experiments and corresponding results of\nMarathi and Hindi language literature information system. We have experimented\non Marathi and Hindi literature which include songs, gazals, powadas, bharud\nand other types in a standard transliteration form like ITRANS.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 09:23:17 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Pathak", "Varsha", ""], ["Joshi", "Manish", ""]]}, {"id": "1910.07295", "submitter": "Yuta Saito", "authors": "Yuta Saito", "title": "Offline Recommender Learning Meets Unsupervised Domain Adaptation", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the offline recommender learning problem in the presence of\nselection bias in rating feedback. A current promising solution to address the\nbias is to use the propensity score. However, the performance of the existing\npropensity-based methods can significantly suffer from propensity estimation\nbias. To solve the problem, we formulate the recommendation with selection bias\nas unsupervised domain adaptation and derive a propensity-independent\ngeneralization error bound. We further propose a novel algorithm that minimizes\nthe bound via adversarial learning. Our theory and algorithm do not depend on\npropensity scores, and thus can result in a well-performing rating predictor\nwithout requiring the true propensity information. Empirical evaluation\ndemonstrates the effectiveness and real-world applicability of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 11:41:17 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 16:36:09 GMT"}, {"version": "v3", "created": "Fri, 13 Dec 2019 21:04:32 GMT"}, {"version": "v4", "created": "Wed, 29 Jan 2020 13:07:20 GMT"}, {"version": "v5", "created": "Thu, 18 Jun 2020 07:56:23 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Saito", "Yuta", ""]]}, {"id": "1910.07351", "submitter": "Naman Jain", "authors": "Monarch Parmar, Naman Jain, Pranjali Jain, P Jayakrishna Sahit, Soham\n  Pachpande, Shruti Singh and Mayank Singh", "title": "NLPExplorer: Exploring the Universe of NLP Papers", "comments": "Submitted in ECIR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the current research trends, problems, and their innovative\nsolutions remains a bottleneck due to the ever-increasing volume of scientific\narticles. In this paper, we propose NLPExplorer, a completely automatic portal\nfor indexing, searching, and visualizing Natural Language Processing (NLP)\nresearch volume. NLPExplorer presents interesting insights from papers,\nauthors, venues, and topics. In contrast to previous topic modelling based\napproaches, we manually curate five course-grained non-exclusive topical\ncategories namely Linguistic Target (Syntax, Discourse, etc.), Tasks (Tagging,\nSummarization, etc.), Approaches (unsupervised, supervised, etc.), Languages\n(English, Chinese,etc.) and Dataset types (news, clinical notes, etc.). Some of\nthe novel features include a list of young popular authors, popular URLs, and\ndatasets, a list of topically diverse papers and recent popular papers. Also,\nit provides temporal statistics such as yearwise popularity of topics,\ndatasets, and seminal papers. To facilitate future research and system\ndevelopment, we make all the processed datasets accessible through API calls.\nThe current system is available at http://nlpexplorer.org.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 13:57:15 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Parmar", "Monarch", ""], ["Jain", "Naman", ""], ["Jain", "Pranjali", ""], ["Sahit", "P Jayakrishna", ""], ["Pachpande", "Soham", ""], ["Singh", "Shruti", ""], ["Singh", "Mayank", ""]]}, {"id": "1910.07394", "submitter": "Thassilo Gadermaier", "authors": "Thassilo Gadermaier and Gerhard Widmer", "title": "A Study of Annotation and Alignment Accuracy for Performance Comparison\n  in Complex Orchestral Music", "comments": null, "journal-ref": "Proceedings of the 20th International Society for Music\n  Information Retrieval Conference, (ISMIR) 2019, Delft, The Netherlands,\n  November 4-8, 2019, pages: 769--775", "doi": null, "report-no": null, "categories": "cs.MM cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantitative analysis of commonalities and differences between recorded music\nperformances is an increasingly common task in computational musicology. A\ntypical scenario involves manual annotation of different recordings of the same\npiece along the time dimension, for comparative analysis of, e.g., the musical\ntempo, or for mapping other performance-related information between\nperformances. This can be done by manually annotating one reference\nperformance, and then automatically synchronizing other performances, using\naudio-to-audio alignment algorithms. In this paper we address several questions\nrelated to those tasks. First, we analyze different annotations of the same\nmusical piece, quantifying timing deviations between the respective human\nannotators. A statistical evaluation of the marker time stamps will provide (a)\nan estimate of the expected timing precision of human annotations and (b) a\nground truth for subsequent automatic alignment experiments. We then carry out\na systematic evaluation of different audio features for audio-to-audio\nalignment, quantifying the degree of alignment accuracy that can be achieved,\nand relate this to the results from the annotation study.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 14:59:59 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Gadermaier", "Thassilo", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1910.07612", "submitter": "Anoosheh Heidarzadeh", "authors": "Anoosheh Heidarzadeh and Fatemeh Kazemi and Alex Sprintson", "title": "The Role of Coded Side Information in Single-Server Private Information\n  Retrieval", "comments": "40 pages; This work was presented in part at the 2018 IEEE\n  Information Theory Workshop, Guangzhou, China, November 2018, and the 2019\n  IEEE International Symposium on Information Theory, Paris, France, July 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the role of coded side information in single-server Private\nInformation Retrieval (PIR). An instance of the single-server PIR problem\nincludes a server that stores a database of $K$ independently and uniformly\ndistributed messages, and a user who wants to retrieve one of these messages\nfrom the server. We consider settings in which the user initially has access to\na coded side information which includes a linear combination of a subset of $M$\nmessages in the database. We assume that the identities of the $M$ messages\nthat form the support set of the coded side information as well as the coding\ncoefficients are initially unknown to the server. We consider two different\nmodels, depending on whether the support set of the coded side information\nincludes the requested message or not. We also consider the following two\nprivacy requirements: (i) the identities of both the demand and the support set\nof the coded side information need to be protected, or (ii) only the identity\nof the demand needs to be protected. For each model and for each of the privacy\nrequirements, we consider the problem of designing a protocol for generating\nthe user's query and the server's answer that enables the user to decode the\nmessage they need while satisfying the privacy requirement. We characterize the\n(scalar-linear) capacity of each setting, defined as the ratio of the number of\ninformation bits in a message to the minimum number of information bits\ndownloaded from the server over all (scalar-linear) protocols that satisfy the\nprivacy condition. Our converse proofs rely on new information-theoretic\narguments---tailored to the setting of single-server PIR and different from the\ncommonly-used techniques in multi-server PIR settings. We also present novel\ncapacity-achieving scalar-linear protocols for each of the settings being\nconsidered.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 21:06:37 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Heidarzadeh", "Anoosheh", ""], ["Kazemi", "Fatemeh", ""], ["Sprintson", "Alex", ""]]}, {"id": "1910.07724", "submitter": "Sagar Verma", "authors": "Sagar Verma and Prince Patel and Angshul Majumdar", "title": "Collaborative Filtering with Label Consistent Restricted Boltzmann\n  Machine", "comments": "6 pages, ICAPR 2017, Code: https://github.com/sagarverma/LC-CFRBM", "journal-ref": null, "doi": "10.1109/ICAPR.2017.8593079", "report-no": null, "categories": "cs.LG cs.IR cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The possibility of employing restricted Boltzmann machine (RBM) for\ncollaborative filtering has been known for about a decade. However, there has\nbeen hardly any work on this topic since 2007. This work revisits the\napplication of RBM in recommender systems. RBM based collaborative filtering\nonly used the rating information; this is an unsupervised architecture. This\nwork adds supervision by exploiting user demographic information and item\nmetadata. A network is learned from the representation layer to the labels\n(metadata). The proposed label consistent RBM formulation improves\nsignificantly on the existing RBM based approach and yield results at par with\nthe state-of-the-art latent factor based models.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 05:58:56 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Verma", "Sagar", ""], ["Patel", "Prince", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1910.07784", "submitter": "Deepanwita Datta", "authors": "Deepanwita Datta", "title": "Indoor Information Retrieval using Lifelog Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying human behaviour through lifelogging has seen an increase in\nattention from researchers over the past decade. The opportunities that\nlifelogging offers are based on the fact that a lifelog, as a \"black box\" of\nour lives, offers rich contextual information, which has been an Achilles heel\nof information discovery. While lifelog data has been put to use in various\ncontexts, its application to indoor environment scenario remains unexplored. In\nthis proposal, I plan to design a method that enables us to capture and record\nindoor lifelog data of a person's life in order to facilitate healthcare\nsystems, emergency response, item tracking etc. To this end, we aim to build an\nIndoor Information Retrieval system that can be queried with natural language\nqueries over lifelog data. Judicious use of the lifelog data for the indoor\napplication may enable us to solve very fundamental but non-avoidable problems\nof our daily life. Analysis of lifelog data coupled with Information Retrieval\nis not only a promising research topic, but the possibility of its indoor\napplication especially for healthcare, lost-item tracking would be an\ninnovative research idea to the best of our knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 09:28:39 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Datta", "Deepanwita", ""]]}, {"id": "1910.07786", "submitter": "Naibo Wang", "authors": "Naibo Wang, Zhiling Luo, Xiya Lyu, Zitong Yang, Jianwei Yin", "title": "Service Wrapper: a system for converting web data into web services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web services are widely used in many areas via callable APIs, however, data\nare not always available in this way. We always need to get some data from web\npages whose structure is not in order. Many developers use web data extraction\nmethods to generate wrappers to get useful contents from websites and convert\nthem into well-structured files. These methods, however, are designed\nspecifically for professional wrapper program developers and not friendly to\nusers without expertise in this domain. In this work, we construct a service\nwrapper system to convert available data in web pages into web services.\nAdditionally, a set of algorithms are introduced to solve problems in the whole\nconversion process. People can use our system to convert web data into web\nservices with fool-style operations and invoke these services by one simple\nstep, which greatly expands the use of web data. Our cases show the ease of\nuse, high availability, and stability of our system.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 09:29:24 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Wang", "Naibo", ""], ["Luo", "Zhiling", ""], ["Lyu", "Xiya", ""], ["Yang", "Zitong", ""], ["Yin", "Jianwei", ""]]}, {"id": "1910.07792", "submitter": "Xu Chen", "authors": "Xu Chen and Kenan Cui and Ya Zhang and Yanfeng Wang", "title": "Cascading: Association Augmented Sequential Recommendation", "comments": "29 pages,13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, recommendation according to sequential user behaviors has shown\npromising results in many application scenarios. Generally speaking, real-world\nsequential user behaviors usually reflect a hybrid of sequential influences and\nassociation relationships. However, most existing sequential recommendation\nmethods mainly concentrate on sequential relationships while ignoring\nassociation relationships. In this paper, we propose a unified method that\nincorporates item association and sequential relationships for sequential\nrecommendation. Specifically, we encode the item association as relations in\nitem co-occurrence graph and mine it through graph embedding by GCNs. In the\nmeanwhile, we model the sequential relationships through a widely used RNNs\nbased sequential recommendation method named GRU4Rec. The two parts are\nconnected into an end-to-end network with cascading style, which guarantees\nthat representations for item associations and sequential relationships are\nlearned simultaneously and make the learning process maintain low complexity.\nWe perform extensive experiments on three widely used real-world datasets:\nTaoBao, Amazon Instant Video and Amazon Cell Phone and Accessories.\nComprehensive results have shown that the proposed method outperforms several\nstate-of-the-art methods. Furthermore, a qualitative analysis is also provided\nto encourage a better understanding about association relationships in\nsequential recommendation and illustrate the performance gain is exactly from\nitem association.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 09:46:26 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Chen", "Xu", ""], ["Cui", "Kenan", ""], ["Zhang", "Ya", ""], ["Wang", "Yanfeng", ""]]}, {"id": "1910.07897", "submitter": "Jishnu Ray Chowdhury", "authors": "Jishnu Ray Chowdhury, Cornelia Caragea, Doina Caragea", "title": "Keyphrase Extraction from Disaster-related Tweets", "comments": "12 pages, 7 figures", "journal-ref": "In The World Wide Web Conference (WWW '19), Ling Liu and Ryen\n  White (Eds.). ACM, New York, NY, USA, 1555-1566 (2019)", "doi": "10.1145/3308558.3313696", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While keyphrase extraction has received considerable attention in recent\nyears, relatively few studies exist on extracting keyphrases from social media\nplatforms such as Twitter, and even fewer for extracting disaster-related\nkeyphrases from such sources. During a disaster, keyphrases can be extremely\nuseful for filtering relevant tweets that can enhance situational awareness.\nPreviously, joint training of two different layers of a stacked Recurrent\nNeural Network for keyword discovery and keyphrase extraction had been shown to\nbe effective in extracting keyphrases from general Twitter data. We improve the\nmodel's performance on both general Twitter data and disaster-related Twitter\ndata by incorporating contextual word embeddings, POS-tags, phonetics, and\nphonological features. Moreover, we discuss the shortcomings of the often used\nF1-measure for evaluating the quality of predicted keyphrases with respect to\nthe ground truth annotations. Instead of the F1-measure, we propose the use of\nembedding-based metrics to better capture the correctness of the predicted\nkeyphrases. In addition, we also present a novel extension of an\nembedding-based metric. The extension allows one to better control the penalty\nfor the difference in the number of ground-truth and predicted keyphrases\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 13:31:45 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Chowdhury", "Jishnu Ray", ""], ["Caragea", "Cornelia", ""], ["Caragea", "Doina", ""]]}, {"id": "1910.07988", "submitter": "Yue Zhao", "authors": "Yue Zhao, Xuejian Wang, Cheng Cheng, Xueying Ding", "title": "Combining Machine Learning Models using combo Library", "comments": "In Proceedings of Thirty-Fourth AAAI Conference on Artificial\n  Intelligence (AAAI 2020)", "journal-ref": null, "doi": "10.1609/aaai.v34i09.7111", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model combination, often regarded as a key sub-field of ensemble learning,\nhas been widely used in both academic research and industry applications. To\nfacilitate this process, we propose and implement an easy-to-use Python\ntoolkit, combo, to aggregate models and scores under various scenarios,\nincluding classification, clustering, and anomaly detection. In a nutshell,\ncombo provides a unified and consistent way to combine both raw and pretrained\nmodels from popular machine learning libraries, e.g., scikit-learn, XGBoost,\nand LightGBM. With accessibility and robustness in mind, combo is designed with\ndetailed documentation, interactive examples, continuous integration, code\ncoverage, and maintainability check; it can be installed easily through Python\nPackage Index (PyPI) or https://github.com/yzhao062/combo.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 15:00:13 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 21:00:53 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Zhao", "Yue", ""], ["Wang", "Xuejian", ""], ["Cheng", "Cheng", ""], ["Ding", "Xueying", ""]]}, {"id": "1910.08219", "submitter": "Zhiwei Liu", "authors": "Zhiwei Liu, Lei Zheng, Jiawei Zhang, Jiayu Han, Philip S. Yu", "title": "JSCN: Joint Spectral Convolutional Network for Cross Domain\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain recommendation can alleviate the data sparsity problem in\nrecommender systems. To transfer the knowledge from one domain to another, one\ncan either utilize the neighborhood information or learn a direct mapping\nfunction. However, all existing methods ignore the high-order connectivity\ninformation in cross-domain recommendation area and suffer from the\ndomain-incompatibility problem. In this paper, we propose a \\textbf{J}oint\n\\textbf{S}pectral \\textbf{C}onvolutional \\textbf{N}etwork (JSCN) for\ncross-domain recommendation. JSCN will simultaneously operate multi-layer\nspectral convolutions on different graphs, and jointly learn a domain-invariant\nuser representation with a domain adaptive user mapping module. As a result,\nthe high-order comprehensive connectivity information can be extracted by the\nspectral convolutions and the information can be transferred across domains\nwith the domain-invariant user mapping. The domain adaptive user mapping module\ncan help the incompatible domains to transfer the knowledge across each other.\nExtensive experiments on $24$ Amazon rating datasets show the effectiveness of\nJSCN in the cross-domain recommendation, with $9.2\\%$ improvement on recall and\n$36.4\\%$ improvement on MAP compared with state-of-the-art methods. Our code is\navailable online ~\\footnote{https://github.com/JimLiu96/JSCN}.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 01:32:23 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Liu", "Zhiwei", ""], ["Zheng", "Lei", ""], ["Zhang", "Jiawei", ""], ["Han", "Jiayu", ""], ["Yu", "Philip S.", ""]]}, {"id": "1910.08252", "submitter": "Gong Cheng", "authors": "Qingxia Liu, Gong Cheng, Kalpa Gunaratna, Yuzhong Qu", "title": "Entity Summarization: State of the Art and Future Challenges", "comments": "25 pages, accepted by Journal of Web Semantics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing availability of semantic data has substantially enhanced Web\napplications. Semantic data such as RDF data is commonly represented as\nentity-property-value triples. The magnitude of semantic data, in particular\nthe large number of triples describing an entity, could overload users with\nexcessive amounts of information. This has motivated fruitful research on\nautomated generation of summaries for entity descriptions to satisfy users'\ninformation needs efficiently and effectively. We focus on this prominent topic\nof entity summarization, and our research objective is to present the first\ncomprehensive survey of entity summarization research. Rather than separately\nreviewing each method, our contributions include (1) identifying and\nclassifying technical features of existing methods to form a high-level\noverview, (2) identifying and classifying frameworks for combining multiple\ntechnical features adopted by existing methods, (3) collecting known benchmarks\nfor intrinsic evaluation and efforts for extrinsic evaluation, and (4)\nsuggesting research directions for future work. By investigating the\nliterature, we synthesized two hierarchies of techniques. The first hierarchy\ncategories generic technical features into several perspectives: frequency and\ncentrality, informativeness, and diversity and coverage. In the second\nhierarchy we present domain-specific and task-specific technical features,\nincluding the use of domain knowledge, context awareness, and personalization.\nOur review demonstrated that existing methods are mainly unsupervised and they\ncombine multiple technical features using various frameworks: random surfer\nmodels, similarity-based grouping, MMR-like re-ranking, or combinatorial\noptimization. We also found a few deep learning based methods in recent\nresearch.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 04:02:33 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 02:22:59 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Liu", "Qingxia", ""], ["Cheng", "Gong", ""], ["Gunaratna", "Kalpa", ""], ["Qu", "Yuzhong", ""]]}, {"id": "1910.08270", "submitter": "Manirupa Das", "authors": "Manirupa Das, Zhen Wang, Evan Jaffe, Madhuja Chattopadhyay, Eric\n  Fosler-Lussier and Rajiv Ramnath", "title": "Learning to Answer Subjective, Specific Product-Related Queries using\n  Customer Reviews by Adversarial Domain Adaptation", "comments": "8 pages, 1 figure, 6 tables, added additional references to end of\n  section 2.1, removed graphics from referenced works, added to argument in\n  section 2.3 corrected typos, results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online customer reviews on large-scale e-commerce websites, represent a rich\nand varied source of opinion data, often providing subjective qualitative\nassessments of product usage that can help potential customers to discover\nfeatures that meet their personal needs and preferences. Thus they have the\npotential to automatically answer specific queries about products, and to\naddress the problems of answer starvation and answer augmentation on associated\nconsumer Q & A forums, by providing good answer alternatives. In this work, we\nexplore several recently successful neural approaches to modeling sentence\npairs, that could better learn the relationship between questions and ground\ntruth answers, and thus help infer reviews that can best answer a question or\naugment a given answer. In particular, we hypothesize that our adversarial\ndomain adaptation-based approach, due to its ability to additionally learn\ndomain-invariant features from a large number of unlabeled, unpaired\nquestion-review samples, would perform better than our proposed baselines, at\nanswering specific, subjective product-related queries using reviews. We\nvalidate this hypothesis using a small gold standard dataset of question-review\npairs evaluated by human experts, significantly surpassing our chosen\nbaselines. Moreover, our approach, using no labeled question-review sentence\npair data for training, gives performance at par with another method utilizing\nlabeled question-review samples for the same task.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 05:28:46 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 07:12:45 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Das", "Manirupa", ""], ["Wang", "Zhen", ""], ["Jaffe", "Evan", ""], ["Chattopadhyay", "Madhuja", ""], ["Fosler-Lussier", "Eric", ""], ["Ramnath", "Rajiv", ""]]}, {"id": "1910.08288", "submitter": "Xiao Sha", "authors": "Xiao Sha and Zhu Sun and Jie Zhang", "title": "Attentive Knowledge Graph Embedding for Personalized Recommendation", "comments": null, "journal-ref": null, "doi": "10.1016/j.elerap.2021.101071", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KGs) have proven to be effective for high-quality\nrecommendation. Most efforts, however, explore KGs by either extracting\nseparate paths connecting user-item pairs, or iteratively propagating user\npreference over the entire KGs, thus failing to efficiently exploit KGs for\nenhanced recommendation. In this paper, we design a novel attentive knowledge\ngraph embedding (AKGE) framework for recommendation, which sufficiently\nexploits both semantics and topology of KGs in an interaction-specific manner.\nSpecifically, AKGE first automatically extracts high-order subgraphs that link\nuser-item pairs with rich semantics, and then encodes the subgraphs by the\nproposed attentive graph neural network to learn accurate user preference.\nExtensive experiments on three real-world datasets demonstrate that AKGE\nconsistently outperforms state-of-the-art methods. It additionally provides\npotential explanations for the recommendation results.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 07:22:02 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 14:57:09 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 09:23:47 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Sha", "Xiao", ""], ["Sun", "Zhu", ""], ["Zhang", "Jie", ""]]}, {"id": "1910.08292", "submitter": "Sagar Verma", "authors": "Sagar Verma and Sukhad Anand and Chetan Arora and Atul Rai", "title": "Diversity in Fashion Recommendation using Semantic Parsing", "comments": "5 pages, ICIP2018, code:\n  https://github.com/sagarverma/fashion_recommendation_stlstm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Developing recommendation system for fashion images is challenging due to the\ninherent ambiguity associated with what criterion a user is looking at.\nSuggesting multiple images where each output image is similar to the query\nimage on the basis of a different feature or part is one way to mitigate the\nproblem. Existing works for fashion recommendation have used Siamese or Triplet\nnetwork to learn features between a similar pair and a similar-dissimilar\ntriplet respectively. However, these methods do not provide basic information\nsuch as, how two clothing images are similar, or which parts present in the two\nimages make them similar. In this paper, we propose to recommend images by\nexplicitly learning and exploiting part based similarity. We propose a novel\napproach of learning discriminative features from weakly-supervised data by\nusing visual attention over the parts and a texture encoding network. We show\nthat the learned features surpass the state-of-the-art in retrieval task on\nDeepFashion dataset. We then use the proposed model to recommend fashion images\nhaving an explicit variation with respect to similarity of any of the parts.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 07:47:41 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Verma", "Sagar", ""], ["Anand", "Sukhad", ""], ["Arora", "Chetan", ""], ["Rai", "Atul", ""]]}, {"id": "1910.08646", "submitter": "Braddock Gaskill", "authors": "Braddock Gaskill", "title": "The Bitwise Hashing Trick for Personalized Search", "comments": null, "journal-ref": "Applied Artificial Intelligence, Volume 33, 2019 - Issue 9, pages\n  829-837", "doi": "10.1080/08839514.2019.1630961", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Many real world problems require fast and efficient lexical comparison of\nlarge numbers of short text strings. Search personalization is one such domain.\nWe introduce the use of feature bit vectors using the hashing trick for\nimproving relevance in personalized search and other personalization\napplications. We present results of several lexical hashing and comparison\nmethods. These methods are applied to a user's historical behavior and are used\nto predict future behavior. Using a single bit per dimension instead of\nfloating point results in an order of magnitude decrease in data structure\nsize, while preserving or even improving quality. We use real data to simulate\na search personalization task. A simple method for combining bit vectors\ndemonstrates an order of magnitude improvement in compute time on the task with\nonly a small decrease in accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 22:17:15 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Gaskill", "Braddock", ""]]}, {"id": "1910.08692", "submitter": "Xiaofei Xu", "authors": "Xiaofei Xu, Ke Deng, Fei Hu, Li Li", "title": "An Improved Historical Embedding without Alignment", "comments": "9 pages, 5 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many words have evolved in meaning as a result of cultural and social change.\nUnderstanding such changes is crucial for modelling language and cultural\nevolution. Low-dimensional embedding methods have shown promise in detecting\nwords' meaning change by encoding them into dense vectors. However, when\nexploring semantic change of words over time, these methods require the\nalignment of word embeddings across different time periods. This process is\ncomputationally expensive, prohibitively time consuming and suffering from\ncontextual variability. In this paper, we propose a new and scalable method for\nencoding words from different time periods into one dense vector space. This\ncan greatly improve performance when it comes to identifying words that have\nchanged in meaning over time. We evaluated our method on dataset from Google\nBooks N-gram. Our method outperformed three other popular methods in terms of\nthe number of words correctly identified to have changed in meaning.\nAdditionally, we provide an intuitive visualization of the semantic evolution\nof some words extracted by our method\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 03:32:16 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Xu", "Xiaofei", ""], ["Deng", "Ke", ""], ["Hu", "Fei", ""], ["Li", "Li", ""]]}, {"id": "1910.08881", "submitter": "Huyen Nguyen", "authors": "Huyen N. Nguyen, Tommy Dang", "title": "EQSA: Earthquake Situational Analytics from Social Media", "comments": "2 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces EQSA, an interactive exploratory tool for earthquake\nsituational analytics using social media. EQSA is designed to support users to\ncharacterize the condition across the area around the earthquake zone,\nregarding related events, resources to be allocated, and responses from the\ncommunity. On the general level, changes in the volume of messages from chosen\ncategories are presented, assisting users in conveying a general idea of the\ncondition. More in-depth analysis is provided with topic evolution, community\nvisualization, and location representation. EQSA is developed with intuitive,\ninteractive features and multiple linked views, visualizing social media data,\nand supporting users to gain a comprehensive insight into the situation. In\nthis paper, we present the application of EQSA with the VAST Challenge 2019:\nMini-Challenge 3 (MC3) dataset.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 03:07:08 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Nguyen", "Huyen N.", ""], ["Dang", "Tommy", ""]]}, {"id": "1910.08887", "submitter": "Mengqi Zhang", "authors": "Mengqi Zhang, Shu Wu, Meng Gao, Xin Jiang, Ke Xu, Liang Wang", "title": "Personalized Graph Neural Networks with Attention Mechanism for\n  Session-Aware Recommendation", "comments": "12 pages", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering 2020", "doi": "10.1109/TKDE.2020.3031329", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of session-aware recommendation aims to predict users' next click\nbased on their current session and historical sessions. Existing session-aware\nrecommendation methods have defects in capturing complex item transition\nrelationships. Other than that, most of them fail to explicitly distinguish the\neffects of different historical sessions on the current session. To this end,\nwe propose a novel method, named Personalized Graph Neural Networks with\nAttention Mechanism (A-PGNN) for brevity. A-PGNN mainly consists of two\ncomponents: one is Personalized Graph Neural Network (PGNN), which is used to\nextract the personalized structural information in each user behavior graph,\ncompared with the traditional Graph Neural Network (GNN) model, which considers\nthe role of the user when the node embeddding is updated. The other is\nDot-Product Attention mechanism, which draws on the Transformer net to\nexplicitly model the effect of historical sessions on the current session.\nExtensive experiments conducted on two real-world data sets show that A-PGNN\nevidently outperforms the state-of-the-art personalized session-aware\nrecommendation methods.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 03:41:20 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 01:47:36 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 02:39:46 GMT"}, {"version": "v4", "created": "Wed, 27 Jan 2021 13:44:52 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Zhang", "Mengqi", ""], ["Wu", "Shu", ""], ["Gao", "Meng", ""], ["Jiang", "Xin", ""], ["Xu", "Ke", ""], ["Wang", "Liang", ""]]}, {"id": "1910.08948", "submitter": "Preslav Nakov", "authors": "Yoan Dinkov, Ahmed Ali, Ivan Koychev, Preslav Nakov", "title": "Predicting the Leading Political Ideology of YouTube Channels Using\n  Acoustic, Textual, and Metadata Information", "comments": "media bias, political ideology, Youtube channels, propaganda,\n  disinformation, fake news", "journal-ref": "INTERSPEECH-2019", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of predicting the leading political ideology, i.e.,\nleft-center-right bias, for YouTube channels of news media. Previous work on\nthe problem has focused exclusively on text and on analysis of the language\nused, topics discussed, sentiment, and the like. In contrast, here we study\nvideos, which yields an interesting multimodal setup. Starting with gold\nannotations about the leading political ideology of major world news media from\nMedia Bias/Fact Check, we searched on YouTube to find their corresponding\nchannels, and we downloaded a recent sample of videos from each channel. We\ncrawled more than 1,000 YouTube hours along with the corresponding subtitles\nand metadata, thus producing a new multimodal dataset. We further developed a\nmultimodal deep-learning architecture for the task. Our analysis shows that the\nuse of acoustic signal helped to improve bias detection by more than 6%\nabsolute over using text and metadata only. We release the dataset to the\nresearch community, hoping to help advance the field of multi-modal political\nbias detection.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 11:05:05 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Dinkov", "Yoan", ""], ["Ali", "Ahmed", ""], ["Koychev", "Ivan", ""], ["Nakov", "Preslav", ""]]}, {"id": "1910.09021", "submitter": "Zehao Wang", "authors": "Zehao Wang, Jingru Li, Xiaoou Chen, Zijin Li, Shicheng Zhang, Baoqiang\n  Han, Deshun Yang", "title": "Musical Instrument Playing Technique Detection Based on FCN: Using\n  Chinese Bowed-Stringed Instrument as an Example", "comments": "Submitted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike melody extraction and other aspects of music transcription, research\non playing technique detection is still in its early stages. Compared to\nexisting work mostly focused on playing technique detection for individual\nsingle notes, we propose a general end-to-end method based on Sound Event\nDetection by FCN for musical instrument playing technique detection. In our\ncase, we choose Erhu, a well-known Chinese bowed-stringed instrument, to\nexperiment with our method. Because of the limitation of FCN, we present an\nalgorithm to detect on variable length audio. The effectiveness of the proposed\nframework is tested on a new dataset, its categorization of techniques is\nsimilar to our training dataset. The highest accuracy of our 3 experiments on\nthe new test set is 87.31%. Furthermore, we also evaluate the performance of\nthe proposed framework on 10 real-world studio music (produced by midi) and 7\nreal-world recording samples to address the ability of generalization on our\nmodel.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 16:50:49 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Wang", "Zehao", ""], ["Li", "Jingru", ""], ["Chen", "Xiaoou", ""], ["Li", "Zijin", ""], ["Zhang", "Shicheng", ""], ["Han", "Baoqiang", ""], ["Yang", "Deshun", ""]]}, {"id": "1910.09114", "submitter": "Vladimir Vargas-Calder\\'on", "authors": "Vladimir Vargas-Calder\\'on and Marlon Steibeck Dominguez and N.\n  Parra-A. and Herbert Vinck-Posada and Jorge E. Camargo", "title": "Using machine learning and information visualisation for discovering\n  latent topics in Twitter news", "comments": "10 pages, 6 figures, to be presented at SmartTech-IC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to discover latent topics and visualise large collections\nof tweets for easy identification and interpretation of topics, and exemplify\nits use with tweets from a Colombian mass media giant in the period 2014--2019.\nThe latent topic analysis is performed in two ways: with the training of a\nLatent Dirichlet Allocation model, and with the combination of the FastText\nunsupervised model to represent tweets as vectors and the implementation of\nK-means clustering to group tweets into topics. Using a classification task, we\nfound that people respond differently according to the various news topics. The\nclassification tasks consists of the following: given a reply to a news tweet,\nwe train a supervised algorithm to predict the topic of the news tweet solely\nfrom the reply. Furthermore, we show how the Colombian peace treaty has had a\nprofound impact on the Colombian society, as it is the topic in which most\npeople engage to show their opinions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 02:13:18 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Vargas-Calder\u00f3n", "Vladimir", ""], ["Dominguez", "Marlon Steibeck", ""], ["Parra-A.", "N.", ""], ["Vinck-Posada", "Herbert", ""], ["Camargo", "Jorge E.", ""]]}, {"id": "1910.09129", "submitter": "Pinky Sitikhu", "authors": "Pinky Sitikhu, Kritish Pahi, Pujan Thapa, Subarna Shakya", "title": "A Comparison of Semantic Similarity Methods for Maximum Human\n  Interpretability", "comments": "Accepted in IEEE International Conference on Artificial Intelligence\n  for Transforming Business and Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inclusion of semantic information in any similarity measures improves the\nefficiency of the similarity measure and provides human interpretable results\nfor further analysis. The similarity calculation method that focuses on\nfeatures related to the text's words only, will give less accurate results.\nThis paper presents three different methods that not only focus on the text's\nwords but also incorporates semantic information of texts in their feature\nvector and computes semantic similarities. These methods are based on\ncorpus-based and knowledge-based methods, which are: cosine similarity using\ntf-idf vectors, cosine similarity using word embedding and soft cosine\nsimilarity using word embedding. Among these three, cosine similarity using\ntf-idf vectors performed best in finding similarities between short news texts.\nThe similar texts given by the method are easy to interpret and can be used\ndirectly in other information retrieval applications.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 03:09:02 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 02:10:04 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Sitikhu", "Pinky", ""], ["Pahi", "Kritish", ""], ["Thapa", "Pujan", ""], ["Shakya", "Subarna", ""]]}, {"id": "1910.09182", "submitter": "Shen Chen", "authors": "Shen Chen, Liujuan Cao, Mingbao Lin, Yan Wang, Xiaoshuai Sun, Chenglin\n  Wu, Jingfei Qiu and Rongrong Ji", "title": "Hadamard Codebook Based Deep Hashing", "comments": "8 pages, 7 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an approximate nearest neighbor search technique, hashing has been widely\napplied in large-scale image retrieval due to its excellent efficiency. Most\nsupervised deep hashing methods have similar loss designs with embedding\nlearning, while quantizing the continuous high-dim feature into compact binary\nspace. We argue that the existing deep hashing schemes are defective in two\nissues that seriously affect the performance, i.e., bit independence and bit\nbalance. The former refers to hash codes of different classes should be\nindependent of each other, while the latter means each bit should have a\nbalanced distribution of +1s and -1s. In this paper, we propose a novel\nsupervised deep hashing method, termed Hadamard Codebook based Deep Hashing\n(HCDH), which solves the above two problems in a unified formulation.\nSpecifically, we utilize an off-the-shelf algorithm to generate a binary\nHadamard codebook to satisfy the requirement of bit independence and bit\nbalance, which subsequently serves as the desired outputs of the hash functions\nlearning. We also introduce a projection matrix to solve the inconsistency\nbetween the order of Hadamard matrix and the number of classes. Besides, the\nproposed HCDH further exploits the supervised labels by constructing a\nclassifier on top of the outputs of hash functions. Extensive experiments\ndemonstrate that HCDH can yield discriminative and balanced binary codes, which\nwell outperforms many state-of-the-arts on three widely-used benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 07:33:42 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Chen", "Shen", ""], ["Cao", "Liujuan", ""], ["Lin", "Mingbao", ""], ["Wang", "Yan", ""], ["Sun", "Xiaoshuai", ""], ["Wu", "Chenglin", ""], ["Qiu", "Jingfei", ""], ["Ji", "Rongrong", ""]]}, {"id": "1910.09242", "submitter": "Andres Ferraro", "authors": "Andres Ferraro, Kjell Lemstr\\\"om", "title": "On large-scale genre classification in symbolically encoded music by\n  automatic identification of repeating patterns", "comments": null, "journal-ref": "Proceedings of the 5th International Conference on Digital\n  Libraries for Musicology (DLfM '18). ACM, New York, NY, USA, 34-37. 2018", "doi": "10.1145/3273024.3273035", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The importance of repetitions in music is well-known. In this paper, we study\nmusic repetitions in the context of effective and efficient automatic genre\nclassification in large-scale music-databases. We aim at enhancing the access\nand organization of pieces of music in Digital Libraries by allowing automatic\ncategorization of entire collections by considering only their musical content.\nWe handover to the public a set of genre-specific patterns to support research\nin musicology. The patterns can be used, for instance, to explore and analyze\nthe relations between musical genres. There are many existing algorithms that\ncould be used to identify and extract repeating patterns in symbolically\nencoded music. In our case, the extracted patterns are used as representations\nof the pieces of music on the underlying corpus and, consecutively, to train\nand evaluate a classifier to automatically identify genres. In this paper, we\napply two very fast algorithms enabling us to experiment on large and diverse\ncorpora. Thus, we are able to find patterns with strong discrimination power\nthat can be used in various applications. We carried out experiments on a\ncorpus containing over 40,000 MIDI files annotated with at least one genre. The\nexperiments suggest that our approach is scalable and capable of dealing with\nreal-world-size music collections.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 09:58:00 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Ferraro", "Andres", ""], ["Lemstr\u00f6m", "Kjell", ""]]}, {"id": "1910.09313", "submitter": "Tobias Weber", "authors": "Tobias Weber, Dieter Kranzlm\\\"uller, Michael Fromm, Nelson Tavares de\n  Sousa", "title": "Using Supervised Learning to Classify Metadata of Research Data by\n  Discipline of Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated classification of metadata of research data by their discipline(s)\nof research can be used in scientometric research, by repository service\nproviders, and in the context of research data aggregation services. Openly\navailable metadata of the DataCite index for research data were used to compile\na large training and evaluation set comprised of 609,524 records, which is\npublished alongside this paper. These data allow to reproducibly assess\nclassification approaches, such as tree-based models and neural networks.\nAccording to our experiments with 20 base classes (multi-label classification),\nmulti-layer perceptron models perform best with a f1-macro score of 0.760\nclosely followed by Long Short-Term Memory models (f1-macro score of 0.755). A\npossible application of the trained classification models is the quantitative\nanalysis of trends towards interdisciplinarity of digital scholarly output or\nthe characterization of growth patterns of research data, stratified by\ndiscipline of research. Both applications perform at scale with the proposed\nmodels which are available for re-use.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 07:51:37 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Weber", "Tobias", ""], ["Kranzlm\u00fcller", "Dieter", ""], ["Fromm", "Michael", ""], ["de Sousa", "Nelson Tavares", ""]]}, {"id": "1910.09324", "submitter": "Nupoor Gandhi", "authors": "Nupoor Gandhi, Alex Morales, Dolores Albarracin", "title": "Multi-dimensional Features for Prediction with Tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of opioid abuse in the US, there has been a growth of\noverlapping hotspots for overdose-related and HIV-related deaths in\nSpringfield, Boston, Fall River, New Bedford, and parts of Cape Cod. With a\nlarge part of population, including rural communities, active on social media,\nit is crucial that we leverage the predictive power of social media as a\npreventive measure. We explore the predictive power of micro-blogging social\nmedia website Twitter with respect to HIV new diagnosis rates per county. While\ntrending work in Twitter NLP has focused on primarily text-based features, we\nshow that multi-dimensional feature construction can significantly improve the\npredictive power of topic features alone with respect STI's (sexually\ntransmitted infections). By multi-dimensional features, we mean leveraging not\nonly the topical features (text) of a corpus, but also location-based\ninformation (counties) about the tweets in feature-construction. We develop\nnovel text-location-based smoothing features to predict new diagnoses of HIV.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 15:45:42 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Gandhi", "Nupoor", ""], ["Morales", "Alex", ""], ["Albarracin", "Dolores", ""]]}, {"id": "1910.09337", "submitter": "Wenhao Zhang", "authors": "Wenhao Zhang, Wentian Bao, Xiao-Yang Liu, Keping Yang, Quan Lin, Hong\n  Wen, Ramin Ramezani", "title": "Large-scale Causal Approaches to Debiasing Post-click Conversion Rate\n  Estimation with Multi-task Learning", "comments": "14 pages, 6 figures, 4 tables; Accepted at The Web Conference 2020;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-click conversion rate (CVR) estimation is a critical task in e-commerce\nrecommender systems. This task is deemed quite challenging under the industrial\nsetting with two major issues: 1) selection bias caused by user self-selection,\nand 2) data sparsity due to the rare click events. A successful conversion\ntypically has the following sequential events: \"exposure -> click ->\nconversion\". Conventional CVR estimators are trained in the click space, but\nthe inference is done in the entire exposure space. They fail to account for\nthe causes of the missing data and treat them as missing at random. Hence,\ntheir estimations are highly likely to deviate from the real values by large.\nIn addition, the data sparsity issue can also handicap many industrial CVR\nestimators which usually have large parameter spaces.\n  In this paper, we propose two principled, efficient and highly effective CVR\nestimators for industrial CVR estimation, namely, Multi-IPW and Multi-DR. The\nproposed models approach the CVR estimation from a causal perspective and\naccount for the causes of missing not at random. In addition, our methods are\nbased on the multi-task learning framework and mitigate the data sparsity\nissue. Extensive experiments on industrial-level datasets show that our methods\noutperform the state-of-the-art CVR models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 17:46:11 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 06:01:09 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Zhang", "Wenhao", ""], ["Bao", "Wentian", ""], ["Liu", "Xiao-Yang", ""], ["Yang", "Keping", ""], ["Lin", "Quan", ""], ["Wen", "Hong", ""], ["Ramezani", "Ramin", ""]]}, {"id": "1910.09513", "submitter": "Jakub Hara\\v{s}ta Ph.D", "authors": "Tereza Novotn\\'a and Jakub Hara\\v{s}ta", "title": "The Czech Court Decisions Corpus (CzCDC): Availability as the First Step", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe the Czech Court Decision Corpus (CzCDC). CzCDC is\na dataset of 237,723 decisions published by the Czech apex (or top-tier)\ncourts, namely the Supreme Court, the Supreme Administrative Court and the\nConstitutional Court. All the decisions were published between 1st January 1993\nand 30th September 2018.\n  Court decisions are available on the webpages of the respective courts or via\ncommercial databases of legal information. This often leads researchers\ninterested in these decisions to reach either to respective court or to\ncommercial provider. This leads to delays and additional costs. These are\nfurther exacerbated by a lack of inter-court standard in the terms of the data\nformat in which courts provide their decisions. Additionally, courts' databases\noften lack proper documentation.\n  Our goal is to make the dataset of court decisions freely available online in\nconsistent (plain) format to lower the cost associated with obtaining data for\nfuture research. We believe that simplified access to court decisions through\nthe CzCDC could benefit other researchers.\n  In this paper, we describe the processing of decisions before their inclusion\ninto CzCDC and basic statistics of the dataset. This dataset contains plain\ntexts of court decisions and these texts are not annotated for any grammatical\nor syntactical features.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 17:06:38 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Novotn\u00e1", "Tereza", ""], ["Hara\u0161ta", "Jakub", ""]]}, {"id": "1910.09645", "submitter": "Harald Steck", "authors": "Harald Steck", "title": "Markov Random Fields for Collaborative Filtering", "comments": "9 pages", "journal-ref": "33rd Conference on Neural Information Processing Systems (NeurIPS\n  2019), Vancouver, Canada", "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we model the dependencies among the items that are recommended\nto a user in a collaborative-filtering problem via a Gaussian Markov Random\nField (MRF). We build upon Besag's auto-normal parameterization and\npseudo-likelihood, which not only enables computationally efficient learning,\nbut also connects the areas of MRFs and sparse inverse covariance estimation\nwith autoencoders and neighborhood models, two successful approaches in\ncollaborative filtering. We propose a novel approximation for learning sparse\nMRFs, where the trade-off between recommendation-accuracy and training-time can\nbe controlled. At only a small fraction of the training-time compared to\nvarious baselines, including deep nonlinear models, the proposed approach\nachieved competitive ranking-accuracy on all three well-known data-sets used in\nour experiments, and notably a 20% gain in accuracy on the data-set with the\nlargest number of items.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 20:46:42 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Steck", "Harald", ""]]}, {"id": "1910.09676", "submitter": "Rama Kumar Pasumarthi", "authors": "Rama Kumar Pasumarthi, Xuanhui Wang, Michael Bendersky, Marc Najork", "title": "Self-Attentive Document Interaction Networks for Permutation Equivariant\n  Ranking", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to leverage cross-document interactions to improve ranking performance is\nan important topic in information retrieval (IR) research. However, this topic\nhas not been well-studied in the learning-to-rank setting and most of the\nexisting work still treats each document independently while scoring. The\nrecent development of deep learning shows strength in modeling complex\nrelationships across sequences and sets. It thus motivates us to study how to\nleverage cross-document interactions for learning-to-rank in the deep learning\nframework. In this paper, we formally define the permutation-equivariance\nrequirement for a scoring function that captures cross-document interactions.\nWe then propose a self-attention based document interaction network and show\nthat it satisfies the permutation-equivariant requirement, and can generate\nscores for document sets of varying sizes. Our proposed methods can\nautomatically learn to capture document interactions without any auxiliary\ninformation, and can scale across large document sets. We conduct experiments\non three ranking datasets: the benchmark Web30k, a Gmail search, and a Google\nDrive Quick Access dataset. Experimental results show that our proposed methods\nare both more effective and efficient than baselines.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 22:14:19 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 05:57:22 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Pasumarthi", "Rama Kumar", ""], ["Wang", "Xuanhui", ""], ["Bendersky", "Michael", ""], ["Najork", "Marc", ""]]}, {"id": "1910.10037", "submitter": "Manish Munikar", "authors": "Pranjal Dhakal, Manish Munikar, Bikram Dahal", "title": "One-Shot Template Matching for Automatic Document Data Capture", "comments": "Accepted in International Conference on AI for Transforming Business\n  (AITB2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel one-shot template-matching algorithm to\nautomatically capture data from business documents with an aim to minimize\nmanual data entry. Given one annotated document, our algorithm can\nautomatically extract similar data from other documents having the same format.\nBased on a set of engineered visual and textual features, our method is\ninvariant to changes in position and value. Experiments on a dataset of 595\nreal invoices demonstrate 86.4% accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 15:15:27 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Dhakal", "Pranjal", ""], ["Munikar", "Manish", ""], ["Dahal", "Bikram", ""]]}, {"id": "1910.10086", "submitter": "Yujie Lin", "authors": "Yujie Lin, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Dongxiao Yu, Jun\n  Ma, Maarten de Rijke, Xiuzhen Cheng", "title": "Meta Matrix Factorization for Federated Rating Predictions", "comments": "The code has been transferred to https://github.com/TempSDU/MetaMF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated recommender systems have distinct advantages in terms of privacy\nprotection over traditional recommender systems that are centralized at a data\ncenter. However, previous work on federated recommender systems does not fully\nconsider the limitations of storage, RAM, energy and communication bandwidth in\na mobile environment. The scales of the models proposed are too large to be\neasily run on mobile devices. And existing federated recommender systems need\nto fine-tune recommendation models on each device, making it hard to\neffectively exploit collaborative filtering information among users/devices.\nOur goal in this paper is to design a novel federated learning framework for\nrating prediction (RP) for mobile environments. We introduce a federated matrix\nfactorization (MF) framework, named meta matrix factorization (MetaMF). Given a\nuser, we first obtain a collaborative vector by collecting useful information\nwith a collaborative memory module. Then, we employ a meta recommender module\nto generate private item embeddings and a RP model based on the collaborative\nvector in the server. To address the challenge of generating a large number of\nhigh-dimensional item embeddings, we devise a rise-dimensional generation\nstrategy that first generates a low-dimensional item embedding matrix and a\nrise-dimensional matrix, and then multiply them to obtain high-dimensional\nembeddings. We use the generated model to produce private RPs for the given\nuser on her device. MetaMF shows a high capacity even with a small RP model,\nwhich can adapt to the limitations of a mobile environment. We conduct\nextensive experiments on four benchmark datasets to compare MetaMF with\nexisting MF methods and find that MetaMF can achieve competitive performance.\nMoreover, we find MetaMF achieves higher RP performance over existing federated\nmethods by better exploiting collaborative filtering among users/devices.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 16:29:51 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 16:39:37 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 08:12:11 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Lin", "Yujie", ""], ["Ren", "Pengjie", ""], ["Chen", "Zhumin", ""], ["Ren", "Zhaochun", ""], ["Yu", "Dongxiao", ""], ["Ma", "Jun", ""], ["de Rijke", "Maarten", ""], ["Cheng", "Xiuzhen", ""]]}, {"id": "1910.10208", "submitter": "Jimmy Lin", "authors": "Tommaso Teofili and Jimmy Lin", "title": "Lucene for Approximate Nearest-Neighbors Search on Arbitrary Dense\n  Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate three approaches for adapting the open-source Lucene search\nlibrary to perform approximate nearest-neighbor search on arbitrary dense\nvectors, using similarity search on word embeddings as a case study. At its\ncore, Lucene is built around inverted indexes of a document collection's\n(sparse) term-document matrix, which is incompatible with the lower-dimensional\ndense vectors that are common in deep learning applications. We evaluate three\ntechniques to overcome these challenges that can all be natively integrated\ninto Lucene: the creation of documents populated with fake words, LSH applied\nto lexical realizations of dense vectors, and k-d trees coupled with\ndimensionality reduction. Experiments show that the \"fake words\" approach\nrepresents the best balance between effectiveness and efficiency. These\ntechniques are integrated into the Anserini open-source toolkit and made\navailable to the community.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 19:48:33 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 01:50:42 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Teofili", "Tommaso", ""], ["Lin", "Jimmy", ""]]}, {"id": "1910.10410", "submitter": "Phanideep Gampa", "authors": "Phanideep Gampa, Sumio Fujita", "title": "BanditRank: Learning to Rank Using Contextual Bandits", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extensible deep learning method that uses reinforcement\nlearning to train neural networks for offline ranking in information retrieval\n(IR). We call our method BanditRank as it treats ranking as a contextual bandit\nproblem. In the domain of learning to rank for IR, current deep learning models\nare trained on objective functions different from the measures they are\nevaluated on. Since most evaluation measures are discrete quantities, they\ncannot be leveraged by directly using gradient descent algorithms without an\napproximation. BanditRank bridges this gap by directly optimizing a\ntask-specific measure, such as mean average precision (MAP), using gradient\ndescent. Specifically, a contextual bandit whose action is to rank input\ndocuments is trained using a policy gradient algorithm to directly maximize the\nreward. The reward can be a single measure, such as MAP, or a combination of\nseveral measures. The notion of ranking is also inherent in BanditRank, similar\nto the current \\textit{listwise} approaches. To evaluate the effectiveness of\nBanditRank, we conducted a series of experiments on datasets related to three\ndifferent tasks, i.e., web search, community, and factoid question answering.\nWe found that it performs better than state-of-the-art methods when applied on\nthe question answering datasets. On the web search dataset, we found that\nBanditRank performed better than four strong listwise baselines including\nLambdaMART, AdaRank, ListNet and Coordinate Ascent.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 08:32:14 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Gampa", "Phanideep", ""], ["Fujita", "Sumio", ""]]}, {"id": "1910.10495", "submitter": "Junhua Liu", "authors": "Junhua Liu, Yung Chuen Ng, Kristin L. Wood, Kwan Hui Lim", "title": "IPOD: An Industrial and Professional Occupations Dataset and its\n  Applications to Occupational Data Mining and Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occupational data mining and analysis is an important task in understanding\ntoday's industry and job market. Various machine learning techniques are\nproposed and gradually deployed to improve companies' operations for upstream\ntasks, such as employee churn prediction, career trajectory modelling and\nautomated interview. Job titles analysis and embedding, as the fundamental\nbuilding blocks, are crucial upstream tasks to address these occupational data\nmining and analysis problems. In this work, we present the Industrial and\nProfessional Occupations Dataset (IPOD), which consists of over 190,000 job\ntitles crawled from over 56,000 profiles from Linkedin. We also illustrate the\nusefulness of IPOD by addressing two challenging upstream tasks, including: (i)\nproposing Title2vec, a contextual job title vector representation using a\nbidirectional Language Model (biLM) approach; and (ii) addressing the important\noccupational Named Entity Recognition problem using Conditional Random Fields\n(CRF) and bidirectional Long Short-Term Memory with CRF (LSTM-CRF). Both CRF\nand LSTM-CRF outperform human and baselines in both exact-match accuracy and F1\nscores. The dataset and pre-trained embeddings are available at\nhttps://www.github.com/junhua/ipod.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 08:30:26 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 16:50:11 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Liu", "Junhua", ""], ["Ng", "Yung Chuen", ""], ["Wood", "Kristin L.", ""], ["Lim", "Kwan Hui", ""]]}, {"id": "1910.10687", "submitter": "Zhuyun Dai", "authors": "Zhuyun Dai and Jamie Callan", "title": "Context-Aware Sentence/Passage Term Importance Estimation For First\n  Stage Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Term frequency is a common method for identifying the importance of a term in\na query or document. But it is a weak signal, especially when the frequency\ndistribution is flat, such as in long queries or short documents where the text\nis of sentence/passage-length. This paper proposes a Deep Contextualized Term\nWeighting framework that learns to map BERT's contextualized text\nrepresentations to context-aware term weights for sentences and passages. When\napplied to passages, DeepCT-Index produces term weights that can be stored in\nan ordinary inverted index for passage retrieval. When applied to query text,\nDeepCT-Query generates a weighted bag-of-words query. Both types of term weight\ncan be used directly by typical first-stage retrieval algorithms. This is novel\nbecause most deep neural network based ranking models have higher computational\ncosts, and thus are restricted to later-stage rankers. Experiments on four\ndatasets demonstrate that DeepCT's deep contextualized text understanding\ngreatly improves the accuracy of first-stage retrieval algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 17:42:35 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 22:44:36 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Dai", "Zhuyun", ""], ["Callan", "Jamie", ""]]}, {"id": "1910.10758", "submitter": "Arijit Das", "authors": "Arijit Das, Jaydeep Mandal, Zargham Danial, Alok Ranjan Pal, Diganta\n  Saha", "title": "A Novel Approach for Automatic Bengali Question Answering System using\n  Semantic Similarity Analysis", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the semantically accurate answer is one of the key challenges in\nadvanced searching. In contrast to keyword-based searching, the meaning of a\nquestion or query is important here and answers are ranked according to\nrelevance. It is very natural that there is almost no common word between the\nquestion sentence and the answer sentence. In this paper, an approach is\ndescribed to find out the semantically relevant answers in the Bengali dataset.\nIn the first part of the algorithm, a set of statistical parameters like\nfrequency, index, part-of-speech (POS), etc. is matched between a question and\nthe probable answers. In the second phase, entropy and similarity are\ncalculated in different modules. Finally, a sense score is generated to rank\nthe answers. The algorithm is tested on a repository containing a total of\n275000 sentences. This Bengali repository is a product of Technology\nDevelopment for Indian Languages (TDIL) project sponsored by Govt. of India and\nprovided by the Language Research Unit of Indian Statistical Institute,\nKolkata. The shallow parser, developed by the LTRC group of IIIT Hyderabad is\nused for POS tagging. The actual answer is ranked as 1st in 82.3% cases. The\nactual answer is ranked within 1st to 5th in 90.0% cases. The accuracy of the\nsystem is coming as 97.32% and precision of the system is coming as 98.14%\nusing confusion matrix. The challenges and pitfalls of the work are reported at\nlast in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 18:24:55 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Das", "Arijit", ""], ["Mandal", "Jaydeep", ""], ["Danial", "Zargham", ""], ["Pal", "Alok Ranjan", ""], ["Saha", "Diganta", ""]]}, {"id": "1910.10872", "submitter": "Ninareh Mehrabi", "authors": "Ninareh Mehrabi, Thamme Gowda, Fred Morstatter, Nanyun Peng, Aram\n  Galstyan", "title": "Man is to Person as Woman is to Location: Measuring Gender Bias in Named\n  Entity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the bias in several state-of-the-art named entity recognition (NER)\nmodels---specifically, a difference in the ability to recognize male and female\nnames as PERSON entity types. We evaluate NER models on a dataset containing\n139 years of U.S. census baby names and find that relatively more female names,\nas opposed to male names, are not recognized as PERSON entities. We study the\nextent of this bias in several NER systems that are used prominently in\nindustry and academia. In addition, we also report a bias in the datasets on\nwhich these models were trained. The result of this analysis yields a new\nbenchmark for gender bias evaluation in named entity recognition systems. The\ndata and code for the application of this benchmark will be publicly available\nfor researchers to use.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 01:32:24 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Mehrabi", "Ninareh", ""], ["Gowda", "Thamme", ""], ["Morstatter", "Fred", ""], ["Peng", "Nanyun", ""], ["Galstyan", "Aram", ""]]}, {"id": "1910.11028", "submitter": "Jimmy Lin", "authors": "Jimmy Lin, Lori Paniak, and Gordon Boerke", "title": "The Performance Envelope of Inverted Indexing on Modern Hardware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the performance envelope of \"traditional\" inverted\nindexing on modern hardware using the implementation in the open-source Lucene\nsearch library. We benchmark indexing throughput on a single high-end\nmulti-core commodity server in a number of configurations varying the media of\nthe source collection and target index, examining a network-attached store, a\ndirect-attached disk array, and an SSD. Experiments show that the largest\ndeterminants of performance are the physical characteristics of the source and\ntarget media, and that physically isolating the two yields the highest indexing\nthroughput. Results suggest that current indexing techniques have reached\nphysical device limits, and that further algorithmic improvements in\nperformance are unlikely without rethinking the inverted indexing pipeline in\nlight of observed bottlenecks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 11:04:59 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 01:45:15 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Lin", "Jimmy", ""], ["Paniak", "Lori", ""], ["Boerke", "Gordon", ""]]}, {"id": "1910.11241", "submitter": "Dattaraj Rao", "authors": "Amogh Kamat Tarcar, Aashis Tiwari, Vineet Naique Dhaimodker, Penjo\n  Rebelo, Rahul Desai, Dattaraj Rao", "title": "Healthcare NER Models Using Language Model Pretraining", "comments": "This work was presented at the first Health Search and Data Mining\n  Workshop (HSDM 2020) as part of WSDM 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present our approach to extracting structured information\nfrom unstructured Electronic Health Records (EHR) [2] which can be used to, for\nexample, study adverse drug reactions in patients due to chemicals in their\nproducts. Our solution uses a combination of Natural Language Processing (NLP)\ntechniques and a web-based annotation tool to optimize the performance of a\ncustom Named Entity Recognition (NER) [1] model trained on a limited amount of\nEHR training data. This work was presented at the first Health Search and Data\nMining Workshop (HSDM 2020) [26]. We showcase a combination of tools and\ntechniques leveraging the recent advancements in NLP aimed at targeting domain\nshifts by applying transfer learning and language model pre-training techniques\n[3]. We present a comparison of our technique to the current popular approaches\nand show the effective increase in performance of the NER model and the\nreduction in time to annotate data.A key observation of the results presented\nis that the F1 score of model (0.734) trained with our approach with just 50%\nof available training data outperforms the F1 score of the blank spaCy model\nwithout language model component (0.704) trained with 100% of the available\ntraining data. We also demonstrate an annotation tool to minimize domain expert\ntime and the manual effort required to generate such a training dataset.\nFurther, we plan to release the annotated dataset as well as the pre-trained\nmodel to the community to further research in medical health records.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 07:37:14 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 07:12:58 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Tarcar", "Amogh Kamat", ""], ["Tiwari", "Aashis", ""], ["Dhaimodker", "Vineet Naique", ""], ["Rebelo", "Penjo", ""], ["Desai", "Rahul", ""], ["Rao", "Dattaraj", ""]]}, {"id": "1910.11377", "submitter": "Sunyang Fu", "authors": "Sunyang Fu, David Chen, Huan He, Sijia Liu, Sungrim Moon, Kevin J\n  Peterson, Feichen Shen, Liwei Wang, Yanshan Wang, Andrew Wen, Yiqing Zhao,\n  Sunghwan Sohn, Hongfang Liu", "title": "Clinical Concept Extraction: a Methodology Review", "comments": null, "journal-ref": "Journal of Biomedical Informatics (2020): 103526", "doi": "10.1016/j.jbi.2020.103526", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background Concept extraction, a subdomain of natural language processing\n(NLP) with a focus on extracting concepts of interest, has been adopted to\ncomputationally extract clinical information from text for a wide range of\napplications ranging from clinical decision support to care quality\nimprovement.\n  Objectives In this literature review, we provide a methodology review of\nclinical concept extraction, aiming to catalog development processes, available\nmethods and tools, and specific considerations when developing clinical concept\nextraction applications.\n  Methods Based on the Preferred Reporting Items for Systematic Reviews and\nMeta-Analyses (PRISMA) guidelines, a literature search was conducted for\nretrieving EHR-based information extraction articles written in English and\npublished from January 2009 through June 2019 from Ovid MEDLINE In-Process &\nOther Non-Indexed Citations, Ovid MEDLINE, Ovid EMBASE, Scopus, Web of Science,\nand the ACM Digital Library.\n  Results A total of 6,686 publications were retrieved. After title and\nabstract screening, 228 publications were selected. The methods used for\ndeveloping clinical concept extraction applications were discussed in this\nreview.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 18:54:25 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 19:17:01 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 19:17:25 GMT"}, {"version": "v4", "created": "Mon, 10 Aug 2020 21:09:35 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Fu", "Sunyang", ""], ["Chen", "David", ""], ["He", "Huan", ""], ["Liu", "Sijia", ""], ["Moon", "Sungrim", ""], ["Peterson", "Kevin J", ""], ["Shen", "Feichen", ""], ["Wang", "Liwei", ""], ["Wang", "Yanshan", ""], ["Wen", "Andrew", ""], ["Zhao", "Yiqing", ""], ["Sohn", "Sunghwan", ""], ["Liu", "Hongfang", ""]]}, {"id": "1910.11430", "submitter": "Kai Shu", "authors": "Kai Shu, Ahmed Hassan Awadallah, Susan Dumais, Huan Liu", "title": "Detecting Fake News with Weak Social Supervision", "comments": "9 pages, 4 figures", "journal-ref": "IEEE Intelligent Systems 2020", "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited labeled data is becoming the largest bottleneck for supervised\nlearning systems. This is especially the case for many real-world tasks where\nlarge scale annotated examples are either too expensive to acquire or\nunavailable due to privacy or data access constraints. Weak supervision has\nshown to be a good means to mitigate the scarcity of annotated data by\nleveraging weak labels or injecting constraints from heuristic rules and/or\nexternal knowledge sources. Social media has little labeled data but possesses\nunique characteristics that make it suitable for generating weak supervision,\nresulting in a new type of weak supervision, i.e., weak social supervision. In\nthis article, we illustrate how various aspects of social media can be used to\ngenerate weak social supervision. Specifically, we use the recent research on\nfake news detection as the use case, where social engagements are abundant but\nannotated examples are scarce, to show that weak social supervision is\neffective when facing the little labeled data problem. This article opens the\ndoor for learning with weak social supervision for other emerging tasks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 21:27:31 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 23:13:25 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Shu", "Kai", ""], ["Awadallah", "Ahmed Hassan", ""], ["Dumais", "Susan", ""], ["Liu", "Huan", ""]]}, {"id": "1910.11494", "submitter": "Jianxun Lian", "authors": "Danyang Liu, Jianxun Lian, Shiyin Wang, Ying Qiao, Jiun-Hung Chen,\n  Guangzhong Sun, Xing Xie", "title": "KRED: Knowledge-Aware Document Representation for News Recommendations", "comments": "RecSys'20", "journal-ref": null, "doi": "10.1145/3383313.3412237", "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News articles usually contain knowledge entities such as celebrities or\norganizations. Important entities in articles carry key messages and help to\nunderstand the content in a more direct way. An industrial news recommender\nsystem contains various key applications, such as personalized recommendation,\nitem-to-item recommendation, news category classification, news popularity\nprediction and local news detection. We find that incorporating knowledge\nentities for better document understanding benefits these applications\nconsistently. However, existing document understanding models either represent\nnews articles without considering knowledge entities (e.g., BERT) or rely on a\nspecific type of text encoding model (e.g., DKN) so that the generalization\nability and efficiency is compromised. In this paper, we propose KRED, which is\na fast and effective model to enhance arbitrary document representation with a\nknowledge graph. KRED first enriches entities' embeddings by attentively\naggregating information from their neighborhood in the knowledge graph. Then a\ncontext embedding layer is applied to annotate the dynamic context of different\nentities such as frequency, category and position. Finally, an information\ndistillation layer aggregates the entity embeddings under the guidance of the\noriginal document representation and transforms the document vector into a new\none. We advocate to optimize the model with a multi-task framework, so that\ndifferent news recommendation applications can be united and useful information\ncan be shared across different tasks. Experiments on a real-world Microsoft\nNews dataset demonstrate that KRED greatly benefits a variety of news\nrecommendation applications.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 02:21:33 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 12:03:47 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 11:25:04 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Liu", "Danyang", ""], ["Lian", "Jianxun", ""], ["Wang", "Shiyin", ""], ["Qiao", "Ying", ""], ["Chen", "Jiun-Hung", ""], ["Sun", "Guangzhong", ""], ["Xie", "Xing", ""]]}, {"id": "1910.11621", "submitter": "Shumin Deng", "authors": "Shumin Deng, Ningyu Zhang, Jiaojian Kang, Yichi Zhang, Wei Zhang,\n  Huajun Chen", "title": "Meta-Learning with Dynamic-Memory-Based Prototypical Network for\n  Few-Shot Event Detection", "comments": "Accepted by WSDM 2020", "journal-ref": null, "doi": "10.1145/3336191.3371796", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event detection (ED), a sub-task of event extraction, involves identifying\ntriggers and categorizing event mentions. Existing methods primarily rely upon\nsupervised learning and require large-scale labeled event datasets which are\nunfortunately not readily available in many real-life applications. In this\npaper, we consider and reformulate the ED task with limited labeled data as a\nFew-Shot Learning problem. We propose a Dynamic-Memory-Based Prototypical\nNetwork (DMB-PN), which exploits Dynamic Memory Network (DMN) to not only learn\nbetter prototypes for event types, but also produce more robust sentence\nencodings for event mentions. Differing from vanilla prototypical networks\nsimply computing event prototypes by averaging, which only consume event\nmentions once, our model is more robust and is capable of distilling contextual\ninformation from event mentions for multiple times due to the multi-hop\nmechanism of DMNs. The experiments show that DMB-PN not only deals with sample\nscarcity better than a series of baseline models but also performs more\nrobustly when the variety of event types is relatively large and the instance\nquantity is extremely small.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 11:14:33 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 15:27:52 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Deng", "Shumin", ""], ["Zhang", "Ningyu", ""], ["Kang", "Jiaojian", ""], ["Zhang", "Yichi", ""], ["Zhang", "Wei", ""], ["Chen", "Huajun", ""]]}, {"id": "1910.12180", "submitter": "Saeid Hosseini", "authors": "Saeed Najafipour, Saeid Hosseini, Wen Hua, Mohammad Reza Kangavari,\n  Xiaofang Zhou", "title": "SoulMate: Short-text author linking through Multi-aspect\n  temporal-textual embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linking authors of short-text contents has important usages in many\napplications, including Named Entity Recognition (NER) and human community\ndetection. However, certain challenges lie ahead. Firstly, the input short-text\ncontents are noisy, ambiguous, and do not follow the grammatical rules.\nSecondly, traditional text mining methods fail to effectively extract concepts\nthrough words and phrases. Thirdly, the textual contents are temporally skewed,\nwhich can affect the semantic understanding by multiple time facets. Finally,\nusing the complementary knowledge-bases makes the results biased to the content\nof the external database and deviates the understanding and interpretation away\nfrom the real nature of the given short text corpus. To overcome these\nchallenges, we devise a neural network-based temporal-textual framework that\ngenerates the tightly connected author subgraphs from microblog short-text\ncontents. Our approach, on the one hand, computes the relevance score (edge\nweight) between the authors through considering a portmanteau of contents and\nconcepts, and on the other hand, employs a stack-wise graph cutting algorithm\nto extract the communities of the related authors. Experimental results show\nthat compared to other knowledge-centered competitors, our multi-aspect vector\nspace model can achieve a higher performance in linking short-text authors.\nAdditionally, given the author linking task, the more comprehensive the dataset\nis, the higher the significance of the extracted concepts will be.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 04:53:35 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Najafipour", "Saeed", ""], ["Hosseini", "Saeid", ""], ["Hua", "Wen", ""], ["Kangavari", "Mohammad Reza", ""], ["Zhou", "Xiaofang", ""]]}, {"id": "1910.12274", "submitter": "Elad Yom-Tov", "authors": "Brit Youngmann, Ran Gilad-Bachrach, Danny Karmon, Elad Yom-Tov", "title": "Algorithmic Copywriting: Automated Generation of Health-Related\n  Advertisements to Improve their Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search advertising, a popular method for online marketing, has been employed\nto improve health by eliciting positive behavioral change. However, writing\neffective advertisements requires expertise and experimentation, which may not\nbe available to health authorities wishing to elicit such changes, especially\nwhen dealing with public health crises such as epidemic outbreaks.\n  Here we develop a framework, comprised of two neural networks models, that\nautomatically generate ads. First, it employs a generator model, which create\nads from web pages. It then employs a translation model, which transcribes ads\nto improve performance.\n  We trained the networks using 114K health-related ads shown on Microsoft\nAdvertising. We measure ads performance using the click-through rates (CTR).\n  Our experiments show that the generated advertisements received approximately\nthe same CTR as human-authored ads. The marginal contribution of the generator\nmodel was, on average, 28\\% lower than that of human-authored ads, while the\ntranslator model received, on average, 32\\% more clicks than human-authored\nads. Our analysis shows that the translator model produces ads reflecting\nhigher values of psychological attributes associated with a user action,\nincluding higher valance and arousal, and more calls-to-actions. In contrast,\nlevels of these attributes in ads produced by the generator model are similar\nto those of human-authored ads.\n  Our results demonstrate the ability to automatically generate useful\nadvertisements for the health domain. We believe that our work offers health\nauthorities an improved ability to nudge people towards healthier behaviors\nwhile saving the time and cost needed to build effective advertising campaigns.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 14:51:53 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 18:00:36 GMT"}, {"version": "v3", "created": "Sun, 12 Jul 2020 07:22:00 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Youngmann", "Brit", ""], ["Gilad-Bachrach", "Ran", ""], ["Karmon", "Danny", ""], ["Yom-Tov", "Elad", ""]]}, {"id": "1910.12279", "submitter": "Suryatej Vyalla", "authors": "Suryatej Reddy Vyalla, Vishaal Udandarao, Tanmoy Chakraborty", "title": "Memeify: A Large-Scale Meme Generation System", "comments": "Accepted at ACM India Joint International Conference on Data Science\n  & Management of Data (CoDS-CoMAD) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in the research areas related to meme propagation and generation has\nbeen increasing rapidly in the last couple of years. Meme datasets available\nonline are either specific to a context or contain no class information. Here,\nwe prepare a large-scale dataset of memes with captions and class labels. The\ndataset consists of 1.1 million meme captions from 128 classes. We also provide\nreasoning for the existence of broad categories, called \"themes\" across the\nmeme dataset; each theme consists of multiple meme classes. Our generation\nsystem uses a trained state-of-the-art transformer-based model for caption\ngeneration by employing an encoder-decoder architecture. We develop a web\ninterface, called Memeify for users to generate memes of their choice, and\nexplain in detail, the working of individual components of the system. We also\nperform a qualitative evaluation of the generated memes by conducting a user\nstudy. A link to the demonstration of the Memeify system is\nhttps://youtu.be/P_Tfs0X-czs.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 15:13:26 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 19:04:19 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Vyalla", "Suryatej Reddy", ""], ["Udandarao", "Vishaal", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "1910.12441", "submitter": "Samaneh Karimi", "authors": "Samaneh Karimi, Azadeh Shakery, Rakesh Verma", "title": "Online News Media Website Ranking Using User Generated Content", "comments": "35 pages, 4 Figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News media websites are important online resources that have drawn great\nattention of text mining researchers. The main aim of this study is to propose\na framework for ranking online news websites from different viewpoints. The\nranking of news websites is useful information, which can benefit many\nnews-related tasks such as news retrieval and news recommendation. In the\nproposed framework, the ranking of news websites is obtained by calculating\nthree measures introduced in the paper and based on user-generated content.\nEach proposed measure is concerned with the performance of news websites from a\nparticular viewpoint including the completeness of news reports, the diversity\nof events being covered by the website and its speed. The use of user-generated\ncontent in this framework, as a partly-unbiased, real-time and low cost content\non the web distinguishes the proposed news website ranking framework from the\nliterature. The results obtained for three prominent news websites, BBC, CNN,\nNYTimes, show that BBC has the best performance in terms of news completeness\nand speed, and NYTimes has the best diversity in comparison with the other two\nwebsites.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 04:50:22 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Karimi", "Samaneh", ""], ["Shakery", "Azadeh", ""], ["Verma", "Rakesh", ""]]}, {"id": "1910.12446", "submitter": "Renhao Cui", "authors": "Renhao Cui, Gagan Agrawal, Rajiv Ramnath", "title": "Towards Successful Social Media Advertising: Predicting the Influence of\n  Commercial Tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Businesses communicate using Twitter for a variety of reasons -- to raise\nawareness of their brands, to market new products, to respond to community\ncomments, and to connect with their customers and potential customers in a\ntargeted manner. For businesses to do this effectively, they need to understand\nwhich content and structural elements about a tweet make it influential, that\nis, widely liked, followed, and retweeted. This paper presents a systematic\nmethodology for analyzing commercial tweets, and predicting the influence on\ntheir readers. Our model, which use a combination of decoration and meta\nfeatures, outperforms the prediction ability of the baseline model as well as\nthe tweet embedding model. Further, in order to demonstrate a practical use of\nthis work, we show how an unsuccessful tweet may be engineered (for example,\nreworded) to increase its potential for success.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 05:14:41 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Cui", "Renhao", ""], ["Agrawal", "Gagan", ""], ["Ramnath", "Rajiv", ""]]}, {"id": "1910.12460", "submitter": "Kyle Xiao", "authors": "Kyle Xiao, Houdong Hu, Yan Wang", "title": "Applications of Generative Adversarial Models in Visual Search\n  Reformulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query reformulation is the process by which a input search query is refined\nby the user to match documents outside the original top-n results. On average,\nroughly 50% of text search queries involve some form of reformulation, and term\nsuggestion tools are used 35% of the time when offered to users. As prevalent\nas text search queries are, however, such a feature has yet to be explored at\nscale for visual search. This is because reformulation for images presents a\nnovel challenge to seamlessly transform visual features to match user intent\nwithin the context of a typical user session. In this paper, we present methods\nof semantically transforming visual queries, such as utilizing operations in\nthe latent space of a generative adversarial model for the scenarios of fashion\nand product search.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 06:27:38 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Xiao", "Kyle", ""], ["Hu", "Houdong", ""], ["Wang", "Yan", ""]]}, {"id": "1910.12574", "submitter": "Marzieh Mozafari", "authors": "Marzieh Mozafari, Reza Farahbakhsh, Noel Crespi", "title": "A BERT-Based Transfer Learning Approach for Hate Speech Detection in\n  Online Social Media", "comments": "This paper has been accepted in The 8th International Conference on\n  Complex Networks and their Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generated hateful and toxic content by a portion of users in social media is\na rising phenomenon that motivated researchers to dedicate substantial efforts\nto the challenging direction of hateful content identification. We not only\nneed an efficient automatic hate speech detection model based on advanced\nmachine learning and natural language processing, but also a sufficiently large\namount of annotated data to train a model. The lack of a sufficient amount of\nlabelled hate speech data, along with the existing biases, has been the main\nissue in this domain of research. To address these needs, in this study we\nintroduce a novel transfer learning approach based on an existing pre-trained\nlanguage model called BERT (Bidirectional Encoder Representations from\nTransformers). More specifically, we investigate the ability of BERT at\ncapturing hateful context within social media content by using new fine-tuning\nmethods based on transfer learning. To evaluate our proposed approach, we use\ntwo publicly available datasets that have been annotated for racism, sexism,\nhate, or offensive content on Twitter. The results show that our solution\nobtains considerable performance on these datasets in terms of precision and\nrecall in comparison to existing approaches. Consequently, our model can\ncapture some biases in data annotation and collection process and can\npotentially lead us to a more accurate model.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 12:13:38 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Mozafari", "Marzieh", ""], ["Farahbakhsh", "Reza", ""], ["Crespi", "Noel", ""]]}, {"id": "1910.12601", "submitter": "Meixin Zhu", "authors": "Meixin Zhu, Jingyun Hu, Hao (Frank) Yang, Ziyuan Pu, and Yinhai Wang", "title": "Personalized Context-Aware Multi-Modal Transportation Recommendation", "comments": "KDD cup 2019 regular machine track solution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes to find the most appropriate transport modes with\nawareness of user preferences (e.g., costs, times) and trip characteristics\n(e.g., purpose, distance). The work was based on real-life trips obtained from\na map application. Several methods including gradient boosting tree, learning\nto rank, multinomial logit model, automated machine learning, random forest,\nand shallow neural network have been tried. For some methods, feature selection\nand over-sampling techniques were also tried. The results show that the best\nperforming method is a gradient boosting tree model with synthetic minority\nover-sampling technique (SMOTE). Also, results of the multinomial logit model\nshow that (1) an increase in travel cost would decrease the utility of all the\ntransportation modes; (2) people are less sensitive to the travel distance for\nthe metro mode or a multi-modal option that containing metro, i.e., compared to\nother modes, people would be more willing to tolerate long-distance metro\ntrips. This indicates that metro lines might be a good candidate for large\ncities.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 20:16:13 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Zhu", "Meixin", "", "Frank"], ["Hu", "Jingyun", "", "Frank"], ["Hao", "", "", "Frank"], ["Yang", "", ""], ["Pu", "Ziyuan", ""], ["Wang", "Yinhai", ""]]}, {"id": "1910.12734", "submitter": "Alberto Purpura", "authors": "Alberto Purpura and Marco Calaresu", "title": "A Semi-Automated Approach for Information Extraction, Classification and\n  Analysis of Unstructured Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show how Quantitative Narrative Analysis and simple Natural\nLanguage Processing techniques apply to the extraction and categorization of\ndata in a sample case study of the Diary of the former President of the Italian\nRepublic (PoR), Giorgio Napolitano. The Diary contains a record of all his\ninstitutional meetings. This information, if properly handled, allows for an\nanalysis of how the PoR used his so-called soft-powers to influence the Italian\npolitical system during his first mandate. In this paper, we propose a way to\nuse simple, yet very effective, Natural Language Processing techniques - such\nas Regular Expressions and Named Entity Recognition - to extract information\nfrom the Diary. Then, we propose an innovative way to organize the extracted\ndata relying on the methodological framework of Quantitative Narrative\nAnalysis. Finally, we show how to analyze the structured data under different\nlevels of detail using PC-ACE (Program for Computer-Assisted Coding of Events),\na software developed specifically for this task and for data visualization.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 12:06:40 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Purpura", "Alberto", ""], ["Calaresu", "Marco", ""]]}, {"id": "1910.12735", "submitter": "Wenlin Wang", "authors": "Wenlin Wang, Hongteng Xu, Ruiyi Zhang, Wenqi Wang, Piyush Rai,\n  Lawrence Carin", "title": "Learning to Recommend from Sparse Data via Generative User Feedback", "comments": "To appear in AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional collaborative filtering (CF) based recommender systems tend to\nperform poorly when the user-item interactions/ratings are highly scarce. To\naddress this, we propose a learning framework that improves collaborative\nfiltering with a synthetic feedback loop (CF-SFL) to simulate the user\nfeedback. The proposed framework consists of a \"recommender\" and a \"virtual\nuser\". The \"recommender\" is formulated as a CF model, recommending items\naccording to observed user preference. The \"virtual user\" estimates rewards\nfrom the recommended items and generates a \\emph{feedback} in addition to the\nobserved user preference. The \"recommender\" connected with the \"virtual user\"\nconstructs a closed loop, that recommends users with items and imitates the\n\\emph{unobserved} feedback of the users to the recommended items. The synthetic\nfeedback is used to augment the observed user preference and improve\nrecommendation results. Theoretically, such model design can be interpreted as\ninverse reinforcement learning, which can be learned effectively via rollout\n(simulation). Experimental results show that the proposed framework is able to\nenrich the learning of user preference and boost the performance of existing\ncollaborative filtering methods on multiple datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 01:31:07 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 03:17:42 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Wang", "Wenlin", ""], ["Xu", "Hongteng", ""], ["Zhang", "Ruiyi", ""], ["Wang", "Wenqi", ""], ["Rai", "Piyush", ""], ["Carin", "Lawrence", ""]]}, {"id": "1910.12757", "submitter": "Aditya Mantha", "authors": "Aditya Mantha, Yokila Arora, Shubham Gupta, Praveenkumar Kanumala,\n  Zhiwei Liu, Stephen Guo, Kannan Achan", "title": "A Large-Scale Deep Architecture for Personalized Grocery Basket\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With growing consumer adoption of online grocery shopping through platforms\nsuch as Amazon Fresh, Instacart, and Walmart Grocery, there is a pressing\nbusiness need to provide relevant recommendations throughout the customer\njourney. In this paper, we introduce a production within-basket grocery\nrecommendation system, RTT2Vec, which generates real-time personalized product\nrecommendations to supplement the user's current grocery basket. We conduct\nextensive offline evaluation of our system and demonstrate a 9.4% uplift in\nprediction metrics over baseline state-of-the-art within-basket recommendation\nmodels. We also propose an approximate inference technique 11.6x times faster\nthan exact inference approaches. In production, our system has resulted in an\nincrease in average basket size, improved product discovery, and enabled faster\nuser check-out\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 06:54:55 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 18:24:28 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 02:03:18 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Mantha", "Aditya", ""], ["Arora", "Yokila", ""], ["Gupta", "Shubham", ""], ["Kanumala", "Praveenkumar", ""], ["Liu", "Zhiwei", ""], ["Guo", "Stephen", ""], ["Achan", "Kannan", ""]]}, {"id": "1910.12781", "submitter": "Malte Ludewig", "authors": "Malte Ludewig, Noemi Mauro, Sara Latifi, Dietmar Jannach", "title": "Empirical Analysis of Session-Based Recommendation Algorithms", "comments": null, "journal-ref": null, "doi": "10.1007/s11257-020-09277-1", "report-no": null, "categories": "cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are tools that support online users by pointing them to\npotential items of interest in situations of information overload. In recent\nyears, the class of session-based recommendation algorithms received more\nattention in the research literature. These algorithms base their\nrecommendations solely on the observed interactions with the user in an ongoing\nsession and do not require the existence of long-term preference profiles. Most\nrecently, a number of deep learning based (\"neural\") approaches to\nsession-based recommendations were proposed. However, previous research\nindicates that today's complex neural recommendation methods are not always\nbetter than comparably simple algorithms in terms of prediction accuracy.\n  With this work, our goal is to shed light on the state-of-the-art in the area\nof session-based recommendation and on the progress that is made with neural\napproaches. For this purpose, we compare twelve algorithmic approaches, among\nthem six recent neural methods, under identical conditions on various datasets.\nWe find that the progress in terms of prediction accuracy that is achieved with\nneural methods is still limited. In most cases, our experiments show that\nsimple heuristic methods based on nearest-neighbors schemes are preferable over\nconceptually and computationally more complex methods. Observations from a user\nstudy furthermore indicate that recommendations based on heuristic methods were\nalso well accepted by the study participants. To support future progress and\nreproducibility in this area, we publicly share the session-rec evaluation\nframework that was used in our research.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 16:08:58 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 15:36:15 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Ludewig", "Malte", ""], ["Mauro", "Noemi", ""], ["Latifi", "Sara", ""], ["Jannach", "Dietmar", ""]]}, {"id": "1910.13027", "submitter": "Farhad Farokhi", "authors": "Farhad Farokhi", "title": "Noiseless Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.IR cs.SY eess.SP eess.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we define noiseless privacy, as a non-stochastic rival to\ndifferential privacy, requiring that the outputs of a mechanism (i.e., function\ncomposition of a privacy-preserving mapping and a query) can attain only a few\nvalues while varying the data of an individual (the logarithm of the number of\nthe distinct values is bounded by the privacy budget). Therefore, the output of\nthe mechanism is not fully informative of the data of the individuals in the\ndataset. We prove several guarantees for noiselessly-private mechanisms. The\ninformation content of the output about the data of an individual, even if an\nadversary knows all the other entries of the private dataset, is bounded by the\nprivacy budget. The zero-error capacity of memory-less channels using\nnoiselessly private mechanisms for transmission is upper bounded by the privacy\nbudget. The performance of a non-stochastic hypothesis-testing adversary is\nbounded again by the privacy budget. Finally, assuming that an adversary has\naccess to a stochastic prior on the dataset, we prove that the estimation error\nof the adversary for individual entries of the dataset is lower bounded by a\ndecreasing function of the privacy budget. In this case, we also show that the\nmaximal information leakage is bounded by the privacy budget. In addition to\nprivacy guarantees, we prove that noiselessly-private mechanisms admit\ncomposition theorem and post-processing does not weaken their privacy\nguarantees. We prove that quantization operators can ensure noiseless privacy\nif the number of quantization levels is appropriately selected based on the\nsensitivity of the query and the privacy budget. Finally, we illustrate the\nprivacy merits of noiseless privacy using multiple datasets in energy and\ntransport.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 01:09:26 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Farokhi", "Farhad", ""]]}, {"id": "1910.13162", "submitter": "Hoang Nhat Suong", "authors": "Suong N. Hoang, Linh V. Nguyen, Tai Huynh and Vuong T. Pham", "title": "An Efficient Model for Sentiment Analysis of Electronic Product Reviews\n  in Vietnamese", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the past few years, the growth of e-commerce and digital marketing in\nVietnam has generated a huge volume of opinionated data. Analyzing those data\nwould provide enterprises with insight for better business decisions. In this\nwork, as part of the Advosights project, we study sentiment analysis of product\nreviews in Vietnamese. The final solution is based on Self-attention neural\nnetworks, a flexible architecture for text classification task with about\n90.16% of accuracy in 0.0124 second, a very fast inference time.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 10:06:56 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Hoang", "Suong N.", ""], ["Nguyen", "Linh V.", ""], ["Huynh", "Tai", ""], ["Pham", "Vuong T.", ""]]}, {"id": "1910.13166", "submitter": "Johanne Trippas R.", "authors": "Johanne R. Trippas, Damiano Spina, Paul Thomas, Mark Sanderson, Hideo\n  Joho, Lawrence Cavedon", "title": "Towards a Model for Spoken Conversational Search", "comments": "Paper accepted at Information Processing & Management on October 29,\n  2019, Spoken Conversational Search, Information Seeking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversation is the natural mode for information exchange in daily life, a\nspoken conversational interaction for search input and output is a logical\nformat for information seeking. However, the conceptualisation of user-system\ninteractions or information exchange in spoken conversational search (SCS) has\nnot been explored. The first step in conceptualising SCS is to understand the\nconversational moves used in an audio-only communication channel for search.\nThis paper explores conversational actions for the task of search. We define a\nqualitative methodology for creating conversational datasets, propose analysis\nprotocols, and develop the SCSdata. Furthermore, we use the SCSdata to create\nthe first annotation schema for SCS: the SCoSAS, enabling us to investigate\ninteractivity in SCS. We further establish that SCS needs to incorporate\ninteractivity and pro-activity to overcome the complexity that the information\nseeking process in an audio-only channel poses. In summary, this exploratory\nstudy unpacks the breadth of SCS. Our results highlight the need for\nintegrating discourse in future SCS models and contributes the advancement in\nthe formalisation of SCS models and the design of SCS systems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 10:22:48 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 01:10:10 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Trippas", "Johanne R.", ""], ["Spina", "Damiano", ""], ["Thomas", "Paul", ""], ["Sanderson", "Mark", ""], ["Joho", "Hideo", ""], ["Cavedon", "Lawrence", ""]]}, {"id": "1910.13197", "submitter": "Ghodai Abdelrahman", "authors": "Ghodai Abdelrahman and Qing Wang", "title": "Knowledge Tracing with Sequential Key-Value Memory Networks", "comments": null, "journal-ref": "Proceedings of the 42Nd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (2019)", "doi": "10.1145/3331184.3331195", "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can machines trace human knowledge like humans? Knowledge tracing (KT) is a\nfundamental task in a wide range of applications in education, such as massive\nopen online courses (MOOCs), intelligent tutoring systems, educational games,\nand learning management systems. It models dynamics in a student's knowledge\nstates in relation to different learning concepts through their interactions\nwith learning activities. Recently, several attempts have been made to use deep\nlearning models for tackling the KT problem. Although these deep learning\nmodels have shown promising results, they have limitations: either lack the\nability to go deeper to trace how specific concepts in a knowledge state are\nmastered by a student, or fail to capture long-term dependencies in an exercise\nsequence. In this paper, we address these limitations by proposing a novel deep\nlearning model for knowledge tracing, namely Sequential Key-Value Memory\nNetworks (SKVMN). This model unifies the strengths of recurrent modelling\ncapacity and memory capacity of the existing deep learning KT models for\nmodelling student learning. We have extensively evaluated our proposed model on\nfive benchmark datasets. The experimental results show that (1) SKVMN\noutperforms the state-of-the-art KT models on all datasets, (2) SKVMN can\nbetter discover the correlation between latent concepts and questions, and (3)\nSKVMN can trace the knowledge state of students dynamics, and a leverage\nsequential dependencies in an exercise sequence for improved predication\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 11:10:50 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Abdelrahman", "Ghodai", ""], ["Wang", "Qing", ""]]}, {"id": "1910.13425", "submitter": "Pratik Kayal", "authors": "Pratik Kayal, Mayank Singh, Pawan Goyal", "title": "Weakly-Supervised Deep Learning for Domain Invariant Sentiment\n  Classification", "comments": "5 Pages, 3 tables", "journal-ref": null, "doi": "10.1145/3371158.3371194", "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of learning a sentiment classification model that adapts well to any\ntarget domain, different from the source domain, is a challenging problem.\nMajority of the existing approaches focus on learning a common representation\nby leveraging both source and target data during training. In this paper, we\nintroduce a two-stage training procedure that leverages weakly supervised\ndatasets for developing simple lift-and-shift-based predictive models without\nbeing exposed to the target domain during the training phase. Experimental\nresults show that transfer with weak supervision from a source domain to\nvarious target domains provides performance very close to that obtained via\nsupervised training on the target domain itself.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 17:43:37 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2019 06:59:48 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Kayal", "Pratik", ""], ["Singh", "Mayank", ""], ["Goyal", "Pawan", ""]]}, {"id": "1910.13527", "submitter": "Yujia Zheng", "authors": "Yujia Zheng, Siyi Liu, Zailei Zhou", "title": "Balancing Multi-level Interactions for Session-based Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting user actions based on anonymous sessions is a challenge to general\nrecommendation systems because the lack of user profiles heavily limits\ndata-driven models. Recently, session-based recommendation methods have\nachieved remarkable results in dealing with this task. However, the upper bound\nof performance can still be boosted through the innovative exploration of\nlimited data. In this paper, we propose a novel method, namely Intra-and\nInter-session Interaction-aware Graph-enhanced Network, to take inter-session\nitem-level interactions into account. Different from existing intra-session\nitem-level interactions and session-level collaborative information, our\nintroduced data represents complex item-level interactions between different\nsessions. For mining the new data without breaking the equilibrium of the model\nbetween different interactions, we construct an intra-session graph and an\ninter-session graph for the current session. The former focuses on item-level\ninteractions within a single session and the latter models those between items\namong neighborhood sessions. Then different approaches are employed to encode\nthe information of two graphs according to different structures, and the\ngenerated latent vectors are combined to balance the model across different\nscopes. Experiments on real-world datasets verify that our method outperforms\nother state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 20:51:19 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Zheng", "Yujia", ""], ["Liu", "Siyi", ""], ["Zhou", "Zailei", ""]]}, {"id": "1910.13561", "submitter": "Mohamed Medhat Gaber", "authors": "Safwan Shatnawi, Mohamed Medhat Gaber, Mihaela Cocea", "title": "A Heuristically Modified FP-Tree for Ontology Learning with Applications\n  in Education", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a heuristically modified FP-Tree for ontology learning from text.\nUnlike previous research, for concept extraction, we use a regular expression\nparser approach widely adopted in compiler construction, i.e., deterministic\nfinite automata (DFA). Thus, the concepts are extracted from unstructured\ndocuments. For ontology learning, we use a frequent pattern mining approach and\nemploy a rule mining heuristic function to enhance its quality. This process\ndoes not rely on predefined lexico-syntactic patterns, thus, it is applicable\nfor different subjects. We employ the ontology in a question-answering system\nfor students' content-related questions. For validation, we used textbook\nquestions/answers and questions from online course forums. Subject experts\nrated the quality of the system's answers on a subset of questions and their\nratings were used to identify the most appropriate automatic semantic text\nsimilarity metric to use as a validation metric for all answers. The Latent\nSemantic Analysis was identified as the closest to the experts' ratings. We\ncompared the use of our ontology with the use of Text2Onto for the\nquestion-answering system and found that with our ontology 80% of the questions\nwere answered, while with Text2Onto only 28.4% were answered, thanks to the\nfiner grained hierarchy our approach is able to produce.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 22:12:44 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Shatnawi", "Safwan", ""], ["Gaber", "Mohamed Medhat", ""], ["Cocea", "Mihaela", ""]]}, {"id": "1910.13641", "submitter": "EPTCS", "authors": "Georgiana Caltais (Konstanz University), Jean Krivine (CNRS)", "title": "Proceedings of the 4th Workshop on Formal Reasoning about Causation,\n  Responsibility, and Explanations in Science and Technology", "comments": null, "journal-ref": "EPTCS 308, 2019", "doi": "10.4204/EPTCS.308", "report-no": null, "categories": "cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fourth edition of the international workshop on Causation, Responsibility\nand Explanation took place in Prague (Czech Republic) as part of ETAPS 2019.\nThe program consisted in 5 invited speakers and 4 regular papers, whose\nselection was based on a careful reviewing process and that are included in\nthese proceedings.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 02:58:45 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Caltais", "Georgiana", "", "Konstanz University"], ["Krivine", "Jean", "", "CNRS"]]}, {"id": "1910.13683", "submitter": "Sasindu Wijeratne", "authors": "Sasindu Wijeratne, Ashen Ekanayake, Sandaruwan Jayaweera, Danuka\n  Ravishan, Ajith Pasqual", "title": "Scalable High Performance SDN Switch Architecture on FPGA for Core\n  Networks", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing heterogeneity in network user requirements, dynamically\nvarying day to day network traffic patterns and delay in-network service\ndeployment, there is a huge demand for scalability and flexibility in modern\nnetworking infrastructure, which in return has paved way for the introduction\nof Software Defined Networking (SDN) in core networks. In this paper, we\npresent an FPGA-based switch that is fully compliant with OpenFlow; the\npioneering protocol for southbound interface of SDN. The switch architecture is\ncompletely implemented on hardware. The design consists of an OpenFlow\nSouthbound agent which can process OpenFlow packets at a rate of 10Gbps. The\nproposed architecture speed scales up to 400Gbps while it consumes only 60% of\nresources on a Xilinx Virtex-7 featuring XC7VX485T FPGA.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 05:45:37 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Wijeratne", "Sasindu", ""], ["Ekanayake", "Ashen", ""], ["Jayaweera", "Sandaruwan", ""], ["Ravishan", "Danuka", ""], ["Pasqual", "Ajith", ""]]}, {"id": "1910.13830", "submitter": "Tharun Kumar Reddy Medini", "authors": "Tharun Medini, Qixuan Huang, Yiqiu Wang, Vijai Mohan, Anshumali\n  Shrivastava", "title": "Extreme Classification in Log Memory using Count-Min Sketch: A Case\n  Study of Amazon Search with 50M Products", "comments": "Published at NeurIPS 2019. arXiv admin note: text overlap with\n  arXiv:1810.04254", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, it has been shown that many hard AI tasks, especially in\nNLP, can be naturally modeled as extreme classification problems leading to\nimproved precision. However, such models are prohibitively expensive to train\ndue to the memory blow-up in the last layer. For example, a reasonable softmax\nlayer for the dataset of interest in this paper can easily reach well beyond\n100 billion parameters (>400 GB memory). To alleviate this problem, we present\nMerged-Average Classifiers via Hashing (MACH), a generic K-classification\nalgorithm where memory provably scales at O(logK) without any strong assumption\non the classes. MACH is subtly a count-min sketch structure in disguise, which\nuses universal hashing to reduce classification with a large number of classes\nto few embarrassingly parallel and independent classification tasks with a\nsmall (constant) number of classes. MACH naturally provides a technique for\nzero communication model parallelism. We experiment with 6 datasets; some\nmulticlass and some multilabel, and show consistent improvement over respective\nstate-of-the-art baselines. In particular, we train an end-to-end deep\nclassifier on a private product search dataset sampled from Amazon Search\nEngine with 70 million queries and 49.46 million products. MACH outperforms, by\na significant margin,the state-of-the-art extreme classification models\ndeployed on commercial search engines: Parabel and dense embedding models. Our\nlargest model has 6.4 billion parameters and trains in less than 35 hours on a\nsingle p3.16x machine. Our training times are 7-10x faster, and our memory\nfootprints are 2-4x smaller than the best baselines. This training time is also\nsignificantly lower than the one reported by Google's mixture of experts (MoE)\nlanguage model on a comparable model size and hardware.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 19:41:28 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Medini", "Tharun", ""], ["Huang", "Qixuan", ""], ["Wang", "Yiqiu", ""], ["Mohan", "Vijai", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1910.13849", "submitter": "Jaber Kakar", "authors": "Jaber Kakar, Anton Khristoforov, Seyedhamed Ebadifar, Aydin Sezgin", "title": "Uplink-Downlink Tradeoff in Secure Distributed Matrix Multiplication", "comments": "Amazon EC2 results includes now encoding time. Second-Order Encoding\n  Strategy added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DC cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In secure distributed matrix multiplication (SDMM) the multiplication\n$\\mathbf{A}\\mathbf{B}$ from two private matrices $\\mathbf{A}$ and $\\mathbf{B}$\nis outsourced by a user to $N$ distributed servers. In $\\ell$-SDMM, the goal is\nto a design a joint communication-computation procedure that optimally balances\nconflicting communication and computation metrics without leaking any\ninformation on both $\\mathbf{A}$ and $\\mathbf{B}$ to any set of $\\ell\\leq N$\nservers. To this end, the user applies coding with $\\tilde{\\mathbf{A}}_i$ and\n$\\tilde{\\mathbf{B}}_i$ representing encoded versions of $\\mathbf{A}$ and\n$\\mathbf{B}$ destined to the $i$-th server. Now, SDMM involves multiple\ntradeoffs. One such tradeoff is the tradeoff between uplink (UL) and downlink\n(DL) costs. To find a good balance between these two metrics, we propose two\nschemes which we term USCSA and GSCSA that are based on secure cross subspace\nalignment (SCSA). We show that there are various scenarios where they\noutperform existing SDMM schemes from the literature with respect to the UL-DL\nefficiency. Next, we implement schemes from the literature, including USCSA and\nGSCSA, and test their performance on Amazon EC2. Our numerical results show\nthat USCSA and GSCSA establish a good balance between the time spend on the\ncommunication and computation in SDMMs. This is because they combine advantages\nof polynomial codes, namely low time for the upload of\n$\\left(\\tilde{\\mathbf{A}}_i,\\tilde{\\mathbf{B}}_i\\right)_{i=1}^{N}$ and the\ncomputation of $\\mathbf{O}_i=\\tilde{\\mathbf{A}}_i\\tilde{\\mathbf{B}}_i$, with\nthose of SCSA, being a low timing overhead for the download of\n$\\left(\\mathbf{O}_i\\right)_{i=1}^{N}$ and the decoding of\n$\\mathbf{A}\\mathbf{B}$.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 13:46:55 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 15:34:14 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 13:22:00 GMT"}, {"version": "v4", "created": "Sat, 2 May 2020 12:45:03 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Kakar", "Jaber", ""], ["Khristoforov", "Anton", ""], ["Ebadifar", "Seyedhamed", ""], ["Sezgin", "Aydin", ""]]}, {"id": "1910.14025", "submitter": "Chen Li", "authors": "Linmei Hu, Chen Li, Chuan Shi, Cheng Yang, Chao Shao", "title": "Graph Neural News Recommendation with Long-term and Short-term Interest\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the information explosion of news articles, personalized news\nrecommendation has become important for users to quickly find news that they\nare interested in. Existing methods on news recommendation mainly include\ncollaborative filtering methods which rely on direct user-item interactions and\ncontent based methods which characterize the content of user reading history.\nAlthough these methods have achieved good performances, they still suffer from\ndata sparse problem, since most of them fail to extensively exploit high-order\nstructure information (similar users tend to read similar news articles) in\nnews recommendation systems. In this paper, we propose to build a heterogeneous\ngraph to explicitly model the interactions among users, news and latent topics.\nThe incorporated topic information would help indicate a user's interest and\nalleviate the sparsity of user-item interactions. Then we take advantage of\ngraph neural networks to learn user and news representations that encode\nhigh-order structure information by propagating embeddings over the graph. The\nlearned user embeddings with complete historic user clicks capture the users'\nlong-term interests. We also consider a user's short-term interest using the\nrecent reading history with an attention based LSTM model. Experimental results\non real-world datasets show that our proposed model significantly outperforms\nstate-of-the-art methods on news recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 08:04:43 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 02:20:03 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Hu", "Linmei", ""], ["Li", "Chen", ""], ["Shi", "Chuan", ""], ["Yang", "Cheng", ""], ["Shao", "Chao", ""]]}, {"id": "1910.14084", "submitter": "Sahisnu Mazumder", "authors": "Sahisnu Mazumder, Bing Liu, Shuai Wang, Sepideh Esmaeilpour", "title": "Building an Application Independent Natural Language Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches to building natural language (NL) interfaces typically\nuse a semantic parser to parse the user command and convert it to a logical\nform, which is then translated to an executable action in an application.\nHowever, it is still challenging for a semantic parser to correctly parse\nnatural language. For a different domain, the parser may need to be retrained\nor tuned, and a new translator also needs to be written to convert the logical\nforms to executable actions. In this work, we propose a novel and application\nindependent approach to building NL interfaces that does not need a semantic\nparser or a translator. It is based on natural language to natural language\nmatching and learning, where the representation of each action and each user\ncommand are both in natural language. To perform a user intended action, the\nsystem only needs to match the user command with the correct action\nrepresentation, and then execute the corresponding action. The system also\ninteractively learns new (paraphrased) commands for actions to expand the\naction representations over time. Our experimental results show the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 18:57:28 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Mazumder", "Sahisnu", ""], ["Liu", "Bing", ""], ["Wang", "Shuai", ""], ["Esmaeilpour", "Sepideh", ""]]}, {"id": "1910.14164", "submitter": "Jacopo Tagliabue", "authors": "Jacopo Tagliabue, Reuben Cohn-Gordon", "title": "Lexical Learning as an Online Optimal Experiment: Building Efficient\n  Search Engines through Human-Machine Collaboration", "comments": null, "journal-ref": "WIP and Demo track, HCOMP 2019", "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information retrieval (IR) systems need to constantly update their knowledge\nas target objects and user queries change over time. Due to the power-law\nnature of linguistic data, learning lexical concepts is a problem resisting\nstandard machine learning approaches: while manual intervention is always\npossible, a more general and automated solution is desirable. In this work, we\npropose a novel end-to-end framework that models the interaction between a\nsearch engine and users as a virtuous human-in-the-loop inference. The proposed\nframework is the first to our knowledge combining ideas from psycholinguistics\nand experiment design to maximize efficiency in IR. We provide a brief overview\nof the main components and initial simulations in a toy world, showing how\ninference works end-to-end and discussing preliminary results and next steps.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 22:35:36 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Tagliabue", "Jacopo", ""], ["Cohn-Gordon", "Reuben", ""]]}, {"id": "1910.14238", "submitter": "Jianxin Ma", "authors": "Jianxin Ma, Chang Zhou, Peng Cui, Hongxia Yang, Wenwu Zhu", "title": "Learning Disentangled Representations for Recommendation", "comments": "To appear in the Proceedings of the Thirty-third Conference on Neural\n  Information Processing Systems (NeurIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User behavior data in recommender systems are driven by the complex\ninteractions of many latent factors behind the users' decision making\nprocesses. The factors are highly entangled, and may range from high-level ones\nthat govern user intentions, to low-level ones that characterize a user's\npreference when executing an intention. Learning representations that uncover\nand disentangle these latent factors can bring enhanced robustness,\ninterpretability, and controllability. However, learning such disentangled\nrepresentations from user behavior is challenging, and remains largely\nneglected by the existing literature. In this paper, we present the MACRo-mIcro\nDisentangled Variational Auto-Encoder (MacridVAE) for learning disentangled\nrepresentations from user behavior. Our approach achieves macro disentanglement\nby inferring the high-level concepts associated with user intentions (e.g., to\nbuy a shirt or a cellphone), while capturing the preference of a user regarding\nthe different concepts separately. A micro-disentanglement regularizer,\nstemming from an information-theoretic interpretation of VAEs, then forces each\ndimension of the representations to independently reflect an isolated low-level\nfactor (e.g., the size or the color of a shirt). Empirical results show that\nour approach can achieve substantial improvement over the state-of-the-art\nbaselines. We further demonstrate that the learned representations are\ninterpretable and controllable, which can potentially lead to a new paradigm\nfor recommendation where users are given fine-grained control over targeted\naspects of the recommendation lists.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 03:31:56 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Ma", "Jianxin", ""], ["Zhou", "Chang", ""], ["Cui", "Peng", ""], ["Yang", "Hongxia", ""], ["Zhu", "Wenwu", ""]]}, {"id": "1910.14353", "submitter": "Valeriya Slovikovskaya", "authors": "Valeriya Slovikovskaya", "title": "Transfer Learning from Transformers to Fake News Challenge Stance\n  Detection (FNC-1) Task", "comments": "12 pages, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report improved results of the Fake News Challenge Stage 1\n(FNC-1) stance detection task. This gain in performance is due to the\ngeneralization power of large language models based on Transformer\narchitecture, invented, trained and publicly released over the last two years.\nSpecifically (1) we improved the FNC-1 best performing model adding BERT\nsentence embedding of input sequences as a model feature, (2) we fine-tuned\nBERT, XLNet, and RoBERTa transformers on FNC-1 extended dataset and obtained\nstate-of-the-art results on FNC-1 task.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 10:32:43 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Slovikovskaya", "Valeriya", ""]]}, {"id": "1910.14395", "submitter": "Selena Savic", "authors": "Selena Savic", "title": "Great New Design: How Do We Talk about Media Architecture in Social\n  Media", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In social media, we communicate through pictures, videos, short codes, links,\npartial phrases. It is a rich, and digitally documented communication channel\nthat relies on a multitude of media and forms. These channels are sorted by\nalgorithms as organizers of discourse, mostly with the goal of channeling\nattention. In this research, we used Twitter to study the way Media\nArchitecture is discussed within the community of architects, designers,\nresearchers and policy makers. We look at the way they spontaneously share\nopinions on their engagement with digital infrastructures, networked places and\nhybrid public spaces. What can we do with all those opinions? We propose here\nthe use of text-mining and machine learning techniques to identify important\nconcepts and patterns in this prolific communication stream. We discuss how\nsuch techniques could inform the practice and emergence of future trends.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 11:49:32 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Savic", "Selena", ""]]}, {"id": "1910.14424", "submitter": "Rodrigo Nogueira", "authors": "Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, Jimmy Lin", "title": "Multi-Stage Document Ranking with BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of deep neural networks pre-trained via language modeling tasks\nhas spurred a number of successful applications in natural language processing.\nThis work explores one such popular model, BERT, in the context of document\nranking. We propose two variants, called monoBERT and duoBERT, that formulate\nthe ranking problem as pointwise and pairwise classification, respectively.\nThese two models are arranged in a multi-stage ranking architecture to form an\nend-to-end search system. One major advantage of this design is the ability to\ntrade off quality against latency by controlling the admission of candidates\ninto each pipeline stage, and by doing so, we are able to find operating points\nthat offer a good balance between these two competing metrics. On two\nlarge-scale datasets, MS MARCO and TREC CAR, experiments show that our model\nproduces results that are either at or comparable to the state of the art.\nAblation studies show the contributions of each component and characterize the\nlatency/quality tradeoff space.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 12:45:40 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Nogueira", "Rodrigo", ""], ["Yang", "Wei", ""], ["Cho", "Kyunghyun", ""], ["Lin", "Jimmy", ""]]}, {"id": "1910.14651", "submitter": "Marc Faddoul", "authors": "Marc Faddoul", "title": "Which Factors Impact Engagement on News Articles on Facebook?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media is increasingly being used as a news-platform. To reach their\nintended audience, newspapers need for their articles to be well ranked by\nFacebook's news-feed algorithm. The number of likes, shares and other reactions\ndetermine the lead scoring criteria. This paper will try to assess how the\nreaction volume is impacted by the following criteria: (1) Delay between event\nand post release; (2) Time of the day the post is published; and (3) Post\nformat: video, photo or text. To isolate the effect of the publication time and\npost format on a post, we need to control for the news-event and the publishing\nnewspaper. For that end, a news-aggregator is designed and implemented, to\ngroup together posts that relate to the same news-event. This tool gave some\nspin-off results, allowing the ability to map newspapers by similarity and to\ndetect some topic omissions.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:41:31 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Faddoul", "Marc", ""]]}]