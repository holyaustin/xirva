[{"id": "1412.0007", "submitter": "Johannes Stegmann Dr.", "authors": "Johannes Stegmann", "title": "Paradigm shifts. Part I. Collagen. Confirming and complementing the work\n  of Henry Small", "comments": "6 pages, 1 figure, 3 tables, corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paradigm shift in collagen research during the early 1970s marked by the\ndiscovery of the collagen precursor molecule procollagen was traced using\nco-citation analysis and title word frequency determination, confirming\nprevious work performed by Henry Small.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2014 22:52:53 GMT"}, {"version": "v2", "created": "Sun, 7 Dec 2014 23:36:54 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Stegmann", "Johannes", ""]]}, {"id": "1412.0744", "submitter": "Artemy Kolchinsky", "authors": "Artemy Kolchinsky, An\\'alia Louren\\c{c}o, Heng-Yi Wu, Lang Li, Luis M.\n  Rocha", "title": "Extraction of Pharmacokinetic Evidence of Drug-drug Interactions from\n  the Literature", "comments": "PLOS One (2015)", "journal-ref": null, "doi": "10.1371/journal.pone.0122199", "report-no": null, "categories": "stat.ML cs.IR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drug-drug interaction (DDI) is a major cause of morbidity and mortality and a\nsubject of intense scientific interest. Biomedical literature mining can aid\nDDI research by extracting evidence for large numbers of potential interactions\nfrom published literature and clinical databases. Though DDI is investigated in\ndomains ranging in scale from intracellular biochemistry to human populations,\nliterature mining has not been used to extract specific types of experimental\nevidence, which are reported differently for distinct experimental goals. We\nfocus on pharmacokinetic evidence for DDI, essential for identifying causal\nmechanisms of putative interactions and as input for further pharmacological\nand pharmaco-epidemiology investigations. We used manually curated corpora of\nPubMed abstracts and annotated sentences to evaluate the efficacy of literature\nmining on two tasks: first, identifying PubMed abstracts containing\npharmacokinetic evidence of DDIs; second, extracting sentences containing such\nevidence from abstracts. We implemented a text mining pipeline and evaluated it\nusing several linear classifiers and a variety of feature transforms. The most\nimportant textual features in the abstract and sentence classification tasks\nwere analyzed. We also investigated the performance benefits of using features\nderived from PubMed metadata fields, various publicly available named entity\nrecognizers, and pharmacokinetic dictionaries. Several classifiers performed\nvery well in distinguishing relevant and irrelevant abstracts (reaching\nF1~=0.93, MCC~=0.74, iAUC~=0.99) and sentences (F1~=0.76, MCC~=0.65,\niAUC~=0.83). We found that word bigram features were important for achieving\noptimal classifier performance and that features derived from Medical Subject\nHeadings (MeSH) terms significantly improved abstract classification. ...\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 00:01:39 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 16:45:42 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Kolchinsky", "Artemy", ""], ["Louren\u00e7o", "An\u00e1lia", ""], ["Wu", "Heng-Yi", ""], ["Li", "Lang", ""], ["Rocha", "Luis M.", ""]]}, {"id": "1412.0879", "submitter": "Sean Gallagher", "authors": "Sean Gallagher, Wlodek Zadrozny, Walid Shalaby, Adarsh Avadhani", "title": "Watsonsim: Overview of a Question Answering Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of the project is to design and run a system similar to Watson,\ndesigned to answer Jeopardy questions. In the course of a semester, we\ndeveloped an open source question answering system using the Indri, Lucene,\nBing and Google search engines, Apache UIMA, Open- and CoreNLP, and Weka among\nadditional modules. By the end of the semester, we achieved 18% accuracy on\nJeopardy questions, and work has not stopped since then.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 12:15:18 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Gallagher", "Sean", ""], ["Zadrozny", "Wlodek", ""], ["Shalaby", "Walid", ""], ["Avadhani", "Adarsh", ""]]}, {"id": "1412.1576", "submitter": "Xun Zheng", "authors": "Jinhui Yuan, Fei Gao, Qirong Ho, Wei Dai, Jinliang Wei, Xun Zheng,\n  Eric P. Xing, Tie-Yan Liu, Wei-Ying Ma", "title": "LightLDA: Big Topic Models on Modest Compute Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When building large-scale machine learning (ML) programs, such as big topic\nmodels or deep neural nets, one usually assumes such tasks can only be\nattempted with industrial-sized clusters with thousands of nodes, which are out\nof reach for most practitioners or academic researchers. We consider this\nchallenge in the context of topic modeling on web-scale corpora, and show that\nwith a modest cluster of as few as 8 machines, we can train a topic model with\n1 million topics and a 1-million-word vocabulary (for a total of 1 trillion\nparameters), on a document collection with 200 billion tokens -- a scale not\nyet reported even with thousands of machines. Our major contributions include:\n1) a new, highly efficient O(1) Metropolis-Hastings sampling algorithm, whose\nrunning cost is (surprisingly) agnostic of model size, and empirically\nconverges nearly an order of magnitude faster than current state-of-the-art\nGibbs samplers; 2) a structure-aware model-parallel scheme, which leverages\ndependencies within the topic model, yielding a sampling strategy that is\nfrugal on machine memory and network communication; 3) a differential\ndata-structure for model storage, which uses separate data structures for high-\nand low-frequency words to allow extremely large models to fit in memory, while\nmaintaining high inference speed; and 4) a bounded asynchronous data-parallel\nscheme, which allows efficient distributed processing of massive data via a\nparameter server. Our distribution strategy is an instance of the\nmodel-and-data-parallel programming model underlying the Petuum framework for\ngeneral distributed ML, and was implemented on top of the Petuum open-source\nsystem. We provide experimental evidence showing how this development puts\nmassive models within reach on a small cluster while still enjoying\nproportional time cost reductions with increasing cluster size, in comparison\nwith alternative options.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 07:49:12 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Yuan", "Jinhui", ""], ["Gao", "Fei", ""], ["Ho", "Qirong", ""], ["Dai", "Wei", ""], ["Wei", "Jinliang", ""], ["Zheng", "Xun", ""], ["Xing", "Eric P.", ""], ["Liu", "Tie-Yan", ""], ["Ma", "Wei-Ying", ""]]}, {"id": "1412.1888", "submitter": "Rafi Muhammad", "authors": "Muhammad Rafi, Farnaz Amin, Mohammad Shahid Shaikh", "title": "Document clustering using graph based document representation with\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document clustering is an unsupervised approach in which a large collection\nof documents (corpus) is subdivided into smaller, meaningful, identifiable, and\nverifiable sub-groups (clusters). Meaningful representation of documents and\nimplicitly identifying the patterns, on which this separation is performed, is\nthe challenging part of document clustering. We have proposed a document\nclustering technique using graph based document representation with\nconstraints. A graph data structure can easily capture the non-linear\nrelationships of nodes, document contains various feature terms that can be\nnon-linearly connected hence a graph can easily represents this information.\nConstrains, are explicit conditions for document clustering where background\nknowledge is use to set the direction for Linking or Not-Linking a set of\ndocuments for a target clusters, thus guiding the clustering process. We deemed\nclustering is an ill-define problem, there can be many clustering results.\nBackground knowledge can be used to drive the clustering algorithm in the right\ndirection. We have proposed three different types of constraints, Instance\nlevel, corpus level and cluster level constraints. A new algorithm Constrained\nHAC is also proposed which will incorporate Instance level constraints as prior\nknowledge; it will guide the clustering process leading to better results.\nExtensive set of experiments have been performed on both synthetic and standard\ndocument clustering datasets, results are compared on standard clustering\nmeasures like: purity, entropy and F-measure. Results clearly establish that\nour proposed approach leads to improvement in cluster quality.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 03:47:34 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Rafi", "Muhammad", ""], ["Amin", "Farnaz", ""], ["Shaikh", "Mohammad Shahid", ""]]}, {"id": "1412.2416", "submitter": "Johannes Stegmann Dr.", "authors": "Johannes Stegmann", "title": "Paradigm shifts. Part II. Reverse Transcriptase. Analysis of reference\n  stability and word frequencies", "comments": "10 pages, 7 tables, 1 figure, corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reverse transcription paradigm shift in RNA tumor virus research marked\nby the discovery of the reverse transcriptase in 1970 was traced using\nco-citation and title word frequency analysis. It is shown that this event is\nassociated with a break in citation patterns and the occurrence of previously\nunknown technical terms.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 23:51:35 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 17:22:04 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Stegmann", "Johannes", ""]]}, {"id": "1412.3056", "submitter": "Mohammad S. Qaseem", "authors": "Mohammad S. Qaseem and A. Govardhan", "title": "Phishing Detection in IMs using Domain Ontology and CBA - An innovative\n  Rule Generation Approach", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": "10.5121/ijist.2014.4601", "report-no": null, "categories": "cs.CR cs.AI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User ignorance towards the use of communication services like Instant\nMessengers, emails, websites, social networks etc. is becoming the biggest\nadvantage for phishers. It is required to create technical awareness in users\nby educating them to create a phishing detection application which would\ngenerate phishing alerts for the user so that phishing messages are not\nignored. The lack of basic security features to detect and prevent phishing has\nhad a profound effect on the IM clients, as they lose their faith in e-banking\nand e-commerce transactions, which will have a disastrous impact on the\ncorporate and banking sectors and businesses which rely heavily on the\ninternet. Very little research contributions were available in for phishing\ndetection in Instant messengers. A context based, dynamic and intelligent\nphishing detection methodology in IMs is proposed, to analyze and detect\nphishing in Instant Messages with relevance to domain ontology (OBIE) and\nutilizes the Classification based on Association (CBA) for generating phishing\nrules and alerting the victims. A PDS Monitoring system algorithm is used to\nidentify the phishing activity during exchange of messages in IMs, with high\nratio of precision and recall. The results have shown improvement by the\nincreased percentage of precision and recall when compared to the existing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 11:02:24 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Qaseem", "Mohammad S.", ""], ["Govardhan", "A.", ""]]}, {"id": "1412.3103", "submitter": "Aniket Chakrabarti", "authors": "Aniket Chakrabarti and Srinivasan Parthasarathy", "title": "Sequential Hypothesis Tests for Adaptive Locality Sensitive Hashing", "comments": null, "journal-ref": null, "doi": "10.1145/2736277.2741665", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All pairs similarity search is a problem where a set of data objects is given\nand the task is to find all pairs of objects that have similarity above a\ncertain threshold for a given similarity measure-of-interest. When the number\nof points or dimensionality is high, standard solutions fail to scale\ngracefully. Approximate solutions such as Locality Sensitive Hashing (LSH) and\nits Bayesian variants (BayesLSH and BayesLSHLite) alleviate the problem to some\nextent and provides substantial speedup over traditional index based\napproaches. BayesLSH is used for pruning the candidate space and computation of\napproximate similarity, whereas BayesLSHLite can only prune the candidates, but\nsimilarity needs to be computed exactly on the original data. Thus where ever\nthe explicit data representation is available and exact similarity computation\nis not too expensive, BayesLSHLite can be used to aggressively prune candidates\nand provide substantial speedup without losing too much on quality. However,\nthe loss in quality is higher in the BayesLSH variant, where explicit data\nrepresentation is not available, rather only a hash sketch is available and\nsimilarity has to be estimated approximately. In this work we revisit the LSH\nproblem from a Frequentist setting and formulate sequential tests for composite\nhypothesis (similarity greater than or less than threshold) that can be\nleveraged by such LSH algorithms for adaptively pruning candidates\naggressively. We propose a vanilla sequential probability ration test (SPRT)\napproach based on this idea and two novel variants. We extend these variants to\nthe case where approximate similarity needs to be computed using fixed-width\nsequential confidence interval generation technique.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 18:31:17 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Chakrabarti", "Aniket", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1412.3352", "submitter": "Neda Pourali", "authors": "Neda Pourali", "title": "Web image annotation by diffusion maps manifold learning algorithm", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": "10.5121/ijfcst.2014.4606", "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image annotation is one of the most challenging problems in machine\nvision areas. The goal of this task is to predict number of keywords\nautomatically for images captured in real data. Many methods are based on\nvisual features in order to calculate similarities between image samples. But\nthe computation cost of these approaches is very high. These methods require\nmany training samples to be stored in memory. To lessen this burden, a number\nof techniques have been developed to reduce the number of features in a\ndataset. Manifold learning is a popular approach to nonlinear dimensionality\nreduction. In this paper, we investigate Diffusion maps manifold learning\nmethod for web image auto-annotation task. Diffusion maps manifold learning\nmethod is used to reduce the dimension of some visual features. Extensive\nexperiments and analysis on NUS-WIDE-LITE web image dataset with different\nvisual features show how this manifold learning dimensionality reduction method\ncan be applied effectively to image annotation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 10:38:28 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Pourali", "Neda", ""]]}, {"id": "1412.3697", "submitter": "Vincenzo Nicosia", "authors": "A. Fiasconaro, M. Tumminello, V. Nicosia, V. Latora, R. N. Mantegna", "title": "Hybrid recommendation methods in complex networks", "comments": "9 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose here two new recommendation methods, based on the appropriate\nnormalization of already existing similarity measures, and on the convex\ncombination of the recommendation scores derived from similarity between users\nand between objects. We validate the proposed measures on three relevant data\nsets, and we compare their performance with several recommendation systems\nrecently proposed in the literature. We show that the proposed similarity\nmeasures allow to attain an improvement of performances of up to 20\\% with\nrespect to existing non-parametric methods, and that the accuracy of a\nrecommendation can vary widely from one specific bipartite network to another,\nwhich suggests that a careful choice of the most suitable method is highly\nrelevant for an effective recommendation on a given system. Finally, we studied\nhow an increasing presence of random links in the network affects the\nrecommendation scores, and we found that one of the two recommendation\nalgorithms introduced here can systematically outperform the others in noisy\ndata sets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 16:34:50 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Fiasconaro", "A.", ""], ["Tumminello", "M.", ""], ["Nicosia", "V.", ""], ["Latora", "V.", ""], ["Mantegna", "R. N.", ""]]}, {"id": "1412.3898", "submitter": "Lu Yu", "authors": "Lu Yu and Junming Huang and Chuang Liu and Zike Zhang", "title": "ILCR: Item-based Latent Factors for Sparse Collaborative Retrieval", "comments": "10 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactions between search and recommendation have recently attracted\nsignificant attention, and several studies have shown that many potential\napplications involve with a joint problem of producing recommendations to users\nwith respect to a given query, termed $Collaborative$ $Retrieval$ (CR).\nSuccessful algorithms designed for CR should be potentially flexible at dealing\nwith the sparsity challenges since the setup of collaborative retrieval\nassociates with a given $query$ $\\times$ $user$ $\\times$ $item$ tensor instead\nof traditional $user$ $\\times$ $item$ matrix. Recently, several works are\nproposed to study CR task from users' perspective. In this paper, we aim to\nsufficiently explore the sophisticated relationship of each $query$ $\\times$\n$user$ $\\times$ $item$ triple from items' perspective. By integrating\nitem-based collaborative information for this joint task, we present an\nalternative factorized model that could better evaluate the ranks of those\nitems with sparse information for the given query-user pair. In addition, we\nsuggest to employ a recently proposed scalable ranking learning algorithm,\nnamely BPR, to optimize the state-of-the-art approach, $Latent$ $Collaborative$\n$Retrieval$ model, instead of the original learning algorithm. The experimental\nresults on two real-world datasets, (i.e. \\emph{Last.fm}, \\emph{Yelp}),\ndemonstrate the efficiency and effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 06:32:47 GMT"}], "update_date": "2014-12-15", "authors_parsed": [["Yu", "Lu", ""], ["Huang", "Junming", ""], ["Liu", "Chuang", ""], ["Zhang", "Zike", ""]]}, {"id": "1412.4160", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen, Dai Quoc Nguyen, Son Bao Pham", "title": "Ripple Down Rules for Question Answering", "comments": "V1: 21 pages, 7 figures, 10 tables. V2: 8 figures, 10 tables; shorten\n  section 2; change sections 4.3 and 5.1.2. V3: Accepted for publication in the\n  Semantic Web journal. V4 (Author's manuscript): camera ready version,\n  available from the Semantic Web journal at\n  http://www.semantic-web-journal.net", "journal-ref": null, "doi": "10.3233/SW-150204", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed a new trend of building ontology-based question\nanswering systems. These systems use semantic web information to produce more\nprecise answers to users' queries. However, these systems are mostly designed\nfor English. In this paper, we introduce an ontology-based question answering\nsystem named KbQAS which, to the best of our knowledge, is the first one made\nfor Vietnamese. KbQAS employs our question analysis approach that\nsystematically constructs a knowledge base of grammar rules to convert each\ninput question into an intermediate representation element. KbQAS then takes\nthe intermediate representation element with respect to a target ontology and\napplies concept-matching techniques to return an answer. On a wide range of\nVietnamese questions, experimental results show that the performance of KbQAS\nis promising with accuracies of 84.1% and 82.4% for analyzing input questions\nand retrieving output answers, respectively. Furthermore, our question analysis\napproach can easily be applied to new domains and new languages, thus saving\ntime and human effort.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 23:30:06 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 14:20:55 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2015 14:14:09 GMT"}, {"version": "v4", "created": "Wed, 4 Nov 2015 23:39:58 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Nguyen", "Dat Quoc", ""], ["Nguyen", "Dai Quoc", ""], ["Pham", "Son Bao", ""]]}, {"id": "1412.4986", "submitter": "Hsiang-Fu Yu", "authors": "Hsiang-Fu Yu and Cho-Jui Hsieh and Hyokun Yun and S.V.N Vishwanathan\n  and Inderjit S. Dhillon", "title": "A Scalable Asynchronous Distributed Algorithm for Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning meaningful topic models with massive document collections which\ncontain millions of documents and billions of tokens is challenging because of\ntwo reasons: First, one needs to deal with a large number of topics (typically\nin the order of thousands). Second, one needs a scalable and efficient way of\ndistributing the computation across multiple machines. In this paper we present\na novel algorithm F+Nomad LDA which simultaneously tackles both these problems.\nIn order to handle large number of topics we use an appropriately modified\nFenwick tree. This data structure allows us to sample from a multinomial\ndistribution over $T$ items in $O(\\log T)$ time. Moreover, when topic counts\nchange the data structure can be updated in $O(\\log T)$ time. In order to\ndistribute the computation across multiple processor we present a novel\nasynchronous framework inspired by the Nomad algorithm of\n\\cite{YunYuHsietal13}. We show that F+Nomad LDA significantly outperform\nstate-of-the-art on massive problems which involve millions of documents,\nbillions of words, and thousands of topics.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 12:52:50 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Yu", "Hsiang-Fu", ""], ["Hsieh", "Cho-Jui", ""], ["Yun", "Hyokun", ""], ["Vishwanathan", "S. V. N", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1412.5083", "submitter": "Qiang Qiu", "authors": "Qiang Qiu, Guillermo Sapiro, Alex Bronstein", "title": "Random Forests Can Hash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hash codes are a very efficient data representation needed to be able to cope\nwith the ever growing amounts of data. We introduce a random forest semantic\nhashing scheme with information-theoretic code aggregation, showing for the\nfirst time how random forest, a technique that together with deep learning have\nshown spectacular results in classification, can also be extended to\nlarge-scale retrieval. Traditional random forest fails to enforce the\nconsistency of hashes generated from each tree for the same class data, i.e.,\nto preserve the underlying similarity, and it also lacks a principled way for\ncode aggregation across trees. We start with a simple hashing scheme, where\nindependently trained random trees in a forest are acting as hashing functions.\nWe the propose a subspace model as the splitting function, and show that it\nenforces the hash consistency in a tree for data from the same class. We also\nintroduce an information-theoretic approach for aggregating codes of individual\ntrees into a single hash code, producing a near-optimal unique hash for each\nclass. Experiments on large-scale public datasets are presented, showing that\nthe proposed approach significantly outperforms state-of-the-art hashing\nmethods for retrieval tasks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 17:02:18 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 18:26:12 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2015 01:00:24 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Qiu", "Qiang", ""], ["Sapiro", "Guillermo", ""], ["Bronstein", "Alex", ""]]}, {"id": "1412.5335", "submitter": "Gr\\'egoire Mesnil", "authors": "Gr\\'egoire Mesnil, Tomas Mikolov, Marc'Aurelio Ranzato, Yoshua Bengio", "title": "Ensemble of Generative and Discriminative Techniques for Sentiment\n  Analysis of Movie Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is a common task in natural language processing that aims\nto detect polarity of a text document (typically a consumer review). In the\nsimplest settings, we discriminate only between positive and negative\nsentiment, turning the task into a standard binary classification problem. We\ncompare several ma- chine learning approaches to this problem, and combine them\nto achieve the best possible results. We show how to use for this task the\nstandard generative lan- guage models, which are slightly complementary to the\nstate of the art techniques. We achieve strong results on a well-known dataset\nof IMDB movie reviews. Our results are easily reproducible, as we publish also\nthe code needed to repeat the experiments. This should simplify further advance\nof the state of the art, as other researchers can combine their techniques with\nours with little effort.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 11:02:04 GMT"}, {"version": "v2", "created": "Thu, 18 Dec 2014 14:17:16 GMT"}, {"version": "v3", "created": "Fri, 19 Dec 2014 11:36:14 GMT"}, {"version": "v4", "created": "Tue, 3 Feb 2015 20:03:35 GMT"}, {"version": "v5", "created": "Wed, 4 Feb 2015 05:17:55 GMT"}, {"version": "v6", "created": "Thu, 16 Apr 2015 14:26:14 GMT"}, {"version": "v7", "created": "Wed, 27 May 2015 06:40:09 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Mesnil", "Gr\u00e9goire", ""], ["Mikolov", "Tomas", ""], ["Ranzato", "Marc'Aurelio", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1412.5404", "submitter": "Jichang Zhao", "authors": "Yuan Zuo, Jichang Zhao, Ke Xu", "title": "Word Network Topic Model: A Simple but General Solution for Short and\n  Imbalanced Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The short text has been the prevalent format for information of Internet in\nrecent decades, especially with the development of online social media, whose\nmillions of users generate a vast number of short messages everyday. Although\nsophisticated signals delivered by the short text make it a promising source\nfor topic modeling, its extreme sparsity and imbalance brings unprecedented\nchallenges to conventional topic models like LDA and its variants. Aiming at\npresenting a simple but general solution for topic modeling in short texts, we\npresent a word co-occurrence network based model named WNTM to tackle the\nsparsity and imbalance simultaneously. Different from previous approaches, WNTM\nmodels the distribution over topics for each word instead of learning topics\nfor each document, which successfully enhance the semantic density of data\nspace without importing too much time or space complexity. Meanwhile, the rich\ncontextual information preserved in the word-word space also guarantees its\nsensitivity in identifying rare topics with convincing quality. Furthermore,\nemploying the same Gibbs sampling with LDA makes WNTM easily to be extended to\nvarious application scenarios. Extensive validations on both short and normal\ntexts testify the outperformance of WNTM as compared to baseline methods. And\nfinally we also demonstrate its potential in precisely discovering newly\nemerging topics or unexpected events in Weibo at pretty early stages.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 14:18:52 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Zuo", "Yuan", ""], ["Zhao", "Jichang", ""], ["Xu", "Ke", ""]]}, {"id": "1412.5448", "submitter": "Micka\\\"el Poussevin", "authors": "Micka\\\"el Poussevin and Vincent Guigue and Patrick Gallinari", "title": "Extended Recommendation Framework: Generating the Text of a User Review\n  as a Personalized Summary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to augment rating based recommender systems by providing the user\nwith additional information which might help him in his choice or in the\nunderstanding of the recommendation. We consider here as a new task, the\ngeneration of personalized reviews associated to items. We use an extractive\nsummary formulation for generating these reviews. We also show that the two\ninformation sources, ratings and items could be used both for estimating\nratings and for generating summaries, leading to improved performance for each\nsystem compared to the use of a single source. Besides these two contributions,\nwe show how a personalized polarity classifier can integrate the rating and\ntextual aspects. Overall, the proposed system offers the user three\npersonalized hints for a recommendation: rating, text and polarity. We evaluate\nthese three components on two datasets using appropriate measures for each\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 15:46:28 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Poussevin", "Micka\u00ebl", ""], ["Guigue", "Vincent", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1412.6082", "submitter": "Petra Budikova", "authors": "Jan Botorek and Petra Budikova and Pavel Zezula", "title": "Visual Concept Ontology for Image Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the development of content-based data management, text-based\nsearching remains the primary means of multimedia retrieval in many areas.\nAutomatic creation of text metadata is thus a crucial tool for increasing the\nfindability of multimedia objects. Search-based annotation tools try to provide\ncontent-descriptive keywords by exploiting web data, which are easily available\nbut unstructured and noisy. Such data need to be analyzed with the help of\nsemantic resources that provide knowledge about objects and relationships in a\ngiven domain. In this paper, we focus on the task of general-purpose image\nannotation and present the VCO, a new ontology of visual concepts developed as\na part of image annotation framework. The ontology is linked with the WordNet\nlexical database, so the annotation tools can easily integrate information from\nboth these resources.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 16:49:29 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Botorek", "Jan", ""], ["Budikova", "Petra", ""], ["Zezula", "Pavel", ""]]}, {"id": "1412.6629", "submitter": "Hamid Palangi", "authors": "H. Palangi, L. Deng, Y. Shen, J. Gao, X. He, J. Chen, X. Song, R. Ward", "title": "Semantic Modelling with Long-Short-Term Memory for Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the following problem in web document and\ninformation retrieval (IR): How can we use long-term context information to\ngain better IR performance? Unlike common IR methods that use bag of words\nrepresentation for queries and documents, we treat them as a sequence of words\nand use long short term memory (LSTM) to capture contextual dependencies. To\nthe best of our knowledge, this is the first time that LSTM is applied to\ninformation retrieval tasks. Unlike training traditional LSTMs, the training\nstrategy is different due to the special nature of information retrieval\nproblem. Experimental evaluation on an IR task derived from the Bing web search\ndemonstrates the ability of the proposed method in addressing both lexical\nmismatch and long-term context modelling issues, thereby, significantly\noutperforming existing state of the art methods for web document retrieval\ntask.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 07:56:29 GMT"}, {"version": "v2", "created": "Sun, 28 Dec 2014 07:06:28 GMT"}, {"version": "v3", "created": "Fri, 27 Feb 2015 09:02:00 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Palangi", "H.", ""], ["Deng", "L.", ""], ["Shen", "Y.", ""], ["Gao", "J.", ""], ["He", "X.", ""], ["Chen", "J.", ""], ["Song", "X.", ""], ["Ward", "R.", ""]]}, {"id": "1412.6815", "submitter": "Misha Denil", "authors": "Misha Denil and Alban Demiraj and Nando de Freitas", "title": "Extraction of Salient Sentences from Labelled Documents", "comments": "arXiv admin note: substantial text overlap with arXiv:1406.3830", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hierarchical convolutional document model with an architecture\ndesigned to support introspection of the document structure. Using this model,\nwe show how to use visualisation techniques from the computer vision literature\nto identify and extract topic-relevant sentences.\n  We also introduce a new scalable evaluation technique for automatic sentence\nextraction systems that avoids the need for time consuming human annotation of\nvalidation data.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 17:38:19 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 23:57:08 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Denil", "Misha", ""], ["Demiraj", "Alban", ""], ["de Freitas", "Nando", ""]]}, {"id": "1412.7156", "submitter": "Ludovic Denoyer", "authors": "Gabriella Contardo and Ludovic Denoyer and Thierry Artieres", "title": "Representation Learning for cold-start recommendation", "comments": "Accepted as workshop contribution at ICLR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard approach to Collaborative Filtering (CF), i.e. prediction of user\nratings on items, relies on Matrix Factorization techniques. Representations\nfor both users and items are computed from the observed ratings and used for\nprediction. Unfortunatly, these transductive approaches cannot handle the case\nof new users arriving in the system, with no known rating, a problem known as\nuser cold-start. A common approach in this context is to ask these incoming\nusers for a few initialization ratings. This paper presents a model to tackle\nthis twofold problem of (i) finding good questions to ask, (ii) building\nefficient representations from this small amount of information. The model can\nalso be used in a more standard (warm) context. Our approach is evaluated on\nthe classical CF problem and on the cold-start problem on four different\ndatasets showing its ability to improve baseline performance in both cases.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 21:58:06 GMT"}, {"version": "v2", "created": "Fri, 27 Feb 2015 18:56:23 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2015 09:59:25 GMT"}, {"version": "v4", "created": "Wed, 8 Apr 2015 15:37:19 GMT"}, {"version": "v5", "created": "Mon, 22 Jun 2015 14:01:33 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Contardo", "Gabriella", ""], ["Denoyer", "Ludovic", ""], ["Artieres", "Thierry", ""]]}, {"id": "1412.7610", "submitter": "Chen Luo", "authors": "Chen Luo, Wei Pang and Zhe Wang", "title": "Hete-CF: Social-Based Collaborative Filtering Recommendation using\n  Heterogeneous Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Collaborative filtering algorithms haven been widely used in recommender\nsystems. However, they often suffer from the data sparsity and cold start\nproblems. With the increasing popularity of social media, these problems may be\nsolved by using social-based recommendation. Social-based recommendation, as an\nemerging research area, uses social information to help mitigate the data\nsparsity and cold start problems, and it has been demonstrated that the\nsocial-based recommendation algorithms can efficiently improve the\nrecommendation performance. However, few of the existing algorithms have\nconsidered using multiple types of relations within one social network. In this\npaper, we investigate the social-based recommendation algorithms on\nheterogeneous social networks and proposed Hete-CF, a Social Collaborative\nFiltering algorithm using heterogeneous relations. Distinct from the exiting\nmethods, Hete-CF can effectively utilize multiple types of relations in a\nheterogeneous social network. In addition, Hete-CF is a general approach and\ncan be used in arbitrary social networks, including event based social\nnetworks, location based social networks, and any other types of heterogeneous\ninformation networks associated with social information. The experimental\nresults on two real-world data sets, DBLP (a typical heterogeneous information\nnetwork) and Meetup (a typical event based social network) show the\neffectiveness and efficiency of our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 06:21:42 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Luo", "Chen", ""], ["Pang", "Wei", ""], ["Wang", "Zhe", ""]]}, {"id": "1412.7782", "submitter": "Roshan Ragel", "authors": "MAC Jiffriya, MAC Akmal Jahan, and Roshan G. Ragel", "title": "Plagiarism Detection on Electronic Text based Assignments using Vector\n  Space Model (ICIAfS14)", "comments": "appears in The 7th International Conference on Information and\n  Automation for Sustainability (ICIAfS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plagiarism is known as illegal use of others' part of work or whole work as\none's own in any field such as art, poetry, literature, cinema, research and\nother creative forms of study. Plagiarism is one of the important issues in\nacademic and research fields and giving more concern in academic systems. The\nsituation is even worse with the availability of ample resources on the web.\nThis paper focuses on an effective plagiarism detection tool on identifying\nsuitable intra-corpal plagiarism detection for text based assignments by\ncomparing unigram, bigram, trigram of vector space model with cosine similarity\nmeasure. Manually evaluated, labelled dataset was tested using unigram, bigram\nand trigram vector. Even though trigram vector consumes comparatively more\ntime, it shows better results with the labelled data. In addition, the selected\ntrigram vector space model with cosine similarity measure is compared with\ntri-gram sequence matching technique with Jaccard measure. In the results,\ncosine similarity score shows slightly higher values than the other. Because,\nit focuses on giving more weight for terms that do not frequently exist in the\ndataset and cosine similarity measure using trigram technique is more\npreferable than the other. Therefore, we present our new tool and it could be\nused as an effective tool to evaluate text based electronic assignments and\nminimize the plagiarism among students.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 03:54:01 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Jiffriya", "MAC", ""], ["Jahan", "MAC Akmal", ""], ["Ragel", "Roshan G.", ""]]}, {"id": "1412.7990", "submitter": "Ernesto Diaz-Aviles", "authors": "Ernesto Diaz-Aviles, Hoang Thanh Lam, Fabio Pinelli, Stefano Braghin,\n  Yiannis Gkoufas, Michele Berlingerio, and Francesco Calabrese", "title": "Predicting User Engagement in Twitter with Collaborative Ranking", "comments": "RecSysChallenge'14 at RecSys 2014, October 10, 2014, Foster City, CA,\n  USA", "journal-ref": "In Proceedings of the 2014 Recommender Systems Challenge\n  (RecSysChallenge'14). ACM, New York, NY, USA, , Pages 41 , 6 pages", "doi": "10.1145/2668067.2668072", "report-no": null, "categories": "cs.IR cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Filtering (CF) is a core component of popular web-based\nservices such as Amazon, YouTube, Netflix, and Twitter. Most applications use\nCF to recommend a small set of items to the user. For instance, YouTube\npresents to a user a list of top-n videos she would likely watch next based on\nher rating and viewing history. Current methods of CF evaluation have been\nfocused on assessing the quality of a predicted rating or the ranking\nperformance for top-n recommended items. However, restricting the recommender\nsystem evaluation to these two aspects is rather limiting and neglects other\ndimensions that could better characterize a well-perceived recommendation. In\nthis paper, instead of optimizing rating or top-n recommendation, we focus on\nthe task of predicting which items generate the highest user engagement. In\nparticular, we use Twitter as our testbed and cast the problem as a\nCollaborative Ranking task where the rich features extracted from the metadata\nof the tweets help to complement the transaction information limited to user\nids, item ids, ratings and timestamps. We learn a scoring function that\ndirectly optimizes the user engagement in terms of nDCG@10 on the predicted\nranking. Experiments conducted on an extended version of the MovieTweetings\ndataset, released as part of the RecSys Challenge 2014, show the effectiveness\nof our approach.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 21:00:14 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Diaz-Aviles", "Ernesto", ""], ["Lam", "Hoang Thanh", ""], ["Pinelli", "Fabio", ""], ["Braghin", "Stefano", ""], ["Gkoufas", "Yiannis", ""], ["Berlingerio", "Michele", ""], ["Calabrese", "Francesco", ""]]}, {"id": "1412.8079", "submitter": "Ayoub Bagheri", "authors": "Ayoub Bagheri, Mohamad Saraee", "title": "Persian Sentiment Analyzer: A Framework based on a Novel Feature\n  Selection Method", "comments": null, "journal-ref": "International Journal of Artificial Intelligence 12.2 (2014):\n  115-129", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent decade, with the enormous growth of digital content in internet\nand databases, sentiment analysis has received more and more attention between\ninformation retrieval and natural language processing researchers. Sentiment\nanalysis aims to use automated tools to detect subjective information from\nreviews. One of the main challenges in sentiment analysis is feature selection.\nFeature selection is widely used as the first stage of analysis and\nclassification tasks to reduce the dimension of problem, and improve speed by\nthe elimination of irrelevant and redundant features. Up to now as there are\nfew researches conducted on feature selection in sentiment analysis, there are\nvery rare works for Persian sentiment analysis. This paper considers the\nproblem of sentiment classification using different feature selection methods\nfor online customer reviews in Persian language. Three of the challenges of\nPersian text are using of a wide variety of declensional suffixes, different\nword spacing and many informal or colloquial words. In this paper we study\nthese challenges by proposing a model for sentiment classification of Persian\nreview documents. The proposed model is based on lemmatization and feature\nselection and is employed Naive Bayes algorithm for classification. We evaluate\nthe performance of the model on a manually gathered collection of cellphone\nreviews, where the results show the effectiveness of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Sat, 27 Dec 2014 21:00:24 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Bagheri", "Ayoub", ""], ["Saraee", "Mohamad", ""]]}, {"id": "1412.8099", "submitter": "Rathipriya R", "authors": "R. Rathipriya, K. Thangavel", "title": "Extraction of Web Usage Profiles using Simulated Annealing Based\n  Biclustering Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the Simulated Annealing (SA) based biclustering approach is\nproposed in which SA is used as an optimization tool for biclustering of web\nusage data to identify the optimal user profile from the given web usage data.\nExtracted biclusters are consists of correlated users whose usage behaviors are\nsimilar across the subset of web pages of a web site where as these users are\nuncorrelated for remaining pages of a web site. These results are very useful\nin web personalization so that it communicates better with its users and for\nmaking customized prediction. Also useful for providing customized web service\ntoo. Experiment was conducted on the real web usage dataset called CTI dataset.\nResults show that proposed SA based biclustering approach can extract highly\ncorrelated user groups from the preprocessed web usage data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 10:06:25 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Rathipriya", "R.", ""], ["Thangavel", "K.", ""]]}, {"id": "1412.8118", "submitter": "Lanbo Zhang", "authors": "Lanbo Zhang and Yi Zhang", "title": "Hierarchical Bayesian Models with Factorization for Content-Based\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing content-based filtering approaches learn user profiles\nindependently without capturing the similarity among users. Bayesian\nhierarchical models \\cite{Zhang:Efficient} learn user profiles jointly and have\nthe advantage of being able to borrow discriminative information from other\nusers through a Bayesian prior. However, the standard Bayesian hierarchical\nmodels assume all user profiles are generated from the same prior. Considering\nthe diversity of user interests, this assumption could be improved by\nintroducing more flexibility. Besides, most existing content-based filtering\napproaches implicitly assume that each user profile corresponds to exactly one\nuser interest and fail to capture a user's multiple interests (information\nneeds).\n  In this paper, we present a flexible Bayesian hierarchical modeling approach\nto model both commonality and diversity among users as well as individual\nusers' multiple interests. We propose two models each with different\nassumptions, and the proposed models are called Discriminative Factored Prior\nModels (DFPM). In our models, each user profile is modeled as a discriminative\nclassifier with a factored model as its prior, and different factors contribute\nin different levels to each user profile. Compared with existing content-based\nfiltering models, DFPM are interesting because they can 1) borrow\ndiscriminative criteria of other users while learning a particular user profile\nthrough the factored prior; 2) trade off well between diversity and commonality\namong users; and 3) handle the challenging classification situation where each\nclass contains multiple concepts. The experimental results on a dataset\ncollected from real users on digg.com show that our models significantly\noutperform the baseline models of L-2 regularized logistic regression and\ntraditional Bayesian hierarchical model with logistic regression.\n", "versions": [{"version": "v1", "created": "Sun, 28 Dec 2014 06:07:48 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Zhang", "Lanbo", ""], ["Zhang", "Yi", ""]]}, {"id": "1412.8125", "submitter": "Lanbo Zhang", "authors": "Lanbo Zhang and Yi Zhang and Qianli Xing", "title": "Learning from Labeled Features for Document Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing document filtering systems learn user profiles based on user\nrelevance feedback on documents. In some cases, users may have prior knowledge\nabout what features are important. For example, a Spanish speaker may only want\nnews written in Spanish, and thus a relevant document should contain the\nfeature \"Language: Spanish\"; a researcher focusing on HIV knows an article with\nthe medical subject \"Subject: AIDS\" is very likely to be relevant to him/her.\n  Semi-structured documents with rich metadata are increasingly prevalent on\nthe Internet. Motivated by the well-adopted faceted search interface in\ne-commerce, we study the exploitation of user prior knowledge on faceted\nfeatures for semi-structured document filtering. We envision two faceted\nfeedback mechanisms, and propose a novel user profile learning algorithm that\ncan incorporate user feedback on features. To evaluate the proposed work, we\nuse two data sets from the TREC filtering track, and conduct a user study on\nAmazon Mechanical Turk. Our experiment results show that user feedback on\nfaceted features is useful for filtering. The proposed user profile learning\nalgorithm can effectively learn from user feedback on both documents and\nfeatures, and performs better than several existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Dec 2014 07:32:09 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Zhang", "Lanbo", ""], ["Zhang", "Yi", ""], ["Xing", "Qianli", ""]]}, {"id": "1412.8147", "submitter": "Saeed Parseh", "authors": "Saeed Parseh and Ahmad Baraani", "title": "Improving Persian Document Classification Using Semantic Relations\n  between Words", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase of information, document classification as one of the\nmethods of text mining, plays vital role in many management and organizing\ninformation. Document classification is the process of assigning a document to\none or more predefined category labels. Document classification includes\ndifferent parts such as text processing, term selection, term weighting and\nfinal classification. The accuracy of document classification is very\nimportant. Thus improvement in each part of classification should lead to\nbetter results and higher precision. Term weighting has a great impact on the\naccuracy of the classification. Most of the existing weighting methods exploit\nthe statistical information of terms in documents and do not consider semantic\nrelations between words. In this paper, an automated document classification\nsystem is presented that uses a novel term weighting method based on semantic\nrelations between terms. To evaluate the proposed method, three standard\nPersian corpuses are used. Experiment results show 2 to 4 percent improvement\nin classification accuracy compared with the best previous designed system for\nPersian documents.\n", "versions": [{"version": "v1", "created": "Sun, 28 Dec 2014 10:56:53 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Parseh", "Saeed", ""], ["Baraani", "Ahmad", ""]]}, {"id": "1412.8281", "submitter": "Lanbo Zhang", "authors": "Lanbo Zhang", "title": "Interactive Retrieval Based on Wikipedia Concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new user feedback mechanism based on Wikipedia concepts\nfor interactive retrieval. In this mechanism, the system presents to the user a\ngroup of Wikipedia concepts, and the user can choose those relevant to refine\nhis/her query. To realize this mechanism, we propose methods to address two\nproblems: 1) how to select a small number of possibly relevant Wikipedia\nconcepts to show the user, and 2) how to re-rank retrieved documents given the\nuser-identified Wikipedia concepts. Our methods are evaluated on three TREC\ndata sets. The experiment results show that our methods can dramatically\nimprove retrieval performances.\n", "versions": [{"version": "v1", "created": "Mon, 29 Dec 2014 08:45:59 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Zhang", "Lanbo", ""]]}]