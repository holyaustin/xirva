[{"id": "1002.0215", "submitter": "Mauro Gaio", "authors": "Marie-No\\\"elle Bessagnet (LIUPPA), Eric Kergosien (LIUPPA), Mauro Gaio\n  (LIUPPA)", "title": "Extraction de termes, reconnaissance et labellisation de relations dans\n  un th\\'esaurus", "comments": null, "journal-ref": "CIDE'12: 12e Colloque International sur le Document Electronique,\n  Montr\\'eal : Canada (2009)", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the documentary system domain, the integration of thesauri for\nindexing and retrieval information steps is usual. In libraries, documents own\nrich descriptive information made by librarians, under descriptive notice based\non Rameau thesaurus. We exploit two kinds of information in order to create a\nfirst semantic structure. A step of conceptualization allows us to define the\nvarious modules used to automatically build the semantic structure of the\nindexation work. Our current work focuses on an approach that aims to define an\nontology based on a thesaurus. We hope to integrate new knowledge\ncharacterizing the territory of our structure (adding \"toponyms\" and links\nbetween concepts) thanks to a geographic information system (GIS).\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2010 10:17:55 GMT"}], "update_date": "2010-10-06", "authors_parsed": [["Bessagnet", "Marie-No\u00eblle", "", "LIUPPA"], ["Kergosien", "Eric", "", "LIUPPA"], ["Gaio", "Mauro", "", "LIUPPA"]]}, {"id": "1002.0239", "submitter": "Mauro Gaio", "authors": "Eric Kergosien (LIUPPA), Mouna Kamel (IRIT), Christian Sallaberry\n  (LIUPPA), Marie-No\\\"elle Bessagnet (LIUPPA), Nathalie Aussenac- Gilles\n  (IRIT), Mauro Gaio (LIUPPA)", "title": "Construction et enrichissement automatique d'ontologie \\`a partir de\n  ressources externes", "comments": null, "journal-ref": "JFO'09: 3es Journ\\'ees Francophones sur les Ontologies, Poitiers :\n  France (2009)", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic construction of ontologies from text is generally based on\nretrieving text content. For a much more rich ontology we extend these\napproaches by taking into account the document structure and some external\nresources (like thesaurus of indexing terms of near domain). In this paper we\ndescribe how these external resources are at first analyzed and then exploited.\nThis method has been applied on a geographical domain and the benefit has been\nevaluated.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2010 13:09:52 GMT"}], "update_date": "2010-02-02", "authors_parsed": [["Kergosien", "Eric", "", "LIUPPA"], ["Kamel", "Mouna", "", "IRIT"], ["Sallaberry", "Christian", "", "LIUPPA"], ["Bessagnet", "Marie-No\u00eblle", "", "LIUPPA"], ["Gilles", "Nathalie Aussenac-", "", "IRIT"], ["Gaio", "Mauro", "", "LIUPPA"]]}, {"id": "1002.0577", "submitter": "Mauro Gaio", "authors": "Tien Nguyen Van (LIUPPA), Mauro Gaio (LIUPPA), Christian Sallaberry\n  (LIUPPA)", "title": "Recherche de relations spatio-temporelles : une m\\'ethode bas\\'ee sur\n  l'analyse de corpus textuels", "comments": null, "journal-ref": "TIA'09WS: Acquisition et mod\\'elisation de relations\n  s\\'emantiques, Toulouse : France (2009)", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a work package realized for the G\\'eOnto project. A new\nmethod is proposed for an enrichment of a first geographical ontology developed\nbeforehand. This method relies on text analysis by lexico-syntactic patterns.\n  From the retrieve of n-ary relations the method automatically detect those\ninvolved in a spatial and/or temporal relation in a context of a description of\njourneys.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2010 20:07:22 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Van", "Tien Nguyen", "", "LIUPPA"], ["Gaio", "Mauro", "", "LIUPPA"], ["Sallaberry", "Christian", "", "LIUPPA"]]}, {"id": "1002.1060", "submitter": "Roberto da Silva", "authors": "Roberto da Silva, Jose Palazzo de Oliveira, Jose Valdeni de Lima,\n  Viviane Moreira", "title": "Statistics for Ranking Program Committees and Editorial Boards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR math.IT physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking groups of researchers is important in several contexts and can serve\nmany purposes such as the fair distribution of grants based on the scientist's\npublication output, concession of research projects, classification of journal\neditorial boards and many other applications in a social context. In this\npaper, we propose a method for measuring the performance of groups of\nresearchers. The proposed method is called alpha-index and it is based on two\nparameters: (i) the homogeneity of the h-indexes of the researchers in the\ngroup; and (ii) the h-group, which is an extension of the h-index for groups.\nOur method integrates the concepts of homogeneity and absolute value of the\nh-index into a single measure which is appropriate for the evaluation of\ngroups. We report on experiments that assess computer science conferences based\non the h-indexes of their program committee members. Our results are similar to\na manual classification scheme adopted by a research agency.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2010 19:27:48 GMT"}], "update_date": "2010-02-05", "authors_parsed": [["da Silva", "Roberto", ""], ["de Oliveira", "Jose Palazzo", ""], ["de Lima", "Jose Valdeni", ""], ["Moreira", "Viviane", ""]]}, {"id": "1002.1951", "submitter": "Rdv Ijcsis", "authors": "Mr. Kondekar V. H., Mr. Kolkure V. S., Prof. Kore S.N", "title": "Image Retrieval Techniques based on Image Features, A State of Art\n  approach for CBIR", "comments": "IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS January 2010, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 7, No. 1, pp. 69-76, January 2010, USA", "doi": null, "report-no": "Journal of Computer Science, ISSN 1947 5500", "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this Paper is to describe our research on different feature\nextraction and matching techniques in designing a Content Based Image Retrieval\n(CBIR) system. Due to the enormous increase in image database sizes, as well as\nits vast deployment in various applications, the need for CBIR development\narose. Firstly, this paper outlines a description of the primitive feature\nextraction techniques like, texture, colour, and shape. Once these features are\nextracted and used as the basis for a similarity check between images, the\nvarious matching techniques are discussed. Furthermore, the results of its\nperformance are illustrated by a detailed example.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2010 19:43:44 GMT"}], "update_date": "2010-02-10", "authors_parsed": [["H.", "Mr. Kondekar V.", ""], ["S.", "Mr. Kolkure V.", ""], ["N", "Prof. Kore S.", ""]]}, {"id": "1002.2193", "submitter": "Rdv Ijcsis", "authors": "Ismail I. Amr, Mohamed Amin, Passent El Kafrawy, Amr M. Sauber", "title": "Using Statistical Moment Invariants and Entropy in Image Retrieval", "comments": "IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS January 2010, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 7, No. 1, pp. 160-164, January 2010, USA", "doi": null, "report-no": "Journal of Computer Science, ISSN 19475500", "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although content-based image retrieval (CBIR) is not a new subject, it keeps\nattracting more and more attention, as the amount of images grow tremendously\ndue to internet, inexpensive hardware and automation of image acquisition. One\nof the applications of CBIR is fetching images from a database. This paper\npresents a new method for automatic image retrieval using moment invariants and\nimage entropy, our technique could be used to find semi or perfect matches\nbased on query by example manner, experimental results demonstrate that the\npurposed technique is scalable and efficient.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2010 19:52:12 GMT"}], "update_date": "2010-02-11", "authors_parsed": [["Amr", "Ismail I.", ""], ["Amin", "Mohamed", ""], ["Kafrawy", "Passent El", ""], ["Sauber", "Amr M.", ""]]}, {"id": "1002.2439", "submitter": "Martin Klein", "authors": "Jeffery L. Shipman, Martin Klein, Michael L. Nelson", "title": "Using Web Page Titles to Rediscover Lost Web Pages", "comments": "49 pages, 18 figures, CS project report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Titles are denoted by the TITLE element within a web page. We queried the\ntitle against the the Yahoo search engine to determine the page's status\n(found, not found). We conducted several tests based on elements of the title.\nThese tests were used to discern whether we could predict a pages status based\non the title. Our results increase our ability to determine bad titles but not\nour ability to determine good titles.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2010 21:45:48 GMT"}], "update_date": "2010-02-15", "authors_parsed": [["Shipman", "Jeffery L.", ""], ["Klein", "Martin", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1002.2858", "submitter": "Massimo Franceschet", "authors": "Massimo Franceschet", "title": "PageRank: Standing on the shoulders of giants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PageRank is a Web page ranking technique that has been a fundamental\ningredient in the development and success of the Google search engine. The\nmethod is still one of the many signals that Google uses to determine which\npages are most important. The main idea behind PageRank is to determine the\nimportance of a Web page in terms of the importance assigned to the pages\nhyperlinking to it. In fact, this thesis is not new, and has been previously\nsuccessfully exploited in different contexts. We review the PageRank method and\nlink it to some renowned previous techniques that we have found in the fields\nof Web information retrieval, bibliometrics, sociometry, and econometrics.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2010 12:47:16 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2010 15:15:50 GMT"}, {"version": "v3", "created": "Sat, 14 Aug 2010 15:14:13 GMT"}], "update_date": "2010-08-17", "authors_parsed": [["Franceschet", "Massimo", ""]]}, {"id": "1002.3238", "submitter": "Benjamin Piwowarski", "authors": "Benjamin Piwowarski and Ingo Frommholz and Mounia Lalmas and Keith van\n  Rijsbergen", "title": "Exploring a Multidimensional Representation of Documents and Queries\n  (extended version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Information Retrieval (IR), whether implicitly or explicitly, queries and\ndocuments are often represented as vectors. However, it may be more beneficial\nto consider documents and/or queries as multidimensional objects. Our belief is\nthis would allow building \"truly\" interactive IR systems, i.e., where\ninteraction is fully incorporated in the IR framework.\n  The probabilistic formalism of quantum physics represents events and\ndensities as multidimensional objects. This paper presents our first step\ntowards building an interactive IR framework upon this formalism, by stating\nhow the first interaction of the retrieval process, when the user types a\nquery, can be formalised. Our framework depends on a number of parameters\naffecting the final document ranking. In this paper we experimentally\ninvestigate the effect of these parameters, showing that the proposed\nrepresentation of documents and queries as multidimensional objects can compete\nwith standard approaches, with the additional prospect to be applied to\ninteractive retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2010 10:25:57 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2010 13:45:17 GMT"}], "update_date": "2010-02-18", "authors_parsed": [["Piwowarski", "Benjamin", ""], ["Frommholz", "Ingo", ""], ["Lalmas", "Mounia", ""], ["van Rijsbergen", "Keith", ""]]}, {"id": "1002.3342", "submitter": "Bertrand Georgeot", "authors": "B. Georgeot, O. Giraud and D.L. Shepelyansky", "title": "Spectral properties of the Google matrix of the World Wide Web and other\n  directed networks", "comments": "8 pages, 12 figures, research done at\n  http://www.quantware.ups-tlse.fr", "journal-ref": "Phys. Rev. E 81, 056109 (2010)", "doi": "10.1103/PhysRevE.81.056109", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study numerically the spectrum and eigenstate properties of the Google\nmatrix of various examples of directed networks such as vocabulary networks of\ndictionaries and university World Wide Web networks. The spectra have gapless\nstructure in the vicinity of the maximal eigenvalue for Google damping\nparameter $\\alpha$ equal to unity. The vocabulary networks have relatively\nhomogeneous spectral density, while university networks have pronounced\nspectral structures which change from one university to another, reflecting\nspecific properties of the networks. We also determine specific properties of\neigenstates of the Google matrix, including the PageRank. The fidelity of the\nPageRank is proposed as a new characterization of its stability.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2010 18:22:10 GMT"}], "update_date": "2010-05-27", "authors_parsed": [["Georgeot", "B.", ""], ["Giraud", "O.", ""], ["Shepelyansky", "D. L.", ""]]}, {"id": "1002.4041", "submitter": "William Jackson", "authors": "Mohammad Syafrullah, Naomie Salim", "title": "Improving Term Extraction Using Particle Swarm Optimization Techniques", "comments": null, "journal-ref": "Journal of Computing, Volume 2, Issue 2, February 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Term extraction is one of the layers in the ontology development process\nwhich has the task to extract all the terms contained in the input document\nautomatically. The purpose of this process is to generate list of terms that\nare relevant to the domain of the input document. In the literature there are\nmany approaches, techniques and algorithms used for term extraction. In this\npaper we propose a new approach using particle swarm optimization techniques in\norder to improve the accuracy of term extraction results. We choose five\nfeatures to represent the term score. The approach has been applied to the\ndomain of religious document. We compare our term extraction method precision\nwith TFIDF, Weirdness, GlossaryExtraction and TermExtractor. The experimental\nresults show that our propose approach achieve better precision than those four\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2010 03:01:49 GMT"}], "update_date": "2010-03-25", "authors_parsed": [["Syafrullah", "Mohammad", ""], ["Salim", "Naomie", ""]]}, {"id": "1002.4048", "submitter": "William Jackson", "authors": "Satadal Saha, Subhadip Basu, Mita Nasipuri, Dipak Kr. Basu", "title": "A Hough Transform based Technique for Text Segmentation", "comments": null, "journal-ref": "Journal of Computing, Volume 2, Issue 2, February 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text segmentation is an inherent part of an OCR system irrespective of the\ndomain of application of it. The OCR system contains a segmentation module\nwhere the text lines, words and ultimately the characters must be segmented\nproperly for its successful recognition. The present work implements a Hough\ntransform based technique for line and word segmentation from digitized images.\nThe proposed technique is applied not only on the document image dataset but\nalso on dataset for business card reader system and license plate recognition\nsystem. For standardization of the performance of the system the technique is\nalso applied on public domain dataset published in the website by CMATER,\nJadavpur University. The document images consist of multi-script printed and\nhand written text lines with variety in script and line spacing in single\ndocument image. The technique performs quite satisfactorily when applied on\nmobile camera captured business card images with low resolution. The usefulness\nof the technique is verified by applying it in a commercial project for\nlocalization of license plate of vehicles from surveillance camera images by\nthe process of segmentation itself. The accuracy of the technique for word\nsegmentation, as verified experimentally, is 85.7% for document images, 94.6%\nfor business card images and 88% for surveillance camera images.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2010 03:16:55 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["Saha", "Satadal", ""], ["Basu", "Subhadip", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kr.", ""]]}, {"id": "1002.4818", "submitter": "Adrian Kuhn", "authors": "Florian S. Gysin and Adrian Kuhn", "title": "A Trustability Metric for Code Search based on Developer Karma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The promise of search-driven development is that developers will save time\nand resources by reusing external code in their local projects. To efficiently\nintegrate this code, users must be able to trust it, thus trustability of code\nsearch results is just as important as their relevance. In this paper, we\nintroduce a trustability metric to help users assess the quality of code search\nresults and therefore ease the cost-benefit analysis they undertake trying to\nfind suitable integration candidates. The proposed trustability metric\nincorporates both user votes and cross-project activity of developers to\ncalculate a \"karma\" value for each developer. Through the karma value of all\nits developers a project is ranked on a trustability scale. We present JBender,\na proof-of-concept code search engine which implements our trustability metric\nand we discuss preliminary results from an evaluation of the prototype.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2010 16:21:00 GMT"}], "update_date": "2010-02-26", "authors_parsed": [["Gysin", "Florian S.", ""], ["Kuhn", "Adrian", ""]]}]