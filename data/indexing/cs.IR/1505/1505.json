[{"id": "1505.00092", "submitter": "Elad Yom-Tov", "authors": "Dan Pelleg, Elad Yom-Tov and Evgeniy Gabrilovich", "title": "On the Effect of Human-Computer Interfaces on Language Expression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language expression is known to be dependent on attributes intrinsic to the\nauthor. To date, however, little attention has been devoted to the effect of\ninterfaces used to articulate language on its expression. Here we study a large\ncorpus of text written using different input devices and show that writers\nunconsciously prefer different letters depending on the interplay between their\nindividual traits (e.g., hand laterality and injuries) and the layout of\nkeyboards. Our results show, for the first time, how the interplay between\ntechnology and its users modifies language expression.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 05:18:57 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Pelleg", "Dan", ""], ["Yom-Tov", "Elad", ""], ["Gabrilovich", "Evgeniy", ""]]}, {"id": "1505.00168", "submitter": "Nayan Jyoti Kalita", "authors": "Manan Mohan Goyal, Neha Agrawal, Manoj Kumar Sarma, Nayan Jyoti Kalita", "title": "Comparison Clustering using Cosine and Fuzzy set based Similarity\n  Measures of Text Documents", "comments": "4 pages, International Conference on Computing and Communication\n  Systems 2015 (I3CS'15), ISBM: 978-1-4799-5857-01, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keeping in consideration the high demand for clustering, this paper focuses\non understanding and implementing K-means clustering using two different\nsimilarity measures. We have tried to cluster the documents using two different\nmeasures rather than clustering it with Euclidean distance. Also a comparison\nis drawn based on accuracy of clustering between fuzzy and cosine similarity\nmeasure. The start time and end time parameters for formation of clusters are\nused in deciding optimum similarity measure.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 12:21:24 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Goyal", "Manan Mohan", ""], ["Agrawal", "Neha", ""], ["Sarma", "Manoj Kumar", ""], ["Kalita", "Nayan Jyoti", ""]]}, {"id": "1505.00401", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "Visualization of Tradeoff in Evaluation: from Precision-Recall & PN to\n  LIFT, ROC & BIRD", "comments": "23 pages, 12 equations, 2 figures, 2 tables, 1 sidebar", "journal-ref": null, "doi": null, "report-no": "KIT-14-002", "categories": "cs.LG cs.AI cs.IR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation often aims to reduce the correctness or error characteristics of a\nsystem down to a single number, but that always involves trade-offs. Another\nway of dealing with this is to quote two numbers, such as Recall and Precision,\nor Sensitivity and Specificity. But it can also be useful to see more than\nthis, and a graphical approach can explore sensitivity to cost, prevalence,\nbias, noise, parameters and hyper-parameters.\n  Moreover, most techniques are implicitly based on two balanced classes, and\nour ability to visualize graphically is intrinsically two dimensional, but we\noften want to visualize in a multiclass context. We review the dichotomous\napproaches relating to Precision, Recall, and ROC as well as the related LIFT\nchart, exploring how they handle unbalanced and multiclass data, and deriving\nnew probabilistic and information theoretic variants of LIFT that help deal\nwith the issues associated with the handling of multiple and unbalanced\nclasses.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 07:27:27 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 04:02:22 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "1505.00519", "submitter": "Cameron Summers", "authors": "Cameron Summers and Phillip Popp", "title": "Large Scale Discovery of Seasonal Music From User Data", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The consumption history of online media content such as music and video\noffers a rich source of data from which to mine information. Trends in this\ndata are of particular interest because they reflect user preferences as well\nas associated cultural contexts that can be exploited in systems such as\nrecommendation or search. This paper classifies songs as seasonal using a\nlarge, real-world dataset of user listening data. Results show strong\nperformance of classification of Christmas music with Gaussian Mixture Models.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 03:38:04 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Summers", "Cameron", ""], ["Popp", "Phillip", ""]]}, {"id": "1505.00641", "submitter": "Immanuel Bayer", "authors": "Immanuel Bayer", "title": "fastFM: A Library for Factorization Machines", "comments": "Source Code is available at https://github.com/ibayer/fastFM", "journal-ref": "Journal of Machine Learning Research 17, pp. 1-5 (2016)", "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorization Machines (FM) are only used in a narrow range of applications\nand are not part of the standard toolbox of machine learning models. This is a\npity, because even though FMs are recognized as being very successful for\nrecommender system type applications they are a general model to deal with\nsparse and high dimensional features. Our Factorization Machine implementation\nprovides easy access to many solvers and supports regression, classification\nand ranking tasks. Such an implementation simplifies the use of FM's for a wide\nfield of applications. This implementation has the potential to improve our\nunderstanding of the FM model and drive new development.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 14:06:11 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 18:43:34 GMT"}, {"version": "v3", "created": "Wed, 23 Nov 2016 14:25:55 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Bayer", "Immanuel", ""]]}, {"id": "1505.00755", "submitter": "Olegs Verhodubs", "authors": "Olegs Verhodubs", "title": "Towards the Ontology Web Search Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The project of the Ontology Web Search Engine is presented in this paper. The\nmain purpose of this paper is to develop such a project that can be easily\nimplemented. Ontology Web Search Engine is software to look for and index\nontologies in the Web. OWL (Web Ontology Languages) ontologies are meant, and\nthey are necessary for the functioning of the SWES (Semantic Web Expert\nSystem). SWES is an expert system that will use found ontologies from the Web,\ngenerating rules from them, and will supplement its knowledge base with these\ngenerated rules. It is expected that the SWES will serve as a universal expert\nsystem for the average user.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 19:04:19 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Verhodubs", "Olegs", ""]]}, {"id": "1505.00841", "submitter": "Antoine Amarilli", "authors": "Aliaksandr Talaika, Joanna Biega, Antoine Amarilli, Fabian M. Suchanek", "title": "Harvesting Entities from the Web Using Unique Identifiers -- IBEX", "comments": "30 pages, 5 figures, 9 tables. Complete technical report for A.\n  Talaika, J. A. Biega, A. Amarilli, and F. M. Suchanek. IBEX: Harvesting\n  Entities from the Web Using Unique Identifiers. WebDB workshop, 2015", "journal-ref": null, "doi": "10.1145/2767109.2767116", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the prevalence of unique entity identifiers on the\nWeb. These are, e.g., ISBNs (for books), GTINs (for commercial products), DOIs\n(for documents), email addresses, and others. We show how these identifiers can\nbe harvested systematically from Web pages, and how they can be associated with\nhuman-readable names for the entities at large scale.\n  Starting with a simple extraction of identifiers and names from Web pages, we\nshow how we can use the properties of unique identifiers to filter out noise\nand clean up the extraction result on the entire corpus. The end result is a\ndatabase of millions of uniquely identified entities of different types, with\nan accuracy of 73--96% and a very high coverage compared to existing knowledge\nbases. We use this database to compute novel statistics on the presence of\nproducts, people, and other entities on the Web.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 23:21:00 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Talaika", "Aliaksandr", ""], ["Biega", "Joanna", ""], ["Amarilli", "Antoine", ""], ["Suchanek", "Fabian M.", ""]]}, {"id": "1505.00855", "submitter": "Babak Saleh", "authors": "Babak Saleh and Ahmed Elgammal", "title": "Large-scale Classification of Fine-Art Paintings: Learning The Right\n  Metric on The Right Feature", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, the number of fine-art collections that are digitized\nand publicly available has been growing rapidly. With the availability of such\nlarge collections of digitized artworks comes the need to develop multimedia\nsystems to archive and retrieve this pool of data. Measuring the visual\nsimilarity between artistic items is an essential step for such multimedia\nsystems, which can benefit more high-level multimedia tasks. In order to model\nthis similarity between paintings, we should extract the appropriate visual\nfeatures for paintings and find out the best approach to learn the similarity\nmetric based on these features. We investigate a comprehensive list of visual\nfeatures and metric learning approaches to learn an optimized similarity\nmeasure between paintings. We develop a machine that is able to make\naesthetic-related semantic-level judgments, such as predicting a painting's\nstyle, genre, and artist, as well as providing similarity measures optimized\nbased on the knowledge available in the domain of art historical\ninterpretation. Our experiments show the value of using this similarity measure\nfor the aforementioned prediction tasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 01:25:26 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Saleh", "Babak", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1505.00862", "submitter": "Shuangyong Song", "authors": "Shuangyong Song and Yao Meng", "title": "Classifying and Ranking Microblogging Hashtags with News Categories", "comments": "2 pages, no figure, to be appeared on RCIS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In microblogging, hashtags are used to be topical markers, and they are\nadopted by users that contribute similar content or express a related idea.\nHowever, hashtags are created in a free style and there is no domain category\ninformation about them, which make users hard to get access to organized\nhashtag presentation. In this paper, we propose an approach that classifies\nhashtags with news categories, and then carry out a domain-sensitive popularity\nranking to get hot hashtags in each domain. The proposed approach first trains\na domain classification model with news content and news category information,\nthen detects microblogs related to a hashtag to be its representative text,\nbased on which we can classify this hashtag with a domain. Finally, we\ncalculate the domain-sensitive popularity of each hashtag with multiple\nfactors, to get most hotly discussed hashtags in each domain. Preliminary\nexperimental results on a dataset from Sina Weibo, one of the largest Chinese\nmicroblogging websites, show usefulness of the proposed approach on describing\nhashtags.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 02:02:23 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Song", "Shuangyong", ""], ["Meng", "Yao", ""]]}, {"id": "1505.00863", "submitter": "Shuangyong Song", "authors": "Shuangyong Song, Yao Meng, Zhongguang Zheng, Jun Sun", "title": "A Feature-based Classification Technique for Answering Multi-choice\n  World History Questions", "comments": "5 pages, no figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our FRDC_QA team participated in the QA-Lab English subtask of the NTCIR-11.\nIn this paper, we describe our system for solving real-world university\nentrance exam questions, which are related to world history. Wikipedia is used\nas the main external resource for our system. Since problems with choosing\nright/wrong sentence from multiple sentence choices account for about\ntwo-thirds of the total, we individually design a classification based model\nfor solving this type of questions. For other types of questions, we also\ndesign some simple methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 02:06:23 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Song", "Shuangyong", ""], ["Meng", "Yao", ""], ["Zheng", "Zhongguang", ""], ["Sun", "Jun", ""]]}, {"id": "1505.00989", "submitter": "Kais Dai", "authors": "Kais Dai, Celia G\\'onzalez Nespereira, Ana Fern\\'andez Vilas, Rebeca\n  P. D\\'iaz Redondo", "title": "Scraping and Clustering Techniques for the Characterization of Linkedin\n  Profiles", "comments": "In proceedings of the Fourth International Conference on Information\n  Technology Convergence and Services (ITCS 2015), pp. 1-15, January 2015,\n  Zurich(Switzerland)", "journal-ref": null, "doi": "10.5121/csit.2015.50101", "report-no": null, "categories": "cs.SI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The socialization of the web has undertaken a new dimension after the\nemergence of the Online Social Networks (OSN) concept. The fact that each\nInternet user becomes a potential content creator entails managing a big amount\nof data. This paper explores the most popular professional OSN: LinkedIn. A\nscraping technique was implemented to get around 5 Million public profiles. The\napplication of natural language processing techniques (NLP) to classify the\neducational background and to cluster the professional background of the\ncollected profiles led us to provide some insights about this OSN's users and\nto evaluate the relationships between educational degrees and professional\ncareers.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 12:46:28 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Dai", "Kais", ""], ["Nespereira", "Celia G\u00f3nzalez", ""], ["Vilas", "Ana Fern\u00e1ndez", ""], ["Redondo", "Rebeca P. D\u00edaz", ""]]}, {"id": "1505.01072", "submitter": "Arun Maiya", "authors": "Arun S. Maiya, Dale Visser, Andrew Wan", "title": "Mining Measured Information from Text", "comments": "4 pages; 38th International ACM SIGIR Conference on Research and\n  Development in Information Retrieval (SIGIR '15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to extract measured information from text (e.g., a\n1370 degrees C melting point, a BMI greater than 29.9 kg/m^2 ). Such\nextractions are critically important across a wide range of domains -\nespecially those involving search and exploration of scientific and technical\ndocuments. We first propose a rule-based entity extractor to mine measured\nquantities (i.e., a numeric value paired with a measurement unit), which\nsupports a vast and comprehensive set of both common and obscure measurement\nunits. Our method is highly robust and can correctly recover valid measured\nquantities even when significant errors are introduced through the process of\nconverting document formats like PDF to plain text. Next, we describe an\napproach to extracting the properties being measured (e.g., the property \"pixel\npitch\" in the phrase \"a pixel pitch as high as 352 {\\mu}m\"). Finally, we\npresent MQSearch: the realization of a search engine with full support for\nmeasured information.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 16:36:27 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Maiya", "Arun S.", ""], ["Visser", "Dale", ""], ["Wan", "Andrew", ""]]}, {"id": "1505.01130", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Marc Bola\\~nos, Ricard Mestre, Estefan\\'ia Talavera, Xavier\n  Gir\\'o-i-Nieto and Petia Radeva", "title": "Visual Summary of Egocentric Photostreams by Representative Keyframes", "comments": "Paper accepted in the IEEE First International Workshop on Wearable\n  and Ego-vision Systems for Augmented Experience (WEsAX). Turin, Italy. July\n  3, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a visual summary from an egocentric photostream captured by a\nlifelogging wearable camera is of high interest for different applications\n(e.g. memory reinforcement). In this paper, we propose a new summarization\nmethod based on keyframes selection that uses visual features extracted by\nmeans of a convolutional neural network. Our method applies an unsupervised\nclustering for dividing the photostreams into events, and finally extracts the\nmost relevant keyframe for each event. We assess the results by applying a\nblind-taste test on a group of 20 people who assessed the quality of the\nsummaries.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 19:14:23 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 14:00:23 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Bola\u00f1os", "Marc", ""], ["Mestre", "Ricard", ""], ["Talavera", "Estefan\u00eda", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""], ["Radeva", "Petia", ""]]}, {"id": "1505.01214", "submitter": "Babak Saleh", "authors": "Babak Saleh and Mira Dontcheva and Aaron Hertzmann and Zhicheng Liu", "title": "Learning Style Similarity for Searching Infographics", "comments": "6 pages, to appear in the 41st annual conference on Graphics\n  Interface (GI) 2015,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.HC cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infographics are complex graphic designs integrating text, images, charts and\nsketches. Despite the increasing popularity of infographics and the rapid\ngrowth of online design portfolios, little research investigates how we can\ntake advantage of these design resources. In this paper we present a method for\nmeasuring the style similarity between infographics. Based on human perception\ndata collected from crowdsourced experiments, we use computer vision and\nmachine learning algorithms to learn a style similarity metric for infographic\ndesigns. We evaluate different visual features and learning algorithms and find\nthat a combination of color histograms and Histograms-of-Gradients (HoG)\nfeatures is most effective in characterizing the style of infographics. We\ndemonstrate our similarity metric on a preliminary image retrieval test.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 22:59:32 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Saleh", "Babak", ""], ["Dontcheva", "Mira", ""], ["Hertzmann", "Aaron", ""], ["Liu", "Zhicheng", ""]]}, {"id": "1505.01303", "submitter": "Joseph Paul Cohen", "authors": "Joseph Paul Cohen and Wei Ding and Abraham Bagherjeiran", "title": "XTreePath: A generalization of XPath to handle real world structural\n  variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a key problem in information extraction which deals with wrapper\nfailures due to changing content templates. A good proportion of wrapper\nfailures are due to HTML templates changing to cause wrappers to become\nincompatible after element inclusion or removal in a DOM (Tree representation\nof HTML). We perform a large-scale empirical analyses of the causes of shift\nand mathematically quantify the levels of domain difficulty based on entropy.\nWe propose the XTreePath annotation method to captures contextual node\ninformation from the training DOM. We then utilize this annotation in a\nsupervised manner at test time with our proposed Recursive Tree Matching method\nwhich locates nodes most similar in context recursively using the tree edit\ndistance. The search is based on a heuristic function that takes into account\nthe similarity of a tree compared to the structure that was present in the\ntraining data. We evaluate XTreePath using 117,422 pages from 75 diverse\nwebsites in 8 vertical markets. Our XTreePath method consistently outperforms\nXPath and a current commercial system in terms of successful extractions in a\nblackbox test. We make our code and datasets publicly available online.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 10:08:12 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 21:44:41 GMT"}, {"version": "v3", "created": "Wed, 27 Dec 2017 01:31:49 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Cohen", "Joseph Paul", ""], ["Ding", "Wei", ""], ["Bagherjeiran", "Abraham", ""]]}, {"id": "1505.01306", "submitter": "Joan Guisado-G\\'amez", "authors": "Joan Guisado-G\\'amez, Arnau Prat-P\\'erez", "title": "Understanding Graph Structure of Wikipedia for Query Expansion", "comments": null, "journal-ref": null, "doi": "10.1145/2764947.2764953", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases are very good sources for knowledge extraction, the ability\nto create knowledge from structured and unstructured sources and use it to\nimprove automatic processes as query expansion. However, extracting knowledge\nfrom unstructured sources is still an open challenge. In this respect,\nunderstanding the structure of knowledge bases can provide significant benefits\nfor the effectiveness of such purpose. In particular, Wikipedia has become a\nvery popular knowledge base in the last years because it is a general\nencyclopedia that has a large amount of information and thus, covers a large\namount of different topics. In this piece of work, we analyze how articles and\ncategories of Wikipedia relate to each other and how these relationships can\nsupport a query expansion technique. In particular, we show that the structures\nin the form of dense cycles with a minimum amount of categories tend to\nidentify the most relevant information.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 10:14:06 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Guisado-G\u00e1mez", "Joan", ""], ["Prat-P\u00e9rez", "Arnau", ""]]}, {"id": "1505.01431", "submitter": "Howard Cohl", "authors": "Howard S. Cohl, Moritz Schubotz, Marjorie A. McClain, Bonita V.\n  Saunders, Cherry Y. Zou, Azeem S. Mohammed, Alex A. Danoff", "title": "Growing the Digital Repository of Mathematical Formulae with Generic\n  LaTeX Sources", "comments": "I included an extra unrelated png file in the zip directory and it\n  was falsely mentioned on a page 9. Previously I tried unsuccessfully to fix\n  this. I removed the png file and now it is only 8 pages how it should be", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  One initial goal for the DRMF is to seed our digital compendium with\nfundamental orthogonal polynomial formulae. We had used the data from the NIST\nDigital Library of Mathematical Functions (DLMF) as initial seed for our DRMF\nproject. The DLMF input LaTeX source already contains some semantic information\nencoded using a highly customized set of semantic LaTeX macros. Those macros\ncould be converted to content MathML using LaTeXML. During that conversion the\nsemantics were translated to an implicit DLMF content dictionary. This year, we\nhave developed a semantic enrichment process whose goal is to infer semantic\ninformation from generic LaTeX sources. The generated context-free semantic\ninformation is used to build DRMF formula home pages for each individual\nformula. We demonstrate this process using selected chapters from the book\n\"Hypergeometric Orthogonal Polynomials and their $q$-Analogues\" (2010) by\nKoekoek, Lesky and Swarttouw (KLS) as well as an actively maintained addendum\nto this book by Koornwinder (KLSadd). The generic input KLS and KLSadd LaTeX\nsources describe the printed representation of the formulae, but does not\ncontain explicit semantic information. See http://drmf.wmflabs.org.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 16:51:31 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 14:33:30 GMT"}, {"version": "v3", "created": "Fri, 8 May 2015 10:27:49 GMT"}, {"version": "v4", "created": "Mon, 11 May 2015 01:03:00 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Cohl", "Howard S.", ""], ["Schubotz", "Moritz", ""], ["McClain", "Marjorie A.", ""], ["Saunders", "Bonita V.", ""], ["Zou", "Cherry Y.", ""], ["Mohammed", "Azeem S.", ""], ["Danoff", "Alex A.", ""]]}, {"id": "1505.01606", "submitter": "Harsh Thakkar", "authors": "Harsh Thakkar, Ganesh Iyer, Prasenjit Majumder", "title": "A comparative study of approaches in user-centered health information\n  retrieval", "comments": "6 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we survey various user-centered or context-based biomedical\nhealth information retrieval systems. We present and discuss the performance of\nsystems submitted in CLEF eHealth 2014 Task 3 for this purpose. We classify and\nfocus on comparing the two most prevalent retrieval models in biomedical\ninformation retrieval namely: Language Model (LM) and Vector Space Model (VSM).\nWe also report on the effectiveness of using external medical resources and\nontologies like MeSH, Metamap, UMLS, etc. We observed that the L.M. based\nretrieval systems outperform VSM based systems on various fronts. From the\nresults we conclude that the state-of-art system scores for MAP was 0.4146,\nP@10 was 0.7560 and NDCG@10 was 0.7445, respectively. All of these score were\nreported by systems built on language modelling approaches.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 07:32:33 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Thakkar", "Harsh", ""], ["Iyer", "Ganesh", ""], ["Majumder", "Prasenjit", ""]]}, {"id": "1505.01621", "submitter": "Anupriya Gogna", "authors": "Anupriya Gogna, Angshul Majumdar", "title": "Blind Compressive Sensing Framework for Collaborative Filtering", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing works based on latent factor models have focused on representing the\nrating matrix as a product of user and item latent factor matrices, both being\ndense. Latent (factor) vectors define the degree to which a trait is possessed\nby an item or the affinity of user towards that trait. A dense user matrix is a\nreasonable assumption as each user will like/dislike a trait to certain extent.\nHowever, any item will possess only a few of the attributes and never all.\nHence, the item matrix should ideally have a sparse structure rather than a\ndense one as formulated in earlier works. Therefore we propose to factor the\nratings matrix into a dense user matrix and a sparse item matrix which leads us\nto the Blind Compressed Sensing (BCS) framework. We derive an efficient\nalgorithm for solving the BCS problem based on Majorization Minimization (MM)\ntechnique. Our proposed approach is able to achieve significantly higher\naccuracy and shorter run times as compared to existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 08:19:05 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Gogna", "Anupriya", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1505.02251", "submitter": "Aris Kosmopoulos", "authors": "Aris Kosmopoulos and Georgios Paliouras and Ion Androutsopoulos", "title": "Probabilistic Cascading for Large Scale Hierarchical Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchies are frequently used for the organization of objects. Given a\nhierarchy of classes, two main approaches are used, to automatically classify\nnew instances: flat classification and cascade classification. Flat\nclassification ignores the hierarchy, while cascade classification greedily\ntraverses the hierarchy from the root to the predicted leaf. In this paper we\npropose a new approach, which extends cascade classification to predict the\nright leaf by estimating the probability of each root-to-leaf path. We provide\nexperimental results which indicate that, using the same classification\nalgorithm, one can achieve better results with our approach, compared to the\ntraditional flat and cascade classifications.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 09:39:04 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Kosmopoulos", "Aris", ""], ["Paliouras", "Georgios", ""], ["Androutsopoulos", "Ion", ""]]}, {"id": "1505.02445", "submitter": "Tomaso Aste", "authors": "Guido Previde Massara, T. Di Matteo, Tomaso Aste", "title": "Network Filtering for Big Data: Triangulated Maximally Filtered Graph", "comments": "16 pages, 7 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cond-mat.stat-mech cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a network-filtering method, the Triangulated Maximally Filtered\nGraph (TMFG), that provides an approximate solution to the Weighted Maximal\nPlanar Graph problem. The underlying idea of TMFG consists in building a\ntriangulation that maximizes a score function associated with the amount of\ninformation retained by the network. TMFG uses as weights any arbitrary\nsimilarity measure to arrange data into a meaningful network structure that can\nbe used for clustering, community detection and modeling. The method is fast,\nadaptable and scalable to very large datasets, it allows online updating and\nlearning as new data can be inserted and deleted with combinations of local and\nnon-local moves. TMFG permits readjustments of the network in consequence of\nchanges in the strength of the similarity measure. The method is based on local\ntopological moves and can therefore take advantage of parallel and GPUs\ncomputing. We discuss how this network-filtering method can be used intuitively\nand efficiently for big data studies and its significance from an\ninformation-theoretic perspective.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2015 21:47:38 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2015 16:02:37 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Massara", "Guido Previde", ""], ["Di Matteo", "T.", ""], ["Aste", "Tomaso", ""]]}, {"id": "1505.02798", "submitter": "Richard Zanibbi", "authors": "Richard Zanibbi and Awelemdy Orakwue", "title": "Math Search for the Masses: Multimodal Search Interfaces and\n  Appearance-Based Retrieval", "comments": "Paper for Invited Talk at 2015 Conference on Intelligent Computer\n  Mathematics (July, Washington DC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We summarize math search engines and search interfaces produced by the\nDocument and Pattern Recognition Lab in recent years, and in particular the min\nmath search interface and the Tangent search engine. Source code for both\nsystems are publicly available. \"The Masses\" refers to our emphasis on creating\nsystems for mathematical non-experts, who may be looking to define unfamiliar\nnotation, or browse documents based on the visual appearance of formulae rather\nthan their mathematical semantics.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 20:39:48 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Zanibbi", "Richard", ""], ["Orakwue", "Awelemdy", ""]]}, {"id": "1505.02867", "submitter": "Charles Mathy", "authors": "Charles Mathy, Nate Derbinsky, Jos\\'e Bento, Jonathan Rosenthal and\n  Jonathan Yedidia", "title": "The Boundary Forest Algorithm for Online Supervised and Unsupervised\n  Learning", "comments": "7 pages, 4 figs, 1 page supp. info", "journal-ref": "Proc. of the 29th AAAI Conference on Artificial Intelligence\n  (AAAI), 2864-2870. Austin, TX, USA. (2015)", "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new instance-based learning algorithm called the Boundary\nForest (BF) algorithm, that can be used for supervised and unsupervised\nlearning. The algorithm builds a forest of trees whose nodes store previously\nseen examples. It can be shown data points one at a time and updates itself\nincrementally, hence it is naturally online. Few instance-based algorithms have\nthis property while being simultaneously fast, which the BF is. This is crucial\nfor applications where one needs to respond to input data in real time. The\nnumber of children of each node is not set beforehand but obtained from the\ntraining procedure, which makes the algorithm very flexible with regards to\nwhat data manifolds it can learn. We test its generalization performance and\nspeed on a range of benchmark datasets and detail in which settings it\noutperforms the state of the art. Empirically we find that training time scales\nas O(DNlog(N)) and testing as O(Dlog(N)), where D is the dimensionality and N\nthe amount of data,\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 03:45:11 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Mathy", "Charles", ""], ["Derbinsky", "Nate", ""], ["Bento", "Jos\u00e9", ""], ["Rosenthal", "Jonathan", ""], ["Yedidia", "Jonathan", ""]]}, {"id": "1505.02891", "submitter": "Abd Elrahman Shafei", "authors": "Abdelrahman Elsayed, Hoda M. O. Mokhtar, Osama Ismail", "title": "Ontology Based Document Clustering Using MapReduce", "comments": "12 page", "journal-ref": "The International Journal of Database Management Systems (IJDMS),\n  April 2015, Volume 7, Number 2", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, document clustering is considered as a data intensive task due to\nthe dramatic, fast increase in the number of available documents. Nevertheless,\nthe features that represent those documents are also too large. The most common\nmethod for representing documents is the vector space model, which represents\ndocument features as a bag of words and does not represent semantic relations\nbetween words. In this paper we introduce a distributed implementation for the\nbisecting k-means using MapReduce programming model. The aim behind our\nproposed implementation is to solve the problem of clustering intensive data\ndocuments. In addition, we propose integrating the WordNet ontology with\nbisecting k-means in order to utilize the semantic relations between words to\nenhance document clustering results. Our presented experimental results show\nthat using lexical categories for nouns only enhances internal evaluation\nmeasures of document clustering; and decreases the documents features from\nthousands to tens features. Our experiments were conducted using Amazon Elastic\nMapReduce to deploy the Bisecting k-means algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 07:41:43 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Elsayed", "Abdelrahman", ""], ["Mokhtar", "Hoda M. O.", ""], ["Ismail", "Osama", ""]]}, {"id": "1505.02973", "submitter": "Konstantinos Tserpes", "authors": "Evangelos Psomakelis, Konstantinos Tserpes, Dimosthenis\n  Anagnostopoulos, Theodora Varvarigou", "title": "Comparing methods for Twitter Sentiment Analysis", "comments": "5 pages, 1 figure, 6th Conference on Knowledge Discovery and\n  Information Retrieval 2014, Rome, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work extends the set of works which deal with the popular problem of\nsentiment analysis in Twitter. It investigates the most popular document\n(\"tweet\") representation methods which feed sentiment evaluation mechanisms. In\nparticular, we study the bag-of-words, n-grams and n-gram graphs approaches and\nfor each of them we evaluate the performance of a lexicon-based and 7\nlearning-based classification algorithms (namely SVM, Na\\\"ive Bayesian\nNetworks, Logistic Regression, Multilayer Perceptrons, Best-First Trees,\nFunctional Trees and C4.5) as well as their combinations, using a set of 4451\nmanually annotated tweets. The results demonstrate the superiority of\nlearning-based methods and in particular of n-gram graphs approaches for\npredicting the sentiment of tweets. They also show that the combinatory\napproach has impressive effects on n-grams, raising the confidence up to 83.15%\non the 5-Grams, using majority vote and a balanced dataset (equal number of\npositive, negative and neutral tweets for training). In the n-gram graph cases\nthe improvement was small to none, reaching 94.52% on the 4-gram graphs, using\nOrthodromic distance and a threshold of 0.001.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 12:05:19 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Psomakelis", "Evangelos", ""], ["Tserpes", "Konstantinos", ""], ["Anagnostopoulos", "Dimosthenis", ""], ["Varvarigou", "Theodora", ""]]}, {"id": "1505.03002", "submitter": "Dima Shepelyansky L", "authors": "Robert Palovics, Balint Daroczy, Andras Benczur, Julia Pap, Leonardo\n  Ermann, Samuel Phan, Alexei D. Chepelianskii, Dima L. Shepelyansky", "title": "Statistical analysis of NOMAO customer votes for spots of France", "comments": "10 pages, 12 figs", "journal-ref": "Eur. Phys. J. B. v.88, p.194 (2015)", "doi": "10.1140/epjb/e2015-60357-1", "report-no": null, "categories": "physics.soc-ph cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the statistical properties of votes of customers for spots of\nFrance collected by the startup company NOMAO. The frequencies of votes per\nspot and per customer are characterized by a power law distributions which\nremain stable on a time scale of a decade when the number of votes is varied by\nalmost two orders of magnitude. Using the computer science methods we explore\nthe spectrum and the eigenvalues of a matrix containing user ratings to\ngeolocalized items. Eigenvalues nicely map to large towns and regions but show\ncertain level of instability as we modify the interpretation of the underlying\nmatrix. We evaluate imputation strategies that provide improved prediction\nperformance by reaching geographically smooth eigenvectors. We point on\npossible links between distribution of votes and the phenomenon of\nself-organized criticality.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 13:35:18 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Palovics", "Robert", ""], ["Daroczy", "Balint", ""], ["Benczur", "Andras", ""], ["Pap", "Julia", ""], ["Ermann", "Leonardo", ""], ["Phan", "Samuel", ""], ["Chepelianskii", "Alexei D.", ""], ["Shepelyansky", "Dima L.", ""]]}, {"id": "1505.03014", "submitter": "Linas Baltrunas", "authors": "Linas Baltrunas, Karen Church, Alexandros Karatzoglou, Nuria Oliver", "title": "Frappe: Understanding the Usage and Perception of Mobile App\n  Recommendations In-The-Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": "11", "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a real world deployment of a context-aware mobile app\nrecommender system (RS) called Frappe. Utilizing a hybrid-approach, we\nconducted a large-scale app market deployment with 1000 Android users combined\nwith a small-scale local user study involving 33 users. The resulting usage\nlogs and subjective feedback enabled us to gather key insights into (1)\ncontext-dependent app usage and (2) the perceptions and experiences of\nend-users while interacting with context-aware mobile app recommendations.\nWhile Frappe performs very well based on usage-centric evaluation metrics\ninsights from the small-scale study reveal some negative user experiences. Our\nresults point to a number of actionable lessons learned specifically related to\ndesigning, deploying and evaluating mobile context-aware RS in-the-wild with\nreal users.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 14:11:58 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Baltrunas", "Linas", ""], ["Church", "Karen", ""], ["Karatzoglou", "Alexandros", ""], ["Oliver", "Nuria", ""]]}, {"id": "1505.03090", "submitter": "Yu Zhong", "authors": "Yu Zhong", "title": "Efficient Similarity Indexing and Searching in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient indexing and searching of high dimensional data has been an area of\nactive research due to the growing exploitation of high dimensional data and\nthe vulnerability of traditional search methods to the curse of dimensionality.\nThis paper presents a new approach for fast and effective searching and\nindexing of high dimensional features using random partitions of the feature\nspace. Experiments on both handwritten digits and 3-D shape descriptors have\nshown the proposed algorithm to be highly effective and efficient in indexing\nand searching real data sets of several hundred dimensions. We also compare its\nperformance to that of the state-of-the-art locality sensitive hashing\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 17:17:28 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Zhong", "Yu", ""]]}, {"id": "1505.03236", "submitter": "Jensi", "authors": "R. Jensi and G. Wiselin Jiji", "title": "Hybrid data clustering approach using K-Means and Flower Pollination\n  Algorithm", "comments": "11 pages, Journal. Advanced Computational Intelligence: An\n  International Journal (ACII), Vol.2, No.2, April 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data clustering is a technique for clustering set of objects into known\nnumber of groups. Several approaches are widely applied to data clustering so\nthat objects within the clusters are similar and objects in different clusters\nare far away from each other. K-Means, is one of the familiar center based\nclustering algorithms since implementation is very easy and fast convergence.\nHowever, K-Means algorithm suffers from initialization, hence trapped in local\noptima. Flower Pollination Algorithm (FPA) is the global optimization\ntechnique, which avoids trapping in local optimum solution. In this paper, a\nnovel hybrid data clustering approach using Flower Pollination Algorithm and\nK-Means (FPAKM) is proposed. The proposed algorithm results are compared with\nK-Means and FPA on eight datasets. From the experimental results, FPAKM is\nbetter than FPA and K-Means.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 04:24:50 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Jensi", "R.", ""], ["Jiji", "G. Wiselin", ""]]}, {"id": "1505.03823", "submitter": "Miao Fan", "authors": "Miao Fan, Qiang Zhou and Thomas Fang Zheng", "title": "Distant Supervision for Entity Linking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity linking is an indispensable operation of populating knowledge\nrepositories for information extraction. It studies on aligning a textual\nentity mention to its corresponding disambiguated entry in a knowledge\nrepository. In this paper, we propose a new paradigm named distantly supervised\nentity linking (DSEL), in the sense that the disambiguated entities that belong\nto a huge knowledge repository (Freebase) are automatically aligned to the\ncorresponding descriptive webpages (Wiki pages). In this way, a large scale of\nweakly labeled data can be generated without manual annotation and fed to a\nclassifier for linking more newly discovered entities. Compared with\ntraditional paradigms based on solo knowledge base, DSEL benefits more via\njointly leveraging the respective advantages of Freebase and Wikipedia.\nSpecifically, the proposed paradigm facilitates bridging the disambiguated\nlabels (Freebase) of entities and their textual descriptions (Wikipedia) for\nWeb-scale entities. Experiments conducted on a dataset of 140,000 items and\n60,000 features achieve a baseline F1-measure of 0.517. Furthermore, we analyze\nthe feature performance and improve the F1-measure to 0.545.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 18:15:49 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 14:45:19 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2015 01:25:26 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Fan", "Miao", ""], ["Zhou", "Qiang", ""], ["Zheng", "Thomas Fang", ""]]}, {"id": "1505.03934", "submitter": "Giancarlo Crocetti", "authors": "Giancarlo Crocetti", "title": "Textual Spatial Cosine Similarity", "comments": "4 pages, 4 tables. Proceedings of 12th Annual Research Day, 2014 -\n  Pace University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with document similarity many methods exist today, like cosine\nsimilarity. More complex methods are also available based on the semantic\nanalysis of textual information, which are computationally expensive and rarely\nused in the real time feeding of content as in enterprise-wide search\nenvironments. To address these real-time constraints, we developed a new\nmeasure of document similarity called Textual Spatial Cosine Similarity, which\nis able to detect similitude at the semantic level using word placement\ninformation contained in the document. We will see in this paper that two\ndegenerate cases exist for this model, which coincide with Cosine Similarity on\none side and with a paraphrasing detection model to the other.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 01:08:57 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Crocetti", "Giancarlo", ""]]}, {"id": "1505.03984", "submitter": "Xiaoming Zhang", "authors": "Xiaoming Zhang, Zhoujun Li, Senzhang Wang, Yang Yang, Xueqiang Lv", "title": "Location Prediction of Social Images via Generative Model", "comments": "8 pages", "journal-ref": null, "doi": "10.1145/2671188.2749308", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast amount of geo-tagged social images has attracted great attention in\nresearch of predicting location using the plentiful content of images, such as\nvisual content and textual description. Most of the existing researches use the\ntext-based or vision-based method to predict location. There still exists a\nproblem: how to effectively exploit the correlation between different types of\ncontent as well as their geographical distributions for location prediction. In\nthis paper, we propose to predict image location by learning the latent\nrelation between geographical location and multiple types of image content. In\nparticularly, we propose a geographical topic model GTMI (geographical topic\nmodel of social image) to integrate multiple types of image content as well as\nthe geographical distributions, In GTMI, image topic is modeled on both text\nvocabulary and visual feature. Each region has its own distribution over topics\nand hence has its own language model and vision pattern. The location of a new\nimage is estimated based on the joint probability of image content and\nsimilarity measure on topic distribution between images. Experiment results\ndemonstrate the performance of location prediction based on GTMI.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 08:19:24 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Zhang", "Xiaoming", ""], ["Li", "Zhoujun", ""], ["Wang", "Senzhang", ""], ["Yang", "Yang", ""], ["Lv", "Xueqiang", ""]]}, {"id": "1505.04094", "submitter": "Yang Yang", "authors": "Yang Yang and Ryan N. Lichtenwalter and Nitesh V. Chawla", "title": "Evaluating Link Prediction Methods", "comments": null, "journal-ref": null, "doi": "10.1007/s10115-014-0789-0", "report-no": null, "categories": "cs.IR cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction is a popular research area with important applications in a\nvariety of disciplines, including biology, social science, security, and\nmedicine. The fundamental requirement of link prediction is the accurate and\neffective prediction of new links in networks. While there are many different\nmethods proposed for link prediction, we argue that the practical performance\npotential of these methods is often unknown because of challenges in the\nevaluation of link prediction, which impact the reliability and reproducibility\nof results. We describe these challenges, provide theoretical proofs and\nempirical examples demonstrating how current methods lead to questionable\nconclusions, show how the fallacy of these conclusions is illuminated by\nmethods we propose, and develop recommendations for consistent, standard, and\napplicable evaluation metrics. We also recommend the use of precision-recall\nthreshold curves and associated areas in lieu of receiver operating\ncharacteristic curves due to complications that arise from extreme imbalance in\nthe link prediction classification problem.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 15:28:58 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Yang", "Yang", ""], ["Lichtenwalter", "Ryan N.", ""], ["Chawla", "Nitesh V.", ""]]}, {"id": "1505.04657", "submitter": "Phong Minh Vu", "authors": "Phong Minh Vu, Tam The Nguyen, Hung Viet Pham, Tung Thanh Nguyen", "title": "Mining User Opinions in Mobile App Reviews: A Keyword-based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User reviews of mobile apps often contain complaints or suggestions which are\nvaluable for app developers to improve user experience and satisfaction.\nHowever, due to the large volume and noisy-nature of those reviews, manually\nanalyzing them for useful opinions is inherently challenging. To address this\nproblem, we propose MARK, a keyword-based framework for semi-automated review\nanalysis. MARK allows an analyst describing his interests in one or some mobile\napps by a set of keywords. It then finds and lists the reviews most relevant to\nthose keywords for further analysis. It can also draw the trends over time of\nthose keywords and detect their sudden changes, which might indicate the\noccurrences of serious issues. To help analysts describe their interests more\neffectively, MARK can automatically extract keywords from raw reviews and rank\nthem by their associations with negative reviews. In addition, based on a\nvector-based semantic representation of keywords, MARK can divide a large set\nof keywords into more cohesive subsets, or suggest keywords similar to the\nselected ones.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 14:25:03 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2015 02:23:32 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Vu", "Phong Minh", ""], ["Nguyen", "Tam The", ""], ["Pham", "Hung Viet", ""], ["Nguyen", "Tung Thanh", ""]]}, {"id": "1505.05007", "submitter": "Paul Blomstedt PhD", "authors": "Paul Blomstedt, Ritabrata Dutta, Sohan Seth, Alvis Brazma and Samuel\n  Kaski", "title": "Modelling-based experiment retrieval: A case study with gene expression\n  clustering", "comments": "Updated figures. The final version of this article will appear in\n  Bioinformatics (https://bioinformatics.oxfordjournals.org/)", "journal-ref": null, "doi": "10.1093/bioinformatics/btv762", "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Public and private repositories of experimental data are growing\nto sizes that require dedicated methods for finding relevant data. To improve\non the state of the art of keyword searches from annotations, methods for\ncontent-based retrieval have been proposed. In the context of gene expression\nexperiments, most methods retrieve gene expression profiles, requiring each\nexperiment to be expressed as a single profile, typically of case vs. control.\nA more general, recently suggested alternative is to retrieve experiments whose\nmodels are good for modelling the query dataset. However, for very noisy and\nhigh-dimensional query data, this retrieval criterion turns out to be very\nnoisy as well.\n  Results: We propose doing retrieval using a denoised model of the query\ndataset, instead of the original noisy dataset itself. To this end, we\nintroduce a general probabilistic framework, where each experiment is modelled\nseparately and the retrieval is done by finding related models. For retrieval\nof gene expression experiments, we use a probabilistic model called product\npartition model, which induces a clustering of genes that show similar\nexpression patterns across a number of samples. The suggested metric for\nretrieval using clusterings is the normalized information distance. Empirical\nresults finally suggest that inference for the full probabilistic model can be\napproximated with good performance using computationally faster heuristic\nclustering approaches (e.g. $k$-means). The method is highly scalable and\nstraightforward to apply to construct a general-purpose gene expression\nexperiment retrieval method.\n  Availability: The method can be implemented using standard clustering\nalgorithms and normalized information distance, available in many statistical\nsoftware packages.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 14:21:34 GMT"}, {"version": "v2", "created": "Tue, 26 May 2015 11:53:47 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 09:12:58 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2016 15:08:26 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Blomstedt", "Paul", ""], ["Dutta", "Ritabrata", ""], ["Seth", "Sohan", ""], ["Brazma", "Alvis", ""], ["Kaski", "Samuel", ""]]}, {"id": "1505.05136", "submitter": "Nicolas Turenne", "authors": "Nicolas Turenne", "title": "A Table-Binning Approach for Visualizing the Past", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large amounts of data are available due to low-cost and high-capacity data\nstorage equipments. We propose a data exploration/visualization method for\ntabular multi-dimensional, time-varying datasets to present selected items in\ntheir global context. The approach is simple and uses a rank-based\nvisualization and a pattern matching functionality based on temporal profiles.\nRanking categories can be specified in a flexible way and are used instead of\nactual values (value reduction into bins) and plotting it over time in an\nunevenly quantized representation. Patterns that emerge are matched against a\nset of eight predefined temporal profiles. The graphical summarization of\nlarge-scale temporal data is proposed and applicability is tested qualitatively\non about eight data sets and the approach is compared to classic line plots and\nSAX representation\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 11:15:39 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Turenne", "Nicolas", ""]]}, {"id": "1505.05187", "submitter": "Abhay Prakash", "authors": "Abhay Prakash and Dhaval Patel", "title": "Techniques for Deep Query Understanding", "comments": "30 pages, 18 figures, 2 tables, student report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query Understanding concerns about inferring the precise intent of search by\nthe user with his formulated query, which is challenging because the queries\nare often very short and ambiguous. The report discusses the various kind of\nqueries that can be put to a Search Engine and illustrates the Role of Query\nUnderstanding for return of relevant results. With different advances in\ntechniques for deep understanding of queries as well as documents, the Search\nTechnology has witnessed three major era. A lot of interesting real world\nexamples have been used to illustrate the role of Query Understanding in each\nof them. The Query Understanding Module is responsible to correct the mistakes\ndone by user in the query, to guide him in formulation of query with precise\nintent, and to precisely infer the intent of the user query. The report\ndescribes the complete architecture to handle aforementioned three tasks, and\nthen discusses basic as well as recent advanced techniques for each of the\ncomponent, through appropriate papers from reputed conferences and journals.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 21:02:44 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Prakash", "Abhay", ""], ["Patel", "Dhaval", ""]]}, {"id": "1505.05240", "submitter": "Siddharth Srivastava", "authors": "Siddharth Srivastava, Prerana Mukherjee, Brejesh Lall", "title": "Benchmarking KAZE and MCM for Multiclass Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": "v01.0", "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for feature generation by\nappropriately fusing KAZE and SIFT features. We then use this feature set along\nwith Minimal Complexity Machine(MCM) for object classification. We show that\nKAZE and SIFT features are complementary. Experimental results indicate that an\nelementary integration of these techniques can outperform the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 04:09:47 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Srivastava", "Siddharth", ""], ["Mukherjee", "Prerana", ""], ["Lall", "Brejesh", ""]]}, {"id": "1505.05613", "submitter": "Chris De Vries", "authors": "Christopher M. de Vries, Lance De Vine, Shlomo Geva, Richi Nayak", "title": "Parallel Streaming Signature EM-tree: A Clustering Algorithm for Web\n  Scale Applications", "comments": "11 pages, WWW 2015", "journal-ref": null, "doi": "10.1145/2736277.2741111", "report-no": null, "categories": "cs.IR cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of the web presents an unsolved problem of automatically\nanalyzing billions of pages of natural language. We introduce a scalable\nalgorithm that clusters hundreds of millions of web pages into hundreds of\nthousands of clusters. It does this on a single mid-range machine using\nefficient algorithms and compressed document representations. It is applied to\ntwo web-scale crawls covering tens of terabytes. ClueWeb09 and ClueWeb12\ncontain 500 and 733 million web pages and were clustered into 500,000 to\n700,000 clusters. To the best of our knowledge, such fine grained clustering\nhas not been previously demonstrated. Previous approaches clustered a sample\nthat limits the maximum number of discoverable clusters. The proposed EM-tree\nalgorithm uses the entire collection in clustering and produces several orders\nof magnitude more clusters than the existing algorithms. Fine grained\nclustering is necessary for meaningful clustering in massive collections where\nthe number of distinct topics grows linearly with collection size. These\nfine-grained clusters show an improved cluster quality when assessed with two\nnovel evaluations using ad hoc search relevance judgments and spam\nclassifications for external validation. These evaluations solve the problem of\nassessing the quality of clusters where categorical labeling is unavailable and\nunfeasible.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 06:22:04 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["de Vries", "Christopher M.", ""], ["De Vine", "Lance", ""], ["Geva", "Shlomo", ""], ["Nayak", "Richi", ""]]}, {"id": "1505.05788", "submitter": "Weinan Zhang", "authors": "Weinan Zhang, Ye Pan, Tianxiong Zhou, Jun Wang", "title": "An Empirical Study on Display Ad Impression Viewability Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Display advertising normally charges advertisers for every single ad\nimpression. Specifically, if an ad in a webpage has been loaded in the browser,\nan ad impression is counted. However, due to the position and size of the ad\nslot, lots of ads are actually not viewed but still measured as impressions and\ncharged. These fraud ad impressions indeed undermine the efficacy of display\nadvertising. A perfect ad impression viewability measurement should match what\nthe user has really viewed with a short memory. In this paper, we conduct\nextensive investigations on display ad impression viewability measurements on\ndimensions of ad creative displayed pixel percentage and exposure time to find\nwhich measurement provides the most accurate ad impression counting. The\nempirical results show that the most accurate measurement counts one ad\nimpression if more than 75% of the ad creative pixels have been exposed for at\nleast 2 continuous seconds.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 17:00:23 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Zhang", "Weinan", ""], ["Pan", "Ye", ""], ["Zhou", "Tianxiong", ""], ["Wang", "Jun", ""]]}, {"id": "1505.05821", "submitter": "Ehsan Amid", "authors": "Ehsan Amid, Onur Dikmen, Erkki Oja", "title": "Optimizing the Information Retrieval Trade-off in Data Visualization\n  Using $\\alpha$-Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualization is one of the major applications of nonlinear\ndimensionality reduction. From the information retrieval perspective, the\nquality of a visualization can be evaluated by considering the extent that the\nneighborhood relation of each data point is maintained while the number of\nunrelated points that are retrieved is minimized. This property can be\nquantified as a trade-off between the mean precision and mean recall of the\nvisualization. While there have been some approaches to formulate the\nvisualization objective directly as a weighted sum of the precision and recall,\nthere is no systematic way to determine the optimal trade-off between these two\nnor a clear interpretation of the optimal value. In this paper, we investigate\nthe properties of $\\alpha$-divergence for information visualization, focusing\nour attention on a particular range of $\\alpha$ values. We show that the\nminimization of the new cost function corresponds to maximizing a geometric\nmean between precision and recall, parameterized by $\\alpha$. Contrary to some\nearlier methods, no hand-tuning is needed, but we can rigorously estimate the\noptimal value of $\\alpha$ for a given input data. For this, we provide a\nstatistical framework using a novel distribution called Exponential Divergence\nwith Augmentation (EDA). By the extensive set of experiments, we show that the\noptimal value of $\\alpha$, obtained by EDA corresponds to the optimal trade-off\nbetween the precision and recall for a given data distribution.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 18:19:28 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 20:44:37 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Amid", "Ehsan", ""], ["Dikmen", "Onur", ""], ["Oja", "Erkki", ""]]}, {"id": "1505.06386", "submitter": "Michele Trevisiol", "authors": "Michele Trevisiol, Luca Maria Aiello, Paolo Boldi, Roi Blanco", "title": "Local Ranking Problem on the BrowseGraph", "comments": null, "journal-ref": null, "doi": "10.1145/2766462.2767704", "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"Local Ranking Problem\" (LRP) is related to the computation of a\ncentrality-like rank on a local graph, where the scores of the nodes could\nsignificantly differ from the ones computed on the global graph. Previous work\nhas studied LRP on the hyperlink graph but never on the BrowseGraph, namely a\ngraph where nodes are webpages and edges are browsing transitions. Recently,\nthis graph has received more and more attention in many different tasks such as\nranking, prediction and recommendation. However, a web-server has only the\nbrowsing traffic performed on its pages (local BrowseGraph) and, as a\nconsequence, the local computation can lead to estimation errors, which hinders\nthe increasing number of applications in the state of the art. Also, although\nthe divergence between the local and global ranks has been measured, the\npossibility of estimating such divergence using only local knowledge has been\nmainly overlooked. These aspects are of great interest for online service\nproviders who want to: (i) gauge their ability to correctly assess the\nimportance of their resources only based on their local knowledge, and (ii)\ntake into account real user browsing fluxes that better capture the actual user\ninterest than the static hyperlink network. We study the LRP problem on a\nBrowseGraph from a large news provider, considering as subgraphs the\naggregations of browsing traces of users coming from different domains. We show\nthat the distance between rankings can be accurately predicted based only on\nstructural information of the local graph, being able to achieve an average\nrank correlation as high as 0.8.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 22:43:00 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Trevisiol", "Michele", ""], ["Aiello", "Luca Maria", ""], ["Boldi", "Paolo", ""], ["Blanco", "Roi", ""]]}, {"id": "1505.06537", "submitter": "Manish Joshi", "authors": "Manish R. Joshi and Varsha M. Pathak", "title": "A survey of SMS based Information Systems", "comments": "17 pages 3 Figures 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short Message Service (SMS) based Information Systems (SMSbIS) provide an\nexcellent alternative to a traditional approach of obtaining specific\ninformation by direct (through phone) or indirect (IVRS, Web, Email) probing.\nInformation and communication technology and far reaching mobile penetration\nhas opened this new research trend Number of key players in Search industry\nincluding Microsoft and Google are attracted by the expected increase in volume\nof use of such applications. The wide range of applications and their public\nacceptance has motivated researchers to work in this research domain. Several\napplications such as SMS based information access using database management\nservices, SMS based information retrieval through internet (search engine), SMS\nbased information extraction, question answering, image retrieval etc. have\nbeen emerged. With the aim to understand the functionality involved in these\nsystems, an extensive review of a few of these SMSbISs has been planned and\nexecuted by us. These systems are classified into four categories based on the\nobjectives and domains of the applications. As a result of this study a well\nstructured functional model is presented here. The model is evaluated in\ndifferent dimensions, which is presented in this paper. In addition to this a\nchronological progress with respect to research and development in this\nupcoming field is compiled in this paper. Such an extensive review presented in\nthis paper would definitely help the researchers and developers to understand\nthe technical aspects of this field. The functional framework presented here\nwould be useful to the system designers to design and develop an SMS based\nInformation System of any specific domain.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 10:08:19 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Joshi", "Manish R.", ""], ["Pathak", "Varsha M.", ""]]}, {"id": "1505.06646", "submitter": "Ferruccio Guidi Dr", "authors": "F. Guidi, C. Sacerdoti Coen", "title": "A Survey on Retrieval of Mathematical Knowledge", "comments": "CICM 2015, 20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a short survey of the literature on indexing and retrieval of\nmathematical knowledge, with pointers to 72 papers and tentative taxonomies of\nboth retrieval problems and recurring techniques.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 14:39:09 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 16:59:50 GMT"}, {"version": "v3", "created": "Sun, 28 Jun 2015 14:13:03 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Guidi", "F.", ""], ["Coen", "C. Sacerdoti", ""]]}, {"id": "1505.06792", "submitter": "Zhiyuan Lin", "authors": "Robert Pienta, Zhiyuan Lin, Minsuk Kahng, Jilles Vreeken, Partha P.\n  Talukdar, James Abello, Ganesh Parameswaran, Duen Horng Chau", "title": "Seeing the Forest through the Trees: Adaptive Local Exploration of Large\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization is a powerful paradigm for exploratory data analysis.\nVisualizing large graphs, however, often results in a meaningless hairball. In\nthis paper, we propose a different approach that helps the user adaptively\nexplore large million-node graphs from a local perspective. For nodes that the\nuser investigates, we propose to only show the neighbors with the most\nsubjectively interesting neighborhoods. We contribute novel ideas to measure\nthis interestingness in terms of how surprising a neighborhood is given the\nbackground distribution, as well as how well it fits the nodes the user chose\nto explore. We introduce FACETS, a fast and scalable method for visually\nexploring large graphs. By implementing our above ideas, it allows users to\nlook into the forest through its trees. Empirical evaluation shows that our\nmethod works very well in practice, providing rankings of nodes that match\ninterests of users. Moreover, as it scales linearly, FACETS is suited for the\nexploration of very large graphs.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 02:47:12 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 03:47:44 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Pienta", "Robert", ""], ["Lin", "Zhiyuan", ""], ["Kahng", "Minsuk", ""], ["Vreeken", "Jilles", ""], ["Talukdar", "Partha P.", ""], ["Abello", "James", ""], ["Parameswaran", "Ganesh", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1505.07130", "submitter": "Kemele M. Endris", "authors": "Kemele M. Endris, Sidra Faisal, Fabrizio Orlandi, S\\\"oren Auer, Simon\n  Scerri", "title": "Interest-based RDF Update Propagation", "comments": "16 pages, Keywords: Change Propagation, Dataset Dynamics, Linked\n  Data, Replication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many LOD datasets, such as DBpedia and LinkedGeoData, are voluminous and\nprocess large amounts of requests from diverse applications. Many data products\nand services rely on full or partial local LOD replications to ensure faster\nquerying and processing. While such replicas enhance the flexibility of\ninformation sharing and integration infrastructures, they also introduce data\nduplication with all the associated undesirable consequences. Given the\nevolving nature of the original and authoritative datasets, to ensure\nconsistent and up-to-date replicas frequent replacements are required at a\ngreat cost. In this paper, we introduce an approach for interest-based RDF\nupdate propagation, which propagates only interesting parts of updates from the\nsource to the target dataset. Effectively, this enables remote applications to\n`subscribe' to relevant datasets and consistently reflect the necessary changes\nlocally without the need to frequently replace the entire dataset (or a\nrelevant subset). Our approach is based on a formal definition for\ngraph-pattern-based interest expressions that is used to filter interesting\nparts of updates from the source. We implement the approach in the iRap\nframework and perform a comprehensive evaluation based on DBpedia Live updates,\nto confirm the validity and value of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 20:36:42 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Endris", "Kemele M.", ""], ["Faisal", "Sidra", ""], ["Orlandi", "Fabrizio", ""], ["Auer", "S\u00f6ren", ""], ["Scerri", "Simon", ""]]}, {"id": "1505.07396", "submitter": "Marko Horvat", "authors": "Marko Horvat, Davor Kukolja, Dragutin Ivanec", "title": "Retrieval of multimedia stimuli with semantic and emotional cues:\n  Suggestions from a controlled study", "comments": "4 pages, 3 figures, 1 table. In the Proceedings of 38th International\n  Convention on Information and Communication Technology, Electronics and\n  Microelectronics MIPRO 2015 (pp. 1399-1402)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to efficiently search pictures with annotated semantics and\nemotion is an important problem for Human-Computer Interaction with\nconsiderable interdisciplinary significance. Accuracy and speed of the\nmultimedia retrieval process depends on the chosen metadata annotation model.\nThe quality of such multifaceted retrieval is opposed to the potential\ncomplexity of data setup procedures and development of multimedia annotations.\nAdditionally, a recent study has shown that databases of emotionally annotated\nmultimedia are still being predominately searched manually which highlights the\nneed to study this retrieval modality. To this regard we present a study with N\n= 75 participants aimed to evaluate the influence of keywords and dimensional\nemotions in manual retrieval of pictures. The study showed that if the\nmultimedia database is comparatively small emotional annotations are sufficient\nto achieve a fast retrieval despite comparatively lesser overall accuracy. In a\nlarger dataset semantic annotations became necessary for efficient retrieval\nalthough they contributed to a slower beginning of the search process. The\nexperiment was performed in a controlled environment with a team of psychology\nexperts. The results were statistically consistent with validates measures of\nthe participants' perceptual speed.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 16:34:55 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 14:09:38 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Horvat", "Marko", ""], ["Kukolja", "Davor", ""], ["Ivanec", "Dragutin", ""]]}, {"id": "1505.07897", "submitter": "Zhigang Lu", "authors": "Zhigang Lu and Hong Shen", "title": "An Accuracy-Assured Privacy-Preserving Recommender System for Internet\n  Commerce", "comments": "replacement for the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems, tool for predicting users' potential preferences by\ncomputing history data and users' interests, show an increasing importance in\nvarious Internet applications such as online shopping. As a well-known\nrecommendation method, neighbourhood-based collaborative filtering has\nattracted considerable attention recently. The risk of revealing users' private\ninformation during the process of filtering has attracted noticeable research\ninterests. Among the current solutions, the probabilistic techniques have shown\na powerful privacy preserving effect. When facing $k$ Nearest Neighbour attack,\nall the existing methods provide no data utility guarantee, for the\nintroduction of global randomness. In this paper, to overcome the problem of\nrecommendation accuracy loss, we propose a novel approach, Partitioned\nProbabilistic Neighbour Selection, to ensure a required prediction accuracy\nwhile maintaining high security against $k$NN attack. We define the sum of $k$\nneighbours' similarity as the accuracy metric alpha, the number of user\npartitions, across which we select the $k$ neighbours, as the security metric\nbeta. We generalise the $k$ Nearest Neighbour attack to beta k Nearest\nNeighbours attack. Differing from the existing approach that selects neighbours\nacross the entire candidate list randomly, our method selects neighbours from\neach exclusive partition of size $k$ with a decreasing probability. Theoretical\nand experimental analysis show that to provide an accuracy-assured\nrecommendation, our Partitioned Probabilistic Neighbour Selection method yields\na better trade-off between the recommendation accuracy and system security.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 01:04:29 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 08:40:55 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2015 01:20:10 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Lu", "Zhigang", ""], ["Shen", "Hong", ""]]}, {"id": "1505.07900", "submitter": "Zhigang Lu", "authors": "Zhigang Lu and Hong Shen", "title": "A Faster Algorithm to Build New Users Similarity List in\n  Neighbourhood-based Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neighbourhood-based Collaborative Filtering (CF) has been applied in the\nindustry for several decades, because of the easy implementation and high\nrecommendation accuracy. As the core of neighbourhood-based CF, the task of\ndynamically maintaining users' similarity list is challenged by cold-start\nproblem and scalability problem. Recently, several methods are presented on\nsolving the two problems. However, these methods applied an $O(n^2)$ algorithm\nto compute the similarity list in a special case, where the new users, with\nenough recommendation data, have the same rating list. To address the problem\nof large computational cost caused by the special case, we design a faster\n($O(\\frac{1}{125}n^2)$) algorithm, TwinSearch Algorithm, to avoid computing and\nsorting the similarity list for the new users repeatedly to save the\ncomputational resources. Both theoretical and experimental results show that\nthe TwinSearch Algorithm achieves better running time than the traditional\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 01:21:00 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Lu", "Zhigang", ""], ["Shen", "Hong", ""]]}, {"id": "1505.07909", "submitter": "Bin Gao", "authors": "Huazheng Wang, Fei Tian, Bin Gao, Jiang Bian, Tie-Yan Liu", "title": "Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered\n  Word Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligence Quotient (IQ) Test is a set of standardized questions designed\nto evaluate human intelligence. Verbal comprehension questions appear very\nfrequently in IQ tests, which measure human's verbal ability including the\nunderstanding of the words with multiple senses, the synonyms and antonyms, and\nthe analogies among words. In this work, we explore whether such tests can be\nsolved automatically by artificial intelligence technologies, especially the\ndeep learning technologies that are recently developed and successfully applied\nin a number of fields. However, we found that the task was quite challenging,\nand simply applying existing technologies (e.g., word embedding) could not\nachieve a good performance, mainly due to the multiple senses of words and the\ncomplex relations among words. To tackle these challenges, we propose a novel\nframework consisting of three components. First, we build a classifier to\nrecognize the specific type of a verbal question (e.g., analogy,\nclassification, synonym, or antonym). Second, we obtain distributed\nrepresentations of words and relations by leveraging a novel word embedding\nmethod that considers the multi-sense nature of words and the relational\nknowledge among words (or their senses) contained in dictionaries. Third, for\neach type of questions, we propose a specific solver based on the obtained\ndistributed word representations and relation representations. Experimental\nresults have shown that the proposed framework can not only outperform existing\nmethods for solving verbal comprehension questions but also exceed the average\nperformance of the Amazon Mechanical Turk workers involved in the study. The\nresults indicate that with appropriate uses of the deep learning technologies\nwe might be a further step closer to the human intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 02:46:44 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 13:29:41 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2015 08:42:22 GMT"}, {"version": "v4", "created": "Tue, 26 Apr 2016 11:37:32 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Wang", "Huazheng", ""], ["Tian", "Fei", ""], ["Gao", "Bin", ""], ["Bian", "Jiang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1505.08155", "submitter": "Qun Zhang", "authors": "Qun Zhang and Abdou Youssef", "title": "Performance Evaluation and Optimization of Math-Similarity Search", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Similarity search in math is to find mathematical expressions that are\nsimilar to a user's query. We conceptualized the similarity factors between\nmathematical expressions, and proposed an approach to math similarity search\n(MSS) by defining metrics based on those similarity factors [11]. Our\npreliminary implementation indicated the advantage of MSS compared to\nnon-similarity based search. In order to more effectively and efficiently\nsearch similar math expressions, MSS is further optimized. This paper focuses\non performance evaluation and optimization of MSS. Our results show that the\nproposed optimization process significantly improved the performance of MSS\nwith respect to both relevance ranking and recall.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 19:17:03 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Zhang", "Qun", ""], ["Youssef", "Abdou", ""]]}]