[{"id": "1801.00215", "submitter": "Simon Stiebellehner", "authors": "Simon Stiebellehner, Jun Wang, Shuai Yuan", "title": "Learning Continuous User Representations through Hybrid Filtering with\n  doc2vec", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Players in the online ad ecosystem are struggling to acquire the user data\nrequired for precise targeting. Audience look-alike modeling has the potential\nto alleviate this issue, but models' performance strongly depends on quantity\nand quality of available data. In order to maximize the predictive performance\nof our look-alike modeling algorithms, we propose two novel hybrid filtering\ntechniques that utilize the recent neural probabilistic language model\nalgorithm doc2vec. We apply these methods to data from a large mobile ad\nexchange and additional app metadata acquired from the Apple App store and\nGoogle Play store. First, we model mobile app users through their app usage\nhistories and app descriptions (user2vec). Second, we introduce context\nawareness to that model by incorporating additional user and app-related\nmetadata in model training (context2vec). Our findings are threefold: (1) the\nquality of recommendations provided by user2vec is notably higher than current\nstate-of-the-art techniques. (2) User representations generated through hybrid\nfiltering using doc2vec prove to be highly valuable features in supervised\nmachine learning models for look-alike modeling. This represents the first\napplication of hybrid filtering user models using neural probabilistic language\nmodels, specifically doc2vec, in look-alike modeling. (3) Incorporating context\nmetadata in the doc2vec model training process to introduce context awareness\nhas positive effects on performance and is superior to directly including the\ndata as features in the downstream supervised models.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 00:51:56 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Stiebellehner", "Simon", ""], ["Wang", "Jun", ""], ["Yuan", "Shuai", ""]]}, {"id": "1801.00377", "submitter": "Walid Shalaby", "authors": "Walid Shalaby, BahaaEddin AlAila, Mohammed Korayem, Layla Pournajaf,\n  Khalifeh AlJadda, Shannon Quinn, and Wlodek Zadrozny", "title": "Help Me Find a Job: A Graph-based Approach for Job Recommendation at\n  Scale", "comments": "Accepted at 2017 IEEE International Conference on Big Data (BIGDATA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online job boards are one of the central components of modern recruitment\nindustry. With millions of candidates browsing through job postings everyday,\nthe need for accurate, effective, meaningful, and transparent job\nrecommendations is apparent more than ever. While recommendation systems are\nsuccessfully advancing in variety of online domains by creating social and\ncommercial value, the job recommendation domain is less explored. Existing\nsystems are mostly focused on content analysis of resumes and job descriptions,\nrelying heavily on the accuracy and coverage of the semantic analysis and\nmodeling of the content in which case, they end up usually suffering from\nrigidity and the lack of implicit semantic relations that are uncovered from\nusers' behavior and could be captured by Collaborative Filtering (CF) methods.\nFew works which utilize CF do not address the scalability challenges of\nreal-world systems and the problem of cold-start. In this paper, we propose a\nscalable item-based recommendation system for online job recommendations. Our\napproach overcomes the major challenges of sparsity and scalability by\nleveraging a directed graph of jobs connected by multi-edges representing\nvarious behavioral and contextual similarity signals. The short lived nature of\nthe items (jobs) in the system and the rapid rate in which new users and jobs\nenter the system make the cold-start a serious problem hindering CF methods. We\naddress this problem by harnessing the power of deep learning in addition to\nuser behavior to serve hybrid recommendations. Our technique has been leveraged\nby CareerBuilder.com which is one of the largest job boards in the world to\ngenerate high-quality recommendations for millions of users.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 00:47:44 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Shalaby", "Walid", ""], ["AlAila", "BahaaEddin", ""], ["Korayem", "Mohammed", ""], ["Pournajaf", "Layla", ""], ["AlJadda", "Khalifeh", ""], ["Quinn", "Shannon", ""], ["Zadrozny", "Wlodek", ""]]}, {"id": "1801.00388", "submitter": "Walid Shalaby", "authors": "Walid Shalaby, Wlodek Zadrozny, and Hongxia Jin", "title": "Beyond Word Embeddings: Learning Entity and Concept Representations from\n  Large Scale Knowledge Bases", "comments": "arXiv admin note: text overlap with arXiv:1702.03342", "journal-ref": "Inf Retrieval J (2018)", "doi": "10.1007/s10791-018-9340-3", "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text representations using neural word embeddings have proven effective in\nmany NLP applications. Recent researches adapt the traditional word embedding\nmodels to learn vectors of multiword expressions (concepts/entities). However,\nthese methods are limited to textual knowledge bases (e.g., Wikipedia). In this\npaper, we propose a novel and simple technique for integrating the knowledge\nabout concepts from two large scale knowledge bases of different structure\n(Wikipedia and Probase) in order to learn concept representations. We adapt the\nefficient skip-gram model to seamlessly learn from the knowledge in Wikipedia\ntext and Probase concept graph. We evaluate our concept embedding models on two\ntasks: (1) analogical reasoning, where we achieve a state-of-the-art\nperformance of 91% on semantic analogies, (2) concept categorization, where we\nachieve a state-of-the-art performance on two benchmark datasets achieving\ncategorization accuracy of 100% on one and 98% on the other. Additionally, we\npresent a case study to evaluate our model on unsupervised argument type\nidentification for neural semantic parsing. We demonstrate the competitive\naccuracy of our unsupervised method and its ability to better generalize to out\nof vocabulary entity mentions compared to the tedious and error prone methods\nwhich depend on gazetteers and regular expressions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 03:43:30 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 21:54:56 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Shalaby", "Walid", ""], ["Zadrozny", "Wlodek", ""], ["Jin", "Hongxia", ""]]}, {"id": "1801.01316", "submitter": "Agnese Chiatti", "authors": "Agnese Chiatti, Mu Jung Cho, Anupriya Gagneja, Xiao Yang, Miriam\n  Brinberg, Katie Roehrick, Sagnik Ray Choudhury, Nilam Ram, Byron Reeves and\n  C. Lee Giles", "title": "Text Extraction and Retrieval from Smartphone Screenshots: Building a\n  Repository for Life in Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.DL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Daily engagement in life experiences is increasingly interwoven with mobile\ndevice use. Screen capture at the scale of seconds is being used in behavioral\nstudies and to implement \"just-in-time\" health interventions. The increasing\npsychological breadth of digital information will continue to make the actual\nscreens that people view a preferred if not required source of data about life\nexperiences. Effective and efficient Information Extraction and Retrieval from\ndigital screenshots is a crucial prerequisite to successful use of screen data.\nIn this paper, we present the experimental workflow we exploited to: (i)\npre-process a unique collection of screen captures, (ii) extract unstructured\ntext embedded in the images, (iii) organize image text and metadata based on a\nstructured schema, (iv) index the resulting document collection, and (v) allow\nfor Image Retrieval through a dedicated vertical search engine application. The\nadopted procedure integrates different open source libraries for traditional\nimage processing, Optical Character Recognition (OCR), and Image Retrieval. Our\naim is to assess whether and how state-of-the-art methodologies can be applied\nto this novel data set. We show how combining OpenCV-based pre-processing\nmodules with a Long short-term memory (LSTM) based release of Tesseract OCR,\nwithout ad hoc training, led to a 74% character-level accuracy of the extracted\ntext. Further, we used the processed repository as baseline for a dedicated\nImage Retrieval system, for the immediate use and application for behavioral\nand prevention scientists. We discuss issues of Text Information Extraction and\nRetrieval that are particular to the screenshot image case and suggest\nimportant future work.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 11:51:26 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Chiatti", "Agnese", ""], ["Cho", "Mu Jung", ""], ["Gagneja", "Anupriya", ""], ["Yang", "Xiao", ""], ["Brinberg", "Miriam", ""], ["Roehrick", "Katie", ""], ["Choudhury", "Sagnik Ray", ""], ["Ram", "Nilam", ""], ["Reeves", "Byron", ""], ["Giles", "C. Lee", ""]]}, {"id": "1801.01430", "submitter": "Anurag Ghosh", "authors": "Anurag Ghosh and C.V. Jawahar", "title": "SmartTennisTV: Automatic indexing of tennis videos", "comments": "10 pages, 4 figures, NCVPRIPG 2017 Accepted Paper (Best Paper Award\n  Winner)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate a score based indexing approach for tennis\nvideos. Given a broadcast tennis video (BTV), we index all the video segments\nwith their scores to create a navigable and searchable match. Our approach\ntemporally segments the rallies in the video and then recognizes the scores\nfrom each of the segments, before refining the scores using the knowledge of\nthe tennis scoring system. We finally build an interface to effortlessly\nretrieve and view the relevant video segments by also automatically tagging the\nsegmented rallies with human accessible tags such as 'fault' and 'deuce'. The\nefficiency of our approach is demonstrated on BTV's from two major tennis\ntournaments.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 16:38:55 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Ghosh", "Anurag", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1801.01443", "submitter": "Wellington Pinheiro dos Santos", "authors": "Filipe Rolim Cordeiro, Wellington Pinheiro dos Santos, Abel\n  Guilhermino da Silva Filho", "title": "A semi-supervised fuzzy GrowCut algorithm to segment and classify\n  regions of interest of mammographic images", "comments": null, "journal-ref": "Expert Systems With Applications, 65 (2016), 116-126", "doi": "10.1016/j.eswa.2016.08.016", "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the World Health Organization, breast cancer is the most common\nform of cancer in women. It is the second leading cause of death among women\nround the world, becoming the most fatal form of cancer. Mammographic image\nsegmentation is a fundamental task to support image analysis and diagnosis,\ntaking into account shape analysis of mammary lesions and their borders.\nHowever, mammogram segmentation is a very hard process, once it is highly\ndependent on the types of mammary tissues. In this work we present a new\nsemi-supervised segmentation algorithm based on the modification of the GrowCut\nalgorithm to perform automatic mammographic image segmentation once a region of\ninterest is selected by a specialist. In our proposal, we used fuzzy Gaussian\nmembership functions to modify the evolution rule of the original GrowCut\nalgorithm, in order to estimate the uncertainty of a pixel being object or\nbackground. The main impact of the proposed method is the significant reduction\nof expert effort in the initialization of seed points of GrowCut to perform\naccurate segmentation, once it removes the need of selection of background\nseeds. We also constructed an automatic point selection process based on the\nsimulated annealing optimization method, avoiding the need of human\nintervention. The proposed approach was qualitatively compared with other\nstate-of-the-art segmentation techniques, considering the shape of segmented\nregions. In order to validate our proposal, we built an image classifier using\na classical multilayer perceptron. We used Zernike moments to extract segmented\nimage features. This analysis employed 685 mammograms from IRMA breast cancer\ndatabase, using fat and fibroid tissues. Results show that the proposed\ntechnique could achieve a classification rate of 91.28\\% for fat tissues,\nevidencing the feasibility of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 17:31:01 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Cordeiro", "Filipe Rolim", ""], ["Santos", "Wellington Pinheiro dos", ""], ["Filho", "Abel Guilhermino da Silva", ""]]}, {"id": "1801.01624", "submitter": "Bilal Abu-Salih", "authors": "Pornpit Wongthontham and Bilal Abu-Salih", "title": "Ontology-based Approach for Identifying the Credibility Domain in Social\n  Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A challenge of managing and extracting useful knowledge from social media\ndata sources has attracted much attention from academic and industry. To\naddress this challenge, semantic analysis of textual data is focused in this\npaper. We propose an ontology-based approach to extract semantics of textual\ndata and define the domain of data. In other words, we semantically analyse the\nsocial data at two levels i.e. the entity level and the domain level. We have\nchosen Twitter as a social channel challenge for a purpose of concept proof.\nDomain knowledge is captured in ontologies which are then used to enrich the\nsemantics of tweets provided with specific semantic conceptual representation\nof entities that appear in the tweets. Case studies are used to demonstrate\nthis approach. We experiment and evaluate our proposed approach with a public\ndataset collected from Twitter and from the politics domain. The ontology-based\napproach leverages entity extraction and concept mappings in terms of quantity\nand accuracy of concept identification.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 04:23:20 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 08:10:46 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Wongthontham", "Pornpit", ""], ["Abu-Salih", "Bilal", ""]]}, {"id": "1801.01641", "submitter": "Liu Yang", "authors": "Liu Yang and Qingyao Ai and Jiafeng Guo and W. Bruce Croft", "title": "aNMM: Ranking Short Answer Texts with Attention-Based Neural Matching\n  Model", "comments": "Accepted as a full paper by CIKM'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an alternative to question answering methods based on feature engineering,\ndeep learning approaches such as convolutional neural networks (CNNs) and Long\nShort-Term Memory Models (LSTMs) have recently been proposed for semantic\nmatching of questions and answers. To achieve good results, however, these\nmodels have been combined with additional features such as word overlap or BM25\nscores. Without this combination, these models perform significantly worse than\nmethods based on linguistic feature engineering. In this paper, we propose an\nattention based neural matching model for ranking short answer text. We adopt\nvalue-shared weighting scheme instead of position-shared weighting scheme for\ncombining different matching signals and incorporate question term importance\nlearning using question attention network. Using the popular benchmark TREC QA\ndata, we show that the relatively simple aNMM model can significantly\noutperform other neural network models that have been used for the question\nanswering task, and is competitive with models that are combined with\nadditional features. When aNMM is combined with additional features, it\noutperforms all baselines.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 06:06:17 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 02:38:18 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Yang", "Liu", ""], ["Ai", "Qingyao", ""], ["Guo", "Jiafeng", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1801.01708", "submitter": "Olivier Gouvert", "authors": "Olivier Gouvert, Thomas Oberlin and C\\'edric F\\'evotte", "title": "Negative Binomial Matrix Factorization for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce negative binomial matrix factorization (NBMF), a matrix\nfactorization technique specially designed for analyzing over-dispersed count\ndata. It can be viewed as an extension of Poisson matrix factorization (PF)\nperturbed by a multiplicative term which models exposure. This term brings a\ndegree of freedom for controlling the dispersion, making NBMF more robust to\noutliers. We show that NBMF allows to skip traditional pre-processing stages,\nsuch as binarization, which lead to loss of information. Two estimation\napproaches are presented: maximum likelihood and variational Bayes inference.\nWe test our model with a recommendation task and show its ability to predict\nuser tastes with better precision than PF.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 11:00:46 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Gouvert", "Olivier", ""], ["Oberlin", "Thomas", ""], ["F\u00e9votte", "C\u00e9dric", ""]]}, {"id": "1801.01712", "submitter": "Subodh Deolekar", "authors": "Subodh Deolekar and Siby Abraham", "title": "Tree based classification of tabla strokes", "comments": "14 pages, 11 figures, current science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper attempts to validate the effectiveness of tree classifiers to\nclassify tabla strokes especially the ones which are overlapping in nature. It\nuses decision tree, ID3 and random forest as classifiers. A custom made data\nsets of 650 samples of 13 different tabla strokes were used for experimental\npurpose. 31 different features with their mean and variances were extracted for\nclassification. Three data sets consisting of 21361, 18802 and 19543 instances\nrespectively were used for the purpose. Validation has been done using measures\nlike ROC curve and accuracy. The experimental results showed that all the\nclassifiers showing excellent results with random forest outperforming the\nother two. The effectiveness of random forest in classifying strokes which are\noverlapping in nature is done by comparing the known results of that with\nmulti-layer perceptron.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 11:09:31 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Deolekar", "Subodh", ""], ["Abraham", "Siby", ""]]}, {"id": "1801.01884", "submitter": "Neil Smalheiser", "authors": "Neil R. Smalheiser, Gary Bonifield", "title": "Unsupervised Low-Dimensional Vector Representations for Words, Phrases\n  and Text that are Transparent, Scalable, and produce Similarity Metrics that\n  are Complementary to Neural Embeddings", "comments": "27 pages, 9 tables, and 6 supplemental files which can be accessed at\n  http://arrowsmith.psych.uic.edu/arrowsmith_uic/word_similarity_metrics.html.\n  Rewrote Introduction. This ms. has been submitted to J. Biomed. Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural embeddings are a popular set of methods for representing words,\nphrases or text as a low dimensional vector (typically 50-500 dimensions).\nHowever, it is difficult to interpret these dimensions in a meaningful manner,\nand creating neural embeddings requires extensive training and tuning of\nmultiple parameters and hyperparameters. We present here a simple unsupervised\nmethod for representing words, phrases or text as a low dimensional vector, in\nwhich the meaning and relative importance of dimensions is transparent to\ninspection. We have created a near-comprehensive vector representation of\nwords, and selected bigrams, trigrams and abbreviations, using the set of\ntitles and abstracts in PubMed as a corpus. This vector is used to create\nseveral novel implicit word-word and text-text similarity metrics. The implicit\nword-word similarity metrics correlate well with human judgement of word pair\nsimilarity and relatedness, and outperform or equal all other reported methods\non a variety of biomedical benchmarks, including several implementations of\nneural embeddings trained on PubMed corpora. Our implicit word-word metrics\ncapture different aspects of word-word relatedness than word2vec-based metrics\nand are only partially correlated (rho = ~0.5-0.8 depending on task and\ncorpus). The vector representations of words, bigrams, trigrams, abbreviations,\nand PubMed title+abstracts are all publicly available from\nhttp://arrowsmith.psych.uic.edu for release under CC-BY-NC license. Several\npublic web query interfaces are also available at the same site, including one\nwhich allows the user to specify a given word and view its most closely related\nterms according to direct co-occurrence as well as different implicit\nsimilarity metrics.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 19:00:04 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 18:22:15 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Smalheiser", "Neil R.", ""], ["Bonifield", "Gary", ""]]}, {"id": "1801.01937", "submitter": "Seyedamin Pouriyeh", "authors": "Seyedamin Pouriyeh, Mehdi Allahyari, Krys Kochut, Hamid Reza Arabnia", "title": "A Comprehensive Survey of Ontology Summarization: Measures and Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Semantic Web is becoming a large scale framework that enables data to be\npublished, shared, and reused in the form of ontologies. The ontology which is\nconsidered as basic building block of semantic web consists of two layers\nincluding data and schema layer. With the current exponential development of\nontologies in both data size and complexity of schemas, ontology understanding\nwhich is playing an important role in different tasks such as ontology\nengineering, ontology learning, etc., is becoming more difficult. Ontology\nsummarization as a way to distill knowledge from an ontology and generate an\nabridge version to facilitate a better understanding is getting more attention\nrecently. There are various approaches available for ontology summarization\nwhich are focusing on different measures in order to produce a proper summary\nfor a given ontology. In this paper, we mainly focus on the common metrics\nwhich are using for ontology summarization and meet the state-of-the-art in\nontology summarization.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 22:53:41 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Pouriyeh", "Seyedamin", ""], ["Allahyari", "Mehdi", ""], ["Kochut", "Krys", ""], ["Arabnia", "Hamid Reza", ""]]}, {"id": "1801.02178", "submitter": "Christophe Van Gysel", "authors": "Tom Kenter and Alexey Borisov and Christophe Van Gysel and Mostafa\n  Dehghani and Maarten de Rijke and Bhaskar Mitra", "title": "Neural Networks for Information Retrieval", "comments": "Overview of full-day tutorial at WSDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning plays a role in many aspects of modern IR systems, and deep\nlearning is applied in all of them. The fast pace of modern-day research has\ngiven rise to many approaches to many IR problems. The amount of information\navailable can be overwhelming both for junior students and for experienced\nresearchers looking for new research topics and directions. The aim of this\nfull-day tutorial is to give a clear overview of current tried-and-trusted\nneural methods in IR and how they benefit IR.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 12:20:56 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Kenter", "Tom", ""], ["Borisov", "Alexey", ""], ["Van Gysel", "Christophe", ""], ["Dehghani", "Mostafa", ""], ["de Rijke", "Maarten", ""], ["Mitra", "Bhaskar", ""]]}, {"id": "1801.02200", "submitter": "Amanda Duarte", "authors": "Didac Sur\\'is, Amanda Duarte, Amaia Salvador, Jordi Torres and Xavier\n  Gir\\'o-i-Nieto", "title": "Cross-modal Embeddings for Video and Audio Retrieval", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing amount of online videos brings several opportunities for\ntraining self-supervised neural networks. The creation of large scale datasets\nof videos such as the YouTube-8M allows us to deal with this large amount of\ndata in manageable way. In this work, we find new ways of exploiting this\ndataset by taking advantage of the multi-modal information it provides. By\nmeans of a neural network, we are able to create links between audio and visual\ndocuments, by projecting them into a common region of the feature space,\nobtaining joint audio-visual embeddings. These links are used to retrieve audio\nsamples that fit well to a given silent video, and also to retrieve images that\nmatch a given a query audio. The results in terms of Recall@K obtained over a\nsubset of YouTube-8M videos show the potential of this unsupervised approach\nfor cross-modal feature learning. We train embeddings for both scales and\nassess their quality in a retrieval problem, formulated as using the feature\nextracted from one modality to retrieve the most similar videos based on the\nfeatures computed in the other modality.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 15:43:22 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Sur\u00eds", "Didac", ""], ["Duarte", "Amanda", ""], ["Salvador", "Amaia", ""], ["Torres", "Jordi", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""]]}, {"id": "1801.02203", "submitter": "Richa Verma", "authors": "Prerna Agarwal, Richa Verma, Angshul Majumdar", "title": "Indian Regional Movie Dataset for Recommender Systems", "comments": "7 pages, 8 figures, open-source Indian movie rating dataset, metadata\n  of movies and users", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indian regional movie dataset is the first database of regional Indian\nmovies, users and their ratings. It consists of movies belonging to 18\ndifferent Indian regional languages and metadata of users with varying\ndemographics. Through this dataset, the diversity of Indian regional cinema and\nits huge viewership is captured. We analyze the dataset that contains roughly\n10K ratings of 919 users and 2,851 movies using some supervised and\nunsupervised collaborative filtering techniques like Probabilistic Matrix\nFactorization, Matrix Completion, Blind Compressed Sensing etc. The dataset\nconsists of metadata information of users like age, occupation, home state and\nknown languages. It also consists of metadata of movies like genre, language,\nrelease year and cast. India has a wide base of viewers which is evident by the\nlarge number of movies released every year and the huge box-office revenue.\nThis dataset can be used for designing recommendation systems for Indian users\nand regional movies, which do not, yet, exist. The dataset can be downloaded\nfrom \\href{https://goo.gl/EmTPv6}{https://goo.gl/EmTPv6}.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 16:02:35 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Agarwal", "Prerna", ""], ["Verma", "Richa", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1801.02294", "submitter": "Han Zhu", "authors": "Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, Kun Gai", "title": "Learning Tree-based Deep Model for Recommender Systems", "comments": "Accepted by KDD 2018", "journal-ref": null, "doi": "10.1145/3219819.3219826", "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based methods for recommender systems have been studied extensively in\nrecent years. In systems with large corpus, however, the calculation cost for\nthe learnt model to predict all user-item preferences is tremendous, which\nmakes full corpus retrieval extremely difficult. To overcome the calculation\nbarriers, models such as matrix factorization resort to inner product form\n(i.e., model user-item preference as the inner product of user, item latent\nfactors) and indexes to facilitate efficient approximate k-nearest neighbor\nsearches. However, it still remains challenging to incorporate more expressive\ninteraction forms between user and item features, e.g., interactions through\ndeep neural networks, because of the calculation cost.\n  In this paper, we focus on the problem of introducing arbitrary advanced\nmodels to recommender systems with large corpus. We propose a novel tree-based\nmethod which can provide logarithmic complexity w.r.t. corpus size even with\nmore expressive models such as deep neural networks. Our main idea is to\npredict user interests from coarse to fine by traversing tree nodes in a\ntop-down fashion and making decisions for each user-node pair. We also show\nthat the tree structure can be jointly learnt towards better compatibility with\nusers' interest distribution and hence facilitate both training and prediction.\nExperimental evaluations with two large-scale real-world datasets show that the\nproposed method significantly outperforms traditional methods. Online A/B test\nresults in Taobao display advertising platform also demonstrate the\neffectiveness of the proposed method in production environments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 02:52:20 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 11:13:21 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 07:40:07 GMT"}, {"version": "v4", "created": "Thu, 1 Nov 2018 04:37:55 GMT"}, {"version": "v5", "created": "Fri, 21 Dec 2018 03:15:54 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Zhu", "Han", ""], ["Li", "Xiang", ""], ["Zhang", "Pengye", ""], ["Li", "Guozheng", ""], ["He", "Jie", ""], ["Li", "Han", ""], ["Gai", "Kun", ""]]}, {"id": "1801.02411", "submitter": "Huan Zhao Dr.", "authors": "Huan Zhao, Quanming Yao, Yangqiu Song, James Kwok, Dik Lun Lee", "title": "Side Information Fusion for Recommender Systems over Heterogeneous\n  Information Network", "comments": "33 pages, 16 figures, Accepted in TKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) has been one of the most important and popular\nrecommendation methods, which aims at predicting users' preferences (ratings)\nbased on their past behaviors. Recently, various types of side information\nbeyond the explicit ratings users give to items, such as social connections\namong users and metadata of items, have been introduced into CF and shown to be\nuseful for improving recommendation performance. However, previous works\nprocess different types of information separately, thus failing to capture the\ncorrelations that might exist across them. To address this problem, in this\nwork, we study the application of heterogeneous information network (HIN) to\nenhance CF-based recommendation methods. However, we face challenging issues in\nHIN-based recommendation, i.e., how to capture similarities of complex\nsemantics between users and items in a HIN, and how to effectively fuse these\nsimilarities to improve final recommendation performance. To address these\nissues, we apply metagraph to similarity computation and solve the information\nfusion problem with a ``matrix factorization (MF) + factorization machine\n(FM)'' framework. For the MF part, we obtain the user-item similarity matrix\nfrom each metagraph and then apply low-rank matrix approximation to obtain\nlatent features for both users and items. For the FM part, we apply FM with\nGroup lasso (FMG) on the features obtained from the MF part to train the\nrecommending model and, at the same time, identify the useful metagraphs.\nBesides FMG, a two-stage method, we further propose an end-to-end method,\nhierarchical attention fusing (HAF), to fuse metagraph based similarities for\nthe final recommendation. Experimental results on four large real-world\ndatasets show that the two proposed frameworks significantly outperform\nexisting state-of-the-art methods in terms of recommendation performance.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 13:14:56 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 15:38:26 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 03:27:39 GMT"}, {"version": "v4", "created": "Mon, 28 Dec 2020 17:18:56 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhao", "Huan", ""], ["Yao", "Quanming", ""], ["Song", "Yangqiu", ""], ["Kwok", "James", ""], ["Lee", "Dik Lun", ""]]}, {"id": "1801.02607", "submitter": "Thijs Vogels", "authors": "Thijs Vogels, Octavian-Eugen Ganea, Carsten Eickhoff", "title": "Web2Text: Deep Structured Boilerplate Removal", "comments": "To appear in ECIR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web pages are a valuable source of information for many natural language\nprocessing and information retrieval tasks. Extracting the main content from\nthose documents is essential for the performance of derived applications. To\naddress this issue, we introduce a novel model that performs sequence labeling\nto collectively classify all text blocks in an HTML page as either boilerplate\nor main content. Our method uses a hidden Markov model on top of potentials\nderived from DOM tree features using convolutional neural networks. The\nproposed method sets a new state-of-the-art performance for boilerplate removal\non the CleanEval benchmark. As a component of information retrieval pipelines,\nit improves retrieval performance on the ClueWeb12 collection.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 18:40:16 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 21:16:15 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 07:12:32 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Vogels", "Thijs", ""], ["Ganea", "Octavian-Eugen", ""], ["Eickhoff", "Carsten", ""]]}, {"id": "1801.02687", "submitter": "Sheikh Muhammad Sarwar", "authors": "Sheikh Muhammad Sarwar, John Foley, James Allan", "title": "Term Relevance Feedback for Contextual Named Entity Retrieval", "comments": null, "journal-ref": null, "doi": "10.1145/3176349.3176886", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the role of a user in Contextual Named Entity Retrieval (CNER),\nshowing (1) that user identification of important context-bearing terms is\nsuperior to automated approaches, and (2) that further gains are possible if\nthe user indicates the relative importance of those terms. CNER is similar in\nspirit to List Question answering and Entity disambiguation. However, the main\nfocus of CNER is to obtain user feedback for constructing a profile for a class\nof entities on the fly and use that to retrieve entities from free text. Given\na sentence, and an entity selected from that sentence, CNER aims to retrieve\nsentences that have entities similar to query entity. This paper explores\nobtaining term relevance feedback and importance weighting from humans in order\nto improve a CNER system. We report our findings based on the efforts of IR\nresearchers as well as crowdsourced workers.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 21:06:49 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Sarwar", "Sheikh Muhammad", ""], ["Foley", "John", ""], ["Allan", "James", ""]]}, {"id": "1801.02808", "submitter": "Bing Liu", "authors": "Zhiyuan Chen, Nianzu Ma, Bing Liu", "title": "Lifelong Learning for Sentiment Classification", "comments": null, "journal-ref": "ACL 2015", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel lifelong learning (LL) approach to sentiment\nclassification. LL mimics the human continuous learning process, i.e.,\nretaining the knowledge learned from past tasks and use it to help future\nlearning. In this paper, we first discuss LL in general and then LL for\nsentiment classification in particular. The proposed LL approach adopts a\nBayesian optimization framework based on stochastic gradient descent. Our\nexperimental results show that the proposed method outperforms baseline methods\nsignificantly, which demonstrates that lifelong learning is a promising\nresearch direction.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 06:11:50 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Chen", "Zhiyuan", ""], ["Ma", "Nianzu", ""], ["Liu", "Bing", ""]]}, {"id": "1801.02832", "submitter": "Carsten Eickhoff", "authors": "Ferenc Galk\\'o and Carsten Eickhoff", "title": "Biomedical Question Answering via Weighted Neural Network Passage\n  Retrieval", "comments": "To appear in ECIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of publicly available biomedical literature has been growing\nrapidly in recent years, yet question answering systems still struggle to\nexploit the full potential of this source of data. In a preliminary processing\nstep, many question answering systems rely on retrieval models for identifying\nrelevant documents and passages. This paper proposes a weighted cosine distance\nretrieval scheme based on neural network word embeddings. Our experiments are\nbased on publicly available data and tasks from the BioASQ biomedical question\nanswering challenge and demonstrate significant performance gains over a wide\nrange of state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 08:23:46 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Galk\u00f3", "Ferenc", ""], ["Eickhoff", "Carsten", ""]]}, {"id": "1801.03032", "submitter": "Ritvik Shrivastava", "authors": "Kuntal Dey, Ritvik Shrivastava, Saroj Kaushik", "title": "Topical Stance Detection for Twitter: A Two-Phase LSTM Model Using\n  Attention", "comments": "Accepted at the 40th European Conference on Information Retrieval\n  (ECIR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topical stance detection problem addresses detecting the stance of the\ntext content with respect to a given topic: whether the sentiment of the given\ntext content is in FAVOR of (positive), is AGAINST (negative), or is NONE\n(neutral) towards the given topic. Using the concept of attention, we develop a\ntwo-phase solution. In the first phase, we classify subjectivity - whether a\ngiven tweet is neutral or subjective with respect to the given topic. In the\nsecond phase, we classify sentiment of the subjective tweets (ignoring the\nneutral tweets) - whether a given subjective tweet has a FAVOR or AGAINST\nstance towards the topic. We propose a Long Short-Term memory (LSTM) based deep\nneural network for each phase, and embed attention at each of the phases. On\nthe SemEval 2016 stance detection Twitter task dataset, we obtain a best-case\nmacro F-score of 68.84% and a best-case accuracy of 60.2%, outperforming the\nexisting deep learning based solutions. Our framework, T-PAN, is the first in\nthe topical stance detection literature, that uses deep learning within a\ntwo-phase architecture.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 17:00:24 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Dey", "Kuntal", ""], ["Shrivastava", "Ritvik", ""], ["Kaushik", "Saroj", ""]]}, {"id": "1801.03039", "submitter": "Patryk Orzechowski", "authors": "Patryk Orzechowski, Moshe Sipper, Xiuzhen Huang, and Jason H. Moore", "title": "EBIC: an evolutionary-based parallel biclustering algorithm for pattern\n  discover", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": "10.1093/bioinformatics/bty401", "report-no": null, "categories": "cs.LG cs.CV cs.IR q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a novel biclustering algorithm based on artificial intelligence\n(AI) is introduced. The method called EBIC aims to detect biologically\nmeaningful, order-preserving patterns in complex data. The proposed algorithm\nis probably the first one capable of discovering with accuracy exceeding 50%\nmultiple complex patterns in real gene expression datasets. It is also one of\nthe very few biclustering methods designed for parallel environments with\nmultiple graphics processing units (GPUs). We demonstrate that EBIC outperforms\nstate-of-the-art biclustering methods, in terms of recovery and relevance, on\nboth synthetic and genetic datasets. EBIC also yields results over 12 times\nfaster than the most accurate reference algorithms. The proposed algorithm is\nanticipated to be added to the repertoire of unsupervised machine learning\nalgorithms for the analysis of datasets, including those from large-scale\ngenomic studies.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 17:13:07 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 14:08:11 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Orzechowski", "Patryk", ""], ["Sipper", "Moshe", ""], ["Huang", "Xiuzhen", ""], ["Moore", "Jason H.", ""]]}, {"id": "1801.03143", "submitter": "Artit Wangperawong", "authors": "Artit Wangperawong, Kettip Kriangchaivech, Austin Lanari, Supui Lam,\n  Panthong Wangperawong", "title": "Comparing heterogeneous entities using artificial neural networks of\n  trainable weighted structural components and machine-learned activation\n  functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To compare entities of differing types and structural components, the\nartificial neural network paradigm was used to cross-compare structural\ncomponents between heterogeneous documents. Trainable weighted structural\ncomponents were input into machine-learned activation functions of the neurons.\nThe model was used for matching news articles and videos, where the inputs and\nactivation functions respectively consisted of term vectors and cosine\nsimilarity measures between the weighted structural components. The model was\ntested with different weights, achieving as high as 59.2% accuracy for matching\nvideos to news articles. A mobile application user interface for recommending\nrelated videos for news articles was developed to demonstrate consumer value,\nincluding its potential usefulness for cross-selling products from unrelated\ncategories.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 21:20:08 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Wangperawong", "Artit", ""], ["Kriangchaivech", "Kettip", ""], ["Lanari", "Austin", ""], ["Lam", "Supui", ""], ["Wangperawong", "Panthong", ""]]}, {"id": "1801.03406", "submitter": "Tanya Piplani", "authors": "Tanya Piplani, David Bamman", "title": "DeepSeek: Content Based Image Search & Retrieval", "comments": "arXiv admin note: text overlap with arXiv:1706.06064 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most of the internet today is composed of digital media that includes videos\nand images. With pixels becoming the currency in which most transactions happen\non the internet, it is becoming increasingly important to have a way of\nbrowsing through this ocean of information with relative ease. YouTube has 400\nhours of video uploaded every minute and many million images are browsed on\nInstagram, Facebook, etc. Inspired by recent advances in the field of deep\nlearning and success that it has gained on various problems like image\ncaptioning and, machine translation , word2vec , skip thoughts, etc, we present\nDeepSeek a natural language processing based deep learning model that allows\nusers to enter a description of the kind of images that they want to search,\nand in response the system retrieves all the images that semantically and\ncontextually relate to the query. Two approaches are described in the following\nsections.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 09:57:40 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 06:28:44 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Piplani", "Tanya", ""], ["Bamman", "David", ""]]}, {"id": "1801.03627", "submitter": "Bilal Abu-Salih", "authors": "Bilal Abu-Salih", "title": "Applying Vector Space Model (VSM) Techniques in Information Retrieval\n  for Arabic Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Information Retrieval (IR) allows the storage, management, processing and\nretrieval of information, documents, websites, etc. Building an IR system for\nany language is imperative. This is evident through the massive conducted\nefforts to build IR systems using any of its models that are valid for certain\nlanguages. This report presents an implementation for a core IR technique which\nis Vector Space Model (VSM). We have chosen VSM model for our project since it\nis a term weighting scheme, and the retrieved documents could be sorted\naccording to their relevancy degree. One other significant feature for such\ntechnique is the ability to get a relevance feedback from the users of the\nsystem; users can judge whether the retrieved document is relative to their\nneed or not. The developed system has been validated through building an Arabic\nIR website using server side scripting. The experiments verifies the\neffectiveness of our system to apply all techniques of vector space model and\nvalid over Arabic language.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 03:59:09 GMT"}, {"version": "v2", "created": "Mon, 15 Jan 2018 03:44:21 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Abu-Salih", "Bilal", ""]]}, {"id": "1801.03815", "submitter": "Tak-Shing Chan", "authors": "Tak-Shing T. Chan and Yi-Hsuan Yang", "title": "Informed Group-Sparse Representation for Singing Voice Separation", "comments": "5 pages, 1 figure", "journal-ref": "IEEE Signal Process. Lett., vol. 24, no. 2, pp. 156-160, Feb. 2017", "doi": "10.1109/LSP.2017.2647810", "report-no": null, "categories": "eess.AS cs.IR cs.SD eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Singing voice separation attempts to separate the vocal and instrumental\nparts of a music recording, which is a fundamental problem in music information\nretrieval. Recent work on singing voice separation has shown that the low-rank\nrepresentation and informed separation approaches are both able to improve\nseparation quality. However, low-rank optimizations are computationally\ninefficient due to the use of singular value decompositions. Therefore, in this\npaper, we propose a new linear-time algorithm called informed group-sparse\nrepresentation, and use it to separate the vocals from music using pitch\nannotations as side information. Experimental results on the iKala dataset\nconfirm the efficacy of our approach, suggesting that the music accompaniment\nfollows a group-sparse structure given a pre-trained instrumental dictionary.\nWe also show how our work can be easily extended to accommodate multiple\ndictionaries using the DSD100 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 00:45:30 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Chan", "Tak-Shing T.", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "1801.03844", "submitter": "Jibril Frej", "authors": "Jibril Frej (LIG), Jean-Pierre Chevallet (LGI - IMAG), Didier Schwab\n  (UGA)", "title": "Enhancing Translation Language Models with Word Embedding for\n  Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the usage of Word Embedding semantic resources for\nInformation Retrieval (IR) task. This embedding, produced by a shallow neural\nnetwork, have been shown to catch semantic similarities between words (Mikolov\net al., 2013). Hence, our goal is to enhance IR Language Models by addressing\nthe term mismatch problem. To do so, we applied the model presented in the\npaper Integrating and Evaluating Neural Word Embedding in Information Retrieval\nby Zuccon et al. (2015) that proposes to estimate the translation probability\nof a Translation Language Model using the cosine similarity between Word\nEmbedding. The results we obtained so far did not show a statistically\nsignificant improvement compared to classical Language Model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 16:08:27 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Frej", "Jibril", "", "LIG"], ["Chevallet", "Jean-Pierre", "", "LGI - IMAG"], ["Schwab", "Didier", "", "UGA"]]}, {"id": "1801.03911", "submitter": "Sahil Garg", "authors": "Sahil Garg and Greg Ver Steeg and Aram Galstyan", "title": "Stochastic Learning of Nonstationary Kernels for Natural Language\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing often involves computations with semantic or\nsyntactic graphs to facilitate sophisticated reasoning based on structural\nrelationships. While convolution kernels provide a powerful tool for comparing\ngraph structure based on node (word) level relationships, they are difficult to\ncustomize and can be computationally expensive. We propose a generalization of\nconvolution kernels, with a nonstationary model, for better expressibility of\nnatural languages in supervised settings. For a scalable learning of the\nparameters introduced with our model, we propose a novel algorithm that\nleverages stochastic sampling on k-nearest neighbor graphs, along with\napproximations based on locality-sensitive hashing. We demonstrate the\nadvantages of our approach on a challenging real-world (structured inference)\nproblem of automatically extracting biological models from the text of\nscientific papers.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 18:24:02 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 21:41:27 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Garg", "Sahil", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1801.04354", "submitter": "Yanjun  Qi Dr.", "authors": "Ji Gao, Jack Lanchantin, Mary Lou Soffa, Yanjun Qi", "title": "Black-box Generation of Adversarial Text Sequences to Evade Deep\n  Learning Classifiers", "comments": "This is an extended version of the 6page Workshop version appearing\n  in 1st Deep Learning and Security Workshop colocated with IEEE S&P", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although various techniques have been proposed to generate adversarial\nsamples for white-box attacks on text, little attention has been paid to\nblack-box attacks, which are more realistic scenarios. In this paper, we\npresent a novel algorithm, DeepWordBug, to effectively generate small text\nperturbations in a black-box setting that forces a deep-learning classifier to\nmisclassify a text input. We employ novel scoring strategies to identify the\ncritical tokens that, if modified, cause the classifier to make an incorrect\nprediction. Simple character-level transformations are applied to the\nhighest-ranked tokens in order to minimize the edit distance of the\nperturbation, yet change the original classification. We evaluated DeepWordBug\non eight real-world text datasets, including text classification, sentiment\nanalysis, and spam detection. We compare the result of DeepWordBug with two\nbaselines: Random (Black-box) and Gradient (White-box). Our experimental\nresults indicate that DeepWordBug reduces the prediction accuracy of current\nstate-of-the-art deep-learning models, including a decrease of 68\\% on average\nfor a Word-LSTM model and 48\\% on average for a Char-CNN model.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 00:42:30 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 02:59:45 GMT"}, {"version": "v3", "created": "Sat, 7 Apr 2018 03:32:12 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 12:59:01 GMT"}, {"version": "v5", "created": "Wed, 23 May 2018 15:55:55 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Gao", "Ji", ""], ["Lanchantin", "Jack", ""], ["Soffa", "Mary Lou", ""], ["Qi", "Yanjun", ""]]}, {"id": "1801.04554", "submitter": "Charles Ferreira", "authors": "Charles Henrique Porto Ferreira, Debora Maria Rossi de Medeiros,\n  Fabricio Olivetti de Fran\\c{c}a", "title": "DCDistance: A Supervised Text Document Feature extraction based on class\n  labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text Mining is a field that aims at extracting information from textual data.\nOne of the challenges of such field of study comes from the pre-processing\nstage in which a vector (and structured) representation should be extracted\nfrom unstructured data. The common extraction creates large and sparse vectors\nrepresenting the importance of each term to a document. As such, this usually\nleads to the curse-of-dimensionality that plagues most machine learning\nalgorithms. To cope with this issue, in this paper we propose a new supervised\nfeature extraction and reduction algorithm, named DCDistance, that creates\nfeatures based on the distance between a document to a representative of each\nclass label. As such, the proposed technique can reduce the features set in\nmore than 99% of the original set. Additionally, this algorithm was also\ncapable of improving the classification accuracy over a set of benchmark\ndatasets when compared to traditional and state-of-the-art features selection\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 13:28:19 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Ferreira", "Charles Henrique Porto", ""], ["de Medeiros", "Debora Maria Rossi", ""], ["de Fran\u00e7a", "Fabricio Olivetti", ""]]}, {"id": "1801.05088", "submitter": "Chandra Khatri", "authors": "Chandra Khatri", "title": "Real-time Road Traffic Information Detection Through Social Media", "comments": "138 Pages, 21 Figures, 15 Tables. Masters Thesis in Computational\n  Science & Engineering Group @ Georgia Tech.\n  https://smartech.gatech.edu/bitstream/handle/1853/53889/KHATRI-THESIS-2015.pdf.\n  arXiv admin note: text overlap with arXiv:1703.03921 by other author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current study, a mechanism to extract traffic related information such as\ncongestion and incidents from textual data from the internet is proposed. The\ncurrent source of data is Twitter. As the data being considered is extremely\nlarge in size automated models are developed to stream, download, and mine the\ndata in real-time. Furthermore, if any tweet has traffic related information\nthen the models should be able to infer and extract this data.\n  Currently, the data is collected only for United States and a total of\n120,000 geo-tagged traffic related tweets are extracted, while six million\ngeo-tagged non-traffic related tweets are retrieved and classification models\nare trained. Furthermore, this data is used for various kinds of spatial and\ntemporal analysis. A mechanism to calculate level of traffic congestion,\nsafety, and traffic perception for cities in U.S. is proposed. Traffic\ncongestion and safety rankings for the various urban areas are obtained and\nthen they are statistically validated with existing widely adopted rankings.\nTraffic perception depicts the attitude and perception of people towards the\ntraffic.\n  It is also seen that traffic related data when visualized spatially and\ntemporally provides the same pattern as the actual traffic flows for various\nurban areas. When visualized at the city level, it is clearly visible that the\nflow of tweets is similar to flow of vehicles and that the traffic related\ntweets are representative of traffic within the cities. With all the findings\nin current study, it is shown that significant amount of traffic related\ninformation can be extracted from Twitter and other sources on internet.\nFurthermore, Twitter and these data sources are freely available and are not\nbound by spatial and temporal limitations. That is, wherever there is a user\nthere is a potential for data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 01:26:33 GMT"}], "update_date": "2018-01-20", "authors_parsed": [["Khatri", "Chandra", ""]]}, {"id": "1801.05532", "submitter": "Sungwoon Choi", "authors": "Sungwoon Choi, Heonseok Ha, Uiwon Hwang, Chanju Kim, Jung-Woo Ha,\n  Sungroh Yoon", "title": "Reinforcement Learning based Recommender System using Biclustering\n  Technique", "comments": "4 pages, 2 figures, IFUP2018(WSDM 2018 workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recommender system aims to recommend items that a user is interested in\namong many items. The need for the recommender system has been expanded by the\ninformation explosion. Various approaches have been suggested for providing\nmeaningful recommendations to users. One of the proposed approaches is to\nconsider a recommender system as a Markov decision process (MDP) problem and\ntry to solve it using reinforcement learning (RL). However, existing RL-based\nmethods have an obvious drawback. To solve an MDP in a recommender system, they\nencountered a problem with the large number of discrete actions that bring RL\nto a larger class of problems. In this paper, we propose a novel RL-based\nrecommender system. We formulate a recommender system as a gridworld game by\nusing a biclustering technique that can reduce the state and action space\nsignificantly. Using biclustering not only reduces space but also improves the\nrecommendation quality effectively handling the cold-start problem. In\naddition, our approach can provide users with some explanation why the system\nrecommends certain items. Lastly, we examine the proposed algorithm on a\nreal-world dataset and achieve a better performance than the widely used\nrecommendation algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 03:03:58 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Choi", "Sungwoon", ""], ["Ha", "Heonseok", ""], ["Hwang", "Uiwon", ""], ["Kim", "Chanju", ""], ["Ha", "Jung-Woo", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1801.05605", "submitter": "Md Mustafizur Rahman", "authors": "Md Mustafizur Rahman, Mucahid Kutlu, Tamer Elsayed, Matthew Lease", "title": "Efficient Test Collection Construction via Active Learning", "comments": "Accepted as a full paper in ICTIR 2020.\n  https://ictir2020.org/accepted-papers/", "journal-ref": null, "doi": "10.1145/3409256.3409837", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To create a new IR test collection at low cost, it is valuable to carefully\nselect which documents merit human relevance judgments. Shared task campaigns\nsuch as NIST TREC pool document rankings from many participating systems (and\noften interactive runs as well) in order to identify the most likely relevant\ndocuments for human judging. However, if one's primary goal is merely to build\na test collection, it would be useful to be able to do so without needing to\nrun an entire shared task. Toward this end, we investigate multiple active\nlearning strategies which, without reliance on system rankings: 1) select which\ndocuments human assessors should judge; and 2) automatically classify the\nrelevance of additional unjudged documents. To assess our approach, we report\nexperiments on five TREC collections with varying scarcity of relevant\ndocuments. We report labeling accuracy achieved, as well as rank correlation\nwhen evaluating participant systems based upon these labels vs.\\ full pool\njudgments. Results show the effectiveness of our approach, and we further\nanalyze how varying relevance scarcity across collections impacts our findings.\nTo support reproducibility and follow-on work, we have shared our code online:\nhttps://github.com/mdmustafizurrahman/ICTIR_AL_TestCollection_2020/.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 09:45:50 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 02:13:02 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 01:03:02 GMT"}, {"version": "v4", "created": "Tue, 4 Aug 2020 20:48:39 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Rahman", "Md Mustafizur", ""], ["Kutlu", "Mucahid", ""], ["Elsayed", "Tamer", ""], ["Lease", "Matthew", ""]]}, {"id": "1801.05734", "submitter": "Xiao-Long Ren", "authors": "Leilei Wu, Zhuoming Ren, Xiao-Long Ren, Jianlin Zhang, Linyuan L\\\"u", "title": "Eliminating the effect of rating bias on reputation systems", "comments": "13 pages, 5 figures. All the authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CY cs.IR cs.SI econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing rapid development of the e-commercial and interest-base websites\nmake it more pressing to evaluate objects' accurate quality before\nrecommendation by employing an effective reputation system. The objects'\nquality are often calculated based on their historical information, such as\nselected records or rating scores, to help visitors to make decisions before\nwatching, reading or buying. Usually high quality products obtain a higher\naverage ratings than low quality products regardless of rating biases or\nerrors. However many empirical cases demonstrate that consumers may be misled\nby rating scores added by unreliable users or deliberate tampering. In this\ncase, users' reputation, i.e., the ability to rating trustily and precisely,\nmake a big difference during the evaluating process. Thus, one of the main\nchallenges in designing reputation systems is eliminating the effects of users'\nrating bias on the evaluation results. To give an objective evaluation of each\nuser's reputation and uncover an object's intrinsic quality, we propose an\niterative balance (IB) method to correct users' rating biases. Experiments on\ntwo online video-provided Web sites, namely MovieLens and Netflix datasets,\nshow that the IB method is a highly self-consistent and robust algorithm and it\ncan accurately quantify movies' actual quality and users' stability of rating.\nCompared with existing methods, the IB method has higher ability to find the\n\"dark horses\", i.e., not so popular yet good movies, in the Academy Awards.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 16:24:03 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Wu", "Leilei", ""], ["Ren", "Zhuoming", ""], ["Ren", "Xiao-Long", ""], ["Zhang", "Jianlin", ""], ["L\u00fc", "Linyuan", ""]]}, {"id": "1801.05881", "submitter": "Mayank Kejriwal", "authors": "Mayank Kejriwal, Yao Gu", "title": "A Pipeline for Post-Crisis Twitter Data Acquisition", "comments": "6 pages, 4 figures, Workshop on Social Web in Emergency and Disaster\n  Management 2018 at the ACM WSDM Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to instant availability of data on social media platforms like Twitter,\nand advances in machine learning and data management technology, real-time\ncrisis informatics has emerged as a prolific research area in the last decade.\nAlthough several benchmarks are now available, especially on portals like\nCrisisLex, an important, practical problem that has not been addressed thus far\nis the rapid acquisition and benchmarking of data from free, publicly available\nstreams like the Twitter API. In this paper, we present ongoing work on a\npipeline for facilitating immediate post-crisis data collection, curation and\nrelevance filtering from the Twitter API. The pipeline is minimally supervised,\nalleviating the need for feature engineering by including a judicious mix of\ndata preprocessing and fast text embeddings, along with an active learning\nframework. We illustrate the utility of the pipeline by describing a recent\ncase study wherein it was used to collect and analyze millions of tweets in the\nimmediate aftermath of the Las Vegas shootings.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 22:38:52 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Kejriwal", "Mayank", ""], ["Gu", "Yao", ""]]}, {"id": "1801.05906", "submitter": "Mayank Kejriwal", "authors": "Yao Gu, Mayank Kejriwal", "title": "Unsupervised Hashtag Retrieval and Visualization for Crisis Informatics", "comments": "2 pages, 3 figures, Workshop on Social Web in Emergency and Disaster\n  Management at ACM WSDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In social media like Twitter, hashtags carry a lot of semantic information\nand can be easily distinguished from the main text. Exploring and visualizing\nthe space of hashtags in a meaningful way can offer important insights into a\ndataset, especially in crisis situations. In this demonstration paper, we\npresent a functioning prototype, HashViz, that ingests a corpus of tweets\ncollected in the aftermath of a crisis situation (such as the Las Vegas\nshootings) and uses the fastText bag-of-tricks semantic embedding algorithm\n(from Facebook Research) to embed words and hashtags into a vector space.\nHashtag vectors obtained in this way can be visualized using the t-SNE\ndimensionality reduction algorithm in 2D. Although multiple Twitter\nvisualization platforms exist, HashViz is distinguished by being simple,\nscalable, interactive and portable enough to be deployed on a server for\nmillion-tweet corpora collected in the aftermath of arbitrary disasters,\nwithout special-purpose installation, technical expertise, manual supervision\nor costly software or infrastructure investment. Although simple, we show that\nHashViz offers an intuitive way to summarize, and gain insight into, a\ndeveloping crisis situation. HashViz is also completely unsupervised, requiring\nno manual inputs to go from a raw corpus to a visualization and search\ninterface. Using the recent Las Vegas mass shooting massacre as a case study,\nwe illustrate the potential of HashViz using only a web browser on the client\nside.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 02:13:54 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Gu", "Yao", ""], ["Kejriwal", "Mayank", ""]]}, {"id": "1801.06323", "submitter": "Hussain Chowdhury Mr.", "authors": "Hussain A Chowdhury, Dhruba K Bhattacharyya", "title": "Plagiarism: Taxonomy, Tools and Detection Techniques", "comments": "Paper of the 19th National Convention on Knowledge, Library and\n  Information Networking (NACLIN 2016) held at Tezpur University, Assam, India\n  from October 26-28, 2016", "journal-ref": "Knowledge, Library and Information Networking, NACLIN 2016, ISBN:\n  978-93-82735-08-3", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To detect plagiarism of any form, it is essential to have broad knowledge of\nits possible forms and classes, and existence of various tools and systems for\nits detection. Based on impact or severity of damages, plagiarism may occur in\nan article or in any production in a number of ways. This survey presents a\ntaxonomy of various plagiarism forms and include discussion on each of these\nforms. Over the years, a good number tools and techniques have been introduced\nto detect plagiarism. This paper highlights few promising methods for\nplagiarism detection based on machine learning techniques. We analyse the pros\nand cons of these methods and finally we highlight a list of issues and\nresearch challenges related to this evolving research problem.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 07:27:42 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Chowdhury", "Hussain A", ""], ["Bhattacharyya", "Dhruba K", ""]]}, {"id": "1801.06482", "submitter": "Amit Awekar", "authors": "Sweta Agrawal, Amit Awekar", "title": "Deep Learning for Detecting Cyberbullying Across Multiple Social Media\n  Platforms", "comments": "Accepted for ECIR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Harassment by cyberbullies is a significant phenomenon on the social media.\nExisting works for cyberbullying detection have at least one of the following\nthree bottlenecks. First, they target only one particular social media platform\n(SMP). Second, they address just one topic of cyberbullying. Third, they rely\non carefully handcrafted features of the data. We show that deep learning based\nmodels can overcome all three bottlenecks. Knowledge learned by these models on\none dataset can be transferred to other datasets. We performed extensive\nexperiments using three real-world datasets: Formspring (12k posts), Twitter\n(16k posts), and Wikipedia(100k posts). Our experiments provide several useful\ninsights about cyberbullying detection. To the best of our knowledge, this is\nthe first work that systematically analyzes cyberbullying detection on various\ntopics across multiple SMPs using deep learning based models and transfer\nlearning.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 16:27:36 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Agrawal", "Sweta", ""], ["Awekar", "Amit", ""]]}, {"id": "1801.06510", "submitter": "Joel Brogan Joel R Brogan", "authors": "Daniel Moreira, Aparna Bharati, Joel Brogan, Allan Pinto, Michael\n  Parowski, Kevin W. Bowyer, Patrick J. Flynn, Anderson Rocha, Walter J.\n  Scheirer", "title": "Image Provenance Analysis at Scale", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior art has shown it is possible to estimate, through image processing and\ncomputer vision techniques, the types and parameters of transformations that\nhave been applied to the content of individual images to obtain new images.\nGiven a large corpus of images and a query image, an interesting further step\nis to retrieve the set of original images whose content is present in the query\nimage, as well as the detailed sequences of transformations that yield the\nquery image given the original images. This is a problem that recently has\nreceived the name of image provenance analysis. In these times of public media\nmanipulation ( e.g., fake news and meme sharing), obtaining the history of\nimage transformations is relevant for fact checking and authorship\nverification, among many other applications. This article presents an\nend-to-end processing pipeline for image provenance analysis, which works at\nreal-world scale. It employs a cutting-edge image filtering solution that is\ncustom-tailored for the problem at hand, as well as novel techniques for\nobtaining the provenance graph that expresses how the images, as nodes, are\nancestrally connected. A comprehensive set of experiments for each stage of the\npipeline is provided, comparing the proposed solution with state-of-the-art\nresults, employing previously published datasets. In addition, this work\nintroduces a new dataset of real-world provenance cases from the social media\nsite Reddit, along with baseline results.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 17:54:22 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 14:41:34 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Moreira", "Daniel", ""], ["Bharati", "Aparna", ""], ["Brogan", "Joel", ""], ["Pinto", "Allan", ""], ["Parowski", "Michael", ""], ["Bowyer", "Kevin W.", ""], ["Flynn", "Patrick J.", ""], ["Rocha", "Anderson", ""], ["Scheirer", "Walter J.", ""]]}, {"id": "1801.06552", "submitter": "Jim Hahn", "authors": "Jim Hahn", "title": "The Bibliotelemetry of Information and Environment: an Evaluation of\n  IoT-Powered Recommender Systems", "comments": "10 pages, 8 figures, 6 tables", "journal-ref": "Proceedings of the ASIS&T 2018 Annual Meeting, p.151-160", "doi": "10.1002/pra2.2018.14505501017", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) infrastructure within the physical library\nenvironment is the basis for an integrative, hybrid approach to digital\nresource recommenders. The IoT infrastructure provides mobile, dynamic\nwayfinding support for items in the collection, which includes features for\nlocation-based recommendations. A modular evaluation and analysis herein\nclarified the nature of users' requests for recommendations based on their\nlocation and describes subject areas of the library for which users request\nrecommendations. The modular mobile design allowed for deep exploration of\nusers' bibliographic identifiers throughout the global module system, serving\nto provide context to the browsing data that are the focus of this study.\nBibliotelemetry is introduced as an evaluation method for IoT middleware within\nlibrary collections.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 19:04:10 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 04:15:19 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 17:49:48 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Hahn", "Jim", ""]]}, {"id": "1801.06664", "submitter": "Noel Nuo Wi Tay", "authors": "Noel Nuo Wi Tay, Sheng-Chi Yang, Chang-Shing Lee, Naoyuki Kubota", "title": "Ontology-based Adaptive e-Textbook Platform for Student and Machine\n  Co-Learning", "comments": "This paper is submitted to IEEE WCCI 2018 Conference for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of electronic textbooks (e-book) has been heavily studied over the\nyears due to their flexibility, accessibility, interactivity and extensibility.\nYet current shortcomings of e-book, which is often just a digitized version of\nthe original book, does not encourage adoption. Consequently, this leads to a\nrethinking of e-book that should incorporate current technologies to augment\nits capabilities, where inclusion of information search and organization tools\nhave shown to be favorable. This paper is on a preliminary work to add\nintelligence into such tools in terms of information retrieval. Construction of\nknowledge graph for e-book material with little overhead is first introduced.\nInformation retrieval through typed similarity query is then performed via\nrandom walk. Case study demonstrate the applicability of the e-book platform,\nwith promising application and advancement in the area of electronic textbooks.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 12:07:36 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Tay", "Noel Nuo Wi", ""], ["Yang", "Sheng-Chi", ""], ["Lee", "Chang-Shing", ""], ["Kubota", "Naoyuki", ""]]}, {"id": "1801.06766", "submitter": "F A Rezaur Rahman Chowdhury", "authors": "Mohammad Hossain Namaki, F A Rezaur Rahman Chowdhury, Md Rakibul\n  Islam, Janardhan Rao Doppa, Yinghui Wu", "title": "Learning to Speed Up Query Planning in Graph Databases", "comments": "Published in the Proceedings of the 27th International Conference on\n  Automated Planning and Scheduling (ICAPS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Querying graph structured data is a fundamental operation that enables\nimportant applications including knowledge graph search, social network\nanalysis, and cyber-network security. However, the growing size of real-world\ndata graphs poses severe challenges for graph databases to meet the\nresponse-time requirements of the applications. Planning the computational\nsteps of query processing - Query Planning - is central to address these\nchallenges. In this paper, we study the problem of learning to speedup query\nplanning in graph databases towards the goal of improving the\ncomputational-efficiency of query processing via training queries.We present a\nLearning to Plan (L2P) framework that is applicable to a large class of query\nreasoners that follow the Threshold Algorithm (TA) approach. First, we define a\ngeneric search space over candidate query plans, and identify target search\ntrajectories (query plans) corresponding to the training queries by performing\nan expensive search. Subsequently, we learn greedy search control knowledge to\nimitate the search behavior of the target query plans. We provide a concrete\ninstantiation of our L2P framework for STAR, a state-of-the-art graph query\nreasoner. Our experiments on benchmark knowledge graphs including DBpedia,\nYAGO, and Freebase show that using the query plans generated by the learned\nsearch control knowledge, we can significantly improve the speed of STAR with\nnegligible loss in accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 04:49:23 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Namaki", "Mohammad Hossain", ""], ["Chowdhury", "F A Rezaur Rahman", ""], ["Islam", "Md Rakibul", ""], ["Doppa", "Janardhan Rao", ""], ["Wu", "Yinghui", ""]]}, {"id": "1801.06965", "submitter": "Thapana Boonchoo", "authors": "Thapana Boonchoo, Xiang Ao, Qing He", "title": "An Efficient Density-based Clustering Algorithm for Higher-Dimensional\n  Data", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DBSCAN is a typically used clustering algorithm due to its clustering ability\nfor arbitrarily-shaped clusters and its robustness to outliers. Generally, the\ncomplexity of DBSCAN is O(n^2) in the worst case, and it practically becomes\nmore severe in higher dimension. Grid-based DBSCAN is one of the recent\nimproved algorithms aiming at facilitating efficiency. However, the performance\nof grid-based DBSCAN still suffers from two problems: neighbour explosion and\nredundancies in merging, which make the algorithms infeasible in\nhigh-dimensional space. In this paper, we propose a novel algorithm named GDPAM\nattempting to extend Grid-based DBSCAN to higher data dimension. In GDPAM, a\nbitmap indexing is utilized to manage non-empty grids so that the neighbour\ngrid queries can be performed efficiently. Furthermore, we adopt an efficient\nunion-find algorithm to maintain the clustering information in order to reduce\nredundancies in the merging. The experimental results on both real-world and\nsynthetic datasets demonstrate that the proposed algorithm outperforms the\nstate-of-the-art exact/approximate DBSCAN and suggests a good scalability.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 06:35:17 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Boonchoo", "Thapana", ""], ["Ao", "Xiang", ""], ["He", "Qing", ""]]}, {"id": "1801.07013", "submitter": "Pierre-Francois Marteau", "authors": "Pierre-Fran\\c{c}ois Marteau (EXPRESSION)", "title": "Sequence Covering Similarity for Symbolic Sequence Comparison", "comments": "arXiv admin note: text overlap with arXiv:1712.02084", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the sequence covering similarity, that we formally\ndefine for evaluating the similarity between a symbolic sequence (string) and a\nset of symbolic sequences (strings). From this covering similarity we derive a\npair-wise distance to compare two symbolic sequences. We show that this\ncovering distance is a semimetric. Few examples are given to show how this\nstring metric in $O(n \\cdot log n)$ compares with the Levenshtein's distance\nthat is in $O(n^2)$. A final example presents its application to plagiarism\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 09:53:25 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 07:24:32 GMT"}, {"version": "v3", "created": "Thu, 8 Mar 2018 15:53:27 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Marteau", "Pierre-Fran\u00e7ois", "", "EXPRESSION"]]}, {"id": "1801.07150", "submitter": "Muhammad Abulaish", "authors": "Muhammad Abulaish, Jahiruddin", "title": "A Novel Weighted Distance Measure for Multi-Attributed Graph", "comments": null, "journal-ref": "Muhammad Abulaish and Jahiruddin, A Novel Weighted Distance\n  Measure for Multi-Attributed Graph, In Proceedings of the 10th Annual ACM\n  India Conference (COMPUTE), Bhopal, India, Nov. 16-18, 2017, pp. 1-9", "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to exponential growth of complex data, graph structure has become\nincreasingly important to model various entities and their interactions, with\nmany interesting applications including, bioinformatics, social network\nanalysis, etc. Depending on the complexity of the data, the underlying graph\nmodel can be a simple directed/undirected and/or weighted/un-weighted graph to\na complex graph (aka multi-attributed graph) where vertices and edges are\nlabelled with multi-dimensional vectors. In this paper, we present a novel\nweighted distance measure based on weighted Euclidean norm which is defined as\na function of both vertex and edge attributes, and it can be used for various\ngraph analysis tasks including classification and cluster analysis. The\nproposed distance measure has flexibility to increase/decrease the weightage of\nedge labels while calculating the distance between vertex-pairs. We have also\nproposed a MAGDist algorithm, which reads multi-attributed graph stored in CSV\nfiles containing the list of vertex vectors and edge vectors, and calculates\nthe distance between each vertex-pair using the proposed weighted distance\nmeasure. Finally, we have proposed a multi-attributed similarity graph\ngeneration algorithm, MAGSim, which reads the output of MAGDist algorithm and\ngenerates a similarity graph that can be analysed using classification and\nclustering algorithms. The significance and accuracy of the proposed distance\nmeasure and algorithms is evaluated on Iris and Twitter data sets, and it is\nfound that the similarity graph generated by our proposed method yields better\nclustering results than the existing similarity graph generation methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 15:46:40 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Abulaish", "Muhammad", ""], ["Jahiruddin", "", ""]]}, {"id": "1801.07743", "submitter": "Pedro Saleiro", "authors": "Pedro Saleiro", "title": "Entity Retrieval and Text Mining for Online Reputation Monitoring", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Reputation Monitoring (ORM) is concerned with the use of computational\ntools to measure the reputation of entities online, such as politicians or\ncompanies. In practice, current ORM methods are constrained to the generation\nof data analytics reports, which aggregate statistics of popularity and\nsentiment on social media. We argue that this format is too restrictive as end\nusers often like to have the flexibility to search for entity-centric\ninformation that is not available in predefined charts. As such, we propose the\ninclusion of entity retrieval capabilities as a first step towards the\nextension of current ORM capabilities. However, an entity's reputation is also\ninfluenced by the entity's relationships with other entities. Therefore, we\naddress the problem of Entity-Relationship (E-R) retrieval in which the goal is\nto search for multiple connected entities. This is a challenging problem which\ntraditional entity search systems cannot cope with. Besides E-R retrieval we\nalso believe ORM would benefit of text-based entity-centric prediction\ncapabilities, such as predicting entity popularity on social media based on\nnews events or the outcome of political surveys. However, none of these tasks\ncan provide useful results if there is no effective entity disambiguation and\nsentiment analysis tailored to the context of ORM. Consequently, this thesis\naddress two computational problems in Online Reputation Monitoring: Entity\nRetrieval and Text Mining. We researched and developed methods to extract,\nretrieve and predict entity-centric information spread across the Web.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 19:36:29 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Saleiro", "Pedro", ""]]}, {"id": "1801.07861", "submitter": "Zhen Wu", "authors": "Zhen Wu, Xin-Yu Dai, Cunyan Yin, Shujian Huang, Jiajun Chen", "title": "Improving Review Representations with User Attention and Product\n  Attention for Sentiment Classification", "comments": "Accepted by AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network methods have achieved great success in reviews sentiment\nclassification. Recently, some works achieved improvement by incorporating user\nand product information to generate a review representation. However, in\nreviews, we observe that some words or sentences show strong user's preference,\nand some others tend to indicate product's characteristic. The two kinds of\ninformation play different roles in determining the sentiment label of a\nreview. Therefore, it is not reasonable to encode user and product information\ntogether into one representation. In this paper, we propose a novel framework\nto encode user and product information. Firstly, we apply two individual\nhierarchical neural networks to generate two representations, with user\nattention or with product attention. Then, we design a combined strategy to\nmake full use of the two representations for training and final prediction. The\nexperimental results show that our model obviously outperforms other\nstate-of-the-art methods on IMDB and Yelp datasets. Through the visualization\nof attention over words related to user or product, we validate our observation\nmentioned above.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 05:11:57 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Wu", "Zhen", ""], ["Dai", "Xin-Yu", ""], ["Yin", "Cunyan", ""], ["Huang", "Shujian", ""], ["Chen", "Jiajun", ""]]}, {"id": "1801.07875", "submitter": "Michael Bloodgood", "authors": "Michael Bloodgood", "title": "Support Vector Machine Active Learning Algorithms with\n  Query-by-Committee versus Closest-to-Hyperplane Selection", "comments": "8 pages, 7 figures, 3 tables; published in Proceedings of the IEEE\n  12th International Conference on Semantic Computing (ICSC 2018), Laguna\n  Hills, CA, USA, pages 148-155, January 2018", "journal-ref": "In Proceedings of the 2018 IEEE 12th International Conference on\n  Semantic Computing (ICSC), pages 148-155, Laguna Hills, CA, USA, January\n  2018. IEEE", "doi": "10.1109/ICSC.2018.00029", "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates and evaluates support vector machine active learning\nalgorithms for use with imbalanced datasets, which commonly arise in many\napplications such as information extraction applications. Algorithms based on\nclosest-to-hyperplane selection and query-by-committee selection are combined\nwith methods for addressing imbalance such as positive amplification based on\nprevalence statistics from initial random samples. Three algorithms (ClosestPA,\nQBagPA, and QBoostPA) are presented and carefully evaluated on datasets for\ntext classification and relation extraction. The ClosestPA algorithm is shown\nto consistently outperform the other two in a variety of ways and insights are\nprovided as to why this is the case.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 06:38:06 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 19:16:47 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Bloodgood", "Michael", ""]]}, {"id": "1801.07883", "submitter": "Lei Zhang", "authors": "Lei Zhang, Shuai Wang, Bing Liu", "title": "Deep Learning for Sentiment Analysis : A Survey", "comments": "34 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has emerged as a powerful machine learning technique that\nlearns multiple layers of representations or features of the data and produces\nstate-of-the-art prediction results. Along with the success of deep learning in\nmany other application domains, deep learning is also popularly used in\nsentiment analysis in recent years. This paper first gives an overview of deep\nlearning and then provides a comprehensive survey of its current applications\nin sentiment analysis.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 07:32:29 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 07:20:41 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Zhang", "Lei", ""], ["Wang", "Shuai", ""], ["Liu", "Bing", ""]]}, {"id": "1801.07887", "submitter": "Michael Bloodgood", "authors": "Garrett Beatty, Ethan Kochis and Michael Bloodgood", "title": "Impact of Batch Size on Stopping Active Learning for Text Classification", "comments": "2 pages, 1 table; published in Proceedings of the IEEE 12th\n  International Conference on Semantic Computing (ICSC 2018), Laguna Hills, CA,\n  USA, pages 306-307, January 2018", "journal-ref": "In Proceedings of the 2018 IEEE 12th International Conference on\n  Semantic Computing (ICSC), pages 306-307, Laguna Hills, CA, USA, January\n  2018. IEEE", "doi": "10.1109/ICSC.2018.00059", "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using active learning, smaller batch sizes are typically more efficient\nfrom a learning efficiency perspective. However, in practice due to speed and\nhuman annotator considerations, the use of larger batch sizes is necessary.\nWhile past work has shown that larger batch sizes decrease learning efficiency\nfrom a learning curve perspective, it remains an open question how batch size\nimpacts methods for stopping active learning. We find that large batch sizes\ndegrade the performance of a leading stopping method over and above the\ndegradation that results from reduced learning efficiency. We analyze this\ndegradation and find that it can be mitigated by changing the window size\nparameter of how many past iterations of learning are taken into account when\nmaking the stopping decision. We find that when using larger batch sizes,\nstopping methods are more effective when smaller window sizes are used.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 07:47:05 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 18:31:45 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Beatty", "Garrett", ""], ["Kochis", "Ethan", ""], ["Bloodgood", "Michael", ""]]}, {"id": "1801.07988", "submitter": "Jonathan Bright", "authors": "Tom Nicholls, Jonathan Bright", "title": "Understanding news story chains using information retrieval and network\n  clustering techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content analysis of news stories (whether manual or automatic) is a\ncornerstone of the communication studies field. However, much research is\nconducted at the level of individual news articles, despite the fact that news\nevents (especially significant ones) are frequently presented as \"stories\" by\nnews outlets: chains of connected articles covering the same event from\ndifferent angles. These stories are theoretically highly important in terms of\nincreasing public recall of news items and enhancing the agenda-setting power\nof the press. Yet thus far, the field has lacked an efficient method for\ndetecting groups of articles which form stories in a way that enables their\nanalysis.\n  In this work, we present a novel, automated method for identifying linked\nnews stories from within a corpus of articles. This method makes use of\ntechniques drawn from the field of information retrieval to identify textual\ncloseness of pairs of articles, and then clustering techniques taken from the\nfield of network analysis to group these articles into stories. We demonstrate\nthe application of the method to a corpus of 61,864 articles, and show how it\ncan efficiently identify valid story clusters within the corpus. We use the\nresults to make observations about the prevalence and dynamics of stories\nwithin the UK news media, showing that more than 50% of news production takes\nplace within stories.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 13:54:57 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Nicholls", "Tom", ""], ["Bright", "Jonathan", ""]]}, {"id": "1801.08439", "submitter": "Maurice-Roman Isele", "authors": "Maurice-Roman Isele", "title": "Analyzing Similarity in Mathematical Content To Enhance the Detection of\n  Academic Plagiarism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the effort put into the detection of academic plagiarism, it\ncontinues to be a ubiquitous problem spanning all disciplines. Various tools\nhave been developed to assist human inspectors by automatically identifying\nsuspicious documents. However, to our knowledge currently none of these tools\nuse mathematical content for their analysis. This is problematic, because\nmathematical content potentially represents a significant amount of the\nscientific contribution in academic documents. Hence, ignoring mathematical\ncontent limits the detection of plagiarism considerably, especially in\ndisciplines with frequent use of mathematics.\n  This paper aims to help close this gap by providing an overview of existing\napproaches in mathematical information retrieval and an analysis of their\napplicability for different possible cases of mathematical plagiarism. I find\nthat whereas syntax-based approaches perform particularly well in detecting\nundisguised plagiarism, structure-based and hybrid approaches promise to also\ndetect forms of disguised mathematical plagiarism, such as plagiarism with\nrenamed identifiers. However, more research in this area is needed to enable\nthe detection of more complex mathematical plagiarism: the scope of current\napproaches is restricted to the formula-level, an extension to the\nsection-level is needed. Additionally, the general detection of equivalence\ntransformations is currently not feasible. Despite these remaining problems, I\nconclude that the presented approaches could already be used for a basic\nautomated detection system targeting mathematical plagiarism and therefore\nenhance current plagiarism detection systems.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 15:02:16 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Isele", "Maurice-Roman", ""]]}, {"id": "1801.08573", "submitter": "Weijian Zhang", "authors": "Weijian Zhang, Jonathan Deakin, Nicholas J. Higham, Shuaiqiang Wang", "title": "Etymo: A New Discovery Engine for AI Research", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Etymo (https://etymo.io), a discovery engine to facilitate\nartificial intelligence (AI) research and development. It aims to help readers\nnavigate a large number of AI-related papers published every week by using a\nnovel form of search that finds relevant papers and displays related papers in\na graphical interface. Etymo constructs and maintains an adaptive\nsimilarity-based network of research papers as an all-purpose knowledge graph\nfor ranking, recommendation, and visualisation. The network is constantly\nevolving and can learn from user feedback to adjust itself.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 19:22:54 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Zhang", "Weijian", ""], ["Deakin", "Jonathan", ""], ["Higham", "Nicholas J.", ""], ["Wang", "Shuaiqiang", ""]]}, {"id": "1801.09079", "submitter": "Alexander Veretennikov Borisovich", "authors": "A. B. Veretennikov", "title": "Using Additional Indexes for Fast Full-Text Search of Phrases That\n  Contain Frequently Used Words", "comments": "These results were published in: Veretennikov A.B. Using Additional\n  Indexes for Fast Full-Text Search of Phrases That Contain Frequently Used\n  Words. Control Systems and Information Technologies. 2013. vol. 52, no. 2.\n  pp. 61-66 (in Russian). This is an English translation of the text", "journal-ref": "Control Systems and Information Technologies. 2013. vol. 52, no.\n  2. pp. 61-66 (in Russian)", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searches for phrases and word sets in large text arrays by means of\nadditional indexes are considered. Their use may reduce the query-processing\ntime by an order of magnitude in comparison with standard inverted files.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 12:02:03 GMT"}, {"version": "v2", "created": "Sun, 25 Nov 2018 14:36:11 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Veretennikov", "A. B.", ""]]}, {"id": "1801.09251", "submitter": "Yi Tay", "authors": "Yi Tay, Luu Anh Tuan, Siu Cheung Hui", "title": "Multi-Pointer Co-Attention Networks for Recommendation", "comments": "Accepted to KDD 2018 (Research Track)", "journal-ref": null, "doi": "10.1145/3219819.3220086", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent state-of-the-art recommender systems such as D-ATT, TransNet and\nDeepCoNN exploit reviews for representation learning. This paper proposes a new\nneural architecture for recommendation with reviews. Our model operates on a\nmulti-hierarchical paradigm and is based on the intuition that not all reviews\nare created equal, i.e., only a select few are important. The importance,\nhowever, should be dynamically inferred depending on the current target. To\nthis end, we propose a review-by-review pointer-based learning scheme that\nextracts important reviews, subsequently matching them in a word-by-word\nfashion. This enables not only the most informative reviews to be utilized for\nprediction but also a deeper word-level interaction. Our pointer-based method\noperates with a novel gumbel-softmax based pointer mechanism that enables the\nincorporation of discrete vectors within differentiable neural architectures.\nOur pointer mechanism is co-attentive in nature, learning pointers which are\nco-dependent on user-item relationships. Finally, we propose a multi-pointer\nlearning scheme that learns to combine multiple views of interactions between\nuser and item. Overall, we demonstrate the effectiveness of our proposed model\nvia extensive experiments on \\textbf{24} benchmark datasets from Amazon and\nYelp. Empirical results show that our approach significantly outperforms\nexisting state-of-the-art, with up to 19% and 71% relative improvement when\ncompared to TransNet and DeepCoNN respectively. We study the behavior of our\nmulti-pointer learning mechanism, shedding light on evidence aggregation\npatterns in review-based recommender systems.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 17:14:13 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 11:27:30 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Tay", "Yi", ""], ["Tuan", "Luu Anh", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1801.09322", "submitter": "Sarvnaz Karimi", "authors": "Vincent Nguyen, Sarvnaz Karimi, Sara Falamaki, Cecile Paris", "title": "Benchmarking Clinical Decision Support Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Finding relevant literature underpins the practice of evidence-based\nmedicine. From 2014 to 2016, TREC conducted a clinical decision support track,\nwherein participants were tasked with finding articles relevant to clinical\nquestions posed by physicians. In total, 87 teams have participated over the\npast three years, generating 395 runs. During this period, each team has\ntrialled a variety of methods. While there was significant overlap in the\nmethods employed by different teams, the results were varied. Due to the\ndiversity of the platforms used, the results arising from the different\ntechniques are not directly comparable, reducing the ability to build on\nprevious work. By using a stable platform, we have been able to compare\ndifferent document and query processing techniques, allowing us to experiment\nwith different search parameters. We have used our system to reproduce leading\nteams runs, and compare the results obtained. By benchmarking our indexing and\nsearch techniques, we can statistically test a variety of hypotheses, paving\nthe way for further research.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 00:09:39 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Nguyen", "Vincent", ""], ["Karimi", "Sarvnaz", ""], ["Falamaki", "Sara", ""], ["Paris", "Cecile", ""]]}, {"id": "1801.09334", "submitter": "Dongdong Yang", "authors": "Dongdong Yang, Senzhang Wang, Zhoujun Li", "title": "Ensemble Neural Relation Extraction with Adaptive Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation extraction has been widely studied to extract new relational facts\nfrom open corpus. Previous relation extraction methods are faced with the\nproblem of wrong labels and noisy data, which substantially decrease the\nperformance of the model. In this paper, we propose an ensemble neural network\nmodel - Adaptive Boosting LSTMs with Attention, to more effectively perform\nrelation extraction. Specifically, our model first employs the recursive neural\nnetwork LSTMs to embed each sentence. Then we import attention into LSTMs by\nconsidering that the words in a sentence do not contribute equally to the\nsemantic meaning of the sentence. Next via adaptive boosting, we build\nstrategically several such neural classifiers. By ensembling multiple such LSTM\nclassifiers with adaptive boosting, we could build a more effective and robust\njoint ensemble neural networks based relation extractor. Experiment results on\nreal dataset demonstrate the superior performance of the proposed model,\nimproving F1-score by about 8% compared to the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 01:06:18 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 19:21:19 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Yang", "Dongdong", ""], ["Wang", "Senzhang", ""], ["Li", "Zhoujun", ""]]}, {"id": "1801.09496", "submitter": "Gaurav Singh", "authors": "Gaurav Singh, James Thomas and John Shawe-Taylor", "title": "Improving Active Learning in Systematic Reviews", "comments": "10 pages, many figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematic reviews are essential to summarizing the results of different\nclinical and social science studies. The first step in a systematic review task\nis to identify all the studies relevant to the review. The task of identifying\nrelevant studies for a given systematic review is usually performed manually,\nand as a result, involves substantial amounts of expensive human resource.\nLately, there have been some attempts to reduce this manual effort using active\nlearning. In this work, we build upon some such existing techniques, and\nvalidate by experimenting on a larger and comprehensive dataset than has been\nattempted until now. Our experiments provide insights on the use of different\nfeature extraction models for different disciplines. More importantly, we\nidentify that a naive active learning based screening process is biased in\nfavour of selecting similar documents. We aimed to improve the performance of\nthe screening process using a novel active learning algorithm with success.\nAdditionally, we propose a mechanism to choose the best feature extraction\nmethod for a given review.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 13:26:48 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Singh", "Gaurav", ""], ["Thomas", "James", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1801.09851", "submitter": "Xuan Wang", "authors": "Xuan Wang, Yu Zhang, Xiang Ren, Yuhao Zhang, Marinka Zitnik, Jingbo\n  Shang, Curtis Langlotz and Jiawei Han", "title": "Cross-type Biomedical Named Entity Recognition with Deep Multi-Task\n  Learning", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: State-of-the-art biomedical named entity recognition (BioNER)\nsystems often require handcrafted features specific to each entity type, such\nas genes, chemicals and diseases. Although recent studies explored using neural\nnetwork models for BioNER to free experts from manual feature engineering, the\nperformance remains limited by the available training data for each entity\ntype. Results: We propose a multi-task learning framework for BioNER to\ncollectively use the training data of different types of entities and improve\nthe performance on each of them. In experiments on 15 benchmark BioNER\ndatasets, our multi-task model achieves substantially better performance\ncompared with state-of-the-art BioNER systems and baseline neural sequence\nlabeling models. Further analysis shows that the large performance gains come\nfrom sharing character- and word-level information among relevant biomedical\nentities across differently labeled corpora.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 04:44:14 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 04:37:50 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2018 19:32:10 GMT"}, {"version": "v4", "created": "Mon, 8 Oct 2018 01:51:11 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Wang", "Xuan", ""], ["Zhang", "Yu", ""], ["Ren", "Xiang", ""], ["Zhang", "Yuhao", ""], ["Zitnik", "Marinka", ""], ["Shang", "Jingbo", ""], ["Langlotz", "Curtis", ""], ["Han", "Jiawei", ""]]}, {"id": "1801.09961", "submitter": "Gerasimos Razis", "authors": "Gerasimos Razis, Ioannis Anagnostopoulos, Sherali Zeadally", "title": "Modeling Influence with Semantics in Social Networks: a Survey", "comments": "61 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of influential entities in all kinds of networks (e.g. social,\ndigital, or computer) has always been an important field of study. In recent\nyears, Online Social Networks (OSNs) have been established as a basic means of\ncommunication and often influencers and opinion makers promote politics,\nevents, brands or products through viral content. In this work, we present a\nsystematic review across i) online social influence metrics, properties, and\napplications and ii) the role of semantic in modeling OSNs information. We end\nup with the conclusion that both areas can jointly provide useful insights\ntowards the qualitative assessment of viral user-generated content, as well as\nfor modeling the dynamic properties of influential content and its flow\ndynamics.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 12:44:14 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 16:25:23 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 07:16:23 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Razis", "Gerasimos", ""], ["Anagnostopoulos", "Ioannis", ""], ["Zeadally", "Sherali", ""]]}, {"id": "1801.09975", "submitter": "Ibrahim Riza Hallac", "authors": "Semiha Makinist, Ibrahim Riza Hallac, Betul Ay Karakus and Galip Aydin", "title": "Preparation of Improved Turkish DataSet for Sentiment Analysis in Social\n  Media", "comments": "Presented at CMES2017", "journal-ref": null, "doi": "10.1051/itmconf/20171301030", "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A public dataset, with a variety of properties suitable for sentiment\nanalysis [1], event prediction, trend detection and other text mining\napplications, is needed in order to be able to successfully perform analysis\nstudies. The vast majority of data on social media is text-based and it is not\npossible to directly apply machine learning processes into these raw data,\nsince several different processes are required to prepare the data before the\nimplementation of the algorithms. For example, different misspellings of same\nword enlarge the word vector space unnecessarily, thereby it leads to reduce\nthe success of the algorithm and increase the computational power requirement.\nThis paper presents an improved Turkish dataset with an effective spelling\ncorrection algorithm based on Hadoop [2]. The collected data is recorded on the\nHadoop Distributed File System and the text based data is processed by\nMapReduce programming model. This method is suitable for the storage and\nprocessing of large sized text based social media data. In this study, movie\nreviews have been automatically recorded with Apache ManifoldCF (MCF) [3] and\ndata clusters have been created. Various methods compared such as Levenshtein\nand Fuzzy String Matching have been proposed to create a public dataset from\ncollected data. Experimental results show that the proposed algorithm, which\ncan be used as an open source dataset in sentiment analysis studies, have been\nperformed successfully to the detection and correction of spelling errors.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 13:18:51 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 14:08:16 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Makinist", "Semiha", ""], ["Hallac", "Ibrahim Riza", ""], ["Karakus", "Betul Ay", ""], ["Aydin", "Galip", ""]]}, {"id": "1801.10084", "submitter": "Faez Ahmed", "authors": "Faez Ahmed, Mark Fuge", "title": "Creative Exploration Using Topic Based Bisociative Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bisociative knowledge discovery is an approach that combines elements from\ntwo or more \"incompatible\" domains to generate creative solutions and insight.\nInspired by Koestler's notion of bisociation, in this paper we propose a\ncomputational framework for the discovery of new connections between domains to\npromote creative discovery and inspiration in design. Specifically, we propose\nusing topic models on a large collection of unstructured text ideas from\nmultiple domains to discover creative sources of inspiration. We use these\ntopics to generate a Bisociative Information Network--- a graph that captures\nconceptual similarity between ideas--- that helps designers find creative links\nwithin that network. Using a dataset of thousands of ideas from OpenIDEO, an\nonline collaborative community, our results show usefulness of representing\nconceptual bridges through collections of words (topics) in finding\ncross-domain inspiration. We show that the discovered links between domains,\nwhether presented on their own or via ideas they inspired, are perceived to be\nmore novel and can also be used as creative stimuli for new idea generation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 16:20:14 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Ahmed", "Faez", ""], ["Fuge", "Mark", ""]]}, {"id": "1801.10095", "submitter": "Alberto Garcia-Duran", "authors": "Alberto Garcia-Duran, Roberto Gonzalez, Daniel Onoro-Rubio, Mathias\n  Niepert, Hui Li", "title": "TransRev: Modeling Reviews as Translations from Users to Items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The text of a review expresses the sentiment a customer has towards a\nparticular product. This is exploited in sentiment analysis where machine\nlearning models are used to predict the review score from the text of the\nreview. Furthermore, the products costumers have purchased in the past are\nindicative of the products they will purchase in the future. This is what\nrecommender systems exploit by learning models from purchase information to\npredict the items a customer might be interested in. We propose TransRev, an\napproach to the product recommendation problem that integrates ideas from\nrecommender systems, sentiment analysis, and multi-relational learning into a\njoint learning objective. TransRev learns vector representations for users,\nitems, and reviews. The embedding of a review is learned such that (a) it\nperforms well as input feature of a regression model for sentiment prediction;\nand (b) it always translates the reviewer embedding to the embedding of the\nreviewed items. This allows TransRev to approximate a review embedding at test\ntime as the difference of the embedding of each item and the user embedding.\nThe approximated review embedding is then used with the regression model to\npredict the review score for each item. TransRev outperforms state of the art\nrecommender systems on a large number of benchmark data sets. Moreover, it is\nable to retrieve, for each user and item, the review text from the training set\nwhose embedding is most similar to the approximated review embedding.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 17:01:01 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 09:39:23 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Garcia-Duran", "Alberto", ""], ["Gonzalez", "Roberto", ""], ["Onoro-Rubio", "Daniel", ""], ["Niepert", "Mathias", ""], ["Li", "Hui", ""]]}, {"id": "1801.10182", "submitter": "Nathaniel Roth", "authors": "Reuben Brasher, Nat Roth, Justin Wagle", "title": "Sometimes You Want to Go Where Everybody Knows your Name", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-22871-2_44", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new metric for measuring how well a model personalizes to a\nuser's specific preferences. We define personalization as a weighting between\nperformance on user specific data and performance on a more general global\ndataset that represents many different users. This global term serves as a form\nof regularization that forces us to not overfit to individual users who have\nsmall amounts of data. In order to protect user privacy, we add the constraint\nthat we may not centralize or share user data. We also contribute a simple\nexperiment in which we simulate classifying sentiment for users with very\ndistinct vocabularies. This experiment functions as an example of the tension\nbetween doing well globally on all users, and doing well on any specific\nindividual user. It also provides a concrete example of how to employ our new\nmetric to help reason about and resolve this tension. We hope this work can\nhelp frame and ground future work into personalization.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 19:29:04 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Brasher", "Reuben", ""], ["Roth", "Nat", ""], ["Wagle", "Justin", ""]]}, {"id": "1801.10253", "submitter": "Spencer Cappallo", "authors": "Spencer Cappallo, Stacey Svetlichnaya, Pierre Garrigues, Thomas\n  Mensink, Cees G. M. Snoek", "title": "The New Modality: Emoji Challenges in Prediction, Anticipation, and\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, emoji have emerged as a new and widespread form of\ndigital communication, spanning diverse social networks and spoken languages.\nWe propose to treat these ideograms as a new modality in their own right,\ndistinct in their semantic structure from both the text in which they are often\nembedded as well as the images which they resemble. As a new modality, emoji\npresent rich novel possibilities for representation and interaction. In this\npaper, we explore the challenges that arise naturally from considering the\nemoji modality through the lens of multimedia research. Specifically, the ways\nin which emoji can be related to other common modalities such as text and\nimages. To do so, we first present a large scale dataset of real-world emoji\nusage collected from Twitter. This dataset contains examples of both text-emoji\nand image-emoji relationships. We present baseline results on the challenge of\npredicting emoji from both text and images, using state-of-the-art neural\nnetworks. Further, we offer a first consideration into the problem of how to\naccount for new, unseen emoji - a relevant issue as the emoji vocabulary\ncontinues to expand on a yearly basis. Finally, we present results for\nmultimedia retrieval using emoji as queries.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 23:19:49 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 14:59:47 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Cappallo", "Spencer", ""], ["Svetlichnaya", "Stacey", ""], ["Garrigues", "Pierre", ""], ["Mensink", "Thomas", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1801.10288", "submitter": "Yongfeng Zhang", "authors": "Xu Chen and Yongfeng Zhang and Hongteng Xu and Yixin Cao and Zheng Qin\n  and Hongyuan Zha", "title": "Visually Explainable Recommendation", "comments": "11 pages, 4 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images account for a significant part of user decisions in many application\nscenarios, such as product images in e-commerce, or user image posts in social\nnetworks. It is intuitive that user preferences on the visual patterns of image\n(e.g., hue, texture, color, etc) can be highly personalized, and this provides\nus with highly discriminative features to make personalized recommendations.\n  Previous work that takes advantage of images for recommendation usually\ntransforms the images into latent representation vectors, which are adopted by\na recommendation component to assist personalized user/item profiling and\nrecommendation. However, such vectors are hardly useful in terms of providing\nvisual explanations to users about why a particular item is recommended, and\nthus weakens the explainability of recommendation systems.\n  As a step towards explainable recommendation models, we propose visually\nexplainable recommendation based on attentive neural networks to model the user\nattention on images, under the supervision of both implicit feedback and\ntextual reviews. By this, we can not only provide recommendation results to the\nusers, but also tell the users why an item is recommended by providing\nintuitive visual highlights in a personalized manner. Experimental results show\nthat our models are not only able to improve the recommendation performance,\nbut also can provide persuasive visual explanations for the users to take the\nrecommendations.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 03:16:39 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Chen", "Xu", ""], ["Zhang", "Yongfeng", ""], ["Xu", "Hongteng", ""], ["Cao", "Yixin", ""], ["Qin", "Zheng", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1801.10603", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Dan Li, Evangelos Kanoulas", "title": "ILPS at TREC 2017 Common Core Track", "comments": "TREC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TREC 2017 Common Core Track aimed at gathering a diverse set of\nparticipating runs and building a new test collection using advanced pooling\nmethods.\n  In this paper, we describe the participation of the IlpsUvA team at the TREC\n2017 Common Core Track. We submitted runs created using two methods to the\ntrack: (1) BOIR uses Bayesian optimization to automatically optimize retrieval\nmodel hyperparameters. (2) NVSM is a latent vector space model where\nrepresentations of documents and query terms are learned from scratch in an\nunsupervised manner.\n  We find that BOIR is able to optimize hyperparameters as to find a system\nthat performs competitively amongst track participants. NVSM provides rankings\nthat are diverse, as it was amongst the top automated unsupervised runs that\nprovided the most unique relevant documents.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 18:48:23 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Van Gysel", "Christophe", ""], ["Li", "Dan", ""], ["Kanoulas", "Evangelos", ""]]}]