[{"id": "0911.0050", "submitter": "Hyoungshick Kim", "authors": "Hyoungshick Kim and Ji Won Yoon", "title": "How to Compare the Scientific Contributions between Research Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": "0911.0049", "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to analyse the scientific contributions between research\ngroups. Given multiple research groups, we construct their journal/proceeding\ngraphs and then compute the similarity/gap between them using network analysis.\nThis analysis can be used for measuring similarity/gap of the topics/qualities\nbetween research groups' scientific contributions. We demonstrate the\npracticality of our method by comparing the scientific contributions by Korean\nresearchers with those by the global researchers for information security in\n2006 - 2008. The empirical analysis shows that the current security research in\nSouth Korea has been isolated from the global research trend.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2009 01:03:27 GMT"}], "update_date": "2009-11-03", "authors_parsed": [["Kim", "Hyoungshick", ""], ["Yoon", "Ji Won", ""]]}, {"id": "0911.0486", "submitter": "Rdv Ijcsis", "authors": "Dang Tuan Nguyen, Ha Quy-Tinh Luong, Tuyen Thi-Thanh Do", "title": "Building a Vietnamese Language Query Processing Framework for ELibrary\n  Searching Systems", "comments": "5 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 1, pp. 092-096, October 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the objective of building intelligent searching systems for Elibraries or\nonline bookstores, we have proposed a searching system model based on a\nVietnamese language query processing component. Such document searching systems\nbased on this model can allow users to use Vietnamese queries that represent\ncontent information as input, instead of entering keywords for searching in\nspecific fields in database. To simplify the realization process of system\nbased on this searching system model, we set a target of building a framework\nto support the rapid development of Vietnamese language query processing\ncomponents. Such framework let the implementation of Vietnamese language query\nprocessing component in similar systems in this domain to be done more easily.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2009 04:13:32 GMT"}], "update_date": "2009-11-04", "authors_parsed": [["Nguyen", "Dang Tuan", ""], ["Luong", "Ha Quy-Tinh", ""], ["Do", "Tuyen Thi-Thanh", ""]]}, {"id": "0911.0505", "submitter": "Kirk D. Borne", "authors": "Kirk Borne (1) ((1) George Mason University)", "title": "Scientific Data Mining in Astronomy", "comments": "26 pages", "journal-ref": "Borne, K., in Next Generation of Data Mining (Taylor & Francis:\n  CRC Press), pp. 91-114 (2009)", "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DB cs.IR physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the application of data mining algorithms to research problems in\nastronomy. We posit that data mining has always been fundamental to\nastronomical research, since data mining is the basis of evidence-based\ndiscovery, including classification, clustering, and novelty discovery. These\nalgorithms represent a major set of computational tools for discovery in large\ndatabases, which will be increasingly essential in the era of data-intensive\nastronomy. Historical examples of data mining in astronomy are reviewed,\nfollowed by a discussion of one of the largest data-producing projects\nanticipated for the coming decade: the Large Synoptic Survey Telescope (LSST).\nTo facilitate data-driven discoveries in astronomy, we envision a new\ndata-oriented research paradigm for astronomy and astrophysics --\nastroinformatics. Astroinformatics is described as both a research approach and\nan educational imperative for modern data-intensive astronomy. An important\napplication area for large time-domain sky surveys (such as LSST) is the rapid\nidentification, characterization, and classification of real-time sky events\n(including moving objects, photometrically variable objects, and the appearance\nof transients). We describe one possible implementation of a classification\nbroker for such events, which incorporates several astroinformatics techniques:\nuser annotation, semantic tagging, metadata markup, heterogeneous data\nintegration, and distributed data mining. Examples of these types of\ncollaborative classification and discovery approaches within other science\ndisciplines are presented.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2009 05:34:54 GMT"}], "update_date": "2009-11-04", "authors_parsed": [["Borne", "Kirk", "", "George Mason University"]]}, {"id": "0911.0781", "submitter": "Rdv Ijcsis", "authors": "Kanak Saxena, D.S Rajpoot", "title": "A Way to Understand Various Patterns of Data Mining Techniques for\n  Selected Domains", "comments": "6 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 1, pp. 186-191, October 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This has much in common with traditional work in statistics and machine\nlearning. However, there are important new issues which arise because of the\nsheer size of the data. One of the important problem in data mining is the\nClassification-rule learning which involves finding rules that partition given\ndata into predefined classes. In the data mining domain where millions of\nrecords and a large number of attributes are involved, the execution time of\nexisting algorithms can become prohibitive, particularly in interactive\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2009 11:19:55 GMT"}], "update_date": "2009-11-05", "authors_parsed": [["Saxena", "Kanak", ""], ["Rajpoot", "D. S", ""]]}, {"id": "0911.0914", "submitter": "Vishal Goyal", "authors": "Sumalatha Ramachandran, Sujaya Paulraj, Sharon Joseph and Vetriselvi\n  Ramaraj", "title": "Enhanced Trustworthy and High-Quality Information Retrieval System for\n  Web Search Engines", "comments": "International Journal of Computer Science Issues, IJCSI Volume 5,\n  pp38-42, October 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The WWW is the most important source of information. But, there is no\nguarantee for information correctness and lots of conflicting information is\nretrieved by the search engines and the quality of provided information also\nvaries from low quality to high quality. We provide enhanced trustworthiness in\nboth specific (entity) and broad (content) queries in web searching. The\nfiltering of trustworthiness is based on 5 factors: Provenance, Authority, Age,\nPopularity, and Related Links. The trustworthiness is calculated based on these\n5 factors and it is stored thereby increasing the performance in retrieving\ntrustworthy websites. The calculated trustworthiness is stored only for static\nwebsites. Quality is provided based on policies selected by the user. Quality\nbased ranking of retrieved trusted information is provided using WIQA (Web\nInformation Quality Assessment) Framework.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2009 19:22:44 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2009 02:42:47 GMT"}], "update_date": "2009-11-18", "authors_parsed": [["Ramachandran", "Sumalatha", ""], ["Paulraj", "Sujaya", ""], ["Joseph", "Sharon", ""], ["Ramaraj", "Vetriselvi", ""]]}, {"id": "0911.1112", "submitter": "Michael Nelson", "authors": "Herbert Van de Sompel, Michael L. Nelson, Robert Sanderson, Lyudmila\n  L. Balakireva, Scott Ainsworth, Harihar Shankar", "title": "Memento: Time Travel for the Web", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The Web is ephemeral. Many resources have representations that change over\ntime, and many of those representations are lost forever. A lucky few manage to\nreappear as archived resources that carry their own URIs. For example, some\ncontent management systems maintain version pages that reflect a frozen prior\nstate of their changing resources. Archives recurrently crawl the web to obtain\nthe actual representation of resources, and subsequently make those available\nvia special-purpose archived resources. In both cases, the archival copies have\nURIs that are protocol-wise disconnected from the URI of the resource of which\nthey represent a prior state. Indeed, the lack of temporal capabilities in the\nmost common Web protocol, HTTP, prevents getting to an archived resource on the\nbasis of the URI of its original. This turns accessing archived resources into\na significant discovery challenge for both human and software agents, which\ntypically involves following a multitude of links from the original to the\narchival resource, or of searching archives for the original URI. This paper\nproposes the protocol-based Memento solution to address this problem, and\ndescribes a proof-of-concept experiment that includes major servers of archival\ncontent, including Wikipedia and the Internet Archive. The Memento solution is\nbased on existing HTTP capabilities applied in a novel way to add the temporal\ndimension. The result is a framework in which archived resources can seamlessly\nbe reached via the URI of their original: protocol-based time travel for the\nWeb.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2009 20:52:22 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2009 21:35:50 GMT"}], "update_date": "2009-11-06", "authors_parsed": [["Van de Sompel", "Herbert", ""], ["Nelson", "Michael L.", ""], ["Sanderson", "Robert", ""], ["Balakireva", "Lyudmila L.", ""], ["Ainsworth", "Scott", ""], ["Shankar", "Harihar", ""]]}, {"id": "0911.1275", "submitter": "Yuri Suhov", "authors": "Mark Kelbert and Yuri Suhov", "title": "Continuity of mutual entropy in the large signal-to-noise ratio limit", "comments": "This paper has been withdrawn since it has been already published,\n  in: {\\it Stochastic Analysis 2010.} Springer-Verlag: Berlin, 2010, pp.\n  281--299; arXiv:0911", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses the issue of the proof of the entropy power inequality\n(EPI), an important tool in the analysis of Gaussian channels of information\ntransmission, proposed by Shannon.\n  We analyse continuity properties of the mutual entropy of the input and\noutput signals in an additive memoryless channel and discuss assumptions under\nwhich the entropy-power inequality holds true.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2009 15:20:54 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2010 11:43:41 GMT"}, {"version": "v3", "created": "Tue, 2 Apr 2013 20:32:51 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Kelbert", "Mark", ""], ["Suhov", "Yuri", ""]]}, {"id": "0911.1305", "submitter": "Loet Leydesdorff", "authors": "Ricardo Arencibia-Jorge, Loet Leydesdorff, Zaida Chinchilla-Rodriguez,\n  Ronald Rousseau, Soren W. Paris", "title": "Retrieval of very large numbers of items in the Web of Science: an\n  exercise to develop accurate search strategies", "comments": null, "journal-ref": "El Profesional de la Informacion 18(5) (2009) 555-559", "doi": null, "report-no": null, "categories": "cs.DL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current communication presents a simple exercise with the aim of solving\na singular problem: the retrieval of extremely large amounts of items in the\nWeb of Science interface. As it is known, Web of Science interface allows a\nuser to obtain at most 100,000 items from a single query. But what about\nqueries that achieve a result of more than 100,000 items? The exercise\ndeveloped one possible way to achieve this objective. The case study is the\nretrieval of the entire scientific production from the United States in a\nspecific year. Different sections of items were retrieved using the field\nSource of the database. Then, a simple Boolean statement was created with the\naim of eliminating overlapping and to improve the accuracy of the search\nstrategy. The importance of team work in the development of advanced search\nstrategies was noted.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2009 17:20:54 GMT"}], "update_date": "2009-11-09", "authors_parsed": [["Arencibia-Jorge", "Ricardo", ""], ["Leydesdorff", "Loet", ""], ["Chinchilla-Rodriguez", "Zaida", ""], ["Rousseau", "Ronald", ""], ["Paris", "Soren W.", ""]]}, {"id": "0911.1318", "submitter": "Loet Leydesdorff", "authors": "Leo Egghe, Loet Leydesdorff", "title": "The relation between Pearson's correlation coefficient r and Salton's\n  cosine measure", "comments": null, "journal-ref": "Journal of the American Society for Information Science and\n  Technology 60(5) (2009) 1027-1036", "doi": "10.1016/j.eswa.2012.07.016", "report-no": null, "categories": "cs.IR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relation between Pearson's correlation coefficient and Salton's cosine\nmeasure is revealed based on the different possible values of the division of\nthe L1-norm and the L2-norm of a vector. These different values yield a sheaf\nof increasingly straight lines which form together a cloud of points, being the\ninvestigated relation. The theoretical results are tested against the author\nco-citation relations among 24 informetricians for whom two matrices can be\nconstructed, based on co-citations: the asymmetric occurrence matrix and the\nsymmetric co-citation matrix. Both examples completely confirm the theoretical\nresults. The results enable us to specify an algorithm which provides a\nthreshold value for the cosine above which none of the corresponding Pearson\ncorrelations would be negative. Using this threshold value can be expected to\noptimize the visualization of the vector space.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2009 18:09:07 GMT"}], "update_date": "2012-07-25", "authors_parsed": [["Egghe", "Leo", ""], ["Leydesdorff", "Loet", ""]]}, {"id": "0911.1320", "submitter": "Loet Leydesdorff", "authors": "Han Park, Loet Leydesdorff", "title": "Knowledge linkage structures in communication studies using citation\n  analysis among communication journals", "comments": null, "journal-ref": "Scientometrics 81(1) (2009) 157-175", "doi": null, "report-no": null, "categories": "cs.DL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research analyzes a \"who cites whom\" matrix in terms of aggregated,\njournal-journal citations to determine the location of communication studies on\nthe academic spectrum. Using the Journal of Communication as the seed journal,\nthe 2006 data in the Journal Citation Reports are used to map communication\nstudies. The results show that social and experimental psychology journals are\nthe most frequently used sources of information in this field. In addition,\nseveral journals devoted to the use and effects of media and advertising are\nweakly integrated into the larger communication research community, whereas\ncommunication studies are dominated by American journals.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2009 18:13:23 GMT"}], "update_date": "2009-11-09", "authors_parsed": [["Park", "Han", ""], ["Leydesdorff", "Loet", ""]]}, {"id": "0911.1447", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff", "title": "On the Normalization and Visualization of Author Co-Citation Data\n  Salton's Cosine versus the Jaccard Index", "comments": null, "journal-ref": "Loet Leydesdorff, On the Normalization and Visualization of Author\n  Co-citation Data: Salton's cosine versus the Jaccard Index, Journal of the\n  American Society for Information Science and Technology, 59(1), 77-85, 2008", "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The debate about which similarity measure one should use for the\nnormalization in the case of Author Co-citation Analysis (ACA) is further\ncomplicated when one distinguishes between the symmetrical co-citation--or,\nmore generally, co-occurrence--matrix and the underlying asymmetrical\ncitation--occurrence--matrix. In the Web environment, the approach of\nretrieving original citation data is often not feasible. In that case, one\nshould use the Jaccard index, but preferentially after adding the number of\ntotal citations (occurrences) on the main diagonal. Unlike Salton's cosine and\nthe Pearson correlation, the Jaccard index abstracts from the shape of the\ndistributions and focuses only on the intersection and the sum of the two sets.\nSince the correlations in the co-occurrence matrix may partially be spurious,\nthis property of the Jaccard index can be considered as an advantage in this\ncase.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2009 18:53:09 GMT"}], "update_date": "2009-11-10", "authors_parsed": [["Leydesdorff", "Loet", ""]]}, {"id": "0911.2632", "submitter": "Henk Moed", "authors": "Henk F. Moed", "title": "Measuring contextual citation impact of scientific journals", "comments": "Version 13 November 2009; 23 pages; 1 appendix; 7 tables; 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores a new indicator of journal citation impact, denoted as\nsource normalized impact per paper (SNIP). It measures a journal's contextual\ncitation impact, taking into account characteristics of its properly defined\nsubject field, especially the frequency at which authors cite other papers in\ntheir reference lists, the rapidity of maturing of citation impact, and the\nextent to which a database used for the assessment covers the field's\nliterature. It further develops Eugene Garfield's notions of a field's\n'citation potential' defined as the average length of references lists in a\nfield and determining the probability of being cited, and the need in fair\nperformance assessments to correct for differences between subject fields. A\njournal's subject field is defined as the set of papers citing that journal.\nSNIP is defined as the ratio of the journal's citation count per paper and the\ncitation potential in its subject field. It aims to allow direct comparison of\nsources in different subject fields. Citation potential is shown to vary not\nonly between journal subject categories - groupings of journals sharing a\nresearch field - or disciplines (e.g., journals in mathematics, engineering and\nsocial sciences tend to have lower values than titles in life sciences), but\nalso between journals within the same subject category. For instance, basic\njournals tend to show higher citation potentials than applied or clinical\njournals, and journals covering emerging topics higher than periodicals in\nclassical subjects or more general journals. SNIP corrects for such\ndifferences. Its strengths and limitations are critically discussed, and\nsuggestions are made for further research. All empirical results are derived\nfrom Elsevier's Scopus.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2009 15:10:31 GMT"}], "update_date": "2009-11-16", "authors_parsed": [["Moed", "Henk F.", ""]]}, {"id": "0911.3318", "submitter": "Francisco Claude", "authors": "Francisco Claude, Antonio Farina and Gonzalo Navarro", "title": "Re-Pair Compression of Inverted Lists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression of inverted lists with methods that support fast intersection\noperations is an active research topic. Most compression schemes rely on\nencoding differences between consecutive positions with techniques that favor\nsmall numbers. In this paper we explore a completely different alternative: We\nuse Re-Pair compression of those differences. While Re-Pair by itself offers\nfast decompression at arbitrary positions in main and secondary memory, we\nintroduce variants that in addition speed up the operations required for\ninverted list intersection. We compare the resulting data structures with\nseveral recent proposals under various list intersection algorithms, to\nconclude that our Re-Pair variants offer an interesting time/space tradeoff for\nthis problem, yet further improvements are required for it to improve upon the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2009 14:39:36 GMT"}], "update_date": "2009-11-18", "authors_parsed": [["Claude", "Francisco", ""], ["Farina", "Antonio", ""], ["Navarro", "Gonzalo", ""]]}, {"id": "0911.3411", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff and Iina Hellsten", "title": "Measuring the Meaning of Words in Contexts: An automated analysis of\n  controversies about Monarch butterflies, Frankenfoods, and stem cells", "comments": null, "journal-ref": "Scientometrics 67(2), 2006, 231-258", "doi": null, "report-no": null, "categories": "cs.CL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-words have been considered as carriers of meaning across different domains\nin studies of science, technology, and society. Words and co-words, however,\nobtain meaning in sentences, and sentences obtain meaning in their contexts of\nuse. At the science/society interface, words can be expected to have different\nmeanings: the codes of communication that provide meaning to words differ on\nthe varying sides of the interface. Furthermore, meanings and interfaces may\nchange over time. Given this structuring of meaning across interfaces and over\ntime, we distinguish between metaphors and diaphors as reflexive mechanisms\nthat facilitate the translation between contexts. Our empirical focus is on\nthree recent scientific controversies: Monarch butterflies, Frankenfoods, and\nstem-cell therapies. This study explores new avenues that relate the study of\nco-word analysis in context with the sociological quest for the analysis and\nprocessing of meaning.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2009 22:03:56 GMT"}], "update_date": "2009-11-19", "authors_parsed": [["Leydesdorff", "Loet", ""], ["Hellsten", "Iina", ""]]}, {"id": "0911.3415", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff", "title": "Can Scientific Journals be Classified in terms of Aggregated\n  Journal-Journal Citation Relations using the Journal Citation Reports?", "comments": null, "journal-ref": "Journal of the American Society for Information Science and\n  Technology, 57(5) (2006) 601-613", "doi": null, "report-no": null, "categories": "cs.DL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aggregated citation relations among journals included in the Science\nCitation Index provide us with a huge matrix which can be analyzed in various\nways. Using principal component analysis or factor analysis, the factor scores\ncan be used as indicators of the position of the cited journals in the citing\ndimensions of the database. Unrotated factor scores are exact, and the\nextraction of principal components can be made stepwise since the principal\ncomponents are independent. Rotation may be needed for the designation, but in\nthe rotated solution a model is assumed. This assumption can be legitimated on\npragmatic or theoretical grounds. Since the resulting outcomes remain sensitive\nto the assumptions in the model, an unambiguous classification is no longer\npossible in this case. However, the factor-analytic solutions allow us to test\nclassifications against the structures contained in the database. This will be\ndemonstrated for the delineation of a set of biochemistry journals.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2009 22:20:18 GMT"}], "update_date": "2009-11-19", "authors_parsed": [["Leydesdorff", "Loet", ""]]}, {"id": "0911.3416", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff, Stephen Bensman", "title": "Classification and Powerlaws: The Logarithmic Transformation", "comments": null, "journal-ref": "Journal of the American Society for Information Science and\n  Technology 57(11) (2006) 1470-1486", "doi": null, "report-no": null, "categories": "cs.IR cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logarithmic transformation of the data has been recommended by the literature\nin the case of highly skewed distributions such as those commonly found in\ninformation science. The purpose of the transformation is to make the data\nconform to the lognormal law of error for inferential purposes. How does this\ntransformation affect the analysis? We factor analyze and visualize the\ncitation environment of the Journal of the American Chemical Society (JACS)\nbefore and after a logarithmic transformation. The transformation strongly\nreduces the variance necessary for classificatory purposes and therefore is\ncounterproductive to the purposes of the descriptive statistics. We recommend\nagainst the logarithmic transformation when sets cannot be defined\nunambiguously. The intellectual organization of the sciences is reflected in\nthe curvilinear parts of the citation distributions, while negative powerlaws\nfit excellently to the tails of the distributions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2009 22:24:31 GMT"}], "update_date": "2009-11-19", "authors_parsed": [["Leydesdorff", "Loet", ""], ["Bensman", "Stephen", ""]]}, {"id": "0911.3422", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff and Liwen Vaughan", "title": "Co-occurrence Matrices and their Applications in Information Science:\n  Extending ACA to the Web Environment", "comments": null, "journal-ref": "Journal of the American Society for Information Science &\n  Technology 57(12) (2006) 1616-1628", "doi": null, "report-no": null, "categories": "cs.IR cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-occurrence matrices, such as co-citation, co-word, and co-link matrices,\nhave been used widely in the information sciences. However, confusion and\ncontroversy have hindered the proper statistical analysis of this data. The\nunderlying problem, in our opinion, involved understanding the nature of\nvarious types of matrices. This paper discusses the difference between a\nsymmetrical co-citation matrix and an asymmetrical citation matrix as well as\nthe appropriate statistical techniques that can be applied to each of these\nmatrices, respectively. Similarity measures (like the Pearson correlation\ncoefficient or the cosine) should not be applied to the symmetrical co-citation\nmatrix, but can be applied to the asymmetrical citation matrix to derive the\nproximity matrix. The argument is illustrated with examples. The study then\nextends the application of co-occurrence matrices to the Web environment where\nthe nature of the available data and thus data collection methods are different\nfrom those of traditional databases such as the Science Citation Index. A set\nof data collected with the Google Scholar search engine is analyzed using both\nthe traditional methods of multivariate analysis and the new visualization\nsoftware Pajek that is based on social network analysis and graph theory.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2009 22:33:53 GMT"}], "update_date": "2009-11-19", "authors_parsed": [["Leydesdorff", "Loet", ""], ["Vaughan", "Liwen", ""]]}, {"id": "0911.3643", "submitter": "Loet Leydesdorff", "authors": "Iina Hellsten, Loet Leydesdorff, Paul Wouters", "title": "Multiple Presents: How Search Engines Re-write the Past", "comments": null, "journal-ref": "New Media & Society, 8(6) (2006), 901-924", "doi": null, "report-no": null, "categories": "cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet search engines function in a present which changes continuously. The\nsearch engines update their indices regularly, overwriting Web pages with newer\nones, adding new pages to the index, and losing older ones. Some search engines\ncan be used to search for information at the internet for specific periods of\ntime. However, these 'date stamps' are not determined by the first occurrence\nof the pages in the Web, but by the last date at which a page was updated or a\nnew page was added, and the search engine's crawler updated this change in the\ndatabase. This has major implications for the use of search engines in\nscholarly research as well as theoretical implications for the conceptions of\ntime and temporality. We examine the interplay between the different updating\nfrequencies by using AltaVista and Google for searches at different moments of\ntime. Both the retrieval of the results and the structure of the retrieved\ninformation erodes over time.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2009 20:05:51 GMT"}], "update_date": "2009-11-19", "authors_parsed": [["Hellsten", "Iina", ""], ["Leydesdorff", "Loet", ""], ["Wouters", "Paul", ""]]}, {"id": "0911.3823", "submitter": "Leonardo Ermann", "authors": "Leonardo Ermann, Dima D.L. Shepelyansky", "title": "Google matrix and Ulam networks of intermittency maps", "comments": "7 pages, 14 figures, research done at Quantware\n  http://www.quantware.ups-tlse.fr/", "journal-ref": "PhysRev E. 81, 03622 (2010)", "doi": "10.1103/PhysRevE.81.036221", "report-no": null, "categories": "cs.IR cond-mat.dis-nn nlin.AO nlin.CD physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the properties of the Google matrix of an Ulam network generated by\nintermittency maps. This network is created by the Ulam method which gives a\nmatrix approximant for the Perron-Frobenius operator of dynamical map. The\nspectral properties of eigenvalues and eigenvectors of this matrix are\nanalyzed. We show that the PageRank of the system is characterized by a power\nlaw decay with the exponent $\\beta$ dependent on map parameters and the Google\ndamping factor $\\alpha$. Under certain conditions the PageRank is completely\ndelocalized so that the Google search in such a situation becomes inefficient.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2009 15:24:51 GMT"}], "update_date": "2010-05-12", "authors_parsed": [["Ermann", "Leonardo", ""], ["Shepelyansky", "Dima D. L.", ""]]}, {"id": "0911.3842", "submitter": "Luciano da Fontoura Costa", "authors": "Debora C. Correa, Jose H. Saito and Luciano da F. Costa", "title": "Musical Genres: Beating to the Rhythms of Different Drums", "comments": "35 pages, 13 figures, 13 tables", "journal-ref": null, "doi": "10.1088/1367-2630/12/5/053030", "report-no": null, "categories": "physics.data-an cs.IR cs.SD physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online music databases have increased signicantly as a consequence of the\nrapid growth of the Internet and digital audio, requiring the development of\nfaster and more efficient tools for music content analysis. Musical genres are\nwidely used to organize music collections. In this paper, the problem of\nautomatic music genre classification is addressed by exploring rhythm-based\nfeatures obtained from a respective complex network representation. A Markov\nmodel is build in order to analyse the temporal sequence of rhythmic notation\nevents. Feature analysis is performed by using two multivariate statistical\napproaches: principal component analysis(unsupervised) and linear discriminant\nanalysis (supervised). Similarly, two classifiers are applied in order to\nidentify the category of rhythms: parametric Bayesian classifier under gaussian\nhypothesis (supervised), and agglomerative hierarchical clustering\n(unsupervised). Qualitative results obtained by Kappa coefficient and the\nobtained clusters corroborated the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2009 20:29:04 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Correa", "Debora C.", ""], ["Saito", "Jose H.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "0911.3979", "submitter": "Daniel Gayo Avello", "authors": "Daniel Gayo-Avello, David J. Brenes", "title": "Making the road by searching - A search engine based on Swarm\n  Information Foraging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines are nowadays one of the most important entry points for\nInternet users and a central tool to solve most of their information needs.\nStill, there exist a substantial amount of users' searches which obtain\nunsatisfactory results. Needless to say, several lines of research aim to\nincrease the relevancy of the results users retrieve. In this paper the authors\nframe this problem within the much broader (and older) one of information\noverload. They argue that users' dissatisfaction with search engines is a\ncurrently common manifestation of such a problem, and propose a different angle\nfrom which to tackle with it. As it will be discussed, their approach shares\ngoals with a current hot research topic (namely, learning to rank for\ninformation retrieval) but, unlike the techniques commonly applied in that\nfield, their technique cannot be exactly considered machine learning and,\nadditionally, it can be used to change the search engine's response in\nreal-time, driven by the users behavior. Their proposal adapts concepts from\nSwarm Intelligence (in particular, Ant Algorithms) from an Information Foraging\npoint of view. It will be shown that the technique is not only feasible, but\nalso an elegant solution to the stated problem; what's more, it achieves\npromising results, both increasing the performance of a major search engine for\ninformational queries, and substantially reducing the time users require to\nanswer complex information needs.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2009 20:57:33 GMT"}], "update_date": "2009-11-23", "authors_parsed": [["Gayo-Avello", "Daniel", ""], ["Brenes", "David J.", ""]]}, {"id": "0911.4178", "submitter": "Morgan Harvey", "authors": "Morgan Harvey, Mark Baillie, Ian Ruthven, David Elsweiler", "title": "Folksonomic Tag Clouds as an Aid to Content Indexing", "comments": "SIGIR 2009 Workshop on Search in Social Media (SSM 2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social tagging systems have recently developed as a popular method of data\norganisation on the Internet. These systems allow users to organise their\ncontent in a way that makes sense to them, rather than forcing them to use a\npre-determined and rigid set of categorisations. These folksonomies provide\nwell populated sources of unstructured tags describing web resources which\ncould potentially be used as semantic index terms for these resources. However\ngetting people to agree on what tags best describe a resource is a difficult\nproblem, therefore any feature which increases the consistency and stability of\nterms chosen would be extremely beneficial. We investigate how the provision of\na tag cloud, a weighted list of terms commonly used to assist in browsing a\nfolksonomy, during the tagging process itself influences the tags produced and\nhow difficult the user perceived the task to be. We show that illustrating the\nmost popular tags to users assists in the tagging process and encourages a\nstable and consistent folksonomy to form.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2009 12:55:54 GMT"}], "update_date": "2009-11-24", "authors_parsed": [["Harvey", "Morgan", ""], ["Baillie", "Mark", ""], ["Ruthven", "Ian", ""], ["Elsweiler", "David", ""]]}, {"id": "0911.4292", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff", "title": "Similarity Measures, Author Cocitation Analysis, and Information Theory", "comments": null, "journal-ref": "Journal of the American Society for Information Science &\n  Technology, 56(7), 2005, 769-772", "doi": null, "report-no": null, "categories": "cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Pearson's correlation coefficient in Author Cocitation Analysis\nwas compared with Salton's cosine measure in a number of recent contributions.\nUnlike the Pearson correlation, the cosine is insensitive to the number of\nzeros. However, one has the option of applying a logarithmic transformation in\ncorrelation analysis. Information calculus is based on both the logarithmic\ntransformation and provides a non-parametric statistics. Using this methodology\none can cluster a document set in a precise way and express the differences in\nterms of bits of information. The algorithm is explained and used on the data\nset which was made the subject of this discussion.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2009 21:35:51 GMT"}], "update_date": "2009-11-24", "authors_parsed": [["Leydesdorff", "Loet", ""]]}, {"id": "0911.4302", "submitter": "Loet Leydesdorff", "authors": "Diana Lucio-Arias, Loet Leydesdorff", "title": "An Indicator of Research Front Activity: Measuring Intellectual\n  Organization as Uncertainty Reduction in Document Sets", "comments": null, "journal-ref": "Journal of the American Society for Information Science &\n  Technology 60(12) (2009) 2488-2498", "doi": null, "report-no": null, "categories": "cs.DL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using scientific literature to model scholarly discourse, a research\nspecialty can be operationalized as an evolving set of related documents. Each\npublication can be expected to contribute to the further development of the\nspecialty at the research front. The specific combinations of title words and\ncited references in a paper can then be considered as a signature of the\nknowledge claim in the paper: new words and combinations of words can be\nexpected to represent variation, while each paper is at the same time\nselectively positioned into the intellectual organization of a field using\ncontext-relevant references. Can the mutual information among these three\ndimensions--title words, cited references, and sequence numbers--be used as an\nindicator of the extent to which intellectual organization structures the\nuncertainty prevailing at a research front? The effect of the discovery of\nnanotubes (1991) on the previously existing field of fullerenes is used as a\ntest case. Thereafter, this method is applied to science studies with a focus\non scientometrics using various sample delineations. An emerging research front\nabout citation analysis can be indicated.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2009 22:12:55 GMT"}], "update_date": "2009-11-24", "authors_parsed": [["Lucio-Arias", "Diana", ""], ["Leydesdorff", "Loet", ""]]}, {"id": "0911.4910", "submitter": "Tao Zhou", "authors": "Ci-Hang Jin, Jian-Guo Liu, Yi-Cheng Zhang, Tao Zhou", "title": "Adaptive information filtering for dynamic recommender systems", "comments": "6 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic environment in the real world calls for the adaptive techniques\nfor information filtering, namely to provide real-time responses to the changes\nof system data. Where many incremental algorithms are designed for this\npurpose, they are usually challenged by the worse and worse performance\nresulted from the cumulative errors over time. In this Letter, we propose two\nincremental diffusion-based algorithms for the personalized recommendations,\nwhich integrate some pieces of local and fast updatings to achieve the\napproximate results. In addition to the fast responses, the errors of the\nproposed algorithms do not cumulate over time, that is to say, the global\nrecomputing is unnecessary. This remarkable advantage is demonstrated by\nseveral metrics on algorithmic accuracy for two movie recommender systems and a\nsocial bookmarking system.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2009 16:34:13 GMT"}], "update_date": "2009-11-26", "authors_parsed": [["Jin", "Ci-Hang", ""], ["Liu", "Jian-Guo", ""], ["Zhang", "Yi-Cheng", ""], ["Zhou", "Tao", ""]]}, {"id": "0911.5046", "submitter": "Joaqu\\'in P\\'erez-Iglesias", "authors": "Joaqu\\'in P\\'erez-Iglesias, Jos\\'e R. P\\'erez-Ag\\\"uera, V\\'ictor\n  Fresno and Yuval Z. Feinstein", "title": "Integrating the Probabilistic Models BM25/BM25F into Lucene", "comments": "Software can be downloaded from:\n  http://nlp.uned.es/~jperezi/Lucene-BM25/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes the BM25 and BM25F implementation using the Lucene\nJava Framework. Both models have stood out at TREC by their performance and are\nconsidered as state-of-the-art in the IR community. BM25 is applied to\nretrieval on plain text documents, that is for documents that do not contain\nfields, while BM25F is applied to documents with structure.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2009 10:27:14 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2009 13:41:05 GMT"}], "update_date": "2009-12-01", "authors_parsed": [["P\u00e9rez-Iglesias", "Joaqu\u00edn", ""], ["P\u00e9rez-Ag\u00fcera", "Jos\u00e9 R.", ""], ["Fresno", "V\u00edctor", ""], ["Feinstein", "Yuval Z.", ""]]}, {"id": "0911.5378", "submitter": "Victor Odumuyiwa", "authors": "Victor Odumuyiwa (LORIA)", "title": "De la recherche sociale d'information \\`a la recherche collaborative\n  d'information", "comments": null, "journal-ref": "7\\`eme colloque du chapitre fran\\c{c}ais de l'ISKO, Lyon : France\n  (2009)", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explain social information retrieval (SIR) and\ncollaborative information retrieval (CIR). We see SIR as a way of knowing who\nto collaborate with in resolving an information problem while CIR entails the\nprocess of mutual understanding and solving of an information problem among\ncollaborators. We are interested in the transition from SIR to CIR hence we\ndeveloped a communication model to facilitate knowledge sharing during CIR.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2009 06:25:00 GMT"}], "update_date": "2009-12-01", "authors_parsed": [["Odumuyiwa", "Victor", "", "LORIA"]]}]