[{"id": "0903.0034", "submitter": "Vladimir Braverman", "authors": "Vladimir Braverman, Rafail Ostrovsky", "title": "Measuring Independence of Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.IR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data stream model represents setting where approximating pairwise, or\n$k$-wise, independence with sublinear memory is of considerable importance. In\nthe streaming model the joint distribution is given by a stream of $k$-tuples,\nwith the goal of testing correlations among the components measured over the\nentire stream. In the streaming model, Indyk and McGregor (SODA 08) recently\ngave exciting new results for measuring pairwise independence. The Indyk and\nMcGregor methods provide $\\log{n}$-approximation under statistical distance\nbetween the joint and product distributions in the streaming model. Indyk and\nMcGregor leave, as their main open question, the problem of improving their\n$\\log n$-approximation for the statistical distance metric.\n  In this paper we solve the main open problem posed by of Indyk and McGregor\nfor the statistical distance for pairwise independence and extend this result\nto any constant $k$. In particular, we present an algorithm that computes an\n$(\\epsilon, \\delta)$-approximation of the statistical distance between the\njoint and product distributions defined by a stream of $k$-tuples. Our\nalgorithm requires $O(({1\\over \\epsilon}\\log({nm\\over \\delta}))^{(30+k)^k})$\nmemory and a single pass over the data stream.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2009 01:29:54 GMT"}], "update_date": "2009-03-03", "authors_parsed": [["Braverman", "Vladimir", ""], ["Ostrovsky", "Rafail", ""]]}, {"id": "0903.0153", "submitter": "Ralph Kretschmer", "authors": "Patricio Galeas (1), Ralph Kretschmer (2), Bernd Freisleben (1) ((1)\n  University of Marburg, Germany, (2) Kretschmer Software, Siegen, Germany)", "title": "Document Relevance Evaluation via Term Distribution Analysis Using\n  Fourier Series Expansion", "comments": "9 pages, submitted to proceedings of JCDL-2009", "journal-ref": "Proceedings of the 2009 Joint international Conference on Digital\n  Libraries (Austin, TX, USA, June 15 - 19, 2009). JCDL '09. ACM, New York, NY,\n  277-284", "doi": "10.1145/1555400.1555446", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to the frequency of terms in a document collection, the\ndistribution of terms plays an important role in determining the relevance of\ndocuments for a given search query. In this paper, term distribution analysis\nusing Fourier series expansion as a novel approach for calculating an abstract\nrepresentation of term positions in a document corpus is introduced. Based on\nthis approach, two methods for improving the evaluation of document relevance\nare proposed: (a) a function-based ranking optimization representing a user\ndefined document region, and (b) a query expansion technique based on\noverlapping the term distributions in the top-ranked documents. Experimental\nresults demonstrate the effectiveness of the proposed approach in providing new\npossibilities for optimizing the retrieval process.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2009 17:08:17 GMT"}], "update_date": "2009-07-18", "authors_parsed": [["Galeas", "Patricio", ""], ["Kretschmer", "Ralph", ""], ["Freisleben", "Bernd", ""]]}, {"id": "0903.0625", "submitter": "Edith Cohen", "authors": "Edith Cohen, Haim Kaplan", "title": "Leveraging Discarded Samples for Tighter Estimation of Multiple-Set\n  Aggregates", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many datasets such as market basket data, text or hypertext documents, and\nsensor observations recorded in different locations or time periods, are\nmodeled as a collection of sets over a ground set of keys. We are interested in\nbasic aggregates such as the weight or selectivity of keys that satisfy some\nselection predicate defined over keys' attributes and membership in particular\nsets. This general formulation includes basic aggregates such as the Jaccard\ncoefficient, Hamming distance, and association rules.\n  On massive data sets, exact computation can be inefficient or infeasible.\nSketches based on coordinated random samples are classic summaries that support\napproximate query processing.\n  Queries are resolved by generating a sketch (sample) of the union of sets\nused in the predicate from the sketches these sets and then applying an\nestimator to this union-sketch.\n  We derive novel tighter (unbiased) estimators that leverage sampled keys that\nare present in the union of applicable sketches but excluded from the union\nsketch. We establish analytically that our estimators dominate estimators\napplied to the union-sketch for {\\em all queries and data sets}. Empirical\nevaluation on synthetic and real data reveals that on typical applications we\ncan expect a 25%-4 fold reduction in estimation error.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2009 21:21:02 GMT"}], "update_date": "2009-03-05", "authors_parsed": [["Cohen", "Edith", ""], ["Kaplan", "Haim", ""]]}, {"id": "0903.1788", "submitter": "Dirk Bollen", "authors": "Dirk Bollen and Harry Halpin", "title": "The Role of Tag Suggestions in Folksonomies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most tagging systems support the user in the tag selection process by\nproviding tag suggestions, or recommendations, based on a popularity\nmeasurement of tags other users provided when tagging the same resource. In\nthis paper we investigate the influence of tag suggestions on the emergence of\npower law distributions as a result of collaborative tag behavior. Although\nprevious research has already shown that power laws emerge in tagging systems,\nthe cause of why power law distributions emerge is not understood empirically.\nThe majority of theories and mathematical models of tagging found in the\nliterature assume that the emergence of power laws in tagging systems is mainly\ndriven by the imitation behavior of users when observing tag suggestions\nprovided by the user interface of the tagging system. This imitation behavior\nleads to a feedback loop in which some tags are reinforced and get more popular\nwhich is also known as the `rich get richer' or a preferential attachment\nmodel. We present experimental results that show that the power law\ndistribution forms regardless of whether or not tag suggestions are presented\nto the users. Furthermore, we show that the real effect of tag suggestions is\nrather subtle; the resulting power law distribution is `compressed' if tag\nsuggestions are given to the user, resulting in a shorter long tail and a\n`compressed' top of the power law distribution. The consequences of this\nexperiment show that tag suggestions by themselves do not account for the\nformation of power law distributions in tagging systems.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2009 14:48:30 GMT"}], "update_date": "2009-03-11", "authors_parsed": [["Bollen", "Dirk", ""], ["Halpin", "Harry", ""]]}, {"id": "0903.2310", "submitter": "Kang Ning", "authors": "Kang Ning, Hoong Kee Ng, Hon Wai Leong", "title": "Analysis of the Relationships among Longest Common Subsequences,\n  Shortest Common Supersequences and Patterns and its application on Pattern\n  Discovery in Biological Sequences", "comments": "Extended version of paper presented in IEEE BIBE 2006 submitted to\n  journal for review", "journal-ref": null, "doi": "10.1504/IJDMB.2011.045413", "report-no": null, "categories": "cs.DS cs.DM cs.IR cs.OH q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a set of mulitple sequences, their patterns,Longest Common Subsequences\n(LCS) and Shortest Common Supersequences (SCS) represent different aspects of\nthese sequences profile, and they can all be used for biological sequence\ncomparisons and analysis. Revealing the relationship between the patterns and\nLCS,SCS might provide us with a deeper view of the patterns of biological\nsequences, in turn leading to better understanding of them. However, There is\nno careful examinaton about the relationship between patterns, LCS and SCS. In\nthis paper, we have analyzed their relation, and given some lemmas. Based on\ntheir relations, a set of algorithms called the PALS (PAtterns by Lcs and Scs)\nalgorithms are propsoed to discover patterns in a set of biological sequences.\nThese algorithms first generate the results for LCS and SCS of sequences by\nheuristic, and consequently derive patterns from these results. Experiments\nshow that the PALS algorithms perform well (both in efficiency and in accuracy)\non a variety of sequences. The PALS approach also provides us with a solution\nfor transforming between the heuristic results of SCS and LCS.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2009 04:45:05 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Ning", "Kang", ""], ["Ng", "Hoong Kee", ""], ["Leong", "Hon Wai", ""]]}, {"id": "0903.2544", "submitter": "Iraklis Varlamis", "authors": "N. Zotos, P. Tzekou, G. Tsatsaronis, L. Kozanidis, S. Stamou, I.\n  Varlamis", "title": "To Click or not to Click? The Role of Contextualized and User-Centric\n  Web Snippets", "comments": "In proceedings of SIGIR 2007 Workshop on Focused Retrieval. 8 pages", "journal-ref": "SIGIR 2007 Workshop on Focused Retrieval", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When searching the web, it is often possible that there are too many results\navailable for ambiguous queries. Text snippets, extracted from the retrieved\npages, are an indicator of the pages' usefulness to the query intention and can\nbe used to focus the scope of search results. In this paper, we propose a novel\nmethod for automatically extracting web page snippets that are highly relevant\nto the query intention and expressive of the pages' entire content. We show\nthat the usage of semantics, as a basis for focused retrieval, produces high\nquality text snippet suggestions. The snippets delivered by our method are\nsignificantly better in terms of retrieval performance compared to those\nderived using the pages' statistical content. Furthermore, our study suggests\nthat semantically-driven snippet generation can also be used to augment\ntraditional passage retrieval algorithms based on word overlap or statistical\nweights, since they typically differ in coverage and produce different results.\nUser clicks on the query relevant snippets can be used to refine the query\nresults and promote the most comprehensive among the relevant documents.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2009 12:22:27 GMT"}], "update_date": "2009-03-24", "authors_parsed": [["Zotos", "N.", ""], ["Tzekou", "P.", ""], ["Tsatsaronis", "G.", ""], ["Kozanidis", "L.", ""], ["Stamou", "S.", ""], ["Varlamis", "I.", ""]]}, {"id": "0903.3257", "submitter": "Marcus Hutter", "authors": "Ke Zhang and Marcus Hutter and Huidong Jin", "title": "A New Local Distance-Based Outlier Detection Approach for Scattered\n  Real-World Data", "comments": "15 LaTeX pages, 7 figures, 2 tables, 1 algorithm, 2 theorems", "journal-ref": "Proc. 13th Pacific-Asia Conf. on Knowledge Discovery and Data\n  Mining (PAKDD 2009) pages 813-822", "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting outliers which are grossly different from or inconsistent with the\nremaining dataset is a major challenge in real-world KDD applications. Existing\noutlier detection methods are ineffective on scattered real-world datasets due\nto implicit data patterns and parameter setting issues. We define a novel\n\"Local Distance-based Outlier Factor\" (LDOF) to measure the {outlier-ness} of\nobjects in scattered datasets which addresses these issues. LDOF uses the\nrelative location of an object to its neighbours to determine the degree to\nwhich the object deviates from its neighbourhood. Properties of LDOF are\ntheoretically analysed including LDOF's lower bound and its false-detection\nprobability, as well as parameter settings. In order to facilitate parameter\nsettings in real-world applications, we employ a top-n technique in our outlier\ndetection approach, where only the objects with the highest LDOF values are\nregarded as outliers. Compared to conventional approaches (such as top-n KNN\nand top-n LOF), our method top-n LDOF is more effective at detecting outliers\nin scattered data. It is also easier to set parameters, since its performance\nis relatively stable over a large range of parameter values, as illustrated by\nexperimental results on both real-world and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2009 23:50:29 GMT"}], "update_date": "2009-12-30", "authors_parsed": [["Zhang", "Ke", ""], ["Hutter", "Marcus", ""], ["Jin", "Huidong", ""]]}, {"id": "0903.4035", "submitter": "Iraklis Varlamis", "authors": "A. Kritikopoulos, M. Sideri, I. Varlamis", "title": "BLOGRANK: Ranking Weblogs Based On Connectivity And Similarity Features", "comments": "9 pages, in 2nd international workshop on Advanced architectures and\n  algorithms for internet delivery and applications", "journal-ref": "Proceedings of the 2nd international Workshop on Advanced\n  Architectures and Algorithms For internet Delivery and Applications (Pisa,\n  Italy, October 10 - 10, 2006). AAA-IDEA '06", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large part of the hidden web resides in weblog servers. New content is\nproduced in a daily basis and the work of traditional search engines turns to\nbe insufficient due to the nature of weblogs. This work summarizes the\nstructure of the blogosphere and highlights the special features of weblogs. In\nthis paper we present a method for ranking weblogs based on the link graph and\non several similarity characteristics between weblogs. First we create an\nenhanced graph of connected weblogs and add new types of edges and weights\nutilising many weblog features. Then, we assign a ranking to each weblog using\nour algorithm, BlogRank, which is a modified version of PageRank. For the\nvalidation of our method we run experiments on a weblog dataset, which we\nprocess and adapt to our search engine. (http://spiderwave.aueb.gr/Blogwave).\nThe results suggest that the use of the enhanced graph and the BlogRank\nalgorithm is preferred by the users.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2009 08:36:21 GMT"}], "update_date": "2009-03-25", "authors_parsed": [["Kritikopoulos", "A.", ""], ["Sideri", "M.", ""], ["Varlamis", "I.", ""]]}, {"id": "0903.4101", "submitter": "Evira Mayordomo", "authors": "Elvira Mayordomo, Philippe Moser, Sylvain Perifel", "title": "Polylog space compression, pushdown compression, and Lempel-Ziv are\n  incomparable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pressing need for efficient compression schemes for XML documents has\nrecently been focused on stack computation, and in particular calls for a\nformulation of information-lossless stack or pushdown compressors that allows a\nformal analysis of their performance and a more ambitious use of the stack in\nXML compression, where so far it is mainly connected to parsing mechanisms. In\nthis paper we introduce the model of pushdown compressor, based on pushdown\ntransducers that compute a single injective function while keeping the widest\ngenerality regarding stack computation.\n  We also consider online compression algorithms that use at most\npolylogarithmic space (plogon). These algorithms correspond to compressors in\nthe data stream model.\n  We compare the performance of these two families of compressors with each\nother and with the general purpose Lempel-Ziv algorithm. This comparison is\nmade without any a priori assumption on the data's source and considering the\nasymptotic compression ratio for infinite sequences. We prove that in all cases\nthey are incomparable.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2009 15:37:21 GMT"}], "update_date": "2009-03-25", "authors_parsed": [["Mayordomo", "Elvira", ""], ["Moser", "Philippe", ""], ["Perifel", "Sylvain", ""]]}, {"id": "0903.4530", "submitter": "Lek-Heng Lim", "authors": "Lek-Heng Lim, Pierre Comon", "title": "Nonnegative approximations of nonnegative tensors", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the decomposition of a nonnegative tensor into a minimal sum of\nouter product of nonnegative vectors and the associated parsimonious naive\nBayes probabilistic model. We show that the corresponding approximation\nproblem, which is central to nonnegative PARAFAC, will always have optimal\nsolutions. The result holds for any choice of norms and, under a mild\nassumption, even Bregman divergences.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2009 08:39:45 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2009 05:12:19 GMT"}], "update_date": "2009-04-12", "authors_parsed": [["Lim", "Lek-Heng", ""], ["Comon", "Pierre", ""]]}, {"id": "0903.5172", "submitter": "Olivier Giraud", "authors": "Olivier Giraud, Bertrand Georgeot, Dima L. Shepelyansky", "title": "Delocalization transition for the Google matrix", "comments": "4 pages, 5 figures. Research done at\n  http://www.quantware.ups-tlse.fr/", "journal-ref": "Phys. Rev. E 80, 026107 (2009)", "doi": "10.1103/PhysRevE.80.026107", "report-no": null, "categories": "cs.IR cond-mat.dis-nn nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the localization properties of eigenvectors of the Google matrix,\ngenerated both from the World Wide Web and from the Albert-Barabasi model of\nnetworks. We establish the emergence of a delocalization phase for the PageRank\nvector when network parameters are changed. In the phase of localized PageRank,\na delocalization takes place in the complex plane of eigenvalues of the matrix,\nleading to delocalized relaxation modes. We argue that the efficiency of\ninformation retrieval by Google-type search is strongly affected in the phase\nof delocalized PageRank.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2009 11:06:03 GMT"}], "update_date": "2009-09-04", "authors_parsed": [["Giraud", "Olivier", ""], ["Georgeot", "Bertrand", ""], ["Shepelyansky", "Dima L.", ""]]}, {"id": "0903.5254", "submitter": "David Campbell", "authors": "Eric Archambault, David Campbell, Yves Gingras, Vincent Lariviere", "title": "Comparing Bibliometric Statistics Obtained from the Web of Science and\n  Scopus", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": "10.1002/asi.21062", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For more than 40 years, the Institute for Scientific Information (ISI, now\npart of Thomson Reuters) produced the only available bibliographic databases\nfrom which bibliometricians could compile large-scale bibliometric indicators.\nISI's citation indexes, now regrouped under the Web of Science (WoS), were the\nmajor sources of bibliometric data until 2004, when Scopus was launched by the\npublisher Reed Elsevier. For those who perform bibliometric analyses and\ncomparisons of countries or institutions, the existence of these two major\ndatabases raises the important question of the comparability and stability of\nstatistics obtained from different data sources. This paper uses macro-level\nbibliometric indicators to compare results obtained from the WoS and Scopus. It\nshows that the correlations between the measures obtained with both databases\nfor the number of papers and the number of citations received by countries, as\nwell as for their ranks, are extremely high (R2 > .99). There is also a very\nhigh correlation when countries' papers are broken down by field. The paper\nthus provides evidence that indicators of scientific production and citations\nat the country level are stable and largely independent of the database.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2009 15:56:49 GMT"}], "update_date": "2009-03-31", "authors_parsed": [["Archambault", "Eric", ""], ["Campbell", "David", ""], ["Gingras", "Yves", ""], ["Lariviere", "Vincent", ""]]}]