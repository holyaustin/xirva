[{"id": "2005.00033", "submitter": "Firoj Alam", "authors": "Firoj Alam, Shaden Shaar, Fahim Dalvi, Hassan Sajjad, Alex Nikolov,\n  Hamdy Mubarak, Giovanni Da San Martino, Ahmed Abdelali, Nadir Durrani, Kareem\n  Darwish, Preslav Nakov", "title": "Fighting the COVID-19 Infodemic: Modeling the Perspective of\n  Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the\n  Society", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of the COVID-19 pandemic, the political and the medical\naspects of disinformation merged as the problem got elevated to a whole new\nlevel to become the first global infodemic. Fighting this infodemic is ranked\nsecond in the list of the most important focus areas of the World Health\nOrganization, with dangers ranging from promoting fake cures, rumors, and\nconspiracy theories to spreading xenophobia and panic. Addressing the issue\nrequires solving a number of challenging problems such as identifying messages\ncontaining claims, determining their check-worthiness and factuality, and their\npotential to do harm as well as the nature of that harm, to mention just a few.\nThus, here we design, annotate, and release to the research community a new\ndataset for fine-grained disinformation analysis that (i)focuses on COVID-19,\n(ii) combines the perspectives and the interests of journalists, fact-checkers,\nsocial media platforms, policy makers, and society as a whole, and (iii) covers\nboth English and Arabic. Finally, we show strong evaluation results using\nstate-of-the-art Transformers, thus confirming the practical utility of the\nannotation schema and of the dataset.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 18:04:20 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 13:33:12 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Alam", "Firoj", ""], ["Shaar", "Shaden", ""], ["Dalvi", "Fahim", ""], ["Sajjad", "Hassan", ""], ["Nikolov", "Alex", ""], ["Mubarak", "Hamdy", ""], ["Martino", "Giovanni Da San", ""], ["Abdelali", "Ahmed", ""], ["Durrani", "Nadir", ""], ["Darwish", "Kareem", ""], ["Nakov", "Preslav", ""]]}, {"id": "2005.00042", "submitter": "Maharshi Pandya", "authors": "Maharshi R. Pandya, Jessica Reyes, Bob Vanderheyden", "title": "Method for Customizable Automated Tagging: Addressing the Problem of\n  Over-tagging and Under-tagging Text Documents", "comments": "Work done by Maharshi R. Pandya and Jessica Reyes as IBM interns\n  under leadership of Bob Vanderheyden. Article to be published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using author provided tags to predict tags for a new document often results\nin the overgeneration of tags. In the case where the author doesn't provide any\ntags, our documents face the severe under-tagging issue. In this paper, we\npresent a method to generate a universal set of tags that can be applied widely\nto a large document corpus. Using IBM Watson's NLU service, first, we collect\nkeywords/phrases that we call \"complex document tags\" from 8,854 popular\nreports in the corpus. We apply LDA model over these complex document tags to\ngenerate a set of 765 unique \"simple tags\". In applying the tags to a corpus of\ndocuments, we run each document through the IBM Watson NLU and apply\nappropriate simple tags. Using only 765 simple tags, our method allows us to\ntag 87,397 out of 88,583 total documents in the corpus with at least one tag.\nAbout 92.1% of the total 87,397 documents are also determined to be\nsufficiently-tagged. In the end, we discuss the performance of our method and\nits limitations.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 18:28:42 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Pandya", "Maharshi R.", ""], ["Reyes", "Jessica", ""], ["Vanderheyden", "Bob", ""]]}, {"id": "2005.00119", "submitter": "Raviteja Anantha", "authors": "Raviteja Anantha, Srinivas Chappidi, and William Dawoodi", "title": "Learning to Rank Intents in Voice Assistants", "comments": "11 pages, 7 figures, 2 tables, accepted at IWSDS 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice Assistants aim to fulfill user requests by choosing the best intent\nfrom multiple options generated by its Automated Speech Recognition and Natural\nLanguage Understanding sub-systems. However, voice assistants do not always\nproduce the expected results. This can happen because voice assistants choose\nfrom ambiguous intents - user-specific or domain-specific contextual\ninformation reduces the ambiguity of the user request. Additionally the user\ninformation-state can be leveraged to understand how relevant/executable a\nspecific intent is for a user request. In this work, we propose a novel\nEnergy-based model for the intent ranking task, where we learn an affinity\nmetric and model the trade-off between extracted meaning from speech utterances\nand relevance/executability aspects of the intent. Furthermore we present a\nMultisource Denoising Autoencoder based pretraining that is capable of learning\nfused representations of data from multiple sources. We empirically show our\napproach outperforms existing state of the art methods by reducing the\nerror-rate by 3.8%, which in turn reduces ambiguity and eliminates undesired\ndead-ends leading to better user experience. Finally, we evaluate the\nrobustness of our algorithm on the intent ranking task and show our algorithm\nimproves the robustness by 33.3%.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 21:51:26 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 03:19:07 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Anantha", "Raviteja", ""], ["Chappidi", "Srinivas", ""], ["Dawoodi", "William", ""]]}, {"id": "2005.00152", "submitter": "Gong Cheng", "authors": "Junyou Li, Gong Cheng, Qingxia Liu, Wen Zhang, Evgeny Kharlamov, Kalpa\n  Gunaratna, Huajun Chen", "title": "Neural Entity Summarization with Joint Encoding and Weak Supervision", "comments": "7 pages, accepted to IJCAI-PRICAI 2020 The paper is temporarily\n  withdrawn due to company policies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a large-scale knowledge graph (KG), an entity is often described by a\nlarge number of triple-structured facts. Many applications require abridged\nversions of entity descriptions, called entity summaries. Existing solutions to\nentity summarization are mainly unsupervised. In this paper, we present a\nsupervised approach NEST that is based on our novel neural model to jointly\nencode graph structure and text in KGs and generate high-quality diversified\nsummaries. Since it is costly to obtain manually labeled summaries for\ntraining, our supervision is weak as we train with programmatically labeled\ndata which may contain noise but is free of manual work. Evaluation results\nshow that our approach significantly outperforms the state of the art on two\npublic benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 00:14:08 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 08:30:29 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Li", "Junyou", ""], ["Cheng", "Gong", ""], ["Liu", "Qingxia", ""], ["Zhang", "Wen", ""], ["Kharlamov", "Evgeny", ""], ["Gunaratna", "Kalpa", ""], ["Chen", "Huajun", ""]]}, {"id": "2005.00153", "submitter": "Gong Cheng", "authors": "Shuxin Li, Zixian Huang, Gong Cheng, Evgeny Kharlamov, Kalpa Gunaratna", "title": "Enriching Documents with Compact, Representative, Relevant Knowledge\n  Graphs", "comments": "7 pages, accepted to IJCAI-PRICAI 2020. The paper is temporarily\n  withdrawn due to company policies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A prominent application of knowledge graph (KG) is document enrichment.\nExisting methods identify mentions of entities in a background KG and enrich\ndocuments with entity types and direct relations. We compute an entity relation\nsubgraph (ERG) that can more expressively represent indirect relations among a\nset of mentioned entities. To find compact, representative, and relevant ERGs\nfor effective enrichment, we propose an efficient best-first search algorithm\nto solve a new combinatorial optimization problem that achieves a trade-off\nbetween representativeness and compactness, and then we exploit ontological\nknowledge to rank ERGs by entity-based document-KG and intra-KG relevance.\nExtensive experiments and user studies show the promising performance of our\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 00:18:31 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 08:29:28 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Li", "Shuxin", ""], ["Huang", "Zixian", ""], ["Cheng", "Gong", ""], ["Kharlamov", "Evgeny", ""], ["Gunaratna", "Kalpa", ""]]}, {"id": "2005.00158", "submitter": "Mohammed Belkhatir", "authors": "M. Maree, M. Belkhatir", "title": "On the Merging of Domain-Specific Heterogeneous Ontologies using Wordnet\n  and Web Pattern-based Queries", "comments": null, "journal-ref": null, "doi": "10.1142/S0219649211002808", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontologies form the basic interest in various computer science disciplines\nsuch as semantic web, information retrieval, database design, etc. They aim at\nproviding a formal, explicit and shared conceptualization and understanding of\ncommon domains between different communities. In addition, they allow for\nconcepts and their constraints of a specific domain to be explicitly defined.\nHowever, the distributed nature of ontology development and the differences in\nviewpoints of the ontology engineers have resulted in the so called \"semantic\nheterogeneity\" between ontologies. Semantic heterogeneity constitutes the major\nobstacle against achieving interoperability between ontologies. To overcome\nthis obstacle, we present a multi-purpose framework which exploits the WordNet\ngeneric knowledge base for: i) Discovering and correcting the incorrect\nsemantic relations between the concepts of the ontology in a specific domain.\nThis step is a primary step of ontology merging. ii) Merging domain-specific\nontologies through computing semantic relations between their concepts. iii)\nHandling the issue of missing concepts in WordNet through the acquisition of\nstatistical information on the Web. And iv) Enriching WordNet with these\nmissing concepts. An experimental instantiation of the framework and\ncomparisons with state-of-the-art syntactic and semantic-based systems validate\nour proposal.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 05:03:50 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Maree", "M.", ""], ["Belkhatir", "M.", ""]]}, {"id": "2005.00171", "submitter": "Muhao Chen", "authors": "Muhao Chen, Weijia Shi, Ben Zhou, Dan Roth", "title": "Cross-lingual Entity Alignment with Incidental Supervision", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much research effort has been put to multilingual knowledge graph (KG)\nembedding methods to address the entity alignment task, which seeks to match\nentities in different languagespecific KGs that refer to the same real-world\nobject. Such methods are often hindered by the insufficiency of seed alignment\nprovided between KGs. Therefore, we propose an incidentally supervised model,\nJEANS , which jointly represents multilingual KGs and text corpora in a shared\nembedding scheme, and seeks to improve entity alignment with incidental\nsupervision signals from text. JEANS first deploys an entity grounding process\nto combine each KG with the monolingual text corpus. Then, two learning\nprocesses are conducted: (i) an embedding learning process to encode the KG and\ntext of each language in one embedding space, and (ii) a selflearning based\nalignment learning process to iteratively induce the matching of entities and\nthat of lexemes between embeddings. Experiments on benchmark datasets show that\nJEANS leads to promising improvement on entity alignment with incidental\nsupervision, and significantly outperforms state-of-the-art methods that solely\nrely on internal information of KGs.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 01:53:56 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 05:15:45 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Chen", "Muhao", ""], ["Shi", "Weijia", ""], ["Zhou", "Ben", ""], ["Roth", "Dan", ""]]}, {"id": "2005.00357", "submitter": "Soujanya Poria", "authors": "Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Rada Mihalcea", "title": "Beneath the Tip of the Iceberg: Current Challenges and New Directions in\n  Sentiment Analysis Research", "comments": "Published in the IEEE Transactions on Affective Computing (TAFFC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Sentiment analysis as a field has come a long way since it was first\nintroduced as a task nearly 20 years ago. It has widespread commercial\napplications in various domains like marketing, risk management, market\nresearch, and politics, to name a few. Given its saturation in specific\nsubtasks -- such as sentiment polarity classification -- and datasets, there is\nan underlying perception that this field has reached its maturity. In this\narticle, we discuss this perception by pointing out the shortcomings and\nunder-explored, yet key aspects of this field that are necessary to attain true\nsentiment understanding. We analyze the significant leaps responsible for its\ncurrent relevance. Further, we attempt to chart a possible course for this\nfield that covers many overlooked and unanswered questions.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 13:05:23 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 13:39:24 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 00:57:08 GMT"}, {"version": "v4", "created": "Wed, 11 Nov 2020 15:45:25 GMT"}, {"version": "v5", "created": "Mon, 16 Nov 2020 15:21:22 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Poria", "Soujanya", ""], ["Hazarika", "Devamanyu", ""], ["Majumder", "Navonil", ""], ["Mihalcea", "Rada", ""]]}, {"id": "2005.00372", "submitter": "Navid Rekabsaz", "authors": "Navid Rekabsaz, Markus Schedl", "title": "Do Neural Ranking Models Intensify Gender Bias?", "comments": "In Proceedings of ACM SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401280", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concerns regarding the footprint of societal biases in information retrieval\n(IR) systems have been raised in several previous studies. In this work, we\nexamine various recent IR models from the perspective of the degree of gender\nbias in their retrieval results. To this end, we first provide a bias\nmeasurement framework which includes two metrics to quantify the degree of the\nunbalanced presence of gender-related concepts in a given IR model's ranking\nlist. To examine IR models by means of the framework, we create a dataset of\nnon-gendered queries, selected by human annotators. Applying these queries to\nthe MS MARCO Passage retrieval collection, we then measure the gender bias of a\nBM25 model and several recent neural ranking models. The results show that\nwhile all models are strongly biased toward male, the neural models, and in\nparticular the ones based on contextualized embedding models, significantly\nintensify gender bias. Our experiments also show an overall increase in the\ngender bias of neural models when they exploit transfer learning, namely when\nthey use (already biased) pre-trained embeddings.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 13:31:11 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 08:10:14 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 13:11:59 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Rekabsaz", "Navid", ""], ["Schedl", "Markus", ""]]}, {"id": "2005.00436", "submitter": "Ying Luo", "authors": "Ying Luo and Hai Zhao", "title": "Bipartite Flat-Graph Network for Nested Named Entity Recognition", "comments": "Accepted by ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel bipartite flat-graph network (BiFlaG) for\nnested named entity recognition (NER), which contains two subgraph modules: a\nflat NER module for outermost entities and a graph module for all the entities\nlocated in inner layers. Bidirectional LSTM (BiLSTM) and graph convolutional\nnetwork (GCN) are adopted to jointly learn flat entities and their inner\ndependencies. Different from previous models, which only consider the\nunidirectional delivery of information from innermost layers to outer ones (or\noutside-to-inside), our model effectively captures the bidirectional\ninteraction between them. We first use the entities recognized by the flat NER\nmodule to construct an entity graph, which is fed to the next graph module. The\nricher representation learned from graph module carries the dependencies of\ninner entities and can be exploited to improve outermost entity predictions.\nExperimental results on three standard nested NER datasets demonstrate that our\nBiFlaG outperforms previous state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 15:14:22 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Luo", "Ying", ""], ["Zhao", "Hai", ""]]}, {"id": "2005.00468", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Iria da Cunha, Juan-Manuel Torres-Moreno", "title": "Automatic Discourse Segmentation: Review and Perspectives", "comments": "5 pages, 1 figure", "journal-ref": "International Workshop on African Human Language Technologies.\n  17-20 Jan 2010", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual discourse parsing is a very prominent research topic. The first\nstage for discourse parsing is discourse segmentation. The study reported in\nthis article addresses a review of two on-line available discourse segmenters\n(for English and Portuguese). We evaluate the possibility of developing similar\ndiscourse segmenters for Spanish, French and African languages.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 16:03:37 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["da Cunha", "Iria", ""], ["Torres-Moreno", "Juan-Manuel", ""]]}, {"id": "2005.00485", "submitter": "Josiane Mothe", "authors": "Bernard Dousset and Josiane Mothe", "title": "Getting Insights from a Large Corpus of Scientific Papers on\n  Specialisted Comprehensive Topics -- the Case of COVID-19", "comments": "14 pages; 5 figures and 3 tables submitted to KES 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 is one of the most important topic these days, specifically on\nsearch engines and news. While fake news are easily shared, scientific papers\nare reliable sources where information can be extracted. With about 24,000\nscientific publications on COVID-19 and related research on PUBMED, automatic\ncomputer-assisted analysis is required. In this paper, we develop two\nmethodologies to get insights on specific sub-topics of interest and latest\nresearch sub-topics. They rely on natural language processing and graph-based\nvisualizations. We run these methodologies on two cases: the virus origin and\nthe uses of existing drugs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:59:25 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Dousset", "Bernard", ""], ["Mothe", "Josiane", ""]]}, {"id": "2005.00512", "submitter": "Sarthak Jain", "authors": "Sarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi, Iz Beltagy", "title": "SciREX: A Challenge Dataset for Document-Level Information Extraction", "comments": "ACL2020 Camera Ready Submission, Work done by first authors while\n  interning at AI2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting information from full documents is an important problem in many\ndomains, but most previous work focus on identifying relationships within a\nsentence or a paragraph. It is challenging to create a large-scale information\nextraction (IE) dataset at the document level since it requires an\nunderstanding of the whole document to annotate entities and their\ndocument-level relationships that usually span beyond sentences or even\nsections. In this paper, we introduce SciREX, a document level IE dataset that\nencompasses multiple IE tasks, including salient entity identification and\ndocument level $N$-ary relation identification from scientific articles. We\nannotate our dataset by integrating automatic and human annotations, leveraging\nexisting scientific knowledge resources. We develop a neural model as a strong\nbaseline that extends previous state-of-the-art IE models to document-level IE.\nAnalyzing the model performance shows a significant gap between human\nperformance and current baselines, inviting the community to use our dataset as\na challenge to develop document-level IE models. Our data and code are publicly\navailable at https://github.com/allenai/SciREX\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 17:30:10 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Jain", "Sarthak", ""], ["van Zuylen", "Madeleine", ""], ["Hajishirzi", "Hannaneh", ""], ["Beltagy", "Iz", ""]]}, {"id": "2005.00517", "submitter": "Chetan Bansal", "authors": "Chetan Bansal, Pantazis Deligiannis, Chandra Maddila, Nikitha Rao", "title": "Studying Ransomware Attacks Using Web Search Logs", "comments": "To appear in the proceedings of SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401189", "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber attacks are increasingly becoming prevalent and causing significant\ndamage to individuals, businesses and even countries. In particular, ransomware\nattacks have grown significantly over the last decade. We do the first study on\nmining insights about ransomware attacks by analyzing query logs from Bing web\nsearch engine. We first extract ransomware related queries and then build a\nmachine learning model to identify queries where users are seeking support for\nransomware attacks. We show that user search behavior and characteristics are\ncorrelated with ransomware attacks. We also analyse trends in the temporal and\ngeographical space and validate our findings against publicly available\ninformation. Lastly, we do a case study on 'Nemty', a popular ransomware, to\nshow that it is possible to derive accurate insights about cyber attacks by\nquery log analysis.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 17:44:42 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 18:38:01 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Bansal", "Chetan", ""], ["Deligiannis", "Pantazis", ""], ["Maddila", "Chandra", ""], ["Rao", "Nikitha", ""]]}, {"id": "2005.00624", "submitter": "Yu Zhang", "authors": "Yu Zhang, Yu Meng, Jiaxin Huang, Frank F. Xu, Xuan Wang, Jiawei Han", "title": "Minimally Supervised Categorization of Text with Metadata", "comments": "10 pages; Accepted to SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document categorization, which aims to assign a topic label to each document,\nplays a fundamental role in a wide variety of applications. Despite the success\nof existing studies in conventional supervised document classification, they\nare less concerned with two real problems: (1) \\textit{the presence of\nmetadata}: in many domains, text is accompanied by various additional\ninformation such as authors and tags. Such metadata serve as compelling topic\nindicators and should be leveraged into the categorization framework; (2)\n\\textit{label scarcity}: labeled training samples are expensive to obtain in\nsome cases, where categorization needs to be performed using only a small set\nof annotated data. In recognition of these two challenges, we propose\n\\textsc{MetaCat}, a minimally supervised framework to categorize text with\nmetadata. Specifically, we develop a generative process describing the\nrelationships between words, documents, labels, and metadata. Guided by the\ngenerative model, we embed text and metadata into the same semantic space to\nencode heterogeneous signals. Then, based on the same generative process, we\nsynthesize training samples to address the bottleneck of label scarcity. We\nconduct a thorough evaluation on a wide range of datasets. Experimental results\nprove the effectiveness of \\textsc{MetaCat} over many competitive baselines.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 21:42:32 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 06:59:54 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Zhang", "Yu", ""], ["Meng", "Yu", ""], ["Huang", "Jiaxin", ""], ["Xu", "Frank F.", ""], ["Wang", "Xuan", ""], ["Han", "Jiawei", ""]]}, {"id": "2005.00625", "submitter": "Zhiwei Liu", "authors": "Zhiwei Liu, Yingtong Dou, Philip S. Yu, Yutong Deng, Hao Peng", "title": "Alleviating the Inconsistency Problem of Applying Graph Neural Network\n  to Fraud Detection", "comments": "Accepted by SIGIR'20. We also released a GNN-based fraud detection\n  toolbox with implementations of SOTA models. The code is available at\n  https://github.com/safe-graph/DGFraud", "journal-ref": null, "doi": "10.1145/3397271.3401253", "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph-based model can help to detect suspicious fraud online. Owing to\nthe development of Graph Neural Networks~(GNNs), prior research work has\nproposed many GNN-based fraud detection frameworks based on either homogeneous\ngraphs or heterogeneous graphs. These work follow the existing GNN framework by\naggregating the neighboring information to learn the node embedding, which lays\non the assumption that the neighbors share similar context, features, and\nrelations. However, the inconsistency problem is hardly investigated, i.e., the\ncontext inconsistency, feature inconsistency, and relation inconsistency. In\nthis paper, we introduce these inconsistencies and design a new GNN framework,\n$\\mathsf{GraphConsis}$, to tackle the inconsistency problem: (1) for the\ncontext inconsistency, we propose to combine the context embeddings with node\nfeatures, (2) for the feature inconsistency, we design a consistency score to\nfilter the inconsistent neighbors and generate corresponding sampling\nprobability, and (3) for the relation inconsistency, we learn a relation\nattention weights associated with the sampled nodes. Empirical analysis on four\ndatasets indicates the inconsistency problem is crucial in a fraud detection\ntask. The extensive experiments prove the effectiveness of\n$\\mathsf{GraphConsis}$. We also released a GNN-based fraud detection toolbox\nwith implementations of SOTA models. The code is available at\nhttps://github.com/safe-graph/DGFraud.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 21:43:58 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 21:05:30 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 03:24:05 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Liu", "Zhiwei", ""], ["Dou", "Yingtong", ""], ["Yu", "Philip S.", ""], ["Deng", "Yutong", ""], ["Peng", "Hao", ""]]}, {"id": "2005.00702", "submitter": "Asad Mahmood", "authors": "Asad Mahmood, Zubair Shafiq and Padmini Srinivasan", "title": "A Girl Has A Name: Detecting Authorship Obfuscation", "comments": "9 pages, 4 figures, 2 tables, ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship attribution aims to identify the author of a text based on the\nstylometric analysis. Authorship obfuscation, on the other hand, aims to\nprotect against authorship attribution by modifying a text's style. In this\npaper, we evaluate the stealthiness of state-of-the-art authorship obfuscation\nmethods under an adversarial threat model. An obfuscator is stealthy to the\nextent an adversary finds it challenging to detect whether or not a text\nmodified by the obfuscator is obfuscated - a decision that is key to the\nadversary interested in authorship attribution. We show that the existing\nauthorship obfuscation methods are not stealthy as their obfuscated texts can\nbe identified with an average F1 score of 0.87. The reason for the lack of\nstealthiness is that these obfuscators degrade text smoothness, as ascertained\nby neural language models, in a detectable manner. Our results highlight the\nneed to develop stealthy authorship obfuscation methods that can better protect\nthe identity of an author seeking anonymity.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 04:52:55 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Mahmood", "Asad", ""], ["Shafiq", "Zubair", ""], ["Srinivasan", "Padmini", ""]]}, {"id": "2005.00705", "submitter": "Thuy Vu", "authors": "Thuy Vu and Alessandro Moschitti", "title": "AVA: an Automatic eValuation Approach to Question Answering Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce AVA, an automatic evaluation approach for Question Answering,\nwhich given a set of questions associated with Gold Standard answers, can\nestimate system Accuracy. AVA uses Transformer-based language models to encode\nquestion, answer, and reference text. This allows for effectively measuring the\nsimilarity between the reference and an automatic answer, biased towards the\nquestion semantics. To design, train and test AVA, we built multiple large\ntraining, development, and test sets on both public and industrial benchmarks.\nOur innovative solutions achieve up to 74.7% in F1 score in predicting human\njudgement for single answers. Additionally, AVA can be used to evaluate the\noverall system Accuracy with an RMSE, ranging from 0.02 to 0.09, depending on\nthe availability of multiple references.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 05:00:16 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Vu", "Thuy", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "2005.00743", "submitter": "Yi Tay", "authors": "Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, Che Zheng", "title": "Synthesizer: Rethinking Self-Attention in Transformer Models", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dot product self-attention is known to be central and indispensable to\nstate-of-the-art Transformer models. But is it really required? This paper\ninvestigates the true importance and contribution of the dot product-based\nself-attention mechanism on the performance of Transformer models. Via\nextensive experiments, we find that (1) random alignment matrices surprisingly\nperform quite competitively and (2) learning attention weights from token-token\n(query-key) interactions is useful but not that important after all. To this\nend, we propose \\textsc{Synthesizer}, a model that learns synthetic attention\nweights without token-token interactions. In our experiments, we first show\nthat simple Synthesizers achieve highly competitive performance when compared\nagainst vanilla Transformer models across a range of tasks, including machine\ntranslation, language modeling, text generation and GLUE/SuperGLUE benchmarks.\nWhen composed with dot product attention, we find that Synthesizers\nconsistently outperform Transformers. Moreover, we conduct additional\ncomparisons of Synthesizers against Dynamic Convolutions, showing that simple\nRandom Synthesizer is not only $60\\%$ faster but also improves perplexity by a\nrelative $3.5\\%$. Finally, we show that simple factorized Synthesizers can\noutperform Linformers on encoding only tasks.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 08:16:19 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 12:16:06 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 12:19:35 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Tay", "Yi", ""], ["Bahri", "Dara", ""], ["Metzler", "Donald", ""], ["Juan", "Da-Cheng", ""], ["Zhao", "Zhe", ""], ["Zheng", "Che", ""]]}, {"id": "2005.00848", "submitter": "Francis Wolinski", "authors": "Francis Wolinski", "title": "Visualization of Diseases at Risk in the COVID-19 Literature", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a project, named VIDAR-19, able to extract automatically\ndiseases from the CORD-19 dataset, and also diseases which might be considered\nas risk factors. The project relies on the ICD-11 classification of diseases\nmaintained by the WHO. This nomenclature is used as a data source of the\nextraction mechanism, and also as the repository for the results. Developed for\nthe COVID-19, the project has the ability to extract diseases at risk and to\ncalculate relevant indicators. The outcome of the project is presented in a\ndashboard which enables the user to explore graphically diseases at risk which\nare put back in the classification hierarchy. Beyond the COVID-19, VIDAR has\nmuch broader applications and might be directly used for any corpus dealing\nwith other pathologies.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 15:04:54 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wolinski", "Francis", ""]]}, {"id": "2005.00950", "submitter": "Marija Stanojevic", "authors": "Quang Pham, Marija Stanojevic, Zoran Obradovic", "title": "Extracting Entities and Topics from News and Connecting Criminal Records", "comments": "This is a report submitted by an undergraduate student as preliminary\n  work on this problem", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to summarize methodologies used in extracting\nentities and topics from a database of criminal records and from a database of\nnewspapers. Statistical models had successfully been used in studying the\ntopics of roughly 300,000 New York Times articles. In addition, these models\nhad also been used to successfully analyze entities related to people,\norganizations, and places (D Newman, 2006). Additionally, analytical\napproaches, especially in hotspot mapping, were used in some researches with an\naim to predict crime locations and circumstances in the future, and those\napproaches had been tested quite successfully (S Chainey, 2008). Based on the\ntwo above notions, this research was performed with the intention to apply data\nscience techniques in analyzing a big amount of data, selecting valuable\nintelligence, clustering violations depending on their types of crime, and\ncreating a crime graph that changes through time. In this research, the task\nwas to download criminal datasets from Kaggle and a collection of news articles\nfrom Kaggle and EAGER project databases, and then to merge these datasets into\none general dataset. The most important goal of this project was performing\nstatistical and natural language processing methods to extract entities and\ntopics as well as to group similar data points into correct clusters, in order\nto understand public data about U.S related crimes better.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 00:06:01 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Pham", "Quang", ""], ["Stanojevic", "Marija", ""], ["Obradovic", "Zoran", ""]]}, {"id": "2005.01148", "submitter": "Masoud Mansoury", "authors": "Masoud Mansoury, Himan Abdollahpouri, Mykola Pechenizkiy, Bamshad\n  Mobasher, Robin Burke", "title": "FairMatch: A Graph-based Approach for Improving Aggregate Diversity in\n  Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are often biased toward popular items. In other words,\nfew items are frequently recommended while the majority of items do not get\nproportionate attention. That leads to low coverage of items in recommendation\nlists across users (i.e. low aggregate diversity) and unfair distribution of\nrecommended items. In this paper, we introduce FairMatch, a general graph-based\nalgorithm that works as a post-processing approach after recommendation\ngeneration for improving aggregate diversity. The algorithm iteratively finds\nitems that are rarely recommended yet are high-quality and add them to the\nusers' final recommendation lists. This is done by solving the maximum flow\nproblem on the recommendation bipartite graph. While we focus on aggregate\ndiversity and fair distribution of recommended items, the algorithm can be\nadapted to other recommendation scenarios using different underlying\ndefinitions of fairness. A comprehensive set of experiments on two datasets and\ncomparison with state-of-the-art baselines show that FairMatch, while\nsignificantly improving aggregate diversity, provides comparable recommendation\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 17:21:31 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Mansoury", "Masoud", ""], ["Abdollahpouri", "Himan", ""], ["Pechenizkiy", "Mykola", ""], ["Mobasher", "Bamshad", ""], ["Burke", "Robin", ""]]}, {"id": "2005.01158", "submitter": "Gerard de Melo", "authors": "Kshitij Shah, Gerard de Melo", "title": "Correcting the Autocorrect: Context-Aware Typographical Error Correction\n  via Training Data Augmentation", "comments": "Accepted for publication at LREC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the artificial generation of typographical errors\nbased on real-world statistics. We first draw on a small set of annotated data\nto compute spelling error statistics. These are then invoked to introduce\nerrors into substantially larger corpora. The generation methodology allows us\nto generate particularly challenging errors that require context-aware error\ndetection. We use it to create a set of English language error detection and\ncorrection datasets. Finally, we examine the effectiveness of machine learning\nmodels for detecting and correcting errors based on this data. The datasets are\navailable at http://typo.nlproc.org\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 18:08:17 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Shah", "Kshitij", ""], ["de Melo", "Gerard", ""]]}, {"id": "2005.01177", "submitter": "Cristina Espa\\~na-Bonet", "authors": "Cristina Espa\\~na-Bonet, Alberto Barr\\'on-Cede\\~no and Llu\\'is\n  M\\`arquez", "title": "Tailoring and Evaluating the Wikipedia for in-Domain Comparable Corpora\n  Extraction", "comments": "26 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automatic language-independent graph-based method to build\n\\`a-la-carte article collections on user-defined domains from the Wikipedia.\nThe core model is based on the exploration of the encyclopaedia's category\ngraph and can produce both monolingual and multilingual comparable collections.\nWe run thorough experiments to assess the quality of the obtained corpora in 10\nlanguages and 743 domains. According to an extensive manual evaluation, our\ngraph-based model outperforms a retrieval-based approach and reaches an average\nprecision of 84% on in-domain articles. As manual evaluations are costly, we\nintroduce the concept of \"domainness\" and design several automatic metrics to\naccount for the quality of the collections. Our best metric for domainness\nshows a strong correlation with the human-judged precision, representing a\nreasonable automatic alternative to assess the quality of domain-specific\ncorpora. We release the WikiTailor toolkit with the implementation of the\nextraction methods, the evaluation measures and several utilities. WikiTailor\nmakes obtaining multilingual in-domain data from the Wikipedia easy.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 20:08:39 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Espa\u00f1a-Bonet", "Cristina", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["M\u00e0rquez", "Llu\u00eds", ""]]}, {"id": "2005.01218", "submitter": "Vikas Yadav", "authors": "Vikas Yadav, Steven Bethard and Mihai Surdeanu", "title": "Unsupervised Alignment-based Iterative Evidence Retrieval for Multi-hop\n  Question Answering", "comments": "Accepted at ACL 2020 as a long conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence retrieval is a critical stage of question answering (QA), necessary\nnot only to improve performance, but also to explain the decisions of the\ncorresponding QA method. We introduce a simple, fast, and unsupervised\niterative evidence retrieval method, which relies on three ideas: (a) an\nunsupervised alignment approach to soft-align questions and answers with\njustification sentences using only GloVe embeddings, (b) an iterative process\nthat reformulates queries focusing on terms that are not covered by existing\njustifications, which (c) a stopping criterion that terminates retrieval when\nthe terms in the given question and candidate answers are covered by the\nretrieved justifications. Despite its simplicity, our approach outperforms all\nthe previous methods (including supervised methods) on the evidence selection\ntask on two datasets: MultiRC and QASC. When these evidence sentences are fed\ninto a RoBERTa answer classification component, we achieve state-of-the-art QA\nperformance on these two datasets.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 00:19:48 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Yadav", "Vikas", ""], ["Bethard", "Steven", ""], ["Surdeanu", "Mihai", ""]]}, {"id": "2005.01389", "submitter": "Elwin Huaman", "authors": "Elwin Huaman, Elias K\\\"arle, Dieter Fensel", "title": "Knowledge Graph Validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KGs) have shown to be an important asset of large companies\nlike Google and Microsoft. KGs play an important role in providing structured\nand semantically rich information, making them available to people and\nmachines, and supplying accurate, correct and reliable knowledge. To do so a\ncritical task is knowledge validation, which measures whether statements from\nKGs are semantically correct and correspond to the so-called \"real\" world. In\nthis paper, we provide an overview and review of the state-of-the-art\napproaches, methods and tools on knowledge validation for KGs, as well as an\nevaluation of them. As a result, we demonstrate a lack of reproducibility of\ntools results, give insights, and state our future research direction.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 11:09:11 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Huaman", "Elwin", ""], ["K\u00e4rle", "Elias", ""], ["Fensel", "Dieter", ""]]}, {"id": "2005.01535", "submitter": "An-Zi Yen", "authors": "An-Zi Yen, Hen-Hsen Huang, Hsin-Hsi Chen", "title": "Ten Questions in Lifelog Mining and Information Recall", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advance of science and technology, people are used to record their\ndaily life events via writing blogs, uploading social media posts, taking\nphotos, or filming videos. Such rich repository personal information is useful\nfor supporting human living assistance. The main challenge is how to store and\nmanage personal knowledge from various sources. In this position paper, we\npropose a research agenda on mining personal knowledge from various sources of\nlifelogs, personal knowledge base construction, and information recall for\nassisting people to recall their experiences.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 14:48:53 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Yen", "An-Zi", ""], ["Huang", "Hen-Hsen", ""], ["Chen", "Hsin-Hsi", ""]]}, {"id": "2005.01573", "submitter": "Fei Mi", "authors": "Fei Mi, Boi Faltings", "title": "Memory Augmented Neural Model for Incremental Session-based\n  Recommendation", "comments": "Accepted as a full paper at IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing concerns with privacy have stimulated interests in Session-based\nRecommendation (SR) using no personal data other than what is observed in the\ncurrent browser session. Existing methods are evaluated in static settings\nwhich rarely occur in real-world applications. To better address the dynamic\nnature of SR tasks, we study an incremental SR scenario, where new items and\npreferences appear continuously. We show that existing neural recommenders can\nbe used in incremental SR scenarios with small incremental updates to alleviate\ncomputation overhead and catastrophic forgetting. More importantly, we propose\na general framework called Memory Augmented Neural model (MAN). MAN augments a\nbase neural recommender with a continuously queried and updated nonparametric\nmemory, and the predictions from the neural and the memory components are\ncombined through another lightweight gating network. We empirically show that\nMAN is well-suited for the incremental SR task, and it consistently outperforms\nstate-of-the-art neural and nonparametric methods. We analyze the results and\ndemonstrate that it is particularly good at incrementally learning preferences\non new and infrequent items.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 19:07:20 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Mi", "Fei", ""], ["Faltings", "Boi", ""]]}, {"id": "2005.01583", "submitter": "Benjamin Lee", "authors": "Benjamin Charles Germain Lee, Jaime Mears, Eileen Jakeway, Meghan\n  Ferriter, Chris Adams, Nathan Yarasavage, Deborah Thomas, Kate Zwaard, Daniel\n  S. Weld", "title": "The Newspaper Navigator Dataset: Extracting And Analyzing Visual Content\n  from 16 Million Historic Newspaper Pages in Chronicling America", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Chronicling America is a product of the National Digital Newspaper Program, a\npartnership between the Library of Congress and the National Endowment for the\nHumanities to digitize historic newspapers. Over 16 million pages of historic\nAmerican newspapers have been digitized for Chronicling America to date,\ncomplete with high-resolution images and machine-readable METS/ALTO OCR. Of\nconsiderable interest to Chronicling America users is a semantified corpus,\ncomplete with extracted visual content and headlines. To accomplish this, we\nintroduce a visual content recognition model trained on bounding box\nannotations of photographs, illustrations, maps, comics, and editorial cartoons\ncollected as part of the Library of Congress's Beyond Words crowdsourcing\ninitiative and augmented with additional annotations including those of\nheadlines and advertisements. We describe our pipeline that utilizes this deep\nlearning model to extract 7 classes of visual content: headlines, photographs,\nillustrations, maps, comics, editorial cartoons, and advertisements, complete\nwith textual content such as captions derived from the METS/ALTO OCR, as well\nas image embeddings for fast image similarity querying. We report the results\nof running the pipeline on 16.3 million pages from the Chronicling America\ncorpus and describe the resulting Newspaper Navigator dataset, the largest\ndataset of extracted visual content from historic newspapers ever produced. The\nNewspaper Navigator dataset, finetuned visual content recognition model, and\nall source code are placed in the public domain for unrestricted re-use.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 15:51:13 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Lee", "Benjamin Charles Germain", ""], ["Mears", "Jaime", ""], ["Jakeway", "Eileen", ""], ["Ferriter", "Meghan", ""], ["Adams", "Chris", ""], ["Yarasavage", "Nathan", ""], ["Thomas", "Deborah", ""], ["Zwaard", "Kate", ""], ["Weld", "Daniel S.", ""]]}, {"id": "2005.01618", "submitter": "Ruiyi Zhang", "authors": "Ruiyi Zhang, Tong Yu, Yilin Shen, Hongxia Jin, Changyou Chen, Lawrence\n  Carin", "title": "Reward Constrained Interactive Recommendation with Natural Language\n  Feedback", "comments": "Appeared in NeurIPS 2019; Updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-based interactive recommendation provides richer user feedback and has\ndemonstrated advantages over traditional interactive recommender systems.\nHowever, recommendations can easily violate preferences of users from their\npast natural-language feedback, since the recommender needs to explore new\nitems for further improvement. To alleviate this issue, we propose a novel\nconstraint-augmented reinforcement learning (RL) framework to efficiently\nincorporate user preferences over time. Specifically, we leverage a\ndiscriminator to detect recommendations violating user historical preference,\nwhich is incorporated into the standard RL objective of maximizing expected\ncumulative future rewards. Our proposed framework is general and is further\nextended to the task of constrained text generation. Empirical results show\nthat the proposed method yields consistent improvement relative to standard RL\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 16:23:34 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Zhang", "Ruiyi", ""], ["Yu", "Tong", ""], ["Shen", "Yilin", ""], ["Jin", "Hongxia", ""], ["Chen", "Changyou", ""], ["Carin", "Lawrence", ""]]}, {"id": "2005.01637", "submitter": "Dorothea Iglezakis", "authors": "Bj\\\"orn Schembera and Dorothea Iglezakis", "title": "EngMeta -- Metadata for Computational Engineering", "comments": "21 pages, 6 figures. Preprint submitted to International Journal of\n  Metadata, Semantics and Ontologies", "journal-ref": "International Journal of Metadata, Semantics and Ontologies 14\n  (2020), 26-38", "doi": "10.1504/IJMSO.2020.107792", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational engineering generates knowledge through the analysis and\ninterpretation of research data, which is produced by computer simulation.\nSupercomputers produce huge amounts of research data. To address a research\nquestion, a lot of simulations are run over a large parameter space. Therefore,\nhandling this data and keeping an overview becomes a challenge. Data\ndocumentation is mostly handled by file and folder names in inflexible file\nsystems, making it almost impossible for data to be findable, accessible,\ninteropable and hence reusable. To enable and improve a structured\ndocumentation of research data from computational engineering, we developed\nEngMeta as a metadata model. We built this model by incorporating existing\nstandards for general descriptive and technical information and adding metadata\nfields for disciplinespecific information like the components and parameters of\nthe simulated target system and information about the research process like the\nused methods, software and computational environment. EngMeta functions, in\npractical use, as the descriptive core for an institutional repository. In\norder to reduce the burden of description on scientists, we have developed an\napproach for automatically extracting metadata information from the output and\nlog files of computer simulations. Through a qualitative analysis, we show that\nEngMeta fulfills the criteria of a good metadata model. Through a quantitative\nsurvey, we can show that it meets the needs of engineering scientists.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 16:50:59 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 14:34:18 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Schembera", "Bj\u00f6rn", ""], ["Iglezakis", "Dorothea", ""]]}, {"id": "2005.01716", "submitter": "Bahareh Sarrafzadeh", "authors": "Bahareh Sarrafzadeh, Adam Roegiest, and Edward Lank", "title": "Hierarchical Knowledge Graphs: A Novel Information Representation for\n  Exploratory Search Tasks", "comments": "35 Pages of main content, Extension of previous work published at\n  SigIR 2017, Currently under review at TOIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In exploratory search tasks, alongside information retrieval, information\nrepresentation is an important factor in sensemaking. In this paper, we explore\na multi-layer extension to knowledge graphs, hierarchical knowledge graphs\n(HKGs), that combines hierarchical and network visualizations into a unified\ndata representation asa tool to support exploratory search. We describe our\nalgorithm to construct these visualizations, analyze interaction logs to\nquantitatively demonstrate performance parity with networks and performance\nadvantages over hierarchies, and synthesize data from interaction logs,\ninterviews, and thinkalouds on a testbed data set to demonstrate the utility of\nthe unified hierarchy+network structure in our HKGs. Alongside the above study,\nwe perform an additional mixed methods analysis of the effect of precision and\nrecall on the performance of hierarchical knowledge graphs for two different\nexploratory search tasks. While the quantitative data shows a limited effect of\nprecision and recall on user performance and user effort, qualitative data\ncombined with post-hoc statistical analysis provides evidence that the type of\nexploratory search task (e.g., learning versus investigating) can be impacted\nby precision and recall. Furthermore, our qualitative analyses find that users\nare unable to perceive differences in the quality of extracted information. We\ndiscuss the implications of our results and analyze other factors that more\nsignificantly impact exploratory search performance in our experimental tasks.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 16:04:09 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Sarrafzadeh", "Bahareh", ""], ["Roegiest", "Adam", ""], ["Lank", "Edward", ""]]}, {"id": "2005.01805", "submitter": "Mark Loyman", "authors": "Mark Loyman and Hayit Greenspan", "title": "Semi-supervised lung nodule retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Content based image retrieval (CBIR) provides the clinician with visual\ninformation that can support, and hopefully improve, his or her decision making\nprocess. Given an input query image, a CBIR system provides as its output a set\nof images, ranked by similarity to the query image. Retrieved images may come\nwith relevant information, such as biopsy-based malignancy labeling, or\ncategorization. Ground truth on similarity between dataset elements (e.g.\nbetween nodules) is not readily available, thus greatly challenging machine\nlearning methods. Such annotations are particularly difficult to obtain, due to\nthe subjective nature of the task, with high inter-observer variability\nrequiring multiple expert annotators. Consequently, past approaches have\nfocused on manual feature extraction, while current approaches use auxiliary\ntasks, such as a binary classification task (e.g. malignancy), for which\nground-true is more readily accessible. However, in a previous study, we have\nshown that binary auxiliary tasks are inferior to the usage of a rough\nsimilarity estimate that are derived from data annotations. The current study\nsuggests a semi-supervised approach that involves two steps: 1) Automatic\nannotation of a given partially labeled dataset; 2) Learning a semantic\nsimilarity metric space based on the predicated annotations. The proposed\nsystem is demonstrated in lung nodule retrieval using the LIDC dataset, and\nshows that it is feasible to learn embedding from predicted ratings. The\nsemi-supervised approach has demonstrated a significantly higher discriminative\nability than the fully-unsupervised reference.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 19:26:14 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Loyman", "Mark", ""], ["Greenspan", "Hayit", ""]]}, {"id": "2005.02127", "submitter": "Mohammed Belkhatir", "authors": "Fariza Fauzi, Mohammed Belkhatir", "title": "Image understanding and the web", "comments": "Journal Intelligent Information Systems 2015", "journal-ref": null, "doi": "10.1007/s10844-014-0323-6", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contextual information of Web images is investigated to address the issue\nof characterizing their content with semantic descriptors and therefore bridge\nthe semantic gap, i.e. the gap between their automated low-level representation\nin terms of colors, textures, shapes. . . and their semantic interpretation.\nSuch characterization allows for understanding the image content and is crucial\nin important Web-based tasks such as image indexing and retrieval. Although we\nare highly motivated by the availability of rich knowledge on the Web and the\nrelative success achieved by commercial search engines in automatically\ncharacterizing the image content using contextual information in Web pages, we\nare aware that the unpredictable quality of the contextual information is a\nmajor limiting factor. Among the reasons explaining the difficulty to leverage\non the image contextual information, some problems are related to the\ncharacterization and extraction of this information. Indeed, the first issue is\nthe lack of large-scale studies to highlight what is considered the relevant\ncontextual information of an image, where it is located in a Web page and\nwhether it is consistent across Web pages of different types, content layouts\nand domains. Also, the matter related to the extraction of this contextual\ninformation is topical as state-of-the-art automated extraction tools are\nunable to handle the heterogeneous Web. As far as the processing of the\ncontextual information is concerned, problems linked to the syntactic and\nsemantic characterizations of the textual components are important to address\nin order to tackle the semantic gap. Furthermore, questions pertaining to the\norganization of these textual components into coherent structures that are\nusable in image indexing and retrieval frameworks shall arise.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 05:27:45 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Fauzi", "Fariza", ""], ["Belkhatir", "Mohammed", ""]]}, {"id": "2005.02149", "submitter": "Jan Zah\\'alka", "authors": "Jan Zah\\'alka, Marcel Worring and Jarke J. van Wijk", "title": "II-20: Intelligent and pragmatic analytic categorization of image\n  collections", "comments": "9 pages, 7 figures, 1 table. Camera-ready paper, to appear in IEEE\n  VIS 2020 and IEEE TVCG in January 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce II-20 (Image Insight 2020), a multimedia analytics approach for\nanalytic categorization of image collections. Advanced visualizations for image\ncollections exist, but they need tight integration with a machine model to\nsupport analytic categorization. Directly employing computer vision and\ninteractive learning techniques gravitates towards search. Analytic\ncategorization, however, is not machine classification (the difference between\nthe two is called the pragmatic gap): a human adds/redefines/deletes categories\nof relevance on the fly to build insight, whereas the machine classifier is\nrigid and non-adaptive. Analytic categorization that brings the user to insight\nrequires a flexible machine model that allows dynamic sliding on the\nexploration-search axis, as well as semantic interactions. II-20 brings 3 major\ncontributions to multimedia analytics on image collections and towards closing\nthe pragmatic gap. Firstly, a machine model that closely follows the user's\ninteractions and dynamically models her categories of relevance. II-20's model,\nin addition to matching and exceeding the state of the art w. r. t. relevance,\nallows the user to dynamically slide on the exploration-search axis without\nadditional input from her side. Secondly, the dynamic, 1-image-at-a-time Tetris\nmetaphor that synergizes with the model. It allows the model to analyze the\ncollection by itself with minimal interaction from the user and complements the\nclassic grid metaphor. Thirdly, the fast-forward interaction, allowing the user\nto harness the model to quickly expand (\"fast-forward\") the categories of\nrelevance, expands the multimedia analytics semantic interaction dictionary.\nAutomated experiments show that II-20's model outperforms the state of the art\nand also demonstrate Tetris's analytic quality. User studies confirm that II-20\nis an intuitive, efficient, and effective multimedia analytics tool.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 13:43:16 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 10:24:46 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 09:30:17 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Zah\u00e1lka", "Jan", ""], ["Worring", "Marcel", ""], ["van Wijk", "Jarke J.", ""]]}, {"id": "2005.02151", "submitter": "Vince Lyzinski", "authors": "Keith Levin, Carey E. Priebe, Vince Lyzinski", "title": "On the role of features in vertex nomination: Content and context\n  together are better (sometimes)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertex nomination is a lightly-supervised network information retrieval (IR)\ntask in which vertices of interest in one graph are used to query a second\ngraph to discover vertices of interest in the second graph. Similar to other IR\ntasks, the output of a vertex nomination scheme is a ranked list of the\nvertices in the second graph, with the heretofore unknown vertices of interest\nideally concentrating at the top of the list. Vertex nomination schemes provide\na useful suite of tools for efficiently mining complex networks for pertinent\ninformation. In this paper, we explore, both theoretically and practically, the\ndual roles of content (i.e., edge and vertex attributes) and context (i.e.,\nnetwork topology) in vertex nomination. We provide necessary and sufficient\nconditions under which vertex nomination schemes that leverage both content and\ncontext outperform schemes that leverage only content or context separately.\nWhile the joint utility of both content and context has been demonstrated\nempirically in the literature, the framework presented in this paper provides a\nnovel theoretical basis for understanding the potential complementary roles of\nnetwork features and topology.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:13:24 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 13:01:43 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Levin", "Keith", ""], ["Priebe", "Carey E.", ""], ["Lyzinski", "Vince", ""]]}, {"id": "2005.02154", "submitter": "Xiu-Shen Wei", "authors": "Benyi Hu, Ren-Jie Song, Xiu-Shen Wei, Yazhou Yao, Xian-Sheng Hua, and\n  Yuehu Liu", "title": "PyRetri: A PyTorch-based Library for Unsupervised Image Retrieval by\n  Deep Convolutional Neural Networks", "comments": "Accepted by ACM Multimedia Conference 2020. PyRetri is open-source\n  and available at https://github.com/PyRetri/PyRetri", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress of applying deep learning methods to the field\nof content-based image retrieval, there has not been a software library that\ncovers these methods in a unified manner. In order to fill this gap, we\nintroduce PyRetri, an open source library for deep learning based unsupervised\nimage retrieval. The library encapsulates the retrieval process in several\nstages and provides functionality that covers various prominent methods for\neach stage. The idea underlying its design is to provide a unified platform for\ndeep learning based image retrieval research, with high usability and\nextensibility. To the best of our knowledge, this is the first open-source\nlibrary for unsupervised image retrieval by deep learning.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 10:17:18 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 13:12:10 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Hu", "Benyi", ""], ["Song", "Ren-Jie", ""], ["Wei", "Xiu-Shen", ""], ["Yao", "Yazhou", ""], ["Hua", "Xian-Sheng", ""], ["Liu", "Yuehu", ""]]}, {"id": "2005.02156", "submitter": "Mohammed Belkhatir", "authors": "Fariza Fauzi, Mohammed Belkhatir", "title": "A User Study to Investigate Semantically Relevant Contextual Information\n  of WWW Images", "comments": null, "journal-ref": "International Journal Human Computer Studies 2010", "doi": "10.1016/j.ijhcs.2010.01.001", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contextual information of Web images is investigated to address the issue\nof enriching their index characterizations with semantic descriptors and\ntherefore bridge the semantic gap (i.e. the gap between the low-level\ncontent-based description of images and their semantic interpretation).\nAlthough we are highly motivated by the availability of rich knowledge on the\nWeb and the relative success achieved by commercial search engines in indexing\nimages using surrounding text-based information in webpages, we are aware that\nthe unpredictable quality of the surrounding text is a major limiting factor.\nIn order to improve its quality, we highlight contextual information which is\nrelevant for the semantic characterization of Web images and study its\nstatistical properties in terms of its location and nature considering a\nclassification into five semantic concept classes: signal, object, scene,\nabstract and relational. A user study is conducted to validate the results. The\nresults suggest that there are several locations that consistently contain\nrelevant textual information with respect to the image. The importance of each\nlocation is influenced by the type of webpage as the results show the different\ndistribution of relevant contextual information across the locations for\ndifferent webpage types. The frequently found semantic concept classes are\nobject and abstract. Another important outcome of the user study shows that a\nwebpage is not an atomic unit and can be further partitioned into smaller\nsegments. Segments containing images are of interest and termed as image\nsegments. We observe that users typically single out textual information which\nthey consider relevant to the image from the textual information bounded within\nthe image segment.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 04:03:46 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Fauzi", "Fariza", ""], ["Belkhatir", "Mohammed", ""]]}, {"id": "2005.02158", "submitter": "Jie Wang", "authors": "Hao Zhang, Jie Wang", "title": "An Unsupervised Semantic Sentence Ranking Scheme for Text Documents", "comments": "To appear in Integrated Computer-Aided Engineering (ICAE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Semantic SentenceRank (SSR), an unsupervised scheme for\nautomatically ranking sentences in a single document according to their\nrelative importance. In particular, SSR extracts essential words and phrases\nfrom a text document, and uses semantic measures to construct, respectively, a\nsemantic phrase graph over phrases and words, and a semantic sentence graph\nover sentences. It applies two variants of article-structure-biased PageRank to\nscore phrases and words on the first graph and sentences on the second graph.\nIt then combines these scores to generate the final score for each sentence.\nFinally, SSR solves a multi-objective optimization problem for ranking\nsentences based on their final scores and topic diversity through semantic\nsubtopic clustering. An implementation of SSR that runs in quadratic time is\npresented, and it outperforms, on the SummBank benchmarks, each individual\njudge's ranking and compares favorably with the combined ranking of all judges.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 20:17:51 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Zhang", "Hao", ""], ["Wang", "Jie", ""]]}, {"id": "2005.02230", "submitter": "Jheng-Hong Yang", "authors": "Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai,\n  Chuan-Ju Wang and Jimmy Lin", "title": "Multi-Stage Conversational Passage Retrieval: An Approach to Fusing Term\n  Importance Estimation and Neural Query Rewriting", "comments": "28 pages. Accepted to ACM Transactions on Information Systems,\n  Special Issue on Conversational Search and Recommendation. The first two\n  authors contributed equally. Code: https://github.com/castorini/chatty-goose", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search plays a vital role in conversational information\nseeking. As queries in information seeking dialogues are ambiguous for\ntraditional ad-hoc information retrieval (IR) systems due to the coreference\nand omission resolution problems inherent in natural language dialogue,\nresolving these ambiguities is crucial. In this paper, we tackle conversational\npassage retrieval (ConvPR), an important component of conversational search, by\naddressing query ambiguities with query reformulation integrated into a\nmulti-stage ad-hoc IR system. Specifically, we propose two conversational query\nreformulation (CQR) methods: (1) term importance estimation and (2) neural\nquery rewriting. For the former, we expand conversational queries using\nimportant terms extracted from the conversational context with frequency-based\nsignals. For the latter, we reformulate conversational queries into natural,\nstandalone, human-understandable queries with a pretrained sequence-tosequence\nmodel. Detailed analyses of the two CQR methods are provided quantitatively and\nqualitatively, explaining their advantages, disadvantages, and distinct\nbehaviors. Moreover, to leverage the strengths of both CQR methods, we propose\ncombining their output with reciprocal rank fusion, yielding state-of-the-art\nretrieval effectiveness, 30% improvement in terms of NDCG@3 compared to the\nbest submission of TREC CAsT 2019.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 14:30:20 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 14:33:53 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Lin", "Sheng-Chieh", ""], ["Yang", "Jheng-Hong", ""], ["Nogueira", "Rodrigo", ""], ["Tsai", "Ming-Feng", ""], ["Wang", "Chuan-Ju", ""], ["Lin", "Jimmy", ""]]}, {"id": "2005.02239", "submitter": "Ruben Verborgh", "authors": "Ruben Verborgh and Ruben Taelman", "title": "Guided Link-Traversal-Based Query Processing", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link-Traversal-Based Query Processing (LTBQP) is a technique for evaluating\nqueries over a web of data by starting with a set of seed documents that is\ndynamically expanded through following hyperlinks. Compared to query evaluation\nover a static set of sources, LTBQP is significantly slower because of the\nnumber of needed network requests. Furthermore, there are concerns regarding\nrelevance and trustworthiness of results, given that sources are selected\ndynamically. To address both issues, we propose guided LTBQP, a technique in\nwhich information about document linking structure and content policies is\npassed to a query processor. Thereby, the processor can prune the search tree\nof documents by only following relevant links, and restrict the result set to\ndesired results by limiting which documents are considered for what kinds of\ncontent. In this exploratory paper, we describe the technique at a high level\nand sketch some of its applications. We argue that such guidance can make LTBQP\na valuable query strategy in decentralized environments, where data is spread\nacross documents with varying levels of user trust.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 22:35:51 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Verborgh", "Ruben", ""], ["Taelman", "Ruben", ""]]}, {"id": "2005.02365", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Arman Cohan, Nazli Goharian", "title": "SLEDGE: A Simple Yet Effective Baseline for COVID-19 Scientific\n  Knowledge Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With worldwide concerns surrounding the Severe Acute Respiratory Syndrome\nCoronavirus 2 (SARS-CoV-2), there is a rapidly growing body of literature on\nthe virus. Clinicians, researchers, and policy-makers need a way to effectively\nsearch these articles. In this work, we present a search system called SLEDGE,\nwhich utilizes SciBERT to effectively re-rank articles. We train the model on a\ngeneral-domain answer ranking dataset, and transfer the relevance signals to\nSARS-CoV-2 for evaluation. We observe SLEDGE's effectiveness as a strong\nbaseline on the TREC-COVID challenge (topping the learderboard with an nDCG@10\nof 0.6844). Insights provided by a detailed analysis provide some potential\nfuture directions to explore, including the importance of filtering by date and\nthe potential of neural methods that rely more heavily on count signals. We\nrelease the code to facilitate future work on this critical task at\nhttps://github.com/Georgetown-IR-Lab/covid-neural-ir\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 17:51:27 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 16:06:33 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 17:24:19 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["MacAvaney", "Sean", ""], ["Cohan", "Arman", ""], ["Goharian", "Nazli", ""]]}, {"id": "2005.02439", "submitter": "Xisen Jin", "authors": "Brendan Kennedy and Xisen Jin and Aida Mostafazadeh Davani and Morteza\n  Dehghani and Xiang Ren", "title": "Contextualizing Hate Speech Classifiers with Post-hoc Explanation", "comments": "To appear in Proceedings of the 2020 Annual Conference of the\n  Association for Computational Linguistics; Updated references and discussions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hate speech classifiers trained on imbalanced datasets struggle to determine\nif group identifiers like \"gay\" or \"black\" are used in offensive or prejudiced\nways. Such biases manifest in false positives when these identifiers are\npresent, due to models' inability to learn the contexts which constitute a\nhateful usage of identifiers. We extract SOC post-hoc explanations from\nfine-tuned BERT classifiers to efficiently detect bias towards identity terms.\nThen, we propose a novel regularization technique based on these explanations\nthat encourages models to learn from the context of group identifiers in\naddition to the identifiers themselves. Our approach improved over baselines in\nlimiting false positives on out-of-domain data while maintaining or improving\nin-domain performance. Project page:\nhttps://inklab.usc.edu/contextualize-hate-speech/.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 18:56:40 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 20:19:29 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 18:54:09 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Kennedy", "Brendan", ""], ["Jin", "Xisen", ""], ["Davani", "Aida Mostafazadeh", ""], ["Dehghani", "Morteza", ""], ["Ren", "Xiang", ""]]}, {"id": "2005.02510", "submitter": "Shantanu Sharma", "authors": "Peeyush Gupta, Sharad Mehrotra, Nisha Panwar, Shantanu Sharma, Nalini\n  Venkatasubramanian, Guoxi Wang", "title": "Quest: Practical and Oblivious Mitigation Strategies for COVID-19 using\n  WiFi Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contact tracing has emerged as one of the main mitigation strategies to\nprevent the spread of pandemics such as COVID-19. Recently, several efforts\nhave been initiated to track individuals, their movements, and interactions\nusing technologies, e.g., Bluetooth beacons, cellular data records, and\nsmartphone applications. Such solutions are often intrusive, potentially\nviolating individual privacy rights and are often subject to regulations (e.g.,\nGDPR and CCPR) that mandate the need for opt-in policies to gather and use\npersonal information. In this paper, we introduce Quest, a system that empowers\norganizations to observe individuals and spaces to implement policies for\nsocial distancing and contact tracing using WiFi connectivity data in a passive\nand privacy-preserving manner. The goal is to ensure the safety of employees\nand occupants at an organization, while protecting the privacy of all parties.\nQuest incorporates computationally- and information-theoretically-secure\nprotocols that prevent adversaries from gaining knowledge of an individual's\nlocation history (based on WiFi data); it includes support for accurately\nidentifying users who were in the vicinity of a confirmed patient, and then\ninforming them via opt-in mechanisms. Quest supports a range of privacy-enabled\napplications to ensure adherence to social distancing, monitor the flow of\npeople through spaces, identify potentially impacted regions, and raise\nexposure alerts. We describe the architecture, design choices, and\nimplementation of the proposed security/privacy techniques in Quest. We, also,\nvalidate the practicality of Quest and evaluate it thoroughly via an actual\ncampus-scale deployment at UC Irvine over a very large dataset of over 50M\ntuples.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 21:39:38 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Gupta", "Peeyush", ""], ["Mehrotra", "Sharad", ""], ["Panwar", "Nisha", ""], ["Sharma", "Shantanu", ""], ["Venkatasubramanian", "Nalini", ""], ["Wang", "Guoxi", ""]]}, {"id": "2005.02553", "submitter": "Honglei Zhuang", "authors": "Honglei Zhuang, Xuanhui Wang, Michael Bendersky, Alexander Grushetsky,\n  Yonghui Wu, Petr Mitrichev, Ethan Sterling, Nathan Bell, Walker Ravina, Hai\n  Qian", "title": "Interpretable Learning-to-Rank with Generalized Additive Models", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability of learning-to-rank models is a crucial yet relatively\nunder-examined research area. Recent progress on interpretable ranking models\nlargely focuses on generating post-hoc explanations for existing black-box\nranking models, whereas the alternative option of building an intrinsically\ninterpretable ranking model with transparent and self-explainable structure\nremains unexplored. Developing fully-understandable ranking models is necessary\nin some scenarios (e.g., due to legal or policy constraints) where post-hoc\nmethods cannot provide sufficiently accurate explanations. In this paper, we\nlay the groundwork for intrinsically interpretable learning-to-rank by\nintroducing generalized additive models (GAMs) into ranking tasks. Generalized\nadditive models (GAMs) are intrinsically interpretable machine learning models\nand have been extensively studied on regression and classification tasks. We\nstudy how to extend GAMs into ranking models which can handle both item-level\nand list-level features and propose a novel formulation of ranking GAMs. To\ninstantiate ranking GAMs, we employ neural networks instead of traditional\nsplines or regression trees. We also show that our neural ranking GAMs can be\ndistilled into a set of simple and compact piece-wise linear functions that are\nmuch more efficient to evaluate with little accuracy loss. We conduct\nexperiments on three data sets and show that our proposed neural ranking GAMs\ncan achieve significantly better performance than other traditional GAM\nbaselines while maintaining similar interpretability.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 01:51:30 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 18:44:23 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Zhuang", "Honglei", ""], ["Wang", "Xuanhui", ""], ["Bendersky", "Michael", ""], ["Grushetsky", "Alexander", ""], ["Wu", "Yonghui", ""], ["Mitrichev", "Petr", ""], ["Sterling", "Ethan", ""], ["Bell", "Nathan", ""], ["Ravina", "Walker", ""], ["Qian", "Hai", ""]]}, {"id": "2005.02557", "submitter": "Wenhao Yu", "authors": "Wenhao Yu, Lingfei Wu, Qingkai Zeng, Shu Tao, Yu Deng, Meng Jiang", "title": "Crossing Variational Autoencoders for Answer Retrieval", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Answer retrieval is to find the most aligned answer from a large set of\ncandidates given a question. Learning vector representations of\nquestions/answers is the key factor. Question-answer alignment and\nquestion/answer semantics are two important signals for learning the\nrepresentations. Existing methods learned semantic representations with dual\nencoders or dual variational auto-encoders. The semantic information was\nlearned from language models or question-to-question (answer-to-answer)\ngenerative processes. However, the alignment and semantics were too separate to\ncapture the aligned semantics between question and answer. In this work, we\npropose to cross variational auto-encoders by generating questions with aligned\nanswers and generating answers with aligned questions. Experiments show that\nour method outperforms the state-of-the-art answer retrieval method on SQuAD.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 01:59:13 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 03:24:23 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Yu", "Wenhao", ""], ["Wu", "Lingfei", ""], ["Zeng", "Qingkai", ""], ["Tao", "Shu", ""], ["Deng", "Yu", ""], ["Jiang", "Meng", ""]]}, {"id": "2005.02558", "submitter": "Desheng Wang", "authors": "Desheng Wang, Jiawei Liu, Xiang Qi, Baolin Sun, Peng Zhang", "title": "Revisiting Regex Generation for Modeling Industrial Applications by\n  Incorporating Byte Pair Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Regular expression is important for many natural language processing tasks\nespecially when used to deal with unstructured and semi-structured data. This\nwork focuses on automatically generating regular expressions and proposes a\nnovel genetic algorithm to deal with this problem. Different from the methods\nwhich generate regular expressions from character level, we first utilize byte\npair encoder (BPE) to extract some frequent items, which are then used to\nconstruct regular expressions. The fitness function of our genetic algorithm\ncontains multi objectives and is solved based on evolutionary procedure\nincluding crossover and mutation operation. In the fitness function, we take\nthe length of generated regular expression, the maximum matching characters and\nsamples for positive training samples, and the minimum matching characters and\nsamples for negative training samples into consideration. In addition, to\naccelerate the training process, we do exponential decay on the population size\nof the genetic algorithm. Our method together with a strong baseline is tested\non 13 kinds of challenging datasets. The results demonstrate the effectiveness\nof our method, which outperforms the baseline on 10 kinds of data and achieves\nnearly 50 percent improvement on average. By doing exponential decay, the\ntraining speed is approximately 100 times faster than the methods without using\nexponential decay. In summary, our method possesses both effectiveness and\nefficiency, and can be implemented for the industry application.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 02:09:10 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 07:52:25 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Wang", "Desheng", ""], ["Liu", "Jiawei", ""], ["Qi", "Xiang", ""], ["Sun", "Baolin", ""], ["Zhang", "Peng", ""]]}, {"id": "2005.02614", "submitter": "Fabian Kirstein", "authors": "Fabian Kirstein, Kyriakos Stefanidis, Benjamin Dittwald, Simon\n  Dutkowski, Sebastian Urbanek, Manfred Hauswirth", "title": "Piveau: A Large-scale Open Data Management Platform based on Semantic\n  Web Technologies", "comments": "16 pages, 2 figures, preprint of an in-use paper at Extended Semantic\n  Web Conference (ESWC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The publication and (re)utilization of Open Data is still facing multiple\nbarriers on technical, organizational and legal levels. This includes\nlimitations in interfaces, search capabilities, provision of quality\ninformation and the lack of definite standards and implementation guidelines.\nMany Semantic Web specifications and technologies are specifically designed to\naddress the publication of data on the web. In addition, many official\npublication bodies encourage and foster the development of Open Data standards\nbased on Semantic Web principles. However, no existing solution for managing\nOpen Data takes full advantage of these possibilities and benfits. In this\npaper, we present our solution \"Piveau\", a fully-fledged Open Data management\nsolution, based on Semantic Web technologies. It harnesses a variety of\nstandards, like RDF, DCAT, DQV, and SKOS, to overcome the barriers in Open Data\npublication. The solution puts a strong focus on assuring data quality and\nscalability. We give a detailed description of the underlying, highly scalable,\nservice-oriented architecture, how we integrated the aforementioned standards,\nand used a triplestore as our primary database. We have evaluated our work in a\ncomprehensive feature comparison to established solutions and through a\npractical application in a production environment, the European Data Portal.\nOur solution is available as Open Source.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 06:46:27 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Kirstein", "Fabian", ""], ["Stefanidis", "Kyriakos", ""], ["Dittwald", "Benjamin", ""], ["Dutkowski", "Simon", ""], ["Urbanek", "Sebastian", ""], ["Hauswirth", "Manfred", ""]]}, {"id": "2005.02843", "submitter": "Faegheh Hasibi", "authors": "Emma J. Gerritse, Faegheh Hasibi, and Arjen P. de Vries", "title": "Graph-Embedding Empowered Entity Retrieval", "comments": null, "journal-ref": "Advances in Information Retrieval. ECIR 2020. Lecture Notes in\n  Computer Science, vol 12035. Springer,", "doi": "10.1007/978-3-030-45439-5_7", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, we improve upon the current state of the art in entity\nretrieval by re-ranking the result list using graph embeddings. The paper shows\nthat graph embeddings are useful for entity-oriented search tasks. We\ndemonstrate empirically that encoding information from the knowledge graph into\n(graph) embeddings contributes to a higher increase in effectiveness of entity\nretrieval results than using plain word embeddings. We analyze the impact of\nthe accuracy of the entity linker on the overall retrieval effectiveness. Our\nanalysis further deploys the cluster hypothesis to explain the observed\nadvantages of graph embeddings over the more widely used word embeddings, for\nuser tasks involving ranking entities.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 14:13:49 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Gerritse", "Emma J.", ""], ["Hasibi", "Faegheh", ""], ["de Vries", "Arjen P.", ""]]}, {"id": "2005.02844", "submitter": "Yanqiao Zhu", "authors": "Feng Yu, Yanqiao Zhu, Qiang Liu, Shu Wu, Liang Wang, Tieniu Tan", "title": "TAGNN: Target Attentive Graph Neural Networks for Session-based\n  Recommendation", "comments": "5 pages, accepted to SIGIR 2020, authors' version", "journal-ref": null, "doi": "10.1145/3397271.3401319", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommendation nowadays plays a vital role in many websites,\nwhich aims to predict users' actions based on anonymous sessions. There have\nemerged many studies that model a session as a sequence or a graph via\ninvestigating temporal transitions of items in a session. However, these\nmethods compress a session into one fixed representation vector without\nconsidering the target items to be predicted. The fixed vector will restrict\nthe representation ability of the recommender model, considering the diversity\nof target items and users' interests. In this paper, we propose a novel target\nattentive graph neural network (TAGNN) model for session-based recommendation.\nIn TAGNN, target-aware attention adaptively activates different user interests\nwith respect to varied target items. The learned interest representation vector\nvaries with different target items, greatly improving the expressiveness of the\nmodel. Moreover, TAGNN harnesses the power of graph neural networks to capture\nrich item transitions in sessions. Comprehensive experiments conducted on\nreal-world datasets demonstrate its superiority over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 14:17:05 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Yu", "Feng", ""], ["Zhu", "Yanqiao", ""], ["Liu", "Qiang", ""], ["Wu", "Shu", ""], ["Wang", "Liang", ""], ["Tan", "Tieniu", ""]]}, {"id": "2005.03008", "submitter": "Artem Kramov", "authors": "Artem Kramov", "title": "Evaluating text coherence based on the graph of the consistency of\n  phrases to identify symptoms of schizophrenia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different state-of-the-art methods of the detection of schizophrenia symptoms\nbased on the estimation of text coherence have been analyzed. The analysis of a\ntext at the level of phrases has been suggested. The method based on the graph\nof the consistency of phrases has been proposed to evaluate the semantic\ncoherence and the cohesion of a text. The semantic coherence, cohesion, and\nother linguistic features (lexical diversity, lexical density) have been taken\ninto account to form feature vectors for the training of a model-classifier.\nThe training of the classifier has been performed on the set of\nEnglish-language interviews. According to the retrieved results, the impact of\neach feature on the output of the model has been analyzed. The results obtained\ncan indicate that the proposed method based on the graph of the consistency of\nphrases may be used in the different tasks of the detection of mental illness.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 08:38:20 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Kramov", "Artem", ""]]}, {"id": "2005.03257", "submitter": "Xiaodong Ge", "authors": "Siwei Fu, Kai Xiong, Xiaodong Ge, Siliang Tang, Wei Chen, Yingcai Wu", "title": "Quda: Natural Language Queries for Visual Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of analytic tasks from free text is critical for\nvisualization-oriented natural language interfaces (V-NLIs) to suggest\neffective visualizations. However, it is challenging due to the ambiguity and\ncomplexity nature of human language. To address this challenge, we present a\nnew dataset, called Quda, that aims to help V-NLIs recognize analytic tasks\nfrom free-form natural language by training and evaluating cutting-edge\nmulti-label classification models. Our dataset contains $14,035$ diverse user\nqueries, and each is annotated with one or multiple analytic tasks. We achieve\nthis goal by first gathering seed queries with data analysts and then employing\nextensive crowd force for paraphrase generation and validation. We demonstrate\nthe usefulness of Quda through three applications. This work is the first\nattempt to construct a large-scale corpus for recognizing analytic tasks. With\nthe release of Quda, we hope it will boost the research and development of\nV-NLIs in data analysis and visualization.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 05:35:16 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 16:00:51 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 12:45:39 GMT"}, {"version": "v4", "created": "Sun, 23 Aug 2020 07:34:50 GMT"}, {"version": "v5", "created": "Thu, 3 Dec 2020 06:58:56 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Fu", "Siwei", ""], ["Xiong", "Kai", ""], ["Ge", "Xiaodong", ""], ["Tang", "Siliang", ""], ["Chen", "Wei", ""], ["Wu", "Yingcai", ""]]}, {"id": "2005.03297", "submitter": "Yunshan Ma", "authors": "Yunshan Ma, Yujuan Ding, Xun Yang, Lizi Liao, Wai Keung Wong, Tat-Seng\n  Chua", "title": "Knowledge Enhanced Neural Fashion Trend Forecasting", "comments": "8 pages, 9 figures, ICMR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion trend forecasting is a crucial task for both academia and industry.\nAlthough some efforts have been devoted to tackling this challenging task, they\nonly studied limited fashion elements with highly seasonal or simple patterns,\nwhich could hardly reveal the real fashion trends. Towards insightful fashion\ntrend forecasting, this work focuses on investigating fine-grained fashion\nelement trends for specific user groups. We first contribute a large-scale\nfashion trend dataset (FIT) collected from Instagram with extracted time series\nfashion element records and user information. Further-more, to effectively\nmodel the time series data of fashion elements with rather complex patterns, we\npropose a Knowledge EnhancedRecurrent Network model (KERN) which takes\nadvantage of the capability of deep recurrent neural networks in modeling\ntime-series data. Moreover, it leverages internal and external knowledge in\nfashion domain that affects the time-series patterns of fashion element trends.\nSuch incorporation of domain knowledge further enhances the deep learning model\nin capturing the patterns of specific fashion elements and predicting the\nfuture trends. Extensive experiments demonstrate that the proposed KERN model\ncan effectively capture the complicated patterns of objective fashion elements,\ntherefore making preferable fashion trend forecast.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 07:42:17 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 09:14:20 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Ma", "Yunshan", ""], ["Ding", "Yujuan", ""], ["Yang", "Xun", ""], ["Liao", "Lizi", ""], ["Wong", "Wai Keung", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2005.03475", "submitter": "Chen Gao", "authors": "Jianxin Chang, Chen Gao, Xiangnan He, Yong Li, Depeng Jin", "title": "Bundle Recommendation with Graph Convolutional Networks", "comments": "Accepted by SIGIR 2020 (Short)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bundle recommendation aims to recommend a bundle of items for a user to\nconsume as a whole. Existing solutions integrate user-item interaction modeling\ninto bundle recommendation by sharing model parameters or learning in a\nmulti-task manner, which cannot explicitly model the affiliation between items\nand bundles, and fail to explore the decision-making when a user chooses\nbundles. In this work, we propose a graph neural network model named BGCN\n(short for \\textit{\\textBF{B}undle \\textBF{G}raph \\textBF{C}onvolutional\n\\textBF{N}etwork}) for bundle recommendation. BGCN unifies user-item\ninteraction, user-bundle interaction and bundle-item affiliation into a\nheterogeneous graph. With item nodes as the bridge, graph convolutional\npropagation between user and bundle nodes makes the learned representations\ncapture the item level semantics. Through training based on hard-negative\nsampler, the user's fine-grained preferences for similar bundles are further\ndistinguished. Empirical results on two real-world datasets demonstrate the\nstrong performance gains of BGCN, which outperforms the state-of-the-art\nbaselines by 10.77\\% to 23.18\\%.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 13:48:26 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Chang", "Jianxin", ""], ["Gao", "Chen", ""], ["He", "Xiangnan", ""], ["Li", "Yong", ""], ["Jin", "Depeng", ""]]}, {"id": "2005.03529", "submitter": "Shrestha Ghosh", "authors": "Shrestha Ghosh, Simon Razniewski, Gerhard Weikum", "title": "CounQER: A System for Discovering and Linking Count Information in\n  Knowledge Bases", "comments": "Accepted at ESWC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicate constraints of general-purpose knowledge bases (KBs) like Wikidata,\nDBpedia and Freebase are often limited to subproperty, domain and range\nconstraints. In this demo we showcase CounQER, a system that illustrates the\nalignment of counting predicates, like staffSize, and enumerating predicates,\nlike workInstitution^{-1} . In the demonstration session, attendees can inspect\nthese alignments, and will learn about the importance of these alignments for\nKB question answering and curation. CounQER is available at\nhttps://counqer.mpi-inf.mpg.de/spo.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 14:53:11 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Ghosh", "Shrestha", ""], ["Razniewski", "Simon", ""], ["Weikum", "Gerhard", ""]]}, {"id": "2005.03531", "submitter": "Noemi Mauro", "authors": "Noemi Mauro, Liliana Ardissono and Maurizio Lucenteforte", "title": "Faceted Search of Heterogeneous Geographic Information for Dynamic Map\n  Projection", "comments": null, "journal-ref": "Information Processing & Management, Volume 57, Issue 4, 2020", "doi": "10.1016/j.ipm.2020.102257", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes a faceted information exploration model that supports\ncoarse-grained and fine-grained focusing of geographic maps by offering a\ngraphical representation of data attributes within interactive widgets. The\nproposed approach enables (i) a multi-category projection of long-lasting\ngeographic maps, based on the proposal of efficient facets for data exploration\nin sparse and noisy datasets, and (ii) an interactive representation of the\nsearch context based on widgets that support data visualization, faceted\nexploration, category-based information hiding and transparency of results at\nthe same time. The integration of our model with a semantic representation of\ngeographical knowledge supports the exploration of information retrieved from\nheterogeneous data sources, such as Public Open Data and OpenStreetMap. We\nevaluated our model with users in the OnToMap collaborative Web GIS. The\nexperimental results show that, when working on geographic maps populated with\nmultiple data categories, it outperforms simple category-based map projection\nand traditional faceted search tools, such as checkboxes, in both user\nperformance and experience.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 14:55:39 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Mauro", "Noemi", ""], ["Ardissono", "Liliana", ""], ["Lucenteforte", "Maurizio", ""]]}, {"id": "2005.03550", "submitter": "Alessandro Balestrucci", "authors": "Alessandro Balestrucci and Rocco De Nicola", "title": "Credulous Users and Fake News: a Real Case Study on the Propagation in\n  Twitter", "comments": "15 pages and 8 tables. Accepted to appear in the Proceedings at IEEE\n  Conference on Evolving and Adaptive Intelligent Systems (EAIS2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have confirmed a growing trend, especially among youngsters,\nof using Online Social Media as favourite information platform at the expense\nof traditional mass media. Indeed, they can easily reach a wide audience at a\nhigh speed; but exactly because of this they are the preferred medium for\ninfluencing public opinion via so-called fake news. Moreover, there is a\ngeneral agreement that the main vehicle of fakes news are malicious software\nrobots (bots) that automatically interact with human users. In previous work we\nhave considered the problem of tagging human users in Online Social Networks as\ncredulous users. Specifically, we have considered credulous those users with\nrelatively high number of bot friends when compared to total number of their\nsocial friends. We consider this group of users worth of attention because they\nmight have a higher exposure to malicious activities and they may contribute to\nthe spreading of fake information by sharing dubious content. In this work,\nstarting from a dataset of fake news, we investigate the behaviour and the\ndegree of involvement of credulous users in fake news diffusion. The study aims\nto: (i) fight fake news by considering the content diffused by credulous users;\n(ii) highlight the relationship between credulous users and fake news\nspreading; (iii) target fake news detection by focusing on the analysis of\nspecific accounts more exposed to malicious activities of bots. Our first\nresults demonstrate a strong involvement of credulous users in fake news\ndiffusion. This findings are calling for tools that, by performing data\nstreaming on credulous' users actions, enables us to perform targeted\nfact-checking.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 15:21:56 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Balestrucci", "Alessandro", ""], ["De Nicola", "Rocco", ""]]}, {"id": "2005.03624", "submitter": "Thanh Nguyen", "authors": "Thanh V. Nguyen, Nikhil Rao and Karthik Subbian", "title": "Learning Robust Models for e-Commerce Product Search", "comments": "This work has been accepted for publication at ACL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Showing items that do not match search query intent degrades customer\nexperience in e-commerce. These mismatches result from counterfactual biases of\nthe ranking algorithms toward noisy behavioral signals such as clicks and\npurchases in the search logs. Mitigating the problem requires a large labeled\ndataset, which is expensive and time-consuming to obtain. In this paper, we\ndevelop a deep, end-to-end model that learns to effectively classify mismatches\nand to generate hard mismatched examples to improve the classifier. We train\nthe model end-to-end by introducing a latent variable into the cross-entropy\nloss that alternates between using the real and generated samples. This not\nonly makes the classifier more robust but also boosts the overall ranking\nperformance. Our model achieves a relative gain compared to baselines by over\n26% in F-score, and over 17% in Area Under PR curve. On live search traffic,\nour model gains significant improvement in multiple countries.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 17:22:21 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Nguyen", "Thanh V.", ""], ["Rao", "Nikhil", ""], ["Subbian", "Karthik", ""]]}, {"id": "2005.03724", "submitter": "Yang Gao", "authors": "Yang Gao, Wei Zhao, Steffen Eger", "title": "SUPERT: Towards New Frontiers in Unsupervised Evaluation Metrics for\n  Multi-Document Summarization", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study unsupervised multi-document summarization evaluation metrics, which\nrequire neither human-written reference summaries nor human annotations (e.g.\npreferences, ratings, etc.). We propose SUPERT, which rates the quality of a\nsummary by measuring its semantic similarity with a pseudo reference summary,\ni.e. selected salient sentences from the source documents, using contextualized\nembeddings and soft token alignment techniques. Compared to the\nstate-of-the-art unsupervised evaluation metrics, SUPERT correlates better with\nhuman ratings by 18-39%. Furthermore, we use SUPERT as rewards to guide a\nneural-based reinforcement learning summarizer, yielding favorable performance\ncompared to the state-of-the-art unsupervised summarizers. All source code is\navailable at https://github.com/yg211/acl20-ref-free-eval.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 19:54:24 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Gao", "Yang", ""], ["Zhao", "Wei", ""], ["Eger", "Steffen", ""]]}, {"id": "2005.03932", "submitter": "Shuo Sun", "authors": "Shuo Sun, Kevin Duh", "title": "Modeling Document Interactions for Learning to Rank with Regularized\n  Self-Attention", "comments": "5 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to rank is an important task that has been successfully deployed in\nmany real-world information retrieval systems. Most existing methods compute\nrelevance judgments of documents independently, without holistically\nconsidering the entire set of competing documents. In this paper, we explore\nmodeling documents interactions with self-attention based neural networks.\nAlthough self-attention networks have achieved state-of-the-art results in many\nNLP tasks, we find empirically that self-attention provides little benefit over\nbaseline neural learning to rank architecture. To improve the learning of\nself-attention weights, We propose simple yet effective regularization terms\ndesigned to model interactions between documents. Evaluations on publicly\navailable Learning to Rank (LETOR) datasets show that training self-attention\nnetwork with our proposed regularization terms can significantly outperform\nexisting learning to rank methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 09:53:31 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Sun", "Shuo", ""], ["Duh", "Kevin", ""]]}, {"id": "2005.03975", "submitter": "Yan Xu", "authors": "Dan Su, Yan Xu, Tiezheng Yu, Farhad Bin Siddique, Elham J. Barezi,\n  Pascale Fung", "title": "CAiRE-COVID: A Question Answering and Query-focused Multi-Document\n  Summarization System for COVID-19 Scholarly Information Management", "comments": "Accepted EMNLP2020 NLP-COVID Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CAiRE-COVID, a real-time question answering (QA) and\nmulti-document summarization system, which won one of the 10 tasks in the\nKaggle COVID-19 Open Research Dataset Challenge, judged by medical experts. Our\nsystem aims to tackle the recent challenge of mining the numerous scientific\narticles being published on COVID-19 by answering high priority questions from\nthe community and summarizing salient question-related information. It combines\ninformation extraction with state-of-the-art QA and query-focused\nmulti-document summarization techniques, selecting and highlighting evidence\nsnippets from existing literature given a query. We also propose query-focused\nabstractive and extractive multi-document summarization methods, to provide\nmore relevant information related to the question. We further conduct\nquantitative experiments that show consistent improvements on various metrics\nfor each module. We have launched our website CAiRE-COVID for broader use by\nthe medical community, and have open-sourced the code for our system, to\nbootstrap further study by other researches.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 15:07:27 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 02:47:06 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 11:30:49 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Su", "Dan", ""], ["Xu", "Yan", ""], ["Yu", "Tiezheng", ""], ["Siddique", "Farhad Bin", ""], ["Barezi", "Elham J.", ""], ["Fung", "Pascale", ""]]}, {"id": "2005.04356", "submitter": "Yunzhong He", "authors": "Yunzhong He, Wenyuan Li, Liang-Wei Chen, Gabriel Forgues, Xunlong Gui,\n  Sui Liang, Bo Hou", "title": "A Social Search Model for Large Scale Social Networks", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of social networks, information on the internet is no longer\nsolely organized by web pages. Rather, content is generated and shared among\nusers and organized around their social relations on social networks. This\npresents new challenges to information retrieval systems. On a social network\nsearch system, the generation of result sets not only needs to consider keyword\nmatches, like a traditional web search engine does, but it also needs to take\ninto account the searcher's social connections and the content's visibility\nsettings. Besides, search ranking should be able to handle both textual\nrelevance and the rich social interaction signals from the social network. In\nthis paper, we present our solution to these two challenges by first\nintroducing a social retrieval mechanism, and then investigate novel deep\nneural networks for the ranking problem. The retrieval system treats social\nconnections as indexing terms, and generates meaningful results sets by biasing\ntowards close social connections in a constrained optimization fashion. The\nresult set is then ranked by a deep neural network that handles textual and\nsocial relevance in a two-tower approach, in which personalization and textual\nrelevance are addressed jointly. The retrieval mechanism is deployed on\nFacebook and is helping billions of users finding postings from their\nconnections efficiently. Based on the postings being retrieved, we evaluate our\ntwo-tower neutral network, and examine the importance of personalization and\ntextual signals in the ranking problem.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 02:59:02 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["He", "Yunzhong", ""], ["Li", "Wenyuan", ""], ["Chen", "Liang-Wei", ""], ["Forgues", "Gabriel", ""], ["Gui", "Xunlong", ""], ["Liang", "Sui", ""], ["Hou", "Bo", ""]]}, {"id": "2005.04361", "submitter": "Qiaoan Chen", "authors": "Qiaoan Chen, Hao Gu, Lingling Yi, Yishi Lin, Peng He, Chuan Chen,\n  Yangqiu Song", "title": "SocialTrans: A Deep Sequential Model with Social Information for\n  Web-Scale Recommendation Systems", "comments": "11 pages,8 figures,4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On social network platforms, a user's behavior is based on his/her personal\ninterests, or influenced by his/her friends. In the literature, it is common to\nmodel either users' personal preference or their socially influenced\npreference. In this paper, we present a novel deep learning model SocialTrans\nfor social recommendations to integrate these two types of preferences.\nSocialTrans is composed of three modules. The first module is based on a\nmulti-layer Transformer to model users' personal preference. The second module\nis a multi-layer graph attention neural network (GAT), which is used to model\nthe social influence strengths between friends in social networks. The last\nmodule merges users' personal preference and socially influenced preference to\nproduce recommendations. Our model can efficiently fit large-scale data and we\ndeployed SocialTrans to a major article recommendation system in China.\nExperiments on three data sets verify the effectiveness of our model and show\nthat it outperforms state-of-the-art social recommendation methods.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 03:39:45 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Chen", "Qiaoan", ""], ["Gu", "Hao", ""], ["Yi", "Lingling", ""], ["Lin", "Yishi", ""], ["He", "Peng", ""], ["Chen", "Chuan", ""], ["Song", "Yangqiu", ""]]}, {"id": "2005.04396", "submitter": "Junheng Huang", "authors": "Junheng Huang, Lu Pan, Kang Xu, Weihua Peng, Fayuan Li", "title": "Generating Pertinent and Diversified Comments with Topic-aware\n  Pointer-Generator Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comment generation, a new and challenging task in Natural Language Generation\n(NLG), attracts a lot of attention in recent years. However, comments generated\nby previous work tend to lack pertinence and diversity. In this paper, we\npropose a novel generation model based on Topic-aware Pointer-Generator\nNetworks (TPGN), which can utilize the topic information hidden in the articles\nto guide the generation of pertinent and diversified comments. Firstly, we\ndesign a keyword-level and topic-level encoder attention mechanism to capture\ntopic information in the articles. Next, we integrate the topic information\ninto pointer-generator networks to guide comment generation. Experiments on a\nlarge scale of comment generation dataset show that our model produces the\nvaluable comments and outperforms competitive baseline models significantly.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 09:04:09 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Huang", "Junheng", ""], ["Pan", "Lu", ""], ["Xu", "Kang", ""], ["Peng", "Weihua", ""], ["Li", "Fayuan", ""]]}, {"id": "2005.04456", "submitter": "Zhiqiang Pan", "authors": "Zhiqiang Pan and Fei Cai and Yanxiang Ling and Maarten de Rijke", "title": "Rethinking Item Importance in Session-based Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommendation aims to predict users' based on anonymous\nsessions. Previous work mainly focuses on the transition relationship between\nitems during an ongoing session. They generally fail to pay enough attention to\nthe importance of the items in terms of their relevance to user's main intent.\nIn this paper, we propose a Session-based Recommendation approach with an\nImportance Extraction Module, i.e., SR-IEM, that considers both a user's\nlong-term and recent behavior in an ongoing session. We employ a modified\nself-attention mechanism to estimate item importance in a session, which is\nthen used to predict user's long-term preference. Item recommendations are\nproduced by combining the user's long-term preference and current interest as\nconveyed by the last interacted item. Experiments conducted on two benchmark\ndatasets validate that SR-IEM outperforms the start-of-the-art in terms of\nRecall and MRR and has a reduced computational complexity.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 14:49:56 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Pan", "Zhiqiang", ""], ["Cai", "Fei", ""], ["Ling", "Yanxiang", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2005.04474", "submitter": "Ellen Voorhees", "authors": "Ellen Voorhees (National Institute of Standards and Technology) and\n  Tasmeer Alam (National Institute of Standards and Technology) and Steven\n  Bedrick (Oregon Health and Science University) and Dina Demner-Fushman (U.S.\n  National Library of Medicine) and William R Hersh (Oregon Health and Science\n  University) and Kyle Lo (Allen Institute for AI) and Kirk Roberts (University\n  of Texas Health Science Center at Houston) and Ian Soboroff (National\n  Institute of Standards and Technology) and Lucy Lu Wang (Allen Institute for\n  AI)", "title": "TREC-COVID: Constructing a Pandemic Information Retrieval Test\n  Collection", "comments": "10 pages, 5 figures. TREC-COVID web site:\n  http://ir.nist.gov/covidSubmit/ Will also appear in June 2020 issue of ACM\n  SIGIR Forum", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TREC-COVID is a community evaluation designed to build a test collection that\ncaptures the information needs of biomedical researchers using the scientific\nliterature during a pandemic. One of the key characteristics of pandemic search\nis the accelerated rate of change: the topics of interest evolve as the\npandemic progresses and the scientific literature in the area explodes. The\nCOVID-19 pandemic provides an opportunity to capture this progression as it\nhappens. TREC-COVID, in creating a test collection around COVID-19 literature,\nis building infrastructure to support new research and technologies in pandemic\nsearch.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 16:14:16 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Voorhees", "Ellen", "", "National Institute of Standards and Technology"], ["Alam", "Tasmeer", "", "National Institute of Standards and Technology"], ["Bedrick", "Steven", "", "Oregon Health and Science University"], ["Demner-Fushman", "Dina", "", "U.S.\n  National Library of Medicine"], ["Hersh", "William R", "", "Oregon Health and Science\n  University"], ["Lo", "Kyle", "", "Allen Institute for AI"], ["Roberts", "Kirk", "", "University\n  of Texas Health Science Center at Houston"], ["Soboroff", "Ian", "", "National\n  Institute of Standards and Technology"], ["Wang", "Lucy Lu", "", "Allen Institute for\n  AI"]]}, {"id": "2005.04518", "submitter": "Preslav Nakov", "authors": "Ramy Baly, Georgi Karadzhov, Jisun An, Haewoon Kwak, Yoan Dinkov,\n  Ahmed Ali, James Glass, Preslav Nakov", "title": "What Was Written vs. Who Read It: News Media Profiling Using Text\n  Analysis and Social Media Context", "comments": "Factuality of reporting, fact-checking, political ideology, media\n  bias, disinformation, propaganda, social media, news media", "journal-ref": "ACL-2020", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the political bias and the factuality of reporting of entire news\noutlets are critical elements of media profiling, which is an understudied but\nan increasingly important research direction. The present level of\nproliferation of fake, biased, and propagandistic content online, has made it\nimpossible to fact-check every single suspicious claim, either manually or\nautomatically. Alternatively, we can profile entire news outlets and look for\nthose that are likely to publish fake or biased content. This approach makes it\npossible to detect likely \"fake news\" the moment they are published, by simply\nchecking the reliability of their source. From a practical perspective,\npolitical bias and factuality of reporting have a linguistic aspect but also a\nsocial context. Here, we study the impact of both, namely (i) what was written\n(i.e., what was published by the target medium, and how it describes itself on\nTwitter) vs. (ii) who read it (i.e., analyzing the readers of the target medium\non Facebook, Twitter, and YouTube). We further study (iii) what was written\nabout the target medium on Wikipedia. The evaluation results show that what was\nwritten matters most, and that putting all information sources together yields\nhuge improvements over the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 22:00:08 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Baly", "Ramy", ""], ["Karadzhov", "Georgi", ""], ["An", "Jisun", ""], ["Kwak", "Haewoon", ""], ["Dinkov", "Yoan", ""], ["Ali", "Ahmed", ""], ["Glass", "James", ""], ["Nakov", "Preslav", ""]]}, {"id": "2005.04534", "submitter": "Kumar Ravi", "authors": "Vishal Vyas, Kumar Ravi, Vadlamani Ravi, V.Uma, Srirangaraj Setlur,\n  Venu Govindaraju", "title": "Article citation study: Context enhanced citation sentiment detection", "comments": "39 pages, 12 Tables, 5 Figures, Journal Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation sentimet analysis is one of the little studied tasks for\nscientometric analysis. For citation analysis, we developed eight datasets\ncomprising citation sentences, which are manually annotated by us into three\nsentiment polarities viz. positive, negative, and neutral. Among eight\ndatasets, three were developed by considering the whole context of citations.\nFurthermore, we proposed an ensembled feature engineering method comprising\nword embeddings obtained for texts, parts-of-speech tags, and dependency\nrelationships together. Ensembled features were considered as input to deep\nlearning based approaches for citation sentiment classification, which is in\nturn compared with Bag-of-Words approach. Experimental results demonstrate that\ndeep learning is useful for higher number of samples, whereas support vector\nmachine is the winner for smaller number of samples. Moreover, context-based\nsamples are proved to be more effective than context-less samples for citation\nsentiment analysis.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 00:27:19 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Vyas", "Vishal", ""], ["Ravi", "Kumar", ""], ["Ravi", "Vadlamani", ""], ["Uma", "V.", ""], ["Setlur", "Srirangaraj", ""], ["Govindaraju", "Venu", ""]]}, {"id": "2005.04588", "submitter": "Javed Qadrud-Din", "authors": "Javed Qadrud-Din, Ashraf Bah Rabiou, Ryan Walker, Ravi Soni, Martin\n  Gajek, Gabriel Pack, Akhil Rangaraj", "title": "Transformer Based Language Models for Similar Text Retrieval and Ranking", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most approaches for similar text retrieval and ranking with long natural\nlanguage queries rely at some level on queries and responses having words in\ncommon with each other. Recent applications of transformer-based neural\nlanguage models to text retrieval and ranking problems have been very\npromising, but still involve a two-step process in which result candidates are\nfirst obtained through bag-of-words-based approaches, and then reranked by a\nneural transformer. In this paper, we introduce novel approaches for\neffectively applying neural transformer models to similar text retrieval and\nranking without an initial bag-of-words-based step. By eliminating the\nbag-of-words-based step, our approach is able to accurately retrieve and rank\nresults even when they have no non-stopwords in common with the query. We\naccomplish this by using bidirectional encoder representations from\ntransformers (BERT) to create vectorized representations of sentence-length\ntexts, along with a vector nearest neighbor search index. We demonstrate both\nsupervised and unsupervised means of using BERT to accomplish this task.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 06:12:53 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 04:21:37 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Qadrud-Din", "Javed", ""], ["Rabiou", "Ashraf Bah", ""], ["Walker", "Ryan", ""], ["Soni", "Ravi", ""], ["Gajek", "Martin", ""], ["Pack", "Gabriel", ""], ["Rangaraj", "Akhil", ""]]}, {"id": "2005.04680", "submitter": "Alexander Heinecke", "authors": "Dhiraj Kalamkar, Evangelos Georganas, Sudarshan Srinivasan, Jianping\n  Chen, Mikhail Shiryaev, Alexander Heinecke", "title": "Optimizing Deep Learning Recommender Systems' Training On CPU Cluster\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last two years, the goal of many researchers has been to squeeze\nthe last bit of performance out of HPC system for AI tasks. Often this\ndiscussion is held in the context of how fast ResNet50 can be trained.\nUnfortunately, ResNet50 is no longer a representative workload in 2020. Thus,\nwe focus on Recommender Systems which account for most of the AI cycles in\ncloud computing centers. More specifically, we focus on Facebook's DLRM\nbenchmark. By enabling it to run on latest CPU hardware and software tailored\nfor HPC, we are able to achieve more than two-orders of magnitude improvement\nin performance (110x) on a single socket compared to the reference CPU\nimplementation, and high scaling efficiency up to 64 sockets, while fitting\nultra-large datasets. This paper discusses the optimization techniques for the\nvarious operators in DLRM and which component of the systems are stressed by\nthese different operators. The presented techniques are applicable to a broader\nset of DL workloads that pose the same scaling challenges/characteristics as\nDLRM.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 14:40:16 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Kalamkar", "Dhiraj", ""], ["Georganas", "Evangelos", ""], ["Srinivasan", "Sudarshan", ""], ["Chen", "Jianping", ""], ["Shiryaev", "Mikhail", ""], ["Heinecke", "Alexander", ""]]}, {"id": "2005.04684", "submitter": "Shen Gao", "authors": "Shen Gao, Xiuying Chen, Zhaochun Ren, Dongyan Zhao and Rui Yan", "title": "From Standard Summarization to New Tasks and Beyond: Summarization with\n  Manifold Information", "comments": "Accepted by IJCAI 2020 Survey Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text summarization is the research area aiming at creating a short and\ncondensed version of the original document, which conveys the main idea of the\ndocument in a few words. This research topic has started to attract the\nattention of a large community of researchers, and it is nowadays counted as\none of the most promising research areas. In general, text summarization\nalgorithms aim at using a plain text document as input and then output a\nsummary. However, in real-world applications, most of the data is not in a\nplain text format. Instead, there is much manifold information to be\nsummarized, such as the summary for a web page based on a query in the search\nengine, extreme long document (e.g., academic paper), dialog history and so on.\nIn this paper, we focus on the survey of these new summarization tasks and\napproaches in the real-world application.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 14:59:36 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Gao", "Shen", ""], ["Chen", "Xiuying", ""], ["Ren", "Zhaochun", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "2005.04806", "submitter": "Lizhen Shi", "authors": "Lizhen Shi, Bo Chen", "title": "Comparison and Benchmark of Graph Clustering Algorithms", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph clustering is widely used in analysis of biological networks, social\nnetworks and etc. For over a decade many graph clustering algorithms have been\npublished, however a comprehensive and consistent performance comparison is not\navailable. In this paper we benchmarked more than 70 graph clustering programs\nto evaluate their runtime and quality performance for both weighted and\nunweighted graphs. We also analyzed the characteristics of ground truth that\naffects the performance. Our work is capable to not only supply a start point\nfor engineers to select clustering algorithms but also could provide a\nviewpoint for researchers to design new algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 22:54:36 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Shi", "Lizhen", ""], ["Chen", "Bo", ""]]}, {"id": "2005.04833", "submitter": "Richard Oentaryo", "authors": "Roy Ka-Wei Lee, Thong Hoang, Richard J. Oentaryo, David Lo", "title": "Keen2Act: Activity Recommendation in Online Social Collaborative\n  Platforms", "comments": "ACM Conference on User Modeling, Adaptation and Personalization", "journal-ref": null, "doi": "10.1145/3340631.3394884", "report-no": null, "categories": "cs.IR cs.LG cs.SE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social collaborative platforms such as GitHub and Stack Overflow have been\nincreasingly used to improve work productivity via collaborative efforts. To\nimprove user experiences in these platforms, it is desirable to have a\nrecommender system that can suggest not only items (e.g., a GitHub repository)\nto a user, but also activities to be performed on the suggested items (e.g.,\nforking a repository). To this end, we propose a new approach dubbed Keen2Act,\nwhich decomposes the recommendation problem into two stages: the Keen and Act\nsteps. The Keen step identifies, for a given user, a (sub)set of items in which\nhe/she is likely to be interested. The Act step then recommends to the user\nwhich activities to perform on the identified set of items. This decomposition\nprovides a practical approach to tackling complex activity recommendation tasks\nwhile producing higher recommendation quality. We evaluate our proposed\napproach using two real-world datasets and obtain promising results whereby\nKeen2Act outperforms several baseline models.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 02:00:52 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Lee", "Roy Ka-Wei", ""], ["Hoang", "Thong", ""], ["Oentaryo", "Richard J.", ""], ["Lo", "David", ""]]}, {"id": "2005.04908", "submitter": "Sebastian Hofst\\\"atter", "authors": "Sebastian Hofst\\\"atter, Hamed Zamani, Bhaskar Mitra, Nick Craswell,\n  Allan Hanbury", "title": "Local Self-Attention over Long Text for Efficient Document Retrieval", "comments": "Accepted at SIGIR 2020 (short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks, particularly Transformer-based architectures, have achieved\nsignificant performance improvements on several retrieval benchmarks. When the\nitems being retrieved are documents, the time and memory cost of employing\nTransformers over a full sequence of document terms can be prohibitive. A\npopular strategy involves considering only the first n terms of the document.\nThis can, however, result in a biased system that under retrieves longer\ndocuments. In this work, we propose a local self-attention which considers a\nmoving window over the document terms and for each term attends only to other\nterms in the same window. This local attention incurs a fraction of the compute\nand memory cost of attention over the whole document. The windowed approach\nalso leads to more compact packing of padded documents in minibatches resulting\nin additional savings. We also employ a learned saturation function and a\ntwo-staged pooling strategy to identify relevant regions of the document. The\nTransformer-Kernel pooling model with these changes can efficiently elicit\nrelevance information from documents with thousands of tokens. We benchmark our\nproposed modifications on the document ranking task from the TREC 2019 Deep\nLearning track and observe significant improvements in retrieval quality as\nwell as increased retrieval of longer documents at moderate increase in compute\nand memory costs.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 08:03:21 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Hofst\u00e4tter", "Sebastian", ""], ["Zamani", "Hamed", ""], ["Mitra", "Bhaskar", ""], ["Craswell", "Nick", ""], ["Hanbury", "Allan", ""]]}, {"id": "2005.04917", "submitter": "Heikki Arponen Dr", "authors": "Heikki Arponen and Tom E. Bishop", "title": "Learning to hash with semantic similarity metrics and empirical KL\n  divergence", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to hash is an efficient paradigm for exact and approximate nearest\nneighbor search from massive databases. Binary hash codes are typically\nextracted from an image by rounding output features from a CNN, which is\ntrained on a supervised binary similar/ dissimilar task. Drawbacks of this\napproach are: (i) resulting codes do not necessarily capture semantic\nsimilarity of the input data (ii) rounding results in information loss,\nmanifesting as decreased retrieval performance and (iii) Using only class-wise\nsimilarity as a target can lead to trivial solutions, simply encoding\nclassifier outputs rather than learning more intricate relations, which is not\ndetected by most performance metrics. We overcome (i) via a novel loss function\nencouraging the relative hash code distances of learned features to match those\nderived from their targets. We address (ii) via a differentiable estimate of\nthe KL divergence between network outputs and a binary target distribution,\nresulting in minimal information loss when the features are rounded to binary.\nFinally, we resolve (iii) by focusing on a hierarchical precision metric.\nEfficiency of the methods is demonstrated with semantic image retrieval on the\nCIFAR-100, ImageNet and Conceptual Captions datasets, using similarities\ninferred from the WordNet label hierarchy or sentence embeddings.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 08:20:26 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Arponen", "Heikki", ""], ["Bishop", "Tom E.", ""]]}, {"id": "2005.04961", "submitter": "Onur G\\\"ok\\c{c}e", "authors": "Onur G\\\"ok\\c{c}e, Jonathan Prada, Nikola I. Nikolov, Nianlong Gu,\n  Richard H. R. Hahnloser", "title": "Embedding-based Scientific Literature Discovery in a Text Editor\n  Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each claim in a research paper requires all relevant prior knowledge to be\ndiscovered, assimilated, and appropriately cited. However, despite the\navailability of powerful search engines and sophisticated text editing\nsoftware, discovering relevant papers and integrating the knowledge into a\nmanuscript remain complex tasks associated with high cognitive load. To define\ncomprehensive search queries requires strong motivation from authors,\nirrespective of their familiarity with the research field. Moreover, switching\nbetween independent applications for literature discovery, bibliography\nmanagement, reading papers, and writing text burdens authors further and\ninterrupts their creative process. Here, we present a web application that\ncombines text editing and literature discovery in an interactive user\ninterface. The application is equipped with a search engine that couples\nBoolean keyword filtering with nearest neighbor search over text embeddings,\nproviding a discovery experience tuned to an author's manuscript and his\ninterests. Our application aims to take a step towards more enjoyable and\neffortless academic writing.\n  The demo of the application (https://SciEditorDemo2020.herokuapp.com/) and a\nshort video tutorial (https://youtu.be/pkdVU60IcRc) are available online.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 09:43:36 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["G\u00f6k\u00e7e", "Onur", ""], ["Prada", "Jonathan", ""], ["Nikolov", "Nikola I.", ""], ["Gu", "Nianlong", ""], ["Hahnloser", "Richard H. R.", ""]]}, {"id": "2005.05046", "submitter": "Paul Diac", "authors": "Paul Diac, Liana \\c{T}uc\\u{a}r, Andrei Netedu", "title": "Relational Model for Parameter Description in Automatic Semantic Web\n  Service Composition", "comments": "International Conference on Knowledge-Based and Intelligent\n  Information & Engineering Systems 10 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.procs.2019.09.219", "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Service Composition is a research direction aimed at facilitating\nthe usage of atomic web services. Particularly, the goal is to build workflows\nof services that solve specific queries, which cannot be resolved by any single\nservice from a known repository. Each of these services is described\nindependently by their providers that can have no interaction with each other,\ntherefore some common standards have been developed, such as WSDL, BPEL, OWL-S.\nOur proposal is to use such standards together with JSON-LD to model a next\nlevel of semantics, mainly based on binary relations between parameters of\nservices. Services relate to a public ontology to describe their functionality.\nBinary relations can be specified between input and/or output parameters in\nservice definition. The ontology includes some relations and inference rules\nthat help to deduce new relations between parameters of services. To our\nknowledge, it is for the first time that parameters are matched not only based\non their type, but on a more meaningful semantic context considering such type\nof relations. This enables the automation of a large part of the reasoning that\na human person would do when manually building a composition. Moreover, the\nproposed model and the composition algorithm can work with multiple objects of\nthe same type, a fundamental feature that was not possible before. We believe\nthat the poor model expressiveness is what is keeping service composition from\nreaching large-scale application in practice.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 09:00:06 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Diac", "Paul", ""], ["\u0162uc\u0103r", "Liana", ""], ["Netedu", "Andrei", ""]]}, {"id": "2005.05516", "submitter": "Venkata Sriram Siddhardh Nadendla", "authors": "Doris E. M. Brown, Venkata Sriram Siddhardh Nadendla", "title": "Framing Effects on Strategic Information Design under Receiver Distrust\n  and Unknown State", "comments": "12 pages, 5 figures; This is a working draft, and can potentially\n  have errors. Any feedback will be greatly appreciated, and will be\n  acknowledged in the subsequent versions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.IR cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strategic information design is a framework where a sender designs\ninformation strategically to steer its receiver's decision towards a desired\nchoice. Traditionally, such frameworks have always assumed that the sender and\nthe receiver comprehends the state of the choice environment, and that the\nreceiver always trusts the sender's signal. This paper deviates from these\nassumptions and re-investigates strategic information design in the presence of\ndistrustful receiver and when both sender and receiver cannot\nobserve/comprehend the environment state space. Specifically, we assume that\nboth sender and receiver has access to non-identical beliefs about choice\nrewards (with sender's belief being more accurate), but not the environment\nstate that determines these rewards. Furthermore, given that the receiver does\nnot trust the sender, we also assume that the receiver updates its prior in a\nnon-Bayesian manner. We evaluate the Stackelberg equilibrium and investigate\neffects of information framing (i.e. send complete signal, or just expected\nvalue of the signal) on the equilibrium. Furthermore, we also investigate trust\ndynamics at the receiver, under the assumption that the receiver minimizes\nregret in hindsight. Simulation results are presented to illustrate signaling\neffects and trust dynamics in strategic information design.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 01:55:56 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 22:16:37 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Brown", "Doris E. M.", ""], ["Nadendla", "Venkata Sriram Siddhardh", ""]]}, {"id": "2005.05754", "submitter": "Angrosh Mandya", "authors": "Angrosh Mandya, James O'Neill, Danushka Bollegala, and Frans Coenen", "title": "Do not let the history haunt you -- Mitigating Compounding Errors in\n  Conversational Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Conversational Question Answering (CoQA) task involves answering a\nsequence of inter-related conversational questions about a contextual\nparagraph. Although existing approaches employ human-written ground-truth\nanswers for answering conversational questions at test time, in a realistic\nscenario, the CoQA model will not have any access to ground-truth answers for\nthe previous questions, compelling the model to rely upon its own previously\npredicted answers for answering the subsequent questions. In this paper, we\nfind that compounding errors occur when using previously predicted answers at\ntest time, significantly lowering the performance of CoQA systems. To solve\nthis problem, we propose a sampling strategy that dynamically selects between\ntarget answers and model predictions during training, thereby closely\nsimulating the situation at test time. Further, we analyse the severity of this\nphenomena as a function of the question type, conversation length and domain\ntype.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 13:29:38 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Mandya", "Angrosh", ""], ["O'Neill", "James", ""], ["Bollegala", "Danushka", ""], ["Coenen", "Frans", ""]]}, {"id": "2005.05768", "submitter": "Jaekeol Choi", "authors": "Jaekeol Choi and Jungin Choi and Wonjong Rhee", "title": "Interpreting Neural Ranking Models using Grad-CAM", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, applying deep neural networks in IR has become an important and\ntimely topic. For instance, Neural Ranking Models(NRMs) have shown promising\nperformance compared to the traditional ranking models. However, explaining the\nranking results has become even more difficult with NRM due to the complex\nstructure of neural networks. On the other hand, a great deal of research is\nunder progress on Interpretable Machine Learning(IML), including Grad-CAM.\nGrad-CAM is an attribution method and it can visualize the input regions that\ncontribute to the network's output. In this paper, we adopt Grad-CAM for\ninterpreting the ranking results of NRM. By adopting Grad-CAM, we analyze how\neach query-document term pair contributes to the matching score for a given\npair of query and document. The visualization results provide insights on why a\ncertain document is relevant to the given query. Also, the results show that\nneural ranking model captures the subtle notion of relevance. Our\ninterpretation method and visualization results can be used for snippet\ngeneration and user-query intent analysis.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 13:43:37 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Choi", "Jaekeol", ""], ["Choi", "Jungin", ""], ["Rhee", "Wonjong", ""]]}, {"id": "2005.05854", "submitter": "Preslav Nakov", "authors": "Giovanni Da San Martino, Shaden Shaar, Yifan Zhang, Seunghak Yu,\n  Alberto Barr\\'on-Cede\\~no, Preslav Nakov", "title": "Prta: A System to Support the Analysis of Propaganda Techniques in the\n  News", "comments": "propaganda, disinformation, fake news, media bias, COVID-19", "journal-ref": "ACL-2020", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent events, such as the 2016 US Presidential Campaign, Brexit and the\nCOVID-19 \"infodemic\", have brought into the spotlight the dangers of online\ndisinformation. There has been a lot of research focusing on fact-checking and\ndisinformation detection. However, little attention has been paid to the\nspecific rhetorical and psychological techniques used to convey propaganda\nmessages. Revealing the use of such techniques can help promote media literacy\nand critical thinking, and eventually contribute to limiting the impact of\n\"fake news\" and disinformation campaigns. Prta (Propaganda Persuasion\nTechniques Analyzer) allows users to explore the articles crawled on a regular\nbasis by highlighting the spans in which propaganda techniques occur and to\ncompare them on the basis of their use of propaganda techniques. The system\nfurther reports statistics about the use of such techniques, overall and over\ntime, or according to filtering criteria specified by the user based on time\ninterval, keywords, and/or political orientation of the media. Moreover, it\nallows users to analyze any text or URL through a dedicated interface or via an\nAPI. The system is available online: https://www.tanbih.org/prta\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 15:20:55 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Martino", "Giovanni Da San", ""], ["Shaar", "Shaden", ""], ["Zhang", "Yifan", ""], ["Yu", "Seunghak", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["Nakov", "Preslav", ""]]}, {"id": "2005.05954", "submitter": "Md. Tawkat Islam Khondaker", "authors": "Junaed Younus Khan, Md. Tawkat Islam Khondaker, Iram Tazim Hoque,\n  Hamada Al-Absi, Mohammad Saifur Rahman, Tanvir Alam, M. Sohel Rahman", "title": "COVID-19Base: A knowledgebase to explore biomedical entities related to\n  COVID-19", "comments": "10 pages, 3 figures", "journal-ref": "JMIR Med Inform 2020;8(11):e21648", "doi": "10.2196/21648", "report-no": null, "categories": "cs.IR cs.CL cs.DL cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are presenting COVID-19Base, a knowledgebase highlighting the biomedical\nentities related to COVID-19 disease based on literature mining. To develop\nCOVID-19Base, we mine the information from publicly available scientific\nliterature and related public resources. We considered seven topic-specific\ndictionaries, including human genes, human miRNAs, human lncRNAs, diseases,\nProtein Databank, drugs, and drug side effects, are integrated to mine all\nscientific evidence related to COVID-19. We have employed an automated\nliterature mining and labeling system through a novel approach to measure the\neffectiveness of drugs against diseases based on natural language processing,\nsentiment analysis, and deep learning. To the best of our knowledge, this is\nthe first knowledgebase dedicated to COVID-19, which integrates such large\nvariety of related biomedical entities through literature mining. Proper\ninvestigation of the mined biomedical entities along with the identified\ninteractions among those, reported in COVID-19Base, would help the research\ncommunity to discover possible ways for the therapeutic treatment of COVID-19.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 17:55:00 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Khan", "Junaed Younus", ""], ["Khondaker", "Md. Tawkat Islam", ""], ["Hoque", "Iram Tazim", ""], ["Al-Absi", "Hamada", ""], ["Rahman", "Mohammad Saifur", ""], ["Alam", "Tanvir", ""], ["Rahman", "M. Sohel", ""]]}, {"id": "2005.05968", "submitter": "Minsoo Rhu", "authors": "Ranggi Hwang, Taehun Kim, Youngeun Kwon, Minsoo Rhu", "title": "Centaur: A Chiplet-based, Hybrid Sparse-Dense Accelerator for\n  Personalized Recommendations", "comments": "Accepted for publication at the 47th IEEE/ACM International Symposium\n  on Computer Architecture (ISCA-47), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized recommendations are the backbone machine learning (ML) algorithm\nthat powers several important application domains (e.g., ads, e-commerce, etc)\nserviced from cloud datacenters. Sparse embedding layers are a crucial building\nblock in designing recommendations yet little attention has been paid in\nproperly accelerating this important ML algorithm. This paper first provides a\ndetailed workload characterization on personalized recommendations and\nidentifies two significant performance limiters: memory-intensive embedding\nlayers and compute-intensive multi-layer perceptron (MLP) layers. We then\npresent Centaur, a chiplet-based hybrid sparse-dense accelerator that addresses\nboth the memory throughput challenges of embedding layers and the compute\nlimitations of MLP layers. We implement and demonstrate our proposal on an\nIntel HARPv2, a package-integrated CPU+FPGA device, which shows a 1.7-17.2x\nperformance speedup and 1.7-19.5x energy-efficiency improvement than\nconventional approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 07:53:35 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Hwang", "Ranggi", ""], ["Kim", "Taehun", ""], ["Kwon", "Youngeun", ""], ["Rhu", "Minsoo", ""]]}, {"id": "2005.06058", "submitter": "Preslav Nakov", "authors": "Shaden Shaar, Giovanni Da San Martino, Nikolay Babulkov, Preslav Nakov", "title": "That is a Known Lie: Detecting Previously Fact-Checked Claims", "comments": "detecting previously fact-checked claims, fact-checking,\n  disinformation, fake news, social media, political debates", "journal-ref": "ACL-2020", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent proliferation of \"fake news\" has triggered a number of responses,\nmost notably the emergence of several manual fact-checking initiatives. As a\nresult and over time, a large number of fact-checked claims have been\naccumulated, which increases the likelihood that a new claim in social media or\na new statement by a politician might have already been fact-checked by some\ntrusted fact-checking organization, as viral claims often come back after a\nwhile in social media, and politicians like to repeat their favorite\nstatements, true or false, over and over again. As manual fact-checking is very\ntime-consuming (and fully automatic fact-checking has credibility issues), it\nis important to try to save this effort and to avoid wasting time on claims\nthat have already been fact-checked. Interestingly, despite the importance of\nthe task, it has been largely ignored by the research community so far. Here,\nwe aim to bridge this gap. In particular, we formulate the task and we discuss\nhow it relates to, but also differs from, previous work. We further create a\nspecialized dataset, which we release to the research community. Finally, we\npresent learning-to-rank experiments that demonstrate sizable improvements over\nstate-of-the-art retrieval and textual similarity approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 21:25:37 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Shaar", "Shaden", ""], ["Martino", "Giovanni Da San", ""], ["Babulkov", "Nikolay", ""], ["Nakov", "Preslav", ""]]}, {"id": "2005.06154", "submitter": "Shantanu Sharma", "authors": "Sharad Mehrotra, Shantanu Sharma, Jeffrey D. Ullman, Dhrubajyoti\n  Ghosh, Peeyush Gupta", "title": "Panda: Partitioned Data Security on Outsourced Sensitive and\n  Non-sensitive Data", "comments": "This version has been accepted in ACM Transactions on Management\n  Information Systems. The final published version of this paper may differ\n  from this accepted version. A preliminary version of this paper\n  [arXiv:1812.09233] was accepted and presented in IEEE ICDE 2019", "journal-ref": null, "doi": "10.1145/3397521", "report-no": null, "categories": "cs.DB cs.CR cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite extensive research on cryptography, secure and efficient query\nprocessing over outsourced data remains an open challenge. This paper continues\nalong with the emerging trend in secure data processing that recognizes that\nthe entire dataset may not be sensitive, and hence, non-sensitivity of data can\nbe exploited to overcome limitations of existing encryption-based approaches.\nWe, first, provide a new security definition, entitled partitioned data\nsecurity for guaranteeing that the joint processing of non-sensitive data (in\ncleartext) and sensitive data (in encrypted form) does not lead to any leakage.\nThen, this paper proposes a new secure approach, entitled query binning (QB)\nthat allows secure execution of queries over non-sensitive and sensitive parts\nof the data. QB maps a query to a set of queries over the sensitive and\nnon-sensitive data in a way that no leakage will occur due to the joint\nprocessing over sensitive and non-sensitive data. In particular, we propose\nsecure algorithms for selection, range, and join queries to be executed over\nencrypted sensitive and cleartext non-sensitive datasets. Interestingly, in\naddition to improving performance, we show that QB actually strengthens the\nsecurity of the underlying cryptographic technique by preventing size,\nfrequency-count, and workload-skew attacks.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 05:27:18 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Mehrotra", "Sharad", ""], ["Sharma", "Shantanu", ""], ["Ullman", "Jeffrey D.", ""], ["Ghosh", "Dhrubajyoti", ""], ["Gupta", "Peeyush", ""]]}, {"id": "2005.06213", "submitter": "Giulio Ermanno Pibiri", "authors": "Simon Gog, Giulio Ermanno Pibiri, and Rossano Venturini", "title": "Efficient and Effective Query Auto-Completion", "comments": "Published in SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401432", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query Auto-Completion (QAC) is an ubiquitous feature of modern textual search\nsystems, suggesting possible ways of completing the query being typed by the\nuser. Efficiency is crucial to make the system have a real-time responsiveness\nwhen operating in the million-scale search space. Prior work has extensively\nadvocated the use of a trie data structure for fast prefix-search operations in\ncompact space. However, searching by prefix has little discovery power in that\nonly completions that are prefixed by the query are returned. This may impact\nnegatively the effectiveness of the QAC system, with a consequent monetary loss\nfor real applications like Web Search Engines and eCommerce. In this work we\ndescribe the implementation that empowers a new QAC system at eBay, and discuss\nits efficiency/effectiveness in relation to other approaches at the\nstate-of-the-art. The solution is based on the combination of an inverted index\nwith succinct data structures, a much less explored direction in the\nliterature. This system is replacing the previous implementation based on\nApache SOLR that was not always able to meet the required\nservice-level-agreement.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 09:07:43 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 08:28:57 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Gog", "Simon", ""], ["Pibiri", "Giulio Ermanno", ""], ["Venturini", "Rossano", ""]]}, {"id": "2005.06259", "submitter": "Johannes Stegmann Dr.", "authors": "Johannes Stegmann", "title": "MeSH descriptors indicate the knowledge growth in the\n  SARS-CoV-2/COVID-19 pandemic", "comments": "6 pages, 3 fgures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scientific papers dealing with the novel betacoronavirus SARS-CoV-2 and\nthe coronavirus disease 2019 (COVID-19) caused by this virus, published in 2020\nand recorded in the database PUBMED, were retrieved on April 27, 2020. About\n20\\% of the records contain Medical Subject Headings (MeSH), keywords assigned\nto records in the course of the indexing process in order to summarise the\narticles' contents. The temporal sequence of the first occurrences of the\nkeywords was determined, thus giving insight into the growth of the knowledge\nbase of the pandemic.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 11:40:51 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Stegmann", "Johannes", ""]]}, {"id": "2005.06331", "submitter": "Jacek Dabrowski", "authors": "Anna Wroblewska (1 and 2), Jacek Dabrowski (1), Michal Pastuszak (1),\n  Andrzej Michalowski (1), Michal Daniluk (1), Barbara Rychalska (1 and 2),\n  Mikolaj Wieczorek (1), Sylwia Sysko-Romanczuk (2) ((1) Synerise, (2) Warsaw\n  University of Technology)", "title": "Multi-modal Embedding Fusion-based Recommender", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems have lately been popularized globally, with primary\nuse cases in online interaction systems, with significant focus on e-commerce\nplatforms. We have developed a machine learning-based recommendation platform,\nwhich can be easily applied to almost any items and/or actions domain. Contrary\nto existing recommendation systems, our platform supports multiple types of\ninteraction data with multiple modalities of metadata natively. This is\nachieved through multi-modal fusion of various data representations. We\ndeployed the platform into multiple e-commerce stores of different kinds, e.g.\nfood and beverages, shoes, fashion items, telecom operators. Here, we present\nour system, its flexibility and performance. We also show benchmark results on\nopen datasets, that significantly outperform state-of-the-art prior work.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 14:13:35 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 11:45:22 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Wroblewska", "Anna", "", "1 and 2"], ["Dabrowski", "Jacek", "", "1 and 2"], ["Pastuszak", "Michal", "", "1 and 2"], ["Michalowski", "Andrzej", "", "1 and 2"], ["Daniluk", "Michal", "", "1 and 2"], ["Rychalska", "Barbara", "", "1 and 2"], ["Wieczorek", "Mikolaj", ""], ["Sysko-Romanczuk", "Sylwia", ""]]}, {"id": "2005.06370", "submitter": "Tomer Wullach", "authors": "Tomer Wullach, Amir Adler, Einat Minkov", "title": "Towards Hate Speech Detection at Large via Deep Generative Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hate speech detection is a critical problem in social media platforms, being\noften accused for enabling the spread of hatred and igniting physical violence.\nHate speech detection requires overwhelming resources including\nhigh-performance computing for online posts and tweets monitoring as well as\nthousands of human experts for daily screening of suspected posts or tweets.\nRecently, Deep Learning (DL)-based solutions have been proposed for automatic\ndetection of hate speech, using modest-sized training datasets of few thousands\nof hate speech sequences. While these methods perform well on the specific\ndatasets, their ability to detect new hate speech sequences is limited and has\nnot been investigated. Being a data-driven approach, it is well known that DL\nsurpasses other methods whenever a scale-up in train dataset size and diversity\nis achieved. Therefore, we first present a dataset of 1 million realistic hate\nand non-hate sequences, produced by a deep generative language model. We\nfurther utilize the generated dataset to train a well-studied DL-based hate\nspeech detector, and demonstrate consistent and significant performance\nimprovements across five public hate speech datasets. Therefore, the proposed\nsolution enables high sensitivity detection of a very large variety of hate\nspeech sequences, paving the way to a fully automatic solution.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 15:25:59 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Wullach", "Tomer", ""], ["Adler", "Amir", ""], ["Minkov", "Einat", ""]]}, {"id": "2005.06377", "submitter": "Forrest Bao", "authors": "Forrest Sheng Bao, Hebi Li, Ge Luo, Cen Chen, Yinfei Yang, Youbiao He,\n  Minghui Qiu", "title": "End-to-end Semantics-based Summary Quality Assessment for\n  Single-document Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical automatic summary evaluation metrics, such as ROUGE, suffer from\ntwo drawbacks. First, semantic similarity and linguistic quality are not\ncaptured well. Second, a reference summary, which is expensive or impossible to\nobtain in many cases, is needed. Existing efforts to address the two drawbacks\nare done separately and have limitations. To holistically address them, we\nintroduce an end-to-end approach for summary quality assessment by leveraging\nsentence or document embedding and introducing two negative sampling approaches\nto create training data for this supervised approach. The proposed approach\nexhibits promising results on several summarization datasets of various domains\nincluding news, legislative bills, scientific papers, and patents. When rating\nmachine-generated summaries in TAC2010, our approach outperforms ROUGE in terms\nof linguistic quality, and achieves a correlation coefficient of up to 0.5702\nwith human evaluations in terms of modified pyramid scores. We hope our\napproach can facilitate summarization research or applications when reference\nsummaries are infeasible or costly to obtain, or when linguistic quality is a\nfocus.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 15:40:13 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 22:43:05 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Bao", "Forrest Sheng", ""], ["Li", "Hebi", ""], ["Luo", "Ge", ""], ["Chen", "Cen", ""], ["Yang", "Yinfei", ""], ["He", "Youbiao", ""], ["Qiu", "Minghui", ""]]}, {"id": "2005.06380", "submitter": "Pierre Le Bras", "authors": "Pierre Le Bras, Azimeh Gharavi, David A. Robb, Ana F. Vidal, Stefano\n  Padilla, Mike J. Chantler", "title": "Visualising COVID-19 Research", "comments": "11 pages. 10 figures. Preprint paper made available here prior to\n  submission. Update: special characters corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The world has seen in 2020 an unprecedented global outbreak of SARS-CoV-2, a\nnew strain of coronavirus, causing the COVID-19 pandemic, and radically\nchanging our lives and work conditions. Many scientists are working tirelessly\nto find a treatment and a possible vaccine. Furthermore, governments,\nscientific institutions and companies are acting quickly to make resources\navailable, including funds and the opening of large-volume data repositories,\nto accelerate innovation and discovery aimed at solving this pandemic. In this\npaper, we develop a novel automated theme-based visualisation method, combining\nadvanced data modelling of large corpora, information mapping and trend\nanalysis, to provide a top-down and bottom-up browsing and search interface for\nquick discovery of topics and research resources. We apply this method on two\nrecently released publications datasets (Dimensions' COVID-19 dataset and the\nAllen Institute for AI's CORD-19). The results reveal intriguing information\nincluding increased efforts in topics such as social distancing; cross-domain\ninitiatives (e.g. mental health and education); evolving research in medical\ntopics; and the unfolding trajectory of the virus in different territories\nthrough publications. The results also demonstrate the need to quickly and\nautomatically enable search and browsing of large corpora. We believe our\nmethodology will improve future large volume visualisation and discovery\nsystems but also hope our visualisation interfaces will currently aid\nscientists, researchers, and the general public to tackle the numerous issues\nin the fight against the COVID-19 pandemic.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 15:45:14 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 10:06:39 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Bras", "Pierre Le", ""], ["Gharavi", "Azimeh", ""], ["Robb", "David A.", ""], ["Vidal", "Ana F.", ""], ["Padilla", "Stefano", ""], ["Chantler", "Mike J.", ""]]}, {"id": "2005.06434", "submitter": "Mohamed Ghalwash", "authors": "Mohamed Ghalwash, Zijun Yao, Prithwish Chakrabotry, James Codella,\n  Daby Sow", "title": "ODVICE: An Ontology-Driven Visual Analytic Tool for Interactive Cohort\n  Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increased availability of electronic health records (EHR) has enabled\nresearchers to study various medical questions. Cohort selection for the\nhypothesis under investigation is one of the main consideration for EHR\nanalysis. For uncommon diseases, cohorts extracted from EHRs contain very\nlimited number of records - hampering the robustness of any analysis. Data\naugmentation methods have been successfully applied in other domains to address\nthis issue mainly using simulated records. In this paper, we present ODVICE, a\ndata augmentation framework that leverages the medical concept ontology to\nsystematically augment records using a novel ontologically guided Monte-Carlo\ngraph spanning algorithm. The tool allows end users to specify a small set of\ninteractive controls to control the augmentation process. We analyze the\nimportance of ODVICE by conducting studies on MIMIC-III dataset for two\nlearning tasks. Our results demonstrate the predictive performance of ODVICE\naugmented cohorts, showing ~30% improvement in area under the curve (AUC) over\nthe non-augmented dataset and other data augmentation strategies.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 17:15:51 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Ghalwash", "Mohamed", ""], ["Yao", "Zijun", ""], ["Chakrabotry", "Prithwish", ""], ["Codella", "James", ""], ["Sow", "Daby", ""]]}, {"id": "2005.06469", "submitter": "Erich Bremer", "authors": "Erich Bremer, Jonas Almeida, Joel Saltz", "title": "Representing Whole Slide Cancer Image Features with Hilbert Curves", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DB cs.IR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regions of Interest (ROI) contain morphological features in pathology whole\nslide images (WSI) are delimited with polygons[1]. These polygons are often\nrepresented in either a textual notation (with the array of edges) or in a\nbinary mask form. Textual notations have an advantage of human readability and\nportability, whereas, binary mask representations are more useful as the input\nand output of feature-extraction pipelines that employ deep learning\nmethodologies. For any given whole slide image, more than a million cellular\nfeatures can be segmented generating a corresponding number of polygons. The\ncorpus of these segmentations for all processed whole slide images creates\nvarious challenges for filtering specific areas of data for use in interactive\nreal-time and multi-scale displays and analysis. Simple range queries of image\nlocations do not scale and, instead, spatial indexing schemes are required. In\nthis paper we propose using Hilbert Curves simultaneously for spatial indexing\nand as a polygonal ROI representation. This is achieved by using a series of\nHilbert Curves[2] creating an efficient and inherently spatially-indexed\nmachine-usable form. The distinctive property of Hilbert curves that enables\nboth mask and polygon delimitation of ROIs is that the elements of the vector\nextracted ro describe morphological features maintain their relative positions\nfor different scales of the same image.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 16:38:24 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Bremer", "Erich", ""], ["Almeida", "Jonas", ""], ["Saltz", "Joel", ""]]}, {"id": "2005.06584", "submitter": "Yusan Lin", "authors": "Maryam Moosaei, Yusan Lin, Hao Yang", "title": "Fashion Recommendation and Compatibility Prediction Using Relational\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fashion is an inherently visual concept and computer vision and artificial\nintelligence (AI) are playing an increasingly important role in shaping the\nfuture of this domain. Many research has been done on recommending fashion\nproducts based on the learned user preferences. However, in addition to\nrecommending single items, AI can also help users create stylish outfits from\nitems they already have, or purchase additional items that go well with their\ncurrent wardrobe. Compatibility is the key factor in creating stylish outfits\nfrom single items. Previous studies have mostly focused on modeling pair-wise\ncompatibility. There are a few approaches that consider an entire outfit, but\nthese approaches have limitations such as requiring rich semantic information,\ncategory labels, and fixed order of items. Thus, they fail to effectively\ndetermine compatibility when such information is not available. In this work,\nwe adopt a Relation Network (RN) to develop new compatibility learning models,\nFashion RN and FashionRN-VSE, that addresses the limitations of existing\napproaches. FashionRN learns the compatibility of an entire outfit, with an\narbitrary number of items, in an arbitrary order. We evaluated our model using\na large dataset of 49,740 outfits that we collected from Polyvore website.\nQuantitatively, our experimental results demonstrate state of the art\nperformance compared with alternative methods in the literature in both\ncompatibility prediction and fill-in-the-blank test. Qualitatively, we also\nshow that the item embedding learned by FashionRN indicate the compatibility\namong fashion items.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 21:00:54 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Moosaei", "Maryam", ""], ["Lin", "Yusan", ""], ["Yang", "Hao", ""]]}, {"id": "2005.06591", "submitter": "Thiago H. Silva", "authors": "Gustavo Santos, Vinicius F. S. Mota, Fabricio Benevenuto, Thiago H.\n  Silva", "title": "Neutrality May Matter: Sentiment Analysis in Reviews of Airbnb, Booking,\n  and Couchsurfing in Brazil and USA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information and communications technologies have enabled the rise of the\nphenomenon named sharing economy, which represents activities between people,\ncoordinated by online platforms, to obtain, provide, or share access to goods\nand services. In hosting services of the sharing economy, it is common to have\na personal contact between the host and guest, and this may affect users'\ndecision to do negative reviews, as negative reviews can damage the offered\nservices. To evaluate this issue, we collected reviews from two sharing economy\nplatforms, Airbnb and Couchsurfing, and from one platform that works mostly\nwith hotels (traditional economy), Booking.com, for some cities in Brazil and\nthe USA. Trough a sentiment analysis, we found that reviews in the sharing\neconomy tend to be considerably more positive than those in the traditional\neconomy. This can represent a problem in those systems, as an experiment with\nvolunteers performed in this study suggests. In addition, we discuss how to\nexploit the results obtained to help improve users' decision making.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 21:24:05 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Santos", "Gustavo", ""], ["Mota", "Vinicius F. S.", ""], ["Benevenuto", "Fabricio", ""], ["Silva", "Thiago H.", ""]]}, {"id": "2005.06601", "submitter": "Tengteng Zhang", "authors": "Tengteng Zhang, Yiqin Yu, Jing Mei, Zefang Tang, Xiang Zhang, Shaochun\n  Li", "title": "Unlocking the Power of Deep PICO Extraction: Step-wise Medical NER\n  Identification", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PICO framework (Population, Intervention, Comparison, and Outcome) is\nusually used to formulate evidence in the medical domain. The major task of\nPICO extraction is to extract sentences from medical literature and classify\nthem into each class. However, in most circumstances, there will be more than\none evidences in an extracted sentence even it has been categorized to a\ncertain class. In order to address this problem, we propose a step-wise disease\nNamed Entity Recognition (DNER) extraction and PICO identification method. With\nour method, sentences in paper title and abstract are first classified into\ndifferent classes of PICO, and medical entities are then identified and\nclassified into P and O. Different kinds of deep learning frameworks are used\nand experimental results show that our method will achieve high performance and\nfine-grained extraction results comparing with conventional PICO extraction\nworks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 03:09:17 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Zhang", "Tengteng", ""], ["Yu", "Yiqin", ""], ["Mei", "Jing", ""], ["Tang", "Zefang", ""], ["Zhang", "Xiang", ""], ["Li", "Shaochun", ""]]}, {"id": "2005.06692", "submitter": "Dehong Gao", "authors": "Dehong Gao, Wenjing Yang, Huiling Zhou, Yi Wei, Yi Hu and Hao Wang", "title": "Deep Hierarchical Classification for Category Prediction in E-commerce\n  System", "comments": "5pages, to be published in ECNLP workshop of ACL20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In e-commerce system, category prediction is to automatically predict\ncategories of given texts. Different from traditional classification where\nthere are no relations between classes, category prediction is reckoned as a\nstandard hierarchical classification problem since categories are usually\norganized as a hierarchical tree. In this paper, we address hierarchical\ncategory prediction. We propose a Deep Hierarchical Classification framework,\nwhich incorporates the multi-scale hierarchical information in neural networks\nand introduces a representation sharing strategy according to the category\ntree. We also define a novel combined loss function to punish hierarchical\nprediction losses. The evaluation shows that the proposed approach outperforms\nexisting approaches in accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 02:29:14 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Gao", "Dehong", ""], ["Yang", "Wenjing", ""], ["Zhou", "Huiling", ""], ["Wei", "Yi", ""], ["Hu", "Yi", ""], ["Wang", "Hao", ""]]}, {"id": "2005.06748", "submitter": "Philipp Mayr", "authors": "S\\'ergio Nunes, Suzanne Little, Sumit Bhatia, Ludovico Boratto,\n  Guillaume Cabanac, Ricardo Campos, Francisco M. Couto, Stefano Faralli, Ingo\n  Frommholz, Adam Jatowt, Al\\'ipio Jorge, Mirko Marras, Philipp Mayr, Giovanni\n  Stilo", "title": "ECIR 2020 Workshops: Assessing the Impact of Going Online", "comments": "10 pages, 3 figures, submitted to ACM SIGIR Forum", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ECIR 2020 https://ecir2020.org/ was one of the many conferences affected by\nthe COVID-19 pandemic. The Conference Chairs decided to keep the initially\nplanned dates (April 14-17, 2020) and move to a fully online event. In this\nreport, we describe the experience of organizing the ECIR 2020 Workshops in\nthis scenario from two perspectives: the workshop organizers and the workshop\nparticipants. We provide a report on the organizational aspect of these events\nand the consequences for participants. Covering the scientific dimension of\neach workshop is outside the scope of this article.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 06:49:22 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Nunes", "S\u00e9rgio", ""], ["Little", "Suzanne", ""], ["Bhatia", "Sumit", ""], ["Boratto", "Ludovico", ""], ["Cabanac", "Guillaume", ""], ["Campos", "Ricardo", ""], ["Couto", "Francisco M.", ""], ["Faralli", "Stefano", ""], ["Frommholz", "Ingo", ""], ["Jatowt", "Adam", ""], ["Jorge", "Al\u00edpio", ""], ["Marras", "Mirko", ""], ["Mayr", "Philipp", ""], ["Stilo", "Giovanni", ""]]}, {"id": "2005.06915", "submitter": "Damiano Spina", "authors": "Kevin Roitero, Michael Soprano, Shaoyang Fan, Damiano Spina, Stefano\n  Mizzaro, and Gianluca Demartini", "title": "Can The Crowd Identify Misinformation Objectively? The Effects of\n  Judgment Scale and Assessor's Background", "comments": "Preprint of the full paper accepted at SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401112", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Truthfulness judgments are a fundamental step in the process of fighting\nmisinformation, as they are crucial to train and evaluate classifiers that\nautomatically distinguish true and false statements. Usually such judgments are\nmade by experts, like journalists for political statements or medical doctors\nfor medical statements. In this paper, we follow a different approach and rely\non (non-expert) crowd workers. This of course leads to the following research\nquestion: Can crowdsourcing be reliably used to assess the truthfulness of\ninformation and to create large-scale labeled collections for information\ncredibility systems? To address this issue, we present the results of an\nextensive study based on crowdsourcing: we collect thousands of truthfulness\nassessments over two datasets, and we compare expert judgments with crowd\njudgments, expressed on scales with various granularity levels. We also measure\nthe political bias and the cognitive background of the workers, and quantify\ntheir effect on the reliability of the data provided by the crowd.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 12:37:48 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 09:11:47 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 01:54:21 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Roitero", "Kevin", ""], ["Soprano", "Michael", ""], ["Fan", "Shaoyang", ""], ["Spina", "Damiano", ""], ["Mizzaro", "Stefano", ""], ["Demartini", "Gianluca", ""]]}, {"id": "2005.06916", "submitter": "Andreas Burgdorf", "authors": "Andreas Burgdorf and Andr\\'e Pomp and Tobias Meisen", "title": "Towards NLP-supported Semantic Data Management", "comments": "Accepted for the Doctoral Consortium of the ICEIS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The heterogeneity of data poses a great challenge when data from different\nsources is to be merged for one application. Solutions for this are offered,\nfor example, by ontology-based data management (OBDM). A challenge of OBDM is\nthe automatic creation of semantic models from datasets. This process is\ntypically performed either data- or label-driven and always involves manual\nhuman intervention. We identified textual descriptions of data, a form of\nmetadata, quickly to be produced and consumed by humans, as third possible\nbasis for automatic semantic modelling. In this paper, we present, how we plan\nto use textual descriptions to enhance semantic data management. We will use\nstate of the art NLP technologies to identify concepts within textual\ndescriptions and build semantic models from this in combination with an\nevolving ontology. We will use automatically identified models in combination\nwith the human data provider to automatically extend the ontology so that it\nlearns new verified concepts over time. Finally, we will use the created\nontology and automatically identified semantic models to either rate\ndescriptions for new data sources or even to automatically generate descriptive\ntexts that are easier to understand by the human user than formal models. We\npresent the procedure which we plan for the ongoing research, as well as\nexpected outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 12:38:02 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Burgdorf", "Andreas", ""], ["Pomp", "Andr\u00e9", ""], ["Meisen", "Tobias", ""]]}, {"id": "2005.06963", "submitter": "Ripon Patgiri", "authors": "Ripon Patgiri and Sabuzima Nayak", "title": "A Survey on Large Scale Metadata Server for Big Data Storage", "comments": "Submitted to ACM for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Big Data is defined as high volume of variety of data with an exponential\ndata growth rate. Data are amalgamated to generate revenue, which results a\nlarge data silo. Data are the oils of modern IT industries. Therefore, the data\nare growing at an exponential pace. The access mechanism of these data silos\nare defined by metadata. The metadata are decoupled from data server for\nvarious beneficial reasons. For instance, ease of maintenance. The metadata are\nstored in metadata server (MDS). Therefore, the study on the MDS is mandatory\nin designing of a large scale storage system. The MDS requires many parameters\nto augment with its architecture. The architecture of MDS depends on the demand\nof the storage system's requirements. Thus, MDS is categorized in various ways\ndepending on the underlying architecture and design methodology. The article\nsurveys on the various kinds of MDS architecture, designs, and methodologies.\nThis article emphasizes on clustered MDS (cMDS) and the reports are prepared\nbased on a) Bloom filter$-$based MDS, b) Client$-$funded MDS, c) Geo$-$aware\nMDS, d) Cache$-$aware MDS, e) Load$-$aware MDS, f) Hash$-$based MDS, and g)\nTree$-$based MDS. Additionally, the article presents the issues and challenges\nof MDS for mammoth sized data.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 20:49:58 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Patgiri", "Ripon", ""], ["Nayak", "Sabuzima", ""]]}, {"id": "2005.07026", "submitter": "Fahad Shamshad", "authors": "Fahad Shamshad, Asif Hanif, Ali Ahmed", "title": "Subsampled Fourier Ptychography using Pretrained Invertible and\n  Untrained Network Priors", "comments": "Part of this work has been accepted in NeurIPS Deep Inverse Workshop,\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.IR cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently pretrained generative models have shown promising results for\nsubsampled Fourier Ptychography (FP) in terms of quality of reconstruction for\nextremely low sampling rate and high noise. However, one of the significant\ndrawbacks of these pretrained generative priors is their limited representation\ncapabilities. Moreover, training these generative models requires access to a\nlarge number of fully-observed clean samples of a particular class of images\nlike faces or digits that is prohibitive to obtain in the context of FP. In\nthis paper, we propose to leverage the power of pretrained invertible and\nuntrained generative models to mitigate the representation error issue and\nrequirement of a large number of example images (for training generative\nmodels) respectively. Through extensive experiments, we demonstrate the\neffectiveness of proposed approaches in the context of FP for low sampling\nrates and high noise levels.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 16:13:01 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Shamshad", "Fahad", ""], ["Hanif", "Asif", ""], ["Ahmed", "Ali", ""]]}, {"id": "2005.07105", "submitter": "Colin Lockard", "authors": "Colin Lockard, Prashant Shiralkar, Xin Luna Dong, Hannaneh Hajishirzi", "title": "ZeroShotCeres: Zero-Shot Relation Extraction from Semi-Structured\n  Webpages", "comments": "Accepted to ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many documents, such as semi-structured webpages, textual semantics are\naugmented with additional information conveyed using visual elements including\nlayout, font size, and color. Prior work on information extraction from\nsemi-structured websites has required learning an extraction model specific to\na given template via either manually labeled or distantly supervised data from\nthat template. In this work, we propose a solution for \"zero-shot\" open-domain\nrelation extraction from webpages with a previously unseen template, including\nfrom websites with little overlap with existing sources of knowledge for\ndistant supervision and websites in entirely new subject verticals. Our model\nuses a graph neural network-based approach to build a rich representation of\ntext fields on a webpage and the relationships between them, enabling\ngeneralization to new templates. Experiments show this approach provides a 31%\nF1 gain over a baseline for zero-shot extraction in a new subject vertical.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 16:15:58 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Lockard", "Colin", ""], ["Shiralkar", "Prashant", ""], ["Dong", "Xin Luna", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2005.07356", "submitter": "Mohammed Belkhatir", "authors": "B. Tahayna, M. Belkhatir", "title": "Near-duplicate video detection featuring coupled temporal and perceptual\n  visual structures and logical inference based matching", "comments": null, "journal-ref": null, "doi": "10.1016/j.ipm.2011.03.003", "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this paper an architecture for near-duplicate video detection\nbased on: (i) index and query signature based structures integrating temporal\nand perceptual visual features and (ii) a matching framework computing the\nlogical inference between index and query documents. As far as indexing is\nconcerned, instead of concatenating low-level visual features in\nhigh-dimensional spaces which results in curse of dimensionality and redundancy\nissues, we adopt a perceptual symbolic representation based on color and\ntexture concepts. For matching, we propose to instantiate a retrieval model\nbased on logical inference through the coupling of an N-gram sliding window\nprocess and theoretically-sound lattice-based structures. The techniques we\ncover are robust and insensitive to general video editing and/or degradation,\nmaking it ideal for re-broadcasted video search. Experiments are carried out on\nlarge quantities of video data collected from the TRECVID 02, 03 and 04\ncollections and real-world video broadcasts recorded from two German TV\nstations. An empirical comparison over two state-of-the-art dynamic programming\ntechniques is encouraging and demonstrates the advantage and feasibility of our\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 04:45:52 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Tahayna", "B.", ""], ["Belkhatir", "M.", ""]]}, {"id": "2005.07382", "submitter": "Ya-Hui An", "authors": "Ya-Hui An and Muthu Kumar Chandresekaran and Min-Yen Kan and Yan Fu", "title": "The MUIR Framework: Cross-Linking MOOC Resources to Enhance Discussion\n  Forums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New learning resources are created and minted in Massive Open Online Courses\nevery week -- new videos, quizzes, assessments and discussion threads are\ndeployed and interacted with -- in the era of on-demand online learning.\nHowever, these resources are often artificially siloed between platforms and\nartificial web application models. Facilitating the linking between such\nresources facilitates learning and multimodal understanding, bettering\nlearners' experience.\n  We create a framework for MOOC Uniform Identifier for Resources (MUIR). MUIR\nenables applications to refer and link to such resources in a cross-platform\nway, allowing the easy minting of identifiers to MOOC resources, akin to\n#hashtags. We demonstrate the feasibility of this approach to the automatic\nidentification, linking and resolution -- a task known as Wikification -- of\nlearning resources mentioned on MOOC discussion forums, from a harvested\ncollection of 100K+ resources. Our Wikification system achieves a high initial\nrate of 54.6% successful resolutions on key resource mentions found in\ndiscussion forums, demonstrating the utility of the MUIR framework. Our\nanalysis on this new problem shows that context is a key factor in determining\nthe correct resolution of such mentions.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 07:09:01 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["An", "Ya-Hui", ""], ["Chandresekaran", "Muthu Kumar", ""], ["Kan", "Min-Yen", ""], ["Fu", "Yan", ""]]}, {"id": "2005.07638", "submitter": "Anastasios Nentidis", "authors": "Anastasios Nentidis, Anastasia Krithara, Grigorios Tsoumakas, Georgios\n  Paliouras", "title": "Beyond MeSH: Fine-Grained Semantic Indexing of Biomedical Literature\n  based on Weak Supervision", "comments": "36 pages, 8 figures; Dictionary-based baselines added and conclusions\n  updated", "journal-ref": "Information Processing and Management 57 (2020) 102282", "doi": "10.1016/j.ipm.2020.102282", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a method for the automated refinement of subject\nannotations in biomedical literature at the level of concepts. Semantic\nindexing and search of biomedical articles in MEDLINE/PubMed are based on\nsemantic subject annotations with MeSH descriptors that may correspond to\nseveral related but distinct biomedical concepts. Such semantic annotations do\nnot adhere to the level of detail available in the domain knowledge and may not\nbe sufficient to fulfil the information needs of experts in the domain. To this\nend, we propose a new method that uses weak supervision to train a concept\nannotator on the literature available for a particular disease. We test this\nmethod on the MeSH descriptors for two diseases: Alzheimer's Disease and\nDuchenne Muscular Dystrophy. The results indicate that concept-occurrence is a\nstrong heuristic for automated subject annotation refinement and its use as\nweak supervision can lead to improved concept-level annotations. The\nfine-grained semantic annotations can enable more precise literature retrieval,\nsustain the semantic integration of subject annotations with other domain\nresources and ease the maintenance of consistent subject annotations, as new\nmore detailed entries are added in the MeSH thesaurus over time.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 16:52:25 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 13:39:05 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Nentidis", "Anastasios", ""], ["Krithara", "Anastasia", ""], ["Tsoumakas", "Grigorios", ""], ["Paliouras", "Georgios", ""]]}, {"id": "2005.07893", "submitter": "Hyokun Yun", "authors": "Hyokun Yun, Michael Froh, Roshan Makhijani, Brian Luc, Alex Smola,\n  Trishul Chilimbi", "title": "Tiering as a Stochastic Submodular Optimization Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tiering is an essential technique for building large-scale information\nretrieval systems. While the selection of documents for high priority tiers\ncritically impacts the efficiency of tiering, past work focuses on optimizing\nit with respect to a static set of queries in the history, and generalizes\npoorly to the future traffic. Instead, we formulate the optimal tiering as a\nstochastic optimization problem, and follow the methodology of regularized\nempirical risk minimization to maximize the \\emph{generalization performance}\nof the system. We also show that the optimization problem can be cast as a\nstochastic submodular optimization problem with a submodular knapsack\nconstraint, and we develop efficient optimization algorithms by leveraging this\nconnection.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 07:39:29 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Yun", "Hyokun", ""], ["Froh", "Michael", ""], ["Makhijani", "Roshan", ""], ["Luc", "Brian", ""], ["Smola", "Alex", ""], ["Chilimbi", "Trishul", ""]]}, {"id": "2005.08129", "submitter": "Yongfeng Zhang", "authors": "Hanxiong Chen, Shaoyun Shi, Yunqi Li, Yongfeng Zhang", "title": "Neural Collaborative Reasoning", "comments": "Accepted to the 30th Web Conference (WWW 2021)", "journal-ref": null, "doi": "10.1145/3442381.3449973", "report-no": null, "categories": "cs.IR cs.AI cs.LG cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Collaborative Filtering (CF) methods are mostly designed based on\nthe idea of matching, i.e., by learning user and item embeddings from data\nusing shallow or deep models, they try to capture the associative relevance\npatterns in data, so that a user embedding can be matched with relevant item\nembeddings using designed or learned similarity functions. However, as a\ncognition rather than a perception intelligent task, recommendation requires\nnot only the ability of pattern recognition and matching from data, but also\nthe ability of cognitive reasoning in data. In this paper, we propose to\nadvance Collaborative Filtering (CF) to Collaborative Reasoning (CR), which\nmeans that each user knows part of the reasoning space, and they collaborate\nfor reasoning in the space to estimate preferences for each other. Technically,\nwe propose a Neural Collaborative Reasoning (NCR) framework to bridge learning\nand reasoning. Specifically, we integrate the power of representation learning\nand logical reasoning, where representations capture similarity patterns in\ndata from perceptual perspectives, and logic facilitates cognitive reasoning\nfor informed decision making. An important challenge, however, is to bridge\ndifferentiable neural networks and symbolic reasoning in a shared architecture\nfor optimization and inference. To solve the problem, we propose a modularized\nreasoning architecture, which learns logical operations such as AND ($\\wedge$),\nOR ($\\vee$) and NOT ($\\neg$) as neural modules for implication reasoning\n($\\rightarrow$). In this way, logical expressions can be equivalently organized\nas neural networks, so that logical reasoning and prediction can be conducted\nin a continuous space. Experiments on real-world datasets verified the\nadvantages of our framework compared with both shallow, deep and reasoning\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 23:29:31 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 17:58:21 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 23:03:39 GMT"}, {"version": "v4", "created": "Tue, 20 Apr 2021 23:16:22 GMT"}, {"version": "v5", "created": "Mon, 3 May 2021 02:06:05 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chen", "Hanxiong", ""], ["Shi", "Shaoyun", ""], ["Li", "Yunqi", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2005.08146", "submitter": "Somin Wadhwa", "authors": "Somin Wadhwa, Kanhua Yin, Kevin S. Hughes, Byron C. Wallace", "title": "Semi-Automating Knowledge Base Construction for Cancer Genetics", "comments": "In proceedings of the Conference on Automated Knowledge Base\n  Construction (AKBC), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the exponentially growing subarea of genetics in\ncancer. The need to synthesize and centralize this evidence for dissemination\nhas motivated a team of physicians to manually construct and maintain a\nknowledge base that distills key results reported in the literature. This is a\nlaborious process that entails reading through full-text articles to understand\nthe study design, assess study quality, and extract the reported cancer risk\nestimates associated with particular hereditary cancer genes (i.e.,\npenetrance). In this work, we propose models to automatically surface key\nelements from full-text cancer genetics articles, with the ultimate aim of\nexpediting the manual workflow currently in place.\n  We propose two challenging tasks that are critical for characterizing the\nfindings reported cancer genetics studies: (i) Extracting snippets of text that\ndescribe \\emph{ascertainment mechanisms}, which in turn inform whether the\npopulation studied may introduce bias owing to deviations from the target\npopulation; (ii) Extracting reported risk estimates (e.g., odds or hazard\nratios) associated with specific germline mutations. The latter task may be\nviewed as a joint entity tagging and relation extraction problem. To train\nmodels for these tasks, we induce distant supervision over tokens and snippets\nin full-text articles using the manually constructed knowledge base. We propose\nand evaluate several model variants, including a transformer-based joint entity\nand relation extraction model to extract <germline mutation, risk-estimate>}\npairs. We observe strong empirical performance, highlighting the practical\npotential for such models to aid KB construction in this space. We ablate\ncomponents of our model, observing, e.g., that a joint model for <germline\nmutation, risk-estimate> fares substantially better than a pipelined approach.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 02:01:43 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 00:47:33 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Wadhwa", "Somin", ""], ["Yin", "Kanhua", ""], ["Hughes", "Kevin S.", ""], ["Wallace", "Byron C.", ""]]}, {"id": "2005.08147", "submitter": "Wenqi Fan", "authors": "Wenqi Fan, Tyler Derr, Xiangyu Zhao, Yao Ma, Hui Liu, Jianping Wang,\n  Jiliang Tang, Qing Li", "title": "Attacking Black-box Recommendations via Copying Cross-domain User\n  Profiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, recommender systems that aim to suggest personalized lists of items\nfor users to interact with online have drawn a lot of attention. In fact, many\nof these state-of-the-art techniques have been deep learning based. Recent\nstudies have shown that these deep learning models (in particular for\nrecommendation systems) are vulnerable to attacks, such as data poisoning,\nwhich generates users to promote a selected set of items. However, more\nrecently, defense strategies have been developed to detect these generated\nusers with fake profiles. Thus, advanced injection attacks of creating more\n`realistic' user profiles to promote a set of items is still a key challenge in\nthe domain of deep learning based recommender systems. In this work, we present\nour framework CopyAttack, which is a reinforcement learning based black-box\nattack method that harnesses real users from a source domain by copying their\nprofiles into the target domain with the goal of promoting a subset of items.\nCopyAttack is constructed to both efficiently and effectively learn policy\ngradient networks that first select, and then further refine/craft, user\nprofiles from the source domain to ultimately copy into the target domain.\nCopyAttack's goal is to maximize the hit ratio of the targeted items in the\nTop-$k$ recommendation list of the users in the target domain. We have\nconducted experiments on two real-world datasets and have empirically verified\nthe effectiveness of our proposed framework and furthermore performed a\nthorough model analysis.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 02:10:38 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Fan", "Wenqi", ""], ["Derr", "Tyler", ""], ["Zhao", "Xiangyu", ""], ["Ma", "Yao", ""], ["Liu", "Hui", ""], ["Wang", "Jianping", ""], ["Tang", "Jiliang", ""], ["Li", "Qing", ""]]}, {"id": "2005.08148", "submitter": "Mohammad Maghsoudi Mehrabani", "authors": "Mohammad Maghsoudi Mehrabani, Hamid Mohayeji and Ali Moeini", "title": "A Hybrid Approach to Enhance Pure Collaborative Filtering based on\n  Content Feature Relationship", "comments": "The 10th Conference on Information and Knowledge Technology (IKT2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recommendation systems get expanding significance because of their\napplications in both the scholarly community and industry. With the development\nof additional data sources and methods of extracting new information other than\nthe rating history of clients on items, hybrid recommendation algorithms, in\nwhich some methods have usually been combined to improve performance, have\nbecome pervasive. In this work, we first introduce a novel method to extract\nthe implicit relationship between content features using a sort of well-known\nmethods from the natural language processing domain, namely Word2Vec. In\ncontrast to the typical use of Word2Vec, we utilize some features of items as\nwords of sentences to produce neural feature embeddings, through which we can\ncalculate the similarity between features. Next, we propose a novel\ncontent-based recommendation system that employs the relationship to determine\nvector representations for items by which the similarity between items can be\ncomputed (RELFsim). Our evaluation results demonstrate that it can predict the\npreference a user would have for a set of items as good as pure collaborative\nfiltering. This content-based algorithm is also embedded in a pure item-based\ncollaborative filtering algorithm to deal with the cold-start problem and\nenhance its accuracy. Our experiments on a benchmark movie dataset corroborate\nthat the proposed approach improves the accuracy of the system.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 02:20:45 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Mehrabani", "Mohammad Maghsoudi", ""], ["Mohayeji", "Hamid", ""], ["Moeini", "Ali", ""]]}, {"id": "2005.08164", "submitter": "Hui Li", "authors": "Chen Lin, Si Chen, Hui Li, Yanghua Xiao, Lianyun Li, Qian Yang", "title": "Attacking Recommender Systems with Augmented User Profiles", "comments": "CIKM 2020. 10 pages, 2 figures", "journal-ref": null, "doi": "10.1145/3340531.3411884", "report-no": null, "categories": "cs.IR cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation Systems (RS) have become an essential part of many online\nservices. Due to its pivotal role in guiding customers towards purchasing,\nthere is a natural motivation for unscrupulous parties to spoof RS for profits.\nIn this paper, we study the shilling attack: a subsistent and profitable attack\nwhere an adversarial party injects a number of user profiles to promote or\ndemote a target item. Conventional shilling attack models are based on simple\nheuristics that can be easily detected, or directly adopt adversarial attack\nmethods without a special design for RS. Moreover, the study on the attack\nimpact on deep learning based RS is missing in the literature, making the\neffects of shilling attack against real RS doubtful. We present a novel\nAugmented Shilling Attack framework (AUSH) and implement it with the idea of\nGenerative Adversarial Network. AUSH is capable of tailoring attacks against RS\naccording to budget and complex attack goals, such as targeting a specific user\ngroup. We experimentally show that the attack impact of AUSH is noticeable on a\nwide range of RS including both classic and modern deep learning based RS,\nwhile it is virtually undetectable by the state-of-the-art attack detection\nmodel.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 04:44:52 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 14:22:49 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Lin", "Chen", ""], ["Chen", "Si", ""], ["Li", "Hui", ""], ["Xiao", "Yanghua", ""], ["Li", "Lianyun", ""], ["Yang", "Qian", ""]]}, {"id": "2005.08223", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Juan-Manuel Torres-Moreno, Luis-Gil Moreno-Jim\\'enez", "title": "LiSSS: A toy corpus of Spanish Literary Sentences for Emotions detection", "comments": "8 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a new small data-set in Computational Creativity (CC)\nfield, the Spanish Literary Sentences for emotions detection corpus (LISSS). We\naddress this corpus of literary sentences in order to evaluate or design\nalgorithms of emotions classification and detection. We have constitute this\ncorpus by manually classifying the sentences in a set of emotions: Love, Fear,\nHappiness, Anger and Sadness/Pain. We also present some baseline classification\nalgorithms applied on our corpus. The LISSS corpus will be available to the\ncommunity as a free resource to evaluate or create CC-like algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 11:14:30 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 10:30:11 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Torres-Moreno", "Juan-Manuel", ""], ["Moreno-Jim\u00e9nez", "Luis-Gil", ""]]}, {"id": "2005.08259", "submitter": "Mohammed Belkhatir", "authors": "Mohammed Maree, Israa Noor, Khaled Rabayah, Mohammed Belkhatir, and\n  Saadat M. Alhashmi", "title": "On the Combined Use of Extrinsic Semantic Resources for Medical\n  Information Search", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.2987568", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic concepts and relations encoded in domain-specific ontologies and\nother medical semantic resources play a crucial role in deciphering terms in\nmedical queries and documents. The exploitation of these resources for tackling\nthe semantic gap issue has been widely studied in the literature. However,\nthere are challenges that hinder their widespread use in real-world\napplications. Among these challenges is the insufficient knowledge individually\nencoded in existing medical ontologies, which is magnified when users express\ntheir information needs using long-winded natural language queries. In this\ncontext, many of the users query terms are either unrecognized by the used\nontologies, or cause retrieving false positives that degrade the quality of\ncurrent medical information search approaches. In this article, we explore the\ncombination of multiple extrinsic semantic resources in the development of a\nfull-fledged medical information search framework to: i) highlight and expand\nhead medical concepts in verbose medical queries (i.e. concepts among query\nterms that significantly contribute to the informativeness and intent of a\ngiven query), ii) build semantically enhanced inverted index documents, iii)\ncontribute to a heuristical weighting technique in the query document matching\nprocess. To demonstrate the effectiveness of the proposed approach, we\nconducted several experiments over the CLEF eHealth 2014 dataset. Findings\nindicate that the proposed method combining several extrinsic semantic\nresources proved to be more effective than related approaches in terms of\nprecision measure.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 14:18:04 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Maree", "Mohammed", ""], ["Noor", "Israa", ""], ["Rabayah", "Khaled", ""], ["Belkhatir", "Mohammed", ""], ["Alhashmi", "Saadat M.", ""]]}, {"id": "2005.08367", "submitter": "Markus Zlabinger", "authors": "Markus Zlabinger, Marta Sabou, Sebastian Hofst\\\"atter, Mete Sertkan,\n  and Allan Hanbury", "title": "DEXA: Supporting Non-Expert Annotators with Dynamic Examples from\n  Experts", "comments": "4 pages, 1 figure, 3 tables, accepted to SIGIR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of crowdsourcing based annotation of text corpora depends on\nensuring that crowdworkers are sufficiently well-trained to perform the\nannotation task accurately. To that end, a frequent approach to train\nannotators is to provide instructions and a few example cases that demonstrate\nhow the task should be performed (referred to as the CONTROL approach). These\nglobally defined \"task-level examples\", however, (i) often only cover the\ncommon cases that are encountered during an annotation task; and (ii) require\neffort from crowdworkers during the annotation process to find the most\nrelevant example for the currently annotated sample. To overcome these\nlimitations, we propose to support workers in addition to task-level examples,\nalso with \"task-instance level\" examples that are semantically similar to the\ncurrently annotated data sample (referred to as Dynamic Examples for\nAnnotation, DEXA). Such dynamic examples can be retrieved from collections\npreviously labeled by experts, which are usually available as gold standard\ndataset. We evaluate DEXA on a complex task of annotating participants,\ninterventions, and outcomes (known as PIO) in sentences of medical studies. The\ndynamic examples are retrieved using BioSent2Vec, an unsupervised semantic\nsentence similarity method specific to the biomedical domain. Results show that\n(i) workers of the DEXA approach reach on average much higher agreements\n(Cohen's Kappa) to experts than workers of the the CONTROL approach (avg. of\n0.68 to experts in DEXA vs. 0.40 in CONTROL); (ii) already three per majority\nvoting aggregated annotations of the DEXA approach reach substantial agreements\nto experts of 0.78/0.75/0.69 for P/I/O (in CONTROL 0.73/0.58/0.46). Finally,\n(iii) we acquire explicit feedback from workers and show that in the majority\nof cases (avg. 72%) workers find the dynamic examples useful.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 20:47:28 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Zlabinger", "Markus", ""], ["Sabou", "Marta", ""], ["Hofst\u00e4tter", "Sebastian", ""], ["Sertkan", "Mete", ""], ["Hanbury", "Allan", ""]]}, {"id": "2005.08416", "submitter": "Yu Gong", "authors": "Yu Gong, Ziwen Jiang, Yufei Feng, Binbin Hu, Kaiqi Zhao, Qingwen Liu,\n  Wenwu Ou", "title": "EdgeRec: Recommender System on Edge in Mobile Taobao", "comments": "Accepted by CIKM 2020; Fully deployed in homepage Guess U Like\n  scenario of Taobao application; Resulting in 9% lift in IPV and 11% lift in\n  GMV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender system (RS) has become a crucial module in most web-scale\napplications. Recently, most RSs are in the waterfall form based on the\ncloud-to-edge framework, where recommended results are transmitted to edge\n(e.g., user mobile) by computing in advance in the cloud server. Despite\neffectiveness, network bandwidth and latency between cloud server and edge may\ncause the delay for system feedback and user perception. Hence, real-time\ncomputing on edge could help capture user preferences more preciously and thus\nmake more satisfactory recommendations. Our work, to our best knowledge, is the\nfirst attempt to design and implement the novel Recommender System on Edge\n(EdgeRec), which achieves Real-time User Perception and Real-time System\nFeedback. Moreover, we propose Heterogeneous User Behavior Sequence Modeling\nand Context-aware Reranking with Behavior Attention Networks to capture user's\ndiverse interests and adjust recommendation results accordingly. Experimental\nresults on both the offline evaluation and online performance in Taobao\nhome-page feeds demonstrate the effectiveness of EdgeRec.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 01:30:52 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 04:06:23 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 02:02:12 GMT"}, {"version": "v4", "created": "Tue, 15 Sep 2020 09:51:01 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Gong", "Yu", ""], ["Jiang", "Ziwen", ""], ["Feng", "Yufei", ""], ["Hu", "Binbin", ""], ["Zhao", "Kaiqi", ""], ["Liu", "Qingwen", ""], ["Ou", "Wenwu", ""]]}, {"id": "2005.08480", "submitter": "Nan Wang", "authors": "Nan Wang, Xuanhui Wang, Hongning Wang", "title": "Unbiased Learning to Rank via Propensity Ratio Scoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit feedback, such as user clicks, is a major source of supervision for\nlearning to rank (LTR) model estimation in modern retrieval systems. However,\nthe inherent bias in such feedback greatly restricts the quality of the learnt\nranker. Recent advances in unbiased LTR leverage Inverse Propensity Scoring\n(IPS) to tackle the bias issue. Though effective, it only corrects the bias\nintroduced by treating clicked documents as relevant, but cannot handle the\nbias caused by treating unclicked ones as irrelevant. Because non-clicks do not\nnecessarily stand for irrelevance (they might not be examined), IPS-based\nmethods inevitably include loss from comparisons on relevant-relevant document\npairs. This directly limits the effectiveness of ranking model learning.\n  In this work, we first prove that in a LTR algorithm that is based on\npairwise comparisons, only pairs with different labels (e.g.,\nrelevant-irrelevant pairs in binary case) should contribute to the loss\nfunction. The proof asserts sub-optimal results of the existing IPS-based\nmethods in practice. We then derive a new weighting scheme called Propensity\nRatio Scoring (PRS) that takes a holistic treatment on both clicks and\nnon-clicks. Besides correcting the bias in clicked documents, PRS avoids\nrelevant-relevant comparisons in LTR training in expectation and enjoys a lower\nvariability. Our empirical study confirms that PRS ensures a more effective use\nof click data in various situations, which leads to its superior performance in\nan extensive set of LTR benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 06:31:12 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Wang", "Nan", ""], ["Wang", "Xuanhui", ""], ["Wang", "Hongning", ""]]}, {"id": "2005.08591", "submitter": "Nikitha Rao", "authors": "Nikitha Rao, Chetan Bansal, Subhabrata Mukherjee and Chandra Maddila", "title": "Product Insights: Analyzing Product Intents in Web Search", "comments": null, "journal-ref": null, "doi": "10.1145/3340531.3412090", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web search engines are frequently used to access information about products.\nThis has increased in recent times with the rising popularity of e-commerce.\nHowever, there is limited understanding of what users search for and their\nintents when it comes to product search on the web. In this work, we study\nsearch logs from Bing web search engine to characterize user intents and study\nuser behavior for product search. We propose a taxonomy of product intents by\nanalyzing product search queries. This is a challenging task given that only\n15%-17% of web search queries are about products. We train machine learning\nclassifiers with query log features to classify queries based on intent with an\noverall F1-score of 78%. We further analyze various characteristics of product\nsearch queries in terms of search metrics like dwell time, success, popularity\nand session-specific information.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 11:07:03 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 06:58:26 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Rao", "Nikitha", ""], ["Bansal", "Chetan", ""], ["Mukherjee", "Subhabrata", ""], ["Maddila", "Chandra", ""]]}, {"id": "2005.08598", "submitter": "Wendi Ji", "authors": "Wendi Ji, Keqiang Wang, Xiaoling Wang, TingWei Chen and Alexandra\n  Cristea", "title": "Sequential Recommender via Time-aware Attentive Memory Network", "comments": "10 pages, 6 figures", "journal-ref": "CIKM 2020", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems aim to assist users to discover most preferred\ncontents from an ever-growing corpus of items. Although recommenders have been\ngreatly improved by deep learning, they still faces several challenges: (1)\nBehaviors are much more complex than words in sentences, so traditional\nattentive and recurrent models may fail in capturing the temporal dynamics of\nuser preferences. (2) The preferences of users are multiple and evolving, so it\nis difficult to integrate long-term memory and short-term intent.\n  In this paper, we propose a temporal gating methodology to improve attention\nmechanism and recurrent units, so that temporal information can be considered\nin both information filtering and state transition. Additionally, we propose a\nMulti-hop Time-aware Attentive Memory network (MTAM) to integrate long-term and\nshort-term preferences. We use the proposed time-aware GRU network to learn the\nshort-term intent and maintain prior records in user memory. We treat the\nshort-term intent as a query and design a multi-hop memory reading operation\nvia the proposed time-aware attention to generate user representation based on\nthe current intent and long-term memory. Our approach is scalable for candidate\nretrieval tasks and can be viewed as a non-linear generalization of latent\nfactorization for dot-product based Top-K recommendation. Finally, we conduct\nextensive experiments on six benchmark datasets and the experimental results\ndemonstrate the effectiveness of our MTAM and temporal gating methodology.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 11:29:38 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 05:39:13 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Ji", "Wendi", ""], ["Wang", "Keqiang", ""], ["Wang", "Xiaoling", ""], ["Chen", "TingWei", ""], ["Cristea", "Alexandra", ""]]}, {"id": "2005.08658", "submitter": "Avishek Anand", "authors": "Avishek Anand, Lawrence Cavedon, Matthias Hagen, Hideo Joho, Mark\n  Sanderson, and Benno Stein", "title": "Conversational Search -- A Report from Dagstuhl Seminar 19461", "comments": "contains arXiv:2001.06910, arXiv:2001.02912", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dagstuhl Seminar 19461 \"Conversational Search\" was held on 10-15 November\n2019. 44~researchers in Information Retrieval and Web Search, Natural Language\nProcessing, Human Computer Interaction, and Dialogue Systems were invited to\nshare the latest development in the area of Conversational Search and discuss\nits research agenda and future directions. A 5-day program of the seminar\nconsisted of six introductory and background sessions, three visionary talk\nsessions, one industry talk session, and seven working groups and reporting\nsessions. The seminar also had three social events during the program. This\nreport provides the executive summary, overview of invited talks, and findings\nfrom the seven working groups which cover the definition, evaluation,\nmodelling, explanation, scenarios, applications, and prototype of\nConversational Search. The ideas and findings presented in this report should\nserve as one of the main sources for diverse research programs on\nConversational Search.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 12:48:33 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Anand", "Avishek", ""], ["Cavedon", "Lawrence", ""], ["Hagen", "Matthias", ""], ["Joho", "Hideo", ""], ["Sanderson", "Mark", ""], ["Stein", "Benno", ""]]}, {"id": "2005.08936", "submitter": "Keping Bi", "authors": "Keping Bi, Qingyao Ai, W. Bruce Croft", "title": "A Transformer-based Embedding Model for Personalized Product Search", "comments": "In the proceedings of SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401192", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Product search is an important way for people to browse and purchase items on\nE-commerce platforms. While customers tend to make choices based on their\npersonal tastes and preferences, analysis of commercial product search logs has\nshown that personalization does not always improve product search quality. Most\nexisting product search techniques, however, conduct undifferentiated\npersonalization across search sessions. They either use a fixed coefficient to\ncontrol the influence of personalization or let personalization take effect all\nthe time with an attention mechanism. The only notable exception is the\nrecently proposed zero-attention model (ZAM) that can adaptively adjust the\neffect of personalization by allowing the query to attend to a zero vector.\nNonetheless, in ZAM, personalization can act at most as equally important as\nthe query and the representations of items are static across the collection\nregardless of the items co-occurring in the user's historical purchases. Aware\nof these limitations, we propose a transformer-based embedding model (TEM) for\npersonalized product search, which could dynamically control the influence of\npersonalization by encoding the sequence of query and user's purchase history\nwith a transformer architecture. Personalization could have a dominant impact\nwhen necessary and interactions between items can be taken into consideration\nwhen computing attention weights. Experimental results show that TEM\noutperforms state-of-the-art personalization product retrieval models\nsignificantly.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 17:59:00 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Bi", "Keping", ""], ["Ai", "Qingyao", ""], ["Croft", "W. Bruce", ""]]}, {"id": "2005.09027", "submitter": "Pavel Levin", "authors": "Benjamin Gutelman and Pavel Levin", "title": "Efficient Image Gallery Representations at Scale Through Multi-Task\n  Learning", "comments": "Proceedings of the 43rd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval", "journal-ref": null, "doi": "10.1145/3397271.3401433", "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image galleries provide a rich source of diverse information about a product\nwhich can be leveraged across many recommendation and retrieval applications.\nWe study the problem of building a universal image gallery encoder through\nmulti-task learning (MTL) approach and demonstrate that it is indeed a\npractical way to achieve generalizability of learned representations to new\ndownstream tasks. Additionally, we analyze the relative predictive performance\nof MTL-trained solutions against optimal and substantially more expensive\nsolutions, and find signals that MTL can be a useful mechanism to address\nsparsity in low-resource binary tasks.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 18:49:22 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 05:50:53 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 10:24:02 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Gutelman", "Benjamin", ""], ["Levin", "Pavel", ""]]}, {"id": "2005.09035", "submitter": "Harrie Oosterhuis", "authors": "Harrie Oosterhuis and Maarten de Rijke", "title": "Policy-Aware Unbiased Learning to Rank for Top-k Rankings", "comments": "SIGIR 2020 full conference paper", "journal-ref": null, "doi": "10.1145/3397271.3401102", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual Learning to Rank (LTR) methods optimize ranking systems using\nlogged user interactions that contain interaction biases. Existing methods are\nonly unbiased if users are presented with all relevant items in every ranking.\nThere is currently no existing counterfactual unbiased LTR method for top-k\nrankings. We introduce a novel policy-aware counterfactual estimator for LTR\nmetrics that can account for the effect of a stochastic logging policy. We\nprove that the policy-aware estimator is unbiased if every relevant item has a\nnon-zero probability to appear in the top-k ranking. Our experimental results\nshow that the performance of our estimator is not affected by the size of k:\nfor any k, the policy-aware estimator reaches the same retrieval performance\nwhile learning from top-k feedback as when learning from feedback on the full\nranking. Lastly, we introduce novel extensions of traditional LTR methods to\nperform counterfactual LTR and to optimize top-k metrics. Together, our\ncontributions introduce the first policy-aware unbiased LTR approach that\nlearns from top-k feedback and optimizes top-k metrics. As a result,\ncounterfactual LTR is now applicable to the very prevalent top-k ranking\nsetting in search and recommendation.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 19:07:52 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 12:00:04 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Oosterhuis", "Harrie", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2005.09109", "submitter": "Liangbei Xu", "authors": "Liangbei Xu, Mark A. Davenport", "title": "Dynamic Knowledge embedding and tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of knowledge tracing is to track the state of a student's knowledge\nas it evolves over time. This plays a fundamental role in understanding the\nlearning process and is a key task in the development of an intelligent\ntutoring system. In this paper we propose a novel approach to knowledge tracing\nthat combines techniques from matrix factorization with recent progress in\nrecurrent neural networks (RNNs) to effectively track the state of a student's\nknowledge. The proposed \\emph{DynEmb} framework enables the tracking of student\nknowledge even without the concept/skill tag information that other knowledge\ntracing models require while simultaneously achieving superior performance. We\nprovide experimental evaluations demonstrating that DynEmb achieves improved\nperformance compared to baselines and illustrating the robustness and\neffectiveness of the proposed framework. We also evaluate our approach using\nseveral real-world datasets showing that the proposed model outperforms the\nprevious state-of-the-art. These results suggest that combining embedding\nmodels with sequential models such as RNNs is a promising new direction for\nknowledge tracing.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 21:56:42 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Xu", "Liangbei", ""], ["Davenport", "Mark A.", ""]]}, {"id": "2005.09183", "submitter": "Seito Kasai", "authors": "Seito Kasai, Yuchi Ishikawa, Masaki Hayashi, Yoshimitsu Aoki, Kensho\n  Hara, Hirokatsu Kataoka", "title": "Retrieving and Highlighting Action with Spatiotemporal Reference", "comments": "Accepted to ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a framework that jointly retrieves and\nspatiotemporally highlights actions in videos by enhancing current deep\ncross-modal retrieval methods. Our work takes on the novel task of action\nhighlighting, which visualizes where and when actions occur in an untrimmed\nvideo setting. Action highlighting is a fine-grained task, compared to\nconventional action recognition tasks which focus on classification or\nwindow-based localization. Leveraging weak supervision from annotated captions,\nour framework acquires spatiotemporal relevance maps and generates local\nembeddings which relate to the nouns and verbs in captions. Through\nexperiments, we show that our model generates various maps conditioned on\ndifferent actions, in which conventional visual reasoning methods only go as\nfar as to show a single deterministic saliency map. Also, our model improves\nretrieval recall over our baseline without alignment by 2-3% on the MSR-VTT\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 03:12:31 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Kasai", "Seito", ""], ["Ishikawa", "Yuchi", ""], ["Hayashi", "Masaki", ""], ["Aoki", "Yoshimitsu", ""], ["Hara", "Kensho", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "2005.09207", "submitter": "Zhiyu Chen", "authors": "Zhiyu Chen, Mohamed Trabelsi, Jeff Heflin, Yinan Xu, Brian D. Davison", "title": "Table Search Using a Deep Contextualized Language Model", "comments": "Accepted at SIGIR 2020 (Long)", "journal-ref": null, "doi": "10.1145/3397271.3401044", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained contextualized language models such as BERT have achieved\nimpressive results on various natural language processing benchmarks.\nBenefiting from multiple pretraining tasks and large scale training corpora,\npretrained models can capture complex syntactic word relations. In this paper,\nwe use the deep contextualized language model BERT for the task of ad hoc table\nretrieval. We investigate how to encode table content considering the table\nstructure and input length limit of BERT. We also propose an approach that\nincorporates features from prior literature on table retrieval and jointly\ntrains them with BERT. In experiments on public datasets, we show that our best\napproach can outperform the previous state-of-the-art method and BERT baselines\nwith a large margin under different evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 04:18:04 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 23:07:15 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Chen", "Zhiyu", ""], ["Trabelsi", "Mohamed", ""], ["Heflin", "Jeff", ""], ["Xu", "Yinan", ""], ["Davison", "Brian D.", ""]]}, {"id": "2005.09252", "submitter": "Anubhav Jangra", "authors": "Anubhav Jangra, Sriparna Saha, Adam Jatowt, Mohammad Hasanuzzaman", "title": "Multi-Modal Summary Generation using Multi-Objective Optimization", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant development of communication technology over the past few years\nhas motivated research in multi-modal summarization techniques. A majority of\nthe previous works on multi-modal summarization focus on text and images. In\nthis paper, we propose a novel extractive multi-objective optimization based\nmodel to produce a multi-modal summary containing text, images, and videos.\nImportant objectives such as intra-modality salience, cross-modal redundancy\nand cross-modal similarity are optimized simultaneously in a multi-objective\noptimization framework to produce effective multi-modal output. The proposed\nmodel has been evaluated separately for different modalities, and has been\nfound to perform better than state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 07:10:28 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Jangra", "Anubhav", ""], ["Saha", "Sriparna", ""], ["Jatowt", "Adam", ""], ["Hasanuzzaman", "Mohammad", ""]]}, {"id": "2005.09272", "submitter": "Lu Yu", "authors": "Lu Yu, Shichao Pei, Chuxu Zhang, Shangsong Liang, Xiao Bai, Nitesh\n  Chawla, Xiangliang Zhang", "title": "Addressing Class-Imbalance Problem in Personalized Ranking", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise ranking models have been widely used to address recommendation\nproblems. The basic idea is to learn the rank of users' preferred items through\nseparating items into \\emph{positive} samples if user-item interactions exist,\nand \\emph{negative} samples otherwise. Due to the limited number of observable\ninteractions, pairwise ranking models face serious \\emph{class-imbalance}\nissues. Our theoretical analysis shows that current sampling-based methods\ncause the vertex-level imbalance problem, which makes the norm of learned item\nembeddings towards infinite after a certain training iterations, and\nconsequently results in vanishing gradient and affects the model inference\nresults. We thus propose an efficient \\emph{\\underline{Vi}tal\n\\underline{N}egative \\underline{S}ampler} (VINS) to alleviate the\nclass-imbalance issue for pairwise ranking model, in particular for deep\nlearning models optimized by gradient methods. The core of VINS is a bias\nsampler with reject probability that will tend to accept a negative candidate\nwith a larger degree weight than the given positive item. Evaluation results on\nseveral real datasets demonstrate that the proposed sampling method speeds up\nthe training procedure 30\\% to 50\\% for ranking models ranging from shallow to\ndeep, while maintaining and even improving the quality of ranking results in\ntop-N item recommendation.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 08:11:26 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 08:47:20 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Yu", "Lu", ""], ["Pei", "Shichao", ""], ["Zhang", "Chuxu", ""], ["Liang", "Shangsong", ""], ["Bai", "Xiao", ""], ["Chawla", "Nitesh", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "2005.09344", "submitter": "Rocky Chen", "authors": "Tong Chen, Hongzhi Yin, Guanhua Ye, Zi Huang, Yang Wang and Meng Wang", "title": "Try This Instead: Personalized and Interpretable Substitute\n  Recommendation", "comments": "To appear in SIGIR'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a fundamental yet significant process in personalized recommendation,\ncandidate generation and suggestion effectively help users spot the most\nsuitable items for them. Consequently, identifying substitutable items that are\ninterchangeable opens up new opportunities to refine the quality of generated\ncandidates. When a user is browsing a specific type of product (e.g., a laptop)\nto buy, the accurate recommendation of substitutes (e.g., better equipped\nlaptops) can offer the user more suitable options to choose from, thus\nsubstantially increasing the chance of a successful purchase. However, existing\nmethods merely treat this problem as mining pairwise item relationships without\nthe consideration of users' personal preferences. Moreover, the substitutable\nrelationships are implicitly identified through the learned latent\nrepresentations of items, leading to uninterpretable recommendation results. In\nthis paper, we propose attribute-aware collaborative filtering (A2CF) to\nperform substitute recommendation by addressing issues from both\npersonalization and interpretability perspectives. Instead of directly\nmodelling user-item interactions, we extract explicit and polarized item\nattributes from user reviews with sentiment analysis, whereafter the\nrepresentations of attributes, users, and items are simultaneously learned.\nThen, by treating attributes as the bridge between users and items, we can\nthoroughly model the user-item preferences (i.e., personalization) and\nitem-item relationships (i.e., substitution) for recommendation. In addition,\nA2CF is capable of generating intuitive interpretations by analyzing which\nattributes a user currently cares the most and comparing the recommended\nsubstitutes with her/his currently browsed items at an attribute level. The\nrecommendation effectiveness and interpretation quality of A2CF are\ndemonstrated via extensive experiments on three real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 10:14:45 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Chen", "Tong", ""], ["Yin", "Hongzhi", ""], ["Ye", "Guanhua", ""], ["Huang", "Zi", ""], ["Wang", "Yang", ""], ["Wang", "Meng", ""]]}, {"id": "2005.09347", "submitter": "Yukuo Cen", "authors": "Yukuo Cen, Jianwei Zhang, Xu Zou, Chang Zhou, Hongxia Yang, Jie Tang", "title": "Controllable Multi-Interest Framework for Recommendation", "comments": "Accepted to KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, neural networks have been widely used in e-commerce recommender\nsystems, owing to the rapid development of deep learning. We formalize the\nrecommender system as a sequential recommendation problem, intending to predict\nthe next items that the user might be interacted with. Recent works usually\ngive an overall embedding from a user's behavior sequence. However, a unified\nuser embedding cannot reflect the user's multiple interests during a period. In\nthis paper, we propose a novel controllable multi-interest framework for the\nsequential recommendation, called ComiRec. Our multi-interest module captures\nmultiple interests from user behavior sequences, which can be exploited for\nretrieving candidate items from the large-scale item pool. These items are then\nfed into an aggregation module to obtain the overall recommendation. The\naggregation module leverages a controllable factor to balance the\nrecommendation accuracy and diversity. We conduct experiments for the\nsequential recommendation on two real-world datasets, Amazon and Taobao.\nExperimental results demonstrate that our framework achieves significant\nimprovements over state-of-the-art models. Our framework has also been\nsuccessfully deployed on the offline Alibaba distributed cloud platform.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 10:18:43 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 02:16:38 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Cen", "Yukuo", ""], ["Zhang", "Jianwei", ""], ["Zou", "Xu", ""], ["Zhou", "Chang", ""], ["Yang", "Hongxia", ""], ["Tang", "Jie", ""]]}, {"id": "2005.09617", "submitter": "Rajesh Bordawekar", "authors": "Apoorva Nitsure and Rajesh Bordawekar and Jose Neves", "title": "Unlocking New York City Crime Insights using Relational Database\n  Embeddings", "comments": "arXiv admin note: This version withdrawn by arXiv administrators\n  because the author did not have the right to agree to our license at the time\n  of submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This version withdrawn by arXiv administrators because the author did not\nhave the right to agree to our license at the time of submission.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 17:46:34 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 14:09:11 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Nitsure", "Apoorva", ""], ["Bordawekar", "Rajesh", ""], ["Neves", "Jose", ""]]}, {"id": "2005.09639", "submitter": "Mohammed Belkhatir", "authors": "F. Fauzi, H. J. Long, M. Belkhatir", "title": "Webpage Segmentation for Extracting Images and Their Surrounding\n  Contextual Information", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.02156", "journal-ref": null, "doi": "10.1145/1631272.1631379", "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web images come in hand with valuable contextual information. Although this\ninformation has long been mined for various uses such as image annotation,\nclustering of images, inference of image semantic content, etc., insufficient\nattention has been given to address issues in mining this contextual\ninformation. In this paper, we propose a webpage segmentation algorithm\ntargeting the extraction of web images and their contextual information based\non their characteristics as they appear on webpages. We conducted a user study\nto obtain a human-labeled dataset to validate the effectiveness of our method\nand experiments demonstrated that our method can achieve better results\ncompared to an existing segmentation algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 19:00:03 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Fauzi", "F.", ""], ["Long", "H. J.", ""], ["Belkhatir", "M.", ""]]}, {"id": "2005.09683", "submitter": "Steffen Rendle", "authors": "Steffen Rendle, Walid Krichene, Li Zhang, John Anderson", "title": "Neural Collaborative Filtering vs. Matrix Factorization Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding based models have been the state of the art in collaborative\nfiltering for over a decade. Traditionally, the dot product or higher order\nequivalents have been used to combine two or more embeddings, e.g., most\nnotably in matrix factorization. In recent years, it was suggested to replace\nthe dot product with a learned similarity e.g. using a multilayer perceptron\n(MLP). This approach is often referred to as neural collaborative filtering\n(NCF). In this work, we revisit the experiments of the NCF paper that\npopularized learned similarities using MLPs. First, we show that with a proper\nhyperparameter selection, a simple dot product substantially outperforms the\nproposed learned similarities. Second, while a MLP can in theory approximate\nany function, we show that it is non-trivial to learn a dot product with an\nMLP. Finally, we discuss practical issues that arise when applying MLP based\nsimilarities and show that MLPs are too costly to use for item recommendation\nin production environments while dot products allow to apply very efficient\nretrieval algorithms. We conclude that MLPs should be used with care as\nembedding combiner and that dot products might be a better default choice.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 18:07:08 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 23:21:33 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Rendle", "Steffen", ""], ["Krichene", "Walid", ""], ["Zhang", "Li", ""], ["Anderson", "John", ""]]}, {"id": "2005.09740", "submitter": "Javad Rafiei Asl", "authors": "Javad Rafiei Asl, Juan M. Banda", "title": "GLEAKE: Global and Local Embedding Automatic Keyphrase Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated methods for granular categorization of large corpora of text\ndocuments have become increasingly more important with the rate scientific,\nnews, medical, and web documents are growing in the last few years. Automatic\nkeyphrase extraction (AKE) aims to automatically detect a small set of single\nor multi-words from within a single textual document that captures the main\ntopics of the document. AKE plays an important role in various NLP and\ninformation retrieval tasks such as document summarization and categorization,\nfull-text indexing, and article recommendation. Due to the lack of sufficient\nhuman-labeled data in different textual contents, supervised learning\napproaches are not ideal for automatic detection of keyphrases from the content\nof textual bodies. With the state-of-the-art advances in text embedding\ntechniques, NLP researchers have focused on developing unsupervised methods to\nobtain meaningful insights from raw datasets. In this work, we introduce Global\nand Local Embedding Automatic Keyphrase Extractor (GLEAKE) for the task of AKE.\nGLEAKE utilizes single and multi-word embedding techniques to explore the\nsyntactic and semantic aspects of the candidate phrases and then combines them\ninto a series of embedding-based graphs. Moreover, GLEAKE applies network\nanalysis techniques on each embedding-based graph to refine the most\nsignificant phrases as a final set of keyphrases. We demonstrate the high\nperformance of GLEAKE by evaluating its results on five standard AKE datasets\nfrom different domains and writing styles and by showing its superiority with\nregards to other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 20:24:02 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Asl", "Javad Rafiei", ""], ["Banda", "Juan M.", ""]]}, {"id": "2005.09801", "submitter": "Dehong Gao", "authors": "Dehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Peng Li, Yi Wei, Yi Hu\n  and Hao Wang", "title": "FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal\n  Retrieval", "comments": "10 pages, to be published in SIGIR20 Industry Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the text and image matching in cross-modal\nretrieval of the fashion industry. Different from the matching in the general\ndomain, the fashion matching is required to pay much more attention to the\nfine-grained information in the fashion images and texts. Pioneer approaches\ndetect the region of interests (i.e., RoIs) from images and use the RoI\nembeddings as image representations. In general, RoIs tend to represent the\n\"object-level\" information in the fashion images, while fashion texts are prone\nto describe more detailed information, e.g. styles, attributes. RoIs are thus\nnot fine-grained enough for fashion text and image matching. To this end, we\npropose FashionBERT, which leverages patches as image features. With the\npre-trained BERT model as the backbone network, FashionBERT learns high level\nrepresentations of texts and images. Meanwhile, we propose an adaptive loss to\ntrade off multitask learning in the FashionBERT modeling. Two tasks (i.e., text\nand image matching and cross-modal retrieval) are incorporated to evaluate\nFashionBERT. On the public dataset, experiments demonstrate FashionBERT\nachieves significant improvements in performances than the baseline and\nstate-of-the-art approaches. In practice, FashionBERT is applied in a concrete\ncross-modal retrieval application. We provide the detailed matching performance\nand inference efficiency analysis.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 00:41:00 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 05:56:10 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Gao", "Dehong", ""], ["Jin", "Linbo", ""], ["Chen", "Ben", ""], ["Qiu", "Minghui", ""], ["Li", "Peng", ""], ["Wei", "Yi", ""], ["Hu", "Yi", ""], ["Wang", "Hao", ""]]}, {"id": "2005.10084", "submitter": "Przemys\u00c5\u0082aw Pobrotyn", "authors": "Przemys{\\l}aw Pobrotyn, Tomasz Bartczak, Miko{\\l}aj Synowiec,\n  Rados{\\l}aw Bia{\\l}obrzeski, Jaros{\\l}aw Bojar", "title": "Context-Aware Learning to Rank with Self-Attention", "comments": "8 pages, published at SIGIR eCom'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to rank is a key component of many e-commerce search engines. In\nlearning to rank, one is interested in optimising the global ordering of a list\nof items according to their utility for users.Popular approaches learn a\nscoring function that scores items individually (i.e. without the context of\nother items in the list) by optimising a pointwise, pairwise or listwise loss.\nThe list is then sorted in the descending order of the scores. Possible\ninteractions between items present in the same list are taken into account in\nthe training phase at the loss level. However, during inference, items are\nscored individually, and possible interactions between them are not considered.\nIn this paper, we propose a context-aware neural network model that learns item\nscores by applying a self-attention mechanism. The relevance of a given item is\nthus determined in the context of all other items present in the list, both in\ntraining and in inference. We empirically demonstrate significant performance\ngains of self-attention based neural architecture over Multi-LayerPerceptron\nbaselines, in particular on a dataset coming from search logs of a large scale\ne-commerce marketplace, Allegro.pl. This effect is consistent across popular\npointwise, pairwise and listwise losses.Finally, we report new state-of-the-art\nresults on MSLR-WEB30K, the learning to rank benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 14:48:16 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 15:34:48 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 11:38:16 GMT"}, {"version": "v4", "created": "Fri, 21 May 2021 20:21:43 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Pobrotyn", "Przemys\u0142aw", ""], ["Bartczak", "Tomasz", ""], ["Synowiec", "Miko\u0142aj", ""], ["Bia\u0142obrzeski", "Rados\u0142aw", ""], ["Bojar", "Jaros\u0142aw", ""]]}, {"id": "2005.10110", "submitter": "Menghan Wang", "authors": "Menghan Wang, Yujie Lin, Guli Lin, Keping Yang, Xiao-ming Wu", "title": "M2GRL: A Multi-task Multi-view Graph Representation Learning Framework\n  for Web-scale Recommender Systems", "comments": "Accepted by KDD 2020 ads track as an oral paper. Code\n  address:https://github.com/99731/M2GRL", "journal-ref": null, "doi": "10.1145/3394486.3403284", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining graph representation learning with multi-view data (side\ninformation) for recommendation is a trend in industry. Most existing methods\ncan be categorized as \\emph{multi-view representation fusion}; they first build\none graph and then integrate multi-view data into a single compact\nrepresentation for each node in the graph. However, these methods are raising\nconcerns in both engineering and algorithm aspects: 1) multi-view data are\nabundant and informative in industry and may exceed the capacity of one single\nvector, and 2) inductive bias may be introduced as multi-view data are often\nfrom different distributions. In this paper, we use a \\emph{multi-view\nrepresentation alignment} approach to address this issue. Particularly, we\npropose a multi-task multi-view graph representation learning framework (M2GRL)\nto learn node representations from multi-view graphs for web-scale recommender\nsystems. M2GRL constructs one graph for each single-view data, learns multiple\nseparate representations from multiple graphs, and performs alignment to model\ncross-view relations. M2GRL chooses a multi-task learning paradigm to learn\nintra-view representations and cross-view relations jointly. Besides, M2GRL\napplies homoscedastic uncertainty to adaptively tune the loss weights of tasks\nduring training. We deploy M2GRL at Taobao and train it on 57 billion examples.\nAccording to offline metrics and online A/B tests, M2GRL significantly\noutperforms other state-of-the-art algorithms. Further exploration on diversity\nrecommendation in Taobao shows the effectiveness of utilizing multiple\nrepresentations produced by \\method{}, which we argue is a promising direction\nfor various industrial recommendation tasks of different focus.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 15:08:57 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 23:09:51 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 03:04:22 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Wang", "Menghan", ""], ["Lin", "Yujie", ""], ["Lin", "Guli", ""], ["Yang", "Keping", ""], ["Wu", "Xiao-ming", ""]]}, {"id": "2005.10150", "submitter": "Shijie Zhang", "authors": "Shijie Zhang, Hongzhi Yin, Tong Chen, Quoc Viet Nguyen Hung, Zi Huang,\n  Lizhen Cui", "title": "GCN-Based User Representation Learning for Unifying Robust\n  Recommendation and Fraudster Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, recommender system has become an indispensable function in\nall e-commerce platforms. The review rating data for a recommender system\ntypically comes from open platforms, which may attract a group of malicious\nusers to deliberately insert fake feedback in an attempt to bias the\nrecommender system to their favour. The presence of such attacks may violate\nmodeling assumptions that high-quality data is always available and these data\ntruly reflect users' interests and preferences. Therefore, it is of great\npractical significance to construct a robust recommender system that is able to\ngenerate stable recommendations even in the presence of shilling attacks. In\nthis paper, we propose GraphRfi - a GCN-based user representation learning\nframework to perform robust recommendation and fraudster detection in a unified\nway. In its end-to-end learning process, the probability of a user being\nidentified as a fraudster in the fraudster detection component automatically\ndetermines the contribution of this user's rating data in the recommendation\ncomponent; while the prediction error outputted in the recommendation component\nacts as an important feature in the fraudster detection component. Thus, these\ntwo components can mutually enhance each other. Extensive experiments have been\nconducted and the experimental results show the superiority of our GraphRfi in\nthe two tasks - robust rating prediction and fraudster detection. Furthermore,\nthe proposed GraphRfi is validated to be more robust to the various types of\nshilling attacks over the state-of-the-art recommender systems.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 15:57:23 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Zhang", "Shijie", ""], ["Yin", "Hongzhi", ""], ["Chen", "Tong", ""], ["Hung", "Quoc Viet Nguyen", ""], ["Huang", "Zi", ""], ["Cui", "Lizhen", ""]]}, {"id": "2005.10321", "submitter": "Marko Stamenovic", "authors": "Marko Stamenovic, Jeibo Luo", "title": "Machine Identification of High Impact Research through Text and Image\n  Analysis", "comments": null, "journal-ref": "2017 IEEE Third International Conference on Multimedia Big Data\n  (BigMM)", "doi": "10.1109/BigMM.2017.63", "report-no": null, "categories": "cs.IR cs.DL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The volume of academic paper submissions and publications is growing at an\never increasing rate. While this flood of research promises progress in various\nfields, the sheer volume of output inherently increases the amount of noise. We\npresent a system to automatically separate papers with a high from those with a\nlow likelihood of gaining citations as a means to quickly find high impact,\nhigh quality research. Our system uses both a visual classifier, useful for\nsurmising a document's overall appearance, and a text classifier, for making\ncontent-informed decisions. Current work in the field focuses on small datasets\ncomposed of papers from individual conferences. Attempts to use similar\ntechniques on larger datasets generally only considers excerpts of the\ndocuments such as the abstract, potentially throwing away valuable data. We\nrectify these issues by providing a dataset composed of PDF documents and\ncitation counts spanning a decade of output within two separate academic\ndomains: computer science and medicine. This new dataset allows us to expand on\ncurrent work in the field by generalizing across time and academic domain.\nMoreover, we explore inter-domain prediction models - evaluating a classifier's\nperformance on a domain it was not trained on - to shed further insight on this\nimportant problem.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 19:12:24 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Stamenovic", "Marko", ""], ["Luo", "Jeibo", ""]]}, {"id": "2005.10322", "submitter": "Felice Antonio Merra", "authors": "Yashar Deldjoo and Tommaso Di Noia and Felice Antonio Merra", "title": "A survey on Adversarial Recommender Systems: from Attack/Defense\n  strategies to Generative Adversarial Networks", "comments": "37 pages, submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent-factor models (LFM) based on collaborative filtering (CF), such as\nmatrix factorization (MF) and deep CF methods, are widely used in modern\nrecommender systems (RS) due to their excellent performance and recommendation\naccuracy. However, success has been accompanied with a major new arising\nchallenge: many applications of machine learning (ML) are adversarial in\nnature. In recent years, it has been shown that these methods are vulnerable to\nadversarial examples, i.e., subtle but non-random perturbations designed to\nforce recommendation models to produce erroneous outputs.\n  The goal of this survey is two-fold: (i) to present recent advances on\nadversarial machine learning (AML) for the security of RS (i.e., attacking and\ndefense recommendation models), (ii) to show another successful application of\nAML in generative adversarial networks (GANs) for generative applications,\nthanks to their ability for learning (high-dimensional) data distributions. In\nthis survey, we provide an exhaustive literature review of 74 articles\npublished in major RS and ML journals and conferences. This review serves as a\nreference for the RS community, working on the security of RS or on generative\nmodels using GANs to improve their quality.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 19:17:11 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 10:48:34 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Deldjoo", "Yashar", ""], ["Di Noia", "Tommaso", ""], ["Merra", "Felice Antonio", ""]]}, {"id": "2005.10334", "submitter": "Arthur Brack", "authors": "Arthur Brack, Anett Hoppe, Markus Stocker, S\\\"oren Auer, Ralph Ewerth", "title": "Requirements Analysis for an Open Research Knowledge Graph", "comments": "Accepted for publishing in 24th International Conference on Theory\n  and Practice of Digital Libraries, TPDL 2020", "journal-ref": "Digital Libraries for Open Knowledge. TPDL 2020. Lecture Notes in\n  Computer Science, vol 12246. Springer, Cham", "doi": "10.1007/978-3-030-54956-5_1", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current science communication has a number of drawbacks and bottlenecks which\nhave been subject of discussion lately: Among others, the rising number of\npublished articles makes it nearly impossible to get an overview of the state\nof the art in a certain field, or reproducibility is hampered by fixed-length,\ndocument-based publications which normally cannot cover all details of a\nresearch work. Recently, several initiatives have proposed knowledge graphs\n(KGs) for organising scientific information as a solution to many of the\ncurrent issues. The focus of these proposals is, however, usually restricted to\nvery specific use cases. In this paper, we aim to transcend this limited\nperspective by presenting a comprehensive analysis of requirements for an Open\nResearch Knowledge Graph (ORKG) by (a) collecting daily core tasks of a\nscientist, (b) establishing their consequential requirements for a KG-based\nsystem, (c) identifying overlaps and specificities, and their coverage in\ncurrent solutions. As a result, we map necessary and desirable requirements for\nsuccessful KG-based science communication, derive implications and outline\npossible solutions.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 19:56:58 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Brack", "Arthur", ""], ["Hoppe", "Anett", ""], ["Stocker", "Markus", ""], ["Auer", "S\u00f6ren", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2005.10473", "submitter": "Adit Krishnan", "authors": "Adit Krishnan, Mahashweta Das, Mangesh Bendre, Hao Yang, Hari Sundaram", "title": "Transfer Learning via Contextual Invariants for One-to-Many Cross-Domain\n  Recommendation", "comments": "SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401078", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid proliferation of new users and items on the social web has\naggravated the gray-sheep user/long-tail item challenge in recommender systems.\nHistorically, cross-domain co-clustering methods have successfully leveraged\nshared users and items across dense and sparse domains to improve inference\nquality. However, they rely on shared rating data and cannot scale to multiple\nsparse target domains (i.e., the one-to-many transfer setting). This, combined\nwith the increasing adoption of neural recommender architectures, motivates us\nto develop scalable neural layer-transfer approaches for cross-domain learning.\nOur key intuition is to guide neural collaborative filtering with\ndomain-invariant components shared across the dense and sparse domains,\nimproving the user and item representations learned in the sparse domains. We\nleverage contextual invariances across domains to develop these shared modules,\nand demonstrate that with user-item interaction context, we can learn-to-learn\ninformative representation spaces even with sparse interaction data. We show\nthe effectiveness and scalability of our approach on two public datasets and a\nmassive transaction dataset from Visa, a global payments technology company\n(19% Item Recall, 3x faster vs. training separate models for each domain). Our\napproach is applicable to both implicit and explicit feedback settings.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 05:51:15 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Krishnan", "Adit", ""], ["Das", "Mahashweta", ""], ["Bendre", "Mangesh", ""], ["Yang", "Hao", ""], ["Sundaram", "Hari", ""]]}, {"id": "2005.10545", "submitter": "Zhihong Chen", "authors": "Zhihong Chen, Rong Xiao, Chenliang Li, Gangfeng Ye, Haochuan Sun,\n  Hongbo Deng", "title": "ESAM: Discriminative Domain Adaptation with Non-Displayed Items to\n  Improve Long-Tail Performance", "comments": "Accept by SIGIR-2020", "journal-ref": null, "doi": "10.1145/3397271.3401043", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of ranking models are trained only with displayed items (most are hot\nitems), but they are utilized to retrieve items in the entire space which\nconsists of both displayed and non-displayed items (most are long-tail items).\nDue to the sample selection bias, the long-tail items lack sufficient records\nto learn good feature representations, i.e. data sparsity and cold start\nproblems. The resultant distribution discrepancy between displayed and\nnon-displayed items would cause poor long-tail performance. To this end, we\npropose an entire space adaptation model (ESAM) to address this problem from\nthe perspective of domain adaptation (DA). ESAM regards displayed and\nnon-displayed items as source and target domains respectively. Specifically, we\ndesign the attribute correlation alignment that considers the correlation\nbetween high-level attributes of the item to achieve distribution alignment.\nFurthermore, we introduce two effective regularization strategies, i.e.\n\\textit{center-wise clustering} and \\textit{self-training} to improve DA\nprocess. Without requiring any auxiliary information and auxiliary domains,\nESAM transfers the knowledge from displayed items to non-displayed items for\nalleviating the distribution inconsistency. Experiments on two public datasets\nand a large-scale industrial dataset collected from Taobao demonstrate that\nESAM achieves state-of-the-art performance, especially in the long-tail space.\nBesides, we deploy ESAM to the Taobao search engine, leading to significant\nimprovement on online performance. The code is available at\n\\url{https://github.com/A-bone1/ESAM.git}\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 09:58:07 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Chen", "Zhihong", ""], ["Xiao", "Rong", ""], ["Li", "Chenliang", ""], ["Ye", "Gangfeng", ""], ["Sun", "Haochuan", ""], ["Deng", "Hongbo", ""]]}, {"id": "2005.10549", "submitter": "Cheng Zhao", "authors": "Cheng Zhao, Chenliang Li, Rong Xiao, Hongbo Deng, Aixin Sun", "title": "CATN: Cross-Domain Recommendation for Cold-Start Users via Aspect\n  Transfer Network", "comments": "Accepted to SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401169", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a large recommender system, the products (or items) could be in many\ndifferent categories or domains. Given two relevant domains (e.g., Book and\nMovie), users may have interactions with items in one domain but not in the\nother domain. To the latter, these users are considered as cold-start users.\nHow to effectively transfer users' preferences based on their interactions from\none domain to the other relevant domain, is the key issue in cross-domain\nrecommendation. Inspired by the advances made in review-based recommendation,\nwe propose to model user preference transfer at aspect-level derived from\nreviews. To this end, we propose a cross-domain recommendation framework via\naspect transfer network for cold-start users (named CATN). CATN is devised to\nextract multiple aspects for each user and each item from their review\ndocuments, and learn aspect correlations across domains with an attention\nmechanism. In addition, we further exploit auxiliary reviews from like-minded\nusers to enhance a user's aspect representations. Then, an end-to-end\noptimization framework is utilized to strengthen the robustness of our model.\nOn real-world datasets, the proposed CATN outperforms SOTA models significantly\nin terms of rating prediction accuracy. Further analysis shows that our model\nis able to reveal user aspect connections across domains at a fine level of\ngranularity, making the recommendation explainable.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 10:05:19 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 07:13:11 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Zhao", "Cheng", ""], ["Li", "Chenliang", ""], ["Xiao", "Rong", ""], ["Deng", "Hongbo", ""], ["Sun", "Aixin", ""]]}, {"id": "2005.10602", "submitter": "Ruiyang Ren", "authors": "Ruiyang Ren, Zhaoyang Liu, Yaliang Li, Wayne Xin Zhao, Hui Wang, Bolin\n  Ding, Ji-Rong Wen", "title": "Sequential Recommendation with Self-Attentive Multi-Adversarial Network", "comments": null, "journal-ref": null, "doi": "10.1145/3397271.3401111", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning has made significant progress in the task of\nsequential recommendation. Existing neural sequential recommenders typically\nadopt a generative way trained with Maximum Likelihood Estimation (MLE). When\ncontext information (called factor) is involved, it is difficult to analyze\nwhen and how each individual factor would affect the final recommendation\nperformance. For this purpose, we take a new perspective and introduce\nadversarial learning to sequential recommendation. In this paper, we present a\nMulti-Factor Generative Adversarial Network (MFGAN) for explicitly modeling the\neffect of context information on sequential recommendation. Specifically, our\nproposed MFGAN has two kinds of modules: a Transformer-based generator taking\nuser behavior sequences as input to recommend the possible next items, and\nmultiple factor-specific discriminators to evaluate the generated sub-sequence\nfrom the perspectives of different factors. To learn the parameters, we adopt\nthe classic policy gradient method, and utilize the reward signal of\ndiscriminators for guiding the learning of the generator. Our framework is\nflexible to incorporate multiple kinds of factor information, and is able to\ntrace how each factor contributes to the recommendation decision over time.\nExtensive experiments conducted on three real-world datasets demonstrate the\nsuperiority of our proposed model over the state-of-the-art methods, in terms\nof effectiveness and interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 12:28:59 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Ren", "Ruiyang", ""], ["Liu", "Zhaoyang", ""], ["Li", "Yaliang", ""], ["Zhao", "Wayne Xin", ""], ["Wang", "Hui", ""], ["Ding", "Bolin", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2005.10615", "submitter": "Rolf Jagerman", "authors": "Rolf Jagerman and Maarten de Rijke", "title": "Accelerated Convergence for Counterfactual Learning to Rank", "comments": "SIGIR 2020 full conference paper", "journal-ref": null, "doi": "10.1145/3397271.3401069", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual Learning to Rank (LTR) algorithms learn a ranking model from\nlogged user interactions, often collected using a production system. Employing\nsuch an offline learning approach has many benefits compared to an online one,\nbut it is challenging as user feedback often contains high levels of bias.\nUnbiased LTR uses Inverse Propensity Scoring (IPS) to enable unbiased learning\nfrom logged user interactions. One of the major difficulties in applying\nStochastic Gradient Descent (SGD) approaches to counterfactual learning\nproblems is the large variance introduced by the propensity weights. In this\npaper we show that the convergence rate of SGD approaches with IPS-weighted\ngradients suffers from the large variance introduced by the IPS weights:\nconvergence is slow, especially when there are large IPS weights. To overcome\nthis limitation, we propose a novel learning algorithm, called CounterSample,\nthat has provably better convergence than standard IPS-weighted gradient\ndescent methods. We prove that CounterSample converges faster and complement\nour theoretical findings with empirical results by performing extensive\nexperimentation in a number of biased LTR scenarios -- across optimizers, batch\nsizes, and different degrees of position bias.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 12:53:36 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Jagerman", "Rolf", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2005.10700", "submitter": "Hayden Helm", "authors": "Hayden S. Helm, Amitabh Basu, Avanti Athreya, Youngser Park, Joshua T.\n  Vogelstein, Michael Winding, Marta Zlatic, Albert Cardona, Patrick Bourke,\n  Jonathan Larson, Chris White, Carey E. Priebe", "title": "Learning to rank via combining representations", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to rank -- producing a ranked list of items specific to a query and\nwith respect to a set of supervisory items -- is a problem of general interest.\nThe setting we consider is one in which no analytic description of what\nconstitutes a good ranking is available. Instead, we have a collection of\nrepresentations and supervisory information consisting of a (target item,\ninteresting items set) pair. We demonstrate -- analytically, in simulation, and\nin real data examples -- that learning to rank via combining representations\nusing an integer linear program is effective when the supervision is as light\nas \"these few items are similar to your item of interest.\" While this\nnomination task is of general interest, for specificity we present our\nmethodology from the perspective of vertex nomination in graphs. The\nmethodology described herein is model agnostic.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 01:53:58 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 13:37:50 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Helm", "Hayden S.", ""], ["Basu", "Amitabh", ""], ["Athreya", "Avanti", ""], ["Park", "Youngser", ""], ["Vogelstein", "Joshua T.", ""], ["Winding", "Michael", ""], ["Zlatic", "Marta", ""], ["Cardona", "Albert", ""], ["Bourke", "Patrick", ""], ["Larson", "Jonathan", ""], ["White", "Chris", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2005.10865", "submitter": "Benjamin Nye", "authors": "Benjamin E. Nye, Ani Nenkova, Iain J. Marshall, Byron C. Wallace", "title": "Trialstreamer: Mapping and Browsing Medical Evidence in Real-Time", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Trialstreamer, a living database of clinical trial reports. Here\nwe mainly describe the evidence extraction component; this extracts from\nbiomedical abstracts key pieces of information that clinicians need when\nappraising the literature, and also the relations between these. Specifically,\nthe system extracts descriptions of trial participants, the treatments compared\nin each arm (the interventions), and which outcomes were measured. The system\nthen attempts to infer which interventions were reported to work best by\ndetermining their relationship with identified trial outcome measures. In\naddition to summarizing individual trials, these extracted data elements allow\nautomatic synthesis of results across many trials on the same topic. We apply\nthe system at scale to all reports of randomized controlled trials indexed in\nMEDLINE, powering the automatic generation of evidence maps, which provide a\nglobal view of the efficacy of different interventions combining data from all\nrelevant clinical trials on a topic. We make all code and models freely\navailable alongside a demonstration of the web interface.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 19:32:04 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Nye", "Benjamin E.", ""], ["Nenkova", "Ani", ""], ["Marshall", "Iain J.", ""], ["Wallace", "Byron C.", ""]]}, {"id": "2005.10898", "submitter": "G. G. Md. Nawaz Ali", "authors": "Jim Samuel, G. G. Md. Nawaz Ali, Md. Mokhlesur Rahman, Ek Esawi, Yana\n  Samuel", "title": "COVID-19 Public Sentiment Insights and Machine Learning for Tweets\n  Classification", "comments": null, "journal-ref": "https://www.mdpi.com/2078-2489/11/6/314/htm", "doi": "10.3390/info11060314", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Along with the Coronavirus pandemic, another crisis has manifested itself in\nthe form of mass fear and panic phenomena, fueled by incomplete and often\ninaccurate information. There is therefore a tremendous need to address and\nbetter understand COVID-19's informational crisis and gauge public sentiment,\nso that appropriate messaging and policy decisions can be implemented. In this\nresearch article, we identify public sentiment associated with the pandemic\nusing Coronavirus specific Tweets and R statistical software, along with its\nsentiment analysis packages. We demonstrate insights into the progress of\nfear-sentiment over time as COVID-19 approached peak levels in the United\nStates, using descriptive textual analytics supported by necessary textual data\nvisualizations. Furthermore, we provide a methodological overview of two\nessential machine learning (ML) classification methods, in the context of\ntextual analytics, and compare their effectiveness in classifying Coronavirus\nTweets of varying lengths. We observe a strong classification accuracy of 91%\nfor short Tweets, with the Naive Bayes method. We also observe that the\nlogistic regression classification method provides a reasonable accuracy of 74%\nwith shorter Tweets, and both methods showed relatively weaker performance for\nlonger Tweets. This research provides insights into Coronavirus fear sentiment\nprogression, and outlines associated methods, implications, limitations and\nopportunities.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 20:53:26 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Samuel", "Jim", ""], ["Ali", "G. G. Md. Nawaz", ""], ["Rahman", "Md. Mokhlesur", ""], ["Esawi", "Ek", ""], ["Samuel", "Yana", ""]]}, {"id": "2005.10899", "submitter": "Diwakar Mahajan", "authors": "Diwakar Mahajan, Jennifer J. Liang, Ching-Huei Tsou", "title": "Extracting Daily Dosage from Medication Instructions in EHRs: An\n  Automated Approach and Lessons Learned", "comments": "10 pages, 4 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding a patient's medication history is essential for physicians to\nprovide appropriate treatment recommendations. A medication's prescribed daily\ndosage is a key element of the medication history; however, it is generally not\nprovided as a discrete quantity and needs to be derived from free text\nmedication instructions (Sigs) in the structured electronic health record\n(EHR). Existing works in daily dosage extraction are narrow in scope, dealing\nwith dosage extraction for a single drug from clinical notes. Here, we present\nan automated approach to calculate daily dosage for all medications in EHR\nstructured data. We describe and characterize the variable language used in\nSigs, and present our hybrid system for calculating daily dosage combining deep\nlearning-based named entity extractor with lexicon dictionaries and regular\nexpressions. Our system achieves 0.98 precision and 0.95 recall on an\nexpert-generated dataset of 1000 Sigs, demonstrating its effectiveness on the\ngeneral purpose daily dosage calculation task.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 20:55:22 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Mahajan", "Diwakar", ""], ["Liang", "Jennifer J.", ""], ["Tsou", "Ching-Huei", ""]]}, {"id": "2005.10961", "submitter": "G. G. Md. Nawaz Ali", "authors": "Jim Samuel, Md. Mokhlesur Rahman, G. G. Md. Nawaz Ali, Yana Samuel,\n  Alexander Pelaez", "title": "Feeling Like It is Time to Reopen Now? COVID-19 New Normal Scenarios\n  based on Reopening Sentiment Analytics", "comments": null, "journal-ref": null, "doi": "10.20944/preprints202005.0318.v1", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Coronavirus pandemic has created complex challenges and adverse\ncircumstances. This research discovers public sentiment amidst problematic\nsocioeconomic consequences of the lockdown, and explores ensuing four potential\nsentiment associated scenarios. The severity and brutality of COVID-19 have led\nto the development of extreme feelings, and emotional and mental healthcare\nchallenges. This research identifies emotional consequences - the presence of\nextreme fear, confusion and volatile sentiments, mixed along with trust and\nanticipation. It is necessary to gauge dominant public sentiment trends for\neffective decisions and policies. This study analyzes public sentiment using\nTwitter Data, time-aligned to COVID-19, to identify dominant sentiment trends\nassociated with the push to 'reopen' the economy. Present research uses textual\nanalytics methodologies to analyze public sentiment support for two potential\ndivergent scenarios - an early opening and a delayed opening, and consequences\nof each. Present research concludes on the basis of exploratory textual\nanalytics and textual data visualization, that Tweets data from American\nTwitter users shows more trust sentiment support, than fear, for reopening the\nUS economy. With additional validation, this could present a valuable time\nsensitive opportunity for state governments, the federal government,\ncorporations and societal leaders to guide the nation into a successful new\nnormal future.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 01:29:51 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Samuel", "Jim", ""], ["Rahman", "Md. Mokhlesur", ""], ["Ali", "G. G. Md. Nawaz", ""], ["Samuel", "Yana", ""], ["Pelaez", "Alexander", ""]]}, {"id": "2005.11017", "submitter": "Mengxi Wei", "authors": "Mengxi Wei, Yifan He, Qiong Zhang", "title": "Robust Layout-aware IE for Visually Rich Documents with Pre-trained\n  Language Models", "comments": "10 pages, to appear in SIGIR 2020 Industry Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many business documents processed in modern NLP and IR pipelines are visually\nrich: in addition to text, their semantics can also be captured by visual\ntraits such as layout, format, and fonts. We study the problem of information\nextraction from visually rich documents (VRDs) and present a model that\ncombines the power of large pre-trained language models and graph neural\nnetworks to efficiently encode both textual and visual information in business\ndocuments. We further introduce new fine-tuning objectives to improve in-domain\nunsupervised fine-tuning to better utilize large amount of unlabeled in-domain\ndata. We experiment on real world invoice and resume data sets and show that\nthe proposed method outperforms strong text-based RoBERTa baselines by 6.3%\nabsolute F1 on invoices and 4.7% absolute F1 on resumes. When evaluated in a\nfew-shot setting, our method requires up to 30x less annotation data than the\nbaseline to achieve the same level of performance at ~90% F1.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 06:04:50 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Wei", "Mengxi", ""], ["He", "Yifan", ""], ["Zhang", "Qiong", ""]]}, {"id": "2005.11021", "submitter": "Moritz Schubotz", "authors": "Philipp Scharpf, Moritz Schubotz, Abdou Youssef, Felix Hamborg, Norman\n  Meuschke, Bela Gipp", "title": "Classification and Clustering of arXiv Documents, Sections, and\n  Abstracts, Comparing Encodings of Natural and Mathematical Language", "comments": null, "journal-ref": "Proceedings of the ACM/IEEE Joint Conference on Digital Libraries\n  JCDL 2020", "doi": "10.1145/3383583.3398529", "report-no": null, "categories": "cs.DL cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we show how selecting and combining encodings of natural and\nmathematical language affect classification and clustering of documents with\nmathematical content. We demonstrate this by using sets of documents, sections,\nand abstracts from the arXiv preprint server that are labeled by their subject\nclass (mathematics, computer science, physics, etc.) to compare different\nencodings of text and formulae and evaluate the performance and runtimes of\nselected classification and clustering algorithms. Our encodings achieve\nclassification accuracies up to $82.8\\%$ and cluster purities up to $69.4\\%$\n(number of clusters equals number of classes), and $99.9\\%$ (unspecified number\nof clusters) respectively. We observe a relatively low correlation between text\nand math similarity, which indicates the independence of text and formulae and\nmotivates treating them as separate features of a document. The classification\nand clustering can be employed, e.g., for document search and recommendation.\nFurthermore, we show that the computer outperforms a human expert when\nclassifying documents. Finally, we evaluate and discuss multi-label\nclassification and formula semantification.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 06:16:32 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Scharpf", "Philipp", ""], ["Schubotz", "Moritz", ""], ["Youssef", "Abdou", ""], ["Hamborg", "Felix", ""], ["Meuschke", "Norman", ""], ["Gipp", "Bela", ""]]}, {"id": "2005.11041", "submitter": "Xovee Xu", "authors": "Fan Zhou, Xovee Xu, Goce Trajcevski, Kunpeng Zhang", "title": "A Survey of Information Cascade Analysis: Models, Predictions, and\n  Recent Advances", "comments": "Author version, with 43 pages, 9 figures, and 11 tables", "journal-ref": "ACM Computing Surveys (CSUR), 54(2), Article 27, Mar 2021, 36\n  pages", "doi": "10.1145/3433000", "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deluge of digital information in our daily life -- from user-generated\ncontent, such as microblogs and scientific papers, to online business, such as\nviral marketing and advertising -- offers unprecedented opportunities to\nexplore and exploit the trajectories and structures of the evolution of\ninformation cascades. Abundant research efforts, both academic and industrial,\nhave aimed to reach a better understanding of the mechanisms driving the spread\nof information and quantifying the outcome of information diffusion. This\narticle presents a comprehensive review and categorization of information\npopularity prediction methods, from feature engineering and stochastic\nprocesses, through graph representation, to deep learning-based approaches.\nSpecifically, we first formally define different types of information cascades\nand summarize the perspectives of existing studies. We then present a taxonomy\nthat categorizes existing works into the aforementioned three main groups as\nwell as the main subclasses in each group, and we systematically review\ncutting-edge research work. Finally, we summarize the pros and cons of existing\nresearch efforts and outline the open challenges and opportunities in this\nfield.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 07:39:48 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 04:46:20 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 03:33:58 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zhou", "Fan", ""], ["Xu", "Xovee", ""], ["Trajcevski", "Goce", ""], ["Zhang", "Kunpeng", ""]]}, {"id": "2005.11177", "submitter": "Muhammad Imran", "authors": "Umair Qazi, Muhammad Imran, Ferda Ofli", "title": "GeoCoV19: A Dataset of Hundreds of Millions of Multilingual COVID-19\n  Tweets with Location Information", "comments": "10 pages, 5 figures, accepted at ACM SIGSPATIAL Special May 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past several years have witnessed a huge surge in the use of social media\nplatforms during mass convergence events such as health emergencies, natural or\nhuman-induced disasters. These non-traditional data sources are becoming vital\nfor disease forecasts and surveillance when preparing for epidemic and pandemic\noutbreaks. In this paper, we present GeoCoV19, a large-scale Twitter dataset\ncontaining more than 524 million multilingual tweets posted over a period of 90\ndays since February 1, 2020. Moreover, we employ a gazetteer-based approach to\ninfer the geolocation of tweets. We postulate that this large-scale,\nmultilingual, geolocated social media data can empower the research communities\nto evaluate how societies are collectively coping with this unprecedented\nglobal crisis as well as to develop computational methods to address challenges\nsuch as identifying fake news, understanding communities' knowledge gaps,\nbuilding disease forecast and surveillance models, among others.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 13:30:42 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Qazi", "Umair", ""], ["Imran", "Muhammad", ""], ["Ofli", "Ferda", ""]]}, {"id": "2005.11223", "submitter": "Yunchang Zhu", "authors": "Yunchang Zhu, Liang Pang, Yanyan Lan, Xueqi Cheng", "title": "L2R2: Leveraging Ranking for Abductive Reasoning", "comments": "SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The abductive natural language inference task ($\\alpha$NLI) is proposed to\nevaluate the abductive reasoning ability of a learning system. In the\n$\\alpha$NLI task, two observations are given and the most plausible hypothesis\nis asked to pick out from the candidates. Existing methods simply formulate it\nas a classification problem, thus a cross-entropy log-loss objective is used\nduring training. However, discriminating true from false does not measure the\nplausibility of a hypothesis, for all the hypotheses have a chance to happen,\nonly the probabilities are different. To fill this gap, we switch to a ranking\nperspective that sorts the hypotheses in order of their plausibilities. With\nthis new perspective, a novel $L2R^2$ approach is proposed under the\nlearning-to-rank framework. Firstly, training samples are reorganized into a\nranking form, where two observations and their hypotheses are treated as the\nquery and a set of candidate documents respectively. Then, an ESIM model or\npre-trained language model, e.g. BERT or RoBERTa, is obtained as the scoring\nfunction. Finally, the loss functions for the ranking task can be either\npair-wise or list-wise for training. The experimental results on the ART\ndataset reach the state-of-the-art in the public leaderboard.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 15:01:23 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Zhu", "Yunchang", ""], ["Pang", "Liang", ""], ["Lan", "Yanyan", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2005.11317", "submitter": "Mohsen Amini Salehi", "authors": "SM Zobaed, Raju Gottmukkala, Mohsen Amini Salehi", "title": "Privacy-Preserving Clustering of Unstructured Big Data for Cloud-Based\n  Enterprise Search Solutions", "comments": "arXiv admin note: text overlap with arXiv:1908.04960", "journal-ref": "ACM Transactions on Information Technology (ACM TOIT), May 2020", "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud-based enterprise search services (e.g., Amazon Kendra) are enchanting\nto big data owners by providing them with convenient search solutions over\ntheir enterprise big datasets. However, individuals and businesses that deal\nwith confidential big data (eg, credential documents) are reluctant to fully\nembrace such services, due to valid concerns about data privacy. Solutions\nbased on client-side encryption have been explored to mitigate privacy\nconcerns. Nonetheless, such solutions hinder data processing, specifically\nclustering, which is pivotal in dealing with different forms of big data. For\ninstance, clustering is critical to limit the search space and perform\nreal-time search operations on big datasets. To overcome the hindrance in\nclustering encrypted big data, we propose privacy-preserving clustering schemes\nfor three forms of unstructured encrypted big datasets, namely static,\nsemi-dynamic, and dynamic datasets. To preserve data privacy, the proposed\nclustering schemes function based on statistical characteristics of the data\nand determine (A) the suitable number of clusters and (B) appropriate content\nfor each cluster. Experimental results obtained from evaluating the clustering\nschemes on three different datasets demonstrate between 30% to 60% improvement\non the clusters' coherency compared to other clustering schemes for encrypted\ndata. Employing the clustering schemes in a privacy-preserving enterprise\nsearch system decreases its search time by up to 78%, while increases the\nsearch accuracy by up to 35%.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 08:42:19 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Zobaed", "SM", ""], ["Gottmukkala", "Raju", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "2005.11364", "submitter": "Chen Qu", "authors": "Chen Qu, Liu Yang, Cen Chen, Minghui Qiu, W. Bruce Croft and Mohit\n  Iyyer", "title": "Open-Retrieval Conversational Question Answering", "comments": "Accepted to SIGIR'20", "journal-ref": null, "doi": "10.1145/3397271.3401110", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search is one of the ultimate goals of information retrieval.\nRecent research approaches conversational search by simplified settings of\nresponse ranking and conversational question answering, where an answer is\neither selected from a given candidate set or extracted from a given passage.\nThese simplifications neglect the fundamental role of retrieval in\nconversational search. To address this limitation, we introduce an\nopen-retrieval conversational question answering (ORConvQA) setting, where we\nlearn to retrieve evidence from a large collection before extracting answers,\nas a further step towards building functional conversational search systems. We\ncreate a dataset, OR-QuAC, to facilitate research on ORConvQA. We build an\nend-to-end system for ORConvQA, featuring a retriever, a reranker, and a reader\nthat are all based on Transformers. Our extensive experiments on OR-QuAC\ndemonstrate that a learnable retriever is crucial for ORConvQA. We further show\nthat our system can make a substantial improvement when we enable history\nmodeling in all system components. Moreover, we show that the reranker\ncomponent contributes to the model performance by providing a regularization\neffect. Finally, further in-depth analyses are performed to provide new\ninsights into ORConvQA.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 19:39:50 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Qu", "Chen", ""], ["Yang", "Liu", ""], ["Chen", "Cen", ""], ["Qiu", "Minghui", ""], ["Croft", "W. Bruce", ""], ["Iyyer", "Mohit", ""]]}, {"id": "2005.11422", "submitter": "Khushboo Thaker", "authors": "Mengdi Wang, Hung Chau, Khushboo Thaker, Peter Brusilovsky, Daqing He", "title": "Concept Annotation for Intelligent Textbooks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased popularity of electronic textbooks, there is a growing\ninterests in developing a new generation of \"intelligent textbooks\", which have\nthe ability to guide the readers according to their learning goals and current\nknowledge. The intelligent textbooks extend regular textbooks by integrating\nmachine-manipulatable knowledge such as a knowledge map or a\nprerequisite-outcome relationship between sections, among which, the most\npopular integrated knowledge is a list of unique knowledge concepts associated\nwith each section. With the help of this concept, multiple intelligent\noperations, such as content linking, content recommendation or student\nmodeling, can be performed. However, annotating a reliable set of concepts to a\ntextbook section is a challenge. Automatic unsupervised methods for extracting\nkey-phrases as the concepts are known to have insufficient accuracy. Manual\nannotation by experts is considered as a preferred approach and can be used to\nproduce both the target outcome and the labeled data for training supervised\nmodels. However, most researchers in education domain still consider the\nconcept annotation process as an ad-hoc activity rather than an engineering\ntask, resulting in low-quality annotated data. In this paper, we present a\ntextbook knowledge engineering method to obtain reliable concept annotations.\nThe outcomes of our work include a validated knowledge engineering procedure, a\ncode-book for technical concept annotation, and a set of concept annotations\nfor the target textbook, which could be used as gold standard in further\nresearch.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 23:30:37 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 03:34:40 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2020 01:07:44 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Wang", "Mengdi", ""], ["Chau", "Hung", ""], ["Thaker", "Khushboo", ""], ["Brusilovsky", "Peter", ""], ["He", "Daqing", ""]]}, {"id": "2005.11458", "submitter": "Boyi Liu", "authors": "Yixian Zhang, Jieren Chen, Boyi Liu, Yifan Yang, Haocheng Li, Xinyi\n  Zheng, Xi Chen, Tenglong Ren and Naixue Xiong", "title": "COVID-19 Public Opinion and Emotion Monitoring System Based on Time\n  Series Thermal New Word Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the spread and development of new epidemics, it is of great reference\nvalue to identify the changing trends of epidemics in public emotions. We\ndesigned and implemented the COVID-19 public opinion monitoring system based on\ntime series thermal new word mining. A new word structure discovery scheme\nbased on the timing explosion of network topics and a Chinese sentiment\nanalysis method for the COVID-19 public opinion environment is proposed.\nEstablish a \"Scrapy-Redis-Bloomfilter\" distributed crawler framework to collect\ndata. The system can judge the positive and negative emotions of the reviewer\nbased on the comments, and can also reflect the depth of the seven emotions\nsuch as Hopeful, Happy, and Depressed. Finally, we improved the sentiment\ndiscriminant model of this system and compared the sentiment discriminant error\nof COVID-19 related comments with the Jiagu deep learning model. The results\nshow that our model has better generalization ability and smaller discriminant\nerror. We designed a large data visualization screen, which can clearly show\nthe trend of public emotions, the proportion of various emotion categories,\nkeywords, hot topics, etc., and fully and intuitively reflect the development\nof public opinion.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 03:42:10 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Zhang", "Yixian", ""], ["Chen", "Jieren", ""], ["Liu", "Boyi", ""], ["Yang", "Yifan", ""], ["Li", "Haocheng", ""], ["Zheng", "Xinyi", ""], ["Chen", "Xi", ""], ["Ren", "Tenglong", ""], ["Xiong", "Naixue", ""]]}, {"id": "2005.11467", "submitter": "Tingting Liang", "authors": "Tingting Liang, Congying Xia, Yuyu Yin, Philip S. Yu", "title": "Joint Training Capsule Network for Cold Start Recommendation", "comments": "Accepted by SIGIR'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel neural network, joint training capsule network\n(JTCN), for the cold start recommendation task. We propose to mimic the\nhigh-level user preference other than the raw interaction history based on the\nside information for the fresh users. Specifically, an attentive capsule layer\nis proposed to aggregate high-level user preference from the low-level\ninteraction history via a dynamic routing-by-agreement mechanism. Moreover,\nJTCN jointly trains the loss for mimicking the user preference and the softmax\nloss for the recommendation together in an end-to-end manner. Experiments on\ntwo publicly available datasets demonstrate the effectiveness of the proposed\nmodel. JTCN improves other state-of-the-art methods at least 7.07% for\nCiteULike and 16.85% for Amazon in terms of Recall@100 in cold start\nrecommendation.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 04:27:38 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Liang", "Tingting", ""], ["Xia", "Congying", ""], ["Yin", "Yuyu", ""], ["Yu", "Philip S.", ""]]}, {"id": "2005.11490", "submitter": "Shuo Zhang", "authors": "Shuo Zhang and Zhuyun Dai and Krisztian Balog and Jamie Callan", "title": "Summarizing and Exploring Tabular Data in Conversational Search", "comments": "Proceedings of the 43rd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR 2020), 2020", "journal-ref": null, "doi": "10.1145/3397271.3401205", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tabular data provide answers to a significant portion of search queries.\nHowever, reciting an entire result table is impractical in conversational\nsearch systems. We propose to generate natural language summaries as answers to\ndescribe the complex information contained in a table. Through crowdsourcing\nexperiments, we build a new conversation-oriented, open-domain table\nsummarization dataset. It includes annotated table summaries, which not only\nanswer questions but also help people explore other information in the table.\nWe utilize this dataset to develop automatic table summarization systems as\nSOTA baselines. Based on the experimental results, we identify challenges and\npoint out future research directions that this resource will support.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 08:29:51 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 18:34:53 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 15:56:31 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Zhang", "Shuo", ""], ["Dai", "Zhuyun", ""], ["Balog", "Krisztian", ""], ["Callan", "Jamie", ""]]}, {"id": "2005.11504", "submitter": "Cornelius Ihle", "authors": "Cornelius Ihle, Moritz Schubotz, Norman Meuschke, Bela Gipp", "title": "A First Step Towards Content Protecting Plagiarism Detection", "comments": "Submitted to JCDL 2020: Proceedings of the ACM/ IEEE Joint Conference\n  on Digital Libraries in 2020 (JCDL '20), August 1-5, 2020, Virtual Event,\n  China", "journal-ref": null, "doi": "10.1145/3383583.3398620", "report-no": null, "categories": "cs.CR cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plagiarism detection systems are essential tools for safeguarding academic\nand educational integrity. However, today's systems require disclosing the full\ncontent of the input documents and the document collection to which the input\ndocuments are compared. Moreover, the systems are centralized and under the\ncontrol of individual, typically commercial providers. This situation raises\nprocedural and legal concerns regarding the confidentiality of sensitive data,\nwhich can limit or prohibit the use of plagiarism detection services. To\neliminate these weaknesses of current systems, we seek to devise a plagiarism\ndetection approach that does not require a centralized provider nor exposing\nany content as cleartext. This paper presents the initial results of our\nresearch. Specifically, we employ Private Set Intersection to devise a\ncontent-protecting variant of the citation-based similarity measure\nBibliographic Coupling implemented in our plagiarism detection system HyPlag.\nOur evaluation shows that the content-protecting method achieves the same\ndetection effectiveness as the original method while making common attacks to\ndisclose the protected content practically infeasible. Our future work will\nextend this successful proof-of-concept by devising plagiarism detection\nmethods that can analyze the entire content of documents without disclosing it\nas cleartext.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 09:58:07 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ihle", "Cornelius", ""], ["Schubotz", "Moritz", ""], ["Meuschke", "Norman", ""], ["Gipp", "Bela", ""]]}, {"id": "2005.11547", "submitter": "Tobias Christiani", "authors": "Tobias Christiani", "title": "DartMinHash: Fast Sketching for Weighted Sets", "comments": "See https://github.com/tobc/dartminhash for the code accompanying the\n  experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted minwise hashing is a standard dimensionality reduction technique\nwith applications to similarity search and large-scale kernel machines. We\nintroduce a simple algorithm that takes a weighted set $x \\in \\mathbb{R}_{\\geq\n0}^{d}$ and computes $k$ independent minhashes in expected time $O(k \\log k +\n\\Vert x \\Vert_{0}\\log( \\Vert x \\Vert_1 + 1/\\Vert x \\Vert_1))$, improving upon\nthe state-of-the-art BagMinHash algorithm (KDD '18) and representing the\nfastest weighted minhash algorithm for sparse data. Our experiments show\nrunning times that scale better with $k$ and $\\Vert x \\Vert_0$ compared to ICWS\n(ICDM '10) and BagMinhash, obtaining $10$x speedups in common use cases. Our\napproach also gives rise to a technique for computing fully independent\nlocality-sensitive hash values for $(L, K)$-parameterized approximate near\nneighbor search under weighted Jaccard similarity in optimal expected time\n$O(LK + \\Vert x \\Vert_0)$, improving on prior work even in the case of\nunweighted sets.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 14:59:25 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Christiani", "Tobias", ""]]}, {"id": "2005.11687", "submitter": "Nikola Milo\\v{s}evi\\'c Dr", "authors": "Nikola Milosevic, Gangamma Kalappa, Hesam Dadafarin, Mahmoud Azimaee,\n  Goran Nenadic", "title": "MASK: A flexible framework to facilitate de-identification of clinical\n  texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Medical health records and clinical summaries contain a vast amount of\nimportant information in textual form that can help advancing research on\ntreatments, drugs and public health. However, the majority of these information\nis not shared because they contain private information about patients, their\nfamilies, or medical staff treating them. Regulations such as HIPPA in the US,\nPHIPPA in Canada and GDPR regulate the protection, processing and distribution\nof this information. In case this information is de-identified and personal\ninformation are replaced or redacted, they could be distributed to the research\ncommunity. In this paper, we present MASK, a software package that is designed\nto perform the de-identification task. The software is able to perform named\nentity recognition using some of the state-of-the-art techniques and then mask\nor redact recognized entities. The user is able to select named entity\nrecognition algorithm (currently implemented are two versions of CRF-based\ntechniques and BiLSTM-based neural network with pre-trained GLoVe and ELMo\nembedding) and masking algorithm (e.g. shift dates, replace names/locations,\ntotally redact entity).\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 08:53:00 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 20:09:00 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Milosevic", "Nikola", ""], ["Kalappa", "Gangamma", ""], ["Dadafarin", "Hesam", ""], ["Azimaee", "Mahmoud", ""], ["Nenadic", "Goran", ""]]}, {"id": "2005.11723", "submitter": "Nikos Voskarides", "authors": "Nikos Voskarides, Dan Li, Pengjie Ren, Evangelos Kanoulas, Maarten de\n  Rijke", "title": "Query Resolution for Conversational Search with Limited Supervision", "comments": "SIGIR 2020 full conference paper", "journal-ref": null, "doi": "10.1145/3397271.3401130", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we focus on multi-turn passage retrieval as a crucial component\nof conversational search. One of the key challenges in multi-turn passage\nretrieval comes from the fact that the current turn query is often\nunderspecified due to zero anaphora, topic change, or topic return. Context\nfrom the conversational history can be used to arrive at a better expression of\nthe current turn query, defined as the task of query resolution. In this paper,\nwe model the query resolution task as a binary term classification problem: for\neach term appearing in the previous turns of the conversation decide whether to\nadd it to the current turn query or not. We propose QuReTeC (Query Resolution\nby Term Classification), a neural query resolution model based on bidirectional\ntransformers. We propose a distant supervision method to automatically generate\ntraining data by using query-passage relevance labels. Such labels are often\nreadily available in a collection either as human annotations or inferred from\nuser interactions. We show that QuReTeC outperforms state-of-the-art models,\nand furthermore, that our distant supervision method can be used to\nsubstantially reduce the amount of human-curated data required to train\nQuReTeC. We incorporate QuReTeC in a multi-turn, multi-stage passage retrieval\narchitecture and demonstrate its effectiveness on the TREC CAsT dataset.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 11:37:22 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Voskarides", "Nikos", ""], ["Li", "Dan", ""], ["Ren", "Pengjie", ""], ["Kanoulas", "Evangelos", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2005.11724", "submitter": "Yonghui Yang", "authors": "Le Wu, Yonghui Yang, Lei Chen, Defu Lian, Richang Hong and Meng Wang", "title": "Learning to Transfer Graph Embeddings for Inductive Graph based\n  Recommendation", "comments": "Accepted by SIGIR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing availability of videos, how to edit them and present the\nmost interesting parts to users, i.e., video highlight, has become an urgent\nneed with many broad applications. As users'visual preferences are subjective\nand vary from person to person, previous generalized video highlight extraction\nmodels fail to tailor to users' unique preferences. In this paper, we study the\nproblem of personalized video highlight recommendation with rich visual\ncontent. By dividing each video into non-overlapping segments, we formulate the\nproblem as a personalized segment recommendation task with many new segments in\nthe test stage. The key challenges of this problem lie in: the cold-start users\nwith limited video highlight records in the training data and new segments\nwithout any user ratings at the test stage. In this paper, we propose an\ninductive Graph based Transfer learning framework for personalized video\nhighlight Recommendation (TransGRec). TransGRec is composed of two parts: a\ngraph neural network followed by an item embedding transfer network.\nSpecifically, the graph neural network part exploits the higher-order proximity\nbetween users and segments to alleviate the user cold-start problem. The\ntransfer network is designed to approximate the learned item embeddings from\ngraph neural networks by taking each item's visual content as input, in order\nto tackle the new segment problem in the test phase. We design two detailed\nimplementations of the transfer learning optimization function, and we show how\nthe two parts of TransGRec can be efficiently optimized with different transfer\nlearning optimization functions. Extensive experimental results on a real-world\ndataset clearly show the effectiveness of our proposed model.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 11:37:48 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Wu", "Le", ""], ["Yang", "Yonghui", ""], ["Chen", "Lei", ""], ["Lian", "Defu", ""], ["Hong", "Richang", ""], ["Wang", "Meng", ""]]}, {"id": "2005.11757", "submitter": "Zhensu Sun", "authors": "Zhensu Sun, Yan Liu, Ziming Cheng, Chen Yang, Pengyu Che", "title": "Req2Lib: A Semantic Neural Model for Software Library Recommendation", "comments": "5 pages", "journal-ref": "2020 IEEE 27th International Conference on Software Analysis,\n  Evolution and Reengineering (SANER)", "doi": "10.1109/SANER48275.2020.9054865", "report-no": null, "categories": "cs.SE cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Third-party libraries are crucial to the development of software projects. To\nget suitable libraries, developers need to search through millions of libraries\nby filtering, evaluating, and comparing. The vast number of libraries places a\nbarrier for programmers to locate appropriate ones. To help developers,\nresearchers have proposed automated approaches to recommend libraries based on\nlibrary usage pattern. However, these prior studies can not sufficiently match\nuser requirements and suffer from cold-start problem. In this work, we would\nlike to make recommendations based on requirement descriptions to avoid these\nproblems. To this end, we propose a novel neural approach called Req2Lib which\nrecommends libraries given descriptions of the project requirement. We use a\nSequence-to-Sequence model to learn the library linked-usage information and\nsemantic information of requirement descriptions in natural language. Besides,\nwe apply a domain-specific pre-trained word2vec model for word embedding, which\nis trained over textual corpus from Stack Overflow posts. In the experiment, we\ntrain and evaluate the model with data from 5,625 java projects. Our\npreliminary evaluation demonstrates that Req2Lib can recommend libraries\naccurately.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 14:37:07 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Sun", "Zhensu", ""], ["Liu", "Yan", ""], ["Cheng", "Ziming", ""], ["Yang", "Chen", ""], ["Che", "Pengyu", ""]]}, {"id": "2005.11888", "submitter": "Dongjun Wei", "authors": "Dongjun Wei and Yaxin Liu and Fuqing Zhu and Liangjun Zang and Wei\n  Zhou and Yijun Lu and Songlin Hu", "title": "AutoSUM: Automating Feature Extraction and Multi-user Preference\n  Simulation for Entity Summarization", "comments": "11 pages, accepted in PAKDD'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Withthegrowthofknowledgegraphs, entity descriptions are becoming extremely\nlengthy. Entity summarization task, aiming to generate diverse, comprehensive,\nand representative summaries for entities, has received increasing interest\nrecently. In most previous methods, features are usually extracted by the\nhandcrafted templates. Then the feature selection and multi-user preference\nsimulation take place, depending too much on human expertise. In this paper, a\nnovel integration method called AutoSUM is proposed for automatic feature\nextraction and multi-user preference simulation to overcome the drawbacks of\nprevious methods. There are two modules in AutoSUM: extractor and simulator.\nThe extractor module operates automatic feature extraction based on a BiLSTM\nwith a combined input representation including word embeddings and graph\nembeddings. Meanwhile, the simulator module automates multi-user preference\nsimulation based on a well-designed two-phase attention mechanism (i.e.,\nentity-phase attention and user-phase attention). Experimental results\ndemonstrate that AutoSUM produces state-of-the-art performance on two widely\nused datasets (i.e., DBpedia and LinkedMDB) in both F-measure and MAP.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 02:20:18 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Wei", "Dongjun", ""], ["Liu", "Yaxin", ""], ["Zhu", "Fuqing", ""], ["Zang", "Liangjun", ""], ["Zhou", "Wei", ""], ["Lu", "Yijun", ""], ["Hu", "Songlin", ""]]}, {"id": "2005.11938", "submitter": "Ali Vardasbi", "authors": "Ali Vardasbi, Maarten de Rijke, Ilya Markov", "title": "Cascade Model-based Propensity Estimation for Counterfactual Learning to\n  Rank", "comments": "4 pages, 2 figures, 43rd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '20)", "journal-ref": null, "doi": "10.1145/3397271.3401299", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unbiased CLTR requires click propensities to compensate for the difference\nbetween user clicks and true relevance of search results via IPS. Current\npropensity estimation methods assume that user click behavior follows the PBM\nand estimate click propensities based on this assumption. However, in reality,\nuser clicks often follow the CM, where users scan search results from top to\nbottom and where each next click depends on the previous one. In this cascade\nscenario, PBM-based estimates of propensities are not accurate, which, in turn,\nhurts CLTR performance. In this paper, we propose a propensity estimation\nmethod for the cascade scenario, called CM-IPS. We show that CM-IPS keeps CLTR\nperformance close to the full-information performance in case the user clicks\nfollow the CM, while PBM-based CLTR has a significant gap towards the\nfull-information. The opposite is true if the user clicks follow PBM instead of\nthe CM. Finally, we suggest a way to select between CM- and PBM-based\npropensity estimation methods based on historical user clicks.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 06:17:49 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Vardasbi", "Ali", ""], ["de Rijke", "Maarten", ""], ["Markov", "Ilya", ""]]}, {"id": "2005.11992", "submitter": "Dongjun Wei", "authors": "Dongjun Wei and Shiyuan Gao and Yaxin Liu and Zhibing Liu and Longtao\n  Hang", "title": "MPSUM: Entity Summarization with Predicate-based Matching", "comments": "6 pages, accepted in EYRE@CIKM'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of Semantic Web, entity summarization has become an\nemerging task to generate concrete summaries for real world entities. To solve\nthis problem, we propose an approach named MPSUM that extends a probabilistic\ntopic model by integrating the idea of predicate-uniqueness and\nobject-importance for ranking triples. The approach aims at generating brief\nbut representative summaries for entities. We compare our approach with the\nstate-of-the-art methods using DBpedia and LinkedMDB datasets.The experimental\nresults show that our work improves the quality of entity summarization.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 09:22:32 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Wei", "Dongjun", ""], ["Gao", "Shiyuan", ""], ["Liu", "Yaxin", ""], ["Liu", "Zhibing", ""], ["Hang", "Longtao", ""]]}, {"id": "2005.12002", "submitter": "Yufei Feng", "authors": "Yufei Feng, Binbin Hu, Fuyu Lv, Qingwen Liu, Zhiqiang Zhang, Wenwu Ou", "title": "ATBRG: Adaptive Target-Behavior Relational Graph Network for Effective\n  Recommendation", "comments": "Accepted by SIGIR 2020, full paper with 10 pages and 5 figures", "journal-ref": null, "doi": "10.1145/3397271.3401428", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender system (RS) devotes to predicting user preference to a given item\nand has been widely deployed in most web-scale applications. Recently,\nknowledge graph (KG) attracts much attention in RS due to its abundant\nconnective information. Existing methods either explore independent meta-paths\nfor user-item pairs over KG, or employ graph neural network (GNN) on whole KG\nto produce representations for users and items separately. Despite\neffectiveness, the former type of methods fails to fully capture structural\ninformation implied in KG, while the latter ignores the mutual effect between\ntarget user and item during the embedding propagation. In this work, we propose\na new framework named Adaptive Target-Behavior Relational Graph network (ATBRG\nfor short) to effectively capture structural relations of target user-item\npairs over KG. Specifically, to associate the given target item with user\nbehaviors over KG, we propose the graph connect and graph prune techniques to\nconstruct adaptive target-behavior relational graph. To fully distill\nstructural information from the sub-graph connected by rich relations in an\nend-to-end fashion, we elaborate on the model design of ATBRG, equipped with\nrelation-aware extractor layer and representation activation layer. We perform\nextensive experiments on both industrial and benchmark datasets. Empirical\nresults show that ATBRG consistently and significantly outperforms\nstate-of-the-art methods. Moreover, ATBRG has also achieved a performance\nimprovement of 5.1% on CTR metric after successful deployment in one popular\nrecommendation scenario of Taobao APP.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 09:34:55 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 03:13:07 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 11:21:40 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Feng", "Yufei", ""], ["Hu", "Binbin", ""], ["Lv", "Fuyu", ""], ["Liu", "Qingwen", ""], ["Zhang", "Zhiqiang", ""], ["Ou", "Wenwu", ""]]}, {"id": "2005.12021", "submitter": "Yonghui Yang", "authors": "Le Wu, Yonghui Yang, Kun Zhang, Richang Hong, Yanjie Fu and Meng Wang", "title": "Joint Item Recommendation and Attribute Inference: An Adaptive Graph\n  Convolutional Network Approach", "comments": "Accepted by SIGIR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many recommender systems, users and items are associated with attributes,\nand users show preferences to items. The attribute information describes\nusers'(items') characteristics and has a wide range of applications, such as\nuser profiling, item annotation, and feature-enhanced recommendation. As\nannotating user (item) attributes is a labor intensive task, the attribute\nvalues are often incomplete with many missing attribute values. Therefore, item\nrecommendation and attribute inference have become two main tasks in these\nplatforms. Researchers have long converged that user (item) attributes and the\npreference behavior are highly correlated. Some researchers proposed to\nleverage one kind of data for the remaining task, and showed to improve\nperformance. Nevertheless, these models either neglected the incompleteness of\nuser (item) attributes or regarded the correlation of the two tasks with simple\nmodels, leading to suboptimal performance of these two tasks. To this end, in\nthis paper, we define these two tasks in an attributed user-item bipartite\ngraph, and propose an Adaptive Graph Convolutional Network (AGCN) approach for\njoint item recommendation and attribute inference. The key idea of AGCN is to\niteratively perform two parts: 1) Learning graph embedding parameters with\npreviously learned approximated attribute values to facilitate two tasks; 2)\nSending the approximated updated attribute values back to the attributed graph\nfor better graph embedding learning. Therefore, AGCN could adaptively adjust\nthe graph embedding learning parameters by incorporating both the given\nattributes and the estimated attribute values, in order to provide weakly\nsupervised information to refine the two tasks. Extensive experimental results\non three real-world datasets clearly show the effectiveness of the proposed\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 10:50:01 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Wu", "Le", ""], ["Yang", "Yonghui", ""], ["Zhang", "Kun", ""], ["Hong", "Richang", ""], ["Fu", "Yanjie", ""], ["Wang", "Meng", ""]]}, {"id": "2005.12206", "submitter": "Qingpeng Cai", "authors": "Jianxiong Wei, Anxiang Zeng, Yueqiu Wu, Peng Guo, Qingsong Hua,\n  Qingpeng Cai", "title": "Generator and Critic: A Deep Reinforcement Learning Approach for Slate\n  Re-ranking in E-commerce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The slate re-ranking problem considers the mutual influences between items to\nimprove user satisfaction in e-commerce, compared with the point-wise ranking.\nPrevious works either directly rank items by an end to end model, or rank items\nby a score function that trades-off the point-wise score and the diversity\nbetween items. However, there are two main existing challenges that are not\nwell studied: (1) the evaluation of the slate is hard due to the complex mutual\ninfluences between items of one slate; (2) even given the optimal evaluation,\nsearching the optimal slate is challenging as the action space is exponentially\nlarge. In this paper, we present a novel Generator and Critic slate re-ranking\napproach, where the Critic evaluates the slate and the Generator ranks the\nitems by the reinforcement learning approach. We propose a Full Slate Critic\n(FSC) model that considers the real impressed items and avoids the impressed\nbias of existing models. For the Generator, to tackle the problem of large\naction space, we propose a new exploration reinforcement learning algorithm,\ncalled PPO-Exploration. Experimental results show that the FSC model\nsignificantly outperforms the state of the art slate evaluation methods, and\nthe PPO-Exploration algorithm outperforms the existing reinforcement learning\nmethods substantially. The Generator and Critic approach improves both the\nslate efficiency(4% gmv and 5% number of orders) and diversity in live\nexperiments on one of the largest e-commerce websites in the world.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 16:24:01 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Wei", "Jianxiong", ""], ["Zeng", "Anxiang", ""], ["Wu", "Yueqiu", ""], ["Guo", "Peng", ""], ["Hua", "Qingsong", ""], ["Cai", "Qingpeng", ""]]}, {"id": "2005.12210", "submitter": "Noveen Sachdeva", "authors": "Noveen Sachdeva, Julian McAuley", "title": "How Useful are Reviews for Recommendation? A Critical Review and\n  Potential Improvements", "comments": "4 pages, 3 figures. Accepted for publication at SIGIR '20", "journal-ref": null, "doi": "10.1145/3397271.3401281", "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a growing body of work that seeks to improve recommender\nsystems through the use of review text. Generally, these papers argue that\nsince reviews 'explain' users' opinions, they ought to be useful to infer the\nunderlying dimensions that predict ratings or purchases. Schemes to incorporate\nreviews range from simple regularizers to neural network approaches. Our\ninitial findings reveal several discrepancies in reported results, partly due\nto (e.g.) copying results across papers despite changes in experimental\nsettings or data pre-processing. First, we attempt a comprehensive analysis to\nresolve these ambiguities. Further investigation calls for discussion on a much\nlarger problem about the \"importance\" of user reviews for recommendation.\nThrough a wide range of experiments, we observe several cases where\nstate-of-the-art methods fail to outperform existing baselines, especially as\nwe deviate from a few narrowly-defined settings where reviews are useful. We\nconclude by providing hypotheses for our observations, that seek to\ncharacterize under what conditions reviews are likely to be helpful. Through\nthis work, we aim to evaluate the direction in which the field is progressing\nand encourage robust empirical evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 16:30:05 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Sachdeva", "Noveen", ""], ["McAuley", "Julian", ""]]}, {"id": "2005.12340", "submitter": "Svitlana Vakulenko", "authors": "Svitlana Vakulenko, Evangelos Kanoulas, Maarten de Rijke", "title": "An Analysis of Mixed Initiative and Collaboration in Information-Seeking\n  Dialogues", "comments": "SIGIR 2020 short conference paper", "journal-ref": null, "doi": "10.1145/3397271.3401297", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to engage in mixed-initiative interaction is one of the core\nrequirements for a conversational search system. How to achieve this is poorly\nunderstood. We propose a set of unsupervised metrics, termed ConversationShape,\nthat highlights the role each of the conversation participants plays by\ncomparing the distribution of vocabulary and utterance types. Using\nConversationShape as a lens, we take a closer look at several conversational\nsearch datasets and compare them with other dialogue datasets to better\nunderstand the types of dialogue interaction they represent, either driven by\nthe information seeker or the assistant. We discover that deviations from the\nConversationShape of a human-human dialogue of the same type is predictive of\nthe quality of a human-machine dialogue.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 18:48:38 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["Kanoulas", "Evangelos", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2005.12371", "submitter": "Ludovico Boratto", "authors": "Guilherme Ramos and Ludovico Boratto", "title": "Reputation (In)dependence in Ranking Systems: Demographics Influence\n  Over Output Disparities", "comments": "4 pages", "journal-ref": "Proceedings of the 43rd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval, July 25--30, 2020, Virtual\n  Event, China", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent literature on ranking systems (RS) has considered users' exposure when\nthey are the object of the ranking. Although items are the object of\nreputation-based RS, users have a central role also in this class of\nalgorithms. Indeed, when ranking the items, user preferences are weighted by\nhow relevant this user is in the platform (i.e., their reputation). In this\npaper, we formulate the concept of disparate reputation (DR) and study if users\ncharacterized by sensitive attributes systematically get a lower reputation,\nleading to a final ranking that reflects less their preferences. We consider\ntwo demographic attributes, i.e., gender and age, and show that DR\nsystematically occurs. Then, we propose mitigation, which ensures that\nreputation is independent of the users' sensitive attributes. Experiments on\nreal-world data show that our approach can overcome DR and also improve ranking\neffectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 20:07:42 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ramos", "Guilherme", ""], ["Boratto", "Ludovico", ""]]}, {"id": "2005.12423", "submitter": "Srijan Kumar", "authors": "Caleb Ziems, Bing He, Sandeep Soni, Srijan Kumar", "title": "Racism is a Virus: Anti-Asian Hate and Counterhate in Social Media\n  during the COVID-19 Crisis", "comments": "The COVID-HATE dataset, classifier, and demo are available at\n  http://claws.cc.gatech.edu/covid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of COVID-19 has sparked racism, hate, and xenophobia in social\nmedia targeted at Chinese and broader Asian communities. However, little is\nknown about how racial hate spreads during a pandemic and the role of\ncounterhate speech in mitigating the spread. Here we study the evolution and\nspread of anti-Asian hate speech through the lens of Twitter. We create\nCOVID-HATE, the largest dataset of anti-Asian hate and counterhate spanning\nthree months, containing over 30 million tweets, and a social network with over\n87 million nodes. By creating a novel hand-labeled dataset of 2,400 tweets, we\ntrain a text classifier to identify hate and counterhate tweets that achieves\nan average AUROC of 0.852. We identify 891,204 hate and 200,198 counterhate\ntweets in COVID-HATE. Using this data to conduct longitudinal analysis, we find\nthat while hateful users are less engaged in the COVID-19 discussions prior to\ntheir first anti-Asian tweet, they become more vocal and engaged afterwards\ncompared to counterhate users. We find that bots comprise 10.4% of hateful\nusers and are more vocal and hateful compared to non-bot users. Comparing bot\naccounts, we show that hateful bots are more successful in attracting followers\ncompared to counterhate bots. Analysis of the social network reveals that\nhateful and counterhate users interact and engage extensively with one another,\ninstead of living in isolated polarized communities. Furthermore, we find that\nhate is contagious and nodes are highly likely to become hateful after being\nexposed to hateful content. Importantly, our analysis reveals that counterhate\nmessages can discourage users from turning hateful in the first place. Overall,\nthis work presents a comprehensive overview of anti-Asian hate and counterhate\ncontent during a pandemic. The COVID-HATE dataset is available at\nhttp://claws.cc.gatech.edu/covid.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 21:58:09 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Ziems", "Caleb", ""], ["He", "Bing", ""], ["Soni", "Sandeep", ""], ["Kumar", "Srijan", ""]]}, {"id": "2005.12439", "submitter": "Haitian Zheng", "authors": "Haitian Zheng, Kefei Wu, Jong-Hwi Park, Wei Zhu, Jiebo Luo", "title": "Personalized Fashion Recommendation from Personal Social Media Data: An\n  Item-to-Set Metric Learning Approach", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of online shopping for fashion products, accurate fashion\nrecommendation has become a critical problem. Meanwhile, social networks\nprovide an open and new data source for personalized fashion analysis. In this\nwork, we study the problem of personalized fashion recommendation from social\nmedia data, i.e. recommending new outfits to social media users that fit their\nfashion preferences. To this end, we present an item-to-set metric learning\nframework that learns to compute the similarity between a set of historical\nfashion items of a user to a new fashion item. To extract features from\nmulti-modal street-view fashion items, we propose an embedding module that\nperforms multi-modality feature extraction and cross-modality gated fusion. To\nvalidate the effectiveness of our approach, we collect a real-world social\nmedia dataset. Extensive experiments on the collected dataset show the superior\nperformance of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 23:24:24 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Zheng", "Haitian", ""], ["Wu", "Kefei", ""], ["Park", "Jong-Hwi", ""], ["Zhu", "Wei", ""], ["Luo", "Jiebo", ""]]}, {"id": "2005.12516", "submitter": "Chang-You Tai", "authors": "Chang-You Tai, Meng-Ru Wu, Yun-Wei Chu, Shao-Yu Chu, Lun-Wei Ku", "title": "MVIN: Learning Multiview Items for Recommendation", "comments": null, "journal-ref": null, "doi": "10.1145/3397271.3401126", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have begun to utilize heterogeneous knowledge graphs (KGs) as\nauxiliary information in recommendation systems to mitigate the cold start and\nsparsity issues. However, utilizing a graph neural network (GNN) to capture\ninformation in KG and further apply in RS is still problematic as it is unable\nto see each item's properties from multiple perspectives. To address these\nissues, we propose the multi-view item network (MVIN), a GNN-based\nrecommendation model which provides superior recommendations by describing\nitems from a unique mixed view from user and entity angles. MVIN learns item\nrepresentations from both the user view and the entity view. From the user\nview, user-oriented modules score and aggregate features to make\nrecommendations from a personalized perspective constructed according to KG\nentities which incorporates user click information. From the entity view, the\nmixing layer contrasts layer-wise GCN information to further obtain\ncomprehensive features from internal entity-entity interactions in the KG. We\nevaluate MVIN on three real-world datasets: MovieLens-1M (ML-1M), LFM-1b 2015\n(LFM-1b), and Amazon-Book (AZ-book). Results show that MVIN significantly\noutperforms state-of-the-art methods on these three datasets. In addition, from\nuser-view cases, we find that MVIN indeed captures entities that attract users.\nFigures further illustrate that mixing layers in a heterogeneous KG plays a\nvital role in neighborhood information aggregation.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 05:19:27 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Tai", "Chang-You", ""], ["Wu", "Meng-Ru", ""], ["Chu", "Yun-Wei", ""], ["Chu", "Shao-Yu", ""], ["Ku", "Lun-Wei", ""]]}, {"id": "2005.12566", "submitter": "Xingchen Li", "authors": "Xingchen Li, Xiang Wang, Xiangnan He, Long Chen, Jun Xiao, Tat-Seng\n  Chua", "title": "Hierarchical Fashion Graph Network for Personalized Outfit\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion outfit recommendation has attracted increasing attentions from online\nshopping services and fashion communities.Distinct from other scenarios (e.g.,\nsocial networking or content sharing) which recommend a single item (e.g., a\nfriend or picture) to a user, outfit recommendation predicts user preference on\na set of well-matched fashion items.Hence, performing high-quality personalized\noutfit recommendation should satisfy two requirements -- 1) the nice\ncompatibility of fashion items and 2) the consistence with user preference.\nHowever, present works focus mainly on one of the requirements and only\nconsider either user-outfit or outfit-item relationships, thereby easily\nleading to suboptimal representations and limiting the performance. In this\nwork, we unify two tasks, fashion compatibility modeling and personalized\noutfit recommendation. Towards this end, we develop a new framework,\nHierarchical Fashion Graph Network(HFGN), to model relationships among users,\nitems, and outfits simultaneously. In particular, we construct a hierarchical\nstructure upon user-outfit interactions and outfit-item mappings. We then get\ninspirations from recent graph neural networks, and employ the embedding\npropagation on such hierarchical graph, so as to aggregate item information\ninto an outfit representation, and then refine a user's representation via\nhis/her historical outfits. Furthermore, we jointly train these two tasks to\noptimize these representations. To demonstrate the effectiveness of HFGN, we\nconduct extensive experiments on a benchmark dataset, and HFGN achieves\nsignificant improvements over the state-of-the-art compatibility matching\nmodels like NGNN and outfit recommenders like FHN.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 08:23:35 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Li", "Xingchen", ""], ["Wang", "Xiang", ""], ["He", "Xiangnan", ""], ["Chen", "Long", ""], ["Xiao", "Jun", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2005.12668", "submitter": "Tom Hope", "authors": "Tom Hope, Jason Portenoy, Kishore Vasan, Jonathan Borchardt, Eric\n  Horvitz, Daniel S. Weld, Marti A. Hearst, Jevin West", "title": "SciSight: Combining faceted navigation and research group detection for\n  COVID-19 exploratory scientific search", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has sparked unprecedented mobilization of scientists,\ngenerating a deluge of papers that makes it hard for researchers to keep track\nand explore new directions. Search engines are designed for targeted queries,\nnot for discovery of connections across a corpus. In this paper, we present\nSciSight, a system for exploratory search of COVID-19 research integrating two\nkey capabilities: first, exploring associations between biomedical facets\nautomatically extracted from papers (e.g., genes, drugs, diseases, patient\noutcomes); second, combining textual and network information to search and\nvisualize groups of researchers and their ties. SciSight has so far served over\n$15K$ users with over $42K$ page views and $13\\%$ returns.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 08:56:21 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 03:05:32 GMT"}, {"version": "v3", "created": "Sun, 20 Sep 2020 15:43:20 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Hope", "Tom", ""], ["Portenoy", "Jason", ""], ["Vasan", "Kishore", ""], ["Borchardt", "Jonathan", ""], ["Horvitz", "Eric", ""], ["Weld", "Daniel S.", ""], ["Hearst", "Marti A.", ""], ["West", "Jevin", ""]]}, {"id": "2005.12739", "submitter": "ByungSoo Ko", "authors": "Yang-Ho Ji, HeeJae Jun, Insik Kim, Jongtack Kim, Youngjoon Kim,\n  Byungsoo Ko, Hyong-Keun Kook, Jingeun Lee, Sangwon Lee, Sanghyuk Park", "title": "An Effective Pipeline for a Real-world Clothes Retrieval System", "comments": "2nd place solution on DeepFashion2 clothes retrieval challenge in\n  CVPR2020 workshop (CVFAD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an effective pipeline for clothes retrieval system\nwhich has sturdiness on large-scale real-world fashion data. Our proposed\nmethod consists of three components: detection, retrieval, and post-processing.\nWe firstly conduct a detection task for precise retrieval on target clothes,\nthen retrieve the corresponding items with the metric learning-based model. To\nimprove the retrieval robustness against noise and misleading bounding boxes,\nwe apply post-processing methods such as weighted boxes fusion and feature\nconcatenation. With the proposed methodology, we achieved 2nd place in the\nDeepFashion2 Clothes Retrieval 2020 challenge.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 14:08:49 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Ji", "Yang-Ho", ""], ["Jun", "HeeJae", ""], ["Kim", "Insik", ""], ["Kim", "Jongtack", ""], ["Kim", "Youngjoon", ""], ["Ko", "Byungsoo", ""], ["Kook", "Hyong-Keun", ""], ["Lee", "Jingeun", ""], ["Lee", "Sangwon", ""], ["Park", "Sanghyuk", ""]]}, {"id": "2005.12781", "submitter": "Bingqing Yu", "authors": "Jacopo Tagliabue, Bingqing Yu, Marie Beaulieu", "title": "How to Grow a (Product) Tree: Personalized Category Suggestions for\n  eCommerce Type-Ahead", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an attempt to balance precision and recall in the search page, leading\ndigital shops have been effectively nudging users into select category facets\nas early as in the type-ahead suggestions. In this work, we present\nSessionPath, a novel neural network model that improves facet suggestions on\ntwo counts: first, the model is able to leverage session embeddings to provide\nscalable personalization; second, SessionPath predicts facets by explicitly\nproducing a probability distribution at each node in the taxonomy path. We\nbenchmark SessionPath on two partnering shops against count-based and neural\nmodels, and show how business requirements and model behavior can be combined\nin a principled way.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 15:03:16 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Tagliabue", "Jacopo", ""], ["Yu", "Bingqing", ""], ["Beaulieu", "Marie", ""]]}, {"id": "2005.12816", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Manos Tsagkias, Ernest Pusateri, Ilya Oparin", "title": "Predicting Entity Popularity to Improve Spoken Entity Recognition by\n  Virtual Assistants", "comments": "SIGIR '20. The 43rd International ACM SIGIR Conference on Research &\n  Development in Information Retrieval", "journal-ref": null, "doi": "10.1145/3397271.3401298", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on improving the effectiveness of a Virtual Assistant (VA) in\nrecognizing emerging entities in spoken queries. We introduce a method that\nuses historical user interactions to forecast which entities will gain in\npopularity and become trending, and it subsequently integrates the predictions\nwithin the Automated Speech Recognition (ASR) component of the VA. Experiments\nshow that our proposed approach results in a 20% relative reduction in errors\non emerging entity name utterances without degrading the overall recognition\nquality of the system.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 15:47:42 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Van Gysel", "Christophe", ""], ["Tsagkias", "Manos", ""], ["Pusateri", "Ernest", ""], ["Oparin", "Ilya", ""]]}, {"id": "2005.12840", "submitter": "Michelangelo Misuraca", "authors": "Michelangelo Misuraca, Alessia Forciniti, Germana Scepi, Maria Spano", "title": "Sentiment Analysis for Education with R: packages, methods and practical\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sentiment Analysis (SA) refers to a family of techniques at the crossroads of\nstatistics, natural language processing, and computational linguistics. The\nprimary goal is to detect the semantic orientation of individual opinions and\ncomments expressed in written texts. There are several practical applications\nof SA in several domains. In an educational context, the use of this approach\nallows processing students' feedback, aiming at monitoring the teaching\neffectiveness of instructors and enhancing the learning experience. This paper\nwants to review the different R packages that can be used to carry on SA,\ncomparing the implemented methods, discussing their characteristics, and\nshowing how they perform by considering a simple example.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 14:10:43 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Misuraca", "Michelangelo", ""], ["Forciniti", "Alessia", ""], ["Scepi", "Germana", ""], ["Spano", "Maria", ""]]}, {"id": "2005.12964", "submitter": "Jianxin Ma", "authors": "Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingren Zhou, Hongxia Yang", "title": "Contrastive Learning for Debiased Candidate Generation in Large-Scale\n  Recommender Systems", "comments": "Accepted by the 27th ACM SIGKDD Conference on Knowledge Discovery and\n  Data Mining (KDD 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep candidate generation (DCG) that narrows down the collection of relevant\nitems from billions to hundreds via representation learning has become\nprevalent in industrial recommender systems. Standard approaches approximate\nmaximum likelihood estimation (MLE) through sampling for better scalability and\naddress the problem of DCG in a way similar to language modeling. However, live\nrecommender systems face severe exposure bias and have a vocabulary several\norders of magnitude larger than that of natural language, implying that MLE\nwill preserve and even exacerbate the exposure bias in the long run in order to\nfaithfully fit the observed samples. In this paper, we theoretically prove that\na popular choice of contrastive loss is equivalent to reducing the exposure\nbias via inverse propensity weighting, which provides a new perspective for\nunderstanding the effectiveness of contrastive learning. Based on the\ntheoretical discovery, we design CLRec, a contrastive learning method to\nimprove DCG in terms of fairness, effectiveness and efficiency in recommender\nsystems with extremely large candidate size. We further improve upon CLRec and\npropose Multi-CLRec, for accurate multi-intention aware bias reduction. Our\nmethods have been successfully deployed in Taobao, where at least four-month\nonline A/B tests and offline analyses demonstrate its substantial improvements,\nincluding a dramatic reduction in the Matthew effect.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 08:15:23 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 17:46:41 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 09:21:25 GMT"}, {"version": "v4", "created": "Fri, 5 Jun 2020 17:15:04 GMT"}, {"version": "v5", "created": "Wed, 10 Jun 2020 14:32:52 GMT"}, {"version": "v6", "created": "Thu, 11 Jun 2020 12:29:48 GMT"}, {"version": "v7", "created": "Thu, 18 Feb 2021 07:41:38 GMT"}, {"version": "v8", "created": "Wed, 19 May 2021 08:14:17 GMT"}, {"version": "v9", "created": "Fri, 4 Jun 2021 16:34:46 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Zhou", "Chang", ""], ["Ma", "Jianxin", ""], ["Zhang", "Jianwei", ""], ["Zhou", "Jingren", ""], ["Yang", "Hongxia", ""]]}, {"id": "2005.12966", "submitter": "Zhiqiang Ma", "authors": "Zhiqiang Ma, Steven Pomerville, Mingyang Di, Armineh Nourbakhsh", "title": "SPot: A tool for identifying operating segments in financial tables", "comments": "This manuscript has been reviewed and accepted by SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present SPot, an automated tool for detecting operating\nsegments and their related performance indicators from earnings reports. Due to\ntheir company-specific nature, operating segments cannot be detected using\ntaxonomy-based approaches. Instead, we train a Bidirectional RNN classifier\nthat can distinguish between common metrics such as \"revenue\" and\ncompany-specific metrics that are likely to be operating segments, such as\n\"iPhone\" or \"cloud services\". SPot surfaces the results in an interactive web\ninterface that allows users to trace and adjust performance metrics for each\noperating segment. This facilitates credit monitoring, enables them to perform\ncompetitive benchmarking more effectively, and can be used for trend analysis\nat company and sector levels.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 15:14:53 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Ma", "Zhiqiang", ""], ["Pomerville", "Steven", ""], ["Di", "Mingyang", ""], ["Nourbakhsh", "Armineh", ""]]}, {"id": "2005.12971", "submitter": "Chuan-Ju Wang", "authors": "Chuan-Ju Wang, Yu-Neng Chuang, Chih-Ming Chen, and Ming-Feng Tsai", "title": "Skewness Ranking Optimization for Personalized Recommendation", "comments": "Accepted by UAI'20. The first two authors contributed equally to this\n  work; author order was determined by seniority", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel optimization criterion that leverages\nfeatures of the skew normal distribution to better model the problem of\npersonalized recommendation. Specifically, the developed criterion borrows the\nconcept and the flexibility of the skew normal distribution, based on which\nthree hyperparameters are attached to the optimization criterion. Furthermore,\nfrom a theoretical point of view, we not only establish the relation between\nthe maximization of the proposed criterion and the shape parameter in the skew\nnormal distribution, but also provide the analogies and asymptotic analysis of\nthe proposed criterion to maximization of the area under the ROC curve.\nExperimental results conducted on a range of large-scale real-world datasets\nshow that our model significantly outperforms the state of the art and yields\nconsistently best performance on all tested datasets.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 00:59:22 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Wang", "Chuan-Ju", ""], ["Chuang", "Yu-Neng", ""], ["Chen", "Chih-Ming", ""], ["Tsai", "Ming-Feng", ""]]}, {"id": "2005.12974", "submitter": "Nasim Sonboli", "authors": "Nasim Sonboli, Farzad Eskandanian, Robin Burke, Weiwen Liu, Bamshad\n  Mobasher", "title": "Opportunistic Multi-aspect Fairness through Personalized Re-ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As recommender systems have become more widespread and moved into areas with\ngreater social impact, such as employment and housing, researchers have begun\nto seek ways to ensure fairness in the results that such systems produce. This\nwork has primarily focused on developing recommendation approaches in which\nfairness metrics are jointly optimized along with recommendation accuracy.\nHowever, the previous work had largely ignored how individual preferences may\nlimit the ability of an algorithm to produce fair recommendations. Furthermore,\nwith few exceptions, researchers have only considered scenarios in which\nfairness is measured relative to a single sensitive feature or attribute (such\nas race or gender). In this paper, we present a re-ranking approach to\nfairness-aware recommendation that learns individual preferences across\nmultiple fairness dimensions and uses them to enhance provider fairness in\nrecommendation results. Specifically, we show that our opportunistic and\nmetric-agnostic approach achieves a better trade-off between accuracy and\nfairness than prior re-ranking approaches and does so across multiple fairness\ndimensions.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 04:25:20 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Sonboli", "Nasim", ""], ["Eskandanian", "Farzad", ""], ["Burke", "Robin", ""], ["Liu", "Weiwen", ""], ["Mobasher", "Bamshad", ""]]}, {"id": "2005.12977", "submitter": "Laure Pr\\'etet", "authors": "Laure Pr\\'etet, Ga\\\"el Richard, Geoffroy Peeters", "title": "Learning to rank music tracks using triplet loss", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP40776.2020.9053135", "report-no": null, "categories": "cs.IR cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most music streaming services rely on automatic recommendation algorithms to\nexploit their large music catalogs. These algorithms aim at retrieving a ranked\nlist of music tracks based on their similarity with a target music track. In\nthis work, we propose a method for direct recommendation based on the audio\ncontent without explicitly tagging the music tracks. To that aim, we propose\nseveral strategies to perform triplet mining from ranked lists. We train a\nConvolutional Neural Network to learn the similarity via triplet loss. These\ndifferent strategies are compared and validated on a large-scale experiment\nagainst an auto-tagging based approach. The results obtained highlight the\nefficiency of our system, especially when associated with an Auto-pooling\nlayer.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 08:20:54 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Pr\u00e9tet", "Laure", ""], ["Richard", "Ga\u00ebl", ""], ["Peeters", "Geoffroy", ""]]}, {"id": "2005.12978", "submitter": "Simra Shahid", "authors": "Simra Shahid, Tanmay Singh, Yash Sharma, Kapil Sharma", "title": "Devising Malware Characterstics using Transformers", "comments": "5 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With the increasing number of cybersecurity threats, it becomes more\ndifficult for researchers to skim through the security reports for malware\nanalysis. There is a need to be able to extract highly relevant sentences\nwithout having to read through the entire malware reports. In this paper, we\nare finding relevant malware behavior mentions from Advanced Persistent Threat\nReports. This main contribution is an opening attempt to Transformer the\napproach for malware behavior analysis.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 10:51:05 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Shahid", "Simra", ""], ["Singh", "Tanmay", ""], ["Sharma", "Yash", ""], ["Sharma", "Kapil", ""]]}, {"id": "2005.12979", "submitter": "Shijun Li", "authors": "Shijun Li, Wenqiang Lei, Qingyun Wu, Xiangnan He, Peng Jiang, Tat-Seng\n  Chua", "title": "Seamlessly Unifying Attributes and Items: Conversational Recommendation\n  for Cold-Start Users", "comments": "TOIS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static recommendation methods like collaborative filtering suffer from the\ninherent limitation of performing real-time personalization for cold-start\nusers. Online recommendation, e.g., multi-armed bandit approach, addresses this\nlimitation by interactively exploring user preference online and pursuing the\nexploration-exploitation (EE) trade-off. However, existing bandit-based methods\nmodel recommendation actions homogeneously. Specifically, they only consider\nthe items as the arms, being incapable of handling the item attributes, which\nnaturally provide interpretable information of user's current demands and can\neffectively filter out undesired items. In this work, we consider the\nconversational recommendation for cold-start users, where a system can both ask\nthe attributes from and recommend items to a user interactively. This important\nscenario was studied in a recent work. However, it employs a hand-crafted\nfunction to decide when to ask attributes or make recommendations. Such\nseparate modeling of attributes and items makes the effectiveness of the system\nhighly rely on the choice of the hand-crafted function, thus introducing\nfragility to the system. To address this limitation, we seamlessly unify\nattributes and items in the same arm space and achieve their EE trade-offs\nautomatically using the framework of Thompson Sampling. Our Conversational\nThompson Sampling (ConTS) model holistically solves all questions in\nconversational recommendation by choosing the arm with the maximal reward to\nplay. Extensive experiments on three benchmark datasets show that ConTS\noutperforms the state-of-the-art methods Conversational UCB (ConUCB) and\nEstimation-Action-Reflection model in both metrics of success rate and average\nnumber of conversation turns.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 08:56:37 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 07:51:45 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2020 10:27:32 GMT"}, {"version": "v4", "created": "Tue, 8 Jun 2021 04:13:52 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Li", "Shijun", ""], ["Lei", "Wenqiang", ""], ["Wu", "Qingyun", ""], ["He", "Xiangnan", ""], ["Jiang", "Peng", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2005.12981", "submitter": "Weinan Xu", "authors": "Weinan Xu, Hengxu He, Minshi Tan, Yunming Li, Jun Lang, Dongbai Guo", "title": "Deep Interest with Hierarchical Attention Network for Click-Through Rate\n  Prediction", "comments": "4 pages, SIGIR 2020 short paper accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Interest Network (DIN) is a state-of-the-art model which uses attention\nmechanism to capture user interests from historical behaviors. User interests\nintuitively follow a hierarchical pattern such that users generally show\ninterests from a higher-level then to a lower-level abstraction. Modeling such\nan interest hierarchy in an attention network can fundamentally improve the\nrepresentation of user behaviors. We, therefore, propose an improvement over\nDIN to model arbitrary interest hierarchy: Deep Interest with Hierarchical\nAttention Network (DHAN). In this model, a multi-dimensional hierarchical\nstructure is introduced on the first attention layer which attends to an\nindividual item, and the subsequent attention layers in the same dimension\nattend to higher-level hierarchy built on top of the lower corresponding\nlayers. To enable modeling of multiple dimensional hierarchies, an expanding\nmechanism is introduced to capture one to many hierarchies. This design enables\nDHAN to attend different importance to different hierarchical abstractions thus\ncan fully capture user interests at different dimensions (e.g. category, price,\nor brand).To validate our model, a simplified DHAN has applied to Click-Through\nRate (CTR) prediction and our experimental results on three public datasets\nwith two levels of the one-dimensional hierarchy only by category. It shows the\nsuperiority of DHAN with significant AUC uplift from 12% to 21% over DIN. DHAN\nis also compared with another state-of-the-art model Deep Interest Evolution\nNetwork (DIEN), which models temporal interest. The simplified DHAN also gets\nslight AUC uplift from 1.0% to 1.7% over DIEN. A potential future work can be a\ncombination of DHAN and DIEN to model both temporal and hierarchical interests.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 04:02:01 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Xu", "Weinan", ""], ["He", "Hengxu", ""], ["Tan", "Minshi", ""], ["Li", "Yunming", ""], ["Lang", "Jun", ""], ["Guo", "Dongbai", ""]]}, {"id": "2005.12982", "submitter": "Makbule Gulcin Ozsoy", "authors": "Makbule Gulcin Ozsoy", "title": "Utilizing FastText for Venue Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Venue recommendation systems model the past interactions (i.e., check-ins) of\nthe users and recommend venues. Traditional recommendation systems employ\ncollaborative filtering, content-based filtering or matrix factorization.\nRecently, vector space embedding and deep learning algorithms are also used for\nrecommendation. In this work, I propose a method for recommending top-k venues\nby utilizing the sequentiality feature of check-ins and a recent vector space\nembedding method, namely the FastText. Our proposed method; forms groups of\ncheck-ins, learns the vector space representations of the venues and utilizes\nthe learned embeddings to make venue recommendations. I measure the performance\nof the proposed method using a Foursquare check-in dataset.The results show\nthat the proposed method performs better than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 14:57:12 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Ozsoy", "Makbule Gulcin", ""]]}, {"id": "2005.12989", "submitter": "Gregory Goren", "authors": "Gregory Goren, Oren Kurland, Moshe Tennenholtz, Fiana Raiber", "title": "Ranking-Incentivized Quality Preserving Content Modification", "comments": "10 pages. 8 figures. 3 tables", "journal-ref": null, "doi": "10.1145/3397271.3401058", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web is a canonical example of a competitive retrieval setting where many\ndocuments' authors consistently modify their documents to promote them in\nrankings. We present an automatic method for quality-preserving modification of\ndocument content -- i.e., maintaining content quality -- so that the document\nis ranked higher for a query by a non-disclosed ranking function whose rankings\ncan be observed. The method replaces a passage in the document with some other\npassage. To select the two passages, we use a learning-to-rank approach with a\nbi-objective optimization criterion: rank promotion and content-quality\nmaintenance. We used the approach as a bot in content-based ranking\ncompetitions. Analysis of the competitions demonstrates the merits of our\napproach with respect to human content modifications in terms of rank\npromotion, content-quality maintenance and relevance.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 19:14:56 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 20:15:00 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Goren", "Gregory", ""], ["Kurland", "Oren", ""], ["Tennenholtz", "Moshe", ""], ["Raiber", "Fiana", ""]]}, {"id": "2005.12992", "submitter": "Ashlee Milton", "authors": "Ashlee Milton and Maria Soledad Pera", "title": "Evaluating Information Retrieval Systems for Kids", "comments": "Accepted at the 4th International and Interdisciplinary Perspectives\n  on Children & Recommender and Information Retrieval Systems (KidRec '20),\n  co-located with the 19th ACM International Conference on Interaction Design\n  and Children (IDC '20), https://kidrec.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Evaluation of information retrieval systems (IRS) is a prominent topic among\ninformation retrieval researchers--mainly directed at a general population.\nChildren require unique IRS and by extension different ways to evaluate these\nsystems, but as a large population that use IRS have largely been ignored on\nthe evaluation front. In this position paper, we explore many perspectives that\nmust be considered when evaluating IRS; we specially discuss problems faced by\nresearchers who work with children IRS, including lack of evaluation\nframeworks, limitations of data, and lack of user judgment understanding.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 18:10:21 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Milton", "Ashlee", ""], ["Pera", "Maria Soledad", ""]]}, {"id": "2005.12994", "submitter": "Puxuan Yu", "authors": "Puxuan Yu and James Allan", "title": "A Study of Neural Matching Models for Cross-lingual IR", "comments": "4 pages, 1 figure, accepted at SIGIR'20", "journal-ref": null, "doi": "10.1145/3397271.3401322", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we investigate interaction-based neural matching models for\nad-hoc cross-lingual information retrieval (CLIR) using cross-lingual word\nembeddings (CLWEs). With experiments conducted on the CLEF collection over four\nlanguage pairs, we evaluate and provide insight into different neural model\narchitectures, different ways to represent query-document interactions and\nword-pair similarity distributions in CLIR. This study paves the way for\nlearning an end-to-end CLIR system using CLWEs.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 19:21:57 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Yu", "Puxuan", ""], ["Allan", "James", ""]]}, {"id": "2005.13007", "submitter": "Gregory Coppola", "authors": "Gregory Coppola", "title": "DimensionRank: Personal Neural Representations for Personalized General\n  Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web Search and Social Media have always been two of the most important\napplications on the internet. We begin by giving a unified framework, called\ngeneral search, of which which all search and social media products can be seen\nas instances.\n  DimensionRank is our main contribution. This is an algorithm for personalized\ngeneral search, based on neural networks. DimensionRank's bold innovation is to\nmodel and represent each user using their own unique personal neural\nrepresentation vector, a learned representation in a real-valued\nmultidimensional vector space. This is the first internet service we are aware\nof that to model each user with their own independent representation vector.\nThis is also the first service we are aware of to attempt personalization for\ngeneral web search. Also, neural representations allows us to present the first\nReddit-style algorithm, that is immune to the problem of \"brigading\". We\nbelieve personalized general search will yield a search product orders of\nmagnitude better than Google's one-size-fits-all web search algorithm.\n  Finally, we announce Deep Revelations, a new search and social network\ninternet application based on DimensionRank.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 20:08:35 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Coppola", "Gregory", ""]]}, {"id": "2005.13094", "submitter": "Baoxu Shi", "authors": "Baoxu Shi, Jaewon Yang, Feng Guo, Qi He", "title": "Salience and Market-aware Skill Extraction for Job Targeting", "comments": "9 pages, to appear in KDD2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At LinkedIn, we want to create economic opportunity for everyone in the\nglobal workforce. To make this happen, LinkedIn offers a reactive Job Search\nsystem, and a proactive Jobs You May Be Interested In (JYMBII) system to match\nthe best candidates with their dream jobs. One of the most challenging tasks\nfor developing these systems is to properly extract important skill entities\nfrom job postings and then target members with matched attributes. In this\nwork, we show that the commonly used text-based \\emph{salience and\nmarket-agnostic} skill extraction approach is sub-optimal because it only\nconsiders skill mention and ignores the salient level of a skill and its market\ndynamics, i.e., the market supply and demand influence on the importance of\nskills. To address the above drawbacks, we present \\model, our deployed\n\\emph{salience and market-aware} skill extraction system. The proposed \\model\n~shows promising results in improving the online performance of job\nrecommendation (JYMBII) ($+1.92\\%$ job apply) and skill suggestions for job\nposters ($-37\\%$ suggestion rejection rate). Lastly, we present case studies to\nshow interesting insights that contrast traditional skill recognition method\nand the proposed \\model~from occupation, industry, country, and individual\nskill levels. Based on the above promising results, we deployed the \\model\n~online to extract job targeting skills for all $20$M job postings served at\nLinkedIn.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 00:20:14 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Shi", "Baoxu", ""], ["Yang", "Jaewon", ""], ["Guo", "Feng", ""], ["He", "Qi", ""]]}, {"id": "2005.13258", "submitter": "Yang Zhang", "authors": "Yang Zhang, Fuli Feng, Chenxu Wang, Xiangnan He, Meng Wang, Yan Li,\n  Yongdong Zhang", "title": "How to Retrain Recommender System? A Sequential Meta-Learning Method", "comments": "Appear in SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401167", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical recommender systems need be periodically retrained to refresh the\nmodel with new interaction data. To pursue high model fidelity, it is usually\ndesirable to retrain the model on both historical and new data, since it can\naccount for both long-term and short-term user preference. However, a full\nmodel retraining could be very time-consuming and memory-costly, especially\nwhen the scale of historical data is large. In this work, we study the model\nretraining mechanism for recommender systems, a topic of high practical values\nbut has been relatively little explored in the research community.\n  Our first belief is that retraining the model on historical data is\nunnecessary, since the model has been trained on it before. Nevertheless,\nnormal training on new data only may easily cause overfitting and forgetting\nissues, since the new data is of a smaller scale and contains fewer information\non long-term user preference. To address this dilemma, we propose a new\ntraining method, aiming to abandon the historical data during retraining\nthrough learning to transfer the past training experience. Specifically, we\ndesign a neural network-based transfer component, which transforms the old\nmodel to a new model that is tailored for future recommendations. To learn the\ntransfer component well, we optimize the \"future performance\" -- i.e., the\nrecommendation accuracy evaluated in the next time period. Our Sequential\nMeta-Learning(SML) method offers a general training paradigm that is applicable\nto any differentiable model. We demonstrate SML on matrix factorization and\nconduct experiments on two real-world datasets. Empirical results show that SML\nnot only achieves significant speed-up, but also outperforms the full model\nretraining in recommendation accuracy, validating the effectiveness of our\nproposals. We release our codes at: https://github.com/zyang1580/SML.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 09:50:46 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Zhang", "Yang", ""], ["Feng", "Fuli", ""], ["Wang", "Chenxu", ""], ["He", "Xiangnan", ""], ["Wang", "Meng", ""], ["Li", "Yan", ""], ["Zhang", "Yongdong", ""]]}, {"id": "2005.13270", "submitter": "Vinay Setty", "authors": "Bjarte Botnevik, Eirik Sakariassen, and Vinay Setty", "title": "BRENDA: Browser Extension for Fake News Detection", "comments": "Accepted as SIGIR demo", "journal-ref": "In Proceedings of the 43rd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR 2020), July 25 to\n  30, 2020, Virtual Event, China. ACM, New York, NY, USA, 4 pages", "doi": "10.1145/3397271.3401396", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Misinformation such as fake news has drawn a lot of attention in recent\nyears. It has serious consequences on society, politics and economy. This has\nlead to a rise of manually fact-checking websites such as Snopes and\nPolitifact. However, the scale of misinformation limits their ability for\nverification. In this demonstration, we propose BRENDA a browser extension\nwhich can be used to automate the entire process of credibility assessments of\nfalse claims. Behind the scenes BRENDA uses a tested deep neural network\narchitecture to automatically identify fact check worthy claims and classifies\nas well as presents the result along with evidence to the user. Since BRENDA is\na browser extension, it facilities fast automated fact checking for the end\nuser without having to leave the Webpage.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 10:29:14 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Botnevik", "Bjarte", ""], ["Sakariassen", "Eirik", ""], ["Setty", "Vinay", ""]]}, {"id": "2005.13524", "submitter": "Sayan Sinha", "authors": "Kaustubh Hiware, Ritam Dutt, Sayan Sinha, Sohan Patro, Kripabandhu\n  Ghosh, Saptarshi Ghosh", "title": "NARMADA: Need and Available Resource Managing Assistant for Disasters\n  and Adversities", "comments": "ACL 2020 Workshop on Natural Language Processing for Social Media\n  (SocialNLP)", "journal-ref": null, "doi": "10.18653/v1/2020.socialnlp-1.3", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although a lot of research has been done on utilising Online Social Media\nduring disasters, there exists no system for a specific task that is critical\nin a post-disaster scenario -- identifying resource-needs and\nresource-availabilities in the disaster-affected region, coupled with their\nsubsequent matching. To this end, we present NARMADA, a semi-automated platform\nwhich leverages the crowd-sourced information from social media posts for\nassisting post-disaster relief coordination efforts. The system employs Natural\nLanguage Processing and Information Retrieval techniques for identifying\nresource-needs and resource-availabilities from microblogs, extracting\nresources from the posts, and also matching the needs to suitable\navailabilities. The system is thus capable of facilitating the judicious\nmanagement of resources during post-disaster relief operations.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 17:52:54 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Hiware", "Kaustubh", ""], ["Dutt", "Ritam", ""], ["Sinha", "Sayan", ""], ["Patro", "Sohan", ""], ["Ghosh", "Kripabandhu", ""], ["Ghosh", "Saptarshi", ""]]}, {"id": "2005.13718", "submitter": "Asia Biega", "authors": "Asia J. Biega, Peter Potash, Hal Daum\\'e III, Fernando Diaz, Mich\\`ele\n  Finck", "title": "Operationalizing the Legal Principle of Data Minimization for\n  Personalization", "comments": "SIGIR 2020 paper: In Proc. of the 43rd International ACM SIGIR\n  Conference on Research and Development in Information Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Article 5(1)(c) of the European Union's General Data Protection Regulation\n(GDPR) requires that \"personal data shall be [...] adequate, relevant, and\nlimited to what is necessary in relation to the purposes for which they are\nprocessed (`data minimisation')\". To date, the legal and computational\ndefinitions of `purpose limitation' and `data minimization' remain largely\nunclear. In particular, the interpretation of these principles is an open issue\nfor information access systems that optimize for user experience through\npersonalization and do not strictly require personal data collection for the\ndelivery of basic service.\n  In this paper, we identify a lack of a homogeneous interpretation of the data\nminimization principle and explore two operational definitions applicable in\nthe context of personalization. The focus of our empirical study in the domain\nof recommender systems is on providing foundational insights about the (i)\nfeasibility of different data minimization definitions, (ii) robustness of\ndifferent recommendation algorithms to minimization, and (iii) performance of\ndifferent minimization strategies.We find that the performance decrease\nincurred by data minimization might not be substantial, but that it might\ndisparately impact different users---a finding which has implications for the\nviability of different formal minimization definitions. Overall, our analysis\nuncovers the complexities of the data minimization problem in the context of\npersonalization and maps the remaining computational and regulatory challenges.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 00:43:06 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Biega", "Asia J.", ""], ["Potash", "Peter", ""], ["Daum\u00e9", "Hal", "III"], ["Diaz", "Fernando", ""], ["Finck", "Mich\u00e8le", ""]]}, {"id": "2005.13751", "submitter": "Iraklis Moutidis", "authors": "Iraklis Moutidis and Hywel T.P. Williams", "title": "Complex networks for event detection in heterogeneous high volume news\n  streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting important events in high volume news streams is an important task\nfor a variety of purposes.The volume and rate of online news increases the need\nfor automated event detection methods thatcan operate in real time. In this\npaper we develop a network-based approach that makes the workingassumption that\nimportant news events always involve named entities (such as persons,\nlocationsand organizations) that are linked in news articles. Our approach uses\nnatural language processingtechniques to detect these entities in a stream of\nnews articles and then creates a time-stamped seriesof networks in which the\ndetected entities are linked by co-occurrence in articles and sentences. Inthis\nprototype, weighted node degree is tracked over time and change-point detection\nused to locateimportant events. Potential events are characterized and\ndistinguished using community detectionon KeyGraphs that relate named entities\nand informative noun-phrases from related articles. Thismethodology already\nproduces promising results and will be extended in future to include a\nwidervariety of complex network analysis techniques.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 02:45:43 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Moutidis", "Iraklis", ""], ["Williams", "Hywel T. P.", ""]]}, {"id": "2005.13783", "submitter": "Ali Ahmadvand", "authors": "Ali Ahmadvand and Surya Kallumadi and Faizan Javed and Eugene\n  Agichtein", "title": "JointMap: Joint Query Intent Understanding For Modeling Intent\n  Hierarchies in E-commerce Search", "comments": "SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate understanding of a user's query intent can help improve the\nperformance of downstream tasks such as query scoping and ranking. In the\ne-commerce domain, recent work in query understanding focuses on the query to\nproduct-category mapping. But, a small yet significant percentage of queries\n(in our website 1.5% or 33M queries in 2019) have non-commercial intent\nassociated with them. These intents are usually associated with non-commercial\ninformation seeking needs such as discounts, store hours, installation guides,\netc. In this paper, we introduce Joint Query Intent Understanding (JointMap), a\ndeep learning model to simultaneously learn two different high-level user\nintent tasks: 1) identifying a query's commercial vs. non-commercial intent,\nand 2) associating a set of relevant product categories in taxonomy to a\nproduct query. JointMap model works by leveraging the transfer bias that exists\nbetween these two related tasks through a joint-learning process. As curating a\nlabeled data set for these tasks can be expensive and time-consuming, we\npropose a distant supervision approach in conjunction with an active learning\nmodel to generate high-quality training data sets. To demonstrate the\neffectiveness of JointMap, we use search queries collected from a large\ncommercial website. Our results show that JointMap significantly improves both\n\"commercial vs. non-commercial\" intent prediction and product category mapping\nby 2.3% and 10% on average over state-of-the-art deep learning methods. Our\nfindings suggest a promising direction to model the intent hierarchies in an\ne-commerce search engine.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 05:20:00 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 21:05:35 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Ahmadvand", "Ali", ""], ["Kallumadi", "Surya", ""], ["Javed", "Faizan", ""], ["Agichtein", "Eugene", ""]]}, {"id": "2005.13808", "submitter": "Ali Ahmadvand", "authors": "Ali Ahmadvand", "title": "User Intent Inference for Web Search and Conversational Agents", "comments": "WSDM2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User intent understanding is a crucial step in designing both conversational\nagents and search engines. Detecting or inferring user intent is challenging,\nsince the user utterances or queries can be short, ambiguous, and contextually\ndependent. To address these research challenges, my thesis work focuses on: 1)\nUtterance topic and intent classification for conversational agents 2) Query\nintent mining and classification for Web search engines, focusing on the\ne-commerce domain. To address the first topic, I proposed novel models to\nincorporate entity information and conversation-context clues to predict both\ntopic and intent of the user's utterances. For the second research topic, I\nplan to extend the existing state of the art methods in Web search intent\nprediction to the e-commerce domain, via: 1) Developing a joint learning model\nto predict search queries' intents and the product categories associated with\nthem, 2) Discovering new hidden users' intents. All the models will be\nevaluated on the real queries available from a major e-commerce site search\nengine. The results from these studies can be leveraged to improve performance\nof various tasks such as natural language understanding, query scoping, query\nsuggestion, and ranking, resulting in an enriched user experience.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 07:04:42 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 16:13:38 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Ahmadvand", "Ali", ""]]}, {"id": "2005.13810", "submitter": "Ziv Vasilisky", "authors": "Ziv Vasilisky, Moshe Tennenholtz, Oren Kurland", "title": "Studying Ranking-Incentivized Web Dynamics", "comments": "4 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ranking incentives of many authors of Web pages play an important role in\nthe Web dynamics. That is, authors who opt to have their pages highly ranked\nfor queries of interest, often respond to rankings for these queries by\nmanipulating their pages; the goal is to improve the pages' future rankings.\nVarious theoretical aspects of this dynamics have recently been studied using\ngame theory. However, empirical analysis of the dynamics is highly constrained\ndue to lack of publicly available datasets.We present an initial such dataset\nthat is based on TREC's ClueWeb09 dataset. Specifically, we used the WayBack\nMachine of the Internet Archive to build a document collection that contains\npast snapshots of ClueWeb documents which are highly ranked by some initial\nsearch performed for ClueWeb queries. Temporal analysis of document changes in\nthis dataset reveals that findings recently presented for small-scale\ncontrolled ranking competitions between documents' authors also hold for Web\ndata. Specifically, documents' authors tend to mimic the content of documents\nthat were highly ranked in the past, and this practice can result in improved\nranking.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 07:08:42 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 17:58:13 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Vasilisky", "Ziv", ""], ["Tennenholtz", "Moshe", ""], ["Kurland", "Oren", ""]]}, {"id": "2005.13829", "submitter": "Yitong Ji", "authors": "Yitong Ji, Aixin Sun, Jie Zhang, Chenliang Li", "title": "A Re-visit of the Popularity Baseline in Recommender Systems", "comments": "Accepted by SIGIR2020", "journal-ref": null, "doi": "10.1145/3397271.3401233", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popularity is often included in experimental evaluation to provide a\nreference performance for a recommendation task. To understand how popularity\nbaseline is defined and evaluated, we sample 12 papers from top-tier\nconferences including KDD, WWW, SIGIR, and RecSys, and 6 open source toolkits.\nWe note that the widely adopted MostPop baseline simply ranks items based on\nthe number of interactions in the training data. We argue that the current\nevaluation of popularity (i) does not reflect the popular items at the time\nwhen a user interacts with the system, and (ii) may recommend items released\nafter a user's last interaction with the system. On the widely used MovieLens\ndataset, we show that the performance of popularity could be significantly\nimproved by 70% or more, if we consider the popular items at the time point\nwhen a user interacts with the system. We further show that, on MovieLens\ndataset, the users having lower tendencies on movies tend to follow the crowd\nand rate more popular movies. Movie lovers who rate a large number of movies,\nrate movies based on their own preferences and interests. Through this study,\nwe call for a re-visit of the popularity baseline in recommender system to\nbetter reflect its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 08:04:40 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 06:37:06 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Ji", "Yitong", ""], ["Sun", "Aixin", ""], ["Zhang", "Jie", ""], ["Li", "Chenliang", ""]]}, {"id": "2005.14024", "submitter": "Felix Hamborg", "authors": "Lukas Gebhard and Felix Hamborg", "title": "The POLUSA Dataset: 0.9M Political News Articles Balanced by Time and\n  Outlet Popularity", "comments": "2 pages, 1 table", "journal-ref": null, "doi": "10.1145/3383583.3398567", "report-no": null, "categories": "cs.DL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News articles covering policy issues are an essential source of information\nin the social sciences and are also frequently used for other use cases, e.g.,\nto train NLP language models. To derive meaningful insights from the analysis\nof news, large datasets are required that represent real-world distributions,\ne.g., with respect to the contained outlets' popularity, topically, or across\ntime. Information on the political leanings of media publishers is often\nneeded, e.g., to study differences in news reporting across the political\nspectrum, which is one of the prime use cases in the social sciences when\nstudying media bias and related societal issues. Concerning these requirements,\nexisting datasets have major flaws, resulting in redundant and cumbersome\neffort in the research community for dataset creation. To fill this gap, we\npresent POLUSA, a dataset that represents the online media landscape as\nperceived by an average US news consumer. The dataset contains 0.9M articles\ncovering policy topics published between Jan. 2017 and Aug. 2019 by 18 news\noutlets representing the political spectrum. Each outlet is labeled by its\npolitical leaning, which we derive using a systematic aggregation of eight data\nsources. The news dataset is balanced with respect to publication date and\noutlet popularity. POLUSA enables studying a variety of subjects, e.g., media\neffects and political partisanship. Due to its size, the dataset allows to\nutilize data-intense deep learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 14:24:11 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Gebhard", "Lukas", ""], ["Hamborg", "Felix", ""]]}, {"id": "2005.14171", "submitter": "Jiarui Qin", "authors": "Jiarui Qin, Weinan Zhang, Xin Wu, Jiarui Jin, Yuchen Fang, Yong Yu", "title": "User Behavior Retrieval for Click-Through Rate Prediction", "comments": "SIGIR 2020 industry track", "journal-ref": null, "doi": "10.1145/3397271.3401440", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) prediction plays a key role in modern online\npersonalization services. In practice, it is necessary to capture user's\ndrifting interests by modeling sequential user behaviors to build an accurate\nCTR prediction model. However, as the users accumulate more and more behavioral\ndata on the platforms, it becomes non-trivial for the sequential models to make\nuse of the whole behavior history of each user. First, directly feeding the\nlong behavior sequence will make online inference time and system load\ninfeasible. Second, there is much noise in such long histories to fail the\nsequential model learning. The current industrial solutions mainly truncate the\nsequences and just feed recent behaviors to the prediction model, which leads\nto a problem that sequential patterns such as periodicity or long-term\ndependency are not embedded in the recent several behaviors but in far back\nhistory. To tackle these issues, in this paper we consider it from the data\nperspective instead of just designing more sophisticated yet complicated models\nand propose User Behavior Retrieval for CTR prediction (UBR4CTR) framework. In\nUBR4CTR, the most relevant and appropriate user behaviors will be firstly\nretrieved from the entire user history sequence using a learnable search\nmethod. These retrieved behaviors are then fed into a deep model to make the\nfinal prediction instead of simply using the most recent ones. It is highly\nfeasible to deploy UBR4CTR into industrial model pipeline with low cost.\nExperiments on three real-world large-scale datasets demonstrate the\nsuperiority and efficacy of our proposed framework and models.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 17:39:22 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Qin", "Jiarui", ""], ["Zhang", "Weinan", ""], ["Wu", "Xin", ""], ["Jin", "Jiarui", ""], ["Fang", "Yuchen", ""], ["Yu", "Yong", ""]]}, {"id": "2005.14255", "submitter": "Jie Zou", "authors": "Jie Zou, Yifan Chen and Evangelos Kanoulas", "title": "Towards Question-based Recommender Systems", "comments": "accepted by SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401180", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational and question-based recommender systems have gained increasing\nattention in recent years, with users enabled to converse with the system and\nbetter control recommendations. Nevertheless, research in the field is still\nlimited, compared to traditional recommender systems. In this work, we propose\na novel Question-based recommendation method, Qrec, to assist users to find\nitems interactively, by answering automatically constructed and algorithmically\nchosen questions. Previous conversational recommender systems ask users to\nexpress their preferences over items or item facets. Our model, instead, asks\nusers to express their preferences over descriptive item features. The model is\nfirst trained offline by a novel matrix factorization algorithm, and then\niteratively updates the user and item latent factors online by a closed-form\nsolution based on the user answers. Meanwhile, our model infers the underlying\nuser belief and preferences over items to learn an optimal question-asking\nstrategy by using Generalized Binary Search, so as to ask a sequence of\nquestions to the user. Our experimental results demonstrate that our proposed\nmatrix factorization model outperforms the traditional Probabilistic Matrix\nFactorization model. Further, our proposed Qrec model can greatly improve the\nperformance of state-of-the-art baselines, and it is also effective in the case\nof cold-start user and item recommendations.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 19:47:13 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Zou", "Jie", ""], ["Chen", "Yifan", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "2005.14271", "submitter": "Hamed Shahbazi", "authors": "Hamed Shahbazi, Xiaoli Z. Fern, Reza Ghaeini, Prasad Tadepalli", "title": "Relation Extraction with Explanation", "comments": "accepted by ACL 2020", "journal-ref": "ACL.2020", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural models for relation extraction with distant supervision\nalleviate the impact of irrelevant sentences in a bag by learning importance\nweights for the sentences. Efforts thus far have focused on improving\nextraction accuracy but little is known about their explainability. In this\nwork we annotate a test set with ground-truth sentence-level explanations to\nevaluate the quality of explanations afforded by the relation extraction\nmodels. We demonstrate that replacing the entity mentions in the sentences with\ntheir fine-grained entity types not only enhances extraction accuracy but also\nimproves explanation. We also propose to automatically generate \"distractor\"\nsentences to augment the bags and train the model to ignore the distractors.\nEvaluations on the widely used FB-NYT dataset show that our methods achieve new\nstate-of-the-art accuracy while improving model explainability.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 20:15:56 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Shahbazi", "Hamed", ""], ["Fern", "Xiaoli Z.", ""], ["Ghaeini", "Reza", ""], ["Tadepalli", "Prasad", ""]]}, {"id": "2005.14288", "submitter": "Naoto Usuyama", "authors": "Naoto Usuyama, Natalia Larios Delgado, Amanda K. Hall, Jessica Lundin", "title": "ePillID Dataset: A Low-Shot Fine-Grained Benchmark for Pill\n  Identification", "comments": "CVPR 2020 VL3. Project Page:\n  https://github.com/usuyama/ePillID-benchmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying prescription medications is a frequent task for patients and\nmedical professionals; however, this is an error-prone task as many pills have\nsimilar appearances (e.g. white round pills), which increases the risk of\nmedication errors. In this paper, we introduce ePillID, the largest public\nbenchmark on pill image recognition, composed of 13k images representing 9804\nappearance classes (two sides for 4902 pill types). For most of the appearance\nclasses, there exists only one reference image, making it a challenging\nlow-shot recognition setting. We present our experimental setup and evaluation\nresults of various baseline models on the benchmark. The best baseline using a\nmulti-head metric-learning approach with bilinear features performed remarkably\nwell; however, our error analysis suggests that they still fail to distinguish\nparticularly confusing classes. The code and data are available at\nhttps://github.com/usuyama/ePillID-benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 20:53:36 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 22:29:24 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Usuyama", "Naoto", ""], ["Delgado", "Natalia Larios", ""], ["Hall", "Amanda K.", ""], ["Lundin", "Jessica", ""]]}, {"id": "2005.14373", "submitter": "Chao Liu Dr.", "authors": "Chao Liu, Xin Xia, David Lo, Zhiwei Liu, Ahmed E. Hassan, and Shanping\n  Li", "title": "Simplifying Deep-Learning-Based Model for Code Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To accelerate software development, developers frequently search and reuse\nexisting code snippets from a large-scale codebase, e.g., GitHub. Over the\nyears, researchers proposed many information retrieval (IR) based models for\ncode search, which match keywords in query with code text. But they fail to\nconnect the semantic gap between query and code. To conquer this challenge, Gu\net al. proposed a deep-learning-based model named DeepCS. It jointly embeds\nmethod code and natural language description into a shared vector space, where\nmethods related to a natural language query are retrieved according to their\nvector similarities. However, DeepCS' working process is complicated and\ntime-consuming. To overcome this issue, we proposed a simplified model\nCodeMatcher that leverages the IR technique but maintains many features in\nDeepCS. Generally, CodeMatcher combines query keywords with the original order,\nperforms a fuzzy search on name and body strings of methods, and returned the\nbest-matched methods with the longer sequence of used keywords. We verified its\neffectiveness on a large-scale codebase with about 41k repositories.\nExperimental results showed the simplified model CodeMatcher outperforms DeepCS\nby 97% in terms of MRR (a widely used accuracy measure for code search), and it\nis over 66 times faster than DeepCS. Besides, comparing with the\nstate-of-the-art IR-based model CodeHow, CodeMatcher also improves the MRR by\n73%. We also observed that: fusing the advantages of IR-based and\ndeep-learning-based models is promising because they compensate with each other\nby nature; improving the quality of method naming helps code search, since\nmethod name plays an important role in connecting query and code.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 03:21:33 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Liu", "Chao", ""], ["Xia", "Xin", ""], ["Lo", "David", ""], ["Liu", "Zhiwei", ""], ["Hassan", "Ahmed E.", ""], ["Li", "Shanping", ""]]}, {"id": "2005.14464", "submitter": "Jiwei Li", "authors": "Xiaoya Li, Mingxin Zhou, Jiawei Wu, Arianna Yuan, Fei Wu and Jiwei Li", "title": "Analyzing COVID-19 on Online Social Media: Trends, Sentiments and\n  Emotions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the time of writing, the ongoing pandemic of coronavirus disease\n(COVID-19) has caused severe impacts on society, economy and people's daily\nlives. People constantly express their opinions on various aspects of the\npandemic on social media, making user-generated content an important source for\nunderstanding public emotions and concerns. In this paper, we perform a\ncomprehensive analysis on the affective trajectories of the American people and\nthe Chinese people based on Twitter and Weibo posts between January 20th, 2020\nand May 11th 2020. Specifically, by identifying people's sentiments, emotions\n(i.e., anger, disgust, fear, happiness, sadness, surprise) and the emotional\ntriggers (e.g., what a user is angry/sad about) we are able to depict the\ndynamics of public affect in the time of COVID-19. By contrasting two very\ndifferent countries, China and the Unites States, we reveal sharp differences\nin people's views on COVID-19 in different cultures. Our study provides a\ncomputational approach to unveiling public emotions and concerns on the\npandemic in real-time, which would potentially help policy-makers better\nunderstand people's need and thus make optimal policy.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 09:24:38 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 09:41:10 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 06:36:29 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Li", "Xiaoya", ""], ["Zhou", "Mingxin", ""], ["Wu", "Jiawei", ""], ["Yuan", "Arianna", ""], ["Wu", "Fei", ""], ["Li", "Jiwei", ""]]}, {"id": "2005.14576", "submitter": "Dieter Schn\\\"app", "authors": "Susanne Arndt, Dieter Schn\\\"app", "title": "Harbsafe-162. A Domain-Specific Data Set for the Intrinsic Evaluation of\n  Semantic Representations for Terminological Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The article presents Harbsafe-162, a domain-specific data set for evaluating\ndistributional semantic models. It originates from a cooperation by Technische\nUniversit\\\"at Braunschweig and the German Commission for Electrical, Electronic\n& Information Technologies of DIN and VDE, the Harbsafe project. One objective\nof the project is to apply distributional semantic models to terminological\nentries, that is, complex lexical data comprising of at least one or several\nterms, term phrases and a definition. This application is needed to solve a\nmore complex problem: the harmonization of terminologies of standards and\nstandards bodies (i.e. resolution of doublettes and inconsistencies). Due to a\nlack of evaluation data sets for terminological entries, the creation of\nHarbsafe-162 was a necessary step towards harmonization assistance.\nHarbsafe-162 covers data from nine electrotechnical standards in the domain of\nfunctional safety, IT security, and dependability. An intrinsic evaluation\nmethod in the form of a similarity rating task has been applied in which two\nlinguists and three domain experts from standardization participated. The data\nset is used to evaluate a specific implementation of an established sentence\nembedding model. This implementation proves to be satisfactory for the\ndomain-specific data so that further implementations for harmonization\nassistance may be brought forward by the project. Considering recent criticism\non intrinsic evaluation methods, the article concludes with an evaluation of\nHarbsafe-162 and joins a more general discussion about the nature of similarity\nrating tasks. Harbsafe-162 has been made available for the community.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 13:56:31 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Arndt", "Susanne", ""], ["Schn\u00e4pp", "Dieter", ""]]}, {"id": "2005.14613", "submitter": "Kalyani Roy", "authors": "Kalyani Roy (1), Smit Shah (1), Nithish Pai (2), Jaidam Ramtej (2),\n  Prajit Prashant Nadkarn (2), Jyotirmoy Banerjee (2), Pawan Goyal (1), and\n  Surender Kumar (2) ((1) Indian Institute of Technology Kharagpur, (2)\n  Flipkart)", "title": "Using Large Pretrained Language Models for Answering User Queries from\n  Product Specifications", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While buying a product from the e-commerce websites, customers generally have\na plethora of questions. From the perspective of both the e-commerce service\nprovider as well as the customers, there must be an effective question\nanswering system to provide immediate answers to the user queries. While\ncertain questions can only be answered after using the product, there are many\nquestions which can be answered from the product specification itself. Our work\ntakes a first step in this direction by finding out the relevant product\nspecifications, that can help answering the user questions. We propose an\napproach to automatically create a training dataset for this problem. We\nutilize recently proposed XLNet and BERT architectures for this problem and\nfind that they provide much better performance than the Siamese model,\npreviously applied for this problem. Our model gives a good performance even\nwhen trained on one vertical and tested across different verticals.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 14:52:33 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Roy", "Kalyani", ""], ["Shah", "Smit", ""], ["Pai", "Nithish", ""], ["Ramtej", "Jaidam", ""], ["Nadkarn", "Prajit Prashant", ""], ["Banerjee", "Jyotirmoy", ""], ["Goyal", "Pawan", ""], ["Kumar", "Surender", ""]]}, {"id": "2005.14627", "submitter": "Md Gulzar Hussain", "authors": "Md Gulzar Hussain, Md Rashidul Hasan, Mahmuda Rahman, Joy Protim, and\n  Sakib Al Hasan", "title": "Detection of Bangla Fake News using MNB and SVM Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fake news has been coming into sight in significant numbers for numerous\nbusiness and political reasons and has become frequent in the online world.\nPeople can get contaminated easily by these fake news for its fabricated words\nwhich have enormous effects on the offline community. Thus, interest in\nresearch in this area has risen. Significant research has been conducted on the\ndetection of fake news from English texts and other languages but a few in\nBangla Language. Our work reflects the experimental analysis on the detection\nof Bangla fake news from social media as this field still requires much focus.\nIn this research work, we have used two supervised machine learning algorithms,\nMultinomial Naive Bayes (MNB) and Support Vector Machine (SVM) classifiers to\ndetect Bangla fake news with CountVectorizer and Term Frequency - Inverse\nDocument Frequency Vectorizer as feature extraction. Our proposed framework\ndetects fake news depending on the polarity of the corresponding article.\nFinally, our analysis shows SVM with the linear kernel with an accuracy of\n96.64% outperform MNB with an accuracy of 93.32%.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 15:38:54 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Hussain", "Md Gulzar", ""], ["Hasan", "Md Rashidul", ""], ["Rahman", "Mahmuda", ""], ["Protim", "Joy", ""], ["Hasan", "Sakib Al", ""]]}, {"id": "2005.14630", "submitter": "Manuel Ruiz-De-Luzuriaga-Pe\\~na", "authors": "Manuel Ruiz de Luzuriaga Pe\\~na, Isabel Mu\\~noz Mouri\\~no and Mercedes\n  Bogino Larrambebere", "title": "Los perfiles de investigaci\\'on y su implantaci\\'on en la Universidad\n  Publica de Navarra", "comments": "in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This work aims to monitor and control the presence of UPNA research staff in\nthe main research profiles platforms, not only in the most obvious ones such as\nGoogle Scholar Citation, Researcher ID, Scopus ID and ORCID, but also in other\nservices that, in practice, they function as research profiles, such as\nMendeley, Linkedin, ResearchGate, Academia.edu and Academica-e. We also find it\ninteresting to analyze that presence and see how it responds to a variables,\nsuch as the department, gender, job category, research group. In this study we\nhave excluded some platforms for different reasons. Dialnet profiles are\nentered from the UPNA library (BUPNA), which means that all those who meet the\nrequirements for inclusion would be there, so their analysis does not make much\nsense, since it depends on factors outside the will of the researcher himself.\nThe same is the case with the UPNA Scientific Production Portal (PPC): the data\nis entered from the Vicerrectorado de Investigaci\\'on and should include all\nmembers of the UPNA PDI. Using as a base the census of university research\nstaff provided by the Vicerrectorado de Investigaci\\'on, it has been verified,\nfor each author, the existence or not of a profile in the different services\nstudied. The results have been tabulated in an Excel file to be able to analyze\nthem later. The data has been collected in March 2018 for Orcid, ResearcherID,\nScopusID, Google Scholar Citations and Mendeley. In November 2018, data from\nAcademica-e, Academia.edu, ResearchGate and Linkedin were taken. For each of\nthe profiles, a search by institutional affiliation was used, when possible, to\nobtain a first list of UPNA research personnel with that profile. Subsequently,\na search was carried out, person by person, of the rest of the research staff\nthat did not appear in that first list.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 15:47:38 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Pe\u00f1a", "Manuel Ruiz de Luzuriaga", ""], ["Mouri\u00f1o", "Isabel Mu\u00f1oz", ""], ["Larrambebere", "Mercedes Bogino", ""]]}, {"id": "2005.14713", "submitter": "Ashudeep Singh", "authors": "Marco Morik, Ashudeep Singh, Jessica Hong, Thorsten Joachims", "title": "Controlling Fairness and Bias in Dynamic Learning-to-Rank", "comments": "First two authors contributed equally. In Proceedings of the 43rd\n  International ACM SIGIR Conference on Research and Development in Information\n  Retrieval 2020", "journal-ref": null, "doi": "10.1145/3397271.3401100", "report-no": null, "categories": "cs.IR cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rankings are the primary interface through which many online platforms match\nusers to items (e.g. news, products, music, video). In these two-sided markets,\nnot only the users draw utility from the rankings, but the rankings also\ndetermine the utility (e.g. exposure, revenue) for the item providers (e.g.\npublishers, sellers, artists, studios). It has already been noted that\nmyopically optimizing utility to the users, as done by virtually all\nlearning-to-rank algorithms, can be unfair to the item providers. We,\ntherefore, present a learning-to-rank approach for explicitly enforcing\nmerit-based fairness guarantees to groups of items (e.g. articles by the same\npublisher, tracks by the same artist). In particular, we propose a learning\nalgorithm that ensures notions of amortized group fairness, while\nsimultaneously learning the ranking function from implicit feedback data. The\nalgorithm takes the form of a controller that integrates unbiased estimators\nfor both fairness and utility, dynamically adapting both as more data becomes\navailable. In addition to its rigorous theoretical foundation and convergence\nguarantees, we find empirically that the algorithm is highly practical and\nrobust.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 17:57:56 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Morik", "Marco", ""], ["Singh", "Ashudeep", ""], ["Hong", "Jessica", ""], ["Joachims", "Thorsten", ""]]}]