[{"id": "1802.00323", "submitter": "Mucahid Kutlu", "authors": "Mucahid Kutlu, Vivek Khetan, Matthew Lease", "title": "Correlation and Prediction of Evaluation Metrics in Information\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because researchers typically do not have the time or space to present more\nthan a few evaluation metrics in any published study, it can be difficult to\nassess relative effectiveness of prior methods for unreported metrics when\nbaselining a new method or conducting a systematic meta-review. While sharing\nof study data would help alleviate this, recent attempts to encourage\nconsistent sharing have been largely unsuccessful. Instead, we propose to\nenable relative comparisons with prior work across arbitrary metrics by\npredicting unreported metrics given one or more reported metrics. In addition,\nwe further investigate prediction of high-cost evaluation measures using\nlow-cost measures as a potential strategy for reducing evaluation cost. We\nbegin by assessing the correlation between 23 IR metrics using 8 TREC test\ncollections. Measuring prediction error wrt. R-square and Kendall's tau, we\nshow that accurate prediction of MAP, P@10, and RBP can be achieved using only\n2-3 other metrics. With regard to lowering evaluation cost, we show that\nRBP(p=0.95) can be predicted with high accuracy using measures with only\nevaluation depth of 30. Taken together, our findings provide a valuable\nproof-of-concept which we expect to spur follow-on work by others in proposing\nmore sophisticated models for metric prediction.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 15:03:20 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Kutlu", "Mucahid", ""], ["Khetan", "Vivek", ""], ["Lease", "Matthew", ""]]}, {"id": "1802.00400", "submitter": "Yanshan Wang", "authors": "Yanshan Wang, Sijia Liu, Naveed Afzal, Majid Rastegar-Mojarad, Liwei\n  Wang, Feichen Shen, Paul Kingsbury, Hongfang Liu", "title": "A Comparison of Word Embeddings for the Biomedical Natural Language\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings have been widely used in biomedical Natural Language\nProcessing (NLP) applications as they provide vector representations of words\ncapturing the semantic properties of words and the linguistic relationship\nbetween words. Many biomedical applications use different textual resources\n(e.g., Wikipedia and biomedical articles) to train word embeddings and apply\nthese word embeddings to downstream biomedical applications. However, there has\nbeen little work on evaluating the word embeddings trained from these\nresources.In this study, we provide an empirical evaluation of word embeddings\ntrained from four different resources, namely clinical notes, biomedical\npublications, Wikipedia, and news. We performed the evaluation qualitatively\nand quantitatively. For the qualitative evaluation, we manually inspected five\nmost similar medical words to a given set of target medical words, and then\nanalyzed word embeddings through the visualization of those word embeddings.\nFor the quantitative evaluation, we conducted both intrinsic and extrinsic\nevaluation. Based on the evaluation results, we can draw the following\nconclusions. First, the word embeddings trained on clinical notes and\nbiomedical publications can capture the semantics of medical terms better, and\nfind more relevant similar medical terms, and are closer to human experts'\njudgments, compared to these trained on Wikipedia and news. Second, there does\nnot exist a consistent global ranking of word embedding quality for downstream\nbiomedical NLP applications. However, adding word embeddings as extra features\nwill improve results on most downstream tasks. Finally, the word embeddings\ntrained on biomedical domain corpora do not necessarily have better performance\nthan those trained on other general domain corpora for any downstream\nbiomedical NLP tasks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 17:20:39 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 16:47:07 GMT"}, {"version": "v3", "created": "Wed, 18 Jul 2018 18:37:43 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Wang", "Yanshan", ""], ["Liu", "Sijia", ""], ["Afzal", "Naveed", ""], ["Rastegar-Mojarad", "Majid", ""], ["Wang", "Liwei", ""], ["Shen", "Feichen", ""], ["Kingsbury", "Paul", ""], ["Liu", "Hongfang", ""]]}, {"id": "1802.00946", "submitter": "Parth Mehta", "authors": "Parth Mehta and Prasenjit Majumder", "title": "Content based Weighted Consensus Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-document summarization has received a great deal of attention in the\npast couple of decades. Several approaches have been proposed, many of which\nperform equally well and it is becoming in- creasingly difficult to choose one\nparticular system over another. An ensemble of such systems that is able to\nleverage the strengths of each individual systems can build a better and more\nrobust summary. Despite this, few attempts have been made in this direction. In\nthis paper, we describe a category of ensemble systems which use consensus\nbetween the candidate systems to build a better meta-summary. We highlight two\nmajor shortcomings of such systems: the inability to take into account relative\nperformance of individual systems and overlooking content of candidate\nsummaries in favour of the sentence rankings. We propose an alternate method,\ncontent-based weighted consensus summarization, which address these concerns.\nWe use pseudo-relevant summaries to estimate the performance of individual\ncandidate systems, and then use this information to generate a better aggregate\nranking. Experiments on DUC 2003 and DUC 2004 datasets show that the proposed\nsystem outperforms existing consensus-based techniques by a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 09:56:54 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Mehta", "Parth", ""], ["Majumder", "Prasenjit", ""]]}, {"id": "1802.00985", "submitter": "Jing Yu", "authors": "Jing Yu, Yuhang Lu, Zengchang Qin, Yanbing Liu, Jianlong Tan, Li Guo,\n  Weifeng Zhang", "title": "Modeling Text with Graph Convolutional Network for Cross-Modal\n  Information Retrieval", "comments": "7 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal information retrieval aims to find heterogeneous data of various\nmodalities from a given query of one modality. The main challenge is to map\ndifferent modalities into a common semantic space, in which distance between\nconcepts in different modalities can be well modeled. For cross-modal\ninformation retrieval between images and texts, existing work mostly uses\noff-the-shelf Convolutional Neural Network (CNN) for image feature extraction.\nFor texts, word-level features such as bag-of-words or word2vec are employed to\nbuild deep learning models to represent texts. Besides word-level semantics,\nthe semantic relations between words are also informative but less explored. In\nthis paper, we model texts by graphs using similarity measure based on\nword2vec. A dual-path neural network model is proposed for couple feature\nlearning in cross-modal information retrieval. One path utilizes Graph\nConvolutional Network (GCN) for text modeling based on graph representations.\nThe other path uses a neural network with layers of nonlinearities for image\nmodeling based on off-the-shelf features. The model is trained by a pairwise\nsimilarity loss function to maximize the similarity of relevant text-image\npairs and minimize the similarity of irrelevant pairs. Experimental results\nshow that the proposed model outperforms the state-of-the-art methods\nsignificantly, with 17% improvement on accuracy for the best case.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 15:05:01 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 04:19:45 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Yu", "Jing", ""], ["Lu", "Yuhang", ""], ["Qin", "Zengchang", ""], ["Liu", "Yanbing", ""], ["Tan", "Jianlong", ""], ["Guo", "Li", ""], ["Zhang", "Weifeng", ""]]}, {"id": "1802.01074", "submitter": "Minh C. Phan", "authors": "Minh C. Phan and Aixin Sun and Yi Tay and Jialong Han and Chenliang Li", "title": "Pair-Linking for Collective Entity Disambiguation: Two Could Be Better\n  Than All", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective entity disambiguation aims to jointly resolve multiple mentions by\nlinking them to their associated entities in a knowledge base. Previous works\nare primarily based on the underlying assumption that entities within the same\ndocument are highly related. However, the extend to which these mentioned\nentities are actually connected in reality is rarely studied and therefore\nraises interesting research questions. For the first time, we show that the\nsemantic relationships between the mentioned entities are in fact less dense\nthan expected. This could be attributed to several reasons such as noise, data\nsparsity and knowledge base incompleteness. As a remedy, we introduce MINTREE,\na new tree-based objective for the entity disambiguation problem. The key\nintuition behind MINTREE is the concept of coherence relaxation which utilizes\nthe weight of a minimum spanning tree to measure the coherence between\nentities. Based on this new objective, we design a novel entity disambiguation\nalgorithms which we call Pair-Linking. Instead of considering all the given\nmentions, Pair-Linking iteratively selects a pair with the highest confidence\nat each step for decision making. Via extensive experiments, we show that our\napproach is not only more accurate but also surprisingly faster than many\nstate-of-the-art collective linking algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 05:50:39 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 21:17:33 GMT"}, {"version": "v3", "created": "Mon, 16 Jul 2018 05:24:37 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Phan", "Minh C.", ""], ["Sun", "Aixin", ""], ["Tay", "Yi", ""], ["Han", "Jialong", ""], ["Li", "Chenliang", ""]]}, {"id": "1802.01786", "submitter": "Amir Karami", "authors": "Amir Karami, London S. Bennett, Xiaoyun He", "title": "Mining Public Opinion about Economic Issues: Twitter and the U.S.\n  Presidential Election", "comments": null, "journal-ref": null, "doi": "10.4018/IJSDS.2018010102", "report-no": null, "categories": "cs.SI cs.CL cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion polls have been the bridge between public opinion and politicians in\nelections. However, developing surveys to disclose people's feedback with\nrespect to economic issues is limited, expensive, and time-consuming. In recent\nyears, social media such as Twitter has enabled people to share their opinions\nregarding elections. Social media has provided a platform for collecting a\nlarge amount of social media data. This paper proposes a computational public\nopinion mining approach to explore the discussion of economic issues in social\nmedia during an election. Current related studies use text mining methods\nindependently for election analysis and election prediction; this research\ncombines two text mining methods: sentiment analysis and topic modeling. The\nproposed approach has effectively been deployed on millions of tweets to\nanalyze economic concerns of people during the 2012 US presidential election.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 03:55:37 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Karami", "Amir", ""], ["Bennett", "London S.", ""], ["He", "Xiaoyun", ""]]}, {"id": "1802.01886", "submitter": "Weinan Zhang", "authors": "Yaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang,\n  Yong Yu", "title": "Texygen: A Benchmarking Platform for Text Generation Models", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Texygen, a benchmarking platform to support research on\nopen-domain text generation models. Texygen has not only implemented a majority\nof text generation models, but also covered a set of metrics that evaluate the\ndiversity, the quality and the consistency of the generated texts. The Texygen\nplatform could help standardize the research on text generation and facilitate\nthe sharing of fine-tuned open-source implementations among researchers for\ntheir work. As a consequence, this would help in improving the reproductivity\nand reliability of future research work in text generation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 11:30:32 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Zhu", "Yaoming", ""], ["Lu", "Sidi", ""], ["Zheng", "Lei", ""], ["Guo", "Jiaxian", ""], ["Zhang", "Weinan", ""], ["Wang", "Jun", ""], ["Yu", "Yong", ""]]}, {"id": "1802.02603", "submitter": "Christina Lioma Assoc. Prof", "authors": "Christina Lioma and Birger Larsen and Peter Ingwersen", "title": "To Phrase or Not to Phrase - Impact of User versus System Term\n  Dependence Upon Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When submitting queries to information retrieval (IR) systems, users often\nhave the option of specifying which, if any, of the query terms are heavily\ndependent on each other and should be treated as a fixed phrase, for instance\nby placing them between quotes. In addition to such cases where users specify\nterm dependence, automatic ways also exist for IR systems to detect dependent\nterms in queries. Most IR systems use both user and algorithmic approaches. It\nis not however clear whether and to what extent user-defined term dependence\nagrees with algorithmic estimates of term dependence, nor which of the two may\nfetch higher performance gains. Simply put, is it better to trust users or the\nsystem to detect term dependence in queries? To answer this question, we\nexperiment with 101 crowdsourced search engine users and 334 queries (52 train\nand 282 test TREC queries) and we record 10 assessments per query. We find that\n(i) user assessments of term dependence differ significantly from algorithmic\nassessments of term dependence (their overlap is approximately 30%); (ii) there\nis little agreement among users about term dependence in queries, and this\ndisagreement increases as queries become longer; (iii) the potential retrieval\ngain that can be fetched by treating term dependence (both user- and\nsystem-defined) over a bag of words baseline is reserved to a small subset\n(approxi-mately 8%) of the queries, and is much higher for low-depth than deep\npreci-sion measures. Points (ii) and (iii) constitute novel insights into term\ndependence.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 19:21:58 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 21:34:36 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Lioma", "Christina", ""], ["Larsen", "Birger", ""], ["Ingwersen", "Peter", ""]]}, {"id": "1802.02668", "submitter": "Yi Zhu", "authors": "Yi Zhu and Xueqing Deng and Shawn Newsam", "title": "Fine-Grained Land Use Classification at the City Scale Using\n  Ground-Level Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform fine-grained land use mapping at the city scale using ground-level\nimages. Mapping land use is considerably more difficult than mapping land cover\nand is generally not possible using overhead imagery as it requires close-up\nviews and seeing inside buildings. We postulate that the growing collections of\ngeoreferenced, ground-level images suggest an alternate approach to this\ngeographic knowledge discovery problem. We develop a general framework that\nuses Flickr images to map 45 different land-use classes for the City of San\nFrancisco. Individual images are classified using a novel convolutional neural\nnetwork containing two streams, one for recognizing objects and another for\nrecognizing scenes. This network is trained in an end-to-end manner directly on\nthe labeled training images. We propose several strategies to overcome the\nnoisiness of our user-generated data including search-based training set\naugmentation and online adaptive training. We derive a ground truth map of San\nFrancisco in order to evaluate our method. We demonstrate the effectiveness of\nour approach through geo-visualization and quantitative analysis. Our framework\nachieves over 29% recall at the individual land parcel level which represents a\nstrong baseline for the challenging 45-way land use classification problem\nespecially given the noisiness of the image data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 23:01:13 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Zhu", "Yi", ""], ["Deng", "Xueqing", ""], ["Newsam", "Shawn", ""]]}, {"id": "1802.03052", "submitter": "Peter Jansen", "authors": "Peter A. Jansen, Elizabeth Wainwright, Steven Marmorstein, Clayton T.\n  Morrison", "title": "WorldTree: A Corpus of Explanation Graphs for Elementary Science\n  Questions supporting Multi-Hop Inference", "comments": "Accepted at the Language Resource and Evaluation Conference (LREC)\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing methods of automated inference that are able to provide users with\ncompelling human-readable justifications for why the answer to a question is\ncorrect is critical for domains such as science and medicine, where user trust\nand detecting costly errors are limiting factors to adoption. One of the\ncentral barriers to training question answering models on explainable inference\ntasks is the lack of gold explanations to serve as training data. In this paper\nwe present a corpus of explanations for standardized science exams, a recent\nchallenge task for question answering. We manually construct a corpus of\ndetailed explanations for nearly all publicly available standardized elementary\nscience question (approximately 1,680 3rd through 5th grade questions) and\nrepresent these as \"explanation graphs\" -- sets of lexically overlapping\nsentences that describe how to arrive at the correct answer to a question\nthrough a combination of domain and world knowledge. We also provide an\nexplanation-centered tablestore, a collection of semi-structured tables that\ncontain the knowledge to construct these elementary science explanations.\nTogether, these two knowledge resources map out a substantial portion of the\nknowledge required for answering and explaining elementary science exams, and\nprovide both structured and free-text training data for the explainable\ninference task.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 21:26:03 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Jansen", "Peter A.", ""], ["Wainwright", "Elizabeth", ""], ["Marmorstein", "Steven", ""], ["Morrison", "Clayton T.", ""]]}, {"id": "1802.03101", "submitter": "Martin Loncaric", "authors": "Martin Loncaric and Bowei Liu and Ryan Weber", "title": "Convolutional Hashing for Automated Scene Matching", "comments": "9 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a powerful new loss function and training scheme for learning\nbinary hash functions. In particular, we demonstrate our method by creating for\nthe first time a neural network that outperforms state-of-the-art Haar wavelets\nand color layout descriptors at the task of automated scene matching. By\naccurately relating distance on the manifold of network outputs to distance in\nHamming space, we achieve a 100-fold reduction in nontrivial false positive\nrate and significantly higher true positive rate. We expect our insights to\nprovide large wins for hashing models applied to other information retrieval\nhashing tasks as well.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 02:11:18 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Loncaric", "Martin", ""], ["Liu", "Bowei", ""], ["Weber", "Ryan", ""]]}, {"id": "1802.03102", "submitter": "Themistoklis Mavridis", "authors": "Themis Mavridis, Pablo Estevez, Lucas Bernardi", "title": "Learning to Match", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Booking.com is a virtual two-sided marketplace where guests and accommodation\nproviders are the two distinct stakeholders. They meet to satisfy their\nrespective and different goals. Guests want to be able to choose accommodations\nfrom a huge and diverse inventory, fast and reliably within their requirements\nand constraints. Accommodation providers desire to reach a reliable and large\nmarket that maximizes their revenue. Finding the best accommodation for the\nguests, a problem typically addressed by the recommender systems community, and\nfinding the best audience for the accommodation providers, are key pieces of a\ngood platform. This work describes how Booking.com extends such approach,\nenabling the guests themselves to find the best accommodation by helping them\nto discover their needs and restrictions, what the market can actually offer,\nreinforcing good decisions, discouraging bad ones, etc. turning the platform\ninto a decision process advisor, as opposed to a decision maker. Booking.com\nimplements this idea with hundreds of Machine Learned Models, all of them\nvalidated through rigorous Randomized Controlled Experiments. We further\nelaborate on model types, techniques, methodological issues and challenges that\nwe have faced.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 02:13:00 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Mavridis", "Themis", ""], ["Estevez", "Pablo", ""], ["Bernardi", "Lucas", ""]]}, {"id": "1802.03597", "submitter": "Ibrahim Riza Hallac", "authors": "Galip Aydin, Ibrahim Riza Hallac", "title": "Document Classification Using Distributed Machine Learning", "comments": null, "journal-ref": null, "doi": "10.15224/978-1-63248-044-6-129", "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the performance and success rates of Na\\\"ive\nBayes Classification Algorithm for automatic classification of Turkish news\ninto predetermined categories like economy, life, health etc. We use Apache Big\nData technologies such as Hadoop, HDFS, Spark and Mahout, and apply these\ndistributed technologies to Machine Learning.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 14:18:51 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Aydin", "Galip", ""], ["Hallac", "Ibrahim Riza", ""]]}, {"id": "1802.03793", "submitter": "Justin Sybrandt", "authors": "Justin Sybrandt, Michael Shtutman and Ilya Safro", "title": "Large-Scale Validation of Hypothesis Generation Systems via Candidate\n  Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first step of many research projects is to define and rank a short list\nof candidates for study. In the modern rapidity of scientific progress, some\nturn to automated hypothesis generation (HG) systems to aid this process. These\nsystems can identify implicit or overlooked connections within a large\nscientific corpus, and while their importance grows alongside the pace of\nscience, they lack thorough validation. Without any standard numerical\nevaluation method, many validate general-purpose HG systems by rediscovering a\nhandful of historical findings, and some wishing to be more thorough may run\nlaboratory experiments based on automatic suggestions. These methods are\nexpensive, time consuming, and cannot scale. Thus, we present a numerical\nevaluation framework for the purpose of validating HG systems that leverages\nthousands of validation hypotheses. This method evaluates a HG system by its\nability to rank hypotheses by plausibility; a process reminiscent of human\ncandidate selection. Because HG systems do not produce a ranking criteria,\nspecifically those that produce topic models, we additionally present novel\nmetrics to quantify the plausibility of hypotheses given topic model system\noutput. Finally, we demonstrate that our proposed validation method aligns with\nreal-world research goals by deploying our method within Moliere, our recent\ntopic-driven HG system, in order to automatically generate a set of candidate\ngenes related to HIV-associated neurodegenerative disease (HAND). By performing\nlaboratory experiments based on this candidate set, we discover a new\nconnection between HAND and Dead Box RNA Helicase 3 (DDX3). Reproducibility:\ncode, validation data, and results can be found at\nsybrandt.com/2018/validation.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 19:04:49 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 17:35:22 GMT"}, {"version": "v3", "created": "Fri, 19 Oct 2018 15:21:13 GMT"}, {"version": "v4", "created": "Wed, 5 Dec 2018 21:06:44 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Sybrandt", "Justin", ""], ["Shtutman", "Michael", ""], ["Safro", "Ilya", ""]]}, {"id": "1802.03855", "submitter": "Feichen Shen PhD", "authors": "Feichen Shen, Yugyung Lee", "title": "MedTQ: Dynamic Topic Discovery and Query Generation for Medical\n  Ontologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biomedical ontology refers to a shared conceptualization for a biomedical\ndomain of interest that has vastly improved data management and data sharing\nthrough the open data movement. The rapid growth and availability of biomedical\ndata make it impractical and computationally expensive to perform manual\nanalysis and query processing with the large scale ontologies. The lack of\nability in analyzing ontologies from such a variety of sources, and supporting\nknowledge discovery for clinical practice and biomedical research should be\novercome with new technologies. In this study, we developed a Medical Topic\ndiscovery and Query generation framework (MedTQ), which was composed by a\nseries of approaches and algorithms. A predicate neighborhood pattern-based\napproach introduced has the ability to compute the similarity of predicates\n(relations) in ontologies. Given a predicate similarity metric, machine\nlearning algorithms have been developed for automatic topic discovery and query\ngeneration. The topic discovery algorithm, called the hierarchical K-Means\nalgorithm was designed by extending an existing supervised algorithm (K-means\nclustering) for the construction of a topic hierarchy. In the hierarchical\nK-Means algorithm, a level-by-level optimization strategy was selected for\nconsistent with the strongly association between elements within a topic.\nAutomatic query generation was facilitated for discovered topic that could be\nguided users for interactive query design and processing. Evaluation was\nconducted to generate topic hierarchy for DrugBank ontology as a case study.\nResults demonstrated that the MedTQ framework can enhance knowledge discovery\nby capturing underlying structures from domain specific data and ontologies.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 01:22:10 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Shen", "Feichen", ""], ["Lee", "Yugyung", ""]]}, {"id": "1802.03971", "submitter": "Deepak Kumar Gupta", "authors": "Deepak Kumar Gupta and Shruti Goyal", "title": "Email Classification into Relevant Category Using Neural Networks", "comments": "9 Pages, 6 figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the real world, many online shopping websites or service provider have\nsingle email-id where customers can send their query, concern etc. At the\nback-end service provider receive million of emails every week, how they can\nidentify which email is belonged of a particular department? This paper\npresents an artificial neural network (ANN) model that is used to solve this\nproblem and experiments are carried out on user personal Gmail emails datasets.\nThis problem can be generalised as typical Text Classification or\nCategorization.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 11:03:00 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Gupta", "Deepak Kumar", ""], ["Goyal", "Shruti", ""]]}, {"id": "1802.04023", "submitter": "L. Elisa Celis", "authors": "L. Elisa Celis, Vijay Keswani, Damian Straszak, Amit Deshpande, Tarun\n  Kathuria and Nisheeth K. Vishnoi", "title": "Fair and Diverse DPP-based Data Summarization", "comments": "A short version of this paper appeared in the workshop FAT/ML 2016 -\n  arXiv:1610.07183", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling methods that choose a subset of the data proportional to its\ndiversity in the feature space are popular for data summarization. However,\nrecent studies have noted the occurrence of bias (under- or over-representation\nof a certain gender or race) in such data summarization methods. In this paper\nwe initiate a study of the problem of outputting a diverse and fair summary of\na given dataset. We work with a well-studied determinantal measure of diversity\nand corresponding distributions (DPPs) and present a framework that allows us\nto incorporate a general class of fairness constraints into such distributions.\nComing up with efficient algorithms to sample from these constrained\ndeterminantal distributions, however, suffers from a complexity barrier and we\npresent a fast sampler that is provably good when the input vectors satisfy a\nnatural property. Our experimental results on a real-world and an image dataset\nshow that the diversity of the samples produced by adding fairness constraints\nis not too far from the unconstrained case, and we also provide a theoretical\nexplanation of it.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 13:12:43 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Celis", "L. Elisa", ""], ["Keswani", "Vijay", ""], ["Straszak", "Damian", ""], ["Deshpande", "Amit", ""], ["Kathuria", "Tarun", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1802.04068", "submitter": "David Lillis", "authors": "Weinan Huang, Junyi Chen, Lei Meng, David Lillis", "title": "Towards an Open Science Platform for the Evaluation of Data Fusion", "comments": "Proceedings of the 3rd IEEE International Conference on Cloud\n  Computing and Big Data Analysis (ICCCBDA 2018), Chengdu, China, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining the results of different search engines in order to improve upon\ntheir performance has been the subject of many research papers. This has become\nknown as the \"Data Fusion\" task, and has great promise in dealing with the vast\nquantity of unstructured textual data that is a feature of many Big Data\nscenarios. However, no universally-accepted evaluation methodology has emerged\nin the community. This makes it difficult to make meaningful comparisons\nbetween the various proposed techniques from reading the literature alone.\nVariations in the datasets, metrics, and baseline results have all contributed\nto this difficulty.\n  This paper argues that a more unified approach is required, and that a\ncentralised software platform should be developed to aid researchers in making\ncomparisons between their algorithms and others. The desirable qualities of\nsuch a system have been identified and proposed, and an early prototype has\nbeen developed. Re-implementing algorithms published by other researchers is a\ngreat burden on those proposing new techniques. The prototype system has the\npotential to greatly reduce this burden and thus encourage more comparable\nresults being generated and published more easily.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 14:33:56 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Huang", "Weinan", ""], ["Chen", "Junyi", ""], ["Meng", "Lei", ""], ["Lillis", "David", ""]]}, {"id": "1802.04538", "submitter": "Mayank Singh", "authors": "Mayank Singh, Rajdeep Sarkar, Atharva Vyas, Pawan Goyal, Animesh\n  Mukherjee and Soumen Chakrabarti", "title": "Automated Early Leaderboard Generation From Comparative Tables", "comments": "Accepted at ECIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A leaderboard is a tabular presentation of performance scores of the best\ncompeting techniques that address a specific scientific problem. Manually\nmaintained leaderboards take time to emerge, which induces a latency in\nperformance discovery and meaningful comparison. This can delay dissemination\nof best practices to non-experts and practitioners. Regarding papers as proxies\nfor techniques, we present a new system to automatically discover and maintain\nleaderboards in the form of partial orders between papers, based on performance\nreported therein. In principle, a leaderboard depends on the task, data set,\nother experimental settings, and the choice of performance metrics. Often there\nare also tradeoffs between different metrics. Thus, leaderboard discovery is\nnot just a matter of accurately extracting performance numbers and comparing\nthem. In fact, the levels of noise and uncertainty around performance\ncomparisons are so large that reliable traditional extraction is infeasible. We\nmitigate these challenges by using relatively cleaner, structured parts of the\npapers, e.g., performance tables. We propose a novel performance improvement\ngraph with papers as nodes, where edges encode noisy performance comparison\ninformation extracted from tables. Every individual performance edge is\nextracted from a table with citations to other papers. These extractions\nresemble (noisy) outcomes of 'matches' in an incomplete tournament. We propose\nseveral approaches to rank papers from these noisy 'match' outcomes. We show\nthat our ranking scheme can reproduce various manually curated leaderboards\nvery well. Using widely-used lists of state-of-the-art papers in 27 areas of\nComputer Science, we demonstrate that our system produces very reliable\nrankings.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 10:18:13 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 02:33:30 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Singh", "Mayank", ""], ["Sarkar", "Rajdeep", ""], ["Vyas", "Atharva", ""], ["Goyal", "Pawan", ""], ["Mukherjee", "Animesh", ""], ["Chakrabarti", "Soumen", ""]]}, {"id": "1802.04606", "submitter": "Shuai Zhang", "authors": "Shuai Zhang, Lina Yao, Yi Tay, Xiwei Xu, Xiang Zhang, Liming Zhu", "title": "Metric Factorization: Recommendation beyond Matrix Factorization", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, matrix factorization has been extensively researched and\nhas become one of the most popular techniques for personalized recommendations.\nNevertheless, the dot product adopted in matrix factorization based recommender\nmodels does not satisfy the inequality property, which may limit their\nexpressiveness and lead to sub-optimal solutions. To overcome this problem, we\npropose a novel recommender technique dubbed as {\\em Metric Factorization}. We\nassume that users and items can be placed in a low dimensional space and their\nexplicit closeness can be measured using Euclidean distance which satisfies the\ninequality property. To demonstrate its effectiveness, we further designed two\nvariants of metric factorization with one for rating estimation and the other\nfor personalized item ranking. Extensive experiments on a number of real-world\ndatasets show that our approach outperforms existing state-of-the-art by a\nlarge margin on both rating prediction and item ranking tasks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 13:19:42 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 04:56:27 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Zhang", "Shuai", ""], ["Yao", "Lina", ""], ["Tay", "Yi", ""], ["Xu", "Xiwei", ""], ["Zhang", "Xiang", ""], ["Zhu", "Liming", ""]]}, {"id": "1802.04675", "submitter": "Parth Mehta", "authors": "Parth Mehta, Gaurav Arora, Prasenjit Majumder", "title": "Attention based Sentence Extraction from Scientific Articles using\n  Pseudo-Labeled data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a weakly supervised sentence extraction technique\nfor identifying important sentences in scientific papers that are worthy of\ninclusion in the abstract. We propose a new attention based deep learning\narchitecture that jointly learns to identify important content, as well as the\ncue phrases that are indicative of summary worthy sentences. We propose a new\ncontext embedding technique for determining the focus of a given paper using\ntopic models and use it jointly with an LSTM based sequence encoder to learn\nattention weights across the sentence words. We use a collection of articles\npublicly available through ACL anthology for our experiments. Our system\nachieves a performance that is better, in terms of several ROUGE metrics, as\ncompared to several state of art extractive techniques. It also generates more\ncoherent summaries and preserves the overall structure of the document.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 15:13:28 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Mehta", "Parth", ""], ["Arora", "Gaurav", ""], ["Majumder", "Prasenjit", ""]]}, {"id": "1802.04883", "submitter": "Yifeng Gao", "authors": "Yifeng Gao and Jessica Lin", "title": "Efficient Discovery of Variable-length Time Series Motifs with Large\n  Length Range in Million Scale Time Series", "comments": "Support Page: https://sites.google.com/site/himeicdm/", "journal-ref": "ICDM 2017", "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting repeated variable-length patterns, also called variable-length\nmotifs, has received a great amount of attention in recent years. Current\nstate-of-the-art algorithm utilizes fixed-length motif discovery algorithm as a\nsubroutine to enumerate variable-length motifs. As a result, it may take hours\nor days to execute when enumeration range is large. In this work, we introduce\nan approximate algorithm called HierarchIcal based Motif Enumeration (HIME) to\ndetect variable-length motifs with a large enumeration range in million-scale\ntime series. We show in the experiments that the scalability of the proposed\nalgorithm is significantly better than that of the state-of-the-art algorithm.\nMoreover, the motif length range detected by HIME is considerably larger than\nprevious sequence-matching based approximate variable-length motif discovery\napproach. We demonstrate that HIME can efficiently detect meaningful\nvariable-length motifs in long, real world time series.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 22:34:38 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Gao", "Yifeng", ""], ["Lin", "Jessica", ""]]}, {"id": "1802.05057", "submitter": "Suman Kalyan Maity", "authors": "Suman Kalyan Maity, Ayush Kumar, Ankan Mullick, Vishnu Choudhary,\n  Animesh Mukherjee", "title": "Understanding Book Popularity on Goodreads", "comments": "5 pages, 4 Tables, GROUP '18", "journal-ref": null, "doi": "10.1145/3148330.3154512", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Goodreads has launched the Readers Choice Awards since 2009 where users are\nable to nominate/vote books of their choice, released in the given year. In\nthis work, we question if the number of votes that a book would receive (aka\nthe popularity of the book) can be predicted based on the characteristics of\nvarious entities on Goodreads. We are successful in predicting the popularity\nof the books with high prediction accuracy (correlation coefficient ~0.61) and\nlow RMSE (~1.25). User engagement and author's prestige are found to be crucial\nfactors for book popularity.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 12:04:18 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Maity", "Suman Kalyan", ""], ["Kumar", "Ayush", ""], ["Mullick", "Ankan", ""], ["Choudhary", "Vishnu", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "1802.05121", "submitter": "Shashank Gupta", "authors": "Shashank Gupta, Manish Gupta, Vasudeva Varma, Sachin Pawar, Nitin\n  Ramrakhiyani, Girish K. Palshikar", "title": "Co-training for Extraction of Adverse Drug Reaction Mentions from Tweets", "comments": "Accepted at ECIR18 as short paper (6 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adverse drug reactions (ADRs) are one of the leading causes of mortality in\nhealth care. Current ADR surveillance systems are often associated with a\nsubstantial time lag before such events are officially published. On the other\nhand, online social media such as Twitter contain information about ADR events\nin real-time, much before any official reporting. Current state-of-the-art\nmethods in ADR mention extraction use Recurrent Neural Networks (RNN), which\ntypically need large labeled corpora. Towards this end, we propose a\nsemi-supervised method based on co-training which can exploit a large pool of\nunlabeled tweets to augment the limited supervised training data, and as a\nresult enhance the performance. Experiments with 0.1M tweets show that the\nproposed approach outperforms the state-of-the-art methods for the ADR mention\nextraction task by 5% in terms of F1 score.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 14:47:56 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Gupta", "Shashank", ""], ["Gupta", "Manish", ""], ["Varma", "Vasudeva", ""], ["Pawar", "Sachin", ""], ["Ramrakhiyani", "Nitin", ""], ["Palshikar", "Girish K.", ""]]}, {"id": "1802.05130", "submitter": "Shashank Gupta", "authors": "Shashank Gupta, Manish Gupta, Vasudeva Varma, Sachin Pawar, Nitin\n  Ramrakhiyani and Girish K. Palshikar", "title": "Multi-Task Learning for Extraction of Adverse Drug Reaction Mentions\n  from Tweets", "comments": "Accepted at ECIR18 as full paper (12 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adverse drug reactions (ADRs) are one of the leading causes of mortality in\nhealth care. Current ADR surveillance systems are often associated with a\nsubstantial time lag before such events are officially published. On the other\nhand, online social media such as Twitter contain information about ADR events\nin real-time, much before any official reporting. Current state-of-the-art in\nADR mention extraction uses Recurrent Neural Networks (RNN), which typically\nneed large labeled corpora. Towards this end, we propose a multi-task learning\nbased method which can utilize a similar auxiliary task (adverse drug event\ndetection) to enhance the performance of the main task, i.e., ADR extraction.\nFurthermore, in the absence of auxiliary task dataset, we propose a novel joint\nmulti-task learning method to automatically generate weak supervision dataset\nfor the auxiliary task when a large pool of unlabeled tweets is available.\nExperiments with 0.48M tweets show that the proposed approach outperforms the\nstate-of-the-art methods for the ADR mention extraction task by 7.2% in terms\nof F1 score.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 14:53:06 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Gupta", "Shashank", ""], ["Gupta", "Manish", ""], ["Varma", "Vasudeva", ""], ["Pawar", "Sachin", ""], ["Ramrakhiyani", "Nitin", ""], ["Palshikar", "Girish K.", ""]]}, {"id": "1802.05382", "submitter": "Himan Abdollahpouri", "authors": "Himan Abdollahpouri, Robin Burke, Bamshad Mobasher", "title": "Popularity-Aware Item Weighting for Long-Tail Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recommender systems suffer from the popularity bias problem: popular\nitems are being recommended frequently while less popular, niche products, are\nrecommended rarely if not at all. However, those ignored products are exactly\nthe products that businesses need to find customers for and their\nrecommendations would be more beneficial. In this paper, we examine an item\nweighting approach to improve long-tail recommendation. Our approach works as a\nsimple yet powerful add-on to existing recommendation algorithms for making a\ntunable trade-off between accuracy and long-tail coverage.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 01:53:59 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 19:49:20 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2018 14:55:44 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Abdollahpouri", "Himan", ""], ["Burke", "Robin", ""], ["Mobasher", "Bamshad", ""]]}, {"id": "1802.05453", "submitter": "Giuseppe Mangioni", "authors": "Marco Buzzanca, Vincenza Carchiolo, Alessandro Longheu, Michele\n  Malgeri, Giuseppe Mangioni", "title": "Black Hole Metric: Overcoming the PageRank Normalization Problem", "comments": "21 pages, 7 figures", "journal-ref": "Information Sciences, Volume 438, 2018, Elsevier", "doi": "10.1016/j.ins.2018.01.033", "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In network science, there is often the need to sort the graph nodes. While\nthe sorting strategy may be different, in general sorting is performed by\nexploiting the network structure. In particular, the metric PageRank has been\nused in the past decade in different ways to produce a ranking based on how\nmany neighbors point to a specific node. PageRank is simple, easy to compute\nand effective in many applications, however it comes with a price: as PageRank\nis an application of the random walker, the arc weights need to be normalized.\nThis normalization, while necessary, introduces a series of unwanted\nside-effects. In this paper, we propose a generalization of PageRank named\nBlack Hole Metric which mitigates the problem. We devise a scenario in which\nthe side-effects are particularily impactful on the ranking, test the new\nmetric in both real and synthetic networks, and show the results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 09:52:02 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Buzzanca", "Marco", ""], ["Carchiolo", "Vincenza", ""], ["Longheu", "Alessandro", ""], ["Malgeri", "Michele", ""], ["Mangioni", "Giuseppe", ""]]}, {"id": "1802.05814", "submitter": "Dawen Liang", "authors": "Dawen Liang, Rahul G. Krishnan, Matthew D. Hoffman, Tony Jebara", "title": "Variational Autoencoders for Collaborative Filtering", "comments": "10 pages, 3 figures. WWW 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend variational autoencoders (VAEs) to collaborative filtering for\nimplicit feedback. This non-linear probabilistic model enables us to go beyond\nthe limited modeling capacity of linear factor models which still largely\ndominate collaborative filtering research.We introduce a generative model with\nmultinomial likelihood and use Bayesian inference for parameter estimation.\nDespite widespread use in language modeling and economics, the multinomial\nlikelihood receives less attention in the recommender systems literature. We\nintroduce a different regularization parameter for the learning objective,\nwhich proves to be crucial for achieving competitive performance. Remarkably,\nthere is an efficient way to tune the parameter using annealing. The resulting\nmodel and learning algorithm has information-theoretic connections to maximum\nentropy discrimination and the information bottleneck principle. Empirically,\nwe show that the proposed approach significantly outperforms several\nstate-of-the-art baselines, including two recently-proposed neural network\napproaches, on several real-world datasets. We also provide extended\nexperiments comparing the multinomial likelihood with other commonly used\nlikelihood functions in the latent factor collaborative filtering literature\nand show favorable results. Finally, we identify the pros and cons of employing\na principled Bayesian inference approach and characterize settings where it\nprovides the most significant improvements.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 01:41:20 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Liang", "Dawen", ""], ["Krishnan", "Rahul G.", ""], ["Hoffman", "Matthew D.", ""], ["Jebara", "Tony", ""]]}, {"id": "1802.05895", "submitter": "Kevin Jasberg", "authors": "Kevin Jasberg, Sergej Sizov", "title": "Dealing with Uncertainties in User Feedback: Strategies Between Denying\n  and Accepting", "comments": "Human Uncertainty, Noise, Magic Barrier, Ranking Error, User Feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latest research revealed a considerable lack of reliability within user\nfeedback and discussed striking impacts for the assessment of adaptive web\nsystems and content personalisation approaches, e.g. ranking errors, systematic\nbiases to accuracy metrics as well as its natural offset (the magic barrier).\nIn order to perform holistic assessments and to improve web systems, a variety\nof strategies have been proposed to deal with this so-called human uncertainty.\nIn this contribution we discuss the most relevant strategies to handle\nuncertain feedback and demonstrate that these approaches are more or less\nineffective to fulfil their objectives. In doing so, we consider human\nuncertainty within a purely probabilistic framework and utilise hypothesis\ntesting as well as a generalisation of the magic barrier to compare the effects\nof recently proposed algorithms. On this basis we recommend a novel strategy of\nacceptance which turns away from mere filtering and discuss potential benefits\nfor the community of the WWW.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 11:15:01 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Jasberg", "Kevin", ""], ["Sizov", "Sergej", ""]]}, {"id": "1802.05929", "submitter": "Jesse Anderton", "authors": "Jesse Anderton, Pavel Metrikov, Virgil Pavlu, Javed Aslam", "title": "Measuring Human-perceived Similarity in Heterogeneous Collections", "comments": "Reviewed but not accepted for KDD 2014; not resubmitted elsewhere", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for estimating the similarity between objects such as\nmovies or foods whose proper representation depends on human perception. Our\ntechnique combines a modest number of human similarity assessments to infer a\npairwise similarity function between the objects. This similarity function\ncaptures some human notion of similarity which may be difficult or impossible\nto automatically extract, such as which movie from a collection would be a\nbetter substitute when the desired one is unavailable. In contrast to prior\ntechniques, our method does not assume that all similarity questions on the\ncollection can be answered or that all users perceive similarity in the same\nway. When combined with a user model, we find how each assessor's tastes vary,\naffecting their perception of similarity.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 13:37:45 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Anderton", "Jesse", ""], ["Metrikov", "Pavel", ""], ["Pavlu", "Virgil", ""], ["Aslam", "Javed", ""]]}, {"id": "1802.05948", "submitter": "Umutcan \\c{S}im\\c{s}ek", "authors": "Boran Taylan Balc{\\i}, Umutcan \\c{S}im\\c{s}ek, Elias K\\\"arle and\n  Dieter Fensel", "title": "Analysis of Schema.org Usage in the Tourism Domain", "comments": "Presented in ENTER 2018 conference in J\\\"onkoping", "journal-ref": "e-Review of Tourism Research, ENTER 2018: Volume 9 Research Notes", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schema.org is an initiative founded in 2011 by the four-big search engine\nBing, Google, Yahoo!, and Yandex. The goal of the initiative is to publish and\nmaintain the schema.org vocabulary, in order to facilitate the publication of\nstructured data on the web which can enable the implementation of automated\nagents like intelligent personal assistants and chatbots. In this paper, the\nusage of schema.org in tourism domain between years 2013 and 2016 is analysed.\nThe analysis shows the adoption of schema.org, which indicates how well the\ntourism sector is prepared for the web that targets automated agents. The\nresults have shown that the adoption of schema.org type and properties is grown\nover the years. While the US is dominating the annotation numbers, a drastic\ndrop is observed for the proportion of the US in 2016. Poorly rated businesses\nare encountered more in 2016 results in comparison to previous years.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 14:31:16 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Balc\u0131", "Boran Taylan", ""], ["\u015eim\u015fek", "Umutcan", ""], ["K\u00e4rle", "Elias", ""], ["Fensel", "Dieter", ""]]}, {"id": "1802.06007", "submitter": "Catalin Stoean", "authors": "Daniel Lichtblau, Catalin Stoean", "title": "Authorship Attribution Using the Chaos Game Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Chaos Game Representation, a method for creating images from nucleotide\nsequences, is modified to make images from chunks of text documents. Machine\nlearning methods are then applied to train classifiers based on authorship.\nExperiments are conducted on several benchmark data sets in English, including\nthe widely used Federalist Papers, and one in Portuguese. Validation results\nfor the trained classifiers are competitive with the best methods in prior\nliterature. The methodology is also successfully applied for text\ncategorization with encouraging results. One classifier method is moreover seen\nto hold promise for the task of digital fingerprinting.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 19:44:24 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Lichtblau", "Daniel", ""], ["Stoean", "Catalin", ""]]}, {"id": "1802.06095", "submitter": "Saurabh Verma", "authors": "Saurabh Agrawal, Saurabh Verma, Gowtham Atluri, Anuj Karpatne, Stefan\n  Liess, Angus Macdonald III, Snigdhansu Chatterjee, Vipin Kumar", "title": "Mining Sub-Interval Relationships In Time Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-series data is being increasingly collected and stud- ied in several\nareas such as neuroscience, climate science, transportation, and social media.\nDiscovery of complex patterns of relationships between individual time-series,\nusing data-driven approaches can improve our understanding of real-world\nsystems. While traditional approaches typically study relationships between two\nentire time series, many interesting relationships in real-world applications\nexist in small sub-intervals of time while remaining absent or feeble during\nother sub-intervals. In this paper, we define the notion of a sub-interval\nrelationship (SIR) to capture inter- actions between two time series that are\nprominent only in certain sub-intervals of time. We propose a novel and\nefficient approach to find most interesting SIR in a pair of time series. We\nevaluate our proposed approach on two real-world datasets from climate science\nand neuroscience domain and demonstrated the scalability and computational\nefficiency of our proposed approach. We further evaluated our discovered SIRs\nbased on a randomization based procedure. Our results indicated the existence\nof several such relationships that are statistically significant, some of which\nwere also found to have physical interpretation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 19:30:19 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Agrawal", "Saurabh", ""], ["Verma", "Saurabh", ""], ["Atluri", "Gowtham", ""], ["Karpatne", "Anuj", ""], ["Liess", "Stefan", ""], ["Macdonald", "Angus", "III"], ["Chatterjee", "Snigdhansu", ""], ["Kumar", "Vipin", ""]]}, {"id": "1802.06159", "submitter": "Shuo Zhang", "authors": "Shuo Zhang and Krisztian Balog", "title": "Ad Hoc Table Retrieval using Semantic Similarity", "comments": "The web conference 2018 (WWW'18)", "journal-ref": null, "doi": "10.1145/3178876.3186067", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and address the problem of ad hoc table retrieval: answering a\nkeyword query with a ranked list of tables. This task is not only interesting\non its own account, but is also being used as a core component in many other\ntable-based information access scenarios, such as table completion or table\nmining. The main novel contribution of this work is a method for performing\nsemantic matching between queries and tables. Specifically, we (i) represent\nqueries and tables in multiple semantic spaces (both discrete sparse and\ncontinuous dense vector representations) and (ii) introduce various similarity\nmeasures for matching those semantic representations. We consider all possible\ncombinations of semantic representations and similarity measures and use these\nas features in a supervised learning model. Using a purpose-built test\ncollection based on Wikipedia tables, we demonstrate significant and\nsubstantial improvements over a state-of-the-art baseline.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 23:20:49 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 08:31:25 GMT"}, {"version": "v3", "created": "Thu, 8 Mar 2018 12:42:14 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Zhang", "Shuo", ""], ["Balog", "Krisztian", ""]]}, {"id": "1802.06185", "submitter": "Amrith Krishna", "authors": "Vikas Reddy, Amrith Krishna, Vishnu Dutt Sharma, Prateek Gupta,\n  Vineeth M R, Pawan Goyal", "title": "Building a Word Segmenter for Sanskrit Overnight", "comments": "The work is accepted at LREC 2018, Miyazaki, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an abundance of digitised texts available in Sanskrit. However, the\nword segmentation task in such texts are challenging due to the issue of\n'Sandhi'. In Sandhi, words in a sentence often fuse together to form a single\nchunk of text, where the word delimiter vanishes and sounds at the word\nboundaries undergo transformations, which is also reflected in the written\ntext. Here, we propose an approach that uses a deep sequence to sequence\n(seq2seq) model that takes only the sandhied string as the input and predicts\nthe unsandhied string. The state of the art models are linguistically involved\nand have external dependencies for the lexical and morphological analysis of\nthe input. Our model can be trained \"overnight\" and be used for production. In\nspite of the knowledge lean approach, our system preforms better than the\ncurrent state of the art by gaining a percentage increase of 16.79 % than the\ncurrent state of the art.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 04:05:36 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Reddy", "Vikas", ""], ["Krishna", "Amrith", ""], ["Sharma", "Vishnu Dutt", ""], ["Gupta", "Prateek", ""], ["R", "Vineeth M", ""], ["Goyal", "Pawan", ""]]}, {"id": "1802.06290", "submitter": "Majid Ghasemi-Gol", "authors": "Majid Ghasemi-Gol and Pedro Szekely", "title": "TabVec: Table Vectors for Classification of Web Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are hundreds of millions of tables in Web pages that contain useful\ninformation for many applications. Leveraging data within these tables is\ndifficult because of the wide variety of structures, formats and data encoded\nin these tables. TabVec is an unsupervised method to embed tables into a vector\nspace to support classification of tables into categories (entity, relational,\nmatrix, list, and non-data) with minimal user intervention. TabVec deploys\nsyntax and semantics of table cells, and embeds the structure of tables in a\ntable vector space. This enables superior classification of tables even in the\nabsence of domain annotations. Our evaluations in four real world domains show\nthat TabVec improves classification accuracy by more than 20% compared to three\nstate of the art systems, and that those systems require significant in domain\ntraining to achieve good results.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 20:45:03 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Ghasemi-Gol", "Majid", ""], ["Szekely", "Pedro", ""]]}, {"id": "1802.06306", "submitter": "Ziming Li", "authors": "Ziming Li, Julia Kiseleva, Alekh Agarwal, Maarten de Rijke", "title": "Learning Data-Driven Objectives to Optimize Interactive Systems", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective optimization is essential for interactive systems to provide a\nsatisfactory user experience. However, it is often challenging to find an\nobjective to optimize for. Generally, such objectives are manually crafted and\nrarely capture complex user needs in an accurate manner. We propose an approach\nthat infers the objective directly from observed user interactions. These\ninferences can be made regardless of prior knowledge and across different types\nof user behavior. We introduce interactive system optimization, a novel\nalgorithm that uses these inferred objectives for optimization. Our main\ncontribution is a new general principled approach to optimizing interactive\nsystems using data-driven objectives. We demonstrate the high effectiveness of\ninteractive system optimization over several simulations.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 23:04:15 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 09:09:57 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 17:24:19 GMT"}, {"version": "v4", "created": "Wed, 2 May 2018 14:45:08 GMT"}, {"version": "v5", "created": "Tue, 8 May 2018 15:33:21 GMT"}, {"version": "v6", "created": "Tue, 16 Oct 2018 15:54:49 GMT"}, {"version": "v7", "created": "Wed, 17 Oct 2018 10:25:51 GMT"}, {"version": "v8", "created": "Fri, 13 Dec 2019 22:11:21 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Li", "Ziming", ""], ["Kiseleva", "Julia", ""], ["Agarwal", "Alekh", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1802.06398", "submitter": "Evgeny Frolov", "authors": "Evgeny Frolov and Ivan Oseledets", "title": "HybridSVD: When Collaborative Information is Not Enough", "comments": "accepted as a long paper at ACM RecSys 2019; 9 pages, 2 figures, 2\n  tables", "journal-ref": null, "doi": "10.1145/3298689.3347055", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new hybrid algorithm that allows incorporating both user and\nitem side information within the standard collaborative filtering technique.\nOne of its key features is that it naturally extends a simple PureSVD approach\nand inherits its unique advantages, such as highly efficient Lanczos-based\noptimization procedure, simplified hyper-parameter tuning and a quick\nfolding-in computation for generating recommendations instantly even in highly\ndynamic online environments. The algorithm utilizes a generalized formulation\nof the singular value decomposition, which adds flexibility to the solution and\nallows imposing the desired structure on its latent space. Conveniently, the\nresulting model also admits an efficient and straightforward solution for the\ncold start scenario. We evaluate our approach on a diverse set of datasets and\nshow its superiority over similar classes of hybrid models.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 16:39:01 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 13:45:53 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 18:30:37 GMT"}, {"version": "v4", "created": "Tue, 13 Aug 2019 10:03:18 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Frolov", "Evgeny", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1802.06466", "submitter": "Ying Shan", "authors": "Ying Shan and Jian Jiao and Jie Zhu and JC Mao", "title": "Recurrent Binary Embedding for GPU-Enabled Exhaustive Retrieval from\n  Billion-Scale Semantic Vectors", "comments": "15 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid advances in GPU hardware and multiple areas of Deep Learning open up a\nnew opportunity for billion-scale information retrieval with exhaustive search.\nBuilding on top of the powerful concept of semantic learning, this paper\nproposes a Recurrent Binary Embedding (RBE) model that learns compact\nrepresentations for real-time retrieval. The model has the unique ability to\nrefine a base binary vector by progressively adding binary residual vectors to\nmeet the desired accuracy. The refined vector enables efficient implementation\nof exhaustive similarity computation with bit-wise operations, followed by a\nnear- lossless k-NN selection algorithm, also proposed in this paper. The\nproposed algorithms are integrated into an end-to-end multi-GPU system that\nretrieves thousands of top items from over a billion candidates in real-time.\nThe RBE model and the retrieval system were evaluated with data from a major\npaid search engine. When measured against the state-of-the-art model for binary\nrepresentation and the full precision model for semantic embedding, RBE\nsignificantly outperformed the former, and filled in over 80% of the AUC gap\nin-between. Experiments comparing with our production retrieval system also\ndemonstrated superior performance. While the primary focus of this paper is to\nbuild RBE based on a particular class of semantic models, generalizing to other\ntypes is straightforward, as exemplified by two different models at the end of\nthe paper.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 23:02:58 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Shan", "Ying", ""], ["Jiao", "Jian", ""], ["Zhu", "Jie", ""], ["Mao", "JC", ""]]}, {"id": "1802.06501", "submitter": "Xiangyu Zhao", "authors": "Xiangyu Zhao and Liang Zhang and Zhuoye Ding and Long Xia and Jiliang\n  Tang and Dawei Yin", "title": "Recommendations with Negative Feedback via Pairwise Deep Reinforcement\n  Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1801.00209", "journal-ref": null, "doi": "10.1145/3219819.3219886", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems play a crucial role in mitigating the problem of\ninformation overload by suggesting users' personalized items or services. The\nvast majority of traditional recommender systems consider the recommendation\nprocedure as a static process and make recommendations following a fixed\nstrategy. In this paper, we propose a novel recommender system with the\ncapability of continuously improving its strategies during the interactions\nwith users. We model the sequential interactions between users and a\nrecommender system as a Markov Decision Process (MDP) and leverage\nReinforcement Learning (RL) to automatically learn the optimal strategies via\nrecommending trial-and-error items and receiving reinforcements of these items\nfrom users' feedback. Users' feedback can be positive and negative and both\ntypes of feedback have great potentials to boost recommendations. However, the\nnumber of negative feedback is much larger than that of positive one; thus\nincorporating them simultaneously is challenging since positive feedback could\nbe buried by negative one. In this paper, we develop a novel approach to\nincorporate them into the proposed deep recommender system (DEERS) framework.\nThe experimental results based on real-world e-commerce data demonstrate the\neffectiveness of the proposed framework. Further experiments have been\nconducted to understand the importance of both positive and negative feedback\nin recommendations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 02:30:10 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 11:49:04 GMT"}, {"version": "v3", "created": "Fri, 10 Aug 2018 02:33:08 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Zhao", "Xiangyu", ""], ["Zhang", "Liang", ""], ["Ding", "Zhuoye", ""], ["Xia", "Long", ""], ["Tang", "Jiliang", ""], ["Yin", "Dawei", ""]]}, {"id": "1802.06565", "submitter": "Andrew Collins Mr", "authors": "Andrew Collins, Dominika Tkaczyk, Akiko Aizawa, Joeran Beel", "title": "A Study of Position Bias in Digital Library Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Position bias\" describes the tendency of users to interact with items on top\nof a list with higher probability than with items at a lower position in the\nlist, regardless of the items' actual relevance. In the domain of recommender\nsystems, particularly recommender systems in digital libraries, position bias\nhas received little attention. We conduct a study in a real-world recommender\nsystem that delivered ten million related-article recommendations to the users\nof the digital library Sowiport, and the reference manager JabRef.\nRecommendations were randomly chosen to be shuffled or non-shuffled, and we\ncompared click-through rate (CTR) for each rank of the recommendations.\nAccording to our analysis, the CTR for the highest rank in the case of Sowiport\nis 53% higher than expected in a hypothetical non-biased situation (0.189% vs.\n0.123%). Similarly, in the case of Jabref the highest rank received a CTR of\n1.276%, which is 87% higher than expected (0.683%). A chi-squared test confirms\nthe strong relationship between the rank of the recommendation shown to the\nuser and whether the user decided to click it (p < 0.01 for both Jabref and\nSowiport). Our study confirms the findings from other domains, that\nrecommendations in the top positions are more often clicked, regardless of\ntheir actual relevance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 09:40:49 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Collins", "Andrew", ""], ["Tkaczyk", "Dominika", ""], ["Aizawa", "Akiko", ""], ["Beel", "Joeran", ""]]}, {"id": "1802.06833", "submitter": "Christina Lioma Assoc. Prof", "authors": "Niels Dalum Hansen and K{\\aa}re M{\\o}lbak and Ingemar J. Cox and\n  Christina Lioma", "title": "Seasonal Web Search Query Selection for Influenza-Like Illness (ILI)\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influenza-like illness (ILI) estimation from web search data is an important\nweb analytics task. The basic idea is to use the frequencies of queries in web\nsearch logs that are correlated with past ILI activity as features when\nestimating current ILI activity. It has been noted that since influenza is\nseasonal, this approach can lead to spurious correlations with features/queries\nthat also exhibit seasonality, but have no relationship with ILI. Spurious\ncorrelations can, in turn, degrade performance. To address this issue, we\npropose modeling the seasonal variation in ILI activity and selecting queries\nthat are correlated with the residual of the seasonal model and the observed\nILI signal. Experimental results show that re-ranking queries obtained by\nGoogle Correlate based on their correlation with the residual strongly favours\nILI-related queries.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 20:15:45 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Hansen", "Niels Dalum", ""], ["M\u00f8lbak", "K\u00e5re", ""], ["Cox", "Ingemar J.", ""], ["Lioma", "Christina", ""]]}, {"id": "1802.06892", "submitter": "Siddharth Dinesh", "authors": "Siddharth Dinesh", "title": "Real World Evaluation of Approaches to Research Paper Recommendation", "comments": "Combined BE/MSc Thesis - BITS Pilani Goa", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we have identified the need for choosing baseline approaches\nfor research-paper recommendation systems. Following a literature survey of all\nresearch paper recommendation approaches described over the last four years, we\nframed criteria that makes for a well-rounded set of baselines. These are\nimplemented on Mr. DLib a literature recommendation platform. User click data\nwas collected as part of an ongoing experiment in collaboration with our\npartner Gesis. We reported the results from our evaluation for the experiments.\nWe will be able to draw clearer conclusions as time passes. We find that a term\nbased similarity search performs better than keyword based approaches. These\nresults are a good starting point in finding performance improvements for\nrelated document searches.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 22:29:50 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Dinesh", "Siddharth", ""]]}, {"id": "1802.07022", "submitter": "Roy Ka-Wei Lee", "authors": "Roy Ka-Wei Lee, Tuan-Anh Hoang and Ee-Peng Lim", "title": "Discovering Hidden Topical Hubs and Authorities in Online Social\n  Networks", "comments": "Pre-print for SIAM International Conference on Data Mining (SDM'18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding influential users in online social networks is an important problem\nwith many possible useful applications. HITS and other link analysis methods,\nin particular, have been often used to identify hub and authority users in web\ngraphs and online social networks. These works, however, have not considered\ntopical aspect of links in their analysis. A straightforward approach to\novercome this limitation is to first apply topic models to learn the user\ntopics before applying the HITS algorithm. In this paper, we instead propose a\nnovel topic model known as Hub and Authority Topic (HAT) model to combine the\ntwo process so as to jointly learn the hub, authority and topical interests. We\nevaluate HAT against several existing state-of-the-art methods in two aspects:\n(i) modeling of topics, and (ii) link recommendation. We conduct experiments on\ntwo real-world datasets from Twitter and Instagram. Our experiment results show\nthat HAT is comparable to state-of-the-art topic models in learning topics and\nit outperforms the state-of-the-art in link recommendation task.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 09:17:01 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Lee", "Roy Ka-Wei", ""], ["Hoang", "Tuan-Anh", ""], ["Lim", "Ee-Peng", ""]]}, {"id": "1802.07034", "submitter": "Christian Schulz", "authors": "Sonja Biedermann, Monika Henzinger, Christian Schulz, Bernhard\n  Schuster", "title": "Memetic Graph Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common knowledge that there is no single best strategy for graph\nclustering, which justifies a plethora of existing approaches. In this paper,\nwe present a general memetic algorithm, VieClus, to tackle the graph clustering\nproblem. This algorithm can be adapted to optimize different objective\nfunctions. A key component of our contribution are natural recombine operators\nthat employ ensemble clusterings as well as multi-level techniques. Lastly, we\ncombine these techniques with a scalable communication protocol, producing a\nsystem that is able to compute high-quality solutions in a short amount of\ntime. We instantiate our scheme with local search for modularity and show that\nour algorithm successfully improves or reproduces all entries of the 10th\nDIMACS implementation~challenge under consideration using a small amount of\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 09:55:37 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Biedermann", "Sonja", ""], ["Henzinger", "Monika", ""], ["Schulz", "Christian", ""], ["Schuster", "Bernhard", ""]]}, {"id": "1802.07281", "submitter": "Ashudeep Singh", "authors": "Ashudeep Singh, Thorsten Joachims", "title": "Fairness of Exposure in Rankings", "comments": "In Proceedings of the 24th ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining, London, UK, 2018", "journal-ref": null, "doi": "10.1145/3219819.3220088", "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rankings are ubiquitous in the online world today. As we have transitioned\nfrom finding books in libraries to ranking products, jobs, job applicants,\nopinions and potential romantic partners, there is a substantial precedent that\nranking systems have a responsibility not only to their users but also to the\nitems being ranked. To address these often conflicting responsibilities, we\npropose a conceptual and computational framework that allows the formulation of\nfairness constraints on rankings in terms of exposure allocation. As part of\nthis framework, we develop efficient algorithms for finding rankings that\nmaximize the utility for the user while provably satisfying a specifiable\nnotion of fairness. Since fairness goals can be application specific, we show\nhow a broad range of fairness constraints can be implemented using our\nframework, including forms of demographic parity, disparate treatment, and\ndisparate impact constraints. We illustrate the effect of these constraints by\nproviding empirical results on two ranking problems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 19:01:19 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 17:03:24 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Singh", "Ashudeep", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1802.07398", "submitter": "Jingbo Shang", "authors": "Jingbo Shang, Tianhang Sun, Jiaming Shen, Xingbang Liu, Anja\n  Gruenheid, Flip Korn, Adam Lelkes, Cong Yu, Jiawei Han", "title": "Investigating Rumor News Using Agreement-Aware Search", "comments": null, "journal-ref": null, "doi": "10.1145/3269206.3272020", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed a widespread increase of rumor news generated by\nhumans and machines. Therefore, tools for investigating rumor news have become\nan urgent necessity. One useful function of such tools is to see ways a\nspecific topic or event is represented by presenting different points of view\nfrom multiple sources.\n  In this paper, we propose Maester, a novel agreement-aware search framework\nfor investigating rumor news. Given an investigative question, Maester will\nretrieve related articles to that question, assign and display top articles\nfrom agree, disagree, and discuss categories to users. Splitting the results\ninto these three categories provides the user a holistic view towards the\ninvestigative question. We build Maester based on the following two key\nobservations: (1) relatedness can commonly be determined by keywords and\nentities occurring in both questions and articles, and (2) the level of\nagreement between the investigative question and the related news article can\noften be decided by a few key sentences. Accordingly, we use gradient boosting\ntree models with keyword/entity matching features for relatedness detection,\nand leverage recurrent neural network to infer the level of agreement. Our\nexperiments on the Fake News Challenge (FNC) dataset demonstrate up to an order\nof magnitude improvement of Maester over the original FNC winning solution, for\nagreement-aware search.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 01:52:01 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2018 23:33:45 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Shang", "Jingbo", ""], ["Sun", "Tianhang", ""], ["Shen", "Jiaming", ""], ["Liu", "Xingbang", ""], ["Gruenheid", "Anja", ""], ["Korn", "Flip", ""], ["Lelkes", "Adam", ""], ["Yu", "Cong", ""], ["Han", "Jiawei", ""]]}, {"id": "1802.07400", "submitter": "Cynthia Rudin", "authors": "Cynthia Rudin, Yining Wang", "title": "Direct Learning to Rank and Rerank", "comments": null, "journal-ref": "AISTATS 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Learning-to-rank techniques have proven to be extremely useful for\nprioritization problems, where we rank items in order of their estimated\nprobabilities, and dedicate our limited resources to the top-ranked items. This\nwork exposes a serious problem with the state of learning-to-rank algorithms,\nwhich is that they are based on convex proxies that lead to poor\napproximations. We then discuss the possibility of \"exact\" reranking algorithms\nbased on mathematical programming. We prove that a relaxed version of the\n\"exact\" problem has the same optimal solution, and provide an empirical\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 02:04:40 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Rudin", "Cynthia", ""], ["Wang", "Yining", ""]]}, {"id": "1802.07459", "submitter": "Bang Liu", "authors": "Bang Liu, Di Niu, Haojie Wei, Jinghong Lin, Yancheng He, Kunfeng Lai,\n  Yu Xu", "title": "Matching Article Pairs with Graphical Decomposition and Convolutions", "comments": "Accepted by ACL 2019 as long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the relationship between two articles, e.g., whether two articles\npublished from different sources describe the same breaking news, is critical\nto many document understanding tasks. Existing approaches for modeling and\nmatching sentence pairs do not perform well in matching longer documents, which\nembody more complex interactions between the enclosed entities than a sentence\ndoes. To model article pairs, we propose the Concept Interaction Graph to\nrepresent an article as a graph of concepts. We then match a pair of articles\nby comparing the sentences that enclose the same concept vertex through a\nseries of encoding techniques, and aggregate the matching signals through a\ngraph convolutional network. To facilitate the evaluation of long article\nmatching, we have created two datasets, each consisting of about 30K pairs of\nbreaking news articles covering diverse topics in the open domain. Extensive\nevaluations of the proposed methods on the two datasets demonstrate significant\nimprovements over a wide range of state-of-the-art methods for natural language\nmatching.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 08:01:41 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 12:40:06 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Liu", "Bang", ""], ["Niu", "Di", ""], ["Wei", "Haojie", ""], ["Lin", "Jinghong", ""], ["He", "Yancheng", ""], ["Lai", "Kunfeng", ""], ["Xu", "Yu", ""]]}, {"id": "1802.07544", "submitter": "Kyrylo Malakhov", "authors": "O. V. Palagin, K. S. Malakhov, V. Yu. Velychko, O. S. Shchurov", "title": "Personal research information system. About developing the methods for\n  searching patent analogs of invention", "comments": "in Russian; 9 pages; \"Bibliography\" section updated for correct\n  identification of references by the Google Scholar parser software; Corrected\n  spelling of the name of one of the authors (Velychko)", "journal-ref": "Computer means, networks and systems 16 (2017) 5-13", "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article describes information model and the method for searching patent\nanalogs for Personal Research Information System.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 12:44:56 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 09:47:28 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Palagin", "O. V.", ""], ["Malakhov", "K. S.", ""], ["Velychko", "V. Yu.", ""], ["Shchurov", "O. S.", ""]]}, {"id": "1802.07578", "submitter": "Tobias Schnabel", "authors": "Tobias Schnabel, Paul N. Bennett, Thorsten Joachims", "title": "Improving Recommender Systems Beyond the Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems rely heavily on the predictive accuracy of the learning\nalgorithm. Most work on improving accuracy has focused on the learning\nalgorithm itself. We argue that this algorithmic focus is myopic. In\nparticular, since learning algorithms generally improve with more and better\ndata, we propose shaping the feedback generation process as an alternate and\ncomplementary route to improving accuracy. To this effect, we explore how\nchanges to the user interface can impact the quality and quantity of feedback\ndata -- and therefore the learning accuracy. Motivated by information foraging\ntheory, we study how feedback quality and quantity are influenced by interface\ndesign choices along two axes: information scent and information access cost.\nWe present a user study of these interface factors for the common task of\npicking a movie to watch, showing that these factors can effectively shape and\nimprove the implicit feedback data that is generated while maintaining the user\nexperience.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 14:13:49 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Schnabel", "Tobias", ""], ["Bennett", "Paul N.", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1802.07876", "submitter": "Fei Chen", "authors": "Fei Chen, Mi Luo, Zhenhua Dong, Zhenguo Li, Xiuqiang He", "title": "Federated Meta-Learning with Fast Convergence and Efficient\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical and systematic challenges in collaboratively training machine\nlearning models across distributed networks of mobile devices have been the\nbottlenecks in the real-world application of federated learning. In this work,\nwe show that meta-learning is a natural choice to handle these issues, and\npropose a federated meta-learning framework FedMeta, where a parameterized\nalgorithm (or meta-learner) is shared, instead of a global model in previous\napproaches. We conduct an extensive empirical evaluation on LEAF datasets and a\nreal-world production dataset, and demonstrate that FedMeta achieves a\nreduction in required communication cost by 2.82-4.33 times with faster\nconvergence, and an increase in accuracy by 3.23%-14.84% as compared to\nFederated Averaging (FedAvg) which is a leading optimization algorithm in\nfederated learning. Moreover, FedMeta preserves user privacy since only the\nparameterized algorithm is transmitted between mobile devices and central\nservers, and no raw data is collected onto the servers.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 02:35:32 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2019 06:39:54 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Chen", "Fei", ""], ["Luo", "Mi", ""], ["Dong", "Zhenhua", ""], ["Li", "Zhenguo", ""], ["He", "Xiuqiang", ""]]}, {"id": "1802.07938", "submitter": "Zhiyong Cheng", "authors": "Zhiyong Cheng, Ying Ding, Lei Zhu, Mohan Kankanhalli", "title": "Aspect-Aware Latent Factor Model: Rating Prediction with Ratings and\n  Reviews", "comments": "This paper has been accepted by the WWW 2018 Conference", "journal-ref": null, "doi": "10.1145/3178876.3186145", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although latent factor models (e.g., matrix factorization) achieve good\naccuracy in rating prediction, they suffer from several problems including\ncold-start, non-transparency, and suboptimal recommendation for local users or\nitems. In this paper, we employ textual review information with ratings to\ntackle these limitations. Firstly, we apply a proposed aspect-aware topic model\n(ATM) on the review text to model user preferences and item features from\ndifferent aspects, and estimate the aspect importance of a user towards an\nitem. The aspect importance is then integrated into a novel aspect-aware latent\nfactor model (ALFM), which learns user's and item's latent factors based on\nratings. In particular, ALFM introduces a weighted matrix to associate those\nlatent factors with the same set of aspects discovered by ATM, such that the\nlatent factors could be used to estimate aspect ratings. Finally, the overall\nrating is computed via a linear combination of the aspect ratings, which are\nweighted by the corresponding aspect importance. To this end, our model could\nalleviate the data sparsity problem and gain good interpretability for\nrecommendation. Besides, an aspect rating is weighted by an aspect importance,\nwhich is dependent on the targeted user's preferences and targeted item's\nfeatures. Therefore, it is expected that the proposed method can model a user's\npreferences on an item more accurately for each user-item pair locally.\nComprehensive experimental studies have been conducted on 19 datasets from\nAmazon and Yelp 2017 Challenge dataset. Results show that our method achieves\nsignificant improvement compared with strong baseline methods, especially for\nusers with only few ratings. Moreover, our model could interpret the\nrecommendation results in depth.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 08:23:51 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Cheng", "Zhiyong", ""], ["Ding", "Ying", ""], ["Zhu", "Lei", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1802.07997", "submitter": "Dar\\'io Garigliotti", "authors": "Heng Ding, Shuo Zhang, Dar\\'io Garigliotti, and Krisztian Balog", "title": "Generating High-Quality Query Suggestion Candidates for Task-Based\n  Search", "comments": "Advances in Information Retrieval. Proceedings of the 40th European\n  Conference on Information Retrieval (ECIR '18), 2018", "journal-ref": null, "doi": "10.1007/978-3-319-76941-7_54", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of generating query suggestions for task-based search.\nThe current state of the art relies heavily on suggestions provided by a major\nsearch engine. In this paper, we solve the task without reliance on search\nengines. Specifically, we focus on the first step of a two-stage pipeline\napproach, which is dedicated to the generation of query suggestion candidates.\nWe present three methods for generating candidate suggestions and apply them on\nmultiple information sources. Using a purpose-built test collection, we find\nthat these methods are able to generate high-quality suggestion candidates.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 11:55:28 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Ding", "Heng", ""], ["Zhang", "Shuo", ""], ["Garigliotti", "Dar\u00edo", ""], ["Balog", "Krisztian", ""]]}, {"id": "1802.08010", "submitter": "Dar\\'io Garigliotti", "authors": "Dar\\'io Garigliotti and Krisztian Balog", "title": "Towards an Understanding of Entity-Oriented Search Intents", "comments": "Advances in Information Retrieval. Proceedings of the 40th European\n  Conference on Information Retrieval (ECIR '18), 2018", "journal-ref": null, "doi": "10.1007/978-3-319-76941-7_57", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity-oriented search deals with a wide variety of information needs, from\ndisplaying direct answers to interacting with services. In this work, we aim to\nunderstand what are prominent entity-oriented search intents and how they can\nbe fulfilled. We develop a scheme of entity intent categories, and use them to\nannotate a sample of queries. Specifically, we annotate unique query refiners\non the level of entity types. We observe that, on average, over half of those\nrefiners seek to interact with a service, while over a quarter of the refiners\nsearch for information that may be looked up in a knowledge base.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 12:30:13 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Garigliotti", "Dar\u00edo", ""], ["Balog", "Krisztian", ""]]}, {"id": "1802.08301", "submitter": "Chandra Sekhar Bhagavatula", "authors": "Chandra Bhagavatula and Sergey Feldman and Russell Power and Waleed\n  Ammar", "title": "Content-Based Citation Recommendation", "comments": "NAACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a content-based method for recommending citations in an academic\npaper draft. We embed a given query document into a vector space, then use its\nnearest neighbors as candidates, and rerank the candidates using a\ndiscriminative model trained to distinguish between observed and unobserved\ncitations. Unlike previous work, our method does not require metadata such as\nauthor names which can be missing, e.g., during the peer review process.\nWithout using metadata, our method outperforms the best reported results on\nPubMed and DBLP datasets with relative improvements of over 18% in F1@20 and\nover 22% in MRR. We show empirically that, although adding metadata improves\nthe performance on standard metrics, it favors self-citations which are less\nuseful in a citation recommendation setup. We release an online portal\n(http://labs.semanticscholar.org/citeomatic/) for citation recommendation based\non our method, and a new dataset OpenCorpus of 7 million research articles to\nfacilitate future research on this task.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 21:13:47 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Bhagavatula", "Chandra", ""], ["Feldman", "Sergey", ""], ["Power", "Russell", ""], ["Ammar", "Waleed", ""]]}, {"id": "1802.08401", "submitter": "Huifeng Guo", "authors": "Feng Liu, Ruiming Tang, Xutao Li, Yunming Ye, Huifeng Guo, Xiuqiang He", "title": "Novel Approaches to Accelerating the Convergence Rate of Markov Decision\n  Process for Search Result Diversification", "comments": "This research work has been accepted by DASFAA'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, some studies have utilized the Markov Decision Process for\ndiversifying (MDP-DIV) the search results in information retrieval. Though\npromising performances can be delivered, MDP-DIV suffers from a very slow\nconvergence, which hinders its usability in real applications. In this paper,\nwe aim to promote the performance of MDP-DIV by speeding up the convergence\nrate without much accuracy sacrifice. The slow convergence is incurred by two\nmain reasons: the large action space and data scarcity. On the one hand, the\nsequential decision making at each position needs to evaluate the\nquery-document relevance for all the candidate set, which results in a huge\nsearching space for MDP; on the other hand, due to the data scarcity, the agent\nhas to proceed more \"trial and error\" interactions with the environment. To\ntackle this problem, we propose MDP-DIV-kNN and MDP-DIV-NTN methods. The\nMDP-DIV-kNN method adopts a $k$ nearest neighbor strategy, i.e., discarding the\n$k$ nearest neighbors of the recently-selected action (document), to reduce the\ndiversification searching space. The MDP-DIV-NTN employs a pre-trained\ndiversification neural tensor network (NTN-DIV) as the evaluation model, and\ncombines the results with MDP to produce the final ranking solution. The\nexperiment results demonstrate that the two proposed methods indeed accelerate\nthe convergence rate of the MDP-DIV, which is 3x faster, while the accuracies\nproduced barely degrade, or even are better.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 06:08:28 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Liu", "Feng", ""], ["Tang", "Ruiming", ""], ["Li", "Xutao", ""], ["Ye", "Yunming", ""], ["Guo", "Huifeng", ""], ["He", "Xiuqiang", ""]]}, {"id": "1802.08452", "submitter": "Massimo Quadrana", "authors": "Massimo Quadrana, Paolo Cremonesi and Dietmar Jannach", "title": "Sequence-Aware Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are one of the most successful applications of data\nmining and machine learning technology in practice. Academic research in the\nfield is historically often based on the matrix completion problem formulation,\nwhere for each user-item-pair only one interaction (e.g., a rating) is\nconsidered. In many application domains, however, multiple user-item\ninteractions of different types can be recorded over time. And, a number of\nrecent works have shown that this information can be used to build richer\nindividual user models and to discover additional behavioral patterns that can\nbe leveraged in the recommendation process. In this work we review existing\nworks that consider information from such sequentially-ordered user- item\ninteraction logs in the recommendation process. Based on this review, we\npropose a categorization of the corresponding recommendation tasks and goals,\nsummarize existing algorithmic solutions, discuss methodological approaches\nwhen benchmarking what we call sequence-aware recommender systems, and outline\nopen challenges in the area.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 09:19:26 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Quadrana", "Massimo", ""], ["Cremonesi", "Paolo", ""], ["Jannach", "Dietmar", ""]]}, {"id": "1802.08614", "submitter": "Baoxu Shi", "authors": "Baoxu Shi and Tim Weninger", "title": "Visualizing the Flow of Discourse with a Concept Ontology", "comments": "2 pages, accepted to WWW2018", "journal-ref": null, "doi": "10.1145/3184558.3186943", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding and visualizing human discourse has long being a challenging\ntask. Although recent work on argument mining have shown success in classifying\nthe role of various sentences, the task of recognizing concepts and\nunderstanding the ways in which they are discussed remains challenging. Given\nan email thread or a transcript of a group discussion, our task is to extract\nthe relevant concepts and understand how they are referenced and re-referenced\nthroughout the discussion. In the present work, we present a preliminary\napproach for extracting and visualizing group discourse by adapting Wikipedia's\ncategory hierarchy to be an external concept ontology. From a user study, we\nfound that our method achieved better results than 4 strong alternative\napproaches, and we illustrate our visualization method based on the extracted\ndiscourse flows.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 15:56:07 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Shi", "Baoxu", ""], ["Weninger", "Tim", ""]]}, {"id": "1802.08674", "submitter": "L. Elisa Celis", "authors": "L. Elisa Celis, Sayash Kapoor, Farnood Salehi, and Nisheeth K. Vishnoi", "title": "An Algorithmic Framework to Control Bias in Bandit-based Personalization", "comments": "A short version of this paper appeared in FAT/ML 2017\n  (arXiv:1707.02260)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalization is pervasive in the online space as it leads to higher\nefficiency and revenue by allowing the most relevant content to be served to\neach user. However, recent studies suggest that personalization methods can\npropagate societal or systemic biases and polarize opinions; this has led to\ncalls for regulatory mechanisms and algorithms to combat bias and inequality.\nAlgorithmically, bandit optimization has enjoyed great success in learning user\npreferences and personalizing content or feeds accordingly. We propose an\nalgorithmic framework that allows for the possibility to control bias or\ndiscrimination in such bandit-based personalization. Our model allows for the\nspecification of general fairness constraints on the sensitive types of the\ncontent that can be displayed to a user. The challenge, however, is to come up\nwith a scalable and low regret algorithm for the constrained optimization\nproblem that arises. Our main technical contribution is a provably fast and\nlow-regret algorithm for the fairness-constrained bandit optimization problem.\nOur proofs crucially leverage the special structure of our problem. Experiments\non synthetic and real-world data sets show that our algorithmic framework can\ncontrol bias with only a minor loss to revenue.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 18:44:01 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Celis", "L. Elisa", ""], ["Kapoor", "Sayash", ""], ["Salehi", "Farnood", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1802.08988", "submitter": "Baoyang Song", "authors": "Baoyang Song", "title": "Deep Neural Network for Learning to Rank Query-Text Pairs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of document ranking in information retrieval\nsystems by Learning to Rank. We propose ConvRankNet combining a Siamese\nConvolutional Neural Network encoder and the RankNet ranking model which could\nbe trained in an end-to-end fashion. We prove a general result justifying the\nlinear test-time complexity of pairwise Learning to Rank approach. Experiments\non the OHSUMED dataset show that ConvRankNet outperforms systematically\nexisting feature-based models.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 11:15:31 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Song", "Baoyang", ""]]}, {"id": "1802.09059", "submitter": "Ahmad Pesaranghader", "authors": "Ahmad Pesaranghader, Ali Pesaranghader, Stan Matwin, Marina Sokolova", "title": "One Single Deep Bidirectional LSTM Network for Word Sense Disambiguation\n  of Text Data", "comments": "12 pages, 1 figure, to appear in the Proceedings of the 31st Canadian\n  Conference on Artificial Intelligence, 8-11 May, 2018, Toronto, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to recent technical and scientific advances, we have a wealth of\ninformation hidden in unstructured text data such as offline/online narratives,\nresearch articles, and clinical reports. To mine these data properly,\nattributable to their innate ambiguity, a Word Sense Disambiguation (WSD)\nalgorithm can avoid numbers of difficulties in Natural Language Processing\n(NLP) pipeline. However, considering a large number of ambiguous words in one\nlanguage or technical domain, we may encounter limiting constraints for proper\ndeployment of existing WSD models. This paper attempts to address the problem\nof one-classifier-per-one-word WSD algorithms by proposing a single\nBidirectional Long Short-Term Memory (BLSTM) network which by considering\nsenses and context sequences works on all ambiguous words collectively.\nEvaluated on SensEval-3 benchmark, we show the result of our model is\ncomparable with top-performing WSD algorithms. We also discuss how applying\nadditional modifications alleviates the model fault and the need for more\ntraining data.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 18:51:53 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Pesaranghader", "Ahmad", ""], ["Pesaranghader", "Ali", ""], ["Matwin", "Stan", ""], ["Sokolova", "Marina", ""]]}, {"id": "1802.09426", "submitter": "Mayank Chaudhari", "authors": "Mayank Chaudhari, Aakash Nelson Mattukoyya", "title": "Tone Biased MMR Text Summarization", "comments": "4 pages, 4 figures, IJCRT, Feb 2018", "journal-ref": "Mayank Chaudhari, Aakash Nelson Mattukoyya,Tone Biased MMR Text\n  Summarization,IJCRT, Feb 2018", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text summarization is an interesting area for researchers to develop new\ntechniques to provide human like summaries for vast amounts of information.\nSummarization techniques tend to focus on providing accurate representation of\ncontent, and often the tone of the content is ignored. Tone of the content sets\na baseline for how a reader perceives the content. As such being able to\ngenerate summary with tone that is appropriate for the reader is important. In\nour work we implement Maximal Marginal Relevance [MMR] based multi-document\ntext summarization and propose a naive model to change tone of the\nsummarization by setting a bias to specific set of words and restricting other\nwords in the summarization output. This bias towards a specified set of words\nproduces a summary whose tone is same as tone of specified words.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 16:12:40 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 10:25:07 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Chaudhari", "Mayank", ""], ["Mattukoyya", "Aakash Nelson", ""]]}, {"id": "1802.09566", "submitter": "Mudasir Ahmad Wani", "authors": "Mudasir Ahmad Wani, Nancy Agarwal, Suraiya Jabin, Syed Zeeshan Hussai", "title": "Design of iMacros-based Data Crawler and the Behavioral Analysis of\n  Facebook Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining the desired dataset is still a prime challenge faced by researchers\nwhile analyzing Online Social Network (OSN) sites. Application Programming\nInterfaces (APIs) provided by OSN service providers for retrieving data impose\nseveral unavoidable restrictions which make it difficult to get a desirable\ndataset. In this paper, we present an iMacros technology-based data crawler\ncalled IMcrawler, capable of collecting every piece of information which is\naccessible through a browser from the Facebook website within the legal\nframework which permits access to publicly shared user content on OSNs. The\nproposed crawler addresses most of the challenges allied with web data\nextraction approaches and most of the APIs provided by OSN service providers.\nTwo broad sections have been extracted from Facebook user profiles, namely,\nPersonal Information and Wall Activities. The present work is the first attempt\ntowards providing the detailed description of crawler design for the Facebook\nwebsite.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 18:36:46 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 11:16:01 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Wani", "Mudasir Ahmad", ""], ["Agarwal", "Nancy", ""], ["Jabin", "Suraiya", ""], ["Hussai", "Syed Zeeshan", ""]]}, {"id": "1802.09728", "submitter": "Farhad Zafari", "authors": "F. Zafari, I. Moser, T. Baarslag", "title": "Modelling and Analysis of Temporal Preference Drifts Using A\n  Component-Based Factorised Latent Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The changes in user preferences can originate from substantial reasons, like\npersonality shift, or transient and circumstantial ones, like seasonal changes\nin item popularities. Disregarding these temporal drifts in modelling user\npreferences can result in unhelpful recommendations. Moreover, different\ntemporal patterns can be associated with various preference domains, and\npreference components and their combinations. These components comprise\npreferences over features, preferences over feature values, conditional\ndependencies between features, socially-influenced preferences, and bias. For\nexample, in the movies domain, the user can change his rating behaviour (bias\nshift), her preference for genre over language (feature preference shift), or\nstart favouring drama over comedy (feature value preference shift). In this\npaper, we first propose a novel latent factor model to capture the\ndomain-dependent component-specific temporal patterns in preferences. The\ncomponent-based approach followed in modelling the aspects of preferences and\ntheir temporal effects enables us to arbitrarily switch components on and off.\nWe evaluate the proposed method on three popular recommendation datasets and\nshow that it significantly outperforms the most accurate state-of-the-art\nstatic models. The experiments also demonstrate the greater robustness and\nstability of the proposed dynamic model in comparison with the most successful\nmodels to date. We also analyse the temporal behaviour of different preference\ncomponents and their combinations and show that the dynamic behaviour of\npreference components is highly dependent on the preference dataset and domain.\nTherefore, the results also highlight the importance of modelling temporal\neffects but also underline the advantages of a component-based architecture\nthat is better suited to capture domain-specific balances in the contributions\nof the aspects.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 06:00:49 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 02:35:29 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Zafari", "F.", ""], ["Moser", "I.", ""], ["Baarslag", "T.", ""]]}, {"id": "1802.09729", "submitter": "Richard Oentaryo", "authors": "Thong Hoang, Richard J. Oentaryo, Tien-Duy B. Le, David Lo", "title": "Network-Clustered Multi-Modal Bug Localization", "comments": "IEEE Transactions on Software Engineering", "journal-ref": null, "doi": "10.1109/TSE.2018.2810892", "report-no": null, "categories": "cs.IR cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developers often spend much effort and resources to debug a program. To help\nthe developers debug, numerous information retrieval (IR)-based and\nspectrum-based bug localization techniques have been devised. IR-based\ntechniques process textual information in bug reports, while spectrum-based\ntechniques process program spectra (i.e., a record of which program elements\nare executed for each test case). While both techniques ultimately generate a\nranked list of program elements that likely contain a bug, they only consider\none source of information--either bug reports or program spectra--which is not\noptimal. In light of this deficiency, this paper presents a new approach dubbed\nNetwork-clustered Multi-modal Bug Localization (NetML), which utilizes\nmulti-modal information from both bug reports and program spectra to localize\nbugs. NetML facilitates an effective bug localization by carrying out a joint\noptimization of bug localization error and clustering of both bug reports and\nprogram elements (i.e., methods). The clustering is achieved through the\nincorporation of network Lasso regularization, which incentivizes the model\nparameters of similar bug reports and similar program elements to be close\ntogether. To estimate the model parameters of both bug reports and methods,\nNetML employs an adaptive learning procedure based on Newton method that\nupdates the parameters on a per-feature basis. Extensive experiments on 355\nreal bugs from seven software systems have been conducted to benchmark NetML\nagainst various state-of-the-art localization methods. The results show that\nNetML surpasses the best-performing baseline by 31.82%, 22.35%, 19.72%, and\n19.24%, in terms of the number of bugs successfully localized when a developer\ninspects the top 1, 5, and 10 methods and Mean Average Precision (MAP),\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 06:02:15 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Hoang", "Thong", ""], ["Oentaryo", "Richard J.", ""], ["Le", "Tien-Duy B.", ""], ["Lo", "David", ""]]}, {"id": "1802.10078", "submitter": "Nicolas Fiorini", "authors": "Sunil Mohan, Nicolas Fiorini, Sun Kim, Zhiyong Lu", "title": "A Fast Deep Learning Model for Textual Relevance in Biomedical\n  Information Retrieval", "comments": "To appear in proceeding of WWW 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Publications in the life sciences are characterized by a large technical\nvocabulary, with many lexical and semantic variations for expressing the same\nconcept. Towards addressing the problem of relevance in biomedical literature\nsearch, we introduce a deep learning model for the relevance of a document's\ntext to a keyword style query. Limited by a relatively small amount of training\ndata, the model uses pre-trained word embeddings. With these, the model first\ncomputes a variable-length Delta matrix between the query and document,\nrepresenting a difference between the two texts, which is then passed through a\ndeep convolution stage followed by a deep feed-forward network to compute a\nrelevance score. This results in a fast model suitable for use in an online\nsearch engine. The model is robust and outperforms comparable state-of-the-art\ndeep learning approaches.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 21:43:23 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Mohan", "Sunil", ""], ["Fiorini", "Nicolas", ""], ["Kim", "Sun", ""], ["Lu", "Zhiyong", ""]]}]