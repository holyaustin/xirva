[{"id": "1004.0092", "submitter": "Dirk Nowotka", "authors": "Benjamin Hoffmann, Mikhail Lifshits, Yury Lifshits, Dirk Nowotka", "title": "Maximal Intersection Queries in Randomized Input Models", "comments": "18 pages", "journal-ref": "Theory of Computing Systems, 46(1):104-119, 2010", "doi": "10.1007/s00224-008-9154-6", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a family of sets and a single set, called the query set. How can one\nquickly find a member of the family which has a maximal intersection with the\nquery set? Time constraints on the query and on a possible preprocessing of the\nset family make this problem challenging. Such maximal intersection queries\narise in a wide range of applications, including web search, recommendation\nsystems, and distributing on-line advertisements. In general, maximal\nintersection queries are computationally expensive. We investigate two\nwell-motivated distributions over all families of sets and propose an algorithm\nfor each of them. We show that with very high probability an almost optimal\nsolution is found in time which is logarithmic in the size of the family.\nMoreover, we point out a threshold phenomenon on the probabilities of\nintersecting sets in each of our two input models which leads to the efficient\nalgorithms mentioned above.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2010 09:34:55 GMT"}], "update_date": "2010-04-02", "authors_parsed": [["Hoffmann", "Benjamin", ""], ["Lifshits", "Mikhail", ""], ["Lifshits", "Yury", ""], ["Nowotka", "Dirk", ""]]}, {"id": "1004.0816", "submitter": "Daniel Gayo Avello", "authors": "Daniel Gayo-Avello", "title": "Nepotistic Relationships in Twitter and their Impact on Rank Prestige\n  Algorithms", "comments": "40 pages, 17 tables, 14 figures. Paper has been restructured, new\n  section \"3.2. The importance of reciprocal linking in Twitter spam\" was\n  added, experiments with verified accounts in addition to spammers have bee\n  conducted to show performance with relevant users and not only regarding spam\n  demotion", "journal-ref": "Information Processing & Management Volume 49, Issue 6, November\n  2013, Pages 1250-1280", "doi": "10.1016/j.ipm.2013.06.003", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-blogging services such as Twitter allow anyone to publish anything,\nanytime. Needless to say, many of the available contents can be diminished as\nbabble or spam. However, given the number and diversity of users, some valuable\npieces of information should arise from the stream of tweets. Thus, such\nservices can develop into valuable sources of up-to-date information (the\nso-called real-time web) provided a way to find the most\nrelevant/trustworthy/authoritative users is available. Hence, this makes a\nhighly pertinent question for which graph centrality methods can provide an\nanswer. In this paper the author offers a comprehensive survey of feasible\nalgorithms for ranking users in social networks, he examines their\nvulnerabilities to linking malpractice in such networks, and suggests an\nobjective criterion against which to compare such algorithms. Additionally, he\nsuggests a first step towards \"desensitizing\" prestige algorithms against\ncheating by spammers and other abusive users.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2010 10:26:48 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2012 12:11:41 GMT"}], "update_date": "2013-09-16", "authors_parsed": [["Gayo-Avello", "Daniel", ""]]}, {"id": "1004.0902", "submitter": "Kimmo Fredriksson", "authors": "Kimmo Fredriksson", "title": "On building minimal automaton for subset matching queries", "comments": "Accepted to IPL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of building an index for a set $D$ of $n$ strings,\nwhere each string location is a subset of some finite integer alphabet of size\n$\\sigma$, so that we can answer efficiently if a given simple query string\n(where each string location is a single symbol) $p$ occurs in the set. That is,\nwe need to efficiently find a string $d \\in D$ such that $p[i] \\in d[i]$ for\nevery $i$. We show how to build such index in\n$O(n^{\\log_{\\sigma/\\Delta}(\\sigma)}\\log(n))$ average time, where $\\Delta$ is\nthe average size of the subsets. Our methods have applications e.g.\\ in\ncomputational biology (haplotype inference) and music information retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2010 17:20:03 GMT"}, {"version": "v2", "created": "Fri, 1 Oct 2010 08:15:22 GMT"}], "update_date": "2010-10-04", "authors_parsed": [["Fredriksson", "Kimmo", ""]]}, {"id": "1004.1257", "submitter": "Rdv Ijcsis", "authors": "V.Chitraa, Dr. Antony Selvdoss Davamani", "title": "A Survey on Preprocessing Methods for Web Usage Data", "comments": "IEEE Publication format, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "IJCSIS, Vol. 7 No. 3, March 2010", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  World Wide Web is a huge repository of web pages and links. It provides\nabundance of information for the Internet users. The growth of web is\ntremendous as approximately one million pages are added daily. Users' accesses\nare recorded in web logs. Because of the tremendous usage of web, the web log\nfiles are growing at a faster rate and the size is becoming huge. Web data\nmining is the application of data mining techniques in web data. Web Usage\nMining applies mining techniques in log data to extract the behavior of users\nwhich is used in various applications like personalized services, adaptive web\nsites, customer profiling, prefetching, creating attractive web sites etc., Web\nusage mining consists of three phases preprocessing, pattern discovery and\npattern analysis. Web log data is usually noisy and ambiguous and preprocessing\nis an important process before mining. For discovering patterns sessions are to\nbe constructed efficiently. This paper reviews existing work done in the\npreprocessing stage. A brief overview of various data mining techniques for\ndiscovering patterns, and pattern analysis are discussed. Finally a glimpse of\nvarious applications of web usage mining is also presented.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2010 07:07:16 GMT"}], "update_date": "2010-04-09", "authors_parsed": [["Chitraa", "V.", ""], ["Davamani", "Dr. Antony Selvdoss", ""]]}, {"id": "1004.1743", "submitter": "Rdv Ijcsis", "authors": "G. Nathiya, S. C. Punitha, M. Punithavalli", "title": "An Analytical Study on Behavior of Clusters Using K Means, EM and K*\n  Means Algorithm", "comments": "IEEE Publication format, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "IJCSIS, Vol. 7 No. 3, March 2010, 185-190", "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Clustering is an unsupervised learning method that constitutes a cornerstone\nof an intelligent data analysis process. It is used for the exploration of\ninter-relationships among a collection of patterns, by organizing them into\nhomogeneous clusters. Clustering has been dynamically applied to a variety of\ntasks in the field of Information Retrieval (IR). Clustering has become one of\nthe most active area of research and the development. Clustering attempts to\ndiscover the set of consequential groups where those within each group are more\nclosely related to one another than the others assigned to different groups.\nThe resultant clusters can provide a structure for organizing large bodies of\ntext for efficient browsing and searching. There exists a wide variety of\nclustering algorithms that has been intensively studied in the clustering\nproblem. Among the algorithms that remain the most common and effectual, the\niterative optimization clustering algorithms have been demonstrated reasonable\nperformance for clustering, e.g. the Expectation Maximization (EM) algorithm\nand its variants, and the well known k-means algorithm. This paper presents an\nanalysis on how partition method clustering techniques - EM, K -means and K*\nMeans algorithm work on heartspect dataset with below mentioned features -\nPurity, Entropy, CPU time, Cluster wise analysis, Mean value analysis and inter\ncluster distance. Thus the paper finally provides the experimental results of\ndatasets for five clusters to strengthen the results that the quality of the\nbehavior in clusters in EM algorithm is far better than k-means algorithm and\nk*means algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2010 21:58:16 GMT"}], "update_date": "2010-04-13", "authors_parsed": [["Nathiya", "G.", ""], ["Punitha", "S. C.", ""], ["Punithavalli", "M.", ""]]}, {"id": "1004.1796", "submitter": "Rdv Ijcsis", "authors": "P.J.Gayathri, S.C. Punitha, M. Punithavalli", "title": "Document Clustering using Sequential Information Bottleneck Method", "comments": "IEEE Publication format, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "IJCSIS, Vol. 7 No. 3, March 2010, 305-312", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper illustrates the Principal Direction Divisive Partitioning (PDDP)\nalgorithm and describes its drawbacks and introduces a combinatorial framework\nof the Principal Direction Divisive Partitioning (PDDP) algorithm, then\ndescribes the simplified version of the EM algorithm called the spherical\nGaussian EM (sGEM) algorithm and Information Bottleneck method (IB) is a\ntechnique for finding accuracy, complexity and time space. The PDDP algorithm\nrecursively splits the data samples into two sub clusters using the hyper plane\nnormal to the principal direction derived from the covariance matrix, which is\nthe central logic of the algorithm. However, the PDDP algorithm can yield poor\nresults, especially when clusters are not well separated from one another. To\nimprove the quality of the clustering results problem, it is resolved by\nreallocating new cluster membership using the IB algorithm with different\nsettings. IB Method gives accuracy but time consumption is more. Furthermore,\nbased on the theoretical background of the sGEM algorithm and sequential\nInformation Bottleneck method(sIB), it can be obvious to extend the framework\nto cover the problem of estimating the number of clusters using the Bayesian\nInformation Criterion. Experimental results are given to show the effectiveness\nof the proposed algorithm with comparison to the existing algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2010 11:23:42 GMT"}], "update_date": "2010-04-13", "authors_parsed": [["Gayathri", "P. J.", ""], ["Punitha", "S. C.", ""], ["Punithavalli", "M.", ""]]}, {"id": "1004.2155", "submitter": "Ahmad Kamran Malik", "authors": "Ahmad Kamran Malik, Muhammad Abdul Qadir, Nadeem Iftikhar, and\n  Muhammad Usman", "title": "Constraint-based Query Distribution Framework for an Integrated Global\n  Schema", "comments": "The Proceedings of the 13th INMIC 2009), Dec. 14-15, 2009, Islamabad,\n  Pakistan. Pages 1 - 6 Print ISBN: 978-1-4244-4872-2 INSPEC Accession Number:\n  11072575 Date of Current Version : 15 January 2010", "journal-ref": null, "doi": "10.1109/INMIC.2009.5383089", "report-no": null, "categories": "cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed heterogeneous data sources need to be queried uniformly using\nglobal schema. Query on global schema is reformulated so that it can be\nexecuted on local data sources. Constraints in global schema and mappings are\nused for source selection, query optimization,and querying partitioned and\nreplicated data sources. The provided system is all XML-based which poses query\nin XML form, transforms, and integrates local results in an XML document.\nContributions include the use of constraints in our existing global schema\nwhich help in source selection and query optimization, and a global query\ndistribution framework for querying distributed heterogeneous data sources.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2010 11:41:35 GMT"}], "update_date": "2010-04-14", "authors_parsed": [["Malik", "Ahmad Kamran", ""], ["Qadir", "Muhammad Abdul", ""], ["Iftikhar", "Nadeem", ""], ["Usman", "Muhammad", ""]]}, {"id": "1004.2222", "submitter": "Jacek Gwizdka", "authors": "Jacek Gwizdka", "title": "What a Difference a Tag Cloud Makes: Effects of Tasks and Cognitive\n  Abilities on Search Results Interface Use", "comments": null, "journal-ref": "Information Research 14(4), 2009, paper 414.", "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this study is to expand our understanding of the relationships\nbetween selected tasks, cognitive abilities and search result interfaces. The\nunderlying objective is to understand how to select search results presentation\nfor tasks and user contexts. Twenty three participants conducted four search\ntasks of two types and used two interfaces (List and Overview) to refine and\nexamine search results. Clickthrough data were recorded. This controlled study\nemployed a mixed model design with two within-subject factors (task and\ninterface) and two between-subject factors (two cognitive abilities: memory\nspan and verbal closure). Quantitative analyses were carried out by means of\nthe statistical package SPSS. Specifically, multivariate analysis of variance\nwith repeated measures and non-parametric tests were performed on the collected\ndata. The overview of search results appeared to have benefited searchers in\nseveral ways. It made them faster; it facilitated formulation of more effective\nqueries and helped to assess search results. Searchers with higher cognitive\nabilities were faster in the Overview interface and in less demanding\nsituations (on simple tasks), while at the same time they issued about the same\nnumber of queries as lower-ability searchers. In more demanding situations (on\ncomplex tasks and in the List interface), the higher ability searchers expended\nmore search effort, although they were not significantly slower than the lower\nability people in these situations. The higher search effort, however, did not\nresult in a measurable improvement of task outcomes for high-ability searchers.\nThese findings have implications for the design of search interfaces. They\nsuggest benefits of providing result overviews. They also suggest the\nimportance of considering cognitive abilities in the design of search results'\npresentation and interaction.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2010 16:07:10 GMT"}], "update_date": "2010-04-14", "authors_parsed": [["Gwizdka", "Jacek", ""]]}, {"id": "1004.2719", "submitter": "Martin Klein", "authors": "Martin Klein, Jeffery Shipman, Michael L. Nelson", "title": "Is This a Good Title?", "comments": "10 pages, 8 figures, 4 tables, 37 references, accepted for\n  publication at Hypertext 2010 in Toronto, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing web pages, URIs that return the 404 \"Page Not Found\" error or the\nHTTP response code 200 but dereference unexpected content, are ubiquitous in\ntoday's browsing experience. We use Internet search engines to relocate such\nmissing pages and provide means that help automate the rediscovery process. We\npropose querying web pages' titles against search engines. We investigate the\nretrieval performance of titles and compare them to lexical signatures which\nare derived from the pages' content. Since titles naturally represent the\ncontent of a document they intuitively change over time. We measure the edit\ndistance between current titles and titles of copies of the same pages obtained\nfrom the Internet Archive and display their evolution. We further investigate\nthe correlation between title changes and content modifications of a web page\nover time. Lastly we provide a predictive model for the quality of any given\nweb page title in terms of its discovery performance. Our results show that\ntitles return more than 60% URIs top ranked and further relevant content\nreturned in the top 10 results. We show that titles decay slowly but are far\nmore stable than the pages' content. We further distill stop titles than can\nhelp identify insufficiently performing search engine queries.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2010 21:30:41 GMT"}], "update_date": "2010-04-19", "authors_parsed": [["Klein", "Martin", ""], ["Shipman", "Jeffery", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1004.3183", "submitter": "Juan Manuel Torres Moreno", "authors": "Juan-Manuel Torres Moreno, Silvia Fernandez and Eric SanJuan", "title": "Statistical Physics for Natural Language Processing", "comments": "This paper has been withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cond-mat.stat-mech cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn by the author.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 13:11:56 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2011 17:08:12 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Moreno", "Juan-Manuel Torres", ""], ["Fernandez", "Silvia", ""], ["SanJuan", "Eric", ""]]}, {"id": "1004.3274", "submitter": "Vishal Goyal", "authors": "Kamal Sarkar, Mita Nasipuri, Suranjan Ghose", "title": "A New Approach to Keyphrase Extraction Using Neural Networks", "comments": "International Journal of Computer Science Issues online at\n  http://ijcsi.org/articles/A-New-Approach-to-Keyphrase-Extraction-Using-Neural-Networks.php", "journal-ref": "IJCSI, Volume 7, Issue 2, March 2010", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrases provide a simple way of describing a document, giving the reader\nsome clues about its contents. Keyphrases can be useful in a various\napplications such as retrieval engines, browsing interfaces, thesaurus\nconstruction, text mining etc.. There are also other tasks for which keyphrases\nare useful, as we discuss in this paper. This paper describes a neural network\nbased approach to keyphrase extraction from scientific articles. Our results\nshow that the proposed method performs better than some state-of-the art\nkeyphrase extraction approaches.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 18:24:02 GMT"}], "update_date": "2010-04-20", "authors_parsed": [["Sarkar", "Kamal", ""], ["Nasipuri", "Mita", ""], ["Ghose", "Suranjan", ""]]}, {"id": "1004.3371", "submitter": "Juan Manuel Torres Moreno", "authors": "Florian Boudin, Juan-Manuel Torres-Moreno and Marc El-B\\`eze", "title": "Improving Update Summarization by Revisiting the MMR Criterion", "comments": "20 pages, 3 figures and 8 tables.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for multi-document update summarization that\nrelies on a double maximization criterion. A Maximal Marginal Relevance like\ncriterion, modified and so called Smmr, is used to select sentences that are\nclose to the topic and at the same time, distant from sentences used in already\nread documents. Summaries are then generated by assembling the high ranked\nmaterial and applying some ruled-based linguistic post-processing in order to\nobtain length reduction and maintain coherency. Through a participation to the\nText Analysis Conference (TAC) 2008 evaluation campaign, we have shown that our\nmethod achieves promising results.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 07:49:07 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Boudin", "Florian", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["El-B\u00e8ze", "Marc", ""]]}, {"id": "1004.3478", "submitter": "Carlos Lorenzetti", "authors": "Carlos M. Lorenzetti and Ana G. Maguitman", "title": "Learning Better Context Characterizations: An Intelligent Information\n  Retrieval Approach", "comments": "10 pages, 3 figures, CLEI 2008", "journal-ref": "XXXIV Conferencia Latinoamericana de Inform\\'{a}tica, pp. 200-209,\n  2008", "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper proposes an incremental method that can be used by an intelligent\nsystem to learn better descriptions of a thematic context. The method starts\nwith a small number of terms selected from a simple description of the topic\nunder analysis and uses this description as the initial search context. Using\nthese terms, a set of queries are built and submitted to a search engine. New\ndocuments and terms are used to refine the learned vocabulary. Evaluations\nperformed on a large number of topics indicate that the learned vocabulary is\nmuch more effective than the original one at the time of constructing queries\nto retrieve relevant material.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 15:21:49 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2010 19:29:16 GMT"}], "update_date": "2010-04-28", "authors_parsed": [["Lorenzetti", "Carlos M.", ""], ["Maguitman", "Ana G.", ""]]}, {"id": "1004.3732", "submitter": "Zi-Ke Zhang Mr.", "authors": "Zi-Ke Zhang, Chuang Liu, Yi-Cheng Zhang, and Tao Zhou", "title": "Solving the Cold-Start Problem in Recommender Systems with Social Tags", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2012.03.025", "report-no": null, "categories": "cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, based on the user-tag-object tripartite graphs, we propose a\nrecommendation algorithm, which considers social tags as an important role for\ninformation retrieval. Besides its low cost of computational time, the\nexperiment results of two real-world data sets, \\emph{Del.icio.us} and\n\\emph{MovieLens}, show it can enhance the algorithmic accuracy and diversity.\nEspecially, it can obtain more personalized recommendation results when users\nhave diverse topics of tags. In addition, the numerical results on the\ndependence of algorithmic accuracy indicates that the proposed algorithm is\nparticularly effective for small degree objects, which reminds us of the\nwell-known \\emph{cold-start} problem in recommender systems. Further empirical\nstudy shows that the proposed algorithm can significantly solve this problem in\nsocial tagging systems with heterogeneous object degree distributions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2010 16:04:04 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2010 02:28:46 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Zhang", "Zi-Ke", ""], ["Liu", "Chuang", ""], ["Zhang", "Yi-Cheng", ""], ["Zhou", "Tao", ""]]}, {"id": "1004.4460", "submitter": "William Jackson", "authors": "Sumalatha Ramachandran, Sharon Joseph, Sujaya Paulraj and Vetriselvi\n  Ramaraj", "title": "Handling Overload Conditions In High Performance Trustworthy Information\n  Retrieval Systems", "comments": "https://sites.google.com/site/journalofcomputing/", "journal-ref": "Journal of Computing, Volume 2, Issue 4, April 2010, 70-75", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web search engines retrieve a vast amount of information for a given search\nquery. But the user needs only trustworthy and high-quality information from\nthis vast retrieved data. The response time of the search engine must be a\nminimum value in order to satisfy the user. An optimum level of response time\nshould be maintained even when the system is overloaded. This paper proposes an\noptimal Load Shedding algorithm which is used to handle overload conditions in\nreal-time data stream applications and is adapted to the Information Retrieval\nSystem of a web search engine. Experiment results show that the proposed\nalgorithm enables a web search engine to provide trustworthy search results to\nthe user within an optimum response time, even during overload conditions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2010 10:03:49 GMT"}], "update_date": "2010-04-27", "authors_parsed": [["Ramachandran", "Sumalatha", ""], ["Joseph", "Sharon", ""], ["Paulraj", "Sujaya", ""], ["Ramaraj", "Vetriselvi", ""]]}, {"id": "1004.4462", "submitter": "William Jackson", "authors": "S.Saraswathi, Asma Siddhiqaa.M, Kalaimagal.K and Kalaiyarasi.M", "title": "BiLingual Information Retrieval System for English and Tamil", "comments": "https://sites.google.com/site/journalofcomputing/", "journal-ref": "Journal of Computing, Volume 2, Issue 4, April 2010, 85-89", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the design and implementation of BiLingual Information\nRetrieval system on the domain, Festivals. A generic platform is built for\nBiLingual Information retrieval which can be extended to any foreign or Indian\nlanguage working with the same efficiency. Search for the solution of the query\nis not done in a specific predefined set of standard languages but is chosen\ndynamically on processing the user's query. This paper deals with Indian\nlanguage Tamil apart from English. The task is to retrieve the solution for the\nuser given query in the same language as that of the query. In this process, a\nOntological tree is built for the domain in such a way that there are entries\nin the above listed two languages in every node of the tree. A Part-Of-Speech\n(POS) Tagger is used to determine the keywords from the given query. Based on\nthe context, the keywords are translated to appropriate languages using the\nOntological tree. A search is performed and documents are retrieved based on\nthe keywords. With the use of the Ontological tree, Information Extraction is\ndone. Finally, the solution for the query is translated back to the query\nlanguage (if necessary) and produced to the user.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2010 10:08:11 GMT"}], "update_date": "2010-04-27", "authors_parsed": [["Saraswathi", "S.", ""], ["M", "Asma Siddhiqaa.", ""], ["K", "Kalaimagal.", ""], ["M", "Kalaiyarasi.", ""]]}, {"id": "1004.4464", "submitter": "William Jackson", "authors": "S. Saraswathi, Narasimha Sravan. V, Sai Vamsi Krishna. B.V and Suresh\n  Reddy. S", "title": "Audio enabled information extraction system for cricket and hockey\n  domains", "comments": "Journal of Computing online at\n  https://sites.google.com/site/journalofcomputing/", "journal-ref": "Journal of Computing, Volume 2, Issue 4, April 2010", "doi": null, "report-no": null, "categories": "cs.IR cs.MM cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proposed system aims at the retrieval of the summarized information from\nthe documents collected from web based search engine as per the user query\nrelated to cricket and hockey domain. The system is designed in a manner that\nit takes the voice commands as keywords for search. The parts of speech in the\nquery are extracted using the natural language extractor for English. Based on\nthe keywords the search is categorized into 2 types: - 1.Concept wise -\ninformation retrieved to the query is retrieved based on the keywords and the\nconcept words related to it. The retrieved information is summarized using the\nprobabilistic approach and weighted means algorithm.2.Keyword search - extracts\nthe result relevant to the query from the highly ranked document retrieved from\nthe search by the search engine. The relevant search results are retrieved and\nthen keywords are used for summarizing part. During summarization it follows\nthe weighted and probabilistic approaches in order to identify the data\ncomparable to the keywords extracted. The extracted information is then refined\nrepeatedly through the aggregation process to reduce redundancy. Finally the\nresultant data is submitted to the user in the form of audio output.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2010 10:11:00 GMT"}], "update_date": "2010-04-27", "authors_parsed": [["Saraswathi", "S.", ""], ["Sravan.", "Narasimha", "V"], ["B.", "Sai Vamsi Krishna.", "V"], ["S", "Suresh Reddy.", ""]]}, {"id": "1004.4489", "submitter": "Djoerd Hiemstra", "authors": "Djoerd Hiemstra and Claudia Hauff", "title": "MIREX: MapReduce Information Retrieval Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": "TR-CTIT-10-15", "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We propose to use MapReduce to quickly test new retrieval approaches on a\ncluster of machines by sequentially scanning all documents. We present a small\ncase study in which we use a cluster of 15 low cost ma- chines to search a web\ncrawl of 0.5 billion pages showing that sequential scanning is a viable\napproach to running large-scale information retrieval experiments with little\neffort. The code is available to other researchers at:\nhttp://mirex.sourceforge.net\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2010 11:36:38 GMT"}], "update_date": "2012-05-02", "authors_parsed": [["Hiemstra", "Djoerd", ""], ["Hauff", "Claudia", ""]]}, {"id": "1004.5168", "submitter": "Charles Clarke", "authors": "Gordon V. Cormack, Mark D. Smucker, and Charles L. A. Clarke", "title": "Efficient and Effective Spam Filtering and Re-ranking for Large Web\n  Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TREC 2009 web ad hoc and relevance feedback tasks used a new document\ncollection, the ClueWeb09 dataset, which was crawled from the general Web in\nearly 2009. This dataset contains 1 billion web pages, a substantial fraction\nof which are spam --- pages designed to deceive search engines so as to deliver\nan unwanted payload. We examine the effect of spam on the results of the TREC\n2009 web ad hoc and relevance feedback tasks, which used the ClueWeb09 dataset.\nWe show that a simple content-based classifier with minimal training is\nefficient enough to rank the \"spamminess\" of every page in the dataset using a\nstandard personal computer in 48 hours, and effective enough to yield\nsignificant and substantive improvements in the fixed-cutoff precision (estP10)\nas well as rank measures (estR-Precision, StatMAP, MAP) of nearly all submitted\nruns. Moreover, using a set of \"honeypot\" queries the labeling of training data\nmay be reduced to an entirely automatic process. The results of classical\ninformation retrieval methods are particularly enhanced by filtering --- from\namong the worst to among the best.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 00:54:25 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Cormack", "Gordon V.", ""], ["Smucker", "Mark D.", ""], ["Clarke", "Charles L. A.", ""]]}, {"id": "1004.5370", "submitter": "Dell Zhang", "authors": "Dell Zhang, Jun Wang, Deng Cai, Jinsong Lu", "title": "Self-Taught Hashing for Fast Similarity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The ability of fast similarity search at large scale is of great importance\nto many Information Retrieval (IR) applications. A promising way to accelerate\nsimilarity search is semantic hashing which designs compact binary codes for a\nlarge number of documents so that semantically similar documents are mapped to\nsimilar codes (within a short Hamming distance). Although some recently\nproposed techniques are able to generate high-quality codes for documents known\nin advance, obtaining the codes for previously unseen documents remains to be a\nvery challenging problem. In this paper, we emphasise this issue and propose a\nnovel Self-Taught Hashing (STH) approach to semantic hashing: we first find the\noptimal $l$-bit binary codes for all documents in the given corpus via\nunsupervised learning, and then train $l$ classifiers via supervised learning\nto predict the $l$-bit code for any query document unseen before. Our\nexperiments on three real-world text datasets show that the proposed approach\nusing binarised Laplacian Eigenmap (LapEig) and linear Support Vector Machine\n(SVM) outperforms state-of-the-art techniques significantly.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 19:25:17 GMT"}], "update_date": "2010-04-30", "authors_parsed": [["Zhang", "Dell", ""], ["Wang", "Jun", ""], ["Cai", "Deng", ""], ["Lu", "Jinsong", ""]]}]