[{"id": "1003.0146", "submitter": "Lihong Li", "authors": "Lihong Li, Wei Chu, John Langford, Robert E. Schapire", "title": "A Contextual-Bandit Approach to Personalized News Article Recommendation", "comments": "10 pages, 5 figures", "journal-ref": "Presented at the Nineteenth International Conference on World Wide\n  Web (WWW 2010), Raleigh, NC, USA, 2010", "doi": "10.1145/1772690.1772758", "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized web services strive to adapt their services (advertisements,\nnews articles, etc) to individual users by making use of both content and user\ninformation. Despite a few recent advances, this problem remains challenging\nfor at least two reasons. First, web service is featured with dynamically\nchanging pools of content, rendering traditional collaborative filtering\nmethods inapplicable. Second, the scale of most web services of practical\ninterest calls for solutions that are both fast in learning and computation.\n  In this work, we model personalized recommendation of news articles as a\ncontextual bandit problem, a principled approach in which a learning algorithm\nsequentially selects articles to serve users based on contextual information\nabout the users and articles, while simultaneously adapting its\narticle-selection strategy based on user-click feedback to maximize total user\nclicks.\n  The contributions of this work are three-fold. First, we propose a new,\ngeneral contextual bandit algorithm that is computationally efficient and well\nmotivated from learning theory. Second, we argue that any bandit algorithm can\nbe reliably evaluated offline using previously recorded random traffic.\nFinally, using this offline evaluation method, we successfully applied our new\nalgorithm to a Yahoo! Front Page Today Module dataset containing over 33\nmillion events. Results showed a 12.5% click lift compared to a standard\ncontext-free bandit algorithm, and the advantage becomes even greater when data\ngets more scarce.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2010 02:18:59 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2012 23:49:42 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Li", "Lihong", ""], ["Chu", "Wei", ""], ["Langford", "John", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1003.0931", "submitter": "Troy Messina", "authors": "Casey W. Miller, Michelle D. Chabot, and Troy C. Messina", "title": "A student's guide to searching the literature using online databases", "comments": "16 pages, 5 figures, and 1 table", "journal-ref": "Am. J. Phys. 77(12), 1112-1117 (2009)", "doi": null, "report-no": null, "categories": "physics.ed-ph cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method is described to empower students to efficiently perform general and\nliterature searches using online resources. The method was tested on\nundergraduate and graduate students with varying backgrounds with scientific\nliterature. Students involved in this study showed marked improvement in their\nawareness of how and where to find accurate scientific information.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2010 23:33:19 GMT"}], "update_date": "2010-03-05", "authors_parsed": [["Miller", "Casey W.", ""], ["Chabot", "Michelle D.", ""], ["Messina", "Troy C.", ""]]}, {"id": "1003.1048", "submitter": "Wolfgang Stock", "authors": "Kathrin Knautz, Simone Soubusta, Wolfgang G. Stock", "title": "Tag Clusters as Information Retrieval Interfaces", "comments": null, "journal-ref": "Proceedings of the 43th Annual Hawaii International Conference on\n  System Sciences (HICSS-43), January 5-8, 2010", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents our design of a next generation information retrieval\nsystem based on tag co-occurrences and subsequent clustering. We help users\ngetting access to digital data through information visualization in the form of\ntag clusters. Current problems like the absence of interactivity and semantics\nbetween tags or the difficulty of adding additional search arguments are\nsolved. In the evaluation, based upon SERVQUAL and IT systems quality\nindicators, we found out that tag clusters are perceived as more useful than\ntag clouds, are much more trustworthy, and are more enjoyable to use.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2010 13:53:26 GMT"}], "update_date": "2010-03-05", "authors_parsed": [["Knautz", "Kathrin", ""], ["Soubusta", "Simone", ""], ["Stock", "Wolfgang G.", ""]]}, {"id": "1003.1141", "submitter": "Peter Turney", "authors": "Peter D. Turney and Patrick Pantel", "title": "From Frequency to Meaning: Vector Space Models of Semantics", "comments": null, "journal-ref": "Journal of Artificial Intelligence Research, (2010), 37, 141-188", "doi": "10.1613/jair.2934", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computers understand very little of the meaning of human language. This\nprofoundly limits our ability to give instructions to computers, the ability of\ncomputers to explain their actions to us, and the ability of computers to\nanalyse and process text. Vector space models (VSMs) of semantics are beginning\nto address these limits. This paper surveys the use of VSMs for semantic\nprocessing of text. We organize the literature on VSMs according to the\nstructure of the matrix in a VSM. There are currently three broad classes of\nVSMs, based on term-document, word-context, and pair-pattern matrices, yielding\nthree classes of applications. We survey a broad range of applications in these\nthree categories and we take a detailed look at a specific open source project\nin each category. Our goal in this survey is to show the breadth of\napplications of VSMs for semantics, to provide a new perspective on VSMs for\nthose who are already familiar with the area, and to provide pointers into the\nliterature for those who are less familiar with the field.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2010 21:07:18 GMT"}], "update_date": "2010-03-08", "authors_parsed": [["Turney", "Peter D.", ""], ["Pantel", "Patrick", ""]]}, {"id": "1003.1460", "submitter": "Rdv Ijcsis", "authors": "M. Barathi, S. Valli", "title": "Ontology Based Query Expansion Using Word Sense Disambiguation", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS February 2010, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 7, No. 2, pp. 022-027, February 2010, USA", "doi": null, "report-no": "Computer Science ISSN 19475500", "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The existing information retrieval techniques do not consider the context of\nthe keywords present in the user's queries. Therefore, the search engines\nsometimes do not provide sufficient information to the users. New methods based\non the semantics of user keywords must be developed to search in the vast web\nspace without incurring loss of information. The semantic based information\nretrieval techniques need to understand the meaning of the concepts in the user\nqueries. This will improve the precision-recall of the search results.\nTherefore, this approach focuses on the concept based semantic information\nretrieval. This work is based on Word sense disambiguation, thesaurus WordNet\nand ontology of any domain for retrieving information in order to capture the\ncontext of particular concept(s) and discover semantic relationships between\nthem.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2010 12:24:29 GMT"}], "update_date": "2010-04-28", "authors_parsed": [["Barathi", "M.", ""], ["Valli", "S.", ""]]}, {"id": "1003.1494", "submitter": "Rdv Ijcsis", "authors": "Abderrahim El Qadi, Driss Aboutajedine, Yassine Ennouary", "title": "Formal Concept Analysis for Information Retrieval", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper we describe a mechanism to improve Information Retrieval (IR)\non the web. The method is based on Formal Concepts Analysis (FCA) that it is\nmakes semantical relations during the queries, and allows a reorganizing, in\nthe shape of a lattice of concepts, the answers provided by a search engine. We\nproposed for the IR an incremental algorithm based on Galois lattice. This\nalgorithm allows a formal clustering of the data sources, and the results which\nit turns over are classified by order of relevance. The control of relevance is\nexploited in clustering, we improved the result by using ontology in field of\nimage processing, and reformulating the user queries which make it possible to\ngive more relevant documents.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2010 17:16:34 GMT"}], "update_date": "2010-03-09", "authors_parsed": [["Qadi", "Abderrahim El", ""], ["Aboutajedine", "Driss", ""], ["Ennouary", "Yassine", ""]]}, {"id": "1003.1795", "submitter": "Rdv Ijcsis", "authors": "Vidhya. K. A, G. Aghila", "title": "A Survey of Na\\\"ive Bayes Machine Learning approach in Text Document\n  Classification", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Text Document classification aims in associating one or more predefined\ncategories based on the likelihood suggested by the training set of labeled\ndocuments. Many machine learning algorithms play a vital role in training the\nsystem with predefined categories among which Na\\\"ive Bayes has some intriguing\nfacts that it is simple, easy to implement and draws better accuracy in large\ndatasets in spite of the na\\\"ive dependence. The importance of Na\\\"ive Bayes\nMachine learning approach has felt hence the study has been taken up for text\ndocument classification and the statistical event models available. This survey\nthe various feature selection methods has been discussed and compared along\nwith the metrics related to text document classification.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2010 06:41:49 GMT"}], "update_date": "2010-03-10", "authors_parsed": [["A", "Vidhya. K.", ""], ["Aghila", "G.", ""]]}, {"id": "1003.1814", "submitter": "Rdv Ijcsis", "authors": "Alok Ranjan, Harish Verma, Eatesh Kandpal, Joydip Dhar", "title": "An Analytical Approach to Document Clustering Based on Internal\n  Criterion Function", "comments": "Pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS, Vol. 7 No. 2, February 2010, USA. ISSN 1947\n  5500, http://sites.google.com/site/ijcsis/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Fast and high quality document clustering is an important task in organizing\ninformation, search engine results obtaining from user query, enhancing web\ncrawling and information retrieval. With the large amount of data available and\nwith a goal of creating good quality clusters, a variety of algorithms have\nbeen developed having quality-complexity trade-offs. Among these, some\nalgorithms seek to minimize the computational complexity using certain\ncriterion functions which are defined for the whole set of clustering solution.\nIn this paper, we are proposing a novel document clustering algorithm based on\nan internal criterion function. Most commonly used partitioning clustering\nalgorithms (e.g. k-means) have some drawbacks as they suffer from local optimum\nsolutions and creation of empty clusters as a clustering solution. The proposed\nalgorithm usually does not suffer from these problems and converge to a global\noptimum, its performance enhances with the increase in number of clusters. We\nhave checked our algorithm against three different datasets for four different\nvalues of k (required number of clusters).\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2010 07:28:07 GMT"}], "update_date": "2010-03-11", "authors_parsed": [["Ranjan", "Alok", ""], ["Verma", "Harish", ""], ["Kandpal", "Eatesh", ""], ["Dhar", "Joydip", ""]]}, {"id": "1003.1931", "submitter": "Zi-Ke Zhang Mr.", "authors": "Zi-Ke Zhang, Chuang Liu", "title": "Hypergraph model of social tagging networks", "comments": "7 pages,7 figures, 32 references", "journal-ref": "J. Stat. Mech. (2010) P100005", "doi": "10.1088/1742-5468/2010/10/P10005", "report-no": null, "categories": "physics.soc-ph cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed the great success of a new family of\nparadigms, so-called folksonomy, which allows users to freely associate tags to\nresources and efficiently manage them. In order to uncover the underlying\nstructures and user behaviors in folksonomy, in this paper, we propose an\nevolutionary hypergrah model to explain the emerging statistical properties.\nThe present model introduces a novel mechanism that one can not only assign\ntags to resources, but also retrieve resources via collaborative tags. We then\ncompare the model with a real-world dataset: \\emph{Del.icio.us}. Indeed, the\npresent model shows considerable agreement with the empirical data in following\naspects: power-law hyperdegree distributions, negtive correlation between\nclustering coefficients and hyperdegrees, and small average distances.\nFurthermore, the model indicates that most tagging behaviors are motivated by\nlabeling tags to resources, and tags play a significant role in effectively\nretrieving interesting resources and making acquaintance with congenial\nfriends. The proposed model may shed some light on the in-depth understanding\nof the structure and function of folksonomy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2010 17:03:41 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Zhang", "Zi-Ke", ""], ["Liu", "Chuang", ""]]}, {"id": "1003.2458", "submitter": "Panigrahy Rina", "authors": "Sreenivas Gollapudi and Rina Panigrahy", "title": "Revisiting the Examination Hypothesis with Query Specific Position Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click through rates (CTR) offer useful user feedback that can be used to\ninfer the relevance of search results for queries. However it is not very\nmeaningful to look at the raw click through rate of a search result because the\nlikelihood of a result being clicked depends not only on its relevance but also\nthe position in which it is displayed. One model of the browsing behavior, the\n{\\em Examination Hypothesis} \\cite{RDR07,Craswell08,DP08}, states that each\nposition has a certain probability of being examined and is then clicked based\non the relevance of the search snippets. This is based on eye tracking studies\n\\cite{Claypool01, GJG04} which suggest that users are less likely to view\nresults in lower positions. Such a position dependent variation in the\nprobability of examining a document is referred to as {\\em position bias}. Our\nmain observation in this study is that the position bias tends to differ with\nthe kind of information the user is looking for. This makes the position bias\n{\\em query specific}. In this study, we present a model for analyzing a query\nspecific position bias from the click data and use these biases to derive\nposition independent relevance values of search results. Our model is based on\nthe assumption that for a given query, the positional click through rate of a\ndocument is proportional to the product of its relevance and a {\\em query\nspecific} position bias. We compare our model with the vanilla examination\nhypothesis model (EH) on a set of queries obtained from search logs of a\ncommercial search engine. We also compare it with the User Browsing Model (UBM)\n\\cite{DP08} which extends the cascade model of Craswell et al\\cite{Craswell08}\nby incorporating multiple clicks in a query session. We show that the our\nmodel, although much simpler to implement, consistently outperforms both EH and\nUBM on well-used measures such as relative error and cross entropy.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2010 01:26:25 GMT"}], "update_date": "2010-03-15", "authors_parsed": [["Gollapudi", "Sreenivas", ""], ["Panigrahy", "Rina", ""]]}, {"id": "1003.2677", "submitter": "Mohammad Doomun", "authors": "Razvi Doomun, Lollmahamod N., Auleear Nadeem, Mozafar Aukin", "title": "Classified Ads Harvesting Agent and Notification System", "comments": "International Conference on Information and Communication Technology\n  for the Muslim World (ICT4M 2006), 21-23 November 2006, Kuala Lumpur,\n  Malaysia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The shift from an information society to a knowledge society require rapid\ninformation harvesting, reliable search and instantaneous on demand delivery.\nInformation extraction agents are used to explore and collect data available\nfrom Web, in order to effectively exploit such data for business purposes, such\nas automatic news filtering, advertisement or product searching and price\ncomparing. In this paper, we develop a real-time automatic harvesting agent for\nadverts posted on Servihoo web portal and an SMS-based notification system. It\nuses the URL of the web portal and the object model, i.e., the fields of\ninterests and a set of rules written using the HTML parsing functions to\nextract latest adverts information. The extraction engine executes the\nextraction rules and stores the information in a database to be processed for\nautomatic notification. This intelligent system helps to tremendously save\ntime. It also enables users or potential product buyers to react more quickly\nto changes and newly posted sales adverts, paving the way to real-time best buy\ndeals.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2010 05:32:33 GMT"}], "update_date": "2010-03-16", "authors_parsed": [["Doomun", "Razvi", ""], ["N.", "Lollmahamod", ""], ["Nadeem", "Auleear", ""], ["Aukin", "Mozafar", ""]]}, {"id": "1003.2682", "submitter": "David Spivak", "authors": "David I. Spivak", "title": "Table manipulation in simplicial databases", "comments": "8 pages.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In \\cite{Spi}, we developed a category of databases in which the schema of a\ndatabase is represented as a simplicial set. Each simplex corresponds to a\ntable in the database. There, our main concern was to find a categorical\nformulation of databases; the simplicial nature of the schemas was to some\ndegree unexpected and unexploited.\n  In the present note, we show how to use this geometric formulation\neffectively on a computer. If we think of each simplex as a polygonal tile, we\ncan imagine assembling custom databases by mixing and matching tiles. Queries\non this database can be performed by drawing paths through the resulting tile\nformations, selecting records at the start-point of this path and retrieving\ncorresponding records at its end-point.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2010 06:22:07 GMT"}], "update_date": "2010-03-16", "authors_parsed": [["Spivak", "David I.", ""]]}, {"id": "1003.3080", "submitter": "Achmad Benny Mutiara", "authors": "A. Muslim, A.B. Mutiara, C.M. Karyati, and P. Musa", "title": "An Algorithm for Index Multimedia Data (Video) using the Movement\n  Oriented Method for Real-time Online Services", "comments": "5 pages, International Conference on Robotics, Informatics,\n  Intelligence control system Technologies (RIIT'09)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia data is a form of data that can represent all types of data\n(images, sound and text). The use of multimedia data for the online application\nrequires a more comprehensive database in the use of storage media, Sorting /\nindexing, search and system / data searching. This is necessary in order to\nhelp providers and users to access multimedia data online. Systems that use of\nthe index image as a reference requires storage media so that the rules and\nrequire special expertise to obtain the desired file. Changes in multimedia\ndata into a series of stories / storyboard in the form of a text will help\nreduce the consumption of media storage, system index / sorting and search\napplications. Oriented Movement is one method that is being developed to change\nthe form of multimedia data into a storyboard.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2010 05:07:10 GMT"}], "update_date": "2010-03-17", "authors_parsed": [["Muslim", "A.", ""], ["Mutiara", "A. B.", ""], ["Karyati", "C. M.", ""], ["Musa", "P.", ""]]}, {"id": "1003.3530", "submitter": "Rajkumar Kannan", "authors": "Rajkumar Kannan", "title": "Topic Map: An Ontology Framework for Information Retrieval", "comments": "National Conference on Advances in Knowledge Management(NCAKM'10),\n  pp195-198, March 2010, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic classification techniques for organizing information are thesauri,\ntaxonomy and faceted classification. Topic map is relatively a new entrant to\nthis information space. Topic map standard describes how complex relationships\nbetween abstract concepts and real world resources can be represented using XML\nsyntax. This paper explores how topic map incorporates the traditional\ntechniques and what are its advantages and disadvantages in several dimensions\nsuch as content management, indexing, knowledge representation, constraint\nspecification and query languages in the context of information retrieval. The\nconstructs of topic maps are illustrated with a use-case implemented in XTM\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2010 09:17:26 GMT"}], "update_date": "2010-03-19", "authors_parsed": [["Kannan", "Rajkumar", ""]]}, {"id": "1003.3533", "submitter": "Rajkumar Kannan", "authors": "Rajkumar Kannan, Frederic Andres", "title": "Towards Automated Lecture Capture, Navigation and Delivery System for\n  Web-Lecture on Demand", "comments": "3rd International Conference on Data Management, March 2010, India", "journal-ref": "Book Chapter in Innovations and Advances in Computer Science and\n  Engineering, MacMillan Publishers, 386-394, 2010", "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Institutions all over the world are continuously exploring ways to use ICT in\nimproving teaching and learning effectiveness. The use of course web pages,\ndiscussion groups, bulletin boards, and e-mails have shown considerable impact\non teaching and learning in significant ways, across all disciplines. ELearning\nhas emerged as an alternative to traditional classroom-based education and\ntraining and web lectures can be a powerful addition to traditional lectures.\nThey can even serve as a main content source for learning, provided users can\nquickly navigate and locate relevant pages in a web lecture. A web lecture\nconsists of video and audio of the presenter and slides complemented with\nscreen capturing. In this paper, an automated approach for recording live\nlectures and for browsing available web lectures for on-demand applications by\nend users is presented.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2010 09:27:16 GMT"}], "update_date": "2010-03-19", "authors_parsed": [["Kannan", "Rajkumar", ""], ["Andres", "Frederic", ""]]}, {"id": "1003.3661", "submitter": "Michael Nelson", "authors": "Herbert Van de Sompel, Robert Sanderson, Michael L. Nelson, Lyudmila\n  L. Balakireva, Harihar Shankar, Scott Ainsworth", "title": "An HTTP-Based Versioning Mechanism for Linked Data", "comments": "Proceedings of Linked Data on the Web (LDOW2010), April 27, 2010,\n  Raleigh, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dereferencing a URI returns a representation of the current state of the\nresource identified by that URI. But, on the Web representations of prior\nstates of a resource are also available, for example, as resource versions in\nContent Management Systems or archival resources in Web Archives such as the\nInternet Archive. This paper introduces a resource versioning mechanism that is\nfully based on HTTP and uses datetime as a global version indicator. The\napproach allows \"follow your nose\" style navigation both from the current\ntime-generic resource to associated time-specific version resources as well as\namong version resources. The proposed versioning mechanism is congruent with\nthe Architecture of the World Wide Web, and is based on the Memento framework\nthat extends HTTP with transparent content negotiation in the datetime\ndimension. The paper shows how the versioning approach applies to Linked Data,\nand by means of a demonstrator built for DBpedia, it also illustrates how it\ncan be used to conduct a time-series analysis across versions of Linked Data\ndescriptions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2010 19:21:11 GMT"}], "update_date": "2010-03-19", "authors_parsed": [["Van de Sompel", "Herbert", ""], ["Sanderson", "Robert", ""], ["Nelson", "Michael L.", ""], ["Balakireva", "Lyudmila L.", ""], ["Shankar", "Harihar", ""], ["Ainsworth", "Scott", ""]]}, {"id": "1003.4067", "submitter": "William Jackson", "authors": "P. G. JansiRani, R. Bhaskaran", "title": "Computation of Reducts Using Topology and Measure of Significance of\n  Attributes", "comments": null, "journal-ref": "Journal of Computing, Volume 2, Issue 3, March 2010,\n  https://sites.google.com/site/journalofcomputing/", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data generated in the fields of science, technology, business and in many\nother fields of research are increasing in an exponential rate. The way to\nextract knowledge from a huge set of data is a challenging task. This paper\naims to propose a hybrid and viable method to deal with an information system\nin data mining, using topological techniques and the significance of the\nattributes measured using rough set theory, to compute the reduct, This will\nreduce the randomness in the process of elimination of redundant attributes,\nwhich, in turn, will reduce the complexity of the computation of reducts of an\ninformation system where a large amount of data have to be processed.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 05:34:18 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["JansiRani", "P. G.", ""], ["Bhaskaran", "R.", ""]]}, {"id": "1003.4146", "submitter": "Michael Bommarito II", "authors": "Michael J. Bommarito II, Daniel Martin Katz", "title": "A Mathematical Approach to the Study of the United States Code", "comments": "5 pages, 6 figures, 2 tables.", "journal-ref": null, "doi": "10.1016/j.physa.2010.05.057", "report-no": null, "categories": "cs.IR cs.CY cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United States Code (Code) is a document containing over 22 million words\nthat represents a large and important source of Federal statutory law. Scholars\nand policy advocates often discuss the direction and magnitude of changes in\nvarious aspects of the Code. However, few have mathematically formalized the\nnotions behind these discussions or directly measured the resulting\nrepresentations. This paper addresses the current state of the literature in\ntwo ways. First, we formalize a representation of the United States Code as the\nunion of a hierarchical network and a citation network over vertices containing\nthe language of the Code. This representation reflects the fact that the Code\nis a hierarchically organized document containing language and explicit\ncitations between provisions. Second, we use this formalization to measure\naspects of the Code as codified in October 2008, November 2009, and March 2010.\nThese measurements allow for a characterization of the actual changes in the\nCode over time. Our findings indicate that in the recent past, the Code has\ngrown in its amount of structure, interdependence, and language.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 12:41:01 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Bommarito", "Michael J.", "II"], ["Katz", "Daniel Martin", ""]]}, {"id": "1003.4418", "submitter": "Stefan Endrullis", "authors": "Stefan Endrullis, Andreas Thor, Erhard Rahm", "title": "Evaluation of Query Generators for Entity Search Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic web applications such as mashups need efficient access to web data\nthat is only accessible via entity search engines (e.g. product or publication\nsearch engines). However, most current mashup systems and applications only\nsupport simple keyword searches for retrieving data from search engines. We\npropose the use of more powerful search strategies building on so-called query\ngenerators. For a given set of entities query generators are able to\nautomatically determine a set of search queries to retrieve these entities from\nan entity search engine. We demonstrate the usefulness of query generators for\non-demand web data integration and evaluate the effectiveness and efficiency of\nquery generators for a challenging real-world integration scenario.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2010 13:55:40 GMT"}], "update_date": "2010-03-24", "authors_parsed": [["Endrullis", "Stefan", ""], ["Thor", "Andreas", ""], ["Rahm", "Erhard", ""]]}, {"id": "1003.5042", "submitter": "Ravindranath Chowdary C", "authors": "C Ravindranath Chowdary", "title": "Local Popularity based Page Link Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper we introduce the concept of dynamic link pages. A web site/page\ncontains a number of links to other pages. All the links are not equally\nimportant. Few links are more frequently visited and few rarely visited. In\nthis scenario, identifying the frequently used links and placing them in the\ntop left corner of the page will increase the user's satisfaction. This process\nwill reduce the time spent by a visitor on the page, as most of the times, the\npopular links are presented in the visible part of the screen itself. Also, a\nsite can be indexed based on the popular links in that page. This will increase\nthe efficiency of the retrieval system. We presented a model to display the\npopular links, and also proposed a method to increase the quality of retrieval\nsystem.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2010 05:51:30 GMT"}], "update_date": "2010-03-29", "authors_parsed": [["Chowdary", "C Ravindranath", ""]]}, {"id": "1003.5327", "submitter": "Bruno Goncalves", "authors": "Mark Meiss, Bruno Gon\\c{c}alves, Jos\\'e J. Ramasco, Alessandro\n  Flammini, Filippo Menczer", "title": "Agents, Bookmarks and Clicks: A topical model of Web traffic", "comments": "10 pages, 16 figures, 1 table - Long version of paper to appear in\n  Proceedings of the 21th ACM conference on Hypertext and Hypermedia", "journal-ref": "Proceedings of the 21th ACM conference on Hypertext and\n  hypermedia, 229 (2010)", "doi": "10.1145/1810617.1810658", "report-no": null, "categories": "cs.NI cs.IR cs.MA physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of aggregate and individual Web traffic has shown that PageRank is a\npoor model of how people navigate the Web. Using the empirical traffic patterns\ngenerated by a thousand users, we characterize several properties of Web\ntraffic that cannot be reproduced by Markovian models. We examine both\naggregate statistics capturing collective behavior, such as page and link\ntraffic, and individual statistics, such as entropy and session size. No model\ncurrently explains all of these empirical observations simultaneously. We show\nthat all of these traffic patterns can be explained by an agent-based model\nthat takes into account several realistic browsing behaviors. First, agents\nmaintain individual lists of bookmarks (a non-Markovian memory mechanism) that\nare used as teleportation targets. Second, agents can retreat along visited\nlinks, a branching mechanism that also allows us to reproduce behaviors such as\nthe use of a back button and tabbed browsing. Finally, agents are sustained by\nvisiting novel pages of topical interest, with adjacent pages being more\ntopically related to each other than distant ones. This modulates the\nprobability that an agent continues to browse or starts a new session, allowing\nus to recreate heterogeneous session lengths. The resulting model is capable of\nreproducing the collective and individual behaviors we observe in the empirical\ndata, reconciling the narrowly focused browsing patterns of individual users\nwith the extreme heterogeneity of aggregate traffic measurements. This result\nallows us to identify a few salient features that are necessary and sufficient\nto interpret the browsing patterns observed in our data. In addition to the\ndescriptive and explanatory power of such a model, our results may lead the way\nto more sophisticated, realistic, and effective ranking and crawling\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2010 22:15:15 GMT"}], "update_date": "2011-11-24", "authors_parsed": [["Meiss", "Mark", ""], ["Gon\u00e7alves", "Bruno", ""], ["Ramasco", "Jos\u00e9 J.", ""], ["Flammini", "Alessandro", ""], ["Menczer", "Filippo", ""]]}, {"id": "1003.5455", "submitter": "Alexei Chepelianskii", "authors": "A.D. Chepelianskii", "title": "Towards physical laws for software architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting from the pioneering works on software architecture precious\nguidelines have emerged to indicate how computer programs should be organized.\nFor example the \"separation of concerns\" suggests to split a program into\nmodules that overlap in functionality as little as possible. However these\nrecommendations are mainly conceptual and are thus hard to express in a\nquantitative form. Hence software architecture relies on the individual\nexperience and skill of the designers rather than on quantitative laws. In this\narticle I apply the methods developed for the classification of information on\nthe World-Wide-Web to study the organization of Open Source programs in an\nattempt to establish the statistical laws governing software architecture.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2010 08:33:46 GMT"}], "update_date": "2010-03-30", "authors_parsed": [["Chepelianskii", "A. D.", ""]]}]