[{"id": "0811.0405", "submitter": "Bernardo Huberman", "authors": "Gabor Szabo and Bernardo A. Huberman", "title": "Predicting the popularity of online content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for accurately predicting the long time popularity of\nonline content from early measurements of user access. Using two content\nsharing portals, Youtube and Digg, we show that by modeling the accrual of\nviews and votes on content offered by these services we can predict the\nlong-term dynamics of individual submissions from initial data. In the case of\nDigg, measuring access to given stories during the first two hours allows us to\nforecast their popularity 30 days ahead with remarkable accuracy, while\ndownloads of Youtube videos need to be followed for 10 days to attain the same\nperformance. The differing time scales of the predictions are shown to be due\nto differences in how content is consumed on the two portals: Digg stories\nquickly become outdated, while Youtube videos are still found long after they\nare initially submitted to the portal. We show that predictions are more\naccurate for submissions for which attention decays quickly, whereas\npredictions for evergreen content will be prone to larger errors.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2008 05:38:58 GMT"}], "update_date": "2008-11-06", "authors_parsed": [["Szabo", "Gabor", ""], ["Huberman", "Bernardo A.", ""]]}, {"id": "0811.0453", "submitter": "Cynthia Wagner CW", "authors": "Cynthia Wagner, and Christoph Schommer", "title": "CoZo+ - A Content Zoning Engine for textual documents", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content zoning can be understood as a segmentation of textual documents into\nzones. This is inspired by [6] who initially proposed an approach for the\nargumentative zoning of textual documents. With the prototypical CoZo+ engine,\nwe focus on content zoning towards an automatic processing of textual streams\nwhile considering only the actors as the zones. We gain information that can be\nused to realize an automatic recognition of content for pre-defined actors. We\nunderstand CoZo+ as a necessary pre-step towards an automatic generation of\nsummaries and to make intellectual ownership of documents detectable.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2008 09:08:32 GMT"}], "update_date": "2008-11-05", "authors_parsed": [["Wagner", "Cynthia", ""], ["Schommer", "Christoph", ""]]}, {"id": "0811.0603", "submitter": "Patricia Gautier", "authors": "Veronila Lux-Pogodalla (INIST), Eric San Juan", "title": "Query Refinement by Multi Word Term expansions and semantic synonymy", "comments": null, "journal-ref": "InSciT2006, medira : Espagne (2006)", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a system, TermWatch\n(https://stid-bdd.iut.univ-metz.fr/TermWatch/index.pl), which combines a\nlinguistic extraction of terms, their structuring into a terminological network\nwith a clustering algorithm. In this paper we explore its ability in\nintegrating the most promising aspects of the studies on query refinement:\nchoice of meaningful text units to cluster (domain terms), choice of tight\nsemantic relations with which to cluster terms, structuring of terms in a\nnetwork enabling abetter perception of domain concepts. We have run this\nexperiment on the 367 645 English abstracts of PASCAL 2005-2006 bibliographic\ndatabase (http://www.inist.fr) and compared the structured terminological\nresource automatically build by TermWarch to the English segment of TermScience\nresource (http://termsciences.inist.fr/) containing 88 211 terms.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2008 20:43:29 GMT"}], "update_date": "2008-11-05", "authors_parsed": [["Lux-Pogodalla", "Veronila", "", "INIST"], ["Juan", "Eric San", ""]]}, {"id": "0811.0717", "submitter": "Patricia Gautier", "authors": "Eric San Juan (INIST), Ivana Roche (INIST)", "title": "Visualization of association graphs for assisting the interpretation of\n  classifications", "comments": "International workshop on Webometrics, informetrics and\n  scientometrics. Seventh collnet meeting, Nancy : France (2005)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a query on the PASCAL database maintained by the INIST, we design user\ninterfaces to visualize and browse two types of graphs extracted from\nabstracts: 1) the graph of all associations between authors (co-author graph),\n2) the graph of strong associations between authors and terms automatically\nextracted from abstracts and grouped using linguistic variations. We adapt for\nthis purpose the TermWatch system that comprises a term extractor, a relation\nidentifier which yields the terminological network and a clustering module. The\nresults are output on two interfaces: a graphic one mapping the clusters in a\n2D space and a terminological hypertext network allowing the user to\ninteractively explore results and return to source texts.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2008 12:58:38 GMT"}], "update_date": "2008-11-06", "authors_parsed": [["Juan", "Eric San", "", "INIST"], ["Roche", "Ivana", "", "INIST"]]}, {"id": "0811.0719", "submitter": "Patricia Gautier", "authors": "Xavier Polanco (INIST), Ivana Roche (INIST), Dominique Besagni (INIST)", "title": "Web Usage Analysis: New Science Indicators and Co-usage", "comments": null, "journal-ref": "S\\'eminaire VSST 2006, Lille : France (2006)", "doi": null, "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new type of statistical analysis of the science and technical information\n(STI) in the Web context is produced. We propose a set of indicators about Web\nusers, visualized bibliographic records, and e-commercial transactions. In\naddition, we introduce two Web usage factors. Finally, we give an overview of\nthe co-usage analysis. For these tasks, we introduce a computer based system,\ncalled Miri@d, which produces descriptive statistical information about the Web\nusers' searching behaviour, and what is effectively used from a free access\ndigital bibliographical database. The system is conceived as a server of\nstatistical data which are carried out beforehand, and as an interactive server\nfor online statistical work. The results will be made available to analysts,\nwho can use this descriptive statistical information as raw data for their\nindicator design tasks, and as input for multivariate data analysis, clustering\nanalysis, and mapping. Managers also can exploit the results in order to\nimprove management and decision-making.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2008 13:00:52 GMT"}], "update_date": "2008-11-06", "authors_parsed": [["Polanco", "Xavier", "", "INIST"], ["Roche", "Ivana", "", "INIST"], ["Besagni", "Dominique", "", "INIST"]]}, {"id": "0811.1250", "submitter": "Ping Li", "authors": "Ping Li", "title": "Adaptive Base Class Boost for Multi-class Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the concept of ABC-Boost (Adaptive Base Class Boost) for\nmulti-class classification and present ABC-MART, a concrete implementation of\nABC-Boost. The original MART (Multiple Additive Regression Trees) algorithm has\nbeen very successful in large-scale applications. For binary classification,\nABC-MART recovers MART. For multi-class classification, ABC-MART considerably\nimproves MART, as evaluated on several public data sets.\n", "versions": [{"version": "v1", "created": "Sat, 8 Nov 2008 23:23:08 GMT"}], "update_date": "2008-11-11", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "0811.4186", "submitter": "Aleksandar Bradic M", "authors": "Aleksandar Bradic", "title": "Search Result Clustering via Randomized Partitioning of Query-Induced\n  Subgraphs", "comments": "16th Telecommunications Forum TELFOR 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an approach to search result clustering, using\npartitioning of underlying link graph. We define the notion of \"query-induced\nsubgraph\" and formulate the problem of search result clustering as a problem of\nefficient partitioning of given subgraph into topic-related clusters. Also, we\npropose a novel algorithm for approximative partitioning of such graph, which\nresults in cluster quality comparable to the one obtained by deterministic\nalgorithms, while operating in more efficient computation time, suitable for\npractical implementations. Finally, we present a practical clustering search\nengine developed as a part of this research and use it to get results about\nreal-world performance of proposed concepts.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2008 23:11:55 GMT"}], "update_date": "2008-11-27", "authors_parsed": [["Bradic", "Aleksandar", ""]]}, {"id": "0811.4603", "submitter": "Massimo Franceschet", "authors": "Massimo Franceschet", "title": "Frozen Footprints", "comments": "Added section about bibliometric distributions; improved section\n  about bibliometric maps; added references; changed title; latex format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bibliometrics has the ambitious goal of measuring science. To this end, it\nexploits the way science is disseminated trough scientific publications and the\nresulting citation network of scientific papers. We survey the main historical\ncontributions to the field, the most interesting bibliometric indicators, and\nthe most popular bibliometric data sources. Moreover, we discuss distributions\ncommonly used to model bibliometric phenomena and give an overview of methods\nto build bibliometric maps of science.\n", "versions": [{"version": "v1", "created": "Thu, 27 Nov 2008 18:12:28 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2009 08:36:07 GMT"}], "update_date": "2009-07-17", "authors_parsed": [["Franceschet", "Massimo", ""]]}, {"id": "0811.4717", "submitter": "Daniel Racoceanu", "authors": "Roxana Teodorescu (UPT, LAB), Daniel Racoceanu (LAB, IPAAL), Wee-Kheng\n  Leow (IPAAL, NUS), Vladimir Cretu (UPT)", "title": "Prospective Study for Semantic Inter-Media Fusion in Content-Based\n  Medical Image Retrieval", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": "Onco-media Teodorescu 2008", "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One important challenge in modern Content-Based Medical Image Retrieval\n(CBMIR) approaches is represented by the semantic gap, related to the\ncomplexity of the medical knowledge. Among the methods that are able to close\nthis gap in CBMIR, the use of medical thesauri/ontologies has interesting\nperspectives due to the possibility of accessing on-line updated relevant\nwebservices and to extract real-time medical semantic structured information.\nThe CBMIR approach proposed in this paper uses the Unified Medical Language\nSystem's (UMLS) Metathesaurus to perform a semantic indexing and fusion of\nmedical media. This fusion operates before the query processing (retrieval) and\nworks at an UMLS-compliant conceptual indexing level. Our purpose is to study\nvarious techniques related to semantic data alignment, preprocessing, fusion,\nclustering and retrieval, by evaluating the various techniques and highlighting\nfuture research directions. The alignment and the preprocessing are based on\npartial text/image retrieval feedback and on the data structure. We analyze\nvarious probabilistic, fuzzy and evidence-based approaches for the fusion\nprocess and different similarity functions for the retrieval process. All the\nproposed methods are evaluated on the Cross Language Evaluation Forum's (CLEF)\nmedical image retrieval benchmark, by focusing also on a more homogeneous\ncomponent medical image database: the Pathology Education Instructional\nResource (PEIR).\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2008 13:30:23 GMT"}], "update_date": "2008-12-01", "authors_parsed": [["Teodorescu", "Roxana", "", "UPT, LAB"], ["Racoceanu", "Daniel", "", "LAB, IPAAL"], ["Leow", "Wee-Kheng", "", "IPAAL, NUS"], ["Cretu", "Vladimir", "", "UPT"]]}]