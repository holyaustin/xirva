[{"id": "0810.0852", "submitter": "Joseph B. Keller Prof.", "authors": "Joseph B. Keller", "title": "Evaluation of Authors and Journals", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.HO cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method is presented for evaluating authors on the basis of citations. It\nassigns to each author a citation score which depends upon the number of times\nhe is cited, and upon the scores of the citers. The scores are found to be the\ncomponents of an eigenvector of a normalized citation matrix. The same method\ncan be applied to citation of journals by other journals, to evaluating teams\nin a league [1], etc.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2008 20:36:19 GMT"}], "update_date": "2008-10-07", "authors_parsed": [["Keller", "Joseph B.", ""]]}, {"id": "0810.1212", "submitter": "Pascal Vaillant", "authors": "Pascal Vaillant, Richard Nock and Claudia Henry", "title": "Analyse spectrale des textes: d\\'etection automatique des fronti\\`eres\n  de langue et de discours", "comments": "In French. 10 pages, 5 figures, LaTeX 2e using EPSF and custom\n  package taln2006.sty (designed by Pierre Zweigenbaum, ATALA). Proceedings of\n  the 13th annual French-speaking conference on Natural Language Processing:\n  `Traitement Automatique des Langues Naturelles' (TALN 2006), Louvain\n  (Leuven), Belgium, 10-13 April 2003", "journal-ref": "Verbum ex machina: Actes de la 13eme conference annuelle sur le\n  Traitement Automatique des Langues Naturelles (TALN 2006), p. 619-629.\n  Louvain (Leuven), Belgique, 10-13 avril 2006", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a theoretical framework within which information on the vocabulary\nof a given corpus can be inferred on the basis of statistical information\ngathered on that corpus. Inferences can be made on the categories of the words\nin the vocabulary, and on their syntactical properties within particular\nlanguages. Based on the same statistical data, it is possible to build matrices\nof syntagmatic similarity (bigram transition matrices) or paradigmatic\nsimilarity (probability for any pair of words to share common contexts). When\nclustered with respect to their syntagmatic similarity, words tend to group\ninto sublanguage vocabularies, and when clustered with respect to their\nparadigmatic similarity, into syntactic or semantic classes. Experiments have\nexplored the first of these two possibilities. Their results are interpreted in\nthe frame of a Markov chain modelling of the corpus' generative processe(s): we\nshow that the results of a spectral analysis of the transition matrix can be\ninterpreted as probability distributions of words within clusters. This method\nyields a soft clustering of the vocabulary into sublanguages which contribute\nto the generation of heterogeneous corpora. As an application, we show how\nmultilingual texts can be visually segmented into linguistically homogeneous\nsegments. Our method is specifically useful in the case of related languages\nwhich happened to be mixed in corpora.\n", "versions": [{"version": "v1", "created": "Tue, 7 Oct 2008 15:25:31 GMT"}], "update_date": "2008-10-08", "authors_parsed": [["Vaillant", "Pascal", ""], ["Nock", "Richard", ""], ["Henry", "Claudia", ""]]}, {"id": "0810.1261", "submitter": "Pascal Vaillant", "authors": "Richard Nock, Pascal Vaillant, Frank Nielsen and Claudia Henry", "title": "Soft Uncoupling of Markov Chains for Permeable Language Distinction: A\n  New Algorithm", "comments": "6 pages, 7 embedded figures, LaTeX 2e using the ecai2006.cls document\n  class and the algorithm2e.sty style file (+ standard packages like epsfig,\n  amsmath, amssymb, amsfonts...). Extends the short version contained in the\n  ECAI 2006 proceedings", "journal-ref": "ECAI 2006: 17th European Conference on Artificial Intelligence.\n  Riva del Garda, Italy, 29 August - 1st September 2006", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Without prior knowledge, distinguishing different languages may be a hard\ntask, especially when their borders are permeable. We develop an extension of\nspectral clustering -- a powerful unsupervised classification toolbox -- that\nis shown to resolve accurately the task of soft language distinction. At the\nheart of our approach, we replace the usual hard membership assignment of\nspectral clustering by a soft, probabilistic assignment, which also presents\nthe advantage to bypass a well-known complexity bottleneck of the method.\nFurthermore, our approach relies on a novel, convenient construction of a\nMarkov chain out of a corpus. Extensive experiments with a readily available\nsystem clearly display the potential of the method, which brings a visually\nappealing soft distinction of languages that may define altogether a whole\ncorpus.\n", "versions": [{"version": "v1", "created": "Tue, 7 Oct 2008 18:09:07 GMT"}], "update_date": "2008-10-08", "authors_parsed": [["Nock", "Richard", ""], ["Vaillant", "Pascal", ""], ["Nielsen", "Frank", ""], ["Henry", "Claudia", ""]]}, {"id": "0810.1732", "submitter": "Christopher Frenz", "authors": "Christopher M. Frenz", "title": "Introduction to Searching with Regular Expressions", "comments": "13 pages. From the Proceedings of the 2008 Trenton Computer Festival", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The explosive rate of information growth and availability often makes it\nincreasingly difficult to locate information pertinent to your needs. These\nproblems are often compounded when keyword based search methodologies are not\nadequate for describing the information you seek. In many instances,\ninformation such as Web site URLs, phone numbers, etc. can often be better\nidentified through the use of a textual pattern than by keyword. For example,\nmany more phone numbers could be picked up by a search for the pattern (XXX)\nXXX-XXXX, where X could be any digit, than would be by a search for any\nspecific phone number (i.e. the keyword approach). Programming languages\ntypically allow for the matching of textual patterns via the usage of regular\nexpressions. This tutorial will provide an introduction to the basics of\nprogramming regular expressions as well as provide an introduction to how\nregular expressions can be applied to data processing tasks such as information\nextraction and search refinement.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2008 19:57:31 GMT"}], "update_date": "2008-10-10", "authors_parsed": [["Frenz", "Christopher M.", ""]]}, {"id": "0810.2390", "submitter": "Simone Faro", "authors": "Simone Faro and Thierry Lecroq", "title": "Efficient Pattern Matching on Binary Strings", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The binary string matching problem consists in finding all the occurrences of\na pattern in a text where both strings are built on a binary alphabet. This is\nan interesting problem in computer science, since binary data are omnipresent\nin telecom and computer network applications. Moreover the problem finds\napplications also in the field of image processing and in pattern matching on\ncompressed texts. Recently it has been shown that adaptations of classical\nexact string matching algorithms are not very efficient on binary data. In this\npaper we present two efficient algorithms for the problem adapted to completely\navoid any reference to bits allowing to process pattern and text byte by byte.\nExperimental results show that the new algorithms outperform existing solutions\nin most cases.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2008 08:44:27 GMT"}, {"version": "v2", "created": "Wed, 15 Oct 2008 12:15:24 GMT"}], "update_date": "2008-10-15", "authors_parsed": [["Faro", "Simone", ""], ["Lecroq", "Thierry", ""]]}, {"id": "0810.2764", "submitter": "Nir Ailon", "authors": "Nir Ailon", "title": "A Simple Linear Ranking Algorithm Using Query Dependent Intercept\n  Variables", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The LETOR website contains three information retrieval datasets used as a\nbenchmark for testing machine learning ideas for ranking. Algorithms\nparticipating in the challenge are required to assign score values to search\nresults for a collection of queries, and are measured using standard IR ranking\nmeasures (NDCG, precision, MAP) that depend only the relative score-induced\norder of the results. Similarly to many of the ideas proposed in the\nparticipating algorithms, we train a linear classifier. In contrast with other\nparticipating algorithms, we define an additional free variable (intercept, or\nbenchmark) for each query. This allows expressing the fact that results for\ndifferent queries are incomparable for the purpose of determining relevance.\nThe cost of this idea is the addition of relatively few nuisance parameters.\nOur approach is simple, and we used a standard logistic regression library to\ntest it. The results beat the reported participating algorithms. Hence, it\nseems promising to combine our approach with other more complex ideas.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2008 19:03:10 GMT"}], "update_date": "2008-10-16", "authors_parsed": [["Ailon", "Nir", ""]]}, {"id": "0810.5057", "submitter": "Patricia Gautier", "authors": "Claire Fran\\c{c}ois (INIST), Jean-Charles Lamirel (INRIA Lorraine -\n  LORIA), Shadi Al Shehabi (INRIA Lorraine - LORIA)", "title": "Combining Advanced Visualization and Automatized Reasoning for\n  Webometrics: A Test Study", "comments": null, "journal-ref": "COLLNET 2006, France (2006)", "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a first attempt at performing a precise and automatic\nidentification of the linking behaviour in a scientific domain through the\nanalysis of the communication of the related academic institutions on the web.\nThe proposed approach is based on the paradigm of multiple viewpoint data\nanalysis (MVDA) than can be fruitfully exploited to highlight relationships\nbetween data, like websites, carrying several kinds of description. It uses the\nMultiSOM clustering and mapping method. The domain that has been chosen for\nthis study is the domain of Computer Science in Germany. The analysis is\nconduced on a set of 438 websites of this domain using all together, thematic,\ngeographic and linking information. It highlights interesting results\nconcerning both global and local linking behaviour.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2008 15:43:45 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2009 15:27:41 GMT"}], "update_date": "2009-10-20", "authors_parsed": [["Fran\u00e7ois", "Claire", "", "INIST"], ["Lamirel", "Jean-Charles", "", "INRIA Lorraine -\n  LORIA"], ["Shehabi", "Shadi Al", "", "INRIA Lorraine - LORIA"]]}, {"id": "0810.5407", "submitter": "Aleksandar Stojmirovi\\'c", "authors": "Aleksandar Stojmirovic", "title": "Quasi-metrics, Similarities and Searches: aspects of geometry of protein\n  datasets", "comments": "299 pages, 44 figures, 10 tables, 9 algorithms. PhD thesis in\n  mathematics defended in May 2005 at the Victoria University of Wellington,\n  Wellington, New Zealand (supervisors: Prof. Vladimir Pestov and Dr. Bill\n  Jordan)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR math.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A quasi-metric is a distance function which satisfies the triangle inequality\nbut is not symmetric: it can be thought of as an asymmetric metric. The central\nresult of this thesis, developed in Chapter 3, is that a natural correspondence\nexists between similarity measures between biological (nucleotide or protein)\nsequences and quasi-metrics.\n  Chapter 2 presents basic concepts of the theory of quasi-metric spaces and\nintroduces a new examples of them: the universal countable rational\nquasi-metric space and its bicompletion, the universal bicomplete separable\nquasi-metric space. Chapter 4 is dedicated to development of a notion of the\nquasi-metric space with Borel probability measure, or pq-space. The main result\nof this chapter indicates that `a high dimensional quasi-metric space is close\nto being a metric space'.\n  Chapter 5 investigates the geometric aspects of the theory of database\nsimilarity search in the context of quasi-metrics. The results about\n$pq$-spaces are used to produce novel theoretical bounds on performance of\nindexing schemes.\n  Finally, the thesis presents some biological applications. Chapter 6\nintroduces FSIndex, an indexing scheme that significantly accelerates\nsimilarity searches of short protein fragment datasets. Chapter 7 presents the\nprototype of the system for discovery of short functional protein motifs called\nPFMFind, which relies on FSIndex for similarity searches.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2008 03:14:17 GMT"}], "update_date": "2008-10-31", "authors_parsed": [["Stojmirovic", "Aleksandar", ""]]}, {"id": "0810.5428", "submitter": "Amitabha Bagchi", "authors": "Amitabha Bagchi, Garima Lahoti", "title": "Relating Web pages to enable information-gathering tasks", "comments": "In Proceedings of ACM Hypertext 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that relationships between Web pages are functions of the user's\nintent. We identify a class of Web tasks - information-gathering - that can be\nfacilitated by a search engine that provides links to pages which are related\nto the page the user is currently viewing. We define three kinds of intentional\nrelationships that correspond to whether the user is a) seeking sources of\ninformation, b) reading pages which provide information, or c) surfing through\npages as part of an extended information-gathering process. We show that these\nthree relationships can be productively mined using a combination of textual\nand link information and provide three scoring mechanisms that correspond to\nthem: {\\em SeekRel}, {\\em FactRel} and {\\em SurfRel}. These scoring mechanisms\nincorporate both textual and link information. We build a set of capacitated\nsubnetworks - each corresponding to a particular keyword - that mirror the\ninterconnection structure of the World Wide Web. The scores are computed by\ncomputing flows on these subnetworks. The capacities of the links are derived\nfrom the {\\em hub} and {\\em authority} values of the nodes they connect,\nfollowing the work of Kleinberg (1998) on assigning authority to pages in\nhyperlinked environments. We evaluated our scoring mechanism by running\nexperiments on four data sets taken from the Web. We present user evaluations\nof the relevance of the top results returned by our scoring mechanisms and\ncompare those to the top results returned by Google's Similar Pages feature,\nand the {\\em Companion} algorithm proposed by Dean and Henzinger (1999).\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2008 07:17:49 GMT"}, {"version": "v2", "created": "Wed, 19 May 2010 11:43:29 GMT"}], "update_date": "2010-05-20", "authors_parsed": [["Bagchi", "Amitabha", ""], ["Lahoti", "Garima", ""]]}]