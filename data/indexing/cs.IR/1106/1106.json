[{"id": "1106.0217", "submitter": "Philipp Schaer", "authors": "Philipp Schaer", "title": "Using Lotkaian Informetrics for Ranking in Digital Libraries", "comments": "4 pages; Proceedings of the ASIS&T European Workshop 2011 (AEW 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to propose the use of models, theories and laws\nin bibliometrics and scientometrics to enhance information retrieval processes,\nespecially ranking. A common pattern in many man-made data sets is Lotka's Law\nwhich follows the well-known power-law distributions. These informetric\ndistributions can be used to give an alternative order to large and scattered\nresult sets and can be applied as a new ranking mechanism. The\npolyrepresentation of information in Digital Library systems is used to enhance\nthe retrieval quality, to overcome the drawbacks of the typical term-based\nranking approaches and to enable users to explore retrieved document sets from\na different perspective.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:13:18 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Schaer", "Philipp", ""]]}, {"id": "1106.0248", "submitter": "C. Basu", "authors": "C. Basu, W. W. Cohen, H. Hirsh, C. Nevill-Manning", "title": "Technical Paper Recommendation: A Study in Combining Multiple\n  Information Sources", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 14, pages\n  231-252, 2001", "doi": "10.1613/jair.739", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing need to manage and exploit the proliferation of online data\nsources is opening up new opportunities for bringing people closer to the\nresources they need. For instance, consider a recommendation service through\nwhich researchers can receive daily pointers to journal papers in their fields\nof interest. We survey some of the known approaches to the problem of technical\npaper recommendation and ask how they can be extended to deal with multiple\ninformation sources. More specifically, we focus on a variant of this problem -\nrecommending conference paper submissions to reviewing committee members -\nwhich offers us a testbed to try different approaches. Using WHIRL - an\ninformation integration system - we are able to implement different\nrecommendation algorithms derived from information retrieval principles. We\nalso use a novel autonomous procedure for gathering reviewer interest\ninformation from the Web. We evaluate our approach and compare it to other\nmethods using preference data provided by members of the AAAI-98 conference\nreviewing committee along with data about the actual submissions.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:39:29 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Basu", "C.", ""], ["Cohen", "W. W.", ""], ["Hirsh", "H.", ""], ["Nevill-Manning", "C.", ""]]}, {"id": "1106.0718", "submitter": "Arun Kumar", "authors": "Arun Kumar, Christopher R\\'e", "title": "Probabilistic Management of OCR Data using an RDBMS", "comments": "41 pages including the appendix. Shorter version (without appendix)\n  to appear as a full research paper in VLDB 2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 4, pp.\n  322-333 (2011)", "doi": null, "report-no": "vol5no4/p322_arunkumar_vldb2012", "categories": "cs.DB cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digitization of scanned forms and documents is changing the data sources\nthat enterprises manage. To integrate these new data sources with enterprise\ndata, the current state-of-the-art approach is to convert the images to ASCII\ntext using optical character recognition (OCR) software and then to store the\nresulting ASCII text in a relational database. The OCR problem is challenging,\nand so the output of OCR often contains errors. In turn, queries on the output\nof OCR may fail to retrieve relevant answers. State-of-the-art OCR programs,\ne.g., the OCR powering Google Books, use a probabilistic model that captures\nmany alternatives during the OCR process. Only when the results of OCR are\nstored in the database, do these approaches discard the uncertainty. In this\nwork, we propose to retain the probabilistic models produced by OCR process in\na relational database management system. A key technical challenge is that the\nprobabilistic data produced by OCR software is very large (a single book blows\nup to 2GB from 400kB as ASCII). As a result, a baseline solution that\nintegrates these models with an RDBMS is over 1000x slower versus standard text\nprocessing for single table select-project queries. However, many applications\nmay have quality-performance needs that are in between these two extremes of\nASCII and the complete model output by the OCR software. Thus, we propose a\nnovel approximation scheme called Staccato that allows a user to trade recall\nfor query performance. Additionally, we provide a formal analysis of our\nscheme's properties, and describe how we integrate our scheme with\nstandard-RDBMS text indexing.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 18:01:59 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2011 00:33:41 GMT"}, {"version": "v3", "created": "Wed, 31 Aug 2011 07:34:44 GMT"}, {"version": "v4", "created": "Fri, 6 Jan 2012 04:59:24 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Kumar", "Arun", ""], ["R\u00e9", "Christopher", ""]]}, {"id": "1106.1521", "submitter": "Min-Hee Jang", "authors": "Min-Hee Jang, Sang-Wook Kim, Christos Faloutsos, Sunju Park", "title": "A Linear-Time Approximation of the Earth Mover's Distance", "comments": "This paper has been withdrawn by the author due to some mistakes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color descriptors are one of the important features used in content-based\nimage retrieval. The Dominant Color Descriptor (DCD) represents a few\nperceptually dominant colors in an image through color quantization. For image\nretrieval based on DCD, the earth mover's distance and the optimal color\ncomposition distance are proposed to measure the dissimilarity between two\nimages. Although providing good retrieval results, both methods are too\ntime-consuming to be used in a large image database. To solve the problem, we\npropose a new distance function that calculates an approximate earth mover's\ndistance in linear time. To calculate the dissimilarity in linear time, the\nproposed approach employs the space-filling curve for multidimensional color\nspace. To improve the accuracy, the proposed approach uses multiple curves and\nadjusts the color positions. As a result, our approach achieves\norder-of-magnitude time improvement but incurs small errors. We have performed\nextensive experiments to show the effectiveness and efficiency of the proposed\napproach. The results reveal that our approach achieves almost the same results\nwith the EMD in linear time.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2011 08:53:21 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2011 03:06:00 GMT"}, {"version": "v3", "created": "Wed, 10 Aug 2011 01:51:55 GMT"}], "update_date": "2011-08-11", "authors_parsed": [["Jang", "Min-Hee", ""], ["Kim", "Sang-Wook", ""], ["Faloutsos", "Christos", ""], ["Park", "Sunju", ""]]}, {"id": "1106.1523", "submitter": "Daniel Hienert", "authors": "Daniel Hienert, Philipp Schaer, Johann Schaible, Philipp Mayr", "title": "A Novel Combined Term Suggestion Service for Domain-Specific Digital\n  Libraries", "comments": "To be published in Proceedings of Theories and Practice in Digital\n  Libraries (TPDL), 2011", "journal-ref": null, "doi": "10.1007/978-3-642-24469-8_21", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive query expansion can assist users during their query formulation\nprocess. We conducted a user study with over 4,000 unique visitors and four\ndifferent design approaches for a search term suggestion service. As a basis\nfor our evaluation we have implemented services which use three different\nvocabularies: (1) user search terms, (2) terms from a terminology service and\n(3) thesaurus terms. Additionally, we have created a new combined service which\nutilizes thesaurus term and terms from a domain-specific search term\nre-commender. Our results show that the thesaurus-based method clearly is used\nmore often compared to the other single-method implementations. We interpret\nthis as a strong indicator that term suggestion mechanisms should be\ndomain-specific to be close to the user terminology. Our novel combined\napproach which interconnects a thesaurus service with additional statistical\nrelations out-performed all other implementations. All our observations show\nthat domain-specific vocabulary can support the user in finding alternative\nconcepts and formulating queries.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2011 09:04:58 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Hienert", "Daniel", ""], ["Schaer", "Philipp", ""], ["Schaible", "Johann", ""], ["Mayr", "Philipp", ""]]}, {"id": "1106.1925", "submitter": "Ryan Adams", "authors": "Ryan Prescott Adams, Richard S. Zemel", "title": "Ranking via Sinkhorn Propagation", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is of increasing importance to develop learning methods for ranking. In\ncontrast to many learning objectives, however, the ranking problem presents\ndifficulties due to the fact that the space of permutations is not smooth. In\nthis paper, we examine the class of rank-linear objective functions, which\nincludes popular metrics such as precision and discounted cumulative gain. In\nparticular, we observe that expectations of these gains are completely\ncharacterized by the marginals of the corresponding distribution over\npermutation matrices. Thus, the expectations of rank-linear objectives can\nalways be described through locations in the Birkhoff polytope, i.e.,\ndoubly-stochastic matrices (DSMs). We propose a technique for learning\nDSM-based ranking functions using an iterative projection operator known as\nSinkhorn normalization. Gradients of this operator can be computed via\nbackpropagation, resulting in an algorithm we call Sinkhorn propagation, or\nSinkProp. This approach can be combined with a wide range of gradient-based\napproaches to rank learning. We demonstrate the utility of SinkProp on several\ninformation retrieval data sets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 21:57:27 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2011 00:11:51 GMT"}], "update_date": "2011-06-15", "authors_parsed": [["Adams", "Ryan Prescott", ""], ["Zemel", "Richard S.", ""]]}, {"id": "1106.2229", "submitter": "Fionn Murtagh", "authors": "Pedro Contreras and Fionn Murtagh", "title": "Fast, Linear Time Hierarchical Clustering using the Baire Metric", "comments": "27 pages, 6 tables, 10 figures", "journal-ref": "Journal of Classification, July 2012, Volume 29, Issue 2, pp\n  118-143", "doi": "10.1007/s00357-012-9106-3", "report-no": null, "categories": "stat.ML cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Baire metric induces an ultrametric on a dataset and is of linear\ncomputational complexity, contrasted with the standard quadratic time\nagglomerative hierarchical clustering algorithm. In this work we evaluate\nempirically this new approach to hierarchical clustering. We compare\nhierarchical clustering based on the Baire metric with (i) agglomerative\nhierarchical clustering, in terms of algorithm properties; (ii) generalized\nultrametrics, in terms of definition; and (iii) fast clustering through k-means\npartititioning, in terms of quality of results. For the latter, we carry out an\nin depth astronomical study. We apply the Baire distance to spectrometric and\nphotometric redshifts from the Sloan Digital Sky Survey using, in this work,\nabout half a million astronomical objects. We want to know how well the (more\ncostly to determine) spectrometric redshifts can predict the (more easily\nobtained) photometric redshifts, i.e. we seek to regress the spectrometric on\nthe photometric redshifts, and we use clusterwise regression for this.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2011 12:05:43 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Contreras", "Pedro", ""], ["Murtagh", "Fionn", ""]]}, {"id": "1106.2289", "submitter": "Abdelkrim Bouramoul", "authors": "Abdelkrim Bouramoul, Mohamed-Khireddine Kholladi, Bich-Lien Doan", "title": "PRESY: A Context Based Query Reformulation Tool for Information\n  Retrieval on the Web", "comments": "8 pages", "journal-ref": "Journal of Computer Science (JCS) - ISSN: 15493636, Vol.6, No.4 :\n  470-477, April 2010", "doi": "10.3844/jcssp.2010.470.477", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problem Statement: The huge number of information on the web as well as the\ngrowth of new inexperienced users creates new challenges for information\nretrieval. It has become increasingly difficult for these users to find\nrelevant documents that satisfy their individual needs. Certainly the current\nsearch engines (such as Google, Bing and Yahoo) offer an efficient way to\nbrowse the web content. However, the result quality is highly based on uses\nqueries which need to be more precise to find relevant documents. This task\nstill complicated for the majority of inept users who cannot express their\nneeds with significant words in the query. For that reason, we believe that a\nreformulation of the initial user's query can be a good alternative to improve\nthe information selectivity. This study proposes a novel approach and presents\na prototype system called PRESY (Profile-based REformulation SYstem) for\ninformation retrieval on the web. Approach: It uses an incremental approach to\ncategorize users by constructing a contextual base. The latter is composed of\ntwo types of context (static and dynamic) obtained using the users' profiles.\nThe architecture proposed was implemented using .Net environment to perform\nqueries reformulating tests. Results: The experiments gives at the end of this\narticle show that the precision of the returned content is effectively\nimproved. The tests were performed with the most popular searching engine (i.e.\nGoogle, Bind and Yahoo) selected in particular for their high selectivity.\nAmong the given results, we found that query reformulation improve the first\nthree results by 10.7% and 11.7% of the next seven returned elements. So as we\ncan see the reformulation of users' initial queries improves the pertinence of\nreturned content.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2011 08:41:40 GMT"}], "update_date": "2011-06-14", "authors_parsed": [["Bouramoul", "Abdelkrim", ""], ["Kholladi", "Mohamed-Khireddine", ""], ["Doan", "Bich-Lien", ""]]}, {"id": "1106.2587", "submitter": "Christopher Hoobin", "authors": "Christopher Hoobin, Simon J. Puglisi and Justin Zobel", "title": "Relative Lempel-Ziv Factorization for Efficient Storage and Retrieval of\n  Web Collections", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 3, pp.\n  265-273 (2011)", "doi": null, "report-no": "vol5no3/p265_christopherhoobin_vldb2012", "categories": "cs.DS cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression techniques that support fast random access are a core component\nof any information system. Current state-of-the-art methods group documents\ninto fixed-sized blocks and compress each block with a general-purpose adaptive\nalgorithm such as GZIP. Random access to a specific document then requires\ndecompression of a block. The choice of block size is critical: it trades\nbetween compression effectiveness and document retrieval times. In this paper\nwe present a scalable compression method for large document collections that\nallows fast random access. We build a representative sample of the collection\nand use it as a dictionary in a LZ77-like encoding of the rest of the\ncollection, relative to the dictionary. We demonstrate on large collections,\nthat using a dictionary as small as 0.1% of the collection size, our algorithm\nis dramatically faster than previous methods, and in general gives much better\ncompression.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 00:53:40 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2011 03:26:13 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Hoobin", "Christopher", ""], ["Puglisi", "Simon J.", ""], ["Zobel", "Justin", ""]]}, {"id": "1106.2946", "submitter": "Jagadeesh Gorla", "authors": "Jagadeesh Gorla, Stephen Robertson, Jun Wang", "title": "A Unified Relevance Retrieval Model by Eliteness Hypothesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an Eliteness Hypothesis for information retrieval is proposed,\nwhere we define two generative processes to create information items and\nqueries. By assuming the deterministic relationships between the eliteness of\nterms and relevance, we obtain a new theoretical retrieval framework. The\nresulting ranking function is a unified one as it is capable of using available\nrelevance information on both the document and the query, which is otherwise\nunachievable by existing retrieval models. Our preliminary experiment on a\nsimple ranking function has demonstrated the potential of the approach.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2011 11:50:31 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2011 10:26:19 GMT"}, {"version": "v3", "created": "Wed, 22 Jun 2011 14:37:08 GMT"}, {"version": "v4", "created": "Tue, 28 Jun 2011 11:07:01 GMT"}, {"version": "v5", "created": "Mon, 4 Jul 2011 12:00:25 GMT"}, {"version": "v6", "created": "Sun, 14 Aug 2011 22:23:46 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Gorla", "Jagadeesh", ""], ["Robertson", "Stephen", ""], ["Wang", "Jun", ""]]}, {"id": "1106.3967", "submitter": "Emilio Ferrara", "authors": "Emilio Ferrara and Robert Baumgartner", "title": "Intelligent Self-Repairable Web Wrappers", "comments": "12 pages, 4 figures; Proceedings of the 12th International Conference\n  of the Italian Association for Artificial Intelligence, 2011", "journal-ref": "Lecture Notes in Computer Science, 6934:274-285, 2011", "doi": "10.1007/978-3-642-23954-0_26", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of information available on the Web grows at an incredible high\nrate. Systems and procedures devised to extract these data from Web sources\nalready exist, and different approaches and techniques have been investigated\nduring the last years. On the one hand, reliable solutions should provide\nrobust algorithms of Web data mining which could automatically face possible\nmalfunctioning or failures. On the other, in literature there is a lack of\nsolutions about the maintenance of these systems. Procedures that extract Web\ndata may be strictly interconnected with the structure of the data source\nitself; thus, malfunctioning or acquisition of corrupted data could be caused,\nfor example, by structural modifications of data sources brought by their\nowners. Nowadays, verification of data integrity and maintenance are mostly\nmanually managed, in order to ensure that these systems work correctly and\nreliably. In this paper we propose a novel approach to create procedures able\nto extract data from Web sources -- the so called Web wrappers -- which can\nface possible malfunctioning caused by modifications of the structure of the\ndata source, and can automatically repair themselves.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2011 17:02:40 GMT"}], "update_date": "2012-02-13", "authors_parsed": [["Ferrara", "Emilio", ""], ["Baumgartner", "Robert", ""]]}, {"id": "1106.4880", "submitter": "Ying Ding", "authors": "Qian Zhu, Yuyin Sun, Sashikiran Challa, Ying Ding, Michael S.\n  Lajiness, David J. Wild", "title": "Semantic Inference using Chemogenomics Data for Drug Discovery", "comments": "23 pages, 9 figures, 4 tables", "journal-ref": null, "doi": "10.1186/1471-2105-12-256", "report-no": null, "categories": "q-bio.QM cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background Semantic Web Technology (SWT) makes it possible to integrate and\nsearch the large volume of life science datasets in the public domain, as\ndemonstrated by well-known linked data projects such as LODD, Bio2RDF, and\nChem2Bio2RDF. Integration of these sets creates large networks of information.\nWe have previously described a tool called WENDI for aggregating information\npertaining to new chemical compounds, effectively creating evidence paths\nrelating the compounds to genes, diseases and so on. In this paper we examine\nthe utility of automatically inferring new compound-disease associations (and\nthus new links in the network) based on semantically marked-up versions of\nthese evidence paths, rule-sets and inference engines.\n  Results Through the implementation of a semantic inference algorithm, rule\nset, Semantic Web methods (RDF, OWL and SPARQL) and new interfaces, we have\ncreated a new tool called Chemogenomic Explorer that uses networks of\nontologically annotated RDF statements along with deductive reasoning tools to\ninfer new associations between the query structure and genes and diseases from\nWENDI results. The tool then permits interactive clustering and filtering of\nthese evidence paths.\n  Conclusions We present a new aggregate approach to inferring links between\nchemical compounds and diseases using semantic inference. This approach allows\nmultiple evidence paths between compounds and diseases to be identified using a\nrule-set and semantically annotated data, and for these evidence paths to be\nclustered to show overall evidence linking the compound to a disease. We\nbelieve this is a powerful approach, because it allows compound-disease\nrelationships to be ranked by the amount of evidence supporting them.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2011 03:21:56 GMT"}], "update_date": "2011-06-27", "authors_parsed": [["Zhu", "Qian", ""], ["Sun", "Yuyin", ""], ["Challa", "Sashikiran", ""], ["Ding", "Ying", ""], ["Lajiness", "Michael S.", ""], ["Wild", "David J.", ""]]}, {"id": "1106.5213", "submitter": "Maarten Clements", "authors": "Maarten Clements, Pavel Serdyukov, Arjen P. de Vries, Marcel J.T.\n  Reinders", "title": "Personalised Travel Recommendation based on Location Co-occurrence", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new task of recommending touristic locations based on a user's\nvisiting history in a geographically remote region. This can be used to plan a\ntouristic visit to a new city or country, or by travel agencies to provide\npersonalised travel deals.\n  A set of geotags is used to compute a location similarity model between two\ndifferent regions. The similarity between two landmarks is derived from the\nnumber of users that have visited both places, using a Gaussian density\nestimation of the co-occurrence space of location visits to cluster related\ngeotags. The standard deviation of the kernel can be used as a scale parameter\nthat determines the size of the recommended landmarks.\n  A personalised recommendation based on the location similarity model is\nevaluated on city and country scale and is able to outperform a location\nranking based on popularity. Especially when a tourist filter based on visit\nduration is enforced, the prediction can be accurately adapted to the\npreference of the user. An extensive evaluation based on manual annotations\nshows that more strict ranking methods like cosine similarity and a proposed\nRankDiff algorithm provide more serendipitous recommendations and are able to\nlink similar locations on opposite sides of the world.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2011 11:55:23 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Clements", "Maarten", ""], ["Serdyukov", "Pavel", ""], ["de Vries", "Arjen P.", ""], ["Reinders", "Marcel J. T.", ""]]}, {"id": "1106.5568", "submitter": "Lin Zhong", "authors": "Ardalan Amiri Sani, Wolfgang Richter, Xuan Bao, Trevor Narayan,\n  Mahadev Satyanarayanan, Lin Zhong, Romit Roy Choudhury", "title": "Opportunistic Content Search of Smartphone Photos", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report TR0627-2011, Rice University", "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photos taken by smartphone users can accidentally contain content that is\ntimely and valuable to others, often in real-time. We report the system design\nand evaluation of a distributed search system, Theia, for crowd-sourced\nreal-time content search of smartphone photos. Because smartphones are\nresource-constrained, Theia incorporates two key innovations to control search\ncost and improve search efficiency. Incremental Search expands search scope\nincrementally and exploits user feedback. Partitioned Search leverages the\ncloud to reduce the energy consumption of search in smartphones. Through user\nstudies, measurement studies, and field studies, we show that Theia reduces the\ncost per relevant photo by an average of 59%. It reduces the energy consumption\nof search by up to 55% and 81% compared to alternative strategies of executing\nentirely locally or entirely in the cloud. Search results from smartphones are\nobtained in seconds. Our experiments also suggest approaches to further improve\nthese results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2011 05:36:11 GMT"}], "update_date": "2011-06-29", "authors_parsed": [["Sani", "Ardalan Amiri", ""], ["Richter", "Wolfgang", ""], ["Bao", "Xuan", ""], ["Narayan", "Trevor", ""], ["Satyanarayanan", "Mahadev", ""], ["Zhong", "Lin", ""], ["Choudhury", "Romit Roy", ""]]}, {"id": "1106.6215", "submitter": "Leonardo Ermann", "authors": "Leonardo Ermann, Alexei D. Chepelianskii and Dima L. Shepelyansky", "title": "Towards two-dimensional search engines", "comments": "22 pages, 16 figures. Additional data available at\n  http://www.quantware.ups-tlse.fr/QWLIB/dvvadi/", "journal-ref": "J. Phys. A: Math. Theor. 45 (2012) 275101 (20pp)", "doi": "10.1088/1751-8113/45/27/275101", "report-no": null, "categories": "cs.IR cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the statistical properties of various directed networks using\nranking of their nodes based on the dominant vectors of the Google matrix known\nas PageRank and CheiRank. On average PageRank orders nodes proportionally to a\nnumber of ingoing links, while CheiRank orders nodes proportionally to a number\nof outgoing links. In this way the ranking of nodes becomes two-dimensional\nthat paves the way for development of two-dimensional search engines of new\ntype. Statistical properties of information flow on PageRank-CheiRank plane are\nanalyzed for networks of British, French and Italian Universities, Wikipedia,\nLinux Kernel, gene regulation and other networks. A special emphasis is done\nfor British Universities networks using the large database publicly available\nat UK. Methods of spam links control are also analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 13:00:49 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2012 13:31:02 GMT"}], "update_date": "2012-06-19", "authors_parsed": [["Ermann", "Leonardo", ""], ["Chepelianskii", "Alexei D.", ""], ["Shepelyansky", "Dima L.", ""]]}]