[{"id": "2001.00051", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Yulong Pei, Katia Sycara", "title": "Simultaneous Identification of Tweet Purpose and Position", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tweet classification has attracted considerable attention recently. Most of\nthe existing work on tweet classification focuses on topic classification,\nwhich classifies tweets into several predefined categories, and sentiment\nclassification, which classifies tweets into positive, negative and neutral.\nSince tweets are different from conventional text in that they generally are of\nlimited length and contain informal, irregular or new words, so it is difficult\nto determine user intention to publish a tweet and user attitude towards\ncertain topic. In this paper, we aim to simultaneously classify tweet purpose,\ni.e., the intention for user to publish a tweet, and position, i.e.,\nsupporting, opposing or being neutral to a given topic. By transforming this\nproblem to a multi-label classification problem, a multi-label classification\nmethod with post-processing is proposed. Experiments on real-world data sets\ndemonstrate the effectiveness of this method and the results outperform the\nindividual classification methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 17:09:54 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Pei", "Yulong", ""], ["Sycara", "Katia", ""]]}, {"id": "2001.00186", "submitter": "S Uskudarli", "authors": "S. Uskudarli, E. G\\\"okdeniz, and R. Canbeyli", "title": "NeuroBoun: An inquiry-based approach for exploring scientific literature\n  -- a use case in neuroscience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online scientific publications provide vast opportunities for researchers.\nAlas, the quantity and the rate of increase in the articles make the\nutilization of these resources very challenging. This work presents as\ninquiry-based approach to support the articulation of complex inter-related\nqueries to gain insights regarding how these subjects have been studied in\nconjunction with one another as reported in the scientific literature. For this\npurpose we introduce inquiries that represent inter-related subqueries that are\nof interest to a researcher. The inquiries are expanded to better capture the\nintent of the inquirer, from which several queries are generated that represent\nvarious juxtapositions of the subjects in consideration. The sets of queries\nare used to search repositories to yield results that reveal quantitative and\ntemporal relations among the subjects of the inquiry. A web-based tool,\nNeuroBoun, is developed as a proof of concept for medical publications found in\nPubMed. A use case related to the asymmetry of amygdala is presented to\nillustrate the potentials of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 10:37:53 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Uskudarli", "S.", ""], ["G\u00f6kdeniz", "E.", ""], ["Canbeyli", "R.", ""]]}, {"id": "2001.00267", "submitter": "Jianing Sun", "authors": "Jianing Sun, Yingxue Zhang, Chen Ma, Mark Coates, Huifeng Guo, Ruiming\n  Tang, Xiuqiang He", "title": "Multi-Graph Convolution Collaborative Filtering", "comments": "Accepted by the 19th IEEE International Conference on Data Mining\n  (IEEE ICDM 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized recommendation is ubiquitous, playing an important role in many\nonline services. Substantial research has been dedicated to learning vector\nrepresentations of users and items with the goal of predicting a user's\npreference for an item based on the similarity of the representations.\nTechniques range from classic matrix factorization to more recent deep learning\nbased methods. However, we argue that existing methods do not make full use of\nthe information that is available from user-item interaction data and the\nsimilarities between user pairs and item pairs. In this work, we develop a\ngraph convolution-based recommendation framework, named Multi-Graph Convolution\nCollaborative Filtering (Multi-GCCF), which explicitly incorporates multiple\ngraphs in the embedding learning process. Multi-GCCF not only expressively\nmodels the high-order information via a partite user-item interaction graph,\nbut also integrates the proximal information by building and processing\nuser-user and item-item graphs. Furthermore, we consider the intrinsic\ndifference between user nodes and item nodes when performing graph convolution\non the bipartite graph. We conduct extensive experiments on four publicly\naccessible benchmarks, showing significant improvements relative to several\nstate-of-the-art collaborative filtering and graph neural network-based\nrecommendation models. Further experiments quantitatively verify the\neffectiveness of each component of our proposed model and demonstrate that the\nlearned embeddings capture the important relationship structure.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 20:52:29 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Sun", "Jianing", ""], ["Zhang", "Yingxue", ""], ["Ma", "Chen", ""], ["Coates", "Mark", ""], ["Guo", "Huifeng", ""], ["Tang", "Ruiming", ""], ["He", "Xiuqiang", ""]]}, {"id": "2001.00293", "submitter": "Xin Wang", "authors": "Wenwu Zhu, Xin Wang, Peng Cui", "title": "Deep Learning for Learning Graph Representations", "comments": "51 pages, 8 figures", "journal-ref": null, "doi": "10.1007/978-3-030-31756-0_6", "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining graph data has become a popular research topic in computer science and\nhas been widely studied in both academia and industry given the increasing\namount of network data in the recent years. However, the huge amount of network\ndata has posed great challenges for efficient analysis. This motivates the\nadvent of graph representation which maps the graph into a low-dimension vector\nspace, keeping original graph structure and supporting graph inference. The\ninvestigation on efficient representation of a graph has profound theoretical\nsignificance and important realistic meaning, we therefore introduce some basic\nideas in graph representation/network embedding as well as some representative\nmodels in this chapter.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 02:13:28 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Zhu", "Wenwu", ""], ["Wang", "Xin", ""], ["Cui", "Peng", ""]]}, {"id": "2001.00471", "submitter": "Haruna Isah", "authors": "Kennedy Ralston, Yuhao Chen, Haruna Isah, Farhana Zulkernine", "title": "A Voice Interactive Multilingual Student Support System using IBM Watson", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems powered by artificial intelligence are being developed to be more\nuser-friendly by communicating with users in a progressively human-like\nconversational way. Chatbots, also known as dialogue systems, interactive\nconversational agents, or virtual agents are an example of such systems used in\na wide variety of applications ranging from customer support in the business\ndomain to companionship in the healthcare sector. It is becoming increasingly\nimportant to develop chatbots that can best respond to the personalized needs\nof their users so that they can be as helpful to the user as possible in a real\nhuman way. This paper investigates and compares three popular existing chatbots\nAPI offerings and then propose and develop a voice interactive and multilingual\nchatbot that can effectively respond to users mood, tone, and language using\nIBM Watson Assistant, Tone Analyzer, and Language Translator. The chatbot was\nevaluated using a use case that was targeted at responding to users needs\nregarding exam stress based on university students survey data generated using\nGoogle Forms. The results of measuring the chatbot effectiveness at analyzing\nresponses regarding exam stress indicate that the chatbot responding\nappropriately to the user queries regarding how they are feeling about exams\n76.5%. The chatbot could also be adapted for use in other application areas\nsuch as student info-centers, government kiosks, and mental health support\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 18:58:25 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Ralston", "Kennedy", ""], ["Chen", "Yuhao", ""], ["Isah", "Haruna", ""], ["Zulkernine", "Farhana", ""]]}, {"id": "2001.00565", "submitter": "Philipp Schaer", "authors": "Andr\\'e Calero Valdez and Lena Adam and Dennis Assenmacher and Laura\n  Burbach and Malte Bonart and Lena Frischlich and Philipp Schaer", "title": "Computational Methods in Professional Communication", "comments": null, "journal-ref": "2019 IEEE International Professional Communication Conference\n  (ProComm), Aachen, Germany, 2019, pp. 275-285", "doi": "10.1109/ProComm.2019.00063", "report-no": null, "categories": "cs.CY cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digitization of the world has also led to a digitization of communication\nprocesses. Traditional research methods fall short in understanding\ncommunication in digital worlds as the scope has become too large in volume,\nvariety, and velocity to be studied using traditional approaches. In this\npaper, we present computational methods and their use in public and mass\ncommunication research and how those could be adapted to professional\ncommunication research. The paper is a proposal for a panel in which the\npanelists, each an expert in their field, will present their current work using\ncomputational methods and will discuss transferability of these methods to\nprofessional communication.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 18:57:42 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Valdez", "Andr\u00e9 Calero", ""], ["Adam", "Lena", ""], ["Assenmacher", "Dennis", ""], ["Burbach", "Laura", ""], ["Bonart", "Malte", ""], ["Frischlich", "Lena", ""], ["Schaer", "Philipp", ""]]}, {"id": "2001.00569", "submitter": "Massimiliano Dal Mas", "authors": "Massimiliano Dal Mas", "title": "Emergent Behaviors from Folksonomy Driven Interactions", "comments": "6 pages, 5 figures; for details see: http://www.maxdalmas.com arXiv\n  admin note: text overlap with arXiv:1612.09574", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To reflect the evolving knowledge on the Web this paper considers ontologies\nbased on folksonomies according to a new concept structure called\n\"Folksodriven\" to represent folksonomies. This paper describes a research\nprogram for studying Folksodriven tags interactions leading to Folksodriven\ncluster behavior. The goal of the research is to understand the type of simple\nlocal interactions which produce complex and purposive group behaviors on\nFolksodriven tags. We describe a synthetic, bottom-up approach to studying\ngroup behavior, consisting of designing and testing a variety of social\ninteractions and cultural scenarios with Folksodriven tags. We propose a set of\nbasic interactions which can be used to structure and simplify the process of\nboth designing and analyzing emergent group behaviors. The presented behavior\nrepertories was developed and tested on a folksonomy environment.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 18:33:03 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Mas", "Massimiliano Dal", ""]]}, {"id": "2001.00802", "submitter": "Xinyi Li", "authors": "Xinyi Li, Chia-Jung Lee, Milad Shokouhi, Susan Dumais", "title": "Characterizing Reading Time on Enterprise Emails", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Email is an integral part of people's work and life, enabling them to perform\nactivities such as communicating, searching, managing tasks and storing\ninformation. Modern email clients take a step forward and help improve users'\nproductivity by automatically creating reminders, tasks or responses. The act\nof reading is arguably the only activity that is in common in most -- if not\nall -- of the interactions that users have with their emails.\n  In this paper, we characterize how users read their enterprise emails, and\nreveal the various contextual factors that impact reading time. Our approach\nstarts with a reading time analysis based on the reading events from a major\nemail platform, followed by a user study to provide explanations for some\ndiscoveries. We identify multiple temporal and user contextual factors that are\ncorrelated with reading time. For instance, email reading time is correlated\nwith user devices: on desktop reading time increases through the morning and\npeaks at noon but on mobile it increases through the evening till midnight. The\nreading time is also negatively correlated with the screen size.\n  We have established the connection between user status and reading time:\nusers spend more time reading emails when they have fewer meetings and busy\nhours during the day. In addition, we find that users also reread emails across\ndevices. Among the cross-device reading events, 76% of reread emails are first\nvisited on mobile and then on desktop. Overall, our study is the first to\ncharacterize enterprise email reading time on a very large scale. The findings\nprovide insights to develop better metrics and user models for understanding\nand improving email interactions.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 12:29:14 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Li", "Xinyi", ""], ["Lee", "Chia-Jung", ""], ["Shokouhi", "Milad", ""], ["Dumais", "Susan", ""]]}, {"id": "2001.00846", "submitter": "Diego Antognini", "authors": "Nikola Milojkovic, Diego Antognini, Giancarlo Bergamin, Boi Faltings\n  and Claudiu Musat", "title": "Multi-Gradient Descent for Multi-Objective Recommender Systems", "comments": "9 pages, 4 figures, Accepted at AAAI 2020 - Workshop on Interactive\n  and Conversational Recommendation Systems (WICRS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems need to mirror the complexity of the environment they are\napplied in. The more we know about what might benefit the user, the more\nobjectives the recommender system has. In addition there may be multiple\nstakeholders - sellers, buyers, shareholders - in addition to legal and ethical\nconstraints. Simultaneously optimizing for a multitude of objectives,\ncorrelated and not correlated, having the same scale or not, has proven\ndifficult so far.\n  We introduce a stochastic multi-gradient descent approach to recommender\nsystems (MGDRec) to solve this problem. We show that this exceeds\nstate-of-the-art methods in traditional objective mixtures, like revenue and\nrecall. Not only that, but through gradient normalization we can combine\nfundamentally different objectives, having diverse scales, into a single\ncoherent framework. We show that uncorrelated objectives, like the proportion\nof quality products, can be improved alongside accuracy. Through the use of\nstochasticity, we avoid the pitfalls of calculating full gradients and provide\na clear setting for its applicability.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 19:56:08 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 08:04:23 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 07:35:21 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Milojkovic", "Nikola", ""], ["Antognini", "Diego", ""], ["Bergamin", "Giancarlo", ""], ["Faltings", "Boi", ""], ["Musat", "Claudiu", ""]]}, {"id": "2001.00861", "submitter": "Kishaloy Halder", "authors": "Kishaloy Halder, Heng-Tze Cheng, Ellie Ka In Chio, Georgios Roumpos,\n  Tao Wu, Ritesh Agarwal", "title": "Modeling Information Need of Users in Search Sessions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users issue queries to Search Engines, and try to find the desired\ninformation in the results produced. They repeat this process if their\ninformation need is not met at the first place. It is crucial to identify the\nimportant words in a query that depict the actual information need of the user\nand will determine the course of a search session. To this end, we propose a\nsequence-to-sequence based neural architecture that leverages the set of past\nqueries issued by users, and results that were explored by them. Firstly, we\nemploy our model for predicting the words in the current query that are\nimportant and would be retained in the next query. Additionally, as a\ndownstream application of our model, we evaluate it on the widely popular task\nof next query suggestion. We show that our intuitive strategy of capturing\ninformation need can yield superior performance at these tasks on two large\nreal-world search log datasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 15:25:45 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Halder", "Kishaloy", ""], ["Cheng", "Heng-Tze", ""], ["Chio", "Ellie Ka In", ""], ["Roumpos", "Georgios", ""], ["Wu", "Tao", ""], ["Agarwal", "Ritesh", ""]]}, {"id": "2001.00994", "submitter": "Guruprasad Nayak", "authors": "Guruprasad Nayak, Rahul Ghosh, Xiaowei Jia, Varun Mithal, Vipin Kumar", "title": "Semi-supervised Classification using Attention-based Regularization on\n  Coarse-resolution Data", "comments": "To appear in the proceedings of the SIAM International Conference on\n  Data Mining (SDM20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world phenomena are observed at multiple resolutions. Predictive\nmodels designed to predict these phenomena typically consider different\nresolutions separately. This approach might be limiting in applications where\npredictions are desired at fine resolutions but available training data is\nscarce. In this paper, we propose classification algorithms that leverage\nsupervision from coarser resolutions to help train models on finer resolutions.\nThe different resolutions are modeled as different views of the data in a\nmulti-view framework that exploits the complementarity of features across\ndifferent views to improve models on both views. Unlike traditional multi-view\nlearning problems, the key challenge in our case is that there is no one-to-one\ncorrespondence between instances across different views in our case, which\nrequires explicit modeling of the correspondence of instances across\nresolutions. We propose to use the features of instances at different\nresolutions to learn the correspondence between instances across resolutions\nusing an attention mechanism.Experiments on the real-world application of\nmapping urban areas using satellite observations and sentiment classification\non text data show the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 21:29:26 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Nayak", "Guruprasad", ""], ["Ghosh", "Rahul", ""], ["Jia", "Xiaowei", ""], ["Mithal", "Varun", ""], ["Kumar", "Vipin", ""]]}, {"id": "2001.01033", "submitter": "Xiaochen Liu", "authors": "Xiaochen Liu, Yurong Jiang, Kyu-Han Kim, Ramesh Govindan", "title": "Grab: Fast and Accurate Sensor Processing for Cashier-Free Shopping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cashier-free shopping systems like Amazon Go improve shopping experience, but\ncan require significant store redesign. In this paper, we propose Grab, a\npractical system that leverages existing infrastructure and devices to enable\ncashier-free shopping. Grab needs to accurately identify and track customers,\nand associate each shopper with items he or she retrieves from shelves. To do\nthis, it uses a keypoint-based pose tracker as a building block for\nidentification and tracking, develops robust feature-based face trackers, and\nalgorithms for associating and tracking arm movements. It also uses a\nprobabilistic framework to fuse readings from camera, weight and RFID sensors\nin order to accurately assess which shopper picks up which item. In experiments\nfrom a pilot deployment in a retail store, Grab can achieve over 90% precision\nand recall even when 40% of shopping actions are designed to confuse the\nsystem. Moreover, Grab has optimizations that help reduce investment in\ncomputing infrastructure four-fold.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 04:12:06 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Liu", "Xiaochen", ""], ["Jiang", "Yurong", ""], ["Kim", "Kyu-Han", ""], ["Govindan", "Ramesh", ""]]}, {"id": "2001.01296", "submitter": "Pedro Ramaciotti Morales", "authors": "Pedro Ramaciotti Morales, Robin Lamarche-Perrin, Raphael\n  Fournier-S'niehotta, Remy Poulain, Lionel Tabourier, and Fabien Tarissan", "title": "Measuring Diversity in Heterogeneous Information Networks", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.IR cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diversity is a concept relevant to numerous domains of research varying from\necology, to information theory, and to economics, to cite a few. It is a notion\nthat is steadily gaining attention in the information retrieval, network\nanalysis, and artificial neural networks communities. While the use of\ndiversity measures in network-structured data counts a growing number of\napplications, no clear and comprehensive description is available for the\ndifferent ways in which diversities can be measured. In this article, we\ndevelop a formal framework for the application of a large family of diversity\nmeasures to heterogeneous information networks (HINs), a flexible, widely-used\nnetwork data formalism. This extends the application of diversity measures,\nfrom systems of classifications and apportionments, to more complex relations\nthat can be better modeled by networks. In doing so, we not only provide an\neffective organization of multiple practices from different domains, but also\nunearth new observables in systems modeled by heterogeneous information\nnetworks. We illustrate the pertinence of our approach by developing different\napplications related to various domains concerned by both diversity and\nnetworks. In particular, we illustrate the usefulness of these new proposed\nobservables in the domains of recommender systems and social media studies,\namong other fields.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 19:21:50 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 18:21:04 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 12:23:01 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Morales", "Pedro Ramaciotti", ""], ["Lamarche-Perrin", "Robin", ""], ["Fournier-S'niehotta", "Raphael", ""], ["Poulain", "Remy", ""], ["Tabourier", "Lionel", ""], ["Tarissan", "Fabien", ""]]}, {"id": "2001.01323", "submitter": "Jishnu Ray Chowdhury", "authors": "Jishnu Ray Chowdhury, Cornelia Caragea, Doina Caragea", "title": "On Identifying Hashtags in Disaster Twitter Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tweet hashtags have the potential to improve the search for information\nduring disaster events. However, there is a large number of disaster-related\ntweets that do not have any user-provided hashtags. Moreover, only a small\nnumber of tweets that contain actionable hashtags are useful for disaster\nresponse. To facilitate progress on automatic identification (or extraction) of\ndisaster hashtags for Twitter data, we construct a unique dataset of\ndisaster-related tweets annotated with hashtags useful for filtering actionable\ninformation. Using this dataset, we further investigate Long Short Term\nMemory-based models within a Multi-Task Learning framework. The best performing\nmodel achieves an F1-score as high as 92.22%. The dataset, code, and other\nresources are available on Github.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 22:37:17 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Chowdhury", "Jishnu Ray", ""], ["Caragea", "Cornelia", ""], ["Caragea", "Doina", ""]]}, {"id": "2001.01588", "submitter": "Chantana Chantrapornchai", "authors": "Chantana Chantrapornchai, Aphisit Tunsakul", "title": "Information Extraction based on Named Entity for Tourism Corpus", "comments": "6 pages, 9 figures", "journal-ref": "16th International Joint Conference on Computer Science and\n  Software Engineering (JCSSE), 2019, pp. 187-192", "doi": "10.1109/JCSSE.2019.8864166", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tourism information is scattered around nowadays. To search for the\ninformation, it is usually time consuming to browse through the results from\nsearch engine, select and view the details of each accommodation. In this\npaper, we present a methodology to extract particular information from full\ntext returned from the search engine to facilitate the users. Then, the users\ncan specifically look to the desired relevant information. The approach can be\nused for the same task in other domains. The main steps are 1) building\ntraining data and 2) building recognition model. First, the tourism data is\ngathered and the vocabularies are built. The raw corpus is used to train for\ncreating vocabulary embedding. Also, it is used for creating annotated data.\nThe process of creating named entity annotation is presented. Then, the\nrecognition model of a given entity type can be built. From the experiments,\ngiven hotel description, the model can extract the desired entity,i.e, name,\nlocation, facility. The extracted data can further be stored as a structured\ninformation, e.g., in the ontology format, for future querying and inference.\nThe model for automatic named entity identification, based on machine learning,\nyields the error ranging 8%-25%.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 17:16:28 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Chantrapornchai", "Chantana", ""], ["Tunsakul", "Aphisit", ""]]}, {"id": "2001.01669", "submitter": "Mi Khine Oo", "authors": "Mi Khine Oo and May Aye Khine", "title": "Topic Extraction of Crawled Documents Collection using Correlated Topic\n  Model in MapReduce Framework", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tremendous increase in the amount of available research documents impels\nresearchers to propose topic models to extract the latent semantic themes of a\ndocuments collection. However, how to extract the hidden topics of the\ndocuments collection has become a crucial task for many topic model\napplications. Moreover, conventional topic modeling approaches suffer from the\nscalability problem when the size of documents collection increases. In this\npaper, the Correlated Topic Model with variational Expectation-Maximization\nalgorithm is implemented in MapReduce framework to solve the scalability\nproblem. The proposed approach utilizes the dataset crawled from the public\ndigital library. In addition, the full-texts of the crawled documents are\nanalysed to enhance the accuracy of MapReduce CTM. The experiments are\nconducted to demonstrate the performance of the proposed algorithm. From the\nevaluation, the proposed approach has a comparable performance in terms of\ntopic coherences with LDA implemented in MapReduce framework.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 17:09:21 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Oo", "Mi Khine", ""], ["Khine", "May Aye", ""]]}, {"id": "2001.01708", "submitter": "Thuan Nguyen", "authors": "Thuan Nguyen and Thinh Nguyen", "title": "Communication-Channel Optimized Partition", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an original discrete source X with the distribution p_X that is\ncorrupted by noise to produce the noisy data Y with the given joint\ndistribution p(X, Y). A quantizer/classifier Q : Y -> Z is then used to\nclassify/quantize the data Y to the discrete partitioned output Z with\nprobability distribution p_Z. Next, Z is transmitted over a deterministic\nchannel with a given channel matrix A that produces the final discrete output\nT. One wants to design the optimal quantizer/classifier Q^* such that the cost\nfunction F(X; T) between the input X and the final output T is minimized while\nthe probability of the partitioned output Z satisfies a concave constraint\nG(p_Z) < C. Our results generalized some famous previous results. First, an\niteration linear time complexity algorithm is proposed to find the local\noptimal quantizer. Second, we show that the optimal partition should produce a\nhard partition that is equivalent to the cuts by hyper-planes in the\nprobability space of the posterior probability p(X|Y). This result finally\nprovides a polynomial-time algorithm to find the globally optimal quantizer.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 18:51:59 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Nguyen", "Thuan", ""], ["Nguyen", "Thinh", ""]]}, {"id": "2001.01828", "submitter": "Xiaofeng Zhu", "authors": "Xiaofeng Zhu, Diego Klabjan", "title": "Listwise Learning to Rank by Exploring Unique Ratings", "comments": null, "journal-ref": "WSDM 2020", "doi": "10.1145/3336191.3371814", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose new listwise learning-to-rank models that mitigate\nthe shortcomings of existing ones. Existing listwise learning-to-rank models\nare generally derived from the classical Plackett-Luce model, which has three\nmajor limitations. (1) Its permutation probabilities overlook ties, i.e., a\nsituation when more than one document has the same rating with respect to a\nquery. This can lead to imprecise permutation probabilities and inefficient\ntraining because of selecting documents one by one. (2) It does not favor\ndocuments having high relevance. (3) It has a loose assumption that sampling\ndocuments at different steps is independent. To overcome the first two\nlimitations, we model ranking as selecting documents from a candidate set based\non unique rating levels in decreasing order. The number of steps in training is\ndetermined by the number of unique rating levels. We propose a new loss\nfunction and associated four models for the entire sequence of weighted\nclassification tasks by assigning high weights to the selected documents with\nhigh ratings for optimizing Normalized Discounted Cumulative Gain (NDCG). To\novercome the final limitation, we further propose a novel and efficient way of\nrefining prediction scores by combining an adapted Vanilla Recurrent Neural\nNetwork (RNN) model with pooling given selected documents at previous steps. We\nencode all of the documents already selected by an RNN model. In a single step,\nwe rank all of the documents with the same ratings using the last cell of the\nRNN multiple times. We have implemented our models using three settings: neural\nnetworks, neural networks with gradient boosting, and regression trees with\ngradient boosting. We have conducted experiments on four public datasets. The\nexperiments demonstrate that the models notably outperform state-of-the-art\nlearning-to-rank models.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 00:50:37 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 03:51:49 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2020 01:55:15 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Zhu", "Xiaofeng", ""], ["Klabjan", "Diego", ""]]}, {"id": "2001.01836", "submitter": "Thuan Nguyen", "authors": "Thuan Nguyen and Thinh Nguyen", "title": "On the Uniqueness of Binary Quantizers for Maximizing Mutual Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a channel with a binary input X being corrupted by a\ncontinuous-valued noise that results in a continuous-valued output Y. An\noptimal binary quantizer is used to quantize the continuous-valued output Y to\nthe final binary output Z to maximize the mutual information I(X; Z). We show\nthat when the ratio of the channel conditional density r(y) = P(Y=y|X=0)/ P(Y\n=y|X=1) is a strictly increasing/decreasing function of y, then a quantizer\nhaving a single threshold can maximize mutual information. Furthermore, we show\nthat an optimal quantizer (possibly with multiple thresholds) is the one with\nthe thresholding vector whose elements are all the solutions of r(y) = r* for\nsome constant r* > 0. Interestingly, the optimal constant r* is unique. This\nuniqueness property allows for fast algorithmic implementation such as a\nbisection algorithm to find the optimal quantizer. Our results also confirm\nsome previous results using alternative elementary proofs. We show some\nnumerical examples of applying our results to channels with additive Gaussian\nnoises.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 01:38:01 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Nguyen", "Thuan", ""], ["Nguyen", "Thinh", ""]]}, {"id": "2001.01895", "submitter": "Philip Andrew Collender", "authors": "Philip A. Collender, Zhiyue Tom Hu, Charles Li, Qu Cheng, Xintong Li,\n  Yue You, Song Liang, Changhong Yang, Justin V. Remais", "title": "Machine-learning classifiers for logographic name matching in public\n  health applications: approaches for incorporating phonetic, visual, and\n  keystroke similarity in large-scale probabilistic record linkage", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Approximate string-matching methods to account for complex variation in\nhighly discriminatory text fields, such as personal names, can enhance\nprobabilistic record linkage. However, discriminating between matching and\nnon-matching strings is challenging for logographic scripts, where similarities\nin pronunciation, appearance, or keystroke sequence are not directly encoded in\nthe string data. We leverage a large Chinese administrative dataset with known\nmatch status to develop logistic regression and Xgboost classifiers integrating\nmeasures of visual, phonetic, and keystroke similarity to enhance\nidentification of potentially-matching name pairs. We evaluate three methods of\nleveraging name similarity scores in large-scale probabilistic record linkage,\nwhich can adapt to varying match prevalence and information in supporting\nfields: (1) setting a threshold score based on predicted quality of\nname-matching across all record pairs; (2) setting a threshold score based on\npredicted discriminatory power of the linkage model; and (3) using empirical\nscore distributions among matches and nonmatches to perform Bayesian adjustment\nof matching probabilities estimated from exact-agreement linkage. In\nexperiments on holdout data, as well as data simulated with varying name error\nrates and supporting fields, a logistic regression classifier incorporated via\nthe Bayesian method demonstrated marked improvements over exact-agreement\nlinkage with respect to discriminatory power, match probability estimation, and\naccuracy, reducing the total number of misclassified record pairs by 21% in\ntest data and up to an average of 93% in simulated datasets. Our results\ndemonstrate the value of incorporating visual, phonetic, and keystroke\nsimilarity for logographic name matching, as well as the promise of our\nBayesian approach to leverage name-matching within large-scale record linkage.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 05:21:21 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Collender", "Philip A.", ""], ["Hu", "Zhiyue Tom", ""], ["Li", "Charles", ""], ["Cheng", "Qu", ""], ["Li", "Xintong", ""], ["You", "Yue", ""], ["Liang", "Song", ""], ["Yang", "Changhong", ""], ["Remais", "Justin V.", ""]]}, {"id": "2001.02214", "submitter": "Nguyen Vo", "authors": "Di You, Nguyen Vo, Kyumin Lee, Qiang Liu", "title": "Attributed Multi-Relational Attention Network for Fact-checking URL\n  Recommendation", "comments": "CIKM2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  To combat fake news, researchers mostly focused on detecting fake news and\njournalists built and maintained fact-checking sites (e.g., Snopes.com and\nPolitifact.com). However, fake news dissemination has been greatly promoted via\nsocial media sites, and these fact-checking sites have not been fully utilized.\nTo overcome these problems and complement existing methods against fake news,\nin this paper we propose a deep-learning based fact-checking URL recommender\nsystem to mitigate impact of fake news in social media sites such as Twitter\nand Facebook. In particular, our proposed framework consists of a\nmulti-relational attentive module and a heterogeneous graph attention network\nto learn complex/semantic relationship between user-URL pairs, user-user pairs,\nand URL-URL pairs. Extensive experiments on a real-world dataset show that our\nproposed framework outperforms eight state-of-the-art recommendation models,\nachieving at least 3~5.3% improvement.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 18:26:38 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["You", "Di", ""], ["Vo", "Nguyen", ""], ["Lee", "Kyumin", ""], ["Liu", "Qiang", ""]]}, {"id": "2001.02344", "submitter": "Yang Zhang", "authors": "Yang Zhang, Qiang Ma", "title": "Citation Recommendations Considering Content and Structural Context\n  Embedding", "comments": null, "journal-ref": "2020 IEEE International Conference on Big Data and Smart Computing", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of academic papers being published is increasing exponentially in\nrecent years, and recommending adequate citations to assist researchers in\nwriting papers is a non-trivial task. Conventional approaches may not be\noptimal, as the recommended papers may already be known to the users, or be\nsolely relevant to the surrounding context but not other ideas discussed in the\nmanuscript. In this work, we propose a novel embedding algorithm DocCit2Vec,\nalong with the new concept of ``structural context'', to tackle the\naforementioned issues. The proposed approach demonstrates superior performances\nto baseline models in extensive experiments designed to simulate practical\nusage scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 02:48:23 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Zhang", "Yang", ""], ["Ma", "Qiang", ""]]}, {"id": "2001.02438", "submitter": "Shruti Tople", "authors": "Bijeeta Pal and Shruti Tople", "title": "To Transfer or Not to Transfer: Misclassification Attacks Against\n  Transfer Learned Text Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning --- transferring learned knowledge --- has brought a\nparadigm shift in the way models are trained. The lucrative benefits of\nimproved accuracy and reduced training time have shown promise in training\nmodels with constrained computational resources and fewer training samples.\nSpecifically, publicly available text-based models such as GloVe and BERT that\nare trained on large corpus of datasets have seen ubiquitous adoption in\npractice. In this paper, we ask, \"can transfer learning in text prediction\nmodels be exploited to perform misclassification attacks?\" As our main\ncontribution, we present novel attack techniques that utilize unintended\nfeatures learnt in the teacher (public) model to generate adversarial examples\nfor student (downstream) models. To the best of our knowledge, ours is the\nfirst work to show that transfer learning from state-of-the-art word-based and\nsentence-based teacher models increase the susceptibility of student models to\nmisclassification attacks. First, we propose a novel word-score based attack\nalgorithm for generating adversarial examples against student models trained\nusing context-free word-level embedding model. On binary classification tasks\ntrained using the GloVe teacher model, we achieve an average attack accuracy of\n97% for the IMDB Movie Reviews and 80% for the Fake News Detection. For\nmulti-class tasks, we divide the Newsgroup dataset into 6 and 20 classes and\nachieve an average attack accuracy of 75% and 41% respectively. Next, we\npresent length-based and sentence-based misclassification attacks for the Fake\nNews Detection task trained using a context-aware BERT model and achieve 78%\nand 39% attack accuracy respectively. Thus, our results motivate the need for\ndesigning training techniques that are robust to unintended feature learning,\nspecifically for transfer learned models.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 10:26:55 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Pal", "Bijeeta", ""], ["Tople", "Shruti", ""]]}, {"id": "2001.02520", "submitter": "Azam Rabiee", "authors": "Marzieh Pourhojjati-Sabet and Azam Rabiee", "title": "A Soft Recommender System for Social Networks", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent social recommender systems benefit from friendship graph to make an\naccurate recommendation, believing that friends in a social network have\nexactly the same interests and preferences. Some studies have benefited from\nhard clustering algorithms (such as K-means) to determine the similarity\nbetween users and consequently to define degree of friendships. In this paper,\nwe went a step further to identify true friends for making even more realistic\nrecommendations. we calculated the similarity between users, as well as the\ndependency between a user and an item. Our hypothesis is that due to the\nuncertainties in user preferences, the fuzzy clustering, instead of the\nclassical hard clustering, is beneficial in accurate recommendations. We\nincorporated the C-means algorithm to get different membership degrees of soft\nusers' clusters. Then, the users' similarity metric is defined according to the\nsoft clusters. Later, in a training scheme we determined the latent\nrepresentations of users and items, extracting from the huge and sparse\nuser-item-tag matrix using matrix factorization. In the parameter tuning, we\nfound the optimum coefficients for the influence of our soft social\nregularization and the user-item dependency terms. Our experimental results\nconvinced that the proposed fuzzy similarity metric improves the\nrecommendations in real data compared to the baseline social recommender system\nwith the hard clustering.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 13:38:09 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Pourhojjati-Sabet", "Marzieh", ""], ["Rabiee", "Azam", ""]]}, {"id": "2001.02660", "submitter": "Joobin Gharibshah", "authors": "Joobin Gharibshah, Evangelos E. Papalexakis, Michalis Faloutsos", "title": "REST: A Thread Embedding Approach for Identifying and Classifying\n  User-specified Information in Security Forums", "comments": "Accepted in ICWSM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How can we extract useful information from a security forum? We focus on\nidentifying threads of interest to a security professional: (a) alerts of\nworrisome events, such as attacks, (b) offering of malicious services and\nproducts, (c) hacking information to perform malicious acts, and (d) useful\nsecurity-related experiences. The analysis of security forums is in its infancy\ndespite several promising recent works. Novel approaches are needed to address\nthe challenges in this domain: (a) the difficulty in specifying the \"topics\" of\ninterest efficiently, and (b) the unstructured and informal nature of the text.\nWe propose, REST, a systematic methodology to: (a) identify threads of interest\nbased on a, possibly incomplete, bag of words, and (b) classify them into one\nof the four classes above. The key novelty of the work is a multi-step weighted\nembedding approach: we project words, threads and classes in appropriate\nembedding spaces and establish relevance and similarity there. We evaluate our\nmethod with real data from three security forums with a total of 164k posts and\n21K threads. First, REST robustness to initial keyword selection can extend the\nuser-provided keyword set and thus, it can recover from missing keywords.\nSecond, REST categorizes the threads into the classes of interest with superior\naccuracy compared to five other methods: REST exhibits an accuracy between\n63.3-76.9%. We see our approach as a first step for harnessing the wealth of\ninformation of online forums in a user-friendly way, since the user can loosely\nspecify her keywords of interest.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 18:04:52 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 19:14:16 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Gharibshah", "Joobin", ""], ["Papalexakis", "Evangelos E.", ""], ["Faloutsos", "Michalis", ""]]}, {"id": "2001.02669", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Manish Sharma, Vijaya Saradhi", "title": "A Correspondence Analysis Framework for Author-Conference\n  Recommendations", "comments": "49 pages including references, 6 figures, 15 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many years, achievements and discoveries made by scientists are made\naware through research papers published in appropriate journals or conferences.\nOften, established scientists and especially newbies are caught up in the\ndilemma of choosing an appropriate conference to get their work through. Every\nscientific conference and journal is inclined towards a particular field of\nresearch and there is a vast multitude of them for any particular field.\nChoosing an appropriate venue is vital as it helps in reaching out to the right\naudience and also to further one's chance of getting their paper published. In\nthis work, we address the problem of recommending appropriate conferences to\nthe authors to increase their chances of acceptance. We present three different\napproaches for the same involving the use of social network of the authors and\nthe content of the paper in the settings of dimensionality reduction and topic\nmodeling. In all these approaches, we apply Correspondence Analysis (CA) to\nderive appropriate relationships between the entities in question, such as\nconferences and papers. Our models show promising results when compared with\nexisting methods such as content-based filtering, collaborative filtering and\nhybrid filtering.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 18:52:39 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Sharma", "Manish", ""], ["Saradhi", "Vijaya", ""]]}, {"id": "2001.02733", "submitter": "Sta\\v{s}a Milojevi\\'c", "authors": "Sta\\v{s}a Milojevi\\'c", "title": "Practical method to reclassify Web of Science articles into unique\n  subject categories and broad disciplines", "comments": "Quantitative Science Studies", "journal-ref": null, "doi": "10.1162/qss_a_00014", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of bibliographic items into subjects and disciplines in large\ndatabases is essential for many quantitative science studies. The Web of\nScience classification of journals into ~250 subject categories, which has\nserved as a basis for many studies, is known to have some fundamental problems\nand several practical limitations that may affect the results from such\nstudies. Here we present an easily reproducible method to perform\nreclassification of the Web of Science into existing subject categories and\ninto 14 broad areas. Our reclassification is at a level of articles, so it\npreserves disciplinary differences that may exist among individual articles\npublished in the same journal. Reclassification also eliminates ambiguous\n(multiple) categories that are found for 50% of items, and assigns a\ndiscipline/field category to all articles that come from broad-coverage\njournals such as Nature and Science. The correctness of the assigned subject\ncategories is evaluated manually and is found to be ~95%.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 20:44:11 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Milojevi\u0107", "Sta\u0161a", ""]]}, {"id": "2001.02912", "submitter": "Laure Soulier", "authors": "Sharon Oviatt and Laure Soulier", "title": "Conversational Search for Learning Technologies", "comments": "Dagstuhl Report on Conversational Search (ID 19461) - This document\n  is a report of the breaking group \"Conversational Search for Learning\n  Technologies\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search is based on a user-system cooperation with the\nobjective to solve an information-seeking task. In this report, we discuss the\nimplication of such cooperation with the learning perspective from both user\nand system side. We also focus on the stimulation of learning through a key\ncomponent of conversational search, namely the multimodality of communication\nway, and discuss the implication in terms of information retrieval. We end with\na research road map describing promising research directions and perspectives.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 10:35:27 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Oviatt", "Sharon", ""], ["Soulier", "Laure", ""]]}, {"id": "2001.03010", "submitter": "Ida Mele", "authors": "Ida Mele, Nicola Tonellotto, Ophir Frieder, Raffaele Perego", "title": "Topical Result Caching in Web Search Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caching search results is employed in information retrieval systems to\nexpedite query processing and reduce back-end server workload. Motivated by the\nobservation that queries belonging to different topics have different\ntemporal-locality patterns, we investigate a novel caching model called STD\n(Static-Topic-Dynamic cache). It improves traditional SDC (Static-Dynamic\nCache) that stores in a static cache the results of popular queries and manages\nthe dynamic cache with a replacement policy for intercepting the temporal\nvariations in the query stream. Our proposed caching scheme includes another\nlayer for topic-based caching, where the entries are allocated to different\ntopics (e.g., weather, education). The results of queries characterized by a\ntopic are kept in the fraction of the cache dedicated to it. This permits to\nadapt the cache-space utilization to the temporal locality of the various\ntopics and reduces cache misses due to those queries that are neither\nsufficiently popular to be in the static portion nor requested within\nshort-time intervals to be in the dynamic portion. We simulate different\nconfigurations for STD using two real-world query streams. Experiments\ndemonstrate that our approach outperforms SDC with an increase up to 3% in\nterms of hit rates, and up to 36% of gap reduction w.r.t. SDC from the\ntheoretical optimal caching algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 14:26:45 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Mele", "Ida", ""], ["Tonellotto", "Nicola", ""], ["Frieder", "Ophir", ""], ["Perego", "Raffaele", ""]]}, {"id": "2001.03067", "submitter": "Arthur Brack", "authors": "Arthur Brack, Jennifer D'Souza, Anett Hoppe, S\\\"oren Auer, Ralph\n  Ewerth", "title": "Domain-independent Extraction of Scientific Concepts from Research\n  Articles", "comments": "Accepted for publishing in 42nd European Conference on IR Research,\n  ECIR 2020", "journal-ref": "Advances in Information Retrieval. 2020", "doi": "10.1007/978-3-030-45439-5_17", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the novel task of domain-independent scientific concept extraction\nfrom abstracts of scholarly articles and present two contributions. First, we\nsuggest a set of generic scientific concepts that have been identified in a\nsystematic annotation process. This set of concepts is utilised to annotate a\ncorpus of scientific abstracts from 10 domains of Science, Technology and\nMedicine at the phrasal level in a joint effort with domain experts. The\nresulting dataset is used in a set of benchmark experiments to (a) provide\nbaseline performance for this task, (b) examine the transferability of concepts\nbetween domains. Second, we present two deep learning systems as baselines. In\nparticular, we propose active learning to deal with different domains in our\ntask. The experimental results show that (1) a substantial agreement is\nachievable by non-experts after consultation with domain experts, (2) the\nbaseline system achieves a fairly high F1 score, (3) active learning enables us\nto nearly halve the amount of required training data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 15:42:22 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Brack", "Arthur", ""], ["D'Souza", "Jennifer", ""], ["Hoppe", "Anett", ""], ["Auer", "S\u00f6ren", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2001.03272", "submitter": "Kaushik Chakrabarti", "authors": "Kaushik Chakrabarti, Zhimin Chen, Siamak Shakeri, Guihong Cao", "title": "Open Domain Question Answering Using Web Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tables extracted from web documents can be used to directly answer many web\nsearch queries. Previous works on question answering (QA) using web tables have\nfocused on factoid queries, i.e., those answerable with a short string like\nperson name or a number. However, many queries answerable using tables are\nnon-factoid in nature. In this paper, we develop an open-domain QA approach\nusing web tables that works for both factoid and non-factoid queries. Our key\ninsight is to combine deep neural network-based semantic similarity between the\nquery and the table with features that quantify the dominance of the table in\nthe document as well as the quality of the information in the table. Our\nexperiments on real-life web search queries show that our approach\nsignificantly outperforms state-of-the-art baseline approaches. Our solution is\nused in production in a major commercial web search engine and serves direct\nanswers for tens of millions of real user queries per month.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 01:25:04 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Chakrabarti", "Kaushik", ""], ["Chen", "Zhimin", ""], ["Shakeri", "Siamak", ""], ["Cao", "Guihong", ""]]}, {"id": "2001.03303", "submitter": "Jacob Danovitch", "authors": "Jacob Danovitch", "title": "Linking Social Media Posts to News with Siamese Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computational social science projects examine online discourse\nsurrounding a specific trending topic. These works often involve the\nacquisition of large-scale corpora relevant to the event in question to analyze\naspects of the response to the event. Keyword searches present a\nprecision-recall trade-off and crowd-sourced annotations, while effective, are\ncostly. This work aims to enable automatic and accurate ad-hoc retrieval of\ncomments discussing a trending topic from a large corpus, using only a handful\nof seed news articles.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 04:39:44 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Danovitch", "Jacob", ""]]}, {"id": "2001.03324", "submitter": "Ibrahim Gashaw", "authors": "Ibrahim Gashaw and H L. Shashirekha", "title": "Machine Learning Approaches for Amharic Parts-of-speech Tagging", "comments": "15th International Conference on Natural Language Processing\n  (ICON-2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part-of-speech (POS) tagging is considered as one of the basic but necessary\ntools which are required for many Natural Language Processing (NLP)\napplications such as word sense disambiguation, information retrieval,\ninformation processing, parsing, question answering, and machine translation.\nPerformance of the current POS taggers in Amharic is not as good as that of the\ncontemporary POS taggers available for English and other European languages.\nThe aim of this work is to improve POS tagging performance for the Amharic\nlanguage, which was never above 91%. Usage of morphological knowledge, an\nextension of the existing annotated data, feature extraction, parameter tuning\nby applying grid search and the tagging algorithms have been examined and\nobtained significant performance difference from the previous works. We have\nused three different datasets for POS experiments.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 06:40:49 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Gashaw", "Ibrahim", ""], ["Shashirekha", "H L.", ""]]}, {"id": "2001.03369", "submitter": "Robin Brochier", "authors": "Robin Brochier, Adrien Guille and Julien Velcin", "title": "Inductive Document Network Embedding with Topic-Word Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document network embedding aims at learning representations for a structured\ntext corpus i.e. when documents are linked to each other. Recent algorithms\nextend network embedding approaches by incorporating the text content\nassociated with the nodes in their formulations. In most cases, it is hard to\ninterpret the learned representations. Moreover, little importance is given to\nthe generalization to new documents that are not observed within the network.\nIn this paper, we propose an interpretable and inductive document network\nembedding method. We introduce a novel mechanism, the Topic-Word Attention\n(TWA), that generates document representations based on the interplay between\nword and topic representations. We train these word and topic vectors through\nour general model, Inductive Document Network Embedding (IDNE), by leveraging\nthe connections in the document network. Quantitative evaluations show that our\napproach achieves state-of-the-art performance on various networks and we\nqualitatively show that our model produces meaningful and interpretable\nrepresentations of the words, topics and documents.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 10:14:07 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Brochier", "Robin", ""], ["Guille", "Adrien", ""], ["Velcin", "Julien", ""]]}, {"id": "2001.03884", "submitter": "Mohammad-Amin Charusaie", "authors": "Mohammad-Amin Charusaie and Arash Amini and Stefano Rini", "title": "Compressibility Measures for Affinely Singular Random Vectors", "comments": null, "journal-ref": null, "doi": "10.1109/ISIT44484.2020.9174417", "report-no": null, "categories": "cs.IT cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several ways to measure the compressibility of a random measure;\nthey include general approaches, such as using the rate-distortion curve, as\nwell as more specific notions, such as the Renyi information dimension (RID),\nand dimensional-rate bias (DRB). The RID parameter indicates the concentration\nof the measure around lower-dimensional subsets of the space while the DRB\nparameter specifies the compressibility of the distribution over these\nlower-dimensional subsets. While the evaluation of such compressibility\nparameters is well-studied for continuous and discrete measures (e.g., the DRB\nis closely related to the entropy and differential entropy in discrete and\ncontinuous cases, respectively), the case of discrete-continuous measures is\nquite subtle. In this paper, we focus on a class of multi-dimensional random\nmeasures that have singularities on affine lower-dimensional subsets. This\nclass of distributions naturally arises when considering linear transformation\nof component-wise independent discrete-continuous random variables. Here, we\nevaluate the RID and DRB for such probability measures. We further provide an\nupper-bound for the RID of multi-dimensional random measures that are obtained\nby Lipschitz functions of component-wise independent discrete-continuous random\nvariables (X). The upper-bound is shown to be achievable when the Lipschitz\nfunction is AX, where A satisfies SPARK(A) = rank(A) + 1 (e.g., Vandermonde\nmatrices). When considering discrete-domain moving-average processes with\nnon-Gaussian excitation noise, the above results allow us to evaluate the\nblock-average RID and DRB, as well as to determine a relationship between these\nparameters and other existing compressibility measures.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 08:10:16 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 08:18:22 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 20:53:39 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Charusaie", "Mohammad-Amin", ""], ["Amini", "Arash", ""], ["Rini", "Stefano", ""]]}, {"id": "2001.04139", "submitter": "Beatrice Mazoyer", "authors": "B\\'eatrice Mazoyer (MICS), Nicolas Herv\\'e (INA), C\\'eline Hudelot\n  (MICS), Julia Cage (ECON)", "title": "Repr\\'esentations lexicales pour la d\\'etection non supervis\\'ee\n  d'\\'ev\\'enements dans un flux de tweets : \\'etude sur des corpus fran\\c{c}ais\n  et anglais", "comments": "in French. Extraction et Gestion des connaissances, EGC 2020, Jan\n  2020, Bruxelles, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we evaluate the performance of recent text embeddings for the\nautomatic detection of events in a stream of tweets. We model this task as a\ndynamic clustering problem.Our experiments are conducted on a publicly\navailable corpus of tweets in English and on a similar dataset in French\nannotated by our team. We show that recent techniques based on deep neural\nnetworks (ELMo, Universal Sentence Encoder, BERT, SBERT), although promising on\nmany applications, are not very suitable for this task. We also experiment with\ndifferent types of fine-tuning to improve these results on French data.\nFinally, we propose a detailed analysis of the results obtained, showing the\nsuperiority of tf-idf approaches for this task.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 10:25:49 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Mazoyer", "B\u00e9atrice", "", "MICS"], ["Herv\u00e9", "Nicolas", "", "INA"], ["Hudelot", "C\u00e9line", "", "MICS"], ["Cage", "Julia", "", "ECON"]]}, {"id": "2001.04170", "submitter": "Simon Razniewski", "authors": "Yohan Chalier, Simon Razniewski, and Gerhard Weikum", "title": "Joint Reasoning for Multi-Faceted Commonsense Knowledge", "comments": "11 pages", "journal-ref": "AKBC 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense knowledge (CSK) supports a variety of AI applications, from\nvisual understanding to chatbots. Prior works on acquiring CSK, such as\nConceptNet, have compiled statements that associate concepts, like everyday\nobjects or activities, with properties that hold for most or some instances of\nthe concept. Each concept is treated in isolation from other concepts, and the\nonly quantitative measure (or ranking) of properties is a confidence score that\nthe statement is valid. This paper aims to overcome these limitations by\nintroducing a multi-faceted model of CSK statements and methods for joint\nreasoning over sets of inter-related statements. Our model captures four\ndifferent dimensions of CSK statements: plausibility, typicality, remarkability\nand salience, with scoring and ranking along each dimension. For example,\nhyenas drinking water is typical but not salient, whereas hyenas eating\ncarcasses is salient. For reasoning and ranking, we develop a method with soft\nconstraints, to couple the inference over concepts that are related in in a\ntaxonomic hierarchy. The reasoning is cast into an integer linear programming\n(ILP), and we leverage the theory of reduction costs of a relaxed LP to compute\ninformative rankings. This methodology is applied to several large CSK\ncollections. Our evaluation shows that we can consolidate these inputs into\nmuch cleaner and more expressive knowledge. Results are available at\nhttps://dice.mpi-inf.mpg.de.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 11:34:25 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 20:58:16 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Chalier", "Yohan", ""], ["Razniewski", "Simon", ""], ["Weikum", "Gerhard", ""]]}, {"id": "2001.04200", "submitter": "Bernard Yannou", "authors": "Tianjun Hou (LGI), Bernard Yannou (LGI), Yann Leroy, Emilie Poirson\n  (IRCCyN)", "title": "Mining customer product reviews for product development: A summarization\n  process", "comments": null, "journal-ref": "Expert Systems with Applications, Elsevier, 2019, 132, pp.141-150", "doi": "10.1016/j.eswa.2019.04.069", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research set out to identify and structure from online reviews the words\nand expressions related to customers' likes and dislikes to guide product\ndevelopment. Previous methods were mainly focused on product features. However,\nreviewers express their preference not only on product features. In this paper,\nbased on an extensive literature review in design science, the authors propose\na summarization model containing multiples aspects of user preference, such as\nproduct affordances, emotions, usage conditions. Meanwhile, the linguistic\npatterns describing these aspects of preference are discovered and drafted as\nannotation guidelines. A case study demonstrates that with the proposed model\nand the annotation guidelines, human annotators can structure the online\nreviews with high inter-agreement. As high inter-agreement human annotation\nresults are essential for automatizing the online review summarization process\nwith the natural language processing, this study provides materials for the\nfuture study of automatization.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 13:01:14 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Hou", "Tianjun", "", "LGI"], ["Yannou", "Bernard", "", "LGI"], ["Leroy", "Yann", "", "IRCCyN"], ["Poirson", "Emilie", "", "IRCCyN"]]}, {"id": "2001.04253", "submitter": "Fajie Yuan", "authors": "Fajie Yuan, Xiangnan He, Alexandros Karatzoglou, Liguang Zhang", "title": "Parameter-Efficient Transfer from Sequential Behaviors for User Modeling\n  and Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inductive transfer learning has had a big impact on computer vision and NLP\ndomains but has not been used in the area of recommender systems. Even though\nthere has been a large body of research on generating recommendations based on\nmodeling user-item interaction sequences, few of them attempt to represent and\ntransfer these models for serving downstream tasks where only limited data\nexists.\n  In this paper, we delve on the task of effectively learning a single user\nrepresentation that can be applied to a diversity of tasks, from cross-domain\nrecommendations to user profile predictions. Fine-tuning a large pre-trained\nnetwork and adapting it to downstream tasks is an effective way to solve such\ntasks. However, fine-tuning is parameter inefficient considering that an entire\nmodel needs to be re-trained for every new task. To overcome this issue, we\ndevelop a parameter efficient transfer learning architecture, termed as\nPeterRec, which can be configured on-the-fly to various downstream tasks.\nSpecifically, PeterRec allows the pre-trained parameters to remain unaltered\nduring fine-tuning by injecting a series of re-learned neural networks, which\nare small but as expressive as learning the entire network. We perform\nextensive experimental ablation to show the effectiveness of the learned user\nrepresentation in five downstream tasks. Moreover, we show that PeterRec\nperforms efficient transfer learning in multiple domains, where it achieves\ncomparable or sometimes better performance relative to fine-tuning the entire\nmodel parameters. Codes and datasets are available at\nhttps://github.com/fajieyuan/sigir2020_peterrec.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 14:09:54 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 04:16:15 GMT"}, {"version": "v3", "created": "Tue, 4 Feb 2020 04:07:51 GMT"}, {"version": "v4", "created": "Tue, 9 Jun 2020 12:36:19 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Yuan", "Fajie", ""], ["He", "Xiangnan", ""], ["Karatzoglou", "Alexandros", ""], ["Zhang", "Liguang", ""]]}, {"id": "2001.04326", "submitter": "Olegs Verhodubs", "authors": "Olegs Verhodubs", "title": "Merging of Ontologies Through Merging of Their Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology merging is important, but not always effective. The main reason, why\nontology merging is not effective, is that ontology merging is performed\nwithout considering goals. Goals define the way, in which ontologies to be\nmerged more effectively. The paper illustrates ontology merging by means of\nrules, which are generated from these ontologies. This is necessary for further\nuse in expert systems.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 15:08:10 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Verhodubs", "Olegs", ""]]}, {"id": "2001.04338", "submitter": "Joy Bose", "authors": "Joy Bose", "title": "Extraction of Relevant Images for Boilerplate Removal in Web Browsers", "comments": "4 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boilerplate refers to unwanted and repeated parts of a webpage (such as ads\nor table of contents) that distracts the user from reading the core content of\nthe webpage, such as a news article. Accurate detection and removal of\nboilerplate content from a webpage can enable the users to have a clutter free\nview of the webpage or news article. This can be useful in features like reader\nmode in web browsers. Current implementations of reader mode in web browsers\nsuch as Firefox, Chrome and Edge perform reasonably well for textual content in\nwebpages. However, they are mostly heuristic based and not flexible when the\nwebpage content is dynamic. Also they often do not perform well for removing\nboilerplate content in the form of images and multimedia in webpages. For\ndetection of boilerplate images, one needs to have knowledge of the actual\nlayout of the images in the webpage, which is only possible when the webpage is\nrendered. In this paper we discuss some of the issues in relevant image\nextraction. We also present the design of a testing framework to measure\naccuracy and a classifier to extract relevant images by leveraging a headless\nbrowser solution that gives the rendering information for images.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 05:47:54 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 04:06:14 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Bose", "Joy", ""]]}, {"id": "2001.04344", "submitter": "Olfa Nasraoui", "authors": "Pegah Sagheb Haghighi, Olurotimi Seton, Olfa Nasraoui", "title": "An Explainable Autoencoder For Collaborative Filtering Recommendation", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoders are a common building block of Deep Learning architectures,\nwhere they are mainly used for representation learning. They have also been\nsuccessfully used in Collaborative Filtering (CF) recommender systems to\npredict missing ratings. Unfortunately, like all black box machine learning\nmodels, they are unable to explain their outputs. Hence, while predictions from\nan Autoencoder-based recommender system might be accurate, it might not be\nclear to the user why a recommendation was generated. In this work, we design\nan explainable recommendation system using an Autoencoder model whose\npredictions can be explained using the neighborhood based explanation style.\nOur preliminary work can be considered to be the first step towards an\nexplainable deep learning architecture based on Autoencoders.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 23:55:30 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Haghighi", "Pegah Sagheb", ""], ["Seton", "Olurotimi", ""], ["Nasraoui", "Olfa", ""]]}, {"id": "2001.04345", "submitter": "Mukul Kumar", "authors": "Mukul Kumar, Youna Hu, Will Headden, Rahul Goutam, Heran Lin, Bing Yin", "title": "Shareable Representations for Search Query Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding search queries is critical for shopping search engines to\ndeliver a satisfying customer experience. Popular shopping search engines\nreceive billions of unique queries yearly, each of which can depict any of\nhundreds of user preferences or intents. In order to get the right results to\ncustomers it must be known queries like \"inexpensive prom dresses\" are intended\nto not only surface results of a certain product type but also products with a\nlow price. Referred to as query intents, examples also include preferences for\nauthor, brand, age group, or simply a need for customer service. Recent works\nsuch as BERT have demonstrated the success of a large transformer encoder\narchitecture with language model pre-training on a variety of NLP tasks. We\nadapt such an architecture to learn intents for search queries and describe\nmethods to account for the noisiness and sparseness of search query data. We\nalso describe cost effective ways of hosting transformer encoder models in\ncontext with low latency requirements. With the right domain-specific training\nwe can build a shareable deep learning model whose internal representation can\nbe reused for a variety of query understanding tasks including query intent\nidentification. Model sharing allows for fewer large models needed to be served\nat inference time and provides a platform to quickly build and roll out new\nsearch query classifiers.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 22:12:47 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Kumar", "Mukul", ""], ["Hu", "Youna", ""], ["Headden", "Will", ""], ["Goutam", "Rahul", ""], ["Lin", "Heran", ""], ["Yin", "Bing", ""]]}, {"id": "2001.04346", "submitter": "Xin Dong", "authors": "Xin Dong, Jingchao Ni, Wei Cheng, Zhengzhang Chen, Bo Zong, Dongjin\n  Song, Yanchi Liu, Haifeng Chen, Gerard de Melo", "title": "Asymmetrical Hierarchical Networks with Attentive Interactions for\n  Interpretable Review-Based Recommendation", "comments": null, "journal-ref": "AAAI 2020", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, recommender systems have been able to emit substantially improved\nrecommendations by leveraging user-provided reviews. Existing methods typically\nmerge all reviews of a given user or item into a long document, and then\nprocess user and item documents in the same manner. In practice, however, these\ntwo sets of reviews are notably different: users' reviews reflect a variety of\nitems that they have bought and are hence very heterogeneous in their topics,\nwhile an item's reviews pertain only to that single item and are thus topically\nhomogeneous. In this work, we develop a novel neural network model that\nproperly accounts for this important difference by means of asymmetric\nattentive modules. The user module learns to attend to only those signals that\nare relevant with respect to the target item, whereas the item module learns to\nextract the most salient contents with regard to properties of the item. Our\nmulti-hierarchical paradigm accounts for the fact that neither are all reviews\nequally useful, nor are all sentences within each review equally pertinent.\nExtensive experimental results on a variety of real datasets demonstrate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 23:48:42 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Dong", "Xin", ""], ["Ni", "Jingchao", ""], ["Cheng", "Wei", ""], ["Chen", "Zhengzhang", ""], ["Zong", "Bo", ""], ["Song", "Dongjin", ""], ["Liu", "Yanchi", ""], ["Chen", "Haifeng", ""], ["de Melo", "Gerard", ""]]}, {"id": "2001.04348", "submitter": "Christine Bauer", "authors": "Christine Bauer and Eva Zangerle", "title": "Leveraging Multi-Method Evaluation for Multi-Stakeholder Settings", "comments": "3 pages, ImpactRS 2019, Copenhagen, Denmark", "journal-ref": "1st Workshop on the Impact of Recommender Systems (ImpactRS 2019),\n  co-located with 13th ACM Conference on Recommender Systems (ACM RecSys 2019),\n  ceur-ws.org, Vol 2462, Short 3", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we focus on recommendation settings with multiple stakeholders\nwith possibly varying goals and interests, and argue that a single evaluation\nmethod or measure is not able to evaluate all relevant aspects in such a\ncomplex setting. We reason that employing a multi-method evaluation, where\nmultiple evaluation methods or measures are combined and integrated, allows for\ngetting a richer picture and prevents blind spots in the evaluation outcome.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 21:45:28 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Bauer", "Christine", ""], ["Zangerle", "Eva", ""]]}, {"id": "2001.04349", "submitter": "Angshul Majumdar Dr.", "authors": "Anupriya Gogna and Angshul Majumdar", "title": "Balancing Accuracy and Diversity in Recommendations using Matrix\n  Completion Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design of recommender systems aimed at achieving high prediction accuracy is\na widely researched area. However, several studies have suggested the need for\ndiversified recommendations, with acceptable level of accuracy, to avoid\nmonotony and improve customers experience. However, increasing diversity comes\nwith an associated reduction in recommendation accuracy; thereby necessitating\nan optimum tradeoff between the two. In this work, we attempt to achieve\naccuracy vs diversity balance, by exploiting available ratings and item\nmetadata, through a single, joint optimization model built over the matrix\ncompletion framework. Most existing works, unlike our formulation, propose a 2\nstage model, a heuristic item ranking scheme on top of an existing\ncollaborative filtering technique. Experimental evaluation on a movie\nrecommender system indicates that our model achieves higher diversity for a\ngiven drop in accuracy as compared to existing state of the art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 11:07:13 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Gogna", "Anupriya", ""], ["Majumdar", "Angshul", ""]]}, {"id": "2001.04351", "submitter": "Liang  Xu", "authors": "Liang Xu, Yu tong, Qianqian Dong, Yixuan Liao, Cong Yu, Yin Tian,\n  Weitang Liu, Lu Li, Caiquan Liu, Xuanwei Zhang", "title": "CLUENER2020: Fine-grained Named Entity Recognition Dataset and Benchmark\n  for Chinese", "comments": "6 pages, 5 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce the NER dataset from CLUE organization\n(CLUENER2020), a well-defined fine-grained dataset for named entity recognition\nin Chinese. CLUENER2020 contains 10 categories. Apart from common labels like\nperson, organization, and location, it contains more diverse categories. It is\nmore challenging than current other Chinese NER datasets and could better\nreflect real-world applications. For comparison, we implement several\nstate-of-the-art baselines as sequence labeling tasks and report human\nperformance, as well as its analysis. To facilitate future work on fine-grained\nNER for Chinese, we release our dataset, baselines, and leader-board.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 15:39:56 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 19:06:49 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 16:18:16 GMT"}, {"version": "v4", "created": "Mon, 20 Jan 2020 16:32:50 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Xu", "Liang", ""], ["tong", "Yu", ""], ["Dong", "Qianqian", ""], ["Liao", "Yixuan", ""], ["Yu", "Cong", ""], ["Tian", "Yin", ""], ["Liu", "Weitang", ""], ["Li", "Lu", ""], ["Liu", "Caiquan", ""], ["Zhang", "Xuanwei", ""]]}, {"id": "2001.04425", "submitter": "Hiba Arnaout", "authors": "Hiba Arnaout, Simon Razniewski, Gerhard Weikum, and Jeff Z. Pan", "title": "Negative Statements Considered Useful", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases (KBs) about notable entities and their properties are an\nimportant asset in applications such as search, question answering and\ndialogue. All popular KBs capture virtually only positive statements, and\nabstain from taking any stance on statements not stored in the KB. This paper\nmakes the case for explicitly stating salient statements that do not hold.\nNegative statements are useful to overcome limitations of question answering,\nand can often contribute to informative summaries of entities. Due to the\nabundance of such invalid statements, any effort to compile them needs to\naddress ranking by saliency. We present a statistical inference method for\ncompiling and ranking negative statements,based on expectations from positive\nstatements of related entities in peer groups. Experimental results, with a\nvariety of datasets, show that the method can effectively discover notable\nnegative statements, and extrinsic studies underline their usefulness for\nentity summarization. Datasets and code are released as resources for further\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 17:49:37 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 14:42:40 GMT"}, {"version": "v3", "created": "Mon, 20 Jan 2020 14:45:01 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2020 09:00:13 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Arnaout", "Hiba", ""], ["Razniewski", "Simon", ""], ["Weikum", "Gerhard", ""], ["Pan", "Jeff Z.", ""]]}, {"id": "2001.04484", "submitter": "Luca Papariello", "authors": "Luca Papariello, Alexandros Bampoulidis, Mihai Lupu", "title": "On the Replicability of Combining Word Embeddings and Retrieval Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We replicate recent experiments attempting to demonstrate an attractive\nhypothesis about the use of the Fisher kernel framework and mixture models for\naggregating word embeddings towards document representations and the use of\nthese representations in document classification, clustering, and retrieval.\nSpecifically, the hypothesis was that the use of a mixture model of von\nMises-Fisher (VMF) distributions instead of Gaussian distributions would be\nbeneficial because of the focus on cosine distances of both VMF and the vector\nspace model traditionally used in information retrieval. Previous experiments\nhad validated this hypothesis. Our replication was not able to validate it,\ndespite a large parameter scan space.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 19:01:07 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Papariello", "Luca", ""], ["Bampoulidis", "Alexandros", ""], ["Lupu", "Mihai", ""]]}, {"id": "2001.04625", "submitter": "Lu Wang", "authors": "Lu Wang, Jie Yang", "title": "Asymmetric Correlation Quantization Hashing for Cross-modal Retrieval", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the superiority in similarity computation and database storage for\nlarge-scale multiple modalities data, cross-modal hashing methods have\nattracted extensive attention in similarity retrieval across the heterogeneous\nmodalities. However, there are still some limitations to be further taken into\naccount: (1) most current CMH methods transform real-valued data points into\ndiscrete compact binary codes under the binary constraints, limiting the\ncapability of representation for original data on account of abundant loss of\ninformation and producing suboptimal hash codes; (2) the discrete binary\nconstraint learning model is hard to solve, where the retrieval performance may\ngreatly reduce by relaxing the binary constraints for large quantization error;\n(3) handling the learning problem of CMH in a symmetric framework, leading to\ndifficult and complex optimization objective. To address above challenges, in\nthis paper, a novel Asymmetric Correlation Quantization Hashing (ACQH) method\nis proposed. Specifically, ACQH learns the projection matrixs of heterogeneous\nmodalities data points for transforming query into a low-dimensional\nreal-valued vector in latent semantic space and constructs the stacked\ncompositional quantization embedding in a coarse-to-fine manner for indicating\ndatabase points by a series of learnt real-valued codeword in the codebook with\nthe help of pointwise label information regression simultaneously. Besides, the\nunified hash codes across modalities can be directly obtained by the discrete\niterative optimization framework devised in the paper. Comprehensive\nexperiments on diverse three benchmark datasets have shown the effectiveness\nand rationality of ACQH.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 04:53:30 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Wang", "Lu", ""], ["Yang", "Jie", ""]]}, {"id": "2001.04719", "submitter": "Andrew Krizhanovsky A", "authors": "Natalia Krizhanovsky and Andrew Krizhanovsky", "title": "Semi-automatic methods for adding words to the dictionary of VepKar\n  corpus based on inflectional rules extracted from Wiktionary", "comments": "10 pages, 1 table, 2 figures, published in the conference proceeding\n  https://events.spbu.ru/eventsContent/events/2019/corpora/corp_sborn.pdf#page=211", "journal-ref": "Corpora 2019, 24-28 June, 2019. Saint-Petersburg. P. 211-217", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The article describes a technique for using English Wiktionary inflection\ntables for generating word forms for Veps verbs and nominals in the Open corpus\nof Veps and Karelian languages. The information concerning Karelian and Veps\nWiktionary entries with inflection tables is given. The operating principle of\nthe Wiktionary static and dynamic templates is explained with the use of the\njogi (river) dictionary entry as an example. The method of constructing the\ninflection table in the dictionary of the VepKar corpus according to the data\nof the dynamic template of the English Wiktionary is presented.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 11:27:46 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Krizhanovsky", "Natalia", ""], ["Krizhanovsky", "Andrew", ""]]}, {"id": "2001.04825", "submitter": "Shahpar Yakhchi", "authors": "Shahpar Yakhchi (1), Amin Beheshti (1), Seyed Mohssen Ghafari (1),\n  Mehmet Orgun (1) ((1) Macquarie University- Sydney-Australia)", "title": "Enabling the Analysis of Personality Aspects in Recommender Systems", "comments": "This article contains 3 figures and 14 pages", "journal-ref": "Twenty-Third Pacific Asia Conference on Information Systems, China\n  2019", "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Recommender Systems mainly focus on exploiting users' feedback,\ne.g., ratings, and reviews on common items to detect similar users. Thus, they\nmight fail when there are no common items of interest among users. We call this\nproblem the Data Sparsity With no Feedback on Common Items (DSW-n-FCI).\nPersonality-based recommender systems have shown a great success to identify\nsimilar users based on their personality types. However, there are only a few\npersonality-based recommender systems in the literature which either discover\npersonality explicitly through filling a questionnaire that is a tedious task,\nor neglect the impact of users' personal interests and level of knowledge, as a\nkey factor to increase recommendations' acceptance. Differently, we identifying\nusers' personality type implicitly with no burden on users and incorporate it\nalong with users' personal interests and their level of knowledge. Experimental\nresults on a real-world dataset demonstrate the effectiveness of our model,\nespecially in DSW-n-FCI situations.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 23:02:07 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Yakhchi", "Shahpar", "", "Macquarie University- Sydney-Australia"], ["Beheshti", "Amin", "", "Macquarie University- Sydney-Australia"], ["Ghafari", "Seyed Mohssen", "", "Macquarie University- Sydney-Australia"], ["Orgun", "Mehmet", "", "Macquarie University- Sydney-Australia"]]}, {"id": "2001.04828", "submitter": "Kaushik Chakrabarti", "authors": "Kaushik Chakrabarti, Zhimin Chen, Siamak Shakeri, Guihong Cao, Surajit\n  Chaudhuri", "title": "TableQnA: Answering List Intent Queries With Web Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The web contains a vast corpus of HTML tables. They can be used to provide\ndirect answers to many web queries. We focus on answering two classes of\nqueries with those tables: those seeking lists of entities (e.g., `cities in\ncalifornia') and those seeking superlative entities (e.g., `largest city in\ncalifornia'). The main challenge is to achieve high precision with significant\ncoverage. Existing approaches train machine learning models to select the\nanswer from the candidates; they rely on textual match features between the\nquery and the content of the table along with features capturing table\nquality/importance. These features alone are inadequate for achieving the above\ngoals. Our main insight is that we can improve precision by (i) first\nextracting intent (structured information) from the query for the above query\nclasses and (ii) then performing structure-aware matching (instead of just\ntextual matching) between the extracted intent and the candidates to select the\nanswer. We model (i) as a sequence tagging task. We leverage state-of-the-art\ndeep neural network models with word embeddings. The model requires large scale\ntraining data which is expensive to obtain via manual labeling; we therefore\ndevelop a novel method to automatically generate the training data. For (ii),\nwe develop novel features to compute structure-aware match and train a machine\nlearning model. Our experiments on real-life web search queries show that (i)\nour intent extractor for list and superlative intent queries has significantly\nhigher precision and coverage compared with baseline approaches and (ii) our\ntable answer selector significantly outperforms the state-of-the-art baseline\napproach. This technology has been used in production by Microsoft's Bing\nsearch engine since 2016.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 01:43:54 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Chakrabarti", "Kaushik", ""], ["Chen", "Zhimin", ""], ["Shakeri", "Siamak", ""], ["Cao", "Guihong", ""], ["Chaudhuri", "Surajit", ""]]}, {"id": "2001.04830", "submitter": "Shoujin Wang", "authors": "Shoujin Wang, Liang Hu, Yan Wang, Longbing Cao, Quan Z. Sheng, Mehmet\n  Orgun", "title": "Sequential Recommender Systems: Challenges, Progress and Prospects", "comments": "IJCAI-2019 Survey Track Paper", "journal-ref": null, "doi": "10.24963/ijcai.2019/883", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging topic of sequential recommender systems has attracted increasing\nattention in recent years.Different from the conventional recommender systems\nincluding collaborative filtering and content-based filtering, SRSs try to\nunderstand and model the sequential user behaviors, the interactions between\nusers and items, and the evolution of users preferences and item popularity\nover time. SRSs involve the above aspects for more precise characterization of\nuser contexts, intent and goals, and item consumption trend, leading to more\naccurate, customized and dynamic recommendations.In this paper, we provide a\nsystematic review on SRSs.We first present the characteristics of SRSs, and\nthen summarize and categorize the key challenges in this research area,\nfollowed by the corresponding research progress consisting of the most recent\nand representative developments on this topic.Finally, we discuss the important\nresearch directions in this vibrant area.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 05:12:28 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Wang", "Shoujin", ""], ["Hu", "Liang", ""], ["Wang", "Yan", ""], ["Cao", "Longbing", ""], ["Sheng", "Quan Z.", ""], ["Orgun", "Mehmet", ""]]}, {"id": "2001.04831", "submitter": "Gabriel de Souza Pereira Moreira", "authors": "Gabriel de Souza Pereira Moreira", "title": "CHAMELEON: A Deep Learning Meta-Architecture for News Recommender\n  Systems [Phd. Thesis]", "comments": "Phd. Thesis presented on Dec. 09, 2019 to the Instituto Tecnol\\'ogico\n  de Aeron\\'autica (ITA), in partial fulfillment of the requirements for the\n  degree of Doctor of Science in the Graduate Program of Electronics and\n  Computing Engineering, Field of Informatics", "journal-ref": null, "doi": null, "report-no": "DCTA/ITA/TD-035/2019", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender Systems (RS) have became a popular research topic and, since\n2016, Deep Learning methods and techniques have been increasingly explored in\nthis area. News RS are aimed to personalize users experiences and help them\ndiscover relevant articles from a large and dynamic search space. The main\ncontribution of this research was named CHAMELEON, a Deep Learning\nmeta-architecture designed to tackle the specific challenges of news\nrecommendation. It consists of a modular reference architecture which can be\ninstantiated using different neural building blocks. As information about\nusers' past interactions is scarce in the news domain, the user context can be\nleveraged to deal with the user cold-start problem. Articles' content is also\nimportant to tackle the item cold-start problem. Additionally, the temporal\ndecay of items (articles) relevance is very accelerated in the news domain.\nFurthermore, external breaking events may temporally attract global readership\nattention, a phenomenon generally known as concept drift in machine learning.\nAll those characteristics are explicitly modeled on this research by a\ncontextual hybrid session-based recommendation approach using Recurrent Neural\nNetworks. The task addressed by this research is session-based news\nrecommendation, i.e., next-click prediction using only information available in\nthe current user session. A method is proposed for a realistic temporal offline\nevaluation of such task, replaying the stream of user clicks and fresh articles\nbeing continuously published in a news portal. Experiments performed with two\nlarge datasets have shown the effectiveness of the CHAMELEON for news\nrecommendation on many quality factors such as accuracy, item coverage,\nnovelty, and reduced item cold-start problem, when compared to other\ntraditional and state-of-the-art session-based recommendation algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 13:40:56 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Moreira", "Gabriel de Souza Pereira", ""]]}, {"id": "2001.04832", "submitter": "Sami Khenissi", "authors": "Sami Khenissi and Olfa Nasraoui", "title": "Modeling and Counteracting Exposure Bias in Recommender Systems", "comments": "9 figures and one table. The paper has 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What we discover and see online, and consequently our opinions and decisions,\nare becoming increasingly affected by automated machine learned predictions.\nSimilarly, the predictive accuracy of learning machines heavily depends on the\nfeedback data that we provide them. This mutual influence can lead to\nclosed-loop interactions that may cause unknown biases which can be exacerbated\nafter several iterations of machine learning predictions and user feedback.\nMachine-caused biases risk leading to undesirable social effects ranging from\npolarization to unfairness and filter bubbles.\n  In this paper, we study the bias inherent in widely used recommendation\nstrategies such as matrix factorization. Then we model the exposure that is\nborne from the interaction between the user and the recommender system and\npropose new debiasing strategies for these systems.\n  Finally, we try to mitigate the recommendation system bias by engineering\nsolutions for several state of the art recommender system models.\n  Our results show that recommender systems are biased and depend on the prior\nexposure of the user. We also show that the studied bias iteratively decreases\ndiversity in the output recommendations. Our debiasing method demonstrates the\nneed for alternative recommendation strategies that take into account the\nexposure process in order to reduce bias.\n  Our research findings show the importance of understanding the nature of and\ndealing with bias in machine learning models such as recommender systems that\ninteract directly with humans, and are thus causing an increasing influence on\nhuman discovery and decision making\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 00:12:34 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Khenissi", "Sami", ""], ["Nasraoui", "Olfa", ""]]}, {"id": "2001.04907", "submitter": "Dmitry Krotov", "authors": "Chaitanya K. Ryali, John J. Hopfield, Leopold Grinberg, Dmitry Krotov", "title": "Bio-Inspired Hashing for Unsupervised Similarity Search", "comments": "Accepted for publication in ICML 2020", "journal-ref": "Proceedings of the International Conference on Machine Learning,\n  2020, pp.8739-8750", "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fruit fly Drosophila's olfactory circuit has inspired a new locality\nsensitive hashing (LSH) algorithm, FlyHash. In contrast with classical LSH\nalgorithms that produce low dimensional hash codes, FlyHash produces sparse\nhigh-dimensional hash codes and has also been shown to have superior empirical\nperformance compared to classical LSH algorithms in similarity search. However,\nFlyHash uses random projections and cannot learn from data. Building on\ninspiration from FlyHash and the ubiquity of sparse expansive representations\nin neurobiology, our work proposes a novel hashing algorithm BioHash that\nproduces sparse high dimensional hash codes in a data-driven manner. We show\nthat BioHash outperforms previously published benchmarks for various hashing\nmethods. Since our learning algorithm is based on a local and biologically\nplausible synaptic plasticity rule, our work provides evidence for the proposal\nthat LSH might be a computational reason for the abundance of sparse expansive\nmotifs in a variety of biological systems. We also propose a convolutional\nvariant BioConvHash that further improves performance. From the perspective of\ncomputer science, BioHash and BioConvHash are fast, scalable and yield\ncompressed binary representations that are useful for similarity search.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 17:04:59 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 17:29:56 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Ryali", "Chaitanya K.", ""], ["Hopfield", "John J.", ""], ["Grinberg", "Leopold", ""], ["Krotov", "Dmitry", ""]]}, {"id": "2001.04909", "submitter": "Jonathan Brophy", "authors": "Jonathan Brophy and Daniel Lowd", "title": "EGGS: A Flexible Approach to Relational Modeling of Social Network Spam", "comments": "10 pages, 6 figures, 5 tables. STARAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networking websites face a constant barrage of spam, unwanted messages\nthat distract, annoy, and even defraud honest users. These messages tend to be\nvery short, making them difficult to identify in isolation. Furthermore,\nspammers disguise their messages to look legitimate, tricking users into\nclicking on links and tricking spam filters into tolerating their malicious\nbehavior. Thus, some spam filters examine relational structure in the domain,\nsuch as connections among users and messages, to better identify deceptive\ncontent. However, even when it is used, relational structure is often exploited\nin an incomplete or ad hoc manner. In this paper, we present Extended\nGroup-based Graphical models for Spam (EGGS), a general-purpose method for\nclassifying spam in online social networks. Rather than labeling each message\nindependently, we group related messages together when they have the same\nauthor, the same content, or other domain-specific connections. To reason about\nrelated messages, we combine two popular methods: stacked graphical learning\n(SGL) and probabilistic graphical models (PGM). Both methods capture the idea\nthat messages are more likely to be spammy when related messages are also\nspammy, but they do so in different ways; SGL uses sequential classifier\npredictions and PGMs use probabilistic inference. We apply our method to four\ndifferent social network domains. EGGS is more accurate than an independent\nmodel in most experimental settings, especially when the correct label is\nuncertain. For the PGM implementation, we compare Markov logic networks to\nprobabilistic soft logic and find that both work well with neither one\ndominating, and the combination of SGL and PGMs usually performs better than\neither on its own.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 17:06:13 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 22:10:00 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Brophy", "Jonathan", ""], ["Lowd", "Daniel", ""]]}, {"id": "2001.04980", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Rohan Kohli, Shrimai Prabhumoye", "title": "Modeling Product Search Relevance in e-Commerce", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of e-Commerce, online product search has emerged as a\npopular and effective paradigm for customers to find desired products and\nengage in online shopping. However, there is still a big gap between the\nproducts that customers really desire to purchase and relevance of products\nthat are suggested in response to a query from the customer. In this paper, we\npropose a robust way of predicting relevance scores given a search query and a\nproduct, using techniques involving machine learning, natural language\nprocessing and information retrieval. We compare conventional information\nretrieval models such as BM25 and Indri with deep learning models such as\nword2vec, sentence2vec and paragraph2vec. We share some of our insights and\nfindings from our experiments.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 21:17:55 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Kohli", "Rohan", ""], ["Prabhumoye", "Shrimai", ""]]}, {"id": "2001.05152", "submitter": "Nilavra Bhattacharya", "authors": "Nilavra Bhattacharya, Somnath Rakshit, Jacek Gwizdka, Paul Kogut", "title": "Relevance Prediction from Eye-movements Using Semi-interpretable\n  Convolutional Neural Networks", "comments": null, "journal-ref": "2020 Conference on Human Information Interaction and Retrieval\n  (CHIIR '20), March 14--18, 2020, Vancouver, BC, Canada", "doi": "10.1145/3343413.3377960", "report-no": null, "categories": "cs.HC cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an image-classification method to predict the perceived-relevance\nof text documents from eye-movements. An eye-tracking study was conducted where\nparticipants read short news articles, and rated them as relevant or irrelevant\nfor answering a trigger question. We encode participants' eye-movement\nscanpaths as images, and then train a convolutional neural network classifier\nusing these scanpath images. The trained classifier is used to predict\nparticipants' perceived-relevance of news articles from the corresponding\nscanpath images. This method is content-independent, as the classifier does not\nrequire knowledge of the screen-content, or the user's information-task. Even\nwith little data, the image classifier can predict perceived-relevance with up\nto 80% accuracy. When compared to similar eye-tracking studies from the\nliterature, this scanpath image classification method outperforms previously\nreported metrics by appreciable margins. We also attempt to interpret how the\nimage classifier differentiates between scanpaths on relevant and irrelevant\ndocuments.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 07:02:14 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Bhattacharya", "Nilavra", ""], ["Rakshit", "Somnath", ""], ["Gwizdka", "Jacek", ""], ["Kogut", "Paul", ""]]}, {"id": "2001.05266", "submitter": "Alexander Schindler", "authors": "Alexander Schindler, Thomas Lidy, Sebastian B\\\"ock", "title": "Deep Learning for MIR Tutorial", "comments": "This is a description of a tutorial held at the 19th International\n  Society for Music Information Retrieval Conference, ISMIR 2018, Paris,\n  France, September 23-27, 2018. 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has become state of the art in visual computing and\ncontinuously emerges into the Music Information Retrieval (MIR) and audio\nretrieval domain. In order to bring attention to this topic we propose an\nintroductory tutorial on deep learning for MIR. Besides a general introduction\nto neural networks, the proposed tutorial covers a wide range of MIR relevant\ndeep learning approaches. \\textbf{Convolutional Neural Networks} are currently\na de-facto standard for deep learning based audio retrieval. \\textbf{Recurrent\nNeural Networks} have proven to be effective in onset detection tasks such as\nbeat or audio-event detection. \\textbf{Siamese Networks} have been shown\neffective in learning audio representations and distance functions specific for\nmusic similarity retrieval. We will incorporate both academic and industrial\npoints of view into the tutorial. Accompanying the tutorial, we will create a\nGithub repository for the content presented at the tutorial as well as\nreferences to state of the art work and literature for further reading. This\nrepository will remain public after the conference.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 12:23:17 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Schindler", "Alexander", ""], ["Lidy", "Thomas", ""], ["B\u00f6ck", "Sebastian", ""]]}, {"id": "2001.05313", "submitter": "Xien Liu", "authors": "Xien Liu, Xinxin You, Xiao Zhang, Ji Wu and Ping Lv", "title": "Tensor Graph Convolutional Networks for Text Classification", "comments": "8 pages, 4 figures", "journal-ref": "AAAI 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to sequential learning models, graph-based neural networks exhibit\nsome excellent properties, such as ability capturing global information. In\nthis paper, we investigate graph-based neural networks for text classification\nproblem. A new framework TensorGCN (tensor graph convolutional networks), is\npresented for this task. A text graph tensor is firstly constructed to describe\nsemantic, syntactic, and sequential contextual information. Then, two kinds of\npropagation learning perform on the text graph tensor. The first is intra-graph\npropagation used for aggregating information from neighborhood nodes in a\nsingle graph. The second is inter-graph propagation used for harmonizing\nheterogeneous information between graphs. Extensive experiments are conducted\non benchmark datasets, and the results illustrate the effectiveness of our\nproposed framework. Our proposed TensorGCN presents an effective way to\nharmonize and integrate heterogeneous information from different kinds of\ngraphs.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 14:28:33 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Liu", "Xien", ""], ["You", "Xinxin", ""], ["Zhang", "Xiao", ""], ["Wu", "Ji", ""], ["Lv", "Ping", ""]]}, {"id": "2001.05357", "submitter": "Markus Zlabinger", "authors": "Markus Zlabinger, Sebastian Hofst\\\"atter, Navid Rekabsaz, Allan\n  Hanbury", "title": "DSR: A Collection for the Evaluation of Graded Disease-Symptom Relations", "comments": "7 pages; 3 tables; accepted as short-paper to the 42nd European\n  Conference on Information Retrieval (ECIR), Lisbon 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effective extraction of ranked disease-symptom relationships is a\ncritical component in various medical tasks, including computer-assisted\nmedical diagnosis or the discovery of unexpected associations between diseases.\nWhile existing disease-symptom relationship extraction methods are used as the\nfoundation in the various medical tasks, no collection is available to\nsystematically evaluate the performance of such methods. In this paper, we\nintroduce the Disease-Symptom Relation collection (DSR-collection), created by\nfive fully trained physicians as expert annotators. We provide graded symptom\njudgments for diseases by differentiating between \"symptoms\" and \"primary\nsymptoms\". Further, we provide several strong baselines, based on the methods\nused in previous studies. The first method is based on word embeddings, and the\nsecond on co-occurrences of keywords in medical articles. For the co-occurrence\nmethod, we propose an adaption in which not only keywords are considered, but\nalso the full text of medical articles. The evaluation on the DSR-collection\nshows the effectiveness of the proposed adaption in terms of nDCG, precision,\nand recall.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 14:56:06 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Zlabinger", "Markus", ""], ["Hofst\u00e4tter", "Sebastian", ""], ["Rekabsaz", "Navid", ""], ["Hanbury", "Allan", ""]]}, {"id": "2001.05399", "submitter": "Jimmy Lin", "authors": "Nick Ruest, Jimmy Lin, Ian Milligan, and Samantha Fritz", "title": "The Archives Unleashed Project: Technology, Process, and Community to\n  Improve Scholarly Access to Web Archives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Archives Unleashed project aims to improve scholarly access to web\narchives through a multi-pronged strategy involving tool creation, process\nmodeling, and community building - all proceeding concurrently in\nmutually-reinforcing efforts. As we near the end of our initially-conceived\nthree-year project, we report on our progress and share lessons learned along\nthe way. The main contribution articulated in this paper is a process model\nthat decomposes scholarly inquiries into four main activities: filter, extract,\naggregate, and visualize. Based on the insight that these activities can be\ndisaggregated across time, space, and tools, it is possible to generate\n\"derivative products\", using our Archives Unleashed Toolkit, that serve as\nuseful starting points for scholarly inquiry. Scholars can download these\nproducts from the Archives Unleashed Cloud and manipulate them just like any\nother dataset, thus providing access to web archives without requiring any\nspecialized knowledge. Over the past few years, our platform has processed over\na thousand different collections from about two hundred users, totaling over\n280 terabytes of web archives.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 16:09:47 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Ruest", "Nick", ""], ["Lin", "Jimmy", ""], ["Milligan", "Ian", ""], ["Fritz", "Samantha", ""]]}, {"id": "2001.05414", "submitter": "Matus Medo", "authors": "Shuqi Xu, Manuel Sebastian Mariani, Linyuan L\\\"u, Mat\\'u\\v{s} Medo", "title": "Unbiased evaluation of ranking metrics reveals consistent performance in\n  science and technology citation data", "comments": "21 pages, 11 figures, 5 tables", "journal-ref": null, "doi": "10.1016/j.joi.2019.101005", "report-no": "Journal of Informetrics 14(1), 101005 (2020)", "categories": "cs.SI cs.DL cs.IR physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the increasing use of citation-based metrics for research evaluation\npurposes, we do not know yet which metrics best deliver on their promise to\ngauge the significance of a scientific paper or a patent. We assess 17\nnetwork-based metrics by their ability to identify milestone papers and patents\nin three large citation datasets. We find that traditional\ninformation-retrieval evaluation metrics are strongly affected by the interplay\nbetween the age distribution of the milestone items and age biases of the\nevaluated metrics. Outcomes of these metrics are therefore not representative\nof the metrics' ranking ability. We argue in favor of a modified evaluation\nprocedure that explicitly penalizes biased metrics and allows us to reveal\nmetrics' performance patterns that are consistent across the datasets. PageRank\nand LeaderRank turn out to be the best-performing ranking metrics when their\nage bias is suppressed by a simple transformation of the scores that they\nproduce, whereas other popular metrics, including citation count, HITS and\nCollective Influence, produce significantly worse ranking results.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 16:34:21 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Xu", "Shuqi", ""], ["Mariani", "Manuel Sebastian", ""], ["L\u00fc", "Linyuan", ""], ["Medo", "Mat\u00fa\u0161", ""]]}, {"id": "2001.05493", "submitter": "Anant Khandelwal", "authors": "Anant Khandelwal, Niraj Kumar", "title": "A Unified System for Aggression Identification in English Code-Mixed and\n  Uni-Lingual Texts", "comments": "10 pages, 5 Figures, 6 Tables, accepted at CoDS-COMAD 2020", "journal-ref": null, "doi": "10.1145/3371158.3371165", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wide usage of social media platforms has increased the risk of aggression,\nwhich results in mental stress and affects the lives of people negatively like\npsychological agony, fighting behavior, and disrespect to others. Majority of\nsuch conversations contains code-mixed languages[28]. Additionally, the way\nused to express thought or communication style also changes from one social\nmedia plat-form to another platform (e.g., communication styles are different\nin twitter and Facebook). These all have increased the complexity of the\nproblem. To solve these problems, we have introduced a unified and robust\nmulti-modal deep learning architecture which works for English code-mixed\ndataset and uni-lingual English dataset both.The devised system, uses\npsycho-linguistic features and very ba-sic linguistic features. Our multi-modal\ndeep learning architecture contains, Deep Pyramid CNN, Pooled BiLSTM, and\nDisconnected RNN(with Glove and FastText embedding, both). Finally, the system\ntakes the decision based on model averaging. We evaluated our system on English\nCode-Mixed TRAC 2018 dataset and uni-lingual English dataset obtained from\nKaggle. Experimental results show that our proposed system outperforms all the\nprevious approaches on English code-mixed dataset and uni-lingual English\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 17:06:29 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 06:50:54 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Khandelwal", "Anant", ""], ["Kumar", "Niraj", ""]]}, {"id": "2001.05727", "submitter": "Antoine Gourru", "authors": "Antoine Gourru, Adrien Guille, Julien Velcin and Julien Jacques", "title": "Document Network Projection in Pretrained Word Embedding Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Regularized Linear Embedding (RLE), a novel method that projects a\ncollection of linked documents (e.g. citation network) into a pretrained word\nembedding space. In addition to the textual content, we leverage a matrix of\npairwise similarities providing complementary information (e.g., the network\nproximity of two documents in a citation graph). We first build a simple word\nvector average for each document, and we use the similarities to alter this\naverage representation. The document representations can help to solve many\ninformation retrieval tasks, such as recommendation, classification and\nclustering. We demonstrate that our approach outperforms or matches existing\ndocument network embedding methods on node classification and link prediction\ntasks. Furthermore, we show that it helps identifying relevant keywords to\ndescribe document classes.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 10:16:37 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Gourru", "Antoine", ""], ["Guille", "Adrien", ""], ["Velcin", "Julien", ""], ["Jacques", "Julien", ""]]}, {"id": "2001.05883", "submitter": "Matteo Allaix", "authors": "Matteo Allaix, Lukas Holzbaur, Tefjol Pllaha, Camilla Hollanti", "title": "Quantum Private Information Retrieval from Coded and Colluding Servers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.IR math.IT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical private information retrieval (PIR) setup, a user wants to\nretrieve a file from a database or a distributed storage system (DSS) without\nrevealing the file identity to the servers holding the data. In the quantum PIR\n(QPIR) setting, a user privately retrieves a classical file by receiving\nquantum information from the servers. The QPIR problem has been treated by Song\n\\emph{et al.} in the case of replicated servers, both without collusion and\nwith all but one servers colluding. In this paper, the QPIR setting is extended\nto account for maximum distance separable (MDS) coded servers. The proposed\nprotocol works for any $[n,k]$-MDS code and $t$-collusion with $t=n-k$.\nSimilarly to the previous cases, the rates achieved are better than those known\nor conjectured in the classical counterparts. Further, it is demonstrated how\nthe protocol can adapted to achieve significantly higher retrieval rates from\nDSSs encoded with a locally repairable code (LRC) with disjoint repair groups,\neach of which is an MDS code.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 15:19:08 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 10:27:29 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 13:42:11 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Allaix", "Matteo", ""], ["Holzbaur", "Lukas", ""], ["Pllaha", "Tefjol", ""], ["Hollanti", "Camilla", ""]]}, {"id": "2001.05917", "submitter": "Daniel Acuna", "authors": "Tong Zeng, Longfeng Wu, Sarah Bratt, Daniel E. Acuna", "title": "Assigning credit to scientific datasets using article citation networks", "comments": "PII: S1751-1577(19)30184-1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A citation is a well-established mechanism for connecting scientific\nartifacts. Citation networks are used by citation analysis for a variety of\nreasons, prominently to give credit to scientists' work. However, because of\ncurrent citation practices, scientists tend to cite only publications, leaving\nout other types of artifacts such as datasets. Datasets then do not get\nappropriate credit even though they are increasingly reused and experimented\nwith. We develop a network flow measure, called DataRank, aimed at solving this\ngap. DataRank assigns a relative value to each node in the network based on how\ncitations flow through the graph, differentiating publication and dataset flow\nrates. We evaluate the quality of DataRank by estimating its accuracy at\npredicting the usage of real datasets: web visits to GenBank and downloads of\nFigshare datasets. We show that DataRank is better at predicting this usage\ncompared to alternatives while offering additional interpretable outcomes. We\ndiscuss improvements to citation behavior and algorithms to properly track and\nassign credit to datasets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 16:08:32 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Zeng", "Tong", ""], ["Wu", "Longfeng", ""], ["Bratt", "Sarah", ""], ["Acuna", "Daniel E.", ""]]}, {"id": "2001.05998", "submitter": "Mohamed Attia", "authors": "Islam Samy, Mohamed A. Attia, Ravi Tandon, Loukas Lazos", "title": "Latent-variable Private Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DB cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, content accessed by users (movies, videos, news\narticles, etc.) can leak sensitive latent attributes, such as religious and\npolitical views, sexual orientation, ethnicity, gender, and others. To prevent\nsuch information leakage, the goal of classical PIR is to hide the identity of\nthe content/message being accessed, which subsequently also hides the latent\nattributes. This solution, while private, can be too costly, particularly, when\nperfect (information-theoretic) privacy constraints are imposed. For instance,\nfor a single database holding $K$ messages, privately retrieving one message is\npossible if and only if the user downloads the entire database of $K$ messages.\nRetrieving content privately, however, may not be necessary to perfectly hide\nthe latent attributes.\n  Motivated by the above, we formulate and study the problem of latent-variable\nprivate information retrieval (LV-PIR), which aims at allowing the user\nefficiently retrieve one out of $K$ messages (indexed by $\\theta$) without\nrevealing any information about the latent variable (modeled by $S$). We focus\non the practically relevant setting of a single database and show that one can\nsignificantly reduce the download cost of LV-PIR (compared to the classical\nPIR) based on the correlation between $\\theta$ and $S$. We present a general\nscheme for LV-PIR as a function of the statistical relationship between\n$\\theta$ and $S$, and also provide new results on the capacity/download cost of\nLV-PIR. Several open problems and new directions are also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 18:58:22 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 17:49:21 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Samy", "Islam", ""], ["Attia", "Mohamed A.", ""], ["Tandon", "Ravi", ""], ["Lazos", "Loukas", ""]]}, {"id": "2001.06086", "submitter": "Johan Pauwels", "authors": "Johan Pauwels, Gy\\\"orgy Fazekas, Mark B. Sandler", "title": "A Critical Look at the Applicability of Markov Logic Networks for Music\n  Signal Analysis", "comments": "Accepted for presentation at the Ninth International Workshop on\n  Statistical Relational AI (StarAI 2020) at the 34th AAAI Conference on\n  Artificial Intelligence (AAAI) in New York, on February 7th 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Markov logic networks (MLNs) have been proposed as a\npotentially useful paradigm for music signal analysis. Because all hidden\nMarkov models can be reformulated as MLNs, the latter can provide an\nall-encompassing framework that reuses and extends previous work in the field.\nHowever, just because it is theoretically possible to reformulate previous work\nas MLNs, does not mean that it is advantageous. In this paper, we analyse some\nproposed examples of MLNs for musical analysis and consider their practical\ndisadvantages when compared to formulating the same musical dependence\nrelationships as (dynamic) Bayesian networks. We argue that a number of\npractical hurdles such as the lack of support for sequences and for arbitrary\ncontinuous probability distributions make MLNs less than ideal for the proposed\nmusical applications, both in terms of easy of formulation and computational\nrequirements due to their required inference algorithms. These conclusions are\nnot specific to music, but apply to other fields as well, especially when\nsequential data with continuous observations is involved. Finally, we show that\nthe ideas underlying the proposed examples can be expressed perfectly well in\nthe more commonly used framework of (dynamic) Bayesian networks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 21:46:13 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Pauwels", "Johan", ""], ["Fazekas", "Gy\u00f6rgy", ""], ["Sandler", "Mark B.", ""]]}, {"id": "2001.06094", "submitter": "Debi Prasanna Mohanty Mr", "authors": "Sumit Kumar, Gopi Ramena, Manoj Goyal, Debi Mohanty, Ankur Agarwal,\n  Benu Changmai, Sukumar Moharana", "title": "On- Device Information Extraction from Screenshots in form of tags", "comments": null, "journal-ref": null, "doi": "10.1145/3371158.3371200", "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a method to make mobile screenshots easily searchable. In this\npaper, we present the workflow in which we: 1) preprocessed a collection of\nscreenshots, 2) identified script presentin image, 3) extracted unstructured\ntext from images, 4) identifiedlanguage of the extracted text, 5) extracted\nkeywords from the text, 6) identified tags based on image features, 7) expanded\ntag set by identifying related keywords, 8) inserted image tags with relevant\nimages after ranking and indexed them to make it searchable on device. We made\nthe pipeline which supports multiple languages and executed it on-device, which\naddressed privacy concerns. We developed novel architectures for components in\nthe pipeline, optimized performance and memory for on-device computation. We\nobserved from experimentation that the solution developed can reduce overall\nuser effort and improve end user experience while searching, whose results are\npublished.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 12:15:30 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kumar", "Sumit", ""], ["Ramena", "Gopi", ""], ["Goyal", "Manoj", ""], ["Mohanty", "Debi", ""], ["Agarwal", "Ankur", ""], ["Changmai", "Benu", ""], ["Moharana", "Sukumar", ""]]}, {"id": "2001.06626", "submitter": "Hengyi Cai", "authors": "Hengyi Cai, Hongshen Chen, Cheng Zhang, Yonghao Song, Xiaofang Zhao,\n  Dawei Yin", "title": "Adaptive Parameterization for Neural Dialogue Generation", "comments": "Published as a long paper in EMNLP 2019", "journal-ref": null, "doi": "10.18653/v1/D19-1188", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural conversation systems generate responses based on the\nsequence-to-sequence (SEQ2SEQ) paradigm. Typically, the model is equipped with\na single set of learned parameters to generate responses for given input\ncontexts. When confronting diverse conversations, its adaptability is rather\nlimited and the model is hence prone to generate generic responses. In this\nwork, we propose an {\\bf Ada}ptive {\\bf N}eural {\\bf D}ialogue generation\nmodel, \\textsc{AdaND}, which manages various conversations with\nconversation-specific parameterization. For each conversation, the model\ngenerates parameters of the encoder-decoder by referring to the input context.\nIn particular, we propose two adaptive parameterization mechanisms: a\ncontext-aware and a topic-aware parameterization mechanism. The context-aware\nparameterization directly generates the parameters by capturing local semantics\nof the given context. The topic-aware parameterization enables parameter\nsharing among conversations with similar topics by first inferring the latent\ntopics of the given context and then generating the parameters with respect to\nthe distributional topics. Extensive experiments conducted on a large-scale\nreal-world conversational dataset show that our model achieves superior\nperformance in terms of both quantitative metrics and human evaluations.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 08:18:19 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Cai", "Hengyi", ""], ["Chen", "Hongshen", ""], ["Zhang", "Cheng", ""], ["Song", "Yonghao", ""], ["Zhao", "Xiaofang", ""], ["Yin", "Dawei", ""]]}, {"id": "2001.06657", "submitter": "Vinay Verma Kumar", "authors": "Anubha Pandey, Ashish Mishra, Vinay Kumar Verma, Anurag Mittal and\n  Hema A. Murthy", "title": "Stacked Adversarial Network for Zero-Shot Sketch based Image Retrieval", "comments": "Accepted in WACV'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional approaches to Sketch-Based Image Retrieval (SBIR) assume that\nthe data of all the classes are available during training. The assumption may\nnot always be practical since the data of a few classes may be unavailable, or\nthe classes may not appear at the time of training. Zero-Shot Sketch-Based\nImage Retrieval (ZS-SBIR) relaxes this constraint and allows the algorithm to\nhandle previously unseen classes during the test. This paper proposes a\ngenerative approach based on the Stacked Adversarial Network (SAN) and the\nadvantage of Siamese Network (SN) for ZS-SBIR. While SAN generates a\nhigh-quality sample, SN learns a better distance metric compared to that of the\nnearest neighbor search. The capability of the generative model to synthesize\nimage features based on the sketch reduces the SBIR problem to that of an\nimage-to-image retrieval problem. We evaluate the efficacy of our proposed\napproach on TU-Berlin, and Sketchy database in both standard ZSL and\ngeneralized ZSL setting. The proposed method yields a significant improvement\nin standard ZSL as well as in a more challenging generalized ZSL setting (GZSL)\nfor SBIR.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 12:18:28 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Pandey", "Anubha", ""], ["Mishra", "Ashish", ""], ["Verma", "Vinay Kumar", ""], ["Mittal", "Anurag", ""], ["Murthy", "Hema A.", ""]]}, {"id": "2001.06674", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Arman Cohan, Nazli Goharian, Ross Filice", "title": "Ranking Significant Discrepancies in Clinical Reports", "comments": "ECIR 2020 (short)", "journal-ref": null, "doi": "10.1007/978-3-030-45442-5_30", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical errors are a major public health concern and a leading cause of death\nworldwide. Many healthcare centers and hospitals use reporting systems where\nmedical practitioners write a preliminary medical report and the report is\nlater reviewed, revised, and finalized by a more experienced physician. The\nrevisions range from stylistic to corrections of critical errors or\nmisinterpretations of the case. Due to the large quantity of reports written\ndaily, it is often difficult to manually and thoroughly review all the\nfinalized reports to find such errors and learn from them. To address this\nchallenge, we propose a novel ranking approach, consisting of textual and\nontological overlaps between the preliminary and final versions of reports. The\napproach learns to rank the reports based on the degree of discrepancy between\nthe versions. This allows medical practitioners to easily identify and learn\nfrom the reports in which their interpretation most substantially differed from\nthat of the attending physician (who finalized the report). This is a crucial\nstep towards uncovering potential errors and helping medical practitioners to\nlearn from such errors, thus improving patient-care in the long run. We\nevaluate our model on a dataset of radiology reports and show that our approach\noutperforms both previously-proposed approaches and more recent language models\nby 4.5% to 15.4%.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 14:47:10 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["MacAvaney", "Sean", ""], ["Cohan", "Arman", ""], ["Goharian", "Nazli", ""], ["Filice", "Ross", ""]]}, {"id": "2001.06765", "submitter": "Amit Kumar Jaiswal", "authors": "Amit Kumar Jaiswal, Haiming Liu, Ingo Frommholz", "title": "Information Foraging for Enhancing Implicit Feedback in Content-based\n  Image Recommendation", "comments": "FIRE '19: Proceedings of the 11th Forum for Information Retrieval\n  Evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User implicit feedback plays an important role in recommender systems.\nHowever, finding implicit features is a tedious task. This paper aims to\nidentify users' preferences through implicit behavioural signals for image\nrecommendation based on the Information Scent Model of Information Foraging\nTheory. In the first part, we hypothesise that the users' perception is\nimproved with visual cues in the images as behavioural signals that provide\nusers' information scent during information seeking. We designed a\ncontent-based image recommendation system to explore which image attributes\n(i.e., visual cues or bookmarks) help users find their desired image. We found\nthat users prefer recommendations predicated by visual cues and therefore\nconsider the visual cues as good information scent for their information\nseeking. In the second part, we investigated if visual cues in the images\ntogether with the images itself can be better perceived by the users than each\nof them on its own. We evaluated the information scent artifacts in image\nrecommendation on the Pinterest image collection and the WikiArt dataset. We\nfind our proposed image recommendation system supports the implicit signals\nthrough Information Foraging explanation of the information scent model.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 03:36:33 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Jaiswal", "Amit Kumar", ""], ["Liu", "Haiming", ""], ["Frommholz", "Ingo", ""]]}, {"id": "2001.06823", "submitter": "David Morris", "authors": "David Morris, Eric M\\\"uller-Budack, Ralph Ewerth", "title": "SlideImages: A Dataset for Educational Image Classification", "comments": "8 pages, 2 figures, to be presented at ECIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, convolutional neural networks (CNNs) have achieved\nimpressive results in computer vision tasks, which however mainly focus on\nphotos with natural scene content. Besides, non-sensor derived images such as\nillustrations, data visualizations, figures, etc. are typically used to convey\ncomplex information or to explore large datasets. However, this kind of images\nhas received little attention in computer vision. CNNs and similar techniques\nuse large volumes of training data. Currently, many document analysis systems\nare trained in part on scene images due to the lack of large datasets of\neducational image data. In this paper, we address this issue and present\nSlideImages, a dataset for the task of classifying educational illustrations.\nSlideImages contains training data collected from various sources, e.g.,\nWikimedia Commons and the AI2D dataset, and test data collected from\neducational slides. We have reserved all the actual educational images as a\ntest dataset in order to ensure that the approaches using this dataset\ngeneralize well to new educational images, and potentially other domains.\nFurthermore, we present a baseline system using a standard deep neural\narchitecture and discuss dealing with the challenge of limited training data.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 13:11:55 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Morris", "David", ""], ["M\u00fcller-Budack", "Eric", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2001.06831", "submitter": "Jaya Prakash Champati Dr", "authors": "Jaya Prakash Champati, Ramana R. Avula, Tobias J. Oechtering, and\n  James Gross", "title": "On the Minimum Achievable Age of Information for General Service-Time\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in analysing the freshness of data in networked\nsystems. Age of Information (AoI) has emerged as a popular metric to quantify\nthis freshness at a given destination. There has been a significant research\neffort in optimizing this metric in communication and networking systems under\ndifferent settings. In contrast to previous works, we are interested in a\nfundamental question, what is the minimum achievable AoI in any\nsingle-server-single-source queuing system for a given service-time\ndistribution? To address this question, we study a problem of optimizing AoI\nunder service preemptions. Our main result is on the characterization of the\nminimum achievable average peak AoI (PAoI). We obtain this result by showing\nthat a fixed-threshold policy is optimal in the set of all randomized-threshold\ncausal policies. We use the characterization to provide necessary and\nsufficient conditions for the service-time distributions under which\npreemptions are beneficial.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 14:02:51 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Champati", "Jaya Prakash", ""], ["Avula", "Ramana R.", ""], ["Oechtering", "Tobias J.", ""], ["Gross", "James", ""]]}, {"id": "2001.06910", "submitter": "Krisztian Balog", "authors": "Krisztian Balog, Lucie Flekova, Matthias Hagen, Rosie Jones, Martin\n  Potthast, Filip Radlinski, Mark Sanderson, Svitlana Vakulenko, Hamed Zamani", "title": "Common Conversational Community Prototype: Scholarly Conversational\n  Assistant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper discusses the potential for creating academic resources (tools,\ndata, and evaluation approaches) to support research in conversational search,\nby focusing on realistic information needs and conversational interactions.\nSpecifically, we propose to develop and operate a prototype conversational\nsearch system for scholarly activities. This Scholarly Conversational Assistant\nwould serve as a useful tool, a means to create datasets, and a platform for\nrunning evaluation challenges by groups across the community. This article\nresults from discussions of a working group at Dagstuhl Seminar 19461 on\nConversational Search.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 22:08:57 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Balog", "Krisztian", ""], ["Flekova", "Lucie", ""], ["Hagen", "Matthias", ""], ["Jones", "Rosie", ""], ["Potthast", "Martin", ""], ["Radlinski", "Filip", ""], ["Sanderson", "Mark", ""], ["Vakulenko", "Svitlana", ""], ["Zamani", "Hamed", ""]]}, {"id": "2001.07075", "submitter": "Sagar Uprety Mr.", "authors": "Sagar Uprety, Prayag Tiwari, Shahram Dehdashti, Lauren Fell, Dawei\n  Song, Peter Bruza, Massimo Melucci", "title": "Quantum-like Structure in Multidimensional Relevance Judgements", "comments": "To be published at 42nd European Conference on Research in IR, ECIR\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of studies in cognitive science have revealed that\nprobabilistic outcomes of certain human decisions do not agree with the axioms\nof classical probability theory. The field of Quantum Cognition provides an\nalternative probabilistic model to explain such paradoxical findings. It posits\nthat cognitive systems have an underlying quantum-like structure, especially in\ndecision-making under uncertainty. In this paper, we hypothesise that relevance\njudgement, being a multidimensional, cognitive concept, can be used to probe\nthe quantum-like structure for modelling users' cognitive states in information\nseeking. Extending from an experiment protocol inspired by the Stern-Gerlach\nexperiment in Quantum Physics, we design a crowd-sourced user study to show\nviolation of the Kolmogorovian probability axioms as a proof of the\nquantum-like structure, and provide a comparison between a quantum\nprobabilistic model and a Bayesian model for predictions of relevance.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 12:00:12 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Uprety", "Sagar", ""], ["Tiwari", "Prayag", ""], ["Dehdashti", "Shahram", ""], ["Fell", "Lauren", ""], ["Song", "Dawei", ""], ["Bruza", "Peter", ""], ["Melucci", "Massimo", ""]]}, {"id": "2001.07098", "submitter": "Carlos-Emiliano Gonz\\'alez-Gallardo", "authors": "Carlos-Emiliano Gonz\\'alez-Gallardo, Romain Deveaud, Eric SanJuan, and\n  Juan-Manuel Torres-Moreno", "title": "Audio Summarization with Audio Features and Probability Distribution\n  Divergence", "comments": "20th International Conference on Computational Linguistics and\n  Intelligent Text Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The automatic summarization of multimedia sources is an important task that\nfacilitates the understanding of an individual by condensing the source while\nmaintaining relevant information. In this paper we focus on audio summarization\nbased on audio features and the probability of distribution divergence. Our\nmethod, based on an extractive summarization approach, aims to select the most\nrelevant segments until a time threshold is reached. It takes into account the\nsegment's length, position and informativeness value. Informativeness of each\nsegment is obtained by mapping a set of audio features issued from its\nMel-frequency Cepstral Coefficients and their corresponding Jensen-Shannon\ndivergence score. Results over a multi-evaluator scheme shows that our approach\nprovides understandable and informative summaries.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 13:10:01 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 09:28:02 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Gonz\u00e1lez-Gallardo", "Carlos-Emiliano", ""], ["Deveaud", "Romain", ""], ["SanJuan", "Eric", ""], ["Torres-Moreno", "Juan-Manuel", ""]]}, {"id": "2001.07139", "submitter": "Diana Sousa", "authors": "Diana Sousa and Francisco M. Couto", "title": "BiOnt: Deep Learning using Multiple Biomedical Ontologies for Relation\n  Extraction", "comments": "ECIR 2020", "journal-ref": "Advances in Information Retrieval: 42nd European Conference on IR\n  Research, Volume 12036. 2020. pp. 367-374", "doi": "10.1007/978-3-030-45442-5_46", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful biomedical relation extraction can provide evidence to researchers\nand clinicians about possible unknown associations between biomedical entities,\nadvancing the current knowledge we have about those entities and their inherent\nmechanisms. Most biomedical relation extraction systems do not resort to\nexternal sources of knowledge, such as domain-specific ontologies. However,\nusing deep learning methods, along with biomedical ontologies, has been\nrecently shown to effectively advance the biomedical relation extraction field.\nTo perform relation extraction, our deep learning system, BiOnt, employs four\ntypes of biomedical ontologies, namely, the Gene Ontology, the Human Phenotype\nOntology, the Human Disease Ontology, and the Chemical Entities of Biological\nInterest, regarding gene-products, phenotypes, diseases, and chemical\ncompounds, respectively. We tested our system with three data sets that\nrepresent three different types of relations of biomedical entities. BiOnt\nachieved, in F-score, an improvement of 4.93 percentage points for drug-drug\ninteractions (DDI corpus), 4.99 percentage points for phenotype-gene relations\n(PGR corpus), and 2.21 percentage points for chemical-induced disease relations\n(BC5CDR corpus), relatively to the state-of-the-art. The code supporting this\nsystem is available at https://github.com/lasigeBioTM/BiOnt.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 15:24:37 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 08:04:35 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Sousa", "Diana", ""], ["Couto", "Francisco M.", ""]]}, {"id": "2001.07158", "submitter": "Suhas Thejaswi", "authors": "Suhas Thejaswi, Aristides Gionis, Juho Lauri", "title": "Finding path motifs in large temporal graphs using algebraic\n  fingerprints", "comments": "version prior to peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a family of pattern-detection problems in vertex-colored temporal\ngraphs. In particular, given a vertex-colored temporal graph and a multiset of\ncolors as a query, we search for temporal paths in the graph that contain the\ncolors specified in the query. These types of problems have several\napplications, for example in recommending tours for tourists or detecting\nabnormal behavior in a network of financial transactions. For the family of\npattern-detection problems we consider, we establish complexity results and\ndesign an algebraic-algorithmic framework based on constrained multilinear\nsieving. We demonstrate that our solution scales to massive graphs with up to a\nbillion edges for a multiset query with five colors and up to hundred million\nedges for a multiset query with ten colors, despite the problems being NP-hard.\nOur implementation, which is publicly available, exhibits practical edge-linear\nscalability and is highly optimized. For instance, in a real-world graph\ndataset with more than six million edges and a multiset query with ten colors,\nwe can extract an optimum solution in less than eight minutes on a Haswell\ndesktop with four cores.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 16:13:27 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 04:31:40 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 13:18:08 GMT"}, {"version": "v4", "created": "Mon, 27 Jul 2020 14:40:49 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Thejaswi", "Suhas", ""], ["Gionis", "Aristides", ""], ["Lauri", "Juho", ""]]}, {"id": "2001.07194", "submitter": "Yichao Zhou", "authors": "Yichao Zhou, Shaunak Mishra, Manisha Verma, Narayan Bhamidipati, and\n  Wei Wang", "title": "Recommending Themes for Ad Creative Design via Visual-Linguistic\n  Representations", "comments": "7 pages, 8 figures, 2 tables, accepted by The Web Conference 2020", "journal-ref": null, "doi": "10.1145/3366423.3380001", "report-no": null, "categories": "cs.CL cs.CV cs.IR cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a perennial need in the online advertising industry to refresh ad\ncreatives, i.e., images and text used for enticing online users towards a\nbrand. Such refreshes are required to reduce the likelihood of ad fatigue among\nonline users, and to incorporate insights from other successful campaigns in\nrelated product categories. Given a brand, to come up with themes for a new ad\nis a painstaking and time consuming process for creative strategists.\nStrategists typically draw inspiration from the images and text used for past\nad campaigns, as well as world knowledge on the brands. To automatically infer\nad themes via such multimodal sources of information in past ad campaigns, we\npropose a theme (keyphrase) recommender system for ad creative strategists. The\ntheme recommender is based on aggregating results from a visual question\nanswering (VQA) task, which ingests the following: (i) ad images, (ii) text\nassociated with the ads as well as Wikipedia pages on the brands in the ads,\nand (iii) questions around the ad. We leverage transformer based cross-modality\nencoders to train visual-linguistic representations for our VQA task. We study\ntwo formulations for the VQA task along the lines of classification and\nranking; via experiments on a public dataset, we show that cross-modal\nrepresentations lead to significantly better classification accuracy and\nranking precision-recall metrics. Cross-modal representations show better\nperformance compared to separate image and text representations. In addition,\nthe use of multimodal information shows a significant lift over using only\ntextual or visual information.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 18:04:10 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 23:05:46 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Zhou", "Yichao", ""], ["Mishra", "Shaunak", ""], ["Verma", "Manisha", ""], ["Bhamidipati", "Narayan", ""], ["Wang", "Wei", ""]]}, {"id": "2001.07380", "submitter": "Takafumi Suzuki", "authors": "Takafumi J. Suzuki", "title": "Random-walk Based Generative Model for Classifying Document Networks", "comments": "7pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document networks are found in various collections of real-world data, such\nas citation networks, hyperlinked web pages, and online social networks. A\nlarge number of generative models have been proposed because they offer\nintuitive and useful pictures for analyzing document networks. Prominent\nexamples are relational topic models, where documents are linked according to\ntheir topic similarities. However, existing generative models do not make full\nuse of network structures because they are largely dependent on topic modeling\nof documents. In particular, centrality of graph nodes is missing in generative\nprocesses of previous models. In this paper, we propose a novel generative\nmodel for document networks by introducing random walkers on networks to\nintegrate the node centrality into link generation processes. The developed\nmethod is evaluated in semi-supervised classification tasks with real-world\ncitation networks. We show that the proposed model outperforms existing\nprobabilistic approaches especially in detecting communities in connected\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 08:26:06 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Suzuki", "Takafumi J.", ""]]}, {"id": "2001.07440", "submitter": "Marcia Barros", "authors": "Marcia Barros, Andr\\'e Moitinho, Francisco M. Couto", "title": "Hybrid Semantic Recommender System for Chemical Compounds", "comments": "8 pages, 3 figures, accepted as short-paper to the 42nd European\n  Conference on Information Retrieval (ECIR), Lisbon 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending Chemical Compounds of interest to a particular researcher is a\npoorly explored field. The few existent datasets with information about the\npreferences of the researchers use implicit feedback. The lack of Recommender\nSystems in this particular field presents a challenge for the development of\nnew recommendations models. In this work, we propose a Hybrid recommender model\nfor recommending Chemical Compounds. The model integrates\ncollaborative-filtering algorithms for implicit feedback (Alternating Least\nSquares (ALS) and Bayesian Personalized Ranking(BPR)) and semantic similarity\nbetween the Chemical Compounds in the ChEBI ontology (ONTO). We evaluated the\nmodel in an implicit dataset of Chemical Compounds, CheRM. The Hybrid model was\nable to improve the results of state-of-the-art collaborative-filtering\nalgorithms, especially for Mean Reciprocal Rank, with an increase of 6.7% when\ncomparing the collaborative-filtering ALS and the Hybrid ALS_ONTO.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 10:57:00 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Barros", "Marcia", ""], ["Moitinho", "Andr\u00e9", ""], ["Couto", "Francisco M.", ""]]}, {"id": "2001.07803", "submitter": "Luis Leiva", "authors": "Ioannis Arapakis and Antonio Penta and Hideo Joho and Luis A. Leiva", "title": "A Price-Per-Attention Auction Scheme Using Mouse Cursor Information", "comments": null, "journal-ref": "ACM Trans. Inf. Syst. 38, 2 (2020)", "doi": "10.1145/3374210", "report-no": null, "categories": "cs.GT cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Payments in online ad auctions are typically derived from click-through\nrates, so that advertisers do not pay for ineffective ads. But advertisers\noften care about more than just clicks. That is, for example, if they aim to\nraise brand awareness or visibility. There is thus an opportunity to devise a\nmore effective ad pricing paradigm, in which ads are paid only if they are\nactually noticed. This article contributes a novel auction format based on a\npay-per-attention (PPA) scheme. We show that the PPA auction inherits the\ndesirable properties (strategy-proofness and efficiency) as its\npay-per-impression and pay-per-click counterparts, and that it also compares\nfavourably in terms of revenues. To make the PPA format feasible, we also\ncontribute a scalable diagnostic technology to predict user attention to ads in\nsponsored search using raw mouse cursor coordinates only, regardless of the\npage content and structure. We use the user attention predictions in numerical\nsimulations to evaluate the PPA auction scheme. Our results show that, in\nrelevant economic settings, the PPA revenues would be strictly higher than the\nexisting auction payment schemes.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 22:54:37 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Arapakis", "Ioannis", ""], ["Penta", "Antonio", ""], ["Joho", "Hideo", ""], ["Leiva", "Luis A.", ""]]}, {"id": "2001.07838", "submitter": "Bilal Abu-Salih", "authors": "Bilal Abu-Salih, Kit Yan Chan, Omar Al-Kadi, Marwan Al-Tawil, Pornpit\n  Wongthongtham, Tomayess Issa, Heba Saadeh, Malak Al-Hassan, Bushra Bremie,\n  Abdulaziz Albahlal", "title": "An Approach for Time-aware Domain-based Social Influence Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Social Networks(OSNs) have established virtual platforms enabling\npeople to express their opinions, interests and thoughts in a variety of\ncontexts and domains, allowing legitimate users as well as spammers and other\nuntrustworthy users to publish and spread their content. Hence, the concept of\nsocial trust has attracted the attention of information processors/data\nscientists and information consumers/business firms. One of the main reasons\nfor acquiring the value of Social Big Data (SBD) is to provide frameworks and\nmethodologies using which the credibility of OSNs users can be evaluated. These\napproaches should be scalable to accommodate large-scale social data. Hence,\nthere is a need for well comprehending of social trust to improve and expand\nthe analysis process and inferring the credibility of SBD. Given the exposed\nenvironment's settings and fewer limitations related to OSNs, the medium allows\nlegitimate and genuine users as well as spammers and other low trustworthy\nusers to publish and spread their content. Hence, this paper presents an\napproach incorporates semantic analysis and machine learning modules to measure\nand predict users' trustworthiness in numerous domains in different time\nperiods. The evaluation of the conducted experiment validates the applicability\nof the incorporated machine learning techniques to predict highly trustworthy\ndomain-based users.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 10:39:37 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Abu-Salih", "Bilal", ""], ["Chan", "Kit Yan", ""], ["Al-Kadi", "Omar", ""], ["Al-Tawil", "Marwan", ""], ["Wongthongtham", "Pornpit", ""], ["Issa", "Tomayess", ""], ["Saadeh", "Heba", ""], ["Al-Hassan", "Malak", ""], ["Bremie", "Bushra", ""], ["Albahlal", "Abdulaziz", ""]]}, {"id": "2001.07853", "submitter": "Priyank Agrawal", "authors": "Priyank Agrawal and Theja Tulabandhula", "title": "Incentivising Exploration and Recommendations for Contextual Bandits\n  with Payments", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a contextual bandit based model to capture the learning and social\nwelfare goals of a web platform in the presence of myopic users. By using\npayments to incentivize these agents to explore different\nitems/recommendations, we show how the platform can learn the inherent\nattributes of items and achieve a sublinear regret while maximizing cumulative\nsocial welfare. We also calculate theoretical bounds on the cumulative costs of\nincentivization to the platform. Unlike previous works in this domain, we\nconsider contexts to be completely adversarial, and the behavior of the\nadversary is unknown to the platform. Our approach can improve various\nengagement metrics of users on e-commerce stores, recommendation engines and\nmatching platforms.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 02:26:22 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Agrawal", "Priyank", ""], ["Tulabandhula", "Theja", ""]]}, {"id": "2001.07861", "submitter": "Xin Bing", "authors": "Xin Bing and Florentina Bunea and Marten Wegkamp", "title": "Optimal estimation of sparse topic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have become popular tools for dimension reduction and\nexploratory analysis of text data which consists in observed frequencies of a\nvocabulary of $p$ words in $n$ documents, stored in a $p\\times n$ matrix. The\nmain premise is that the mean of this data matrix can be factorized into a\nproduct of two non-negative matrices: a $p\\times K$ word-topic matrix $A$ and a\n$K\\times n$ topic-document matrix $W$. This paper studies the estimation of $A$\nthat is possibly element-wise sparse, and the number of topics $K$ is unknown.\nIn this under-explored context, we derive a new minimax lower bound for the\nestimation of such $A$ and propose a new computationally efficient algorithm\nfor its recovery. We derive a finite sample upper bound for our estimator, and\nshow that it matches the minimax lower bound in many scenarios. Our estimate\nadapts to the unknown sparsity of $A$ and our analysis is valid for any finite\n$n$, $p$, $K$ and document lengths. Empirical results on both synthetic data\nand semi-synthetic data show that our proposed estimator is a strong competitor\nof the existing state-of-the-art algorithms for both non-sparse $A$ and sparse\n$A$, and has superior performance is many scenarios of interest.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 03:19:50 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Bing", "Xin", ""], ["Bunea", "Florentina", ""], ["Wegkamp", "Marten", ""]]}, {"id": "2001.07866", "submitter": "Xingyu Wang", "authors": "Xingyu Wang, Lida Zhang, Diego Klabjan", "title": "Keyword-based Topic Modeling and Keyword Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Certain type of documents such as tweets are collected by specifying a set of\nkeywords. As topics of interest change with time it is beneficial to adjust\nkeywords dynamically. The challenge is that these need to be specified ahead of\nknowing the forthcoming documents and the underlying topics. The future topics\nshould mimic past topics of interest yet there should be some novelty in them.\nWe develop a keyword-based topic model that dynamically selects a subset of\nkeywords to be used to collect future documents. The generative process first\nselects keywords and then the underlying documents based on the specified\nkeywords. The model is trained by using a variational lower bound and\nstochastic gradient optimization. The inference consists of finding a subset of\nkeywords where given a subset the model predicts the underlying topic-word\nmatrix for the unknown forthcoming documents. We compare the keyword topic\nmodel against a benchmark model using viral predictions of tweets combined with\na topic model. The keyword-based topic model outperforms this sophisticated\nbaseline model by 67%.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 03:41:10 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Wang", "Xingyu", ""], ["Zhang", "Lida", ""], ["Klabjan", "Diego", ""]]}, {"id": "2001.07876", "submitter": "Xingbo Wang", "authors": "Xingbo Wang, Haipeng Zeng, Yong Wang, Aoyu Wu, Zhida Sun, Xiaojuan Ma,\n  Huamin Qu", "title": "VoiceCoach: Interactive Evidence-based Training for Voice Modulation\n  Skills in Public Speaking", "comments": "Accepted by CHI '20", "journal-ref": "Proceedings of the 2020 CHI Conference on Human Factors in\n  Computing Systems", "doi": "10.1145/3313831.3376726", "report-no": null, "categories": "cs.HC cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modulation of voice properties, such as pitch, volume, and speed, is\ncrucial for delivering a successful public speech. However, it is challenging\nto master different voice modulation skills. Though many guidelines are\navailable, they are often not practical enough to be applied in different\npublic speaking situations, especially for novice speakers. We present\nVoiceCoach, an interactive evidence-based approach to facilitate the effective\ntraining of voice modulation skills. Specifically, we have analyzed the voice\nmodulation skills from 2623 high-quality speeches (i.e., TED Talks) and use\nthem as the benchmark dataset. Given a voice input, VoiceCoach automatically\nrecommends good voice modulation examples from the dataset based on the\nsimilarity of both sentence structures and voice modulation skills. Immediate\nand quantitative visual feedback is provided to guide further improvement. The\nexpert interviews and the user study provide support for the effectiveness and\nusability of VoiceCoach.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 04:52:06 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wang", "Xingbo", ""], ["Zeng", "Haipeng", ""], ["Wang", "Yong", ""], ["Wu", "Aoyu", ""], ["Sun", "Zhida", ""], ["Ma", "Xiaojuan", ""], ["Qu", "Huamin", ""]]}, {"id": "2001.07906", "submitter": "Arnau Prat-P\\'erez", "authors": "Angela Bonifati, Irena Holubov\\'a, Arnau Prat-P\\'erez, Sherif Sakr", "title": "Graph Generators: State of the Art and Open Challenges", "comments": "ACM Computing Surveys, 32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of interconnected data has fueled the design and implementation\nof graph generators reproducing real-world linking properties, or gauging the\neffectiveness of graph algorithms, techniques and applications manipulating\nthese data. We consider graph generation across multiple subfields, such as\nSemantic Web, graph databases, social networks, and community detection, along\nwith general graphs. Despite the disparate requirements of modern graph\ngenerators throughout these communities, we analyze them under a common\numbrella, reaching out the functionalities, the practical usage, and their\nsupported operations. We argue that this classification is serving the need of\nproviding scientists, researchers and practitioners with the right data\ngenerator at hand for their work. This survey provides a comprehensive overview\nof the state-of-the-art graph generators by focusing on those that are\npertinent and suitable for several data-intensive tasks. Finally, we discuss\nopen challenges and missing requirements of current graph generators along with\ntheir future extensions to new emerging fields.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 08:11:06 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Bonifati", "Angela", ""], ["Holubov\u00e1", "Irena", ""], ["Prat-P\u00e9rez", "Arnau", ""], ["Sakr", "Sherif", ""]]}, {"id": "2001.07987", "submitter": "Patrice Bellot", "authors": "Patrice Bellot (R2I, LIS), Lerch So\\\"elie (R2I, DIAMS), Bruno Emmanuel\n  (DIAMS), Murisasco Elisabeth (DIAMS)", "title": "Emotion and Sentiment Lexicon Impact on Sentiment Analysis Applied to\n  Book Reviews", "comments": "in French", "journal-ref": "COnf{\\'e}rence en Recherche d'Informations et Applications - CORIA\n  2019, 16th French Information Retrieval Conference, Mar 2019, Lyon, France", "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumers are used to consulting posted reviews on the Internet before buying\na product. But it's difficult to know the global opinion considering the\nimportant number of those reviews. Sentiment analysis afford detecting polarity\n(positive, negative, neutral) in a expressed opinion and therefore classifying\nthose reviews. Our purpose is to determine the influence of emotions on the\npolarity of books reviews. We define \"bag-of-words\" representation models of\nreviews which use a lexicon containing emotional (anticipation, sadness, fear,\nanger, joy, surprise, trust, disgust) and sentimental (positive, negative)\nwords. This lexicon afford measuring felt emotions types by readers. The\nimplemented supervised learning used is a Random Forest type. The application\nconcerns Amazon platform's reviews. Mots-cl{\\'e}s : Analyse de sentiments,\nAnalyse d'{\\'e}motions (texte), Classification de polarit{\\'e} de sentiments\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 12:41:05 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Bellot", "Patrice", "", "R2I, LIS"], ["So\u00eblie", "Lerch", "", "R2I, DIAMS"], ["Emmanuel", "Bruno", "", "DIAMS"], ["Elisabeth", "Murisasco", "", "DIAMS"]]}, {"id": "2001.08010", "submitter": "Mahieddine Djoudi", "authors": "Zitouni Abdelhafid (LIRE), Hichem Rahab (ICOSI, LIRE), Abdelhafid\n  Zitouni (LIRE), Mahieddine Djoudi (TECHN\\'E - EA 6316)", "title": "ARAACOM: ARAbic Algerian Corpus for Opinion Mining", "comments": null, "journal-ref": "ICCES '17: Proceedings of the International Conference on\n  Computing for Engineering and Sciences, Jul 2017, Istanbul, France. pp.35-39", "doi": "10.1145/3129186.3129193", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, it is no more needed to do an enormous effort to distribute a lot\nof forms to thousands of people and collect them, then convert this from into\nelectronic format to track people opinion about some subjects. A lot of web\nsites can today reach a large spectrum with less effort. The majority of web\nsites suggest to their visitors to leave backups about their feeling of the\nsite or events. So, this makes for us a lot of data which need powerful mean to\nexploit. Opinion mining in the web becomes more and more an attracting task,\ndue the increasing need for individuals and societies to track the mood of\npeople against several subjects of daily life (sports, politics,\ntelevision,...). A lot of works in opinion mining was developed in western\nlanguages especially English, such works in Arabic language still very scarce.\nIn this paper, we propose our approach, for opinion mining in Arabic Algerian\nnews paper. CCS CONCEPTS $\\bullet$Information systems~Sentiment analysis\n$\\bullet$ Computing methodologies~Natural language processing\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 13:45:34 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Abdelhafid", "Zitouni", "", "LIRE"], ["Rahab", "Hichem", "", "ICOSI, LIRE"], ["Zitouni", "Abdelhafid", "", "LIRE"], ["Djoudi", "Mahieddine", "", "TECHN\u00c9 - EA 6316"]]}, {"id": "2001.08013", "submitter": "Balaji Ganesan", "authors": "Balaji Ganesan, Riddhiman Dasgupta, Akshay Parekh, Hima Patel, and\n  Berthold Reinwald", "title": "A Neural Architecture for Person Ontology population", "comments": "6 pages, 10 figures. arXiv admin note: substantial text overlap with\n  arXiv:1811.09368", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A person ontology comprising concepts, attributes and relationships of people\nhas a number of applications in data protection, didentification, population of\nknowledge graphs for business intelligence and fraud prevention. While\nartificial neural networks have led to improvements in Entity Recognition,\nEntity Classification, and Relation Extraction, creating an ontology largely\nremains a manual process, because it requires a fixed set of semantic relations\nbetween concepts. In this work, we present a system for automatically\npopulating a person ontology graph from unstructured data using neural models\nfor Entity Classification and Relation Extraction. We introduce a new dataset\nfor these tasks and discuss our results.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 13:49:14 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Ganesan", "Balaji", ""], ["Dasgupta", "Riddhiman", ""], ["Parekh", "Akshay", ""], ["Patel", "Hima", ""], ["Reinwald", "Berthold", ""]]}, {"id": "2001.08085", "submitter": "Hardik Joshi Mr.", "authors": "Hardik Joshi, Jyoti Pareek", "title": "Experiments on Manual Thesaurus based Query Expansion for Ad-hoc\n  Monolingual Gujarati Information Retrieval Tasks", "comments": "arXiv admin note: substantial text overlap with arXiv:1209.0126", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the experimental work done on Query Expansion (QE)\nfor retrieval tasks of Gujarati text documents. In information retrieval, it is\nvery difficult to estimate the exact user need, query expansion adds terms to\nthe original query, which provides more information about the user need. There\nare various approaches to query expansion. In our work, manual thesaurus based\nquery expansion was performed to evaluate the performance of widely used\ninformation retrieval models for Gujarati text documents. Results show that\nquery expansion improves the recall of text documents.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 10:23:48 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Joshi", "Hardik", ""], ["Pareek", "Jyoti", ""]]}, {"id": "2001.08546", "submitter": "Preslav Nakov", "authors": "Alberto Barron-Cedeno, Tamer Elsayed, Preslav Nakov, Giovanni Da San\n  Martino, Maram Hasanain, Reem Suwaileh, and Fatima Haouari", "title": "CheckThat! at CLEF 2020: Enabling the Automatic Identification and\n  Verification of Claims in Social Media", "comments": "Computational journalism, Check-worthiness, Fact-checking, Veracity,\n  CLEF-2020 CheckThat! Lab", "journal-ref": "CLEF-2018 ECIR-2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the third edition of the CheckThat! Lab, which is part of the\n2020 Cross-Language Evaluation Forum (CLEF). CheckThat! proposes four\ncomplementary tasks and a related task from previous lab editions, offered in\nEnglish, Arabic, and Spanish. Task 1 asks to predict which tweets in a Twitter\nstream are worth fact-checking. Task 2 asks to determine whether a claim posted\nin a tweet can be verified using a set of previously fact-checked claims. Task\n3 asks to retrieve text snippets from a given set of Web pages that would be\nuseful for verifying a target tweet's claim. Task 4 asks to predict the\nveracity of a target tweet's claim using a set of Web pages and potentially\nuseful snippets in them. Finally, the lab offers a fifth task that asks to\npredict the check-worthiness of the claims made in English political debates\nand speeches. CheckThat! features a full evaluation framework. The evaluation\nis carried out using mean average precision or precision at rank k for ranking\ntasks, and F1 for classification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 06:47:11 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Barron-Cedeno", "Alberto", ""], ["Elsayed", "Tamer", ""], ["Nakov", "Preslav", ""], ["Martino", "Giovanni Da San", ""], ["Hasanain", "Maram", ""], ["Suwaileh", "Reem", ""], ["Haouari", "Fatima", ""]]}, {"id": "2001.08615", "submitter": "Alberto Tejero", "authors": "Alberto Tejero, Victor Rodriguez-Doncel and Ivan Pau", "title": "Knowledge Graphs for Innovation Ecosystems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Innovation ecosystems can be naturally described as a collection of networked\nentities, such as experts, institutions, projects, technologies and products.\nRepresenting in a machine-readable form these entities and their relations is\nnot entirely attainable, due to the existence of abstract concepts such as\nknowledge and due to the confidential, non-public nature of this information,\nbut even its partial depiction is of strong interest. The representation of\ninnovation ecosystems incarnated as knowledge graphs would enable the\ngeneration of reports with new insights, the execution of advanced data\nanalysis tasks. An ontology to capture the essential entities and relations is\npresented, as well as the description of data sources, which can be used to\npopulate innovation knowledge graphs. Finally, the application case of the\nUniversidad Politecnica de Madrid is presented, as well as an insight of future\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 08:02:32 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Tejero", "Alberto", ""], ["Rodriguez-Doncel", "Victor", ""], ["Pau", "Ivan", ""]]}, {"id": "2001.08635", "submitter": "Chao Wang", "authors": "Chao Wang", "title": "A Study of the Tasks and Models in Machine Reading Comprehension", "comments": "PhD Qualifying Examination Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To provide a survey on the existing tasks and models in Machine Reading\nComprehension (MRC), this report reviews: 1) the dataset collection and\nperformance evaluation of some representative simple-reasoning and\ncomplex-reasoning MRC tasks; 2) the architecture designs, attention mechanisms,\nand performance-boosting approaches for developing neural-network-based MRC\nmodels; 3) some recently proposed transfer learning approaches to incorporating\ntext-style knowledge contained in external corpora into the neural networks of\nMRC models; 4) some recently proposed knowledge base encoding approaches to\nincorporating graph-style knowledge contained in external knowledge bases into\nthe neural networks of MRC models. Besides, according to what has been achieved\nand what are still deficient, this report also proposes some open problems for\nthe future research.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 16:11:44 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Wang", "Chao", ""]]}, {"id": "2001.08647", "submitter": "Morane Gruenpeter", "authors": "Roberto Di Cosmo (UPD7), Morane Gruenpeter (UNIVAQ), Stefano\n  Zacchiroli (UPD7)", "title": "Referencing Source Code Artifacts: a Separate Concern in Software\n  Citation", "comments": "Computing in Science & Engineering, IEEE, In press", "journal-ref": null, "doi": "10.1109/MCSE.2019.2963148", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the entities involved in software citation, software source code\nrequires special attention, due to the role it plays in ensuring scientific\nreproducibility. To reference source code we need identifiers that are not only\nunique and persistent, but also support \\emph{integrity} checking\nintrinsically. Suitable identifiers must guarantee that denotedobjects will\nalways stay the same, without relying on external third parties and\nadministrative processes. We analyze the role of identifiers for digital\nobjects (IDOs), whose properties are different from, and complementary to,\nthose of the various digital identifiers of objects (DIOs) that are today\npopular building blocks of software and data citation toolchains.We argue that\nboth kinds of identifiers are needed and detail the syntax, semantics, and\npractical implementation of the persistent identifiers (PIDs) adopted by the\nSoftware Heritage project to reference billions of softwaresource code\nartifacts such as source code files, directories, and commits.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 16:36:34 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Di Cosmo", "Roberto", "", "UPD7"], ["Gruenpeter", "Morane", "", "UNIVAQ"], ["Zacchiroli", "Stefano", "", "UPD7"]]}, {"id": "2001.08687", "submitter": "Rodrigo Nogueira", "authors": "Rodrigo Nogueira, Zhiying Jiang, Kyunghyun Cho, Jimmy Lin", "title": "Navigation-Based Candidate Expansion and Pretrained Language Models for\n  Citation Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation recommendation systems for the scientific literature, to help\nauthors find papers that should be cited, have the potential to speed up\ndiscoveries and uncover new routes for scientific exploration. We treat this\ntask as a ranking problem, which we tackle with a two-stage approach: candidate\ngeneration followed by re-ranking. Within this framework, we adapt to the\nscientific domain a proven combination based on \"bag of words\" retrieval\nfollowed by re-scoring with a BERT model. We experimentally show the effects of\ndomain adaptation, both in terms of pretraining on in-domain data and\nexploiting in-domain vocabulary. In addition, we introduce a novel\nnavigation-based document expansion strategy to enrich the candidate documents\nprocessed by our neural models. On three different collections from different\nscientific disciplines, we achieve the best-reported results in the citation\nrecommendation task.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 17:29:04 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Nogueira", "Rodrigo", ""], ["Jiang", "Zhiying", ""], ["Cho", "Kyunghyun", ""], ["Lin", "Jimmy", ""]]}, {"id": "2001.08700", "submitter": "Abhijit Suprem", "authors": "Abhijit Suprem and Calton Pu", "title": "EventMapper: Detecting Real-World Physical Events Using Corroborative\n  and Probabilistic Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of social media makes it a rich source for physical event\ndetection, such as disasters, and as a potential resource for crisis management\nresource allocation. There have been some recent works on leveraging social\nmedia sources for retrospective, after-the-fact event detection of large events\nsuch as earthquakes or hurricanes. Similarly, there is a long history of using\ntraditional physical sensors such as climate satellites to perform regional\nevent detection. However, combining social media with corroborative physical\nsensors for real-time, accurate, and global physical detection has remained\nunexplored.\n  This paper presents EventMapper, a framework to support event recognition of\nsmall yet equally costly events (landslides, flooding, wildfires). EventMapper\nintegrates high-latency, high-accuracy corroborative sources such as physical\nsensors with low-latency, noisy probabilistic sources such as social media\nstreams to deliver real-time, global event recognition. Furthermore,\nEventMapper is resilient to the concept drift phenomenon, where machine\nlearning models require continuous fine-tuning to maintain high performance.\n  By exploiting the common features of probabilistic and corroborative sources,\nEventMapper automates machine learning model updates, maintenance, and\nfine-tuning. We describe three applications built on EventMapper for landslide,\nwildfire, and flooding detection.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 17:47:31 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Suprem", "Abhijit", ""], ["Pu", "Calton", ""]]}, {"id": "2001.08767", "submitter": "Anay Mehrotra", "authors": "L. Elisa Celis and Anay Mehrotra and Nisheeth K. Vishnoi", "title": "Interventions for Ranking in the Presence of Implicit Bias", "comments": "This paper will appear at the ACM FAT* 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit bias is the unconscious attribution of particular qualities (or lack\nthereof) to a member from a particular social group (e.g., defined by gender or\nrace). Studies on implicit bias have shown that these unconscious stereotypes\ncan have adverse outcomes in various social contexts, such as job screening,\nteaching, or policing. Recently, (Kleinberg and Raghavan, 2018) considered a\nmathematical model for implicit bias and showed the effectiveness of the Rooney\nRule as a constraint to improve the utility of the outcome for certain cases of\nthe subset selection problem. Here we study the problem of designing\ninterventions for the generalization of subset selection -- ranking -- that\nrequires to output an ordered set and is a central primitive in various social\nand computational contexts. We present a family of simple and interpretable\nconstraints and show that they can optimally mitigate implicit bias for a\ngeneralization of the model studied in (Kleinberg and Raghavan, 2018).\nSubsequently, we prove that under natural distributional assumptions on the\nutilities of items, simple, Rooney Rule-like, constraints can also surprisingly\nrecover almost all the utility lost due to implicit biases. Finally, we augment\nour theoretical results with empirical findings on real-world distributions\nfrom the IIT-JEE (2009) dataset and the Semantic Scholar Research corpus.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 19:11:31 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Celis", "L. Elisa", ""], ["Mehrotra", "Anay", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "2001.08810", "submitter": "Valerio Lorini", "authors": "Valerio Lorini, Javier Rando, Diego Saez-Trumper, Carlos Castillo", "title": "Uneven Coverage of Natural Disasters in Wikipedia: the Case of Flood", "comments": "17 pages, submitted to ISCRAM 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The usage of non-authoritative data for disaster management presents the\nopportunity of accessing timely information that might not be available through\nother means, as well as the challenge of dealing with several layers of biases.\nWikipedia, a collaboratively-produced encyclopedia, includes in-depth\ninformation about many natural and human-made disasters, and its editors are\nparticularly good at adding information in real-time as a crisis unfolds. In\nthis study, we focus on the English version of Wikipedia, that is by far the\nmost comprehensive version of this encyclopedia. Wikipedia tends to have good\ncoverage of disasters, particularly those having a large number of fatalities.\nHowever, we also show that a tendency to cover events in wealthy countries and\nnot cover events in poorer ones permeates Wikipedia as a source for\ndisaster-related information. By performing careful automatic content analysis\nat a large scale, we show how the coverage of floods in Wikipedia is skewed\ntowards rich, English-speaking countries, in particular the US and Canada. We\nalso note how coverage of floods in countries with the lowest income, as well\nas countries in South America, is substantially lower than the coverage of\nfloods in middle-income countries. These results have implications for systems\nusing Wikipedia or similar collaborative media platforms as an information\nsource for detecting emergencies or for gathering valuable information for\ndisaster response.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 21:13:34 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Lorini", "Valerio", ""], ["Rando", "Javier", ""], ["Saez-Trumper", "Diego", ""], ["Castillo", "Carlos", ""]]}, {"id": "2001.08895", "submitter": "Abhijit Suprem", "authors": "Abhijit Suprem, Calton Pu, Joao Eduardo Ferreira", "title": "Small, Accurate, and Fast Vehicle Re-ID on the Edge: the SAFR Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Small, Accurate, and Fast Re-ID (SAFR) design for flexible\nvehicle re-id under a variety of compute environments such as cloud, mobile,\nedge, or embedded devices by only changing the re-id model backbone. Through\nbest-fit design choices, feature extraction, training tricks, global attention,\nand local attention, we create a reid model design that optimizes\nmulti-dimensionally along model size, speed, & accuracy for deployment under\nvarious memory and compute constraints. We present several variations of our\nflexible SAFR model: SAFR-Large for cloud-type environments with large compute\nresources, SAFR-Small for mobile devices with some compute constraints, and\nSAFR-Micro for edge devices with severe memory & compute constraints.\n  SAFR-Large delivers state-of-the-art results with mAP 81.34 on the VeRi-776\nvehicle re-id dataset (15% better than related work). SAFR-Small trades a 5.2%\ndrop in performance (mAP 77.14 on VeRi-776) for over 60% model compression and\n150% speedup. SAFR-Micro, at only 6MB and 130MFLOPS, trades 6.8% drop in\naccuracy (mAP 75.80 on VeRi-776) for 95% compression and 33x speedup compared\nto SAFR-Large.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 06:07:17 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Suprem", "Abhijit", ""], ["Pu", "Calton", ""], ["Ferreira", "Joao Eduardo", ""]]}, {"id": "2001.08961", "submitter": "Hossein A. Rahmani", "authors": "Hossein A. Rahmani, Mohammad Aliannejadi, Mitra Baratchi, Fabio\n  Crestani", "title": "Joint Geographical and Temporal Modeling based on Matrix Factorization\n  for Point-of-Interest Recommendation", "comments": "To be appear in ECIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the popularity of Location-based Social Networks, Point-of-Interest\n(POI) recommendation has become an important task, which learns the users'\npreferences and mobility patterns to recommend POIs. Previous studies show that\nincorporating contextual information such as geographical and temporal\ninfluences is necessary to improve POI recommendation by addressing the data\nsparsity problem. However, existing methods model the geographical influence\nbased on the physical distance between POIs and users, while ignoring the\ntemporal characteristics of such geographical influences. In this paper, we\nperform a study on the user mobility patterns where we find out that users'\ncheck-ins happen around several centers depending on their current temporal\nstate. Next, we propose a spatio-temporal activity-centers algorithm to model\nusers' behavior more accurately. Finally, we demonstrate the effectiveness of\nour proposed contextual model by incorporating it into the matrix factorization\nmodel under two different settings: i) static and ii) temporal. To show the\neffectiveness of our proposed method, which we refer to as STACP, we conduct\nexperiments on two well-known real-world datasets acquired from Gowalla and\nFoursquare LBSNs. Experimental results show that the STACP model achieves a\nstatistically significant performance improvement, compared to the\nstate-of-the-art techniques. Also, we demonstrate the effectiveness of\ncapturing geographical and temporal information for modeling users' activity\ncenters and the importance of modeling them jointly.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 12:25:37 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Rahmani", "Hossein A.", ""], ["Aliannejadi", "Mohammad", ""], ["Baratchi", "Mitra", ""], ["Crestani", "Fabio", ""]]}, {"id": "2001.09006", "submitter": "Nagy Marcell", "authors": "Roland Molontay, Marcell Nagy", "title": "Twenty Years of Network Science: A Bibliographic and Co-Authorship\n  Network Analysis", "comments": "20 pages, 18 figures. arXiv admin note: substantial text overlap with\n  arXiv:1908.08478", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.DL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two decades ago three pioneering papers turned the attention to complex\nnetworks and initiated a new era of research, establishing an interdisciplinary\nfield called network science. Namely, these highly-cited seminal papers were\nwritten by Watts&Strogatz, Barab\\'asi&Albert, and Girvan&Newman on small-world\nnetworks, on scale-free networks and on the community structure of complex\nnetworks, respectively. In the past 20 years - due to the multidisciplinary\nnature of the field - a diverse but not divided network science community has\nemerged. In this paper, we investigate how this community has evolved over time\nwith respect to speed, diversity and interdisciplinary nature as seen through\nthe growing co-authorship network of network scientists (here the notion refers\nto a scholar with at least one paper citing at least one of the three\naforementioned milestone papers). After providing a bibliographic analysis of\n31,763 network science papers, we construct the co-authorship network of 56,646\nnetwork scientists and we analyze its topology and dynamics. We shed light on\nthe collaboration patterns of the last 20 years of network science by\ninvestigating numerous structural properties of the co-authorship network and\nby using enhanced data visualization techniques. We also identify the most\ncentral authors, the largest communities, investigate the spatiotemporal\nchanges, and compare the properties of the network to scientometric indicators.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 02:13:08 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 21:33:42 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 10:04:34 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Molontay", "Roland", ""], ["Nagy", "Marcell", ""]]}, {"id": "2001.09099", "submitter": "Jie Lei", "authors": "Jie Lei, Licheng Yu, Tamara L. Berg, Mohit Bansal", "title": "TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval", "comments": "ECCV 2020 (extended version, with TVC dataset+models; 35 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce TV show Retrieval (TVR), a new multimodal retrieval dataset. TVR\nrequires systems to understand both videos and their associated subtitle\n(dialogue) texts, making it more realistic. The dataset contains 109K queries\ncollected on 21.8K videos from 6 TV shows of diverse genres, where each query\nis associated with a tight temporal window. The queries are also labeled with\nquery types that indicate whether each of them is more related to video or\nsubtitle or both, allowing for in-depth analysis of the dataset and the methods\nthat built on top of it. Strict qualification and post-annotation verification\ntests are applied to ensure the quality of the collected data. Further, we\npresent several baselines and a novel Cross-modal Moment Localization (XML )\nnetwork for multimodal moment retrieval tasks. The proposed XML model uses a\nlate fusion design with a novel Convolutional Start-End detector (ConvSE),\nsurpassing baselines by a large margin and with better efficiency, providing a\nstrong starting point for future work. We have also collected additional\ndescriptions for each annotated moment in TVR to form a new multimodal\ncaptioning dataset with 262K captions, named TV show Caption (TVC). Both\ndatasets are publicly available. TVR: https://tvr.cs.unc.edu, TVC:\nhttps://tvr.cs.unc.edu/tvc.html.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 17:09:39 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 15:12:14 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Lei", "Jie", ""], ["Yu", "Licheng", ""], ["Berg", "Tamara L.", ""], ["Bansal", "Mohit", ""]]}, {"id": "2001.09386", "submitter": "Xiaotao Gu", "authors": "Xiaotao Gu, Yuning Mao, Jiawei Han, Jialu Liu, Hongkun Yu, You Wu,\n  Cong Yu, Daniel Finnie, Jiaqi Zhai, Nicholas Zukoski", "title": "Generating Representative Headlines for News Stories", "comments": "WebConf 2020 (WWW 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Millions of news articles are published online every day, which can be\noverwhelming for readers to follow. Grouping articles that are reporting the\nsame event into news stories is a common way of assisting readers in their news\nconsumption. However, it remains a challenging research problem to efficiently\nand effectively generate a representative headline for each story. Automatic\nsummarization of a document set has been studied for decades, while few studies\nhave focused on generating representative headlines for a set of articles.\nUnlike summaries, which aim to capture most information with least redundancy,\nheadlines aim to capture information jointly shared by the story articles in\nshort length, and exclude information that is too specific to each individual\narticle. In this work, we study the problem of generating representative\nheadlines for news stories. We develop a distant supervision approach to train\nlarge-scale generation models without any human annotation. This approach\ncenters on two technical components. First, we propose a multi-level\npre-training framework that incorporates massive unlabeled corpus with\ndifferent quality-vs.-quantity balance at different levels. We show that models\ntrained within this framework outperform those trained with pure human curated\ncorpus. Second, we propose a novel self-voting-based article attention layer to\nextract salient information shared by multiple articles. We show that models\nthat incorporate this layer are robust to potential noises in news stories and\noutperform existing baselines with or without noises. We can further enhance\nour model by incorporating human labels, and we show our distant supervision\napproach significantly reduces the demand on labeled data.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 02:08:22 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 02:13:59 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2020 21:39:47 GMT"}, {"version": "v4", "created": "Mon, 13 Apr 2020 21:47:52 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Gu", "Xiaotao", ""], ["Mao", "Yuning", ""], ["Han", "Jiawei", ""], ["Liu", "Jialu", ""], ["Yu", "Hongkun", ""], ["Wu", "You", ""], ["Yu", "Cong", ""], ["Finnie", "Daniel", ""], ["Zhai", "Jiaqi", ""], ["Zukoski", "Nicholas", ""]]}, {"id": "2001.09410", "submitter": "Han Xiao", "authors": "Han Xiao, Bruno Ordozgoiti, Aristides Gionis", "title": "Searching for polarization in signed graphs: a local spectral approach", "comments": "11 pages, 6 figures, accepted by WWW 2020, April 20-24, 2020, Taipei,\n  Taiwan", "journal-ref": null, "doi": "10.1145/3366423.3380121", "report-no": null, "categories": "cs.SI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signed graphs have been used to model interactions in social net-works, which\ncan be either positive (friendly) or negative (antagonistic). The model has\nbeen used to study polarization and other related phenomena in social networks,\nwhich can be harmful to the process of democratic deliberation in our society.\nAn interesting and challenging task in this application domain is to detect\npolarized communities in signed graphs. A number of different methods have been\nproposed for this task. However, existing approaches aim at finding globally\noptimal solutions. Instead, in this paper we are interested in finding\npolarized communities that are related to a small set of seed nodes provided as\ninput. Seed nodes may consist of two sets, which constitute the two sides of a\npolarized structure.\n  In this paper we formulate the problem of finding local polarized communities\nin signed graphs as a locally-biased eigen-problem. By viewing the eigenvector\nassociated with the smallest eigenvalue of the Laplacian matrix as the solution\nof a constrained optimization problem, we are able to incorporate the local\ninformation as an additional constraint. In addition, we show that the\nlocally-biased vector can be used to find communities with approximation\nguarantee with respect to a local analogue of the Cheeger constant on signed\ngraphs. By exploiting the sparsity in the input graph, an indicator vector for\nthe polarized communities can be found in time linear to the graph size.\n  Our experiments on real-world networks validate the proposed algorithm and\ndemonstrate its usefulness in finding local structures in this semi-supervised\nmanner.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 06:30:16 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Xiao", "Han", ""], ["Ordozgoiti", "Bruno", ""], ["Gionis", "Aristides", ""]]}, {"id": "2001.09455", "submitter": "Michael Ekstrand", "authors": "Mucun Tian, Michael D. Ekstrand", "title": "Estimating Error and Bias in Offline Evaluation Results", "comments": "Published in CHIIR 2020", "journal-ref": null, "doi": "10.1145/3343413.3378004", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline evaluations of recommender systems attempt to estimate users'\nsatisfaction with recommendations using static data from prior user\ninteractions. These evaluations provide researchers and developers with first\napproximations of the likely performance of a new system and help weed out bad\nideas before presenting them to users. However, offline evaluation cannot\naccurately assess novel, relevant recommendations, because the most novel items\nwere previously unknown to the user, so they are missing from the historical\ndata and cannot be judged as relevant.\n  We present a simulation study to estimate the error that such missing data\ncauses in commonly-used evaluation metrics in order to assess its prevalence\nand impact. We find that missing data in the rating or observation process\ncauses the evaluation protocol to systematically mis-estimate metric values,\nand in some cases erroneously determine that a popularity-based recommender\noutperforms even a perfect personalized recommender. Substantial breakthroughs\nin recommendation quality, therefore, will be difficult to assess with existing\noffline techniques.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 14:00:56 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Tian", "Mucun", ""], ["Ekstrand", "Michael D.", ""]]}, {"id": "2001.09522", "submitter": "Jiaming Shen", "authors": "Jiaming Shen, Zhihong Shen, Chenyan Xiong, Chi Wang, Kuansan Wang,\n  Jiawei Han", "title": "TaxoExpan: Self-supervised Taxonomy Expansion with Position-Enhanced\n  Graph Neural Network", "comments": "WWW 2020", "journal-ref": null, "doi": "10.1145/3366423.3380132", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taxonomies consist of machine-interpretable semantics and provide valuable\nknowledge for many web applications. For example, online retailers (e.g.,\nAmazon and eBay) use taxonomies for product recommendation, and web search\nengines (e.g., Google and Bing) leverage taxonomies to enhance query\nunderstanding. Enormous efforts have been made on constructing taxonomies\neither manually or semi-automatically. However, with the fast-growing volume of\nweb content, existing taxonomies will become outdated and fail to capture\nemerging knowledge. Therefore, in many applications, dynamic expansions of an\nexisting taxonomy are in great demand. In this paper, we study how to expand an\nexisting taxonomy by adding a set of new concepts. We propose a novel\nself-supervised framework, named TaxoExpan, which automatically generates a set\nof <query concept, anchor concept> pairs from the existing taxonomy as training\ndata. Using such self-supervision data, TaxoExpan learns a model to predict\nwhether a query concept is the direct hyponym of an anchor concept. We develop\ntwo innovative techniques in TaxoExpan: (1) a position-enhanced graph neural\nnetwork that encodes the local structure of an anchor concept in the existing\ntaxonomy, and (2) a noise-robust training objective that enables the learned\nmodel to be insensitive to the label noise in the self-supervision data.\nExtensive experiments on three large-scale datasets from different domains\ndemonstrate both the effectiveness and the efficiency of TaxoExpan for taxonomy\nexpansion.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 21:30:21 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Shen", "Jiaming", ""], ["Shen", "Zhihong", ""], ["Xiong", "Chenyan", ""], ["Wang", "Chi", ""], ["Wang", "Kuansan", ""], ["Han", "Jiawei", ""]]}, {"id": "2001.09595", "submitter": "Xi Liu", "authors": "Xi Liu, Li Li, Ping-Chun Hsieh, Muhe Xie, Yong Ge, Rui Chen", "title": "Developing Multi-Task Recommendations with Long-Term Rewards via Policy\n  Distilled Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the explosive growth of online products and content, recommendation\ntechniques have been considered as an effective tool to overcome information\noverload, improve user experience, and boost business revenue. In recent years,\nwe have observed a new desideratum of considering long-term rewards of multiple\nrelated recommendation tasks simultaneously. The consideration of long-term\nrewards is strongly tied to business revenue and growth. Learning multiple\ntasks simultaneously could generally improve the performance of individual task\ndue to knowledge sharing in multi-task learning. While a few existing works\nhave studied long-term rewards in recommendations, they mainly focus on a\nsingle recommendation task. In this paper, we propose {\\it PoDiRe}: a\n\\underline{po}licy \\underline{di}stilled \\underline{re}commender that can\naddress long-term rewards of recommendations and simultaneously handle multiple\nrecommendation tasks. This novel recommendation solution is based on a marriage\nof deep reinforcement learning and knowledge distillation techniques, which is\nable to establish knowledge sharing among different tasks and reduce the size\nof a learning model. The resulting model is expected to attain better\nperformance and lower response latency for real-time recommendation services.\nIn collaboration with Samsung Game Launcher, one of the world's largest\ncommercial mobile game platforms, we conduct a comprehensive experimental study\non large-scale real data with hundreds of millions of events and show that our\nsolution outperforms many state-of-the-art methods in terms of several standard\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 06:05:42 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Liu", "Xi", ""], ["Li", "Li", ""], ["Hsieh", "Ping-Chun", ""], ["Xie", "Muhe", ""], ["Ge", "Yong", ""], ["Chen", "Rui", ""]]}, {"id": "2001.09694", "submitter": "Zhuosheng Zhang", "authors": "Zhuosheng Zhang, Junjie Yang, Hai Zhao", "title": "Retrospective Reader for Machine Reading Comprehension", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine reading comprehension (MRC) is an AI challenge that requires machine\nto determine the correct answers to questions based on a given passage. MRC\nsystems must not only answer question when necessary but also distinguish when\nno answer is available according to the given passage and then tactfully\nabstain from answering. When unanswerable questions are involved in the MRC\ntask, an essential verification module called verifier is especially required\nin addition to the encoder, though the latest practice on MRC modeling still\nmost benefits from adopting well pre-trained language models as the encoder\nblock by only focusing on the \"reading\". This paper devotes itself to exploring\nbetter verifier design for the MRC task with unanswerable questions. Inspired\nby how humans solve reading comprehension questions, we proposed a\nretrospective reader (Retro-Reader) that integrates two stages of reading and\nverification strategies: 1) sketchy reading that briefly investigates the\noverall interactions of passage and question, and yield an initial judgment; 2)\nintensive reading that verifies the answer and gives the final prediction. The\nproposed reader is evaluated on two benchmark MRC challenge datasets SQuAD2.0\nand NewsQA, achieving new state-of-the-art results. Significance tests show\nthat our model is significantly better than the strong ELECTRA and ALBERT\nbaselines. A series of analysis is also conducted to interpret the\neffectiveness of the proposed reader.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 11:14:34 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 17:42:50 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2020 02:52:35 GMT"}, {"version": "v4", "created": "Fri, 11 Dec 2020 09:28:12 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Zhang", "Zhuosheng", ""], ["Yang", "Junjie", ""], ["Zhao", "Hai", ""]]}, {"id": "2001.09698", "submitter": "Ehtesham Iqbal", "authors": "Ehtesham Iqbal, Risha Govind, Alvin Romero, Olubanke Dzahini, Matthew\n  Broadbent, Robert Stewart, Tanya Smith, Chi-Hun Kim, Nomi Werbeloff, Richard\n  Dobson and Zina Ibrahim", "title": "The side effect profile of Clozapine in real world data of three large\n  mental hospitals", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0243437", "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Mining the data contained within Electronic Health Records (EHRs)\ncan potentially generate a greater understanding of medication effects in the\nreal world, complementing what we know from Randomised control trials (RCTs).\nWe Propose a text mining approach to detect adverse events and medication\nepisodes from the clinical text to enhance our understanding of adverse effects\nrelated to Clozapine, the most effective antipsychotic drug for the management\nof treatment-resistant schizophrenia, but underutilised due to concerns over\nits side effects. Material and Methods: We used data from de-identified EHRs of\nthree mental health trusts in the UK (>50 million documents, over 500,000\npatients, 2835 of which were prescribed Clozapine). We explored the prevalence\nof 33 adverse effects by age, gender, ethnicity, smoking status and admission\ntype three months before and after the patients started Clozapine treatment. We\ncompared the prevalence of adverse effects with those reported in the Side\nEffects Resource (SIDER) where possible. Results: Sedation, fatigue, agitation,\ndizziness, hypersalivation, weight gain, tachycardia, headache, constipation\nand confusion were amongst the highest recorded Clozapine adverse effect in the\nthree months following the start of treatment. Higher percentages of all\nadverse effects were found in the first month of Clozapine therapy. Using a\nsignificance level of (p< 0.05) out chi-square tests show a significant\nassociation between most of the ADRs in smoking status and hospital admissions\nand some in gender and age groups. Further, the data was combined from three\ntrusts, and chi-square tests were applied to estimate the average effect of\nADRs in each monthly interval. Conclusion: A better understanding of how the\ndrug works in the real world can complement clinical trials and precision\nmedicine.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 11:21:54 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Iqbal", "Ehtesham", ""], ["Govind", "Risha", ""], ["Romero", "Alvin", ""], ["Dzahini", "Olubanke", ""], ["Broadbent", "Matthew", ""], ["Stewart", "Robert", ""], ["Smith", "Tanya", ""], ["Kim", "Chi-Hun", ""], ["Werbeloff", "Nomi", ""], ["Dobson", "Richard", ""], ["Ibrahim", "Zina", ""]]}, {"id": "2001.09896", "submitter": "Vinicius Carid\\'a", "authors": "Amir Jalilifard, Vinicius F. Carid\\'a, Alex F. Mansano, Rogers S.\n  Cristo, Felipe Penhorate C. da Fonseca", "title": "Semantic Sensitive TF-IDF to Determine Word Relevance in Documents", "comments": "11 pages, 2 figures, 22 references", "journal-ref": null, "doi": "10.1007/978-981-33-6977-1", "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword extraction has received an increasing attention as an important\nresearch topic which can lead to have advancements in diverse applications such\nas document context categorization, text indexing and document classification.\nIn this paper we propose STF-IDF, a novel semantic method based on TF-IDF, for\nscoring word importance of informal documents in a corpus. A set of nearly four\nmillion documents from health-care social media was collected and was trained\nin order to draw semantic model and to find the word embeddings. Then, the\nfeatures of semantic space were utilized to rearrange the original TF-IDF\nscores through an iterative solution so as to improve the moderate performance\nof this algorithm on informal texts. After testing the proposed method with 200\nrandomly chosen documents, our method managed to decrease the TF-IDF mean error\nrate by a factor of 50% and reaching the mean error of 13.7%, as opposed to\n27.2% of the original TF-IDF.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 00:23:11 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 23:52:07 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Jalilifard", "Amir", ""], ["Carid\u00e1", "Vinicius F.", ""], ["Mansano", "Alex F.", ""], ["Cristo", "Rogers S.", ""], ["da Fonseca", "Felipe Penhorate C.", ""]]}, {"id": "2001.09897", "submitter": "Soumi Chattopadhyay", "authors": "Ranjana Roy Chowdhury, Soumi Chattopadhyay, Chandranath Adak", "title": "CAHPHF: Context-Aware Hierarchical QoS Prediction with Hybrid Filtering", "comments": null, "journal-ref": "IEEE Transactions on Services Computing 2020", "doi": "10.1109/TSC.2020.3041626", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of Internet-of-Things and continuous growth in the\nnumber of web services at the Internet-scale, the service recommendation is\nbecoming a challenge nowadays. One of the prime aspects influencing the service\nrecommendation is the Quality-of-Service (QoS) parameter, which depicts the\nperformance of a web service. In general, the service provider furnishes the\nvalue of the QoS parameters during service deployment. However, in reality, the\nQoS values of service vary across different users, time, locations, etc.\nTherefore, estimating the QoS value of service before its execution is an\nimportant task, and thus the QoS prediction has gained significant research\nattention. Multiple approaches are available in the literature for predicting\nservice QoS. However, these approaches are yet to reach the desired accuracy\nlevel. In this paper, we study the QoS prediction problem across different\nusers, and propose a novel solution by taking into account the contextual\ninformation of both services and users. Our proposal includes two key steps:\n(a) hybrid filtering and (b) hierarchical prediction mechanism. On the one\nhand, the hybrid filtering method aims to obtain a set of similar users and\nservices, given a target user and a service. On the other hand, the goal of the\nhierarchical prediction mechanism is to estimate the QoS value accurately by\nleveraging hierarchical neural-regression. We evaluate our framework on the\npublicly available WS-DREAM datasets. The experimental results show the\noutperformance of our framework over the major state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 10:48:02 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Chowdhury", "Ranjana Roy", ""], ["Chattopadhyay", "Soumi", ""], ["Adak", "Chandranath", ""]]}, {"id": "2001.09898", "submitter": "Bernard Yannou", "authors": "Tianjun Hou (LGI), Bernard Yannou (LGI), Yann Leroy, Emilie Poirson\n  (IRCCyN)", "title": "Mining Changes in User Expectation Over Time From Online Reviews", "comments": null, "journal-ref": "Journal of Mechanical Design, American Society of Mechanical\n  Engineers, 2019, 141 (9)", "doi": "10.1115/1.4042793", "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customers post online reviews at any time. With the timestamp of online\nreviews, they can be regarded as a flow of information. With this\ncharacteristic, designers can capture the changes in customer feedback to help\nset up product improvement strategies. Here we propose an approach for\ncapturing changes of user expectation on product affordances based on the\nonline reviews for two generations of products. First, the approach uses a\nrule-based natural language processing method to automatically identify and\nstructure product affordances from review text. Then, inspired by the Kano\nmodel which classifies preferences of product attributes in five categories,\nconjoint analysis is used to quantitatively categorize the structured\naffordances. Finally, changes of user expectation can be found by applying the\nconjoint analysis on the online reviews posted for two successive generations\nof products. A case study based on the online reviews of Kindle e-readers\ndownloaded from amazon.com shows that designers can use our proposed approach\nto evaluate their product improvement strategies for previous products and\ndevelop new product improvement strategies for future products.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 12:57:06 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Hou", "Tianjun", "", "LGI"], ["Yannou", "Bernard", "", "LGI"], ["Leroy", "Yann", "", "IRCCyN"], ["Poirson", "Emilie", "", "IRCCyN"]]}, {"id": "2001.09899", "submitter": "Juan Manuel Ortiz De Zarate", "authors": "Juan Manuel Ortiz de Zarate and Esteban Feuerstein", "title": "Vocabulary-based Method for Quantifying Controversy in Social Media", "comments": "arXiv admin note: text overlap with arXiv:1507.05224 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying controversial topics is not only interesting from a social point\nof view, it also enables the application of methods to avoid the information\nsegregation, creating better discussion contexts and reaching agreements in the\nbest cases. In this paper we develop a systematic method for controversy\ndetection based primarily on the jargon used by the communities in social\nmedia. Our method dispenses with the use of domain-specific knowledge, is\nlanguage-agnostic, efficient and easy to apply. We perform an extensive set of\nexperiments across many languages, regions and contexts, taking controversial\nand non-controversial topics. We find that our vocabulary-based measure\nperforms better than state of the art measures that are based only on the\ncommunity graph structure. Moreover, we shows that it is possible to detect\npolarization through text analysis.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 17:43:21 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["de Zarate", "Juan Manuel Ortiz", ""], ["Feuerstein", "Esteban", ""]]}, {"id": "2001.09900", "submitter": "Zhiwei Liu", "authors": "Zhiwei Liu, Mengting Wan, Stephen Guo, Kannan Achan, Philip S. Yu", "title": "BasConv: Aggregating Heterogeneous Interactions for Basket\n  Recommendation with Graph Convolutional Neural Network", "comments": "Accepted to SDM 2020, Our code is available online at\n  https://github.com/JimLiu96/basConv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within-basket recommendation reduces the exploration time of users, where the\nuser's intention of the basket matters. The intent of a shopping basket can be\nretrieved from both user-item collaborative filtering signals and multi-item\ncorrelations. By defining a basket entity to represent the basket intent, we\ncan model this problem as a basket-item link prediction task in the\nUser-Basket-Item~(UBI) graph. Previous work solves the problem by leveraging\nuser-item interactions and item-item interactions simultaneously. However,\ncollectivity and heterogeneity characteristics are hardly investigated before.\nCollectivity defines the semantics of each node which should be aggregated from\nboth directly and indirectly connected neighbors. Heterogeneity comes from\nmulti-type interactions as well as multi-type nodes in the UBI graph. To this\nend, we propose a new framework named \\textbf{BasConv}, which is based on the\ngraph convolutional neural network. Our BasConv model has three types of\naggregators specifically designed for three types of nodes. They collectively\nlearn node embeddings from both neighborhood and high-order context.\nAdditionally, the interactive layers in the aggregators can distinguish\ndifferent types of interactions. Extensive experiments on two real-world\ndatasets prove the effectiveness of BasConv. Our code is available online at\nhttps://github.com/JimLiu96/basConv.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 16:27:32 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 03:17:23 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Liu", "Zhiwei", ""], ["Wan", "Mengting", ""], ["Guo", "Stephen", ""], ["Achan", "Kannan", ""], ["Yu", "Philip S.", ""]]}, {"id": "2001.10071", "submitter": "Lucas Oliveira E S", "authors": "Lucas Emanuel Silva e Oliveira, Ana Carolina Peters, Adalniza Moura\n  Pucca da Silva, Caroline P. Gebeluca, Yohan Bonescki Gumiel, Lilian Mie Mukai\n  Cintho, Deborah Ribeiro Carvalho, Sadid A. Hasan, Claudia Maria Cabral Moro", "title": "SemClinBr -- a multi institutional and multi specialty semantically\n  annotated corpus for Portuguese clinical NLP tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high volume of research focusing on extracting patient's information from\nelectronic health records (EHR) has led to an increase in the demand for\nannotated corpora, which are a very valuable resource for both the development\nand evaluation of natural language processing (NLP) algorithms. The absence of\na multi-purpose clinical corpus outside the scope of the English language,\nespecially in Brazilian Portuguese, is glaring and severely impacts scientific\nprogress in the biomedical NLP field. In this study, we developed a\nsemantically annotated corpus using clinical texts from multiple medical\nspecialties, document types, and institutions. We present the following: (1) a\nsurvey listing common aspects and lessons learned from previous research, (2) a\nfine-grained annotation schema which could be replicated and guide other\nannotation initiatives, (3) a web-based annotation tool focusing on an\nannotation suggestion feature, and (4) both intrinsic and extrinsic evaluation\nof the annotations. The result of this work is the SemClinBr, a corpus that has\n1,000 clinical notes, labeled with 65,117 entities and 11,263 relations, and\ncan support a variety of clinical NLP tasks and boost the EHR's secondary use\nfor the Portuguese language.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 20:39:32 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Oliveira", "Lucas Emanuel Silva e", ""], ["Peters", "Ana Carolina", ""], ["da Silva", "Adalniza Moura Pucca", ""], ["Gebeluca", "Caroline P.", ""], ["Gumiel", "Yohan Bonescki", ""], ["Cintho", "Lilian Mie Mukai", ""], ["Carvalho", "Deborah Ribeiro", ""], ["Hasan", "Sadid A.", ""], ["Moro", "Claudia Maria Cabral", ""]]}, {"id": "2001.10112", "submitter": "Zhiyu Chen", "authors": "Zhiyu Chen, Haiyan Jia, Jeff Heflin, Brian D. Davison", "title": "Leveraging Schema Labels to Enhance Dataset Search", "comments": "Accepted at the 42nd European Conference on IR Research, ECIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A search engine's ability to retrieve desirable datasets is important for\ndata sharing and reuse. Existing dataset search engines typically rely on\nmatching queries to dataset descriptions. However, a user may not have enough\nprior knowledge to write a query using terms that match with description\ntext.We propose a novel schema label generation model which generates possible\nschema labels based on dataset table content. We incorporate the generated\nschema labels into a mixed ranking model which not only considers the relevance\nbetween the query and dataset metadata but also the similarity between the\nquery and generated schema labels. To evaluate our method on real-world\ndatasets, we create a new benchmark specifically for the dataset retrieval\ntask. Experiments show that our approach can effectively improve the precision\nand NDCG scores of the dataset retrieval task compared with baseline methods.\nWe also test on a collection of Wikipedia tables to show that the features\ngenerated from schema labels can improve the unsupervised and supervised web\ntable retrieval task as well.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 22:41:02 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Chen", "Zhiyu", ""], ["Jia", "Haiyan", ""], ["Heflin", "Jeff", ""], ["Davison", "Brian D.", ""]]}, {"id": "2001.10167", "submitter": "Lei Chen", "authors": "Lei Chen, Le Wu, Richang Hong, Kun Zhang, Meng Wang", "title": "Revisiting Graph based Collaborative Filtering: A Linear Residual Graph\n  Convolutional Network Approach", "comments": "The updated version is publised in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) are state-of-the-art graph based\nrepresentation learning models by iteratively stacking multiple layers of\nconvolution aggregation operations and non-linear activation operations.\nRecently, in Collaborative Filtering (CF) based Recommender Systems (RS), by\ntreating the user-item interaction behavior as a bipartite graph, some\nresearchers model higher-layer collaborative signals with GCNs. These GCN based\nrecommender models show superior performance compared to traditional works.\nHowever, these models suffer from training difficulty with non-linear\nactivations for large user-item graphs. Besides, most GCN based models could\nnot model deeper layers due to the over smoothing effect with the graph\nconvolution operation. In this paper, we revisit GCN based CF models from two\naspects. First, we empirically show that removing non-linearities would enhance\nrecommendation performance, which is consistent with the theories in simple\ngraph convolutional networks. Second, we propose a residual network structure\nthat is specifically designed for CF with user-item interaction modeling, which\nalleviates the over smoothing problem in graph convolution aggregation\noperation with sparse user-item interaction data. The proposed model is a\nlinear model and it is easy to train, scale to large datasets, and yield better\nefficiency and effectiveness on two real datasets. We publish the source code\nat https://github.com/newlei/LRGCCF.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 04:41:25 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Chen", "Lei", ""], ["Wu", "Le", ""], ["Hong", "Richang", ""], ["Zhang", "Kun", ""], ["Wang", "Meng", ""]]}, {"id": "2001.10336", "submitter": "Philipp Mayr", "authors": "Guillaume Cabanac and Ingo Frommholz and Philipp Mayr", "title": "Bibliometric-enhanced Information Retrieval 10th Anniversary Workshop\n  Edition", "comments": "Overview paper submitted to ECIR 2020, Lisbon, PT. arXiv admin note:\n  substantial text overlap with arXiv:1909.04954", "journal-ref": null, "doi": "10.1007/978-3-030-45442-5_85", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bibliometric-enhanced Information Retrieval workshop series (BIR) was\nlaunched at ECIR in 2014 \\cite{MayrEtAl2014} and it was held at ECIR each year\nsince then. This year we organize the 10th iteration of BIR. The workshop\nseries at ECIR and JCDL/SIGIR tackles issues related to academic search, at the\ncrossroads between Information Retrieval, Natural Language Processing and\nBibliometrics. In this overview paper, we summarize the past workshops, present\nthe workshop topics for 2020 and reflect on some future steps for this workshop\nseries.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 13:16:12 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Cabanac", "Guillaume", ""], ["Frommholz", "Ingo", ""], ["Mayr", "Philipp", ""]]}, {"id": "2001.10337", "submitter": "Michael Bloodgood", "authors": "Thomas Orth and Michael Bloodgood", "title": "Early Forecasting of Text Classification Accuracy and F-Measure with\n  Active Learning", "comments": "8 pages, 9 figures, 2 tables; published in Proceedings of the IEEE\n  14th International Conference on Semantic Computing (ICSC), San Diego, CA,\n  USA, pages 77-84, February 2020", "journal-ref": "In Proceedings of the 2020 IEEE 14th International Conference on\n  Semantic Computing (ICSC), pages 77-84, San Diego, CA, USA, February 2020.\n  IEEE", "doi": "10.1109/ICSC.2020.00018", "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When creating text classification systems, one of the major bottlenecks is\nthe annotation of training data. Active learning has been proposed to address\nthis bottleneck using stopping methods to minimize the cost of data annotation.\nAn important capability for improving the utility of stopping methods is to\neffectively forecast the performance of the text classification models.\nForecasting can be done through the use of logarithmic models regressed on some\nportion of the data as learning is progressing. A critical unexplored question\nis what portion of the data is needed for accurate forecasting. There is a\ntension, where it is desirable to use less data so that the forecast can be\nmade earlier, which is more useful, versus it being desirable to use more data,\nso that the forecast can be more accurate. We find that when using active\nlearning it is even more important to generate forecasts earlier so as to make\nthem more useful and not waste annotation effort. We investigate the difference\nin forecasting difficulty when using accuracy and F-measure as the text\nclassification system performance metrics and we find that F-measure is more\ndifficult to forecast. We conduct experiments on seven text classification\ndatasets in different semantic domains with different characteristics and with\nthree different base machine learning algorithms. We find that forecasting is\neasiest for decision tree learning, moderate for Support Vector Machines, and\nmost difficult for neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 06:27:33 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 08:59:27 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Orth", "Thomas", ""], ["Bloodgood", "Michael", ""]]}, {"id": "2001.10338", "submitter": "Wei Pang Xubu", "authors": "Wei Pang", "title": "Short Text Classification via Term Graph", "comments": "9 pages, 15 figures, Short Text Classification, Term Graph", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short text classi cation is a method for classifying short sentence with\nprede ned labels. However, short text is limited in shortness in text length\nthat leads to a challenging problem of sparse features. Most of existing\nmethods treat each short sentences as independently and identically distributed\n(IID), local context only in the sentence itself is focused and the relational\ninformation between sentences are lost. To overcome these limitations, we\npropose a PathWalk model that combine the strength of graph networks and short\nsentences to solve the sparseness of short text. Experimental results on four\ndifferent available datasets show that our PathWalk method achieves the\nstate-of-the-art results, demonstrating the efficiency and robustness of graph\nnetworks for short text classification.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 04:32:13 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Pang", "Wei", ""]]}, {"id": "2001.10340", "submitter": "Raviraj Joshi", "authors": "Ramchandra Joshi, Purvi Goel, Raviraj Joshi", "title": "Deep Learning for Hindi Text Classification: A Comparison", "comments": "Accepted at International Conference on Intelligent Human Computer\n  Interaction(IHCI) 2019", "journal-ref": null, "doi": "10.1007/978-3-030-44689-5_9", "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Processing (NLP) and especially natural language text\nanalysis have seen great advances in recent times. Usage of deep learning in\ntext processing has revolutionized the techniques for text processing and\nachieved remarkable results. Different deep learning architectures like CNN,\nLSTM, and very recent Transformer have been used to achieve state of the art\nresults variety on NLP tasks. In this work, we survey a host of deep learning\narchitectures for text classification tasks. The work is specifically concerned\nwith the classification of Hindi text. The research in the classification of\nmorphologically rich and low resource Hindi language written in Devanagari\nscript has been limited due to the absence of large labeled corpus. In this\nwork, we used translated versions of English data-sets to evaluate models based\non CNN, LSTM and Attention. Multilingual pre-trained sentence embeddings based\non BERT and LASER are also compared to evaluate their effectiveness for the\nHindi language. The paper also serves as a tutorial for popular text\nclassification techniques.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 09:29:12 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Joshi", "Ramchandra", ""], ["Goel", "Purvi", ""], ["Joshi", "Raviraj", ""]]}, {"id": "2001.10341", "submitter": "Ning Yang", "authors": "Huanrui Luo, Ning Yang, Philip S. Yu", "title": "Hybrid Deep Embedding for Recommendations with Dynamic Aspect-Level\n  Explanations", "comments": "2019 IEEE International Conference on Big Data (Big Data) Best Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable recommendation is far from being well solved partly due to three\nchallenges. The first is the personalization of preference learning, which\nrequires that different items/users have different contributions to the\nlearning of user preference or item quality. The second one is dynamic\nexplanation, which is crucial for the timeliness of recommendation\nexplanations. The last one is the granularity of explanations. In practice,\naspect-level explanations are more persuasive than item-level or user-level\nones. In this paper, to address these challenges simultaneously, we propose a\nnovel model called Hybrid Deep Embedding (HDE) for aspect-based explainable\nrecommendations, which can make recommendations with dynamic aspect-level\nexplanations. The main idea of HDE is to learn the dynamic embeddings of users\nand items for rating prediction and the dynamic latent aspect\npreference/quality vectors for the generation of aspect-level explanations,\nthrough fusion of the dynamic implicit feedbacks extracted from reviews and the\nattentive user-item interactions. Particularly, as the aspect\npreference/quality of users/items is learned automatically, HDE is able to\ncapture the impact of aspects that are not mentioned in reviews of a user or an\nitem. The extensive experiments conducted on real datasets verify the\nrecommending performance and explainability of HDE. The source code of our work\nis available at \\url{https://github.com/lola63/HDE-Python}\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 13:16:32 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Luo", "Huanrui", ""], ["Yang", "Ning", ""], ["Yu", "Philip S.", ""]]}, {"id": "2001.10343", "submitter": "Pratikkumar Prajapati", "authors": "Pratikkumar Prajapati", "title": "Predictive analysis of Bitcoin price considering social sentiments", "comments": "12 pages, 4 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on the use of sentiment analysis on news and social media to\nanalyze and predict the price of Bitcoin. Bitcoin is the leading cryptocurrency\nand has the highest market capitalization among digital currencies. Predicting\nBitcoin values may help understand and predict potential market movement and\nfuture growth of the technology. Unlike (mostly) repeating phenomena like\nweather, cryptocurrency values do not follow a repeating pattern and mere past\nvalue of Bitcoin does not reveal any secret of future Bitcoin value. Humans\nfollow general sentiments and technical analysis to invest in the market. Hence\nconsidering people's sentiment can give a good degree of prediction. We focus\non using social sentiment as a feature to predict future Bitcoin value, and in\nparticular, consider Google News and Reddit posts. We find that social\nsentiment gives a good estimate of how future Bitcoin values may move. We\nachieve the lowest test RMSE of 434.87 using an LSTM that takes as inputs the\nhistorical price of various cryptocurrencies, the sentiment of news articles\nand the sentiment of Reddit posts.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 18:08:05 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Prajapati", "Pratikkumar", ""]]}, {"id": "2001.10374", "submitter": "David Noever", "authors": "David Noever", "title": "The Enron Corpus: Where the Email Bodies are Buried?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To probe the largest public-domain email database for indicators of fraud, we\napply machine learning and accomplish four investigative tasks. First, we\nidentify persons of interest (POI), using financial records and email, and\nreport a peak accuracy of 95.7%. Secondly, we find any publicly exposed\npersonally identifiable information (PII) and discover 50,000 previously\nunreported instances. Thirdly, we automatically flag legally responsive emails\nas scored by human experts in the California electricity blackout lawsuit, and\nfind a peak 99% accuracy. Finally, we track three years of primary topics and\nsentiment across over 10,000 unique people before, during and after the onset\nof the corporate crisis. Where possible, we compare accuracy against execution\ntimes for 51 algorithms and report human-interpretable business rules that can\nscale to vast datasets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 13:59:54 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Noever", "David", ""]]}, {"id": "2001.10378", "submitter": "Fei Chen", "authors": "Mi Luo, Fei Chen, Pengxiang Cheng, Zhenhua Dong, Xiuqiang He, Jiashi\n  Feng, Zhenguo Li", "title": "MetaSelector: Meta-Learning for Recommendation with User-Level Adaptive\n  Model Selection", "comments": "Accepted by WWW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems often face heterogeneous datasets containing highly\npersonalized historical data of users, where no single model could give the\nbest recommendation for every user. We observe this ubiquitous phenomenon on\nboth public and private datasets and address the model selection problem in\npursuit of optimizing the quality of recommendation for each user. We propose a\nmeta-learning framework to facilitate user-level adaptive model selection in\nrecommender systems. In this framework, a collection of recommenders is trained\nwith data from all users, on top of which a model selector is trained via\nmeta-learning to select the best single model for each user with the\nuser-specific historical data. We conduct extensive experiments on two public\ndatasets and a real-world production dataset, demonstrating that our proposed\nframework achieves improvements over single model baselines and sample-level\nmodel selector in terms of AUC and LogLoss. In particular, the improvements may\nlead to huge profit gain when deployed in online recommender systems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 16:05:01 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 08:03:25 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 03:18:32 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Luo", "Mi", ""], ["Chen", "Fei", ""], ["Cheng", "Pengxiang", ""], ["Dong", "Zhenhua", ""], ["He", "Xiuqiang", ""], ["Feng", "Jiashi", ""], ["Li", "Zhenguo", ""]]}, {"id": "2001.10379", "submitter": "Qianyu Guo", "authors": "Qianyu Guo, Jianzhong Qi", "title": "SANST: A Self-Attentive Network for Next Point-of-Interest\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next point-of-interest (POI) recommendation aims to offer suggestions on\nwhich POI to visit next, given a user's POI visit history. This problem has a\nwide application in the tourism industry, and it is gaining an increasing\ninterest as more POI check-in data become available. The problem is often\nmodeled as a sequential recommendation problem to take advantage of the\nsequential patterns of user check-ins, e.g., people tend to visit Central Park\nafter The Metropolitan Museum of Art in New York City. Recently, self-attentive\nnetworks have been shown to be both effective and efficient in general\nsequential recommendation problems, e.g., to recommend products, video games,\nor movies. Directly adopting self-attentive networks for next POI\nrecommendation, however, may produce sub-optimal recommendations. This is\nbecause vanilla self-attentive networks do not consider the spatial and\ntemporal patterns of user check-ins, which are two critical features in next\nPOI recommendation. To address this limitation, in this paper, we propose a\nmodel named SANST that incorporates spatio-temporal patterns of user check-ins\ninto self-attentive networks. To incorporate the spatial patterns, we encode\nthe relative positions of POIs into their embeddings before feeding the\nembeddings into the self-attentive network. To incorporate the temporal\npatterns, we discretize the time of POI check-ins and model the temporal\nrelationship between POI check-ins by a relation-aware self-attention module.\nWe evaluate the performance of our SANST model with three real-world datasets.\nThe results show that SANST consistently outperforms the state-of-theart\nmodels, and the advantage in nDCG@10 is up to 13.65%.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 14:21:09 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Guo", "Qianyu", ""], ["Qi", "Jianzhong", ""]]}, {"id": "2001.10380", "submitter": "Aladdin Ayesh", "authors": "Qadri Mishael and Aladdin Ayesh", "title": "Investigating Classification Techniques with Feature Selection For\n  Intention Mining From Twitter Feed", "comments": "24 pages, 7 figures, 6 tables, DRAFT journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the last decade, social networks became most popular medium for\ncommunication and interaction. As an example, micro-blogging service Twitter\nhas more than 200 million registered users who exchange more than 65 million\nposts per day. Users express their thoughts, ideas, and even their intentions\nthrough these tweets. Most of the tweets are written informally and often in\nslang language, that contains misspelt and abbreviated words. This paper\ninvestigates the problem of selecting features that affect extracting user's\nintention from Twitter feeds based on text mining techniques. It starts by\npresenting the method we used to construct our own dataset from extracted\nTwitter feeds. Following that, we present two techniques of feature selection\nfollowed by classification. In the first technique, we use Information Gain as\na one-phase feature selection, followed by supervised classification\nalgorithms. In the second technique, we use a hybrid approach based on forward\nfeature selection algorithm in which two feature selection techniques employed\nfollowed by classification algorithms. We examine these two techniques with\nfour classification algorithms. We evaluate them using our own dataset, and we\ncritically review the results.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 11:55:33 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Mishael", "Qadri", ""], ["Ayesh", "Aladdin", ""]]}, {"id": "2001.10382", "submitter": "Kaitao Zhang", "authors": "Kaitao Zhang, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu", "title": "Selective Weak Supervision for Neural Information Retrieval", "comments": "Accepted by WWW 2020", "journal-ref": null, "doi": "10.1145/3366423.3380131", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper democratizes neural information retrieval to scenarios where large\nscale relevance training signals are not available. We revisit the classic IR\nintuition that anchor-document relations approximate query-document relevance\nand propose a reinforcement weak supervision selection method, ReInfoSelect,\nwhich learns to select anchor-document pairs that best weakly supervise the\nneural ranker (action), using the ranking performance on a handful of relevance\nlabels as the reward. Iteratively, for a batch of anchor-document pairs,\nReInfoSelect back propagates the gradients through the neural ranker, gathers\nits NDCG reward, and optimizes the data selection network using policy\ngradients, until the neural ranker's performance peaks on target relevance\nmetrics (convergence). In our experiments on three TREC benchmarks, neural\nrankers trained by ReInfoSelect, with only publicly available anchor data,\nsignificantly outperform feature-based learning to rank methods and match the\neffectiveness of neural rankers trained with private commercial search logs.\nOur analyses show that ReInfoSelect effectively selects weak supervision\nsignals based on the stage of the neural ranker training, and intuitively picks\nanchor-document pairs similar to query-document pairs.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 14:47:07 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Zhang", "Kaitao", ""], ["Xiong", "Chenyan", ""], ["Liu", "Zhenghao", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "2001.10495", "submitter": "Amar Budhiraja", "authors": "Amar Budhiraja, Gaurush Hiranandani, Darshak Chhatbar, Aditya Sinha,\n  Navya Yarrabelly, Ayush Choure, Oluwasanmi Koyejo, Prateek Jain", "title": "Rich-Item Recommendations for Rich-Users: Exploiting Dynamic and Static\n  Side Information", "comments": "The first two authors contributed equally. 21 pages, 8 figures and 6\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of recommendation system where the users\nand items to be recommended are rich data structures with multiple entity types\nand with multiple sources of side-information in the form of graphs. We provide\na general formulation for the problem that captures the complexities of modern\nreal-world recommendations and generalizes many existing formulations. In our\nformulation, each user/document that requires a recommendation and each item or\ntag that is to be recommended, both are modeled by a set of static entities and\na dynamic component. The relationships between entities are captured by several\nweighted bipartite graphs. To effectively exploit these complex interactions\nand learn the recommendation model, we propose MEDRES- a multiple graph-CNN\nbased novel deep-learning architecture. MEDRES uses AL-GCN, a novel graph\nconvolution network block, that harnesses strong representative features from\nthe underlying graphs. Moreover, in order to capture highly heterogeneous\nengagement of different users with the system and constraints on the number of\nitems to be recommended, we propose a novel ranking metric pAp@k along with a\nmethod to optimize the metric directly. We demonstrate effectiveness of our\nmethod on two benchmarks: a) citation data, b) Flickr data. In addition, we\npresent two real-world case studies of our formulation and the MEDRES\narchitecture. We show how our technique can be used to naturally model the\nmessage recommendation problem and the teams recommendation problem in the\nMicrosoft Teams (MSTeams) product and demonstrate that it is 5-6% points more\naccurate than the production-grade models.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 17:53:38 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 12:34:37 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Budhiraja", "Amar", ""], ["Hiranandani", "Gaurush", ""], ["Chhatbar", "Darshak", ""], ["Sinha", "Aditya", ""], ["Yarrabelly", "Navya", ""], ["Choure", "Ayush", ""], ["Koyejo", "Oluwasanmi", ""], ["Jain", "Prateek", ""]]}, {"id": "2001.10613", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Alexandre Nadjem and Juan-Manuel Torres-Moreno and Marc El-B\\`eze and\n  Guillaume Marrel and Beno\\^it Bonte", "title": "Predicting Personalized Academic and Career Roads: First Steps Toward a\n  Multi-Uses Recommender System", "comments": "4 pages, 3 figures, 4 tables", "journal-ref": "Digital Tools & Uses Congress (DTUC '18), pp 1--4, 2018, Paris,\n  France", "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nobody knows what one's do in the future and everyone will have had a\ndifferent answer to the question : how do you see yourself in five years after\nyour current job/diploma? In this paper we introduce concepts, large categories\nof fields of studies or job domains in order to represent the vision of the\nfuture of the user's trajectory. Then, we show how they can influence the\nprediction when proposing him a set of next steps to take.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 11:00:54 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Nadjem", "Alexandre", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["El-B\u00e8ze", "Marc", ""], ["Marrel", "Guillaume", ""], ["Bonte", "Beno\u00eet", ""]]}, {"id": "2001.10667", "submitter": "Serena Khoo", "authors": "Ling Min Serena Khoo, Hai Leong Chieu, Zhong Qian and Jing Jiang", "title": "Interpretable Rumor Detection in Microblogs by Attending to User\n  Interactions", "comments": "8 pages, 3 figures, AAAI 2020 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address rumor detection by learning to differentiate between the\ncommunity's response to real and fake claims in microblogs. Existing\nstate-of-the-art models are based on tree models that model conversational\ntrees. However, in social media, a user posting a reply might be replying to\nthe entire thread rather than to a specific user. We propose a post-level\nattention model (PLAN) to model long distance interactions between tweets with\nthe multi-head attention mechanism in a transformer network. We investigated\nvariants of this model: (1) a structure aware self-attention model (StA-PLAN)\nthat incorporates tree structure information in the transformer network, and\n(2) a hierarchical token and post-level attention model (StA-HiTPLAN) that\nlearns a sentence representation with token-level self-attention. To the best\nof our knowledge, we are the first to evaluate our models on two rumor\ndetection data sets: the PHEME data set as well as the Twitter15 and Twitter16\ndata sets. We show that our best models outperform current state-of-the-art\nmodels for both data sets. Moreover, the attention mechanism allows us to\nexplain rumor detection predictions at both token-level and post-level.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 02:37:11 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Khoo", "Ling Min Serena", ""], ["Chieu", "Hai Leong", ""], ["Qian", "Zhong", ""], ["Jiang", "Jing", ""]]}, {"id": "2001.10781", "submitter": "Prajna Upadhyay", "authors": "Prajna Upadhyay, Srikanta Bedathur, Tanmoy Chakraborty, Maya Ramanath", "title": "Aspect-based Academic Search using Domain-specific KB", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Academic search engines allow scientists to explore related work relevant to\na given query. Often, the user is also aware of the \"aspect\" to retrieve a\nrelevant document. In such cases, existing search engines can be used by\nexpanding the query with terms describing that aspect. However, this approach\ndoes not guarantee good results since plain keyword matches do not always imply\nrelevance. To address this issue, we define and solve a novel academic search\ntask, called \"aspect-based retrieval\", which allows the user to specify the\naspect along with the query to retrieve a ranked list of relevant documents.\nThe primary idea is to estimate a language model for the aspect as well as the\nquery using a domain-specific knowledge base and use a mixture of the two to\ndetermine the relevance of the article. Our evaluation of the results over the\nOpen Research Corpus dataset shows that our method outperforms keyword-based\nexpansion of query with aspect with and without relevance feedback.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 12:41:32 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Upadhyay", "Prajna", ""], ["Bedathur", "Srikanta", ""], ["Chakraborty", "Tanmoy", ""], ["Ramanath", "Maya", ""]]}, {"id": "2001.11021", "submitter": "Artur Strzelecki", "authors": "Artur Strzelecki and Mariia Rizun", "title": "Infodemiological Study Using Google Trends on Coronavirus Epidemic in\n  Wuhan, China", "comments": "8 pages, 5 figures", "journal-ref": "International Journal of Online and Biomedical Engineering, Vol\n  16, No 04 (2020), pp. 139-146", "doi": "10.3991/ijoe.v16i04.13351", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent emergence of a new coronavirus (COVID-19) has gained a high cover\nin public media and worldwide news. The virus has caused a viral pneumonia in\ntens of thousands of people in Wuhan, a central city of China. This short paper\ngives a brief introduction on how the demand for information on this new\nepidemic is reported through Google Trends. The reported period is 31 December\n2020 to 20 March 2020. The authors draw conclusions on current infodemiological\ndata on COVID-19 using three main search keywords: coronavirus, SARS and MERS.\nTwo approaches are set. First is the worldwide perspective, second - the\nChinese one, which reveals that in China this disease in the first days was\nmore often referred to SARS then to general coronaviruses, whereas worldwide,\nsince the beginning, it is more often referred to coronaviruses.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 18:59:12 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 15:10:57 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2020 14:01:17 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Strzelecki", "Artur", ""], ["Rizun", "Mariia", ""]]}, {"id": "2001.11274", "submitter": "Alfonso White", "authors": "Alfonso White, Daniela M. Romano", "title": "Scalable Psychological Momentum Forecasting in Esports", "comments": "8 pages, 8 figures", "journal-ref": "Proceedings of Workshop SUM '20: State-based User Modelling, The\n  13th ACM International Conference on Web Search and Data Mining (WSDM '20),\n  2020", "doi": "10.13140/RG.2.2.21224.21769", "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world of competitive Esports and video gaming has seen and continues to\nexperience steady growth in popularity and complexity. Correspondingly, more\nresearch on the topic is being published, ranging from social network analyses\nto the benchmarking of advanced artificial intelligence systems in playing\nagainst humans. In this paper, we present ongoing work on an intelligent agent\nrecommendation engine that suggests actions to players in order to maximise\nsuccess and enjoyment, both in the space of in-game choices, as well as\ndecisions made around play session timing in the broader context. By leveraging\ntemporal data and appropriate models, we show that a learned representation of\nplayer psychological momentum, and of tilt, can be used, in combination with\nplayer expertise, to achieve state-of-the-art performance in pre- and\npost-draft win prediction. Our progress toward fulfilling the potential for\nderiving optimal recommendations is documented.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 11:57:40 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 01:16:19 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["White", "Alfonso", ""], ["Romano", "Daniela M.", ""]]}, {"id": "2001.11358", "submitter": "Zohreh Ovaisi", "authors": "Zohreh Ovaisi, Ragib Ahsan, Yifan Zhang, Kathryn Vasilaky, Elena\n  Zheleva", "title": "Correcting for Selection Bias in Learning-to-rank Systems", "comments": "This paper appeared in The Web Conference (WWW'20), April 20-24,\n  2020, Taipei, Taiwan", "journal-ref": null, "doi": "10.1145/3366423.3380255", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click data collected by modern recommendation systems are an important source\nof observational data that can be utilized to train learning-to-rank (LTR)\nsystems. However, these data suffer from a number of biases that can result in\npoor performance for LTR systems. Recent methods for bias correction in such\nsystems mostly focus on position bias, the fact that higher ranked results\n(e.g., top search engine results) are more likely to be clicked even if they\nare not the most relevant results given a user's query. Less attention has been\npaid to correcting for selection bias, which occurs because clicked documents\nare reflective of what documents have been shown to the user in the first\nplace. Here, we propose new counterfactual approaches which adapt Heckman's\ntwo-stage method and accounts for selection and position bias in LTR systems.\nOur empirical evaluation shows that our proposed methods are much more robust\nto noise and have better accuracy compared to existing unbiased LTR algorithms,\nespecially when there is moderate to no position bias.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 08:23:52 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 06:52:33 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Ovaisi", "Zohreh", ""], ["Ahsan", "Ragib", ""], ["Zhang", "Yifan", ""], ["Vasilaky", "Kathryn", ""], ["Zheleva", "Elena", ""]]}, {"id": "2001.11369", "submitter": "Renqin Cai", "authors": "Renqin Cai, Qinglei Wang, Chong Wang, Xiaobing Liu", "title": "Learning to Structure Long-term Dependence for Sequential Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential recommendation recommends items based on sequences of users'\nhistorical actions. The key challenge in it is how to effectively model the\ninfluence from distant actions to the action to be predicted, i.e., recognizing\nthe long-term dependence structure; and it remains an underexplored problem. To\nbetter model the long-term dependence structure, we propose a GatedLongRec\nsolution in this work. To account for the long-term dependence, GatedLongRec\nextracts distant actions of top-$k$ related categories to the user's ongoing\nintent with a top-$k$ gating network, and utilizes a long-term encoder to\nencode the transition patterns among these identified actions. As user intent\nis not directly observable, we take advantage of available side-information\nabout the actions, i.e., the category of their associated items, to infer the\nintents. End-to-end training is performed to estimate the intent representation\nand predict the next action for sequential recommendation. Extensive\nexperiments on two large datasets show that the proposed solution can recognize\nthe structure of long-term dependence, thus greatly improving the sequential\nrecommendation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 14:51:24 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Cai", "Renqin", ""], ["Wang", "Qinglei", ""], ["Wang", "Chong", ""], ["Liu", "Xiaobing", ""]]}, {"id": "2001.11382", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Jean Val\\`ere Cossu, Juan-Manuel Torres-Moreno, Eric SanJuan, Marc\n  El-B\\`eze", "title": "Intweetive Text Summarization", "comments": "8 pages, 4 tables", "journal-ref": "International Journal of Computational Linguistics and\n  Applications vol. 7, no. 1, 2016, pp. 67-83", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of user generated contents from various social medias allows\nanalyst to handle a wide view of conversations on several topics related to\ntheir business. Nevertheless keeping up-to-date with this amount of information\nis not humanly feasible. Automatic Summarization then provides an interesting\nmean to digest the dynamics and the mass volume of contents. In this paper, we\naddress the issue of tweets summarization which remains scarcely explored. We\npropose to automatically generated summaries of Micro-Blogs conversations\ndealing with public figures E-Reputation. These summaries are generated using\nkey-word queries or sample tweet and offer a focused view of the whole\nMicro-Blog network. Since state-of-the-art is lacking on this point we conduct\nand evaluate our experiments over the multilingual CLEF RepLab Topic-Detection\ndataset according to an experimental evaluation process.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 08:38:40 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Cossu", "Jean Val\u00e8re", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["SanJuan", "Eric", ""], ["El-B\u00e8ze", "Marc", ""]]}, {"id": "2001.11402", "submitter": "Jiancan Wu", "authors": "Jiancan Wu, Xiangnan He, Xiang Wang, Qifan Wang, Weijian Chen, Jianxun\n  Lian, Xing Xie", "title": "Graph Convolution Machine for Context-aware Recommender System", "comments": "19 pages, 4 figures, 4 tables. Accepted by Frontiers of Computer\n  Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The latest advance in recommendation shows that better user and item\nrepresentations can be learned via performing graph convolutions on the\nuser-item interaction graph. However, such finding is mostly restricted to the\ncollaborative filtering (CF) scenario, where the interaction contexts are not\navailable. In this work, we extend the advantages of graph convolutions to\ncontext-aware recommender system (CARS, which represents a generic type of\nmodels that can handle various side information). We propose \\textit{Graph\nConvolution Machine} (GCM), an end-to-end framework that consists of three\ncomponents: an encoder, graph convolution (GC) layers, and a decoder. The\nencoder projects users, items, and contexts into embedding vectors, which are\npassed to the GC layers that refine user and item embeddings with context-aware\ngraph convolutions on user-item graph. The decoder digests the refined\nembeddings to output the prediction score by considering the interactions among\nuser, item, and context embeddings. We conduct experiments on three real-world\ndatasets from Yelp and Amazon, validating the effectiveness of GCM and the\nbenefits of performing graph convolutions for CARS. Our implementations are\navailable at \\url{https://github.com/wujcan/GCM}.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 15:32:08 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 12:32:00 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 09:46:14 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Wu", "Jiancan", ""], ["He", "Xiangnan", ""], ["Wang", "Xiang", ""], ["Wang", "Qifan", ""], ["Chen", "Weijian", ""], ["Lian", "Jianxun", ""], ["Xie", "Xing", ""]]}, {"id": "2001.11631", "submitter": "Md Rashadul Hasan Rakib", "authors": "Md Rashadul Hasan Rakib, Norbert Zeh, Magdalena Jankowska, Evangelos\n  Milios", "title": "Enhancement of Short Text Clustering by Iterative Classification", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short text clustering is a challenging task due to the lack of signal\ncontained in such short texts. In this work, we propose iterative\nclassification as a method to b o ost the clustering quality (e.g., accuracy)\nof short texts. Given a clustering of short texts obtained using an arbitrary\nclustering algorithm, iterative classification applies outlier removal to\nobtain outlier-free clusters. Then it trains a classification algorithm using\nthe non-outliers based on their cluster distributions. Using the trained\nclassification model, iterative classification reclassifies the outliers to\nobtain a new set of clusters. By repeating this several times, we obtain a much\nimproved clustering of texts. Our experimental results show that the proposed\nclustering enhancement method not only improves the clustering quality of\ndifferent clustering methods (e.g., k-means, k-means--, and hierarchical\nclustering) but also outperforms the state-of-the-art short text clustering\nmethods on several short text datasets by a statistically significant margin.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 02:12:05 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Rakib", "Md Rashadul Hasan", ""], ["Zeh", "Norbert", ""], ["Jankowska", "Magdalena", ""], ["Milios", "Evangelos", ""]]}, {"id": "2001.11658", "submitter": "ByungSoo Ko", "authors": "Geonmo Gu, Byungsoo Ko", "title": "Symmetrical Synthesis for Deep Metric Learning", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning aims to learn embeddings that contain semantic\nsimilarity information among data points. To learn better embeddings, methods\nto generate synthetic hard samples have been proposed. Existing methods of\nsynthetic hard sample generation are adopting autoencoders or generative\nadversarial networks, but this leads to more hyper-parameters, harder\noptimization, and slower training speed. In this paper, we address these\nproblems by proposing a novel method of synthetic hard sample generation called\nsymmetrical synthesis. Given two original feature points from the same class,\nthe proposed method firstly generates synthetic points with each other as an\naxis of symmetry. Secondly, it performs hard negative pair mining within the\noriginal and synthetic points to select a more informative negative pair for\ncomputing the metric learning loss. Our proposed method is hyper-parameter free\nand plug-and-play for existing metric learning losses without network\nmodification. We demonstrate the superiority of our proposed method over\nexisting methods for a variety of loss functions on clustering and image\nretrieval tasks. Our implementations is publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 04:56:47 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 08:13:08 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 06:17:18 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Gu", "Geonmo", ""], ["Ko", "Byungsoo", ""]]}, {"id": "2001.11913", "submitter": "Mohammad Aliannejadi", "authors": "Luca Costa and Mohammad Aliannejadi and Fabio Crestani", "title": "A Tool for Conducting User Studies on Mobile Devices", "comments": "To appear in ACM CHIIR 2020, Vancouver, BC, Canada", "journal-ref": null, "doi": "10.1145/3343413.3377985", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever-growing interest in the area of mobile information retrieval\nand the ongoing fast development of mobile devices and, as a consequence,\nmobile apps, an active research area lies in studying users' behavior and\nsearch queries users submit on mobile devices. However, many researchers\nrequire to develop an app that collects useful information from users while\nthey search on their phones or participate in a user study. In this paper, we\naim to address this need by providing a comprehensive Android app, called\nOmicron, which can be used to collect mobile query logs and perform user\nstudies on mobile devices. Omicron, at its current version, can collect users'\nmobile queries, relevant documents, sensor data as well as user activity and\ninteraction data in various study settings. Furthermore, we designed Omicron in\nsuch a way that it is conveniently extendable to conduct more specific studies\nand collect other types of sensor data. Finally, we provide a tool to monitor\nthe participants and their data both during and after the collection process.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 15:41:27 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Costa", "Luca", ""], ["Aliannejadi", "Mohammad", ""], ["Crestani", "Fabio", ""]]}]