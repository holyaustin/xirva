[{"id": "1409.0080", "submitter": "Wei Lu", "authors": "Wei Lu, Shanshan Chen, Keqian Li, Laks V.S. Lakshmanan", "title": "Show Me the Money: Dynamic Recommendations for Revenue Maximization", "comments": "Conference version published in PVLDB 7(14). To be presented in the\n  VLDB Conference 2015, in Hawaii. This version gives a detailed submodularity\n  proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.GT cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender Systems (RS) play a vital role in applications such as e-commerce\nand on-demand content streaming. Research on RS has mainly focused on the\ncustomer perspective, i.e., accurate prediction of user preferences and\nmaximization of user utilities. As a result, most existing techniques are not\nexplicitly built for revenue maximization, the primary business goal of\nenterprises. In this work, we explore and exploit a novel connection between RS\nand the profitability of a business. As recommendations can be seen as an\ninformation channel between a business and its customers, it is interesting and\nimportant to investigate how to make strategic dynamic recommendations leading\nto maximum possible revenue. To this end, we propose a novel \\model that takes\ninto account a variety of factors including prices, valuations, saturation\neffects, and competition amongst products. Under this model, we study the\nproblem of finding revenue-maximizing recommendation strategies over a finite\ntime horizon. We show that this problem is NP-hard, but approximation\nguarantees can be obtained for a slightly relaxed version, by establishing an\nelegant connection to matroid theory. Given the prohibitively high complexity\nof the approximation algorithm, we also design intelligent heuristics for the\noriginal problem. Finally, we conduct extensive experiments on two real and\nsynthetic datasets and demonstrate the efficiency, scalability, and\neffectiveness our algorithms, and that they significantly outperform several\nintuitive baselines.\n", "versions": [{"version": "v1", "created": "Sat, 30 Aug 2014 04:15:15 GMT"}, {"version": "v2", "created": "Sat, 6 Sep 2014 01:37:15 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2015 18:21:48 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Lu", "Wei", ""], ["Chen", "Shanshan", ""], ["Li", "Keqian", ""], ["Lakshmanan", "Laks V. S.", ""]]}, {"id": "1409.0104", "submitter": "Christian Bauckhage", "authors": "Christian Bauckhage", "title": "Marginalizing over the PageRank Damping Factor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we show how to marginalize over the damping parameter of the\nPageRank equation so as to obtain a parameter-free version known as TotalRank.\nOur discussion is meant as a reference and intended to provide a guided tour\ntowards an interesting result that has applications in information retrieval\nand classification.\n", "versions": [{"version": "v1", "created": "Sat, 30 Aug 2014 11:39:32 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Bauckhage", "Christian", ""]]}, {"id": "1409.0491", "submitter": "Winfried G\\\"odert", "authors": "Winfried G\\\"odert", "title": "Facets and Typed Relations as Tools for Reasoning Processes in\n  Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faceted arrangement of entities and typed relations for representing\ndifferent associations between the entities are established tools in knowledge\nrepresentation. In this paper, a proposal is being discussed combining both\ntools to draw inferences along relational paths. This approach may yield new\nbenefit for information retrieval processes, especially when modeled for\nheterogeneous environments in the Semantic Web. Faceted arrangement can be used\nas a se-lection tool for the semantic knowledge modeled within the knowledge\nrepre-sentation. Typed relations between the entities of different facets can\nbe used as restrictions for selecting them across the facets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 17:20:13 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["G\u00f6dert", "Winfried", ""]]}, {"id": "1409.0749", "submitter": "Vikas Verma", "authors": "Vikas Verma", "title": "Image Retrieval And Classification Using Local Feature Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content Based Image Retrieval(CBIR) is one of the important subfield in the\nfield of Information Retrieval. The goal of a CBIR algorithm is to retrieve\nsemantically similar images in response to a query image submitted by the end\nuser. CBIR is a hard problem because of the phenomenon known as $\\textit\n{semantic gap}$.\n  In this thesis, we aim at analyzing the performance of a CBIR system build\nusing local feature vectors and Intermediate Matching Kernel. We also propose a\nTwo-Step Matching process for reducing the response time of the CBIR systems.\nFurther, we develop a Meta-Learning framework for improving the retrieval\nperformance of these systems. Our results show that the Two-Step Matching\nprocess significantly reduces response time and the Meta-Learning Framework\nimproves the retrieval performance by more than two fold. We also analyze the\nperformance of various image classification systems that use different image\nrepresentations constructed from the local feature vectors.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 15:17:33 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Verma", "Vikas", ""]]}, {"id": "1409.0814", "submitter": "Swakkhar Shatabda", "authors": "Rezaul Karim, Mohd. Momin Al Aziz, Swakkhar Shatabda, M. Sohel Rahman,\n  Md. Abul Kashem Mia, Farhana Zaman and Salman Rakin", "title": "CoMOGrad and PHOG: From Computer Vision to Fast and Accurate Protein\n  Tertiary Structure Retrieval", "comments": "draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the advancements in technology number of entries in the structural\ndatabase of proteins are increasing day by day. Methods for retrieving protein\ntertiary structures from this large database is the key to comparative analysis\nof structures which plays an important role to understand proteins and their\nfunction. In this paper, we present fast and accurate methods for the retrieval\nof proteins from a large database with tertiary structures similar to a query\nprotein. Our proposed methods borrow ideas from the field of computer vision.\nThe speed and accuracy of our methods comes from the two newly introduced\nfeatures, the co-occurrence matrix of the oriented gradient and pyramid\nhistogram of oriented gradient and from the use of Euclidean distance as the\ndistance measure. Experimental results clearly indicate the superiority of our\napproach in both running time and accuracy. Our method is readily available for\nuse from this website: http://research.buet.ac.bd:8080/Comograd/.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 18:26:50 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Karim", "Rezaul", ""], ["Aziz", "Mohd. Momin Al", ""], ["Shatabda", "Swakkhar", ""], ["Rahman", "M. Sohel", ""], ["Mia", "Md. Abul Kashem", ""], ["Zaman", "Farhana", ""], ["Rakin", "Salman", ""]]}, {"id": "1409.0921", "submitter": "Amir Zidi", "authors": "Amir Zidi and Mourad Abed", "title": "A Generalized Framework for Ontology-Based Information Retrieval\n  Application to a public-transportation system", "comments": null, "journal-ref": null, "doi": "10.1109/ICAdLT.2013.6568453", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a generic framework for ontology-based information\nretrieval. We focus on the recognition of semantic information extracted from\ndata sources and the mapping of this knowledge into ontology. In order to\nachieve more scalability, we propose an approach for semantic indexing based on\nentity retrieval model. In addition, we have used ontology of public\ntransportation domain in order to validate these proposals. Finally, we\nevaluated our system using ontology mapping and real world data sources.\nExperiments show that our framework can provide meaningful search results.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 23:41:58 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Zidi", "Amir", ""], ["Abed", "Mourad", ""]]}, {"id": "1409.1284", "submitter": "Sawood Alam", "authors": "Sawood Alam and Fateh ud din B Mehmood and Michael L. Nelson", "title": "Improving Accessibility of Archived Raster Dictionaries of Complex\n  Script Languages", "comments": "11 pages, 5 images, 2 codes, 1 table", "journal-ref": null, "doi": "10.1145/2756406.2756926", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We propose an approach to index raster images of dictionary pages which in\nturn would require very little manual effort to enable direct access to the\nappropriate pages of the dictionary for lookup. Accessibility is further\nimproved by feedback and crowdsourcing that enables highlighting of the\nspecific location on the page where the lookup word is found, annotation,\ndigitization, and fielded searching. This approach is equally applicable on\nsimple scripts as well as complex writing systems. Using our proposed approach,\nwe have built a Web application called \"Dictionary Explorer\" which supports\nword indexes in various languages and every language can have multiple\ndictionaries associated with it. Word lookup gives direct access to appropriate\npages of all the dictionaries of that language simultaneously. The application\nhas exploration features like searching, pagination, and navigating the word\nindex through a tree-like interface. The application also supports feedback,\nannotation, and digitization features. Apart from the scanned images,\n\"Dictionary Explorer\" aggregates results from various sources and user\ncontributions in Unicode. We have evaluated the time required for indexing\ndictionaries of different sizes and complexities in the Urdu language and\nexamined various trade-offs in our implementation. Using our approach, a single\nperson can make a dictionary of 1,000 pages searchable in less than an hour.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 23:27:18 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Alam", "Sawood", ""], ["Mehmood", "Fateh ud din B", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1409.1357", "submitter": "Kris Jack", "authors": "Roman Kern, Kris Jack, Michael Granitzer", "title": "Recommending Scientific Literature: Comparing Use-Cases and Algorithms", "comments": "12 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important aspect of a researcher's activities is to find relevant and\nrelated publications. The task of a recommender system for scientific\npublications is to provide a list of papers that match these criteria. Based on\nthe collection of publications managed by Mendeley, four data sets have been\nassembled that reflect different aspects of relatedness. Each of these\nrelatedness scenarios reflect a user's search strategy. These scenarios are\npublic groups, venues, author publications and user libraries. The first three\nof these data sets are being made publicly available for other researchers to\ncompare algorithms against. Three recommender systems have been implemented: a\ncollaborative filtering system; a content-based filtering system; and a hybrid\nof these two systems. Results from testing demonstrate that collaborative\nfiltering slightly outperforms the content-based approach, but fails in some\nscenarios. The hybrid system, that combines the two recommendation methods,\nprovides the best performance, achieving a precision of up to 70%. This\nsuggests that both techniques contribute complementary information in the\ncontext of recommending scientific literature and different approaches suite\nfor different information needs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 08:30:36 GMT"}], "update_date": "2014-09-05", "authors_parsed": [["Kern", "Roman", ""], ["Jack", "Kris", ""], ["Granitzer", "Michael", ""]]}, {"id": "1409.1461", "submitter": "David Flatow", "authors": "David Flatow, Mor Naaman, Ke Eddie Xie, Yana Volkovich, Yaron Kanza", "title": "On the Accuracy of Hyper-local Geotagging of Social Media Content", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media users share billions of items per year, only a small fraction of\nwhich is geotagged. We present a data- driven approach for identifying\nnon-geotagged content items that can be associated with a hyper-local\ngeographic area by modeling the location distributions of hyper-local n-grams\nthat appear in the text. We explore the trade-off between accuracy, precision\nand coverage of this method. Further, we explore differences across content\nreceived from multiple platforms and devices, and show, for example, that\ncontent shared via different sources and applications produces significantly\ndifferent geographic distributions, and that it is best to model and predict\nlocation for items according to their source. Our findings show the potential\nand the bounds of a data-driven approach to geotag short social media texts,\nand offer implications for all applications that use data-driven approaches to\nlocate content.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 15:10:32 GMT"}, {"version": "v2", "created": "Sun, 1 Feb 2015 05:52:55 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Flatow", "David", ""], ["Naaman", "Mor", ""], ["Xie", "Ke Eddie", ""], ["Volkovich", "Yana", ""], ["Kanza", "Yaron", ""]]}, {"id": "1409.1612", "submitter": "Andrey Kutuzov", "authors": "Andrey Kutuzov", "title": "Semantic clustering of Russian web search results: possibilities and\n  problems", "comments": "Presented at Russian Summer School in Information Retrieval (RuSSIR\n  2014). To be published in Springer Communications in Computer and Information\n  Science series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The paper deals with word sense induction from lexical co-occurrence graphs.\nWe construct such graphs on large Russian corpora and then apply this data to\ncluster Mail.ru Search results according to meanings of the query. We compare\ndifferent methods of performing such clustering and different source corpora.\nModels of applying distributional semantics to big linguistic data are\ndescribed.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 21:09:26 GMT"}, {"version": "v2", "created": "Sun, 26 Oct 2014 13:27:11 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Kutuzov", "Andrey", ""]]}, {"id": "1409.2042", "submitter": "Arda Antikacioglu", "authors": "Arda Antikacioglu, R. Ravi, Srinath Srihdar", "title": "Recommendation Subgraphs for Web Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendations are central to the utility of many websites including\nYouTube, Quora as well as popular e-commerce stores. Such sites typically\ncontain a set of recommendations on every product page that enables visitors to\neasily navigate the website. Choosing an appropriate set of recommendations at\neach page is one of the key features of backend engines that have been deployed\nat several e-commerce sites.\n  Specifically at BloomReach, an engine consisting of several independent\ncomponents analyzes and optimizes its clients' websites. This paper focuses on\nthe structure optimizer component which improves the website navigation\nexperience that enables the discovery of novel content.\n  We begin by formalizing the concept of recommendations used for discovery. We\nformulate this as a natural graph optimization problem which in its simplest\ncase, reduces to a bipartite matching problem. In practice, solving these\nmatching problems requires superlinear time and is not scalable. Also,\nimplementing simple algorithms is critical in practice because they are\nsignificantly easier to maintain in production. This motivated us to analyze\nthree methods for solving the problem in increasing order of sophistication: a\nsampling algorithm, a greedy algorithm and a more involved partitioning based\nalgorithm.\n  We first theoretically analyze the performance of these three methods on\nrandom graph models characterizing when each method will yield a solution of\nsufficient quality and the parameter ranges when more sophistication is needed.\nWe complement this by providing an empirical analysis of these algorithms on\nsimulated and real-world production data. Our results confirm that it is not\nalways necessary to implement complicated algorithms in the real-world and that\nvery good practical results can be obtained by using heuristics that are backed\nby the confidence of concrete theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Sat, 6 Sep 2014 18:21:21 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Antikacioglu", "Arda", ""], ["Ravi", "R.", ""], ["Srihdar", "Srinath", ""]]}, {"id": "1409.2530", "submitter": "Camilo Ortiz", "authors": "Khalifeh AlJadda, Mohammed Korayem, Camilo Ortiz, Chris Russell, David\n  Bernal, Lamar Payson, Scott Brown, and Trey Grainger", "title": "Augmenting recommendation systems using a model of semantically-related\n  terms extracted from user behavior", "comments": "RecSys2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common difficulties like the cold-start problem and a lack of sufficient\ninformation about users due to their limited interactions have been major\nchallenges for most recommender systems (RS). To overcome these challenges and\nmany similar ones that result in low accuracy (precision and recall)\nrecommendations, we propose a novel system that extracts semantically-related\nsearch keywords based on the aggregate behavioral data of many users. These\nsemantically-related search keywords can be used to substantially increase the\namount of knowledge about a specific user's interests based upon even a few\nsearches and thus improve the accuracy of the RS. The proposed system is\ncapable of mining aggregate user search logs to discover semantic relationships\nbetween key phrases in a manner that is language agnostic, human\nunderstandable, and virtually noise-free. These semantically related keywords\nare obtained by looking at the links between queries of similar users which, we\nbelieve, represent a largely untapped source for discovering latent semantic\nrelationships between search terms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 21:10:19 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["AlJadda", "Khalifeh", ""], ["Korayem", "Mohammed", ""], ["Ortiz", "Camilo", ""], ["Russell", "Chris", ""], ["Bernal", "David", ""], ["Payson", "Lamar", ""], ["Brown", "Scott", ""], ["Grainger", "Trey", ""]]}, {"id": "1409.2590", "submitter": "EPTCS", "authors": "Juli\\'an Alarte (Universitat Polit\\`ecnica de Val\\`encia, Valencia,\n  Spain), David Insa (Universitat Polit\\`ecnica de Val\\`encia, Valencia,\n  Spain), Josep Silva (Universitat Polit\\`ecnica de Val\\`encia, Valencia,\n  Spain), Salvador Tamarit (Universidad Polit\\'ecnica de Madrid, Madrid, Spain)", "title": "Automatic Detection of Webpages that Share the Same Web Template", "comments": "In Proceedings WWV 2014, arXiv:1409.2294", "journal-ref": "EPTCS 163, 2014, pp. 2-15", "doi": "10.4204/EPTCS.163.2", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template extraction is the process of isolating the template of a given\nwebpage. It is widely used in several disciplines, including webpages\ndevelopment, content extraction, block detection, and webpages indexing. One of\nthe main goals of template extraction is identifying a set of webpages with the\nsame template without having to load and analyze too many webpages prior to\nidentifying the template. This work introduces a new technique to automatically\ndiscover a reduced set of webpages in a website that implement the template.\nThis set is computed with an hyperlink analysis that computes a very small set\nwith a high level of confidence.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 04:12:36 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Alarte", "Juli\u00e1n", "", "Universitat Polit\u00e8cnica de Val\u00e8ncia, Valencia,\n  Spain"], ["Insa", "David", "", "Universitat Polit\u00e8cnica de Val\u00e8ncia, Valencia,\n  Spain"], ["Silva", "Josep", "", "Universitat Polit\u00e8cnica de Val\u00e8ncia, Valencia,\n  Spain"], ["Tamarit", "Salvador", "", "Universidad Polit\u00e9cnica de Madrid, Madrid, Spain"]]}, {"id": "1409.2668", "submitter": "Martha Larson", "authors": "Mark Melenhorst (1), Mar\\'ia Men\\'endez Blanco (2), Martha Larson (1)\n  ((1) Delft University of Technology, (2) University of Trento)", "title": "A Crowdsourcing Procedure for the Discovery of Non-Obvious Attributes of\n  Social Image", "comments": "6 pages, 3 figures, Extended version of paper to appear in CrowdMM\n  2014: International ACM Workshop on Crowdsourcing for Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on mid-level image representations has conventionally concentrated\nrelatively obvious attributes and overlooked non-obvious attributes, i.e.,\ncharacteristics that are not readily observable when images are viewed\nindependently of their context or function. Non-obvious attributes are not\nnecessarily easily nameable, but nonetheless they play a systematic role in\npeople`s interpretation of images. Clusters of related non-obvious attributes,\ncalled interpretation dimensions, emerge when people are asked to compare\nimages, and provide important insight on aspects of social images that are\nconsidered relevant. In contrast to aesthetic or affective approaches to image\nanalysis, non-obvious attributes are not related to the personal perspective of\nthe viewer. Instead, they encode a conventional understanding of the world,\nwhich is tacit, rather than explicitly expressed. This paper introduces a\nprocedure for discovering non-obvious attributes using crowdsourcing. We\ndiscuss this procedure using a concrete example of a crowdsourcing task on\nAmazon Mechanical Turk carried out in the domain of fashion. An analysis\ncomparing discovered non-obvious attributes with user tags demonstrated the\nadded value delivered by our procedure.\n", "versions": [{"version": "v1", "created": "Sat, 6 Sep 2014 21:26:28 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Melenhorst", "Mark", "", "Delft University of Technology"], ["Blanco", "Mar\u00eda Men\u00e9ndez", "", "University of Trento"], ["Larson", "Martha", "", "Delft University of Technology"]]}, {"id": "1409.2762", "submitter": "Thalia Karydi", "authors": "Efthalia Karydi and Konstantinos G. Margaritis", "title": "Parallel and Distributed Collaborative Filtering: A Survey", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Collaborative filtering is amongst the most preferred techniques when\nimplementing recommender systems. Recently, great interest has turned towards\nparallel and distributed implementations of collaborative filtering algorithms.\nThis work is a survey of the parallel and distributed collaborative filtering\nimplementations, aiming not only to provide a comprehensive presentation of the\nfield's development, but also to offer future research orientation by\nhighlighting the issues that need to be further developed.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 14:54:49 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Karydi", "Efthalia", ""], ["Margaritis", "Konstantinos G.", ""]]}, {"id": "1409.2944", "submitter": "Hao Wang", "authors": "Hao Wang and Naiyan Wang and Dit-Yan Yeung", "title": "Collaborative Deep Learning for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) is a successful approach commonly used by many\nrecommender systems. Conventional CF-based methods use the ratings given to\nitems by users as the sole source of information for learning to make\nrecommendation. However, the ratings are often very sparse in many\napplications, causing CF-based methods to degrade significantly in their\nrecommendation performance. To address this sparsity problem, auxiliary\ninformation such as item content information may be utilized. Collaborative\ntopic regression (CTR) is an appealing recent method taking this approach which\ntightly couples the two components that learn from two different sources of\ninformation. Nevertheless, the latent representation learned by CTR may not be\nvery effective when the auxiliary information is very sparse. To address this\nproblem, we generalize recent advances in deep learning from i.i.d. input to\nnon-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian\nmodel called collaborative deep learning (CDL), which jointly performs deep\nrepresentation learning for the content information and collaborative filtering\nfor the ratings (feedback) matrix. Extensive experiments on three real-world\ndatasets from different domains show that CDL can significantly advance the\nstate of the art.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 03:05:22 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 09:23:37 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Wang", "Hao", ""], ["Wang", "Naiyan", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1409.2993", "submitter": "Jian Tang", "authors": "Jian Tang, Ming Zhang, Qiaozhu Mei", "title": "\"Look Ma, No Hands!\" A Parameter-Free Topic Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has always been a burden to the users of statistical topic models to\npredetermine the right number of topics, which is a key parameter of most topic\nmodels. Conventionally, automatic selection of this parameter is done through\neither statistical model selection (e.g., cross-validation, AIC, or BIC) or\nBayesian nonparametric models (e.g., hierarchical Dirichlet process). These\nmethods either rely on repeated runs of the inference algorithm to search\nthrough a large range of parameter values which does not suit the mining of big\ndata, or replace this parameter with alternative parameters that are less\nintuitive and still hard to be determined. In this paper, we explore to\n\"eliminate\" this parameter from a new perspective. We first present a\nnonparametric treatment of the PLSA model named nonparametric probabilistic\nlatent semantic analysis (nPLSA). The inference procedure of nPLSA allows for\nthe exploration and comparison of different numbers of topics within a single\nexecution, yet remains as simple as that of PLSA. This is achieved by\nsubstituting the parameter of the number of topics with an alternative\nparameter that is the minimal goodness of fit of a document. We show that the\nnew parameter can be further eliminated by two parameter-free treatments:\neither by monitoring the diversity among the discovered topics or by a weak\nsupervision from users in the form of an exemplar topic. The parameter-free\ntopic model finds the appropriate number of topics when the diversity among the\ndiscovered topics is maximized, or when the granularity of the discovered\ntopics matches the exemplar topic. Experiments on both synthetic and real data\nprove that the parameter-free topic model extracts topics with a comparable\nquality comparing to classical topic models with \"manual transmission\". The\nquality of the topics outperforms those extracted through classical Bayesian\nnonparametric models.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 08:41:35 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Tang", "Jian", ""], ["Zhang", "Ming", ""], ["Mei", "Qiaozhu", ""]]}, {"id": "1409.3021", "submitter": "Ibrahim El Bitar", "authors": "Ibrahim El Bitar, Fatima-Zahra Belouadha and Ounsa Roudies", "title": "Semantic web service discovery approaches: overview and limitations", "comments": "16 pages, 1 figure, 2 tables,\n  http://airccse.org/journal/ijcses/papers/5414ijcses02.pdf volume 5, Number 4,\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantic Web service discovery has been given massive attention within\nthe last few years. With the increasing number of Web services available on the\nweb, looking for a particular service has become very difficult, especially\nwith the evolution of the clients needs. In this context, various approaches to\ndiscover semantic Web services have been proposed. In this paper, we compare\nthese approaches in order to assess their maturity and their adaptation to the\ncurrent domain requirements. The outcome of this comparison will help us to\nidentify the mechanisms that constitute the strengths of the existing\napproaches, and thereafter will serve as guideline to determine the basis for a\ndiscovery approach more adapted to the current context of Web services.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 11:01:04 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Bitar", "Ibrahim El", ""], ["Belouadha", "Fatima-Zahra", ""], ["Roudies", "Ounsa", ""]]}, {"id": "1409.3518", "submitter": "Do-kyum Kim", "authors": "Do-kyum Kim, Geoffrey M. Voelker, Lawrence K. Saul", "title": "Topic Modeling of Hierarchical Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of topic modeling in corpora whose documents are\norganized in a multi-level hierarchy. We explore a parametric approach to this\nproblem, assuming that the number of topics is known or can be estimated by\ncross-validation. The models we consider can be viewed as special\n(finite-dimensional) instances of hierarchical Dirichlet processes (HDPs). For\nthese models we show that there exists a simple variational approximation for\nprobabilistic inference. The approximation relies on a previously unexploited\ninequality that handles the conditional dependence between Dirichlet latent\nvariables in adjacent levels of the model's hierarchy. We compare our approach\nto existing implementations of nonparametric HDPs. On several benchmarks we\nfind that our approach is faster than Gibbs sampling and able to learn more\npredictive models than existing variational methods. Finally, we demonstrate\nthe large-scale viability of our approach on two newly available corpora from\nresearchers in computer security---one with 350,000 documents and over 6,000\ninternal subcategories, the other with a five-level deep hierarchy.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 18:00:59 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 06:29:46 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Kim", "Do-kyum", ""], ["Voelker", "Geoffrey M.", ""], ["Saul", "Lawrence K.", ""]]}, {"id": "1409.3771", "submitter": "Gerasimos Razis", "authors": "Gerasimos Razis and Ioannis Anagnostopoulos", "title": "Semantifying Twitter: the influenceTracker ontology", "comments": "arXiv admin note: text overlap with arXiv:1404.5239", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an ontology schema towards semantification\nprovision of Twitter social analytics. The ontology is deployed over a publicly\navailable service that measures how influential a Twitter account is, by\ncombining its social activity and interaction over Twittersphere. Apart from\ninfluential quantity and quality measures, the service provides a SPARQL\nendpoint where users can perform advance semantic queries through the RDFized\nTwitter entities (mentions, replies, hashtags, photos, URLs) over the semantic\ngraph.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 15:28:18 GMT"}], "update_date": "2014-09-15", "authors_parsed": [["Razis", "Gerasimos", ""], ["Anagnostopoulos", "Ioannis", ""]]}, {"id": "1409.3867", "submitter": "Vishwakarma Singh", "authors": "Vishwakarma Singh and Ambuj K. Singh", "title": "Nearest Keyword Set Search in Multi-dimensional Datasets", "comments": "Accepted as Full Research Paper to ICDE 2014, Chicago, IL, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword-based search in text-rich multi-dimensional datasets facilitates many\nnovel applications and tools. In this paper, we consider objects that are\ntagged with keywords and are embedded in a vector space. For these datasets, we\nstudy queries that ask for the tightest groups of points satisfying a given set\nof keywords. We propose a novel method called ProMiSH (Projection and Multi\nScale Hashing) that uses random projection and hash-based index structures, and\nachieves high scalability and speedup. We present an exact and an approximate\nversion of the algorithm. Our empirical studies, both on real and synthetic\ndatasets, show that ProMiSH has a speedup of more than four orders over\nstate-of-the-art tree-based techniques. Our scalability tests on datasets of\nsizes up to 10 million and dimensions up to 100 for queries having up to 9\nkeywords show that ProMiSH scales linearly with the dataset size, the dataset\ndimension, the query size, and the result size.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 21:12:16 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Singh", "Vishwakarma", ""], ["Singh", "Ambuj K.", ""]]}, {"id": "1409.3942", "submitter": "Richa Sharma", "authors": "Richa Sharma, Shweta Nigam, Rekha Jain", "title": "Polarity detection movie reviews in hindi language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays peoples are actively involved in giving comments and reviews on\nsocial networking websites and other websites like shopping websites, news\nwebsites etc. large number of people everyday share their opinion on the web,\nresults is a large number of user data is collected .users also find it trivial\ntask to read all the reviews and then reached into the decision. It would be\nbetter if these reviews are classified into some category so that the user\nfinds it easier to read. Opinion Mining or Sentiment Analysis is a natural\nlanguage processing task that mines information from various text forms such as\nreviews, news, and blogs and classify them on the basis of their polarity as\npositive, negative or neutral. But, from the last few years, user content in\nHindi language is also increasing at a rapid rate on the Web. So it is very\nimportant to perform opinion mining in Hindi language as well. In this paper a\nHindi language opinion mining system is proposed. The system classifies the\nreviews as positive, negative and neutral for Hindi language. Negation is also\nhandled in the proposed system. Experimental results using reviews of movies\nshow the effectiveness of the system\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 12:36:56 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Sharma", "Richa", ""], ["Nigam", "Shweta", ""], ["Jain", "Rekha", ""]]}, {"id": "1409.3970", "submitter": "Yin Zheng", "authors": "Yin Zheng, Yu-Jin Zhang, Hugo Larochelle", "title": "A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data", "comments": "24 pages, 10 figures. A version has been accepted by TPAMI on Aug\n  4th, 2015. Add footnote about how to train the model in practice in Section\n  5.1. arXiv admin note: substantial text overlap with arXiv:1305.5306", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2476802", "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling based on latent Dirichlet allocation (LDA) has been a\nframework of choice to deal with multimodal data, such as in image annotation\ntasks. Another popular approach to model the multimodal data is through deep\nneural networks, such as the deep Boltzmann machine (DBM). Recently, a new type\nof topic model called the Document Neural Autoregressive Distribution Estimator\n(DocNADE) was proposed and demonstrated state-of-the-art performance for text\ndocument modeling. In this work, we show how to successfully apply and extend\nthis model to multimodal data, such as simultaneous image classification and\nannotation. First, we propose SupDocNADE, a supervised extension of DocNADE,\nthat increases the discriminative power of the learned hidden topic features\nand show how to employ it to learn a joint representation from image visual\nwords, annotation words and class label information. We test our model on the\nLabelMe and UIUC-Sports data sets and show that it compares favorably to other\ntopic models. Second, we propose a deep extension of our model and provide an\nefficient way of training the deep model. Experimental results show that our\ndeep model outperforms its shallow version and reaches state-of-the-art\nperformance on the Multimedia Information Retrieval (MIR) Flickr data set.\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 17:17:05 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2015 02:44:29 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2015 16:12:31 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Zheng", "Yin", ""], ["Zhang", "Yu-Jin", ""], ["Larochelle", "Hugo", ""]]}, {"id": "1409.4627", "submitter": "Petra Budikova", "authors": "Petra Budikova, Jan Botorek, Michal Batko, Pavel Zezula", "title": "DISA at ImageCLEF 2014 Revised: Search-based Image Annotation with DeCAF\n  Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper constitutes an extension to the report on DISA-MU team\nparticipation in the ImageCLEF 2014 Scalable Concept Image Annotation Task as\npublished in [3]. Specifically, we introduce a new similarity search component\nthat was implemented into the system, report on the results achieved by\nutilizing this component, and analyze the influence of different similarity\nsearch parameters on the annotation quality.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 13:24:44 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Budikova", "Petra", ""], ["Botorek", "Jan", ""], ["Batko", "Michal", ""], ["Zezula", "Pavel", ""]]}, {"id": "1409.4814", "submitter": "Patrice Simard", "authors": "Patrice Simard, David Chickering, Aparna Lakshmiratan, Denis Charles,\n  Leon Bottou, Carlos Garcia Jurado Suarez, David Grangier, Saleema Amershi,\n  Johan Verwey, Jina Suh", "title": "ICE: Enabling Non-Experts to Build Models Interactively for Large-Scale\n  Lopsided Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quick interaction between a human teacher and a learning machine presents\nnumerous benefits and challenges when working with web-scale data. The human\nteacher guides the machine towards accomplishing the task of interest. The\nlearning machine leverages big data to find examples that maximize the training\nvalue of its interaction with the teacher. When the teacher is restricted to\nlabeling examples selected by the machine, this problem is an instance of\nactive learning. When the teacher can provide additional information to the\nmachine (e.g., suggestions on what examples or predictive features should be\nused) as the learning task progresses, then the problem becomes one of\ninteractive learning.\n  To accommodate the two-way communication channel needed for efficient\ninteractive learning, the teacher and the machine need an environment that\nsupports an interaction language. The machine can access, process, and\nsummarize more examples than the teacher can see in a lifetime. Based on the\nmachine's output, the teacher can revise the definition of the task or make it\nmore precise. Both the teacher and the machine continuously learn and benefit\nfrom the interaction.\n  We have built a platform to (1) produce valuable and deployable models and\n(2) support research on both the machine learning and user interface challenges\nof the interactive learning problem. The platform relies on a dedicated,\nlow-latency, distributed, in-memory architecture that allows us to construct\nweb-scale learning machines with quick interaction speed. The purpose of this\npaper is to describe this architecture and demonstrate how it supports our\nresearch efforts. Preliminary results are presented as illustrations of the\narchitecture but are not the primary focus of the paper.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 21:45:22 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Simard", "Patrice", ""], ["Chickering", "David", ""], ["Lakshmiratan", "Aparna", ""], ["Charles", "Denis", ""], ["Bottou", "Leon", ""], ["Suarez", "Carlos Garcia Jurado", ""], ["Grangier", "David", ""], ["Amershi", "Saleema", ""], ["Verwey", "Johan", ""], ["Suh", "Jina", ""]]}, {"id": "1409.5443", "submitter": "Vasileios Kolias", "authors": "Vasilis Kolias, Ioannis Anagnostopoulos, Eleftherios Kayafas", "title": "Exploratory Analysis of a Terabyte Scale Web Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a preliminary analysis over the largest publicly\naccessible web dataset: the Common Crawl Corpus. We measure nine web\ncharacteristics from two levels of granularity using MapReduce and we comment\non the initial observations over a fraction of it. To the best of our knowledge\ntwo of the characteristics, the language distribution and the HTML version of\npages have not been analyzed in previous work, while the specific dataset has\nbeen only analyzed on page level.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 20:00:52 GMT"}, {"version": "v2", "created": "Sat, 27 Sep 2014 08:23:24 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Kolias", "Vasilis", ""], ["Anagnostopoulos", "Ioannis", ""], ["Kayafas", "Eleftherios", ""]]}, {"id": "1409.5524", "submitter": "Shuguang Han", "authors": "Shuguang Han, Daqing He and Zhen Yue", "title": "Benchmarking the Privacy-Preserving People Search", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  People search is an important topic in information retrieval. Many previous\nstudies on this topic employed social networks to boost search performance by\nincorporating either local network features (e.g. the common connections\nbetween the querying user and candidates in social networks), or global network\nfeatures (e.g. the PageRank), or both. However, the available social network\ninformation can be restricted because of the privacy settings of involved\nusers, which in turn would affect the performance of people search. Therefore,\nin this paper, we focus on the privacy issues in people search. We propose\nsimulating different privacy settings with a public social network due to the\nunavailability of privacy-concerned networks. Our study examines the influences\nof privacy concerns on the local and global network features, and their impacts\non the performance of people search. Our results show that: 1) the privacy\nconcerns of different people in the networks have different influences. People\nwith higher association (i.e. higher degree in a network) have much greater\nimpacts on the performance of people search; 2) local network features are more\nsensitive to the privacy concerns, especially when such concerns come from high\nassociation peoples in the network who are also related to the querying user.\nAs the first study on this topic, we hope to generate further discussions on\nthese issues.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 05:52:32 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Han", "Shuguang", ""], ["He", "Daqing", ""], ["Yue", "Zhen", ""]]}, {"id": "1409.5623", "submitter": "Samuel R\\\"onnqvist", "authors": "Samuel R\\\"onnqvist, Xiaolu Wang, Peter Sarlin", "title": "Interactive Visual Exploration of Topic Models using Graphs", "comments": "Online demo at http://risklab.fi/demo/topics/. appears in Proceedings\n  of the 2014 Eurographics Conference on Visualization (EuroVis)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Probabilistic topic modeling is a popular and powerful family of tools for\nuncovering thematic structure in large sets of unstructured text documents.\nWhile much attention has been directed towards the modeling algorithms and\ntheir various extensions, comparatively few studies have concerned how to\npresent or visualize topic models in meaningful ways. In this paper, we present\na novel design that uses graphs to visually communicate topic structure and\nmeaning. By connecting topic nodes via descriptive keyterms, the graph\nrepresentation reveals topic similarities, topic meaning and shared, ambiguous\nkeyterms. At the same time, the graph can be used for information retrieval\npurposes, to find documents by topic or topic subsets. To exemplify the utility\nof the design, we illustrate its use for organizing and exploring corpora of\nfinancial patents.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 12:26:39 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2014 12:31:03 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["R\u00f6nnqvist", "Samuel", ""], ["Wang", "Xiaolu", ""], ["Sarlin", "Peter", ""]]}, {"id": "1409.6182", "submitter": "Josep Silva", "authors": "Juli\\'an Alarte and Josep Silva", "title": "A Benchmark Suite for Template Detection and Content Extraction", "comments": "13 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template detection and content extraction are two of the main areas of\ninformation retrieval applied to the Web. They perform different analyses over\nthe structure and content of webpages to extract some part of the document.\nHowever, their objective is different. While template detection identifies the\ntemplate of a webpage (usually comparing with other webpages of the same\nwebsite), content extraction identifies the main content of the webpage\ndiscarding the other part. Therefore, they are somehow complementary, because\nthe main content is not part of the template. It has been measured that\ntemplates represent between 40% and 50% of data on the Web. Therefore,\nidentifying templates is essential for indexing tasks because templates usually\ncontain irrelevant information such as advertisements, menus and banners.\nProcessing and storing this information is likely to lead to a waste of\nresources (storage space, bandwidth, etc.). Similarly, identifying the main\ncontent is essential for many information retrieval tasks. In this paper, we\npresent a benchmark suite to test different approaches for template detection\nand content extraction. The suite is public, and it contains real heterogeneous\nwebpages that have been labelled so that different techniques can be suitable\n(and automatically) compared.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 14:21:33 GMT"}, {"version": "v2", "created": "Tue, 23 Sep 2014 23:05:29 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2018 14:33:01 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2020 23:19:23 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Alarte", "Juli\u00e1n", ""], ["Silva", "Josep", ""]]}, {"id": "1409.6512", "submitter": "Bilel Moulahi", "authors": "Bilel Moulahi and Lynda Tamine and Sadok Ben Yahia", "title": "Learning to Match for Multi-criteria Document Relevance", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In light of the tremendous amount of data produced by social media, a large\nbody of research have revisited the relevance estimation of the users'\ngenerated content. Most of the studies have stressed the multidimensional\nnature of relevance and proved the effectiveness of combining the different\ncriteria that it embodies. Traditional relevance estimates combination methods\nare often based on linear combination schemes. However, despite being\neffective, those aggregation mechanisms are not effective in real-life\napplications since they heavily rely on the non-realistic independence property\nof the relevance dimensions. In this paper, we propose to tackle this issue\nthrough the design of a novel fuzzy-based document ranking model. We also\npropose an automated methodology to capture the importance of relevance\ndimensions, as well as information about their interaction. This model, based\non the Choquet Integral, allows to optimize the aggregated documents relevance\nscores using any target information retrieval relevance metric. Experiments\nwithin the TREC Microblog task and a social personalized information retrieval\ntask highlighted that our model significantly outperforms a wide range of\nstate-of-the-art aggregation operators, as well as a representative learning to\nrank methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 12:38:33 GMT"}, {"version": "v2", "created": "Sun, 28 Sep 2014 12:59:14 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Moulahi", "Bilel", ""], ["Tamine", "Lynda", ""], ["Yahia", "Sadok Ben", ""]]}, {"id": "1409.6805", "submitter": "Siting Ren", "authors": "Siting Ren, Sheng Gao", "title": "Improving Cross-domain Recommendation through Probabilistic\n  Cluster-level Latent Factor Model--Extended Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain recommendation has been proposed to transfer user behavior\npattern by pooling together the rating data from multiple domains to alleviate\nthe sparsity problem appearing in single rating domains. However, previous\nmodels only assume that multiple domains share a latent common rating pattern\nbased on the user-item co-clustering. To capture diversities among different\ndomains, we propose a novel Probabilistic Cluster-level Latent Factor (PCLF)\nmodel to improve the cross-domain recommendation performance. Experiments on\nseveral real world datasets demonstrate that our proposed model outperforms the\nstate-of-the-art methods for the cross-domain recommendation task.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 02:55:31 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Ren", "Siting", ""], ["Gao", "Sheng", ""]]}, {"id": "1409.7165", "submitter": "Liang Wu", "authors": "Liang Wu, Hui Xiong, Liang Du, Bo Liu, Guandong Xu, Yong Ge, Yanjie\n  Fu, Yuanchun Zhou, Jianhui Li", "title": "Heterogeneous Metric Learning with Content-based Regularization for\n  Software Artifact Retrieval", "comments": "to appear in IEEE International Conference on Data Mining (ICDM),\n  Shen Zhen, China, December 2014", "journal-ref": null, "doi": "10.1109/ICDM.2014.147", "report-no": null, "categories": "cs.LG cs.IR cs.SE", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The problem of software artifact retrieval has the goal to effectively locate\nsoftware artifacts, such as a piece of source code, in a large code repository.\nThis problem has been traditionally addressed through the textual query. In\nother words, information retrieval techniques will be exploited based on the\ntextual similarity between queries and textual representation of software\nartifacts, which is generated by collecting words from comments, identifiers,\nand descriptions of programs. However, in addition to these semantic\ninformation, there are rich information embedded in source codes themselves.\nThese source codes, if analyzed properly, can be a rich source for enhancing\nthe efforts of software artifact retrieval. To this end, in this paper, we\ndevelop a feature extraction method on source codes. Specifically, this method\ncan capture both the inherent information in the source codes and the semantic\ninformation hidden in the comments, descriptions, and identifiers of the source\ncodes. Moreover, we design a heterogeneous metric learning approach, which\nallows to integrate code features and text features into the same latent\nsemantic space. This, in turn, can help to measure the artifact similarity by\nexploiting the joint power of both code and text features. Finally, extensive\nexperiments on real-world data show that the proposed method can help to\nimprove the performances of software artifact retrieval with a significant\nmargin.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 06:33:57 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Wu", "Liang", ""], ["Xiong", "Hui", ""], ["Du", "Liang", ""], ["Liu", "Bo", ""], ["Xu", "Guandong", ""], ["Ge", "Yong", ""], ["Fu", "Yanjie", ""], ["Zhou", "Yuanchun", ""], ["Li", "Jianhui", ""]]}, {"id": "1409.7591", "submitter": "Arun Maiya", "authors": "Arun S. Maiya and Robert M. Rolfe", "title": "Topic Similarity Networks: Visual Analytics for Large Document Sets", "comments": "9 pages; 2014 IEEE International Conference on Big Data (IEEE BigData\n  2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate ways in which to improve the interpretability of LDA topic\nmodels by better analyzing and visualizing their outputs. We focus on examining\nwhat we refer to as topic similarity networks: graphs in which nodes represent\nlatent topics in text collections and links represent similarity among topics.\nWe describe efficient and effective approaches to both building and labeling\nsuch networks. Visualizations of topic models based on these networks are shown\nto be a powerful means of exploring, characterizing, and summarizing large\ncollections of unstructured text documents. They help to \"tease out\"\nnon-obvious connections among different sets of documents and provide insights\ninto how topics form larger themes. We demonstrate the efficacy and\npracticality of these approaches through two case studies: 1) NSF grants for\nbasic research spanning a 14 year period and 2) the entire English portion of\nWikipedia.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 15:11:57 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Maiya", "Arun S.", ""], ["Rolfe", "Robert M.", ""]]}, {"id": "1409.7729", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Context-Based Information Retrieval in Risky Environment", "comments": "arXiv admin note: substantial text overlap with arXiv:1408.2195", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-Based Information Retrieval is recently modelled as an exploration/\nexploitation trade-off (exr/exp) problem, where the system has to choose\nbetween maximizing its expected rewards dealing with its current knowledge\n(exploitation) and learning more about the unknown user's preferences to\nimprove its knowledge (exploration). This problem has been addressed by the\nreinforcement learning community but they do not consider the risk level of the\ncurrent user's situation, where it may be dangerous to explore the\nnon-top-ranked documents the user may not desire in his/her current situation\nif the risk level is high. We introduce in this paper an algorithm named\nCBIR-R-greedy that considers the risk level of the user's situation to\nadaptively balance between exr and exp.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 06:40:13 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1409.7938", "submitter": "Baharan Mirzasoleiman", "authors": "Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru, Amin Karbasi, Jan\n  Vondrak, and Andreas Krause", "title": "Lazier Than Lazy Greedy", "comments": "In Proc. Conference on Artificial Intelligence (AAAI), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to maximize a monotone submodular function faster than the\nwidely used lazy greedy algorithm (also known as accelerated greedy), both in\ntheory and practice? In this paper, we develop the first linear-time algorithm\nfor maximizing a general monotone submodular function subject to a cardinality\nconstraint. We show that our randomized algorithm, STOCHASTIC-GREEDY, can\nachieve a $(1-1/e-\\varepsilon)$ approximation guarantee, in expectation, to the\noptimum solution in time linear in the size of the data and independent of the\ncardinality constraint. We empirically demonstrate the effectiveness of our\nalgorithm on submodular functions arising in data summarization, including\ntraining large-scale kernel methods, exemplar-based clustering, and sensor\nplacement. We observe that STOCHASTIC-GREEDY practically achieves the same\nutility value as lazy greedy but runs much faster. More surprisingly, we\nobserve that in many practical scenarios STOCHASTIC-GREEDY does not evaluate\nthe whole fraction of data points even once and still achieves\nindistinguishable results compared to lazy greedy.\n", "versions": [{"version": "v1", "created": "Sun, 28 Sep 2014 18:06:23 GMT"}, {"version": "v2", "created": "Wed, 26 Nov 2014 08:45:32 GMT"}, {"version": "v3", "created": "Fri, 28 Nov 2014 13:06:54 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Mirzasoleiman", "Baharan", ""], ["Badanidiyuru", "Ashwinkumar", ""], ["Karbasi", "Amin", ""], ["Vondrak", "Jan", ""], ["Krause", "Andreas", ""]]}, {"id": "1409.8518", "submitter": "David Lillis", "authors": "David Lillis, Fergus Toolan, Rem Collier, John Dunnion", "title": "ProbFuse: A Probabilistic Approach to Data Fusion", "comments": "Proceedings of the 29th Annual International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '06), 2006", "journal-ref": null, "doi": "10.1145/1148170.1148197", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data fusion is the combination of the results of independent searches on a\ndocument collection into one single output result set. It has been shown in the\npast that this can greatly improve retrieval effectiveness over that of the\nindividual results.\n  This paper presents probFuse, a probabilistic approach to data fusion.\nProbFuse assumes that the performance of the individual input systems on a\nnumber of training queries is indicative of their future performance. The fused\nresult set is based on probabilities of relevance calculated during this\ntraining process. Retrieval experiments using data from the TREC ad hoc\ncollection demonstrate that probFuse achieves results superior to that of the\npopular CombMNZ fusion algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 12:27:28 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Lillis", "David", ""], ["Toolan", "Fergus", ""], ["Collier", "Rem", ""], ["Dunnion", "John", ""]]}, {"id": "1409.8572", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Freshness-Aware Thompson Sampling", "comments": "21st International Conference on Neural Information Processing. arXiv\n  admin note: text overlap with arXiv:1409.7729", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To follow the dynamicity of the user's content, researchers have recently\nstarted to model interactions between users and the Context-Aware Recommender\nSystems (CARS) as a bandit problem where the system needs to deal with\nexploration and exploitation dilemma. In this sense, we propose to study the\nfreshness of the user's content in CARS through the bandit problem. We\nintroduce in this paper an algorithm named Freshness-Aware Thompson Sampling\n(FA-TS) that manages the recommendation of fresh document according to the\nuser's risk of the situation. The intensive evaluation and the detailed\nanalysis of the experimental results reveals several important discoveries in\nthe exploration/exploitation (exr/exp) behaviour.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 17:17:52 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}]