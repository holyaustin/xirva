[{"id": "1109.0166", "submitter": "Bahram Amini", "authors": "Bahram Amini, Roliana Ibrahim, Mohd Shahizan Othman", "title": "Discovering the Impact of Knowledge in Recommender Systems: A\n  Comparative Study", "comments": "14 pages, 3 tables; International Journal of Computer Science &\n  Engineering Survey (IJCSES) Vol.2, No.3, August 2011", "journal-ref": null, "doi": "10.5121/ijcses.2011.2301", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems engage user profiles and appropriate filtering techniques\nto assist users in finding more relevant information over the large volume of\ninformation. User profiles play an important role in the success of\nrecommendation process since they model and represent the actual user needs.\nHowever, a comprehensive literature review of recommender systems has\ndemonstrated no concrete study on the role and impact of knowledge in user\nprofiling and filtering approache. In this paper, we review the most prominent\nrecommender systems in the literature and examine the impression of knowledge\nextracted from different sources. We then come up with this finding that\nsemantic information from the user context has substantial impact on the\nperformance of knowledge based recommender systems. Finally, some new clues for\nimprovement the knowledge-based profiles have been proposed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 12:08:19 GMT"}], "update_date": "2011-09-02", "authors_parsed": [["Amini", "Bahram", ""], ["Ibrahim", "Roliana", ""], ["Othman", "Mohd Shahizan", ""]]}, {"id": "1109.0420", "submitter": "Yizhao Ni", "authors": "Yizhao Ni, Matt Mcvicar, Raul Santos-Rodriguez and Tijl De Bie", "title": "Meta-song evaluation for chord recognition", "comments": "technique report and preparation for conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to evaluate chord recognition systems on songs\nwhich do not have full annotations. The principle is to use online chord\ndatabases to generate high accurate \"pseudo annotations\" for these songs and\ncompute \"pseudo accuracies\" of test systems. Statistical models that model the\nrelationship between \"pseudo accuracy\" and real performance are then applied to\nestimate test systems' performance. The approach goes beyond the existing\nevaluation metrics, allowing us to carry out extensive analysis on chord\nrecognition systems, such as their generalizations to different genres. In the\nexperiments we applied this method to evaluate three state-of-the-art chord\nrecognition systems, of which the results verified its reliability.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2011 12:04:07 GMT"}], "update_date": "2011-09-05", "authors_parsed": [["Ni", "Yizhao", ""], ["Mcvicar", "Matt", ""], ["Santos-Rodriguez", "Raul", ""], ["De Bie", "Tijl", ""]]}, {"id": "1109.0530", "submitter": "Margareta Ackerman Margareta Ackerman", "authors": "Margareta Ackerman, David Loker, Alejandro Lopez-Ortiz", "title": "Orthogonal Query Expansion", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last fifteen years, web searching has seen tremendous improvements.\nStarting from a nearly random collection of matching pages in 1995, today,\nsearch engines tend to satisfy the user's informational need on well-formulated\nqueries. One of the main remaining challenges is to satisfy the users' needs\nwhen they provide a poorly formulated query. When the pages matching the user's\noriginal keywords are judged to be unsatisfactory, query expansion techniques\nare used to alter the result set. These techniques find keywords that are\nsimilar to the keywords given by the user, which are then appended to the\noriginal query leading to a perturbation of the result set. However, when the\noriginal query is sufficiently ill-posed, the user's informational need is best\nmet using entirely different keywords, and a small perturbation of the original\nresult set is bound to fail.\n  We propose a novel approach that is not based on the keywords of the original\nquery. We intentionally seek out orthogonal queries, which are related queries\nthat have low similarity to the user's query. The result sets of orthogonal\nqueries intersect with the result set of the original query on a small number\nof pages. An orthogonal query can access the user's informational need while\nconsisting of entirely different terms than the original query. We illustrate\nthe effectiveness of our approach by proposing a query expansion method derived\nfrom these observations that improves upon results obtained using the Yahoo\nBOSS infrastructure.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2011 19:50:20 GMT"}], "update_date": "2011-09-05", "authors_parsed": [["Ackerman", "Margareta", ""], ["Loker", "David", ""], ["Lopez-Ortiz", "Alejandro", ""]]}, {"id": "1109.0732", "submitter": "Andrew Krizhanovsky A", "authors": "Feiyu Lin and Andrew Krizhanovsky", "title": "Multilingual ontology matching based on Wiktionary data accessible via\n  SPARQL endpoint", "comments": "8 pages, 3 tables, 4 figures, In: Proceedings of the 13th Russian\n  Conference on Digital Libraries RCDL'2011. October 19-22, Voronezh, Russia. -\n  pp. 19-26. (preprint)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Interoperability is a feature required by the Semantic Web. It is provided by\nthe ontology matching methods and algorithms. But now ontologies are presented\nnot only in English, but in other languages as well. It is important to use an\nautomatic translation for obtaining correct matching pairs in multilingual\nontology matching. The translation into many languages could be based on the\nGoogle Translate API, the Wiktionary database, etc. From the point of view of\nthe balance of presence of many languages, of manually crafted translations, of\na huge size of a dictionary, the most promising resource is the Wiktionary. It\nis a collaborative project working on the same principles as the Wikipedia. The\nparser of the Wiktionary was developed and the machine-readable dictionary was\ndesigned. The data of the machine-readable Wiktionary are stored in a\nrelational database, but with the help of D2R server the database is presented\nas an RDF store. Thus, it is possible to get lexicographic information\n(definitions, translations, synonyms) from web service using SPARQL requests.\nIn the case study, the problem entity is a task of multilingual ontology\nmatching based on Wiktionary data accessible via SPARQL endpoint. Ontology\nmatching results obtained using Wiktionary were compared with results based on\nGoogle Translate API.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2011 17:08:23 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2011 08:08:08 GMT"}], "update_date": "2011-10-27", "authors_parsed": [["Lin", "Feiyu", ""], ["Krizhanovsky", "Andrew", ""]]}, {"id": "1109.0758", "submitter": "Mao Ye", "authors": "Mao Ye and Xingjie Liu and Wang-Chien Lee", "title": "Exploring Social Influence for Recommendation - A Probabilistic\n  Generative Model Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a probabilistic generative model, called unified\nmodel, which naturally unifies the ideas of social influence, collaborative\nfiltering and content-based methods for item recommendation. To address the\nissue of hidden social influence, we devise new algorithms to learn the model\nparameters of our proposal based on expectation maximization (EM). In addition\nto a single-machine version of our EM algorithm, we further devise a\nparallelized implementation on the Map-Reduce framework to process two\nlarge-scale datasets we collect. Moreover, we show that the social influence\nobtained from our generative models can be used for group recommendation.\nFinally, we conduct comprehensive experiments using the datasets crawled from\nlast.fm and whrrl.com to validate our ideas. Experimental results show that the\ngenerative models with social influence significantly outperform those without\nincorporating social influence. The unified generative model proposed in this\npaper obtains the best performance. Moreover, our study on social influence\nfinds that users in whrrl.com are more likely to get influenced by friends than\nthose in last.fm. The experimental results also confirm that our social\ninfluence based group recommendation algorithm outperforms the state-of-the-art\nalgorithms for group recommendation.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2011 21:15:12 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Ye", "Mao", ""], ["Liu", "Xingjie", ""], ["Lee", "Wang-Chien", ""]]}, {"id": "1109.0916", "submitter": "Dirk Lewandowski", "authors": "Dirk Lewandowski and Ulrike Spree", "title": "Ranking of Wikipedia articles in search engines revisited: Fair ranking\n  for reasonable quality?", "comments": null, "journal-ref": "Journal of the American Society for Information Science and\n  Technology 62(2011)1, 117-132", "doi": "10.1002/asi.21423", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to review the fiercely discussed question of whether the\nranking of Wikipedia articles in search engines is justified by the quality of\nthe articles. After an overview of current research on information quality in\nWikipedia, a summary of the extended discussion on the quality of encyclopedic\nentries in general is given. On this basis, a heuristic method for evaluating\nWikipedia entries is developed and applied to Wikipedia articles that scored\nhighly in a search engine retrieval effectiveness test and compared with the\nrelevance judgment of jurors. In all search engines tested, Wikipedia results\nare unanimously judged better by the jurors than other results on the\ncorresponding results position. Relevance judgments often roughly correspond\nwith the results from the heuristic evaluation. Cases in which high relevance\njudgments are not in accordance with the comparatively low score from the\nheuristic evaluation are interpreted as an indicator of a high degree of trust\nin Wikipedia. One of the systemic shortcomings of Wikipedia lies in its\nnecessarily incoherent user model. A further tuning of the suggested criteria\ncatalogue, for instance the different weighing of the supplied criteria, could\nserve as a starting point for a user model differentiated evaluation of\nWikipedia articles. Approved methods of quality evaluation of reference works\nare applied to Wikipedia articles and integrated with the question of search\nengine evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2011 14:38:21 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Lewandowski", "Dirk", ""], ["Spree", "Ulrike", ""]]}, {"id": "1109.1059", "submitter": "Seok-Ho Yoon", "authors": "Seok-Ho Yoon, Sang-Wook Kim, and Sunju Park", "title": "C-Rank: A Link-based Similarity Measure for Scientific Literature\n  Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the number of people who use scientific literature databases grows, the\ndemand for literature retrieval services has been steadily increased. One of\nthe most popular retrieval services is to find a set of papers similar to the\npaper under consideration, which requires a measure that computes similarities\nbetween papers. Scientific literature databases exhibit two interesting\ncharacteristics that are different from general databases. First, the papers\ncited by old papers are often not included in the database due to technical and\neconomic reasons. Second, since a paper references the papers published before\nit, few papers cite recently-published papers. These two characteristics cause\nall existing similarity measures to fail in at least one of the following\ncases: (1) measuring the similarity between old, but similar papers, (2)\nmeasuring the similarity between recent, but similar papers, and (3) measuring\nthe similarity between two similar papers: one old, the other recent. In this\npaper, we propose a new link-based similarity measure called C-Rank, which uses\nboth in-link and out-link by disregarding the direction of references. In\naddition, we discuss the most suitable normalization method for scientific\nliterature databases and propose an evaluation method for measuring the\naccuracy of similarity measures. We have used a database with real-world papers\nfrom DBLP and their reference information crawled from Libra for experiments\nand compared the performance of C-Rank with those of existing similarity\nmeasures. Experimental results show that C-Rank achieves a higher accuracy than\nexisting similarity measures.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 04:35:39 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["Yoon", "Seok-Ho", ""], ["Kim", "Sang-Wook", ""], ["Park", "Sunju", ""]]}, {"id": "1109.1088", "submitter": "Martin Aruldoss Mr", "authors": "A. Martin, D. Maladhy, V. Prasanna Venkatesan", "title": "A Framework for Business Intelligence Application using Ontological\n  Classification", "comments": "Classification, Ontology, Business Intelligence, Datamining, Inverted\n  Index, Ontology Tree Index", "journal-ref": "International Journal of Engineering Science and Technology\n  (IJEST) Vol. 3 No. 2, (2011) 1213-1221", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every business needs knowledge about their competitors to survive better. One\nof the information repositories is web. Retrieving Specific information from\nthe web is challenging. An Ontological model is developed to capture specific\ninformation by using web semantics. From the Ontology model, the relations\nbetween the data are mined using decision tree. From all these a new framework\nis developed for Business Intelligence.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 07:07:26 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["Martin", "A.", ""], ["Maladhy", "D.", ""], ["Venkatesan", "V. Prasanna", ""]]}, {"id": "1109.1168", "submitter": "Arezoo Rajaei", "authors": "Arezoo Rajaei, Ahmad Baraani Dastjerdi and Nasser Ghasem Aghaee", "title": "An Extension of Semantic Proximity for Fuzzy Multivalued Dependencies in\n  Fuzzy Relational Database", "comments": "13 pages, 2 tables, Journal", "journal-ref": "International Journal of Database Management Systems (IJDMS),\n  Vol.3, No.3, August 2011, 157-169", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the development of fuzzy logic theory by Lotfi Zadeh, its\napplications were investigated by researchers in different fields. Presenting\nand working with uncertain data is a complex problem. To solve for such a\ncomplex problem, the structure of relationships and operators dependent on such\nrelationships must be repaired. The fuzzy database has integrity limitations\nincluding data dependencies. In this paper, first fuzzy multivalued dependency\nbased semantic proximity and its problems are studied. To solve these problems,\nthe semantic proximity's formula is modified, and fuzzy multivalued dependency\nbased on the concept of extension of semantic proximity with \\alpha degree is\ndefined in fuzzy relational database which includes Crisp, NULL and fuzzy\nvalues, and also inference rules for this dependency are defined, and their\ncompleteness is proved. Finally, we will show that fuzzy functional dependency\nbased on this concept is a special case of fuzzy multivalued dependency in\nfuzzy relational database.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 13:04:41 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["Rajaei", "Arezoo", ""], ["Dastjerdi", "Ahmad Baraani", ""], ["Aghaee", "Nasser Ghasem", ""]]}, {"id": "1109.1989", "submitter": "Joshila Grace jebin", "authors": "L.K. Joshila Grace, V.Maheswari, Dhinaharan Nagamalai", "title": "Efficient Personalized Web Mining: Utilizing The Most Utilized Data", "comments": "conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Looking into the growth of information in the web it is a very tedious\nprocess of getting the exact information the user is looking for. Many search\nengines generate user profile related data listing. This paper involves one\nsuch process where the rating is given to the link that the user is clicking\non. Rather than avoiding the uninterested links both interested links and the\nuninterested links are listed. But sorted according to the weightings given to\neach link by the number of visit made by the particular user and the amount of\ntime spent on the particular link.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 13:00:18 GMT"}], "update_date": "2011-09-12", "authors_parsed": [["Grace", "L. K. Joshila", ""], ["Maheswari", "V.", ""], ["Nagamalai", "Dhinaharan", ""]]}, {"id": "1109.1991", "submitter": "Joshila Grace jebin", "authors": "L.K. Joshila Grace, V.Maheswari, Dhinaharan Nagamalai", "title": "Effective Personalized Web Mining by Utilizing The Most Utilized Data", "comments": "9 pages, journal paper", "journal-ref": "International Journal of Database Management Systems ( IJDMS ),\n  Vol.3, No.3, August 2011", "doi": "10.5121/ijdms.2011.3309", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Looking into the growth of information in the web it is a very tedious\nprocess of getting the exact information the user is looking for. Many search\nengines generate user profile related data listing. This paper involves one\nsuch process where the rating is given to the link that the user is clicking\non. Rather than avoiding the uninterested links both interested links and the\nuninterested links are listed. But sorted according to the weightings given to\neach link by the number of visit made by the particular user and the amount of\ntime spent on the particular link.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2011 13:01:49 GMT"}], "update_date": "2011-09-12", "authors_parsed": [["Grace", "L. K. Joshila", ""], ["Maheswari", "V.", ""], ["Nagamalai", "Dhinaharan", ""]]}, {"id": "1109.2271", "submitter": "Tianqi Chen", "authors": "Tianqi Chen, Zhao Zheng, Qiuxia Lu, Weinan Zhang, Yong Yu", "title": "Feature-Based Matrix Factorization", "comments": "Minor update, add some related works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender system has been more and more popular and widely used in many\napplications recently. The increasing information available, not only in\nquantities but also in types, leads to a big challenge for recommender system\nthat how to leverage these rich information to get a better performance. Most\ntraditional approaches try to design a specific model for each scenario, which\ndemands great efforts in developing and modifying models. In this technical\nreport, we describe our implementation of feature-based matrix factorization.\nThis model is an abstract of many variants of matrix factorization models, and\nnew types of information can be utilized by simply defining new features,\nwithout modifying any lines of code. Using the toolkit, we built the best\nsingle model reported on track 1 of KDDCup'11.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2011 01:10:06 GMT"}, {"version": "v2", "created": "Sat, 17 Sep 2011 07:32:03 GMT"}, {"version": "v3", "created": "Thu, 29 Dec 2011 02:05:01 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Chen", "Tianqi", ""], ["Zheng", "Zhao", ""], ["Lu", "Qiuxia", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""]]}, {"id": "1109.2321", "submitter": "Jegatha Deborah", "authors": "L.Jegatha Deborah, R.Baskaran, A.Kannan", "title": "Visualizing Domain Ontology using Enhanced Anaphora Resolution Algorithm", "comments": "13 pages in total, 11 figures, 2 tables, Older version of the Paper\n  published in the International Workshop on Database Management Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enormous explosion in the number of the World Wide Web pages occur every day\nand since the efficiency of most of the information processing systems is found\nto be less, the potential of the Internet applications is often underutilized.\nEfficient utilization of the web can be exploited when similar web pages are\nrigorously, exhaustively organized and clustered based on some domain knowledge\n(semantic-based) .Ontology which is a formal representation of domain knowledge\naids in such efficient utilization. The performance of almost all the\nsemantic-based clustering techniques depends on the constructed ontology,\ndescribing the domain knowledge . The proposed methodology provides an enhanced\npronominal anaphora resolution, one of the key aspects of semantic analysis in\nNatural Language Processing for obtaining cross references within a web page\nproviding better ontology construction. The experimental data sets exhibits\nbetter efficiency of the proposed method compared to earlier traditional\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2011 15:08:11 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Deborah", "L. Jegatha", ""], ["Baskaran", "R.", ""], ["Kannan", "A.", ""]]}, {"id": "1109.2793", "submitter": "Bowen Yan", "authors": "Bowen Yan and Steve Gregory", "title": "Finding missing edges in networks based on their community structure", "comments": "7 pages, 6 figures", "journal-ref": "Phys. Rev. E 85, 056112 (2012)", "doi": "10.1103/PhysRevE.85.056112", "report-no": null, "categories": "cs.IR cs.SI physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many edge prediction methods have been proposed, based on various local or\nglobal properties of the structure of an incomplete network. Community\nstructure is another significant feature of networks: Vertices in a community\nare more densely connected than average. It is often true that vertices in the\nsame community have \"similar\" properties, which suggests that missing edges are\nmore likely to be found within communities than elsewhere. We use this insight\nto propose a strategy for edge prediction that combines existing edge\nprediction methods with community detection. We show that this method gives\nbetter prediction accuracy than existing edge prediction methods alone.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2011 14:08:20 GMT"}, {"version": "v2", "created": "Tue, 15 May 2012 16:17:10 GMT"}], "update_date": "2012-05-16", "authors_parsed": [["Yan", "Bowen", ""], ["Gregory", "Steve", ""]]}, {"id": "1109.3138", "submitter": "Massimiliano Dal Mas", "authors": "Massimiliano Dal Mas", "title": "Folksodriven Structure Network", "comments": "4 pages, 2 figures; for details see: http://www.maxdalmas.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays folksonomy is used as a system derived from user-generated\nelectronic tags or keywords that annotate and describe online content. But it\nis not a classification system as an ontology. To consider it as a\nclassification system it would be necessary to share a representation of\ncontexts by all the users. This paper is proposing the use of folksonomies and\nnetwork theory to devise a new concept: a \"Folksodriven Structure Network\" to\nrepresent folksonomies. This paper proposed and analyzed the network structure\nof Folksodriven tags thought as folsksonomy tags suggestions for the user on a\ndataset built on chosen websites. It is observed that the Folksodriven Network\nhas relative low path lengths checking it with classic networking measures\n(clustering coefficient). Experiment result shows it can facilitate\nserendipitous discovery of content among users. Neat examples and clear\nformulas can show how a \"Folksodriven Structure Network\" can be used to tackle\nontology mapping challenges.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2011 17:06:21 GMT"}], "update_date": "2011-09-15", "authors_parsed": [["Mas", "Massimiliano Dal", ""]]}, {"id": "1109.4257", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Ruma Dutta, Debajyoti Mukhopadhyay", "title": "Offering A Product Recommendation System in E-commerce", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a number of explicit and implicit ratings in product\nrecommendation system for Business-to-customer e-commerce purposes. The system\nrecommends the products to a new user. It depends on the purchase pattern of\nprevious users whose purchase pattern is close to that of a user who asks for a\nrecommendation. The system is based on weighted cosine similarity measure to\nfind out the closest user profile among the profiles of all users in database.\nIt also implements Association rule mining rule in recommending the products.\nAlso, this product recommendation system takes into consideration the time of\ntransaction of purchasing the items, thus eliminating sequence recognition\nproblem. Experimental result shows for implicit rating, the proposed method\ngives acceptable performance in recommending the products. It also shows\nintroduction of association rule improves the performance measure of\nrecommendation system.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2011 10:13:02 GMT"}], "update_date": "2011-09-21", "authors_parsed": [["Dutta", "Ruma", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "1109.4920", "submitter": "Reza Farrahi Moghaddam", "authors": "Reza Farrahi Moghaddam and Mohamed Cheriet", "title": "Beyond pixels and regions: A non local patch means (NLPM) method for\n  content-level restoration, enhancement, and reconstruction of degraded\n  document images", "comments": "This paper has been withdrawn by the author to avoid duplication on\n  the DBLP bibliography", "journal-ref": "Pattern Recognition 44 (2011) 363-374", "doi": "10.1016/j.patcog.2010.07.027", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A patch-based non-local restoration and reconstruction method for\npreprocessing degraded document images is introduced. The method collects\nrelative data from the whole input image, while the image data are first\nrepresented by a content-level descriptor based on patches. This\npatch-equivalent representation of the input image is then corrected based on\nsimilar patches identified using a modified genetic algorithm (GA) resulting in\na low computational load. The corrected patch-equivalent is then converted to\nthe output restored image. The fact that the method uses the patches at the\ncontent level allows it to incorporate high-level restoration in an objective\nand self-sufficient way. The method has been applied to several degraded\ndocument images, including the DIBCO'09 contest dataset with promising results.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2011 19:24:58 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2011 16:46:52 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2011 22:33:13 GMT"}], "update_date": "2013-02-07", "authors_parsed": [["Moghaddam", "Reza Farrahi", ""], ["Cheriet", "Mohamed", ""]]}, {"id": "1109.5053", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Debajyoti Mukhopadhyay, Sukanta Sinha", "title": "A New Approach to Design Graph Based Search Engine for Multiple Domains\n  Using Different Ontologies", "comments": "8 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search Engine has become a major tool for searching any information from the\nWorld Wide Web (WWW). While searching the huge digital library available in the\nWWW, every effort is made to retrieve the most relevant results. But in WWW\nmajority of the Web pages are in HTML format and there are no such tags which\ntells the crawler to find any specific domain. To find more relevant result we\nuse Ontology for that particular domain. If we are working with multiple\ndomains then we use multiple ontologies. Now in order to design a domain\nspecific search engine for multiple domains, crawler must crawl through the\ndomain specific Web pages in the WWW according to the predefined ontologies.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2011 12:27:25 GMT"}], "update_date": "2011-09-26", "authors_parsed": [["Mukhopadhyay", "Debajyoti", ""], ["Sinha", "Sukanta", ""]]}, {"id": "1109.5370", "submitter": "Jia Zeng", "authors": "Jia Zeng, Wei Feng, William K. Cheung, Chun-Hung Li", "title": "Higher-Order Markov Tag-Topic Models for Tagged Documents and Images", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the topic modeling problem of tagged documents and images.\nHigher-order relations among tagged documents and images are major and\nubiquitous characteristics, and play positive roles in extracting reliable and\ninterpretable topics. In this paper, we propose the tag-topic models (TTM) to\ndepict such higher-order topic structural dependencies within the Markov random\nfield (MRF) framework. First, we use the novel factor graph representation of\nlatent Dirichlet allocation (LDA)-based topic models from the MRF perspective,\nand present an efficient loopy belief propagation (BP) algorithm for\napproximate inference and parameter estimation. Second, we propose the factor\nhypergraph representation of TTM, and focus on both pairwise and higher-order\nrelation modeling among tagged documents and images. Efficient loopy BP\nalgorithm is developed to learn TTM, which encourages the topic labeling\nsmoothness among tagged documents and images. Extensive experimental results\nconfirm the incorporation of higher-order relations to be effective in\nenhancing the overall topic modeling performance, when compared with current\nstate-of-the-art topic models, in many text and image mining tasks of broad\ninterests such as word and link prediction, document classification, and tag\nrecommendation.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2011 16:58:06 GMT"}], "update_date": "2011-09-27", "authors_parsed": [["Zeng", "Jia", ""], ["Feng", "Wei", ""], ["Cheung", "William K.", ""], ["Li", "Chun-Hung", ""]]}, {"id": "1109.5433", "submitter": "Jun Fang", "authors": "Jun Fang, Hongbin Li, Zhi Chen, and Shaoqian Li", "title": "Optimal Precoding Design and Power Allocation for Decentralized\n  Detection of Deterministic Signals", "comments": "13 pages in two-column format; 6 figures", "journal-ref": null, "doi": "10.1109/TSP.2012.2190598", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a decentralized detection problem in a power-constrained wireless\nsensor networks (WSNs), in which a number of sensor nodes collaborate to detect\nthe presence of a deterministic vector signal. The signal to be detected is\nassumed known \\emph{a priori}. Given a constraint on the total amount of\ntransmit power, we investigate the optimal linear precoding design for each\nsensor node. More specifically, in order to achieve the best detection\nperformance, shall sensor nodes transmit their raw data to the fusion center\n(FC), or transmit compressed versions of their original data? The optimal power\nallocation among sensors is studied as well. Also, assuming a fixed total\ntransmit power, we examine how the detection performance behaves with the\nnumber of sensors in the network. A new concept \"detection outage\" is proposed\nto quantify the reliability of the overall detection system. Finally,\ndecentralized detection with unknown signals is studied. Numerical results are\nconducted to corroborate our theoretical analysis and to illustrate the\nperformance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2011 03:09:28 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2012 08:07:08 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Fang", "Jun", ""], ["Li", "Hongbin", ""], ["Chen", "Zhi", ""], ["Li", "Shaoqian", ""]]}, {"id": "1109.6018", "submitter": "Lillian Lee", "authors": "Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, Ming Zhou, and Ping Li", "title": "User-level sentiment analysis incorporating social networks", "comments": "Proceedings of KDD 2011. Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that information about social relationships can be used to improve\nuser-level sentiment analysis. The main motivation behind our approach is that\nusers that are somehow \"connected\" may be more likely to hold similar opinions;\ntherefore, relationship information can complement what we can extract about a\nuser's viewpoints from their utterances. Employing Twitter as a source for our\nexperimental data, and working within a semi-supervised framework, we propose\nmodels that are induced either from the Twitter follower/followee network or\nfrom the network in Twitter formed by users referring to each other using \"@\"\nmentions. Our transductive learning results reveal that incorporating\nsocial-network information can indeed lead to statistically significant\nsentiment-classification improvements over the performance of an approach based\non Support Vector Machines having access only to textual features.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 20:00:47 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Tan", "Chenhao", ""], ["Lee", "Lillian", ""], ["Tang", "Jie", ""], ["Jiang", "Long", ""], ["Zhou", "Ming", ""], ["Li", "Ping", ""]]}, {"id": "1109.6206", "submitter": "Jyoti Verma", "authors": "Jyoti, A. K. Sharma, Amit Goel", "title": "A Framework for Prefetching Relevant Web Pages using Predictive\n  Prefetching Engine (PPE)", "comments": "9 pages", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 7,\n  Issue 6, November 2010 ISSN (Online): 1694-0814 www.IJCSI.org", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework for increasing the relevancy of the web pages\nretrieved by the search engine. The approach introduces a Predictive\nPrefetching Engine (PPE) which makes use of various data mining algorithms on\nthe log maintained by the search engine. The underlying premise of the approach\nis that in the case of cluster accesses, the next pages requested by users of\nthe Web server are typically based on the current and previous pages requested.\nBased on same, rules are drawn which then lead the path for prefetching the\ndesired pages. To carry out the desired task of prefetching the more relevant\npages, agents have been introduced.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 13:51:56 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Jyoti", "", ""], ["Sharma", "A. K.", ""], ["Goel", "Amit", ""]]}, {"id": "1109.6263", "submitter": "Greg Linden", "authors": "Greg Linden (Microsoft), Christopher Meek (Microsoft Research), Max\n  Chickering (Microsoft)", "title": "The Pollution Effect: Optimizing Keyword Auctions by Favoring Relevant\n  Advertising", "comments": "Presented at the Fifth Workshop on Ad Auctions, July 6, 2009,\n  Stanford, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most search engines sell slots to place advertisements on the search results\npage through keyword auctions. Advertisers offer bids for how much they are\nwilling to pay when someone enters a search query, sees the search results, and\nthen clicks on one of their ads. Search engines typically order the\nadvertisements for a query by a combination of the bids and expected\nclickthrough rates for each advertisement. In this paper, we extend a model of\nYahoo's and Google's advertising auctions to include an effect where repeatedly\nshowing less relevant ads has a persistent impact on all advertising on the\nsearch engine, an impact we designate as the pollution effect. In Monte-Carlo\nsimulations using distributions fitted to Yahoo data, we show that a modest\npollution effect is sufficient to dramatically change the advertising rank\norder that yields the optimal advertising revenue for a search engine. In\naddition, if a pollution effect exists, it is possible to maximize revenue\nwhile also increasing advertiser, and publisher utility. Our results suggest\nthat search engines could benefit from making relevant advertisements less\nexpensive and irrelevant advertisements more costly for advertisers than is the\ncurrent practice.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 16:33:38 GMT"}], "update_date": "2011-09-29", "authors_parsed": [["Linden", "Greg", "", "Microsoft"], ["Meek", "Christopher", "", "Microsoft Research"], ["Chickering", "Max", "", "Microsoft"]]}, {"id": "1109.6698", "submitter": "Emilio Ferrara", "authors": "Pasquale De Meo, Emilio Ferrara, Giacomo Fiumara, Alessandro Provetti", "title": "Improving Recommendation Quality by Merging Collaborative Filtering and\n  Social Relationships", "comments": "6 pages, Proceedings of the 11th International Conference on\n  Intelligent Systems Design and Applications", "journal-ref": "Proceedings of the 11th International Conference on Intelligent\n  Systems Design and Applications, pp. 587-592, 2011", "doi": "10.1109/ISDA.2011.6121719", "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix Factorization techniques have been successfully applied to raise the\nquality of suggestions generated by Collaborative Filtering Systems (CFSs).\nTraditional CFSs based on Matrix Factorization operate on the ratings provided\nby users and have been recently extended to incorporate demographic aspects\nsuch as age and gender. In this paper we propose to merge CFS based on Matrix\nFactorization and information regarding social friendships in order to provide\nusers with more accurate suggestions and rankings on items of their interest.\nThe proposed approach has been evaluated on a real-life online social network;\nthe experimental results show an improvement against existing CFSs. A detailed\ncomparison with related literature is also present.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 00:45:17 GMT"}], "update_date": "2012-02-13", "authors_parsed": [["De Meo", "Pasquale", ""], ["Ferrara", "Emilio", ""], ["Fiumara", "Giacomo", ""], ["Provetti", "Alessandro", ""]]}, {"id": "1109.6726", "submitter": "Rathipriya R", "authors": "R.Rathipriya, K.Thangavel", "title": "A Fuzzy Co-Clustering approach for Clickstream Data Pattern", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web Usage mining is a very important tool to extract the hidden business\nintelligence data from large databases. The extracted information provides the\norganizations with the ability to produce results more effectively to improve\ntheir businesses and increasing of sales. Co-clustering is a powerful\nbipartition technique which identifies group of users associated to group of\nweb pages. These associations are quantified to reveal the users' interest in\nthe different web pages' clusters. In this paper, Fuzzy Co-Clustering algorithm\nis proposed for clickstream data to identify the subset of users of similar\nnavigational behavior /interest over a subset of web pages of a website.\nTargeting the users group for various promotional activities is an important\naspect of marketing practices. Experiments are conducted on real dataset to\nprove the efficiency of proposed algorithm. The results and findings of this\nalgorithm could be used to enhance the marketing strategy for directing\nmarketing, advertisements for web based businesses and so on.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 06:45:41 GMT"}], "update_date": "2011-10-03", "authors_parsed": [["Rathipriya", "R.", ""], ["Thangavel", "K.", ""]]}, {"id": "1109.6862", "submitter": "Pravin Kamde Mr", "authors": "Sankirti S. and P. M. Kamade", "title": "Video OCR for Video Indexing", "comments": "3 Pages", "journal-ref": "IACSIT International Journal of Engineering and Technology, Vol.3,\n  No.3, June 2011", "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video OCR is a technique that can greatly help to locate the topics of\ninterest in video via the automatic extraction and reading of captions and\nannotations. Text in video can provide key indexing information. Recognizing\nsuch text for search application is critical. Major difficult problem for\ncharacter recognition for videos is degraded and deformated characters, low\nresolution characters or very complex background. To tackle the problem\npreprocessing on text image plays vital role. Most of the OCR engines are\nworking on the binary image so to find a better binarization procedure for\nimage to get a desired result is important.Accurate binarization process\nminimizes the error rate of video OCR.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 15:51:12 GMT"}], "update_date": "2013-01-03", "authors_parsed": [["S.", "Sankirti", ""], ["Kamade", "P. M.", ""]]}]