[{"id": "1906.00041", "submitter": "Shuo Zhang", "authors": "Li Deng, Shuo Zhang, and Krisztian Balog", "title": "Table2Vec: Neural Word and Entity Embeddings for Table Population and\n  Retrieval", "comments": "Proceedings of the 42nd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '19), 2019", "journal-ref": null, "doi": "10.1145/3331184.3331333", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tables contain valuable knowledge in a structured form. We employ neural\nlanguage modeling approaches to embed tabular data into vector spaces.\nSpecifically, we consider different table elements, such caption, column\nheadings, and cells, for training word and entity embeddings. These embeddings\nare then utilized in three particular table-related tasks, row population,\ncolumn population, and table retrieval, by incorporating them into existing\nretrieval models as additional semantic similarity signals. Evaluation results\nshow that table embeddings can significantly improve upon the performance of\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 19:22:29 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Deng", "Li", ""], ["Zhang", "Shuo", ""], ["Balog", "Krisztian", ""]]}, {"id": "1906.00058", "submitter": "Martin Klein", "authors": "Martin Klein and Lyudmila Balakireva and Harihar Shankar", "title": "Evaluating Memento Service Optimizations", "comments": "short paper accepted at JCDL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Services and applications based on the Memento Aggregator can suffer from\nslow response times due to the federated search across web archives performed\nby the Memento infrastructure. In an effort to decrease the response times, we\nestablished a cache system and experimented with machine learning models to\npredict archival holdings. We reported on the experimental results in previous\nwork and can now, after these optimizations have been in production for two\nyears, evaluate their efficiency, based on long-term log data. During our\ninvestigation we find that the cache is very effective with a 70-80% cache hit\nrate for human-driven services. The machine learning prediction operates at an\nacceptable average recall level of 0.727 but our results also show that a more\nfrequent retraining of the models is needed to further improve prediction\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 20:09:41 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Klein", "Martin", ""], ["Balakireva", "Lyudmila", ""], ["Shankar", "Harihar", ""]]}, {"id": "1906.00091", "submitter": "Maxim Naumov", "authors": "Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang,\n  Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean\n  Wu, Alisson G. Azzolini, Dmytro Dzhulgakov, Andrey Mallevich, Ilia\n  Cherniavskii, Yinghai Lu, Raghuraman Krishnamoorthi, Ansha Yu, Volodymyr\n  Kondratenko, Stephanie Pereira, Xianjie Chen, Wenlin Chen, Vijay Rao, Bill\n  Jia, Liang Xiong, Misha Smelyanskiy", "title": "Deep Learning Recommendation Model for Personalization and\n  Recommendation Systems", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of deep learning, neural network-based recommendation models\nhave emerged as an important tool for tackling personalization and\nrecommendation tasks. These networks differ significantly from other deep\nlearning networks due to their need to handle categorical features and are not\nwell studied or understood. In this paper, we develop a state-of-the-art deep\nlearning recommendation model (DLRM) and provide its implementation in both\nPyTorch and Caffe2 frameworks. In addition, we design a specialized\nparallelization scheme utilizing model parallelism on the embedding tables to\nmitigate memory constraints while exploiting data parallelism to scale-out\ncompute from the fully-connected layers. We compare DLRM against existing\nrecommendation models and characterize its performance on the Big Basin AI\nplatform, demonstrating its usefulness as a benchmark for future algorithmic\nexperimentation and system co-design.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 21:51:16 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Naumov", "Maxim", ""], ["Mudigere", "Dheevatsa", ""], ["Shi", "Hao-Jun Michael", ""], ["Huang", "Jianyu", ""], ["Sundaraman", "Narayanan", ""], ["Park", "Jongsoo", ""], ["Wang", "Xiaodong", ""], ["Gupta", "Udit", ""], ["Wu", "Carole-Jean", ""], ["Azzolini", "Alisson G.", ""], ["Dzhulgakov", "Dmytro", ""], ["Mallevich", "Andrey", ""], ["Cherniavskii", "Ilia", ""], ["Lu", "Yinghai", ""], ["Krishnamoorthi", "Raghuraman", ""], ["Yu", "Ansha", ""], ["Kondratenko", "Volodymyr", ""], ["Pereira", "Stephanie", ""], ["Chen", "Xianjie", ""], ["Chen", "Wenlin", ""], ["Rao", "Vijay", ""], ["Jia", "Bill", ""], ["Xiong", "Liang", ""], ["Smelyanskiy", "Misha", ""]]}, {"id": "1906.00095", "submitter": "Bonggun Shin", "authors": "Bonggun Shin, Hao Yang, Jinho D. Choi", "title": "The Pupil Has Become the Master: Teacher-Student Model-Based Word\n  Embedding Distillation with Ensemble Learning", "comments": "7 pages, Proceedings of the 28th International Joint Conference on\n  Artificial Intelligence, 2019 (IJCAI'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have facilitated the demand of neural models\nfor real applications. In practice, these applications often need to be\ndeployed with limited resources while keeping high accuracy. This paper touches\nthe core of neural models in NLP, word embeddings, and presents a new embedding\ndistillation framework that remarkably reduces the dimension of word embeddings\nwithout compromising accuracy. A novel distillation ensemble approach is also\nproposed that trains a high-efficient student model using multiple teacher\nmodels. In our approach, the teacher models play roles only during training\nsuch that the student model operates on its own without getting supports from\nthe teacher models during decoding, which makes it eighty times faster and\nlighter than other typical ensemble methods. All models are evaluated on seven\ndocument classification datasets and show a significant advantage over the\nteacher models for most cases. Our analysis depicts insightful transformation\nof word embeddings from distillation and suggests a future direction to\nensemble approaches using neural models.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 21:58:51 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Shin", "Bonggun", ""], ["Yang", "Hao", ""], ["Choi", "Jinho D.", ""]]}, {"id": "1906.00112", "submitter": "Armin Seyeditabari", "authors": "Armin Seyeditabari, Narges Tabari, Shafie Gholizade, Wlodek Zadrozny", "title": "Emotional Embeddings: Refining Word Embeddings to Capture Emotional\n  Content of Words", "comments": "5 pages, 1 figure, 2 tables, $ equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are one of the most useful tools in any modern natural\nlanguage processing expert's toolkit. They contain various types of information\nabout each word which makes them the best way to represent the terms in any NLP\ntask. But there are some types of information that cannot be learned by these\nmodels. Emotional information of words are one of those. In this paper, we\npresent an approach to incorporate emotional information of words into these\nmodels. We accomplish this by adding a secondary training stage which uses an\nemotional lexicon and a psychological model of basic emotions. We show that\nfitting an emotional model into pre-trained word vectors can increase the\nperformance of these models in emotional similarity metrics. Retrained models\nperform better than their original counterparts from 13% improvement for\nWord2Vec model, to 29% for GloVe vectors. This is the first such model\npresented in the literature, and although preliminary, these emotion sensitive\nmodels can open the way to increase performance in variety of emotion detection\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 22:46:03 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 15:43:27 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Seyeditabari", "Armin", ""], ["Tabari", "Narges", ""], ["Gholizade", "Shafie", ""], ["Zadrozny", "Wlodek", ""]]}, {"id": "1906.00156", "submitter": "Binbin Jin", "authors": "Binbin Jin, Enhong Chen, Hongke Zhao, Zhenya Huang, Qi Liu, Hengshu\n  Zhu, Shui Yu", "title": "Promotion of Answer Value Measurement with Domain Effects in Community\n  Question Answering Systems", "comments": "IEEE Transactions on Systems, Man, and Cybernetics: Systems", "journal-ref": null, "doi": "10.1109/TSMC.2019.2917673", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of community question answering (CQA), answer selection and\nanswer ranking are two tasks which are applied to help users quickly access\nvaluable answers. Existing solutions mainly exploit the syntactic or semantic\ncorrelation between a question and its related answers (Q&A), where the\nmulti-facet domain effects in CQA are still underexplored. In this paper, we\npropose a unified model, Enhanced Attentive Recurrent Neural Network (EARNN),\nfor both answer selection and answer ranking tasks by taking full advantages of\nboth Q&A semantics and multi-facet domain effects (i.e., topic effects and\ntimeliness). Specifically, we develop a serialized LSTM to learn the unified\nrepresentations of Q&A, where two attention mechanisms at either sentence-level\nor word-level are designed for capturing the deep effects of topics. Meanwhile,\nthe emphasis of Q&A can be automatically distinguished. Furthermore, we design\na time-sensitive ranking function to model the timeliness in CQA. To\neffectively train EARNN, a question-dependent pairwise learning strategy is\nalso developed. Finally, we conduct extensive experiments on a real-world\ndataset from Quora. Experimental results validate the effectiveness and\ninterpretability of our proposed EARNN model.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 05:55:52 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2019 06:46:16 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Jin", "Binbin", ""], ["Chen", "Enhong", ""], ["Zhao", "Hongke", ""], ["Huang", "Zhenya", ""], ["Liu", "Qi", ""], ["Zhu", "Hengshu", ""], ["Yu", "Shui", ""]]}, {"id": "1906.00246", "submitter": "Lei Chen", "authors": "Le Wu, Lei Chen, Yonghui Yang, Richang Hong, Yong Ge, Xing Xie, Meng\n  Wang", "title": "Personalized Multimedia Item and Key Frame Recommendation", "comments": "The updated version is publised in IJCAI 2019,\n  https://doi.org/10.24963/ijcai.2019/198", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When recommending or advertising items to users, an emerging trend is to\npresent each multimedia item with a key frame image (e.g., the poster of a\nmovie). As each multimedia item can be represented as multiple fine-grained\nvisual images (e.g., related images of the movie), personalized key frame\nrecommendation is necessary in these applications to attract users' unique\nvisual preferences. However, previous personalized key frame recommendation\nmodels relied on users' fine-grained image behavior of multimedia items (e.g.,\nuser-image interaction behavior), which is often not available in real\nscenarios. In this paper, we study the general problem of joint multimedia item\nand key frame recommendation in the absence of the fine-grained user-image\nbehavior. We argue that the key challenge of this problem lies in discovering\nusers' visual profiles for key frame recommendation, as most recommendation\nmodels would fail without any users' fine-grained image behavior. To tackle\nthis challenge, we leverage users' item behavior by projecting users (items) in\ntwo latent spaces: a collaborative latent space and a visual latent space. We\nfurther design a model to discern both the collaborative and visual dimensions\nof users, and model how users make decisive item preferences from these two\nspaces. As a result, the learned user visual profiles could be directly applied\nfor key frame recommendation. Finally, experimental results on a real-world\ndataset clearly show the effectiveness of our proposed model on the two\nrecommendation tasks.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 15:34:59 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 06:38:25 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Wu", "Le", ""], ["Chen", "Lei", ""], ["Yang", "Yonghui", ""], ["Hong", "Richang", ""], ["Ge", "Yong", ""], ["Xie", "Xing", ""], ["Wang", "Meng", ""]]}, {"id": "1906.00309", "submitter": "Jisheng Dai", "authors": "Jisheng Dai, An Liu, and Hing Cheung So", "title": "Sparse Bayesian Learning Approach for Discrete Signal Reconstruction", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study addresses the problem of discrete signal reconstruction from the\nperspective of sparse Bayesian learning (SBL). Generally, it is intractable to\nperform the Bayesian inference with the ideal discretization prior under the\nSBL framework. To overcome this challenge, we introduce a novel discretization\nenforcing prior to exploit the knowledge of the discrete nature of the\nsignal-of-interest. By integrating the discretization enforcing prior into the\nSBL framework and applying the variational Bayesian inference (VBI)\nmethodology, we devise an alternating update algorithm to jointly characterize\nthe finite alphabet feature and reconstruct the unknown signal. When the\nmeasurement matrix is i.i.d. Gaussian per component, we further embed the\ngeneralized approximate message passing (GAMP) into the VBI-based method, so as\nto directly adopt the ideal prior and significantly reduce the computational\nburden. Simulation results demonstrate substantial performance improvement of\nthe two proposed methods over existing schemes. Moreover, the GAMP-based\nvariant outperforms the VBI-based method with an i.i.d. Gaussian measurement\nmatrix but it fails to work for non i.i.d. Gaussian matrices.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 23:20:45 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Dai", "Jisheng", ""], ["Liu", "An", ""], ["So", "Hing Cheung", ""]]}, {"id": "1906.00318", "submitter": "Tal Baumel", "authors": "Matan Eyal, Tal Baumel, Michael Elhadad", "title": "Question Answering as an Automatic Evaluation Metric for News Article\n  Summarization", "comments": "Accepted to NAACL2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in the field of automatic summarization and headline generation\nfocuses on maximizing ROUGE scores for various news datasets. We present an\nalternative, extrinsic, evaluation metric for this task, Answering Performance\nfor Evaluation of Summaries. APES utilizes recent progress in the field of\nreading-comprehension to quantify the ability of a summary to answer a set of\nmanually created questions regarding central entities in the source article. We\nfirst analyze the strength of this metric by comparing it to known manual\nevaluation metrics. We then present an end-to-end neural abstractive model that\nmaximizes APES, while increasing ROUGE scores to competitive results.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 00:29:05 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Eyal", "Matan", ""], ["Baumel", "Tal", ""], ["Elhadad", "Michael", ""]]}, {"id": "1906.00365", "submitter": "Tharindu Ranasinghe Mr", "authors": "Lasitha Uyangoda and Supunmali Ahangama and Tharindu Ranasinghe", "title": "User Profile Feature-Based Approach to Address the Cold Start Problem in\n  Collaborative Filtering for Personalized Movie Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A huge amount of user generated content related to movies is created with the\npopularization of web 2.0. With these continues exponential growth of data,\nthere is an inevitable need for recommender systems as people find it difficult\nto make informed and timely decisions. Movie recommendation systems assist\nusers to find the next interest or the best recommendation. In this proposed\napproach the authors apply the relationship of user feature-scores derived from\nuser-item interaction via ratings to optimize the prediction algorithm's input\nparameters used in the recommender system to improve the accuracy of\npredictions when there are less past user records. This addresses a major\ndrawback in collaborative filtering, the cold start problem by showing an\nimprovement of 8.4% compared to the base collaborative filtering algorithm. The\nuser-feature generation and evaluation of the system is carried out using the\n'MovieLens 100k dataset'. The proposed system can be generalized to other\ndomains as well.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 08:44:48 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Uyangoda", "Lasitha", ""], ["Ahangama", "Supunmali", ""], ["Ranasinghe", "Tharindu", ""]]}, {"id": "1906.00391", "submitter": "Zhengxiao Du", "authors": "Zhengxiao Du, Xiaowei Wang, Hongxia Yang, Jingren Zhou, Jie Tang", "title": "Sequential Scenario-Specific Meta Learner for Online Recommendation", "comments": "Accepted to KDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cold-start problems are long-standing challenges for practical\nrecommendations. Most existing recommendation algorithms rely on extensive\nobserved data and are brittle to recommendation scenarios with few\ninteractions. This paper addresses such problems using few-shot learning and\nmeta learning. Our approach is based on the insight that having a good\ngeneralization from a few examples relies on both a generic model\ninitialization and an effective strategy for adapting this model to newly\narising tasks. To accomplish this, we combine the scenario-specific learning\nwith a model-agnostic sequential meta-learning and unify them into an\nintegrated end-to-end framework, namely Scenario-specific Sequential Meta\nlearner (or s^2 meta). By doing so, our meta-learner produces a generic initial\nmodel through aggregating contextual information from a variety of prediction\ntasks while effectively adapting to specific tasks by leveraging\nlearning-to-learn knowledge. Extensive experiments on various real-world\ndatasets demonstrate that our proposed model can achieve significant gains over\nthe state-of-the-arts for cold-start problems in online recommendation.\nDeployment is at the Guess You Like session, the front page of the Mobile\nTaobao.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 11:53:19 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Du", "Zhengxiao", ""], ["Wang", "Xiaowei", ""], ["Yang", "Hongxia", ""], ["Zhou", "Jingren", ""], ["Tang", "Jie", ""]]}, {"id": "1906.00411", "submitter": "Serhad Sarica", "authors": "Serhad Sarica, Jianxi Luo and Kristin L. Wood", "title": "TechNet: Technology Semantic Network Based on Patent Data", "comments": "Expert Systems With Applications", "journal-ref": null, "doi": "10.1016/j.eswa.2019.112995", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing developments in general semantic networks, knowledge graphs and\nontology databases have motivated us to build a large-scale comprehensive\nsemantic network of technology-related data for engineering knowledge\ndiscovery, technology search and retrieval, and artificial intelligence for\nengineering design and innovation. Specially, we constructed a technology\nsemantic network (TechNet) that covers the elemental concepts in all domains of\ntechnology and their semantic associations by mining the complete U.S. patent\ndatabase from 1976. To derive the TechNet, natural language processing\ntechniques were utilized to extract terms from massive patent texts and recent\nword embedding algorithms were employed to vectorize such terms and establish\ntheir semantic relationships. We report and evaluate the TechNet for retrieving\nterms and their pairwise relevance that is meaningful from a technology and\nengineering design perspective. The TechNet may serve as an infrastructure to\nsupport a wide range of applications, e.g., technical text summaries, search\nquery predictions, relational knowledge discovery, and design ideation support,\nin the context of engineering and technology, and complement or enrich existing\nsemantic databases. To enable such applications, the TechNet is made public via\nan online interface and APIs for public users to retrieve technology-related\nterms and their relevancies.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 14:11:37 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 07:45:57 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 06:49:27 GMT"}, {"version": "v4", "created": "Fri, 4 Oct 2019 17:28:20 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Sarica", "Serhad", ""], ["Luo", "Jianxi", ""], ["Wood", "Kristin L.", ""]]}, {"id": "1906.00529", "submitter": "Amir Ziai", "authors": "Zhengyu Ma, Tianjiao Qi, James Route, Amir Ziai", "title": "Mining Data from the Congressional Record", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data storage and analysis method for using the US Congressional\nrecord as a policy analysis tool. We use Amazon Web Services and the Solr\nsearch engine to store and process Congressional record data from 1789 to the\npresent, and then query Solr to find how frequently language related to tax\nincreases and decreases appears. This frequency data is compared to six\neconomic indicators. Our preliminary results indicate potential relationships\nbetween incidence of tax discussion and multiple indicators. We present our\ndata storage and analysis procedures, as well as results from comparisons to\nall six indicators.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 02:22:35 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ma", "Zhengyu", ""], ["Qi", "Tianjiao", ""], ["Route", "James", ""], ["Ziai", "Amir", ""]]}, {"id": "1906.00566", "submitter": "Andrew McLeod", "authors": "Andrew McLeod", "title": "Evaluating Non-aligned Musical Score Transcriptions with MV2H", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.IR cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original MV2H metric was designed to evaluate systems which transcribe\nfrom an input audio (or MIDI) piece to a complete musical score. However, it\nrequires both the transcribed score and the ground truth score to be\ntime-aligned with the input. Some recent work has begun to transcribe directly\nfrom an audio signal into a musical score, skipping the alignment step. This\npaper introduces an automatic alignment method based on dynamic time warp which\nallows for MV2H to be used to evaluate such non-aligned transcriptions. This\nhas the additional benefit of allowing non-aligned musical scores---which are\nsignificantly more widely available than aligned ones---to be used as ground\ntruth. The code for the improved MV2H, which now also includes a MusicXML\nparser, and allows for key and time signature changes, is available at\nwww.github.com/apmcleod/MV2H.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 04:30:04 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 04:31:51 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["McLeod", "Andrew", ""]]}, {"id": "1906.00638", "submitter": "Hankz Hankui Zhuo", "authors": "Feng Liao and Hankz Hankui Zhuo and Xiaoling Huang and Yu Zhang", "title": "Federated Hierarchical Hybrid Networks for Clickbait Detection", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Online media outlets adopt clickbait techniques to lure readers to click on\narticles in a bid to expand their reach and subsequently increase revenue\nthrough ad monetization. As the adverse effects of clickbait attract more and\nmore attention, researchers have started to explore machine learning techniques\nto automatically detect clickbaits. Previous work on clickbait detection\nassumes that all the training data is available locally during training. In\nmany real-world applications, however, training data is generally distributedly\nstored by different parties (e.g., different parties maintain data with\ndifferent feature spaces), and the parties cannot share their data with each\nother due to data privacy issues. It is challenging to build models of\nhigh-quality federally for detecting clickbaits effectively without data\nsharing. In this paper, we propose a federated training framework, which is\ncalled federated hierarchical hybrid networks, to build clickbait detection\nmodels, where the titles and contents are stored by different parties, whose\nrelationships must be exploited for clickbait detection. We empirically\ndemonstrate that our approach is effective by comparing our approach to the\nstate-of-the-art approaches using datasets from social media.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 08:50:04 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Liao", "Feng", ""], ["Zhuo", "Hankz Hankui", ""], ["Huang", "Xiaoling", ""], ["Zhang", "Yu", ""]]}, {"id": "1906.00671", "submitter": "Casper Hansen", "authors": "Casper Hansen and Christian Hansen and Jakob Grue Simonsen and Stephen\n  Alstrup and Christina Lioma", "title": "Unsupervised Neural Generative Semantic Hashing", "comments": "SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast similarity search is a key component in large-scale information\nretrieval, where semantic hashing has become a popular strategy for\nrepresenting documents as binary hash codes. Recent advances in this area have\nbeen obtained through neural network based models: generative models trained by\nlearning to reconstruct the original documents. We present a novel unsupervised\ngenerative semantic hashing approach, \\textit{Ranking based Semantic Hashing}\n(RBSH) that consists of both a variational and a ranking based component.\nSimilarly to variational autoencoders, the variational component is trained to\nreconstruct the original document conditioned on its generated hash code, and\nas in prior work, it only considers documents individually. The ranking\ncomponent solves this limitation by incorporating inter-document similarity\ninto the hash code generation, modelling document ranking through a hinge loss.\nTo circumvent the need for labelled data to compute the hinge loss, we use a\nweak labeller and thus keep the approach fully unsupervised.\n  Extensive experimental evaluation on four publicly available datasets against\ntraditional baselines and recent state-of-the-art methods for semantic hashing\nshows that RBSH significantly outperforms all other methods across all\nevaluated hash code lengths. In fact, RBSH hash codes are able to perform\nsimilarly to state-of-the-art hash codes while using 2-4x fewer bits.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 09:52:17 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Hansen", "Casper", ""], ["Hansen", "Christian", ""], ["Simonsen", "Jakob Grue", ""], ["Alstrup", "Stephen", ""], ["Lioma", "Christina", ""]]}, {"id": "1906.00674", "submitter": "Casper Hansen", "authors": "Casper Hansen and Christian Hansen and Stephen Alstrup and Jakob Grue\n  Simonsen and Christina Lioma", "title": "Contextually Propagated Term Weights for Document Representation", "comments": "SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings predict a word from its neighbours by learning small, dense\nembedding vectors. In practice, this prediction corresponds to a semantic score\ngiven to the predicted word (or term weight). We present a novel model that,\ngiven a target word, redistributes part of that word's weight (that has been\ncomputed with word embeddings) across words occurring in similar contexts as\nthe target word. Thus, our model aims to simulate how semantic meaning is\nshared by words occurring in similar contexts, which is incorporated into\nbag-of-words document representations. Experimental evaluation in an\nunsupervised setting against 8 state of the art baselines shows that our model\nyields the best micro and macro F1 scores across datasets of increasing\ndifficulty.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 09:52:47 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Hansen", "Casper", ""], ["Hansen", "Christian", ""], ["Alstrup", "Stephen", ""], ["Simonsen", "Jakob Grue", ""], ["Lioma", "Christina", ""]]}, {"id": "1906.00781", "submitter": "Jiaoyan Chen", "authors": "Jiaoyan Chen and Ernesto Jimenez-Ruiz and Ian Horrocks and Charles\n  Sutton", "title": "Learning Semantic Annotations for Tabular Data", "comments": "7 pages", "journal-ref": "IJCAI 2019", "doi": null, "report-no": null, "categories": "cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usefulness of tabular data such as web tables critically depends on\nunderstanding their semantics. This study focuses on column type prediction for\ntables without any meta data. Unlike traditional lexical matching-based\nmethods, we propose a deep prediction model that can fully exploit a table's\ncontextual semantics, including table locality features learned by a Hybrid\nNeural Network (HNN), and inter-column semantics features learned by a\nknowledge base (KB) lookup and query answering algorithm.It exhibits good\nperformance not only on individual table sets, but also when transferring from\none table set to another.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 20:10:14 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Chen", "Jiaoyan", ""], ["Jimenez-Ruiz", "Ernesto", ""], ["Horrocks", "Ian", ""], ["Sutton", "Charles", ""]]}, {"id": "1906.01219", "submitter": "Xiaoying Zhang", "authors": "Xiaoying Zhang, Hong Xie, Hang Li, John C.S. Lui", "title": "Conversational Contextual Bandit: Algorithm and Application", "comments": "11 pages; Accepted by WWW2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandit algorithms provide principled online learning solutions to\nbalance the exploitation-exploration trade-off in various applications such as\nrecommender systems. However, the learning speed of the traditional contextual\nbandit algorithms is often slow due to the need for extensive exploration. This\nposes a critical issue in applications like recommender systems, since users\nmay need to provide feedbacks on a lot of uninterested items. To accelerate the\nlearning speed, we generalize contextual bandit to conversational contextual\nbandit. Conversational contextual bandit leverages not only behavioral\nfeedbacks on arms (e.g., articles in news recommendation), but also occasional\nconversational feedbacks on key-terms from the user. Here, a key-term can\nrelate to a subset of arms, for example, a category of articles in news\nrecommendation. We then design the Conversational UCB algorithm (ConUCB) to\naddress two challenges in conversational contextual bandit: (1) which key-terms\nto select to conduct conversation, (2) how to leverage conversational feedbacks\nto accelerate the speed of bandit learning. We theoretically prove that ConUCB\ncan achieve a smaller regret upper bound than the traditional contextual bandit\nalgorithm LinUCB, which implies a faster learning speed. Experiments on\nsynthetic data, as well as real datasets from Yelp and Toutiao, demonstrate the\nefficacy of the ConUCB algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 06:31:27 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2020 09:29:09 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Zhang", "Xiaoying", ""], ["Xie", "Hong", ""], ["Li", "Hang", ""], ["Lui", "John C. S.", ""]]}, {"id": "1906.01435", "submitter": "Himan Abdollahpouri", "authors": "Himan Abdollahpouri", "title": "Incorporating System-Level Objectives into Recommender Systems", "comments": "arXiv admin note: text overlap with arXiv:1901.07555", "journal-ref": null, "doi": "10.1145/3308560.3314201", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most essential parts of any recommender system is\npersonalization-- how acceptable the recommendations are from the user's\nperspective. However, in many real-world applications, there are other\nstakeholders whose needs and interests should be taken into account. In this\nwork, we define the problem of multistakeholder recommendation and we focus on\nfinding algorithms for a special case where the recommender system itself is\nalso a stakeholder. In addition, we will explore the idea of incremental\nincorporation of system-level objectives into recommender systems over time to\ntackle the existing problems in the optimization techniques which only look for\noptimizing the individual users' lists.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 23:20:01 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Abdollahpouri", "Himan", ""]]}, {"id": "1906.01511", "submitter": "Hongtao Liu", "authors": "Xianchen Wang, Hongtao Liu, Peiyi Wang, Fangzhao Wu, Hongyan Xu,\n  Wenjun Wang, Xing Xie", "title": "Neural Review Rating Prediction with Hierarchical Attentions and Latent\n  Factors", "comments": "4pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text reviews can provide rich useful semantic information for modeling users\nand items, which can benefit rating prediction in recommendation. Different\nwords and reviews may have different informativeness for users or items.\nBesides, different users and items should be personalized. Most existing works\nregard all reviews equally or utilize a general attention mechanism. In this\npaper, we propose a hierarchical attention model fusing latent factor model for\nrating prediction with reviews, which can focus on important words and\ninformative reviews. Specially, we use the factor vectors of Latent Factor\nModel to guide the attention network and combine the factor vectors with\nfeature representation learned from reviews to predict the final ratings.\nExperiments on real-world datasets validate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 14:16:28 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Wang", "Xianchen", ""], ["Liu", "Hongtao", ""], ["Wang", "Peiyi", ""], ["Wu", "Fangzhao", ""], ["Xu", "Hongyan", ""], ["Wang", "Wenjun", ""], ["Xie", "Xing", ""]]}, {"id": "1906.01599", "submitter": "Marco Bressan", "authors": "Marco Bressan, Stefano Leucci, Alessandro Panconesi", "title": "Motivo: fast motif counting via succinct color coding and adaptive\n  sampling", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The randomized technique of color coding is behind state-of-the-art\nalgorithms for estimating graph motif counts. Those algorithms, however, are\nnot yet capable of scaling well to very large graphs with billions of edges. In\nthis paper we develop novel tools for the `motif counting via color coding'\nframework. As a result, our new algorithm, Motivo, is able to scale well to\nlarger graphs while at the same time provide more accurate graphlet counts than\never before. This is achieved thanks to two types of improvements. First, we\ndesign new succinct data structures that support fast common color coding\noperations, and a biased coloring trick that trades accuracy versus running\ntime and memory usage. These adaptations drastically reduce the time and memory\nrequirements of color coding. Second, we develop an adaptive graphlet sampling\nstrategy, based on a fractional set cover problem, that breaks the additive\napproximation barrier of standard sampling. This strategy gives multiplicative\napproximations for all graphlets at once, allowing us to count not only the\nmost frequent graphlets but also extremely rare ones.\n  To give an idea of the improvements, in $40$ minutes Motivo counts $7$-nodes\nmotifs on a graph with $65$M nodes and $1.8$B edges; this is $30$ and $500$\ntimes larger than the state of the art, respectively in terms of nodes and\nedges. On the accuracy side, in one hour Motivo produces accurate counts of\n$\\approx \\! 10.000$ distinct $8$-node motifs on graphs where state-of-the-art\nalgorithms fail even to find the second most frequent motif. Our method\nrequires just a high-end desktop machine. These results show how color coding\ncan bring motif mining to the realm of truly massive graphs using only ordinary\nhardware.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 17:22:07 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Bressan", "Marco", ""], ["Leucci", "Stefano", ""], ["Panconesi", "Alessandro", ""]]}, {"id": "1906.01637", "submitter": "Chanyoung Park", "authors": "Chanyoung Park, Donghyun Kim, Xing Xie, Hwanjo Yu", "title": "Collaborative Translational Metric Learning", "comments": "ICDM 2018 Full Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, matrix factorization-based recommendation methods have been\ncriticized for the problem raised by the triangle inequality violation.\nAlthough several metric learning-based approaches have been proposed to\novercome this issue, existing approaches typically project each user to a\nsingle point in the metric space, and thus do not suffice for properly modeling\nthe intensity and the heterogeneity of user-item relationships in implicit\nfeedback. In this paper, we propose TransCF to discover such latent user-item\nrelationships embodied in implicit user-item interactions. Inspired by the\ntranslation mechanism popularized by knowledge graph embedding, we construct\nuser-item specific translation vectors by employing the neighborhood\ninformation of users and items, and translate each user toward items according\nto the user's relationships with the items. Our proposed method outperforms\nseveral state-of-the-art methods for top-N recommendation on seven real-world\ndata by up to 17% in terms of hit ratio. We also conduct extensive qualitative\nevaluations on the translation vectors learned by our proposed method to\nascertain the benefit of adopting the translation mechanism for implicit\nfeedback-based recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 16:23:25 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Park", "Chanyoung", ""], ["Kim", "Donghyun", ""], ["Xie", "Xing", ""], ["Yu", "Hwanjo", ""]]}, {"id": "1906.01829", "submitter": "Haoyu Wang", "authors": "Haoyu Wang, Defu Lian, Yong Ge", "title": "Binarized Collaborative Filtering with Distilling Graph Convolutional\n  Networks", "comments": "7 pages, 3 figures,ijcai", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency of top-K item recommendation based on implicit feedback are\nvital to recommender systems in real world, but it is very challenging due to\nthe lack of negative samples and the large number of candidate items. To\naddress the challenges, we firstly introduce an improved Graph Convolutional\nNetwork~(GCN) model with high-order feature interaction considered. Then we\ndistill the ranking information derived from GCN into binarized collaborative\nfiltering, which makes use of binary representation to improve the efficiency\nof online recommendation. However, binary codes are not only hard to be\noptimized but also likely to incur the loss of information during the training\nprocessing. Therefore, we propose a novel framework to convert the binary\nconstrained optimization problem into an equivalent continuous optimization\nproblem with a stochastic penalty. The binarized collaborative filtering model\nis then easily optimized by many popular solvers like SGD and Adam. The\nproposed algorithm is finally evaluated on three real-world datasets and shown\nthe superiority to the competing baselines.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 05:11:43 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Wang", "Haoyu", ""], ["Lian", "Defu", ""], ["Ge", "Yong", ""]]}, {"id": "1906.01830", "submitter": "Ramy Baly", "authors": "Ramy Baly (1), Alaa Khaddaj (2), Hazem Hajj (2), Wassim El-Hajj (3),\n  Khaled Bashir Shaban (4) ((1) MIT Computer Science and Artificial\n  Intelligence Laboratory, Cambridge, MA, USA, (2) American University of\n  Beirut, Electrical and Computer Engineering Department, Beirut, Lebanon, (3)\n  American University of Beirut, Computer Science Department, Beirut, Lebanon,\n  (4) Qatar University, Computer Science and Engineering Department, Doha,\n  Qatar)", "title": "ArSentD-LEV: A Multi-Topic Corpus for Target-based Sentiment Analysis in\n  Arabic Levantine Tweets", "comments": "Corpus development, Levantine tweets, multi-topic, sentiment\n  analysis, sentiment target, LREC-2018, OSACT-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is a highly subjective and challenging task. Its\ncomplexity further increases when applied to the Arabic language, mainly\nbecause of the large variety of dialects that are unstandardized and widely\nused in the Web, especially in social media. While many datasets have been\nreleased to train sentiment classifiers in Arabic, most of these datasets\ncontain shallow annotation, only marking the sentiment of the text unit, as a\nword, a sentence or a document. In this paper, we present the Arabic Sentiment\nTwitter Dataset for the Levantine dialect (ArSenTD-LEV). Based on findings from\nanalyzing tweets from the Levant region, we created a dataset of 4,000 tweets\nwith the following annotations: the overall sentiment of the tweet, the target\nto which the sentiment was expressed, how the sentiment was expressed, and the\ntopic of the tweet. Results confirm the importance of these annotations at\nimproving the performance of a baseline sentiment classifier. They also confirm\nthe gap of training in a certain domain, and testing in another domain.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 13:31:52 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Baly", "Ramy", ""], ["Khaddaj", "Alaa", ""], ["Hajj", "Hazem", ""], ["El-Hajj", "Wassim", ""], ["Shaban", "Khaled Bashir", ""]]}, {"id": "1906.01859", "submitter": "Francesco Silvestri", "authors": "Martin Aum\\\"uller and Rasmus Pagh and Francesco Silvestri", "title": "Fair Near Neighbor Search: Independent Range Sampling in High Dimensions", "comments": "Proceedings of the 39th ACM SIGMOD-SIGACT-SIGAI Symposium on\n  Principles of Database Systems (PODS), Pages 191-204, June 2020", "journal-ref": null, "doi": "10.1145/3375395.3387648", "report-no": null, "categories": "cs.DS cs.CG cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search is a fundamental algorithmic primitive, widely used in many\ncomputer science disciplines. There are several variants of the similarity\nsearch problem, and one of the most relevant is the $r$-near neighbor ($r$-NN)\nproblem: given a radius $r>0$ and a set of points $S$, construct a data\nstructure that, for any given query point $q$, returns a point $p$ within\ndistance at most $r$ from $q$. In this paper, we study the $r$-NN problem in\nthe light of fairness. We consider fairness in the sense of equal opportunity:\nall points that are within distance $r$ from the query should have the same\nprobability to be returned. In the low-dimensional case, this problem was first\nstudied by Hu, Qiao, and Tao (PODS 2014). Locality sensitive hashing (LSH), the\ntheoretically strongest approach to similarity search in high dimensions, does\nnot provide such a fairness guarantee. To address this, we propose efficient\ndata structures for $r$-NN where all points in $S$ that are near $q$ have the\nsame probability to be selected and returned by the query. Specifically, we\nfirst propose a black-box approach that, given any LSH scheme, constructs a\ndata structure for uniformly sampling points in the neighborhood of a query.\nThen, we develop a data structure for fair similarity search under inner\nproduct that requires nearly-linear space and exploits locality sensitive\nfilters. The paper concludes with an experimental evaluation that highlights\n(un)fairness in a recommendation setting on real-world datasets and discusses\nthe inherent unfairness introduced by solving other variants of the problem.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 07:27:48 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 13:13:00 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Aum\u00fcller", "Martin", ""], ["Pagh", "Rasmus", ""], ["Silvestri", "Francesco", ""]]}, {"id": "1906.01910", "submitter": "Kit Kuksenok", "authors": "Kit Kuksenok and Andriy Martyniv", "title": "Evaluation and Improvement of Chatbot Text Classification Data Quality\n  Using Plausible Negative Examples", "comments": "Included in the ACL2019 1st workshop on NLP for Conversational AI\n  (Florence, Italy). Code available: https://github.com/jobpal/nex-cv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and validate a metric for estimating multi-class classifier\nperformance based on cross-validation and adapted for improvement of small,\nunbalanced natural-language datasets used in chatbot design. Our experiences\ndraw upon building recruitment chatbots that mediate communication between\njob-seekers and recruiters by exposing the ML/NLP dataset to the recruiting\nteam. Evaluation approaches must be understandable to various stakeholders, and\nuseful for improving chatbot performance. The metric, nex-cv, uses negative\nexamples in the evaluation of text classification, and fulfils three\nrequirements. First, it is actionable: it can be used by non-developer staff.\nSecond, it is not overly optimistic compared to human ratings, making it a fast\nmethod for comparing classifiers. Third, it allows model-agnostic comparison,\nmaking it useful for comparing systems despite implementation differences. We\nvalidate the metric based on seven recruitment-domain datasets in English and\nGerman over the course of one year.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 09:52:22 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Kuksenok", "Kit", ""], ["Martyniv", "Andriy", ""]]}, {"id": "1906.01950", "submitter": "Tarcisio Mendes de Farias", "authors": "Tarcisio Mendes de Farias and Kurt Stockinger and Christophe Dessimoz", "title": "VoIDext: Vocabulary and Patterns for Enhancing Interoperable Datasets\n  with Virtual Links", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic heterogeneity remains a problem when interoperating with data from\nsources of different scopes and knowledge domains. Causes for this challenge\nare context-specific requirements (i.e. no \"one model fits all\"), different\ndata modelling decisions, domain-specific purposes, and technical constraints.\nMoreover, even if the problem of semantic heterogeneity among different RDF\npublishers and knowledge domains is solved, querying and accessing the data of\ndistributed RDF datasets on the Web is not straightforward. This is because of\nthe complex and fastidious process needed to understand how these datasets can\nbe related or linked, and consequently, queried. To address this issue, we\npropose to extend the existing Vocabulary of Interlinked Datasets (VoID) by\nintroducing new terms such as the Virtual Link Set concept and data model\npatterns. A virtual link is a connection between resources such as literals and\nIRIs (Internationalized Resource Identifier) with some commonality where each\nof these resources is from a different RDF dataset. The links are required in\norder to understand how to semantically relate datasets. In addition, we\ndescribe several benefits of using virtual links to improve interoperability\nbetween heterogenous and independent datasets. Finally, we exemplify and apply\nour approach to multiple world-wide used RDF datasets.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 11:29:01 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 07:02:08 GMT"}, {"version": "v3", "created": "Sun, 8 Sep 2019 07:56:58 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["de Farias", "Tarcisio Mendes", ""], ["Stockinger", "Kurt", ""], ["Dessimoz", "Christophe", ""]]}, {"id": "1906.02037", "submitter": "Nan Wang", "authors": "Yiyi Tao, Yiling Jia, Nan Wang, Hongning Wang", "title": "The FacT: Taming Latent Factor Models for Explainability with\n  Factorization Trees", "comments": "In proceedings of SIGIR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Latent factor models have achieved great success in personalized\nrecommendations, but they are also notoriously difficult to explain. In this\nwork, we integrate regression trees to guide the learning of latent factor\nmodels for recommendation, and use the learnt tree structure to explain the\nresulting latent factors. Specifically, we build regression trees on users and\nitems respectively with user-generated reviews, and associate a latent profile\nto each node on the trees to represent users and items. With the growth of\nregression tree, the latent factors are gradually refined under the\nregularization imposed by the tree structure. As a result, we are able to track\nthe creation of latent profiles by looking into the path of each factor on\nregression trees, which thus serves as an explanation for the resulting\nrecommendations. Extensive experiments on two large collections of Amazon and\nYelp reviews demonstrate the advantage of our model over several competitive\nbaseline algorithms. Besides, our extensive user study also confirms the\npractical value of explainable recommendations generated by our model.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 20:31:57 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Tao", "Yiyi", ""], ["Jia", "Yiling", ""], ["Wang", "Nan", ""], ["Wang", "Hongning", ""]]}, {"id": "1906.02083", "submitter": "Eilon Sheetrit", "authors": "Eilon Sheetrit, Anna Shtok and Oren Kurland", "title": "A Passage-Based Approach to Learning to Rank Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to common relevance-judgments regimes, such as TREC's, a document\ncan be deemed relevant to a query even if it contains a very short passage of\ntext with pertinent information. This fact has motivated work on passage-based\ndocument retrieval: document ranking methods that induce information from the\ndocument's passages. However, the main source of passage-based information\nutilized was passage-query similarities. We address the challenge of utilizing\nricher sources of passage-based information to improve document retrieval\neffectiveness. Specifically, we devise a suite of learning-to-rank-based\ndocument retrieval methods that utilize an effective ranking of passages\nproduced in response to the query; the passage ranking is also induced using a\nlearning-to-rank approach. Some of the methods quantify the ranking of the\npassages of a document. Others utilize the feature-based representation of\npassages used for learning a passage ranker. Empirical evaluation attests to\nthe clear merits of our methods with respect to highly effective baselines. Our\nbest performing method is based on learning a document ranking function using\ndocument-query features and passage-query features of the document's passage\nmost highly ranked.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 15:44:02 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Sheetrit", "Eilon", ""], ["Shtok", "Anna", ""], ["Kurland", "Oren", ""]]}, {"id": "1906.02325", "submitter": "Rafael Dowsley", "authors": "Devin Reich and Ariel Todoki and Rafael Dowsley and Martine De Cock\n  and Anderson C. A. Nascimento", "title": "Privacy-Preserving Classification of Personal Text Messages with Secure\n  Multi-Party Computation: An Application to Hate-Speech Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of personal text messages has many useful applications in\nsurveillance, e-commerce, and mental health care, to name a few. Giving\napplications access to personal texts can easily lead to (un)intentional\nprivacy violations. We propose the first privacy-preserving solution for text\nclassification that is provably secure. Our method, which is based on Secure\nMultiparty Computation (SMC), encompasses both feature extraction from texts,\nand subsequent classification with logistic regression and tree ensembles. We\nprove that when using our secure text classification method, the application\ndoes not learn anything about the text, and the author of the text does not\nlearn anything about the text classification model used by the application\nbeyond what is given by the classification result itself. We perform end-to-end\nexperiments with an application for detecting hate speech against women and\nimmigrants, demonstrating excellent runtime results without loss of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 21:56:33 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 04:56:11 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 05:14:56 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Reich", "Devin", ""], ["Todoki", "Ariel", ""], ["Dowsley", "Rafael", ""], ["De Cock", "Martine", ""], ["Nascimento", "Anderson C. A.", ""]]}, {"id": "1906.02329", "submitter": "Wasi Ahmad", "authors": "Wasi Uddin Ahmad and Kai-Wei Chang and Hongning Wang", "title": "Context Attentive Document Ranking and Query Suggestion", "comments": "Accepted to SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a context-aware neural ranking model to exploit users' on-task\nsearch activities and enhance retrieval performance. In particular, a two-level\nhierarchical recurrent neural network is introduced to learn search context\nrepresentation of individual queries, search tasks, and corresponding\ndependency structure by jointly optimizing two companion retrieval tasks:\ndocument ranking and query suggestion. To identify the variable dependency\nstructure between search context and users' ongoing search activities,\nattention at both levels of recurrent states are introduced. Extensive\nexperiment comparisons against a rich set of baseline methods and an in-depth\nablation analysis confirm the value of our proposed approach for modeling\nsearch context buried in search tasks.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 22:05:36 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Ahmad", "Wasi Uddin", ""], ["Chang", "Kai-Wei", ""], ["Wang", "Hongning", ""]]}, {"id": "1906.02408", "submitter": "Arindam Bose", "authors": "Aria Ameri, Arindam Bose, Mojtaba Soltanalian", "title": "Comprehensive Personalized Ranking Using One-Bit Comparison Data", "comments": "2019 IEEE Data Science Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of a personalization system is to recommend items or a set of items\naccording to the users' taste, and thus predicting their future needs. In this\npaper, we address such personalized recommendation problems for which one-bit\ncomparison data of user preferences for different items as well as the\ndifferent user inclinations toward an item are available. We devise a\ncomprehensive personalized ranking (CPR) system by employing a Bayesian\ntreatment. We also provide a connection to the learning method with respect to\nthe CPR optimization criterion to learn the underlying low-rank structure of\nthe rating matrix based on the well-established matrix factorization method.\nNumerical results are provided to verify the performance of our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 04:14:44 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Ameri", "Aria", ""], ["Bose", "Arindam", ""], ["Soltanalian", "Mojtaba", ""]]}, {"id": "1906.02416", "submitter": "Alexander Terenin", "authors": "Alexander Terenin, M{\\aa}ns Magnusson, Leif Jonsson", "title": "Sparse Parallel Training of Hierarchical Dirichlet Process Topic Models", "comments": null, "journal-ref": "Conference on Empirical Methods in Natural Language Processing,\n  2020", "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To scale non-parametric extensions of probabilistic topic models such as\nLatent Dirichlet allocation to larger data sets, practitioners rely\nincreasingly on parallel and distributed systems. In this work, we study\ndata-parallel training for the hierarchical Dirichlet process (HDP) topic\nmodel. Based upon a representation of certain conditional distributions within\nan HDP, we propose a doubly sparse data-parallel sampler for the HDP topic\nmodel. This sampler utilizes all available sources of sparsity found in natural\nlanguage - an important way to make computation efficient. We benchmark our\nmethod on a well-known corpus (PubMed) with 8m documents and 768m tokens, using\na single multi-core machine in under four days.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 05:04:08 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 12:00:09 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Terenin", "Alexander", ""], ["Magnusson", "M\u00e5ns", ""], ["Jonsson", "Leif", ""]]}, {"id": "1906.02497", "submitter": "Zhu Zhang", "authors": "Zhu Zhang, Zhijie Lin, Zhou Zhao and Zhenxin Xiao", "title": "Cross-Modal Interaction Networks for Query-Based Moment Retrieval in\n  Videos", "comments": "Accepted by SIGIR 2019 as a full paper", "journal-ref": "SIGIR, 2019, pages 655-664", "doi": "10.1145/3331184.3331235", "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query-based moment retrieval aims to localize the most relevant moment in an\nuntrimmed video according to the given natural language query. Existing works\noften only focus on one aspect of this emerging task, such as the query\nrepresentation learning, video context modeling or multi-modal fusion, thus\nfail to develop a comprehensive system for further performance improvement. In\nthis paper, we introduce a novel Cross-Modal Interaction Network (CMIN) to\nconsider multiple crucial factors for this challenging task, including (1) the\nsyntactic structure of natural language queries; (2) long-range semantic\ndependencies in video context and (3) the sufficient cross-modal interaction.\nSpecifically, we devise a syntactic GCN to leverage the syntactic structure of\nqueries for fine-grained representation learning, propose a multi-head\nself-attention to capture long-range semantic dependencies from video context,\nand next employ a multi-stage cross-modal interaction to explore the potential\nrelations of video and query contents. The extensive experiments demonstrate\nthe effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 09:45:58 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 10:18:43 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Zhang", "Zhu", ""], ["Lin", "Zhijie", ""], ["Zhao", "Zhou", ""], ["Xiao", "Zhenxin", ""]]}, {"id": "1906.02594", "submitter": "Shuai Zhang", "authors": "Shuai Zhang and Lina Yao and Lucas Vinh Tran and Aston Zhang and Yi\n  Tay", "title": "Quaternion Collaborative Filtering for Recommendation", "comments": "Accepted at IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Quaternion Collaborative Filtering (QCF), a novel\nrepresentation learning method for recommendation. Our proposed QCF relies on\nand exploits computation with Quaternion algebra, benefiting from the\nexpressiveness and rich representation learning capability of Hamilton\nproducts. Quaternion representations, based on hypercomplex numbers, enable\nrich inter-latent dependencies between imaginary components. This encourages\nintricate relations to be captured when learning user-item interactions,\nserving as a strong inductive bias as compared with the real-space inner\nproduct. All in all, we conduct extensive experiments on six real-world\ndatasets, demonstrating the effectiveness of Quaternion algebra in recommender\nsystems. The results exhibit that QCF outperforms a wide spectrum of strong\nneural baselines on all datasets. Ablative experiments confirm the\neffectiveness of Hamilton-based composition over multi-embedding composition in\nreal space.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 13:58:38 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Zhang", "Shuai", ""], ["Yao", "Lina", ""], ["Tran", "Lucas Vinh", ""], ["Zhang", "Aston", ""], ["Tay", "Yi", ""]]}, {"id": "1906.02882", "submitter": "Hussein Alrubaye", "authors": "Hussein Alrubaye, Mohamed Wiem Mkaouer, Igor Khokhlov, Leon Reznik,\n  Ali Ouni, Jason Mcgoff", "title": "Learning to Recommend Third-Party Library Migration Opportunities at the\n  API Level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The manual migration between different third-party libraries represents a\nchallenge for software developers. Developers typically need to explore both\nlibraries Application Programming Interfaces, along with reading their\ndocumentation, in order to locate the suitable mappings between replacing and\nreplaced methods. In this paper, we introduce RAPIM, a novel machine learning\napproach that recommends mappings between methods from two different libraries.\nOur model learns from previous migrations, manually performed in mined software\nsystems, and extracts a set of features related to the similarity between\nmethod signatures and method textual documentation. We evaluate our model using\n8 popular migrations, collected from 57,447 open-source Java projects. Results\nshow that RAPIM is able to recommend relevant library API mappings with an\naverage accuracy score of 87%. Finally, we provide the community with an API\nrecommendation web service that could be used to support the migration process.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 03:20:46 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Alrubaye", "Hussein", ""], ["Mkaouer", "Mohamed Wiem", ""], ["Khokhlov", "Igor", ""], ["Reznik", "Leon", ""], ["Ouni", "Ali", ""], ["Mcgoff", "Jason", ""]]}, {"id": "1906.03053", "submitter": "Maurice Tchoup\\'e Tchendji", "authors": "Maurice Tchoup\\'e Tchendji, Lionel Tadonfouet, Thomas T\\'ebougang\n  Tchendji", "title": "A Tree Pattern Matching Algorithm for XML Queries with Structural\n  Preferences", "comments": null, "journal-ref": "Journal of Computer and Communications, 2019, 7, 61-83", "doi": "10.4236/jcc.2019.71006", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the XML community, exact queries allow users to specify exactly what they\nwant to check and/or retrieve in an XML document. When they are applied to a\nsemi-structured document or to a document with an overly complex model, the\nlack or the ignorance of the explicit document model (DTD-Document Type\nDefinition, Schema, etc.) increases the risk of ob-taining an empty result set\nwhen the query is too specific, or, too large result set when it is too vague\n(e.g. it contains wildcards such as \"*\"). The reason is that in both cases,\nusers write queries according to the document model they have in mind; this can\nbe very far from the one that can actually be extracted from the document.\nOpposed to exact queries, preference queries are more flexible and can be\nrelaxed to expand the search space during their evalua-tions. Indeed, during\ntheir evaluation, certain constraints (the preferences they contain) can be\nrelaxed if necessary to avoid precisely empty results; moreover, the returned\nanswers can be filtered to retain only the best ones. This paper presents an\nalgorithm for evaluating such queries inspired by the TreeMatch algorithm\nproposed by Yao et al. for exact queries. In the pro-posed algorithm, the best\nanswers are obtained by using an adaptation of the Skyline operator (defined in\nrelational databases) in the context of documents (trees) to incrementally\nfilter into the partial solutions set, those which satisfy the maximum of\npreferential constraints. The only restriction imposed on documents is\nNo-Self-Containment.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 12:46:40 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Tchendji", "Maurice Tchoup\u00e9", ""], ["Tadonfouet", "Lionel", ""], ["Tchendji", "Thomas T\u00e9bougang", ""]]}, {"id": "1906.03064", "submitter": "Simon D. Duque Anton", "authors": "Simon Duque Anton, Daniel Fraunholz, Janis Zemitis, Frederic Pohl, and\n  Hans Dieter Schotten", "title": "Highly Scalable and Flexible Model for Effective Aggregation of\n  Context-based Data in Generic IIoT Scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interconnectivity of production machines is a key feature of the Industrial\nInternet of Things (IIoT). This feature allows for many advantages in\nproducing. Configuration and maintenance gets easier, as access to the given\nproduction unit is not necessarily coupled to physical presence. Customized\nproduction of goods is easily possible, reducing production times and\nincreasing throughput. There are, however, also dangers to the increasing\ntalkativeness of industrial production machines. The more open a system is, the\nmore points of entry for an attacker exist. Furthermore, the amount of data a\nproduction site also increases rapidly due to the integrated intelligence and\ninterconnectivity. To keep track of this data in order to detect attacks and\nerrors in the production site, it is necessary to smartly aggregate and\nevaluate the data. In this paper, we present a new approach for collecting,\naggregating and analysing data from different sources and on three different\nlevels of abstraction. Our model is event-centric, considering every occurrence\nof information inside the system as an event. In the lowest level of\nabstraction, singular packets are collected, correlated with log-entries and\nanalysed. On the highest level of abstraction, networks are pictured as a\nconnectivity graph, enriched with information about host-based activities.\nFurthermore, we describe our work in progress of evaluating our aggregation\nmodel on two different system settings. In the first scenario, we verify the\nusability of our model in a remote maintenance application. In the second\nscenario, we evaluate our model in the context of network sniffing and\ncorrelation with log-files. First results show that our model is a promising\nsolution to cope with increasing amounts of data and to correlate information\nfrom different types of sources.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 11:08:07 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Anton", "Simon Duque", ""], ["Fraunholz", "Daniel", ""], ["Zemitis", "Janis", ""], ["Pohl", "Frederic", ""], ["Schotten", "Hans Dieter", ""]]}, {"id": "1906.03114", "submitter": "Felix Beierle", "authors": "Felix Beierle and Tobias Eichinger", "title": "Collaborating with Users in Proximity for Decentralized Mobile\n  Recommender Systems", "comments": "Accepted for publication at the 2019 IEEE 16th International\n  Conference on Ubiquitous Intelligence and Computing (IEEE UIC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, recommender systems from any domain, be it movies, music,\nrestaurants, etc., are organized in a centralized fashion. The service provider\nholds all the data, biases in the recommender algorithms are not transparent to\nthe user, and the service providers often create lock-in effects making it\ninconvenient for the user to switch providers. In this paper, we argue that the\nuser's smartphone already holds a lot of the data that feeds into typical\nrecommender systems for movies, music, or POIs. With the ubiquity of the\nsmartphone and other users in proximity in public places or public\ntransportation, data can be exchanged directly between users in a\ndevice-to-device manner. This way, each smartphone can build its own database\nand calculate its own recommendations. One of the benefits of such a system is\nthat it is not restricted to recommendations for just one user - ad-hoc group\nrecommendations are also possible. While the infrastructure for such a platform\nalready exists - the smartphones already in the palms of the users - there are\nchallenges both with respect to the mobile recommender system platform as well\nas to its recommender algorithms. In this paper, we present a mobile\narchitecture for the described system - consisting of data collection, data\nexchange, and recommender system - and highlight its challenges and\nopportunities.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 14:07:55 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Beierle", "Felix", ""], ["Eichinger", "Tobias", ""]]}, {"id": "1906.03183", "submitter": "Amir Karami", "authors": "Amir Karami, Mehdi Ghasemi, Souvik Sen, Marcos Moraes, Vishal Shah", "title": "Exploring Diseases and Syndromes in Neurology Case Reports from 1955 to\n  2017 with Text Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CL cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: A large number of neurology case reports have been published, but\nit is a challenging task for human medical experts to explore all of these\npublications. Text mining offers a computational approach to investigate\nneurology literature and capture meaningful patterns. The overarching goal of\nthis study is to provide a new perspective on case reports of neurological\ndisease and syndrome analysis over the last six decades using text mining.\n  Methods: We extracted diseases and syndromes (DsSs) from more than 65,000\nneurology case reports from 66 journals in PubMed over the last six decades\nfrom 1955 to 2017. Text mining was applied to reports on the detected DsSs to\ninvestigate high-frequency DsSs, categorize them, and explore the linear trends\nover the 63-year time frame.\n  Results: The text mining methods explored high-frequency neurologic DsSs and\ntheir trends and the relationships between them from 1955 to 2017. We detected\nmore than 18,000 unique DsSs and found 10 categories of neurologic DsSs. While\nthe trend analysis showed the increasing trends in the case reports for top-10\nhigh-frequency DsSs, the categories had mixed trends.\n  Conclusion: Our study provided new insights into the application of text\nmining methods to investigate DsSs in a large number of medical case reports\nthat occur over several decades. The proposed approach can be used to provide a\nmacro level analysis of medical literature by discovering interesting patterns\nand tracking them over several years to help physicians explore these case\nreports more efficiently.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 17:38:06 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Karami", "Amir", ""], ["Ghasemi", "Mehdi", ""], ["Sen", "Souvik", ""], ["Moraes", "Marcos", ""], ["Shah", "Vishal", ""]]}, {"id": "1906.03219", "submitter": "Yazhou Yao", "authors": "Yazhou Yao, Jian Zhang, Xiansheng Hua, Fumin Shen, Zhenmin Tang", "title": "Extracting Visual Knowledge from the Internet: Making Sense of Image\n  Data", "comments": "Accepted by International Conference on MultiMedia Modeling, 2016\n  (MMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent successes in visual recognition can be primarily attributed to feature\nrepresentation, learning algorithms, and the ever-increasing size of labeled\ntraining data. Extensive research has been devoted to the first two, but much\nless attention has been paid to the third. Due to the high cost of manual\nlabeling, the size of recent efforts such as ImageNet is still relatively small\nin respect to daily applications. In this work, we mainly focus on how to\nautomatically generate identifying image data for a given visual concept on a\nvast scale. With the generated image data, we can train a robust recognition\nmodel for the given concept. We evaluate the proposed webly supervised approach\non the benchmark Pascal VOC 2007 dataset and the results demonstrates the\nsuperiority of our proposed approach in image data collection.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 16:35:33 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Yao", "Yazhou", ""], ["Zhang", "Jian", ""], ["Hua", "Xiansheng", ""], ["Shen", "Fumin", ""], ["Tang", "Zhenmin", ""]]}, {"id": "1906.03392", "submitter": "Trung Hieu Tran", "authors": "Hieu Tran, Maxim Shcherbakov", "title": "Detection and Prediction of Users Attitude Based on Real-Time and Batch\n  Sentiment Analysis of Facebook Comments", "comments": "12 pages, 6 figures, CSoNet 2016", "journal-ref": null, "doi": "10.1007/978-3-319-42345-6_24", "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most of the people have their account on social networks (e.g. Facebook,\nVkontakte) where they express their attitude to different situations and\nevents. Facebook provides only the positive mark as a like button and share.\nHowever, it is important to know the position of a certain user on posts even\nthough the opinion is negative. Positive, negative and neutral attitude can be\nextracted from the comments of users. Overall information about positive,\nnegative and neutral opinion can bring the understanding of how people react in\na position. Moreover, it is important to know how attitude is changing during\nthe time period. The contribution of the paper is a new method based on\nsentiment text analysis for detection and prediction negative and positive\npatterns for Facebook comments which combines (i) real-time sentiment text\nanalysis for pattern discovery and (ii) batch data processing for creating\nopinion forecasting algorithm. To perform forecast we propose two-steps\nalgorithm where: (i) patterns are clustered using unsupervised clustering\ntechniques and (ii) trend prediction is performed based on finding the nearest\npattern from the certain cluster. Case studies show the efficiency and accuracy\n(Avg. MAE = 0.008) of the proposed method and its practical applicability.\nAlso, we discovered three types of users attitude patterns and described them.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 05:10:28 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Tran", "Hieu", ""], ["Shcherbakov", "Maxim", ""]]}, {"id": "1906.03450", "submitter": "Thanh Tran", "authors": "Thanh Tran, Renee Sweeney, Kyumin Lee", "title": "Adversarial Mahalanobis Distance-based Attentive Song Recommender for\n  Automatic Playlist Continuation", "comments": null, "journal-ref": "SIGIR 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to solve the automatic playlist continuation (APC)\nproblem by modeling complex interactions among users, playlists, and songs\nusing only their interaction data. Prior methods mainly rely on dot product to\naccount for similarities, which is not ideal as dot product is not metric\nlearning, so it does not convey the important inequality property. Based on\nthis observation, we propose three novel deep learning approaches that utilize\nMahalanobis distance. Our first approach uses user-playlist-song interactions,\nand combines Mahalanobis distance scores between (i) a target user and a target\nsong, and (ii) between a target playlist and the target song to account for\nboth the user's preference and the playlist's theme. Our second approach\nmeasures song-song similarities by considering Mahalanobis distance scores\nbetween the target song and each member song (i.e., existing song) in the\ntarget playlist. The contribution of each distance score is measured by our\nproposed memory metric-based attention mechanism. In the third approach, we\nfuse the two previous models into a unified model to further enhance their\nperformance. In addition, we adopt and customize Adversarial Personalized\nRanking (APR) for our three approaches to further improve their robustness and\npredictive capabilities. Through extensive experiments, we show that our\nproposed models outperform eight state-of-the-art models in two large-scale\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 12:53:23 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Tran", "Thanh", ""], ["Sweeney", "Renee", ""], ["Lee", "Kyumin", ""]]}, {"id": "1906.03488", "submitter": "Trung Hieu Tran", "authors": "Hieu Tran, Ngoc Tran, Son Nguyen, Hoan Nguyen, Tien Nguyen", "title": "Recovering Variable Names for Minified Code with Usage Contexts", "comments": "11 pages, 6 figures, ICSE 2019", "journal-ref": null, "doi": "10.1109/ICSE.2019.00119", "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern Web technology, JavaScript (JS) code plays an important role. To\navoid the exposure of original source code, the variable names in JS code\ndeployed in the wild are often replaced by short, meaningless names, thus\nmaking the code extremely difficult to manually understand and analysis. This\npaper presents JSNeat, an information retrieval (IR)-based approach to recover\nthe variable names in minified JS code. JSNeat follows a data-driven approach\nto recover names by searching for them in a large corpus of open-source JS\ncode. We use three types of contexts to match a variable in given minified code\nagainst the corpus including the context of properties and roles of the\nvariable, the context of that variable and relations with other variables under\nrecovery, and the context of the task of the function to which the variable\ncontributes. We performed several empirical experiments to evaluate JSNeat on\nthe dataset of more than 322K JS files with 1M functions, and 3.5M variables\nwith 176K unique variable names. We found that JSNeat achieves a high accuracy\nof 69.1%, which is the relative improvements of 66.1% and 43% over two\nstate-of-the-art approaches JSNice and JSNaughty, respectively. The time to\nrecover for a file or for a variable with JSNeat is twice as fast as with\nJSNice and 4x as fast as with JNaughty, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 17:00:55 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Tran", "Hieu", ""], ["Tran", "Ngoc", ""], ["Nguyen", "Son", ""], ["Nguyen", "Hoan", ""], ["Nguyen", "Tien", ""]]}, {"id": "1906.03626", "submitter": "Ruihan Yang", "authors": "Ruihan Yang, Dingsu Wang, Ziyu Wang, Tianyao Chen, Junyan Jiang and\n  Gus Xia", "title": "Deep Music Analogy Via Latent Representation Disentanglement", "comments": "Accepted at the International Society for Music Information Retrieval\n  (ISMIR), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analogy-making is a key method for computer algorithms to generate both\nnatural and creative music pieces. In general, an analogy is made by partially\ntransferring the music abstractions, i.e., high-level representations and their\nrelationships, from one piece to another; however, this procedure requires\ndisentangling music representations, which usually takes little effort for\nmusicians but is non-trivial for computers. Three sub-problems arise:\nextracting latent representations from the observation, disentangling the\nrepresentations so that each part has a unique semantic interpretation, and\nmapping the latent representations back to actual music. In this paper, we\ncontribute an explicitly-constrained variational autoencoder (EC$^2$-VAE) as a\nunified solution to all three sub-problems. We focus on disentangling the pitch\nand rhythm representations of 8-beat music clips conditioned on chords. In\nproducing music analogies, this model helps us to realize the imaginary\nsituation of \"what if\" a piece is composed using a different pitch contour,\nrhythm pattern, or chord progression by borrowing the representations from\nother pieces. Finally, we validate the proposed disentanglement method using\nobjective measurements and evaluate the analogy examples by a subjective study.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 12:22:06 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 10:10:52 GMT"}, {"version": "v3", "created": "Mon, 8 Jul 2019 12:40:00 GMT"}, {"version": "v4", "created": "Sun, 20 Oct 2019 03:57:00 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Yang", "Ruihan", ""], ["Wang", "Dingsu", ""], ["Wang", "Ziyu", ""], ["Chen", "Tianyao", ""], ["Jiang", "Junyan", ""], ["Xia", "Gus", ""]]}, {"id": "1906.03766", "submitter": "Huazheng Wang", "authors": "Huazheng Wang, Sonwoo Kim, Eric McCord-Snook, Qingyun Wu, Hongning\n  Wang", "title": "Variance Reduction in Gradient Exploration for Online Learning to Rank", "comments": "Proceedings of the 42nd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '19); Key Words:\n  Online learning to rank, Dueling bandit, Variance Reduction", "journal-ref": null, "doi": "10.1145/3331184.3331264", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Learning to Rank (OL2R) algorithms learn from implicit user feedback\non the fly. The key of such algorithms is an unbiased estimation of gradients,\nwhich is often (trivially) achieved by uniformly sampling from the entire\nparameter space. This unfortunately introduces high-variance in gradient\nestimation, and leads to a worse regret of model estimation, especially when\nthe dimension of parameter space is large.\n  In this paper, we aim at reducing the variance of gradient estimation in OL2R\nalgorithms. We project the selected updating direction into a space spanned by\nthe feature vectors from examined documents under the current query (termed the\n\"document space\" for short), after interleaved test. Our key insight is that\nthe result of interleaved test solely is governed by a user's relevance\nevaluation over the examined documents. Hence, the true gradient introduced by\nthis test result should lie in the constructed document space, and components\northogonal to the document space in the proposed gradient can be safely removed\nfor variance reduction. We prove that the projected gradient is an unbiased\nestimation of the true gradient, and show that this lower-variance gradient\nestimation results in significant regret reduction. Our proposed method is\ncompatible with all existing OL2R algorithms which rank documents using a\nlinear model. Extensive experimental comparisons with several state-of-the-art\nOL2R algorithms have confirmed the effectiveness of our proposed method in\nreducing the variance of gradient estimation and improving overall performance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 02:12:08 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 20:49:27 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 06:29:57 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Wang", "Huazheng", ""], ["Kim", "Sonwoo", ""], ["McCord-Snook", "Eric", ""], ["Wu", "Qingyun", ""], ["Wang", "Hongning", ""]]}, {"id": "1906.03768", "submitter": "Jiangning Chen", "authors": "Jiangning Chen, Zhibo Dai, Juntao Duan, Qianli Hu, Ruilin Li, Heinrich\n  Matzinger, Ionel Popescu, Haoyan Zhai", "title": "A cost-reducing partial labeling estimator in text classification\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to address the text classification problems when\nlearning with partial labels is beneficial. Instead of offering each training\nsample a set of candidate labels, we assign negative-oriented labels to the\nambiguous training examples if they are unlikely fall into certain classes. We\nconstruct our new maximum likelihood estimators with self-correction property,\nand prove that under some conditions, our estimators converge faster. Also we\ndiscuss the advantages of applying one of our estimator to a fully supervised\nlearning problem. The proposed method has potential applicability in many\nareas, such as crowdsourcing, natural language processing and medical image\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 02:22:58 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Chen", "Jiangning", ""], ["Dai", "Zhibo", ""], ["Duan", "Juntao", ""], ["Hu", "Qianli", ""], ["Li", "Ruilin", ""], ["Matzinger", "Heinrich", ""], ["Popescu", "Ionel", ""], ["Zhai", "Haoyan", ""]]}, {"id": "1906.03776", "submitter": "Wentao Ouyang", "authors": "Wentao Ouyang, Xiuwu Zhang, Li Li, Heng Zou, Xin Xing, Zhaojie Liu,\n  Yanlong Du", "title": "Deep Spatio-Temporal Neural Networks for Click-Through Rate Prediction", "comments": "Accepted by KDD 2019", "journal-ref": null, "doi": "10.1145/3292500.3330655", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) prediction is a critical task in online advertising\nsystems. A large body of research considers each ad independently, but ignores\nits relationship to other ads that may impact the CTR. In this paper, we\ninvestigate various types of auxiliary ads for improving the CTR prediction of\nthe target ad. In particular, we explore auxiliary ads from two viewpoints: one\nis from the spatial domain, where we consider the contextual ads shown above\nthe target ad on the same page; the other is from the temporal domain, where we\nconsider historically clicked and unclicked ads of the user. The intuitions are\nthat ads shown together may influence each other, clicked ads reflect a user's\npreferences, and unclicked ads may indicate what a user dislikes to certain\nextent. In order to effectively utilize these auxiliary data, we propose the\nDeep Spatio-Temporal neural Networks (DSTNs) for CTR prediction. Our model is\nable to learn the interactions between each type of auxiliary data and the\ntarget ad, to emphasize more important hidden information, and to fuse\nheterogeneous data in a unified framework. Offline experiments on one public\ndataset and two industrial datasets show that DSTNs outperform several\nstate-of-the-art methods for CTR prediction. We have deployed the\nbest-performing DSTN in Shenma Search, which is the second largest search\nengine in China. The A/B test results show that the online CTR is also\nsignificantly improved compared to our last serving model.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 02:49:55 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 09:07:09 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Ouyang", "Wentao", ""], ["Zhang", "Xiuwu", ""], ["Li", "Li", ""], ["Zou", "Heng", ""], ["Xing", "Xin", ""], ["Liu", "Zhaojie", ""], ["Du", "Yanlong", ""]]}, {"id": "1906.03870", "submitter": "Bijue Jia", "authors": "Bijue Jia, Jiancheng Lv, Dayiheng Liu", "title": "Deep Learning-Based Automatic Downbeat Tracking: A Brief Review", "comments": "22 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1605.08396 by other authors", "journal-ref": "Multimedia Systems, 2019, 25(6): 617-638", "doi": "10.1007/s00530-019-00607-x", "report-no": null, "categories": "cs.IR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important format of multimedia, music has filled almost everyone's\nlife. Automatic analyzing music is a significant step to satisfy people's need\nfor music retrieval and music recommendation in an effortless way. Thereinto,\ndownbeat tracking has been a fundamental and continuous problem in Music\nInformation Retrieval (MIR) area. Despite significant research efforts,\ndownbeat tracking still remains a challenge. Previous researches either focus\non feature engineering (extracting certain features by signal processing, which\nare semi-automatic solutions); or have some limitations: they can only model\nmusic audio recordings within limited time signatures and tempo ranges.\nRecently, deep learning has surpassed traditional machine learning methods and\nhas become the primary algorithm in feature learning; the combination of\ntraditional and deep learning methods also has made better performance. In this\npaper, we begin with a background introduction of downbeat tracking problem.\nThen, we give detailed discussions of the following topics: system\narchitecture, feature extraction, deep neural network algorithms, datasets, and\nevaluation strategy. In addition, we take a look at the results from the annual\nbenchmark evaluation--Music Information Retrieval Evaluation eXchange\n(MIREX)--as well as the developments in software implementations. Although much\nhas been achieved in the area of automatic downbeat tracking, some problems\nstill remain. We point out these problems and conclude with possible directions\nand challenges for future research.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 09:38:38 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Jia", "Bijue", ""], ["Lv", "Jiancheng", ""], ["Liu", "Dayiheng", ""]]}, {"id": "1906.04281", "submitter": "Chunyuan Li", "authors": "Sam Lobel, Chunyuan Li, Jianfeng Gao, Lawrence Carin", "title": "Towards Amortized Ranking-Critical Training for Collaborative Filtering", "comments": "The first two authors contributed equally to this manuscript. Code:\n  https://github.com/samlobel/RaCT_CF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is widely used in modern recommender systems. Recent\nresearch shows that variational autoencoders (VAEs) yield state-of-the-art\nperformance by integrating flexible representations from deep neural networks\ninto latent variable models, mitigating limitations of traditional linear\nfactor models. VAEs are typically trained by maximizing the likelihood (MLE) of\nusers interacting with ground-truth items. While simple and often effective,\nMLE-based training does not directly maximize the recommendation-quality\nmetrics one typically cares about, such as top-N ranking. In this paper we\ninvestigate new methods for training collaborative filtering models based on\nactor-critic reinforcement learning, to directly optimize the\nnon-differentiable quality metrics of interest. Specifically, we train a critic\nnetwork to approximate ranking-based metrics, and then update the actor network\n(represented here by a VAE) to directly optimize against the learned metrics.\nIn contrast to traditional learning-to-rank methods that require to re-run the\noptimization procedure for new lists, our critic-based method amortizes the\nscoring process with a neural network, and can directly provide the\n(approximate) ranking scores for new lists. Empirically, we show that the\nproposed methods outperform several state-of-the-art baselines, including\nrecently-proposed deep learning approaches, on three large-scale real-world\ndatasets. The code to reproduce the experimental results and figure plots is on\nGithub: https://github.com/samlobel/RaCT_CF\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 21:30:16 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 07:48:28 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Lobel", "Sam", ""], ["Li", "Chunyuan", ""], ["Gao", "Jianfeng", ""], ["Carin", "Lawrence", ""]]}, {"id": "1906.04365", "submitter": "Wentao Ouyang", "authors": "Wentao Ouyang, Xiuwu Zhang, Shukui Ren, Chao Qi, Zhaojie Liu, Yanlong\n  Du", "title": "Representation Learning-Assisted Click-Through Rate Prediction", "comments": "Accepted by IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) prediction is a critical task in online advertising\nsystems. Most existing methods mainly model the feature-CTR relationship and\nsuffer from the data sparsity issue. In this paper, we propose DeepMCP, which\nmodels other types of relationships in order to learn more informative and\nstatistically reliable feature representations, and in consequence to improve\nthe performance of CTR prediction. In particular, DeepMCP contains three parts:\na matching subnet, a correlation subnet and a prediction subnet. These subnets\nmodel the user-ad, ad-ad and feature-CTR relationship respectively. When these\nsubnets are jointly optimized under the supervision of the target labels, the\nlearned feature representations have both good prediction powers and good\nrepresentation abilities. Experiments on two large-scale datasets demonstrate\nthat DeepMCP outperforms several state-of-the-art models for CTR prediction.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 03:08:35 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 10:58:36 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2019 09:12:04 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Ouyang", "Wentao", ""], ["Zhang", "Xiuwu", ""], ["Ren", "Shukui", ""], ["Qi", "Chao", ""], ["Liu", "Zhaojie", ""], ["Du", "Yanlong", ""]]}, {"id": "1906.04367", "submitter": "Haozhen Zhao", "authors": "Christian J. Mahoney, Nathaniel Huber-Fliflet, Haozhen Zhao, Jianping\n  Zhang, Peter Gronvall, Shi Ye", "title": "Evaluation of Seed Set Selection Approaches and Active Learning\n  Strategies in Predictive Coding", "comments": "1st International Workshop on AI and Intelligent Assistance for Legal\n  Professionals in the Digital Workplace (LegalAIIA) at The 17th International\n  Conference on Artificial Intelligence and Law (ICAIL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning is a popular methodology in text classification - known in\nthe legal domain as \"predictive coding\" or \"Technology Assisted Review\" or\n\"TAR\" - due to its potential to minimize the required review effort to build\neffective classifiers. In this study, we use extensive experimentation to\nexamine the impact of popular seed set selection strategies in active learning,\nwithin a predictive coding exercise, and evaluate different active learning\nstrategies against well-researched continuous active learning strategies for\nthe purpose of determining efficient training methods for classifying large\npopulations quickly and precisely. We study how random sampling, keyword models\nand clustering based seed set selection strategies combined together with\ntop-ranked, uncertain, random, recall inspired, and hybrid active learning\ndocument selection strategies affect the performance of active learning for\npredictive coding. We use the percentage of documents requiring review to reach\n75% recall as the \"benchmark\" metric to evaluate and compare our approaches. In\nmost cases we find that seed set selection methods have a minor impact, though\nthey do show significant impact in lower richness data sets or when choosing a\ntop-ranked active learning selection strategy. Our results also show that\nactive learning selection strategies implementing uncertainty, random, or 75%\nrecall selection strategies has the potential to reach the optimum active\nlearning round much earlier than the popular continuous active learning\napproach (top-ranked selection). The results of our research shed light on the\nimpact of active learning seed set selection strategies and also the\neffectiveness of the selection strategies for the following learning rounds.\nLegal practitioners can use the results of this study to enhance the\nefficiency, precision, and simplicity of their predictive coding process.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 03:17:22 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Mahoney", "Christian J.", ""], ["Huber-Fliflet", "Nathaniel", ""], ["Zhao", "Haozhen", ""], ["Zhang", "Jianping", ""], ["Gronvall", "Peter", ""], ["Ye", "Shi", ""]]}, {"id": "1906.04386", "submitter": "Qingquan Song", "authors": "Qingquan Song, Shiyu Chang and Xia Hu", "title": "Coupled Variational Recurrent Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of streaming recommender system and explore novel\ncollaborative filtering algorithms to handle the data dynamicity and complexity\nin a streaming manner. Although deep neural networks have demonstrated the\neffectiveness of recommendation tasks, it is lack of explorations on\nintegrating probabilistic models and deep architectures under streaming\nrecommendation settings. Conjoining the complementary advantages of\nprobabilistic models and deep neural networks could enhance both model\neffectiveness and the understanding of inference uncertainties. To bridge the\ngap, in this paper, we propose a Coupled Variational Recurrent Collaborative\nFiltering (CVRCF) framework based on the idea of Deep Bayesian Learning to\nhandle the streaming recommendation problem. The framework jointly combines\nstochastic processes and deep factorization models under a Bayesian paradigm to\nmodel the generation and evolution of users' preferences and items'\npopularities. To ensure efficient optimization and streaming update, we further\npropose a sequential variational inference algorithm based on a cross\nvariational recurrent neural network structure. Experimental results on three\nbenchmark datasets demonstrate that the proposed framework performs favorably\nagainst the state-of-the-art methods in terms of both temporal dependency\nmodeling and predictive accuracy. The learned latent variables also provide\nvisualized interpretations for the evolution of temporal dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 04:29:07 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Song", "Qingquan", ""], ["Chang", "Shiyu", ""], ["Hu", "Xia", ""]]}, {"id": "1906.04473", "submitter": "Fajie Yuan", "authors": "Fajie Yuan, Xiangnan He, Haochuan Jiang, Guibing Guo, Jian Xiong,\n  Zhezhao Xu, Yilin Xiong", "title": "Future Data Helps Training: Modeling Future Contexts for Session-based\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommender systems have attracted much attention recently. To\ncapture the sequential dependencies, existing methods resort either to data\naugmentation techniques or left-to-right style autoregressive training.Since\nthese methods are aimed to model the sequential nature of user behaviors, they\nignore the future data of a target interaction when constructing the prediction\nmodel for it. However, we argue that the future interactions after a target\ninteraction, which are also available during training, provide valuable signal\non user preference and can be used to enhance the recommendation quality.\n  Properly integrating future data into model training, however, is non-trivial\nto achieve, since it disobeys machine learning principles and can easily cause\ndata leakage. To this end, we propose a new encoder-decoder framework named\nGap-filling based Recommender (GRec), which trains the encoder and decoder by a\ngap-filling mechanism. Specifically, the encoder takes a partially-complete\nsession sequence (where some items are masked by purpose) as input, and the\ndecoder predicts these masked items conditioned on the encoded representation.\nWe instantiate the general GRec framework using convolutional neural network\nwith sparse kernels, giving consideration to both accuracy and efficiency. We\nconduct experiments on two real-world datasets covering short-, medium-, and\nlong-range user sessions, showing that GRec significantly outperforms the\nstate-of-the-art sequential recommendation methods. More empirical studies\nverify the high utility of modeling future contexts under our GRec framework.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 10:07:26 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 02:06:30 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2019 10:33:48 GMT"}, {"version": "v4", "created": "Sat, 25 Jan 2020 08:41:20 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Yuan", "Fajie", ""], ["He", "Xiangnan", ""], ["Jiang", "Haochuan", ""], ["Guo", "Guibing", ""], ["Xiong", "Jian", ""], ["Xu", "Zhezhao", ""], ["Xiong", "Yilin", ""]]}, {"id": "1906.04484", "submitter": "Behnam Ghavimi", "authors": "Behnam Ghavimi, Wolfgang Otto, and Philipp Mayr", "title": "EXmatcher: Combining Features Based on Reference Strings and Segments to\n  Enhance Citation Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation matching is a challenging task due to different problems such as the\nvariety of citation styles, mistakes in reference strings and the quality of\nidentified reference segments. The classic citation matching configuration used\nin this paper is the combination of blocking technique and a binary classifier.\nThree different possible inputs (reference strings, reference segments and a\ncombination of reference strings and segments) were tested to find the most\nefficient strategy for citation matching. In the classification step, we\ndescribe the effect which the probabilities of reference segments can have in\ncitation matching. Our evaluation on a manually curated gold standard showed\nthat the input data consisting of the combination of reference segments and\nreference strings lead to the best result. In addition, the usage of the\nprobabilities of the segmentation slightly improves the result.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 10:19:58 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Ghavimi", "Behnam", ""], ["Otto", "Wolfgang", ""], ["Mayr", "Philipp", ""]]}, {"id": "1906.04497", "submitter": "Artur Strzelecki", "authors": "Artur Strzelecki and Paulina Rutecka", "title": "The Snippets Taxonomy in Web Search Engines", "comments": "12 pages, 3 tables", "journal-ref": "Perspectives in Business Informatics Research 2019", "doi": "10.1007/978-3-030-31143-8_13", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper authors analyzed 50 000 keywords results collected from\nlocalized Polish Google search engine. We proposed a taxonomy for snippets\ndisplayed in search results as regular, rich, news, featured and entity types\nsnippets. We observed some correlations between overlapping snippets in the\nsame keywords. Results show that commercial keywords do not cause results\nhaving rich or entity types snippets, whereas keywords resulting with snippets\nare not commercial nature. We found that significant number of snippets are\nscholarly articles and rich cards carousel. We conclude our findings with\nconclusion and research limitations.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 11:06:26 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 21:22:28 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Strzelecki", "Artur", ""], ["Rutecka", "Paulina", ""]]}, {"id": "1906.04684", "submitter": "Fenia Christopoulou", "authors": "Sunil Kumar Sahu, Fenia Christopoulou, Makoto Miwa, Sophia Ananiadou", "title": "Inter-sentence Relation Extraction with Document-level Graph\n  Convolutional Neural Network", "comments": "Accepted in Association for Computational Linguistics (ACL) 2019 8\n  pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-sentence relation extraction deals with a number of complex semantic\nrelationships in documents, which require local, non-local, syntactic and\nsemantic dependencies. Existing methods do not fully exploit such dependencies.\nWe present a novel inter-sentence relation extraction model that builds a\nlabelled edge graph convolutional neural network model on a document-level\ngraph. The graph is constructed using various inter- and intra-sentence\ndependencies to capture local and non-local dependency information. In order to\npredict the relation of an entity pair, we utilise multi-instance learning with\nbi-affine pairwise scoring. Experimental results show that our model achieves\ncomparable performance to the state-of-the-art neural models on two\nbiochemistry datasets. Our analysis shows that all the types in the graph are\neffective for inter-sentence relation extraction.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 16:30:27 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Sahu", "Sunil Kumar", ""], ["Christopoulou", "Fenia", ""], ["Miwa", "Makoto", ""], ["Ananiadou", "Sophia", ""]]}, {"id": "1906.04898", "submitter": "Qiran Gong", "authors": "Hao Peng, Jianxin Li, Qiran Gong, Senzhang Wang, Lifang He, Bo Li,\n  Lihong Wang, Philip S. Yu", "title": "Hierarchical Taxonomy-Aware and Attentional Graph Capsule RCNNs for\n  Large-Scale Multi-Label Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNNs, RNNs, GCNs, and CapsNets have shown significant insights in\nrepresentation learning and are widely used in various text mining tasks such\nas large-scale multi-label text classification. However, most existing deep\nmodels for multi-label text classification consider either the non-consecutive\nand long-distance semantics or the sequential semantics, but how to consider\nthem both coherently is less studied. In addition, most existing methods treat\noutput labels as independent methods, but ignore the hierarchical relations\namong them, leading to useful semantic information loss. In this paper, we\npropose a novel hierarchical taxonomy-aware and attentional graph capsule\nrecurrent CNNs framework for large-scale multi-label text classification.\nSpecifically, we first propose to model each document as a word order preserved\ngraph-of-words and normalize it as a corresponding words-matrix representation\nwhich preserves both the non-consecutive, long-distance and local sequential\nsemantics. Then the words-matrix is input to the proposed attentional graph\ncapsule recurrent CNNs for more effectively learning the semantic features. To\nleverage the hierarchical relations among the class labels, we propose a\nhierarchical taxonomy embedding method to learn their representations, and\ndefine a novel weighted margin loss by incorporating the label representation\nsimilarity. Extensive evaluations on three datasets show that our model\nsignificantly improves the performance of large-scale multi-label text\nclassification by comparing with state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 07:23:45 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Peng", "Hao", ""], ["Li", "Jianxin", ""], ["Gong", "Qiran", ""], ["Wang", "Senzhang", ""], ["He", "Lifang", ""], ["Li", "Bo", ""], ["Wang", "Lihong", ""], ["Yu", "Philip S.", ""]]}, {"id": "1906.04914", "submitter": "Suraj Tripathi", "authors": "Abhay Kumar, Nishant Jain, Suraj Tripathi, Chirag Singh", "title": "From Fully Supervised to Zero Shot Settings for Twitter Hashtag\n  Recommendation", "comments": "Accepted in CICLing 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a comprehensive end-to-end pipeline for Twitter hashtags\nrecommendation system including data collection, supervised training setting\nand zero shot training setting. In the supervised training setting, we have\nproposed and compared the performance of various deep learning architectures,\nnamely Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) and\nTransformer Network. However, it is not feasible to collect data for all\npossible hashtag labels and train a classifier model on them. To overcome this\nlimitation, we propose a Zero Shot Learning (ZSL) paradigm for predicting\nunseen hashtag labels by learning the relationship between the semantic space\nof tweets and the embedding space of hashtag labels. We evaluated various\nstate-of-the-art ZSL methods like Convex combination of Semantic Embedding\n(ConSE), Embarrassingly Simple Zero-Shot Learning (ESZSL) and Deep Embedding\nModel for Zero-Shot Learning (DEM-ZSL) for the hashtag recommendation task. We\ndemonstrate the effectiveness and scalability of ZSL methods for the\nrecommendation of unseen hashtags. To the best of our knowledge, this is the\nfirst quantitative evaluation of ZSL methods to date for unseen hashtags\nrecommendations from tweet text.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 17:38:28 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Kumar", "Abhay", ""], ["Jain", "Nishant", ""], ["Tripathi", "Suraj", ""], ["Singh", "Chirag", ""]]}, {"id": "1906.04915", "submitter": "Ciprian-Octavian Truica", "authors": "Ciprian-Octavian Truic\\u{a} and Adriana Barnoschi", "title": "Innovating HR Using an Expert System for Recruiting IT Specialists --\n  ESRIT", "comments": "11 pages", "journal-ref": "Journal of Software & Systems Development, 2015", "doi": "10.5171/2015.762987", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most rapidly evolving and dynamic business sector is the IT\ndomain, where there is a problem finding experienced, skilled and qualified\nemployees. Specialists are essential for developing and implementing new ideas\ninto products. Human resources (HR) department plays a major role in the\nrecruitment of qualified employees by assessing their skills, using different\nHR metrics, and selecting the best candidates for a specific job. Most\nrecruiters are not qualified to evaluate IT specialists. In order to decrease\nthe gap between the HR department and IT specialists, we designed, implemented\nand tested an Expert System for Recruiting IT specialist - ESRIT. The expert\nsystem uses text mining, natural language processing, and classification\nalgorithms to extract relevant information from resumes by using a knowledge\nbase that stores the relevant key skills and phrases. The recruiter is looking\nfor the same abilities and certificates, trying to place the best applicant\ninto a specific position. The article presents a developing picture of the top\nmajor IT skills that will be required in 2014 and it argues for the choice of\nthe IT abilities domain.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 13:15:01 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Truic\u0103", "Ciprian-Octavian", ""], ["Barnoschi", "Adriana", ""]]}, {"id": "1906.04940", "submitter": "Qiang Ning", "authors": "Qiang Ning, Ben Zhou, Zhili Feng, Haoruo Peng, Dan Roth", "title": "CogCompTime: A Tool for Understanding Time in Natural Language Text", "comments": "Demo paper appeared in EMNLP'18. 6 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic extraction of temporal information in text is an important\ncomponent of natural language understanding. It involves two basic tasks: (1)\nUnderstanding time expressions that are mentioned explicitly in text (e.g.,\nFebruary 27, 1998 or tomorrow), and (2) Understanding temporal information that\nis conveyed implicitly via relations. In this paper, we introduce CogCompTime,\na system that has these two important functionalities. It incorporates the most\nrecent progress, achieves state-of-the-art performance, and is publicly\navailable.1 We believe that this demo will be useful for multiple time-aware\napplications and provide valuable insight for future research in temporal\nunderstanding.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 04:48:27 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Ning", "Qiang", ""], ["Zhou", "Ben", ""], ["Feng", "Zhili", ""], ["Peng", "Haoruo", ""], ["Roth", "Dan", ""]]}, {"id": "1906.04941", "submitter": "Qiang Ning", "authors": "Qiang Ning, Zhili Feng, Hao Wu, Dan Roth", "title": "Joint Reasoning for Temporal and Causal Relations", "comments": "Long paper appeared in ACL'18. 11 pages and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding temporal and causal relations between events is a fundamental\nnatural language understanding task. Because a cause must be before its effect\nin time, temporal and causal relations are closely related and one relation\neven dictates the other one in many cases. However, limited attention has been\npaid to studying these two relations jointly. This paper presents a joint\ninference framework for them using constrained conditional models (CCMs).\nSpecifically, we formulate the joint problem as an integer linear programming\n(ILP) problem, enforcing constraints inherently in the nature of time and\ncausality. We show that the joint inference framework results in statistically\nsignificant improvement in the extraction of both temporal and causal relations\nfrom text.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 04:58:51 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Ning", "Qiang", ""], ["Feng", "Zhili", ""], ["Wu", "Hao", ""], ["Roth", "Dan", ""]]}, {"id": "1906.05012", "submitter": "Xiaojun Quan", "authors": "Kai Wang, Xiaojun Quan and Rui Wang", "title": "BiSET: Bi-directional Selective Encoding with Template for Abstractive\n  Summarization", "comments": "The 57th Annual Meeting of the Association for Computational\n  Linguistics (ACL 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The success of neural summarization models stems from the meticulous\nencodings of source articles. To overcome the impediments of limited and\nsometimes noisy training data, one promising direction is to make better use of\nthe available training data by applying filters during summarization. In this\npaper, we propose a novel Bi-directional Selective Encoding with Template\n(BiSET) model, which leverages template discovered from training data to softly\nselect key information from each source article to guide its summarization\nprocess. Extensive experiments on a standard summarization dataset were\nconducted and the results show that the template-equipped BiSET model manages\nto improve the summarization performance significantly with a new state of the\nart.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 09:03:26 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Wang", "Kai", ""], ["Quan", "Xiaojun", ""], ["Wang", "Rui", ""]]}, {"id": "1906.05022", "submitter": "Yudan Liu", "authors": "Yudan Liu, Kaikai Ge, Xu Zhang, Leyu Lin", "title": "Real-time Attention Based Look-alike Model for Recommender System", "comments": "Accepted by KDD 2019", "journal-ref": null, "doi": "10.1145/3292500.3330707", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning models play more and more important roles in contents\nrecommender systems. However, although the performance of recommendations is\ngreatly improved, the \"Matthew effect\" becomes increasingly evident. While the\nhead contents get more and more popular, many competitive long-tail contents\nare difficult to achieve timely exposure because of lacking behavior features.\nThis issue has badly impacted the quality and diversity of recommendations. To\nsolve this problem, look-alike algorithm is a good choice to extend audience\nfor high quality long-tail contents. But the traditional look-alike models\nwhich widely used in online advertising are not suitable for recommender\nsystems because of the strict requirement of both real-time and effectiveness.\nThis paper introduces a real-time attention based look-alike model (RALM) for\nrecommender systems, which tackles the challenge of conflict between real-time\nand effectiveness. RALM realizes real-time look-alike audience extension\nbenefiting from seeds-to-user similarity prediction and improves the\neffectiveness through optimizing user representation learning and look-alike\nlearning modeling. For user representation learning, we propose a novel neural\nnetwork structure named attention merge layer to replace the concatenation\nlayer, which significantly improves the expressive ability of multi-fields\nfeature learning. On the other hand, considering the various members of seeds,\nwe design global attention unit and local attention unit to learn robust and\nadaptive seeds representation with respect to a certain target user. At last,\nwe introduce seeds clustering mechanism which not only reduces the time\ncomplexity of attention units prediction but also minimizes the loss of seeds\ninformation at the same time. According to our experiments, RALM shows superior\neffectiveness and performance than popular look-alike models.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 09:21:16 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Liu", "Yudan", ""], ["Ge", "Kaikai", ""], ["Zhang", "Xu", ""], ["Lin", "Leyu", ""]]}, {"id": "1906.05059", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi, Anup Rao, Sungchul Kim, Eunyee Koh, Nesreen K. Ahmed,\n  Gang Wu", "title": "Higher-Order Ranking and Link Prediction: From Closing Triangles to\n  Closing Higher-Order Motifs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the notion of motif closure and describe\nhigher-order ranking and link prediction methods based on the notion of closing\nhigher-order network motifs. The methods are fast and efficient for real-time\nranking and link prediction-based applications such as web search, online\nadvertising, and recommendation. In such applications, real-time performance is\ncritical. The proposed methods do not require any explicit training data, nor\ndo they derive an embedding from the graph data, or perform any explicit\nlearning. Existing methods with the above desired properties are all based on\nclosing triangles (common neighbors, Jaccard similarity, and the ilk). In this\nwork, we investigate higher-order network motifs and develop techniques based\non the notion of closing higher-order motifs that move beyond closing simple\ntriangles. All methods described in this work are fast with a runtime that is\nsublinear in the number of nodes. The experimental results indicate the\nimportance of closing higher-order motifs for ranking and link prediction\napplications. Finally, the proposed notion of higher-order motif closure can\nserve as a basis for studying and developing better ranking and link prediction\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 11:07:10 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Rao", "Anup", ""], ["Kim", "Sungchul", ""], ["Koh", "Eunyee", ""], ["Ahmed", "Nesreen K.", ""], ["Wu", "Gang", ""]]}, {"id": "1906.05143", "submitter": "Hossein Monshizadeh Naeen", "authors": "Hossein Monshizadeh Naeen, Mehrdad Jalali", "title": "A decentralized trust-aware collaborative filtering recommender system\n  based on weighted items for social tagging systems", "comments": "17 pages, 6 figures", "journal-ref": "SGIoT (2018) 2nd EAI Int. Conf", "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are used with the purpose of suggesting contents and\nresources to the users in a social network. These systems use ranks or tags\neach user assign to different resources to predict or make suggestions to\nusers. Lately, social tagging systems, in which users can insert new contents,\ntag, organize, share, and search for contents are becoming more popular. These\nsystems have a lot of valuable information, but data growth is one of its\nbiggest challenges and this has led to the need for recommender systems that\nwill predict what each user may like or need. One approach to the design of\nthese systems which uses social environment of users is known as collaborative\nfiltering (CF). One of the problems in CF systems is trustworthy of users and\ntheir tags. In this work, we consider a trust metric (which is concluded from\nusers tagging behavior) beside the similarities to give suggestions and examine\nits effect on results. On the other hand, a decentralized approach is\nintroduced which calculates similarity and trust relationships between users in\na distributed manner. This causes the capability of implementing the proposed\napproach among all types of users with respect to different types of items,\nwhich are accessed by unique id across heterogeneous networks and environments.\nFinally, we show that the proposed model for calculating similarities between\nusers reduces the size of the user-item matrix and considering trust in\ncollaborative systems can lead to a better performance in generating\nsuggestions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 13:59:51 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Naeen", "Hossein Monshizadeh", ""], ["Jalali", "Mehrdad", ""]]}, {"id": "1906.05237", "submitter": "Yikun Xian", "authors": "Yikun Xian, Zuohui Fu, S. Muthukrishnan, Gerard de Melo, Yongfeng\n  Zhang", "title": "Reinforcement Knowledge Graph Reasoning for Explainable Recommendation", "comments": "Accepted in SIGIR 2019", "journal-ref": null, "doi": "10.1145/3331184.3331203", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in personalized recommendation have sparked great interest in\nthe exploitation of rich structured information provided by knowledge graphs.\nUnlike most existing approaches that only focus on leveraging knowledge graphs\nfor more accurate recommendation, we perform explicit reasoning with knowledge\nfor decision making so that the recommendations are generated and supported by\nan interpretable causal inference procedure. To this end, we propose a method\ncalled Policy-Guided Path Reasoning (PGPR), which couples recommendation and\ninterpretability by providing actual paths in a knowledge graph. Our\ncontributions include four aspects. We first highlight the significance of\nincorporating knowledge graphs into recommendation to formally define and\ninterpret the reasoning process. Second, we propose a reinforcement learning\n(RL) approach featuring an innovative soft reward strategy, user-conditional\naction pruning and a multi-hop scoring function. Third, we design a\npolicy-guided graph search algorithm to efficiently and effectively sample\nreasoning paths for recommendation. Finally, we extensively evaluate our method\non several large-scale real-world benchmark datasets, obtaining favorable\nresults compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 16:19:17 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Xian", "Yikun", ""], ["Fu", "Zuohui", ""], ["Muthukrishnan", "S.", ""], ["de Melo", "Gerard", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "1906.05255", "submitter": "Finn Kuusisto", "authors": "Finn Kuusisto, John Steill, Zhaobin Kuang, James Thomson, David Page,\n  Ron Stewart", "title": "A Simple Text Mining Approach for Ranking Pairwise Associations in\n  Biomedical Applications", "comments": null, "journal-ref": "AMIA Joint Summits on Translational Science Proceedings (2017)\n  166-174", "doi": null, "report-no": null, "categories": "cs.IR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple text mining method that is easy to implement, requires\nminimal data collection and preparation, and is easy to use for proposing\nranked associations between a list of target terms and a key phrase. We call\nthis method KinderMiner, and apply it to two biomedical applications. The first\napplication is to identify relevant transcription factors for cell\nreprogramming, and the second is to identify potential drugs for investigation\nin drug repositioning. We compare the results from our algorithm to existing\ndata and state-of-the-art algorithms, demonstrating compelling results for both\napplication areas. While we apply the algorithm here for biomedical\napplications, we argue that the method is generalizable to any available corpus\nof sufficient size.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 17:28:21 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Kuusisto", "Finn", ""], ["Steill", "John", ""], ["Kuang", "Zhaobin", ""], ["Thomson", "James", ""], ["Page", "David", ""], ["Stewart", "Ron", ""]]}, {"id": "1906.05466", "submitter": "Aditya Joshi", "authors": "Adith Iyer, Aditya Joshi, Sarvnaz Karimi, Ross Sparks, Cecile Paris", "title": "Figurative Usage Detection of Symptom Words to Improve Personal Health\n  Mention Detection", "comments": "To appear at the 57th Annual Meeting of the Association for\n  Computational Linguistics (ACL 2019) (The second version updates the name of\n  a cited paper. A detailed note from the cited author is here :\n  https://github.com/commonsense/conceptnet5/wiki/Citation-complications )", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personal health mention detection deals with predicting whether or not a\ngiven sentence is a report of a health condition. Past work mentions errors in\nthis prediction when symptom words, i.e. names of symptoms of interest, are\nused in a figurative sense. Therefore, we combine a state-of-the-art figurative\nusage detection with CNN-based personal health mention detection. To do so, we\npresent two methods: a pipeline-based approach and a feature augmentation-based\napproach. The introduction of figurative usage detection results in an average\nimprovement of 2.21% F-score of personal health mention detection, in the case\nof the feature augmentation-based approach. This paper demonstrates the promise\nof using figurative usage detection to improve personal health mention\ndetection.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 03:42:34 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 00:16:01 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Iyer", "Adith", ""], ["Joshi", "Aditya", ""], ["Karimi", "Sarvnaz", ""], ["Sparks", "Ross", ""], ["Paris", "Cecile", ""]]}, {"id": "1906.05468", "submitter": "Aditya Joshi PhD", "authors": "Aditya Joshi, Sarvnaz Karimi, Ross Sparks, Cecile Paris, C Raina\n  MacIntyre", "title": "A Comparison of Word-based and Context-based Representations for\n  Classification Problems in Health Informatics", "comments": "To Appear in the 18th ACL Workshop on Biomedical Natural Language\n  Processing (BioNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed representations of text can be used as features when training a\nstatistical classifier. These representations may be created as a composition\nof word vectors or as context-based sentence vectors. We compare the two kinds\nof representations (word versus context) for three classification problems:\ninfluenza infection classification, drug usage classification and personal\nhealth mention classification. For statistical classifiers trained for each of\nthese problems, context-based representations based on ELMo, Universal Sentence\nEncoder, Neural-Net Language Model and FLAIR are better than Word2Vec, GloVe\nand the two adapted using the MESH ontology. There is an improvement of 2-4% in\nthe accuracy when these context-based representations are used instead of\nword-based representations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 03:48:34 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Joshi", "Aditya", ""], ["Karimi", "Sarvnaz", ""], ["Sparks", "Ross", ""], ["Paris", "Cecile", ""], ["MacIntyre", "C Raina", ""]]}, {"id": "1906.05596", "submitter": "Mithun Das Gupta", "authors": "Sudhir Kumar and Mithun Das Gupta", "title": "$c^+$GAN: Complementary Fashion Item Recommendation", "comments": "KDD'19: Workshop on AI for Fashion. arXiv admin note: text overlap\n  with arXiv:1806.08977, arXiv:1411.1784, arXiv:1609.04802 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a conditional generative adversarial model to draw realistic\nsamples from paired fashion clothing distribution and provide real samples to\npair with arbitrary fashion units. More concretely, given an image of a shirt,\nobtained from a fashion magazine, a brochure or even any random click on ones\nphone, we draw realistic samples from a parameterized conditional distribution\nlearned as a conditional generative adversarial network ($c^+$GAN) to generate\nthe possible pants which can go with the shirt. We start with a classical cGAN\nmodel as proposed by Mirza and Osindero [arXiv:1411.1784] and modify both the\ngenerator and discriminator to work on captured-in-the-wild data with no human\nalignment. We gather a dataset from web crawled data, systematically develop a\nmethod which counters the problems inherent to such data, and finally present\nplausible results based on our technique. We propose simple ideas to evaluate\nhow these techniques can conquer the cognitive gap that exists when arbitrary\nclothing articles need to be paired with another relevant article, based on\nsimilarity of search results.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 10:47:56 GMT"}], "update_date": "2019-07-06", "authors_parsed": [["Kumar", "Sudhir", ""], ["Gupta", "Mithun Das", ""]]}, {"id": "1906.05986", "submitter": "Omar Alonso", "authors": "Omar Alonso, Vasileios Kandylas, Serge-Eric Tremblay", "title": "Scalable Knowledge Graph Construction from Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a knowledge graph derived from Twitter data with the goal of\ndiscovering relationships between people, links, and topics. The goal is to\nfilter out noise from Twitter and surface an inside-out view that relies on\nhigh quality content. The generated graph contains many relationships where the\nuser can query and traverse the structure from different angles allowing the\ndevelopment of new applications.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 02:28:55 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Alonso", "Omar", ""], ["Kandylas", "Vasileios", ""], ["Tremblay", "Serge-Eric", ""]]}, {"id": "1906.06015", "submitter": "Shunsuke Kanda", "authors": "Shunsuke Kanda, Dominik K\\\"oppl, Yasuo Tabei, Kazuhiro Morita and\n  Masao Fuketa", "title": "Dynamic Path-Decomposed Tries", "comments": "Accepted by ACM Journal of Experimental Algorithmics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A keyword dictionary is an associative array whose keys are strings. Recent\napplications handling massive keyword dictionaries in main memory have a need\nfor a space-efficient implementation. When limited to static applications,\nthere are a number of highly-compressed keyword dictionaries based on the\nadvancements of practical succinct data structures. However, as most succinct\ndata structures are only efficient in the static case, it is still difficult to\nimplement a keyword dictionary that is space efficient and dynamic. In this\narticle, we propose such a keyword dictionary. Our main idea is to embrace the\npath decomposition technique, which was proposed for constructing\ncache-friendly tries. To store the path-decomposed trie in small memory, we\ndesign data structures based on recent compact hash trie representations.\nExperiments on real-world datasets reveal that our dynamic keyword dictionary\nneeds up to 68% less space than the existing smallest ones, while achieving a\nrelevant space-time tradeoff.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 04:31:12 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 03:28:39 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Kanda", "Shunsuke", ""], ["K\u00f6ppl", "Dominik", ""], ["Tabei", "Yasuo", ""], ["Morita", "Kazuhiro", ""], ["Fuketa", "Masao", ""]]}, {"id": "1906.06170", "submitter": "Tianmou Liu", "authors": "Lijun Wang, Jianbing Gong, Yingxia Zhang, Tianmou Liu, Junhui Gao", "title": "FPScreen: A Rapid Similarity Search Tool for Massive Molecular Library\n  Based on Molecular Fingerprint Comparison", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We designed a fast similarity search engine for large molecular libraries:\nFPScreen. We downloaded 100 million molecules' structure files in PubChem with\nSDF extension, then applied a computational chemistry tool RDKit to convert\neach structure file into one line of text in MACCS format and stored them in a\ntext file as our molecule library. The similarity search engine compares the\nsimilarity while traversing the 166-bit strings in the library file line by\nline. FPScreen can complete similarity search through 100 million entries in\nour molecule library within one hour. That is very fast as a biology\ncomputation tool. Additionally, we divided our library into several strides for\nparallel processing. FPScreen was developed in WEB mode.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 02:51:40 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Wang", "Lijun", ""], ["Gong", "Jianbing", ""], ["Zhang", "Yingxia", ""], ["Liu", "Tianmou", ""], ["Gao", "Junhui", ""]]}, {"id": "1906.06181", "submitter": "Mark Kozdoba", "authors": "Dan Fisher, Mark Kozdoba, Shie Mannor", "title": "Topic Modeling via Full Dependence Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new approach to topic modelling that scales to\nlarge datasets by using a compact representation of the data and by leveraging\nthe GPU architecture. In this approach, topics are learned directly from the\nco-occurrence data of the corpus. In particular, we introduce a novel mixture\nmodel which we term the Full Dependence Mixture (FDM) model. FDMs model second\nmoment under general generative assumptions on the data. While there is\nprevious work on topic modeling using second moments, we develop a direct\nstochastic optimization procedure for fitting an FDM with a single Kullback\nLeibler objective. Moment methods in general have the benefit that an iteration\nno longer needs to scale with the size of the corpus. Our approach allows us to\nleverage standard optimizers and GPUs for the problem of topic modeling. In\nparticular, we evaluate the approach on two large datasets, NeurIPS papers and\na Twitter corpus, with a large number of topics, and show that the approach\nperforms comparably or better than the the standard benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 10:47:41 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 10:14:17 GMT"}, {"version": "v3", "created": "Sun, 1 Mar 2020 15:40:08 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Fisher", "Dan", ""], ["Kozdoba", "Mark", ""], ["Mannor", "Shie", ""]]}, {"id": "1906.06390", "submitter": "Meisam Hejazi Nia", "authors": "Meisam Hejazinia, Majid Hosseini, Bryant Sih", "title": "A/B Testing Measurement Framework for Recommendation Models Based on\n  Expected Revenue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a method to determine whether a new recommendation system improves\nthe revenue per visit (RPV) compared to the status quo. We achieve our goal by\nsplitting RPV into conversion rate and average order value (AOV). We use the\ntwo-part test suggested by Lachenbruch to determine if the data generating\nprocess in the new system is different. In cases that this test does not give\nus a definitive answer about the change in RPV, we propose two alternative\ntests to determine if RPV has changed. Both of these tests rely on the\nassumption that non-zero purchase values follow a log-normal distribution. We\nempirically validate this assumption using data collected at different points\nin time from Staples.com. On average, our method needs a smaller sample size\nthan other methods. Furthermore, it does not require any subjective outlier\nremoval. Finally, it characterizes the uncertainty around RPV by providing a\nconfidence interval.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 20:24:47 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Hejazinia", "Meisam", ""], ["Hosseini", "Majid", ""], ["Sih", "Bryant", ""]]}, {"id": "1906.06437", "submitter": "S\\'ergio De Sousa Sr.", "authors": "S\\'ergio Jos\\'e de Sousa, Thiago Magela Rodrigues Dias and Adilson\n  Luiz Pinto", "title": "A Strategy for Expert Recommendation From Open Data Available on the\n  Lattes Platform", "comments": "7 pages, in Portuguese, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increasing volume of data and users of curriculum systems, the\ndifficulty of finding specialists is increasing.This work proposes an open data\nextraction methodology of the Lattes Platform curricula, a treatment for this\ndata and investigates a Recommendation Agent approach based on deep neural\nnetworks with autoencoder.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 23:36:53 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["de Sousa", "S\u00e9rgio Jos\u00e9", ""], ["Dias", "Thiago Magela Rodrigues", ""], ["Pinto", "Adilson Luiz", ""]]}, {"id": "1906.06492", "submitter": "Umutcan \\c{S}im\\c{s}ek", "authors": "Umutcan \\c{S}im\\c{s}ek, Kevin Angele, Elias K\\\"arle, Oleksandra\n  Panasiuk and Dieter Fensel", "title": "A formal approach for customization of schema.org based on SHACL", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schema.org is a widely adopted vocabulary for semantic annotation of content\nand data. However, its generic nature makes it complicated for data publishers\nto pick right types and properties for a specific domain and task. In this\npaper we propose a formal approach, a domain specification process that\ngenerates domain specific patterns by applying operators implemented in SHACL\nto the schema.org vocabulary. These patterns can support knowledge generation\nand assessment processes for specific domains and tasks. We demonstrated our\napproach with use cases in tourism domain.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 08:09:19 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["\u015eim\u015fek", "Umutcan", ""], ["Angele", "Kevin", ""], ["K\u00e4rle", "Elias", ""], ["Panasiuk", "Oleksandra", ""], ["Fensel", "Dieter", ""]]}, {"id": "1906.06526", "submitter": "Simone Santini", "authors": "Simone Santini", "title": "Relevance Feedback with Latent Variables in Riemann spaces", "comments": "33 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop and evaluate two methods for relevance feedback\nbased on endowing a suitable \"semantic query space\" with a Riemann metric\nderived from the probability distribution of the positive samples of the\nfeedback. The first method uses a Gaussian distribution to model the data,\nwhile the second uses a more complex Latent Semantic variable model. A mixed\n(discrete-continuous) version of the Expectation-Maximization algorithm is\ndeveloped for this model.\n  We motivate the need for the semantic query space by analyzing in some depth\nthree well-known relevance feedback methods, and we develop a new experimental\nmethodology to evaluate these methods and compare their performance in a\nneutral way, that is, without making assumptions on the system in which they\nwill be embedded.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 11:07:55 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Santini", "Simone", ""]]}, {"id": "1906.06542", "submitter": "Jing Chen", "authors": "Xinyu Wei, Jiahui Chen, Jing Chen, Bernie Liu", "title": "A Books Recommendation Approach Based on Online Bookstore Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of information explosion, facing complex information, it is\ndifficult for users to choose the information of interest, and businesses also\nneed detailed information on ways to let the ad stand out. By this time, it is\nrecommended that a good way. We firstly by using random interviews,\nsimulations, asking experts, summarizes methods outlined the main factors\naffecting the scores of books that users drew. In order to further illustrate\nthe impact of these factors, we also by combining the AHP consistency test,\nthen fuzzy evaluation method, empowered each factor, influencing factors and\nthe degree of influence come. For the second question, predict user evaluation\nof the listed books from the predict annex. First, given the books Annex\nlabels, user data extraction scorebooks and mathematical analysis of data\nobtained from SPSS user preferences and then use software to nearest neighbor\nanalysis to result in predicted value.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 12:32:17 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Wei", "Xinyu", ""], ["Chen", "Jiahui", ""], ["Chen", "Jing", ""], ["Liu", "Bernie", ""]]}, {"id": "1906.06581", "submitter": "Rajhans Samdani", "authors": "Rajhans Samdani, Pierre Rappolt, Ankit Goyal, Pratyus Patnaik", "title": "Practical User Feedback-driven Internal Search Using Online Learning to\n  Rank", "comments": "Proceedings of the 2019 IJCAI Workshop SCAI: The 4th International\n  Workshop on Search-Oriented Conversational AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system, Spoke, for creating and searching internal knowledge\nbase (KB) articles for organizations. Spoke is available as a SaaS\n(Software-as-a-Service) product deployed across hundreds of organizations with\na diverse set of domains. Spoke continually improves search quality using\nconversational user feedback which allows it to provide better search\nexperience than standard information retrieval systems without encoding any\nexplicit domain knowledge. We achieve this by using a real-time online\nlearning-to-rank (L2R) algorithm that automatically customizes relevance\nscoring for each organization deploying Spoke by using a query similarity\nkernel.\n  The focus of this paper is on incorporating practical considerations into our\nrelevance scoring function and algorithm that make Spoke easy to deploy and\nsuitable for handling events that naturally happen over the life-cycle of any\nKB deployment. We show that Spoke outperforms competitive baselines by up to\n41% in offline F1 comparisons.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 15:55:43 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 20:12:41 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Samdani", "Rajhans", ""], ["Rappolt", "Pierre", ""], ["Goyal", "Ankit", ""], ["Patnaik", "Pratyus", ""]]}, {"id": "1906.06606", "submitter": "Yair Feldman", "authors": "Yair Feldman, Ran El-Yaniv", "title": "Multi-Hop Paragraph Retrieval for Open-Domain Question Answering", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the task of multi-hop open-domain Question\nAnswering (QA). This task is particularly challenging since it requires the\nsimultaneous performance of textual reasoning and efficient searching. We\npresent a method for retrieving multiple supporting paragraphs, nested amidst a\nlarge knowledge base, which contain the necessary evidence to answer a given\nquestion. Our method iteratively retrieves supporting paragraphs by forming a\njoint vector representation of both a question and a paragraph. The retrieval\nis performed by considering contextualized sentence-level representations of\nthe paragraphs in the knowledge source. Our method achieves state-of-the-art\nperformance over two well-known datasets, SQuAD-Open and HotpotQA, which serve\nas our single- and multi-hop open-domain QA benchmarks, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 19:17:10 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Feldman", "Yair", ""], ["El-Yaniv", "Ran", ""]]}, {"id": "1906.06620", "submitter": "Gil Sadeh", "authors": "Gil Sadeh, Lior Fritz, Gabi Shalev and Eduard Oks", "title": "Joint Visual-Textual Embedding for Multimodal Style Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a multimodal visual-textual search refinement method for fashion\ngarments. Existing search engines do not enable intuitive, interactive,\nrefinement of retrieved results based on the properties of a particular\nproduct. We propose a method to retrieve similar items, based on a query item\nimage and textual refinement properties. We believe this method can be\nleveraged to solve many real-life customer scenarios, in which a similar item\nin a different color, pattern, length or style is desired. We employ a joint\nembedding training scheme in which product images and their catalog textual\nmetadata are mapped closely in a shared space. This joint visual-textual\nembedding space enables manipulating catalog images semantically, based on\ntextual refinement requirements. We propose a new training objective function,\nMini-Batch Match Retrieval, and demonstrate its superiority over the commonly\nused triplet loss. Additionally, we demonstrate the feasibility of adding an\nattribute extraction module, trained on the same catalog data, and demonstrate\nhow to integrate it within the multimodal search to boost its performance. We\nintroduce an evaluation protocol with an associated benchmark, and compare\nseveral approaches.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 21:50:31 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Sadeh", "Gil", ""], ["Fritz", "Lior", ""], ["Shalev", "Gabi", ""], ["Oks", "Eduard", ""]]}, {"id": "1906.06746", "submitter": "Nima Hamidi Ghalehjegh", "authors": "Nima Hamidi, Mohsen Vahidzadeh, Stephen Baek", "title": "Multi-scale Embedded CNN for Music Tagging (MsE-CNN)", "comments": "Proceedings of the 36th International Conference on Machine Learning\n  (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) recently gained notable attraction in a\nvariety of machine learning tasks: including music classification and style\ntagging. In this work, we propose implementing intermediate connections to the\nCNN architecture to facilitate the transfer of multi-scale/level knowledge\nbetween different layers. Our novel model for music tagging shows significant\nimprovement in comparison to the proposed approaches in the literature, due to\nits ability to carry low-level timbral features to the last layer.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 18:16:21 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Hamidi", "Nima", ""], ["Vahidzadeh", "Mohsen", ""], ["Baek", "Stephen", ""]]}, {"id": "1906.06752", "submitter": "Kobkaew Opasjumruskit", "authors": "Kobkaew Opasjumruskit, Diana Peters, Sirko Schindler", "title": "ConTrOn: Continuously Trained Ontology based on Technical Data Sheets\n  and Wikidata", "comments": "Pending for notification for a Submission at ISWC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In engineering projects involving various parts from global suppliers, one\ncommon task is to determine which parts are best suited for the project\nrequirements. Information about specific parts' characteristics is published in\nso called data sheets. However, these data sheets are oftentimes only published\nin textual form, e.g., as a PDF. Hence, they have to be transformed into a\nmachine-interpretable format. This transformation process still requires a lot\nof manual intervention and is prone to errors. Automated approaches make use of\nontologies to capture the given domain and thus improve automated information\nextraction from the data sheets. However, ontologies rely solely on experiences\nand perspectives of their creators at the time of creation and cannot\naccumulate knowledge over time on their own. This paper presents ConTrOn --\nContinuously Trained Ontology -- a system that automatically augments\nontologies. ConTrOn tackles terminology problems by combining the knowledge\nextracted from data sheets with an ontology created by domain experts and\nexternal knowledge bases such as WordNet and Wikidata. To demonstrate how the\nenriched ontology can improve the information extraction process, we selected\ndata sheets from spacecraft development as a use case. The evaluation results\nshow that the amount of information extracted from data sheets based on\nontologies is significantly increased after the ontology enrichment.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 19:04:25 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Opasjumruskit", "Kobkaew", ""], ["Peters", "Diana", ""], ["Schindler", "Sirko", ""]]}, {"id": "1906.06788", "submitter": "Jiahuan Pei", "authors": "Jiahuan Pei, Arent Stienstra, Julia Kiseleva, Maarten de Rijke", "title": "SEntNet: Source-aware Recurrent Entity Network for Dialogue Response\n  Selection", "comments": "Proceedings of the 2019 IJCAI Workshop SCAI: The 4th International\n  Workshop on Search-Oriented Conversational AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue response selection is an important part of Task-oriented Dialogue\nSystems (TDSs); it aims to predict an appropriate response given a dialogue\ncontext. Obtaining key information from a complex, long dialogue context is\nchallenging, especially when different sources of information are available,\ne.g., the user's utterances, the system's responses, and results retrieved from\na knowledge base (KB). Previous work ignores the type of information source and\nmerges sources for response selection. However, accounting for the source type\nmay lead to remarkable differences in the quality of response selection. We\npropose the Source-aware Recurrent Entity Network (SEntNet), which is aware of\ndifferent information sources for the response selection process. SEntNet\nachieves this by employing source-specific memories to exploit differences in\nthe usage of words and syntactic structure from different information sources\n(user, system, and KB). Experimental results show that SEntNet obtains 91.0%\naccuracy on the Dialog bAbI dataset, outperforming prior work by 4.7%. On the\nDSTC2 dataset, SEntNet obtains an accuracy of 41.2%, beating source unaware\nrecurrent entity networks by 2.4%.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 22:36:33 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 17:24:59 GMT"}, {"version": "v3", "created": "Thu, 20 Jun 2019 00:38:00 GMT"}, {"version": "v4", "created": "Mon, 16 Sep 2019 09:29:48 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Pei", "Jiahuan", ""], ["Stienstra", "Arent", ""], ["Kiseleva", "Julia", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1906.06849", "submitter": "Sheikh Muhammad Sarwar", "authors": "Sheikh Muhammad Sarwar, Hamed Bonab, James Allan", "title": "A Multi-Task Architecture on Relevance-based Neural Query Translation", "comments": "Accepted for publication at ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a multi-task learning approach to train a Neural Machine\nTranslation (NMT) model with a Relevance-based Auxiliary Task (RAT) for search\nquery translation. The translation process for Cross-lingual Information\nRetrieval (CLIR) task is usually treated as a black box and it is performed as\nan independent step. However, an NMT model trained on sentence-level parallel\ndata is not aware of the vocabulary distribution of the retrieval corpus. We\naddress this problem with our multi-task learning architecture that achieves\n16% improvement over a strong NMT baseline on Italian-English query-document\ndataset. We show using both quantitative and qualitative analysis that our\nmodel generates balanced and precise translations with the regularization\neffect it achieves from multi-task learning paradigm.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 05:37:27 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Sarwar", "Sheikh Muhammad", ""], ["Bonab", "Hamed", ""], ["Allan", "James", ""]]}, {"id": "1906.06945", "submitter": "Bin Liang", "authors": "Bin Liang, Jiachen Du, Ruifeng Xu, Binyang Li, Hejiao Huang", "title": "Context-aware Embedding for Targeted Aspect-based Sentiment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention-based neural models were employed to detect the different aspects\nand sentiment polarities of the same target in targeted aspect-based sentiment\nanalysis (TABSA). However, existing methods do not specifically pre-train\nreasonable embeddings for targets and aspects in TABSA. This may result in\ntargets or aspects having the same vector representations in different contexts\nand losing the context-dependent information. To address this problem, we\npropose a novel method to refine the embeddings of targets and aspects. Such\npivotal embedding refinement utilizes a sparse coefficient vector to adjust the\nembeddings of target and aspect from the context. Hence the embeddings of\ntargets and aspects can be refined from the highly correlative words instead of\nusing context-independent or randomly initialized vectors. Experiment results\non two benchmark datasets show that our approach yields the state-of-the-art\nperformance in TABSA task.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 10:59:50 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Liang", "Bin", ""], ["Du", "Jiachen", ""], ["Xu", "Ruifeng", ""], ["Li", "Binyang", ""], ["Huang", "Hejiao", ""]]}, {"id": "1906.07145", "submitter": "Anders Elowsson", "authors": "Anders Elowsson and Anders Friberg", "title": "Modeling Music Modality with a Key-Class Invariant Pitch Chroma CNN", "comments": "Accepted for publication in ISMIR, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a convolutional neural network (CNN) that uses input from\na polyphonic pitch estimation system to predict perceived minor/major modality\nin music audio. The pitch activation input is structured to allow the first CNN\nlayer to compute two pitch chromas focused on different octaves. The following\nlayers perform harmony analysis across chroma and time scales. Through max\npooling across pitch, the CNN becomes invariant with regards to the key class\n(i.e., key disregarding mode) of the music. A multilayer perceptron combines\nthe modality activation output with spectral features for the final prediction.\nThe study uses a dataset of 203 excerpts rated by around 20 listeners each, a\nsmall challenging data size requiring a carefully designed parameter sharing.\nWith an R2 of about 0.71, the system clearly outperforms previous systems as\nwell as individual human listeners. A final ablation study highlights the\nimportance of using pitch activations processed across longer time scales, and\nusing pooling to facilitate invariance with regards to the key class.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 17:33:07 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Elowsson", "Anders", ""], ["Friberg", "Anders", ""]]}, {"id": "1906.07591", "submitter": "Julien Rossi", "authors": "Julien Rossi, Matthias Wirth, Evangelos Kanoulas", "title": "Query Generation for Patent Retrieval with Keyword Extraction based on\n  Syntactic Features", "comments": "Presented as short paper at JURIX 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a new method to extract relevant keywords from patent\nclaims, as part of the task of retrieving other patents with similar claims\n(search for prior art). The method combines a qualitative analysis of the\nwriting style of the claims with NLP methods to parse text, in order to\nrepresent a legal text as a specialization arborescence of terms. In this\nsetting, the set of extracted keywords are yielding better search results than\nkeywords extracted with traditional methods such as tf-idf. The performance is\nmeasured on the search results of a query consisting of the extracted keywords.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 14:03:14 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Rossi", "Julien", ""], ["Wirth", "Matthias", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "1906.07622", "submitter": "Pranava Madhyastha", "authors": "Rishabh Jain and Pranava Madhyastha", "title": "Model Explanations under Calibration", "comments": "Accepted for publication in SIGIR 2019 Workshop on ExplainAble\n  Recommendation and Search (EARS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Explaining and interpreting the decisions of recommender systems are becoming\nextremely relevant both, for improving predictive performance, and providing\nvalid explanations to users. While most of the recent interest has focused on\nproviding local explanations, there has been a much lower emphasis on studying\nthe effects of model dynamics and its impact on explanation. In this paper, we\nperform a focused study on the impact of model interpretability in the context\nof calibration. Specifically, we address the challenges of both over-confident\nand under-confident predictions with interpretability using attention\ndistribution. Our results indicate that the means of using attention\ndistributions for interpretability are highly unstable for un-calibrated\nmodels. Our empirical analysis on the stability of attention distribution\nraises questions on the utility of attention for explainability.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 14:50:36 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Jain", "Rishabh", ""], ["Madhyastha", "Pranava", ""]]}, {"id": "1906.07871", "submitter": "Sankardeep Chakraborty", "authors": "Sankardeep Chakraborty, Kunihiko Sadakane", "title": "Indexing Graph Search Trees and Applications", "comments": "23 pages, Preliminary version of this paper will appear in MFCS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of compactly representing the Depth First Search\n(DFS) tree of a given undirected or directed graph having $n$ vertices and $m$\nedges while supporting various DFS related queries efficiently in the RAM with\nlogarithmic word size. We study this problem in two well-known models: {\\it\nindexing} and {\\it encoding} models. While most of these queries can be\nsupported easily in constant time using $O(n \\lg n)$ bits\\footnote{We use $\\lg$\nto denote logarithm to the base $2$.} of extra space, our goal here is, more\nspecifically, to beat this trivial $O(n \\lg n)$ bit space bound, yet not\ncompromise too much on the running time of these queries. In the {\\it indexing}\nmodel, the space bound of our solution involves the quantity $m$, hence, we\nobtain different bounds for sparse and dense graphs respectively. In the {\\it\nencoding} model, we first give a space lower bound, followed by an almost\noptimal data structure with extremely fast query time. Central to our algorithm\nis a partitioning of the DFS tree into connected subtrees, and a compact way to\nstore these connections. Finally, we also apply these techniques to compactly\nindex the shortest path structure, biconnectivity structures among others.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 01:34:47 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Chakraborty", "Sankardeep", ""], ["Sadakane", "Kunihiko", ""]]}, {"id": "1906.07874", "submitter": "Sankardeep Chakraborty", "authors": "Sankardeep Chakraborty, Anish Mukherjee, Srinivasa Rao Satti", "title": "Space Efficient Algorithms for Breadth-Depth Search", "comments": "12 pages, This work will appear in FCT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuing the recent trend, in this article we design several\nspace-efficient algorithms for two well-known graph search methods. Both these\nsearch methods share the same name {\\it breadth-depth search} (henceforth {\\sf\nBDS}), although they work entirely in different fashion. The classical\nimplementation for these graph search methods takes $O(m+n)$ time and $O(n \\lg\nn)$ bits of space in the standard word RAM model (with word size being\n$\\Theta(\\lg n)$ bits), where $m$ and $n$ denotes the number of edges and\nvertices of the input graph respectively. Our goal here is to beat the space\nbound of the classical implementations, and design $o(n \\lg n)$ space\nalgorithms for these search methods by paying little to no penalty in the\nrunning time. Note that our space bounds (i.e., with $o(n \\lg n)$ bits of\nspace) do not even allow us to explicitly store the required information to\nimplement the classical algorithms, yet our algorithms visits and reports all\nthe vertices of the input graph in correct order.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 01:45:10 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Chakraborty", "Sankardeep", ""], ["Mukherjee", "Anish", ""], ["Satti", "Srinivasa Rao", ""]]}, {"id": "1906.07897", "submitter": "Brandon Tran", "authors": "Brandon Tran, Maryam Karimzadehgan, Rama Kumar Pasumarthi, Michael\n  Bendersky, Donald Metzler", "title": "Domain Adaptation for Enterprise Email Search", "comments": "Proceedings of the 42nd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval", "journal-ref": "Proceedings of the 42nd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval, 2019", "doi": "10.1145/3331184.3331204", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the enterprise email search setting, the same search engine often powers\nmultiple enterprises from various industries: technology, education,\nmanufacturing, etc. However, using the same global ranking model across\ndifferent enterprises may result in suboptimal search quality, due to the\ncorpora differences and distinct information needs. On the other hand, training\nan individual ranking model for each enterprise may be infeasible, especially\nfor smaller institutions with limited data. To address this data challenge, in\nthis paper we propose a domain adaptation approach that fine-tunes the global\nmodel to each individual enterprise. In particular, we propose a novel\napplication of the Maximum Mean Discrepancy (MMD) approach to information\nretrieval, which attempts to bridge the gap between the global data\ndistribution and the data distribution for a given individual enterprise. We\nconduct a comprehensive set of experiments on a large-scale email search\nengine, and demonstrate that the MMD approach consistently improves the search\nquality for multiple individual domains, both in comparison to the global\nranking model, as well as several competitive domain adaptation baselines\nincluding adversarial learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 03:22:50 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Tran", "Brandon", ""], ["Karimzadehgan", "Maryam", ""], ["Pasumarthi", "Rama Kumar", ""], ["Bendersky", "Michael", ""], ["Metzler", "Donald", ""]]}, {"id": "1906.08092", "submitter": "Antonin Delpeuch", "authors": "Antonin Delpeuch", "title": "A survey of OpenRefine reconciliation services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We review the services implementing the OpenRefine reconciliation API,\ncomparing their design to the state of the art in record linkage. Due to the\ndesign of the API, the matching scores returned by the services are of little\nhelp to guide matching decisions. This suggests possible improvements to the\nspecifications of the API, which could improve user workflows by giving more\ncontrol over the scoring mechanism to the client.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 13:35:41 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 13:23:17 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Delpeuch", "Antonin", ""]]}, {"id": "1906.08231", "submitter": "Maurice Tchoup\\'e Tchendji", "authors": "Maurice Tchoup\\'e Tchendji, Adolphe Gaius Nkuefone, and Thomas\n  T\\'ebougang Tchendji", "title": "Holistic evaluation of XML queries with structural preferences on an\n  annotated strong dataguide", "comments": null, "journal-ref": "International Journal of Software Engineering & Applications\n  (IJSEA), Vol.10, No.3, May 2019", "doi": "10.5121/ijsea.2019.10304", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  With the emergence of XML as de facto format for storing and exchanging\ninformation over the Internet, the search for ever more innovative and\neffective techniques for their querying is a major and current concern of the\nXML database community. Several studies carried out to help solve this problem\nare mostly oriented towards the evaluation of so-called exact queries which,\nunfortunately, are likely (especially in the case of semi-structured documents)\nto yield abundant results (in the case of vague queries) or empty results (in\nthe case of very precise queries). From the observation that users who make\nrequests are not necessarily interested in all possible solutions, but rather\nin those that are closest to their needs, an important field of research has\nbeen opened on the evaluation of preferences queries. In this paper, we propose\nan approach for the evaluation of such queries, in case the preferences concern\nthe structure of the document. The solution investigated revolves around the\nproposal of an evaluation plan in three phases: rewriting-evaluation-merge. The\nrewriting phase makes it possible to obtain, from a partitioning\n-transformation operation of the initial query, a hierarchical set of\npreferences path queries which are holistically evaluated in the second phase\nby an instrumented version of the algorithm TwigStack. The merge phase is the\nsynthesis of the best results.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 11:23:18 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Tchendji", "Maurice Tchoup\u00e9", ""], ["Nkuefone", "Adolphe Gaius", ""], ["Tchendji", "Thomas T\u00e9bougang", ""]]}, {"id": "1906.08361", "submitter": "Rene Haberland", "authors": "Ren\\'e Haberland and Igor L. Bratchikov", "title": "Transformation of XML Documents with Prolog", "comments": "5 pages, 1 figure, 4 appendices", "journal-ref": "SCOPUS PURE/Elsevier/FDPW'2008, Advances in Methods of Information\n  and Communication Technology (AMICT2018). vol.10, 2008, pp.99-111, RSCI\n  26444969, ISBN 975-5-8021-1020-1", "doi": null, "report-no": null, "categories": "cs.LO cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transforming XML documents with conventional XML languages, like XSL-T, is\ndisadvantageous because there is too lax abstraction on the target language and\nit is rather difficult to recognize rule-oriented transformations. Prolog as a\nprogramming language of declarative paradigm is especially good for\nimplementation of analysis of formal languages. Prolog seems also to be good\nfor term manipulation, complex schema-transformation and text retrieval. In\nthis report an appropriate model for XML documents is proposed, the basic\ntransformation language for Prolog LTL is defined and the expressiveness power\ncompared with XSL-T is demonstrated, the implementations used throughout are\nmulti paradigmatic.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 21:20:14 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Haberland", "Ren\u00e9", ""], ["Bratchikov", "Igor L.", ""]]}, {"id": "1906.08369", "submitter": "Rene Haberland", "authors": "Ren\\'e Haberland", "title": "Unification of Template-Expansion and XML-Validation", "comments": "4 pages (with Extended Abstract)", "journal-ref": "39th Int.Conf. on Control Processes and Stability (CPaS08),\n  vol.34, Saint Petersburg State University, 2008, pp.389-394, RSCI\n  517.51:517.9:518.9, ISBN 978-5-288-04680-3, ISSN 2313-7304", "doi": null, "report-no": null, "categories": "cs.LO cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The processing of XML documents often includes creation and validation. These\ntwo operations are typically performed in two different nodes within a computer\nnetwork that do not correlate with each other. The process of creation is also\ncalled instantiation of a template and can be described by filling a template\nwith data from external repositories. Initial access to arbitrary sources can\nbe formulated as an expression of certain command languages like XPath. Filling\nmeans copying invariant element nodes to the target document and unfolding\nvariable parts from a given template. Validation is a descision problem\nreturning true if a given XML document satisfies a schema and false otherwise.\nThe main subject is to find a language that unions the template expansion and\nthe validation. [..].\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 21:38:05 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Haberland", "Ren\u00e9", ""]]}, {"id": "1906.08470", "submitter": "Athar Sefid", "authors": "Athar Sefid, Jian Wu, Allen C. Ge, Jing Zhao, Lu Liu, Cornelia\n  Caragea, Prasenjit Mitra, C. Lee Giles", "title": "Cleaning Noisy and Heterogeneous Metadata for Record Linking Across\n  Scholarly Big Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically extracted metadata from scholarly documents in PDF formats is\nusually noisy and heterogeneous, often containing incomplete fields and\nerroneous values. One common way of cleaning metadata is to use a bibliographic\nreference dataset. The challenge is to match records between corpora with high\nprecision. The existing solution which is based on information retrieval and\nstring similarity on titles works well only if the titles are cleaned. We\nintroduce a system designed to match scholarly document entities with noisy\nmetadata against a reference dataset. The blocking function uses the classic\nBM25 algorithm to find the matching candidates from the reference data that has\nbeen indexed by ElasticSearch. The core components use supervised methods which\ncombine features extracted from all available metadata fields. The system also\nleverages available citation information to match entities. The combination of\nmetadata and citation achieves high accuracy that significantly outperforms the\nbaseline method on the same test dataset. We apply this system to match the\ndatabase of CiteSeerX against Web of Science, PubMed, and DBLP. This method\nwill be deployed in the CiteSeerX system to clean metadata and link records to\nother scholarly big datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 07:21:33 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Sefid", "Athar", ""], ["Wu", "Jian", ""], ["Ge", "Allen C.", ""], ["Zhao", "Jing", ""], ["Liu", "Lu", ""], ["Caragea", "Cornelia", ""], ["Mitra", "Prasenjit", ""], ["Giles", "C. Lee", ""]]}, {"id": "1906.08511", "submitter": "Jingjing Li", "authors": "Jingjing Li, Mengmeng Jing, Ke Lu, Lei Zhu, Yang Yang, and Zi Huang", "title": "From Zero-Shot Learning to Cold-Start Recommendation", "comments": "Accepted to AAAI 2019. Codes are available at\n  https://github.com/lijin118/LLAE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) and cold-start recommendation (CSR) are two\nchallenging problems in computer vision and recommender system, respectively.\nIn general, they are independently investigated in different communities. This\npaper, however, reveals that ZSL and CSR are two extensions of the same\nintension. Both of them, for instance, attempt to predict unseen classes and\ninvolve two spaces, one for direct feature representation and the other for\nsupplementary description. Yet there is no existing approach which addresses\nCSR from the ZSL perspective. This work, for the first time, formulates CSR as\na ZSL problem, and a tailor-made ZSL method is proposed to handle CSR.\nSpecifically, we propose a Low-rank Linear Auto-Encoder (LLAE), which\nchallenges three cruxes, i.e., domain shift, spurious correlations and\ncomputing efficiency, in this paper. LLAE consists of two parts, a low-rank\nencoder maps user behavior into user attributes and a symmetric decoder\nreconstructs user behavior from user attributes. Extensive experiments on both\nZSL and CSR tasks verify that the proposed method is a win-win formulation,\ni.e., not only can CSR be handled by ZSL models with a significant performance\nimprovement compared with several conventional state-of-the-art methods, but\nthe consideration of CSR can benefit ZSL as well.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 09:13:17 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 02:52:11 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Li", "Jingjing", ""], ["Jing", "Mengmeng", ""], ["Lu", "Ke", ""], ["Zhu", "Lei", ""], ["Yang", "Yang", ""], ["Huang", "Zi", ""]]}, {"id": "1906.08595", "submitter": "Christian Otto", "authors": "Christian Otto, Matthias Springstein, Avishek Anand, Ralph Ewerth", "title": "Understanding, Categorizing and Predicting Semantic Image-Text Relations", "comments": "8 pages, 8 Figures, 5 tables", "journal-ref": "In Proceedings of the 2019 on International Conference on\n  Multimedia Retrieval (ICMR '19). ACM, New York, NY, USA, 168-176", "doi": "10.1145/3323873.3325049", "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two modalities are often used to convey information in a complementary and\nbeneficial manner, e.g., in online news, videos, educational resources, or\nscientific publications. The automatic understanding of semantic correlations\nbetween text and associated images as well as their interplay has a great\npotential for enhanced multimodal web search and recommender systems. However,\nautomatic understanding of multimodal information is still an unsolved research\nproblem. Recent approaches such as image captioning focus on precisely\ndescribing visual content and translating it to text, but typically address\nneither semantic interpretations nor the specific role or purpose of an\nimage-text constellation. In this paper, we go beyond previous work and\ninvestigate, inspired by research in visual communication, useful semantic\nimage-text relations for multimodal information retrieval. We derive a\ncategorization of eight semantic image-text classes (e.g., \"illustration\" or\n\"anchorage\") and show how they can systematically be characterized by a set of\nthree metrics: cross-modal mutual information, semantic correlation, and the\nstatus relation of image and text. Furthermore, we present a deep learning\nsystem to predict these classes by utilizing multimodal embeddings. To obtain a\nsufficiently large amount of training data, we have automatically collected and\naugmented data from a variety of data sets and web resources, which enables\nfuture research on this topic. Experimental results on a demanding test set\ndemonstrate the feasibility of the approach.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 13:20:22 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Otto", "Christian", ""], ["Springstein", "Matthias", ""], ["Anand", "Avishek", ""], ["Ewerth", "Ralph", ""]]}, {"id": "1906.08615", "submitter": "Jeong Choi", "authors": "Jeong Choi, Jongpil Lee, Jiyoung Park and Juhan Nam", "title": "Zero-shot Learning and Knowledge Transfer in Music Classification and\n  Tagging", "comments": "International Conference on Machine Learning (ICML) 2019, Machine\n  Learning for Music Discovery Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music classification and tagging is conducted through categorical supervised\nlearning with a fixed set of labels. In principle, this cannot make predictions\non unseen labels. Zero-shot learning is an approach to solve the problem by\nusing side information about the semantic labels. We recently investigated this\nconcept of zero-shot learning in music classification and tagging task by\nprojecting both audio and label space on a single semantic space. In this work,\nwe extend the work to verify the generalization ability of zero-shot learning\nmodel by conducting knowledge transfer to different music corpora.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 13:45:17 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Choi", "Jeong", ""], ["Lee", "Jongpil", ""], ["Park", "Jiyoung", ""], ["Nam", "Juhan", ""]]}, {"id": "1906.08693", "submitter": "Deepak Uniyal", "authors": "Deepak Uniyal, Ankit Rai", "title": "Citizens' Emotion on GST: A Spatio-Temporal Analysis over Twitter Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People might not be close-at-hand but they still are - by virtue of the\nsocial network. The social network has transformed lives in many ways. People\ncan express their views, opinions and life experiences on various platforms be\nit Twitter, Facebook or any other medium there is. Such events constitute of\nreviewing a product or service, conveying views on political banters,\npredicting share prices or giving feedback on the government policies like\nDemonetization or GST. These social platforms can be used to investigate the\ninsights of the emotional curve that the general public is generating. This\nkind of analysis can help make a product better, predict the future prospects\nand also to implement the public policies in a better way. Such kind of\nresearch on sentiment analysis is increasing rapidly. In this research paper,\nwe have performed temporal analysis and spatial analysis on 1,42,508 and 58,613\ntweets respectively and these tweets were posted during the post-GST\nimplementation period from July 04, 2017 to July 25, 2017. The tweets were\ncollected using the Twitter streaming API. A well-known lexicon, National\nResearch Council Canada (NRC) emotion Lexicon is used for opinion mining that\nexhibits a blend of eight basic emotions i.e. joy, trust, anticipation,\nsurprise, fear sadness, anger, disgust and two sentiments i.e. positive and\nnegative for 6,554 words.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 15:33:05 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Uniyal", "Deepak", ""], ["Rai", "Ankit", ""]]}, {"id": "1906.08973", "submitter": "Gaurav Verma", "authors": "Aadhavan M. Nambhi, Bhanu Prakash Reddy, Aarsh Prakash Agarwal, Gaurav\n  Verma, Harvineet Singh, Iftikhar Ahamath Burhanuddin", "title": "Stuck? No worries!: Task-aware Command Recommendation and Proactive Help\n  for Analysts", "comments": "27th Conference on User Modeling, Adaptation and Personalization\n  (UMAP'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analytics software applications have become an integral part of the\ndecision-making process of analysts. Users of such a software face challenges\ndue to insufficient product and domain knowledge, and find themselves in need\nof help. To alleviate this, we propose a task-aware command recommendation\nsystem, to guide the user on what commands could be executed next. We rely on\ntopic modeling techniques to incorporate information about user's task into our\nmodels. We also present a help prediction model to detect if a user is in need\nof help, in which case the system proactively provides the aforementioned\ncommand recommendations. We leverage the log data of a web-based analytics\nsoftware to quantify the superior performance of our neural models, in\ncomparison to competitive baselines.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 06:30:08 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Nambhi", "Aadhavan M.", ""], ["Reddy", "Bhanu Prakash", ""], ["Agarwal", "Aarsh Prakash", ""], ["Verma", "Gaurav", ""], ["Singh", "Harvineet", ""], ["Burhanuddin", "Iftikhar Ahamath", ""]]}, {"id": "1906.09217", "submitter": "Chen Ma", "authors": "Chen Ma, Peng Kang, and Xue Liu", "title": "Hierarchical Gating Networks for Sequential Recommendation", "comments": "Accepted by the 25th ACM SIGKDD Conference on Knowledge Discovery and\n  Data Mining (KDD 2019 Research Track), code\n  available:https://github.com/allenjack/HGN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chronological order of user-item interactions is a key feature in many\nrecommender systems, where the items that users will interact may largely\ndepend on those items that users just accessed recently. However, with the\ntremendous increase of users and items, sequential recommender systems still\nface several challenging problems: (1) the hardness of modeling the long-term\nuser interests from sparse implicit feedback; (2) the difficulty of capturing\nthe short-term user interests given several items the user just accessed. To\ncope with these challenges, we propose a hierarchical gating network (HGN),\nintegrated with the Bayesian Personalized Ranking (BPR) to capture both the\nlong-term and short-term user interests. Our HGN consists of a feature gating\nmodule, an instance gating module, and an item-item product module. In\nparticular, our feature gating and instance gating modules select what item\nfeatures can be passed to the downstream layers from the feature and instance\nlevels, respectively. Our item-item product module explicitly captures the item\nrelations between the items that users accessed in the past and those items\nusers will access in the future. We extensively evaluate our model with several\nstate-of-the-art methods and different validation metrics on five real-world\ndatasets. The experimental results demonstrate the effectiveness of our model\non Top-N sequential recommendation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 16:02:41 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Ma", "Chen", ""], ["Kang", "Peng", ""], ["Liu", "Xue", ""]]}, {"id": "1906.09222", "submitter": "Sergio G\\'omez", "authors": "Alberto Fern\\'andez, Sergio G\\'omez", "title": "Versatile linkage: a family of space-conserving strategies for\n  agglomerative hierarchical clustering", "comments": "To appear in Journal of Classification. Software for Versatile\n  linkage available at http://deim.urv.cat/~sergio.gomez/multidendrograms.php", "journal-ref": "Journal of Classification 37 (2020) 584-597", "doi": "10.1007/s00357-019-09339-z", "report-no": null, "categories": "stat.ME cs.IR physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agglomerative hierarchical clustering can be implemented with several\nstrategies that differ in the way elements of a collection are grouped together\nto build a hierarchy of clusters. Here we introduce versatile linkage, a new\ninfinite system of agglomerative hierarchical clustering strategies based on\ngeneralized means, which go from single linkage to complete linkage, passing\nthrough arithmetic average linkage and other clustering methods yet unexplored\nsuch as geometric linkage and harmonic linkage. We compare the different\nclustering strategies in terms of cophenetic correlation, mean absolute error,\nand also tree balance and space distortion, two new measures proposed to\ndescribe hierarchical trees. Unlike the $\\beta$-flexible clustering system, we\nshow that the versatile linkage family is space-conserving.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 16:10:24 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Fern\u00e1ndez", "Alberto", ""], ["G\u00f3mez", "Sergio", ""]]}, {"id": "1906.09380", "submitter": "Omer Anjum", "authors": "Omer Anjum, Wen-Mei Hwu, Jinjun Xiong", "title": "A Retrospective Recount of Computer Architecture Research with a\n  Data-Driven Study of Over Four Decades of ISCA Publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study began with a research project, called DISCvR, conducted at the\nIBM-ILLINOIS Center for Cognitive Computing Systems Reseach. The goal of DISCvR\nwas to build a practical NLP based AI pipeline for document understanding which\nwill help us better understand the computation patterns and requirements of\nmodern computing systems. While building such a prototype, an early use case\ncame to us thanks to the 2017 IEEE/ACM International Symposium on\nMicroarchitecture (MICRO-50) Program Co-chairs, Drs. Hillery Hunter and Jaime\nMoreno. They asked us if we can perform some data-driven analysis of the past\n50 years of MICRO papers and show some interesting historical perspectives on\nMICRO's 50 years of publication. We learned two important lessons from that\nexperience: (1) building an AI solution to truly understand unstructured data\nis hard in spite of the many claimed successes in natural language\nunderstanding; and (2) providing a data-driven perspective on computer\narchitecture research is a very interesting and fun project. Recently we\ndecided to conduct a more thorough study based on all past papers of\nInternational Symposium on Computer Architecture (ISCA) from 1973 to 2018,\nwhich resulted this article. We recognize that we have just scratched the\nsurface of natural language understanding of unstructured data, and there are\nmany more aspects that we can improve. But even with our current study, we felt\nthere were enough interesting findings that may be worthwhile to share with the\ncommunity. Hence we decided to write this article to summarize our findings so\nfar based only on ISCA publications. Our hope is to generate further interests\nfrom the community in this topic, and we welcome collaboration from the\ncommunity to deepen our understanding both of the computer architecture\nresearch and of the challenges of NLP-based AI solutions.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 03:38:41 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Anjum", "Omer", ""], ["Hwu", "Wen-Mei", ""], ["Xiong", "Jinjun", ""]]}, {"id": "1906.09404", "submitter": "Yu Sun", "authors": "Chen Zheng, Yu Sun, Shengxian Wan, Dianhai Yu", "title": "RLTM: An Efficient Neural IR Framework for Long Documents", "comments": "Accepted by IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have achieved significant improvements in information\nretrieval (IR). However, most existing models are computational costly and can\nnot efficiently scale to long documents. This paper proposes a novel End-to-End\nneural ranking framework called Reinforced Long Text Matching (RLTM) which\nmatches a query with long documents efficiently and effectively. The core idea\nbehind the framework can be analogous to the human judgment process which\nfirstly locates the relevance parts quickly from the whole document and then\nmatches these parts with the query carefully to obtain the final label.\nFirstly, we select relevant sentences from the long documents by a coarse and\nefficient matching model. Secondly, we generate a relevance score by a more\nsophisticated matching model based on the sentence selected. The whole model is\ntrained jointly with reinforcement learning in a pairwise manner by maximizing\nthe expected score gaps between positive and negative examples. Experimental\nresults demonstrate that RLTM has greatly improved the efficiency and\neffectiveness of the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 07:32:15 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 01:33:15 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zheng", "Chen", ""], ["Sun", "Yu", ""], ["Wan", "Shengxian", ""], ["Yu", "Dianhai", ""]]}, {"id": "1906.09450", "submitter": "Mohamed Yahya", "authors": "Konstantine Arkoudas and Mohamed Yahya", "title": "Semantically Driven Auto-completion", "comments": "12 pages, under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bloomberg Terminal has been a leading source of financial data and\nanalytics for over 30 years. Through its thousands of functions, the Terminal\nallows its users to query and run analytics over a large array of data sources,\nincluding structured, semi-structured, and unstructured data; as well as plot\ncharts, set up event-driven alerts and triggers, create interactive maps,\nexchange information via instant and email-style messages, and so on. To\nimprove user experience, we have been building question answering systems that\ncan understand a wide range of natural language constructions for various\ndomains that are of fundamental interest to our users. Such natural language\ninterfaces, while exceedingly helpful to users, introduce a number of usability\nchallenges of their own. We tackle some of these challenges through\nauto-completion for query formulation. A distinguishing mark of our\nauto-complete systems is that they are based on and guided by corresponding\nsemantic parsing systems. We describe the auto-complete problem as it arises in\nthis setting, the novel algorithms that we use to solve it, and report on the\nquality of the results and the efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 14:02:14 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Arkoudas", "Konstantine", ""], ["Yahya", "Mohamed", ""]]}, {"id": "1906.09506", "submitter": "Weiping Song", "authors": "Weiping Song, Zhijian Duan, Ziqing Yang, Hao Zhu, Ming Zhang, Jian\n  Tang", "title": "Explainable Knowledge Graph-based Recommendation via Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies recommender systems with knowledge graphs, which can\neffectively address the problems of data sparsity and cold start. Recently, a\nvariety of methods have been developed for this problem, which generally try to\nlearn effective representations of users and items and then match items to\nusers according to their representations. Though these methods have been shown\nquite effective, they lack good explanations, which are critical to recommender\nsystems. In this paper, we take a different path and propose generating\nrecommendations by finding meaningful paths from users to items. Specifically,\nwe formulate the problem as a sequential decision process, where the target\nuser is defined as the initial state, and the walks on the graphs are defined\nas actions. We shape the rewards according to existing state-of-the-art methods\nand then train a policy function with policy gradient methods. Experimental\nresults on three real-world datasets show that our proposed method not only\nprovides effective recommendations but also offers good explanations.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 21:02:57 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Song", "Weiping", ""], ["Duan", "Zhijian", ""], ["Yang", "Ziqing", ""], ["Zhu", "Hao", ""], ["Zhang", "Ming", ""], ["Tang", "Jian", ""]]}, {"id": "1906.09543", "submitter": "Jun Jiang", "authors": "Jun Jiang, Shumao Pang, Xia Zhao, Liwei Wang, Andrew Wen, Hongfang\n  Liu, Qianjin Feng", "title": "Cross-lingual Data Transformation and Combination for Text\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Text classification is a fundamental task for text data mining. In order to\ntrain a generalizable model, a large volume of text must be collected. To\naddress data insufficiency, cross-lingual data may occasionally be necessary.\nCross-lingual data sources may however suffer from data incompatibility, as\ntext written in different languages can hold distinct word sequences and\nsemantic patterns. Machine translation and word embedding alignment provide an\neffective way to transform and combine data for cross-lingual data training. To\nthe best of our knowledge, there has been little work done on evaluating how\nthe methodology used to conduct semantic space transformation and data\ncombination affects the performance of classification models trained from\ncross-lingual resources. In this paper, we systematically evaluated the\nperformance of two commonly used CNN (Convolutional Neural Network) and RNN\n(Recurrent Neural Network) text classifiers with differing data transformation\nand combination strategies. Monolingual models were trained from English and\nFrench alongside their translated and aligned embeddings. Our results suggested\nthat semantic space transformation may conditionally promote the performance of\nmonolingual models. Bilingual models were trained from a combination of both\nEnglish and French. Our results indicate that a cross-lingual classification\nmodel can significantly benefit from cross-lingual data by learning from\ntranslated or aligned embedding spaces.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 02:56:02 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Jiang", "Jun", ""], ["Pang", "Shumao", ""], ["Zhao", "Xia", ""], ["Wang", "Liwei", ""], ["Wen", "Andrew", ""], ["Liu", "Hongfang", ""], ["Feng", "Qianjin", ""]]}, {"id": "1906.09882", "submitter": "Xiao Zhou", "authors": "Xiao Zhou, Danyang Liu, Jianxun Lian and Xing Xie", "title": "Collaborative Metric Learning with Memory Network for Multi-Relational\n  Recommender Systems", "comments": "7 pages, 4 figures, IJCAI19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of recommender systems in modern online platforms is inseparable\nfrom the accurate capture of users' personal tastes. In everyday life, large\namounts of user feedback data are created along with user-item online\ninteractions in a variety of ways, such as browsing, purchasing, and sharing.\nThese multiple types of user feedback provide us with tremendous opportunities\nto detect individuals' fine-grained preferences. Different from most existing\nrecommender systems that rely on a single type of feedback, we advocate\nincorporating multiple types of user-item interactions for better\nrecommendations. Based on the observation that the underlying spectrum of user\npreferences is reflected in various types of interactions with items and can be\nuncovered by latent relational learning in metric space, we propose a unified\nneural learning framework, named Multi-Relational Memory Network (MRMN). It can\nnot only model fine-grained user-item relations but also enable us to\ndiscriminate between feedback types in terms of the strength and diversity of\nuser preferences. Extensive experiments show that the proposed MRMN model\noutperforms competitive state-of-the-art algorithms in a wide range of\nscenarios, including e-commerce, local services, and job recommendations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 12:29:18 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Zhou", "Xiao", ""], ["Liu", "Danyang", ""], ["Lian", "Jianxun", ""], ["Xie", "Xing", ""]]}, {"id": "1906.10095", "submitter": "Cun Mu", "authors": "Cun Mu, Binwei Yang, Zheng Yan", "title": "An Empirical Comparison of FAISS and FENSHSES for Nearest Neighbor\n  Search in Hamming Space", "comments": "SIGIR eCom'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compare the performances of FAISS and FENSHSES on nearest\nneighbor search in Hamming space--a fundamental task with ubiquitous\napplications in nowadays eCommerce. Comprehensive evaluations are made in terms\nof indexing speed, search latency and RAM consumption. This comparison is\nconducted towards a better understanding on trade-offs between nearest neighbor\nsearch systems implemented in main memory and the ones implemented in secondary\nmemory, which is largely unaddressed in literature.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 17:24:11 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 20:46:33 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Mu", "Cun", ""], ["Yang", "Binwei", ""], ["Yan", "Zheng", ""]]}, {"id": "1906.10547", "submitter": "Federico Simonetta", "authors": "Federico Simonetta and Carlos Cancino-Chac\\'on and Stavros Ntalampiras\n  and Gerhard Widmer", "title": "A Convolutional Approach to Melody Line Identification in Symbolic\n  Scores", "comments": "In Proceedings of 20th International Society for Music Information\n  Retrieval Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In many musical traditions, the melody line is of primary significance in a\npiece. Human listeners can readily distinguish melodies from accompaniment;\nhowever, making this distinction given only the written score -- i.e. without\nlistening to the music performed -- can be a difficult task. Solving this task\nis of great importance for both Music Information Retrieval and musicological\napplications. In this paper, we propose an automated approach to identifying\nthe most salient melody line in a symbolic score. The backbone of the method\nconsists of a convolutional neural network (CNN) estimating the probability\nthat each note in the score (more precisely: each pixel in a piano roll\nencoding of the score) belongs to the melody line. We train and evaluate the\nmethod on various datasets, using manual annotations where available and solo\ninstrument parts where not. We also propose a method to inspect the CNN and to\nanalyze the influence exerted by notes on the prediction of other notes; this\nmethod can be applied whenever the output of a neural network has the same size\nas the input.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 13:07:08 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Simonetta", "Federico", ""], ["Cancino-Chac\u00f3n", "Carlos", ""], ["Ntalampiras", "Stavros", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1906.10607", "submitter": "Daniel Lee", "authors": "Rakesh Verma, Samaneh Karimi, Daniel Lee, Omprakash Gnawali, Azadeh\n  Shakery", "title": "Newswire versus Social Media for Disaster Response and Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a disaster situation, first responders need to quickly acquire situational\nawareness and prioritize response based on the need, resources available and\nimpact. Can they do this based on digital media such as Twitter alone, or\nnewswire alone, or some combination of the two? We examine this question in the\ncontext of the 2015 Nepal Earthquakes. Because newswire articles are longer,\neffective summaries can be helpful in saving time yet giving key content. We\nevaluate the effectiveness of several unsupervised summarization techniques in\ncapturing key content. We propose a method to link tweets written by the public\nand newswire articles, so that we can compare their key characteristics:\ntimeliness, whether tweets appear earlier than their corresponding news\narticles, and content. A novel idea is to view relevant tweets as a summary of\nthe matching news article and evaluate these summaries. Whenever possible, we\npresent both quantitative and qualitative evaluations. One of our main findings\nis that tweets and newswire articles provide complementary perspectives that\nform a holistic view of the disaster situation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 15:31:14 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Verma", "Rakesh", ""], ["Karimi", "Samaneh", ""], ["Lee", "Daniel", ""], ["Gnawali", "Omprakash", ""], ["Shakery", "Azadeh", ""]]}, {"id": "1906.10827", "submitter": "Mikhail Yurochkin", "authors": "Mikhail Yurochkin, Sebastian Claici, Edward Chien, Farzaneh Mirzazadeh\n  and Justin Solomon", "title": "Hierarchical Optimal Transport for Document Representation", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to measure similarity between documents enables intelligent\nsummarization and analysis of large corpora. Past distances between documents\nsuffer from either an inability to incorporate semantic similarities between\nwords or from scalability issues. As an alternative, we introduce hierarchical\noptimal transport as a meta-distance between documents, where documents are\nmodeled as distributions over topics, which themselves are modeled as\ndistributions over words. We then solve an optimal transport problem on the\nsmaller topic space to compute a similarity score. We give conditions on the\ntopics under which this construction defines a distance, and we relate it to\nthe word mover's distance. We evaluate our technique for k-NN classification\nand show better interpretability and scalability with comparable performance to\ncurrent methods at a fraction of the cost.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 03:26:23 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 22:07:08 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Yurochkin", "Mikhail", ""], ["Claici", "Sebastian", ""], ["Chien", "Edward", ""], ["Mirzazadeh", "Farzaneh", ""], ["Solomon", "Justin", ""]]}, {"id": "1906.10948", "submitter": "Pan Li", "authors": "Pan Li, Alexander Tuzhilin", "title": "Latent Multi-Criteria Ratings for Recommendations", "comments": "Accepted to RecSys19'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-criteria recommender systems have been increasingly valuable for\nhelping consumers identify the most relevant items based on different\ndimensions of user experiences. However, previously proposed multi-criteria\nmodels did not take into account latent embeddings generated from user reviews,\nwhich capture latent semantic relations between users and items. To address\nthese concerns, we utilize variational autoencoders to map user reviews into\nlatent embeddings, which are subsequently compressed into low-dimensional\ndiscrete vectors. The resulting compressed vectors constitute latent\nmulti-criteria ratings that we use for the recommendation purposes via standard\nmulti-criteria recommendation methods. We show that the proposed latent\nmulti-criteria rating approach outperforms several baselines significantly and\nconsistently across different datasets and performance evaluation measures.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 10:10:07 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Li", "Pan", ""], ["Tuzhilin", "Alexander", ""]]}, {"id": "1906.10996", "submitter": "Stefan Balke", "authors": "Stefan Balke, Matthias Dorfer, Luis Carvalho, Andreas Arzt, Gerhard\n  Widmer", "title": "Learning Soft-Attention Models for Tempo-invariant Audio-Sheet Music\n  Retrieval", "comments": "Accepted for publication at ISMIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Connecting large libraries of digitized audio recordings to their\ncorresponding sheet music images has long been a motivation for researchers to\ndevelop new cross-modal retrieval systems. In recent years, retrieval systems\nbased on embedding space learning with deep neural networks got a step closer\nto fulfilling this vision. However, global and local tempo deviations in the\nmusic recordings still require careful tuning of the amount of temporal context\ngiven to the system. In this paper, we address this problem by introducing an\nadditional soft-attention mechanism on the audio input. Quantitative and\nqualitative results on synthesized piano data indicate that this attention\nincreases the robustness of the retrieval system by focusing on different parts\nof the input representation based on the tempo of the audio. Encouraged by\nthese results, we argue for the potential of attention models as a very general\ntool for many MIR tasks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 11:52:49 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Balke", "Stefan", ""], ["Dorfer", "Matthias", ""], ["Carvalho", "Luis", ""], ["Arzt", "Andreas", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1906.11171", "submitter": "Xiaoyu Du", "authors": "Xiaoyu Du, Xiangnan He, Fajie Yuan, Jinhui Tang, Zhiguang Qin and\n  Tat-Seng Chua", "title": "Modeling Embedding Dimension Correlations via Convolutional Neural\n  Collaborative Filtering", "comments": "TOIS Minor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the core of recommender system, collaborative filtering (CF) models the\naffinity between a user and an item from historical user-item interactions,\nsuch as clicks, purchases, and so on. Benefited from the strong representation\npower, neural networks have recently revolutionized the recommendation\nresearch, setting up a new standard for CF. However, existing neural\nrecommender models do not explicitly consider the correlations among embedding\ndimensions, making them less effective in modeling the interaction function\nbetween users and items. In this work, we emphasize on modeling the\ncorrelations among embedding dimensions in neural networks to pursue higher\neffectiveness for CF. We propose a novel and general neural collaborative\nfiltering framework, namely ConvNCF, which is featured with two designs: 1)\napplying outer product on user embedding and item embedding to explicitly model\nthe pairwise correlations between embedding dimensions, and 2) employing\nconvolutional neural network above the outer product to learn the high-order\ncorrelations among embedding dimensions. To justify our proposal, we present\nthree instantiations of ConvNCF by using different inputs to represent a user\nand conduct experiments on two real-world datasets. Extensive results verify\nthe utility of modeling embedding dimension correlations with ConvNCF, which\noutperforms several competitive CF methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 15:36:37 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Du", "Xiaoyu", ""], ["He", "Xiangnan", ""], ["Yuan", "Fajie", ""], ["Tang", "Jinhui", ""], ["Qin", "Zhiguang", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1906.11285", "submitter": "Shameem Puthiya Parambath Mr.", "authors": "Shameem A Puthiya Parambath", "title": "Re-ranking Based Diversification: A Unifying View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze different re-ranking algorithms for diversification and show that\nmajority of them are based on maximizing submodular/modular functions from the\nclass of parameterized concave/linear over modular functions. We study the\noptimality of such algorithms in terms of the `total curvature'. We also show\nthat by adjusting the hyperparameter of the concave/linear composition to\ntrade-off relevance and diversity, if any, one is in fact tuning the `total\ncurvature' of the function for relevance-diversity trade-off.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 18:22:02 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Parambath", "Shameem A Puthiya", ""]]}, {"id": "1906.11290", "submitter": "Aurelio F. Bariviera", "authors": "Augusto Villa-Monte, Laura Lanzarini, Aurelio F. Bariviera, Jos\\'e A.\n  Olivas", "title": "User-Oriented Summaries Using a PSO Based Scoring Optimization Method", "comments": null, "journal-ref": "Entropy. 2019; 21(6):617", "doi": "10.3390/e21060617", "report-no": null, "categories": "cs.LG cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic text summarization tools have a great impact on many fields, such\nas medicine, law, and scientific research in general. As information overload\nincreases, automatic summaries allow handling the growing volume of documents,\nusually by assigning weights to the extracted phrases based on their\nsignificance in the expected summary. Obtaining the main contents of any given\ndocument in less time than it would take to do that manually is still an issue\nof interest. In~this~ article, a new method is presented that allows\nautomatically generating extractive summaries from documents by adequately\nweighting sentence scoring features using \\textit{Particle Swarm Optimization}.\nThe key feature of the proposed method is the identification of those features\nthat are closest to the criterion used by the individual when summarizing. The\nproposed method combines a binary representation and a continuous one, using an\noriginal variation of the technique developed by the authors of this paper. Our\npaper shows that using user labeled information in the training set helps to\nfind better metrics and weights. The empirical results yield an improved\naccuracy compared to previous methods used in this field\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 18:27:54 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Villa-Monte", "Augusto", ""], ["Lanzarini", "Laura", ""], ["Bariviera", "Aurelio F.", ""], ["Olivas", "Jos\u00e9 A.", ""]]}, {"id": "1906.11336", "submitter": "Pavlos Mitsoulis-Ntompos", "authors": "Pavlos Mitsoulis-Ntompos, Meisam Hejazinia, Serena Zhang, Travis Brady", "title": "A Simple Deep Personalized Recommendation System", "comments": "arXiv admin note: substantial text overlap with arXiv:1907.02822", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are critical tools to match listings and travelers in\ntwo-sided vacation rental marketplaces. Such systems require high capacity to\nextract user preferences for items from implicit signals at scale. To learn\nthose preferences, we propose a Simple Deep Personalized Recommendation System\nto compute travelers' conditional embeddings. Our method combines listing\nembeddings in a supervised structure to build short-term historical context to\npersonalize recommendations for travelers. Deployed in the production\nenvironment, this approach is computationally efficient and scalable, and\nallows us to capture non-linear dependencies. Our offline evaluation indicates\nthat traveler embeddings created using a Deep Average Network can improve the\nprecision of a downstream conversion prediction model by seven percent,\noutperforming more complex benchmark methods for online shopping experience\npersonalization.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 20:27:38 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 14:50:15 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Mitsoulis-Ntompos", "Pavlos", ""], ["Hejazinia", "Meisam", ""], ["Zhang", "Serena", ""], ["Brady", "Travis", ""]]}, {"id": "1906.11431", "submitter": "Ningxia Wang", "authors": "Li Chen, Ningxia Wang, Yonghua Yang, Keping Yang and Quan Yuan", "title": "User Validation of Recommendation Serendipity Metrics", "comments": "Some errors exist in this article, and another article with more\n  complete experiments and analysis is being prepared", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though it has been recognized that recommending serendipitous (i.e.,\nsurprising and relevant) items can be helpful for increasing users'\nsatisfaction and behavioral intention, how to measure serendipity in the\noffline environment is still an open issue. In recent years, a number of\nmetrics have been proposed, but most of them were based on researchers'\nassumptions due to the serendipity's subjective nature. In order to validate\nthese metrics' actual performance, we collected over 10,000 users' real\nfeedback data and compared with the metrics' results. It turns out the user\nprofile based metrics, especially content-based ones, perform better than those\nbased on item popularity, in terms of estimating the unexpectedness facet of\nrecommendations. Moreover, the full metrics, which involve the unexpectedness\ncomponent, relevance, timeliness, and user curiosity, can more accurately\nindicate the recommendation's serendipity degree, relative to those that just\ninvolve some of them. The application of these metrics to several recommender\nalgorithms further consolidates their practical usage, because the comparison\nresults are consistent with those from user evaluation. Thus, this work is\nconstructive for filling the gap between offline measurement and user study on\nrecommendation serendipity.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 04:25:53 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 08:26:38 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Chen", "Li", ""], ["Wang", "Ningxia", ""], ["Yang", "Yonghua", ""], ["Yang", "Keping", ""], ["Yuan", "Quan", ""]]}, {"id": "1906.11462", "submitter": "Xiangyu Zhao", "authors": "Xiangyu Zhao, Long Xia, Lixin Zou, Dawei Yin, Jiliang Tang", "title": "Toward Simulating Environments in Reinforcement Learning Based\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advances in Reinforcement Learning (RL), there have been\ntremendous interests in employing RL for recommender systems. However, directly\ntraining and evaluating a new RL-based recommendation algorithm needs to\ncollect users' real-time feedback in the real system, which is time and efforts\nconsuming and could negatively impact on users' experiences. Thus, it calls for\na user simulator that can mimic real users' behaviors where we can pre-train\nand evaluate new recommendation algorithms. Simulating users' behaviors in a\ndynamic system faces immense challenges -- (i) the underlining item\ndistribution is complex, and (ii) historical logs for each user are limited. In\nthis paper, we develop a user simulator base on Generative Adversarial Network\n(GAN). To be specific, the generator captures the underlining distribution of\nusers' historical logs and generates realistic logs that can be considered as\naugmentations of real logs; while the discriminator not only distinguishes real\nand fake logs but also predicts users' behaviors. The experimental results\nbased on real-world e-commerce data demonstrate the effectiveness of the\nproposed simulator.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 07:12:38 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 03:40:09 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Zhao", "Xiangyu", ""], ["Xia", "Long", ""], ["Zou", "Lixin", ""], ["Yin", "Dawei", ""], ["Tang", "Jiliang", ""]]}, {"id": "1906.11711", "submitter": "Himan Abdollahpouri", "authors": "Himan Abdollahpouri and Robin Burke", "title": "Reducing Popularity Bias in Recommendation Over Time", "comments": "arXiv admin note: substantial text overlap with arXiv:1901.07555", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recommendation algorithms suffer from popularity bias: a small number of\npopular items being recommended too frequently, while other items get\ninsufficient exposure. Research in this area so far has concentrated on a\none-shot representation of this bias, and on algorithms to improve the\ndiversity of individual recommendation lists. In this work, we take a\ntime-sensitive view of popularity bias, in which the algorithm assesses its\nlong-tail coverage at regular intervals, and compensates in the present moment\nfor omissions in the past. In particular, we present a temporal version of the\nwell-known xQuAD diversification algorithm adapted for long-tail\nrecommendation. Experimental results on two public datasets show that our\nmethod is more effective in terms of the long-tail coverage and accuracy\ntradeoff compared to some other existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 14:59:23 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Abdollahpouri", "Himan", ""], ["Burke", "Robin", ""]]}, {"id": "1906.11731", "submitter": "Mario Blaum", "authors": "Mario Blaum and Steven R. Hetzler", "title": "Array Codes with Local Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, array codes consist of $m\\times n$ arrays and in many cases, the\narrays satisfy parity constraints along lines of different slopes (generally\nwith a toroidal topology). Such codes are useful for RAID type of\narchitectures, since they allow to replace finite field operations by XORs. We\npresent expansions to traditional array codes of this type, like Blaum-Roth\n(BR) and extended EVENODD codes, by adding parity on columns. This vertical\nparity allows for recovery of one or more symbols in a column locally, i.e., by\nusing the remaining symbols in the column without invoking the rest of the\narray. Properties and applications of the new codes are discussed, in\nparticular to Locally Recoverable (LRC) codes.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 15:26:44 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 22:03:53 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Blaum", "Mario", ""], ["Hetzler", "Steven R.", ""]]}, {"id": "1906.11759", "submitter": "Francisco Afonso Raposo", "authors": "Francisco Afonso Raposo and David Martins de Matos and Ricardo Ribeiro", "title": "Low-dimensional Embodied Semantics for Music and Language", "comments": "6 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.IR cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied cognition states that semantics is encoded in the brain as firing\npatterns of neural circuits, which are learned according to the statistical\nstructure of human multimodal experience. However, each human brain is\nidiosyncratically biased, according to its subjective experience history,\nmaking this biological semantic machinery noisy with respect to the overall\nsemantics inherent to media artifacts, such as music and language excerpts. We\npropose to represent shared semantics using low-dimensional vector embeddings\nby jointly modeling several brains from human subjects. We show these\nunsupervised efficient representations outperform the original high-dimensional\nfMRI voxel spaces in proxy music genre and language topic classification tasks.\nWe further show that joint modeling of several subjects increases the semantic\nrichness of the learned latent vector spaces.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 11:09:01 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Raposo", "Francisco Afonso", ""], ["de Matos", "David Martins", ""], ["Ribeiro", "Ricardo", ""]]}, {"id": "1906.11761", "submitter": "Moritz Schubotz", "authors": "Norman Meuschke, Vincent Stange, Moritz Schubotz, Michael Karmer, Bela\n  Gipp", "title": "Improving Academic Plagiarism Detection for STEM Documents by Analyzing\n  Mathematical Content and Citations", "comments": "Proceedings of the ACM/IEEE-CS Joint Conference on Digital Libraries\n  (JCDL) 2019. The data and code of our study are openly available at\n  https://purl.org/hybridPD", "journal-ref": null, "doi": "10.1109/JCDL.2019.00026", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying academic plagiarism is a pressing task for educational and\nresearch institutions, publishers, and funding agencies. Current plagiarism\ndetection systems reliably find instances of copied and moderately reworded\ntext. However, reliably detecting concealed plagiarism, such as strong\nparaphrases, translations, and the reuse of nontextual content and ideas is an\nopen research problem. In this paper, we extend our prior research on analyzing\nmathematical content and academic citations. Both are promising approaches for\nimproving the detection of concealed academic plagiarism primarily in Science,\nTechnology, Engineering and Mathematics (STEM). We make the following\ncontributions: i) We present a two-stage detection process that combines\nsimilarity assessments of mathematical content, academic citations, and text.\nii) We introduce new similarity measures that consider the order of\nmathematical features and outperform the measures in our prior research. iii)\nWe compare the effectiveness of the math-based, citation-based, and text-based\ndetection approaches using confirmed cases of academic plagiarism. iv) We\ndemonstrate that the combined analysis of math-based and citation-based content\nfeatures allows identifying potentially suspicious cases in a collection of\n102K STEM documents. Overall, we show that analyzing the similarity of\nmathematical content and academic citations is a striking supplement for\nconventional text-based detection approaches for academic literature in the\nSTEM disciplines.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 16:07:47 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Meuschke", "Norman", ""], ["Stange", "Vincent", ""], ["Schubotz", "Moritz", ""], ["Karmer", "Michael", ""], ["Gipp", "Bela", ""]]}, {"id": "1906.11783", "submitter": "Jongpil Lee", "authors": "Jongpil Lee, Jiyoung Park, Juhan Nam", "title": "Representation Learning of Music Using Artist, Album, and Track\n  Information", "comments": "International Conference on Machine Learning (ICML) 2019, Machine\n  Learning for Music Discovery Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised music representation learning has been performed mainly using\nsemantic labels such as music genres. However, annotating music with semantic\nlabels requires time and cost. In this work, we investigate the use of factual\nmetadata such as artist, album, and track information, which are naturally\nannotated to songs, for supervised music representation learning. The results\nshow that each of the metadata has individual concept characteristics, and\nusing them jointly improves overall performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 16:42:40 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Lee", "Jongpil", ""], ["Park", "Jiyoung", ""], ["Nam", "Juhan", ""]]}, {"id": "1906.11901", "submitter": "Jean-Luc Meunier", "authors": "St\\'ephane Clinchant, Herv\\'e D\\'ejean, Jean-Luc Meunier, Eva Lang,\n  Florian Kleber", "title": "Comparing Machine Learning Approaches for Table Recognition in\n  Historical Register Books", "comments": "DAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper experiments on Table Recognition in hand-written\nregistry books. We first explain how the problem of row and column detection is\nmodeled, and then compare two Machine Learning approaches (Conditional Random\nField and Graph Convolutional Network) for detecting these table elements.\nEvaluation was conducted on death records provided by the Archive of the\nDiocese of Passau. Both methods show similar results, a 89 F1 score, a quality\nwhich allows for Information Extraction. Software and dataset are open\nsource/data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 06:42:47 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Clinchant", "St\u00e9phane", ""], ["D\u00e9jean", "Herv\u00e9", ""], ["Meunier", "Jean-Luc", ""], ["Lang", "Eva", ""], ["Kleber", "Florian", ""]]}, {"id": "1906.12089", "submitter": "Nicolas Heist", "authors": "Nicolas Heist and Heiko Paulheim", "title": "Uncovering the Semantics of Wikipedia Categories", "comments": "Preprint of a research track paper at the International Semantic Web\n  Conference (ISWC) 2019, Auckland, NZ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wikipedia category graph serves as the taxonomic backbone for large-scale\nknowledge graphs like YAGO or Probase, and has been used extensively for tasks\nlike entity disambiguation or semantic similarity estimation. Wikipedia's\ncategories are a rich source of taxonomic as well as non-taxonomic information.\nThe category 'German science fiction writers', for example, encodes the type of\nits resources (Writer), as well as their nationality (German) and genre\n(Science Fiction). Several approaches in the literature make use of fractions\nof this encoded information without exploiting its full potential. In this\npaper, we introduce an approach for the discovery of category axioms that uses\ninformation from the category network, category instances, and their\nlexicalisations. With DBpedia as background knowledge, we discover 703k axioms\ncovering 502k of Wikipedia's categories and populate the DBpedia knowledge\ngraph with additional 4.4M relation assertions and 3.3M type assertions at more\nthan 87% and 90% precision, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 08:32:46 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Heist", "Nicolas", ""], ["Paulheim", "Heiko", ""]]}, {"id": "1906.12120", "submitter": "Loveperteek Singh", "authors": "Loveperteek Singh, Shreya Singh, Sagar Arora, Sumit Borar", "title": "One Embedding To Do Them All", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Online shopping caters to the needs of millions of users daily. Search,\nrecommendations, personalization have become essential building blocks for\nserving customer needs. Efficacy of such systems is dependent on a thorough\nunderstanding of products and their representation. Multiple information\nsources and data types provide a complete picture of the product on the\nplatform. While each of these tasks shares some common characteristics,\ntypically product embeddings are trained and used in isolation.\n  In this paper, we propose a framework to combine multiple data sources and\nlearn unified embeddings for products on our e-commerce platform. Our product\nembeddings are built from three types of data sources - catalog text data, a\nuser's clickstream session data and product images. We use various techniques\nlike denoising auto-encoders for text, Bayesian personalized ranking (BPR) for\nclickstream data, Siamese neural network architecture for image data and\ncombined ensemble over the above methods for unified embeddings. Further, we\ncompare and analyze the performance of these embeddings across three unrelated\nreal-world e-commerce tasks specifically checking product attribute coverage,\nfinding similar products and predicting returns. We show that unified product\nembeddings perform uniformly well across all these tasks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 10:08:13 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Singh", "Loveperteek", ""], ["Singh", "Shreya", ""], ["Arora", "Sagar", ""], ["Borar", "Sumit", ""]]}, {"id": "1906.12165", "submitter": "Zhu Zhang", "authors": "Zhu Zhang, Zhou Zhao, Zhijie Lin, Jingkuan Song and Deng Cai", "title": "Localizing Unseen Activities in Video via Image Query", "comments": "Accepted by IJCAI 2019 as a poster paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action localization in untrimmed videos is an important topic in the field of\nvideo understanding. However, existing action localization methods are\nrestricted to a pre-defined set of actions and cannot localize unseen\nactivities. Thus, we consider a new task to localize unseen activities in\nvideos via image queries, named Image-Based Activity Localization. This task\nfaces three inherent challenges: (1) how to eliminate the influence of\nsemantically inessential contents in image queries; (2) how to deal with the\nfuzzy localization of inaccurate image queries; (3) how to determine the\nprecise boundaries of target segments. We then propose a novel self-attention\ninteraction localizer to retrieve unseen activities in an end-to-end fashion.\nSpecifically, we first devise a region self-attention method with relative\nposition encoding to learn fine-grained image region representations. Then, we\nemploy a local transformer encoder to build multi-step fusion and reasoning of\nimage and video contents. We next adopt an order-sensitive localizer to\ndirectly retrieve the target segment. Furthermore, we construct a new dataset\nActivityIBAL by reorganizing the ActivityNet dataset. The extensive experiments\nshow the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 12:32:41 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Zhang", "Zhu", ""], ["Zhao", "Zhou", ""], ["Lin", "Zhijie", ""], ["Song", "Jingkuan", ""], ["Cai", "Deng", ""]]}, {"id": "1906.12321", "submitter": "Iman Tahamtan", "authors": "Iman Tahamtan, Javad Seif", "title": "The Online Resources Shared on Twitter About the #MeToo Movement: The\n  Pareto Principle", "comments": "2 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine the most influential resources shared on Twitter\nabout the #MeToo movement. We also examine whether a small proportion of domain\nnames and URLs (e.g. 20%) appear in a large number of tweets (e.g. 80%) that\ncontain #MeToo (known as the 80/20 rule or Pareto principle). R and Python were\nused to analyze the data. Results demonstrated that the most frequently shared\ndomains were twitter.com (47.20%), nytimes.com (4.42%) and youtube.com (3.69%).\nThe most frequently shared content was a recent poll which indicated \"men are\nafraid to mentor women after the #MeToo movement\". In accordance with the\nPareto principle, 8% of domain names accounted for 80% of the shared content on\nTwitter that contained #MeToo. This study provides a base for researchers who\nare interested in understanding what online resources people rely on when\nsharing information about online social movements (e.g. #MeToo).\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 23:10:53 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 14:11:46 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Tahamtan", "Iman", ""], ["Seif", "Javad", ""]]}, {"id": "1906.12348", "submitter": "Shubhra Kanti Karmaker Santu", "authors": "Lei Xu, Shubhra Kanti Karmaker Santu and Kalyan Veeramachaneni", "title": "MLFriend: Interactive Prediction Task Recommendation for Event-Driven\n  Time-Series Data", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most automation in machine learning focuses on model selection and hyper\nparameter tuning, and many overlook the challenge of automatically defining\npredictive tasks. We still heavily rely on human experts to define prediction\ntasks, and generate labels by aggregating raw data. In this paper, we tackle\nthe challenge of defining useful prediction problems on event-driven\ntime-series data. We introduce MLFriend to address this challenge. MLFriend\nfirst generates all possible prediction tasks under a predefined space, then\ninteracts with a data scientist to learn the context of the data and recommend\ngood prediction tasks from all the tasks in the space. We evaluate our system\non three different datasets and generate a total of 2885 prediction tasks and\nsolve them. Out of these 722 were deemed useful by expert data scientists. We\nalso show that an automatic prediction task discovery system is able to\nidentify top 10 tasks that a user may like within a batch of 100 tasks.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 17:59:10 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Xu", "Lei", ""], ["Santu", "Shubhra Kanti Karmaker", ""], ["Veeramachaneni", "Kalyan", ""]]}]