[{"id": "1703.00034", "submitter": "Fatemeh Vahedian", "authors": "Fatemeh Vahedian, Robin Burke and Bamshad Mobasher", "title": "Weighted Random Walk Sampling for Multi-Relational Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the information overloaded web, personalized recommender systems are\nessential tools to help users find most relevant information. The most\nheavily-used recommendation frameworks assume user interactions that are\ncharacterized by a single relation. However, for many tasks, such as\nrecommendation in social networks, user-item interactions must be modeled as a\ncomplex network of multiple relations, not only a single relation. Recently\nresearch on multi-relational factorization and hybrid recommender models has\nshown that using extended meta-paths to capture additional information about\nboth users and items in the network can enhance the accuracy of recommendations\nin such networks. Most of this work is focused on unweighted heterogeneous\nnetworks, and to apply these techniques, weighted relations must be simplified\ninto binary ones. However, information associated with weighted edges, such as\nuser ratings, which may be crucial for recommendation, are lost in such\nbinarization. In this paper, we explore a random walk sampling method in which\nthe frequency of edge sampling is a function of edge weight, and apply this\ngenerate extended meta-paths in weighted heterogeneous networks. With this\nsampling technique, we demonstrate improved performance on multiple data sets\nboth in terms of recommendation accuracy and model generation efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 19:35:12 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 19:07:21 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Vahedian", "Fatemeh", ""], ["Burke", "Robin", ""], ["Mobasher", "Bamshad", ""]]}, {"id": "1703.00304", "submitter": "Angelos Valsamis", "authors": "Angelos Valsamis, Alexandros Psychas, Fotis Aisopos, Andreas Menychtas\n  and Theodora Varvarigou", "title": "Second Screen User Profiling and Multi-level Smart Recommendations in\n  the context of Social TVs", "comments": "In: Wu TT., Gennari R., Huang YM., Xie H., Cao Y. (eds) Emerging\n  Technologies for Education. SETE 2016", "journal-ref": "Lecture Notes in Computer Science, vol 10108. Springer, Cham,\n  2017, pp 514-525", "doi": "10.1007/978-3-319-52836-6_55", "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of Social TV, the increasing popularity of first and second\nscreen users, interacting and posting content online, illustrates new business\nopportunities and related technical challenges, in order to enrich user\nexperience on such environments. SAM (Socializing Around Media) project uses\nSocial Media-connected infrastructure to deal with the aforementioned\nchallenges, providing intelligent user context management models and mechanisms\ncapturing social patterns, to apply collaborative filtering techniques and\npersonalized recommendations towards this direction. This paper presents the\nContext Management mechanism of SAM, running in a Social TV environment to\nprovide smart recommendations for first and second screen content. Work\npresented is evaluated using real movie rating dataset found online, to\nvalidate the SAM's approach in terms of effectiveness as well as efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 14:06:44 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Valsamis", "Angelos", ""], ["Psychas", "Alexandros", ""], ["Aisopos", "Fotis", ""], ["Menychtas", "Andreas", ""], ["Varvarigou", "Theodora", ""]]}, {"id": "1703.00397", "submitter": "Sampoorna Biswas", "authors": "Sampoorna Biswas, Laks V.S. Lakshmanan, Senjuti Basu Ray", "title": "Combating the Cold Start User Problem in Model Based Collaborative\n  Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For tackling the well known cold-start user problem in model-based\nrecommender systems, one approach is to recommend a few items to a cold-start\nuser and use the feedback to learn a profile. The learned profile can then be\nused to make good recommendations to the cold user. In the absence of a good\ninitial profile, the recommendations are like random probes, but if not chosen\njudiciously, both bad recommendations and too many recommendations may turn off\na user. We formalize the cold-start user problem by asking what are the $b$\nbest items we should recommend to a cold-start user, in order to learn her\nprofile most accurately, where $b$, a given budget, is typically a small\nnumber. We formalize the problem as an optimization problem and present\nmultiple non-trivial results, including NP-hardness as well as hardness of\napproximation. We furthermore show that the objective function, i.e., the least\nsquare error of the learned profile w.r.t. the true user profile, is neither\nsubmodular nor supermodular, suggesting efficient approximations are unlikely\nto exist. Finally, we discuss several scalable heuristic approaches for\nidentifying the $b$ best items to recommend to the user and experimentally\nevaluate their performance on 4 real datasets. Our experiments show that our\nproposed accelerated algorithms significantly outperform the prior art in\nrunnning time, while achieving similar error in the learned user profile as\nwell as in the rating predictions.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 03:06:09 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Biswas", "Sampoorna", ""], ["Lakshmanan", "Laks V. S.", ""], ["Ray", "Senjuti Basu", ""]]}, {"id": "1703.00440", "submitter": "Ke Li", "authors": "Ke Li and Jitendra Malik", "title": "Fast k-Nearest Neighbour Search via Prioritized DCI", "comments": "14 pages, 6 figures; International Conference on Machine Learning\n  (ICML), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most exact methods for k-nearest neighbour search suffer from the curse of\ndimensionality; that is, their query times exhibit exponential dependence on\neither the ambient or the intrinsic dimensionality. Dynamic Continuous Indexing\n(DCI) offers a promising way of circumventing the curse and successfully\nreduces the dependence of query time on intrinsic dimensionality from\nexponential to sublinear. In this paper, we propose a variant of DCI, which we\ncall Prioritized DCI, and show a remarkable improvement in the dependence of\nquery time on intrinsic dimensionality. In particular, a linear increase in\nintrinsic dimensionality, or equivalently, an exponential increase in the\nnumber of points near a query, can be mostly counteracted with just a linear\nincrease in space. We also demonstrate empirically that Prioritized DCI\nsignificantly outperforms prior methods. In particular, relative to\nLocality-Sensitive Hashing (LSH), Prioritized DCI reduces the number of\ndistance evaluations by a factor of 14 to 116 and the memory consumption by a\nfactor of 21.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 18:51:13 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 17:46:04 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Li", "Ke", ""], ["Malik", "Jitendra", ""]]}, {"id": "1703.00518", "submitter": "Aron Culotta", "authors": "Shreesh Kumara Bhat and Aron Culotta", "title": "Identifying leading indicators of product recalls from online reviews\n  using positive unlabeled learning and domain adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumer protection agencies are charged with safeguarding the public from\nhazardous products, but the thousands of products under their jurisdiction make\nit challenging to identify and respond to consumer complaints quickly. From the\nconsumer's perspective, online reviews can provide evidence of product defects,\nbut manually sifting through hundreds of reviews is not always feasible. In\nthis paper, we propose a system to mine Amazon.com reviews to identify products\nthat may pose safety or health hazards. Since labeled data for this task are\nscarce, our approach combines positive unlabeled learning with domain\nadaptation to train a classifier from consumer complaints submitted to the U.S.\nConsumer Product Safety Commission. On a validation set of manually annotated\nAmazon product reviews, we find that our approach results in an absolute F1\nscore improvement of 8% over the best competing baseline. Furthermore, we apply\nthe classifier to Amazon reviews of known recalled products; the classifier\nidentifies reviews reporting safety hazards prior to the recall date for 45% of\nthe products. This suggests that the system may be able to provide an early\nwarning system to alert consumers to hazardous products before an official\nrecall is announced.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 21:34:18 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Bhat", "Shreesh Kumara", ""], ["Culotta", "Aron", ""]]}, {"id": "1703.00565", "submitter": "Jason Kessler", "authors": "Jason S. Kessler", "title": "Scattertext: a Browser-Based Tool for Visualizing how Corpora Differ", "comments": "ACL 2017 Demos. 6 pages, 5 figures. See the Githup repo\n  https://github.com/JasonKessler/scattertext for source code and documentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scattertext is an open source tool for visualizing linguistic variation\nbetween document categories in a language-independent way. The tool presents a\nscatterplot, where each axis corresponds to the rank-frequency a term occurs in\na category of documents. Through a tie-breaking strategy, the tool is able to\ndisplay thousands of visible term-representing points and find space to legibly\nlabel hundreds of them. Scattertext also lends itself to a query-based\nvisualization of how the use of terms with similar embeddings differs between\ndocument categories, as well as a visualization for comparing the importance\nscores of bag-of-words features to univariate metrics.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 00:48:15 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 19:15:04 GMT"}, {"version": "v3", "created": "Thu, 20 Apr 2017 21:39:34 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Kessler", "Jason S.", ""]]}, {"id": "1703.00948", "submitter": "Preeti Bhargava", "authors": "Nemanja Spasojevic, Preeti Bhargava, Guoning Hu", "title": "DAWT: Densely Annotated Wikipedia Texts across multiple languages", "comments": "8 pages, 3 figures, 7 tables, WWW2017, WWW 2017 Companion proceedings", "journal-ref": null, "doi": "10.1145/3041021.3053367", "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we open up the DAWT dataset - Densely Annotated Wikipedia Texts\nacross multiple languages. The annotations include labeled text mentions\nmapping to entities (represented by their Freebase machine ids) as well as the\ntype of the entity. The data set contains total of 13.6M articles, 5.0B tokens,\n13.8M mention entity co-occurrences. DAWT contains 4.8 times more anchor text\nto entity links than originally present in the Wikipedia markup. Moreover, it\nspans several languages including English, Spanish, Italian, German, French and\nArabic. We also present the methodology used to generate the dataset which\nenriches Wikipedia markup in order to increase number of links. In addition to\nthe main dataset, we open up several derived datasets including mention entity\nco-occurrence counts and entity embeddings, as well as mappings between\nFreebase ids and Wikidata item ids. We also discuss two applications of these\ndatasets and hope that opening them up would prove useful for the Natural\nLanguage Processing and Information Retrieval communities, as well as\nfacilitate multi-lingual research.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 20:55:20 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Spasojevic", "Nemanja", ""], ["Bhargava", "Preeti", ""], ["Hu", "Guoning", ""]]}, {"id": "1703.01049", "submitter": "Ayan Sinha", "authors": "Ayan Sinha, David F. Gleich and Karthik Ramani", "title": "Deconvolving Feedback Loops in Recommender Systems", "comments": "Neural Information Processing Systems, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is a popular technique to infer users' preferences on\nnew content based on the collective information of all users preferences.\nRecommender systems then use this information to make personalized suggestions\nto users. When users accept these recommendations it creates a feedback loop in\nthe recommender system, and these loops iteratively influence the collaborative\nfiltering algorithm's predictions over time. We investigate whether it is\npossible to identify items affected by these feedback loops. We state\nsufficient assumptions to deconvolve the feedback loops while keeping the\ninverse solution tractable. We furthermore develop a metric to unravel the\nrecommender system's influence on the entire user-item rating matrix. We use\nthis metric on synthetic and real-world datasets to (1) identify the extent to\nwhich the recommender system affects the final rating matrix, (2) rank\nfrequently recommended items, and (3) distinguish whether a user's rated item\nwas recommended or an intrinsic preference. Our results indicate that it is\npossible to recover the ratings matrix of intrinsic user preferences using a\nsingle snapshot of the ratings matrix without any temporal information.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 06:27:52 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Sinha", "Ayan", ""], ["Gleich", "David F.", ""], ["Ramani", "Karthik", ""]]}, {"id": "1703.01093", "submitter": "Doaa Shawky Doaa Shawky", "authors": "Doaa M. Shawky", "title": "Employing Spectral Domain Features for Efficient Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) is a powerful recommender system that generates\na list of recommended items for an active user based on the ratings of similar\nusers. This paper presents a novel approach to CF by first finding the set of\nusers similar to the active user by adopting self-organizing maps (SOM),\nfollowed by k-means clustering. Then, the ratings for each item in the cluster\nclosest to the active user are mapped to the frequency domain using the\nDiscrete Fourier Transform (DFT). The power spectra of the mapped ratings are\ngenerated, and a new similarity measure based on the coherence of these power\nspectra is calculated. The proposed similarity measure is more time efficient\nthan current state-of-the-art measures. Moreover, it can capture the global\nsimilarity between the profiles of users. Experimental results show that the\nproposed approach overcomes the major problems in existing CF algorithms as\nfollows: First, it mitigates the scalability problem by creating clusters of\nsimilar users and applying the time-efficient similarity measure. Second, its\nfrequency-based similarity measure is less sensitive to sparsity problems\nbecause the DFT performs efficiently even with sparse data. Third, it\noutperforms standard similarity measures in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 09:53:58 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Shawky", "Doaa M.", ""]]}, {"id": "1703.01760", "submitter": "Yiteng Pan", "authors": "Yiteng Pan, Fazhi He, Haiping Yu", "title": "A Correlative Denoising Autoencoder to Model Social Influence for Top-N\n  Recommender System", "comments": "Accepted by Frontiers of Computer Science", "journal-ref": null, "doi": "10.1007/s11704-019-8123-3", "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there are numerous works been proposed to leverage the\ntechniques of deep learning to improve social-aware recommendation performance.\nIn most cases, it requires a larger number of data to train a robust deep\nlearning model, which contains a lot of parameters to fit training data.\nHowever, both data of user ratings and social networks are facing critical\nsparse problem, which makes it not easy to train a robust deep neural network\nmodel. Towards this problem, we propose a novel Correlative Denoising\nAutoencoder (CoDAE) method by taking correlations between users with multiple\nroles into account to learn robust representations from sparse inputs of\nratings and social networks for recommendation. We develop the CoDAE model by\nutilizing three separated autoencoders to learn user features with roles of\nrater, truster and trustee, respectively. Especially, on account of that each\ninput unit of user vectors with roles of truster and trustee is corresponding\nto a particular user, we propose to utilize shared parameters to learn common\ninformation of the units that corresponding to same users. Moreover, we propose\na related regularization term to learn correlations between user features that\nlearnt by the three subnetworks of CoDAE model. We further conduct a series of\nexperiments to evaluate the proposed method on two public datasets for Top-N\nrecommendation task. The experimental results demonstrate that the proposed\nmodel outperforms state-of-the-art algorithms on rank-sensitive metrics of MAP\nand NDCG.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 08:35:58 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 02:10:39 GMT"}, {"version": "v3", "created": "Thu, 5 Dec 2019 06:26:13 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Pan", "Yiteng", ""], ["He", "Fazhi", ""], ["Yu", "Haiping", ""]]}, {"id": "1703.01900", "submitter": "Jipeng Qiang", "authors": "Jipeng Qiang and Wei Ding and John Quackenbush and Ping Chen", "title": "Network-based Distance Metric with Application to Discover Disease\n  Subtypes in Cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.IR q-bio.GN q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While we once thought of cancer as single monolithic diseases affecting a\nspecific organ site, we now understand that there are many subtypes of cancer\ndefined by unique patterns of gene mutations. These gene mutational data, which\ncan be more reliably obtained than gene expression data, help to determine how\nthe subtypes develop, evolve, and respond to therapies. Different from dense\ncontinuous-value gene expression data, which most existing cancer subtype\ndiscovery algorithms use, somatic mutational data are extremely sparse and\nheterogeneous, because there are less than 0.5\\% mutated genes in discrete\nvalue 1/0 out of 20,000 human protein-coding genes, and identical mutated genes\nare rarely shared by cancer patients.\n  Our focus is to search for cancer subtypes from extremely sparse and high\ndimensional gene mutational data in discrete 1 and 0 values using unsupervised\nlearning. We propose a new network-based distance metric. We project cancer\npatients' mutational profile into their gene network structure and measure the\ndistance between two patients using the similarity between genes and between\nthe gene vertexes of the patients in the network. Experimental results in\nsynthetic data and real-world data show that our approach outperforms the top\ncompetitors in cancer subtype discovery. Furthermore, our approach can identify\ncancer subtypes that cannot be detected by other clustering algorithms in real\ncancer data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 02:09:50 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Qiang", "Jipeng", ""], ["Ding", "Wei", ""], ["Quackenbush", "John", ""], ["Chen", "Ping", ""]]}, {"id": "1703.02314", "submitter": "Duanbing Chen", "authors": "Liang Yin, Li-Chen Shi, Jun-Yan Zhao, Song-Yang Du, Wen-Bo Xie,\n  Duan-Bing Chen", "title": "Heterogeneous information network model for equipment-standard system", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2017.08.055", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Entity information network is used to describe structural relationships\nbetween entities. Taking advantage of its extension and heterogeneity, entity\ninformation network is more and more widely applied to relationship modeling.\nRecent years, lots of researches about entity information network modeling have\nbeen proposed, while seldom of them concentrate on equipment-standard system\nwith properties of multi-layer, multi-dimension and multi-scale. In order to\nefficiently deal with some complex issues in equipment-standard system such as\nstandard revising, standard controlling, and production designing, a\nheterogeneous information network model for equipment-standard system is\nproposed in this paper. Three types of entities and six types of relationships\nare considered in the proposed model. Correspondingly, several different\nsimilarity-measuring methods are used in the modeling process. The experiments\nshow that the heterogeneous information network model established in this paper\ncan reflect relationships between entities accurately. Meanwhile, the modeling\nprocess has a good performance on time consumption.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 10:29:52 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Yin", "Liang", ""], ["Shi", "Li-Chen", ""], ["Zhao", "Jun-Yan", ""], ["Du", "Song-Yang", ""], ["Xie", "Wen-Bo", ""], ["Chen", "Duan-Bing", ""]]}, {"id": "1703.02504", "submitter": "Martin Jaggi", "authors": "Jan Deriu, Aurelien Lucchi, Valeria De Luca, Aliaksei Severyn, Simon\n  M\\\"uller, Mark Cieliebak, Thomas Hofmann, Martin Jaggi", "title": "Leveraging Large Amounts of Weakly Supervised Data for Multi-Language\n  Sentiment Classification", "comments": "appearing at WWW 2017 - 26th International World Wide Web Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel approach for multi-lingual sentiment\nclassification in short texts. This is a challenging task as the amount of\ntraining data in languages other than English is very limited. Previously\nproposed multi-lingual approaches typically require to establish a\ncorrespondence to English for which powerful classifiers are already available.\nIn contrast, our method does not require such supervision. We leverage large\namounts of weakly-supervised data in various languages to train a multi-layer\nconvolutional network and demonstrate the importance of using pre-training of\nsuch networks. We thoroughly evaluate our approach on various multi-lingual\ndatasets, including the recent SemEval-2016 sentiment prediction benchmark\n(Task 4), where we achieved state-of-the-art performance. We also compare the\nperformance of our model trained individually for each language to a variant\ntrained for all languages at once. We show that the latter model reaches\nslightly worse - but still acceptable - performance when compared to the single\nlanguage model, while benefiting from better generalization properties across\nlanguages.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 18:15:57 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Deriu", "Jan", ""], ["Lucchi", "Aurelien", ""], ["De Luca", "Valeria", ""], ["Severyn", "Aliaksei", ""], ["M\u00fcller", "Simon", ""], ["Cieliebak", "Mark", ""], ["Hofmann", "Thomas", ""], ["Jaggi", "Martin", ""]]}, {"id": "1703.02507", "submitter": "Martin Jaggi", "authors": "Matteo Pagliardini, Prakhar Gupta, Martin Jaggi", "title": "Unsupervised Learning of Sentence Embeddings using Compositional n-Gram\n  Features", "comments": "NAACL 2018", "journal-ref": "NAACL 2018 - Conference of the North American Chapter of the\n  Association for Computational Linguistics, pages 528-540", "doi": "10.18653/v1/N18-1049", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent tremendous success of unsupervised word embeddings in a multitude\nof applications raises the obvious question if similar methods could be derived\nto improve embeddings (i.e. semantic representations) of word sequences as\nwell. We present a simple but efficient unsupervised objective to train\ndistributed representations of sentences. Our method outperforms the\nstate-of-the-art unsupervised models on most benchmark tasks, highlighting the\nrobustness of the produced general-purpose sentence embeddings.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 18:19:11 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 18:05:48 GMT"}, {"version": "v3", "created": "Fri, 28 Dec 2018 15:12:58 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Pagliardini", "Matteo", ""], ["Gupta", "Prakhar", ""], ["Jaggi", "Martin", ""]]}, {"id": "1703.02596", "submitter": "Benjamin Chamberlain", "authors": "Benjamin Paul Chamberlain, Angelo Cardoso, C.H. Bryan Liu, Roberto\n  Pagliari, Marc Peter Deisenroth", "title": "Customer Lifetime Value Prediction Using Embeddings", "comments": "10 pages, 11 figures", "journal-ref": "Proceedings of the 23rd ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining Pages 1753-1762, 2017", "doi": "10.1145/3097983.3098123", "report-no": null, "categories": "cs.LG cs.CY cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the Customer LifeTime Value (CLTV) prediction system deployed at\nASOS.com, a global online fashion retailer. CLTV prediction is an important\nproblem in e-commerce where an accurate estimate of future value allows\nretailers to effectively allocate marketing spend, identify and nurture high\nvalue customers and mitigate exposure to losses. The system at ASOS provides\ndaily estimates of the future value of every customer and is one of the\ncornerstones of the personalised shopping experience. The state of the art in\nthis domain uses large numbers of handcrafted features and ensemble regressors\nto forecast value, predict churn and evaluate customer loyalty. Recently,\ndomains including language, vision and speech have shown dramatic advances by\nreplacing handcrafted features with features that are learned automatically\nfrom data. We detail the system deployed at ASOS and show that learning feature\nrepresentations is a promising extension to the state of the art in CLTV\nmodelling. We propose a novel way to generate embeddings of customers, which\naddresses the issue of the ever changing product catalogue and obtain a\nsignificant improvement over an exhaustive set of handcrafted features.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 21:18:11 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 12:20:06 GMT"}, {"version": "v3", "created": "Thu, 6 Jul 2017 16:40:44 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Chamberlain", "Benjamin Paul", ""], ["Cardoso", "Angelo", ""], ["Liu", "C. H. Bryan", ""], ["Pagliari", "Roberto", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "1703.02625", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed, Nick Duffield, Theodore Willke, Ryan A. Rossi", "title": "On Sampling from Massive Graph Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DS cs.IR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Graph Priority Sampling (GPS), a new paradigm for order-based\nreservoir sampling from massive streams of graph edges. GPS provides a general\nway to weight edge sampling according to auxiliary and/or size variables so as\nto accomplish various estimation goals of graph properties. In the context of\nsubgraph counting, we show how edge sampling weights can be chosen so as to\nminimize the estimation variance of counts of specified sets of subgraphs. In\ndistinction with many prior graph sampling schemes, GPS separates the functions\nof edge sampling and subgraph estimation. We propose two estimation frameworks:\n(1) Post-Stream estimation, to allow GPS to construct a reference sample of\nedges to support retrospective graph queries, and (2) In-Stream estimation, to\nallow GPS to obtain lower variance estimates by incrementally updating the\nsubgraph count estimates during stream processing. Unbiasedness of subgraph\nestimators is established through a new Martingale formulation of graph stream\norder sampling, which shows that subgraph estimators, written as a product of\nconstituent edge estimators are unbiased, even when computed at different\npoints in the stream. The separation of estimation and sampling enables\nsignificant resource savings relative to previous work. We illustrate our\nframework with applications to triangle and wedge counting. We perform a\nlarge-scale experimental study on real-world graphs from various domains and\ntypes. GPS achieves high accuracy with less than 1% error for triangle and\nwedge counting, while storing a small fraction of the graph with average update\ntimes of a few microseconds per edge. Notably, for a large Twitter graph with\nmore than 260M edges, GPS accurately estimates triangle counts with less than\n1% error, while storing only 40K edges.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 22:18:35 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Duffield", "Nick", ""], ["Willke", "Theodore", ""], ["Rossi", "Ryan A.", ""]]}, {"id": "1703.02819", "submitter": "Dmitry Ignatov", "authors": "Dmitry I. Ignatov", "title": "Introduction to Formal Concept Analysis and Its Applications in\n  Information Retrieval and Related Fields", "comments": null, "journal-ref": "RuSSIR 2014, Nizhniy Novgorod, Russia, CCIS vol. 505, Springer\n  42-141", "doi": "10.1007/978-3-319-25485-2_3", "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a tutorial on Formal Concept Analysis (FCA) and its\napplications. FCA is an applied branch of Lattice Theory, a mathematical\ndiscipline which enables formalisation of concepts as basic units of human\nthinking and analysing data in the object-attribute form. Originated in early\n80s, during the last three decades, it became a popular human-centred tool for\nknowledge representation and data analysis with numerous applications. Since\nthe tutorial was specially prepared for RuSSIR 2014, the covered FCA topics\ninclude Information Retrieval with a focus on visualisation aspects, Machine\nLearning, Data Mining and Knowledge Discovery, Text Mining and several others.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 12:53:21 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Ignatov", "Dmitry I.", ""]]}, {"id": "1703.02898", "submitter": "Biswa Sengupta", "authors": "B Sengupta, E Vazquez, M Sasdelli, Y Qian, M Peniak, L Netherton and G\n  Delfino", "title": "Large-scale image analysis using docker sandboxing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of specialized hardware such as Graphics Processing Units\n(GPUs), large scale image localization, classification and retrieval have seen\nincreased prevalence. Designing scalable software architecture that co-evolves\nwith such specialized hardware is a challenge in the commercial setting. In\nthis paper, we describe one such architecture (\\textit{Cortexica}) that\nleverages scalability of GPUs and sandboxing offered by docker containers. This\nallows for the flexibility of mixing different computer architectures as well\nas computational algorithms with the security of a trusted environment. We\nillustrate the utility of this framework in a commercial setting i.e.,\nsearching for multiple products in an image by combining image localisation and\nretrieval.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 09:40:48 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Sengupta", "B", ""], ["Vazquez", "E", ""], ["Sasdelli", "M", ""], ["Qian", "Y", ""], ["Peniak", "M", ""], ["Netherton", "L", ""], ["Delfino", "G", ""]]}, {"id": "1703.02915", "submitter": "Gourav Ganesh Shenoy", "authors": "Gourav G. Shenoy, Mangirish A. Wagle, Anwar Shaikh", "title": "Kaggle Competition: Expedia Hotel Recommendations", "comments": "12 pages, 8 tables, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With hundreds, even thousands, of hotels to choose from at every destination,\nit's difficult to know which will suit your personal preferences. Expedia wants\nto take the proverbial rabbit hole out of hotel search by providing\npersonalized hotel recommendations to their users. This is no small task for a\nsite with hundreds of millions of visitors every month! Currently, Expedia uses\nsearch parameters to adjust their hotel recommendations, but there aren't\nenough customer specific data to personalize them for each user. In this\nproject, we have taken up the challenge to contextualize customer data and\npredict the likelihood a user will stay at 100 different hotel groups.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 21:05:42 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Shenoy", "Gourav G.", ""], ["Wagle", "Mangirish A.", ""], ["Shaikh", "Anwar", ""]]}, {"id": "1703.03112", "submitter": "Shuai Zhang", "authors": "Shuai Zhang and Lina Yao", "title": "Dynamic Intention-Aware Recommendation System", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have been actively and extensively studied over past\ndecades. In the meanwhile, the boom of Big Data is driving fundamental changes\nin the development of recommender systems. In this paper, we propose a dynamic\nintention-aware recommender system to better facilitate users to find desirable\nproducts and services. Compare to prior work, our proposal possesses the\nfollowing advantages: (1) it takes user intentions and demands into account\nthrough intention mining techniques. By unearthing user intentions from the\nhistorical user-item interactions, and various user digital traces harvested\nfrom social media and Internet of Things, it is capable of delivering more\nsatisfactory recommendations by leveraging rich online and offline user data;\n(2) it embraces the benefits of embedding heterogeneous source information and\nshared representations of multiple domains to provide accurate and effective\nrecommendations comprehensively; (3) it recommends products or services\nproactively and timely by capturing the dynamic influences, which can\nsignificantly reduce user involvements and efforts.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 02:51:30 GMT"}, {"version": "v2", "created": "Fri, 10 Mar 2017 06:40:19 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Zhang", "Shuai", ""], ["Yao", "Lina", ""]]}, {"id": "1703.03385", "submitter": "Matthias Zeppelzauer", "authors": "J\\\"urgen Bernard and Christian Ritter and David Sessler and Matthias\n  Zeppelzauer and J\\\"orn Kohlhammer and Dieter Fellner", "title": "Visual-Interactive Similarity Search for Complex Objects by Example of\n  Soccer Player Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The definition of similarity is a key prerequisite when analyzing complex\ndata types in data mining, information retrieval, or machine learning. However,\nthe meaningful definition is often hampered by the complexity of data objects\nand particularly by different notions of subjective similarity latent in\ntargeted user groups. Taking the example of soccer players, we present a\nvisual-interactive system that learns users' mental models of similarity. In a\nvisual-interactive interface, users are able to label pairs of soccer players\nwith respect to their subjective notion of similarity. Our proposed similarity\nmodel automatically learns the respective concept of similarity using an active\nlearning strategy. A visual-interactive retrieval technique is provided to\nvalidate the model and to execute downstream retrieval tasks for soccer player\nanalysis. The applicability of the approach is demonstrated in different\nevaluation strategies, including usage scenarions and cross-validation tests.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 18:37:00 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Bernard", "J\u00fcrgen", ""], ["Ritter", "Christian", ""], ["Sessler", "David", ""], ["Zeppelzauer", "Matthias", ""], ["Kohlhammer", "J\u00f6rn", ""], ["Fellner", "Dieter", ""]]}, {"id": "1703.03609", "submitter": "Mostafa Salehi", "authors": "Saeedreza Shehnepoor, Mostafa Salehi, Reza Farahbakhsh and Noel Crespi", "title": "NetSpam: a Network-based Spam Detection Framework for Reviews in Online\n  Social Media", "comments": null, "journal-ref": null, "doi": "10.1109/TIFS.2017.2675361", "report-no": null, "categories": "cs.SI cs.CL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, a big part of people rely on available content in social media in\ntheir decisions (e.g. reviews and feedback on a topic or product). The\npossibility that anybody can leave a review provide a golden opportunity for\nspammers to write spam reviews about products and services for different\ninterests. Identifying these spammers and the spam content is a hot topic of\nresearch and although a considerable number of studies have been done recently\ntoward this end, but so far the methodologies put forth still barely detect\nspam reviews, and none of them show the importance of each extracted feature\ntype. In this study, we propose a novel framework, named NetSpam, which\nutilizes spam features for modeling review datasets as heterogeneous\ninformation networks to map spam detection procedure into a classification\nproblem in such networks. Using the importance of spam features help us to\nobtain better results in terms of different metrics experimented on real-world\nreview datasets from Yelp and Amazon websites. The results show that NetSpam\noutperforms the existing methods and among four categories of features;\nincluding review-behavioral, user-behavioral, reviewlinguistic,\nuser-linguistic, the first type of features performs better than the other\ncategories.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 10:17:27 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Shehnepoor", "Saeedreza", ""], ["Salehi", "Mostafa", ""], ["Farahbakhsh", "Reza", ""], ["Crespi", "Noel", ""]]}, {"id": "1703.03861", "submitter": "Amir Sarabadani", "authors": "Amir Sarabadani, Aaron Halfaker, Dario Taraborelli", "title": "Building automated vandalism detection tools for Wikidata", "comments": null, "journal-ref": null, "doi": "10.1145/3041021.3053366", "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wikidata, like Wikipedia, is a knowledge base that anyone can edit. This open\ncollaboration model is powerful in that it reduces barriers to participation\nand allows a large number of people to contribute. However, it exposes the\nknowledge base to the risk of vandalism and low-quality contributions. In this\nwork, we build on past work detecting vandalism in Wikipedia to detect\nvandalism in Wikidata. This work is novel in that identifying damaging changes\nin a structured knowledge-base requires substantially different feature\nengineering work than in a text-based wiki like Wikipedia. We also discuss the\nutility of these classifiers for reducing the overall workload of vandalism\npatrollers in Wikidata. We describe a machine classification strategy that is\nable to catch 89% of vandalism while reducing patrollers' workload by 98%, by\ndrawing lightly from contextual features of an edit and heavily from the\ncharacteristics of the user making the edit.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 22:41:51 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Sarabadani", "Amir", ""], ["Halfaker", "Aaron", ""], ["Taraborelli", "Dario", ""]]}, {"id": "1703.03923", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Juan-Manuel Torres-Moreno, Gerardo Sierra, Peter Peinl", "title": "A German Corpus for Text Similarity Detection Tasks", "comments": "1 figure; 13 pages", "journal-ref": "Preprint of International Journal of Computational Linguistics and\n  Applications, vol. 5, no. 2, 2014, pp. 9-24", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text similarity detection aims at measuring the degree of similarity between\na pair of texts. Corpora available for text similarity detection are designed\nto evaluate the algorithms to assess the paraphrase level among documents. In\nthis paper we present a textual German corpus for similarity detection. The\npurpose of this corpus is to automatically assess the similarity between a pair\nof texts and to evaluate different similarity measures, both for whole\ndocuments or for individual sentences. Therefore we have calculated several\nsimple measures on our corpus based on a library of similarity functions.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 07:35:28 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Torres-Moreno", "Juan-Manuel", ""], ["Sierra", "Gerardo", ""], ["Peinl", "Peter", ""]]}, {"id": "1703.04040", "submitter": "Francesco Silvestri", "authors": "Anne Driemel and Francesco Silvestri", "title": "Locality-sensitive hashing of curves", "comments": "Proc. of 33rd International Symposium on Computational Geometry\n  (SoCG), 2017", "journal-ref": null, "doi": "10.4230/LIPIcs.SoCG.2017.37", "report-no": null, "categories": "cs.CG cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study data structures for storing a set of polygonal curves in ${\\rm R}^d$\nsuch that, given a query curve, we can efficiently retrieve similar curves from\nthe set, where similarity is measured using the discrete Fr\\'echet distance or\nthe dynamic time warping distance. To this end we devise the first\nlocality-sensitive hashing schemes for these distance measures. A major\nchallenge is posed by the fact that these distance measures internally optimize\nthe alignment between the curves. We give solutions for different types of\nalignments including constrained and unconstrained versions. For unconstrained\nalignments, we improve over a result by Indyk from 2002 for short curves. Let\n$n$ be the number of input curves and let $m$ be the maximum complexity of a\ncurve in the input. In the particular case where $m \\leq \\frac{\\alpha}{4d} \\log\nn$, for some fixed $\\alpha>0$, our solutions imply an approximate near-neighbor\ndata structure for the discrete Fr\\'echet distance that uses space in\n$O(n^{1+\\alpha}\\log n)$ and achieves query time in $O(n^{\\alpha}\\log^2 n)$ and\nconstant approximation factor. Furthermore, our solutions provide a trade-off\nbetween approximation quality and computational performance: for any parameter\n$k \\in [m]$, we can give a data structure that uses space in $O(2^{2k}m^{k-1} n\n\\log n + nm)$, answers queries in $O( 2^{2k} m^{k}\\log n)$ time and achieves\napproximation factor in $O(m/k)$.\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 23:16:23 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Driemel", "Anne", ""], ["Silvestri", "Francesco", ""]]}, {"id": "1703.04215", "submitter": "Jinliang Xu", "authors": "Jinliang Xu, Shangguang Wang, Fangchun Yang, Jie Tang", "title": "Multiple User Context Inference by Fusing Data Sources", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in some equations and figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference of user context information, including user's gender, age, marital\nstatus, location and so on, has been proven to be valuable for building context\naware recommender system. However, prevalent existing studies on user context\ninference have two shortcommings: 1. focusing on only a single data source\n(e.g. Internet browsing logs, or mobile call records), and 2. ignoring the\ninterdependence of multiple user contexts (e.g. interdependence between age and\nmarital status), which have led to poor inference performance. To solve this\nproblem, in this paper, we first exploit tensor outer product to fuse multiple\ndata sources in the feature space to obtain an extensional user feature\nrepresentation. Following this, by taking this extensional user feature\nrepresentation as input, we propose a multiple attribute probabilistic model\ncalled MulAProM to infer user contexts that can take advantage of the\ninterdependence between them. Our study is based on large telecommunication\ndatasets from the local mobile operator of Shanghai, China, and consists of two\ndata sources, 4.6 million call detail records and 7.5 million data traffic\nrecords of 8,000 mobile users, collected in the course of six months. The\nexperimental results show that our model can outperform other models in terms\nof \\emph{recall}, \\emph{precision}, and the \\emph{F1-measure}.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 01:23:17 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 15:06:20 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Xu", "Jinliang", ""], ["Wang", "Shangguang", ""], ["Yang", "Fangchun", ""], ["Tang", "Jie", ""]]}, {"id": "1703.04216", "submitter": "Jinliang Xu", "authors": "Jinliang Xu, Shangguang Wang, Fangchun Yang, Rong N. Chang", "title": "Cognitive Inference of Demographic Data by User Ratings", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in some equations and figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive inference of user demographics, such as gender and age, plays an\nimportant role in creating user profiles for adjusting marketing strategies and\ngenerating personalized recommendations because user demographic data is\nusually not available due to data privacy concerns. At present, users can\nreadily express feedback regarding products or services that they have\npurchased. During this process, user demographics are concealed, but the data\nhas never yet been successfully utilized to contribute to the cognitive\ninference of user demographics. In this paper, we investigate the inference\npower of user ratings data, and propose a simple yet general cognitive\ninference model, called rating to profile (R2P), to infer user demographics\nfrom user provided ratings. In particular, the proposed R2P model can achieve\nthe following: 1. Correctly integrate user ratings into model training. 2.Infer\nmultiple demographic attributes of users simultaneously, capturing the\nunderlying relevance between different demographic attributes. 3. Train its two\ncomponents, i.e. feature extractor and classifier, in an integrated manner\nunder a supervised learning paradigm, which effectively helps to discover\nuseful hidden patterns from highly sparse ratings data. We introduce how to\nincorporate user ratings data into the research field of cognitive inference of\nuser demographic data, and detail the model development and optimization\nprocess for the proposed R2P. Extensive experiments are conducted on two\nreal-world ratings datasets against various compared state-of-the-art methods,\nand the results from multiple aspects demonstrate that our proposed R2P model\ncan significantly improve on the cognitive inference performance of user\ndemographic data.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 01:23:31 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 15:07:33 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Xu", "Jinliang", ""], ["Wang", "Shangguang", ""], ["Yang", "Fangchun", ""], ["Chang", "Rong N.", ""]]}, {"id": "1703.04247", "submitter": "Huifeng Guo", "authors": "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He", "title": "DeepFM: A Factorization-Machine based Neural Network for CTR Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning sophisticated feature interactions behind user behaviors is critical\nin maximizing CTR for recommender systems. Despite great progress, existing\nmethods seem to have a strong bias towards low- or high-order interactions, or\nrequire expertise feature engineering. In this paper, we show that it is\npossible to derive an end-to-end learning model that emphasizes both low- and\nhigh-order feature interactions. The proposed model, DeepFM, combines the power\nof factorization machines for recommendation and deep learning for feature\nlearning in a new neural network architecture. Compared to the latest Wide \\&\nDeep model from Google, DeepFM has a shared input to its \"wide\" and \"deep\"\nparts, with no need of feature engineering besides raw features. Comprehensive\nexperiments are conducted to demonstrate the effectiveness and efficiency of\nDeepFM over the existing models for CTR prediction, on both benchmark data and\ncommercial data.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 04:55:19 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Guo", "Huifeng", ""], ["Tang", "Ruiming", ""], ["Ye", "Yunming", ""], ["Li", "Zhenguo", ""], ["He", "Xiuqiang", ""]]}, {"id": "1703.04336", "submitter": "Sergiu Nisioi", "authors": "Anca Bucur and Sergiu Nisioi", "title": "A Visual Representation of Wittgenstein's Tractatus Logico-Philosophicus", "comments": "Workshop on Language Technology Resources and Tools for Digital\n  Humanities (LT4DH)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a data visualization method together with its\npotential usefulness in digital humanities and philosophy of language. We\ncompile a multilingual parallel corpus from different versions of\nWittgenstein's Tractatus Logico-Philosophicus, including the original in German\nand translations into English, Spanish, French, and Russian. Using this corpus,\nwe compute a similarity measure between propositions and render a visual\nnetwork of relations for different languages.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 11:19:56 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Bucur", "Anca", ""], ["Nisioi", "Sergiu", ""]]}, {"id": "1703.04498", "submitter": "Preeti Bhargava", "authors": "Preeti Bhargava, Nemanja Spasojevic, Guoning Hu", "title": "High-Throughput and Language-Agnostic Entity Disambiguation and Linking\n  on User Generated Data", "comments": "10 pages, 7 figures, 5 tables, WWW2017, Linked Data on the Web\n  workshop 2017, LDOW'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Entity Disambiguation and Linking (EDL) task matches entity mentions in\ntext to a unique Knowledge Base (KB) identifier such as a Wikipedia or Freebase\nid. It plays a critical role in the construction of a high quality information\nnetwork, and can be further leveraged for a variety of information retrieval\nand NLP tasks such as text categorization and document tagging. EDL is a\ncomplex and challenging problem due to ambiguity of the mentions and real world\ntext being multi-lingual. Moreover, EDL systems need to have high throughput\nand should be lightweight in order to scale to large datasets and run on\noff-the-shelf machines. More importantly, these systems need to be able to\nextract and disambiguate dense annotations from the data in order to enable an\nInformation Retrieval or Extraction task running on the data to be more\nefficient and accurate. In order to address all these challenges, we present\nthe Lithium EDL system and algorithm - a high-throughput, lightweight,\nlanguage-agnostic EDL system that extracts and correctly disambiguates 75% more\nentities than state-of-the-art EDL systems and is significantly faster than\nthem.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 17:34:18 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Bhargava", "Preeti", ""], ["Spasojevic", "Nemanja", ""], ["Hu", "Guoning", ""]]}, {"id": "1703.04854", "submitter": "Junhua He", "authors": "Junhua He, Hankz Hankui Zhuo and Jarvan Law", "title": "Distributed-Representation Based Hybrid Recommender System with Short\n  Item Descriptions", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) aims to build a model from users' past behaviors\nand/or similar decisions made by other users, and use the model to recommend\nitems for users. Despite of the success of previous collaborative filtering\napproaches, they are all based on the assumption that there are sufficient\nrating scores available for building high-quality recommendation models. In\nreal world applications, however, it is often difficult to collect sufficient\nrating scores, especially when new items are introduced into the system, which\nmakes the recommendation task challenging. We find that there are often \"short\"\ntexts describing features of items, based on which we can approximate the\nsimilarity of items and make recommendation together with rating scores. In\nthis paper we \"borrow\" the idea of vector representation of words to capture\nthe information of short texts and embed it into a matrix factorization\nframework. We empirically show that our approach is effective by comparing it\nwith state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 00:47:28 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["He", "Junhua", ""], ["Zhuo", "Hankz Hankui", ""], ["Law", "Jarvan", ""]]}, {"id": "1703.04914", "submitter": "Ikuya Yamada", "authors": "Ikuya Yamada, Motoki Sato, Hiroyuki Shindo", "title": "Ensemble of Neural Classifiers for Scoring Knowledge Base Triples", "comments": "WSDM Cup 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our approach for the triple scoring task at the WSDM Cup\n2017. The task required participants to assign a relevance score for each pair\nof entities and their types in a knowledge base in order to enhance the ranking\nresults in entity retrieval tasks. We propose an approach wherein the outputs\nof multiple neural network classifiers are combined using a supervised machine\nlearning model. The experimental results showed that our proposed method\nachieved the best performance in one out of three measures (i.e., Kendall's\ntau), and performed competitively in the other two measures (i.e., accuracy and\naverage score difference).\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 04:00:27 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 13:58:02 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Yamada", "Ikuya", ""], ["Sato", "Motoki", ""], ["Shindo", "Hiroyuki", ""]]}, {"id": "1703.04960", "submitter": "Liu Liu", "authors": "Liu Liu, Alireza Rahimpour, Ali Taalimi, Hairong Qi", "title": "End-to-end Binary Representation Learning via Direct Binary Embedding", "comments": "Accepted by ICIP'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning binary representation is essential to large-scale computer vision\ntasks. Most existing algorithms require a separate quantization constraint to\nlearn effective hashing functions. In this work, we present Direct Binary\nEmbedding (DBE), a simple yet very effective algorithm to learn binary\nrepresentation in an end-to-end fashion. By appending an ingeniously designed\nDBE layer to the deep convolutional neural network (DCNN), DBE learns binary\ncode directly from the continuous DBE layer activation without quantization\nerror. By employing the deep residual network (ResNet) as DCNN component, DBE\ncaptures rich semantics from images. Furthermore, in the effort of handling\nmultilabel images, we design a joint cross entropy loss that includes both\nsoftmax cross entropy and weighted binary cross entropy in consideration of the\ncorrelation and independence of labels, respectively. Extensive experiments\ndemonstrate the significant superiority of DBE over state-of-the-art methods on\ntasks of natural object recognition, image retrieval and image annotation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 06:41:31 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 04:44:40 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Liu", "Liu", ""], ["Rahimpour", "Alireza", ""], ["Taalimi", "Ali", ""], ["Qi", "Hairong", ""]]}, {"id": "1703.05123", "submitter": "Svitlana Vakulenko", "authors": "Svitlana Vakulenko, Lyndon Nixon, Mihai Lupu", "title": "Character-based Neural Embeddings for Tweet Clustering", "comments": "Accepted at the SocialNLP 2017 workshop held in conjunction with EACL\n  2017, April 3, 2017, Valencia, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how the performance of tweet clustering can be improved\nby leveraging character-based neural networks. The proposed approach overcomes\nthe limitations related to the vocabulary explosion in the word-based models\nand allows for the seamless processing of the multilingual content. Our\nevaluation results and code are available on-line at\nhttps://github.com/vendi12/tweet2vec_clustering\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 12:37:22 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 08:57:29 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["Nixon", "Lyndon", ""], ["Lupu", "Mihai", ""]]}, {"id": "1703.05706", "submitter": "Myungha Jang", "authors": "Myungha Jang, Jinho D. Choi, James Allan", "title": "Improving Document Clustering by Eliminating Unnatural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technical documents contain a fair amount of unnatural language, such as\ntables, formulas, pseudo-codes, etc. Unnatural language can be an important\nfactor of confusing existing NLP tools. This paper presents an effective method\nof distinguishing unnatural language from natural language, and evaluates the\nimpact of unnatural language detection on NLP tasks such as document\nclustering. We view this problem as an information extraction task and build a\nmulticlass classification model identifying unnatural language components into\nfour categories. First, we create a new annotated corpus by collecting slides\nand papers in various formats, PPT, PDF, and HTML, where unnatural language\ncomponents are annotated into four categories. We then explore features\navailable from plain text to build a statistical model that can handle any\nformat as long as it is converted into plain text. Our experiments show that\nremoving unnatural language components gives an absolute improvement in\ndocument clustering up to 15%. Our corpus and tool are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 16:32:06 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 04:03:36 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Jang", "Myungha", ""], ["Choi", "Jinho D.", ""], ["Allan", "James", ""]]}, {"id": "1703.05851", "submitter": "Yuanliang Meng", "authors": "Yuanliang Meng, Anna Rumshisky, Alexey Romanov", "title": "Temporal Information Extraction for Question Answering Using Syntactic\n  Dependencies in an LSTM-based Architecture", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to use a set of simple, uniform in architecture\nLSTM-based models to recover different kinds of temporal relations from text.\nUsing the shortest dependency path between entities as input, the same\narchitecture is used to extract intra-sentence, cross-sentence, and document\ncreation time relations. A \"double-checking\" technique reverses entity pairs in\nclassification, boosting the recall of positive cases and reducing\nmisclassifications between opposite classes. An efficient pruning algorithm\nresolves conflicts globally. Evaluated on QA-TempEval (SemEval2015 Task 5), our\nproposed technique outperforms state-of-the-art methods by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 00:02:42 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 21:38:08 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Meng", "Yuanliang", ""], ["Rumshisky", "Anna", ""], ["Romanov", "Alexey", ""]]}, {"id": "1703.05922", "submitter": "Xiao-Yang Liu", "authors": "Cai Fu, Chenchen Peng and Xiao-Yang Liu", "title": "Search Engine Drives the Evolution of Social Networks", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search engine is tightly coupled with social networks and is primarily\ndesigned for users to acquire interested information. Specifically, the search\nengine assists the information dissemination for social networks, i.e.,\nenabling users to access interested contents with keywords-searching and\npromoting the process of contents-transferring from the source users directly\nto potential interested users. Accompanying such processes, the social network\nevolves as new links emerge between users with common interests. However, there\nis no clear understanding of such a \"chicken-and-egg\" problem, namely, new\nlinks encourage more social interactions, and vice versa. In this paper, we aim\nto quantitatively characterize the social network evolution phenomenon driven\nby a search engine. First, we propose a search network model for social network\nevolution. Second, we adopt two performance metrics, namely, degree\ndistribution and network diameter. Theoretically, we prove that the degree\ndistribution follows an intensified power-law, and the network diameter\nshrinks. Third, we quantitatively show that the search engine accelerates the\nrumor propagation in social networks. Finally, based on four real-world data\nsets (i.e., CDBLP, Facebook, Weibo Tweets, P2P), we verify our theoretical\nfindings. Furthermore, we find that the search engine dramatically increases\nthe speed of rumor propagation.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 08:46:32 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Fu", "Cai", ""], ["Peng", "Chenchen", ""], ["Liu", "Xiao-Yang", ""]]}, {"id": "1703.06108", "submitter": "Nemanja Spasojevic", "authors": "Prantik Bhattacharyya, Nemanja Spasojevic", "title": "Global Entity Ranking Across Multiple Languages", "comments": "2 Pages, 1 Figure, 2 Tables, WWW2017 Companion, WWW 2017 Companion", "journal-ref": null, "doi": "10.1145/3041021.3054213", "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present work on building a global long-tailed ranking of entities across\nmultiple languages using Wikipedia and Freebase knowledge bases. We identify\nmultiple features and build a model to rank entities using a ground-truth\ndataset of more than 10 thousand labels. The final system ranks 27 million\nentities with 75% precision and 48% F1 score. We provide performance evaluation\nand empirical evidence of the quality of ranking across languages, and open the\nfinal ranked lists for future research.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 17:16:02 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Bhattacharyya", "Prantik", ""], ["Spasojevic", "Nemanja", ""]]}, {"id": "1703.06180", "submitter": "Tobias Schnabel", "authors": "Aman Agarwal, Soumya Basu, Tobias Schnabel, Thorsten Joachims", "title": "Effective Evaluation using Logged Bandit Feedback from Multiple Loggers", "comments": "KDD 2018", "journal-ref": null, "doi": "10.1145/3097983.3098155", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately evaluating new policies (e.g. ad-placement models, ranking\nfunctions, recommendation functions) is one of the key prerequisites for\nimproving interactive systems. While the conventional approach to evaluation\nrelies on online A/B tests, recent work has shown that counterfactual\nestimators can provide an inexpensive and fast alternative, since they can be\napplied offline using log data that was collected from a different policy\nfielded in the past. In this paper, we address the question of how to estimate\nthe performance of a new target policy when we have log data from multiple\nhistoric policies. This question is of great relevance in practice, since\npolicies get updated frequently in most online systems. We show that naively\ncombining data from multiple logging policies can be highly suboptimal. In\nparticular, we find that the standard Inverse Propensity Score (IPS) estimator\nsuffers especially when logging and target policies diverge -- to a point where\nthrowing away data improves the variance of the estimator. We therefore propose\ntwo alternative estimators which we characterize theoretically and compare\nexperimentally. We find that the new estimators can provide substantially\nimproved estimation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 19:29:36 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 11:52:23 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Agarwal", "Aman", ""], ["Basu", "Soumya", ""], ["Schnabel", "Tobias", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1703.06324", "submitter": "Biswa Sengupta", "authors": "B Sengupta and E Vasquez and Y Qian", "title": "Deep Tensor Encoding", "comments": "KDD Workshop on ML meets Fashion 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning an encoding of feature vectors in terms of an over-complete\ndictionary or a information geometric (Fisher vectors) construct is wide-spread\nin statistical signal processing and computer vision. In content based\ninformation retrieval using deep-learning classifiers, such encodings are\nlearnt on the flattened last layer, without adherence to the multi-linear\nstructure of the underlying feature tensor. We illustrate a variety of feature\nencodings incl. sparse dictionary coding and Fisher vectors along with\nproposing that a structured tensor factorization scheme enables us to perform\nretrieval that can be at par, in terms of average precision, with Fisher vector\nencoded image signatures. In short, we illustrate how structural constraints\nincrease retrieval fidelity.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 17:49:42 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 09:08:48 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Sengupta", "B", ""], ["Vasquez", "E", ""], ["Qian", "Y", ""]]}, {"id": "1703.06587", "submitter": "Han Tian", "authors": "Han Tian, Hankz Hankui Zhuo", "title": "Paper2vec: Citation-Context Based Document Distributed Representation\n  for Scholar Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the availability of references of research papers and the rich\ninformation contained in papers, various citation analysis approaches have been\nproposed to identify similar documents for scholar recommendation. Despite of\nthe success of previous approaches, they are, however, based on co-occurrence\nof items. Once there are no co-occurrence items available in documents, they\nwill not work well. Inspired by distributed representations of words in the\nliterature of natural language processing, we propose a novel approach to\nmeasuring the similarity of papers based on distributed representations learned\nfrom the citation context of papers. We view the set of papers as the\nvocabulary, define the weighted citation context of papers, and convert it to\nweight matrix similar to the word-word cooccurrence matrix in natural language\nprocessing. After that we explore a variant of matrix factorization approach to\ntrain distributed representations of papers on the matrix, and leverage the\ndistributed representations to measure similarities of papers. In the\nexperiment, we exhibit that our approach outperforms state-of-theart\ncitation-based approaches by 25%, and better than other distributed\nrepresentation based methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 03:53:48 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Tian", "Han", ""], ["Zhuo", "Hankz Hankui", ""]]}, {"id": "1703.06630", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Mohamed Morchid, Juan-Manuel Torres-Moreno, Richard Dufour, Javier\n  Ram\\'irez-Rodr\\'iguez, Georges Linar\\`es", "title": "Automatic Text Summarization Approaches to Speed up Topic Model Learning\n  Process", "comments": "16 pages, 4 tables, 8 figures", "journal-ref": "International Journal of Computational Linguistics and\n  Applications, 7(2):87-109, 2016", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of documents available into Internet moves each day up. For this\nreason, processing this amount of information effectively and expressibly\nbecomes a major concern for companies and scientists. Methods that represent a\ntextual document by a topic representation are widely used in Information\nRetrieval (IR) to process big data such as Wikipedia articles. One of the main\ndifficulty in using topic model on huge data collection is related to the\nmaterial resources (CPU time and memory) required for model estimate. To deal\nwith this issue, we propose to build topic spaces from summarized documents. In\nthis paper, we present a study of topic space representation in the context of\nbig data. The topic space representation behavior is analyzed on different\nlanguages. Experiments show that topic spaces estimated from text summaries are\nas relevant as those estimated from the complete documents. The real advantage\nof such an approach is the processing time gain: we showed that the processing\ntime can be drastically reduced using summarized documents (more than 60\\% in\ngeneral). This study finally points out the differences between thematic\nrepresentations of documents depending on the targeted languages such as\nEnglish or latin languages.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 08:19:43 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Morchid", "Mohamed", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["Dufour", "Richard", ""], ["Ram\u00edrez-Rodr\u00edguez", "Javier", ""], ["Linar\u00e8s", "Georges", ""]]}, {"id": "1703.07381", "submitter": "Vishal Jain", "authors": "Gagandeep Singh Narula, Vishal Jain", "title": "Improving Statistical Multimedia Information Retrieval Model by using\n  Ontology", "comments": null, "journal-ref": "International Journal of Computer Applications ISSN No 0975 8887\n  Volume 94 No 2, May 2014", "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical IR system that delivers and stores information is affected by\nproblem of matching between user query and available content on web. Use of\nOntology represents the extracted terms in form of network graph consisting of\nnodes, edges, index terms etc. The above mentioned IR approaches provide\nrelevance thus satisfying users query. The paper also emphasis on analyzing\nmultimedia documents and performs calculation for extracted terms using\ndifferent statistical formulas. The proposed model developed reduces semantic\ngap and satisfies user needs efficiently.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 18:29:05 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Narula", "Gagandeep Singh", ""], ["Jain", "Vishal", ""]]}, {"id": "1703.07384", "submitter": "Vishal Jain", "authors": "Vishal Jain and Dr. Mayank Singh", "title": "Ontology Based Pivoted normalization using Vector Based Approach for\n  information Retrieval", "comments": null, "journal-ref": "7th International Conference on Advanced Computing and\n  Communication Technologies, 16th November, 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proposed methodology is procedural i.e. it follows finite number of steps\nthat extracts relevant documents according to users query. It is based on\nprinciples of Data Mining for analyzing web data. Data Mining first adapts\nintegration of data to generate warehouse. Then, it extracts useful information\nwith the help of algorithm. The task of representing extracted documents is\ndone by using Vector Based Statistical Approach that represents each document\nin set of Terms.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 18:34:34 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Jain", "Vishal", ""], ["Singh", "Dr. Mayank", ""]]}, {"id": "1703.08013", "submitter": "Mao Tan", "authors": "Mao Tan, Si-Ping Yuan, Yong-Xin Su", "title": "Content-based similar document image retrieval using fusion of CNN\n  features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid increase of digitized document give birth to high demand of document\nimage retrieval. While conventional document image retrieval approaches depend\non complex OCR-based text recognition and text similarity detection, this paper\nproposes a new content-based approach, in which more attention is paid to\nfeatures extraction and fusion. In the proposed approach, multiple features of\ndocument images are extracted by different CNN models. After that, the\nextracted CNN features are reduced and fused into weighted average feature.\nFinally, the document images are ranked based on feature similarity to a\nprovided query image. Experimental procedure is performed on a group of\ndocument images that transformed from academic papers, which contain both\nEnglish and Chinese document, the results show that the proposed approach has\ngood ability to retrieve document images with similar text content, and the\nfusion of CNN features can effectively improve the retrieval accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 11:35:27 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 09:30:41 GMT"}, {"version": "v3", "created": "Fri, 1 Sep 2017 00:34:52 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Tan", "Mao", ""], ["Yuan", "Si-Ping", ""], ["Su", "Yong-Xin", ""]]}, {"id": "1703.08071", "submitter": "Manuel Sebastian Mariani", "authors": "Giacomo Vaccario, Matus Medo, Nicolas Wider, Manuel Sebastian Mariani", "title": "Quantifying and suppressing ranking bias in a large citation network", "comments": "Main text (pp. 1-12) and Appendices (pp. 13-17)", "journal-ref": "Journal of Informetrics 11, 766-782 (2017)", "doi": "10.1016/j.joi.2017.05.014", "report-no": null, "categories": "physics.soc-ph cs.DL cs.IR physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely recognized that citation counts for papers from different fields\ncannot be directly compared because different scientific fields adopt different\ncitation practices. Citation counts are also strongly biased by paper age since\nolder papers had more time to attract citations. Various procedures aim at\nsuppressing these biases and give rise to new normalized indicators, such as\nthe relative citation count. We use a large citation dataset from Microsoft\nAcademic Graph and a new statistical framework based on the Mahalanobis\ndistance to show that the rankings by well known indicators, including the\nrelative citation count and Google's PageRank score, are significantly biased\nby paper field and age. We propose a general normalization procedure motivated\nby the $z$-score which produces much less biased rankings when applied to\ncitation count and PageRank score.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 13:53:06 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Vaccario", "Giacomo", ""], ["Medo", "Matus", ""], ["Wider", "Nicolas", ""], ["Mariani", "Manuel Sebastian", ""]]}, {"id": "1703.08098", "submitter": "Dat Quoc Nguyen", "authors": "Dat Quoc Nguyen", "title": "A survey of embedding models of entities and relationships for knowledge\n  graph completion", "comments": "In Proceedings of the 14th Workshop on Graph-Based Natural Language\n  Processing (TextGraphs 2020); 16 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KGs) of real-world facts about entities and their\nrelationships are useful resources for a variety of natural language processing\ntasks. However, because knowledge graphs are typically incomplete, it is useful\nto perform knowledge graph completion or link prediction, i.e. predict whether\na relationship not in the knowledge graph is likely to be true. This paper\nserves as a comprehensive survey of embedding models of entities and\nrelationships for knowledge graph completion, summarizing up-to-date\nexperimental results on standard benchmark datasets and pointing out potential\nfuture research directions.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 15:15:26 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 15:28:08 GMT"}, {"version": "v3", "created": "Sat, 3 Feb 2018 04:39:45 GMT"}, {"version": "v4", "created": "Tue, 9 Apr 2019 02:26:26 GMT"}, {"version": "v5", "created": "Sat, 27 Apr 2019 13:33:30 GMT"}, {"version": "v6", "created": "Fri, 28 Feb 2020 07:06:36 GMT"}, {"version": "v7", "created": "Wed, 22 Apr 2020 11:58:35 GMT"}, {"version": "v8", "created": "Mon, 10 Aug 2020 08:35:07 GMT"}, {"version": "v9", "created": "Tue, 27 Oct 2020 04:11:25 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Nguyen", "Dat Quoc", ""]]}, {"id": "1703.08593", "submitter": "Roberto Camacho Barranco", "authors": "Roberto Camacho Barranco, Arnold P. Boedihardjo, M. Shahriar Hossain", "title": "Analyzing Evolving Stories in News Articles", "comments": "This is a pre-print of an article published in the International\n  Journal of Data Science and Analytics. The final authenticated version is\n  available online at: https://doi.org/10.1007/s41060-017-0091-9", "journal-ref": null, "doi": "10.1007/s41060-017-0091-9", "report-no": null, "categories": "cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an overwhelming number of news articles published every day around\nthe globe. Following the evolution of a news-story is a difficult task given\nthat there is no such mechanism available to track back in time to study the\ndiffusion of the relevant events in digital news feeds. The techniques\ndeveloped so far to extract meaningful information from a massive corpus rely\non similarity search, which results in a myopic loopback to the same topic\nwithout providing the needed insights to hypothesize the origin of a story that\nmay be completely different than the news today. In this paper, we present an\nalgorithm that mines historical data to detect the origin of an event, segments\nthe timeline into disjoint groups of coherent news articles, and outlines the\nmost important documents in a timeline with a soft probability to provide a\nbetter understanding of the evolution of a story. Qualitative and quantitative\napproaches to evaluate our framework demonstrate that our algorithm discovers\nstatistically significant and meaningful stories in reasonable time.\nAdditionally, a relevant case study on a set of news articles demonstrates that\nthe generated output of the algorithm holds the promise to aid prediction of\nfuture entities in a story.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 20:52:32 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 15:46:59 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Barranco", "Roberto Camacho", ""], ["Boedihardjo", "Arnold P.", ""], ["Hossain", "M. Shahriar", ""]]}, {"id": "1703.08855", "submitter": "Joeran Beel", "authors": "Stefan Langer and Joeran Beel", "title": "Apache Lucene as Content-Based-Filtering Recommender System: 3 Lessons\n  Learned", "comments": "Accepted for publication at the 5th International Workshop on\n  Bibliometric-enhanced Information Retrieval (BIR2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the past few years, we used Apache Lucene as recommendation frame-work in\nour scholarly-literature recommender system of the reference-management\nsoftware Docear. In this paper, we share three lessons learned from our work\nwith Lucene. First, recommendations with relevance scores below 0.025 tend to\nhave significantly lower click-through rates than recommendations with\nrelevance scores above 0.025. Second, by picking ten recommendations randomly\nfrom Lucene's top50 search results, click-through rate decreased by 15%,\ncompared to recommending the top10 results. Third, the number of returned\nsearch results tend to predict how high click-through rates will be: when\nLucene returns less than 1,000 search results, click-through rates tend to be\naround half as high as if 1,000+ results are returned.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 17:37:58 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Langer", "Stefan", ""], ["Beel", "Joeran", ""]]}, {"id": "1703.09108", "submitter": "Joeran Beel", "authors": "Joeran Beel and Akiko Aizawa and Corinna Breitinger and Bela Gipp", "title": "Mr. DLib: Recommendations-as-a-Service (RaaS) for Academia", "comments": "Accepted for publication at the JCDL conference 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Only few digital libraries and reference managers offer recommender systems,\nalthough such systems could assist users facing information overload. In this\npaper, we introduce Mr. DLib's recommendations-as-a-service, which allows third\nparties to easily integrate a recommender system into their products. We\nexplain the recommender approaches implemented in Mr. DLib (content-based\nfiltering among others), and present details on 57 million recommendations,\nwhich Mr. DLib delivered to its partner GESIS Sowiport. Finally, we outline our\nplans for future development, including integration into JabRef, establishing a\nliving lab, and providing personalized recommendations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 14:35:37 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 09:50:03 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Beel", "Joeran", ""], ["Aizawa", "Akiko", ""], ["Breitinger", "Corinna", ""], ["Gipp", "Bela", ""]]}, {"id": "1703.09109", "submitter": "Joeran Beel", "authors": "Joeran Beel", "title": "Towards Effective Research-Paper Recommender Systems and User Modeling\n  based on Mind Maps", "comments": "PhD Thesis, Otto-von-Guericke University Magdeburg, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While user-modeling and recommender systems successfully utilize items like\nemails, news, and movies, they widely neglect mind-maps as a source for user\nmodeling. We consider this a serious shortcoming since we assume user modeling\nbased on mind maps to be equally effective as user modeling based on other\nitems. Hence, millions of mind-mapping users could benefit from user-modeling\napplications such as recommender systems. The objective of this doctoral thesis\nis to develop an effective user-modeling approach based on mind maps. To\nachieve this objective, we integrate a recommender system in our mind-mapping\nand reference-management software Docear. The recommender system builds user\nmodels based on the mind maps, and recommends research papers based on the user\nmodels. As part of our research, we identify several variables relating to\nmind-map-based user modeling, and evaluate the variables' impact on\nuser-modeling effectiveness with an offline evaluation, a user study, and an\nonline evaluation based on 430,893 recommendations displayed to 4,700 users. We\nfind, among others, that the number of analyzed nodes, modification time,\nvisibility of nodes, relations between nodes, and number of children and\nsiblings of a node affect the effectiveness of user modeling. When all\nvariables are combined in a favorable way, this novel approach achieves\nclick-through rates of 7.20%, which is nearly twice as effective as the best\nbaseline. In addition, we show that user modeling based on mind maps performs\nabout as well as user modeling based on other items, namely the research\narticles users downloaded or cited. Our findings let us to conclude that user\nmodeling based on mind maps is a promising research field, and that developers\nof mind-mapping applications should integrate recommender systems into their\napplications. Such systems could create additional value for millions of\nmind-mapping users.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 14:35:58 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Beel", "Joeran", ""]]}, {"id": "1703.09662", "submitter": "Dorna Bandari Ph.D.", "authors": "Dorna Bandari, Shuo Xiang, Jure Leskovec", "title": "Categorizing User Sessions at Pinterest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different users can use a given Internet application in many different ways.\nThe ability to record detailed event logs of user in-application activity\nallows us to discover ways in which the application is being used. This enables\npersonalization and also leads to important insights with actionable business\nand product outcomes.\n  Here we study the problem of user session categorization, where the goal is\nto automatically discover categories/classes of user in-session behavior using\nevent logs, and then consistently categorize each user session into the\ndiscovered classes. We develop a three stage approach which uses clustering to\ndiscover categories of sessions, then builds classifiers to classify new\nsessions into the discovered categories, and finally performs daily\nclassification in a distributed pipeline. An important innovation of our\napproach is selecting a set of events as long-tail features, and replacing them\nwith a new feature that is less sensitive to product experimentation and\nlogging changes. This allows for robust and stable identification of session\ntypes even though the underlying application is constantly changing. We deploy\nthe approach to Pinterest and demonstrate its effectiveness. We discover\ninsights that have consequences for product monetization, growth, and design.\nOur solution classifies millions of user sessions daily and leads to actionable\ninsights.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 16:32:56 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 01:29:26 GMT"}, {"version": "v3", "created": "Wed, 25 Oct 2017 06:31:08 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Bandari", "Dorna", ""], ["Xiang", "Shuo", ""], ["Leskovec", "Jure", ""]]}, {"id": "1703.09845", "submitter": "Krishnaram Kenthapadi", "authors": "Krishnaram Kenthapadi, Stuart Ambler, Liang Zhang, Deepak Agarwal", "title": "Bringing Salary Transparency to the World: Computing Robust Compensation\n  Insights via LinkedIn Salary", "comments": "Conference information: ACM International Conference on Information\n  and Knowledge Management (CIKM 2017)", "journal-ref": null, "doi": "10.1145/3132847.3132863", "report-no": null, "categories": "cs.SI cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently launched LinkedIn Salary product has been designed with the goal\nof providing compensation insights to the world's professionals and thereby\nhelping them optimize their earning potential. We describe the overall design\nand architecture of the statistical modeling system underlying this product. We\nfocus on the unique data mining challenges while designing and implementing the\nsystem, and describe the modeling components such as Bayesian hierarchical\nsmoothing that help to compute and present robust compensation insights to\nusers. We report on extensive evaluation with nearly one year of de-identified\ncompensation data collected from over one million LinkedIn users, thereby\ndemonstrating the efficacy of the statistical models. We also highlight the\nlessons learned through the deployment of our system at LinkedIn.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 00:21:02 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 16:49:58 GMT"}, {"version": "v3", "created": "Fri, 1 Sep 2017 18:24:08 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Kenthapadi", "Krishnaram", ""], ["Ambler", "Stuart", ""], ["Zhang", "Liang", ""], ["Agarwal", "Deepak", ""]]}, {"id": "1703.10111", "submitter": "Myungha Jang", "authors": "Shiri Dori-Hacohen, Myungha Jang, James Allan", "title": "Is Climate Change Controversial? Modeling Controversy as Contention\n  Within Populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing body of research focuses on computationally detecting controversial\ntopics and understanding the stances people hold on them. Yet gaps remain in\nour theoretical and practical understanding of how to define controversy, how\nit manifests, and how to measure it. In this paper, we introduce a novel\nmeasure we call \"contention\", defined with respect to a topic and a population.\nWe model contention from a mathematical standpoint. We validate our model by\nexamining a diverse set of sources: real-world polling data sets, actual voter\ndata, and Twitter coverage on several topics. In our publicly-released Twitter\ndata set of nearly 100M tweets, we examine several topics such as Brexit, the\n2016 U.S. Elections, and \"The Dress\", and cross-reference them with other\nsources. We demonstrate that the contention measure holds explanatory power for\na wide variety of observed phenomena, such as controversies over climate change\nand other topics that are well within scientific consensus. Finally, we\nre-examine the notion of controversy, and present a theoretical framework that\ndefines it in terms of population. We present preliminary evidence suggesting\nthat contention is one dimension of controversy, along with others, such as\n\"importance\". Our new contention measure, along with the hypothesized model of\ncontroversy, suggest several avenues for future work in this emerging\ninterdisciplinary research area.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 16:03:10 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Dori-Hacohen", "Shiri", ""], ["Jang", "Myungha", ""], ["Allan", "James", ""]]}, {"id": "1703.10339", "submitter": "Besnik Fetahu", "authors": "Besnik Fetahu and Katja Markert and Wolfgang Nejdl and Avishek Anand", "title": "Finding News Citations for Wikipedia", "comments": null, "journal-ref": null, "doi": "10.1145/2983323.2983808", "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important editing policy in Wikipedia is to provide citations for added\nstatements in Wikipedia pages, where statements can be arbitrary pieces of\ntext, ranging from a sentence to a paragraph. In many cases citations are\neither outdated or missing altogether.\n  In this work we address the problem of finding and updating news citations\nfor statements in entity pages. We propose a two-stage supervised approach for\nthis problem. In the first step, we construct a classifier to find out whether\nstatements need a news citation or other kinds of citations (web, book,\njournal, etc.). In the second step, we develop a news citation algorithm for\nWikipedia statements, which recommends appropriate citations from a given news\ncollection. Apart from IR techniques that use the statement to query the news\ncollection, we also formalize three properties of an appropriate citation,\nnamely: (i) the citation should entail the Wikipedia statement, (ii) the\nstatement should be central to the citation, and (iii) the citation should be\nfrom an authoritative source.\n  We perform an extensive evaluation of both steps, using 20 million articles\nfrom a real-world news collection. Our results are quite promising, and show\nthat we can perform this task with high precision and at scale.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 07:48:31 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 18:28:09 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Fetahu", "Besnik", ""], ["Markert", "Katja", ""], ["Nejdl", "Wolfgang", ""], ["Anand", "Avishek", ""]]}, {"id": "1703.10344", "submitter": "Besnik Fetahu", "authors": "Besnik Fetahu and Katja Markert and Avishek Anand", "title": "Automated News Suggestions for Populating Wikipedia Entity Pages", "comments": null, "journal-ref": null, "doi": "10.1145/2806416.2806531", "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikipedia entity pages are a valuable source of information for direct\nconsumption and for knowledge-base construction, update and maintenance. Facts\nin these entity pages are typically supported by references. Recent studies\nshow that as much as 20\\% of the references are from online news sources.\nHowever, many entity pages are incomplete even if relevant information is\nalready available in existing news articles. Even for the already present\nreferences, there is often a delay between the news article publication time\nand the reference time. In this work, we therefore look at Wikipedia through\nthe lens of news and propose a novel news-article suggestion task to improve\nnews coverage in Wikipedia, and reduce the lag of newsworthy references. Our\nwork finds direct application, as a precursor, to Wikipedia page generation and\nknowledge-base acceleration tasks that rely on relevant and high quality input\nsources.\n  We propose a two-stage supervised approach for suggesting news articles to\nentity pages for a given state of Wikipedia. First, we suggest news articles to\nWikipedia entities (article-entity placement) relying on a rich set of features\nwhich take into account the \\emph{salience} and \\emph{relative authority} of\nentities, and the \\emph{novelty} of news articles to entity pages. Second, we\ndetermine the exact section in the entity page for the input article\n(article-section placement) guided by class-based section templates. We perform\nan extensive evaluation of our approach based on ground-truth data that is\nextracted from external references in Wikipedia. We achieve a high precision\nvalue of up to 93\\% in the \\emph{article-entity} suggestion stage and upto 84\\%\nfor the \\emph{article-section placement}. Finally, we compare our approach\nagainst competitive baselines and show significant improvements.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 07:56:42 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Fetahu", "Besnik", ""], ["Markert", "Katja", ""], ["Anand", "Avishek", ""]]}, {"id": "1703.10345", "submitter": "Besnik Fetahu", "authors": "Besnik Fetahu and Abhijit Anand and Avishek Anand", "title": "How much is Wikipedia Lagging Behind News?", "comments": null, "journal-ref": null, "doi": "10.1145/2786451.2786460", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikipedia, rich in entities and events, is an invaluable resource for various\nknowledge harvesting, extraction and mining tasks. Numerous resources like\nDBpedia, YAGO and other knowledge bases are based on extracting entity and\nevent based knowledge from it. Online news, on the other hand, is an\nauthoritative and rich source for emerging entities, events and facts relating\nto existing entities. In this work, we study the creation of entities in\nWikipedia with respect to news by studying how entity and event based\ninformation flows from news to Wikipedia.\n  We analyze the lag of Wikipedia (based on the revision history of the English\nWikipedia) with 20 years of \\emph{The New York Times} dataset (NYT). We model\nand analyze the lag of entities and events, namely their first appearance in\nWikipedia and in NYT, respectively. In our extensive experimental analysis, we\nfind that almost 20\\% of the external references in entity pages are news\narticles encoding the importance of news to Wikipedia. Second, we observe that\nthe entity-based lag follows a normal distribution with a high standard\ndeviation, whereas the lag for news-based events is typically very low.\nFinally, we find that events are responsible for creation of emergent entities\nwith as many as 12\\% of the entities mentioned in the event page are created\nafter the creation of the event page.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 08:05:17 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Fetahu", "Besnik", ""], ["Anand", "Abhijit", ""], ["Anand", "Avishek", ""]]}, {"id": "1703.10349", "submitter": "Besnik Fetahu", "authors": "Besnik Fetahu and Ujwal Gadiraju and Stefan Dietze", "title": "Improving Entity Retrieval on Structured Data", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-25007-6_28", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing amount of data on the Web, in particular of Linked Data, has\nled to a diverse landscape of datasets, which make entity retrieval a\nchallenging task. Explicit cross-dataset links, for instance to indicate\nco-references or related entities can significantly improve entity retrieval.\nHowever, only a small fraction of entities are interlinked through explicit\nstatements. In this paper, we propose a two-fold entity retrieval approach. In\na first, offline preprocessing step, we cluster entities based on the\n\\emph{x--means} and \\emph{spectral} clustering algorithms. In the second step,\nwe propose an optimized retrieval model which takes advantage of our\nprecomputed clusters. For a given set of entities retrieved by the BM25F\nretrieval approach and a given user query, we further expand the result set\nwith relevant entities by considering features of the queries, entities and the\nprecomputed clusters. Finally, we re-rank the expanded result set with respect\nto the relevance to the query. We perform a thorough experimental evaluation on\nthe Billions Triple Challenge (BTC12) dataset. The proposed approach shows\nsignificant improvements compared to the baseline and state of the art\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 08:25:35 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Fetahu", "Besnik", ""], ["Gadiraju", "Ujwal", ""], ["Dietze", "Stefan", ""]]}]