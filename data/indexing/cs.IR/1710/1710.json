[{"id": "1710.00232", "submitter": "Mohammad Abdel-Qader", "authors": "Mohammad Abdel-Qader and Ansgar Scherp", "title": "Towards Understanding the Evolution of Vocabulary Terms in Knowledge\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vocabularies are used for modeling data in Knowledge Graphs (KG) like the\nLinked Open Data Cloud and Wikidata. During their lifetime, the vocabularies of\nthe KGs are subject to changes. New terms are coined, while existing terms are\nmodified or declared as deprecated. We first quantify the amount and frequency\nof changes in vocabularies. Subsequently, we investigate to which extend and\nwhen the changes are adopted in the evolution of the KGs. We conduct our\nexperiments on three large-scale KGs for which time-stamped snapshots are\navailable, namely the Billion Triples Challenge datasets, Dynamic Linked Data\nObservatory dataset, and Wikidata. Our results show that the change frequency\nof terms is rather low, but can have high impact when adopted on a large amount\nof distributed graph data on the web. Furthermore, not all coined terms are\nused and most of the deprecated terms are still used by data publishers. There\nare variations in the adoption time of terms coming from different vocabularies\nranging from very fast (few days) to very slow (few years). Surprisingly, there\nare also adoptions we could observe even before the vocabulary changes are\npublished. Understanding this adoption is important, since otherwise it may\nlead to wrong assumptions about the modeling status of data published on the\nweb and may result in difficulties when querying the data from distributed\nsources.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 17:32:29 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Abdel-Qader", "Mohammad", ""], ["Scherp", "Ansgar", ""]]}, {"id": "1710.00284", "submitter": "Liqun Shao", "authors": "Liqun Shao, Hao Zhang, Ming Jia, Jie Wang", "title": "Efficient and Effective Single-Document Summarizations and A\n  Word-Embedding Measurement of Quality", "comments": "10 pages, conference; accepted by KDIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our task is to generate an effective summary for a given document with\nspecific realtime requirements. We use the softplus function to enhance keyword\nrankings to favor important sentences, based on which we present a number of\nsummarization algorithms using various keyword extraction and topic clustering\nmethods. We show that our algorithms meet the realtime requirements and yield\nthe best ROUGE recall scores on DUC-02 over all previously-known algorithms. We\nshow that our algorithms meet the realtime requirements and yield the best\nROUGE recall scores on DUC-02 over all previously-known algorithms. To evaluate\nthe quality of summaries without human-generated benchmarks, we define a\nmeasure called WESM based on word-embedding using Word Mover's Distance. We\nshow that the orderings of the ROUGE and WESM scores of our algorithms are\nhighly comparable, suggesting that WESM may serve as a viable alternative for\nmeasuring the quality of a summary.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 03:36:45 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Shao", "Liqun", ""], ["Zhang", "Hao", ""], ["Jia", "Ming", ""], ["Wang", "Jie", ""]]}, {"id": "1710.00286", "submitter": "Liqun Shao", "authors": "Liqun Shao, Jie Wang", "title": "DTATG: An Automatic Title Generator based on Dependency Trees", "comments": "8 pages, conference: accepted by KDIR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study automatic title generation for a given block of text and present a\nmethod called DTATG to generate titles. DTATG first extracts a small number of\ncentral sentences that convey the main meanings of the text and are in a\nsuitable structure for conversion into a title. DTATG then constructs a\ndependency tree for each of these sentences and removes certain branches using\na Dependency Tree Compression Model we devise. We also devise a title test to\ndetermine if a sentence can be used as a title. If a trimmed sentence passes\nthe title test, then it becomes a title candidate. DTATG selects the title\ncandidate with the highest ranking score as the final title. Our experiments\nshowed that DTATG can generate adequate titles. We also showed that\nDTATG-generated titles have higher F1 scores than those generated by the\nprevious methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 03:52:37 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Shao", "Liqun", ""], ["Wang", "Jie", ""]]}, {"id": "1710.00310", "submitter": "Chengwei Huang", "authors": "Yun Liu, Tianmeng Gao, Baolin Song, Chengwei Huang", "title": "Personalized Fuzzy Text Search Using Interest Prediction and Word\n  Vectorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the personalized text search problem. The keyword\nbased search method in conventional algorithms has a low efficiency in\nunderstanding users' intention since the semantic meaning, user profile, user\ninterests are not always considered. Firstly, we propose a novel text search\nalgorithm using a inverse filtering mechanism that is very efficient for label\nbased item search. Secondly, we adopt the Bayesian network to implement the\nuser interest prediction for an improved personalized search. According to user\ninput, it searches the related items using keyword information, predicted user\ninterest. Thirdly, the word vectorization is used to discover potential targets\naccording to the semantic meaning. Experimental results show that the proposed\nsearch engine has an improved efficiency and accuracy and it can operate on\nembedded devices with very limited computational resources.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 08:22:24 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Liu", "Yun", ""], ["Gao", "Tianmeng", ""], ["Song", "Baolin", ""], ["Huang", "Chengwei", ""]]}, {"id": "1710.00398", "submitter": "Volodymyr Miz", "authors": "Volodymyr Miz, Kirell Benzi, Benjamin Ricaud, Pierre Vandergheynst", "title": "Wikipedia graph mining: dynamic structure of collective memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wikipedia is the biggest encyclopedia ever created and the fifth most visited\nwebsite in the world. Tens of millions of people surf it every day, seeking\nanswers to various questions. Collective user activity on its pages leaves\npublicly available footprints of human behavior, making Wikipedia an excellent\nsource for analysis of collective behavior. In this work, we propose a\ndistributed graph-based event extraction model, inspired by the Hebbian\nlearning theory. The model exploits collective effect of the dynamics to\ndiscover events. We focus on data-streams with underlying graph structure and\nperform several large-scale experiments on the Wikipedia visitor activity data.\nWe show that the presented model is scalable regarding time-series length and\ngraph density, providing a distributed implementation of the proposed\nalgorithm. We extract dynamical patterns of collective activity and demonstrate\nthat they correspond to meaningful clusters of associated events, reflected in\nthe Wikipedia articles. We also illustrate evolutionary dynamics of the graphs\nover time to highlight changing nature of visitors' interests. Finally, we\ndiscuss clusters of events that model collective recall process and represent\ncollective memories - common memories shared by a group of people.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 19:39:53 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 13:07:47 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 13:30:33 GMT"}, {"version": "v4", "created": "Fri, 22 Dec 2017 14:09:32 GMT"}, {"version": "v5", "created": "Wed, 14 Feb 2018 13:33:15 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Miz", "Volodymyr", ""], ["Benzi", "Kirell", ""], ["Ricaud", "Benjamin", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1710.00399", "submitter": "Alexey Grigorev", "authors": "Alexey Grigorev", "title": "Identifying Clickbait Posts on Social Media with an Ensemble of Linear\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of a clickbait is to make a link so appealing that people click\non it. However, the content of such articles is often not related to the title,\nshows poor quality, and at the end leaves the reader unsatisfied.\n  To help the readers, the organizers of the clickbait challenge\n(http://www.clickbait-challenge.org/) asked the participants to build a machine\nlearning model for scoring articles with respect to their \"clickbaitness\".\n  In this paper we propose to solve the clickbait problem with an ensemble of\nLinear SVM models, and our approach was tested successfully in the challenge:\nit showed great performance of 0.036 MSE and ranked 3rd among all the solutions\nto the contest.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 19:42:53 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Grigorev", "Alexey", ""]]}, {"id": "1710.00454", "submitter": "Amanpreet Singh", "authors": "Amanpreet Singh, Karthik Venkatesan and Simranjyot Singh Gill", "title": "Building a Structured Query Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding patterns in data and being able to retrieve information from those\npatterns is an important task in Information retrieval. Complex search\nrequirements which are not fulfilled by simple string matching and require\nexploring certain patterns in data demand a better query engine that can\nsupport searching via structured queries. In this article, we built a\nstructured query engine which supports searching data through structured\nqueries on the lines of ElasticSearch. We will show how we achieved real time\nindexing and retrieving of data through a RESTful API and how complex queries\ncan be created and processed using efficient data structures we created for\nstoring the data in structured way. Finally, we will conclude with an example\nof movie recommendation system built on top of this query engine.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 01:54:40 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Singh", "Amanpreet", ""], ["Venkatesan", "Karthik", ""], ["Gill", "Simranjyot Singh", ""]]}, {"id": "1710.00482", "submitter": "Hung-Hsuan Chen", "authors": "Hung-Hsuan Chen", "title": "Weighted-SVD: Matrix Factorization with Weights on the Latent Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Matrix Factorization models, sometimes called the latent factor models,\nare a family of methods in the recommender system research area to (1) generate\nthe latent factors for the users and the items and (2) predict users' ratings\non items based on their latent factors. However, current Matrix Factorization\nmodels presume that all the latent factors are equally weighted, which may not\nalways be a reasonable assumption in practice. In this paper, we propose a new\nmodel, called Weighted-SVD, to integrate the linear regression model with the\nSVD model such that each latent factor accompanies with a corresponding weight\nparameter. This mechanism allows the latent factors have different weights to\ninfluence the final ratings. The complexity of the Weighted-SVD model is\nslightly larger than the SVD model but much smaller than the SVD++ model. We\ncompared the Weighted-SVD model with several latent factor models on five\npublic datasets based on the Root-Mean-Squared-Errors (RMSEs). The results show\nthat the Weighted-SVD model outperforms the baseline methods in all the\nexperimental datasets under almost all settings.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 04:56:09 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Chen", "Hung-Hsuan", ""]]}, {"id": "1710.00867", "submitter": "Yanfeng Zhang", "authors": "Shufeng Gong, Yanfeng Zhang, Ge Yu", "title": "Clustering Stream Data by Exploring the Evolution of Density Mountain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream clustering is a fundamental problem in many streaming data analysis\napplications. Comparing to classical batch-mode clustering, there are two key\nchallenges in stream clustering: (i) Given that input data are changing\ncontinuously, how to incrementally update clustering results efficiently? (ii)\nGiven that clusters continuously evolve with the evolution of data, how to\ncapture the cluster evolution activities? Unfortunately, most of existing\nstream clustering algorithms can neither update the cluster result in real time\nnor track the evolution of clusters.\n  In this paper, we propose an stream clustering algorithm EDMStream by\nexploring the Evolution of Density Mountain. The density mountain is used to\nabstract the data distribution, the changes of which indicate data distribution\nevolution. We track the evolution of clusters by monitoring the changes of\ndensity mountains. We further provide efficient data structures and filtering\nschemes to ensure the update of density mountains in real time, which makes\nonline clustering possible. The experimental results on synthetic and real\ndatasets show that, comparing to the state-of-the-art stream clustering\nalgorithms, e.g., D-Stream, DenStream, DBSTREAM and MR-Stream, our algorithm\ncan response to a cluster update much faster (say 7-15x faster than the best of\nthe competitors) and at the same time achieve comparable cluster quality.\nFurthermore, EDMStream can successfully capture the cluster evolution\nactivities.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 18:59:09 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Gong", "Shufeng", ""], ["Zhang", "Yanfeng", ""], ["Yu", "Ge", ""]]}, {"id": "1710.00888", "submitter": "Jose Berengueres Ph.D", "authors": "Jose Berengueres and Dani Castro", "title": "Sentiment Perception of Readers and Writers in Emoji use", "comments": "8 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous research has traditionally analyzed emoji sentiment from the point\nof view of the reader of the content not the author. Here, we analyze emoji\nsentiment from the point of view of the author and present a emoji sentiment\nbenchmark that was built from an employee happiness dataset where emoji happen\nto be annotated with daily happiness of the author of the comment. The data\nspans over 3 years, and 4k employees of 56 companies based in Barcelona. We\ncompare sentiment of writers to readers. Results indicate that, there is an 82%\nagreement in how emoji sentiment is perceived by readers and writers. Finally,\nwe report that when authors use emoji they report higher levels of happiness.\nEmoji use was not found to be correlated with differences in author moodiness.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 20:07:18 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 23:26:06 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Berengueres", "Jose", ""], ["Castro", "Dani", ""]]}, {"id": "1710.01127", "submitter": "Alex Olieman", "authors": "Alex Olieman, Kaspar Beelen, and Jaap Kamps", "title": "Finding Talk About the Past in the Discourse of Non-Historians", "comments": "Presented at Drift-a-LOD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A heightened interest in the presence of the past has given rise to the new\nfield of memory studies, but there is a lack of search and research tools to\nsupport studying how and why the past is evoked in diachronic discourses.\nSearching for temporal references is not straightforward. It entails bridging\nthe gap between conceptually-based information needs on one side, and\nterm-based inverted indexes on the other.\n  Our approach enables the search for references to (intersubjective)\nhistorical periods in diachronic corpora. It consists of a\nsemantically-enhanced search engine that is able to find references to many\nentities at a time, which is combined with a novel interface that invites its\nuser to actively sculpt the search result set. Until now we have been concerned\nmostly with user-friendly retrieval and selection of sources, but our tool can\nalso contribute to existing efforts to create reusable linked data from and for\nresearch in the humanities.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 13:11:44 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Olieman", "Alex", ""], ["Beelen", "Kaspar", ""], ["Kamps", "Jaap", ""]]}, {"id": "1710.01507", "submitter": "Yash Kumar Lal", "authors": "Vaibhav Kumar, Dhruv Khattar, Siddhartha Gairola, Yash Kumar Lal,\n  Vasudeva Varma", "title": "Identifying Clickbait: A Multi-Strategy Approach Using Neural Networks", "comments": "Accepted at SIGIR 2018 as Short Paper", "journal-ref": "\"Identifying Clickbait: A Multi-Strategy Approach Using Neural\n  Networks\". In Proceedings of the 41st International ACM SIGIR Conference on\n  Research and Development in Information Retrieval 2018. Pages: 1225-1228", "doi": "10.1145/3209978.3210144", "report-no": null, "categories": "cs.IR cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online media outlets, in a bid to expand their reach and subsequently\nincrease revenue through ad monetisation, have begun adopting clickbait\ntechniques to lure readers to click on articles. The article fails to fulfill\nthe promise made by the headline. Traditional methods for clickbait detection\nhave relied heavily on feature engineering which, in turn, is dependent on the\ndataset it is built for. The application of neural networks for this task has\nonly been explored partially. We propose a novel approach considering all\ninformation found in a social media post. We train a bidirectional LSTM with an\nattention mechanism to learn the extent to which a word contributes to the\npost's clickbait score in a differential manner. We also employ a Siamese net\nto capture the similarity between source and target information. Information\ngleaned from images has not been considered in previous approaches. We learn\nimage embeddings from large amounts of data using Convolutional Neural Networks\nto add another layer of complexity to our model. Finally, we concatenate the\noutputs from the three separate components, serving it as input to a fully\nconnected layer. We conduct experiments over a test corpus of 19538 social\nmedia posts, attaining an F1 score of 65.37% on the dataset bettering the\nprevious state-of-the-art, as well as other proposed approaches, feature\nengineering or otherwise.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 08:53:12 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 22:41:19 GMT"}, {"version": "v3", "created": "Fri, 9 Feb 2018 13:49:13 GMT"}, {"version": "v4", "created": "Wed, 1 Aug 2018 17:17:16 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Kumar", "Vaibhav", ""], ["Khattar", "Dhruv", ""], ["Gairola", "Siddhartha", ""], ["Lal", "Yash Kumar", ""], ["Varma", "Vasudeva", ""]]}, {"id": "1710.02255", "submitter": "Long Sha", "authors": "Long Sha, Patrick Lucey, Stephan Zheng, Taehwan Kim, Yisong Yue, and\n  Sridha Sridharan", "title": "Fine-Grained Retrieval of Sports Plays using Tree-Based Alignment of\n  Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for effective retrieval of multi-agent\nspatiotemporal tracking data. Retrieval of spatiotemporal tracking data offers\nseveral unique challenges compared to conventional text-based retrieval\nsettings. Most notably, the data is fine-grained meaning that the specific\nlocation of agents is important in describing behavior. Additionally, the data\noften contains tracks of multiple agents (e.g., multiple players in a sports\ngame), which generally leads to a permutational alignment problem when\nperforming relevance estimation. Due to the frequent position swap of agents,\nit is difficult to maintain the correspondence of agents, and such issues make\nthe pairwise comparison problematic for multi-agent spatiotemporal data. To\naddress this issue, we propose a tree-based method to estimate the relevance\nbetween multi-agent spatiotemporal tracks. It uses a hierarchical structure to\nperform multi-agent data alignment and partitioning in a coarse-to-fine\nfashion. We validate our approach via user studies with domain experts. Our\nresults show that our method boosts performance in retrieving similar sports\nplays -- especially in interactive situations where the user selects a subset\nof trajectories compared to current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 01:53:01 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Sha", "Long", ""], ["Lucey", "Patrick", ""], ["Zheng", "Stephan", ""], ["Kim", "Taehwan", ""], ["Yue", "Yisong", ""], ["Sridharan", "Sridha", ""]]}, {"id": "1710.02261", "submitter": "Sejoon Oh", "authors": "Sejoon Oh, Namyong Park, Lee Sael, and U Kang", "title": "Scalable Tucker Factorization for Sparse Tensors - Algorithms and\n  Discoveries", "comments": "IEEE International Conference on Data Engineering (ICDE 2018)", "journal-ref": null, "doi": "10.1109/ICDE.2018.00104", "report-no": null, "categories": "cs.NA cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given sparse multi-dimensional data (e.g., (user, movie, time; rating) for\nmovie recommendations), how can we discover latent concepts/relations and\npredict missing values? Tucker factorization has been widely used to solve such\nproblems with multi-dimensional data, which are modeled as tensors. However,\nmost Tucker factorization algorithms regard and estimate missing entries as\nzeros, which triggers a highly inaccurate decomposition. Moreover, few methods\nfocusing on an accuracy exhibit limited scalability since they require huge\nmemory and heavy computational costs while updating factor matrices. In this\npaper, we propose P-Tucker, a scalable Tucker factorization method for sparse\ntensors. P-Tucker performs alternating least squares with a row-wise update\nrule in a fully parallel way, which significantly reduces memory requirements\nfor updating factor matrices. Furthermore, we offer two variants of P-Tucker: a\ncaching algorithm P-Tucker-Cache and an approximation algorithm\nP-Tucker-Approx, both of which accelerate the update process. Experimental\nresults show that P-Tucker exhibits 1.7-14.1x speed-up and 1.4-4.8x less error\ncompared to the state-of-the-art. In addition, P-Tucker scales near linearly\nwith the number of observable entries in a tensor and number of threads. Thanks\nto P-Tucker, we successfully discover hidden concepts and relations in a\nlarge-scale real-world tensor, while existing methods cannot reveal latent\nfeatures due to their limited scalability or low accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 02:54:44 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 05:24:52 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Oh", "Sejoon", ""], ["Park", "Namyong", ""], ["Sael", "Lee", ""], ["Kang", "U", ""]]}, {"id": "1710.02271", "submitter": "Aravind Sankar", "authors": "Adit Krishnan, Aravind Sankar, Shi Zhi, Jiawei Han", "title": "Unsupervised Extraction of Representative Concepts from Scientific\n  Literature", "comments": "Published as a conference paper at CIKM 2017", "journal-ref": null, "doi": "10.1145/3132847.3133023", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the automated categorization and extraction of scientific\nconcepts from titles of scientific articles, in order to gain a deeper\nunderstanding of their key contributions and facilitate the construction of a\ngeneric academic knowledgebase. Towards this goal, we propose an unsupervised,\ndomain-independent, and scalable two-phase algorithm to type and extract key\nconcept mentions into aspects of interest (e.g., Techniques, Applications,\netc.). In the first phase of our algorithm we propose PhraseType, a\nprobabilistic generative model which exploits textual features and limited POS\ntags to broadly segment text snippets into aspect-typed phrases. We extend this\nmodel to simultaneously learn aspect-specific features and identify academic\ndomains in multi-domain corpora, since the two tasks mutually enhance each\nother. In the second phase, we propose an approach based on adaptor grammars to\nextract fine grained concept mentions from the aspect-typed phrases without the\nneed for any external resources or human effort, in a purely data-driven\nmanner. We apply our technique to study literature from diverse scientific\ndomains and show significant gains over state-of-the-art concept extraction\ntechniques. We also present a qualitative analysis of the results obtained.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 04:04:43 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 20:58:20 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 04:54:52 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Krishnan", "Adit", ""], ["Sankar", "Aravind", ""], ["Zhi", "Shi", ""], ["Han", "Jiawei", ""]]}, {"id": "1710.02650", "submitter": "Johannes Schneider", "authors": "Johannes Schneider", "title": "Topic Modeling based on Keywords and Context", "comments": "SIAM International Conference on Data Mining (SDM), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current topic models often suffer from discovering topics not matching human\nintuition, unnatural switching of topics within documents and high\ncomputational demands. We address these concerns by proposing a topic model and\nan inference algorithm based on automatically identifying characteristic\nkeywords for topics. Keywords influence topic-assignments of nearby words. Our\nalgorithm learns (key)word-topic scores and it self-regulates the number of\ntopics. Inference is simple and easily parallelizable. Qualitative analysis\nyields comparable results to state-of-the-art models (eg. LDA), but with\ndifferent strengths and weaknesses. Quantitative analysis using 9 datasets\nshows gains in terms of classification accuracy, PMI score, computational\nperformance and consistency of topic assignments within documents, while most\noften using less topics.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 08:18:12 GMT"}, {"version": "v2", "created": "Sat, 3 Feb 2018 23:36:54 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Schneider", "Johannes", ""]]}, {"id": "1710.02772", "submitter": "Zheqian Chen", "authors": "Zheqian Chen, Rongqin Yang, Bin Cao, Zhou Zhao, Deng Cai, Xiaofei He", "title": "Smarnet: Teaching Machines to Read and Comprehend Like Human", "comments": "8 pages, paper for SQuAD machine comprehension", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Comprehension (MC) is a challenging task in Natural Language\nProcessing field, which aims to guide the machine to comprehend a passage and\nanswer the given question. Many existing approaches on MC task are suffering\nthe inefficiency in some bottlenecks, such as insufficient lexical\nunderstanding, complex question-passage interaction, incorrect answer\nextraction and so on. In this paper, we address these problems from the\nviewpoint of how humans deal with reading tests in a scientific way.\nSpecifically, we first propose a novel lexical gating mechanism to dynamically\ncombine the words and characters representations. We then guide the machines to\nread in an interactive way with attention mechanism and memory network. Finally\nwe add a checking layer to refine the answer for insurance. The extensive\nexperiments on two popular datasets SQuAD and TriviaQA show that our method\nexceeds considerable performance than most state-of-the-art solutions at the\ntime of submission.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 02:51:26 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Chen", "Zheqian", ""], ["Yang", "Rongqin", ""], ["Cao", "Bin", ""], ["Zhao", "Zhou", ""], ["Cai", "Deng", ""], ["He", "Xiaofei", ""]]}, {"id": "1710.02861", "submitter": "Vijayasaradhi Indurthi", "authors": "Vijayasaradhi Indurthi, Subba Reddy Oota", "title": "Clickbait detection using word embeddings", "comments": "Clickbait Challenge 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clickbait is a pejorative term describing web content that is aimed at\ngenerating online advertising revenue, especially at the expense of quality or\naccuracy, relying on sensationalist headlines or eye-catching thumbnail\npictures to attract click-throughs and to encourage forwarding of the material\nover online social networks. We use distributed word representations of the\nwords in the title as features to identify clickbaits in online news media. We\ntrain a machine learning model using linear regression to predict the cickbait\nscore of a given tweet. Our methods achieve an F1-score of 64.98\\% and an MSE\nof 0.0791. Compared to other methods, our method is simple, fast to train, does\nnot require extensive feature engineering and yet moderately effective.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 17:34:03 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Indurthi", "Vijayasaradhi", ""], ["Oota", "Subba Reddy", ""]]}, {"id": "1710.02973", "submitter": "Panagiotis Papadakos", "authors": "Alexandros Papangelis, Panagiotis Papadakos, Margarita Kotti, Yannis\n  Stylianou, Yannis Tzitzikas, Dimitris Plexousakis", "title": "LD-SDS: Towards an Expressive Spoken Dialogue System based on\n  Linked-Data", "comments": "Presented in Search Oriented Conversational AI SCAI 17 Workshop,\n  Co-located with ICTIR 17, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we discuss the related challenges and describe an approach\ntowards the fusion of state-of-the-art technologies from the Spoken Dialogue\nSystems (SDS) and the Semantic Web and Information Retrieval domains. We\nenvision a dialogue system named LD-SDS that will support advanced, expressive,\nand engaging user requests, over multiple, complex, rich, and open-domain data\nsources that will leverage the wealth of the available Linked Data.\nSpecifically, we focus on: a) improving the identification, disambiguation and\nlinking of entities occurring in data sources and user input; b) offering\nadvanced query services for exploiting the semantics of the data, with\nreasoning and exploratory capabilities; and c) expanding the typical\ninformation seeking dialogue model (slot filling) to better reflect real-world\nconversational search scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 07:36:18 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Papangelis", "Alexandros", ""], ["Papadakos", "Panagiotis", ""], ["Kotti", "Margarita", ""], ["Stylianou", "Yannis", ""], ["Tzitzikas", "Yannis", ""], ["Plexousakis", "Dimitris", ""]]}, {"id": "1710.03208", "submitter": "Hamed Zamani", "authors": "Markus Schedl, Hamed Zamani, Ching-Wei Chen, Yashar Deldjoo, Mehdi\n  Elahi", "title": "Current Challenges and Visions in Music Recommender Systems Research", "comments": null, "journal-ref": null, "doi": "10.1007/s13735-018-0154-2", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music recommender systems (MRS) have experienced a boom in recent years,\nthanks to the emergence and success of online streaming services, which\nnowadays make available almost all music in the world at the user's fingertip.\nWhile today's MRS considerably help users to find interesting music in these\nhuge catalogs, MRS research is still facing substantial challenges. In\nparticular when it comes to build, incorporate, and evaluate recommendation\nstrategies that integrate information beyond simple user--item interactions or\ncontent-based descriptors, but dig deep into the very essence of listener\nneeds, preferences, and intentions, MRS research becomes a big endeavor and\nrelated publications quite sparse.\n  The purpose of this trends and survey article is twofold. We first identify\nand shed light on what we believe are the most pressing challenges MRS research\nis facing, from both academic and industry perspectives. We review the state of\nthe art towards solving these challenges and discuss its limitations. Second,\nwe detail possible future directions and visions we contemplate for the further\nevolution of the field. The article should therefore serve two purposes: giving\nthe interested reader an overview of current challenges in MRS research and\nproviding guidance for young researchers by identifying interesting, yet\nunder-researched, directions in the field.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 17:39:54 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 21:36:43 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Schedl", "Markus", ""], ["Zamani", "Hamed", ""], ["Chen", "Ching-Wei", ""], ["Deldjoo", "Yashar", ""], ["Elahi", "Mehdi", ""]]}, {"id": "1710.03275", "submitter": "Theja Tulabandhula", "authors": "Theja Tulabandhula, Shailesh Vaya, Aritra Dhar", "title": "Privacy-preserving Targeted Advertising", "comments": "A preliminary version was presented at the 11th INFORMS Workshop on\n  Data Mining and Decision Analytics (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems form the center piece of a rapidly growing trillion\ndollar online advertisement industry. Even with numerous optimizations and\napproximations, collaborative filtering (CF) based approaches require real-time\ncomputations involving very large vectors. Curating and storing such related\nprofile information vectors on web portals seriously breaches the user's\nprivacy. Modifying such systems to achieve private recommendations further\nrequires communication of long encrypted vectors, making the whole process\ninefficient. We present a more efficient recommendation system alternative, in\nwhich user profiles are maintained entirely on their device, and appropriate\nrecommendations are fetched from web portals in an efficient privacy preserving\nmanner. We base this approach on association rules.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 19:31:22 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 00:28:51 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Tulabandhula", "Theja", ""], ["Vaya", "Shailesh", ""], ["Dhar", "Aritra", ""]]}, {"id": "1710.03323", "submitter": "Gerasimos Spanakis", "authors": "Tom Rolandus Hagedoorn, Gerasimos Spanakis", "title": "Massive Open Online Courses Temporal Profiling for Dropout Prediction", "comments": "8 pages, ICTAI17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive Open Online Courses (MOOCs) are attracting the attention of people\nall over the world. Regardless the platform, numbers of registrants for online\ncourses are impressive but in the same time, completion rates are\ndisappointing. Understanding the mechanisms of dropping out based on the\nlearner profile arises as a crucial task in MOOCs, since it will allow\nintervening at the right moment in order to assist the learner in completing\nthe course. In this paper, the dropout behaviour of learners in a MOOC is\nthoroughly studied by first extracting features that describe the behavior of\nlearners within the course and then by comparing three classifiers (Logistic\nRegression, Random Forest and AdaBoost) in two tasks: predicting which users\nwill have dropped out by a certain week and predicting which users will drop\nout on a specific week. The former has showed to be considerably easier, with\nall three classifiers performing equally well. However, the accuracy for the\nsecond task is lower, and Logistic Regression tends to perform slightly better\nthan the other two algorithms. We found that features that reflect an active\nattitude of the user towards the MOOC, such as submitting their assignment,\nposting on the Forum and filling their Profile, are strong indicators of\npersistence.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 21:31:44 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Hagedoorn", "Tom Rolandus", ""], ["Spanakis", "Gerasimos", ""]]}, {"id": "1710.03346", "submitter": "Hao Chen", "authors": "Hao Chen, Maria Vasardani, Stephan Winter", "title": "Geo-referencing Place from Everyday Natural Language Descriptions", "comments": "28 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language place descriptions in everyday communication provide a rich\nsource of spatial knowledge about places. An important step to utilize such\nknowledge in information systems is geo-referencing all the places referred to\nin these descriptions. Current techniques for geo-referencing places from text\ndocuments are using place name recognition and disambiguation; however, place\ndescriptions often contain place references that are not known by gazetteers,\nor that are expressed in other, more flexible ways. Hence, the approach for\ngeo-referencing presented in this paper starts from a place graph that contains\nthe place references as well as spatial relationships extracted from place\ndescriptions. Spatial relationships are used to constrain the locations of\nplaces and allow the later best-matching process for geo-referencing. The novel\ngeo-referencing process results in higher precision and recall compared to\nstate-of-art toponym resolution approaches on several tested place description\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 23:06:17 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Chen", "Hao", ""], ["Vasardani", "Maria", ""], ["Winter", "Stephan", ""]]}, {"id": "1710.04031", "submitter": "Robin Haunschild", "authors": "Robin Haunschild, Sven E. Hug, Martin P. Br\\\"andle, and Lutz Bornmann", "title": "The number of linked references of publications in Microsoft Academic in\n  comparison with the Web of Science", "comments": "6 pages", "journal-ref": null, "doi": "10.1007/s11192-017-2567-8", "report-no": null, "categories": "cs.DL cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of a comprehensive Microsoft Academic (MA) study, we explored\nin an initial step the quality of linked references data in MA in comparison\nwith Web of Science (WoS). Linked references are the backbone of bibliometrics,\nbecause they are the basis of the times cited information in citation indexes.\nWe found that the concordance of linked references between MA and WoS ranges\nfrom weak to non-existent for the full sample (publications of the University\nof Zurich with less than 50 linked references in MA). An analysis with a sample\nrestricted to less than 50 linked references in WoS showed a strong agreement\nbetween linked references in MA and WoS.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 12:15:55 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Haunschild", "Robin", ""], ["Hug", "Sven E.", ""], ["Br\u00e4ndle", "Martin P.", ""], ["Bornmann", "Lutz", ""]]}, {"id": "1710.04312", "submitter": "Kyle Hundman", "authors": "Kyle Hundman, Chris A. Mattmann", "title": "Measurement Context Extraction from Text: Discovering Opportunities and\n  Gaps in Earth Science", "comments": null, "journal-ref": "23rd ACM SIGKDD International Conference on Knowledge Discovery\n  and Data Mining, Data-Driven Discovery Workshop, Halifax, Canada, August 2017", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Marve, a system for extracting measurement values, units, and\nrelated words from natural language text. Marve uses conditional random fields\n(CRF) to identify measurement values and units, followed by a rule-based system\nto find related entities, descriptors and modifiers within a sentence. Sentence\ntokens are represented by an undirected graphical model, and rules are based on\npart-of-speech and word dependency patterns connecting values and units to\ncontextual words. Marve is unique in its focus on measurement context and early\nexperimentation demonstrates Marve's ability to generate high-precision\nextractions with strong recall. We also discuss Marve's role in refining\nmeasurement requirements for NASA's proposed HyspIRI mission, a hyperspectral\ninfrared imaging satellite that will study the world's ecosystems. In general,\nour work with HyspIRI demonstrates the value of semantic measurement\nextractions in characterizing quantitative discussion contained in large\ncorpuses of natural language text. These extractions accelerate broad,\ncross-cutting research and expose scientists new algorithmic approaches and\nexperimental nuances. They also facilitate identification of scientific\nopportunities enabled by HyspIRI leading to more efficient scientific\ninvestment and research.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 21:37:07 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Hundman", "Kyle", ""], ["Mattmann", "Chris A.", ""]]}, {"id": "1710.04735", "submitter": "Dhruv Choudhary", "authors": "Dhruv Choudhary, Arun Kejariwal, Francois Orsini", "title": "On the Runtime-Efficacy Trade-off of Anomaly Detection Techniques for\n  Real-Time Streaming Data", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ever growing volume and velocity of data coupled with decreasing attention\nspan of end users underscore the critical need for real-time analytics. In this\nregard, anomaly detection plays a key role as an application as well as a means\nto verify data fidelity. Although the subject of anomaly detection has been\nresearched for over 100 years in a multitude of disciplines such as, but not\nlimited to, astronomy, statistics, manufacturing, econometrics, marketing, most\nof the existing techniques cannot be used as is on real-time data streams.\nFurther, the lack of characterization of performance -- both with respect to\nreal-timeliness and accuracy -- on production data sets makes model selection\nvery challenging. To this end, we present an in-depth analysis, geared towards\nreal-time streaming data, of anomaly detection techniques. Given the\nrequirements with respect to real-timeliness and accuracy, the analysis\npresented in this paper should serve as a guide for selection of the \"best\"\nanomaly detection technique. To the best of our knowledge, this is the first\ncharacterization of anomaly detection techniques proposed in very diverse set\nof fields, using production data sets corresponding to a wide set of\napplication domains.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 21:57:55 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Choudhary", "Dhruv", ""], ["Kejariwal", "Arun", ""], ["Orsini", "Francois", ""]]}, {"id": "1710.04822", "submitter": "Fang Zhang", "authors": "Fang Zhang, Xiaochen Wang, Jingfei Han, Jie Tang, Shiyin Wang,\n  Marie-Francine Moens", "title": "Fast Top-k Area Topics Extraction with Knowledge Base", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What are the most popular research topics in Artificial Intelligence (AI)? We\nformulate the problem as extracting top-$k$ topics that can best represent a\ngiven area with the help of knowledge base. We theoretically prove that the\nproblem is NP-hard and propose an optimization model, FastKATE, to address this\nproblem by combining both explicit and latent representations for each topic.\nWe leverage a large-scale knowledge base (Wikipedia) to generate topic\nembeddings using neural networks and use this kind of representations to help\ncapture the representativeness of topics for given areas. We develop a fast\nheuristic algorithm to efficiently solve the problem with a provable error\nbound. We evaluate the proposed model on three real-world datasets.\nExperimental results demonstrate our model's effectiveness, robustness,\nreal-timeness (return results in $<1$s), and its superiority over several\nalternative methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 06:34:44 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 17:23:41 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Zhang", "Fang", ""], ["Wang", "Xiaochen", ""], ["Han", "Jingfei", ""], ["Tang", "Jie", ""], ["Wang", "Shiyin", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1710.05526", "submitter": "Yiming Zhang", "authors": "Yiming Zhang, Jiacheng Luo, Xiaofeng Gao, Guihai Chen", "title": "Which is better? A Modularized Evaluation for Topic Popularity\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic popularity prediction in social networks has drawn much attention\nrecently. Various elegant models have been proposed for this issue. However,\ndifferent datasets and evaluation metrics they use lead to low comparability.\nSo far there is no unified scheme to evaluate them, making it difficult to\nselect and compare models. We conduct a comprehensible survey, propose an\nevaluation scheme and apply it to existing methods. Our scheme consists of four\nmodules: classification; qualitative evaluation on several metrics;\nquantitative experiment on real world data; final ranking with risk matrix and\n$\\textit{MinDis}$ to reflect performances under different scenarios.\nFurthermore, we analyze the efficiency and contribution of features used in\nfeature oriented methods. The results show that feature oriented methods are\nmore suitable for scenarios requiring high accuracy, while relation based\nmethods have better consistency. Our work helps researchers compare and choose\nmethods appropriately, and provides insights for further improvements.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 06:09:21 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Zhang", "Yiming", ""], ["Luo", "Jiacheng", ""], ["Gao", "Xiaofeng", ""], ["Chen", "Guihai", ""]]}, {"id": "1710.05561", "submitter": "Jukka Ruohonen", "authors": "Jukka Ruohonen", "title": "Classifying Web Exploits with Topic Modeling", "comments": "Proceedings of the 2017 28th International Workshop on Database and\n  Expert Systems Applications (DEXA).\n  http://ieeexplore.ieee.org/abstract/document/8049693/", "journal-ref": null, "doi": "10.1109/DEXA.2017.35", "report-no": null, "categories": "cs.CR cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short empirical paper investigates how well topic modeling and database\nmeta-data characteristics can classify web and other proof-of-concept (PoC)\nexploits for publicly disclosed software vulnerabilities. By using a dataset\ncomprised of over 36 thousand PoC exploits, near a 0.9 accuracy rate is\nobtained in the empirical experiment. Text mining and topic modeling are a\nsignificant boost factor behind this classification performance. In addition to\nthese empirical results, the paper contributes to the research tradition of\nenhancing software vulnerability information with text mining, providing also a\nfew scholarly observations about the potential for semi-automatic\nclassification of exploits in the existing tracking infrastructures.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 08:34:24 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Ruohonen", "Jukka", ""]]}, {"id": "1710.05649", "submitter": "Liang Pang", "authors": "Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Jingfang Xu, Xueqi Cheng", "title": "DeepRank: A New Deep Architecture for Relevance Ranking in Information\n  Retrieval", "comments": "Published as a conference paper at CIKM 2017, CIKM'17, November\n  6--10, 2017, Singapore TextNet (https://github.com/pl8787/textnet-release)\n  PyTorch (https://github.com/pl8787/DeepRank_PyTorch)", "journal-ref": null, "doi": "10.1145/3132847.3132914", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns a deep learning approach to relevance ranking in\ninformation retrieval (IR). Existing deep IR models such as DSSM and CDSSM\ndirectly apply neural networks to generate ranking scores, without explicit\nunderstandings of the relevance. According to the human judgement process, a\nrelevance label is generated by the following three steps: 1) relevant\nlocations are detected, 2) local relevances are determined, 3) local relevances\nare aggregated to output the relevance label. In this paper we propose a new\ndeep learning architecture, namely DeepRank, to simulate the above human\njudgment process. Firstly, a detection strategy is designed to extract the\nrelevant contexts. Then, a measure network is applied to determine the local\nrelevances by utilizing a convolutional neural network (CNN) or two-dimensional\ngated recurrent units (2D-GRU). Finally, an aggregation network with sequential\nintegration and term gating mechanism is used to produce a global relevance\nscore. DeepRank well captures important IR characteristics, including\nexact/semantic matching signals, proximity heuristics, query term importance,\nand diverse relevance requirement. Experiments on both benchmark LETOR dataset\nand a large scale clickthrough data show that DeepRank can significantly\noutperform learning to ranking methods, and existing deep learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 12:21:51 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 12:06:43 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Pang", "Liang", ""], ["Lan", "Yanyan", ""], ["Guo", "Jiafeng", ""], ["Xu", "Jun", ""], ["Xu", "Jingfang", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1710.05780", "submitter": "Gerasimos Spanakis", "authors": "Alexander Bartl, Gerasimos Spanakis", "title": "A retrieval-based dialogue system utilizing utterance and context\n  embeddings", "comments": "A shorter version is accepted at ICMLA2017 conference;\n  acknowledgement added; typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding semantically rich and computer-understandable representations for\ntextual dialogues, utterances and words is crucial for dialogue systems (or\nconversational agents), as their performance mostly depends on understanding\nthe context of conversations. Recent research aims at finding distributed\nvector representations (embeddings) for words, such that semantically similar\nwords are relatively close within the vector-space. Encoding the \"meaning\" of\ntext into vectors is a current trend, and text can range from words, phrases\nand documents to actual human-to-human conversations. In recent research\napproaches, responses have been generated utilizing a decoder architecture,\ngiven the vector representation of the current conversation. In this paper, the\nutilization of embeddings for answer retrieval is explored by using\nLocality-Sensitive Hashing Forest (LSH Forest), an Approximate Nearest Neighbor\n(ANN) model, to find similar conversations in a corpus and rank possible\ncandidates. Experimental results on the well-known Ubuntu Corpus (in English)\nand a customer service chat dataset (in Dutch) show that, in combination with a\ncandidate selection method, retrieval-based approaches outperform generative\nones and reveal promising future research directions towards the usability of\nsuch a system.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 15:23:56 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 07:32:32 GMT"}, {"version": "v3", "created": "Fri, 20 Oct 2017 10:16:43 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Bartl", "Alexander", ""], ["Spanakis", "Gerasimos", ""]]}, {"id": "1710.05980", "submitter": "Meng Wang", "authors": "Fang Gong, Meng Wang, Haofen Wang, Sen Wang, Mengyue Liu", "title": "SMR: Medical Knowledge Graph Embedding for Safe Medicine Recommendation", "comments": "8 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing medicine recommendation systems that are mainly based on\nelectronic medical records (EMRs) are significantly assisting doctors to make\nbetter clinical decisions benefiting both patients and caregivers. Even though\nthe growth of EMRs is at a lighting fast speed in the era of big data, content\nlimitations in EMRs restrain the existed recommendation systems to reflect\nrelevant medical facts, such as drug-drug interactions. Many medical knowledge\ngraphs that contain drug-related information, such as DrugBank, may give hope\nfor the recommendation systems. However, the direct use of these knowledge\ngraphs in the systems suffers from robustness caused by the incompleteness of\nthe graphs. To address these challenges, we stand on recent advances in graph\nembedding learning techniques and propose a novel framework, called Safe\nMedicine Recommendation (SMR), in this paper. Specifically, SMR first\nconstructs a high-quality heterogeneous graph by bridging EMRs (MIMIC-III) and\nmedical knowledge graphs (ICD-9 ontology and DrugBank). Then, SMR jointly\nembeds diseases, medicines, patients, and their corresponding relations into a\nshared lower dimensional space. Finally, SMR uses the embeddings to decompose\nthe medicine recommendation into a link prediction process while considering\nthe patient's diagnoses and adverse drug reactions. To our best knowledge, SMR\nis the first to learn embeddings of a patient-disease-medicine graph for\nmedicine recommendation in the world. Extensive experiments on real datasets\nare conducted to evaluate the effectiveness of proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 20:06:13 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 08:02:44 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2020 05:51:23 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Gong", "Fang", ""], ["Wang", "Meng", ""], ["Wang", "Haofen", ""], ["Wang", "Sen", ""], ["Liu", "Mengyue", ""]]}, {"id": "1710.06061", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Bhaskar Mitra, Matteo Venanzi, Roy Rosemarin,\n  Grzegorz Kukla, Piotr Grudzien, Nicola Cancedda", "title": "Reply With: Proactive Recommendation of Email Attachments", "comments": "CIKM2017. Proceedings of the 26th ACM International Conference on\n  Information and Knowledge Management. 2017", "journal-ref": null, "doi": "10.1145/3132847.3132979", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Email responses often contain items-such as a file or a hyperlink to an\nexternal document-that are attached to or included inline in the body of the\nmessage. Analysis of an enterprise email corpus reveals that 35% of the time\nwhen users include these items as part of their response, the attachable item\nis already present in their inbox or sent folder. A modern email client can\nproactively retrieve relevant attachable items from the user's past emails\nbased on the context of the current conversation, and recommend them for\ninclusion, to reduce the time and effort involved in composing the response. In\nthis paper, we propose a weakly supervised learning framework for recommending\nattachable items to the user. As email search systems are commonly available,\nwe constrain the recommendation task to formulating effective search queries\nfrom the context of the conversations. The query is submitted to an existing IR\nsystem to retrieve relevant items for attachment. We also present a novel\nstrategy for generating labels from an email corpus---without the need for\nmanual annotations---that can be used to train and evaluate the query\nformulation model. In addition, we describe a deep convolutional neural network\nthat demonstrates satisfactory performance on this query formulation task when\nevaluated on the publicly available Avocado dataset and a proprietary dataset\nof internal emails obtained through an employee participation program.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 02:37:18 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 10:29:50 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Van Gysel", "Christophe", ""], ["Mitra", "Bhaskar", ""], ["Venanzi", "Matteo", ""], ["Rosemarin", "Roy", ""], ["Kukla", "Grzegorz", ""], ["Grudzien", "Piotr", ""], ["Cancedda", "Nicola", ""]]}, {"id": "1710.06062", "submitter": "Vinh Nguyen", "authors": "Vinh Nguyen, Md Yasin Kabir, and Tommy Dang", "title": "CancerLinker: Explorations of Cancer Study Network", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive visualization tools are highly desirable to biologist and cancer\nresearchers to explore the complex structures, detect patterns and find out the\nrelationships among bio-molecules responsible for a cancer type. A pathway\ncontains various bio-molecules in different layers of the cell which is\nresponsible for specific cancer type. Researchers are highly interested in\nunderstanding the relationships among the proteins of different pathways and\nfurthermore want to know how those proteins are interacting in different\npathways for various cancer types. Biologists find it useful to merge the data\nof different cancer studies in a single network and see the relationships among\nthe different proteins which can help them detect the common proteins in cancer\nstudies and hence reveal the pattern of interactions of those proteins. We\nintroduce the CancerLinker, a visual analytic tool that helps researchers\nexplore cancer study interaction network. Twenty-six cancer studies are merged\nto explore pathway data and bio-molecules relationships that can provide the\nanswers to some significant questions which are helpful in cancer research. The\nCancerLinker also helps biologists explore the critical mutated proteins in\nmultiple cancer studies. A bubble graph is constructed to visualize common\nprotein based on its frequency and biological assemblies. Parallel coordinates\nhighlight patterns of patient profiles (obtained from cBioportal by WebAPI\nservices) on different attributes for a specified cancer study\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 02:40:15 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 20:04:59 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Nguyen", "Vinh", ""], ["Kabir", "Md Yasin", ""], ["Dang", "Tommy", ""]]}, {"id": "1710.06997", "submitter": "Yixing Fan", "authors": "Yixing Fan, Jiafeng Guo, Yanyan Lan, Jun Xu, Liang Pang, Xueqi Cheng", "title": "Learning Visual Features from Snapshots for Web Search", "comments": "CIKM 2017", "journal-ref": null, "doi": "10.1145/3132847.3132943", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When applying learning to rank algorithms to Web search, a large number of\nfeatures are usually designed to capture the relevance signals. Most of these\nfeatures are computed based on the extracted textual elements, link analysis,\nand user logs. However, Web pages are not solely linked texts, but have\nstructured layout organizing a large variety of elements in different styles.\nSuch layout itself can convey useful visual information, indicating the\nrelevance of a Web page. For example, the query-independent layout (i.e., raw\npage layout) can help identify the page quality, while the query-dependent\nlayout (i.e., page rendered with matched query words) can further tell rich\nstructural information (e.g., size, position and proximity) of the matching\nsignals. However, such visual information of layout has been seldom utilized in\nWeb search in the past. In this work, we propose to learn rich visual features\nautomatically from the layout of Web pages (i.e., Web page snapshots) for\nrelevance ranking. Both query-independent and query-dependent snapshots are\nconsidered as the new inputs. We then propose a novel visual perception model\ninspired by human's visual search behaviors on page viewing to extract the\nvisual features. This model can be learned end-to-end together with traditional\nhuman-crafted features. We also show that such visual features can be\nefficiently acquired in the online setting with an extended inverted indexing\nscheme. Experiments on benchmark collections demonstrate that learning visual\nfeatures from Web page snapshots can significantly improve the performance of\nrelevance ranking in ad-hoc Web retrieval tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 03:30:45 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Fan", "Yixing", ""], ["Guo", "Jiafeng", ""], ["Lan", "Yanyan", ""], ["Xu", "Jun", ""], ["Pang", "Liang", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1710.07072", "submitter": "Junhua Chen", "authors": "Junhua Chen and Wei Zeng and Junming Shao and Ge Fan", "title": "Preference Modeling by Exploiting Latent Components of Ratings", "comments": "Knowledge and Information Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding user preference is essential to the optimization of recommender\nsystems. As a feedback of user's taste, rating scores can directly reflect the\npreference of a given user to a given product. Uncovering the latent components\nof user ratings is thus of significant importance for learning user interests.\nIn this paper, a new recommendation approach, called LCR, was proposed by\ninvestigating the latent components of user ratings. The basic idea is to\ndecompose an existing rating into several components via a cost-sensitive\nlearning strategy. Specifically, each rating is assigned to several latent\nfactor models and each model is updated according to its predictive errors.\nAfterwards, these accumulated predictive errors of models are utilized to\ndecompose a rating into several components, each of which is treated as an\nindependent part to retrain the latent factor models. Finally, all latent\nfactor models are combined linearly to estimate predictive ratings for users.\nIn contrast to existing methods, LCR provides an intuitive preference modeling\nstrategy via multiple component analysis at an individual perspective.\nMeanwhile, it is verified by the experimental results on several benchmark\ndatasets that the proposed method is superior to the state-of-art methods in\nterms of recommendation accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 10:41:13 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Chen", "Junhua", ""], ["Zeng", "Wei", ""], ["Shao", "Junming", ""], ["Fan", "Ge", ""]]}, {"id": "1710.07134", "submitter": "Haekyu Park", "authors": "Haekyu Park, Hyunsik Jeon, Junghwan Kim, Beunguk Ahn, and U Kang", "title": "UniWalk: Explainable and Accurate Recommendation for Rating and Network\n  Data", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we leverage social network data and observed ratings to correctly\nrecommend proper items and provide a persuasive explanation for the\nrecommendations? Many online services provide social networks among users, and\nit is crucial to utilize social information since recommendation by a friend is\nmore likely to grab attention than the one from a random user. Also, explaining\nwhy items are recommended is very important in encouraging the users' actions\nsuch as actual purchases. Exploiting both ratings and social graph for\nrecommendation, however, is not trivial because of the heterogeneity of the\ndata.\n  In this paper, we propose UniWalk, an explainable and accurate recommender\nsystem that exploits both social network and rating data. UniWalk combines both\ndata into a unified graph, learns latent features of users and items, and\nrecommends items to each user through the features. Importantly, it explains\nwhy items are recommended together with the recommendation results. Extensive\nexperiments show that UniWalk provides the best explainability and achieves the\nstate-of-the-art-accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 12:40:12 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Park", "Haekyu", ""], ["Jeon", "Hyunsik", ""], ["Kim", "Junghwan", ""], ["Ahn", "Beunguk", ""], ["Kang", "U", ""]]}, {"id": "1710.08045", "submitter": "Annie Marsden", "authors": "Annie Marsden, Sergio Bacallado", "title": "Sequential Matrix Completion", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for sequential matrix completion in a\nrecommender system setting, where the $(i,j)$th entry of the matrix corresponds\nto a user $i$'s rating of product $j$. The objective of the algorithm is to\nprovide a sequential policy for user-product pair recommendation which will\nyield the highest possible ratings after a finite time horizon. The algorithm\nuses a Gamma process factor model with two posterior-focused bandit policies,\nThompson Sampling and Information-Directed Sampling. While Thompson Sampling\nshows competitive performance in simulations, state-of-the-art performance is\nobtained from Information-Directed Sampling, which makes its recommendations\nbased off a ratio between the expected reward and a measure of information\ngain. To our knowledge, this is the first implementation of Information\nDirected Sampling on large real datasets.\n  This approach contributes to a recent line of research on bandit approaches\nto collaborative filtering including Kawale et al. (2015), Li et al. (2010),\nBresler et al. (2014), Li et al. (2016), Deshpande & Montanari (2012), and Zhao\net al. (2013). The setting of this paper, as has been noted in Kawale et al.\n(2015) and Zhao et al. (2013), presents significant challenges to bounding\nregret after finite horizons. We discuss these challenges in relation to\nsimpler models for bandits with side information, such as linear or gaussian\nprocess bandits, and hope the experiments presented here motivate further\nresearch toward theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 00:20:32 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Marsden", "Annie", ""], ["Bacallado", "Sergio", ""]]}, {"id": "1710.08321", "submitter": "Muktabh Mayank Srivastava", "authors": "Nishant Nikhil, Muktabh Mayank Srivastava", "title": "Content Based Document Recommender using Deep Learning", "comments": "Accepted in ICICI 2017, Coimbatore, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the recent advancements in information technology there has been a huge\nsurge in amount of data available. But information retrieval technology has not\nbeen able to keep up with this pace of information generation resulting in over\nspending of time for retrieving relevant information. Even though systems exist\nfor assisting users to search a database along with filtering and recommending\nrelevant information, but recommendation system which uses content of documents\nfor recommendation still have a long way to mature. Here we present a Deep\nLearning based supervised approach to recommend similar documents based on the\nsimilarity of content. We combine the C-DSSM model with Word2Vec distributed\nrepresentations of words to create a novel model to classify a document pair as\nrelevant/irrelavant by assigning a score to it. Using our model retrieval of\ndocuments can be done in O(1) time and the memory complexity is O(n), where n\nis number of documents.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 15:08:38 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Nikhil", "Nishant", ""], ["Srivastava", "Muktabh Mayank", ""]]}, {"id": "1710.08389", "submitter": "Dirk Lewandowski", "authors": "Dirk Lewandowski, Friederike Kerkmann, Sandra Ruemmele, Sebastian\n  Suenkler", "title": "An Empirical Investigation On Search Engine Ad Disclosure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This representative study of German search engine users (N=1,000) focuses on\nthe ability of users to distinguish between organic results and advertisements\non Google results pages. We combine questions about Google's business with\ntask-based studies in which users were asked to distinguish between ads and\norganic results in screenshots of results pages. We find that only a small\npercentage of users is able to reliably distinguish between ads and organic\nresults, and that user knowledge of Google's business model is very limited. We\nconclude that ads are insufficiently labelled as such, and that many users may\nclick on ads assuming that they are selecting organic results.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 17:00:13 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Lewandowski", "Dirk", ""], ["Kerkmann", "Friederike", ""], ["Ruemmele", "Sandra", ""], ["Suenkler", "Sebastian", ""]]}, {"id": "1710.08516", "submitter": "Yong Zheng", "authors": "Yong Zheng", "title": "Interpreting Contextual Effects By Contextual Modeling In Recommender\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have been widely applied to assist user's decision making\nby providing a list of personalized item recommendations. Context-aware\nrecommender systems (CARS) additionally take context information into\nconsidering in the recommendation process, since user's tastes on the items may\nvary from contexts to contexts. Several context-aware recommendation algorithms\nhave been proposed and developed to improve the quality of recommendations.\nHowever, there are limited research which explore and discuss the capability of\ninterpreting the contextual effects by the recommendation models. In this\npaper, we specifically focus on different contextual modeling approaches,\nreshape the structure of the models, and exploit how to utilize the existing\ncontextual modeling to interpret the contextual effects in the recommender\nsystems. We compare the explanations of contextual effects, as well as the\nrecommendation performance over two-real world data sets in order to examine\nthe quality of interpretations.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 21:49:49 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Zheng", "Yong", ""]]}, {"id": "1710.08634", "submitter": "Ricardo Usbeck", "authors": "Ricardo Usbeck and Michael Hoffmann and Michael R\\\"oder and Jens\n  Lehmann and Axel-Cyrille Ngonga Ngomo", "title": "Using Multi-Label Classification for Improved Question Answering", "comments": "15 pages, 4 Tables, 3 Figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plethora of diverse approaches for question answering over RDF data have\nbeen developed in recent years. While the accuracy of these systems has\nincreased significantly over time, most systems still focus on particular types\nof questions or particular challenges in question answering. What is a curse\nfor single systems is a blessing for the combination of these systems. We show\nin this paper how machine learning techniques can be applied to create a more\naccurate question answering metasystem by reusing existing systems. In\nparticular, we develop a multi-label classification-based metasystem for\nquestion answering over 6 existing systems using an innovative set of 14\nquestion features. The metasystem outperforms the best single system by 14%\nF-measure on the recent QALD-6 benchmark. Furthermore, we analyzed the\ninfluence and correlation of the underlying features on the metasystem quality.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 07:40:16 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Usbeck", "Ricardo", ""], ["Hoffmann", "Michael", ""], ["R\u00f6der", "Michael", ""], ["Lehmann", "Jens", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "1710.09085", "submitter": "Prasenjit Majumder", "authors": "Sounak Banerjee, Prasenjit Majumder, Mandar Mitra", "title": "Re-evaluating the need for Modelling Term-Dependence in Text\n  Classification Problems", "comments": "23 Pages, 16 Figures, 3 Tables, Some Figures at the end of the\n  document because of limiting factors in the Latex format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A substantial amount of research has been carried out in developing machine\nlearning algorithms that account for term dependence in text classification.\nThese algorithms offer acceptable performance in most cases but they are\nassociated with a substantial cost. They require significantly greater\nresources to operate. This paper argues against the justification of the higher\ncosts of these algorithms, based on their performance in text classification\nproblems. In order to prove the conjecture, the performance of one of the best\ndependence models is compared to several well established algorithms in text\nclassification. A very specific collection of datasets have been designed,\nwhich would best reflect the disparity in the nature of text data, that are\npresent in real world applications. The results show that even one of the best\nterm dependence models, performs decent at best when compared to other\nindependence models. Coupled with their substantially greater requirement for\nhardware resources for operation, this makes them an impractical choice for\nbeing used in real world scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 06:26:28 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Banerjee", "Sounak", ""], ["Majumder", "Prasenjit", ""], ["Mitra", "Mandar", ""]]}, {"id": "1710.09182", "submitter": "Manuel Sebastian Mariani", "authors": "Manuel Sebastian Mariani, Matus Medo, Fran\\c{c}ois Lafond", "title": "Early identification of important patents through network centrality", "comments": "14 pages", "journal-ref": "Technological Forecasting and Social Change (2018)", "doi": "10.1016/j.techfore.2018.01.036", "report-no": null, "categories": "cs.SI cs.DL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenging problems in technological forecasting is to\nidentify as early as possible those technologies that have the potential to\nlead to radical changes in our society. In this paper, we use the US patent\ncitation network (1926-2010) to test our ability to early identify a list of\nhistorically significant patents through citation network analysis. We show\nthat in order to effectively uncover these patents shortly after they are\nissued, we need to go beyond raw citation counts and take into account both the\ncitation network topology and temporal information. In particular, an\nage-normalized measure of patent centrality, called rescaled PageRank, allows\nus to identify the significant patents earlier than citation count and PageRank\nscore. In addition, we find that while high-impact patents tend to rely on\nother high-impact patents in a similar way as scientific papers, the patents'\ncitation dynamics is significantly slower than that of papers, which makes the\nearly identification of significant patents more challenging than that of\nsignificant papers.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 11:43:52 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Mariani", "Manuel Sebastian", ""], ["Medo", "Matus", ""], ["Lafond", "Fran\u00e7ois", ""]]}, {"id": "1710.09824", "submitter": "Preeti Bhargava", "authors": "Sarah Ellinger, Prantik Bhattacharyya, Preeti Bhargava, Nemanja\n  Spasojevic", "title": "Klout Topics for Modeling Interests and Expertise of Users Across Social\n  Networks", "comments": "4 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Klout Topics, a lightweight ontology to describe social\nmedia users' topics of interest and expertise. Klout Topics is designed to: be\nhuman-readable and consumer-friendly; cover multiple domains of knowledge in\ndepth; and promote data extensibility via knowledge base entities. We discuss\nwhy this ontology is well-suited for text labeling and interest modeling\napplications, and how it compares to available alternatives. We show its\ncoverage against common social media interest sets, and examples of how it is\nused to model the interests of over 780M social media users on Klout.com.\nFinally, we open the ontology for external use.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 17:42:13 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Ellinger", "Sarah", ""], ["Bhattacharyya", "Prantik", ""], ["Bhargava", "Preeti", ""], ["Spasojevic", "Nemanja", ""]]}, {"id": "1710.10177", "submitter": "Juergen Mueller", "authors": "Juergen Mueller", "title": "Combining Aspects of Genetic Algorithms with Weighted Recommender\n  Hybridization", "comments": "10 pages, 6 figures, 2 tables, iiWAS '17, December 4-6, 2017,\n  Salzburg, Austria", "journal-ref": null, "doi": "10.1145/3151759.3151765", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are established means to inspire users to watch\ninteresting movies, discover baby names, or read books. The recommendation\nquality further improves by combining the results of multiple recommendation\nalgorithms using hybridization methods. In this paper, we focus on the task of\ncombining unscored recommendations into a single ensemble. Our proposed method\nis inspired by genetic algorithms. It repeatedly selects items from the\nrecommendations to create a population of items that will be used for the final\nensemble. We compare our method with a weighted voting method and test the\nperformance of both in a movie- and name-recommendation scenario. We were able\nto outperform the weighted method on both datasets by 20.3 % and 31.1 % and\ndecreased the overall execution time by up to 19.9 %. Our results do not only\npropose a new kind of hybridization method, but introduce the field of\nrecommender hybridization to further work with genetic algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 14:43:15 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Mueller", "Juergen", ""]]}, {"id": "1710.10201", "submitter": "Dominika Tkaczyk", "authors": "Dominika Tkaczyk", "title": "New Methods for Metadata Extraction from Scientific Literature", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the past few decades we have witnessed digital revolution, which moved\nscholarly communication to electronic media and also resulted in a substantial\nincrease in its volume. Nowadays keeping track with the latest scientific\nachievements poses a major challenge for the researchers. Scientific\ninformation overload is a severe problem that slows down scholarly\ncommunication and knowledge propagation across the academia. Modern research\ninfrastructures facilitate studying scientific literature by providing\nintelligent search tools, proposing similar and related documents, visualizing\ncitation and author networks, assessing the quality and impact of the articles,\nand so on. In order to provide such high quality services the system requires\nthe access not only to the text content of stored documents, but also to their\nmachine-readable metadata. Since in practice good quality metadata is not\nalways available, there is a strong demand for a reliable automatic method of\nextracting machine-readable metadata directly from source documents. This\nresearch addresses these problems by proposing an automatic, accurate and\nflexible algorithm for extracting wide range of metadata directly from\nscientific articles in born-digital form. Extracted information includes basic\ndocument metadata, structured full text and bibliography section. Designed as a\nuniversal solution, proposed algorithm is able to handle a vast variety of\npublication layouts with high precision and thus is well-suited for analyzing\nheterogeneous document collections. This was achieved by employing supervised\nand unsupervised machine-learning algorithms trained on large, diverse\ndatasets. The evaluation we conducted showed good performance of proposed\nmetadata extraction algorithm. The comparison with other similar solutions also\nproved our algorithm performs better than competition for most metadata types.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 15:33:29 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Tkaczyk", "Dominika", ""]]}, {"id": "1710.10498", "submitter": "Shubhangi Tandon", "authors": "Sharath T. S. and Shubhangi Tandon", "title": "Topic Based Sentiment Analysis Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper , we tackle Sentiment Analysis conditioned on a Topic in\nTwitter data using Deep Learning . We propose a 2-tier approach : In the first\nphase we create our own Word Embeddings and see that they do perform better\nthan state-of-the-art embeddings when used with standard classifiers. We then\nperform inference on these embeddings to learn more about a word with respect\nto all the topics being considered, and also the top n-influencing words for\neach topic. In the second phase we use these embeddings to predict the\nsentiment of the tweet with respect to a given topic, and all other topics\nunder discussion.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 17:13:49 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["S.", "Sharath T.", ""], ["Tandon", "Shubhangi", ""]]}, {"id": "1710.10814", "submitter": "Lang-Chi Yu", "authors": "Lang-Chi Yu, Yi-Hsuan Yang, Yun-Ning Hung, Yi-An Chen", "title": "Hit Song Prediction for Pop Music by Siamese CNN with Ranking Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model for hit song prediction can be used in the pop music industry to\nidentify emerging trends and potential artists or songs before they are\nmarketed to the public. While most previous work formulates hit song prediction\nas a regression or classification problem, we present in this paper a\nconvolutional neural network (CNN) model that treats it as a ranking problem.\nSpecifically, we use a commercial dataset with daily play-counts to train a\nmulti-objective Siamese CNN model with Euclidean loss and pairwise ranking loss\nto learn from audio the relative ranking relations among songs. Besides, we\ndevise a number of pair sampling methods according to some empirical\nobservation of the data. Our experiment shows that the proposed model with a\nsampling method called A/B sampling leads to much higher accuracy in hit song\nprediction than the baseline regression model. Moreover, we can further improve\nthe accuracy by using a neural attention mechanism to extract the highlights of\nsongs and by using a separate CNN model to offer high-level features of songs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 09:10:11 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Yu", "Lang-Chi", ""], ["Yang", "Yi-Hsuan", ""], ["Hung", "Yun-Ning", ""], ["Chen", "Yi-An", ""]]}, {"id": "1710.10974", "submitter": "Pranay Manocha Mr.", "authors": "Pranay Manocha, Rohan Badlani, Anurag Kumar, Ankit Shah, Benjamin\n  Elizalde, Bhiksha Raj", "title": "Content-based Representations of audio using Siamese neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the problem of content-based retrieval for audio,\nwhich aims to retrieve all semantically similar audio recordings for a given\naudio clip query. This problem is similar to the problem of query by example of\naudio, which aims to retrieve media samples from a database, which are similar\nto the user-provided example. We propose a novel approach which encodes the\naudio into a vector representation using Siamese Neural Networks. The goal is\nto obtain an encoding similar for files belonging to the same audio class, thus\nallowing retrieval of semantically similar audio. Using simple similarity\nmeasures such as those based on simple euclidean distance and cosine similarity\nwe show that these representations can be very effectively used for retrieving\nrecordings similar in audio content.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 14:32:02 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 02:57:38 GMT"}, {"version": "v3", "created": "Thu, 15 Feb 2018 15:05:30 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Manocha", "Pranay", ""], ["Badlani", "Rohan", ""], ["Kumar", "Anurag", ""], ["Shah", "Ankit", ""], ["Elizalde", "Benjamin", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1710.10994", "submitter": "Mohammad Ebrahim Khademi", "authors": "Mohammad Ebrahim Khademi, Mohammad Fakhredanesh and Seyed Mojtaba\n  Hoseini", "title": "Conceptual Text Summarizer: A new model in continuous vector space", "comments": "The experimental results completed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional methods of summarization are not cost-effective and possible\ntoday. Extractive summarization is a process that helps to extract the most\nimportant sentences from a text automatically and generates a short informative\nsummary. In this work, we propose an unsupervised method to summarize Persian\ntexts. This method is a novel hybrid approach that clusters the concepts of the\ntext using deep learning and traditional statistical methods. First we produce\na word embedding based on Hamshahri2 corpus and a dictionary of word\nfrequencies. Then the proposed algorithm extracts the keywords of the document,\nclusters its concepts, and finally ranks the sentences to produce the summary.\nWe evaluated the proposed method on Pasokh single-document corpus using the\nROUGE evaluation measure. Without using any hand-crafted features, our proposed\nmethod achieves state-of-the-art results. We compared our unsupervised method\nwith the best supervised Persian methods and we achieved an overall improvement\nof ROUGE-2 recall score of 7.5%.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 14:55:25 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 13:34:02 GMT"}, {"version": "v3", "created": "Sat, 1 Sep 2018 11:40:10 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Khademi", "Mohammad Ebrahim", ""], ["Fakhredanesh", "Mohammad", ""], ["Hoseini", "Seyed Mojtaba", ""]]}, {"id": "1710.11027", "submitter": "Diego Esteves", "authors": "Diego Esteves and Rafael Peres and Jens Lehmann and Giulio Napolitano", "title": "Named Entity Recognition in Twitter using Images and Text", "comments": "The 3rd International Workshop on Natural Language Processing for\n  Informal Text (NLPIT 2017), 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Recognition (NER) is an important subtask of information\nextraction that seeks to locate and recognise named entities. Despite recent\nachievements, we still face limitations with correctly detecting and\nclassifying entities, prominently in short and noisy text, such as Twitter. An\nimportant negative aspect in most of NER approaches is the high dependency on\nhand-crafted features and domain-specific knowledge, necessary to achieve\nstate-of-the-art results. Thus, devising models to deal with such\nlinguistically complex contexts is still challenging. In this paper, we propose\na novel multi-level architecture that does not rely on any specific linguistic\nresource or encoded rule. Unlike traditional approaches, we use features\nextracted from images and text to classify named entities. Experimental tests\nagainst state-of-the-art NER for Twitter on the Ritter dataset present\ncompetitive results (0.59 F-measure), indicating that this approach may lead\ntowards better NER models.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 15:56:03 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Esteves", "Diego", ""], ["Peres", "Rafael", ""], ["Lehmann", "Jens", ""], ["Napolitano", "Giulio", ""]]}, {"id": "1710.11231", "submitter": "Philipp Mayr", "authors": "Philipp Mayr, Ingo Frommholz, Guillaume Cabanac", "title": "Bibliometric-Enhanced Information Retrieval: 5th International BIR\n  Workshop", "comments": "6 pages, workshop paper accepted at 39th European Conference on IR\n  Research, ECIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bibliometric-enhanced Information Retrieval (BIR) workshops serve as the\nannual gathering of IR researchers who address various information-related\ntasks on scientific corpora and bibliometrics. The workshop features original\napproaches to search, browse, and discover value-added knowledge from\nscientific documents and related information networks (e.g., terms, authors,\ninstitutions, references). We welcome contributions elaborating on dedicated IR\nsystems, as well as studies revealing original characteristics on how\nscientific knowledge is created, communicated, and used. In this paper we\nintroduce the BIR workshop series and discuss some selected papers presented at\nprevious BIR workshops.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 20:39:31 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Mayr", "Philipp", ""], ["Frommholz", "Ingo", ""], ["Cabanac", "Guillaume", ""]]}]