[{"id": "1411.0129", "submitter": "Stevan Harnad", "authors": "Philippe Vincent-Lamarre, Alexandre Blondin Mass\\'e, Marcos Lopes,\n  M\\'elanie Lord, Odile Marcotte, Stevan Harnad", "title": "The Latent Structure of Dictionaries", "comments": "38 pages, 10 figures, 2 tables, 73 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How many words (and which ones) are sufficient to define all other words?\nWhen dictionaries are analyzed as directed graphs with links from defining\nwords to defined words, they reveal a latent structure. Recursively removing\nall words that are reachable by definition but that do not define any further\nwords reduces the dictionary to a Kernel of about 10%. This is still not the\nsmallest number of words that can define all the rest. About 75% of the Kernel\nturns out to be its Core, a Strongly Connected Subset of words with a\ndefinitional path to and from any pair of its words and no word's definition\ndepending on a word outside the set. But the Core cannot define all the rest of\nthe dictionary. The 25% of the Kernel surrounding the Core consists of small\nstrongly connected subsets of words: the Satellites. The size of the smallest\nset of words that can define all the rest (the graph's Minimum Feedback Vertex\nSet or MinSet) is about 1% of the dictionary, 15% of the Kernel, and half-Core,\nhalf-Satellite. But every dictionary has a huge number of MinSets. The Core\nwords are learned earlier, more frequent, and less concrete than the\nSatellites, which in turn are learned earlier and more frequent but more\nconcrete than the rest of the Dictionary. In principle, only one MinSet's words\nwould need to be grounded through the sensorimotor capacity to recognize and\ncategorize their referents. In a dual-code sensorimotor-symbolic model of the\nmental lexicon, the symbolic code could do all the rest via re-combinatory\ndefinition.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 15:52:05 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2016 13:49:33 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Vincent-Lamarre", "Philippe", ""], ["Mass\u00e9", "Alexandre Blondin", ""], ["Lopes", "Marcos", ""], ["Lord", "M\u00e9lanie", ""], ["Marcotte", "Odile", ""], ["Harnad", "Stevan", ""]]}, {"id": "1411.0541", "submitter": "Baharan Mirzasoleiman", "authors": "Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar, and Andreas Krause", "title": "Distributed Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large-scale machine learning problems--clustering, non-parametric\nlearning, kernel machines, etc.--require selecting a small yet representative\nsubset from a large dataset. Such problems can often be reduced to maximizing a\nsubmodular set function subject to various constraints. Classical approaches to\nsubmodular optimization require centralized access to the full dataset, which\nis impractical for truly large-scale problems. In this paper, we consider the\nproblem of submodular function maximization in a distributed fashion. We\ndevelop a simple, two-stage protocol GreeDi, that is easily implemented using\nMapReduce style computations. We theoretically analyze our approach, and show\nthat under certain natural conditions, performance close to the centralized\napproach can be achieved. We begin with monotone submodular maximization\nsubject to a cardinality constraint, and then extend this approach to obtain\napproximation guarantees for (not necessarily monotone) submodular maximization\nsubject to more general constraints including matroid or knapsack constraints.\nIn our extensive experiments, we demonstrate the effectiveness of our approach\non several applications, including sparse Gaussian process inference and\nexemplar based clustering on tens of millions of examples using Hadoop.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 16:03:05 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 16:32:35 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Mirzasoleiman", "Baharan", ""], ["Karbasi", "Amin", ""], ["Sarkar", "Rik", ""], ["Krause", "Andreas", ""]]}, {"id": "1411.1006", "submitter": "Javid Dadashkarimi", "authors": "Javid Dadashkarimi and Azadeh Shakery and Heshaam Faili", "title": "A Probabilistic Translation Method for Dictionary-based Cross-lingual\n  Information Retrieval in Agglutinative Languages", "comments": "The 3rd conference of Computational Linguistic, Sharif University of\n  Technology, November 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translation ambiguity, out of vocabulary words and missing some translations\nin bilingual dictionaries make dictionary-based Cross-language Information\nRetrieval (CLIR) a challenging task. Moreover, in agglutinative languages which\ndo not have reliable stemmers, missing various lexical formations in bilingual\ndictionaries degrades CLIR performance. This paper aims to introduce a\nprobabilistic translation model to solve the ambiguity problem, and also to\nprovide most likely formations of a dictionary candidate. We propose Minimum\nEdit Support Candidates (MESC) method that exploits a monolingual corpus and a\nbilingual dictionary to translate users' native language queries to documents'\nlanguage. Our experiments show that the proposed method outperforms\nstate-of-the-art dictionary-based English-Persian CLIR.\n", "versions": [{"version": "v1", "created": "Tue, 4 Nov 2014 19:15:59 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 06:55:11 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Dadashkarimi", "Javid", ""], ["Shakery", "Azadeh", ""], ["Faili", "Heshaam", ""]]}, {"id": "1411.1220", "submitter": "Peter Sanders", "authors": "Jonathan Dimond and Peter Sanders", "title": "Faster Exact Search using Document Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how full-text search based on inverted indices can be accelerated by\nclustering the documents without losing results (SeCluD -- SEarch with\nCLUstered Documents). We develop a fast multilevel clustering algorithm that\nexplicitly uses query cost for conjunctive queries as an objective function.\nDepending on the inputs we get up to four times faster than non-clustered\nsearch. The resulting clusters are also useful for data compression and for\ndistributing the work over many machines.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 10:42:29 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Dimond", "Jonathan", ""], ["Sanders", "Peter", ""]]}, {"id": "1411.1635", "submitter": "Philipp Mayr", "authors": "Philipp Mayr, Andrea Scharnhorst", "title": "Scientometrics and Information Retrieval - weak-links revitalized", "comments": "8 pages, 1 figure, editorial for a special issue to appear in\n  Scientometrics", "journal-ref": null, "doi": "10.1007/s11192-014-1484-3", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This special issue brings together eight papers from experts of communities\nwhich often have been perceived as different once: bibliometrics,\nscientometrics and informetrics on the one side and information retrieval on\nthe other. The idea of this special issue started at the workshop \"Combining\nBibliometrics and Information Retrieval\" held at the 14th International\nConference of Scientometrics and Informetrics, Vienna, July 14-19, 2013. Our\nmotivation as guest editors started from the observation that main discourses\nin both fields are different, that communities are only partly overlapping and\nfrom the belief that a knowledge transfer would be profitable for both sides.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 15:26:37 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Mayr", "Philipp", ""], ["Scharnhorst", "Andrea", ""]]}, {"id": "1411.1752", "submitter": "Adarsh Prasad", "authors": "Adarsh Prasad, Stefanie Jegelka and Dhruv Batra", "title": "Submodular meets Structured: Finding Diverse Subsets in\n  Exponentially-Large Structured Item Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To cope with the high level of ambiguity faced in domains such as Computer\nVision or Natural Language processing, robust prediction methods often search\nfor a diverse set of high-quality candidate solutions or proposals. In\nstructured prediction problems, this becomes a daunting task, as the solution\nspace (image labelings, sentence parses, etc.) is exponentially large. We study\ngreedy algorithms for finding a diverse subset of solutions in\nstructured-output spaces by drawing new connections between submodular\nfunctions over combinatorial item sets and High-Order Potentials (HOPs) studied\nfor graphical models. Specifically, we show via examples that when marginal\ngains of submodular diversity functions allow structured representations, this\nenables efficient (sub-linear time) approximate maximization by reducing the\ngreedy augmentation step to inference in a factor graph with appropriately\nconstructed HOPs. We discuss benefits, tradeoffs, and show that our\nconstructions lead to significantly better proposals.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 20:07:37 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Prasad", "Adarsh", ""], ["Jegelka", "Stefanie", ""], ["Batra", "Dhruv", ""]]}, {"id": "1411.3302", "submitter": "Atanu Roy", "authors": "Chandrima Sarkar, Atanu Roy", "title": "Using Gaussian Measures for Efficient Constraint Based Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel iterative multiphase clustering technique\nfor efficiently clustering high dimensional data points. For this purpose we\nimplement clustering feature (CF) tree on a real data set and a Gaussian\ndensity distribution constraint on the resultant CF tree. The post processing\nby the application of Gaussian density distribution function on the\nmicro-clusters leads to refinement of the previously formed clusters thus\nimproving their quality. This algorithm also succeeds in overcoming the\ninherent drawbacks of conventional hierarchical methods of clustering like\ninability to undo the change made to the dendogram of the data points.\nMoreover, the constraint measure applied in the algorithm makes this clustering\ntechnique suitable for need driven data analysis. We provide veracity of our\nclaim by evaluating our algorithm with other similar clustering algorithms.\nIntroduction\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 20:14:48 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Sarkar", "Chandrima", ""], ["Roy", "Atanu", ""]]}, {"id": "1411.3315", "submitter": "Rami Al-Rfou", "authors": "Vivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and Steven Skiena", "title": "Statistically Significant Detection of Linguistic Change", "comments": "11 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new computational approach for tracking and detecting\nstatistically significant linguistic shifts in the meaning and usage of words.\nSuch linguistic shifts are especially prevalent on the Internet, where the\nrapid exchange of ideas can quickly change a word's meaning. Our meta-analysis\napproach constructs property time series of word usage, and then uses\nstatistically sound change point detection algorithms to identify significant\nlinguistic shifts.\n  We consider and analyze three approaches of increasing complexity to generate\nsuch linguistic property time series, the culmination of which uses\ndistributional characteristics inferred from word co-occurrences. Using\nrecently proposed deep neural language models, we first train vector\nrepresentations of words for each time period. Second, we warp the vector\nspaces into one unified coordinate system. Finally, we construct a\ndistance-based distributional time series for each word to track it's\nlinguistic displacement over time.\n  We demonstrate that our approach is scalable by tracking linguistic change\nacross years of micro-blogging using Twitter, a decade of product reviews using\na corpus of movie reviews from Amazon, and a century of written books using the\nGoogle Book-ngrams. Our analysis reveals interesting patterns of language usage\nchange commensurate with each medium.\n", "versions": [{"version": "v1", "created": "Wed, 12 Nov 2014 20:37:08 GMT"}], "update_date": "2014-11-13", "authors_parsed": [["Kulkarni", "Vivek", ""], ["Al-Rfou", "Rami", ""], ["Perozzi", "Bryan", ""], ["Skiena", "Steven", ""]]}, {"id": "1411.3650", "submitter": "Azin Ashkan", "authors": "Azin Ashkan, Branislav Kveton, Shlomo Berkovsky, Zheng Wen", "title": "DUM: Diversity-Weighted Utility Maximization for Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for diversification of recommendation lists manifests in a number of\nrecommender systems use cases. However, an increase in diversity may undermine\nthe utility of the recommendations, as relevant items in the list may be\nreplaced by more diverse ones. In this work we propose a novel method for\nmaximizing the utility of the recommended items subject to the diversity of\nuser's tastes, and show that an optimal solution to this problem can be found\ngreedily. We evaluate the proposed method in two online user studies as well as\nin an offline analysis incorporating a number of evaluation metrics. The\nresults of evaluations show the superiority of our method over a number of\nbaselines.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 18:27:10 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Ashkan", "Azin", ""], ["Kveton", "Branislav", ""], ["Berkovsky", "Shlomo", ""], ["Wen", "Zheng", ""]]}, {"id": "1411.3675", "submitter": "Linhong Zhu", "authors": "Linhong Zhu, Dong Guo, Junming Yin, Greg Ver Steeg, Aram Galstyan", "title": "Scalable Link Prediction in Dynamic Networks via Non-Negative Matrix\n  Factorization", "comments": "Technical report for paper \"Scalable Temporal Latent Space Inference\n  for Link Prediction in Dynamic Social Networks\" that appears in IEEE\n  Transactions on Knowledge and Data Engineering 2016", "journal-ref": null, "doi": "10.1109/TKDE.2016.2591009", "report-no": null, "categories": "cs.SI cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable temporal latent space model for link prediction in\ndynamic social networks, where the goal is to predict links over time based on\na sequence of previous graph snapshots. The model assumes that each user lies\nin an unobserved latent space and interactions are more likely to form between\nsimilar users in the latent space representation. In addition, the model allows\neach user to gradually move its position in the latent space as the network\nstructure evolves over time. We present a global optimization algorithm to\neffectively infer the temporal latent space, with a quadratic convergence rate.\nTwo alternative optimization algorithms with local and incremental updates are\nalso proposed, allowing the model to scale to larger networks without\ncompromising prediction accuracy. Empirically, we demonstrate that our model,\nwhen evaluated on a number of real-world dynamic networks, significantly\noutperforms existing approaches for temporal link prediction in terms of both\nscalability and predictive power.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 19:36:54 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 00:03:01 GMT"}, {"version": "v3", "created": "Sat, 23 Jul 2016 22:37:22 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Zhu", "Linhong", ""], ["Guo", "Dong", ""], ["Yin", "Junming", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1411.3737", "submitter": "Ahmed Elmisery", "authors": "Ahmed M. Elmisery, Seungmin Rho, Dmitri Botvich", "title": "Holistic Collaborative Privacy Framework for Users' Privacy in Social\n  Recommender Service", "comments": null, "journal-ref": "Journal of Platform Technology, March 2014 Volume 02-01 Pages\n  11-31", "doi": "10.13140/2.1.1714.5289", "report-no": "ICT Platform Society Volume 02-01", "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current business model for existing recommender services is centered\naround the availability of users' personal data at their side whereas consumers\nhave to trust that the recommender service providers will not use their data in\na malicious way. With the increasing number of cases for privacy breaches,\ndifferent countries and corporations have issued privacy laws and regulations\nto define the best practices for the protection of personal information. The\ndata protection directive 95/46/EC and the privacy principles established by\nthe Organization for Economic Cooperation and Development (OECD) are examples\nof such regulation frameworks. In this paper, we assert that utilizing\nthird-party recommender services to generate accurate referrals are feasible,\nwhile preserving the privacy of the users' sensitive information which will be\nresiding on a clear form only on his/her own device. As a result, each user who\nbenefits from the third-party recommender service will have absolute control\nover what to release from his/her own preferences. We proposed a collaborative\nprivacy middleware that executes a two stage concealment process within a\ndistributed data collection protocol in order to attain this claim.\nAdditionally, the proposed solution complies with one of the common privacy\nregulation frameworks for fair information practice in a natural and functional\nway -which is OECD privacy principles. The approach presented in this paper is\neasily integrated into the current business model as it is implemented using a\nmiddleware that runs at the end-users side and utilizes the social nature of\ncontent distribution services to implement a topological data collection\nprotocol.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 21:07:53 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Elmisery", "Ahmed M.", ""], ["Rho", "Seungmin", ""], ["Botvich", "Dmitri", ""]]}, {"id": "1411.3761", "submitter": "Amit Sheth", "authors": "Delroy Cameron, Amit Sheth, Nishita Jaykumar, Krishnaprasad\n  Thirunarayan, Gaurish Anand, Gary A. Smith", "title": "A Hybrid Approach to Finding Relevant Social Media Content for Complex\n  Domain Specific Information Needs", "comments": "Accepted for publication: Journal of Web Semantics, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While contemporary semantic search systems offer to improve classical\nkeyword-based search, they are not always adequate for complex domain specific\ninformation needs. The domain of prescription drug abuse, for example, requires\nknowledge of both ontological concepts and 'intelligible constructs' not\ntypically modeled in ontologies. These intelligible constructs convey essential\ninformation that include notions of intensity, frequency, interval, dosage and\nsentiments, which could be important to the holistic needs of the information\nseeker. We present a hybrid approach to domain specific information retrieval\n(or knowledge-aware search) that integrates ontology-driven query\ninterpretation with synonym-based query expansion and domain specific rules, to\nfacilitate search in social media. Our framework is based on a context-free\ngrammar (CFG) that defines the query language of constructs interpretable by\nthe search system. The grammar provides two levels of semantic interpretation:\n1) a top-level CFG that facilitates retrieval of diverse textual patterns,\nwhich belong to broad templates and 2) a low-level CFG that enables\ninterpretation of certain specific expressions that belong to such patterns.\nThese low-level expressions occur as concepts from four different categories of\ndata: 1) ontological concepts, 2) concepts in lexicons (such as emotions and\nsentiments), 3) concepts in lexicons with only partial ontology representation,\ncalled lexico-ontology concepts (such as side effects and routes of\nadministration (ROA)), and 4) domain specific expressions (such as date, time,\ninterval, frequency and dosage) derived solely through rules. Our approach is\nembodied in a novel Semantic Web platform called PREDOSE developed for\nprescription drug abuse epidemiology.\n  Keywords: Knowledge-Aware Search, Ontology, Semantic Search, Background\nKnowledge, Context-Free Grammar\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 23:05:41 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Cameron", "Delroy", ""], ["Sheth", "Amit", ""], ["Jaykumar", "Nishita", ""], ["Thirunarayan", "Krishnaprasad", ""], ["Anand", "Gaurish", ""], ["Smith", "Gary A.", ""]]}, {"id": "1411.3787", "submitter": "Ping Li", "authors": "Anshumali Shrivastava, Ping Li", "title": "Asymmetric Minwise Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DB cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minwise hashing (Minhash) is a widely popular indexing scheme in practice.\nMinhash is designed for estimating set resemblance and is known to be\nsuboptimal in many applications where the desired measure is set overlap (i.e.,\ninner product between binary vectors) or set containment. Minhash has inherent\nbias towards smaller sets, which adversely affects its performance in\napplications where such a penalization is not desirable. In this paper, we\npropose asymmetric minwise hashing (MH-ALSH), to provide a solution to this\nproblem. The new scheme utilizes asymmetric transformations to cancel the bias\nof traditional minhash towards smaller sets, making the final \"collision\nprobability\" monotonic in the inner product. Our theoretical comparisons show\nthat for the task of retrieving with binary inner products asymmetric minhash\nis provably better than traditional minhash and other recently proposed hashing\nalgorithms for general inner products. Thus, we obtain an algorithmic\nimprovement over existing approaches in the literature. Experimental\nevaluations on four publicly available high-dimensional datasets validate our\nclaims and the proposed scheme outperforms, often significantly, other hashing\nalgorithms on the task of near neighbor retrieval with set containment. Our\nproposal is simple and easy to implement in practice.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 04:18:33 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Shrivastava", "Anshumali", ""], ["Li", "Ping", ""]]}, {"id": "1411.3843", "submitter": "Rom\\`an R. Zapatrin", "authors": "Rom\\`an Zapatrin", "title": "Quantum emulation of query extension in information retrieval", "comments": "Latex, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An operationalistic scheme, called Melucci metaphor, is suggested\nrepresenting Information Retrieval as physical measurements with beam of\nparticles playing the role of the flow of retrieved documents. The\npossibilities of query expansion by extra term are studied from this\nperspective, when the particles-`docuscles' are assumed to be of classical or\nquantum nature. It is shown that in both cases the choice of an extra term\nbased on Bayesian belief revision is still valid on the qualitative level for\nboosting the relevance of the retrieved documents.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 09:43:24 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Zapatrin", "Rom\u00e0n", ""]]}, {"id": "1411.4366", "submitter": "Prashant Dahiwale Prof", "authors": "Prashant Dahiwale, M M Raghuwanshi, Latesh malik", "title": "PDD Crawler: A focused web crawler using link and content analysis for\n  relevance prediction", "comments": "9 pages, SEAS-2014, Dubai, UAE, International Conference 7-8 Nov 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Majority of the computer or mobile phone enthusiasts make use of the web for\nsearching activity. Web search engines are used for the searching; The results\nthat the search engines get are provided to it by a software module known as\nthe Web Crawler. The size of this web is increasing round-the-clock. The\nprincipal problem is to search this huge database for specific information. To\nstate whether a web page is relevant to a search topic is a dilemma. This paper\nproposes a crawler called as PDD crawler which will follow both a link based as\nwell as a content based approach. This crawler follows a completely new\ncrawling strategy to compute the relevance of the page. It analyses the content\nof the page based on the information contained in various tags within the HTML\nsource code and then computes the total weight of the page. The page with the\nhighest weight, thus has the maximum content and highest relevance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 05:33:51 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Dahiwale", "Prashant", ""], ["Raghuwanshi", "M M", ""], ["malik", "Latesh", ""]]}, {"id": "1411.4738", "submitter": "Cuicui Kang", "authors": "Cuicui Kang, Shengcai Liao, Yonghao He, Jian Wang, Wenjia Niu, Shiming\n  Xiang, Chunhong Pan", "title": "Cross-Modal Similarity Learning : A Low Rank Bilinear Formulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cross-media retrieval problem has received much attention in recent years\ndue to the rapid increasing of multimedia data on the Internet. A new approach\nto the problem has been raised which intends to match features of different\nmodalities directly. In this research, there are two critical issues: how to\nget rid of the heterogeneity between different modalities and how to match the\ncross-modal features of different dimensions. Recently metric learning methods\nshow a good capability in learning a distance metric to explore the\nrelationship between data points. However, the traditional metric learning\nalgorithms only focus on single-modal features, which suffer difficulties in\naddressing the cross-modal features of different dimensions. In this paper, we\npropose a cross-modal similarity learning algorithm for the cross-modal feature\nmatching. The proposed method takes a bilinear formulation, and with the\nnuclear-norm penalization, it achieves low-rank representation. Accordingly,\nthe accelerated proximal gradient algorithm is successfully imported to find\nthe optimal solution with a fast convergence rate O(1/t^2). Experiments on\nthree well known image-text cross-media retrieval databases show that the\nproposed method achieves the best performance compared to the state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 05:53:06 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 01:25:27 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Kang", "Cuicui", ""], ["Liao", "Shengcai", ""], ["He", "Yonghao", ""], ["Wang", "Jian", ""], ["Niu", "Wenjia", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1411.4890", "submitter": "Xing Zhang", "authors": "Xing Zhang", "title": "Which Are You In A Photo?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image tagging has been a long standing problem, it mainly relies on\nimage recognition techniques of which the accuracy is still not satisfying.\nThis paper attempts to explore out-of-band sensing base on the mobile phone to\nsense the people in a picture while the picture is being taken and create name\ntags on-the-fly. The major challenges pertain to two aspects - \"Who\" and\n\"Which\". (1) \"Who\": discriminating people who are in the picture from those\nthat are not; (2) \"Which\": correlating each name tag with its corresponding\npeople in the picture. We propose an accurate acoustic scheme applying on the\nmobile phones, which leverages the Doppler effect of sound wave to address\nthese two challenges. As a proof of concept, we implement the scheme on 7\nandroid phones and take pictures in various real-life scenarios with people\npositioning in different ways. Extensive experiments show that the accuracy of\ntag correlation is above 85% within 3m for picturing.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 05:06:50 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Zhang", "Xing", ""]]}, {"id": "1411.4972", "submitter": "An Zeng", "authors": "Hao Liao, An Zeng, Yi-Cheng Zhang", "title": "Towards an objective ranking in online reputation systems: the effect of\n  the rating projection", "comments": "6 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reputation systems are commonly used by e-commerce providers nowadays.\nIn order to generate an objective ranking of online items' quality according to\nusers' ratings, many sophisticated algorithms have been proposed in the\nliterature. In this paper, instead of proposing new algorithms we focus on a\nmore fundamental problem: the rating projection. The basic idea is that even\nthough the rating values given by users are linearly separated, the real\npreference of users to items between different values gave is nonlinear. We\nthus design an approach to project the original ratings of users to more\nrepresentative values. This approach can be regarded as a data pretreatment\nmethod. Simulation in both artificial and real networks shows that the\nperformance of the ranking algorithms can be improved when the projected\nratings are used.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 15:22:08 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Liao", "Hao", ""], ["Zeng", "An", ""], ["Zhang", "Yi-Cheng", ""]]}, {"id": "1411.5307", "submitter": "Robinson Piramuthu Robinson Piramuthu", "authors": "Kevin Shih, Wei Di, Vignesh Jagadeesh, Robinson Piramuthu", "title": "Efficient Media Retrieval from Non-Cooperative Queries", "comments": "8 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text is ubiquitous in the artificial world and easily attainable when it\ncomes to book title and author names. Using the images from the book cover set\nfrom the Stanford Mobile Visual Search dataset and additional book covers and\nmetadata from openlibrary.org, we construct a large scale book cover retrieval\ndataset, complete with 100K distractor covers and title and author strings for\neach. Because our query images are poorly conditioned for clean text\nextraction, we propose a method for extracting a matching noisy and erroneous\nOCR readings and matching it against clean author and book title strings in a\nstandard document look-up problem setup. Finally, we demonstrate how to use\nthis text-matching as a feature in conjunction with popular retrieval features\nsuch as VLAD using a simple learning setup to achieve significant improvements\nin retrieval accuracy over that of either VLAD or the text alone.\n", "versions": [{"version": "v1", "created": "Wed, 19 Nov 2014 18:34:28 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Shih", "Kevin", ""], ["Di", "Wei", ""], ["Jagadeesh", "Vignesh", ""], ["Piramuthu", "Robinson", ""]]}, {"id": "1411.5726", "submitter": "Ramakrishna  Vedantam", "authors": "Ramakrishna Vedantam, C. Lawrence Zitnick and Devi Parikh", "title": "CIDEr: Consensus-based Image Description Evaluation", "comments": "To appear in CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically describing an image with a sentence is a long-standing\nchallenge in computer vision and natural language processing. Due to recent\nprogress in object detection, attribute classification, action recognition,\netc., there is renewed interest in this area. However, evaluating the quality\nof descriptions has proven to be challenging. We propose a novel paradigm for\nevaluating image descriptions that uses human consensus. This paradigm consists\nof three main parts: a new triplet-based method of collecting human annotations\nto measure consensus, a new automated metric (CIDEr) that captures consensus,\nand two new datasets: PASCAL-50S and ABSTRACT-50S that contain 50 sentences\ndescribing each image. Our simple metric captures human judgment of consensus\nbetter than existing metrics across sentences generated by various sources. We\nalso evaluate five state-of-the-art image description approaches using this new\nprotocol and provide a benchmark for future comparisons. A version of CIDEr\nnamed CIDEr-D is available as a part of MS COCO evaluation server to enable\nsystematic evaluation and benchmarking.\n", "versions": [{"version": "v1", "created": "Thu, 20 Nov 2014 23:54:35 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 01:42:20 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Vedantam", "Ramakrishna", ""], ["Zitnick", "C. Lawrence", ""], ["Parikh", "Devi", ""]]}, {"id": "1411.5732", "submitter": "Suleyman Cetintas", "authors": "Suleyman Cetintas, Luo Si, Yan Ping Xin, Dake Zhang, Joo Young Park,\n  Ron Tzur", "title": "A Joint Probabilistic Classification Model of Relevant and Irrelevant\n  Sentences in Mathematical Word Problems", "comments": "appears in Journal of Educational Data Mining (JEDM, 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the difficulty level of math word problems is an important task\nfor many educational applications. Identification of relevant and irrelevant\nsentences in math word problems is an important step for calculating the\ndifficulty levels of such problems. This paper addresses a novel application of\ntext categorization to identify two types of sentences in mathematical word\nproblems, namely relevant and irrelevant sentences. A novel joint probabilistic\nclassification model is proposed to estimate the joint probability of\nclassification decisions for all sentences of a math word problem by utilizing\nthe correlation among all sentences along with the correlation between the\nquestion sentence and other sentences, and sentence text. The proposed model is\ncompared with i) a SVM classifier which makes independent classification\ndecisions for individual sentences by only using the sentence text and ii) a\nnovel SVM classifier that considers the correlation between the question\nsentence and other sentences along with the sentence text. An extensive set of\nexperiments demonstrates the effectiveness of the joint probabilistic\nclassification model for identifying relevant and irrelevant sentences as well\nas the novel SVM classifier that utilizes the correlation between the question\nsentence and other sentences. Furthermore, empirical results and analysis show\nthat i) it is highly beneficial not to remove stopwords and ii) utilizing part\nof speech tagging does not make a significant improvement although it has been\nshown to be effective for the related task of math word problem type\nclassification.\n", "versions": [{"version": "v1", "created": "Fri, 21 Nov 2014 00:53:02 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Cetintas", "Suleyman", ""], ["Si", "Luo", ""], ["Xin", "Yan Ping", ""], ["Zhang", "Dake", ""], ["Park", "Joo Young", ""], ["Tzur", "Ron", ""]]}, {"id": "1411.6496", "submitter": "Luis Torres", "authors": "Arnau Raventos, Raul Quijada, Luis Torres, Francesc Tarres", "title": "Automatic Summarization of Soccer Highlights Using Audio-visual\n  Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic summarization generation of sports video content has been object of\ngreat interest for many years. Although semantic descriptions techniques have\nbeen proposed, many of the approaches still rely on low-level video descriptors\nthat render quite limited results due to the complexity of the problem and to\nthe low capability of the descriptors to represent semantic content. In this\npaper, a new approach for automatic highlights summarization generation of\nsoccer videos using audio-visual descriptors is presented. The approach is\nbased on the segmentation of the video sequence into shots that will be further\nanalyzed to determine its relevance and interest. Of special interest in the\napproach is the use of the audio information that provides additional\nrobustness to the overall performance of the summarization system. For every\nvideo shot a set of low and mid level audio-visual descriptors are computed and\nlately adequately combined in order to obtain different relevance measures\nbased on empirical knowledge rules. The final summary is generated by selecting\nthose shots with highest interest according to the specifications of the user\nand the results of relevance measures. A variety of results are presented with\nreal soccer video sequences that prove the validity of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 15:56:43 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Raventos", "Arnau", ""], ["Quijada", "Raul", ""], ["Torres", "Luis", ""], ["Tarres", "Francesc", ""]]}, {"id": "1411.6591", "submitter": "George Chen", "authors": "Guy Bresler, George H. Chen, Devavrat Shah", "title": "A Latent Source Model for Online Collaborative Filtering", "comments": "Advances in Neural Information Processing Systems (NIPS 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the prevalence of collaborative filtering in recommendation systems,\nthere has been little theoretical development on why and how well it works,\nespecially in the \"online\" setting, where items are recommended to users over\ntime. We address this theoretical gap by introducing a model for online\nrecommendation systems, cast item recommendation under the model as a learning\nproblem, and analyze the performance of a cosine-similarity collaborative\nfiltering method. In our model, each of $n$ users either likes or dislikes each\nof $m$ items. We assume there to be $k$ types of users, and all the users of a\ngiven type share a common string of probabilities determining the chance of\nliking each item. At each time step, we recommend an item to each user, where a\nkey distinction from related bandit literature is that once a user consumes an\nitem (e.g., watches a movie), then that item cannot be recommended to the same\nuser again. The goal is to maximize the number of likable items recommended to\nusers over time. Our main result establishes that after nearly $\\log(km)$\ninitial learning time steps, a simple collaborative filtering algorithm\nachieves essentially optimal performance without knowing $k$. The algorithm has\nan exploitation step that uses cosine similarity and two types of exploration\nsteps, one to explore the space of items (standard in the literature) and the\nother to explore similarity between users (novel to this work).\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 19:59:59 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Bresler", "Guy", ""], ["Chen", "George H.", ""], ["Shah", "Devavrat", ""]]}, {"id": "1411.6754", "submitter": "Xiaosong Hu", "authors": "Xiaosong Hu, Wen Zhu, Qing Li", "title": "HCRS: A hybrid clothes recommender system based on user ratings and\n  product features", "comments": "ICMECG '13 Proceedings of the 2013 International Conference on\n  Management of e-Commerce and e-Government Pages 270-274", "journal-ref": null, "doi": "10.1109/ICMeCG.2013.60", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, online clothes-selling business has become popular and extremely\nattractive because of its convenience and cheap-and-fine price. Good examples\nof these successful Web sites include Yintai.com, Vancl.com and\nShop.vipshop.com which provide thousands of clothes for online shoppers. The\nchallenge for online shoppers lies on how to find a good product from lots of\noptions. In this article, we propose a collaborative clothes recommender for\neasy shopping. One of the unique features of this system is the ability to\nrecommend clothes in terms of both user ratings and clothing attributes.\nExperiments in our simulation environment show that the proposed recommender\ncan better satisfy the needs of users.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 07:55:07 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Hu", "Xiaosong", ""], ["Zhu", "Wen", ""], ["Li", "Qing", ""]]}, {"id": "1411.6773", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Simran Bijral, Debajyoti Mukhopadhyay", "title": "Efficient Fuzzy Search Engine with B-Tree Search Mechanism", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines play a vital role in day to day life on internet. People use\nsearch engines to find content on internet. Cloud computing is the computing\nconcept in which data is stored and accessed with the help of a third party\nserver called as cloud. Data is not stored locally on our machines and the\nsoftwares and information are provided to user if user demands for it. Search\nqueries are the most important part in searching data on internet. A search\nquery consists of one or more than one keywords. A search query is searched\nfrom the database for exact match, and the traditional searchable schemes do\nnot tolerate minor typos and format inconsistencies, which happen quite\nfrequently. This drawback makes the existing techniques unsuitable and they\noffer very low efficiency. In this paper, we will for the first time formulate\nthe problem of effective fuzzy search by introducing tree search methodologies.\nWe will explore the benefits of B trees in search mechanism and use them to\nhave an efficient keyword search. We have taken into consideration the security\nanalysis strictly so as to get a secure and privacy-preserving system.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 09:03:22 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Bijral", "Simran", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "1411.7336", "submitter": "Mohammed  alzaidi", "authors": "Mohammed A. Talab, Siti Norul Huda Sheikh Abdullah, Bilal Bataineh", "title": "Edge direction matrixes-based local binar patterns descriptor for shape\n  pattern recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Shapes and texture image recognition usage is an essential branch of pattern\nrecognition. It is made up of techniques that aim at extracting information\nfrom images via human knowledge and works. Local Binary Pattern (LBP) ensures\nencoding global and local information and scaling invariance by introducing a\nlook-up table to reflect the uniformity structure of an object. However, edge\ndirection matrixes (EDMS) only apply global invariant descriptor which employs\nfirst and secondary order relationships. The main idea behind this methodology\nis the need of improved recognition capabilities, a goal achieved by the\ncombinative use of these descriptors. This collaboration aims to make use of\nthe major advantages each one presents, by simultaneously complementing each\nother, in order to elevate their weak points. By using multiple classifier\napproaches such as random forest and multi-layer perceptron neural network, the\nproposed combinative descriptor are compared with the state of the art\ncombinative methods based on Gray-Level Co-occurrence matrix (GLCM with EDMS),\nLBP and moment invariant on four benchmark dataset MPEG-7 CE-Shape-1, KTH-TIPS\nimage, Enghlishfnt and Arabic calligraphy . The experiments have shown the\nsuperiority of the introduced descriptor over the GLCM with EDMS, LBP and\nmoment invariants and other well-known descriptor such as Scale Invariant\nFeature Transform from the literature.\n", "versions": [{"version": "v1", "created": "Wed, 26 Nov 2014 19:12:33 GMT"}], "update_date": "2014-11-27", "authors_parsed": [["Talab", "Mohammed A.", ""], ["Abdullah", "Siti Norul Huda Sheikh", ""], ["Bataineh", "Bilal", ""]]}]