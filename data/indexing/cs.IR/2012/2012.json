[{"id": "2012.00056", "submitter": "Anoop Vallabhajosyula", "authors": "Shreya Malani, Dinesh Gaurav, Anoop Vallabhajosyula, Rahul Agrawal", "title": "Diversifying Relevant Phrases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diverse keyword suggestions for a given landing page or matching queries to\ndiverse documents is an active research area in online advertising. Modern\nsearch engines provide advertisers with products like Dynamic Search Ads and\nSmart Campaigns where they extract meaningful keywords/phrases from the\nadvertiser's product inventory. These keywords/phrases are representative of a\ndiverse spectrum of advertiser's interests. In this paper, we address the\nproblem of obtaining relevant yet diverse keywords/phrases for any given\ndocument. We formulate this as an optimization problem, maximizing the\nparameterized trade-off between diversity and relevance constrained over number\nof possible keywords/phrases. We show that this is a combinatorial NP-hard\noptimization problem. We propose two approaches based on convex relaxations\nvarying in complexity and performance. In the first approach, we show that the\noptimization problem reduces to an eigen value problem. In the second approach,\nwe show that the optimization problem reduces to minimizing a quadratic form\nover an l1-ball. Subsequently, we show that this is equivalent to a\nsemi-definite optimization problem. To prove the efficacy of our proposed\nformulation, we evaluate it on various real-world datasets and compare it to\nthe state-of-the-art heuristic approaches.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 19:15:55 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Malani", "Shreya", ""], ["Gaurav", "Dinesh", ""], ["Vallabhajosyula", "Anoop", ""], ["Agrawal", "Rahul", ""]]}, {"id": "2012.00290", "submitter": "Donghuo Zeng", "authors": "Donghuo Zeng, Yi Yu, Keizo Oyama", "title": "MusicTM-Dataset for Joint Representation Learning among Sheet Music,\n  Lyrics, and Musical Audio", "comments": "12 pages, 5 figures, 2 tables", "journal-ref": "CSMT2020", "doi": null, "report-no": null, "categories": "cs.SD cs.DB cs.IR cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This work present a music dataset named MusicTM-Dataset, which is utilized in\nimproving the representation learning ability of different types of cross-modal\nretrieval (CMR). Little large music dataset including three modalities is\navailable for learning representations for CMR. To collect a music dataset, we\nexpand the original musical notation to synthesize audio and generated\nsheet-music image, and build musical notation based sheet-music image, audio\nclip and syllable-denotation text as fine-grained alignment, such that the\nMusicTM-Dataset can be exploited to receive shared representation for\nmultimodal data points. The MusicTM-Dataset presents 3 kinds of modalities,\nwhich consists of the image of sheet-music, the text of lyrics and synthesized\naudio, their representations are extracted by some advanced models. In this\npaper, we introduce the background of music dataset and express the process of\nour data collection. Based on our dataset, we achieve some basic methods for\nCMR tasks. The MusicTM-Dataset are accessible in https:\n//github.com/dddzeng/MusicTM-Dataset.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 06:18:53 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 14:20:13 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Zeng", "Donghuo", ""], ["Yu", "Yi", ""], ["Oyama", "Keizo", ""]]}, {"id": "2012.00318", "submitter": "Mingbao Lin", "authors": "Mingbao Lin, Rongrong Ji, Xiaoshuai Sun, Baochang Zhang, Feiyue Huang,\n  Yonghong Tian, Dacheng Tao", "title": "Fast Class-wise Updating for Online Hashing", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online image hashing has received increasing research attention recently,\nwhich processes large-scale data in a streaming fashion to update the hash\nfunctions on-the-fly. To this end, most existing works exploit this problem\nunder a supervised setting, i.e., using class labels to boost the hashing\nperformance, which suffers from the defects in both adaptivity and efficiency:\nFirst, large amounts of training batches are required to learn up-to-date hash\nfunctions, which leads to poor online adaptivity. Second, the training is\ntime-consuming, which contradicts with the core need of online learning. In\nthis paper, a novel supervised online hashing scheme, termed Fast Class-wise\nUpdating for Online Hashing (FCOH), is proposed to address the above two\nchallenges by introducing a novel and efficient inner product operation. To\nachieve fast online adaptivity, a class-wise updating method is developed to\ndecompose the binary code learning and alternatively renew the hash functions\nin a class-wise fashion, which well addresses the burden on large amounts of\ntraining batches. Quantitatively, such a decomposition further leads to at\nleast 75% storage saving. To further achieve online efficiency, we propose a\nsemi-relaxation optimization, which accelerates the online training by treating\ndifferent binary constraints independently. Without additional constraints and\nvariables, the time complexity is significantly reduced. Such a scheme is also\nquantitatively shown to well preserve past information during updating hashing\nfunctions. We have quantitatively demonstrated that the collective effort of\nclass-wise updating and semi-relaxation optimization provides a superior\nperformance comparing to various state-of-the-art methods, which is verified\nthrough extensive experiments on three widely-used datasets.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 07:41:54 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Lin", "Mingbao", ""], ["Ji", "Rongrong", ""], ["Sun", "Xiaoshuai", ""], ["Zhang", "Baochang", ""], ["Huang", "Feiyue", ""], ["Tian", "Yonghong", ""], ["Tao", "Dacheng", ""]]}, {"id": "2012.00387", "submitter": "Alireza Gharahighehi", "authors": "Alireza Gharahighehi, Celine Vens, Konstantinos Pliakos", "title": "Fair Multi-Stakeholder News Recommender System with Hypergraph ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommender systems are typically designed to fulfill end user needs.\nHowever, in some domains the users are not the only stakeholders in the system.\nFor instance, in a news aggregator website users, authors, magazines as well as\nthe platform itself are potential stakeholders. Most of the collaborative\nfiltering recommender systems suffer from popularity bias. Therefore, if the\nrecommender system only considers users' preferences, presumably it\nover-represents popular providers and under-represents less popular providers.\nTo address this issue one should consider other stakeholders in the generated\nranked lists. In this paper we demonstrate that hypergraph learning has the\nnatural capability of handling a multi-stakeholder recommendation task. A\nhypergraph can model high order relations between different types of objects\nand therefore is naturally inclined to generate recommendation lists\nconsidering multiple stakeholders. We form the recommendations in time-wise\nrounds and learn to adapt the weights of stakeholders to increase the coverage\nof low-covered stakeholders over time. The results show that the proposed\napproach counters popularity bias and produces fairer recommendations with\nrespect to authors in two news datasets, at a low cost in precision.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 10:37:00 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 08:59:50 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Gharahighehi", "Alireza", ""], ["Vens", "Celine", ""], ["Pliakos", "Konstantinos", ""]]}, {"id": "2012.00398", "submitter": "Naveen Elango", "authors": "Naveen Elango, Pawan Prasad K", "title": "Introducing Inter-Relatedness between Wikipedia Articles in Explicit\n  Semantic Analysis", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explicit Semantic Analysis (ESA) is a technique used to represent a piece of\ntext as a vector in the space of concepts, such as Articles found in Wikipedia.\nWe propose a methodology to incorporate knowledge of Inter-relatedness between\nWikipedia Articles to the vectors obtained from ESA using a technique called\nRetrofitting to improve the performance of subsequent tasks that use ESA to\nform vector embeddings. Especially we use an undirected Graph to represent this\nknowledge with nodes as Articles and edges as inter relations between two\nArticles. Here, we also emphasize how the ESA step could be seen as a\npredominantly bottom-up approach using a corpus to come up with vector\nrepresentations and the incorporation of top-down knowledge which is the\nrelations between Articles to further improve it. We test our hypothesis on\nseveral smaller subsets of the Wikipedia corpus and show that our proposed\nmethodology leads to decent improvements in performance measures including\nSpearman's Rank correlation coefficient in most cases.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 10:55:07 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Elango", "Naveen", ""], ["K", "Pawan Prasad", ""]]}, {"id": "2012.00485", "submitter": "Muyang Ma", "authors": "Muyang Ma and Pengjie Ren and Zhumin Chen and Zhaochun Ren and Lifan\n  Zhao and Jun Ma and Maarten de Rijke", "title": "Mixed Information Flow for Cross-domain Sequential Recommendations", "comments": "26 pages, 6 figures, TKDD journal, 7 co-authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-domain sequential recommendation is the task of predict the next item\nthat the user is most likely to interact with based on past sequential behavior\nfrom multiple domains. One of the key challenges in cross-domain sequential\nrecommendation is to grasp and transfer the flow of information from multiple\ndomains so as to promote recommendations in all domains. Previous studies have\ninvestigated the flow of behavioral information by exploring the connection\nbetween items from different domains. The flow of knowledge (i.e., the\nconnection between knowledge from different domains) has so far been neglected.\nIn this paper, we propose a mixed information flow network for cross-domain\nsequential recommendation to consider both the flow of behavioral information\nand the flow of knowledge by incorporating a behavior transfer unit and a\nknowledge transfer unit. The proposed mixed information flow network is able to\ndecide when cross-domain information should be used and, if so, which\ncross-domain information should be used to enrich the sequence representation\naccording to users' current preferences. Extensive experiments conducted on\nfour e-commerce datasets demonstrate that mixed information flow network is\nable to further improve recommendation performance in different domains by\nmodeling mixed information flow.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 13:45:26 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 08:35:48 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 13:22:57 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Ma", "Muyang", ""], ["Ren", "Pengjie", ""], ["Chen", "Zhumin", ""], ["Ren", "Zhaochun", ""], ["Zhao", "Lifan", ""], ["Ma", "Jun", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2012.00501", "submitter": "Minhas Kamal", "authors": "Md Rifat Arefin, Minhas Kamal, Kishan Kumar Ganguly, Tarek Salah Uddin\n  Mahmud", "title": "A Statistical Real-Time Prediction Model for Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recommender system has become an inseparable part of online shopping and its\nusability is increasing with the advancement of these e-commerce sites. An\neffective and efficient recommender system benefits both the seller and the\nbuyer significantly. We considered user activities and product information for\nthe filtering process in our proposed recommender system. Our model has\nachieved inspiring result (approximately 58% true-positive and 13%\nfalse-positive) for the data set provided by RecSys Challenge 2015. This paper\naims to describe a statistical model that will help to predict the buying\nbehavior of a user in real-time during a session.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 14:16:50 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Arefin", "Md Rifat", ""], ["Kamal", "Minhas", ""], ["Ganguly", "Kishan Kumar", ""], ["Mahmud", "Tarek Salah Uddin", ""]]}, {"id": "2012.00555", "submitter": "Mohnish Dubey", "authors": "Nandana Mihindukulasooriya and Mohnish Dubey and Alfio Gliozzo and\n  Jens Lehmann and Axel-Cyrille Ngonga Ngomo and Ricardo Usbeck", "title": "SeMantic AnsweR Type prediction task (SMART) at ISWC 2020 Semantic Web\n  Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Each year the International Semantic Web Conference accepts a set of Semantic\nWeb Challenges to establish competitions that will advance the state of the art\nsolutions in any given problem domain. The SeMantic AnsweR Type prediction task\n(SMART) was part of ISWC 2020 challenges. Question type and answer type\nprediction can play a key role in knowledge base question answering systems\nproviding insights that are helpful to generate correct queries or rank the\nanswer candidates. More concretely, given a question in natural language, the\ntask of SMART challenge is, to predict the answer type using a target ontology\n(e.g., DBpedia or Wikidata).\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 15:02:11 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Mihindukulasooriya", "Nandana", ""], ["Dubey", "Mohnish", ""], ["Gliozzo", "Alfio", ""], ["Lehmann", "Jens", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""], ["Usbeck", "Ricardo", ""]]}, {"id": "2012.00584", "submitter": "Andres Carvallo", "authors": "Andres Carvallo, Denis Parra, Gabriel Rada, Daniel Perez, Juan Ignacio\n  Vasquez and Camilo Vergara", "title": "Neural language models for text classification in evidence-based\n  medicine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 has brought about a significant challenge to the whole of\nhumanity, but with a special burden upon the medical community. Clinicians must\nkeep updated continuously about symptoms, diagnoses, and effectiveness of\nemergent treatments under a never-ending flood of scientific literature. In\nthis context, the role of evidence-based medicine (EBM) for curating the most\nsubstantial evidence to support public health and clinical practice turns\nessential but is being challenged as never before due to the high volume of\nresearch articles published and pre-prints posted daily. Artificial\nIntelligence can have a crucial role in this situation. In this article, we\nreport the results of an applied research project to classify scientific\narticles to support Epistemonikos, one of the most active foundations worldwide\nconducting EBM. We test several methods, and the best one, based on the XLNet\nneural language model, improves the current approach by 93\\% on average\nF1-score, saving valuable time from physicians who volunteer to curate COVID-19\nresearch articles manually.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 15:53:44 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Carvallo", "Andres", ""], ["Parra", "Denis", ""], ["Rada", "Gabriel", ""], ["Perez", "Daniel", ""], ["Vasquez", "Juan Ignacio", ""], ["Vergara", "Camilo", ""]]}, {"id": "2012.00600", "submitter": "Mustafa Jarrar", "authors": "Mustafa Jarrar, Eman Karajah, Muhammad Khalifa, Khaled Shaalan", "title": "Extracting Synonyms from Bilingual Dictionaries", "comments": "In Proceedings - 11th International Global Wordnet Conference\n  (GWC2021). Global Wordnet Association (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present our progress in developing a novel algorithm to extract synonyms\nfrom bilingual dictionaries. Identification and usage of synonyms play a\nsignificant role in improving the performance of information access\napplications. The idea is to construct a translation graph from translation\npairs, then to extract and consolidate cyclic paths to form bilingual sets of\nsynonyms. The initial evaluation of this algorithm illustrates promising\nresults in extracting Arabic-English bilingual synonyms. In the evaluation, we\nfirst converted the synsets in the Arabic WordNet into translation pairs (i.e.,\nlosing word-sense memberships). Next, we applied our algorithm to rebuild these\nsynsets. We compared the original and extracted synsets obtaining an F-Measure\nof 82.3% and 82.1% for Arabic and English synsets extraction, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 16:09:22 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Jarrar", "Mustafa", ""], ["Karajah", "Eman", ""], ["Khalifa", "Muhammad", ""], ["Shaalan", "Khaled", ""]]}, {"id": "2012.00633", "submitter": "Rahul Dubey Dr", "authors": "Shree Charran R, Rahul Kumar Dubey (Senior Member IEEE)", "title": "Meta-Embeddings for Natural Language Inference and Semantic Similarity\n  tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Word Representations form the core component for almost all advanced Natural\nLanguage Processing (NLP) applications such as text mining, question-answering,\nand text summarization, etc. Over the last two decades, immense research is\nconducted to come up with one single model to solve all major NLP tasks. The\nmajor problem currently is that there are a plethora of choices for different\nNLP tasks. Thus for NLP practitioners, the task of choosing the right model to\nbe used itself becomes a challenge. Thus combining multiple pre-trained word\nembeddings and forming meta embeddings has become a viable approach to improve\ntackle NLP tasks. Meta embedding learning is a process of producing a single\nword embedding from a given set of pre-trained input word embeddings. In this\npaper, we propose to use Meta Embedding derived from few State-of-the-Art\n(SOTA) models to efficiently tackle mainstream NLP tasks like classification,\nsemantic relatedness, and text similarity. We have compared both ensemble and\ndynamic variants to identify an efficient approach. The results obtained show\nthat even the best State-of-the-Art models can be bettered. Thus showing us\nthat meta-embeddings can be used for several NLP tasks by harnessing the power\nof several individual representations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 16:58:01 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["R", "Shree Charran", "", "Senior Member IEEE"], ["Dubey", "Rahul Kumar", "", "Senior Member IEEE"]]}, {"id": "2012.00743", "submitter": "Stefanos Antaris", "authors": "Dimitrios Rafailidis, Stefanos Antaris", "title": "Adaptive Neural Architectures for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has proved an effective means to capture the non-linear\nassociations of user preferences. However, the main drawback of existing deep\nlearning architectures is that they follow a fixed recommendation strategy,\nignoring users' real time-feedback. Recent advances of deep reinforcement\nstrategies showed that recommendation policies can be continuously updated\nwhile users interact with the system. In doing so, we can learn the optimal\npolicy that fits to users' preferences over the recommendation sessions. The\nmain drawback of deep reinforcement strategies is that are based on predefined\nand fixed neural architectures. To shed light on how to handle this issue, in\nthis study we first present deep reinforcement learning strategies for\nrecommendation and discuss the main limitations due to the fixed neural\narchitectures. Then, we detail how recent advances on progressive neural\narchitectures are used for consecutive tasks in other research domains.\nFinally, we present the key challenges to fill the gap between deep\nreinforcement learning and adaptive neural architectures. We provide guidelines\nfor searching for the best neural architecture based on each user feedback via\nreinforcement learning, while considering the prediction performance on\nreal-time recommendations and the model complexity.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 09:43:20 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Rafailidis", "Dimitrios", ""], ["Antaris", "Stefanos", ""]]}, {"id": "2012.01184", "submitter": "J\\'er\\^ome Darmont", "authors": "Pegdwend\\'e Sawadogo, J\\'er\\^ome Darmont, Fabien Duchateau", "title": "Feedback from the participants of the ADBIS, TPDL and EDA 2020 joint\n  conferences", "comments": "7 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents the way the joint ADBIS, TPDL and EDA 2020 conferences\nwere organized online and the results of the participant survey conducted\nthereafter. We present the lessons learned from the participants' feedback.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 17:02:26 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2020 22:03:07 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Sawadogo", "Pegdwend\u00e9", ""], ["Darmont", "J\u00e9r\u00f4me", ""], ["Duchateau", "Fabien", ""]]}, {"id": "2012.01345", "submitter": "Ricardo Guerrero", "authors": "Ricardo Guerrero, Hai Xuan Pham and Vladimir Pavlovic", "title": "Cross-modal Retrieval and Synthesis (X-MRS): Closing the modality gap in\n  shared subspace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational food analysis (CFA), a broad set of methods that attempt to\nautomate food understanding, naturally requires analysis of multi-modal\nevidence of a particular food or dish, e.g. images, recipe text, preparation\nvideo, nutrition labels, etc. A key to making CFA possible is multi-modal\nshared subspace learning, which in turn can be used for cross-modal retrieval\nand/or synthesis, particularly, between food images and their corresponding\ntextual recipes. In this work we propose a simple yet novel architecture for\nshared subspace learning, which is used to tackle the food image-to-recipe\nretrieval problem. Our proposed method employs an effective transformer based\nmultilingual recipe encoder coupled with a traditional image embedding\narchitecture. Experimental analysis on the public Recipe1M dataset shows that\nthe subspace learned via the proposed method outperforms the current\nstate-of-the-arts (SoTA) in food retrieval by a large margin, obtaining\nrecall@1 of 0.64. Furthermore, in order to demonstrate the representational\npower of the learned subspace, we propose a generative food image synthesis\nmodel conditioned on the embeddings of recipes. Synthesized images can\neffectively reproduce the visual appearance of paired samples, achieving R@1 of\n0.68 in the image-to-recipe retrieval experiment, thus effectively capturing\nthe semantics of the textual recipe.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 17:27:00 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 22:49:07 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Guerrero", "Ricardo", ""], ["Pham", "Hai Xuan", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "2012.01414", "submitter": "Revanth Reddy", "authors": "Revanth Gangi Reddy, Bhavani Iyer, Md Arafat Sultan, Rong Zhang, Avi\n  Sil, Vittorio Castelli, Radu Florian, Salim Roukos", "title": "End-to-End QA on COVID-19: Domain Adaptation with Synthetic Training", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  End-to-end question answering (QA) requires both information retrieval (IR)\nover a large document collection and machine reading comprehension (MRC) on the\nretrieved passages. Recent work has successfully trained neural IR systems\nusing only supervised question answering (QA) examples from open-domain\ndatasets. However, despite impressive performance on Wikipedia, neural IR lags\nbehind traditional term matching approaches such as BM25 in more specific and\nspecialized target domains such as COVID-19. Furthermore, given little or no\nlabeled data, effective adaptation of QA systems can also be challenging in\nsuch target domains. In this work, we explore the application of synthetically\ngenerated QA examples to improve performance on closed-domain retrieval and\nMRC. We combine our neural IR and MRC systems and show significant improvements\nin end-to-end QA on the CORD-19 collection over a state-of-the-art open-domain\nQA baseline.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 18:59:59 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Reddy", "Revanth Gangi", ""], ["Iyer", "Bhavani", ""], ["Sultan", "Md Arafat", ""], ["Zhang", "Rong", ""], ["Sil", "Avi", ""], ["Castelli", "Vittorio", ""], ["Florian", "Radu", ""], ["Roukos", "Salim", ""]]}, {"id": "2012.01635", "submitter": "Xiaoming Liu", "authors": "Xiaoming Liu, Shaocong Wu, Zhaohan Zhang, Zhanwei Zhang, Yu Lan, Chao\n  Shen", "title": "A Duet Recommendation Algorithm Based on Jointly Local and Global\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Knowledge graph (KG), as the side information, is widely utilized to learn\nthe semantic representations of item/user for recommendation system. The\ntraditional recommendation algorithms usually just depend on user-item\ninteractions, but ignore the inherent web information describing the item/user,\nwhich could be formulated by the knowledge graph embedding (KGE) methods to\nsignificantly improve applications' performance. In this paper, we propose a\nknowledge-aware-based recommendation algorithm to capture the local and global\nrepresentation learning from heterogeneous information. Specifically, the local\nmodel and global model can naturally depict the inner patterns in the\ncontent-based heterogeneous information and interactive behaviors among the\nusers and items. Based on the method that local and global representations are\nlearned jointly by graph convolutional networks with attention mechanism, the\nfinal recommendation probability is calculated by a fully-connected neural\nnetwork. Extensive experiments are conducted on two real-world datasets to\nverify the proposed algorithm's validation. The evaluation results indicate\nthat the proposed algorithm surpasses state-of-arts by $10.0\\%$, $5.1\\%$,\n$2.5\\%$ and $1.8\\%$ in metrics of MAE, RMSE, AUC and F1-score at least,\nrespectively. The significant improvements reveal the capacity of our proposal\nto recommend user/item effectively.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 01:52:14 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Liu", "Xiaoming", ""], ["Wu", "Shaocong", ""], ["Zhang", "Zhaohan", ""], ["Zhang", "Zhanwei", ""], ["Lan", "Yu", ""], ["Shen", "Chao", ""]]}, {"id": "2012.01915", "submitter": "Xiang Hui Nicholas Lim", "authors": "Nicholas Lim, Bryan Hooi, See-Kiong Ng, Xueou Wang, Yong Liang Goh,\n  Renrong Weng, Rui Tan", "title": "Origin-Aware Next Destination Recommendation with Personalized\n  Preference Attention", "comments": "To appear in the Proceedings of the 14th ACM International Conference\n  on Web Search and Data Mining (WSDM), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next destination recommendation is an important task in the transportation\ndomain of taxi and ride-hailing services, where users are recommended with\npersonalized destinations given their current origin location. However, recent\nrecommendation works do not satisfy this origin-awareness property, and only\nconsider learning from historical destination locations, without origin\ninformation. Thus, the resulting approaches are unable to learn and predict\norigin-aware recommendations based on the user's current location, leading to\nsub-optimal performance and poor real-world practicality. Hence, in this work,\nwe study the origin-aware next destination recommendation task. We propose the\nSpatial-Temporal Origin-Destination Personalized Preference Attention\n(STOD-PPA) encoder-decoder model to learn origin-origin (OO),\ndestination-destination (DD), and origin-destination (OD) relationships by\nfirst encoding both origin and destination sequences with spatial and temporal\nfactors in local and global views, then decoding them through personalized\npreference attention to predict the next destination. Experimental results on\nseven real-world user trajectory taxi datasets show that our model\nsignificantly outperforms baseline and state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 13:53:36 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 08:01:15 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 10:30:27 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Lim", "Nicholas", ""], ["Hooi", "Bryan", ""], ["Ng", "See-Kiong", ""], ["Wang", "Xueou", ""], ["Goh", "Yong Liang", ""], ["Weng", "Renrong", ""], ["Tan", "Rui", ""]]}, {"id": "2012.01953", "submitter": "Carlos Badenes-Olmedo", "authors": "Carlos Badenes-Olmedo, David Chaves-Fraga, Mar\\'Ia Poveda-Villal\\'On,\n  Ana Iglesias-Molina, Pablo Calleja, Socorro Bernardos, Patricia\n  Mart\\'In-Chozas, Alba Fern\\'andez-Izquierdo, Elvira Amador-Dom\\'inguez, Paola\n  Espinoza-Arias, Luis Pozo, Edna Ruckhaus, Esteban Gonz\\'alez-Guardia, Raquel\n  Cedazo, Beatriz L\\'opez-Centeno, and Oscar Corcho", "title": "Drugs4Covid: Drug-driven Knowledge Exploitation based on Scientific\n  Publications", "comments": "Ontology-based technologies, NLP, Bio-annotations, Drugs-catalogue,\n  Knowledge Graph, COVID-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the absence of sufficient medication for COVID patients due to the\nincreased demand, disused drugs have been employed or the doses of those\navailable were modified by hospital pharmacists. Some evidences for the use of\nalternative drugs can be found in the existing scientific literature that could\nassist in such decisions. However, exploiting large corpus of documents in an\nefficient manner is not easy, since drugs may not appear explicitly related in\nthe texts and could be mentioned under different brand names. Drugs4Covid\ncombines word embedding techniques and semantic web technologies to enable a\ndrug-oriented exploration of large medical literature. Drugs and diseases are\nidentified according to the ATC classification and MeSH categories\nrespectively. More than 60K articles and 2M paragraphs have been processed from\nthe CORD-19 corpus with information of COVID-19, SARS, and other related\ncoronaviruses. An open catalogue of drugs has been created and results are\npublicly available through a drug browser, a keyword-guided text explorer, and\na knowledge graph.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 14:26:54 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Badenes-Olmedo", "Carlos", ""], ["Chaves-Fraga", "David", ""], ["Poveda-Villal\u00d3n", "Mar\u00cda", ""], ["Iglesias-Molina", "Ana", ""], ["Calleja", "Pablo", ""], ["Bernardos", "Socorro", ""], ["Mart\u00cdn-Chozas", "Patricia", ""], ["Fern\u00e1ndez-Izquierdo", "Alba", ""], ["Amador-Dom\u00ednguez", "Elvira", ""], ["Espinoza-Arias", "Paola", ""], ["Pozo", "Luis", ""], ["Ruckhaus", "Edna", ""], ["Gonz\u00e1lez-Guardia", "Esteban", ""], ["Cedazo", "Raquel", ""], ["L\u00f3pez-Centeno", "Beatriz", ""], ["Corcho", "Oscar", ""]]}, {"id": "2012.02287", "submitter": "Allen Schmaltz", "authors": "Allen Schmaltz and Andrew Beam", "title": "Coarse-to-Fine Memory Matching for Joint Retrieval and Classification", "comments": "19 pages, 3 figures, 7 tables (main: 11 pages, 2 figures, 4 tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel end-to-end language model for joint retrieval and\nclassification, unifying the strengths of bi- and cross- encoders into a single\nlanguage model via a coarse-to-fine memory matching search procedure for\nlearning and inference. Evaluated on the standard blind test set of the FEVER\nfact verification dataset, classification accuracy is significantly higher than\napproaches that only rely on the language model parameters as a knowledge base,\nand approaches some recent multi-model pipeline systems, using only a single\nBERT base model augmented with memory layers. We further demonstrate how\ncoupled retrieval and classification can be leveraged to identify low\nconfidence instances, and we extend exemplar auditing to this setting for\nanalyzing and constraining the model. As a result, our approach yields a means\nof updating language model behavior through two distinct mechanisms: The\nretrieved information can be updated explicitly, and the model behavior can be\nmodified via the exemplar database.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 05:06:03 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Schmaltz", "Allen", ""], ["Beam", "Andrew", ""]]}, {"id": "2012.02291", "submitter": "Anubha Kabra Ms", "authors": "Anubha Kabra, Anu Agarwal, Anil Singh Parihar", "title": "Cluster Based Deep Contextual Reinforcement Learning for top-k\n  Recommendations", "comments": "To be published in : Springer Lecture Notes in Networks and Systems\n  ISSN 2367-3370", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid advancements in the E-commerce sector over the last few decades have\nled to an imminent need for personalised, efficient and dynamic recommendation\nsystems. To sufficiently cater to this need, we propose a novel method for\ngenerating top-k recommendations by creating an ensemble of clustering with\nreinforcement learning. We have incorporated DB Scan clustering to tackle vast\nitem space, hence in-creasing the efficiency multi-fold. Moreover, by using\ndeep contextual reinforcement learning, our proposed work leverages the user\nfeatures to its full potential. With partial updates and batch updates, the\nmodel learns user patterns continuously. The Duelling Bandit based exploration\nprovides robust exploration as compared to the state-of-art strategies due to\nits adaptive nature. Detailed experiments conducted on a public dataset verify\nour claims about the efficiency of our technique as com-pared to existing\ntechniques.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 20:24:39 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Kabra", "Anubha", ""], ["Agarwal", "Anu", ""], ["Parihar", "Anil Singh", ""]]}, {"id": "2012.02292", "submitter": "Yao Wu", "authors": "Yao Wu, Jian Cao, Guandong Xu", "title": "FAST: A Fairness Assured Service Recommendation Strategy Considering\n  Service Capacity Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An excessive number of customers often leads to a degradation in service\nquality. However, the capacity constraints of services are ignored by\nrecommender systems, which may lead to unsatisfactory recommendation. This\nproblem can be solved by limiting the number of users who receive the\nrecommendation for a service, but this may be viewed as unfair. In this paper,\nwe propose a novel metric Top-N Fairness to measure the individual fairness of\nmulti-round recommendations of services with capacity constraints. By\nconsidering the fact that users are often only affected by top-ranked items in\na recommendation, Top-N Fairness only considers a sub-list consisting of top N\nservices. Based on the metric, we design FAST, a Fairness Assured service\nrecommendation STrategy. FAST adjusts the original recommendation list to\nprovide users with recommendation results that guarantee the long-term fairness\nof multi-round recommendations. We prove the convergence property of the\nvariance of Top-N Fairness of FAST theoretically. FAST is tested on the Yelp\ndataset and synthetic datasets. The experimental results show that FAST\nachieves better recommendation fairness while still maintaining high\nrecommendation quality.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 12:21:31 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Wu", "Yao", ""], ["Cao", "Jian", ""], ["Xu", "Guandong", ""]]}, {"id": "2012.02294", "submitter": "Farah Alshanik", "authors": "Farah Alshanik, Amy Apon, Alexander Herzog, Ilya Safro, Justin\n  Sybrandt", "title": "Accelerating Text Mining Using Domain-Specific Stop Word Lists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text preprocessing is an essential step in text mining. Removing words that\ncan negatively impact the quality of prediction algorithms or are not\ninformative enough is a crucial storage-saving technique in text indexing and\nresults in improved computational efficiency. Typically, a generic stop word\nlist is applied to a dataset regardless of the domain. However, many common\nwords are different from one domain to another but have no significance within\na particular domain. Eliminating domain-specific common words in a corpus\nreduces the dimensionality of the feature space, and improves the performance\nof text mining tasks. In this paper, we present a novel mathematical approach\nfor the automatic extraction of domain-specific words called the\nhyperplane-based approach. This new approach depends on the notion of low\ndimensional representation of the word in vector space and its distance from\nhyperplane. The hyperplane-based approach can significantly reduce text\ndimensionality by eliminating irrelevant features. We compare the\nhyperplane-based approach with other feature selection methods, namely \\c{hi}2\nand mutual information. An experimental study is performed on three different\ndatasets and five classification algorithms, and measure the dimensionality\nreduction and the increase in the classification performance. Results indicate\nthat the hyperplane-based approach can reduce the dimensionality of the corpus\nby 90% and outperforms mutual information. The computational time to identify\nthe domain-specific words is significantly lower than mutual information.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 17:42:32 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Alshanik", "Farah", ""], ["Apon", "Amy", ""], ["Herzog", "Alexander", ""], ["Safro", "Ilya", ""], ["Sybrandt", "Justin", ""]]}, {"id": "2012.02295", "submitter": "Da Xu", "authors": "Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan", "title": "Adversarial Counterfactual Learning and Evaluation for Recommender\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The feedback data of recommender systems are often subject to what was\nexposed to the users; however, most learning and evaluation methods do not\naccount for the underlying exposure mechanism. We first show in theory that\napplying supervised learning to detect user preferences may end up with\ninconsistent results in the absence of exposure information. The counterfactual\npropensity-weighting approach from causal inference can account for the\nexposure mechanism; nevertheless, the partial-observation nature of the\nfeedback data can cause identifiability issues. We propose a principled\nsolution by introducing a minimax empirical risk formulation. We show that the\nrelaxation of the dual problem can be converted to an adversarial game between\ntwo recommendation models, where the opponent of the candidate model\ncharacterizes the underlying exposure mechanism. We provide learning bounds and\nconduct extensive simulation studies to illustrate and justify the proposed\napproach over a broad range of recommendation settings, which shed insights on\nthe various benefits of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 00:40:51 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Xu", "Da", ""], ["Ruan", "Chuanwei", ""], ["Korpeoglu", "Evren", ""], ["Kumar", "Sushant", ""], ["Achan", "Kannan", ""]]}, {"id": "2012.02297", "submitter": "Burcu Sayin", "authors": "Evgeny Krivosheev, Burcu Sayin, Alessandro Bozzon, Zolt\\'an Szl\\'avik", "title": "Active Learning from Crowd in Document Screening", "comments": "Crowd Science Workshop at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we explore how to efficiently combine crowdsourcing and\nmachine intelligence for the problem of document screening, where we need to\nscreen documents with a set of machine-learning filters. Specifically, we focus\non building a set of machine learning classifiers that evaluate documents, and\nthen screen them efficiently. It is a challenging task since the budget is\nlimited and there are countless number of ways to spend the given budget on the\nproblem. We propose a multi-label active learning screening specific sampling\ntechnique -- objective-aware sampling -- for querying unlabelled documents for\nannotating. Our algorithm takes a decision on which machine filter need more\ntraining data and how to choose unlabeled items to annotate in order to\nminimize the risk of overall classification errors rather than minimizing a\nsingle filter error. We demonstrate that objective-aware sampling significantly\noutperforms the state of the art active learning sampling strategies.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 16:17:28 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Krivosheev", "Evgeny", ""], ["Sayin", "Burcu", ""], ["Bozzon", "Alessandro", ""], ["Szl\u00e1vik", "Zolt\u00e1n", ""]]}, {"id": "2012.02298", "submitter": "Chao Du", "authors": "Chao Du, Zhifeng Gao, Shuo Yuan, Lining Gao, Ziyan Li, Yifan Zeng,\n  Xiaoqiang Zhu, Jian Xu, Kun Gai, Kuang-chih Lee", "title": "Exploration in Online Advertising Systems with Deep Uncertainty-Aware\n  Learning", "comments": "SIGKDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467089", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern online advertising systems inevitably rely on personalization methods,\nsuch as click-through rate (CTR) prediction. Recent progress in CTR prediction\nenjoys the rich representation capabilities of deep learning and achieves great\nsuccess in large-scale industrial applications. However, these methods can\nsuffer from lack of exploration. Another line of prior work addresses the\nexploration-exploitation trade-off problem with contextual bandit methods,\nwhich are recently less studied in the industry due to the difficulty in\nextending their flexibility with deep models. In this paper, we propose a novel\nDeep Uncertainty-Aware Learning (DUAL) method to learn CTR models based on\nGaussian processes, which can provide predictive uncertainty estimations while\nmaintaining the flexibility of deep neural networks. DUAL can be easily\nimplemented on existing models and deployed in real-time systems with minimal\nextra computational overhead. By linking the predictive uncertainty estimation\nability of DUAL to well-known bandit algorithms, we further present DUAL-based\nAd-ranking strategies to boost up long-term utilities such as the social\nwelfare in advertising systems. Experimental results on several public datasets\ndemonstrate the effectiveness of our methods. Remarkably, an online A/B test\ndeployed in the Alibaba display advertising platform shows an 8.2% social\nwelfare improvement and an 8.0% revenue lift.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 17:23:52 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 06:28:13 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Du", "Chao", ""], ["Gao", "Zhifeng", ""], ["Yuan", "Shuo", ""], ["Gao", "Lining", ""], ["Li", "Ziyan", ""], ["Zeng", "Yifan", ""], ["Zhu", "Xiaoqiang", ""], ["Xu", "Jian", ""], ["Gai", "Kun", ""], ["Lee", "Kuang-chih", ""]]}, {"id": "2012.02299", "submitter": "Anton Smerdov", "authors": "Anton Smerdov, Andrey Somov, Evgeny Burnaev, Bo Zhou, Paul Lukowicz", "title": "Detecting Video Game Player Burnout with the Use of Sensor Data and\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current research in eSports lacks the tools for proper game practising and\nperformance analytics. The majority of prior work relied only on in-game data\nfor advising the players on how to perform better. However, in-game mechanics\nand trends are frequently changed by new patches limiting the lifespan of the\nmodels trained exclusively on the in-game logs. In this article, we propose the\nmethods based on the sensor data analysis for predicting whether a player will\nwin the future encounter. The sensor data were collected from 10 participants\nin 22 matches in League of Legends video game. We have trained machine learning\nmodels including Transformer and Gated Recurrent Unit to predict whether the\nplayer wins the encounter taking place after some fixed time in the future. For\n10 seconds forecasting horizon Transformer neural network architecture achieves\nROC AUC score 0.706. This model is further developed into the detector capable\nof predicting that a player will lose the encounter occurring in 10 seconds in\n88.3% of cases with 73.5% accuracy. This might be used as a players' burnout or\nfatigue detector, advising players to retreat. We have also investigated which\nphysiological features affect the chance to win or lose the next in-game\nencounter.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 21:16:09 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Smerdov", "Anton", ""], ["Somov", "Andrey", ""], ["Burnaev", "Evgeny", ""], ["Zhou", "Bo", ""], ["Lukowicz", "Paul", ""]]}, {"id": "2012.02360", "submitter": "Jing Qin", "authors": "Jing Qin", "title": "Research Progress of News Recommendation Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to researchers'aim to study personalized recommendations for different\nbusiness fields, the summary of recommendation methods in specific fields is of\npractical significance. News recommendation systems were the earliest research\nfield regarding recommendation systems, and were also the earliest\nrecommendation field to apply the collaborative filtering method. In addition,\nnews is real-time and rich in content, which makes news recommendation methods\nmore challenging than in other fields. Thus, this paper summarizes the research\nprogress regarding news recommendation methods. From 2018 to 2020, developed\nnews recommendation methods were mainly deep learning-based, attention-based,\nand knowledge graphs-based. As of 2020, there are many news recommendation\nmethods that combine attention mechanisms and knowledge graphs. However, these\nmethods were all developed based on basic methods (the collaborative filtering\nmethod, the content-based recommendation method, and a mixed recommendation\nmethod combining the two). In order to allow researchers to have a detailed\nunderstanding of the development process of news recommendation methods, the\nnews recommendation methods surveyed in this paper, which cover nearly 10\nyears, are divided into three categories according to the abovementioned basic\nmethods. Firstly, the paper introduces the basic ideas of each category of\nmethods and then summarizes the recommendation methods that are combined with\nother methods based on each category of methods and according to the time\nsequence of research results. Finally, this paper also summarizes the\nchallenges confronting news recommendation systems.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 01:47:24 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 01:53:42 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Qin", "Jing", ""]]}, {"id": "2012.02476", "submitter": "Yn W", "authors": "Yanan Wang, Yong Ge, Li Li, Rui Chen, Tong Xu", "title": "Offline Meta-level Model-based Reinforcement Learning Approach for\n  Cold-Start Recommendation", "comments": "Accepted to Offline Reinforcement Learning Workshop at Neural\n  Information Processing Systems (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) has shown great promise in optimizing long-term\nuser interest in recommender systems. However, existing RL-based recommendation\nmethods need a large number of interactions for each user to learn a robust\nrecommendation policy. The challenge becomes more critical when recommending to\nnew users who have a limited number of interactions. To that end, in this\npaper, we address the cold-start challenge in the RL-based recommender systems\nby proposing a meta-level model-based reinforcement learning approach for fast\nuser adaptation. In our approach, we learn to infer each user's preference with\na user context variable that enables recommendation systems to better adapt to\nnew users with few interactions. To improve adaptation efficiency, we learn to\nrecover the user policy and reward from only a few interactions via an inverse\nreinforcement learning method to assist a meta-level recommendation agent.\nMoreover, we model the interaction relationship between the user model and\nrecommendation agent from an information-theoretic perspective. Empirical\nresults show the effectiveness of the proposed method when adapting to new\nusers with only a single interaction sequence. We further provide a theoretical\nanalysis of the recommendation performance bound.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 08:58:35 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Wang", "Yanan", ""], ["Ge", "Yong", ""], ["Li", "Li", ""], ["Chen", "Rui", ""], ["Xu", "Tong", ""]]}, {"id": "2012.02507", "submitter": "Damai Dai", "authors": "Damai Dai, Jing Ren, Shuang Zeng, Baobao Chang, Zhifang Sui", "title": "Coarse-to-Fine Entity Representations for Document-level Relation\n  Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document-level Relation Extraction (RE) requires extracting relations\nexpressed within and across sentences. Recent works show that graph-based\nmethods, usually constructing a document-level graph that captures\ndocument-aware interactions, can obtain useful entity representations thus\nhelping tackle document-level RE. These methods either focus more on the entire\ngraph, or pay more attention to a part of the graph, e.g., paths between the\ntarget entity pair. However, we find that document-level RE may benefit from\nfocusing on both of them simultaneously. Therefore, to obtain more\ncomprehensive entity representations, we propose the Coarse-to-Fine Entity\nRepresentation model (CFER) that adopts a coarse-to-fine strategy involving two\nphases. First, CFER uses graph neural networks to integrate global information\nin the entire graph at a coarse level. Next, CFER utilizes the global\ninformation as a guidance to selectively aggregate path information between the\ntarget entity pair at a fine level. In classification, we combine the entity\nrepresentations from both two levels into more comprehensive representations\nfor relation extraction. Experimental results on two document-level RE\ndatasets, DocRED and CDR, show that CFER outperforms existing models and is\nrobust to the uneven label distribution.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 10:18:59 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 13:07:31 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Dai", "Damai", ""], ["Ren", "Jing", ""], ["Zeng", "Shuang", ""], ["Chang", "Baobao", ""], ["Sui", "Zhifang", ""]]}, {"id": "2012.02594", "submitter": "Barun Patra", "authors": "Barun Patra, Chala Fufa, Pamela Bhattacharya and Charles Lee", "title": "To Schedule or not to Schedule: Extracting Task Specific Temporal\n  Entities and Associated Negation Constraints", "comments": "Proceedings of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art research for date-time entity extraction from text is task\nagnostic. Consequently, while the methods proposed in literature perform well\nfor generic date-time extraction from texts, they don't fare as well on task\nspecific date-time entity extraction where only a subset of the date-time\nentities present in the text are pertinent to solving the task. Furthermore,\nsome tasks require identifying negation constraints associated with the\ndate-time entities to correctly reason over time. We showcase a novel model for\nextracting task-specific date-time entities along with their negation\nconstraints. We show the efficacy of our method on the task of date-time\nunderstanding in the context of scheduling meetings for an email-based digital\nAI scheduling assistant. Our method achieves an absolute gain of 19\\% f-score\npoints compared to baseline methods in detecting the date-time entities\nrelevant to scheduling meetings and a 4\\% improvement over baseline methods for\ndetecting negation constraints over date-time entities.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 10:07:19 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Patra", "Barun", ""], ["Fufa", "Chala", ""], ["Bhattacharya", "Pamela", ""], ["Lee", "Charles", ""]]}, {"id": "2012.02629", "submitter": "Hong Xiong", "authors": "Hong Xiong", "title": "Linear Regression Evaluation of Search Engine Automatic Search\n  Performance Based on Hadoop and R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The automatic search performance of search engines has become an essential\npart of measuring the difference in user experience. An efficient automatic\nsearch system can significantly improve the performance of search engines and\nincrease user traffic. Hadoop has strong data integration and analysis\ncapabilities, while R has excellent statistical capabilities in linear\nregression. This article will propose a linear regression based on Hadoop and R\nto quantify the efficiency of the automatic retrieval system. We use R's\nfunctional properties to transform the user's search results upon linear\ncorrelations. In this way, the final output results have multiple display forms\ninstead of web page preview interfaces. This article provides feasible\nsolutions to the drawbacks of current search engine algorithms lacking once or\ntwice search accuracies and multiple types of search results. We can conduct\npersonalized regression analysis for user's needs with public datasets and\noptimize resources integration for most relevant information.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 18:28:50 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Xiong", "Hong", ""]]}, {"id": "2012.02639", "submitter": "Edward Fish", "authors": "Edward Fish, Jon Weinbren, Andrew Gilbert", "title": "Rethinking movie genre classification with fine-grained semantic\n  clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Movie genre classification is an active research area in machine learning.\nHowever, due to the limited labels available, there can be large semantic\nvariations between movies within a single genre definition. We expand these\n'coarse' genre labels by identifying 'fine-grained' semantic information within\nthe multi-modal content of movies. By leveraging pre-trained 'expert' networks,\nwe learn the influence of different combinations of modes for multi-label genre\nclassification. Using a contrastive loss, we continue to fine-tune this\n'coarse' genre classification network to identify high-level intertextual\nsimilarities between the movies across all genre labels. This leads to a more\n'fine-grained' and detailed clustering, based on semantic similarities while\nstill retaining some genre information. Our approach is demonstrated on a newly\nintroduced multi-modal 37,866,450 frame, 8,800 movie trailer dataset,\nMMX-Trailer-20, which includes pre-computed audio, location, motion, and image\nembeddings.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 14:58:31 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 10:30:01 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 16:46:09 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Fish", "Edward", ""], ["Weinbren", "Jon", ""], ["Gilbert", "Andrew", ""]]}, {"id": "2012.02930", "submitter": "Xiangyu Liu", "authors": "Zhilin Zhang, Xiangyu Liu, Zhenzhe Zheng, Chenrui Zhang, Miao Xu,\n  Junwei Pan, Chuan Yu, Fan Wu, Jian Xu and Kun Gai", "title": "Optimizing Multiple Performance Metrics with Deep GSP Auctions for\n  E-commerce Advertising", "comments": "To appear in the Proceedings of the 14th ACM International Conference\n  on Web Search and Data Mining (WSDM), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In e-commerce advertising, the ad platform usually relies on auction\nmechanisms to optimize different performance metrics, such as user experience,\nadvertiser utility, and platform revenue. However, most of the state-of-the-art\nauction mechanisms only focus on optimizing a single performance metric, e.g.,\neither social welfare or revenue, and are not suitable for e-commerce\nadvertising with various, dynamic, difficult to estimate, and even conflicting\nperformance metrics. In this paper, we propose a new mechanism called Deep GSP\nauction, which leverages deep learning to design new rank score functions\nwithin the celebrated GSP auction framework. These new rank score functions are\nimplemented via deep neural network models under the constraints of monotone\nallocation and smooth transition. The requirement of monotone allocation\nensures Deep GSP auction nice game theoretical properties, while the\nrequirement of smooth transition guarantees the advertiser utilities would not\nfluctuate too much when the auction mechanism switches among candidate\nmechanisms to achieve different optimization objectives. We deployed the\nproposed mechanisms in a leading e-commerce ad platform and conducted\ncomprehensive experimental evaluations with both offline simulations and online\nA/B tests. The results demonstrated the effectiveness of the Deep GSP auction\ncompared to the state-of-the-art auction mechanisms.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 02:51:11 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 08:27:49 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Zhang", "Zhilin", ""], ["Liu", "Xiangyu", ""], ["Zheng", "Zhenzhe", ""], ["Zhang", "Chenrui", ""], ["Xu", "Miao", ""], ["Pan", "Junwei", ""], ["Yu", "Chuan", ""], ["Wu", "Fan", ""], ["Xu", "Jian", ""], ["Gai", "Kun", ""]]}, {"id": "2012.02957", "submitter": "Junmo Kang", "authors": "Junmo Kang, Jeonghwan Kim, Suwon Shin, Sung-Hyon Myaeng", "title": "A Sequence-Oblivious Generation Method for Context-Aware Hashtag\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like search, a recommendation task accepts an input query or cue and provides\ndesirable items, often based on a ranking function. Such a ranking approach\nrarely considers explicit dependency among the recommended items. In this work,\nwe propose a generative approach to tag recommendation, where semantic tags are\nselected one at a time conditioned on the previously generated tags to model\ninter-dependency among the generated tags. We apply this tag recommendation\napproach to an Instagram data set where an array of context feature types\n(image, location, time, and text) are available for posts. To exploit the\ninter-dependency among the distinct types of features, we adopt a simple yet\neffective architecture using self-attention, making deep interactions possible.\nEmpirical results show that our method is significantly superior to not only\nthe usual ranking schemes but also autoregressive models for tag\nrecommendation. They indicate that it is critical to fuse mutually supporting\nfeatures at an early stage to induce extensive and comprehensive view on\ninter-context interaction in generating tags in a recurrent feedback loop.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 06:10:56 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Kang", "Junmo", ""], ["Kim", "Jeonghwan", ""], ["Shin", "Suwon", ""], ["Myaeng", "Sung-Hyon", ""]]}, {"id": "2012.03069", "submitter": "Yingjie Hu", "authors": "Kai Sun, Yingjie Hu, Jia Song, Yunqiang Zhu", "title": "Aligning geographic entities from historical maps for building knowledge\n  graphs", "comments": null, "journal-ref": "International Journal of Geographical Information Science, 1-30\n  (2020)", "doi": "10.1080/13658816.2020.1845702", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical maps contain rich geographic information about the past of a\nregion. They are sometimes the only source of information before the\navailability of digital maps. Despite their valuable content, it is often\nchallenging to access and use the information in historical maps, due to their\nforms of paper-based maps or scanned images. It is even more time-consuming and\nlabor-intensive to conduct an analysis that requires a synthesis of the\ninformation from multiple historical maps. To facilitate the use of the\ngeographic information contained in historical maps, one way is to build a\ngeographic knowledge graph (GKG) from them. This paper proposes a general\nworkflow for completing one important step of building such a GKG, namely\naligning the same geographic entities from different maps. We present this\nworkflow and the related methods for implementation, and systematically\nevaluate their performances using two different datasets of historical maps.\nThe evaluation results show that machine learning and deep learning models for\nmatching place names are sensitive to the thresholds learned from the training\ndata, and a combination of measures based on string similarity, spatial\ndistance, and approximate topological relation achieves the best performance\nwith an average F-score of 0.89.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 16:14:09 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Sun", "Kai", ""], ["Hu", "Yingjie", ""], ["Song", "Jia", ""], ["Zhu", "Yunqiang", ""]]}, {"id": "2012.03323", "submitter": "Seyed Danial Mohseni Taheri", "authors": "Mehrnaz Amjadi, Seyed Danial Mohseni Taheri, Theja Tulabandhula", "title": "KATRec: Knowledge Aware aTtentive Sequential Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential recommendation systems model dynamic preferences of users based on\ntheir historical interactions with platforms. Despite recent progress, modeling\nshort-term and long-term behavior of users in such systems is nontrivial and\nchallenging. To address this, we present a solution enhanced by a knowledge\ngraph called KATRec (Knowledge Aware aTtentive sequential Recommendations).\nKATRec learns the short and long-term interests of users by modeling their\nsequence of interacted items and leveraging pre-existing side information\nthrough a knowledge graph attention network. Our novel knowledge graph-enhanced\nsequential recommender contains item multi-relations at the entity-level and\nusers' dynamic sequences at the item-level. KATRec improves item representation\nlearning by considering higher-order connections and incorporating them in user\npreference representation while recommending the next item. Experiments on\nthree public datasets show that KATRec outperforms state-of-the-art\nrecommendation models and demonstrates the importance of modeling both temporal\nand side information to achieve high-quality recommendations.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 17:03:52 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 02:14:31 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 01:46:28 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Amjadi", "Mehrnaz", ""], ["Taheri", "Seyed Danial Mohseni", ""], ["Tulabandhula", "Theja", ""]]}, {"id": "2012.03397", "submitter": "Yasith Jayawardana", "authors": "Yasith Jayawardana, Alexander C. Nwala, Gavindya Jayawardena, Jian Wu,\n  Sampath Jayarathna, Michael L. Nelson, C. Lee Giles", "title": "Modeling Updates of Scholarly Webpages Using Archived Data", "comments": "12 pages, 2 appendix pages, 18 figures, to be published in\n  Proceedings of IEEE Big Data 2020 - 5th Computational Archival Science (CAS)\n  Workshop", "journal-ref": null, "doi": "10.1109/BigData50022.2020.9377796", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vastness of the web imposes a prohibitive cost on building large-scale\nsearch engines with limited resources. Crawl frontiers thus need to be\noptimized to improve the coverage and freshness of crawled content. In this\npaper, we propose an approach for modeling the dynamics of change in the web\nusing archived copies of webpages. To evaluate its utility, we conduct a\npreliminary study on the scholarly web using 19,977 seed URLs of authors'\nhomepages obtained from their Google Scholar profiles. We first obtain archived\ncopies of these webpages from the Internet Archive (IA), and estimate when\ntheir actual updates occurred. Next, we apply maximum likelihood to estimate\ntheir mean update frequency ($\\lambda$) values. Our evaluation shows that\n$\\lambda$ values derived from a short history of archived data provide a good\nestimate for the true update frequency in the short-term, and that our method\nprovides better estimations of updates at a fraction of resources compared to\nthe baseline models. Based on this, we demonstrate the utility of archived data\nto optimize the crawling strategy of web crawlers, and uncover important\nchallenges that inspire future research directions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 00:22:00 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Jayawardana", "Yasith", ""], ["Nwala", "Alexander C.", ""], ["Jayawardena", "Gavindya", ""], ["Wu", "Jian", ""], ["Jayarathna", "Sampath", ""], ["Nelson", "Michael L.", ""], ["Giles", "C. Lee", ""]]}, {"id": "2012.03433", "submitter": "Jiansheng Fang", "authors": "Jiansheng Fang, Xiaoqing Zhang, Yan Hu, Yanwu Xu, Ming Yang, Jiang Liu", "title": "Probabilistic Latent Factor Model for Collaborative Filtering with\n  Bayesian Inference", "comments": "8 pages, 5 figures, ICPR2020 conference", "journal-ref": null, "doi": "10.1109/ICPR48806.2021.9412376", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Latent Factor Model (LFM) is one of the most successful methods for\nCollaborative filtering (CF) in the recommendation system, in which both users\nand items are projected into a joint latent factor space. Base on matrix\nfactorization applied usually in pattern recognition, LFM models user-item\ninteractions as inner products of factor vectors of user and item in that space\nand can be efficiently solved by least square methods with optimal estimation.\nHowever, such optimal estimation methods are prone to overfitting due to the\nextreme sparsity of user-item interactions. In this paper, we propose a\nBayesian treatment for LFM, named Bayesian Latent Factor Model (BLFM). Based on\nobserved user-item interactions, we build a probabilistic factor model in which\nthe regularization is introduced via placing prior constraint on latent\nfactors, and the likelihood function is established over observations and\nparameters. Then we draw samples of latent factors from the posterior\ndistribution with Variational Inference (VI) to predict expected value. We\nfurther make an extension to BLFM, called BLFMBias, incorporating\nuser-dependent and item-dependent biases into the model for enhancing\nperformance. Extensive experiments on the movie rating dataset show the\neffectiveness of our proposed models by compared with several strong baselines.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 03:05:47 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Fang", "Jiansheng", ""], ["Zhang", "Xiaoqing", ""], ["Hu", "Yan", ""], ["Xu", "Yanwu", ""], ["Yang", "Ming", ""], ["Liu", "Jiang", ""]]}, {"id": "2012.03491", "submitter": "Anton Smerdov", "authors": "Anton Smerdov, Evgeny Burnaev, Andrey Somov", "title": "AI-enabled Prediction of eSports Player Performance Using the Data from\n  Heterogeneous Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging progress of eSports lacks the tools for ensuring high-quality\nanalytics and training in Pro and amateur eSports teams. We report on an\nArtificial Intelligence (AI) enabled solution for predicting the eSports player\nin-game performance using exclusively the data from sensors. For this reason,\nwe collected the physiological, environmental, and the game chair data from Pro\nand amateur players. The player performance is assessed from the game logs in a\nmultiplayer game for each moment of time using a recurrent neural network. We\nhave investigated that attention mechanism improves the generalization of the\nnetwork and provides the straightforward feature importance as well. The best\nmodel achieves ROC AUC score 0.73. The prediction of the performance of\nparticular player is realized although his data are not utilized in the\ntraining set. The proposed solution has a number of promising applications for\nPro eSports teams as well as a learning tool for amateur players.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 07:31:53 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Smerdov", "Anton", ""], ["Burnaev", "Evgeny", ""], ["Somov", "Andrey", ""]]}, {"id": "2012.03704", "submitter": "Svitlana Vakulenko", "authors": "Svitlana Vakulenko, Vadim Savenkov, Maarten de Rijke", "title": "Conversational Browsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we better understand the mechanisms behind multi-turn information\nseeking dialogues? How can we use these insights to design a dialogue system\nthat does not require explicit query formulation upfront as in question\nanswering? To answer these questions, we collected observations of human\nparticipants performing a similar task to obtain inspiration for the system\ndesign. Then, we studied the structure of conversations that occurred in these\nsettings and used the resulting insights to develop a grounded theory, design\nand evaluate a first system prototype. Evaluation results show that our\napproach is effective and can complement query-based information retrieval\napproaches. We contribute new insights about information-seeking behavior by\nanalyzing and providing automated support for a type of information-seeking\nstrategy that is effective when the clarity of the information need and\nfamiliarity with the collection content are low.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 14:03:04 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["Savenkov", "Vadim", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2012.03830", "submitter": "Alexandre Sava", "authors": "Fei Huang (LCOMS, HYIT), Alexandre Sava (LCOMS), Kondo H. Adjallah\n  (LCOMS), Wang Zhouhang (LCOMS)", "title": "Bearings degradation monitoring indicators based on discarded projected\n  space information and piecewise linear representation", "comments": null, "journal-ref": "International Journal of Mechatronics and Automation, 2020, 7 (1),\n  pp.23-31", "doi": "10.1504/IJMA.2020.108185", "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Condition-based maintenance of rotating mechanics requests efficient bearings\ndegradation monitoring. The accuracy of bearings degradation measure depends\nlargely on degradation indicators. To extract efficient indicators, in this\npaper we propose a method based on the discarded projected space information\nand piecewise linear representation (PLR) to build three bearings degradation\nmonitoring indicators which are named SDHT2, VSDHT2 and NVSDHT2. The discarded\nprojected space information is measured by the segmented discarded Hotelling T\nsquare we propose in this paper. For illustration, the IEEE PHM 2012 benchmark\ndataset is used in this paper. The results show that the three new indicators\nare all sensitive and monotonic during the bearings whole lifecycle. They\ndescribe the whole degradation process history and carry the real-time\ninformation of bearings degradation. And NVSDHT2 is the generalised version of\nVSDHT2, which is promising to monitor bearings degradation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 16:26:48 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Huang", "Fei", "", "LCOMS, HYIT"], ["Sava", "Alexandre", "", "LCOMS"], ["Adjallah", "Kondo H.", "", "LCOMS"], ["Zhouhang", "Wang", "", "LCOMS"]]}, {"id": "2012.04005", "submitter": "Veysel Kocaman Vk", "authors": "Veysel Kocaman, David Talby", "title": "Improving Clinical Document Understanding on COVID-19 Research with\n  Spark NLP", "comments": "Accepted to SDU (Scientific Document Understanding) workshop at AAAI\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Following the global COVID-19 pandemic, the number of scientific papers\nstudying the virus has grown massively, leading to increased interest in\nautomated literate review. We present a clinical text mining system that\nimproves on previous efforts in three ways. First, it can recognize over 100\ndifferent entity types including social determinants of health, anatomy, risk\nfactors, and adverse events in addition to other commonly used clinical and\nbiomedical entities. Second, the text processing pipeline includes assertion\nstatus detection, to distinguish between clinical facts that are present,\nabsent, conditional, or about someone other than the patient. Third, the deep\nlearning models used are more accurate than previously available, leveraging an\nintegrated pipeline of state-of-the-art pretrained named entity recognition\nmodels, and improving on the previous best performing benchmarks for assertion\nstatus detection. We illustrate extracting trends and insights, e.g. most\nfrequent disorders and symptoms, and most common vital signs and EKG findings,\nfrom the COVID-19 Open Research Dataset (CORD-19). The system is built using\nthe Spark NLP library which natively supports scaling to use distributed\nclusters, leveraging GPUs, configurable and reusable NLP pipelines, healthcare\nspecific embeddings, and the ability to train models to support new entity\ntypes or human languages with no code changes.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 19:17:05 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Kocaman", "Veysel", ""], ["Talby", "David", ""]]}, {"id": "2012.04203", "submitter": "Yuqi Kong", "authors": "Yuqi Kong, Fanchao Meng, Benjamin Carterette", "title": "A Topological Method for Comparing Document Semantics", "comments": "9 pages, 3 tables, 9th International Conference on Natural Language\n  Processing (NLP 2020)", "journal-ref": "pp. 143-151, 2020. CS & IT - CSCP 2020", "doi": "10.5121/csit.2020.101411", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Comparing document semantics is one of the toughest tasks in both Natural\nLanguage Processing and Information Retrieval. To date, on one hand, the tools\nfor this task are still rare. On the other hand, most relevant methods are\ndevised from the statistic or the vector space model perspectives but nearly\nnone from a topological perspective. In this paper, we hope to make a different\nsound. A novel algorithm based on topological persistence for comparing\nsemantics similarity between two documents is proposed. Our experiments are\nconducted on a document dataset with human judges' results. A collection of\nstate-of-the-art methods are selected for comparison. The experimental results\nshow that our algorithm can produce highly human-consistent results, and also\nbeats most state-of-the-art methods though ties with NLTK.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 04:21:40 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Kong", "Yuqi", ""], ["Meng", "Fanchao", ""], ["Carterette", "Benjamin", ""]]}, {"id": "2012.04357", "submitter": "SeongKu Kang", "authors": "SeongKu Kang, Junyoung Hwang, Wonbin Kweon, Hwanjo Yu", "title": "DE-RRD: A Knowledge Distillation Framework for Recommender System", "comments": null, "journal-ref": null, "doi": "10.1145/3340531.3412005", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent recommender systems have started to employ knowledge distillation,\nwhich is a model compression technique distilling knowledge from a cumbersome\nmodel (teacher) to a compact model (student), to reduce inference latency while\nmaintaining performance. The state-of-the-art methods have only focused on\nmaking the student model accurately imitate the predictions of the teacher\nmodel. They have a limitation in that the prediction results incompletely\nreveal the teacher's knowledge. In this paper, we propose a novel knowledge\ndistillation framework for recommender system, called DE-RRD, which enables the\nstudent model to learn from the latent knowledge encoded in the teacher model\nas well as from the teacher's predictions. Concretely, DE-RRD consists of two\nmethods: 1) Distillation Experts (DE) that directly transfers the latent\nknowledge from the teacher model. DE exploits \"experts\" and a novel expert\nselection strategy for effectively distilling the vast teacher's knowledge to\nthe student with limited capacity. 2) Relaxed Ranking Distillation (RRD) that\ntransfers the knowledge revealed from the teacher's prediction with\nconsideration of the relaxed ranking orders among items. Our extensive\nexperiments show that DE-RRD outperforms the state-of-the-art competitors and\nachieves comparable or even better performance to that of the teacher model\nwith faster inference time.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 11:09:22 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Kang", "SeongKu", ""], ["Hwang", "Junyoung", ""], ["Kweon", "Wonbin", ""], ["Yu", "Hwanjo", ""]]}, {"id": "2012.04388", "submitter": "Amit Kumar", "authors": "Chiranjib Bhattacharyya and Ravindran Kannan and Amit Kumar", "title": "Algorithms for finding $k$ in $k$-means", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  $k-$means Clustering requires as input the exact value of $k$, the number of\nclusters. Two challenges are open: (i) Is there a data-determined definition of\n$k$ which is provably correct and (ii) Is there a polynomial time algorithm to\nfind $k$ from data ? This paper provides the first affirmative answers to both\nthese questions. As common in the literature, we assume that the data admits an\nunknown Ground Truth (GT) clustering with cluster centers separated. This\nassumption alone is not sufficient to answer Yes to (i). We assume a novel, but\nnatural second constraint called no tight sub-cluster (NTSC) which stipulates\nthat no substantially large subset of a GT cluster can be \"tighter\" (in a sense\nwe define) than the cluster. Our yes answer to (i) and (ii) are under these two\ndeterministic assumptions. We also give polynomial time algorithm to identify\n$k$. Our algorithm relies on NTSC to peel off one cluster at a time by\nidentifying points which are tightly packed. We are also able to show that our\nalgorithm(s) apply to data generated by mixtures of Gaussians and more\ngenerally to mixtures of sub-Gaussian pdf's and hence are able to find the\nnumber of components of the mixture from data. To our knowledge, previous\nresults for these specialized settings as well, assume generally that $k$ is\ngiven besides the data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 12:08:05 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Bhattacharyya", "Chiranjib", ""], ["Kannan", "Ravindran", ""], ["Kumar", "Amit", ""]]}, {"id": "2012.04426", "submitter": "Harrie Oosterhuis", "authors": "Harrie Oosterhuis and Maarten de Rijke", "title": "Unifying Online and Counterfactual Learning to Rank", "comments": "Harrie Oosterhuis and Maarten de Rijke. 2021. Unifying Online and\n  Counterfactual Learning to Rank: A Novel Counterfactual Estimator that\n  Effectively Utilizes Online Interventions. In The 14th ACM International\n  Conference on Web Search and Data Mining (WSDM '21), March 8-12, 2021,\n  Jerusalem, Israel. ACM, New York, NY, USA, 9 pages.\n  https://doi.org/10.1145/3437963.3441794", "journal-ref": null, "doi": "10.1145/3437963.3441794", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing ranking systems based on user interactions is a well-studied\nproblem. State-of-the-art methods for optimizing ranking systems based on user\ninteractions are divided into online approaches - that learn by directly\ninteracting with users - and counterfactual approaches - that learn from\nhistorical interactions. Existing online methods are hindered without online\ninterventions and thus should not be applied counterfactually. Conversely,\ncounterfactual methods cannot directly benefit from online interventions. We\npropose a novel intervention-aware estimator for both counterfactual and online\nLearning to Rank (LTR). With the introduction of the intervention-aware\nestimator, we aim to bridge the online/counterfactual LTR division as it is\nshown to be highly effective in both online and counterfactual scenarios. The\nestimator corrects for the effect of position bias, trust bias, and\nitem-selection bias by using corrections based on the behavior of the logging\npolicy and on online interventions: changes to the logging policy made during\nthe gathering of click data. Our experimental results, conducted in a\nsemi-synthetic experimental setup, show that, unlike existing counterfactual\nLTR methods, the intervention-aware estimator can greatly benefit from online\ninterventions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 13:56:27 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Oosterhuis", "Harrie", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2012.04545", "submitter": "Angelo Ziletti", "authors": "Angelo Ziletti, Christoph Berns, Oliver Treichel, Thomas Weber,\n  Jennifer Liang, Stephanie Kammerath, Marion Schwaerzler, Jagatheswari\n  Virayah, David Ruau, Xin Ma, Andreas Mattern", "title": "Discovering key topics from short, real-world medical inquiries via\n  natural language processing and unsupervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Millions of unsolicited medical inquiries are received by pharmaceutical\ncompanies every year. It has been hypothesized that these inquiries represent a\ntreasure trove of information, potentially giving insight into matters\nregarding medicinal products and the associated medical treatments. However,\ndue to the large volume and specialized nature of the inquiries, it is\ndifficult to perform timely, recurrent, and comprehensive analyses. Here, we\npropose a machine learning approach based on natural language processing and\nunsupervised learning to automatically discover key topics in real-world\nmedical inquiries from customers. This approach does not require ontologies nor\nannotations. The discovered topics are meaningful and medically relevant, as\njudged by medical information specialists, thus demonstrating that unsolicited\nmedical inquiries are a source of valuable customer insights. Our work paves\nthe way for the machine-learning-driven analysis of medical inquiries in the\npharmaceutical industry, which ultimately aims at improving patient care.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 16:37:34 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Ziletti", "Angelo", ""], ["Berns", "Christoph", ""], ["Treichel", "Oliver", ""], ["Weber", "Thomas", ""], ["Liang", "Jennifer", ""], ["Kammerath", "Stephanie", ""], ["Schwaerzler", "Marion", ""], ["Virayah", "Jagatheswari", ""], ["Ruau", "David", ""], ["Ma", "Xin", ""], ["Mattern", "Andreas", ""]]}, {"id": "2012.04551", "submitter": "Ajinkya Kadu", "authors": "Ajinkya Kadu, Tristan van Leeuwen, and K. Joost Batenburg", "title": "CoShaRP: A Convex Program for Single-shot Tomographic Shape Sensing", "comments": "Paper is currently under consideration for Pattern Recognition\n  Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE cs.IR eess.IV math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce single-shot X-ray tomography that aims to estimate the target\nimage from a single cone-beam projection measurement. This linear inverse\nproblem is extremely under-determined since the measurements are far fewer than\nthe number of unknowns. Moreover, it is more challenging than conventional\ntomography where a sufficiently large number of projection angles forms the\nmeasurements, allowing for a simple inversion process. However, single-shot\ntomography becomes less severe if the target image is only composed of known\nshapes. Hence, the shape prior transforms a linear ill-posed image estimation\nproblem to a non-linear problem of estimating the roto-translations of the\nshapes. In this paper, we circumvent the non-linearity by using a dictionary of\npossible roto-translations of the shapes. We propose a convex program CoShaRP\nto recover the dictionary-coefficients successfully. CoShaRP relies on\nsimplex-type constraint and can be solved quickly using a primal-dual\nalgorithm. The numerical experiments show that CoShaRP recovers shapes stably\nfrom moderately noisy measurements.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 16:44:34 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 21:02:42 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kadu", "Ajinkya", ""], ["van Leeuwen", "Tristan", ""], ["Batenburg", "K. Joost", ""]]}, {"id": "2012.04558", "submitter": "Yuexin Wu", "authors": "Yuexin Wu, Tianyu Gao, Sihao Wang, Zhongmin Xiong", "title": "TADO: Time-varying Attention with Dual-Optimizer Model", "comments": null, "journal-ref": "ICDM2020", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The review-based recommender systems are commonly utilized to measure users\npreferences towards different items. In this paper, we focus on addressing\nthree main problems existing in the review-based methods. Firstly, these\nmethods suffer from the class-imbalanced problem where rating levels with lower\nproportions will be ignored to some extent. Thus, their performance on\nrelatively rare rating levels is unsatisfactory. As the first attempt in this\nfield to address this problem, we propose a flexible dual-optimizer model to\ngain robustness from both regression loss and classification loss. Secondly, to\naddress the problem caused by the insufficient contextual information\nextraction ability of word embedding, we first introduce BERT into the\nreview-based method to improve the performance of the semantic analysis.\nThirdly, the existing methods ignore the feature information of the\ntime-varying user preferences. Therefore, we propose a time-varying feature\nextraction module with bidirectional long short-term memory and multi-scale\nconvolutional neural network. Afterward, an interaction component is proposed\nto further summarize the contextual information of the user-item pairs. To\nverify the effectiveness of the proposed TADO, we conduct extensive experiments\non 23 benchmark datasets selected from Amazon Product Reviews. Compared with\nseveral recently proposed state-of-the-art methods, our model obtains\nsignificant gain over ALFM, MPCN, and ANR averagely with 20.98\\%, 9.84\\%, and\n15.46\\%, respectively. Further analysis proves the necessity of jointly using\nthe proposed components in TADO.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 16:53:20 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 03:59:54 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Wu", "Yuexin", ""], ["Gao", "Tianyu", ""], ["Wang", "Sihao", ""], ["Xiong", "Zhongmin", ""]]}, {"id": "2012.04681", "submitter": "Aditya Mantha", "authors": "Aditya Mantha, Anirudha Sundaresan, Shashank Kedia, Yokila Arora,\n  Shubham Gupta, Gaoyang Wang, Praveenkumar Kanumala, Stephen Guo, Kannan Achan", "title": "A Real-Time Whole Page Personalization Framework for E-Commerce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  E-commerce platforms consistently aim to provide personalized recommendations\nto drive user engagement, enhance overall user experience, and improve business\nmetrics. Most e-commerce platforms contain multiple carousels on their\nhomepage, each attempting to capture different facets of the shopping\nexperience. Given varied user preferences, optimizing the placement of these\ncarousels is critical for improved user satisfaction. Furthermore, items within\na carousel may change dynamically based on sequential user actions, thus\nnecessitating online ranking of carousels. In this work, we present a scalable\nend-to-end production system to optimally rank item-carousels in real-time on\nthe Walmart online grocery homepage. The proposed system utilizes a novel model\nthat captures the user's affinity for different carousels and their likelihood\nto interact with previously unseen items. Our system is flexible in design and\nis easily extendable to settings where page components need to be ranked. We\nprovide the system architecture consisting of a model development phase and an\nonline inference framework. To ensure low-latency, various optimizations across\nthese stages are implemented. We conducted extensive online evaluations to\nbenchmark against the prior experience. In production, our system resulted in\nan improvement in item discovery, an increase in online engagement, and a\nsignificant lift on add-to-carts (ATCs) per visitor on the homepage.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 19:08:41 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Mantha", "Aditya", ""], ["Sundaresan", "Anirudha", ""], ["Kedia", "Shashank", ""], ["Arora", "Yokila", ""], ["Gupta", "Shubham", ""], ["Wang", "Gaoyang", ""], ["Kanumala", "Praveenkumar", ""], ["Guo", "Stephen", ""], ["Achan", "Kannan", ""]]}, {"id": "2012.04682", "submitter": "Leo Tam", "authors": "Leo K. Tam and Xiaosong Wang and Daguang Xu", "title": "Transformer Query-Target Knowledge Discovery (TEND): Drug Discovery from\n  CORD-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work established skip-gram word2vec models could be used to mine\nknowledge in the materials science literature for the discovery of\nthermoelectrics. Recent transformer architectures have shown great progress in\nlanguage modeling and associated fine-tuned tasks, but they have yet to be\nadapted for drug discovery. We present a RoBERTa transformer-based method that\nextends the masked language token prediction using query-target conditioning to\ntreat the specificity challenge. The transformer discovery method entails\nseveral benefits over the word2vec method including domain-specific (antiviral)\nanalogy performance, negation handling, and flexible query analysis (specific)\nand is demonstrated on influenza drug discovery. To stimulate COVID-19\nresearch, we release an influenza clinical trials and antiviral analogies\ndataset used in conjunction with the COVID-19 Open Research Dataset Challenge\n(CORD-19) literature dataset in the study. We examine k-shot fine-tuning to\nimprove the downstream analogies performance as well as to mine analogies for\nmodel explainability. Further, the query-target analysis is verified in a\nforward chaining analysis against the influenza drug clinical trials dataset,\nbefore adapted for COVID-19 drugs (combinations and side-effects) and on-going\nclinical trials. In consideration of the present topic, we release the model,\ndataset, and code.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 04:30:31 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 00:09:17 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Tam", "Leo K.", ""], ["Wang", "Xiaosong", ""], ["Xu", "Daguang", ""]]}, {"id": "2012.04780", "submitter": "Sarthak Dash", "authors": "Sarthak Dash, Gaetano Rossiello, Nandana Mihindukulasooriya, Sugato\n  Bagchi, Alfio Gliozzo", "title": "Joint Entity and Relation Canonicalization in Open Knowledge Graphs\n  using Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noun phrases and relation phrases in open knowledge graphs are not\ncanonicalized, leading to an explosion of redundant and ambiguous\nsubject-relation-object triples. Existing approaches to face this problem take\na two-step approach: first, they generate embedding representations for both\nnoun and relation phrases, then a clustering algorithm is used to group them\nusing the embeddings as features. In this work, we propose Canonicalizing Using\nVariational AutoEncoders (CUVA), a joint model to learn both embeddings and\ncluster assignments in an end-to-end approach, which leads to a better vector\nrepresentation for the noun and relation phrases. Our evaluation over multiple\nbenchmarks shows that CUVA outperforms the existing state of the art\napproaches. Moreover, we introduce CanonicNell a novel dataset to evaluate\nentity canonicalization systems.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 22:58:30 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Dash", "Sarthak", ""], ["Rossiello", "Gaetano", ""], ["Mihindukulasooriya", "Nandana", ""], ["Bagchi", "Sugato", ""], ["Gliozzo", "Alfio", ""]]}, {"id": "2012.04864", "submitter": "Haipeng Chen", "authors": "Qi Zhou, Haipeng Chen, Yitao Zheng, Zhen Wang", "title": "EvaLDA: Efficient Evasion Attacks Towards Latent Dirichlet Allocation", "comments": "Accepted for publication at AAAI'2021. 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the most powerful topic models, Latent Dirichlet Allocation (LDA)\nhas been used in a vast range of tasks, including document understanding,\ninformation retrieval and peer-reviewer assignment. Despite its tremendous\npopularity, the security of LDA has rarely been studied. This poses severe\nrisks to security-critical tasks such as sentiment analysis and peer-reviewer\nassignment that are based on LDA. In this paper, we are interested in knowing\nwhether LDA models are vulnerable to adversarial perturbations of benign\ndocument examples during inference time. We formalize the evasion attack to LDA\nmodels as an optimization problem and prove it to be NP-hard. We then propose a\nnovel and efficient algorithm, EvaLDA to solve it. We show the effectiveness of\nEvaLDA via extensive empirical evaluations. For instance, in the NIPS dataset,\nEvaLDA can averagely promote the rank of a target topic from 10 to around 7 by\nonly replacing 1% of the words with similar words in a victim document. Our\nwork provides significant insights into the power and limitations of evasion\nattacks to LDA models.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 04:57:20 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 03:12:53 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhou", "Qi", ""], ["Chen", "Haipeng", ""], ["Zheng", "Yitao", ""], ["Wang", "Zhen", ""]]}, {"id": "2012.04979", "submitter": "Arash Khoeini", "authors": "Arash Khoeini, Saman Haratizadeh, Ehsan Hoseinzade", "title": "Representation Extraction and Deep Neural Recommendation for\n  Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many Deep Learning approaches solve complicated classification and regression\nproblems by hierarchically constructing complex features from the raw input\ndata. Although a few works have investigated the application of deep neural\nnetworks in recommendation domain, they mostly extract entity features by\nexploiting unstructured auxiliary data such as visual and textual information,\nand when it comes to using user-item rating matrix, feature extraction is done\nby using matrix factorization. As matrix factorization has some limitations,\nsome works have been done to replace it with deep neural network. but these\nworks either need to exploit unstructured data such item's reviews or images,\nor are specially designed to use implicit data and don't take user-item rating\nmatrix into account. In this paper, we investigate the usage of novel\nrepresentation learning algorithms to extract users and items representations\nfrom rating matrix, and offer a deep neural network for Collaborative\nFiltering. Our proposed approach is a modular algorithm consisted of two main\nphases: REpresentation eXtraction and a deep neural NETwork (RexNet). Using two\njoint and parallel neural networks in RexNet enables it to extract a hierarchy\nof features for each entity in order to predict the degree of interest of users\nto items. The resulted predictions are then used for the final recommendation.\nUnlike other deep learning recommendation approaches, RexNet is not dependent\nto unstructured auxiliary data such as visual and textual information, instead,\nit uses only the user-item rate matrix as its input. We evaluated RexNet in an\nextensive set of experiments against state of the art recommendation methods.\nThe results show that RexNet significantly outperforms the baseline algorithms\nin a variety of data sets with different degrees of density.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 11:15:23 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Khoeini", "Arash", ""], ["Haratizadeh", "Saman", ""], ["Hoseinzade", "Ehsan", ""]]}, {"id": "2012.05009", "submitter": "Yuexin Wu", "authors": "Yuexin Wu, Tianyu Gao, Hongtao Liu", "title": "GRP: A Gumbel-based Rating Prediction Framework for Imbalanced\n  Recommendation", "comments": "13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rating prediction is a core problem in recommender systems to quantify users\npreferences towards different items. Due to the imbalanced rating distributions\nin training data, existing recommendation methods suffer from the biased\nprediction problem that generates biased prediction results. Thus, their\nperformance on predicting ratings which rarely appear in training data is\nunsatisfactory. In this paper, inspired by the superior capability of Extreme\nValue Distribution (EVD)-based methods in modeling the distribution of rare\ndata, we propose a novel \\underline{\\emph{G}}umbel Distribution-based\n\\underline{\\emph{R}}ating \\underline{\\emph{P}}rediction framework (GRP) which\ncan accurately predict both frequent and rare ratings between users and items.\nIn our approach, we first define different Gumbel distributions for each rating\nlevel, which can be learned by historical rating statistics of users and items.\nSecond, we incorporate the Gumbel-based representations of users and items with\ntheir original representations learned from the rating matrix and/or reviews to\nenrich the representations of users and items via a proposed multi-scale\nconvolutional fusion layer. Third, we propose a data-driven rating prediction\nmodule to predict the ratings of user-item pairs. It's worthy to note that our\napproach can be readily applied to existing recommendation methods for\naddressing their biased prediction problem. To verify the effectiveness of GRP,\nwe conduct extensive experiments on eight benchmark datasets. Compared with\nseveral baseline models, the results show that: 1) GRP achieves\nstate-of-the-art overall performance on all eight datasets; 2) GRP makes a\nsubstantial improvement in predicting rare ratings, which shows the\neffectiveness of our model in addressing the bias prediction problem.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 12:40:43 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Wu", "Yuexin", ""], ["Gao", "Tianyu", ""], ["Liu", "Hongtao", ""]]}, {"id": "2012.05031", "submitter": "Liu Yunfei", "authors": "Yunfei Liu, Yang Yang, Xianyu Chen, Jian Shen, Haifeng Zhang, Yong Yu", "title": "Improving Knowledge Tracing via Pre-training Question Embeddings", "comments": "7 pages,3 figures, IJCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge tracing (KT) defines the task of predicting whether students can\ncorrectly answer questions based on their historical response. Although much\nresearch has been devoted to exploiting the question information, plentiful\nadvanced information among questions and skills hasn't been well extracted,\nmaking it challenging for previous work to perform adequately. In this paper,\nwe demonstrate that large gains on KT can be realized by pre-training\nembeddings for each question on abundant side information, followed by training\ndeep KT models on the obtained embeddings. To be specific, the side information\nincludes question difficulty and three kinds of relations contained in a\nbipartite graph between questions and skills. To pre-train the question\nembeddings, we propose to use product-based neural networks to recover the side\ninformation. As a result, adopting the pre-trained embeddings in existing deep\nKT models significantly outperforms state-of-the-art baselines on three common\nKT datasets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 13:21:23 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Liu", "Yunfei", ""], ["Yang", "Yang", ""], ["Chen", "Xianyu", ""], ["Shen", "Jian", ""], ["Zhang", "Haifeng", ""], ["Yu", "Yong", ""]]}, {"id": "2012.05101", "submitter": "Erwan Le Merrer", "authors": "Erwan Le Merrer and Benoit Morgan and Gilles Tr\\'edan", "title": "Setting the Record Straighter on Shadow Banning", "comments": "Appearing in INFOCOM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shadow banning consists for an online social network in limiting the\nvisibility of some of its users, without them being aware of it. Twitter\ndeclares that it does not use such a practice, sometimes arguing about the\noccurrence of \"bugs\" to justify restrictions on some users. This paper is the\nfirst to address the plausibility or not of shadow banning on a major online\nplatform, by adopting both a statistical and a graph topological approach. We\nfirst conduct an extensive data collection and analysis campaign, gathering\noccurrences of visibility limitations on user profiles (we crawl more than 2.5\nmillion of them). In such a black-box observation setup, we highlight the\nsalient user profile features that may explain a banning practice (using\nmachine learning predictors). We then pose two hypotheses for the phenomenon:\ni) limitations are bugs, as claimed by Twitter, and ii) shadow banning\npropagates as an epidemic on user-interactions ego-graphs. We show that\nhypothesis i) is statistically unlikely with regards to the data we collected.\nWe then show some interesting correlation with hypothesis ii), suggesting that\nthe interaction topology is a good indicator of the presence of groups of\nshadow banned users on the service.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 15:17:33 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 12:08:39 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Merrer", "Erwan Le", ""], ["Morgan", "Benoit", ""], ["Tr\u00e9dan", "Gilles", ""]]}, {"id": "2012.05397", "submitter": "Dengya Zhu", "authors": "Dengya Zhu, Shastri Lakshman Nimmagadda, Torsten Reiners and Amit\n  Rudra", "title": "An Integrated Search Framework for Leveraging the Knowledge-Based Web\n  Ecosystem", "comments": null, "journal-ref": "Australasian Journal of Information Systems, 2020, Vol 24,\n  Research Article, pp. 1-24", "doi": "10.3127/ajis.v24i0.2331", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The explosion of information constrains the judgement of search terms\nassociated with Knowledge-Based Web Ecosystem (KBWE), making the retrieval of\nrelevant information and its knowledge management challenging. The existing\ninformation retrieval (IR) tools and their fusion in a framework need\nattention, in which search results can effectively be managed. In this article,\nwe demonstrate the effective use of information retrieval services by a variety\nof users and agents in various KBWE scenarios. An innovative Integrated Search\nFramework (ISF) is proposed, which utilises crawling strategies, web search\ntechnologies and traditional database search methods. Besides, ISF offers\ncomprehensive, dynamic, personalized, and organization-oriented information\nretrieval services, ranging from the Internet, extranet, intranet, to personal\ndesktop. In this empirical research, experiments are carried out demonstrating\nthe improvements in the search process, as discerned in the conceptual ISF. The\nexperimental results show improved precision compared with other popular search\nengines.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 01:32:33 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Zhu", "Dengya", ""], ["Nimmagadda", "Shastri Lakshman", ""], ["Reiners", "Torsten", ""], ["Rudra", "Amit", ""]]}, {"id": "2012.05422", "submitter": "Ziyang Wang", "authors": "Ziyang Wang, Wei Wei, Xian-Ling Mao, Xiao-Li Li and Shanshan Feng", "title": "Exploiting Group-level Behavior Pattern forSession-based Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommendation (SBR) is a challenging task, which aims to\npredict users' future interests based on anonymous behavior sequences. Existing\nmethods leverage powerful representation learning approaches to encode sessions\ninto a low-dimensional space. However, despite such achievements, all the\nexisting studies focus on the instance-level session learning, while neglecting\nthe group-level users' preference, which is significant to model the users'\nbehavior. To this end, we propose a novel Repeat-aware Neural Mechanism for\nSession-based Recommendation (RNMSR). In RNMSR, we propose to learn the user\npreference from both instance-level and group-level, respectively: (i)\ninstance-level, which employs GNNs on a similarity-based item-pairwise session\ngraph to capture the users' preference in instance-level. (ii) group-level,\nwhich converts sessions into group-level behavior patterns to model the\ngroup-level users' preference. In RNMSR, we combine instance-level user\npreference and group-level user preference to model the repeat consumption of\nusers, \\ie whether users take repeated consumption and which items are\npreferred by users. Extensive experiments are conducted on three real-world\ndatasets, \\ie Diginetica, Yoochoose, and Nowplaying, demonstrating that the\nproposed method consistently achieves state-of-the-art performance in all the\ntests.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 02:45:33 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 16:04:00 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wang", "Ziyang", ""], ["Wei", "Wei", ""], ["Mao", "Xian-Ling", ""], ["Li", "Xiao-Li", ""], ["Feng", "Shanshan", ""]]}, {"id": "2012.05462", "submitter": "Yujia Zheng", "authors": "Yujia Zheng, Siyi Liu, Zekun Li, Shu Wu", "title": "Cold-start Sequential Recommendation via Meta Learner", "comments": "Accepted at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores meta-learning in sequential recommendation to alleviate\nthe item cold-start problem. Sequential recommendation aims to capture user's\ndynamic preferences based on historical behavior sequences and acts as a key\ncomponent of most online recommendation scenarios. However, most previous\nmethods have trouble recommending cold-start items, which are prevalent in\nthose scenarios. As there is generally no side information in the setting of\nsequential recommendation task, previous cold-start methods could not be\napplied when only user-item interactions are available. Thus, we propose a\nMeta-learning-based Cold-Start Sequential Recommendation Framework, namely\nMecos, to mitigate the item cold-start problem in sequential recommendation.\nThis task is non-trivial as it targets at an important problem in a novel and\nchallenging context. Mecos effectively extracts user preference from limited\ninteractions and learns to match the target cold-start item with the potential\nuser. Besides, our framework can be painlessly integrated with neural\nnetwork-based models. Extensive experiments conducted on three real-world\ndatasets verify the superiority of Mecos, with the average improvement up to\n99%, 91%, and 70% in HR@10 over state-of-the-art baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 05:23:13 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Zheng", "Yujia", ""], ["Liu", "Siyi", ""], ["Li", "Zekun", ""], ["Wu", "Shu", ""]]}, {"id": "2012.05750", "submitter": "Matthias Samwald", "authors": "Simon Ott, Laura Graf, Asan Agibetov, Christian Meilicke, Matthias\n  Samwald", "title": "Scalable and interpretable rule-based link prediction for large\n  heterogeneous knowledge graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural embedding-based machine learning models have shown promise for\npredicting novel links in biomedical knowledge graphs. Unfortunately, their\npractical utility is diminished by their lack of interpretability. Recently,\nthe fully interpretable, rule-based algorithm AnyBURL yielded highly\ncompetitive results on many general-purpose link prediction benchmarks.\nHowever, its applicability to large-scale prediction tasks on complex\nbiomedical knowledge bases is limited by long inference times and difficulties\nwith aggregating predictions made by multiple rules. We improve upon AnyBURL by\nintroducing the SAFRAN rule application framework which aggregates rules\nthrough a scalable clustering algorithm. SAFRAN yields new state-of-the-art\nresults for fully interpretable link prediction on the established\ngeneral-purpose benchmark FB15K-237 and the large-scale biomedical benchmark\nOpenBioLink. Furthermore, it exceeds the results of multiple established\nembedding-based algorithms on FB15K-237 and narrows the gap between rule-based\nand embedding-based algorithms on OpenBioLink. We also show that SAFRAN\nincreases inference speeds by up to two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 15:36:47 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Ott", "Simon", ""], ["Graf", "Laura", ""], ["Agibetov", "Asan", ""], ["Meilicke", "Christian", ""], ["Samwald", "Matthias", ""]]}, {"id": "2012.05818", "submitter": "Qingqing Cao", "authors": "Qingqing Cao, Oriana Riva, Aruna Balasubramanian, Niranjan\n  Balasubramanian", "title": "Bew: Towards Answering Business-Entity-Related Web Questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present BewQA, a system specifically designed to answer a class of\nquestions that we call Bew questions. Bew questions are related to\nbusinesses/services such as restaurants, hotels, and movie theaters; for\nexample, \"Until what time is happy hour?\". These questions are challenging to\nanswer because the answers are found in open-domain Web, are present in short\nsentences without surrounding context, and are dynamic since the webpage\ninformation can be updated frequently. Under these conditions, existing QA\nsystems perform poorly. We present a practical approach, called BewQA, that can\nanswer Bew queries by mining a template of the business-related webpages and\nusing the template to guide the search. We show how we can extract the template\nautomatically by leveraging aggregator websites that aggregate information\nabout business entities in a domain (e.g., restaurants). We answer a given\nquestion by identifying the section from the extracted template that is most\nlikely to contain the answer. By doing so we can extract the answers even when\nthe answer span does not have sufficient context. Importantly, BewQA does not\nrequire any training. We crowdsource a new dataset of 1066 Bew questions and\nground-truth answers in the restaurant domain. Compared to state-of-the-art QA\nmodels, BewQA has a 27 percent point improvement in F1 score. Compared to a\ncommercial search engine, BewQA answered correctly 29% more Bew questions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 16:46:55 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Cao", "Qingqing", ""], ["Riva", "Oriana", ""], ["Balasubramanian", "Aruna", ""], ["Balasubramanian", "Niranjan", ""]]}, {"id": "2012.05852", "submitter": "Valerio Lorini", "authors": "Valerio Lorini, Carlos Castillo, Domenico Nappo, Francesco Dottori,\n  Peter Salamon", "title": "Social Media Alerts can Improve, but not Replace Hydrological Models for\n  Forecasting Floods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Social media can be used for disaster risk reduction as a complement to\ntraditional information sources, and the literature has suggested numerous ways\nto achieve this. In the case of floods, for instance, data collection from\nsocial media can be triggered by a severe weather forecast and/or a flood\nprediction. By way of contrast, in this paper we explore the possibility of\nhaving an entirely independent flood monitoring system which is based\ncompletely on social media, and which is completely self-activated. This\nindependence and self-activation would bring increased robustness, as the\nsystem would not depend on other mechanisms for forecasting. We observe that\nsocial media can indeed help in the early detection of some flood events that\nwould otherwise not be detected until later, albeit at the cost of many false\npositives. Overall, our experiments suggest that social media signals should\nonly be used to complement existing monitoring systems, and we provide various\nexplanations to support this argument.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 17:56:29 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Lorini", "Valerio", ""], ["Castillo", "Carlos", ""], ["Nappo", "Domenico", ""], ["Dottori", "Francesco", ""], ["Salamon", "Peter", ""]]}, {"id": "2012.05982", "submitter": "John Leung", "authors": "John Kalung Leung and Igor Griva and William G. Kennedy", "title": "Making Cross-Domain Recommendations by Associating Disjoint Users and\n  Items Through the Affective Aware Pseudo Association Method", "comments": "17 pages, 8 tables, 4 figures and paper has been accepted by the 2nd\n  International Conference on Natural Language Processing, Information\n  Retrieval and AI (NIAI 2021) to be held on January 23~24, 2021 in Zurich,\n  Switzerland", "journal-ref": "David C. Wyld et al. (Eds): AIAP, SIGML, CNSA, NIAI - 2021 pp.\n  113-129, 2021. CS & IT - CSCP 2021", "doi": "10.5121/csit.2021.110108", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper utilizes an ingenious text-based affective aware pseudo\nassociation method (AAPAM) to link disjoint users and items across different\ninformation domains and leverage them to make cross-domain content-based and\ncollaborative filtering recommendations. This paper demonstrates that the AAPAM\nmethod could seamlessly join different information domain datasets to act as\none without any additional cross-domain information retrieval protocols.\nBesides making cross-domain recommendations, the benefit of joining datasets\nfrom different information domains through AAPAM is that it eradicates cold\nstart issues while making serendipitous recommendations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 21:16:13 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 19:08:23 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 18:31:48 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Leung", "John Kalung", ""], ["Griva", "Igor", ""], ["Kennedy", "William G.", ""]]}, {"id": "2012.06025", "submitter": "Yasas Senarath", "authors": "Yasas Senarath, Uthayasanker Thayasivam", "title": "Exploring Deep Neural Networks and Transfer Learning for Analyzing\n  Emotions in Tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present an experiment on using deep learning and transfer\nlearning techniques for emotion analysis in tweets and suggest a method to\ninterpret our deep learning models. The proposed approach for emotion analysis\ncombines a Long Short Term Memory (LSTM) network with a Convolutional Neural\nNetwork (CNN). Then we extend this approach for emotion intensity prediction\nusing transfer learning technique. Furthermore, we propose a technique to\nvisualize the importance of each word in a tweet to get a better understanding\nof the model. Experimentally, we show in our analysis that the proposed models\noutperform the state-of-the-art in emotion classification while maintaining\ncompetitive results in predicting emotion intensity.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 23:45:53 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Senarath", "Yasas", ""], ["Thayasivam", "Uthayasanker", ""]]}, {"id": "2012.06146", "submitter": "Jie Gu", "authors": "Jie Gu, Feng Wang, Qinghui Sun, Zhiquan Ye, Xiaoxiao Xu, Jingmin Chen,\n  Jun Zhang", "title": "Exploiting Behavioral Consistence for Universal User Representation", "comments": "Preprint of accepted AAAI2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User modeling is critical for developing personalized services in industry. A\ncommon way for user modeling is to learn user representations that can be\ndistinguished by their interests or preferences. In this work, we focus on\ndeveloping universal user representation model. The obtained universal\nrepresentations are expected to contain rich information, and be applicable to\nvarious downstream applications without further modifications (e.g., user\npreference prediction and user profiling). Accordingly, we can be free from the\nheavy work of training task-specific models for every downstream task as in\nprevious works. In specific, we propose Self-supervised User Modeling Network\n(SUMN) to encode behavior data into the universal representation. It includes\ntwo key components. The first one is a new learning objective, which guides the\nmodel to fully identify and preserve valuable user information under a\nself-supervised learning framework. The other one is a multi-hop aggregation\nlayer, which benefits the model capacity in aggregating diverse behaviors.\nExtensive experiments on benchmark datasets show that our approach can\noutperform state-of-the-art unsupervised representation methods, and even\ncompete with supervised ones.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 06:10:14 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Gu", "Jie", ""], ["Wang", "Feng", ""], ["Sun", "Qinghui", ""], ["Ye", "Zhiquan", ""], ["Xu", "Xiaoxiao", ""], ["Chen", "Jingmin", ""], ["Zhang", "Jun", ""]]}, {"id": "2012.06186", "submitter": "Shervin Rasoulzadeh", "authors": "Shervin Rasoulzadeh, Bagher Babaali", "title": "Writer Identification and Writer Retrieval Based on NetVLAD with\n  Re-ranking", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": "10.1049/bme2.12039", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses writer identification and writer retrieval which is\nconsidered as a challenging problem in the document analysis and recognition\nfield. In this work, a novel pipeline is proposed for the problem at hand by\nemploying a unified neural network architecture consisting of the ResNet-20 as\na feature extractor and an integrated NetVLAD layer, inspired by the vector of\nlocally aggregated descriptors (VLAD), in the head of the latter part. Having\ndefined this architecture, the triplet semi-hard loss function is used to\ndirectly learn an embedding for individual input image patches. Subsequently,\ngeneralized max-pooling technique is employed for the aggregation of embedded\ndescriptors of each handwritten image. Also, a novel re-ranking strategy is\nintroduced for the task of identification and retrieval based on $k$-reciprocal\nnearest neighbors, and it is shown that the pipeline can benefit tremendously\nfrom this step. Experimental evaluation has been done on the three publicly\navailable datasets: the ICDAR 2013, CVL, and KHATT datasets. Results indicate\nthat while we perform comparably to the state-of-the-art on the KHATT, our\nwriter identification and writer retrieval pipeline achieves superior\nperformance on the ICDAR 2013 and CVL datasets in terms of mAP.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 08:22:28 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 17:45:14 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 18:27:50 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Rasoulzadeh", "Shervin", ""], ["Babaali", "Bagher", ""]]}, {"id": "2012.06200", "submitter": "Federico Becattini", "authors": "Lavinia De Divitiis, Federico Becattini, Claudio Baecchi, Alberto Del\n  Bimbo", "title": "Garment Recommendation with Memory Augmented Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fashion plays a pivotal role in society. Combining garments appropriately is\nessential for people to communicate their personality and style. Also different\nevents require outfits to be thoroughly chosen to comply with underlying social\nclothing rules. Therefore, combining garments appropriately might not be\ntrivial. The fashion industry has turned this into a massive source of income,\nrelying on complex recommendation systems to retrieve and suggest appropriate\nclothing items for customers. To perform better recommendations, personalized\nsuggestions can be performed, taking into account user preferences or purchase\nhistories. In this paper, we propose a garment recommendation system to pair\ndifferent clothing items, namely tops and bottoms, exploiting a Memory\nAugmented Neural Network (MANN). By training a memory writing controller, we\nare able to store a non-redundant subset of samples, which is then used to\nretrieve a ranked list of suitable bottoms to complement a given top. In\nparticular, we aim at retrieving a variety of modalities in which a certain\ngarment can be combined. To refine our recommendations, we then include user\npreferences via Matrix Factorization. We experiment on IQON3000, a dataset\ncollected from an online fashion community, reporting state of the art results.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 09:13:14 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["De Divitiis", "Lavinia", ""], ["Becattini", "Federico", ""], ["Baecchi", "Claudio", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2012.06209", "submitter": "Lynnette Hui Xian Ng", "authors": "Chua Hao Yang, Yong Shan Jie, Boon Kok Chin, Lander Chin, Lynnette Hui\n  Xian Ng", "title": "KOSMOS: Knowledge-graph Oriented Social media and Mainstream media\n  Overview System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce KOSMOS, a knowledge retrieval system based on the constructed\nknowledge graph of social media and mainstream media documents. The system\nfirst identifies key events from the documents at each time frame through\nclustering, extracting a document to represent each cluster, then describing\nthe document in terms of 5W1H (Who, What, When, Where, Why, How). The event\ncentric knowledge graph is enhanced by relation triplets and entity\ndisambiguation from the representative document. This knowledge retrieval is\nsupported by a web interface that presents a graph visualisation of related\nnodes and relevant articles based on a user query. The interface facilitates\nunderstanding relationships between events reported in mainstream and social\nmedia journalism through the KOSMOS information extraction pipeline, which is\nvaluable to understand media slant and public opinions. Finally, we explore a\nuse case in extracting events and relations from documents to understand the\nmedia and community's view to the 2020 COVID19 pandemic.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 09:36:06 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 13:48:17 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Yang", "Chua Hao", ""], ["Jie", "Yong Shan", ""], ["Chin", "Boon Kok", ""], ["Chin", "Lander", ""], ["Ng", "Lynnette Hui Xian", ""]]}, {"id": "2012.06238", "submitter": "Georgios Balikas", "authors": "Francisco Borges, Georgios Balikas, Marc Brette, Guillaume Kempf,\n  Arvind Srikantan, Matthieu Landos, Darya Brazouskaya, Qianqian Shi", "title": "Query Understanding for Natural Language Enterprise Search", "comments": "accepted at DeepNLP @ SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural Language Search (NLS) extends the capabilities of search engines that\nperform keyword search allowing users to issue queries in a more \"natural\"\nlanguage. The engine tries to understand the meaning of the queries and to map\nthe query words to the symbols it supports like Persons, Organizations, Time\nExpressions etc.. It, then, retrieves the information that satisfies the user's\nneed in different forms like an answer, a record or a list of records. We\npresent an NLS system we implemented as part of the Search service of a major\nCRM platform. The system is currently in production serving thousands of\ncustomers. Our user studies showed that creating dynamic reports with NLS saved\nmore than 50% of our user's time compared to achieving the same result with\nnavigational search. We describe the architecture of the system, the\nparticularities of the CRM domain as well as how they have influenced our\ndesign decisions. Among several submodules of the system we detail the role of\na Deep Learning Named Entity Recognizer. The paper concludes with discussion\nover the lessons learned while developing this product.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 10:57:25 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Borges", "Francisco", ""], ["Balikas", "Georgios", ""], ["Brette", "Marc", ""], ["Kempf", "Guillaume", ""], ["Srikantan", "Arvind", ""], ["Landos", "Matthieu", ""], ["Brazouskaya", "Darya", ""], ["Shi", "Qianqian", ""]]}, {"id": "2012.06274", "submitter": "Damir Koren\\v{c}i\\'c", "authors": "Damir Koren\\v{c}i\\'c (1), Strahil Ristov (1), Jelena Repar (1), Jan\n  \\v{S}najder (2) ((1) Rudjer Bo\\v{s}kovi\\'c Institute, Croatia, (2) University\n  of Zagreb, Faculty of Electrical Engineering and Computing, Croatia)", "title": "A Topic Coverage Approach to Evaluation of Topic Models", "comments": "Results and contributions unchanged; Added new references; Improved\n  the contextualization and the description of the work (abstr, intro, 7.1\n  concl, rw, concl); Moved technical details of data and model building to\n  appendices; Improved layout;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Topic models are widely used unsupervised models of text capable of learning\ntopics - weighted lists of words and documents - from large collections of text\ndocuments. When topic models are used for discovery of topics in text\ncollections, a question that arises naturally is how well the model-induced\ntopics correspond to topics of interest to the analyst. In this paper we\nrevisit and extend a so far neglected approach to topic model evaluation based\non measuring topic coverage - computationally matching model topics with a set\nof reference topics that models are expected to uncover. The approach is well\nsuited for analyzing models' performance in topic discovery and for large-scale\nanalysis of both topic models and measures of model quality. We propose new\nmeasures of coverage and evaluate, in a series of experiments, different types\nof topic models on two distinct text domains for which interest for topic\ndiscovery exists. The experiments include evaluation of model quality, analysis\nof coverage of distinct topic categories, and the analysis of the relationship\nbetween coverage and other methods of topic model evaluation. The contributions\nof the paper include new measures of coverage, insights into both topic models\nand other methods of model evaluation, and the datasets and code for\nfacilitating future research of both topic coverage and other approaches to\ntopic model evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 12:08:27 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 16:27:00 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Koren\u010di\u0107", "Damir", ""], ["Ristov", "Strahil", ""], ["Repar", "Jelena", ""], ["\u0160najder", "Jan", ""]]}, {"id": "2012.06366", "submitter": "Matus Medo", "authors": "Yuhao Zhou, Ruijie Wang, Yi-Cheng Zhang, An Zeng, Mat\\'u\\v{s} Medo", "title": "Limits of PageRank-based ranking methods in sports data", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While PageRank has been extensively used to rank sport tournament\nparticipants (teams or individuals), its superiority over simpler ranking\nmethods has been never clearly demonstrated. We use sports results from 18\nmajor leagues to calibrate a state-of-art model for synthetic sports results.\nModel data are then used to assess the ranking performance of PageRank in a\ncontrolled setting. We find that PageRank outperforms the benchmark ranking by\nthe number of wins only when a small fraction of all games have been played.\nIncreased randomness in the data, such as intrinsic randomness of outcomes or\nadvantage of home teams, further reduces the range of PageRank's superiority.\nWe propose a new PageRank variant which outperforms PageRank in all evaluated\nsettings, yet shares its sensitivity to increased randomness in the data. Our\nmain findings are confirmed by evaluating the ranking algorithms on real data.\nOur work demonstrates the danger of using novel metrics and algorithms without\nconsidering their limits of applicability.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 14:16:17 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Zhou", "Yuhao", ""], ["Wang", "Ruijie", ""], ["Zhang", "Yi-Cheng", ""], ["Zeng", "An", ""], ["Medo", "Mat\u00fa\u0161", ""]]}, {"id": "2012.06416", "submitter": "Wenjie Wang", "authors": "Wenjie Wang, Ling-yu Duan, Hao Jiang, Peiguang Jing, Xuemeng Song,\n  Liqiang Nie", "title": "Market2Dish: Health-aware Food Recommendation", "comments": null, "journal-ref": "ACM Transactions on Multimedia Computing, Communications, and\n  Applications, April 2021", "doi": "10.1145/3418211", "report-no": "Article No.: 33", "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rising incidence of some diseases, such as obesity and diabetes, a\nhealthy diet is arousing increasing attention. However, most existing\nfood-related research efforts focus on recipe retrieval, user preference-based\nfood recommendation, cooking assistance, or the nutrition and calorie\nestimation of dishes, ignoring the personalized health-aware food\nrecommendation. Therefore, in this work, we present a personalized health-aware\nfood recommendation scheme, namely Market2Dish, mapping the ingredients\ndisplayed in the market to the healthy dishes eaten at home. The proposed\nscheme comprises three components, namely recipe retrieval, user-health\nprofiling, and health-aware food recommendation. In particular, recipe\nretrieval aims to acquire the ingredients available to the users, and then\nretrieve recipe candidates from a large-scale recipe dataset. User health\nprofiling is to characterize the health conditions of users by capturing the\ntextual health-related information crawled from social networks. Specifically,\nto solve the issue that the health-related information is extremely sparse, we\nincorporate a word-class interaction mechanism into the proposed deep model to\nlearn the fine-grained correlations between the textual tweets and pre-defined\nhealth concepts. For the health-aware food recommendation, we present a novel\ncategory-aware hierarchical memory network-based recommender to learn the\nhealth-aware user-recipe interactions for better food recommendation. Moreover,\nextensive experiments demonstrate the effectiveness of the health-aware food\nrecommendation scheme.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 15:19:19 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Wenjie", ""], ["Duan", "Ling-yu", ""], ["Jiang", "Hao", ""], ["Jing", "Peiguang", ""], ["Song", "Xuemeng", ""], ["Nie", "Liqiang", ""]]}, {"id": "2012.06547", "submitter": "Akshay Gadi Patil", "authors": "Akshay Gadi Patil, Manyi Li, Matthew Fisher, Manolis Savva, Hao Zhang", "title": "LayoutGMN: Neural Graph Matching for Structural Layout Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep neural network to predict structural similarity between 2D\nlayouts by leveraging Graph Matching Networks (GMN). Our network, coined\nLayoutGMN, learns the layout metric via neural graph matching, using an\nattention-based GMN designed under a triplet network setting. To train our\nnetwork, we utilize weak labels obtained by pixel-wise Intersection-over-Union\n(IoUs) to define the triplet loss. Importantly, LayoutGMN is built with a\nstructural bias which can effectively compensate for the lack of structure\nawareness in IoUs. We demonstrate this on two prominent forms of layouts, viz.,\nfloorplans and UI designs, via retrieval experiments on large-scale datasets.\nIn particular, retrieval results by our network better match human judgement of\nstructural layout similarity compared to both IoUs and other baselines\nincluding a state-of-the-art method based on graph neural networks and image\nconvolution. In addition, LayoutGMN is the first deep model to offer both\nmetric learning of structural layout similarity and structural matching between\nlayout elements.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:24:18 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 02:58:30 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Patil", "Akshay Gadi", ""], ["Li", "Manyi", ""], ["Fisher", "Matthew", ""], ["Savva", "Manolis", ""], ["Zhang", "Hao", ""]]}, {"id": "2012.06576", "submitter": "Harrie Oosterhuis", "authors": "Harrie Oosterhuis", "title": "Learning from User Interactions with Rankings: A Unification of the\n  Field", "comments": "PhD Thesis of Harrie Oosterhuis defended at the University of\n  Amsterdam on November 27th 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking systems form the basis for online search engines and recommendation\nservices. They process large collections of items, for instance web pages or\ne-commerce products, and present the user with a small ordered selection. The\ngoal of a ranking system is to help a user find the items they are looking for\nwith the least amount of effort. Thus the rankings they produce should place\nthe most relevant or preferred items at the top of the ranking. Learning to\nrank is a field within machine learning that covers methods which optimize\nranking systems w.r.t. this goal. Traditional supervised learning to rank\nmethods utilize expert-judgements to evaluate and learn, however, in many\nsituations such judgements are impossible or infeasible to obtain. As a\nsolution, methods have been introduced that perform learning to rank based on\nuser clicks instead. The difficulty with clicks is that they are not only\naffected by user preferences, but also by what rankings were displayed.\nTherefore, these methods have to prevent being biased by other factors than\nuser preference. This thesis concerns learning to rank methods based on user\nclicks and specifically aims to unify the different families of these methods.\n  As a whole, the second part of this thesis proposes a framework that bridges\nmany gaps between areas of online, counterfactual, and supervised learning to\nrank. It has taken approaches, previously considered independent, and unified\nthem into a single methodology for widely applicable and effective learning to\nrank from user clicks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 14:47:59 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Oosterhuis", "Harrie", ""]]}, {"id": "2012.06683", "submitter": "Vikram Nathan", "authors": "Vikram Nathan, Jialin Ding, Tim Kraska, Mohammad Alizadeh", "title": "Cortex: Harnessing Correlations to Boost Query Performance", "comments": "13 pages, including references. Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Databases employ indexes to filter out irrelevant records, which reduces scan\noverhead and speeds up query execution. However, this optimization is only\navailable to queries that filter on the indexed attribute. To extend these\nspeedups to queries on other attributes, database systems have turned to\nsecondary and multi-dimensional indexes. Unfortunately, these approaches are\nrestrictive: secondary indexes have a large memory footprint and can only speed\nup queries that access a small number of records, and multi-dimensional indexes\ncannot scale to more than a handful of columns. We present Cortex, an approach\nthat takes advantage of correlations to extend the reach of primary indexes to\nmore attributes. Unlike prior work, Cortex can adapt itself to any existing\nprimary index, whether single or multi-dimensional, to harness a broad variety\nof correlations, such as those that exist between more than two attributes or\nhave a large number of outliers. We demonstrate that on real datasets\nexhibiting these diverse types of correlations, Cortex matches or outperforms\ntraditional secondary indexes with $5\\times$ less space, and it is $2-8\\times$\nfaster than existing approaches to indexing correlations.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 00:22:51 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Nathan", "Vikram", ""], ["Ding", "Jialin", ""], ["Kraska", "Tim", ""], ["Alizadeh", "Mohammad", ""]]}, {"id": "2012.06690", "submitter": "Zefang Liu", "authors": "Zefang Liu", "title": "Yelp Review Rating Prediction: Machine Learning and Deep Learning Models", "comments": "8 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We predict restaurant ratings from Yelp reviews based on Yelp Open Dataset.\nData distribution is presented, and one balanced training dataset is built. Two\nvectorizers are experimented for feature engineering. Four machine learning\nmodels including Naive Bayes, Logistic Regression, Random Forest, and Linear\nSupport Vector Machine are implemented. Four transformer-based models\ncontaining BERT, DistilBERT, RoBERTa, and XLNet are also applied. Accuracy,\nweighted F1 score, and confusion matrix are used for model evaluation. XLNet\nachieves 70% accuracy for 5-star classification compared with Logistic\nRegression with 64% accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 01:07:48 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Liu", "Zefang", ""]]}, {"id": "2012.06731", "submitter": "Robin Swezey", "authors": "Robin Swezey, Aditya Grover, Bruno Charron, Stefano Ermon", "title": "PiRank: Learning To Rank via Differentiable Sorting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge with machine learning approaches for ranking is the gap\nbetween the performance metrics of interest and the surrogate loss functions\nthat can be optimized with gradient-based methods. This gap arises because\nranking metrics typically involve a sorting operation which is not\ndifferentiable w.r.t. the model parameters. Prior works have proposed\nsurrogates that are loosely related to ranking metrics or simple smoothed\nversions thereof. We propose PiRank, a new class of differentiable surrogates\nfor ranking, which employ a continuous, temperature-controlled relaxation to\nthe sorting operator. We show that PiRank exactly recovers the desired metrics\nin the limit of zero temperature and scales favorably with the problem size,\nboth in theory and practice. Empirically, we demonstrate that PiRank\nsignificantly improves over existing approaches on publicly available\ninternet-scale learning-to-rank benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 05:07:36 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Swezey", "Robin", ""], ["Grover", "Aditya", ""], ["Charron", "Bruno", ""], ["Ermon", "Stefano", ""]]}, {"id": "2012.06852", "submitter": "Junliang Yu", "authors": "Xin Xia, Hongzhi Yin, Junliang Yu, Qinyong Wang, Lizhen Cui,\n  Xiangliang Zhang", "title": "Self-Supervised Hypergraph Convolutional Networks for Session-based\n  Recommendation", "comments": "9 pages, 4 figures, accepted by AAAI'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Session-based recommendation (SBR) focuses on next-item prediction at a\ncertain time point. As user profiles are generally not available in this\nscenario, capturing the user intent lying in the item transitions plays a\npivotal role. Recent graph neural networks (GNNs) based SBR methods regard the\nitem transitions as pairwise relations, which neglect the complex high-order\ninformation among items. Hypergraph provides a natural way to capture\nbeyond-pairwise relations, while its potential for SBR has remained unexplored.\nIn this paper, we fill this gap by modeling session-based data as a hypergraph\nand then propose a hypergraph convolutional network to improve SBR. Moreover,\nto enhance hypergraph modeling, we devise another graph convolutional network\nwhich is based on the line graph of the hypergraph and then integrate\nself-supervised learning into the training of the networks by maximizing mutual\ninformation between the session representations learned via the two networks,\nserving as an auxiliary task to improve the recommendation task. Since the two\ntypes of networks both are based on hypergraph, which can be seen as two\nchannels for hypergraph modeling, we name our model \\textbf{DHCN} (Dual Channel\nHypergraph Convolutional Networks). Extensive experiments on three benchmark\ndatasets demonstrate the superiority of our model over the SOTA methods, and\nthe results validate the effectiveness of hypergraph modeling and\nself-supervised task. The implementation of our model is available at\nhttps://github.com/xiaxin1998/DHCN\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 16:19:49 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 03:33:02 GMT"}, {"version": "v3", "created": "Wed, 3 Mar 2021 04:09:09 GMT"}, {"version": "v4", "created": "Mon, 15 Mar 2021 07:45:25 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Xia", "Xin", ""], ["Yin", "Hongzhi", ""], ["Yu", "Junliang", ""], ["Wang", "Qinyong", ""], ["Cui", "Lizhen", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "2012.06901", "submitter": "Yao Zhou", "authors": "Yao Zhou, Jianpeng Xu, Jun Wu, Zeinab Taghavi Nasrabadi, Evren\n  Korpeoglu, Kannan Achan, Jingrui He", "title": "GAN-based Recommendation with Positive-Unlabeled Sampling", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are popular tools for information retrieval tasks on a\nlarge variety of web applications and personalized products. In this work, we\npropose a Generative Adversarial Network based recommendation framework using a\npositive-unlabeled sampling strategy. Specifically, we utilize the generator to\nlearn the continuous distribution of user-item tuples and design the\ndiscriminator to be a binary classifier that outputs the relevance score\nbetween each user and each item. Meanwhile, positive-unlabeled sampling is\napplied in the learning procedure of the discriminator. Theoretical bounds\nregarding positive-unlabeled sampling and optimalities of convergence for the\ndiscriminators and the generators are provided. We show the effectiveness and\nefficiency of our framework on three publicly accessible data sets with eight\nranking-based evaluation metrics in comparison with thirteen popular baselines.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 20:08:51 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhou", "Yao", ""], ["Xu", "Jianpeng", ""], ["Wu", "Jun", ""], ["Nasrabadi", "Zeinab Taghavi", ""], ["Korpeoglu", "Evren", ""], ["Achan", "Kannan", ""], ["He", "Jingrui", ""]]}, {"id": "2012.06910", "submitter": "Massih-Reza Amini", "authors": "Aleksandra Burashnikova, Marianne Clausel, Charlotte Laclau, Frack\n  Iutzeller, Yury Maximov, Massih-Reza Amini", "title": "Learning over no-Preferred and Preferred Sequence of items for Robust\n  Recommendation", "comments": "21 pages, 9 figures. arXiv admin note: substantial text overlap with\n  arXiv:1902.08495", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a theoretically founded sequential strategy for\ntraining large-scale Recommender Systems (RS) over implicit feedback, mainly in\nthe form of clicks. The proposed approach consists in minimizing pairwise\nranking loss over blocks of consecutive items constituted by a sequence of\nnon-clicked items followed by a clicked one for each user. We present two\nvariants of this strategy where model parameters are updated using either the\nmomentum method or a gradient-based approach. To prevent from updating the\nparameters for an abnormally high number of clicks over some targeted items\n(mainly due to bots), we introduce an upper and a lower threshold on the number\nof updates for each user. These thresholds are estimated over the distribution\nof the number of blocks in the training set. The thresholds affect the decision\nof RS and imply a shift over the distribution of items that are shown to the\nusers. Furthermore, we provide a convergence analysis of both algorithms and\ndemonstrate their practical efficiency over six large-scale collections, both\nregarding different ranking measures and computational time.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 22:10:15 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Burashnikova", "Aleksandra", ""], ["Clausel", "Marianne", ""], ["Laclau", "Charlotte", ""], ["Iutzeller", "Frack", ""], ["Maximov", "Yury", ""], ["Amini", "Massih-Reza", ""]]}, {"id": "2012.06923", "submitter": "Marko Kabi\\'c", "authors": "Marko Kabi\\'c, Gabriel Duque L\\'opez, Daniel Keller", "title": "A Refined SVD Algorithm for Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering tries to predict the ratings of a user over some\nitems based on opinions of other users with similar taste. The ratings are\nusually given in the form of a sparse matrix, the goal being to find the\nmissing entries (i.e. ratings). Various approaches to collaborative filtering\nexist, some of the most popular ones being the Singular Value Decomposition\n(SVD) and K-means clustering. One of the challenges in the SVD approach is\nfinding a good initialization of the unknown ratings. A possible initialization\nis suggested by [1]. In this paper we explain how K-means approach can be used\nto achieve the further refinement of this initialization for SVD. We show that\nour technique outperforms both initialization techniques used separately.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 00:04:11 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kabi\u0107", "Marko", ""], ["L\u00f3pez", "Gabriel Duque", ""], ["Keller", "Daniel", ""]]}, {"id": "2012.06968", "submitter": "Kai Zhang", "authors": "Kai Zhang, Hao Qian, Qing Cui, Qi Liu, Longfei Li, Jun Zhou, Jianhui\n  Ma, Enhong Chen", "title": "Multi-Interactive Attention Network for Fine-grained Feature Learning in\n  CTR Prediction", "comments": "9 pages, 6 figures, WSDM2021, accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the Click-Through Rate (CTR) prediction scenario, user's sequential\nbehaviors are well utilized to capture the user interest in the recent\nliterature. However, despite being extensively studied, these sequential\nmethods still suffer from three limitations. First, existing methods mostly\nutilize attention on the behavior of users, which is not always suitable for\nCTR prediction, because users often click on new products that are irrelevant\nto any historical behaviors. Second, in the real scenario, there exist numerous\nusers that have operations a long time ago, but turn relatively inactive in\nrecent times. Thus, it is hard to precisely capture user's current preferences\nthrough early behaviors. Third, multiple representations of user's historical\nbehaviors in different feature subspaces are largely ignored. To remedy these\nissues, we propose a Multi-Interactive Attention Network (MIAN) to\ncomprehensively extract the latent relationship among all kinds of fine-grained\nfeatures (e.g., gender, age and occupation in user-profile). Specifically, MIAN\ncontains a Multi-Interactive Layer (MIL) that integrates three local\ninteraction modules to capture multiple representations of user preference\nthrough sequential behaviors and simultaneously utilize the fine-grained\nuser-specific as well as context information. In addition, we design a Global\nInteraction Module (GIM) to learn the high-order interactions and balance the\ndifferent impacts of multiple features. Finally, Offline experiment results\nfrom three datasets, together with an Online A/B test in a large-scale\nrecommendation system, demonstrate the effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 05:46:19 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 13:29:34 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhang", "Kai", ""], ["Qian", "Hao", ""], ["Cui", "Qing", ""], ["Liu", "Qi", ""], ["Li", "Longfei", ""], ["Zhou", "Jun", ""], ["Ma", "Jianhui", ""], ["Chen", "Enhong", ""]]}, {"id": "2012.07064", "submitter": "Bowen Hao", "authors": "Bowen Hao, Jing Zhang, Hongzhi Yin, Cuiping Li and Hong Chen", "title": "Pre-Training Graph Neural Networks for Cold-Start Users and Items\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cold-start problem is a fundamental challenge for recommendation tasks.\nDespite the recent advances on Graph Neural Networks (GNNs) incorporate the\nhigh-order collaborative signal to alleviate the problem, the embeddings of the\ncold-start users and items aren't explicitly optimized, and the cold-start\nneighbors are not dealt with during the graph convolution in GNNs. This paper\nproposes to pre-train a GNN model before applying it for recommendation. Unlike\nthe goal of recommendation, the pre-training GNN simulates the cold-start\nscenarios from the users/items with sufficient interactions and takes the\nembedding reconstruction as the pretext task, such that it can directly improve\nthe embedding quality and can be easily adapted to the new cold-start\nusers/items. To further reduce the impact from the cold-start neighbors, we\nincorporate a self-attention-based meta aggregator to enhance the aggregation\nability of each graph convolution step, and an adaptive neighbor sampler to\nselect the effective neighbors according to the feedbacks from the pre-training\nGNN model. Experiments on three public recommendation datasets show the\nsuperiority of our pre-training GNN model against the original GNN models on\nuser/item embedding inference and the recommendation task.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 13:57:12 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Hao", "Bowen", ""], ["Zhang", "Jing", ""], ["Yin", "Hongzhi", ""], ["Li", "Cuiping", ""], ["Chen", "Hong", ""]]}, {"id": "2012.07149", "submitter": "Daniel Poh", "authors": "Daniel Poh, Bryan Lim, Stefan Zohren and Stephen Roberts", "title": "Building Cross-Sectional Systematic Strategies By Learning to Rank", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.IR cs.LG q-fin.PM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The success of a cross-sectional systematic strategy depends critically on\naccurately ranking assets prior to portfolio construction. Contemporary\ntechniques perform this ranking step either with simple heuristics or by\nsorting outputs from standard regression or classification models, which have\nbeen demonstrated to be sub-optimal for ranking in other domains (e.g.\ninformation retrieval). To address this deficiency, we propose a framework to\nenhance cross-sectional portfolios by incorporating learning-to-rank\nalgorithms, which lead to improvements of ranking accuracy by learning pairwise\nand listwise structures across instruments. Using cross-sectional momentum as a\ndemonstrative case study, we show that the use of modern machine learning\nranking algorithms can substantially improve the trading performance of\ncross-sectional strategies -- providing approximately threefold boosting of\nSharpe Ratios compared to traditional approaches.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 20:52:35 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Poh", "Daniel", ""], ["Lim", "Bryan", ""], ["Zohren", "Stefan", ""], ["Roberts", "Stephen", ""]]}, {"id": "2012.07436", "submitter": "Haoyi Zhou", "authors": "Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui\n  Xiong, Wancai Zhang", "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series\n  Forecasting", "comments": "8 pages (main), 5 pages (appendix) and to be appeared in AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications require the prediction of long sequence\ntime-series, such as electricity consumption planning. Long sequence\ntime-series forecasting (LSTF) demands a high prediction capacity of the model,\nwhich is the ability to capture precise long-range dependency coupling between\noutput and input efficiently. Recent studies have shown the potential of\nTransformer to increase the prediction capacity. However, there are several\nsevere issues with Transformer that prevent it from being directly applicable\nto LSTF, including quadratic time complexity, high memory usage, and inherent\nlimitation of the encoder-decoder architecture. To address these issues, we\ndesign an efficient transformer-based model for LSTF, named Informer, with\nthree distinctive characteristics: (i) a $ProbSparse$ self-attention mechanism,\nwhich achieves $O(L \\log L)$ in time complexity and memory usage, and has\ncomparable performance on sequences' dependency alignment. (ii) the\nself-attention distilling highlights dominating attention by halving cascading\nlayer input, and efficiently handles extreme long input sequences. (iii) the\ngenerative style decoder, while conceptually simple, predicts the long\ntime-series sequences at one forward operation rather than a step-by-step way,\nwhich drastically improves the inference speed of long-sequence predictions.\nExtensive experiments on four large-scale datasets demonstrate that Informer\nsignificantly outperforms existing methods and provides a new solution to the\nLSTF problem.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 11:43:09 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 03:05:27 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 14:45:04 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhou", "Haoyi", ""], ["Zhang", "Shanghang", ""], ["Peng", "Jieqi", ""], ["Zhang", "Shuai", ""], ["Li", "Jianxin", ""], ["Xiong", "Hui", ""], ["Zhang", "Wancai", ""]]}, {"id": "2012.07565", "submitter": "Le Bao", "authors": "Xiaoxiao Li, Rabah Al-Zaidy, Amy Zhang, Stefan Baral, Le Bao, C. Lee\n  Giles", "title": "Automating Document Classification with Distant Supervision to Increase\n  the Efficiency of Systematic Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Systematic reviews of scholarly documents often provide complete\nand exhaustive summaries of literature relevant to a research question.\nHowever, well-done systematic reviews are expensive, time-demanding, and\nlabor-intensive. Here, we propose an automatic document classification approach\nto significantly reduce the effort in reviewing documents. Methods: We first\ndescribe a manual document classification procedure that is used to curate a\npertinent training dataset and then propose three classifiers: a keyword-guided\nmethod, a cluster analysis-based refined method, and a random forest approach\nthat utilizes a large set of feature tokens. As an example, this approach is\nused to identify documents studying female sex workers that are assumed to\ncontain content relevant to either HIV or violence. We compare the performance\nof the three classifiers by cross-validation and conduct a sensitivity analysis\non the portion of data utilized in training the model. Results: The random\nforest approach provides the highest area under the curve (AUC) for both\nreceiver operating characteristic (ROC) and precision/recall (PR). Analyses of\nprecision and recall suggest that random forest could facilitate manually\nreviewing 20\\% of the articles while containing 80\\% of the relevant cases.\nFinally, we found a good classifier could be obtained by using a relatively\nsmall training sample size. Conclusions: In sum, the automated procedure of\ndocument classification presented here could improve both the precision and\nefficiency of systematic reviews, as well as facilitating live reviews, where\nreviews are updated regularly.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 22:45:40 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Li", "Xiaoxiao", ""], ["Al-Zaidy", "Rabah", ""], ["Zhang", "Amy", ""], ["Baral", "Stefan", ""], ["Bao", "Le", ""], ["Giles", "C. Lee", ""]]}, {"id": "2012.07589", "submitter": "Ananda Das", "authors": "Ananda Das, Partha Pratim Das", "title": "Incorporating Domain Knowledge To Improve Topic Segmentation Of Long\n  MOOC Lecture Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Topical Segmentation poses a great role in reducing search space of the\ntopics taught in a lecture video specially when the video metadata lacks topic\nwise segmentation information. This segmentation information eases user efforts\nof searching, locating and browsing a topic inside a lecture video. In this\nwork we propose an algorithm, that combines state-of-the art language model and\ndomain knowledge graph for automatically detecting different coherent topics\npresent inside a long lecture video. We use the language model on\nspeech-to-text transcription to capture the implicit meaning of the whole video\nwhile the knowledge graph provides us the domain specific dependencies between\ndifferent concepts of that subjects. Also leveraging the domain knowledge we\ncan capture the way instructor binds and connects different concepts while\nteaching, which helps us in achieving better segmentation accuracy. We tested\nour approach on NPTEL lecture videos and holistic evaluation shows that it out\nperforms the other methods described in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 13:37:40 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Das", "Ananda", ""], ["Das", "Partha Pratim", ""]]}, {"id": "2012.07598", "submitter": "Jiachun Wang", "authors": "Jiachun Wang, Fajie Yuan, Jian Chen, Qingyao Wu, Min Yang, Yang Sun\n  and Guoxiao Zhang", "title": "StackRec: Efficient Training of Very Deep Sequential Recommender Models\n  by Iterative Stacking", "comments": null, "journal-ref": null, "doi": "10.1145/3404835.3462890", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has brought great progress for the sequential recommendation\n(SR) tasks. With advanced network architectures, sequential recommender models\ncan be stacked with many hidden layers, e.g., up to 100 layers on real-world\nrecommendation datasets. Training such a deep network is difficult because it\ncan be computationally very expensive and takes much longer time, especially in\nsituations where there are tens of billions of user-item interactions. To deal\nwith such a challenge, we present StackRec, a simple, yet very effective and\nefficient training framework for deep SR models by iterative layer stacking.\nSpecifically, we first offer an important insight that hidden layers/blocks in\na well-trained deep SR model have very similar distributions. Enlightened by\nthis, we propose the stacking operation on the pre-trained layers/blocks to\ntransfer knowledge from a shallower model to a deep model, then we perform\niterative stacking so as to yield a much deeper but easier-to-train SR model.\nWe validate the performance of StackRec by instantiating it with four\nstate-of-the-art SR models in three practical scenarios with real-world\ndatasets. Extensive experiments show that StackRec achieves not only comparable\nperformance, but also substantial acceleration in training time, compared to SR\nmodels that are trained from scratch. Codes are available at\nhttps://github.com/wangjiachun0426/StackRec.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 14:41:43 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 10:15:30 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wang", "Jiachun", ""], ["Yuan", "Fajie", ""], ["Chen", "Jian", ""], ["Wu", "Qingyao", ""], ["Yang", "Min", ""], ["Sun", "Yang", ""], ["Zhang", "Guoxiao", ""]]}, {"id": "2012.07619", "submitter": "Maartje ter Hoeve", "authors": "Maartje ter Hoeve, Julia Kiseleva, Maarten de Rijke", "title": "What Makes a Good Summary? Investigating the Focus of Automatic\n  Summarization in an Educational Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic text summarization has enjoyed great progress over the last years.\nHowever, there is little research that investigates whether the current\nresearch focus adheres to users' needs. Importantly, these needs are dependent\non the envisioned target group of the generated summaries. One such important\ntarget group is formed by students, due to their usage of summaries in their\nstudy activities. For this reason, we investigate students' needs regarding\nautomatically generated summaries by means of a survey amongst university\nstudents and find that the current direction of the field does not fully align\nwith their needs. Motivated by our findings, we formulate three groups of\nimplications that together help us formulate a renewed perspective on future\nresearch on automatic summarization. First, the educational domain requires a\nbroader perspective on automatic summarization, beyond the approaches that are\ncurrently the standard. We illustrate how we can expand these approaches\nregarding the input material, the purpose of the summaries and their potential\nformat and we define requirements for datasets that can facilitate these\nresearch directions. Second, we propose a methodology to evaluate the\nusefulness of a summary based on the identified needs of a target group. Third,\nin more general terms, we hope that our survey will be reused to investigate\nthe needs of different user groups of automatically generated summaries to\nbroaden our perspective even further.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 15:12:35 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 19:08:43 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["ter Hoeve", "Maartje", ""], ["Kiseleva", "Julia", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2012.07654", "submitter": "Nishant Yadav", "authors": "Nishant Yadav, Rajat Sen, Daniel N. Hill, Arya Mazumdar, Inderjit S.\n  Dhillon", "title": "Session-Aware Query Auto-completion using Extreme Multi-label Ranking", "comments": "Accepted in KDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467087", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query auto-completion (QAC) is a fundamental feature in search engines where\nthe task is to suggest plausible completions of a prefix typed in the search\nbar. Previous queries in the user session can provide useful context for the\nuser's intent and can be leveraged to suggest auto-completions that are more\nrelevant while adhering to the user's prefix. Such session-aware QACs can be\ngenerated by recent sequence-to-sequence deep learning models; however, these\ngenerative approaches often do not meet the stringent latency requirements of\nresponding to each user keystroke. Moreover, these generative approaches pose\nthe risk of showing nonsensical queries.\n  In this paper, we provide a solution to this problem: we take the novel\napproach of modeling session-aware QAC as an eXtreme Multi-Label Ranking (XMR)\nproblem where the input is the previous query in the session and the user's\ncurrent prefix, while the output space is the set of tens of millions of\nqueries entered by users in the recent past. We adapt a popular XMR algorithm\nfor this purpose by proposing several modifications to the key steps in the\nalgorithm. The proposed modifications yield a 10x improvement in terms of Mean\nReciprocal Rank (MRR) over the baseline XMR approach on a public search logs\ndataset. We are able to maintain an inference latency of less than 10 ms while\nstill using session context. When compared against baseline models of\nacceptable latency, we observed a 33% improvement in MRR for short prefixes of\nup to 3 characters. Moreover, our model yielded a statistically significant\nimprovement of 2.81% over a production QAC system in terms of suggestion\nacceptance rate, when deployed on the search bar of an online shopping store as\npart of an A/B test.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 17:56:22 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 19:05:55 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Yadav", "Nishant", ""], ["Sen", "Rajat", ""], ["Hill", "Daniel N.", ""], ["Mazumdar", "Arya", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "2012.08000", "submitter": "Sharan Srinivas", "authors": "Sharan Srinivas, Surya Ramachandiran", "title": "Discovering Airline-Specific Business Intelligence from Online Passenger\n  Reviews: An Unsupervised Text Analytics Approach", "comments": "34 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  To understand the important dimensions of service quality from the\npassenger's perspective and tailor service offerings for competitive advantage,\nairlines can capitalize on the abundantly available online customer reviews\n(OCR). The objective of this paper is to discover company- and\ncompetitor-specific intelligence from OCR using an unsupervised text analytics\napproach. First, the key aspects (or topics) discussed in the OCR are extracted\nusing three topic models - probabilistic latent semantic analysis (pLSA) and\ntwo variants of Latent Dirichlet allocation (LDA-VI and LDA-GS). Subsequently,\nwe propose an ensemble-assisted topic model (EA-TM), which integrates the\nindividual topic models, to classify each review sentence to the most\nrepresentative aspect. Likewise, to determine the sentiment corresponding to a\nreview sentence, an ensemble sentiment analyzer (E-SA), which combines the\npredictions of three opinion mining methods (AFINN, SentiStrength, and VADER),\nis developed. An aspect-based opinion summary (AOS), which provides a snapshot\nof passenger-perceived strengths and weaknesses of an airline, is established\nby consolidating the sentiments associated with each aspect. Furthermore, a\nbi-gram analysis of the labeled OCR is employed to perform root cause analysis\nwithin each identified aspect. A case study involving 99,147 airline reviews of\na US-based target carrier and four of its competitors is used to validate the\nproposed approach. The results indicate that a cost- and time-effective\nperformance summary of an airline and its competitors can be obtained from OCR.\nFinally, besides providing theoretical and managerial implications based on our\nresults, we also provide implications for post-pandemic preparedness in the\nairline industry considering the unprecedented impact of coronavirus disease\n2019 (COVID-19) and predictions on similar pandemics in the future.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 23:09:10 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Srinivas", "Sharan", ""], ["Ramachandiran", "Surya", ""]]}, {"id": "2012.08020", "submitter": "Leonid Boytsov", "authors": "Leonid Boytsov", "title": "Traditional IR rivals neural models on the MS MARCO Document Ranking\n  Leaderboard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short document describes a traditional IR system that achieved MRR@100\nequal to 0.298 on the MS MARCO Document Ranking leaderboard (on 2020-12-06).\nAlthough inferior to most BERT-based models, it outperformed several neural\nruns (as well as all non-neural ones), including two submissions that used a\nlarge pretrained Transformer model for re-ranking. We provide software and data\nto reproduce our results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 00:35:41 GMT"}, {"version": "v2", "created": "Sat, 19 Dec 2020 11:03:16 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 18:20:00 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Boytsov", "Leonid", ""]]}, {"id": "2012.08134", "submitter": "Saurav Manchanda", "authors": "Saurav Manchanda and Mohit Sharma and George Karypis", "title": "Distant-Supervised Slot-Filling for E-Commerce Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slot-filling refers to the task of annotating individual terms in a query\nwith the corresponding intended product characteristics (product type, brand,\ngender, size, color, etc.). These characteristics can then be used by a search\nengine to return results that better match the query's product intent.\nTraditional methods for slot-filling require the availability of training data\nwith ground truth slot-annotation information. However, generating such labeled\ndata, especially in e-commerce is expensive and time-consuming because the\nnumber of slots increases as new products are added. In this paper, we present\ndistant-supervised probabilistic generative models, that require no manual\nannotation. The proposed approaches leverage the readily available historical\nquery logs and the purchases that these queries led to, and also exploit\nco-occurrence information among the slots in order to identify intended product\ncharacteristics. We evaluate our approaches by considering how they affect\nretrieval performance, as well as how well they classify the slots. In terms of\nretrieval, our approaches achieve better ranking performance (up to 156%) over\nOkapi BM25. Moreover, our approach that leverages co-occurrence information\nleads to better performance than the one that does not on both the retrieval\nand slot classification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 07:46:07 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Manchanda", "Saurav", ""], ["Sharma", "Mohit", ""], ["Karypis", "George", ""]]}, {"id": "2012.08575", "submitter": "Gustavo Penha", "authors": "Gustavo Penha and Claudia Hauff", "title": "Weakly Supervised Label Smoothing", "comments": "Accepted for publication in the 43nd European Conference on\n  Information Retrieval (ECIR'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study Label Smoothing (LS), a widely used regularization technique, in the\ncontext of neural learning to rank (L2R) models. LS combines the ground-truth\nlabels with a uniform distribution, encouraging the model to be less confident\nin its predictions. We analyze the relationship between the non-relevant\ndocuments-specifically how they are sampled-and the effectiveness of LS,\ndiscussing how LS can be capturing \"hidden similarity knowledge\" between the\nrelevantand non-relevant document classes. We further analyze LS by testing if\na curriculum-learning approach, i.e., starting with LS and after anumber of\niterations using only ground-truth labels, is beneficial. Inspired by our\ninvestigation of LS in the context of neural L2R models, we propose a novel\ntechnique called Weakly Supervised Label Smoothing (WSLS) that takes advantage\nof the retrieval scores of the negative sampled documents as a weak supervision\nsignal in the process of modifying the ground-truth labels. WSLS is simple to\nimplement, requiring no modification to the neural ranker architecture. Our\nexperiments across three retrieval tasks-passage retrieval, similar question\nretrieval and conversation response ranking-show that WSLS for pointwise\nBERT-based rankers leads to consistent effectiveness gains. The source code is\navailable at\nhttps://anonymous.4open.science/r/dac85d48-6f71-4261-a7d8-040da6021c52/.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 19:36:52 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Penha", "Gustavo", ""], ["Hauff", "Claudia", ""]]}, {"id": "2012.08695", "submitter": "Weizhou Shen", "authors": "Weizhou Shen, Junqing Chen, Xiaojun Quan and Zhixian Xie", "title": "DialogXL: All-in-One XLNet for Multi-Party Conversation Emotion\n  Recognition", "comments": "Accepted by AAAI 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents our pioneering effort for emotion recognition in\nconversation (ERC) with pre-trained language models. Unlike regular documents,\nconversational utterances appear alternately from different parties and are\nusually organized as hierarchical structures in previous work. Such structures\nare not conducive to the application of pre-trained language models such as\nXLNet. To address this issue, we propose an all-in-one XLNet model, namely\nDialogXL, with enhanced memory to store longer historical context and\ndialog-aware self-attention to deal with the multi-party structures.\nSpecifically, we first modify the recurrence mechanism of XLNet from\nsegment-level to utterance-level in order to better model the conversational\ndata. Second, we introduce dialog-aware self-attention in replacement of the\nvanilla self-attention in XLNet to capture useful intra- and inter-speaker\ndependencies. Extensive experiments are conducted on four ERC benchmarks with\nmainstream models presented for comparison. The experimental results show that\nthe proposed model outperforms the baselines on all the datasets. Several other\nexperiments such as ablation study and error analysis are also conducted and\nthe results confirm the role of the critical modules of DialogXL.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 01:50:46 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Shen", "Weizhou", ""], ["Chen", "Junqing", ""], ["Quan", "Xiaojun", ""], ["Xie", "Zhixian", ""]]}, {"id": "2012.08777", "submitter": "Mariya Hendriksen", "authors": "Mariya Hendriksen, Ernst Kuiper, Pim Nauts, Sebastian Schelter,\n  Maarten de Rijke", "title": "Analyzing and Predicting Purchase Intent in E-commerce: Anonymous vs.\n  Identified Customers", "comments": "10 pages, accepted at SIGIR eCommerce 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The popularity of e-commerce platforms continues to grow. Being able to\nunderstand, and predict customer behavior is essential for customizing the user\nexperience through personalized result presentations, recommendations, and\nspecial offers. Previous work has considered a broad range of prediction models\nas well as features inferred from clickstream data to record session\ncharacteristics, and features inferred from user data to record customer\ncharacteristics. So far, most previous work in the area of purchase prediction\nhas focused on known customers, largely ignoring anonymous sessions, i.e.,\nsessions initiated by a non-logged-in or unrecognized customer. However, in the\nde-identified data from a large European e-commerce platform available to us,\nmore than 50% of the sessions start as anonymous sessions. In this paper, we\nfocus on purchase prediction for both anonymous and identified sessions on an\ne-commerce platform. We start with a descriptive analysis of purchase vs.\nnon-purchase sessions. This analysis informs the definition of a feature-based\nmodel for purchase prediction for anonymous sessions and identified sessions;\nour models consider a range of session-based features for anonymous sessions,\nsuch as the channel type, the number of visited pages, and the device type. For\nidentified user sessions, our analysis points to customer history data as a\nvaluable discriminator between purchase and non-purchase sessions. Based on our\nanalysis, we build two types of predictors: (1) a predictor for anonymous that\nbeats a production-ready predictor by over 17.54% F1; and (2) a predictor for\nidentified customers that uses session data as well as customer history and\nachieves an F1 of 96.20%. Finally, we discuss the broader practical\nimplications of our findings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 07:43:44 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Hendriksen", "Mariya", ""], ["Kuiper", "Ernst", ""], ["Nauts", "Pim", ""], ["Schelter", "Sebastian", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2012.08787", "submitter": "Vincent Claveau", "authors": "Vincent Claveau", "title": "Query expansion with artificially generated texts", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A well-known way to improve the performance of document retrieval is to\nexpand the user's query. Several approaches have been proposed in the\nliterature, and some of them are considered as yielding state-of-the-art\nresults in IR. In this paper, we explore the use of text generation to\nautomatically expand the queries. We rely on a well-known neural generative\nmodel, GPT-2, that comes with pre-trained models for English but can also be\nfine-tuned on specific corpora. Through different experiments, we show that\ntext generation is a very effective way to improve the performance of an IR\nsystem, with a large margin (+10% MAP gains), and that it outperforms strong\nbaselines also relying on query expansion (LM+RM3). This conceptually simple\napproach can easily be implemented on any IR system thanks to the availability\nof GPT code and models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 08:13:08 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Claveau", "Vincent", ""]]}, {"id": "2012.08793", "submitter": "Miroslav Rac", "authors": "Miroslav Rac, Michal Kompan, Maria Bielikova", "title": "Session-based k-NNs with Semantic Suggestions for Next-item Prediction", "comments": "11 pages, 3 figures, 3 tables, submitted to and presented at RecSys20\n  CARS workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the most critical problems in e-commerce domain is the information\noverload problem. Usually, an enormous number of products is offered to a user.\nThe characteristics of this domain force researchers to opt for session-based\nrecommendation methods, from which nearest-neighbors-based (SkNN) approaches\nhave been shown to be competitive with and even outperform neural network-based\nmodels. Existing SkNN approaches, however, lack the ability to detect sudden\ninterest changes at a micro-level, i.e., during an individual session; and to\nadapt their recommendations to these changes. In this paper, we propose a\nconceptual (cSkNN) model extension for the next-item prediction allowing better\nadaptation to the interest changes via the semantic-level properties. We use an\nNLP technique to parse salient concepts from the product titles to create\nlinguistically based product generalizations that are used for change detection\nand a recommendation list post-filtering. We conducted experiments with two\nversions of our extension that differ in semantics derivation procedure while\nboth showing an improvement over the existing SkNN method on a sparse fashion\ne-commerce dataset.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 08:33:25 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Rac", "Miroslav", ""], ["Kompan", "Michal", ""], ["Bielikova", "Maria", ""]]}, {"id": "2012.08907", "submitter": "Abdulmalik Johar Abdulmalik Johar", "authors": "Abdulmalik Johar", "title": "Information retrieval system for silte language using BM25 weighting", "comments": "3 pages, 5 figures,2 table,3 chart", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The main aim of an information retrieval system is to extract appropriate\ninformation from an enormous collection of data based on users need. The basic\nconcept of the information retrieval system is that when a user sends out a\nquery, the system would try to generate a list of related documents ranked in\norder, according to their degree of relevance. Digital unstructured Silte text\ndocuments increase from time to time. The growth of digital text information\nmakes the utilization and access of the right information difficult. Thus,\ndeveloping an information retrieval system for Silte language allows searching\nand retrieving relevant documents that satisfy information need of users. In\nthis research, we design probabilistic information retrieval system for Silte\nlanguage. The system has both indexing and searching part was created. In these\nmodules, different text operations such as tokenization, stemming, stop word\nremoval and synonym is included.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 12:36:19 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Johar", "Abdulmalik", ""]]}, {"id": "2012.08919", "submitter": "Denisa A. O. Roberts", "authors": "Denisa A.O. Roberts", "title": "Multilingual Evidence Retrieval and Fact Verification to Combat Global\n  Disinformation: The Power of Polyglotism", "comments": "Accepted ECIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article investigates multilingual evidence retrieval and fact\nverification as a step to combat global disinformation, a first effort of this\nkind, to the best of our knowledge. The goal is building multilingual systems\nthat retrieve in evidence-rich languages to verify claims in evidence-poor\nlanguages that are more commonly targeted by disinformation. To this end, our\nEnmBERT fact verification system shows evidence of transfer learning ability\nand 400 example mixed English-Romanian dataset is made available for\ncross-lingual transfer learning evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 13:10:56 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 23:02:58 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Roberts", "Denisa A. O.", ""]]}, {"id": "2012.08952", "submitter": "Yanshi Wang", "authors": "Yuting Chen, Yanshi Wang, Yabo Ni, An-Xiang Zeng and Lanfen Lin", "title": "Scenario-aware and Mutual-based approach for Multi-scenario\n  Recommendation in E-Commerce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommender systems (RSs) are essential for e-commerce platforms to help meet\nthe enormous needs of users. How to capture user interests and make accurate\nrecommendations for users in heterogeneous e-commerce scenarios is still a\ncontinuous research topic. However, most existing studies overlook the\nintrinsic association of the scenarios: the log data collected from platforms\ncan be naturally divided into different scenarios (e.g., country, city,\nculture).\n  We observed that the scenarios are heterogeneous because of the huge\ndifferences among them. Therefore, a unified model is difficult to effectively\ncapture complex correlations (e.g., differences and similarities) between\nmultiple scenarios thus seriously reducing the accuracy of recommendation\nresults.\n  In this paper, we target the problem of multi-scenario recommendation in\ne-commerce, and propose a novel recommendation model named Scenario-aware\nMutual Learning (SAML) that leverages the differences and similarities between\nmultiple scenarios. We first introduce scenario-aware feature representation,\nwhich transforms the embedding and attention modules to map the features into\nboth global and scenario-specific subspace in parallel. Then we introduce an\nauxiliary network to model the shared knowledge across all scenarios, and use a\nmulti-branch network to model differences among specific scenarios. Finally, we\nemploy a novel mutual unit to adaptively learn the similarity between various\nscenarios and incorporate it into multi-branch network. We conduct extensive\nexperiments on both public and industrial datasets, empirical results show that\nSAML consistently and significantly outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 13:52:14 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Chen", "Yuting", ""], ["Wang", "Yanshi", ""], ["Ni", "Yabo", ""], ["Zeng", "An-Xiang", ""], ["Lin", "Lanfen", ""]]}, {"id": "2012.08984", "submitter": "Diksha Garg", "authors": "Diksha Garg, Priyanka Gupta, Pankaj Malhotra, Lovekesh Vig, Gautam\n  Shroff", "title": "Batch-Constrained Distributional Reinforcement Learning for\n  Session-based Recommendation", "comments": "Presented at Offline Reinforcement Learning Workshop at Neural\n  Information Processing Systems, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing deep reinforcement learning (RL) approaches for\nsession-based recommendations either rely on costly online interactions with\nreal users, or rely on potentially biased rule-based or data-driven\nuser-behavior models for learning. In this work, we instead focus on learning\nrecommendation policies in the pure batch or offline setting, i.e. learning\npolicies solely from offline historical interaction logs or batch data\ngenerated from an unknown and sub-optimal behavior policy, without further\naccess to data from the real-world or user-behavior models. We propose BCD4Rec:\nBatch-Constrained Distributional RL for Session-based Recommendations. BCD4Rec\nbuilds upon the recent advances in batch (offline) RL and distributional RL to\nlearn from offline logs while dealing with the intrinsically stochastic nature\nof rewards from the users due to varied latent interest preferences\n(environments). We demonstrate that BCD4Rec significantly improves upon the\nbehavior policy as well as strong RL and non-RL baselines in the batch setting\nin terms of standard performance metrics like Click Through Rates or Buy Rates.\nOther useful properties of BCD4Rec include: i. recommending items from the\ncorrect latent categories indicating better value estimates despite large\naction space (of the order of number of items), and ii. overcoming popularity\nbias in clicked or bought items typically present in the offline logs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 14:27:05 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Garg", "Diksha", ""], ["Gupta", "Priyanka", ""], ["Malhotra", "Pankaj", ""], ["Vig", "Lovekesh", ""], ["Shroff", "Gautam", ""]]}, {"id": "2012.08986", "submitter": "Huifeng Guo", "authors": "Huifeng Guo, Bo Chen, Ruiming Tang, Weinan Zhang, Zhenguo Li, Xiuqiang\n  He", "title": "An Embedding Learning Framework for Numerical Features in CTR Prediction", "comments": "9 pages", "journal-ref": null, "doi": "10.1145/3447548.3467077", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Click-Through Rate (CTR) prediction is critical for industrial recommender\nsystems, where most deep CTR models follow an Embedding \\& Feature Interaction\nparadigm. However, the majority of methods focus on designing network\narchitectures to better capture feature interactions while the feature\nembedding, especially for numerical features, has been overlooked. Existing\napproaches for numerical features are difficult to capture informative\nknowledge because of the low capacity or hard discretization based on the\noffline expertise feature engineering. In this paper, we propose a novel\nembedding learning framework for numerical features in CTR prediction (AutoDis)\nwith high model capacity, end-to-end training and unique representation\nproperties preserved. AutoDis consists of three core components:\nmeta-embeddings, automatic discretization and aggregation. Specifically, we\npropose meta-embeddings for each numerical field to learn global knowledge from\nthe perspective of field with a manageable number of parameters. Then the\ndifferentiable automatic discretization performs soft discretization and\ncaptures the correlations between the numerical features and meta-embeddings.\nFinally, distinctive and informative embeddings are learned via an aggregation\nfunction. Comprehensive experiments on two public and one industrial datasets\nare conducted to validate the effectiveness of AutoDis. Moreover, AutoDis has\nbeen deployed onto a mainstream advertising platform, where online A/B test\ndemonstrates the improvement over the base model by 2.1% and 2.7% in terms of\nCTR and eCPM, respectively. In addition, the code of our framework is publicly\navailable in\nMindSpore(https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/recommend/autodis).\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 14:31:31 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 05:20:45 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Guo", "Huifeng", ""], ["Chen", "Bo", ""], ["Tang", "Ruiming", ""], ["Zhang", "Weinan", ""], ["Li", "Zhenguo", ""], ["He", "Xiuqiang", ""]]}, {"id": "2012.09263", "submitter": "Sidharth Singla", "authors": "Sidharth Singla", "title": "Checking Fact Worthiness using Sentence Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Checking and confirming factual information in texts and speeches is vital to\ndetermine the veracity and correctness of the factual statements. This work was\npreviously done by journalists and other manual means but it is a\ntime-consuming task. With the advancements in Information Retrieval and NLP,\nresearch in the area of Fact-checking is getting attention for automating it.\nCLEF-2018 and 2019 organised tasks related to Fact-checking and invited\nparticipants. This project focuses on CLEF-2019 Task-1 Check-Worthiness and\nexperiments using the latest Sentence-BERT pre-trained embeddings, topic\nModeling and sentiment score are performed. Evaluation metrics such as MAP,\nMean Reciprocal Rank, Mean R-Precision and Mean Precision@N present the\nimprovement in the results using the techniques.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 21:00:24 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Singla", "Sidharth", ""]]}, {"id": "2012.09342", "submitter": "Nethra Viswanathan", "authors": "Nethra Viswanathan", "title": "Adaptive Multi-Agent E-Learning Recommender Systems", "comments": "4 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Educational recommender systems have become a necessity in the recent years\ndue to overload of available educational resource which makes it difficult for\nan individual to manually hunt for the required resource on the internet.\nE-learning recommender systems simplify the tedious task of gathering the right\nweb pages and web documents from the scattered world wide web repositories\naccording to every users' requirements thus increasing the demand and hence the\ncuriosity to study them. Retrieval of a handful of recommendations from a very\nhuge collection of web pages using different recommendation techniques becomes\na productive and time efficient process when the system functions with a set of\ncooperative agents. The system is also required to keep up with the changing\nuser interests and web resources in the dynamic web environment, and hence\nadaptivity is an important factor in determining the efficiency of recommender\nsystems. The paper provides an overview of such adaptive multi-agent e-learning\nrecommender systems and the concepts employed to implement them. It precisely\nprovides all the information required by a researcher who wants to study the\nstate-of-the-art work on such systems thus enabling him to decide on the\nimplementation concepts for his own system.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 01:02:14 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Viswanathan", "Nethra", ""]]}, {"id": "2012.09355", "submitter": "Jiho Noh", "authors": "Jiho Noh and Ramakanth Kavuluru", "title": "Literature Retrieval for Precision Medicine with Neural Matching and\n  Faceted Summarization", "comments": "Accepted to EMNLP 2020 Findings as Long Paper (11 page, 4 figures)", "journal-ref": null, "doi": "10.18653/v1/2020.findings-emnlp.304", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Information retrieval (IR) for precision medicine (PM) often involves looking\nfor multiple pieces of evidence that characterize a patient case. This\ntypically includes at least the name of a condition and a genetic variation\nthat applies to the patient. Other factors such as demographic attributes,\ncomorbidities, and social determinants may also be pertinent. As such, the\nretrieval problem is often formulated as ad hoc search but with multiple facets\n(e.g., disease, mutation) that may need to be incorporated. In this paper, we\npresent a document reranking approach that combines neural query-document\nmatching and text summarization toward such retrieval scenarios. Our\narchitecture builds on the basic BERT model with three specific components for\nreranking: (a). document-query matching (b). keyword extraction and (c).\nfacet-conditioned abstractive summarization. The outcomes of (b) and (c) are\nused to essentially transform a candidate document into a concise summary that\ncan be compared with the query at hand to compute a relevance score. Component\n(a) directly generates a matching score of a candidate document for a query.\nThe full architecture benefits from the complementary potential of\ndocument-query matching and the novel document transformation approach based on\nsummarization along PM facets. Evaluations using NIST's TREC-PM track datasets\n(2017--2019) show that our model achieves state-of-the-art performance. To\nfoster reproducibility, our code is made available here:\nhttps://github.com/bionlproc/text-summ-for-doc-retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 02:01:32 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Noh", "Jiho", ""], ["Kavuluru", "Ramakanth", ""]]}, {"id": "2012.09369", "submitter": "Ravi Sharma", "authors": "Ravi Sharma, Sri Divya Pagadala, Pratool Bharti, Sriram Chellappan,\n  Trine Schmidt and Raj Goyal", "title": "Assessing COVID-19 Impacts on College Students via Automated Processing\n  of Free-form Text", "comments": "8 pages, 5 figures, HEALTHINF - 14th International Conference on\n  Health Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we report experimental results on assessing the impact of\nCOVID-19 on college students by processing free-form texts generated by them.\nBy free-form texts, we mean textual entries posted by college students\n(enrolled in a four year US college) via an app specifically designed to assess\nand improve their mental health. Using a dataset comprising of more than 9000\ntextual entries from 1451 students collected over four months (split between\npre and post COVID-19), and established NLP techniques, a) we assess how topics\nof most interest to student change between pre and post COVID-19, and b) we\nassess the sentiments that students exhibit in each topic between pre and post\nCOVID-19. Our analysis reveals that topics like Education became noticeably\nless important to students post COVID-19, while Health became much more\ntrending. We also found that across all topics, negative sentiment among\nstudents post COVID-19 was much higher compared to pre-COVID-19. We expect our\nstudy to have an impact on policy-makers in higher education across several\nspectra, including college administrators, teachers, parents, and mental health\ncounselors.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 02:46:48 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Sharma", "Ravi", ""], ["Pagadala", "Sri Divya", ""], ["Bharti", "Pratool", ""], ["Chellappan", "Sriram", ""], ["Schmidt", "Trine", ""], ["Goyal", "Raj", ""]]}, {"id": "2012.09442", "submitter": "Masahiro Sato", "authors": "Masahiro Sato, Sho Takemori, Janmajay Singh, Qian Zhang", "title": "Causality-Aware Neighborhood Methods for Recommender Systems", "comments": "accepted at ECIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The business objectives of recommenders, such as increasing sales, are\naligned with the causal effect of recommendations. Previous recommenders\ntargeting for the causal effect employ the inverse propensity scoring (IPS) in\ncausal inference. However, IPS is prone to suffer from high variance. The\nmatching estimator is another representative method in causal inference field.\nIt does not use propensity and hence free from the above variance problem. In\nthis work, we unify traditional neighborhood recommendation methods with the\nmatching estimator, and develop robust ranking methods for the causal effect of\nrecommendations. Our experiments demonstrate that the proposed methods\noutperform various baselines in ranking metrics for the causal effect. The\nresults suggest that the proposed methods can achieve more sales and user\nengagement than previous recommenders.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 08:23:17 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 05:57:52 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Sato", "Masahiro", ""], ["Takemori", "Sho", ""], ["Singh", "Janmajay", ""], ["Zhang", "Qian", ""]]}, {"id": "2012.09650", "submitter": "St\\'ephane Clinchant", "authors": "Thibault Formal, Benjamin Piwowarski and St\\'ephane Clinchant", "title": "A White Box Analysis of ColBERT", "comments": "to appear in ECIR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based models are nowadays state-of-the-art in ad-hoc Information\nRetrieval, but their behavior is far from being understood. Recent work has\nclaimed that BERT does not satisfy the classical IR axioms. However, we propose\nto dissect the matching process of ColBERT, through the analysis of term\nimportance and exact/soft matching patterns. Even if the traditional axioms are\nnot formally verified, our analysis reveals that ColBERT: (i) is able to\ncapture a notion of term importance; (ii) relies on exact matches for important\nterms.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 14:59:01 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Formal", "Thibault", ""], ["Piwowarski", "Benjamin", ""], ["Clinchant", "St\u00e9phane", ""]]}, {"id": "2012.09807", "submitter": "Federico Bianchi", "authors": "Federico Bianchi and Bingqing Yu and Jacopo Tagliabue", "title": "BERT Goes Shopping: Comparing Distributional Models for Product\n  Representations", "comments": "Updated version. Published as a workshop paper at ECNLP 4 at\n  ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings (e.g., word2vec) have been applied successfully to eCommerce\nproducts through~\\textit{prod2vec}. Inspired by the recent performance\nimprovements on several NLP tasks brought by contextualized embeddings, we\npropose to transfer BERT-like architectures to eCommerce: our model --\n~\\textit{Prod2BERT} -- is trained to generate representations of products\nthrough masked session modeling. Through extensive experiments over multiple\nshops, different tasks, and a range of design choices, we systematically\ncompare the accuracy of~\\textit{Prod2BERT} and~\\textit{prod2vec} embeddings:\nwhile~\\textit{Prod2BERT} is found to be superior in several scenarios, we\nhighlight the importance of resources and hyperparameters in the best\nperforming models. Finally, we provide guidelines to practitioners for training\nembeddings under a variety of computational and data constraints.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:18:03 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 13:05:44 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Bianchi", "Federico", ""], ["Yu", "Bingqing", ""], ["Tagliabue", "Jacopo", ""]]}, {"id": "2012.09936", "submitter": "Stavroula Skylaki", "authors": "Stavroula Skylaki, Ali Oskooei, Omar Bari, Nadja Herger, Zac Kriegman\n  (Thomson Reuters Labs)", "title": "Named Entity Recognition in the Legal Domain using a Pointer Generator\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Entity Recognition (NER) is the task of identifying and classifying\nnamed entities in unstructured text. In the legal domain, named entities of\ninterest may include the case parties, judges, names of courts, case numbers,\nreferences to laws etc. We study the problem of legal NER with noisy text\nextracted from PDF files of filed court cases from US courts. The \"gold\nstandard\" training data for NER systems provide annotation for each token of\nthe text with the corresponding entity or non-entity label. We work with only\npartially complete training data, which differ from the gold standard NER data\nin that the exact location of the entities in the text is unknown and the\nentities may contain typos and/or OCR mistakes. To overcome the challenges of\nour noisy training data, e.g. text extraction errors and/or typos and unknown\nlabel indices, we formulate the NER task as a text-to-text sequence generation\ntask and train a pointer generator network to generate the entities in the\ndocument rather than label them. We show that the pointer generator can be\neffective for NER in the absence of gold standard data and outperforms the\ncommon NER neural network architectures in long legal documents.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 21:10:34 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Skylaki", "Stavroula", "", "Thomson Reuters Labs"], ["Oskooei", "Ali", "", "Thomson Reuters Labs"], ["Bari", "Omar", "", "Thomson Reuters Labs"], ["Herger", "Nadja", "", "Thomson Reuters Labs"], ["Kriegman", "Zac", "", "Thomson Reuters Labs"]]}, {"id": "2012.10185", "submitter": "Sanne Vrijenhoek", "authors": "Sanne Vrijenhoek, Mesut Kaya, Nadia Metoui, Judith M\\\"oller, Daan\n  Odijk, Natali Helberger", "title": "Recommenders with a mission: assessing diversity in newsrecommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  News recommenders help users to find relevant online content and have the\npotential to fulfill a crucial role in a democratic society, directing the\nscarce attention of citizens towards the information that is most important to\nthem. Simultaneously, recent concerns about so-called filter bubbles,\nmisinformation and selective exposure are symptomatic of the disruptive\npotential of these digital news recommenders. Recommender systems can make or\nbreak filter bubbles, and as such can be instrumental in creating either a more\nclosed or a more open internet. Current approaches to evaluating recommender\nsystems are often focused on measuring an increase in user clicks and\nshort-term engagement, rather than measuring the user's longer term interest in\ndiverse and important information.\n  This paper aims to bridge the gap between normative notions of diversity,\nrooted in democratic theory, and quantitative metrics necessary for evaluating\nthe recommender system. We propose a set of metrics grounded in social science\ninterpretations of diversity and suggest ways for practical implementations.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 12:10:13 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Vrijenhoek", "Sanne", ""], ["Kaya", "Mesut", ""], ["Metoui", "Nadia", ""], ["M\u00f6ller", "Judith", ""], ["Odijk", "Daan", ""], ["Helberger", "Natali", ""]]}, {"id": "2012.10226", "submitter": "Omkar Gurjar", "authors": "Omkar Gurjar and Manish Gupta", "title": "Should I visit this place? Inclusion and Exclusion Phrase Mining from\n  Reviews", "comments": "Accepted at European Conference On Information Retrieval (ECIR) 2021;\n  8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although several automatic itinerary generation services have made travel\nplanning easy, often times travellers find themselves in unique situations\nwhere they cannot make the best out of their trip. Visitors differ in terms of\nmany factors such as suffering from a disability, being of a particular dietary\npreference, travelling with a toddler, etc. While most tourist spots are\nuniversal, others may not be inclusive for all. In this paper, we focus on the\nproblem of mining inclusion and exclusion phrases associated with 11 such\nfactors, from reviews related to a tourist spot. While existing work on tourism\ndata mining mainly focuses on structured extraction of trip related\ninformation, personalized sentiment analysis, and automatic itinerary\ngeneration, to the best of our knowledge this is the first work on\ninclusion/exclusion phrase mining from tourism reviews. Using a dataset of 2000\nreviews related to 1000 tourist spots, our broad level classifier provides a\nbinary overlap F1 of $\\sim$80 and $\\sim$82 to classify a phrase as inclusion or\nexclusion respectively. Further, our inclusion/exclusion classifier provides an\nF1 of $\\sim$98 and $\\sim$97 for 11-class inclusion and exclusion classification\nrespectively. We believe that our work can significantly improve the quality of\nan automatic itinerary generation service.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 13:43:13 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Gurjar", "Omkar", ""], ["Gupta", "Manish", ""]]}, {"id": "2012.10820", "submitter": "Kai Wang", "authors": "Kai Wang, Chunxu Shen, Wenye Ma", "title": "AdnFM: An Attentive DenseNet based Factorization Machine for CTR\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider the Click-Through-Rate (CTR) prediction problem.\nFactorization Machines and their variants consider pair-wise feature\ninteractions, but normally we won't do high-order feature interactions using FM\ndue to high time complexity. Given the success of deep neural networks (DNNs)\nin many fields, researchers have proposed several DNN-based models to learn\nhigh-order feature interactions. Multi-layer perceptrons (MLP) have been widely\nemployed to learn reliable mappings from feature embeddings to final logits. In\nthis paper, we aim to explore more about these high-order features\ninteractions. However, high-order feature interaction deserves more attention\nand further development. Inspired by the great achievements of Densely\nConnected Convolutional Networks (DenseNet) in computer vision, we propose a\nnovel model called Attentive DenseNet based Factorization Machines (AdnFM).\nAdnFM can extract more comprehensive deep features by using all the hidden\nlayers from a feed-forward neural network as implicit high-order features, then\nselects dominant features via an attention mechanism. Also, high-order\ninteractions in the implicit way using DNNs are more cost-efficient than in the\nexplicit way, for example in FM. Extensive experiments on two real-world\ndatasets show that the proposed model can effectively improve the performance\nof CTR prediction.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 01:00:39 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Wang", "Kai", ""], ["Shen", "Chunxu", ""], ["Ma", "Wenye", ""]]}, {"id": "2012.11172", "submitter": "Amin Javari", "authors": "Amin Javari, Mehrab Norouzitallab, Mahdi Jalili", "title": "Who will accept my request? Predicting response of link initiation in\n  two-way relation networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Popularity of social networks has rapidly increased over the past few years,\nand daily lives interrupt without their proper functioning. Social networking\nplatform provide multiple interaction types between individuals, such as\ncreating and joining groups, sending and receiving messages, sharing interests\nand creating friendship relationships. This paper addresses an important\nproblem in social networks analysis and mining that is how to predict link\ninitiation feedback in two-way networks. Relationships between two individuals\nin a two-way network include a link invitation from one of the individuals,\nwhich will be an established link if it is accepted by the invitee. We consider\na sport gaming social networking platform and construct a multilayer social\nnetwork between a number of users. The network formed by the link initiation\nprocess is on one of the layers, while the other two layers include a messaging\nrelationships and interactions between the users. We propose a methodology to\nsolve the link initiation feedback prediction problem in this multilayer\nfashion. The proposed method is based on features extracted from meta-paths,\ni.e. paths defined between different individuals from multiples layers in\nmultilayer networks. We proposed a cluster-based approach to handle the\nsparsity issue in the dataset. Experimental results show that the proposed\nmethod can provide accurate prediction that outperforms state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 08:14:37 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Javari", "Amin", ""], ["Norouzitallab", "Mehrab", ""], ["Jalili", "Mahdi", ""]]}, {"id": "2012.11213", "submitter": "Shintaro Yamamoto", "authors": "Shintaro Yamamoto, Anne Lauscher, Simone Paolo Ponzetto, Goran\n  Glava\\v{s}, Shigeo Morishima", "title": "Self-Supervised Learning for Visual Summary Identification in Scientific\n  Publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Providing visual summaries of scientific publications can increase\ninformation access for readers and thereby help deal with the exponential\ngrowth in the number of scientific publications. Nonetheless, efforts in\nproviding visual publication summaries have been few and far apart, primarily\nfocusing on the biomedical domain. This is primarily because of the limited\navailability of annotated gold standards, which hampers the application of\nrobust and high-performing supervised learning techniques. To address these\nproblems we create a new benchmark dataset for selecting figures to serve as\nvisual summaries of publications based on their abstracts, covering several\ndomains in computer science. Moreover, we develop a self-supervised learning\napproach, based on heuristic matching of inline references to figures with\nfigure captions. Experiments in both biomedical and computer science domains\nshow that our model is able to outperform the state of the art despite being\nself-supervised and therefore not relying on any annotated training data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 09:48:58 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 09:00:18 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Yamamoto", "Shintaro", ""], ["Lauscher", "Anne", ""], ["Ponzetto", "Simone Paolo", ""], ["Glava\u0161", "Goran", ""], ["Morishima", "Shigeo", ""]]}, {"id": "2012.11321", "submitter": "Ruben Cartuyvels", "authors": "Ruben Cartuyvels, Graham Spinks and Marie-Francine Moens", "title": "Autoregressive Reasoning over Chains of Facts with Transformers", "comments": "Published at International Conference on Computational Linguistics\n  2020 (ICCL) (COLING)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes an iterative inference algorithm for multi-hop\nexplanation regeneration, that retrieves relevant factual evidence in the form\nof text snippets, given a natural language question and its answer. Combining\nmultiple sources of evidence or facts for multi-hop reasoning becomes\nincreasingly hard when the number of sources needed to make an inference grows.\nOur algorithm copes with this by decomposing the selection of facts from a\ncorpus autoregressively, conditioning the next iteration on previously selected\nfacts. This allows us to use a pairwise learning-to-rank loss. We validate our\nmethod on datasets of the TextGraphs 2019 and 2020 Shared Tasks for explanation\nregeneration. Existing work on this task either evaluates facts in isolation or\nartificially limits the possible chains of facts, thus limiting multi-hop\ninference. We demonstrate that our algorithm, when used with a pre-trained\ntransformer model, outperforms the previous state-of-the-art in terms of\nprecision, training time and inference efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 13:17:27 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Cartuyvels", "Ruben", ""], ["Spinks", "Graham", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "2012.11327", "submitter": "Yassien Shaalan", "authors": "Yassien Shaalan, Alexander Dokumentov, Piyapong Khumrin, Krit\n  Khwanngern, Anawat Wisetborisu, Thanakom Hatsadeang, Nattapat Karaket,\n  Witthawin Achariyaviriya, Sansanee Auephanwiriyakul, Nipon Theera-Umpon,\n  Terence Siganakis", "title": "Collaborative residual learners for automatic icd10 prediction using\n  prescribed medications", "comments": "6 Pages, 5 Figures and 4 tables. Presented at AIDH (Australian\n  Institute of Digital Health) Conference 2020", "journal-ref": "AIDH (Australian Institute of Digital Health) Conference 2020", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Clinical coding is an administrative process that involves the translation of\ndiagnostic data from episodes of care into a standard code format such as\nICD10. It has many critical applications such as billing and aetiology\nresearch. The automation of clinical coding is very challenging due to data\nsparsity, low interoperability of digital health systems, complexity of\nreal-life diagnosis coupled with the huge size of ICD10 code space. Related\nwork suffer from low applicability due to reliance on many data sources,\ninefficient modelling and less generalizable solutions. We propose a novel\ncollaborative residual learning based model to automatically predict ICD10\ncodes employing only prescriptions data. Extensive experiments were performed\non two real-world clinical datasets (outpatient & inpatient) from Maharaj\nNakorn Chiang Mai Hospital with real case-mix distributions. We obtain\nmulti-label classification accuracy of 0.71 and 0.57 of average precision, 0.57\nand 0.38 of F1-score and 0.73 and 0.44 of accuracy in predicting principal\ndiagnosis for inpatient and outpatient datasets respectively.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 07:07:27 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Shaalan", "Yassien", ""], ["Dokumentov", "Alexander", ""], ["Khumrin", "Piyapong", ""], ["Khwanngern", "Krit", ""], ["Wisetborisu", "Anawat", ""], ["Hatsadeang", "Thanakom", ""], ["Karaket", "Nattapat", ""], ["Achariyaviriya", "Witthawin", ""], ["Auephanwiriyakul", "Sansanee", ""], ["Theera-Umpon", "Nipon", ""], ["Siganakis", "Terence", ""]]}, {"id": "2012.11328", "submitter": "Antonio Ferrara", "authors": "Vito Walter Anelli, Yashar Deldjoo, Tommaso Di Noia, Antonio Ferrara,\n  Fedelucio Narducci", "title": "FedeRank: User Controlled Feedback with Federated Recommender Systems", "comments": "Accepted for publishing at 43rd European Conference on Information\n  Retrieval (ECIR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommender systems have shown to be a successful representative of how data\navailability can ease our everyday digital life. However, data privacy is one\nof the most prominent concerns in the digital era. After several data breaches\nand privacy scandals, the users are now worried about sharing their data. In\nthe last decade, Federated Learning has emerged as a new privacy-preserving\ndistributed machine learning paradigm. It works by processing data on the user\ndevice without collecting data in a central repository. We present FedeRank\n(https://split.to/federank), a federated recommendation algorithm. The system\nlearns a personal factorization model onto every device. The training of the\nmodel is a synchronous process between the central server and the federated\nclients. FedeRank takes care of computing recommendations in a distributed\nfashion and allows users to control the portion of data they want to share. By\ncomparing with state-of-the-art algorithms, extensive experiments show the\neffectiveness of FedeRank in terms of recommendation accuracy, even with a\nsmall portion of shared user data. Further analysis of the recommendation\nlists' diversity and novelty guarantees the suitability of the algorithm in\nreal production environments.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 22:26:54 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 09:25:24 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 10:28:21 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Anelli", "Vito Walter", ""], ["Deldjoo", "Yashar", ""], ["Di Noia", "Tommaso", ""], ["Ferrara", "Antonio", ""], ["Narducci", "Fedelucio", ""]]}, {"id": "2012.11333", "submitter": "Yassien Shaalan", "authors": "Yassien Shaalan, Alexander Dokumentov, Piyapong Khumrin, Krit\n  Khwanngern, Anawat Wisetborisu, Thanakom Hatsadeang, Nattapat Karaket,\n  Witthawin Achariyaviriya, Sansanee Auephanwiriyakul, Nipon Theera-Umpon,\n  Terence Siganakis", "title": "Ensemble model for pre-discharge icd10 coding prediction", "comments": "6 Pages, 2 Figures and 5 tables. Presented at AIDH (Australian\n  Institute of Digital Health) Conference 2020", "journal-ref": "AIDH (Australian Institute of Digital Health) 2020", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The translation of medical diagnosis to clinical coding has wide range of\napplications in billing, aetiology analysis, and auditing. Currently, coding is\na manual effort while the automation of such task is not straight forward.\nAmong the challenges are the messy and noisy clinical records, case\ncomplexities, along with the huge ICD10 code space. Previous work mainly relied\non discharge notes for prediction and was applied to a very limited data scale.\nWe propose an ensemble model incorporating multiple clinical data sources for\naccurate code predictions. We further propose an assessment mechanism to\nprovide confidence rates in predicted outcomes. Extensive experiments were\nperformed on two new real-world clinical datasets (inpatient & outpatient) with\nunaltered case-mix distributions from Maharaj Nakorn Chiang Mai Hospital. We\nobtain multi-label classification accuracies of 0.73 and 0.58 for average\nprecision, 0.56 and 0.35 for F1-scores and 0.71 and 0.4 accuracy in predicting\nprincipal diagnosis for inpatient and outpatient datasets respectively.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 07:02:56 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Shaalan", "Yassien", ""], ["Dokumentov", "Alexander", ""], ["Khumrin", "Piyapong", ""], ["Khwanngern", "Krit", ""], ["Wisetborisu", "Anawat", ""], ["Hatsadeang", "Thanakom", ""], ["Karaket", "Nattapat", ""], ["Achariyaviriya", "Witthawin", ""], ["Auephanwiriyakul", "Sansanee", ""], ["Theera-Umpon", "Nipon", ""], ["Siganakis", "Terence", ""]]}, {"id": "2012.11336", "submitter": "Bo Chen", "authors": "Bo Chen, Jing Zhang, Xiaokang Zhang, Xiaobin Tang, Lingfan Cai, Hong\n  Chen, Cuiping Li, Peng Zhang, and Jie Tang", "title": "COAD: Contrastive Pre-training with Adversarial Fine-tuning for\n  Zero-shot Expert Linking", "comments": "TKDE under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expert finding, a popular service provided by many online websites such as\nExpertise Finder, LinkedIn, and AMiner, benefits seeking consultants,\ncollaborators, and candidate qualifications. However, its quality is suffered\nfrom a single source of support information for experts. This paper employs\nAMiner, a free online academic search and mining system, having collected more\nthan over 100 million researcher profiles together with 200 million papers from\nmultiple publication databases, as the basis for investigating the problem of\nexpert linking, which aims at linking any external information of persons to\nexperts in AMiner. A critical challenge is how to perform zero shot expert\nlinking without any labeled linkages from the external information to AMiner\nexperts, as it is infeasible to acquire sufficient labels for arbitrary\nexternal sources. Inspired by the success of self supervised learning in\ncomputer vision and natural language processing, we propose to train a self\nsupervised expert linking model, which is first pretrained by contrastive\nlearning on AMiner data to capture the common representation and matching\npatterns of experts across AMiner and external sources, and is then fine-tuned\nby adversarial learning on AMiner and the unlabeled external sources to improve\nthe model transferability. Experimental results demonstrate that COAD\nsignificantly outperforms various baselines without contrastive learning of\nexperts on two widely studied downstream tasks: author identification\n(improving up to 32.1% in HitRatio@1) and paper clustering (improving up to\n14.8% in Pairwise-F1). Expert linking on two genres of external sources also\nindicates the superiority of the proposed adversarial fine-tuning method\ncompared with other domain adaptation ways (improving up to 2.3% in\nHitRatio@1).\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 03:11:11 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Chen", "Bo", ""], ["Zhang", "Jing", ""], ["Zhang", "Xiaokang", ""], ["Tang", "Xiaobin", ""], ["Cai", "Lingfan", ""], ["Chen", "Hong", ""], ["Li", "Cuiping", ""], ["Zhang", "Peng", ""], ["Tang", "Jie", ""]]}, {"id": "2012.11405", "submitter": "Sophia Althammer", "authors": "Sophia Althammer, Sebastian Hofst\\\"atter, Allan Hanbury", "title": "Cross-domain Retrieval in the Legal and Patent Domains: a\n  Reproducibility Study", "comments": "Accepted at ECIR 2021 (Reproducibility paper track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain specific search has always been a challenging information retrieval\ntask due to several challenges such as the domain specific language, the unique\ntask setting, as well as the lack of accessible queries and corresponding\nrelevance judgements. In the last years, pretrained language models, such as\nBERT, revolutionized web and news search. Naturally, the community aims to\nadapt these advancements to cross-domain transfer of retrieval models for\ndomain specific search. In the context of legal document retrieval, Shao et al.\npropose the BERT-PLI framework by modeling the Paragraph Level Interactions\nwith the language model BERT. In this paper we reproduce the original\nexperiments, we clarify pre-processing steps, add missing scripts for framework\nsteps and investigate different evaluation approaches, however we are not able\nto reproduce the evaluation results. Contrary to the original paper, we\ndemonstrate that the domain specific paragraph-level modelling does not appear\nto help the performance of the BERT-PLI model compared to paragraph-level\nmodelling with the original BERT. In addition to our legal search\nreproducibility study, we investigate BERT-PLI for document retrieval in the\npatent domain. We find that the BERT-PLI model does not yet achieve performance\nimprovements for patent document retrieval compared to the BM25 baseline.\nFurthermore, we evaluate the BERT-PLI model for cross-domain retrieval between\nthe legal and patent domain on individual components, both on a paragraph and\ndocument-level. We find that the transfer of the BERT-PLI model on the\nparagraph-level leads to comparable results between both domains as well as\nfirst promising results for the cross-domain transfer on the document-level.\nFor reproducibility and transparency as well as to benefit the community we\nmake our source code and the trained models publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 15:06:15 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 09:18:44 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Althammer", "Sophia", ""], ["Hofst\u00e4tter", "Sebastian", ""], ["Hanbury", "Allan", ""]]}, {"id": "2012.11480", "submitter": "Markus Viljanen", "authors": "Markus Viljanen, Tapio Pahikkala", "title": "New Recommendation Algorithm for Implicit Data Motivated by the\n  Multivariate Normal Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of recommender systems is to help users find useful items from a\nlarge catalog of items by producing a list of item recommendations for every\nuser. Data sets based on implicit data collection have a number of special\ncharacteristics. The user and item interaction matrix is often complete, i.e.\nevery user and item pair has an interaction value or zero for no interaction,\nand the goal is to rank the items for every user. This study presents a simple\nnew algorithm for implicit data that matches or outperforms baselines in\naccuracy. The algorithm can be motivated intuitively by the Multivariate Normal\nDistribution (MVN), where have a closed form expression for the ranking of\nnon-interactions given user's interactions. The main difference to kNN and SVD\nbaselines is that predictions are carried out using only the known\ninteractions. Modified baselines with this trick have a better accuracy,\nhowever it also results in simpler models with fewer hyperparameters for\nimplicit data. Our results suggest that this idea should used in Top-N\nrecommendation with small seed sizes and the MVN is a simple way to do so.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 16:50:19 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Viljanen", "Markus", ""], ["Pahikkala", "Tapio", ""]]}, {"id": "2012.11685", "submitter": "Bhaskar Mitra", "authors": "Bhaskar Mitra", "title": "Neural Methods for Effective, Efficient, and Exposure-Aware Information\n  Retrieval", "comments": "PhD thesis, Univ College London (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks with deep architectures have demonstrated significant\nperformance improvements in computer vision, speech recognition, and natural\nlanguage processing. The challenges in information retrieval (IR), however, are\ndifferent from these other application areas. A common form of IR involves\nranking of documents--or short passages--in response to keyword-based queries.\nEffective IR systems must deal with query-document vocabulary mismatch problem,\nby modeling relationships between different query and document terms and how\nthey indicate relevance. Models should also consider lexical matches when the\nquery contains rare terms--such as a person's name or a product model\nnumber--not seen during training, and to avoid retrieving semantically related\nbut irrelevant results. In many real-life IR tasks, the retrieval involves\nextremely large collections--such as the document index of a commercial Web\nsearch engine--containing billions of documents. Efficient IR methods should\ntake advantage of specialized IR data structures, such as inverted index, to\nefficiently retrieve from large collections. Given an information need, the IR\nsystem also mediates how much exposure an information artifact receives by\ndeciding whether it should be displayed, and where it should be positioned,\namong other results. Exposure-aware IR systems may optimize for additional\nobjectives, besides relevance, such as parity of exposure for retrieved items\nand content publishers. In this thesis, we present novel neural architectures\nand methods motivated by the specific needs and challenges of IR tasks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 21:20:16 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 21:47:04 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Mitra", "Bhaskar", ""]]}, {"id": "2012.11842", "submitter": "Runsheng Yu", "authors": "Runsheng Yu, Yu Gong, Xu He, Bo An, Yu Zhu, Qingwen Liu, Wenwu Ou", "title": "Personalized Adaptive Meta Learning for Cold-start User Preference\n  Prediction", "comments": "Preprint Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common challenge in personalized user preference prediction is the\ncold-start problem. Due to the lack of user-item interactions, directly\nlearning from the new users' log data causes serious over-fitting problem.\nRecently, many existing studies regard the cold-start personalized preference\nprediction as a few-shot learning problem, where each user is the task and\nrecommended items are the classes, and the gradient-based meta learning method\n(MAML) is leveraged to address this challenge. However, in real-world\napplication, the users are not uniformly distributed (i.e., different users may\nhave different browsing history, recommended items, and user profiles. We\ndefine the major users as the users in the groups with large numbers of users\nsharing similar user information, and other users are the minor users),\nexisting MAML approaches tend to fit the major users and ignore the minor\nusers. To address this cold-start task-overfitting problem, we propose a novel\npersonalized adaptive meta learning approach to consider both the major and the\nminor users with three key contributions: 1) We are the first to present a\npersonalized adaptive learning rate meta-learning approach to improve the\nperformance of MAML by focusing on both the major and minor users. 2) To\nprovide better personalized learning rates for each user, we introduce a\nsimilarity-based method to find similar users as a reference and a tree-based\nmethod to store users' features for fast search. 3) To reduce the memory usage,\nwe design a memory agnostic regularizer to further reduce the space complexity\nto constant while maintain the performance. Experiments on MovieLens,\nBookCrossing, and real-world production datasets reveal that our method\noutperforms the state-of-the-art methods dramatically for both the minor and\nmajor users.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 05:48:08 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Yu", "Runsheng", ""], ["Gong", "Yu", ""], ["He", "Xu", ""], ["An", "Bo", ""], ["Zhu", "Yu", ""], ["Liu", "Qingwen", ""], ["Ou", "Wenwu", ""]]}, {"id": "2012.11876", "submitter": "Salman Mousaeirad", "authors": "Salman Mousaeirad", "title": "Intelligent Vector-based Customer Segmentation in the Banking Industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer Segmentation is the process of dividing customers into groups based\non common characteristics. An intelligent Customer Segmentation will not only\nenable an organization to effectively allocate marketing resources (e.g.,\nRecommender Systems in the Banking sector) but also it will enable identifying\nthe customer cohorts that are most likely to benefit from a specific policy\n(e.g., to discover diverse patient groups in the Health sector). While there\nhas been a significant improvement in approaches to Customer Segmentation, the\nmain challenge remains to be the understanding of the reasons behind the\nsegmentation need. This task is challenging as it is subjective and depends on\nthe goal of segmentation as well as the analyst's perspective. To address this\nchallenge, in this paper, we present an intelligent vector-based customer\nsegmentation approach. The proposed approach will leverage feature engineering\nto enable analysts to identify important features (from a pool of features such\nas demographics, geography, psychographics, behavioral, and more) and feed them\ninto a neural embedding framework named Customer2Vec. The Customer2Vec combines\nthe neural network classification and clustering methods as supervised and\nunsupervised learning techniques to embed the customer vector. We adopt a\ntypical scenario in the Banking Sector to highlight how Customer2Vec\nsignificantly improves the quality of the segmentation and detecting customer\nsimilarities.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 08:27:42 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Mousaeirad", "Salman", ""]]}, {"id": "2012.11967", "submitter": "Anna Glazkova", "authors": "Anna Glazkova, Maksim Glazkov, Timofey Trifonov", "title": "g2tmn at Constraint@AAAI2021: Exploiting CT-BERT and Ensembling Learning\n  for COVID-19 Fake News Detection", "comments": "The winning solution at the Constraint shared task (AAAI-2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has had a huge impact on various areas of human life.\nHence, the coronavirus pandemic and its consequences are being actively\ndiscussed on social media. However, not all social media posts are truthful.\nMany of them spread fake news that cause panic among readers, misinform people\nand thus exacerbate the effect of the pandemic. In this paper, we present our\nresults at the Constraint@AAAI2021 Shared Task: COVID-19 Fake News Detection in\nEnglish. In particular, we propose our approach using the transformer-based\nensemble of COVID-Twitter-BERT (CT-BERT) models. We describe the models used,\nthe ways of text preprocessing and adding extra data. As a result, our best\nmodel achieved the weighted F1-score of 98.69 on the test set (the first place\nin the leaderboard) of this shared task that attracted 166 submitted teams in\ntotal.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 12:43:12 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 09:29:13 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 11:36:32 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Glazkova", "Anna", ""], ["Glazkov", "Maksim", ""], ["Trifonov", "Timofey", ""]]}, {"id": "2012.12065", "submitter": "Guy Rosin", "authors": "Guy D. Rosin, Ido Guy, Kira Radinsky", "title": "Event-Driven Query Expansion", "comments": "9 pages, WSDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A significant number of event-related queries are issued in Web search. In\nthis paper, we seek to improve retrieval performance by leveraging events and\nspecifically target the classic task of query expansion. We propose a method to\nexpand an event-related query by first detecting the events related to it.\nThen, we derive the candidates for expansion as terms semantically related to\nboth the query and the events. To identify the candidates, we utilize a novel\nmechanism to simultaneously embed words and events in the same vector space. We\nshow that our proposed method of leveraging events improves query expansion\nperformance significantly compared with state-of-the-art methods on various\nnewswire TREC datasets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 14:56:54 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Rosin", "Guy D.", ""], ["Guy", "Ido", ""], ["Radinsky", "Kira", ""]]}, {"id": "2012.12498", "submitter": "Maor Reuben", "authors": "Aviad Elyashar, Maor Reuben, and Rami Puzis", "title": "Fake News Data Collection and Classification: Iterative Query Selection\n  for Opaque Search Engines with Pseudo Relevance Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retrieving information from an online search engine, is the first and most\nimportant step in many data mining tasks. Most of the search engines currently\navailable on the web, including all social media platforms, are black-boxes\n(a.k.a opaque) supporting short keyword queries. In these settings, retrieving\nall posts and comments discussing a particular news item automatically and at\nlarge scales is a challenging task. In this paper, we propose a method for\ngenerating short keyword queries given a prototype document. The proposed\niterative query selection algorithm (IQS) interacts with the opaque search\nengine to iteratively improve the query. It is evaluated on the Twitter TREC\nMicroblog 2012 and TREC-COVID 2019 datasets showing superior performance\ncompared to state-of-the-art. IQS is applied to automatically collect a\nlarge-scale fake news dataset of about 70K true and fake news items. The\ndataset, publicly available for research, includes more than 22M accounts and\n61M tweets in Twitter approved format. We demonstrate the usefulness of the\ndataset for fake news detection task achieving state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 05:40:15 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 11:38:57 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Elyashar", "Aviad", ""], ["Reuben", "Maor", ""], ["Puzis", "Rami", ""]]}, {"id": "2012.12522", "submitter": "Xiaobing Yu", "authors": "Xiaobing Yu, Mike Stahr, Han Chen, and Runming Yan", "title": "Design and Implementation of Curriculum System Based on Knowledge Graph", "comments": "4 pages, 4 figures, accepted by ICCECE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the fact that the knowledge in each field in university is keeping\nincreasing, the number of university courses is becoming larger, and the\ncontent and curriculum system is becoming much more complicated than it used to\nbe, which bring many inconveniences to the course arrangement and analysis. In\nthis paper, we aim to construct a method to visualize all courses based on\nGoogle Knowledge Graph. By analysing the properties of the courses and their\npreceding requirements, we want to extract the relationship between the\nprecursors and the successors, so as to build the knowledge graph of the\ncurriculum system. Using the graph database Neo4j [7] as the core aspect for\ndata storage and display for our new curriculum system will be our approach to\nimplement our knowledge graph. Based on this graph, the venation relationship\nbetween courses can be clearly analysed, and some difficult information can be\nobtained, which can help to combine the outline of courses and the need to\nquickly query the venation information of courses.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 07:21:56 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Yu", "Xiaobing", ""], ["Stahr", "Mike", ""], ["Chen", "Han", ""], ["Yan", "Runming", ""]]}, {"id": "2012.12795", "submitter": "Tom S\\\"uhr", "authors": "Meike Zehlike, Tom S\\\"uhr, Carlos Castillo", "title": "A Note on the Significance Adjustment for FA*IR with Two Protected\n  Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we provide an improvement of the significance adjustment from\nthe FA*IR algorithm of Zehlike et al., which did not work for very short\nrankings in combination with a low minimum proportion $p$ for the protected\ngroup. We show how the minimum number of protected candidates per ranking\nposition can be calculated exactly and provide a mapping from the continuous\nspace of significance levels ($\\alpha$) to a discrete space of tables, which\nallows us to find $\\alpha_c$ using a binary search heuristic.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 16:56:38 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Zehlike", "Meike", ""], ["S\u00fchr", "Tom", ""], ["Castillo", "Carlos", ""]]}, {"id": "2012.12862", "submitter": "G\\\"okhan \\c{C}apan", "authors": "G\\\"okhan \\c{C}apan, \\\"Ozge Bozal, \\.Ilker G\\\"undo\\u{g}du, Ali Taylan\n  Cemgil", "title": "Towards Fair Personalization by Avoiding Feedback Loops", "comments": "NeurIPS 2019 Workshop on Human-Centric Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-reinforcing feedback loops are both cause and effect of over and/or\nunder-presentation of some content in interactive recommender systems. This\nleads to erroneous user preference estimates, namely, overestimation of\nover-presented content while violating the right to be presented of each\nalternative, contrary of which we define as a fair system. We consider two\nmodels that explicitly incorporate, or ignore the systematic and limited\nexposure to alternatives. By simulations, we demonstrate that ignoring the\nsystematic presentations overestimates promoted options and underestimates\ncensored alternatives. Simply conditioning on the limited exposure is a remedy\nfor these biases.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 19:28:57 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["\u00c7apan", "G\u00f6khan", ""], ["Bozal", "\u00d6zge", ""], ["G\u00fcndo\u011fdu", "\u0130lker", ""], ["Cemgil", "Ali Taylan", ""]]}, {"id": "2012.13023", "submitter": "Nurendra Choudhary", "authors": "Nurendra Choudhary, Nikhil Rao, Sumeet Katariya, Karthik Subbian,\n  Chandan K. Reddy", "title": "Self-Supervised Hyperboloid Representations from Logical Queries over\n  Knowledge Graphs", "comments": "Accepted at the Web Conference 2021 (WWW '21)", "journal-ref": null, "doi": "10.1145/3442381.3449974", "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge Graphs (KGs) are ubiquitous structures for information storagein\nseveral real-world applications such as web search, e-commerce, social\nnetworks, and biology. Querying KGs remains a foundational and challenging\nproblem due to their size and complexity. Promising approaches to tackle this\nproblem include embedding the KG units (e.g., entities and relations) in a\nEuclidean space such that the query embedding contains the information relevant\nto its results. These approaches, however, fail to capture the hierarchical\nnature and semantic information of the entities present in the graph.\nAdditionally, most of these approaches only utilize multi-hop queries (that can\nbe modeled by simple translation operations) to learn embeddings and ignore\nmore complex operations such as intersection and union of simpler queries. To\ntackle such complex operations, in this paper, we formulate KG representation\nlearning as a self-supervised logical query reasoning problem that utilizes\ntranslation, intersection and union queries over KGs. We propose Hyperboloid\nEmbeddings (HypE), a novel self-supervised dynamic reasoning framework, that\nutilizes positive first-order existential queries on a KG to learn\nrepresentations of its entities and relations as hyperboloids in a Poincar\\'e\nball. HypE models the positive first-order queries as geometrical translation,\nintersection, and union. For the problem of KG reasoning in real-world\ndatasets, the proposed HypE model significantly outperforms the state-of-the\nart results. We also apply HypE to an anomaly detection task on a popular\ne-commerce website product taxonomy as well as hierarchically organized web\narticles and demonstrate significant performance improvements compared to\nexisting baseline methods. Finally, we also visualize the learned HypE\nembeddings in a Poincar\\'e ball to clearly interpret and comprehend the\nrepresentation space.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 23:19:00 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 02:17:31 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 19:23:14 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Choudhary", "Nurendra", ""], ["Rao", "Nikhil", ""], ["Katariya", "Sumeet", ""], ["Subbian", "Karthik", ""], ["Reddy", "Chandan K.", ""]]}, {"id": "2012.13102", "submitter": "Yunqiu Shao", "authors": "Yunqiu Shao, Bulou Liu, Jiaxin Mao, Yiqun Liu, Min Zhang, Shaoping Ma", "title": "THUIR@COLIEE-2020: Leveraging Semantic Understanding and Exact Matching\n  for Legal Case Retrieval and Entailment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present our methodologies for tackling the challenges of\nlegal case retrieval and entailment in the Competition on Legal Information\nExtraction / Entailment 2020 (COLIEE-2020). We participated in the two case law\ntasks, i.e., the legal case retrieval task and the legal case entailment task.\nTask 1 (the retrieval task) aims to automatically identify supporting cases\nfrom the case law corpus given a new case, and Task 2 (the entailment task) to\nidentify specific paragraphs that entail the decision of a new case in a\nrelevant case. In both tasks, we employed the neural models for semantic\nunderstanding and the traditional retrieval models for exact matching. As a\nresult, our team (TLIR) ranked 2nd among all of the teams in Task 1 and 3rd\namong teams in Task 2. Experimental results suggest that combing models of\nsemantic understanding and exact matching benefits the legal case retrieval\ntask while the legal case entailment task relies more on semantic\nunderstanding.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 04:59:45 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Shao", "Yunqiu", ""], ["Liu", "Bulou", ""], ["Mao", "Jiaxin", ""], ["Liu", "Yiqun", ""], ["Zhang", "Min", ""], ["Ma", "Shaoping", ""]]}, {"id": "2012.13114", "submitter": "Daniela Vianna", "authors": "Daniela Vianna, Am\\'elie Marian", "title": "A Frequency-Based Learning-To-Rank Approach for Personal Digital Traces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personal digital traces are constantly produced by connected devices,\ninternet services and interactions. These digital traces are typically small,\nheterogeneous and stored in various locations in the cloud or on local devices,\nmaking it a challenge for users to interact with and search their own data. By\nadopting a multidimensional data model based on the six natural questions --\nwhat, when, where, who, why and how -- to represent and unify heterogeneous\npersonal digital traces, we can propose a learning-to-rank approach using the\nstate of the art LambdaMART algorithm and frequency-based features that\nleverage the correlation between content (what), users (who), time (when),\nlocation (where) and data source (how) to improve the accuracy of search\nresults. Due to the lack of publicly available personal training data, a\ncombination of known-item query generation techniques and an unsupervised\nranking model (field-based BM25) is used to build our own training sets.\nExperiments performed over a publicly available email collection and a personal\ndigital data trace collection from a real user show that the frequency-based\nlearning approach improves search accuracy when compared with traditional\nsearch tools.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 05:24:10 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Vianna", "Daniela", ""], ["Marian", "Am\u00e9lie", ""]]}, {"id": "2012.13245", "submitter": "Yong Liu Stephen", "authors": "Qinxu Ding, Yong Liu, Chunyan Miao, Fei Cheng, Haihong Tang", "title": "A Hybrid Bandit Framework for Diversified Recommendation", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The interactive recommender systems involve users in the recommendation\nprocedure by receiving timely user feedback to update the recommendation\npolicy. Therefore, they are widely used in real application scenarios. Previous\ninteractive recommendation methods primarily focus on learning users'\npersonalized preferences on the relevance properties of an item set. However,\nthe investigation of users' personalized preferences on the diversity\nproperties of an item set is usually ignored. To overcome this problem, we\npropose the Linear Modular Dispersion Bandit (LMDB) framework, which is an\nonline learning setting for optimizing a combination of modular functions and\ndispersion functions. Specifically, LMDB employs modular functions to model the\nrelevance properties of each item, and dispersion functions to describe the\ndiversity properties of an item set. Moreover, we also develop a learning\nalgorithm, called Linear Modular Dispersion Hybrid (LMDH) to solve the LMDB\nproblem and derive a gap-free bound on its n-step regret. Extensive experiments\non real datasets are performed to demonstrate the effectiveness of the proposed\nLMDB framework in balancing the recommendation accuracy and diversity.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 13:24:40 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Ding", "Qinxu", ""], ["Liu", "Yong", ""], ["Miao", "Chunyan", ""], ["Cheng", "Fei", ""], ["Tang", "Haihong", ""]]}, {"id": "2012.13292", "submitter": "Md Mustafizur Rahman", "authors": "Md Mustafizur Rahman, Mucahid Kutlu, Matthew Lease", "title": "Understanding and Predicting the Characteristics of Test Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Shared-task campaigns such as NIST TREC select documents to judge by pooling\nrankings from many participant systems. Therefore, the quality of the test\ncollection greatly depends on the number of participants and the quality of\nsubmitted runs. In this work, we investigate i) how the number of participants,\ncoupled with other factors, affects the quality of a test collection; and ii)\nwhether the quality of a test collection can be inferred prior to collecting\nrelevance judgments. Experiments on six TREC collections demonstrate that the\nrequired number of participants to construct a high-quality test collection\nvaries significantly across different test collections due to a variety of\nfactors. Furthermore, results suggest that the quality of test collections can\nbe predicted.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 15:26:52 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Rahman", "Md Mustafizur", ""], ["Kutlu", "Mucahid", ""], ["Lease", "Matthew", ""]]}, {"id": "2012.13529", "submitter": "Xuejiao Zhao", "authors": "Xuejiao Zhao, Huanhuan Chen, Zhenchang Xing, Chunyan Miao", "title": "Brain-inspired Search Engine Assistant based on Knowledge Graph", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines can quickly response a hyperlink list according to query\nkeywords. However, when a query is complex, developers need to repeatedly\nrefine the search keywords and open a large number of web pages to find and\nsummarize answers. Many research works of question and answering (Q and A)\nsystem attempt to assist search engines by providing simple, accurate and\nunderstandable answers. However, without original semantic contexts, these\nanswers lack explainability, making them difficult for users to trust and\nadopt. In this paper, a brain-inspired search engine assistant named\nDeveloperBot based on knowledge graph is proposed, which aligns to the\ncognitive process of human and has the capacity to answer complex queries with\nexplainability. Specifically, DeveloperBot firstly constructs a multi-layer\nquery graph by splitting a complex multi-constraint query into several ordered\nconstraints. Then it models the constraint reasoning process as subgraph search\nprocess inspired by the spreading activation model of cognitive science. In the\nend, novel features of the subgraph will be extracted for decision-making. The\ncorresponding reasoning subgraph and answer confidence will be derived as\nexplanations. The results of the decision-making demonstrate that DeveloperBot\ncan estimate the answers and answer confidences with high accuracy. We\nimplement a prototype and conduct a user study to evaluate whether and how the\ndirect answers and the explanations provided by DeveloperBot can assist\ndevelopers' information needs.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 06:36:11 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhao", "Xuejiao", ""], ["Chen", "Huanhuan", ""], ["Xing", "Zhenchang", ""], ["Miao", "Chunyan", ""]]}, {"id": "2012.13538", "submitter": "Yibing Zhan", "authors": "Jun Yu, Hao Zhou, Yibing Zhan, Dacheng Tao", "title": "Comprehensive Graph-conditional Similarity Preserving Network for\n  Unsupervised Cross-modal Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Unsupervised cross-modal hashing (UCMH) has become a hot topic recently.\nCurrent UCMH focuses on exploring data similarities. However, current UCMH\nmethods calculate the similarity between two data, mainly relying on the two\ndata's cross-modal features. These methods suffer from inaccurate similarity\nproblems that result in a suboptimal retrieval Hamming space, because the\ncross-modal features between the data are not sufficient to describe the\ncomplex data relationships, such as situations where two data have different\nfeature representations but share the inherent concepts. In this paper, we\ndevise a deep graph-neighbor coherence preserving network (DGCPN).\nSpecifically, DGCPN stems from graph models and explores graph-neighbor\ncoherence by consolidating the information between data and their neighbors.\nDGCPN regulates comprehensive similarity preserving losses by exploiting three\ntypes of data similarities (i.e., the graph-neighbor coherence, the coexistent\nsimilarity, and the intra- and inter-modality consistency) and designs a\nhalf-real and half-binary optimization strategy to reduce the quantization\nerrors during hashing. Essentially, DGCPN addresses the inaccurate similarity\nproblem by exploring and exploiting the data's intrinsic relationships in a\ngraph. We conduct extensive experiments on three public UCMH datasets. The\nexperimental results demonstrate the superiority of DGCPN, e.g., by improving\nthe mean average precision from 0.722 to 0.751 on MIRFlickr-25K using 64-bit\nhashing codes to retrieve texts from images. We will release the source code\npackage and the trained model on https://github.com/Atmegal/DGCPN.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 07:40:59 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Yu", "Jun", ""], ["Zhou", "Hao", ""], ["Zhan", "Yibing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2012.13569", "submitter": "Yan Gao", "authors": "Yan Gao, Jiafeng Guo, Yanyan Lan, Huaming Liao", "title": "Dynamic-K Recommendation with Personalized Decision Boundary", "comments": "12 pages", "journal-ref": "CCIR 2017", "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the recommendation task in the most common\nscenario with implicit feedback (e.g., clicks, purchases). State-of-the-art\nmethods in this direction usually cast the problem as to learn a personalized\nranking on a set of items (e.g., webpages, products). The top-N results are\nthen provided to users as recommendations, where the N is usually a fixed\nnumber pre-defined by the system according to some heuristic criteria (e.g.,\npage size, screen size). There is one major assumption underlying this\nfixed-number recommendation scheme, i.e., there are always sufficient relevant\nitems to users' preferences. Unfortunately, this assumption may not always hold\nin real-world scenarios. In some applications, there might be very limited\ncandidate items to recommend, and some users may have very high relevance\nrequirement in recommendation. In this way, even the top-1 ranked item may not\nbe relevant to a user's preference. Therefore, we argue that it is critical to\nprovide a dynamic-K recommendation, where the K should be different with\nrespect to the candidate item set and the target user. We formulate this\ndynamic-K recommendation task as a joint learning problem with both ranking and\nclassification objectives. The ranking objective is the same as existing\nmethods, i.e., to create a ranking list of items according to users' interests.\nThe classification objective is unique in this work, which aims to learn a\npersonalized decision boundary to differentiate the relevant items from\nirrelevant items. Based on these ideas, we extend two state-of-the-art\nranking-based recommendation methods, i.e., BPRMF and HRM, to the corresponding\ndynamic-K versions, namely DK-BPRMF and DK-HRM. Our experimental results on two\ndatasets show that the dynamic-K models are more effective than the original\nfixed-N recommendation methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 13:02:57 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Gao", "Yan", ""], ["Guo", "Jiafeng", ""], ["Lan", "Yanyan", ""], ["Liao", "Huaming", ""]]}, {"id": "2012.13915", "submitter": "Zhuosheng Zhang", "authors": "Zhuosheng Zhang, Yuwei Wu, Junru Zhou, Sufeng Duan, Hai Zhao, Rui Wang", "title": "SG-Net: Syntax Guided Transformer for Language Representation", "comments": "The early version accepted by IEEE Transactions on Pattern Analysis\n  and Machine Intelligence (TPAMI). Journal extension of arXiv:1908.05147 (AAAI\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding human language is one of the key themes of artificial\nintelligence. For language representation, the capacity of effectively modeling\nthe linguistic knowledge from the detail-riddled and lengthy texts and getting\nrid of the noises is essential to improve its performance. Traditional\nattentive models attend to all words without explicit constraint, which results\nin inaccurate concentration on some dispensable words. In this work, we propose\nusing syntax to guide the text modeling by incorporating explicit syntactic\nconstraints into attention mechanisms for better linguistically motivated word\nrepresentations. In detail, for self-attention network (SAN) sponsored\nTransformer-based encoder, we introduce syntactic dependency of interest (SDOI)\ndesign into the SAN to form an SDOI-SAN with syntax-guided self-attention.\nSyntax-guided network (SG-Net) is then composed of this extra SDOI-SAN and the\nSAN from the original Transformer encoder through a dual contextual\narchitecture for better linguistics inspired representation. The proposed\nSG-Net is applied to typical Transformer encoders. Extensive experiments on\npopular benchmark tasks, including machine reading comprehension, natural\nlanguage inference, and neural machine translation show the effectiveness of\nthe proposed SG-Net design.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 11:09:35 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 05:48:45 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Zhang", "Zhuosheng", ""], ["Wu", "Yuwei", ""], ["Zhou", "Junru", ""], ["Duan", "Sufeng", ""], ["Zhao", "Hai", ""], ["Wang", "Rui", ""]]}, {"id": "2012.13919", "submitter": "Julian Risch", "authors": "Julian Risch, Nicolas Alder, Christoph Hewel, Ralf Krestel", "title": "PatentMatch: A Dataset for Matching Patent Claims & Prior Art", "comments": "https://hpi.de/naumann/s/patentmatch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patent examiners need to solve a complex information retrieval task when they\nassess the novelty and inventive step of claims made in a patent application.\nGiven a claim, they search for prior art, which comprises all relevant publicly\navailable information. This time-consuming task requires a deep understanding\nof the respective technical domain and the patent-domain-specific language. For\nthese reasons, we address the computer-assisted search for prior art by\ncreating a training dataset for supervised machine learning called PatentMatch.\nIt contains pairs of claims from patent applications and semantically\ncorresponding text passages of different degrees from cited patent documents.\nEach pair has been labeled by technically-skilled patent examiners from the\nEuropean Patent Office. Accordingly, the label indicates the degree of semantic\ncorrespondence (matching), i.e., whether the text passage is prejudicial to the\nnovelty of the claimed invention or not. Preliminary experiments using a\nbaseline system show that PatentMatch can indeed be used for training a binary\ntext pair classifier on this challenging information retrieval task. The\ndataset is available online: https://hpi.de/naumann/s/patentmatch.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 11:22:25 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Risch", "Julian", ""], ["Alder", "Nicolas", ""], ["Hewel", "Christoph", ""], ["Krestel", "Ralf", ""]]}, {"id": "2012.13980", "submitter": "Tatiana Kozitsina", "authors": "Tatiana Kozitsina (Babkina), Viacheslav Goiko, Roman Palkin, Valentin\n  Khomutenko, Yulia Mundrievskaya, Maria Sukhareva, Isak Froumin, and Mikhail\n  Myagkov", "title": "Measuring University Impact: Wikipedia approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of Universities on the social, economic and political landscape is\none of the key directions in contemporary educational evaluation. In this\npaper, we discuss the new methodological technique that evaluates the impact of\nuniversity based on popularity (number of page-views) of their alumni's pages\non Wikipedia. It allows revealing the alumni popularity dynamics and tracking\nits state. Preliminary analysis shows that the number of page-views is higher\nfor the contemporary persons that prove the perspectives of this approach.\nThen, universities were ranked based on the methodology and compared to the\nfamous international university rankings ARWU and QS based only on alumni\nscales: for the top 10 universities, there is an intersection of two\nuniversities (Columbia University, Stanford University). The correlation\ncoefficients between different university rankings are provided in the paper.\nFinally, the ranking based on the alumni popularity was compared with the\nranking of universities based on the popularity of their webpages on Wikipedia:\nthere is a strong connection between these indicators.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 17:41:56 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Kozitsina", "Tatiana", "", "Babkina"], ["Goiko", "Viacheslav", ""], ["Palkin", "Roman", ""], ["Khomutenko", "Valentin", ""], ["Mundrievskaya", "Yulia", ""], ["Sukhareva", "Maria", ""], ["Froumin", "Isak", ""], ["Myagkov", "Mikhail", ""]]}, {"id": "2012.13990", "submitter": "Mitsuo Yoshida", "authors": "Kenshin Sekimoto, Yoshifumi Seki, Mitsuo Yoshida, Kyoji Umemura", "title": "The metrics of keywords to understand the difference between Retweet and\n  Like in each category", "comments": "The 2020 IEEE/WIC/ACM International Joint Conference on Web\n  Intelligence and Intelligent Agent Technology (WI-IAT '20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to clarify what kind of news is easily retweeted\nand what kind of news is easily Liked. We believe these actions, retweeting and\nLiking, have different meanings for users. Understanding this difference is\nimportant for understanding people's interest in Twitter. To analyze the\ndifference between retweets (RT) and Likes on Twitter in detail, we focus on\nword appearances in news titles. First, we calculate basic statistics and\nconfirm that tweets containing news URLs have different RT and Like tendencies\ncompared to other tweets. Next, we compared RTs and Likes for each category and\nconfirmed that the tendency of categories is different. Therefore, we propose\nmetrics for clarifying the differences in each action for each category used in\nthe $\\chi$-square test in order to perform an analysis focusing on the topic.\nThe proposed metrics are more useful than simple counts and TF-IDF for\nextracting meaningful words to understand the difference between RTs and Likes.\nWe analyzed each category using the proposed metrics and quantitatively\nconfirmed that the difference in the role of retweeting and Liking appeared in\nthe content depending on the category. Moreover, by aggregating tweets\nchronologically, the results showed the trend of RT and Like as a list of words\nand clarified how the characteristic words of each week were related to current\nevents for retweeting and Liking.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 18:32:19 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Sekimoto", "Kenshin", ""], ["Seki", "Yoshifumi", ""], ["Yoshida", "Mitsuo", ""], ["Umemura", "Kyoji", ""]]}, {"id": "2012.13992", "submitter": "Mitsuo Yoshida", "authors": "Ryosuke Homma, Yoshifumi Seki, Mitsuo Yoshida, Kyoji Umemura", "title": "Analysis of Short Dwell Time in Relation to User Interest in a News\n  Application", "comments": "The 2020 IEEE/WIC/ACM International Joint Conference on Web\n  Intelligence and Intelligent Agent Technology (WI-IAT '20), Best in Practice\n  Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dwell time has been widely used in various fields to evaluate content quality\nand user engagement. Although many studies shown that content with long dwell\ntime is good quality, contents with short dwell time have not been discussed in\ndetail. We hypothesize that content with short dwell time is not always low\nquality and does not always have low user engagement, but is instead related to\nuser interest. The purpose of this study is to clarify the meanings of short\ndwell time browsing in mobile news application. First, we analyze the relation\nof short dwell time to user interest using large scale user behavior logs from\na mobile news application. This analysis was conducted on a vector space based\non users click histories and then users and articles were mapped in the same\nspace. The users with short dwell time are concentrated on a specific position\nin this space; thus, the length of dwell time is related to their interest.\nMoreover, we also analyze the characteristics of short dwell time browsing by\nexcluding these browses from their click histories. Surprisingly, excluding\nshort dwell time click history, it was found that short dwell time click\nhistory included some aspect of user interest in 30.87% of instances where the\ncluster of users changed. These findings demonstrate that short dwell time does\nnot always indicate a low level of user engagement, but also level of user\ninterest.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 18:36:52 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Homma", "Ryosuke", ""], ["Seki", "Yoshifumi", ""], ["Yoshida", "Mitsuo", ""], ["Umemura", "Kyoji", ""]]}, {"id": "2012.14005", "submitter": "Cheng Tang", "authors": "Cheng Tang, Andrew Arnold", "title": "Neural document expansion for ad-hoc information retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, Nogueira et al. [2019] proposed a new approach to document\nexpansion based on a neural Seq2Seq model, showing significant improvement on\nshort text retrieval task. However, this approach needs a large amount of\nin-domain training data. In this paper, we show that this neural document\nexpansion approach can be effectively adapted to standard IR tasks, where\nlabels are scarce and many long documents are present.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 20:00:08 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Tang", "Cheng", ""], ["Arnold", "Andrew", ""]]}, {"id": "2012.14164", "submitter": "Martin Andrews", "authors": "Yew Ken Chia and Sam Witteveen and Martin Andrews", "title": "Red Dragon AI at TextGraphs 2020 Shared Task: LIT : LSTM-Interleaved\n  Transformer for Multi-Hop Explanation Ranking", "comments": "Accepted paper for TextGraphs-14 workshop at COLING 2020. (6 pages\n  including references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable question answering for science questions is a challenging task\nthat requires multi-hop inference over a large set of fact sentences. To\ncounter the limitations of methods that view each query-document pair in\nisolation, we propose the LSTM-Interleaved Transformer which incorporates\ncross-document interactions for improved multi-hop ranking. The LIT\narchitecture can leverage prior ranking positions in the re-ranking setting.\nOur model is competitive on the current leaderboard for the TextGraphs 2020\nshared task, achieving a test-set MAP of 0.5607, and would have gained third\nplace had we submitted before the competition deadline. Our code implementation\nis made available at\nhttps://github.com/mdda/worldtree_corpus/tree/textgraphs_2020\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 09:54:00 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Chia", "Yew Ken", ""], ["Witteveen", "Sam", ""], ["Andrews", "Martin", ""]]}, {"id": "2012.14210", "submitter": "Nils Reimers", "authors": "Nils Reimers and Iryna Gurevych", "title": "The Curse of Dense Low-Dimensional Information Retrieval for Large Index\n  Sizes", "comments": "Published at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Information Retrieval using dense low-dimensional representations recently\nbecame popular and showed out-performance to traditional sparse-representations\nlike BM25. However, no previous work investigated how dense representations\nperform with large index sizes. We show theoretically and empirically that the\nperformance for dense representations decreases quicker than sparse\nrepresentations for increasing index sizes. In extreme cases, this can even\nlead to a tipping point where at a certain index size sparse representations\noutperform dense representations. We show that this behavior is tightly\nconnected to the number of dimensions of the representations: The lower the\ndimension, the higher the chance for false positives, i.e. returning irrelevant\ndocuments.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 12:25:25 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 07:25:23 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Reimers", "Nils", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2012.14234", "submitter": "Bowen Hao", "authors": "Bowen Hao, Jing Zhang, Cuiping Li, Hong Chen, Hongzhi Yin", "title": "Recommending Courses in MOOCs for Jobs: An Auto Weak Supervision\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of massive open online courses (MOOCs) demands an effective\nway of course recommendation for jobs posted in recruitment websites,\nespecially for the people who take MOOCs to find new jobs. Despite the advances\nof supervised ranking models, the lack of enough supervised signals prevents us\nfrom directly learning a supervised ranking model. This paper proposes a\ngeneral automated weak supervision framework AutoWeakS via reinforcement\nlearning to solve the problem. On the one hand, the framework enables training\nmultiple supervised ranking models upon the pseudo labels produced by multiple\nunsupervised ranking models. On the other hand, the framework enables\nautomatically searching the optimal combination of these supervised and\nunsupervised models. Systematically, we evaluate the proposed model on several\ndatasets of jobs from different recruitment websites and courses from a MOOCs\nplatform. Experiments show that our model significantly outperforms the\nclassical unsupervised, supervised and weak supervision baselines.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 14:03:18 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Hao", "Bowen", ""], ["Zhang", "Jing", ""], ["Li", "Cuiping", ""], ["Chen", "Hong", ""], ["Yin", "Hongzhi", ""]]}, {"id": "2012.14541", "submitter": "Matan Orbach", "authors": "Matan Orbach, Orith Toledo-Ronen, Artem Spector, Ranit Aharonov, Yoav\n  Katz and Noam Slonim", "title": "YASO: A New Benchmark for Targeted Sentiment Analysis", "comments": "For the associated TSA corpus, see\n  https://www.research.ibm.com/haifa/dept/vst/debating_data.shtml#Targeted%20Sentiment%20Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis research has shifted over the years from the analysis of\nfull documents or single sentences to a finer-level of detail -- identifying\nthe sentiment towards single words or phrases -- with the task of Targeted\nSentiment Analysis (TSA). While this problem is attracting a plethora of works\nfocusing on algorithmic aspects, they are typically evaluated on a selection\nfrom a handful of datasets, and little effort, if any, is dedicated to the\nexpansion of the available evaluation data. In this work, we present YASO -- a\nnew crowd-sourced TSA evaluation dataset, collected using a new annotation\nscheme for labeling targets and their sentiments. The dataset contains 2,215\nEnglish sentences from movie, business and product reviews, and 7,415 terms and\ntheir corresponding sentiments annotated within these sentences. Our analysis\nverifies the reliability of our annotations, and explores the characteristics\nof the collected data. Lastly, benchmark results using five contemporary TSA\nsystems lay the foundation for future work, and show there is ample room for\nimprovement on this challenging new dataset.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 00:25:15 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Orbach", "Matan", ""], ["Toledo-Ronen", "Orith", ""], ["Spector", "Artem", ""], ["Aharonov", "Ranit", ""], ["Katz", "Yoav", ""], ["Slonim", "Noam", ""]]}, {"id": "2012.14700", "submitter": "Sangwoong Yoon", "authors": "Sangwoong Yoon, Woo Young Kang, Sungwook Jeon, SeongEun Lee, Changjin\n  Han, Jonghun Park, Eun-Sol Kim", "title": "Image-to-Image Retrieval by Learning Similarity between Scene Graphs", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As a scene graph compactly summarizes the high-level content of an image in a\nstructured and symbolic manner, the similarity between scene graphs of two\nimages reflects the relevance of their contents. Based on this idea, we propose\na novel approach for image-to-image retrieval using scene graph similarity\nmeasured by graph neural networks. In our approach, graph neural networks are\ntrained to predict the proxy image relevance measure, computed from\nhuman-annotated captions using a pre-trained sentence similarity model. We\ncollect and publish the dataset for image relevance measured by human\nannotators to evaluate retrieval algorithms. The collected dataset shows that\nour method agrees well with the human perception of image similarity than other\ncompetitive baselines.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 10:45:20 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Yoon", "Sangwoong", ""], ["Kang", "Woo Young", ""], ["Jeon", "Sungwook", ""], ["Lee", "SeongEun", ""], ["Han", "Changjin", ""], ["Park", "Jonghun", ""], ["Kim", "Eun-Sol", ""]]}, {"id": "2012.14770", "submitter": "Lifang Deng", "authors": "Lifang Deng, Jin Niu, Angulia Yang, Qidi Xu, Xiang Fu, Jiandong Zhang,\n  Anxiang Zeng", "title": "Hybrid Interest Modeling for Long-tailed Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User behavior modeling is a key technique for recommender systems. However,\nmost methods focus on head users with large-scale interactions and hence suffer\nfrom data sparsity issues. Several solutions integrate side information such as\ndemographic features and product reviews, another is to transfer knowledge from\nother rich data sources. We argue that current methods are limited by the\nstrict privacy policy and have low scalability in real-world applications and\nfew works consider the behavioral characteristics behind long-tailed users. In\nthis work, we propose the Hybrid Interest Modeling (HIM) network to hybrid both\npersonalized interest and semi-personalized interest in learning long-tailed\nusers' preferences in the recommendation. To achieve this, we first design the\nUser Behavior Pyramid (UBP) module to capture the fine-grained personalized\ninterest of high confidence from sparse even noisy positive feedbacks.\nMoreover, the individual interaction is too sparse and not enough for modeling\nuser interest adequately, we design the User Behavior Clustering (UBC) module\nto learn latent user interest groups with self-supervised learning mechanism\nnovelly, which capture coarse-grained semi-personalized interest from\ngroup-item interaction data. Extensive experiments on both public and\nindustrial datasets verify the superiority of HIM compared with the\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 14:33:01 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Deng", "Lifang", ""], ["Niu", "Jin", ""], ["Yang", "Angulia", ""], ["Xu", "Qidi", ""], ["Fu", "Xiang", ""], ["Zhang", "Jiandong", ""], ["Zeng", "Anxiang", ""]]}, {"id": "2012.14774", "submitter": "Yumo Xu", "authors": "Yumo Xu and Mirella Lapata", "title": "Generating Query Focused Summaries from Query-Free Resources", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large-scale datasets has driven the development of neural\nmodels that create generic summaries from single or multiple documents. In this\nwork we consider query focused summarization (QFS), a task for which training\ndata in the form of queries, documents, and summaries is not readily available.\nWe propose to decompose QFS into (1) query modeling (i.e., finding supportive\nevidence within a set of documents for a query) and (2) conditional language\nmodeling (i.e., summary generation). We introduce MaRGE, a Masked ROUGE\nRegression framework for evidence estimation and ranking which relies on a\nunified representation for summaries and queries, so that summaries in generic\ndata can be converted into proxy queries for learning a query model.\nExperiments across QFS benchmarks and query types show that our model achieves\nstate-of-the-art performance despite learning from weak supervision.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 14:39:35 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 21:34:37 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Xu", "Yumo", ""], ["Lapata", "Mirella", ""]]}, {"id": "2012.14803", "submitter": "Varvara Kalokyri", "authors": "Varvara Kalokyri, Alexander Borgida, Am\\'elie Marian", "title": "Supporting Human Memory by Reconstructing Personal Episodic Narratives\n  from Digital Traces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerous applications capture in digital form aspects of people's lives. The\nresulting data, which we call Personal Digital Traces - PDTs, can be used to\nhelp reconstruct people's episodic memories and connect to their past personal\nevents. This reconstruction has several applications, from helping patients\nwith neurodegenerative diseases recall past events to gathering clues from\nmultiple sources to identify recent contacts and places visited - a critical\nnew application for the current health crisis. This paper takes steps towards\nintegrating, connecting and summarizing the heterogeneous collection of data\ninto episodic narratives using scripts - prototypical plans for everyday\nactivities. Specifically, we propose a matching algorithm that groups several\ndigital traces from many different sources into script instances (episodes),\nand we provide a technique for ranking the likelihood of candidate episodes. We\nreport on the results of a study based on the personal data of real users,\nwhich gives evidence that our episode reconstruction technique 1) successfully\nintegrates and combines traces from different sources into coherent episodes,\nand 2) augments users' memory of their past actions.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 15:31:52 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Kalokyri", "Varvara", ""], ["Borgida", "Alexander", ""], ["Marian", "Am\u00e9lie", ""]]}, {"id": "2012.14862", "submitter": "Si Sun", "authors": "Si Sun, Yingzhuo Qian, Zhenghao Liu, Chenyan Xiong, Kaitao Zhang, Jie\n  Bao, Zhiyuan Liu and Paul Bennett", "title": "Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision", "comments": "14 pages, accepted by ACL-IJCNLP 2021 (long paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of Neural Information Retrieval (Neu-IR) often depends on a\nlarge scale of in-domain relevance training signals, which are not always\navailable in real-world ranking scenarios. To democratize the benefits of\nNeu-IR, this paper presents MetaAdaptRank, a domain adaptive learning method\nthat generalizes Neu-IR models from label-rich source domains to few-shot\ntarget domains. Drawing on source-domain massive relevance supervision,\nMetaAdaptRank contrastively synthesizes a large number of weak supervision\nsignals for target domains and meta-learns to reweight these synthetic \"weak\"\ndata based on their benefits to the target-domain ranking accuracy of Neu-IR\nmodels. Experiments on three TREC benchmarks in the web, news, and biomedical\ndomains show that MetaAdaptRank significantly improves the few-shot ranking\naccuracy of Neu-IR models. Further analyses indicate that MetaAdaptRank thrives\nfrom both its contrastive weak data synthesis and meta-reweighted data\nselection. The code and data of this paper can be obtained from\nhttps://github.com/thunlp/MetaAdaptRank.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 17:28:53 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 16:35:46 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Sun", "Si", ""], ["Qian", "Yingzhuo", ""], ["Liu", "Zhenghao", ""], ["Xiong", "Chenyan", ""], ["Zhang", "Kaitao", ""], ["Bao", "Jie", ""], ["Liu", "Zhiyuan", ""], ["Bennett", "Paul", ""]]}, {"id": "2012.14978", "submitter": "Chunyuan Li", "authors": "Jiaxin Huang, Chunyuan Li, Krishan Subudhi, Damien Jose, Shobana\n  Balakrishnan, Weizhu Chen, Baolin Peng, Jianfeng Gao, Jiawei Han", "title": "Few-Shot Named Entity Recognition: A Comprehensive Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a comprehensive study to efficiently build named entity\nrecognition (NER) systems when a small number of in-domain labeled data is\navailable. Based upon recent Transformer-based self-supervised pre-trained\nlanguage models (PLMs), we investigate three orthogonal schemes to improve the\nmodel generalization ability for few-shot settings: (1) meta-learning to\nconstruct prototypes for different entity types, (2) supervised pre-training on\nnoisy web data to extract entity-related generic representations and (3)\nself-training to leverage unlabeled in-domain data. Different combinations of\nthese schemes are also considered. We perform extensive empirical comparisons\non 10 public NER datasets with various proportions of labeled data, suggesting\nuseful insights for future research. Our experiments show that (i) in the\nfew-shot learning setting, the proposed NER schemes significantly improve or\noutperform the commonly used baseline, a PLM-based linear classifier fine-tuned\non domain labels; (ii) We create new state-of-the-art results on both few-shot\nand training-free settings compared with existing methods. We will release our\ncode and pre-trained models for reproducible research.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 23:43:16 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Huang", "Jiaxin", ""], ["Li", "Chunyuan", ""], ["Subudhi", "Krishan", ""], ["Jose", "Damien", ""], ["Balakrishnan", "Shobana", ""], ["Chen", "Weizhu", ""], ["Peng", "Baolin", ""], ["Gao", "Jianfeng", ""], ["Han", "Jiawei", ""]]}, {"id": "2012.15151", "submitter": "Andrew Collins Mr", "authors": "Andrew Collins, Laura Tierney, Joeran Beel", "title": "Per-Instance Algorithm Selection for Recommender Systems via Instance\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation algorithms perform differently if the users, recommendation\ncontexts, applications, and user interfaces vary even slightly. It is similarly\nobserved in other fields, such as combinatorial problem solving, that\nalgorithms perform differently for each instance presented. In those fields,\nmeta-learning is successfully used to predict an optimal algorithm for each\ninstance, to improve overall system performance. Per-instance algorithm\nselection has thus far been unsuccessful for recommender systems. In this paper\nwe propose a per-instance meta-learner that clusters data instances and\npredicts the best algorithm for unseen instances according to cluster\nmembership. We test our approach using 10 collaborative- and 4 content-based\nfiltering algorithms, for varying clustering parameters, and find a significant\nimprovement over the best performing base algorithm at alpha=0.053 (MAE: 0.7107\nvs LightGBM 0.7214; t-test). We also explore the performances of our base\nalgorithms on a ratings dataset and empirically show that the error of a\nperfect algorithm selector monotonically decreases for larger pools of\nalgorithm. To the best of our knowledge, this is the first effective\nmeta-learning technique for per-instance algorithm selection in recommender\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 13:35:57 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Collins", "Andrew", ""], ["Tierney", "Laura", ""], ["Beel", "Joeran", ""]]}, {"id": "2012.15192", "submitter": "Thanasis Vergoulis", "authors": "Thanasis Vergoulis, Ilias Kanellos, Giorgos Giannopoulos, Theodore\n  Dalamagas", "title": "Simplifying Impact Prediction for Scientific Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the expected impact of an article is valuable for various\napplications (e.g., article/cooperator recommendation). Most existing\napproaches attempt to predict the exact number of citations each article will\nreceive in the near future, however this is a difficult regression analysis\nproblem. Moreover, most approaches rely on the existence of rich metadata for\neach article, a requirement that cannot be adequately fulfilled for a large\nnumber of them. In this work, we take advantage of the fact that solving a\nsimpler machine learning problem, that of classifying articles based on their\nexpected impact, is adequate for many real world applications and we propose a\nsimplified model that can be trained using minimal article metadata. Finally,\nwe examine various configurations of this model and evaluate their\neffectiveness in solving the aforementioned classification problem.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 15:24:55 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Vergoulis", "Thanasis", ""], ["Kanellos", "Ilias", ""], ["Giannopoulos", "Giorgos", ""], ["Dalamagas", "Theodore", ""]]}, {"id": "2012.15518", "submitter": "Serhad Sarica", "authors": "Serhad Sarica, Jianxi Luo", "title": "Design Knowledge Representation with Technology Semantic Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Engineers often need to discover and learn designs from unfamiliar domains\nfor inspiration or other particular uses. However, the complexity of the\ntechnical design descriptions and the unfamiliarity to the domain make it hard\nfor engineers to comprehend the function, behavior, and structure of a design.\nTo help engineers quickly understand a complex technical design description new\nto them, one approach is to represent it as a network graph of the\ndesign-related entities and their relations as an abstract summary of the\ndesign. While graph or network visualizations are widely adopted in the\nengineering design literature, the challenge remains in retrieving the design\nentities and deriving their relations. In this paper, we propose a network\nmapping method that is powered by Technology Semantic Network (TechNet).\nThrough a case study, we showcase how TechNet's unique characteristic of being\ntrained on a large technology-related data source advantages itself over\ncommon-sense knowledge bases, such as WordNet and ConceptNet, for design\nknowledge representation.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 09:43:04 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Sarica", "Serhad", ""], ["Luo", "Jianxi", ""]]}, {"id": "2012.15728", "submitter": "Iyad Batal", "authors": "Iyad Batal and Akshay Soni", "title": "Multi-Channel Sequential Behavior Networks for User Modeling in Online\n  Advertising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple content providers rely on native advertisement for revenue by\nplacing ads within the organic content of their pages. We refer to this setting\nas ``queryless'' to differentiate from search advertisement where a user\nsubmits a search query and gets back related ads. Understanding user intent is\ncritical because relevant ads improve user experience and increase the\nlikelihood of delivering clicks that have value to our advertisers.\n  This paper presents Multi-Channel Sequential Behavior Network (MC-SBN), a\ndeep learning approach for embedding users and ads in a semantic space in which\nrelevance can be evaluated. Our proposed user encoder architecture summarizes\nuser activities from multiple input channels--such as previous search queries,\nvisited pages, or clicked ads--into a user vector. It uses multiple RNNs to\nencode sequences of event sessions from the different channels and then applies\nan attention mechanism to create the user representation. A key property of our\napproach is that user vectors can be maintained and updated incrementally,\nwhich makes it feasible to be deployed for large-scale serving. We conduct\nextensive experiments on real-world datasets. The results demonstrate that\nMC-SBN can improve the ranking of relevant ads and boost the performance of\nboth click prediction and conversion prediction in the queryless native\nadvertising setting.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 06:13:29 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Batal", "Iyad", ""], ["Soni", "Akshay", ""]]}, {"id": "2012.15843", "submitter": "Shabnam Daghaghi", "authors": "Shabnam Daghaghi, Tharun Medini, Nicholas Meisburger, Beidi Chen,\n  Mengnan Zhao, Anshumali Shrivastava", "title": "A Tale of Two Efficient and Informative Negative Sampling Distributions", "comments": "Published at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Softmax classifiers with a very large number of classes naturally occur in\nmany applications such as natural language processing and information\nretrieval. The calculation of full softmax is costly from the computational and\nenergy perspective. There have been various sampling approaches to overcome\nthis challenge, popularly known as negative sampling (NS). Ideally, NS should\nsample negative classes from a distribution that is dependent on the input\ndata, the current parameters, and the correct positive class. Unfortunately,\ndue to the dynamically updated parameters and data samples, there is no\nsampling scheme that is provably adaptive and samples the negative classes\nefficiently. Therefore, alternative heuristics like random sampling, static\nfrequency-based sampling, or learning-based biased sampling, which primarily\ntrade either the sampling cost or the adaptivity of samples per iteration are\nadopted. In this paper, we show two classes of distributions where the sampling\nscheme is truly adaptive and provably generates negative samples in\nnear-constant time. Our implementation in C++ on CPU is significantly superior,\nboth in terms of wall-clock time and accuracy, compared to the most optimized\nTensorFlow implementations of other popular negative sampling approaches on\npowerful NVIDIA V100 GPU.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:56:41 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 03:02:26 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Daghaghi", "Shabnam", ""], ["Medini", "Tharun", ""], ["Meisburger", "Nicholas", ""], ["Chen", "Beidi", ""], ["Zhao", "Mengnan", ""], ["Shrivastava", "Anshumali", ""]]}]