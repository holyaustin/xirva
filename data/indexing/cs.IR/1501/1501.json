[{"id": "1501.00311", "submitter": "Jun Ping Ng", "authors": "Jun-Ping Ng and Min-Yen Kan", "title": "QANUS: An Open-source Question-Answering Platform", "comments": "6 pages, 3 figures, demo paper describing QANUS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we motivate the need for a publicly available, generic\nsoftware framework for question-answering (QA) systems. We present an\nopen-source QA framework QANUS which researchers can leverage on to build new\nQA systems easily and rapidly. The framework implements much of the code that\nwill otherwise have been repeated across different QA systems. To demonstrate\nthe utility and practicality of the framework, we further present a fully\nfunctioning factoid QA system QA-SYS built on top of QANUS.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2015 20:51:25 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Ng", "Jun-Ping", ""], ["Kan", "Min-Yen", ""]]}, {"id": "1501.00677", "submitter": "Tao Zhou", "authors": "Jian Gao, Yu-Wei Dong, Mingsheng Shang, Shi-Min Cai, Tao Zhou", "title": "Group-based ranking method for online rating systems with spamming\n  attacks", "comments": "6 pages, 5 figures, 2 tables", "journal-ref": null, "doi": "10.1209/0295-5075/110/28003", "report-no": null, "categories": "cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking problem has attracted much attention in real systems. How to design a\nrobust ranking method is especially significant for online rating systems under\nthe threat of spamming attacks. By building reputation systems for users, many\nwell-performed ranking methods have been applied to address this issue. In this\nLetter, we propose a group-based ranking method that evaluates users'\nreputations based on their grouping behaviors. More specifically, users are\nassigned with high reputation scores if they always fall into large rating\ngroups. Results on three real data sets indicate that the present method is\nmore accurate and robust than correlation-based method in the presence of\nspamming attacks.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 13:45:47 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Gao", "Jian", ""], ["Dong", "Yu-Wei", ""], ["Shang", "Mingsheng", ""], ["Cai", "Shi-Min", ""], ["Zhou", "Tao", ""]]}, {"id": "1501.00744", "submitter": "Lanbo Zhang", "authors": "Lanbo Zhang", "title": "Identifying Relevant Document Facets for Keyword-Based Search Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As structured documents with rich metadata (such as products, movies, etc.)\nbecome increasingly prevalent, searching those documents has become an\nimportant IR problem. Although advanced search interfaces are widely available,\nmost users still prefer to use keyword-based queries to search those documents.\nQuery keywords often imply some hidden restrictions on the desired documents,\nwhich can be represented as document facet-value pairs. To achieve high\nretrieval performance, it's important to be able to identify the relevant\nfacet-value pairs hidden in a query. In this paper, we study the problem of\nidentifying document facet-value pairs that are relevant to a keyword-based\nsearch query. We propose a machine learning approach and a set of useful\nfeatures, and evaluate our approach using a movie data set from INEX.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 01:49:11 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Zhang", "Lanbo", ""]]}, {"id": "1501.01318", "submitter": "Aymen Abu-Errub Ph.D.", "authors": "Ashraf Odeh, Aymen Abu-Errub, Qusai Shambour and Nidal Turab", "title": "Arabic Text Categorization Algorithm using Vector Evaluation Method", "comments": null, "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 6, No 6, December 2014", "doi": "10.5121/ijcsit.2014.6606", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text categorization is the process of grouping documents into categories\nbased on their contents. This process is important to make information\nretrieval easier, and it became more important due to the huge textual\ninformation available online. The main problem in text categorization is how to\nimprove the classification accuracy. Although Arabic text categorization is a\nnew promising field, there are a few researches in this field. This paper\nproposes a new method for Arabic text categorization using vector evaluation.\nThe proposed method uses a categorized Arabic documents corpus, and then the\nweights of the tested document's words are calculated to determine the document\nkeywords which will be compared with the keywords of the corpus categorizes to\ndetermine the tested document's best category.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 21:10:26 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Odeh", "Ashraf", ""], ["Abu-Errub", "Aymen", ""], ["Shambour", "Qusai", ""], ["Turab", "Nidal", ""]]}, {"id": "1501.01386", "submitter": "Rafiullah Khan", "authors": "Misbah Daud, Rafiullah Khan, Mohibullah and Aitazaz Daud", "title": "Roman Urdu Opinion Mining System (RUOMiS)", "comments": "8 pages, 2 figures, 4 tables, Computer Science & Engineering: An\n  International Journal (CSEIJ)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convincing a customer is always considered as a challenging task in every\nbusiness. But when it comes to online business, this task becomes even more\ndifficult. Online retailers try everything possible to gain the trust of the\ncustomer. One of the solutions is to provide an area for existing users to\nleave their comments. This service can effectively develop the trust of the\ncustomer however normally the customer comments about the product in their\nnative language using Roman script. If there are hundreds of comments this\nmakes difficulty even for the native customers to make a buying decision. This\nresearch proposes a system which extracts the comments posted in Roman Urdu,\ntranslate them, find their polarity and then gives us the rating of the\nproduct. This rating will help the native and non-native customers to make\nbuying decision efficiently from the comments posted in Roman Urdu.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 08:04:32 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Daud", "Misbah", ""], ["Khan", "Rafiullah", ""], ["Mohibullah", "", ""], ["Daud", "Aitazaz", ""]]}, {"id": "1501.01996", "submitter": "Amin Javari", "authors": "Amin Javari, Mahdi Jalili", "title": "A probabilistic model to resolve diversity-accuracy challenge of\n  recommendation systems", "comments": "19 pages, 5 figures", "journal-ref": "Knowledge and Information Systems, 1-19 (2014)", "doi": "10.1007/s10115-014-0779-2", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems have wide-spread applications in both academia and\nindustry. Traditionally, performance of recommendation systems has been\nmeasured by their precision. By introducing novelty and diversity as key\nqualities in recommender systems, recently increasing attention has been\nfocused on this topic. Precision and novelty of recommendation are not in the\nsame direction, and practical systems should make a trade-off between these two\nquantities. Thus, it is an important feature of a recommender system to make it\npossible to adjust diversity and accuracy of the recommendations by tuning the\nmodel. In this paper, we introduce a probabilistic structure to resolve the\ndiversity-accuracy dilemma in recommender systems. We propose a hybrid model\nwith adjustable level of diversity and precision such that one can perform this\nby tuning a single parameter. The proposed recommendation model consists of two\nmodels: one for maximization of the accuracy and the other one for\nspecification of the recommendation list to tastes of users. Our experiments on\ntwo real datasets show the functionality of the model in resolving\naccuracy-diversity dilemma and outperformance of the model over other classic\nmodels. The proposed method could be extensively applied to real commercial\nsystems due to its low computational complexity and significant performance.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 22:42:39 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Javari", "Amin", ""], ["Jalili", "Mahdi", ""]]}, {"id": "1501.02031", "submitter": "EPTCS", "authors": "Juli\\'an Alarte (Universitat Polit\\`ecnica de Val\\`encia), David Insa\n  (Universitat Polit\\`ecnica de Val\\`encia), Josep Silva (Universitat\n  Polit\\`ecnica de Val\\`encia), Salvador Tamarit (Universidad Polit\\'ecnica de\n  Madrid)", "title": "Web Template Extraction Based on Hyperlink Analysis", "comments": "In Proceedings PROLE 2014, arXiv:1501.01693", "journal-ref": "EPTCS 173, 2015, pp. 16-26", "doi": "10.4204/EPTCS.173.2", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web templates are one of the main development resources for website\nengineers. Templates allow them to increase productivity by plugin content into\nalready formatted and prepared pagelets. For the final user templates are also\nuseful, because they provide uniformity and a common look and feel for all\nwebpages. However, from the point of view of crawlers and indexers, templates\nare an important problem, because templates usually contain irrelevant\ninformation such as advertisements, menus, and banners. Processing and storing\nthis information is likely to lead to a waste of resources (storage space,\nbandwidth, etc.). It has been measured that templates represent between 40% and\n50% of data on the Web. Therefore, identifying templates is essential for\nindexing tasks. In this work we propose a novel method for automatic template\nextraction that is based on similarity analysis between the DOM trees of a\ncollection of webpages that are detected using menus information. Our\nimplementation and experiments demonstrate the usefulness of the technique.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 03:59:36 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Alarte", "Juli\u00e1n", "", "Universitat Polit\u00e8cnica de Val\u00e8ncia"], ["Insa", "David", "", "Universitat Polit\u00e8cnica de Val\u00e8ncia"], ["Silva", "Josep", "", "Universitat\n  Polit\u00e8cnica de Val\u00e8ncia"], ["Tamarit", "Salvador", "", "Universidad Polit\u00e9cnica de\n  Madrid"]]}, {"id": "1501.02398", "submitter": "Denis Shestakov", "authors": "Denis Shestakov, Diana Moise", "title": "Scalable high-dimensional indexing and searching with Hadoop", "comments": "This paper has been withdrawn by the authors. The manuscript has been\n  withdrawn as having no new material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While high-dimensional search-by-similarity techniques reached their maturity\nand in overall provide good performance, most of them are unable to cope with\nvery large multimedia collections. The 'big data' challenge however has to be\naddressed as multimedia collections have been explosively growing and will grow\neven faster than ever within the next few years. Luckily, computational\nprocessing power has become more available to researchers due to easier access\nto distributed grid infrastructures. In this paper, we show how\nhigh-dimensional indexing and searching methods can be used on scientific grid\nenvironments and present a scalable workflow for indexing and searching over 30\nbillion SIFT descriptors using a cluster running Hadoop. Besides its\nscalability, the proposed scheme not only provides good search quality, but\nalso achieves a stable throughput of around 210ms per image when searching a\n100M image collection. Our findings could help other researchers and\npractitioners to cope with huge multimedia collections.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2015 22:05:45 GMT"}, {"version": "v2", "created": "Fri, 30 Jan 2015 00:56:17 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Shestakov", "Denis", ""], ["Moise", "Diana", ""]]}, {"id": "1501.02527", "submitter": "Nicholas Locascio", "authors": "Harini Suresh, Nicholas Locascio", "title": "Autodetection and Classification of Hidden Cultural City Districts from\n  Yelp Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models are a way to discover underlying themes in an otherwise\nunstructured collection of documents. In this study, we specifically used the\nLatent Dirichlet Allocation (LDA) topic model on a dataset of Yelp reviews to\nclassify restaurants based off of their reviews. Furthermore, we hypothesize\nthat within a city, restaurants can be grouped into similar \"clusters\" based on\nboth location and similarity. We used several different clustering methods,\nincluding K-means Clustering and a Probabilistic Mixture Model, in order to\nuncover and classify districts, both well-known and hidden (i.e. cultural areas\nlike Chinatown or hearsay like \"the best street for Italian restaurants\")\nwithin a city. We use these models to display and label different clusters on a\nmap. We also introduce a topic similarity heatmap that displays the similarity\ndistribution in a city to a new restaurant.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 03:10:01 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Suresh", "Harini", ""], ["Locascio", "Nicholas", ""]]}, {"id": "1501.02530", "submitter": "Anna Senina", "authors": "Anna Rohrbach, Marcus Rohrbach, Niket Tandon, Bernt Schiele", "title": "A Dataset for Movie Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Descriptive video service (DVS) provides linguistic descriptions of movies\nand allows visually impaired people to follow a movie along with their peers.\nSuch descriptions are by design mainly visual and thus naturally form an\ninteresting data source for computer vision and computational linguistics. In\nthis work we propose a novel dataset which contains transcribed DVS, which is\ntemporally aligned to full length HD movies. In addition we also collected the\naligned movie scripts which have been used in prior work and compare the two\ndifferent sources of descriptions. In total the Movie Description dataset\ncontains a parallel corpus of over 54,000 sentences and video snippets from 72\nHD movies. We characterize the dataset by benchmarking different approaches for\ngenerating video descriptions. Comparing DVS to scripts, we find that DVS is\nfar more visual and describes precisely what is shown rather than what should\nhappen according to the scripts created prior to movie production.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 03:31:33 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Rohrbach", "Anna", ""], ["Rohrbach", "Marcus", ""], ["Tandon", "Niket", ""], ["Schiele", "Bernt", ""]]}, {"id": "1501.02646", "submitter": "Philipp Mayr", "authors": "Philipp Mayr, Ingo Frommholz, Andrea Scharnhorst, Peter Mutschke", "title": "Bibliometric-enhanced Information Retrieval: 2nd International BIR\n  Workshop", "comments": "4 pages, 37th European Conference on Information Retrieval, BIR\n  workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This workshop brings together experts of communities which often have been\nperceived as different once: bibliometrics / scientometrics / informetrics on\nthe one side and information retrieval on the other. Our motivation as\norganizers of the workshop started from the observation that main discourses in\nboth fields are different, that communities are only partly overlapping and\nfrom the belief that a knowledge transfer would be profitable for both sides.\nBibliometric techniques are not yet widely used to enhance retrieval processes\nin digital libraries, although they offer value-added effects for users. On the\nother side, more and more information professionals, working in libraries and\narchives are confronted with applying bibliometric techniques in their\nservices. This way knowledge exchange becomes more urgent. The first workshop\nset the research agenda, by introducing in each other methods, reporting about\ncurrent research problems and brainstorming about common interests. This\nfollow-up workshop continues the overall communication, but also puts one\nproblem into the focus. In particular, we will explore how statistical\nmodelling of scholarship can improve retrieval services for specific\ncommunities, as well as for large, cross-domain collections like Mendeley or\nResearchGate. This second BIR workshop continues to raise awareness of the\nmissing link between Information Retrieval (IR) and bibliometrics and\ncontributes to create a common ground for the incorporation of\nbibliometric-enhanced services into retrieval at the scholarly search engine\ninterface.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 13:59:13 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Mayr", "Philipp", ""], ["Frommholz", "Ingo", ""], ["Scharnhorst", "Andrea", ""], ["Mutschke", "Peter", ""]]}, {"id": "1501.02655", "submitter": "Alexander Sagel", "authors": "Alexander Sagel, Dominik Meyer, Hao Shen", "title": "Texture Retrieval via the Scattering Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the problem of content-based image retrieval, specifically,\ntexture retrieval. It focuses on feature extraction and similarity measure for\ntexture images. Our approach employs a recently developed method, the so-called\nScattering transform, for the process of feature extraction in texture\nretrieval. It shares a distinctive property of providing a robust\nrepresentation, which is stable with respect to spatial deformations. Recent\nwork has demonstrated its capability for texture classification, and hence as a\npromising candidate for the problem of texture retrieval.\n  Moreover, we adopt a common approach of measuring the similarity of textures\nby comparing the subband histograms of a filterbank transform. To this end we\nderive a similarity measure based on the popular Bhattacharyya Kernel. Despite\nthe popularity of describing histograms using parametrized probability density\nfunctions, such as the Generalized Gaussian Distribution, it is unfortunately\nnot applicable for describing most of the Scattering transform subbands, due to\nthe complex modulus performed on each one of them. In this work, we propose to\nuse the Weibull distribution to model the Scattering subbands of descendant\nlayers.\n  Our numerical experiments demonstrated the effectiveness of the proposed\napproach, in comparison with several state of the arts.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 14:22:28 GMT"}, {"version": "v2", "created": "Tue, 13 Jan 2015 12:48:04 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2015 08:31:05 GMT"}, {"version": "v4", "created": "Mon, 1 Jun 2015 10:44:19 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Sagel", "Alexander", ""], ["Meyer", "Dominik", ""], ["Shen", "Hao", ""]]}, {"id": "1501.03210", "submitter": "Piyush Bansal", "authors": "Piyush Bansal, Romil Bansal and Vasudeva Varma", "title": "Towards Deep Semantic Analysis Of Hashtags", "comments": "To Appear in 37th European Conference on Information Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashtags are semantico-syntactic constructs used across various social\nnetworking and microblogging platforms to enable users to start a topic\nspecific discussion or classify a post into a desired category. Segmenting and\nlinking the entities present within the hashtags could therefore help in better\nunderstanding and extraction of information shared across the social media.\nHowever, due to lack of space delimiters in the hashtags (e.g #nsavssnowden),\nthe segmentation of hashtags into constituent entities (\"NSA\" and \"Edward\nSnowden\" in this case) is not a trivial task. Most of the current\nstate-of-the-art social media analytics systems like Sentiment Analysis and\nEntity Linking tend to either ignore hashtags, or treat them as a single word.\nIn this paper, we present a context aware approach to segment and link entities\nin the hashtags to a knowledge base (KB) entry, based on the context within the\ntweet. Our approach segments and links the entities in hashtags such that the\ncoherence between hashtag semantics and the tweet is maximized. To the best of\nour knowledge, no existing study addresses the issue of linking entities in\nhashtags for extracting semantic information. We evaluate our method on two\ndifferent datasets, and demonstrate the effectiveness of our technique in\nimproving the overall entity linking in tweets via additional semantic\ninformation provided by segmenting and linking entities in a hashtag.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 23:51:29 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Bansal", "Piyush", ""], ["Bansal", "Romil", ""], ["Varma", "Vasudeva", ""]]}, {"id": "1501.03577", "submitter": "Tao Zhou", "authors": "Xuzhen Zhu, Hui Tian, Zheng Hu, Ping Zhang, Tao Zhou", "title": "Consistence beats causality in recommender systems", "comments": "16 pages, 4 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive growth of information challenges people's capability in finding\nout items fitting to their own interests. Recommender systems provide an\nefficient solution by automatically push possibly relevant items to users\naccording to their past preferences. Recommendation algorithms usually embody\nthe causality from what having been collected to what should be recommended. In\nthis article, we argue that in many cases, a user's interests are stable, and\nthus the previous and future preferences are highly consistent. The temporal\norder of collections then does not necessarily imply a causality relationship.\nWe further propose a consistence-based algorithm that outperforms the\nstate-of-the-art recommendation algorithms in disparate real data sets,\nincluding \\textit{Netflix}, \\textit{MovieLens}, \\textit{Amazon} and\n\\textit{Rate Your Music}.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 05:17:31 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Zhu", "Xuzhen", ""], ["Tian", "Hui", ""], ["Hu", "Zheng", ""], ["Zhang", "Ping", ""], ["Zhou", "Tao", ""]]}, {"id": "1501.04298", "submitter": "Yutao Ma", "authors": "Mingming Chen and Yutao Ma", "title": "A Hybrid Approach to Web Service Recommendation Based on QoS-Aware\n  Rating and Ranking", "comments": "23 pages, 9 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the number of Web services with the same or similar functions increases\nsteadily on the Internet, nowadays more and more service consumers pay great\nattention to the non-functional properties of Web services, also known as\nquality of service (QoS), when finding and selecting appropriate Web services.\nFor most of the QoS-aware Web service recommendation systems, the list of\nrecommended Web services is generally obtained based on a rating-oriented\nprediction approach, aiming at predicting the potential ratings that an active\nuser may assign to the unrated services as accurately as possible. However, in\nsome application scenarios, high accuracy of rating prediction may not\nnecessarily lead to a satisfactory recommendation result. In this paper, we\npropose a ranking-oriented hybrid approach by combining the item-based\ncollaborative filtering and latent factor models to address the problem of Web\nservices ranking. In particular, the similarity between two Web services is\nmeasured in terms of the correlation coefficient between their rankings instead\nof between the traditional QoS ratings. Besides, we also improve the measure\nNDCG (Normalized Discounted Cumulative Gain) for evaluating the accuracy of the\ntop K recommendations returned in ranked order. Comprehensive experiments on\nthe QoS data set composed of real-world Web services are conducted to test our\napproach, and the experimental results demonstrate that our approach\noutperforms other competing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 13:50:27 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Chen", "Mingming", ""], ["Ma", "Yutao", ""]]}, {"id": "1501.04509", "submitter": "Ravindranath Chowdary C", "authors": "C Ravindranath Chowdary, Anil Kumar Singh and Anil Nelakanti", "title": "Responding to Retrieval: A Proposal to Use Retrieval Information for\n  Better Presentation of Website Content", "comments": null, "journal-ref": "Current Trends in Web Engineering. ICWE 2015. Lecture Notes in\n  Computer Science, PP. 103--114, vol 9396. Springer, Cham", "doi": "10.1007/978-3-319-24800-4_9", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Retrieval and content management are assumed to be mutually exclusive. In\nthis paper we suggest that they need not be so. In the usual information\nretrieval scenario, some information about queries leading to a website (due to\n`hits' or `visits') is available to the server administrator of the concerned\nwebsite. This information can used to better present the content on the\nwebsite. Further, we suggest that some more information can be shared by the\nretrieval system with the content provider. This will enable the content\nprovider (any website) to have a more dynamic presentation of the content that\nis in tune with the query trends, without violating the privacy of the querying\nuser. The result will be a better synchronization between retrieval systems and\ncontent providers, with the purpose of improving the user's web search\nexperience. This will also give the content provider a say in this process,\ngiven that the content provider is the one who knows much more about the\ncontent than the retrieval system. It also means that the content presentation\nmay change in response to a query. In the end, the user will be able to find\nthe relevant content more easily and quickly.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 14:59:10 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Chowdary", "C Ravindranath", ""], ["Singh", "Anil Kumar", ""], ["Nelakanti", "Anil", ""]]}, {"id": "1501.04711", "submitter": "Vijay Chandrasekhar", "authors": "Jie Lin, Olivier Morere, Vijay Chandrasekhar, Antoine Veillard, Hanlin\n  Goh", "title": "DeepHash: Getting Regularization, Depth and Fine-Tuning Right", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on representing very high-dimensional global image\ndescriptors using very compact 64-1024 bit binary hashes for instance\nretrieval. We propose DeepHash: a hashing scheme based on deep networks. Key to\nmaking DeepHash work at extremely low bitrates are three important\nconsiderations -- regularization, depth and fine-tuning -- each requiring\nsolutions specific to the hashing problem. In-depth evaluation shows that our\nscheme consistently outperforms state-of-the-art methods across all data sets\nfor both Fisher Vectors and Deep Convolutional Neural Network features, by up\nto 20 percent over other schemes. The retrieval performance with 256-bit hashes\nis close to that of the uncompressed floating point features -- a remarkable\n512 times compression.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 04:36:12 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Lin", "Jie", ""], ["Morere", "Olivier", ""], ["Chandrasekhar", "Vijay", ""], ["Veillard", "Antoine", ""], ["Goh", "Hanlin", ""]]}, {"id": "1501.04894", "submitter": "Arindam Pal", "authors": "Arindam Pal and Sushmita Ruj", "title": "CITEX: A new citation index to measure the relative importance of\n  authors and papers in scientific publications", "comments": "Accepted for publication in IEEE International Conference on\n  Communications (ICC) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.DM cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the performance of researchers and measuring the impact of papers\nwritten by scientists is the main objective of citation analysis. Various\nindices and metrics have been proposed for this. In this paper, we propose a\nnew citation index CITEX, which gives normalized scores to authors and papers\nto determine their rankings. To the best of our knowledge, this is the first\ncitation index which simultaneously assigns scores to both authors and papers.\nUsing these scores, we can get an objective measure of the reputation of an\nauthor and the impact of a paper.\n  We model this problem as an iterative computation on a publication graph,\nwhose vertices are authors and papers, and whose edges indicate which author\nhas written which paper. We prove that this iterative computation converges in\nthe limit, by using a powerful theorem from linear algebra. We run this\nalgorithm on several examples, and find that the author and paper scores match\nclosely with what is suggested by our intuition. The algorithm is theoretically\nsound and runs very fast in practice. We compare this index with several\nexisting metrics and find that CITEX gives far more accurate scores compared to\nthe traditional metrics.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 17:44:59 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Pal", "Arindam", ""], ["Ruj", "Sushmita", ""]]}, {"id": "1501.04920", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Gerardo Sierra, Juan-Manuel Torres-Moreno, Alejandro Molina", "title": "Regroupement s\\'emantique de d\\'efinitions en espagnol", "comments": "11 pages, in French, 5 figures. Workshop Evaluation des m\\'ethodes\n  d'Extraction de Connaissances dans les Donn\\'ees EvalECD EGC'10, 2010 Tunis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article focuses on the description and evaluation of a new unsupervised\nlearning method of clustering of definitions in Spanish according to their\nsemantic. Textual Energy was used as a clustering measure, and we study an\nadaptation of the Precision and Recall to evaluate our method.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 19:01:20 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Sierra", "Gerardo", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["Molina", "Alejandro", ""]]}, {"id": "1501.05132", "submitter": "Catarina Moreira", "authors": "Catarina Moreira and Bruno Martins and P\\'avel Calado", "title": "Learning to Rank Academic Experts in the DBLP Dataset", "comments": "Expert Systems, 2013. arXiv admin note: text overlap with\n  arXiv:1302.0413", "journal-ref": null, "doi": "10.1111/exsy.12062", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expert finding is an information retrieval task that is concerned with the\nsearch for the most knowledgeable people with respect to a specific topic, and\nthe search is based on documents that describe people's activities. The task\ninvolves taking a user query as input and returning a list of people who are\nsorted by their level of expertise with respect to the user query. Despite\nrecent interest in the area, the current state-of-the-art techniques lack in\nprincipled approaches for optimally combining different sources of evidence.\nThis article proposes two frameworks for combining multiple estimators of\nexpertise. These estimators are derived from textual contents, from\ngraph-structure of the citation patterns for the community of experts, and from\nprofile information about the experts. More specifically, this article explores\nthe use of supervised learning to rank methods, as well as rank aggregation\napproaches, for combing all of the estimators of expertise. Several supervised\nlearning algorithms, which are representative of the pointwise, pairwise and\nlistwise approaches, were tested, and various state-of-the-art data fusion\ntechniques were also explored for the rank aggregation framework. Experiments\nthat were performed on a dataset of academic publications from the Computer\nScience domain attest the adequacy of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 11:25:33 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Moreira", "Catarina", ""], ["Martins", "Bruno", ""], ["Calado", "P\u00e1vel", ""]]}, {"id": "1501.05140", "submitter": "Catarina Moreira", "authors": "Catarina Moreira and Bruno Martins and P\\'avel Calado", "title": "Using Rank Aggregation for Expert Search in Academic Digital Libraries", "comments": "In Simp\\'{o}sio de Inform\\'{a}tica, INForum, Portugal, 2011. arXiv\n  admin note: substantial text overlap with arXiv:1302.0413, arXiv:1501.05132", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of expert finding has been getting increasing attention in\ninformation retrieval literature. However, the current state-of-the-art is\nstill lacking in principled approaches for combining different sources of\nevidence. This paper explores the usage of unsupervised rank aggregation\nmethods as a principled approach for combining multiple estimators of\nexpertise, derived from the textual contents, from the graph-structure of the\ncitation patterns for the community of experts, and from profile information\nabout the experts. We specifically experimented two unsupervised rank\naggregation approaches well known in the information retrieval literature,\nnamely CombSUM and CombMNZ. Experiments made over a dataset of academic\npublications for the area of Computer Science attest for the adequacy of these\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 11:41:57 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 01:01:30 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Moreira", "Catarina", ""], ["Martins", "Bruno", ""], ["Calado", "P\u00e1vel", ""]]}, {"id": "1501.05940", "submitter": "Taoufik Rachad", "authors": "T. Rachad, J. Boutahar and S. El ghazi", "title": "A New Efficient Method for Calculating Similarity Between Web Services", "comments": "7 pages, 4 figures, 8 tables, International Journal of Advanced\n  Computer Science and Applications (IJACSA),Vol. 5, No. 8, 2014", "journal-ref": null, "doi": "10.14569/IJACSA.2014.050809", "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web services allow communication between heterogeneous systems in a\ndistributed environment. Their enormous success and their increased use led to\nthe fact that thousands of Web services are present on the Internet. This\nsignificant number of Web services which not cease to increase has led to\nproblems of the difficulty in locating and classifying web services, these\nproblems are encountered mainly during the operations of web services discovery\nand substitution. Traditional ways of search based on keywords are not\nsuccessful in this context, their results do not support the structure of Web\nservices and they consider in their search only the identifiers of the web\nservice description language (WSDL) interface elements. The methods based on\nsemantics (WSDLS, OWLS, SAWSDL...) which increase the WSDL description of a Web\nservice with a semantic description allow raising partially this problem, but\ntheir complexity and difficulty delays their adoption in real cases. Measuring\nthe similarity between the web services interfaces is the most suitable\nsolution for this kind of problems, it will classify available web services so\nas to know those that best match the searched profile and those that do not\nmatch. Thus, the main goal of this work is to study the degree of similarity\nbetween any two web services by offering a new method that is more effective\nthan existing works.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 22:18:45 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Rachad", "T.", ""], ["Boutahar", "J.", ""], ["ghazi", "S. El", ""]]}, {"id": "1501.05983", "submitter": "Taoufik Rachad", "authors": "J. Boutahar, T. Rachad and S. El ghazi", "title": "A new efficient Matching method for web services substitution", "comments": "9 pages, 11 figures, 2 tables", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 11,\n  Issue 5, No 2,196-204, September 2014", "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internet is considered as the most extensive market in the world. To keep\nits gradual reputation, it must confront real problems that result from its\ndistribution and from the diversity of the protocols used to insure\ncommunications. The Web service technology has diminished significantly the\neffects of distribution and heterogeneity, but there are several problems that\nweaken their performance (unavailability, load increase of use, high cost of\nCPU time...). Faced with this situation, we are forced to move in the direction\nof the substitution of web services. In this context, we propose an effective\ntechnique of substitution based on a new method of matching that allows\ndetecting and expressing the matching between the web services pairwise by\nconsidering that each of them is ontology. Also, our method performs a\ndiscovery of the most similar web service to that to be replaced by using an\nefficient method of similarity measurement.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 23:25:17 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Boutahar", "J.", ""], ["Rachad", "T.", ""], ["ghazi", "S. El", ""]]}, {"id": "1501.06370", "submitter": "Martin Halvey", "authors": "Laura Hasler, Martin Halvey, Robert Villa", "title": "Augmented Test Collections: A Step in the Right Direction", "comments": "SIGIR 2014 Workshop on Gathering Efficient Assessments of Relevance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this position paper we argue that certain aspects of relevance assessment\nin the evaluation of IR systems are oversimplified and that human assessments\nrepresented by qrels should be augmented to take account of contextual factors\nand the subjectivity of the task at hand. We propose enhancing test collections\nused in evaluation with information related to human assessors and their\ninterpretation of the task. Such augmented collections would provide a more\nrealistic and user-focused evaluation, enabling us to better understand the\nevaluation process, the performance of systems and user interactions. A first\nstep is to conduct user studies to examine in more detail what people actually\ndo when we ask them to judge the relevance of a document.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 13:08:43 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Hasler", "Laura", ""], ["Halvey", "Martin", ""], ["Villa", "Robert", ""]]}, {"id": "1501.06380", "submitter": "Martin Halvey", "authors": "Diego Moll\\'a, Iman Amini, David Martinez", "title": "Document Distance for the Automated Expansion of Relevance Judgements\n  for Information Retrieval Evaluation", "comments": "SIGIR 2014 Workshop on Gathering Efficient Assessments of Relevance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports the use of a document distance-based approach to\nautomatically expand the number of available relevance judgements when these\nare limited and reduced to only positive judgements. This may happen, for\nexample, when the only available judgements are extracted from a list of\nreferences in a published review paper. We compare the results on two document\nsets: OHSUMED, based on medical research publications, and TREC-8, based on\nnews feeds. We show that evaluations based on these expanded relevance\njudgements are more reliable than those using only the initially available\njudgements, especially when the number of available judgements is very limited.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 13:21:09 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Moll\u00e1", "Diego", ""], ["Amini", "Iman", ""], ["Martinez", "David", ""]]}, {"id": "1501.06412", "submitter": "Martin Halvey", "authors": "Aleksandr Chuklin, Maarten de Rijke", "title": "The Anatomy of Relevance: Topical, Snippet and Perceived Relevance in\n  Search Result Evaluation", "comments": "SIGIR 2014 Workshop on Gathering Efficient Assessments of Relevance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, the quality of a search engine is often determined using so-called\ntopical relevance, i.e., the match between the user intent (expressed as a\nquery) and the content of the document. In this work we want to draw attention\nto two aspects of retrieval system performance affected by the presentation of\nresults: result attractiveness (\"perceived relevance\") and immediate usefulness\nof the snippets (\"snippet relevance\"). Perceived relevance may influence\ndiscoverability of good topical documents and seemingly better rankings may in\nfact be less useful to the user if good-looking snippets lead to irrelevant\ndocuments or vice-versa. And result items on a search engine result page (SERP)\nwith high snippet relevance may add towards the total utility gained by the\nuser even without the need to click those items.\n  We start by motivating the need to collect different aspects of relevance\n(topical, perceived and snippet relevances) and how these aspects can improve\nevaluation measures. We then discuss possible ways to collect these relevance\naspects using crowdsourcing and the challenges arising from that.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 14:30:17 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Chuklin", "Aleksandr", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1501.06595", "submitter": "Sahin Geyik", "authors": "Sahin Cem Geyik, Ali Dasdan, Kuang-Chih Lee", "title": "User Clustering in Online Advertising via Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the domain of online advertising, our aim is to serve the best ad to a\nuser who visits a certain webpage, to maximize the chance of a desired action\nto be performed by this user after seeing the ad. While it is possible to\ngenerate a different prediction model for each user to tell if he/she will act\non a given ad, the prediction result typically will be quite unreliable with\nhuge variance, since the desired actions are extremely sparse, and the set of\nusers is huge (hundreds of millions) and extremely volatile, i.e., a lot of new\nusers are introduced everyday, or are no longer valid. In this paper we aim to\nimprove the accuracy in finding users who will perform the desired action, by\nassigning each user to a cluster, where the number of clusters is much smaller\nthan the number of users (in the order of hundreds). Each user will fall into\nthe same cluster with another user if their event history are similar. For this\npurpose, we modify the probabilistic latent semantic analysis (pLSA) model by\nassuming the independence of the user and the cluster id, given the history of\nevents. This assumption helps us to identify a cluster of a new user without\nre-clustering all the users. We present the details of the algorithm we\nemployed as well as the distributed implementation on Hadoop, and some initial\nresults on the clusters that were generated by the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 21:44:08 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 02:49:32 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Geyik", "Sahin Cem", ""], ["Dasdan", "Ali", ""], ["Lee", "Kuang-Chih", ""]]}, {"id": "1501.06715", "submitter": "Carmen De Maio", "authors": "Carmen De Maio, Giuseppe Fenza, Vincenzo Loia, Mimmo Parente", "title": "Time Aware Knowledge Extraction for Microblog Summarization on Twitter", "comments": "33 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microblogging services like Twitter and Facebook collect millions of user\ngenerated content every moment about trending news, occurring events, and so\non. Nevertheless, it is really a nightmare to find information of interest\nthrough the huge amount of available posts that are often noise and redundant.\nIn general, social media analytics services have caught increasing attention\nfrom both side research and industry. Specifically, the dynamic context of\nmicroblogging requires to manage not only meaning of information but also the\nevolution of knowledge over the timeline. This work defines Time Aware\nKnowledge Extraction (briefly TAKE) methodology that relies on temporal\nextension of Fuzzy Formal Concept Analysis. In particular, a microblog\nsummarization algorithm has been defined filtering the concepts organized by\nTAKE in a time-dependent hierarchy. The algorithm addresses topic-based\nsummarization on Twitter. Besides considering the timing of the concepts,\nanother distinguish feature of the proposed microblog summarization framework\nis the possibility to have more or less detailed summary, according to the\nuser's needs, with good levels of quality and completeness as highlighted in\nthe experimental results.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 09:49:54 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["De Maio", "Carmen", ""], ["Fenza", "Giuseppe", ""], ["Loia", "Vincenzo", ""], ["Parente", "Mimmo", ""]]}, {"id": "1501.07467", "submitter": "Hamed Zamani", "authors": "Hamed Zamani, Azadeh Shakery, Pooya Moradi", "title": "Regression and Learning to Rank Aggregation for User Engagement\n  Evaluation", "comments": "In Proceedings of the 2014 ACM Recommender Systems Challenge,\n  RecSysChallenge '14", "journal-ref": null, "doi": "10.1145/2668067.2668077", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User engagement refers to the amount of interaction an instance (e.g., tweet,\nnews, and forum post) achieves. Ranking the items in social media websites\nbased on the amount of user participation in them, can be used in different\napplications, such as recommender systems. In this paper, we consider a tweet\ncontaining a rating for a movie as an instance and focus on ranking the\ninstances of each user based on their engagement, i.e., the total number of\nretweets and favorites it will gain.\n  For this task, we define several features which can be extracted from the\nmeta-data of each tweet. The features are partitioned into three categories:\nuser-based, movie-based, and tweet-based. We show that in order to obtain good\nresults, features from all categories should be considered. We exploit\nregression and learning to rank methods to rank the tweets and propose to\naggregate the results of regression and learning to rank methods to achieve\nbetter performance. We have run our experiments on an extended version of\nMovieTweeting dataset provided by ACM RecSys Challenge 2014. The results show\nthat learning to rank approach outperforms most of the regression models and\nthe combination can improve the performance significantly.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 14:54:12 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Zamani", "Hamed", ""], ["Shakery", "Azadeh", ""], ["Moradi", "Pooya", ""]]}, {"id": "1501.07716", "submitter": "Dominik Kowald", "authors": "Paul Seitlinger, Dominik Kowald, Simone Kopeinik, Ilire\n  Hasani-Mavriqi, Tobias Ley, Elisabeth Lex", "title": "Attention Please! A Hybrid Resource Recommender Mimicking\n  Attention-Interpretation Dynamics", "comments": "Submitted to WWW'15 WebScience Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classic resource recommenders like Collaborative Filtering (CF) treat users\nas being just another entity, neglecting non-linear user-resource dynamics\nshaping attention and interpretation. In this paper, we propose a novel hybrid\nrecommendation strategy that refines CF by capturing these dynamics. The\nevaluation results reveal that our approach substantially improves CF and,\ndepending on the dataset, successfully competes with a computationally much\nmore expensive Matrix Factorization variant.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 09:55:24 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Seitlinger", "Paul", ""], ["Kowald", "Dominik", ""], ["Kopeinik", "Simone", ""], ["Hasani-Mavriqi", "Ilire", ""], ["Ley", "Tobias", ""], ["Lex", "Elisabeth", ""]]}]