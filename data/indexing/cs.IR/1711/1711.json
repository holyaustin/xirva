[{"id": "1711.00227", "submitter": "Chih-Ming Chen", "authors": "Chih-Ming Chen, Yi-Hsuan Yang, Yian Chen and Ming-Feng Tsai", "title": "Vertex-Context Sampling for Weighted Network Embedding", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, network embedding methods have garnered increasing attention\nbecause of their effectiveness in various information retrieval tasks. The goal\nis to learn low-dimensional representations of vertexes in an information\nnetwork and simultaneously capture and preserve the network structure. Critical\nto the performance of a network embedding method is how the edges/vertexes of\nthe network is sampled for the learning process. Many existing methods adopt a\nuniform sampling method to reduce learning complexity, but when the network is\nnon-uniform (i.e. a weighted network) such uniform sampling incurs information\nloss. The goal of this paper is to present a generalized vertex sampling\nframework that works seamlessly with most existing network embedding methods to\nsupport weighted instead of uniform vertex/edge sampling. For efficiency, we\npropose a delicate sequential vertex-to-context graph data structure, such that\nsampling a training pair for learning takes only constant time. For scalability\nand memory efficiency, we design the graph data structure in a way that keeps\nspace consumption low without requiring additional space. In addition to\nimplementing existing network embedding methods, the proposed framework can be\nused to implement extensions that feature high-order proximity modeling and\nweighted relation modeling. Experiments conducted on three datasets, including\na commercial large-scale one, verify the effectiveness and efficiency of the\nproposed weighted network embedding methods on a variety of tasks, including\nword similarity search, multi-label classification, and item recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 07:04:59 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Chen", "Chih-Ming", ""], ["Yang", "Yi-Hsuan", ""], ["Chen", "Yian", ""], ["Tsai", "Ming-Feng", ""]]}, {"id": "1711.00248", "submitter": "Ya Zhang", "authors": "Zhuoxiang Chen, Zhe Xu, Ya Zhang, Xiao Gu", "title": "Query-free Clothing Retrieval via Implicit Relevance Feedback", "comments": "12 pages, under review at IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based clothing retrieval is receiving increasing interest with the\ngrowth of online shopping. In practice, users may often have a desired piece of\nclothing in mind (e.g., either having seen it before on the street or requiring\ncertain specific clothing attributes) but may be unable to supply an image as a\nquery. We model this problem as a new type of image retrieval task in which the\ntarget image resides only in the user's mind (called \"mental image retrieval\"\nhereafter). Because of the absence of an explicit query image, we propose to\nsolve this problem through relevance feedback. Specifically, a new Bayesian\nformulation is proposed that simultaneously models the retrieval target and its\nhigh-level representation in the mind of the user (called the \"user metric\"\nhereafter) as posterior distributions of pre-fetched shop images and\nheterogeneous features extracted from multiple clothing attributes,\nrespectively. Requiring only clicks as user feedback, the proposed algorithm is\nable to account for the variability in human decision-making. Experiments with\nreal users demonstrate the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 08:33:39 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Chen", "Zhuoxiang", ""], ["Xu", "Zhe", ""], ["Zhang", "Ya", ""], ["Gu", "Xiao", ""]]}, {"id": "1711.00310", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani, Glorianna Jagfeld, Hosein Azarbonyad, Alex Olieman,\n  Jaap Kamps, Maarten Marx", "title": "On Search Powered Navigation", "comments": "Accepted for publication in ACM SIGIR International Conference on the\n  Theory of Information Retrieval", "journal-ref": null, "doi": "10.1145/3121050.3121105", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query-based searching and browsing-based navigation are the two main\ncomponents of exploratory search. Search lets users dig in deep by controlling\ntheir actions to focus on and find just the information they need, whereas\nnavigation helps them to get an overview to decide which content is most\nimportant. In this paper, we introduce the concept of \"search powered\nnavigation\" and investigate the effect of empowering navigation with search\nfunctionality on information seeking behavior of users and their experience by\nconducting a user study on exploratory search tasks, differentiated by\ndifferent types of information needs. Our main findings are as follows: First,\nwe observe radically different search tactics. Using search, users are able to\ncontrol and augment their search focus, hence they explore the data in a\ndepth-first, bottom-up manner. Conversely, using pure navigation they tend to\ncheck different options to be able to decide on their path into the data, which\ncorresponds to a breadth-first, top-down exploration. Second, we observe a\ngeneral natural tendency to combine aspects of search and navigation, however,\nour experiments show that the search functionality is essential to solve\nexploratory search tasks that require finding documents related to a narrow\ndomain. Third, we observe a natural need for search powered navigation: users\nusing a system without search functionality find creative ways to mimic\nsearching using navigation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 12:25:47 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Jagfeld", "Glorianna", ""], ["Azarbonyad", "Hosein", ""], ["Olieman", "Alex", ""], ["Kamps", "Jaap", ""], ["Marx", "Maarten", ""]]}, {"id": "1711.00681", "submitter": "Akbar Karimi", "authors": "Akbar Karimi, Ebrahim Ansari and Bahram Sadeghi Bigham", "title": "Extracting an English-Persian Parallel Corpus from Comparable Corpora", "comments": "6 pages, 3 figures, 3 tables and published and presented at LREC2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel data are an important part of a reliable Statistical Machine\nTranslation (SMT) system. The more of these data are available, the better the\nquality of the SMT system. However, for some language pairs such as\nPersian-English, parallel sources of this kind are scarce. In this paper, a\nbidirectional method is proposed to extract parallel sentences from English and\nPersian document aligned Wikipedia. Two machine translation systems are\nemployed to translate from Persian to English and the reverse after which an IR\nsystem is used to measure the similarity of the translated sentences. Adding\nthe extracted sentences to the training data of the existing SMT systems is\nshown to improve the quality of the translation. Furthermore, the proposed\nmethod slightly outperforms the one-directional approach. The extracted corpus\nconsists of about 200,000 sentences which have been sorted by their degree of\nsimilarity calculated by the IR system and is freely available for public\naccess on the Web.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 11:00:09 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 11:22:21 GMT"}, {"version": "v3", "created": "Sun, 31 Mar 2019 18:01:37 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Karimi", "Akbar", ""], ["Ansari", "Ebrahim", ""], ["Bigham", "Bahram Sadeghi", ""]]}, {"id": "1711.00715", "submitter": "Sreya Guha", "authors": "Sreya Guha", "title": "Related Fact Checks: a tool for combating fake news", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of \"Fake News\" and misinformation via online news and social\nmedia has spurred an interest in computational tools to combat this phenomenon.\nIn this paper we present a new \"Related Fact Checks\" service, which can help a\nreader critically evaluate an article and make a judgment on its veracity by\nbringing up fact checks that are relevant to the article. We describe the core\ntechnical problems that need to be solved in building a \"Related Fact Checks\"\nservice, and present results from an evaluation of an implementation.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 18:52:28 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Guha", "Sreya", ""]]}, {"id": "1711.00804", "submitter": "Ankit Parag Shah", "authors": "Rohan Badlani, Ankit Shah, Benjamin Elizalde, Anurag Kumar, Bhiksha\n  Raj", "title": "Framework for evaluation of sound event detection in web videos", "comments": "Camera Ready Version of Paper accepted at International Conference on\n  Acoustics, Speech, and Signal Processing (ICASSP) 2018. First two Authors -\n  Rohan Badlani and Ankit Shah contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.IR eess.AS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The largest source of sound events is web videos. Most videos lack sound\nevent labels at segment level, however, a significant number of them do respond\nto text queries, from a match found using metadata by search engines. In this\npaper we explore the extent to which a search query can be used as the true\nlabel for detection of sound events in videos. We present a framework for\nlarge-scale sound event recognition on web videos. The framework crawls videos\nusing search queries corresponding to 78 sound event labels drawn from three\ndatasets. The datasets are used to train three classifiers, and we obtain a\nprediction on 3.7 million web video segments. We evaluated performance using\nthe search query as true label and compare it with human labeling. Both types\nof ground truth exhibited close performance, to within 10%, and similar\nperformance trend with increasing number of evaluated segments. Hence, our\nexperiments show potential for using search query as a preliminary true label\nfor sound event recognition in web videos.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 16:32:23 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 15:05:37 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Badlani", "Rohan", ""], ["Shah", "Ankit", ""], ["Elizalde", "Benjamin", ""], ["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1711.00953", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Joachim Denzler", "title": "Automatic Query Image Disambiguation for Content-Based Image Retrieval", "comments": "VISAPP 2018 paper, 8 pages, 5 figures. Source code:\n  https://github.com/cvjena/aid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query images presented to content-based image retrieval systems often have\nvarious different interpretations, making it difficult to identify the search\nobjective pursued by the user. We propose a technique for overcoming this\nambiguity, while keeping the amount of required user interaction at a minimum.\nTo achieve this, the neighborhood of the query image is divided into coherent\nclusters from which the user may choose the relevant ones. A novel feedback\nintegration technique is then employed to re-rank the entire database with\nregard to both the user feedback and the original query. We evaluate our\napproach on the publicly available MIRFLICKR-25K dataset, where it leads to a\nrelative improvement of average precision by 23% over the baseline retrieval,\nwhich does not distinguish between different image senses.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 21:55:11 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["Denzler", "Joachim", ""]]}, {"id": "1711.01377", "submitter": "Devin Guillory", "authors": "Kamelia Aryafar and Devin Guillory and Liangjie Hong", "title": "An Ensemble-based Approach to Click-Through Rate Prediction for Promoted\n  Listings at Etsy", "comments": null, "journal-ref": null, "doi": "10.1145/3124749.3124758", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Etsy is a global marketplace where people across the world connect to make,\nbuy and sell unique goods. Sellers at Etsy can promote their product listings\nvia advertising campaigns similar to traditional sponsored search ads.\nClick-Through Rate (CTR) prediction is an integral part of online search\nadvertising systems where it is utilized as an input to auctions which\ndetermine the final ranking of promoted listings to a particular user for each\nquery. In this paper, we provide a holistic view of Etsy's promoted listings'\nCTR prediction system and propose an ensemble learning approach which is based\non historical or behavioral signals for older listings as well as content-based\nfeatures for new listings. We obtain representations from texts and images by\nutilizing state-of-the-art deep learning techniques and employ multimodal\nlearning to combine these different signals. We compare the system to\nnon-trivial baselines on a large-scale real world dataset from Etsy,\ndemonstrating the effectiveness of the model and strong correlations between\noffline experiments and online performance. The paper is also the first\ntechnical overview to this kind of product in e-commerce context.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 01:21:53 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 19:42:30 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Aryafar", "Kamelia", ""], ["Guillory", "Devin", ""], ["Hong", "Liangjie", ""]]}, {"id": "1711.01563", "submitter": "Daochen Zha", "authors": "Daochen Zha, Chenliang Li", "title": "Multi-label Dataless Text Classification with Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manually labeling documents is tedious and expensive, but it is essential for\ntraining a traditional text classifier. In recent years, a few dataless text\nclassification techniques have been proposed to address this problem. However,\nexisting works mainly center on single-label classification problems, that is,\neach document is restricted to belonging to a single category. In this paper,\nwe propose a novel Seed-guided Multi-label Topic Model, named SMTM. With a few\nseed words relevant to each category, SMTM conducts multi-label classification\nfor a collection of documents without any labeled document. In SMTM, each\ncategory is associated with a single category-topic which covers the meaning of\nthe category. To accommodate with multi-labeled documents, we explicitly model\nthe category sparsity in SMTM by using spike and slab prior and weak smoothing\nprior. That is, without using any threshold tuning, SMTM automatically selects\nthe relevant categories for each document. To incorporate the supervision of\nthe seed words, we propose a seed-guided biased GPU (i.e., generalized Polya\nurn) sampling procedure to guide the topic inference of SMTM. Experiments on\ntwo public datasets show that SMTM achieves better classification accuracy than\nstate-of-the-art alternatives and even outperforms supervised solutions in some\nscenarios.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 11:34:46 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Zha", "Daochen", ""], ["Li", "Chenliang", ""]]}, {"id": "1711.01587", "submitter": "Yi Wang", "authors": "Yi Wang, Jianwu Wan, Jun Guo, Yiu-Ming Cheung and Pong C Yuen", "title": "Inference-Based Similarity Search in Randomized Montgomery Domains for\n  Privacy-Preserving Biometric Identification", "comments": "14 pages, 10 figures, 2 tables, regular paper", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2727048", "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search is essential to many important applications and often\ninvolves searching at scale on high-dimensional data based on their similarity\nto a query. In biometric applications, recent vulnerability studies have shown\nthat adversarial machine learning can compromise biometric recognition systems\nby exploiting the biometric similarity information. Existing methods for\nbiometric privacy protection are in general based on pairwise matching of\nsecured biometric templates and have inherent limitations in search efficiency\nand scalability. In this paper, we propose an inference-based framework for\nprivacy-preserving similarity search in Hamming space. Our approach builds on\nan obfuscated distance measure that can conceal Hamming distance in a dynamic\ninterval. Such a mechanism enables us to systematically design statistically\nreliable methods for retrieving most likely candidates without knowing the\nexact distance values. We further propose to apply Montgomery multiplication\nfor generating search indexes that can withstand adversarial similarity\nanalysis, and show that information leakage in randomized Montgomery domains\ncan be made negligibly small. Our experiments on public biometric datasets\ndemonstrate that the inference-based approach can achieve a search accuracy\nclose to the best performance possible with secure computation methods, but the\nassociated cost is reduced by orders of magnitude compared to cryptographic\nprimitives.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 13:46:45 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Wang", "Yi", ""], ["Wan", "Jianwu", ""], ["Guo", "Jun", ""], ["Cheung", "Yiu-Ming", ""], ["Yuen", "Pong C", ""]]}, {"id": "1711.01647", "submitter": "Alper Kose", "authors": "Alper Kose, Can Kanbak, Noyan Evirgen", "title": "Performance Comparison of Algorithms for Movie Rating Estimation", "comments": "This work has been accepted to the 2017 IEEE ICMLA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, our goal is to compare performances of three different\nalgorithms to predict the ratings that will be given to movies by potential\nusers where we are given a user-movie rating matrix based on the past\nobservations. To this end, we evaluate User-Based Collaborative Filtering,\nIterative Matrix Factorization and Yehuda Koren's Integrated model using\nneighborhood and factorization where we use root mean square error (RMSE) as\nthe performance evaluation metric. In short, we do not observe significant\ndifferences between performances, especially when the complexity increase is\nconsidered. We can conclude that Iterative Matrix Factorization performs fairly\nwell despite its simplicity.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 19:29:21 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Kose", "Alper", ""], ["Kanbak", "Can", ""], ["Evirgen", "Noyan", ""]]}, {"id": "1711.02231", "submitter": "Wang-Cheng Kang", "authors": "Wang-Cheng Kang, Chen Fang, Zhaowen Wang, Julian McAuley", "title": "Visually-Aware Fashion Recommendation and Design with Generative Image\n  Models", "comments": "10 pages, 6 figures. Accepted by ICDM'17 as a long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building effective recommender systems for domains like fashion is\nchallenging due to the high level of subjectivity and the semantic complexity\nof the features involved (i.e., fashion styles). Recent work has shown that\napproaches to `visual' recommendation (e.g.~clothing, art, etc.) can be made\nmore accurate by incorporating visual signals directly into the recommendation\nobjective, using `off-the-shelf' feature representations derived from deep\nnetworks. Here, we seek to extend this contribution by showing that\nrecommendation performance can be significantly improved by learning `fashion\naware' image representations directly, i.e., by training the image\nrepresentation (from the pixel level) and the recommender system jointly; this\ncontribution is related to recent work using Siamese CNNs, though we are able\nto show improvements over state-of-the-art recommendation techniques such as\nBPR and variants that make use of pre-trained visual features. Furthermore, we\nshow that our model can be used \\emph{generatively}, i.e., given a user and a\nproduct category, we can generate new images (i.e., clothing items) that are\nmost consistent with their personal taste. This represents a first step towards\nbuilding systems that go beyond recommending existing items from a product\ncorpus, but which can be used to suggest styles and aid the design of new\nproducts.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 00:17:51 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Kang", "Wang-Cheng", ""], ["Fang", "Chen", ""], ["Wang", "Zhaowen", ""], ["McAuley", "Julian", ""]]}, {"id": "1711.02295", "submitter": "Ricardo Baeza-Yates", "authors": "Ricardo Baeza-Yates, Zeinab Liaghat", "title": "Quality-Efficiency Trade-offs in Machine Learning for Text Processing", "comments": "Ten pages, long version of paper that will be presented at IEEE Big\n  Data 2017 (8 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data mining, machine learning, and natural language processing are powerful\ntechniques that can be used together to extract information from large texts.\nDepending on the task or problem at hand, there are many different approaches\nthat can be used. The methods available are continuously being optimized, but\nnot all these methods have been tested and compared in a set of problems that\ncan be solved using supervised machine learning algorithms. The question is\nwhat happens to the quality of the methods if we increase the training data\nsize from, say, 100 MB to over 1 GB? Moreover, are quality gains worth it when\nthe rate of data processing diminishes? Can we trade quality for time\nefficiency and recover the quality loss by just being able to process more\ndata? We attempt to answer these questions in a general way for text processing\ntasks, considering the trade-offs involving training data size, learning time,\nand quality obtained. We propose a performance trade-off framework and apply it\nto three important text processing problems: Named Entity Recognition,\nSentiment Analysis and Document Classification. These problems were also chosen\nbecause they have different levels of object granularity: words, paragraphs,\nand documents. For each problem, we selected several supervised machine\nlearning algorithms and we evaluated the trade-offs of them on large publicly\navailable data sets (news, reviews, patents). To explore these trade-offs, we\nuse different data subsets of increasing size ranging from 50 MB to several GB.\nWe also consider the impact of the data set and the evaluation technique. We\nfind that the results do not change significantly and that most of the time the\nbest algorithms is the fastest. However, we also show that the results for\nsmall data (say less than 100 MB) are different from the results for big data\nand in those cases the best algorithm is much harder to determine.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 05:43:34 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Baeza-Yates", "Ricardo", ""], ["Liaghat", "Zeinab", ""]]}, {"id": "1711.02487", "submitter": "Stavros Theodorakis PhD", "authors": "Yoel Zeldes, Stavros Theodorakis, Efrat Solodnik, Aviv Rotman, Gil\n  Chamiel and Dan Friedman", "title": "Deep density networks and uncertainty in recommender systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building robust online content recommendation systems requires learning\ncomplex interactions between user preferences and content features. The field\nhas evolved rapidly in recent years from traditional multi-arm bandit and\ncollaborative filtering techniques, with new methods employing Deep Learning\nmodels to capture non-linearities. Despite progress, the dynamic nature of\nonline recommendations still poses great challenges, such as finding the\ndelicate balance between exploration and exploitation. In this paper we show\nhow uncertainty estimations can be incorporated by employing them in an\noptimistic exploitation/exploration strategy for more efficient exploration of\nnew recommendations. We provide a novel hybrid deep neural network model, Deep\nDensity Networks (DDN), which integrates content-based deep learning models\nwith a collaborative scheme that is able to robustly model and estimate\nuncertainty. Finally, we present online and offline results after incorporating\nDNN into a real world content recommendation system that serves billions of\nrecommendations per day, and show the benefit of using DDN in practice.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 14:30:04 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 22:44:10 GMT"}, {"version": "v3", "created": "Sun, 6 May 2018 19:47:10 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Zeldes", "Yoel", ""], ["Theodorakis", "Stavros", ""], ["Solodnik", "Efrat", ""], ["Rotman", "Aviv", ""], ["Chamiel", "Gil", ""], ["Friedman", "Dan", ""]]}, {"id": "1711.02760", "submitter": "Christoph Trattner", "authors": "Christoph Trattner, David Elsweiler", "title": "Food Recommender Systems: Important Contributions, Challenges and Future\n  Research Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recommendation of food items is important for many reasons. Attaining\ncooking inspiration via digital sources is becoming evermore popular; as are\nsystems, which recommend other types of food, such as meals in restaurants or\nproducts in supermarkets. Researchers have been studying these kinds of systems\nfor many years, suggesting not only that can they be a means to help people\nfind food they might want to eat, but also help them nourish themselves more\nhealthily. This paper provides a summary of the state-of-the-art of so-called\nfood recommender systems, highlighting both seminal and most recent approaches\nto the problem, as well as important specializations, such as food\nrecommendation systems for groups of users or systems which promote healthy\neating. We moreover discuss the diverse challenges involved in designing recsys\nfor food, summarise the lessons learned from past research and outline what we\nbelieve to be important future directions and open questions for the field. In\nproviding these contributions we hope to provide a useful resource for\nresearchers and practitioners alike.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 22:52:12 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 10:23:29 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Trattner", "Christoph", ""], ["Elsweiler", "David", ""]]}, {"id": "1711.02927", "submitter": "Arjumand Younus Dr.", "authors": "Arjumand Younus, Muhammad Atif Qureshi", "title": "An Analysis of Privacy-Aware Personalization Signals by Using Online\n  Evaluation Methods", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalization despite being an effective solution to the problem\ninformation overload remains tricky on account of multiple dimensions to\nconsider. Furthermore, the challenge of avoiding overdoing personalization\ninvolves estimation of a user's preferences in relation to different queries.\nThis work is an attempt to make inferences about when personalization would be\nbeneficial by relating observable user behavior to his/her social network usage\npatterns and user-generated content. User behavior on a search system is\nobserved by means of team-draft interleaving whereby results from two retrieval\nfunctions are presented in an interleaved manner, and user clicks are utilised\nto infer preference for a certain retrieval function. This improves upon\nearlier work which had limited usefulness due to reliance on user survey\nresults; our findings may aid real-time personalization in search systems by\ndetecting a user-related and query-related personalization signals.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 12:46:25 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Younus", "Arjumand", ""], ["Qureshi", "Muhammad Atif", ""]]}, {"id": "1711.03066", "submitter": "Leonid Boytsov", "authors": "Leonid Boytsov", "title": "A Simple Derivation of the Heap's Law from the Generalized Zipf's Law", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I reproduce a rather simple formal derivation of the Heaps' law from the\ngeneralized Zipf's law, which I previously published in Russian.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 17:45:46 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Boytsov", "Leonid", ""]]}, {"id": "1711.03373", "submitter": "Ziqi Zhang", "authors": "Ziqi Zhang, Jie Gao, Fabio Ciravegna", "title": "SemRe-Rank: Improving Automatic Term Extraction By Incorporating\n  Semantic Relatedness With Personalised PageRank", "comments": "Accepted by ACM TKDD. This is a pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic Term Extraction deals with the extraction of terminology from a\ndomain specific corpus, and has long been an established research area in data\nand knowledge acquisition. ATE remains a challenging task as it is known that\nthere is no existing ATE methods that can consistently outperform others in any\ndomain. This work adopts a refreshed perspective to this problem: instead of\nsearching for such a 'one-size-fit-all' solution that may never exist, we\npropose to develop generic methods to 'enhance' existing ATE methods. We\nintroduce SemRe-Rank, the first method based on this principle, to incorporate\nsemantic relatedness - an often overlooked venue - into an existing ATE method\nto further improve its performance. SemRe-Rank incorporates word embeddings\ninto a personalised PageRank process to compute 'semantic importance' scores\nfor candidate terms from a graph of semantically related words (nodes), which\nare then used to revise the scores of candidate terms computed by a base ATE\nalgorithm. Extensively evaluated with 13 state-of-the-art base ATE methods on\nfour datasets of diverse nature, it is shown to have achieved widespread\nimprovement over all base methods and across all datasets, with up to 15\npercentage points when measured by the Precision in the top ranked K candidate\nterms (the average for a set of K's), or up to 28 percentage points in F1\nmeasured at a K that equals to the expected real terms in the candidates (F1 in\nshort). Compared to an alternative approach built on the well-known TextRank\nalgorithm, SemRe-Rank can potentially outperform by up to 8 points in Precision\nat top K, or up to 17 points in F1.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 13:39:21 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 20:55:54 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 20:52:19 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Zhang", "Ziqi", ""], ["Gao", "Jie", ""], ["Ciravegna", "Fabio", ""]]}, {"id": "1711.03425", "submitter": "Oleksandra Panasiuk", "authors": "Oleksandra Panasiuk, Elias K\\\"arle, Umutcan Simsek, Dieter Fensel", "title": "Defining Tourism Domains for Semantic Annotation of Web Content", "comments": "ENTER 2018 Conference on Information and Communication Technologies\n  in Tourism, Published as Research Notes in e-Review of Tourism Research,\n  vol.9", "journal-ref": "e-Review of Tourism Research (eRTR), volume 9.\n  https://ertr.tamu.edu/files/2018/01/ENTER2018_Submission_94-ok.pdf", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schema.org is an initiative by Bing, Google, Yahoo! and Yandex that publishes\na vocabulary for creating structured data markup on web pages. The use of\nschema.org is necessary to increase the visibility of a website, making the\ncontent understandable to different automated agents (e.g. search engines,\nchatbots or personal assistant systems). The domain specifications are the\nsubsets of types from the schema.org vocabulary, each associated with a set of\nproperties. The challenge is to choose the right classes and properties for an\nannotation in a given domain. In this paper we address the problem of finding a\nsubset of types and properties for complete and correct annotation of different\ntourism domains. The approach provides a collection of domain specifications\nthat were built based on domain analysis and vocabulary selection.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 15:34:06 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 12:24:47 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Panasiuk", "Oleksandra", ""], ["K\u00e4rle", "Elias", ""], ["Simsek", "Umutcan", ""], ["Fensel", "Dieter", ""]]}, {"id": "1711.03736", "submitter": "Masoud Fatemi", "authors": "Masoud Fatemi and Mehran Safayani", "title": "Joint Sentiment/Topic Modeling on Text Data Using Boosted Restricted\n  Boltzmann Machine", "comments": null, "journal-ref": null, "doi": "10.1007/s11042-019-7427-5", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently by the development of the Internet and the Web, different types of\nsocial media such as web blogs become an immense source of text data. Through\nthe processing of these data, it is possible to discover practical information\nabout different topics, individuals opinions and a thorough understanding of\nthe society. Therefore, applying models which can automatically extract the\nsubjective information from the documents would be efficient and helpful. Topic\nmodeling methods, also sentiment analysis are the most raised topics in the\nnatural language processing and text mining fields. In this paper a new\nstructure for joint sentiment-topic modeling based on Restricted Boltzmann\nMachine (RBM) which is a type of neural networks is proposed. By modifying the\nstructure of RBM as well as appending a layer which is analogous to sentiment\nof text data to it, we propose a generative structure for joint sentiment topic\nmodeling based on neutral networks. The proposed method is supervised and\ntrained by the Contrastive Divergence algorithm. The new attached layer in the\nproposed model is a layer with the multinomial probability distribution which\ncan be used in text data sentiment classification or any other supervised\napplication. The proposed model is compared with existing models in the\nexperiments such as evaluating as a generative model, sentiment classification,\ninformation retrieval and the corresponding results demonstrate the efficiency\nof the method.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 09:17:02 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Fatemi", "Masoud", ""], ["Safayani", "Mehran", ""]]}, {"id": "1711.03889", "submitter": "Konstantinos Bougiatiotis", "authors": "Konstantinos Bougiatiotis and Theodore Giannakopoulos", "title": "Enhanced Movie Content Similarity Based on Textual, Auditory and Visual\n  Information", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2017.11.050", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine the ability of low-level multimodal features to\nextract movie similarity, in the context of a content-based movie\nrecommendation approach. In particular, we demonstrate the extraction of\nmultimodal representation models of movies, based on textual information from\nsubtitles, as well as cues from the audio and visual channels. With regards to\nthe textual domain, we emphasize our research in topic modeling of movies based\non their subtitles, in order to extract topics that discriminate between\nmovies. Regarding the visual domain, we focus on the extraction of semantically\nuseful features that model camera movements, colors and faces, while for the\naudio domain we adopt simple classification aggregates based on pretrained\nmodels. The three domains are combined with static metadata (e.g. directors,\nactors) to prove that the content-based movie similarity procedure can be\nenhanced with low-level multimodal information. In order to demonstrate the\nproposed content representation approach, we have built a small dataset of 160\nwidely known movies. We assert movie similarities, as propagated by the\nindividual modalities and fusion models, in the form of recommendation\nrankings. Extensive experimentation proves that all three low-level modalities\n(text, audio and visual) boost the performance of a content-based\nrecommendation system, compared to the typical metadata-based content\nrepresentation, by more than $50\\%$ relative increase. To our knowledge, this\nis the first approach that utilizes a wide range of features from all involved\nmodalities, in order to enhance the performance of the content similarity\nestimation, compared to the metadata-based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 15:21:21 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Bougiatiotis", "Konstantinos", ""], ["Giannakopoulos", "Theodore", ""]]}, {"id": "1711.04015", "submitter": "Kuan Liu", "authors": "Kuan Liu and Prem Natarajan", "title": "WMRB: Learning to Rank in a Scalable Batch Training Approach", "comments": "RecSys 2017 Poster Proceedings, August 27-31, Como, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new learning to rank algorithm, named Weighted Margin-Rank Batch\nloss (WMRB), to extend the popular Weighted Approximate-Rank Pairwise loss\n(WARP). WMRB uses a new rank estimator and an efficient batch training\nalgorithm. The approach allows more accurate item rank approximation and\nexplicit utilization of parallel computation to accelerate training. In three\nitem recommendation tasks, WMRB consistently outperforms WARP and other\nbaselines. Moreover, WMRB shows clear time efficiency advantages as data scale\nincreases.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 21:18:21 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Liu", "Kuan", ""], ["Natarajan", "Prem", ""]]}, {"id": "1711.04019", "submitter": "Kuan Liu", "authors": "Kuan Liu and Prem Natarajan", "title": "A Batch Learning Framework for Scalable Personalized Ranking", "comments": "AAAI 2018, Feb 2-7, New Orleans, USA", "journal-ref": "AAAI Conference on Artificial Intelligence 2018; Thirty-Second\n  AAAI Conference on Artificial Intelligence", "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In designing personalized ranking algorithms, it is desirable to encourage a\nhigh precision at the top of the ranked list. Existing methods either seek a\nsmooth convex surrogate for a non-smooth ranking metric or directly modify\nupdating procedures to encourage top accuracy. In this work we point out that\nthese methods do not scale well to a large-scale setting, and this is partly\ndue to the inaccurate pointwise or pairwise rank estimation. We propose a new\nframework for personalized ranking. It uses batch-based rank estimators and\nsmooth rank-sensitive loss functions. This new batch learning framework leads\nto more stable and accurate rank approximations compared to previous work.\nMoreover, it enables explicit use of parallel computation to speed up training.\nWe conduct empirical evaluation on three item recommendation tasks. Our method\nshows consistent accuracy improvements over state-of-the-art methods.\nAdditionally, we observe time efficiency advantages when data scale increases.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 21:25:30 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Liu", "Kuan", ""], ["Natarajan", "Prem", ""]]}, {"id": "1711.04044", "submitter": "Sahil Garg", "authors": "Sahil Garg and Aram Galstyan and Greg Ver Steeg and Irina Rish and\n  Guillermo Cecchi and Shuyang Gao", "title": "Kernelized Hashcode Representations for Relation Extraction", "comments": "To appear in the proceedings of conference, AAAI-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods have produced state-of-the-art results for a number of NLP\ntasks such as relation extraction, but suffer from poor scalability due to the\nhigh cost of computing kernel similarities between natural language structures.\nA recently proposed technique, kernelized locality-sensitive hashing (KLSH),\ncan significantly reduce the computational cost, but is only applicable to\nclassifiers operating on kNN graphs. Here we propose to use random subspaces of\nKLSH codes for efficiently constructing an explicit representation of NLP\nstructures suitable for general classification methods. Further, we propose an\napproach for optimizing the KLSH model for classification problems by\nmaximizing an approximation of mutual information between the KLSH codes\n(feature vectors) and the class labels. We evaluate the proposed approach on\nbiomedical relation extraction datasets, and observe significant and robust\nimprovements in accuracy w.r.t. state-of-the-art classifiers, along with\ndrastic (orders-of-magnitude) speedup compared to conventional kernel methods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 23:42:42 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 23:10:27 GMT"}, {"version": "v3", "created": "Fri, 17 Aug 2018 16:28:56 GMT"}, {"version": "v4", "created": "Wed, 31 Oct 2018 22:48:42 GMT"}, {"version": "v5", "created": "Mon, 3 Dec 2018 17:17:25 GMT"}, {"version": "v6", "created": "Mon, 25 Feb 2019 04:10:53 GMT"}, {"version": "v7", "created": "Mon, 20 May 2019 22:01:52 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Garg", "Sahil", ""], ["Galstyan", "Aram", ""], ["Steeg", "Greg Ver", ""], ["Rish", "Irina", ""], ["Cecchi", "Guillermo", ""], ["Gao", "Shuyang", ""]]}, {"id": "1711.04101", "submitter": "Laknath Semage", "authors": "Laknath Semage", "title": "Recommender Systems with Random Walks: A Survey", "comments": "15 pages, a survey paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recommender engines have become an integral component in today's e-commerce\nsystems. From recommending books in Amazon to finding friends in social\nnetworks such as Facebook, they have become omnipresent.\n  Generally, recommender systems can be classified into two main categories:\ncontent based and collaborative filtering based models. Both these models build\nrelationships between users and items to provide recommendations. Content based\nsystems achieve this task by utilizing features extracted from the context\navailable, whereas collaborative systems use shared interests between user-item\nsubsets.\n  There is another relatively unexplored approach for providing recommendations\nthat utilizes a stochastic process named random walks. This study is a survey\nexploring use cases of random walks in recommender systems and an attempt at\nclassifying them.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 08:43:06 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Semage", "Laknath", ""]]}, {"id": "1711.04189", "submitter": "Valmir C. Barbosa", "authors": "Ricardo M. Oliveira, Flavio B. Gonzaga, Valmir C. Barbosa, Geraldo B.\n  Xex\\'eo", "title": "A distributed system for SearchOnMath based on the Microsoft BizSpark\n  program", "comments": null, "journal-ref": "Proceedings of the 33rd Brazilian Symposium on Databases, 289-294,\n  2018", "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical information retrieval is a relatively new area, so the first\nsearch tools capable of retrieving mathematical formulas began to appear only a\nfew years ago. The proposals made public so far mostly implement searches on\ninternal university databases, small sets of scientific papers, or Wikipedia in\nEnglish. As such, only modest computing power is required. In this context,\nSearchOnMath has emerged as a pioneering tool in that it indexes several\ndifferent databases and is compatible with several mathematical representation\nlanguages. Given the significantly greater number of formulas it handles, a\ndistributed system becomes necessary to support it. The present study is based\non the Microsoft BizSpark program and has aimed, for 38 different\ndistributed-system scenarios, to pinpoint the one affording the best response\ntimes when searching the SearchOnMath databases for a collection of 120\nformulas.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 20:12:42 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Oliveira", "Ricardo M.", ""], ["Gonzaga", "Flavio B.", ""], ["Barbosa", "Valmir C.", ""], ["Xex\u00e9o", "Geraldo B.", ""]]}, {"id": "1711.04248", "submitter": "Yunsung Kim", "authors": "Yunsung Kim", "title": "Linking Sequences of Events with Sparse or No Common Occurrence across\n  Data Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data of practical interest - such as personal records, transaction logs, and\nmedical histories - are sequential collections of events relevant to a\nparticular source entity. Recent studies have attempted to link sequences that\nrepresent a common entity across data sets to allow more comprehensive\nstatistical analyses and to identify potential privacy failures. Yet, current\napproaches remain tailored to their specific domains of application, and they\nfail when co-referent sequences in different data sets contain sparse or no\ncommon events, which occurs frequently in many cases.\n  To address this, we formalize the general problem of \"sequence linkage\" and\ndescribe \"LDA-Link,\" a generic solution that is applicable even when\nco-referent event sequences contain no common items at all. LDA-Link is built\nupon \"Split-Document\" model, a new mixed-membership probabilistic model for the\ngeneration of event sequence collections. It detects the latent similarity of\nsequences and thus achieves robustness particularly when co-referent sequences\nshare sparse or no event overlap. We apply LDA-Link in the context of social\nmedia profile reconciliation where users make no common posts across platforms,\ncomparing to the state-of-the-art generic solution to sequence linkage.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 07:46:28 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Kim", "Yunsung", ""]]}, {"id": "1711.04305", "submitter": "Hamed Jelodar", "authors": "Hamed Jelodar, Yongli Wang, Chi Yuan, Xia Feng, Xiahui Jiang, Yanchao\n  Li, Liang Zhao", "title": "Latent Dirichlet Allocation (LDA) and Topic modeling: models,\n  applications, a survey", "comments": "arXiv admin note: text overlap with arXiv:1505.07302 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling is one of the most powerful techniques in text mining for data\nmining, latent data discovery, and finding relationships among data, text\ndocuments. Researchers have published many articles in the field of topic\nmodeling and applied in various fields such as software engineering, political\nscience, medical and linguistic science, etc. There are various methods for\ntopic modeling, which Latent Dirichlet allocation (LDA) is one of the most\npopular methods in this field. Researchers have proposed various models based\non the LDA in topic modeling. According to previous work, this paper can be\nvery useful and valuable for introducing LDA approaches in topic modeling. In\nthis paper, we investigated scholarly articles highly (between 2003 to 2016)\nrelated to Topic Modeling based on LDA to discover the research development,\ncurrent trends and intellectual structure of topic modeling. Also, we summarize\nchallenges and introduce famous tools and datasets in topic modeling based on\nLDA.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 14:50:14 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 04:20:48 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Jelodar", "Hamed", ""], ["Wang", "Yongli", ""], ["Yuan", "Chi", ""], ["Feng", "Xia", ""], ["Jiang", "Xiahui", ""], ["Li", "Yanchao", ""], ["Zhao", "Liang", ""]]}, {"id": "1711.04434", "submitter": "Ziqiang Cao", "authors": "Ziqiang Cao, Furu Wei, Wenjie Li, Sujian Li", "title": "Faithful to the Original: Fact Aware Neural Abstractive Summarization", "comments": "8 pages, 3 figures, AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Unlike extractive summarization, abstractive summarization has to fuse\ndifferent parts of the source text, which inclines to create fake facts. Our\npreliminary study reveals nearly 30% of the outputs from a state-of-the-art\nneural summarization system suffer from this problem. While previous\nabstractive summarization approaches usually focus on the improvement of\ninformativeness, we argue that faithfulness is also a vital prerequisite for a\npractical abstractive summarization system. To avoid generating fake facts in a\nsummary, we leverage open information extraction and dependency parse\ntechnologies to extract actual fact descriptions from the source text. The\ndual-attention sequence-to-sequence framework is then proposed to force the\ngeneration conditioned on both the source text and the extracted fact\ndescriptions. Experiments on the Gigaword benchmark dataset demonstrate that\nour model can greatly reduce fake summaries by 80%. Notably, the fact\ndescriptions also bring significant improvement on informativeness since they\noften condense the meaning of the source text.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 06:34:29 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Cao", "Ziqiang", ""], ["Wei", "Furu", ""], ["Li", "Wenjie", ""], ["Li", "Sujian", ""]]}, {"id": "1711.04498", "submitter": "Yong Zhang", "authors": "Yong Zhang, Hongming Zhou, Nganmeng Tan, Saeed Bagheri, Meng Joo Er", "title": "Targeted Advertising Based on Browsing History", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Audience interest, demography, purchase behavior and other possible\nclassifications are ex- tremely important factors to be carefully studied in a\ntargeting campaign. This information can help advertisers and publishers\ndeliver advertisements to the right audience group. How- ever, it is not easy\nto collect such information, especially for the online audience with whom we\nhave limited interaction and minimum deterministic knowledge. In this paper, we\npro- pose a predictive framework that can estimate online audience demographic\nattributes based on their browsing histories. Under the proposed framework,\nfirst, we retrieve the content of the websites visited by audience, and\nrepresent the content as website feature vectors; second, we aggregate the\nvectors of websites that audience have visited and arrive at feature vectors\nrepresenting the users; finally, the support vector machine is exploited to\npredict the audience demographic attributes. The key to achieving good\nprediction performance is preparing representative features of the audience.\nWord Embedding, a widely used tech- nique in natural language processing tasks,\ntogether with term frequency-inverse document frequency weighting scheme is\nused in the proposed method. This new representation ap- proach is unsupervised\nand very easy to implement. The experimental results demonstrate that the new\naudience feature representation method is more powerful than existing baseline\nmethods, leading to a great improvement in prediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 10:06:22 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Zhang", "Yong", ""], ["Zhou", "Hongming", ""], ["Tan", "Nganmeng", ""], ["Bagheri", "Saeed", ""], ["Er", "Meng Joo", ""]]}, {"id": "1711.04725", "submitter": "Zhaochun Ren", "authors": "Jing Li, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Jun Ma", "title": "Neural Attentive Session-based Recommendation", "comments": "Proceedings of the 2017 ACM on Conference on Information and\n  Knowledge Management. arXiv admin note: text overlap with arXiv:1511.06939,\n  arXiv:1606.08117 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given e-commerce scenarios that user profiles are invisible, session-based\nrecommendation is proposed to generate recommendation results from short\nsessions. Previous work only considers the user's sequential behavior in the\ncurrent session, whereas the user's main purpose in the current session is not\nemphasized. In this paper, we propose a novel neural networks framework, i.e.,\nNeural Attentive Recommendation Machine (NARM), to tackle this problem.\nSpecifically, we explore a hybrid encoder with an attention mechanism to model\nthe user's sequential behavior and capture the user's main purpose in the\ncurrent session, which are combined as a unified session representation later.\nWe then compute the recommendation scores for each candidate item with a\nbi-linear matching scheme based on this unified session representation. We\ntrain NARM by jointly learning the item and session representations as well as\ntheir matchings. We carried out extensive experiments on two benchmark\ndatasets. Our experimental results show that NARM outperforms state-of-the-art\nbaselines on both datasets. Furthermore, we also find that NARM achieves a\nsignificant improvement on long sessions, which demonstrates its advantages in\nmodeling the user's sequential behavior and main purpose simultaneously.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 17:44:01 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Li", "Jing", ""], ["Ren", "Pengjie", ""], ["Chen", "Zhumin", ""], ["Ren", "Zhaochun", ""], ["Ma", "Jun", ""]]}, {"id": "1711.05114", "submitter": "Qiang Cui", "authors": "Qiang Cui, Shu Wu, Yan Huang, Liang Wang", "title": "A Hierarchical Contextual Attention-based GRU Network for Sequential\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential recommendation is one of fundamental tasks for Web applications.\nPrevious methods are mostly based on Markov chains with a strong Markov\nassumption. Recently, recurrent neural networks (RNNs) are getting more and\nmore popular and has demonstrated its effectiveness in many tasks. The last\nhidden state is usually applied as the sequence's representation to make\nrecommendation. Benefit from the natural characteristics of RNN, the hidden\nstate is a combination of long-term dependency and short-term interest to some\ndegrees. However, the monotonic temporal dependency of RNN impairs the user's\nshort-term interest. Consequently, the hidden state is not sufficient to\nreflect the user's final interest. In this work, to deal with this problem, we\npropose a Hierarchical Contextual Attention-based GRU (HCA-GRU) network. The\nfirst level of HCA-GRU is conducted on the input. We construct a contextual\ninput by using several recent inputs based on the attention mechanism. This can\nmodel the complicated correlations among recent items and strengthen the hidden\nstate. The second level is executed on the hidden state. We fuse the current\nhidden state and a contextual hidden state built by the attention mechanism,\nwhich leads to a more suitable user's overall interest. Experiments on two\nreal-world datasets show that HCA-GRU can effectively generate the personalized\nranking list and achieve significant improvement.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 14:38:04 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 03:10:35 GMT"}, {"version": "v3", "created": "Mon, 5 Feb 2018 05:36:25 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Cui", "Qiang", ""], ["Wu", "Shu", ""], ["Huang", "Yan", ""], ["Wang", "Liang", ""]]}, {"id": "1711.05237", "submitter": "Pierre Hanna", "authors": "Pierre Hanna", "title": "Considering Durations and Replays to Improve Music Recommender Systems", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The consumption of music has its specificities in comparison with other\nmedia, especially in relation to listening durations and replays. Music\nrecommendation can take these properties into account in order to predict the\nbehaviours of the users. Their impact is investigated in this paper. A large\ndatabase was thus created using logs collected on a streaming platform, notably\ncollecting the listening times. The proposed study shows that a high proportion\nof the listening events implies a skip action, which may indicate that the user\ndid not appreciate the track listened. Implicit like and dislike can be deduced\nfrom this information of durations and replays and can be taken into account\nfor music recommendation and for the evaluation of music recommendation\nengines. A quantitative study as usually found in the literature confirms that\nneighborhood-based systems considering binary data give the best results in\nterms of MAP@k. However, a more qualitative evaluation of the recommended\ntracks shows that many tracks recommended, usually evaluated in a positive way,\nlead to skips or thus are actually not appreciated. We propose the\nconsideration of implicit like/dislike as recommendation engine inputs.\nEvaluations show that neighbourhood-based engines remain the most precise, but\nfiltering inputs according to durations and/or replays have a significant\npositive impact on the objective of the recommendation engine. The\nrecommendation process can thus be improved by taking account of listening\ndurations and replays. We also study the possibility of post-filtering a list\nof recommended tracks so as to limit the number of tracks that will be\nunpleasantly listened (skip and implicit dislike) and to increase the\nproportion of tracks appreciated (implicit like). Several simple algorithms\nshow that this post-filtering operation leads to an improvement of the quality\nof the music recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 18:22:40 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Hanna", "Pierre", ""]]}, {"id": "1711.05626", "submitter": "Pankaj Gupta", "authors": "Pankaj Gupta, Subburam Rajaram, Hinrich Sch\\\"utze, Bernt Andrassy", "title": "Deep Temporal-Recurrent-Replicated-Softmax for Topical Trends over Time", "comments": "In Proceedings of the 16th Annual Conference of the North American\n  Chapter of the Association for Computational Linguistics: Human Language\n  Technologies (NAACL-HLT 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic topic modeling facilitates the identification of topical trends over\ntime in temporal collections of unstructured documents. We introduce a novel\nunsupervised neural dynamic topic model named as Recurrent Neural\nNetwork-Replicated Softmax Model (RNNRSM), where the discovered topics at each\ntime influence the topic discovery in the subsequent time steps. We account for\nthe temporal ordering of documents by explicitly modeling a joint distribution\nof latent topical dependencies over time, using distributional estimators with\ntemporal recurrent connections. Applying RNN-RSM to 19 years of articles on NLP\nresearch, we demonstrate that compared to state-of-the art topic models, RNNRSM\nshows better generalization, topic interpretation, evolution and trends. We\nalso introduce a metric (named as SPAN) to quantify the capability of dynamic\ntopic model to capture word evolution in topics over time.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 15:33:59 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 09:17:46 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Gupta", "Pankaj", ""], ["Rajaram", "Subburam", ""], ["Sch\u00fctze", "Hinrich", ""], ["Andrassy", "Bernt", ""]]}, {"id": "1711.05789", "submitter": "Yuan Yang", "authors": "Yuan Yang, Jingcheng Yu, Ye Hu, Xiaoyao Xu and Eric Nyberg", "title": "CMU LiveMedQA at TREC 2017 LiveQA: A Consumer Health Question Answering\n  System", "comments": "To appear in Proceedings of TREC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present LiveMedQA, a question answering system that is\noptimized for consumer health question. On top of the general QA system\npipeline, we introduce several new features that aim to exploit domain-specific\nknowledge and entity structures for better performance. This includes a\nquestion type/focus analyzer based on deep text classification model, a\ntree-based knowledge graph for answer generation and a complementary\nstructure-aware searcher for answer retrieval. LiveMedQA system is evaluated in\nthe TREC 2017 LiveQA medical subtask, where it received an average score of\n0.356 on a 3 point scale. Evaluation results revealed 3 substantial drawbacks\nin current LiveMedQA system, based on which we provide a detailed discussion\nand propose a few solutions that constitute the main focus of our subsequent\nwork.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 20:26:42 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Yang", "Yuan", ""], ["Yu", "Jingcheng", ""], ["Hu", "Ye", ""], ["Xu", "Xiaoyao", ""], ["Nyberg", "Eric", ""]]}, {"id": "1711.05828", "submitter": "Rhicheek Patra", "authors": "Rhicheek Patra, Egor Samosvat, Michael Roizner, Andrei Mishchenko", "title": "BoostJet: Towards Combining Statistical Aggregates with Neural\n  Embeddings for Recommendations", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommenders have become widely popular in recent years because of their\nbroader applicability in many e-commerce applications. These applications rely\non recommenders for generating advertisements for various offers or providing\ncontent recommendations. However, the quality of the generated recommendations\ndepends on user features (like demography, temporality), offer features (like\npopularity, price), and user-offer features (like implicit or explicit\nfeedback). Current state-of-the-art recommenders do not explore such diverse\nfeatures concurrently while generating the recommendations.\n  In this paper, we first introduce the notion of Trackers which enables us to\ncapture the above-mentioned features and thus incorporate users' online\nbehaviour through statistical aggregates of different features (demography,\ntemporality, popularity, price). We also show how to capture offer-to-offer\nrelations, based on their consumption sequence, leveraging neural embeddings\nfor offers in our Offer2Vec algorithm. We then introduce BoostJet, a novel\nrecommender which integrates the Trackers along with the neural embeddings\nusing MatrixNet, an efficient distributed implementation of gradient boosted\ndecision tree, to improve the recommendation quality significantly. We provide\nan in-depth evaluation of BoostJet on Yandex's dataset, collecting online\nbehaviour from tens of millions of online users, to demonstrate the\npracticality of BoostJet in terms of recommendation quality as well as\nscalability.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 22:25:49 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 21:16:53 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Patra", "Rhicheek", ""], ["Samosvat", "Egor", ""], ["Roizner", "Michael", ""], ["Mishchenko", "Andrei", ""]]}, {"id": "1711.06004", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel", "title": "Remedies against the Vocabulary Gap in Information Retrieval", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines rely heavily on term-based approaches that represent queries\nand documents as bags of words. Text---a document or a query---is represented\nby a bag of its words that ignores grammar and word order, but retains word\nfrequency counts. When presented with a search query, the engine then ranks\ndocuments according to their relevance scores by computing, among other things,\nthe matching degrees between query and document terms. While term-based\napproaches are intuitive and effective in practice, they are based on the\nhypothesis that documents that exactly contain the query terms are highly\nrelevant regardless of query semantics. Inversely, term-based approaches assume\ndocuments that do not contain query terms as irrelevant. However, it is known\nthat a high matching degree at the term level does not necessarily mean high\nrelevance and, vice versa, documents that match null query terms may still be\nrelevant. Consequently, there exists a vocabulary gap between queries and\ndocuments that occurs when both use different words to describe the same\nconcepts. It is the alleviation of the effect brought forward by this\nvocabulary gap that is the topic of this dissertation. More specifically, we\npropose (1) methods to formulate an effective query from complex textual\nstructures and (2) latent vector space models that circumvent the vocabulary\ngap in information retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 09:50:52 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Van Gysel", "Christophe", ""]]}, {"id": "1711.06100", "submitter": "Rhicheek Patra", "authors": "Rachid Guerraoui, Erwan Le Merrer, Rhicheek Patra, Jean-Ronan\n  Vigouroux", "title": "Sequences, Items And Latent Links: Recommendation With Consumed Item\n  Packs", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommenders personalize the web content by typically using collaborative\nfiltering to relate users (or items) based on explicit feedback, e.g., ratings.\nThe difficulty of collecting this feedback has recently motivated to consider\nimplicit feedback (e.g., item consumption along with the corresponding time).\n  In this paper, we introduce the notion of consumed item pack (CIP) which\nenables to link users (or items) based on their implicit analogous consumption\nbehavior. Our proposal is generic, and we show that it captures three novel\nimplicit recommenders: a user-based (CIP-U), an item-based (CIP-I), and a word\nembedding-based (DEEPCIP), as well as a state-of-the-art technique using\nimplicit feedback (FISM). We show that our recommenders handle incremental\nupdates incorporating freshly consumed items. We demonstrate that all three\nrecommenders provide a recommendation quality that is competitive with\nstate-of-the-art ones, including one incorporating both explicit and implicit\nfeedback.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 14:11:53 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 21:16:41 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Guerraoui", "Rachid", ""], ["Merrer", "Erwan Le", ""], ["Patra", "Rhicheek", ""], ["Vigouroux", "Jean-Ronan", ""]]}, {"id": "1711.06196", "submitter": "Navid Rekabsaz", "authors": "Navid Rekabsaz, Mihai Lupu, Allan Hanbury, Andres Duque", "title": "Addressing Cross-Lingual Word Sense Disambiguation on Low-Density\n  Languages: Application to Persian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of unsupervised methods in Cross-Lingual Word Sense\nDisambiguation (CL-WSD) with the application of English to Persian. Our\nproposed approach targets the languages with scarce resources (low-density) by\nexploiting word embedding and semantic similarity of the words in context. We\nevaluate the approach on a recent evaluation benchmark and compare it with the\nstate-of-the-art unsupervised system (CO-Graph). The results show that our\napproach outperforms both the standard baseline and the CO-Graph system in both\nof the task evaluation metrics (Out-Of-Five and Best result).\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 16:59:33 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 22:10:38 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2018 14:34:04 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Rekabsaz", "Navid", ""], ["Lupu", "Mihai", ""], ["Hanbury", "Allan", ""], ["Duque", "Andres", ""]]}, {"id": "1711.06446", "submitter": "Ke Ma", "authors": "Ke Ma, Jinshan Zeng, Jiechao Xiong, Qianqian Xu, Xiaochun Cao, Wei\n  Liu, Yuan Yao", "title": "Stochastic Non-convex Ordinal Embedding with Stabilized Barzilai-Borwein\n  Step Size", "comments": "11 pages, 3 figures, 2 tables, accepted by AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning representation from relative similarity comparisons, often called\nordinal embedding, gains rising attention in recent years. Most of the existing\nmethods are batch methods designed mainly based on the convex optimization,\nsay, the projected gradient descent method. However, they are generally\ntime-consuming due to that the singular value decomposition (SVD) is commonly\nadopted during the update, especially when the data size is very large. To\novercome this challenge, we propose a stochastic algorithm called SVRG-SBB,\nwhich has the following features: (a) SVD-free via dropping convexity, with\ngood scalability by the use of stochastic algorithm, i.e., stochastic variance\nreduced gradient (SVRG), and (b) adaptive step size choice via introducing a\nnew stabilized Barzilai-Borwein (SBB) method as the original version for convex\nproblems might fail for the considered stochastic \\textit{non-convex}\noptimization problem. Moreover, we show that the proposed algorithm converges\nto a stationary point at a rate $\\mathcal{O}(\\frac{1}{T})$ in our setting,\nwhere $T$ is the number of total iterations. Numerous simulations and\nreal-world data experiments are conducted to show the effectiveness of the\nproposed algorithm via comparing with the state-of-the-art methods,\nparticularly, much lower computational cost with good prediction performance.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 08:01:07 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 02:47:26 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Ma", "Ke", ""], ["Zeng", "Jinshan", ""], ["Xiong", "Jiechao", ""], ["Xu", "Qianqian", ""], ["Cao", "Xiaochun", ""], ["Liu", "Wei", ""], ["Yao", "Yuan", ""]]}, {"id": "1711.06632", "submitter": "Chang Zhou", "authors": "Chang Zhou, Jinze Bai, Junshuai Song, Xiaofei Liu, Zhengchao Zhao,\n  Xiusi Chen, Jun Gao", "title": "ATRank: An Attention-Based User Behavior Modeling Framework for\n  Recommendation", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A user can be represented as what he/she does along the history. A common way\nto deal with the user modeling problem is to manually extract all kinds of\naggregated features over the heterogeneous behaviors, which may fail to fully\nrepresent the data itself due to limited human instinct. Recent works usually\nuse RNN-based methods to give an overall embedding of a behavior sequence,\nwhich then could be exploited by the downstream applications. However, this can\nonly preserve very limited information, or aggregated memories of a person.\nWhen a downstream application requires to facilitate the modeled user features,\nit may lose the integrity of the specific highly correlated behavior of the\nuser, and introduce noises derived from unrelated behaviors. This paper\nproposes an attention based user behavior modeling framework called ATRank,\nwhich we mainly use for recommendation tasks. Heterogeneous user behaviors are\nconsidered in our model that we project all types of behaviors into multiple\nlatent semantic spaces, where influence can be made among the behaviors via\nself-attention. Downstream applications then can use the user behavior vectors\nvia vanilla attention. Experiments show that ATRank can achieve better\nperformance and faster training process. We further explore ATRank to use one\nunified model to predict different types of user behaviors at the same time,\nshowing a comparable performance with the highly optimized individual models.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 17:15:53 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 06:57:20 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Zhou", "Chang", ""], ["Bai", "Jinze", ""], ["Song", "Junshuai", ""], ["Liu", "Xiaofei", ""], ["Zhao", "Zhengchao", ""], ["Chen", "Xiusi", ""], ["Gao", "Jun", ""]]}, {"id": "1711.06955", "submitter": "Hamed Jelodar", "authors": "Hamed Jelodar, Yongli Wang, Chi Yuan, Xiaohui Jiang", "title": "A systematic framework to discover pattern for web spam classification", "comments": "Proceedings of IEEE IEMCON 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web spam is a big problem for search engine users in World Wide Web. They use\ndeceptive techniques to achieve high rankings. Although many researchers have\npresented the different approach for classification and web spam detection\nstill it is an open issue in computer science. Analyzing and evaluating these\nwebsites can be an effective step for discovering and categorizing the features\nof these websites. There are several methods and algorithms for detecting those\nwebsites, such as decision tree algorithm. In this paper, we present a\nsystematic framework based on CHAID algorithm and a modified string matching\nalgorithm (KMP) for extract features and analysis of these websites. We\nevaluated our model and other methods with a dataset of Alexa Top 500 Global\nSites and Bing search engine results in 500 queries.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 02:15:26 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Jelodar", "Hamed", ""], ["Wang", "Yongli", ""], ["Yuan", "Chi", ""], ["Jiang", "Xiaohui", ""]]}, {"id": "1711.06968", "submitter": "Imon Banerjee", "authors": "Imon Banerjee, Sriraman Madhavan, Roger Eric Goldman, Daniel L. Rubin", "title": "Intelligent Word Embeddings of Free-Text Radiology Reports", "comments": "AMIA Annual Symposium 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiology reports are a rich resource for advancing deep learning\napplications in medicine by leveraging the large volume of data continuously\nbeing updated, integrated, and shared. However, there are significant\nchallenges as well, largely due to the ambiguity and subtlety of natural\nlanguage. We propose a hybrid strategy that combines semantic-dictionary\nmapping and word2vec modeling for creating dense vector embeddings of free-text\nradiology reports. Our method leverages the benefits of both\nsemantic-dictionary mapping as well as unsupervised learning. Using the vector\nrepresentation, we automatically classify the radiology reports into three\nclasses denoting confidence in the diagnosis of intracranial hemorrhage by the\ninterpreting radiologist. We performed experiments with varying hyperparameter\nsettings of the word embeddings and a range of different classifiers. Best\nperformance achieved was a weighted precision of 88% and weighted recall of\n90%. Our work offers the potential to leverage unstructured electronic health\nrecord data by allowing direct analysis of narrative clinical notes.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 05:14:36 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Banerjee", "Imon", ""], ["Madhavan", "Sriraman", ""], ["Goldman", "Roger Eric", ""], ["Rubin", "Daniel L.", ""]]}, {"id": "1711.07065", "submitter": "Moontae Lee", "authors": "Moontae Lee, David Bindel, David Mimno", "title": "Prior-aware Dual Decomposition: Document-specific Topic Inference for\n  Spectral Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral topic modeling algorithms operate on matrices/tensors of word\nco-occurrence statistics to learn topic-specific word distributions. This\napproach removes the dependence on the original documents and produces\nsubstantial gains in efficiency and provable topic inference, but at a cost:\nthe model can no longer provide information about the topic composition of\nindividual documents. Recently Thresholded Linear Inverse (TLI) is proposed to\nmap the observed words of each document back to its topic composition. However,\nits linear characteristics limit the inference quality without considering the\nimportant prior information over topics. In this paper, we evaluate Simple\nProbabilistic Inverse (SPI) method and novel Prior-aware Dual Decomposition\n(PADD) that is capable of learning document-specific topic compositions in\nparallel. Experiments show that PADD successfully leverages topic correlations\nas a prior, notably outperforming TLI and learning quality topic compositions\ncomparable to Gibbs sampling on various data.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 19:56:23 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Lee", "Moontae", ""], ["Bindel", "David", ""], ["Mimno", "David", ""]]}, {"id": "1711.07227", "submitter": "Kubilay Atasu", "authors": "Kubilay Atasu, Thomas Parnell, Celestine D\\\"unner, Manolis Sifalakis,\n  Haralampos Pozidis, Vasileios Vasileiadis, Michail Vlachos, Cesar Berrospi,\n  Abdel Labbi", "title": "Linear-Complexity Relaxed Word Mover's Distance with GPU Acceleration", "comments": "To appear in the 2017 IEEE International Conference on Big Data (Big\n  Data 2017) http://cci.drexel.edu/bigdata/bigdata2017/ December 11-14, 2017,\n  Boston, MA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of unstructured text-based data is growing every day. Querying,\nclustering, and classifying this big data requires similarity computations\nacross large sets of documents. Whereas low-complexity similarity metrics are\navailable, attention has been shifting towards more complex methods that\nachieve a higher accuracy. In particular, the Word Mover's Distance (WMD)\nmethod proposed by Kusner et al. is a promising new approach, but its time\ncomplexity grows cubically with the number of unique words in the documents.\nThe Relaxed Word Mover's Distance (RWMD) method, again proposed by Kusner et\nal., reduces the time complexity from qubic to quadratic and results in a\nlimited loss in accuracy compared with WMD. Our work contributes a\nlow-complexity implementation of the RWMD that reduces the average time\ncomplexity to linear when operating on large sets of documents. Our\nlinear-complexity RWMD implementation, henceforth referred to as LC-RWMD, maps\nwell onto GPUs and can be efficiently distributed across a cluster of GPUs. Our\nexperiments on real-life datasets demonstrate 1) a performance improvement of\ntwo orders of magnitude with respect to our GPU-based distributed\nimplementation of the quadratic RWMD, and 2) a performance improvement of three\nto four orders of magnitude with respect to our distributed WMD implementation\nthat uses GPU-based RWMD for pruning.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 09:45:02 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Atasu", "Kubilay", ""], ["Parnell", "Thomas", ""], ["D\u00fcnner", "Celestine", ""], ["Sifalakis", "Manolis", ""], ["Pozidis", "Haralampos", ""], ["Vasileiadis", "Vasileios", ""], ["Vlachos", "Michail", ""], ["Berrospi", "Cesar", ""], ["Labbi", "Abdel", ""]]}, {"id": "1711.07551", "submitter": "Rong Gong", "authors": "Rong Gong and Xavier Serra", "title": "Identification of potential Music Information Retrieval technologies for\n  computer-aided jingju singing training", "comments": "Chinese traditional music technology session - China conference on\n  sound and music technology 2017, Suzhou, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music Information Retrieval (MIR) technologies have been proven useful in\nassisting western classical singing training. Jingju (also known as Beijing or\nPeking opera) singing is different from western singing in terms of most of the\nperceptual dimensions, and the trainees are taught by using mouth/heart method.\nIn this paper, we first present the training method used in the professional\njingju training classroom scenario and show the potential benefits of\nintroducing the MIR technologies into the training process. The main part of\nthis paper dedicates to identify the potential MIR technologies for jingju\nsinging training. To this intent, we answer the question: how the jingju\nsinging tutors and trainees value the importance of each jingju musical\ndimension-intonation, rhythm, loudness, tone quality and pronunciation? This is\ndone by (i) classifying the classroom singing practices, tutor's verbal\nfeedbacks into these 5 dimensions, (ii) surveying the trainees. Then, with the\nhelp of the music signal analysis, a finer inspection on the classroom practice\nrecording examples reveals the detailed elements in the training process.\nFinally, based on the above analysis, several potential MIR technologies are\nidentified and would be useful for the jingju singing training.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 04:37:58 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Gong", "Rong", ""], ["Serra", "Xavier", ""]]}, {"id": "1711.07593", "submitter": "Ahmed Mohamed Elmisery", "authors": "Ahmed M. Elmisery, Dmitri Botvich", "title": "An Enhanced Middleware for Collaborative Privacy in IPTV Recommender\n  Services", "comments": "10 pages, 7 figures, Journal Paper", "journal-ref": "Journal of Convergence Information Technology-December 2011-vol.2\n  issue 2 Pages 33-42", "doi": null, "report-no": "Future Technology Research Association International Volume 02-02", "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the concerns users have to confronted when using IPTV system is the\ninformation overload that makes it difficult for them to find a suitable\ncontent according to their personal preferences. Recommendation service is one\nof the most widely adopted technologies for alleviating this problem, these\nservices intend to provide people with referrals of items they will appreciate\nbased on their preferences. IPTV users must ensure their sensitive preferences\ncollected by any recommendation service are properly secured. In this work, we\nintroduce a framework for private recommender service based on Enhanced\nMiddleware for Collaborative Privacy (EMCP). EMCP executes a two-stage\nconcealment process that gives the user a complete control on the privacy level\nof his/her profile. We utilize trust mechanism to augment the accuracy and\nprivacy of the recommendations. Trust heuristic spot users who are trustworthy\nwith respect to the user requesting the recommendation. Later, the neighborhood\nformation is calculated using proximity metrics based on these trustworthy\nusers. Finally, Users submit their profiles in an obfuscated form without\nrevealing any information about their data, and the computation of\nrecommendations proceeds over the obfuscated data using secure multiparty\ncomputation protocol. We expand the obfuscation scope from single obfuscation\nlevel for all users to arbitrary obfuscation levels based on trustworthy\nbetween users. In other words, we correlate the obfuscation level with\ndifferent trust levels, so the more trusted a target user is the less\nobfuscation copy of profile he can access. We also provide an IPTV network\nscenario and experimentation results. Our results and analysis show that our\ntwo-stage concealment process not only protects the privacy of users but also\ncan maintain the recommendations accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 01:11:38 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Elmisery", "Ahmed M.", ""], ["Botvich", "Dmitri", ""]]}, {"id": "1711.07601", "submitter": "Jure Leskovec", "authors": "Chantat Eksombatchai, Pranav Jindal, Jerry Zitao Liu, Yuchen Liu,\n  Rahul Sharma, Charles Sugnet, Mark Ulrich, Jure Leskovec", "title": "Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users\n  in Real-Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.PF cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User experience in modern content discovery applications critically depends\non high-quality personalized recommendations. However, building systems that\nprovide such recommendations presents a major challenge due to a massive pool\nof items, a large number of users, and requirements for recommendations to be\nresponsive to user actions and generated on demand in real-time. Here we\npresent Pixie, a scalable graph-based real-time recommender system that we\ndeveloped and deployed at Pinterest. Given a set of user-specific pins as a\nquery, Pixie selects in real-time from billions of possible pins those that are\nmost related to the query. To generate recommendations, we develop Pixie Random\nWalk algorithm that utilizes the Pinterest object graph of 3 billion nodes and\n17 billion edges. Experiments show that recommendations provided by Pixie lead\nup to 50% higher user engagement when compared to the previous Hadoop-based\nproduction system. Furthermore, we develop a graph pruning strategy at that\nleads to an additional 58% improvement in recommendations. Last, we discuss\nsystem aspects of Pixie, where a single server executes 1,200 recommendation\nrequests per second with 60 millisecond latency. Today, systems backed by Pixie\ncontribute to more than 80% of all user engagement on Pinterest.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 01:51:35 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Eksombatchai", "Chantat", ""], ["Jindal", "Pranav", ""], ["Liu", "Jerry Zitao", ""], ["Liu", "Yuchen", ""], ["Sharma", "Rahul", ""], ["Sugnet", "Charles", ""], ["Ulrich", "Mark", ""], ["Leskovec", "Jure", ""]]}, {"id": "1711.07656", "submitter": "Yi Tay", "authors": "Yi Tay, Luu Anh Tuan, Siu Cheung Hui", "title": "Cross Temporal Recurrent Networks for Ranking Question Answer Pairs", "comments": "Accepted to AAAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal gates play a significant role in modern recurrent-based neural\nencoders, enabling fine-grained control over recursive compositional operations\nover time. In recurrent models such as the long short-term memory (LSTM),\ntemporal gates control the amount of information retained or discarded over\ntime, not only playing an important role in influencing the learned\nrepresentations but also serving as a protection against vanishing gradients.\nThis paper explores the idea of learning temporal gates for sequence pairs\n(question and answer), jointly influencing the learned representations in a\npairwise manner. In our approach, temporal gates are learned via 1D\nconvolutional layers and then subsequently cross applied across question and\nanswer for joint learning. Empirically, we show that this conceptually simple\nsharing of temporal gates can lead to competitive performance across multiple\nbenchmarks. Intuitively, what our network achieves can be interpreted as\nlearning representations of question and answer pairs that are aware of what\neach other is remembering or forgetting, i.e., pairwise temporal gating. Via\nextensive experiments, we show that our proposed model achieves\nstate-of-the-art performance on two community-based QA datasets and competitive\nperformance on one factoid-based QA dataset.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 07:26:39 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Tay", "Yi", ""], ["Tuan", "Luu Anh", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1711.07762", "submitter": "Emanuel Laci\\'c", "authors": "Emanuel Lacic, Dominik Kowald, Markus Reiter-Haas, Valentin Slawicek,\n  Elisabeth Lex", "title": "Beyond Accuracy Optimization: On the Value of Item Embeddings for\n  Student Job Recommendations", "comments": "4 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of recommending jobs to university\nstudents. For this, we explore the utilization of neural item embeddings for\nthe task of content-based recommendation, and we propose to integrate the\nfactors of frequency and recency of interactions with job postings to combine\nthese item embeddings. We evaluate our job recommendation system on a dataset\nof the Austrian student job portal Studo using prediction accuracy, diversity\nand an adapted novelty metric. This paper demonstrates that utilizing frequency\nand recency of interactions with job postings for combining item embeddings\nresults in a robust model with respect to accuracy and diversity, which also\nprovides the best adapted novelty results.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 13:09:46 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Lacic", "Emanuel", ""], ["Kowald", "Dominik", ""], ["Reiter-Haas", "Markus", ""], ["Slawicek", "Valentin", ""], ["Lex", "Elisabeth", ""]]}, {"id": "1711.07798", "submitter": "Qingjie Liu", "authors": "Xingyue Chen, Yunhong Wang, Qingjie Liu", "title": "Visual and Textual Sentiment Analysis Using Deep Fusion Convolutional\n  Neural Networks", "comments": "Accepted as oral presentation by ICIP2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is attracting more and more attentions and has become a\nvery hot research topic due to its potential applications in personalized\nrecommendation, opinion mining, etc. Most of the existing methods are based on\neither textual or visual data and can not achieve satisfactory results, as it\nis very hard to extract sufficient information from only one single modality\ndata. Inspired by the observation that there exists strong semantic correlation\nbetween visual and textual data in social medias, we propose an end-to-end deep\nfusion convolutional neural network to jointly learn textual and visual\nsentiment representations from training examples. The two modality information\nare fused together in a pooling layer and fed into fully-connected layers to\npredict the sentiment polarity. We evaluate the proposed approach on two widely\nused data sets. Results show that our method achieves promising result compared\nwith the state-of-the-art methods which clearly demonstrate its competency.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 14:19:48 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Chen", "Xingyue", ""], ["Wang", "Yunhong", ""], ["Liu", "Qingjie", ""]]}, {"id": "1711.08379", "submitter": "Maciej Kula", "authors": "Maciej Kula", "title": "Mixture-of-tastes Models for Representing Users with Diverse Interests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing recommendation approaches implicitly treat user tastes as\nunimodal, resulting in an average-of-tastes representations when multiple\ndistinct interests are present. We show that appropriately modelling the\nmulti-faceted nature of user tastes through a mixture-of-tastes model leads to\nlarge increases in recommendation quality. Our result holds both for deep\nsequence-based and traditional factorization models, and is robust to careful\nselection and tuning of baseline models. In sequence-based models, this\nimprovement is achieved at a very modest cost in model complexity, making\nmixture-of-tastes models a straightforward improvement on existing baselines.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 16:37:42 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 03:32:21 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Kula", "Maciej", ""]]}, {"id": "1711.08521", "submitter": "Ibrahim Aljarah", "authors": "Wadi' Hijawi, Hossam Faris, Ja'far Alqatawna, Ibrahim Aljarah, Ala' M.\n  Al-Zoubi, and Maria Habib", "title": "EMFET: E-mail Features Extraction Tool", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.32995.45603", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EMFET is an open source and flexible tool that can be used to extract a large\nnumber of features from any email corpus with emails saved in EML format. The\nextracted features can be categorized into three main groups: header features,\npayload (body) features, and attachment features. The purpose of the tool is to\nhelp practitioners and researchers to build datasets that can be used for\ntraining machine learning models for spam detection. So far, 140 features can\nbe extracted using EMFET. EMFET is extensible and easy to use. The source code\nof EMFET is publicly available at GitHub\n(https://github.com/WadeaHijjawi/EmailFeaturesExtraction)\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 22:24:20 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Hijawi", "Wadi'", ""], ["Faris", "Hossam", ""], ["Alqatawna", "Ja'far", ""], ["Aljarah", "Ibrahim", ""], ["Al-Zoubi", "Ala' M.", ""], ["Habib", "Maria", ""]]}, {"id": "1711.08609", "submitter": "Seyed Mahdi Rezaeinia", "authors": "Seyed Mahdi Rezaeinia, Ali Ghodsi, Rouhollah Rahmani", "title": "Improving the Accuracy of Pre-trained Word Embeddings for Sentiment\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is one of the well-known tasks and fast growing research\nareas in natural language processing (NLP) and text classifications. This\ntechnique has become an essential part of a wide range of applications\nincluding politics, business, advertising and marketing. There are various\ntechniques for sentiment analysis, but recently word embeddings methods have\nbeen widely used in sentiment classification tasks. Word2Vec and GloVe are\ncurrently among the most accurate and usable word embedding methods which can\nconvert words into meaningful vectors. However, these methods ignore sentiment\ninformation of texts and need a huge corpus of texts for training and\ngenerating exact vectors which are used as inputs of deep learning models. As a\nresult, because of the small size of some corpuses, researcher often have to\nuse pre-trained word embeddings which were trained on other large text corpus\nsuch as Google News with about 100 billion words. The increasing accuracy of\npre-trained word embeddings has a great impact on sentiment analysis research.\nIn this paper we propose a novel method, Improved Word Vectors (IWV), which\nincreases the accuracy of pre-trained word embeddings in sentiment analysis.\nOur method is based on Part-of-Speech (POS) tagging techniques, lexicon-based\napproaches and Word2Vec/GloVe methods. We tested the accuracy of our method via\ndifferent deep learning models and sentiment datasets. Our experiment results\nshow that Improved Word Vectors (IWV) are very effective for sentiment\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 08:25:23 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Rezaeinia", "Seyed Mahdi", ""], ["Ghodsi", "Ali", ""], ["Rahmani", "Rouhollah", ""]]}, {"id": "1711.08611", "submitter": "Yixing Fan", "authors": "Jiafeng Guo, Yixing Fan, Qingyao Ai, W. Bruce Croft", "title": "A Deep Relevance Matching Model for Ad-hoc Retrieval", "comments": "CIKM 2016, long paper", "journal-ref": null, "doi": "10.1145/2983323.2983769", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural networks have led to exciting breakthroughs in\nspeech recognition, computer vision, and natural language processing (NLP)\ntasks. However, there have been few positive results of deep models on ad-hoc\nretrieval tasks. This is partially due to the fact that many important\ncharacteristics of the ad-hoc retrieval task have not been well addressed in\ndeep models yet. Typically, the ad-hoc retrieval task is formalized as a\nmatching problem between two pieces of text in existing work using deep models,\nand treated equivalent to many NLP tasks such as paraphrase identification,\nquestion answering and automatic conversation. However, we argue that the\nad-hoc retrieval task is mainly about relevance matching while most NLP\nmatching tasks concern semantic matching, and there are some fundamental\ndifferences between these two matching tasks. Successful relevance matching\nrequires proper handling of the exact matching signals, query term importance,\nand diverse matching requirements. In this paper, we propose a novel deep\nrelevance matching model (DRMM) for ad-hoc retrieval. Specifically, our model\nemploys a joint deep architecture at the query term level for relevance\nmatching. By using matching histogram mapping, a feed forward matching network,\nand a term gating network, we can effectively deal with the three relevance\nmatching factors mentioned above. Experimental results on two representative\nbenchmark collections show that our model can significantly outperform some\nwell-known retrieval models as well as state-of-the-art deep matching models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 08:29:22 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Guo", "Jiafeng", ""], ["Fan", "Yixing", ""], ["Ai", "Qingyao", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1711.08730", "submitter": "Diyah Puspitaningrum", "authors": "D. Puspitaningrum, G. Yulianti, I. S. W. B. Prasetya", "title": "Wiki-MetaSemantik: A Wikipedia-derived Query Expansion Approach based on\n  Network Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the use of Wikipedia for building semantic ontologies to\ndo Query Expansion (QE) in order to improve the search results of search\nengines. In this technique, selecting related Wikipedia concepts becomes\nimportant. We propose the use of network properties (degree, closeness, and\npageRank) to build an ontology graph of user query concepts which is derived\ndirectly from Wikipedia structures. The resulting expansion system is called\nWiki-MetaSemantik. We tested this system against other online thesauruses and\nontology based QE in both individual and meta-search engines setups. Despite\nthat our system has to build a Wikipedia ontology graph in order to do its\nwork, the technique turns out to work very fast (1:281) compared to another\nontology QE baseline (Wikipedia Persian ontology QE). It has thus the potential\nto be utilized online. Furthermore, it shows significant improvement in\naccuracy. Wiki-MetaSemantik also shows better performance in a meta-search\nengine (MSE) set up rather than in an individual search engine set up.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 15:05:24 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Puspitaningrum", "D.", ""], ["Yulianti", "G.", ""], ["Prasetya", "I. S. W. B.", ""]]}, {"id": "1711.08913", "submitter": "Danping Liao", "authors": "Danping Liao, Yuntao Qian", "title": "Paper evolution graph: Multi-view structural retrieval for academic\n  literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Academic literature retrieval is concerned with the selection of papers that\nare most likely to match a user's information needs. Most of the retrieval\nsystems are limited to list-output models, in which the retrieval results are\nisolated from each other. In this work, we aim to uncover the relationships of\nthe retrieval results and propose a method for building structural retrieval\nresults for academic literatures, which we call a paper evolution graph (PEG).\nA PEG describes the evolution of the diverse aspects of input queries through\nseveral evolution chains of papers. By utilizing the author, citation and\ncontent information, PEGs can uncover the various underlying relationships\namong the papers and present the evolution of articles from multiple\nviewpoints. Our system supports three types of input queries: keyword,\nsingle-paper and two-paper queries. The construction of a PEG mainly consists\nof three steps. First, the papers are soft-clustered into communities via\nmetagraph factorization during which the topic distribution of each paper is\nobtained. Second, topically cohesive evolution chains are extracted from the\ncommunities that are relevant to the query. Each chain focuses on one aspect of\nthe query. Finally, the extracted chains are combined to generate a PEG, which\nfully covers all the topics of the query. The experimental results on a\nreal-world dataset demonstrate that the proposed method is able to construct\nmeaningful PEGs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 10:18:55 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Liao", "Danping", ""], ["Qian", "Yuntao", ""]]}, {"id": "1711.08976", "submitter": "Yi Yu", "authors": "Yi Yu, Suhua Tang, Francisco Raposo, Lei Chen", "title": "Deep Cross-Modal Correlation Learning for Audio and Lyrics in Music\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Little research focuses on cross-modal correlation learning where temporal\nstructures of different data modalities such as audio and lyrics are taken into\naccount. Stemming from the characteristic of temporal structures of music in\nnature, we are motivated to learn the deep sequential correlation between audio\nand lyrics. In this work, we propose a deep cross-modal correlation learning\narchitecture involving two-branch deep neural networks for audio modality and\ntext modality (lyrics). Different modality data are converted to the same\ncanonical space where inter modal canonical correlation analysis is utilized as\nan objective function to calculate the similarity of temporal structures. This\nis the first study on understanding the correlation between language and music\naudio through deep architectures for learning the paired temporal correlation\nof audio and lyrics. Pre-trained Doc2vec model followed by fully-connected\nlayers (fully-connected deep neural network) is used to represent lyrics. Two\nsignificant contributions are made in the audio branch, as follows: i)\npre-trained CNN followed by fully-connected layers is investigated for\nrepresenting music audio. ii) We further suggest an end-to-end architecture\nthat simultaneously trains convolutional layers and fully-connected layers to\nbetter learn temporal structures of music audio. Particularly, our end-to-end\ndeep architecture contains two properties: simultaneously implementing feature\nlearning and cross-modal correlation learning, and learning joint\nrepresentation by considering temporal structures. Experimental results, using\naudio to retrieve lyrics or using lyrics to retrieve audio, verify the\neffectiveness of the proposed deep correlation learning architectures in\ncross-modal music retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 14:21:46 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 01:15:53 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Yu", "Yi", ""], ["Tang", "Suhua", ""], ["Raposo", "Francisco", ""], ["Chen", "Lei", ""]]}, {"id": "1711.09174", "submitter": "Hamed Zamani", "authors": "Hamed Zamani, Bhaskar Mitra, Xia Song, Nick Craswell, Saurabh Tiwary", "title": "Neural Ranking Models with Multiple Document Fields", "comments": "To Appear in Proceedings of the 11th ACM International Conference on\n  Web Search and Data Mining (WSDM '17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have recently shown promise in the ad-hoc retrieval\ntask. However, such models have often been based on one field of the document,\nfor example considering document title only or document body only. Since in\npractice documents typically have multiple fields, and given that non-neural\nranking models such as BM25F have been developed to take advantage of document\nstructure, this paper investigates how neural models can deal with multiple\ndocument fields. We introduce a model that can consume short text fields such\nas document title and long text fields such as document body. It can also\nhandle multi-instance fields with variable number of instances, for example\nwhere each document has zero or more instances of incoming anchor text. Since\nfields vary in coverage and quality, we introduce a masking method to handle\nmissing field instances, as well as a field-level dropout method to avoid\nrelying too much on any one field. As in the studies of non-neural field\nweighting, we find it is better for the ranker to score the whole document\njointly, rather than generate a per-field score and aggregate. We find that\ndifferent document fields may match different aspects of the query and\ntherefore benefit from comparing with separate representations of the query\ntext. The combination of techniques introduced here leads to a neural ranker\nthat can take advantage of full document structure, including multiple instance\nand missing instance data, of variable length. The techniques significantly\nenhance the performance of the ranker, and also outperform a learning to rank\nbaseline with hand-crafted features.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 01:12:50 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Zamani", "Hamed", ""], ["Mitra", "Bhaskar", ""], ["Song", "Xia", ""], ["Craswell", "Nick", ""], ["Tiwary", "Saurabh", ""]]}, {"id": "1711.09446", "submitter": "Harrie Oosterhuis", "authors": "Harrie Oosterhuis and Maarten de Rijke", "title": "Balancing Speed and Quality in Online Learning to Rank for Information\n  Retrieval", "comments": "CIKM 2017, Proceedings of the 2017 ACM on Conference on Information\n  and Knowledge Management", "journal-ref": "Proceedings of the 2017 ACM on Conference on Information and\n  Knowledge Management, Pages 277-286", "doi": "10.1145/3132847.3132896", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Online Learning to Rank (OLTR) the aim is to find an optimal ranking model\nby interacting with users. When learning from user behavior, systems must\ninteract with users while simultaneously learning from those interactions.\nUnlike other Learning to Rank (LTR) settings, existing research in this field\nhas been limited to linear models. This is due to the speed-quality tradeoff\nthat arises when selecting models: complex models are more expressive and can\nfind the best rankings but need more user interactions to do so, a requirement\nthat risks frustrating users during training. Conversely, simpler models can be\noptimized on fewer interactions and thus provide a better user experience, but\nthey will converge towards suboptimal rankings. This tradeoff creates a\ndeadlock, since novel models will not be able to improve either the user\nexperience or the final convergence point, without sacrificing the other. Our\ncontribution is twofold. First, we introduce a fast OLTR model called Sim-MGD\nthat addresses the speed aspect of the speed-quality tradeoff. Sim-MGD ranks\ndocuments based on similarities with reference documents. It converges rapidly\nand, hence, gives a better user experience but it does not converge towards the\noptimal rankings. Second, we contribute Cascading Multileave Gradient Descent\n(C-MGD) for OLTR that directly addresses the speed-quality tradeoff by using a\ncascade that enables combinations of the best of two worlds: fast learning and\nhigh quality final convergence. C-MGD can provide the better user experience of\nSim-MGD while maintaining the same convergence as the state-of-the-art MGD\nmodel. This opens the door for future work to design new models for OLTR\nwithout having to deal with the speed-quality tradeoff.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 19:56:19 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Oosterhuis", "Harrie", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1711.09454", "submitter": "Harrie Oosterhuis", "authors": "Harrie Oosterhuis and Maarten de Rijke", "title": "Sensitive and Scalable Online Evaluation with Theoretical Guarantees", "comments": "CIKM 2017, Proceedings of the 2017 ACM on Conference on Information\n  and Knowledge Management", "journal-ref": "Proceedings of the 2017 ACM on Conference on Information and\n  Knowledge Management, Pages 77-86", "doi": "10.1145/3132847.3132895", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multileaved comparison methods generalize interleaved comparison methods to\nprovide a scalable approach for comparing ranking systems based on regular user\ninteractions. Such methods enable the increasingly rapid research and\ndevelopment of search engines. However, existing multileaved comparison methods\nthat provide reliable outcomes do so by degrading the user experience during\nevaluation. Conversely, current multileaved comparison methods that maintain\nthe user experience cannot guarantee correctness. Our contribution is two-fold.\nFirst, we propose a theoretical framework for systematically comparing\nmultileaved comparison methods using the notions of considerateness, which\nconcerns maintaining the user experience, and fidelity, which concerns reliable\ncorrect outcomes. Second, we introduce a novel multileaved comparison method,\nPairwise Preference Multileaving (PPM), that performs comparisons based on\ndocument-pair preferences, and prove that it is considerate and has fidelity.\nWe show empirically that, compared to previous multileaved comparison methods,\nPPM is more sensitive to user preferences and scalable with the number of\nrankers being compared.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 20:40:57 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Oosterhuis", "Harrie", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1711.09471", "submitter": "Samson Chibuta Mr", "authors": "Billy Zimba, Samson Chibuta, David Chisanga, Fredah Banda, Jackson\n  Phiri", "title": "Point of Interest Recommendation Methods in Location Based Social\n  Networks: Traveling to a new geographical region", "comments": "This research is under review due to conflict of interest", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommender systems in location based social networks mainly take advantage\nof social and geographical influence in making personalized Points-of-interest\n(POI) recommendations. The social influence is obtained from social network\nfriends or similar users based on matching visit history whilst the\ngeographical influence is obtained from the geographical footprints users'\nleave when they check-in at different POIs. However, this approach may fall\nshort when a user moves to a new region where they have little or no activity\nhistory. We propose a location aware POI recommendation system that models user\npreferences mainly based on; user reviews and categories of POIs. We evaluate\nour algorithm on the Yelp dataset and the experimental results show that our\nalgorithm achieves a better accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 22:14:35 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 09:48:18 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2018 14:36:58 GMT"}, {"version": "v4", "created": "Sat, 25 Jan 2020 09:56:37 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Zimba", "Billy", ""], ["Chibuta", "Samson", ""], ["Chisanga", "David", ""], ["Banda", "Fredah", ""], ["Phiri", "Jackson", ""]]}, {"id": "1711.09559", "submitter": "Xiaohui Xie", "authors": "Xiaohui Xie, Yiqun Liu, Maarten de Rijke, Jiyin He, Min Zhang,\n  Shaoping Ma", "title": "Why People Search for Images using Web Search Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What are the intents or goals behind human interactions with image search\nengines? Knowing why people search for images is of major concern to Web image\nsearch engines because user satisfaction may vary as intent varies. Previous\nanalyses of image search behavior have mostly been query-based, focusing on\nwhat images people search for, rather than intent-based, that is, why people\nsearch for images. To date, there is no thorough investigation of how different\nimage search intents affect users' search behavior.\n  In this paper, we address the following questions: (1)Why do people search\nfor images in text-based Web image search systems? (2)How does image search\nbehavior change with user intent? (3)Can we predict user intent effectively\nfrom interactions during the early stages of a search session? To this end, we\nconduct both a lab-based user study and a commercial search log analysis.\n  We show that user intents in image search can be grouped into three classes:\nExplore/Learn, Entertain, and Locate/Acquire. Our lab-based user study reveals\ndifferent user behavior patterns under these three intents, such as first click\ntime, query reformulation, dwell time and mouse movement on the result page.\nBased on user interaction features during the early stages of an image search\nsession, that is, before mouse scroll, we develop an intent classifier that is\nable to achieve promising results for classifying intents into our three intent\nclasses. Given that all features can be obtained online and unobtrusively, the\npredicted intents can provide guidance for choosing ranking methods immediately\nafter scrolling.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 06:38:40 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Xie", "Xiaohui", ""], ["Liu", "Yiqun", ""], ["de Rijke", "Maarten", ""], ["He", "Jiyin", ""], ["Zhang", "Min", ""], ["Ma", "Shaoping", ""]]}, {"id": "1711.09645", "submitter": "Stefanos Angelidis", "authors": "Stefanos Angelidis, Mirella Lapata", "title": "Multiple Instance Learning Networks for Fine-Grained Sentiment Analysis", "comments": "Final published version. Please cite using appropriate date (2018).\n  Link to journal:\n  http://www.transacl.org/ojs/index.php/tacl/article/view/1225/277", "journal-ref": "Transactions of the Association for Computational Linguistics\n  (TACL), 2018, Volume 6, pages 17-31", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the task of fine-grained sentiment analysis from the perspective\nof multiple instance learning (MIL). Our neural model is trained on document\nsentiment labels, and learns to predict the sentiment of text segments, i.e.\nsentences or elementary discourse units (EDUs), without segment-level\nsupervision. We introduce an attention-based polarity scoring method for\nidentifying positive and negative text snippets and a new dataset which we call\nSPOT (as shorthand for Segment-level POlariTy annotations) for evaluating\nMIL-style sentiment models like ours. Experimental results demonstrate superior\nperformance against multiple baselines, whereas a judgement elicitation study\nshows that EDU-level opinion extraction produces more informative summaries\nthan sentence-based alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 12:21:22 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 15:53:12 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Angelidis", "Stefanos", ""], ["Lapata", "Mirella", ""]]}, {"id": "1711.09708", "submitter": "Marta Arias", "authors": "Marta Arias, Argimiro Arratia, Ariel Duarte-Lopez", "title": "Classifier Selection with Permutation Tests", "comments": "20th International Conference of the Catalan Association for\n  Artificial Intelligence (CCIA 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a content-based recommender system for machine learning\nclassifier algorithms. Given a new data set, a recommendation of what\nclassifier is likely to perform best is made based on classifier performance\nover similar known data sets. This similarity is measured according to a data\nset characterization that includes several state-of-the-art metrics taking into\naccount physical structure, statis- tics, and information theory. A novelty\nwith respect to prior work is the use of a robust approach based on permutation\ntests to directly assess whether a given learning algorithm is able to exploit\nthe attributes in a data set to predict class labels, and compare it to the\nmore commonly used F-score metric for evalu- ating classifier performance. To\nevaluate our approach, we have conducted an extensive experimentation including\n8 of the main machine learning classification methods with varying\nconfigurations and 65 bi- nary data sets, leading to over 2331 experiments. Our\nresults show that using the information from the permutation test clearly\nimproves the quality of the recommendations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 14:37:59 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Arias", "Marta", ""], ["Arratia", "Argimiro", ""], ["Duarte-Lopez", "Ariel", ""]]}, {"id": "1711.09822", "submitter": "Clemens Marschner", "authors": "Aayush Garg, Thilo Will, William Darling, Willi Richert, Clemens\n  Marschner", "title": "Scalable Object Detection for Stylized Objects", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following recent breakthroughs in convolutional neural networks and\nmonolithic model architectures, state-of-the-art object detection models can\nreliably and accurately scale into the realm of up to thousands of classes.\nThings quickly break down, however, when scaling into the tens of thousands,\nor, eventually, to millions or billions of unique objects. Further, bounding\nbox-trained end-to-end models require extensive training data. Even though -\nwith some tricks using hierarchies - one can sometimes scale up to thousands of\nclasses, the labor requirements for clean image annotations quickly get out of\ncontrol. In this paper, we present a two-layer object detection method for\nbrand logos and other stylized objects for which prototypical images exist. It\ncan scale to large numbers of unique classes. Our first layer is a CNN from the\nSingle Shot Multibox Detector family of models that learns to propose regions\nwhere some stylized object is likely to appear. The contents of a proposed\nbounding box is then run against an image index that is targeted for the\nretrieval task at hand. The proposed architecture scales to a large number of\nobject classes, allows to continously add new classes without retraining, and\nexhibits state-of-the-art quality on a stylized object detection task such as\nlogo recognition.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 16:46:09 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 10:04:56 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Garg", "Aayush", ""], ["Will", "Thilo", ""], ["Darling", "William", ""], ["Richert", "Willi", ""], ["Marschner", "Clemens", ""]]}, {"id": "1711.09825", "submitter": "Andreas Veit", "authors": "Andreas Veit, Maximilian Nickel, Serge Belongie, Laurens van der\n  Maaten", "title": "Separating Self-Expression and Visual Content in Hashtag Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variety, abundance, and structured nature of hashtags make them an\ninteresting data source for training vision models. For instance, hashtags have\nthe potential to significantly reduce the problem of manual supervision and\nannotation when learning vision models for a large number of concepts. However,\na key challenge when learning from hashtags is that they are inherently\nsubjective because they are provided by users as a form of self-expression. As\na consequence, hashtags may have synonyms (different hashtags referring to the\nsame visual content) and may be ambiguous (the same hashtag referring to\ndifferent visual content). These challenges limit the effectiveness of\napproaches that simply treat hashtags as image-label pairs. This paper presents\nan approach that extends upon modeling simple image-label pairs by modeling the\njoint distribution of images, hashtags, and users. We demonstrate the efficacy\nof such approaches in image tagging and retrieval experiments, and show how the\njoint model can be used to perform user-conditional retrieval and tagging.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 16:50:52 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Veit", "Andreas", ""], ["Nickel", "Maximilian", ""], ["Belongie", "Serge", ""], ["van der Maaten", "Laurens", ""]]}, {"id": "1711.10046", "submitter": "Morteza Mardani", "authors": "Morteza Mardani, Hatef Monajemi, Vardan Papyan, Shreyas Vasanawala,\n  David Donoho, and John Pauly", "title": "Recurrent Generative Adversarial Networks for Proximal Learning and\n  Automated Compressive Image Recovery", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering images from undersampled linear measurements typically leads to an\nill-posed linear inverse problem, that asks for proper statistical priors.\nBuilding effective priors is however challenged by the low train and test\noverhead dictated by real-time tasks; and the need for retrieving visually\n\"plausible\" and physically \"feasible\" images with minimal hallucination. To\ncope with these challenges, we design a cascaded network architecture that\nunrolls the proximal gradient iterations by permeating benefits from generative\nresidual networks (ResNet) to modeling the proximal operator. A mixture of\npixel-wise and perceptual costs is then deployed to train proximals. The\noverall architecture resembles back-and-forth projection onto the intersection\nof feasible and plausible images. Extensive computational experiments are\nexamined for a global task of reconstructing MR images of pediatric patients,\nand a more local task of superresolving CelebA faces, that are insightful to\ndesign efficient architectures. Our observations indicate that for MRI\nreconstruction, a recurrent ResNet with a single residual block effectively\nlearns the proximal. This simple architecture appears to significantly\noutperform the alternative deep ResNet architecture by 2dB SNR, and the\nconventional compressed-sensing MRI by 4dB SNR with 100x faster inference. For\nimage superresolution, our preliminary results indicate that modeling the\ndenoising proximal demands deep ResNets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 23:45:02 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Mardani", "Morteza", ""], ["Monajemi", "Hatef", ""], ["Papyan", "Vardan", ""], ["Vasanawala", "Shreyas", ""], ["Donoho", "David", ""], ["Pauly", "John", ""]]}, {"id": "1711.10307", "submitter": "Jean-Fran\\c{c}ois Delpech", "authors": "Jean-Fran\\c{c}ois Delpech", "title": "Semantic Technology-Assisted Review (STAR) Document analysis and\n  monitoring using random vectors", "comments": "13 pages, 9 tables, 21 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The review and analysis of large collections of documents and the periodic\nmonitoring of new additions thereto has greatly benefited from new developments\nin computer software. This paper demonstrates how using random vectors to\nconstruct a low-dimensional Euclidean space embedding words and documents\nenables fast and accurate computation of semantic similarities between them.\nWith this technique of Semantic Technology-Assisted Review (STAR), documents\ncan be selected, compared, classified, summarized and evaluated very quickly\nwith minimal expert involvement and high-quality results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 14:28:54 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 13:50:46 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Delpech", "Jean-Fran\u00e7ois", ""]]}, {"id": "1711.10327", "submitter": "Danijar Hafner", "authors": "Danijar Hafner, Alexander Immer, Willi Raschkowski, Fabian Windheuser", "title": "Generative Interest Estimation for Document Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning distributed representations of documents has pushed the\nstate-of-the-art in several natural language processing tasks and was\nsuccessfully applied to the field of recommender systems recently. In this\npaper, we propose a novel content-based recommender system based on learned\nrepresentations and a generative model of user interest. Our method works as\nfollows: First, we learn representations on a corpus of text documents. Then,\nwe capture a user's interest as a generative model in the space of the document\nrepresentations. In particular, we model the distribution of interest for each\nuser as a Gaussian mixture model (GMM). Recommendations can be obtained\ndirectly by sampling from a user's generative model. Using Latent semantic\nanalysis (LSA) as comparison, we compute and explore document representations\non the Delicious bookmarks dataset, a standard benchmark for recommender\nsystems. We then perform density estimation in both spaces and show that\nlearned representations outperform LSA in terms of predictive performance.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 15:00:24 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Hafner", "Danijar", ""], ["Immer", "Alexander", ""], ["Raschkowski", "Willi", ""], ["Windheuser", "Fabian", ""]]}, {"id": "1711.10377", "submitter": "Hamid Bagheri", "authors": "Hamid Bagheri, Md Johirul Islam", "title": "Sentiment analysis of twitter data", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks are the main resources to gather information about people's\nopinion and sentiments towards different topics as they spend hours daily on\nsocial media and share their opinion. In this technical paper, we show the\napplication of sentimental analysis and how to connect to Twitter and run\nsentimental analysis queries. We run experiments on different queries from\npolitics to humanity and show the interesting results. We realized that the\nneutral sentiments for tweets are significantly high which clearly shows the\nlimitations of the current works.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 17:32:59 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 03:51:43 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Bagheri", "Hamid", ""], ["Islam", "Md Johirul", ""]]}, {"id": "1711.10399", "submitter": "Xiao-Long Ren", "authors": "Xiaofang Deng, Leilei Wu, Xiaolong Ren, Chunxiao Jia, Yuansheng Zhong,\n  Linyuan L\\\"u", "title": "Inferring users' preferences through leveraging their social\n  relationships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems, inferring users' preferences from their historical\nactivities and personal profiles, have been an enormous success in the last\nseveral years. Most of the existing works are based on the similarities of\nusers, objects or both that derived from their purchases records in the online\nshopping platforms. Such approaches, however, are facing bottlenecks when the\nknown information is limited. The extreme case is how to recommend products to\nnew users, namely the so-called cold-start problem. The rise of the online\nsocial networks gives us a chance to break the glass ceiling. Birds of a\nfeather flock together. Close friends may have similar hidden pattern of\nselecting products and the advices from friends are more trustworthy.\n  In this paper, we integrate the individual's social relationships into\nrecommender systems and propose a new method, called Social Mass Diffusion\n(SMD), based on a mass diffusion process in the combined network of users'\nsocial network and user-item bipartite network. The results show that the SMD\nalgorithm can achieve higher recommendation accuracy than the Mass Diffusion\n(MD) purely on the bipartite network. Especially, the improvement is striking\nfor small degree users. Moreover, SMD provides a good solution to the\ncold-start problem. The recommendation accuracy for new users significantly\nhigher than that of the conventional popularity-based algorithm. These results\nmay shed some light on the new designs of better personalized recommender\nsystems and information services.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 16:53:18 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Deng", "Xiaofang", ""], ["Wu", "Leilei", ""], ["Ren", "Xiaolong", ""], ["Jia", "Chunxiao", ""], ["Zhong", "Yuansheng", ""], ["L\u00fc", "Linyuan", ""]]}, {"id": "1711.10558", "submitter": "Biswarup Bhattacharya", "authors": "Biswarup Bhattacharya, Iftikhar Burhanuddin, Abhilasha Sancheti,\n  Kushal Satya", "title": "Intent-Aware Contextual Recommendation System", "comments": "Presented at the 5th International Workshop on Data Science and Big\n  Data Analytics (DSBDA), 17th IEEE International Conference on Data Mining\n  (ICDM) 2017; 8 pages; 4 figures; Due to the limitation \"The abstract field\n  cannot be longer than 1,920 characters,\" the abstract appearing here is\n  slightly shorter than the one in the PDF file", "journal-ref": null, "doi": "10.1109/ICDMW.2017.8", "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems take inputs from user history, use an internal ranking\nalgorithm to generate results and possibly optimize this ranking based on\nfeedback. However, often the recommender system is unaware of the actual intent\nof the user and simply provides recommendations dynamically without properly\nunderstanding the thought process of the user. An intelligent recommender\nsystem is not only useful for the user but also for businesses which want to\nlearn the tendencies of their users. Finding out tendencies or intents of a\nuser is a difficult problem to solve.\n  Keeping this in mind, we sought out to create an intelligent system which\nwill keep track of the user's activity on a web-application as well as\ndetermine the intent of the user in each session. We devised a way to encode\nthe user's activity through the sessions. Then, we have represented the\ninformation seen by the user in a high dimensional format which is reduced to\nlower dimensions using tensor factorization techniques. The aspect of intent\nawareness (or scoring) is dealt with at this stage. Finally, combining the user\nactivity data with the contextual information gives the recommendation score.\nThe final recommendations are then ranked using filtering and collaborative\nrecommendation techniques to show the top-k recommendations to the user. A\nprovision for feedback is also envisioned in the current system which informs\nthe model to update the various weights in the recommender system. Our overall\nmodel aims to combine both frequency-based and context-based recommendation\nsystems and quantify the intent of a user to provide better recommendations.\n  We ran experiments on real-world timestamped user activity data, in the\nsetting of recommending reports to the users of a business analytics tool and\nthe results are better than the baselines. We also tuned certain aspects of our\nmodel to arrive at optimized results.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:58:52 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Bhattacharya", "Biswarup", ""], ["Burhanuddin", "Iftikhar", ""], ["Sancheti", "Abhilasha", ""], ["Satya", "Kushal", ""]]}, {"id": "1711.10795", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Eva Mohedano, Kevin McGuinness, Xavier Giro-i-Nieto and Noel E.\n  O'Connor", "title": "Saliency Weighted Convolutional Features for Instance Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores attention models to weight the contribution of local\nconvolutional representations for the instance search task. We present a\nretrieval framework based on bags of local convolutional features (BLCF) that\nbenefits from saliency weighting to build an efficient image representation.\nThe use of human visual attention models (saliency) allows significant\nimprovements in retrieval performance without the need to conduct region\nanalysis or spatial verification, and without requiring any feature fine\ntuning. We investigate the impact of different saliency models, finding that\nhigher performance on saliency benchmarks does not necessarily equate to\nimproved performance when used in instance search tasks. The proposed approach\noutperforms the state-of-the-art on the challenging INSTRE benchmark by a large\nmargin, and provides similar performance on the Oxford and Paris benchmarks\ncompared to more complex methods that use off-the-shelf representations. The\nsource code used in this project is available at\nhttps://imatge-upc.github.io/salbow/\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 11:46:56 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Mohedano", "Eva", ""], ["McGuinness", "Kevin", ""], ["Giro-i-Nieto", "Xavier", ""], ["O'Connor", "Noel E.", ""]]}, {"id": "1711.10816", "submitter": "Piotr Mardziel", "authors": "Anupam Datta and Sophia Kovaleva and Piotr Mardziel and Shayak Sen", "title": "Latent Factor Interpretations for Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning systems utilize latent factors as internal\nrepresentations for making predictions. Since these latent factors are largely\nuninterpreted, however, predictions made using them are opaque. Collaborative\nfiltering via matrix factorization is a prime example of such an algorithm that\nuses uninterpreted latent features, and yet has seen widespread adoption for\nmany recommendation tasks. We present Latent Factor Interpretation (LFI), a\nmethod for interpreting models by leveraging interpretations of latent factors\nin terms of human-understandable features. The interpretation of latent factors\ncan then replace the uninterpreted latent factors, resulting in a new model\nthat expresses predictions in terms of interpretable features. This new model\ncan then be interpreted using recently developed model explanation techniques.\nIn this paper we develop LFI for collaborative filtering based recommender\nsystems. We illustrate the use of LFI interpretations on the MovieLens dataset,\nintegrating auxiliary features from IMDB and DB tropes, and show that latent\nfactors can be predicted with sufficient accuracy for replicating the\npredictions of the true model.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 12:34:18 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2018 22:31:07 GMT"}, {"version": "v3", "created": "Mon, 9 Apr 2018 21:04:32 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Datta", "Anupam", ""], ["Kovaleva", "Sophia", ""], ["Mardziel", "Piotr", ""], ["Sen", "Shayak", ""]]}, {"id": "1711.11071", "submitter": "Antonia Korba", "authors": "Antonia Korba", "title": "HSC: A Novel Method for Clustering Hierarchies of Networked Data", "comments": "This is a thesis project, it isn't sufficiently exhaustive", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical clustering is one of the most powerful solutions to the problem\nof clustering, on the grounds that it performs a multi scale organization of\nthe data. In recent years, research on hierarchical clustering methods has\nattracted considerable interest due to the demanding modern application\ndomains.\n  We present a novel divisive hierarchical clustering framework called\nHierarchical Stochastic Clustering (HSC), that acts in two stages. In the first\nstage, it finds a primary hierarchy of clustering partitions in a dataset. In\nthe second stage, feeds a clustering algorithm with each one of the clusters of\nthe very detailed partition, in order to settle the final result. The output is\na hierarchy of clusters. Our method is based on the previous research of Meyer\nand Weissel Stochastic Data Clustering and the theory of Simon and Ando on\nVariable Aggregation.\n  Our experiments show that our framework builds a meaningful hierarchy of\nclusters and benefits consistently the clustering algorithm that acts in the\nsecond stage, not only computationally but also in terms of cluster quality.\nThis result suggest that HSC framework is ideal for obtaining hierarchical\nsolutions of large volumes of data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 19:29:16 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 11:56:28 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Korba", "Antonia", ""]]}, {"id": "1711.11458", "submitter": "Menghan Wang", "authors": "Menghan Wang, Xiaolin Zheng, Yang Yang, Kun Zhang", "title": "Collaborative Filtering with Social Exposure: A Modular Approach to\n  Social Recommendation", "comments": "Accepted for publication at the 32nd Conference on Artificial\n  Intelligence (AAAI 2018), New Orleans, Louisiana", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with how to make efficient use of social information\nto improve recommendations. Most existing social recommender systems assume\npeople share similar preferences with their social friends. Which, however, may\nnot hold true due to various motivations of making online friends and dynamics\nof online social networks. Inspired by recent causal process based\nrecommendations that first model user exposures towards items and then use\nthese exposures to guide rating prediction, we utilize social information to\ncapture user exposures rather than user preferences. We assume that people get\ninformation of products from their online friends and they do not have to share\nsimilar preferences, which is less restrictive and seems closer to reality.\nUnder this new assumption, in this paper, we present a novel recommendation\napproach (named SERec) to integrate social exposure into collaborative\nfiltering. We propose two methods to implement SERec, namely social\nregularization and social boosting, each with different ways to construct\nsocial exposures. Experiments on four real-world datasets demonstrate that our\nmethods outperform the state-of-the-art methods on top-N recommendations.\nFurther study compares the robustness and scalability of the two proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 15:06:02 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Wang", "Menghan", ""], ["Zheng", "Xiaolin", ""], ["Yang", "Yang", ""], ["Zhang", "Kun", ""]]}, {"id": "1711.11508", "submitter": "Ming Liu", "authors": "Ming Liu, Bo Lang, and Zepeng Gu", "title": "Calculating Semantic Similarity between Academic Articles using Topic\n  Event and Ontology", "comments": "21 pages, 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining semantic similarity between academic documents is crucial to many\ntasks such as plagiarism detection, automatic technical survey and semantic\nsearch. Current studies mostly focus on semantic similarity between concepts,\nsentences and short text fragments. However, document-level semantic matching\nis still based on statistical information in surface level, neglecting article\nstructures and global semantic meanings, which may cause the deviation in\ndocument understanding. In this paper, we focus on the document-level semantic\nsimilarity issue for academic literatures with a novel method. We represent\nacademic articles with topic events that utilize multiple information profiles,\nsuch as research purposes, methodologies and domains to integrally describe the\nresearch work, and calculate the similarity between topic events based on the\ndomain ontology to acquire the semantic similarity between articles.\nExperiments show that our approach achieves significant performance compared to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 16:58:46 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Liu", "Ming", ""], ["Lang", "Bo", ""], ["Gu", "Zepeng", ""]]}]