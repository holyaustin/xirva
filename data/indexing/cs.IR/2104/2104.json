[{"id": "2104.00305", "submitter": "Dong Yao", "authors": "Dong Yao, Shengyu Zhang, Zhou Zhao, Wenyan Fan, Jieming Zhu, Xiuqiang\n  He, Fei Wu", "title": "Modeling High-order Interactions across Multi-interests for Micro-video\n  Recommendation", "comments": "accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalized recommendation system has become pervasive in various video\nplatform. Many effective methods have been proposed, but most of them didn't\ncapture the user's multi-level interest trait and dependencies between their\nviewed micro-videos well. To solve these problems, we propose a Self-over-Co\nAttention module to enhance user's interest representation. In particular, we\nfirst use co-attention to model correlation patterns across different levels\nand then use self-attention to model correlation patterns within a specific\nlevel. Experimental results on filtered public datasets verify that our\npresented module is useful.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:20:15 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 15:02:57 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yao", "Dong", ""], ["Zhang", "Shengyu", ""], ["Zhao", "Zhou", ""], ["Fan", "Wenyan", ""], ["Zhu", "Jieming", ""], ["He", "Xiuqiang", ""], ["Wu", "Fei", ""]]}, {"id": "2104.00437", "submitter": "Andres Ferraro", "authors": "Andres Ferraro, Xavier Favory, Konstantinos Drossos, Yuntae Kim and\n  Dmitry Bogdanov", "title": "Enriched Music Representations with Multiple Cross-modal Contrastive\n  Learning", "comments": "Accepted for publication to IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2021.3071082", "report-no": "SPL-30069-2021", "categories": "cs.SD cs.IR cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modeling various aspects that make a music piece unique is a challenging\ntask, requiring the combination of multiple sources of information. Deep\nlearning is commonly used to obtain representations using various sources of\ninformation, such as the audio, interactions between users and songs, or\nassociated genre metadata. Recently, contrastive learning has led to\nrepresentations that generalize better compared to traditional supervised\nmethods. In this paper, we present a novel approach that combines multiple\ntypes of information related to music using cross-modal contrastive learning,\nallowing us to learn an audio feature from heterogeneous data simultaneously.\nWe align the latent representations obtained from playlists-track interactions,\ngenre metadata, and the tracks' audio, by maximizing the agreement between\nthese modality representations using a contrastive loss. We evaluate our\napproach in three tasks, namely, genre classification, playlist continuation\nand automatic tagging. We compare the performances with a baseline audio-based\nCNN trained to predict these modalities. We also study the importance of\nincluding multiple sources of information when training our embedding model.\nThe results suggest that the proposed method outperforms the baseline in all\nthe three downstream tasks and achieves comparable performance to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 12:41:15 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Ferraro", "Andres", ""], ["Favory", "Xavier", ""], ["Drossos", "Konstantinos", ""], ["Kim", "Yuntae", ""], ["Bogdanov", "Dmitry", ""]]}, {"id": "2104.00782", "submitter": "Filipo Sharevski", "authors": "Peter Jachim, Filipo Sharevski, Emma Pieroni", "title": "\"TL;DR:\" Out-of-Context Adversarial Text Summarization and Hashtag\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents Out-of-Context Summarizer, a tool that takes arbitrary\npublic news articles out of context by summarizing them to coherently fit\neither a liberal- or conservative-leaning agenda. The Out-of-Context Summarizer\nalso suggests hashtag keywords to bolster the polarization of the summary, in\ncase one is inclined to take it to Twitter, Parler or other platforms for\ntrolling. Out-of-Context Summarizer achieved 79% precision and 99% recall when\nsummarizing COVID-19 articles, 93% precision and 93% recall when summarizing\npolitically-centered articles, and 87% precision and 88% recall when taking\nliberally-biased articles out of context. Summarizing valid sources instead of\nsynthesizing fake text, the Out-of-Context Summarizer could fairly pass the\n\"adversarial disclosure\" test, but we didn't take this easy route in our paper.\nInstead, we used the Out-of-Context Summarizer to push the debate of potential\nmisuse of automated text generation beyond the boilerplate text of responsible\ndisclosure of adversarial language models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 22:03:44 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Jachim", "Peter", ""], ["Sharevski", "Filipo", ""], ["Pieroni", "Emma", ""]]}, {"id": "2104.00860", "submitter": "Yufei Feng", "authors": "Yufei Feng, Binbin Hu, Yu Gong, Fei Sun, Qingwen Liu, Wenwu Ou", "title": "GRN: Generative Rerank Network for Context-wise Recommendation", "comments": "Better read with arXiv:2102.12057. arXiv admin note: text overlap\n  with arXiv:2102.12057", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reranking is attracting incremental attention in the recommender systems,\nwhich rearranges the input ranking list into the final rank-ing list to better\nmeet user demands. Most existing methods greedily rerank candidates through the\nrating scores from point-wise or list-wise models. Despite effectiveness,\nneglecting the mutual influence between each item and its contexts in the final\nranking list often makes the greedy strategy based reranking methods\nsub-optimal. In this work, we propose a new context-wise reranking framework\nnamed Generative Rerank Network (GRN). Specifically, we first design the\nevaluator, which applies Bi-LSTM and self-attention mechanism to model the\ncontextual information in the labeled final ranking list and predict the\ninteraction probability of each item more precisely. Afterwards, we elaborate\non the generator, equipped with GRU, attention mechanism and pointer network to\nselect the item from the input ranking list step by step. Finally, we apply\ncross-entropy loss to train the evaluator and, subsequently, policy gradient to\noptimize the generator under the guidance of the evaluator. Empirical results\nshow that GRN consistently and significantly outperforms state-of-the-art\npoint-wise and list-wise methods. Moreover, GRN has achieved a performance\nimprovement of 5.2% on PV and 6.1% on IPV metric after the successful\ndeployment in one popular recommendation scenario of Taobao application.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 02:40:23 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 01:48:53 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Feng", "Yufei", ""], ["Hu", "Binbin", ""], ["Gong", "Yu", ""], ["Sun", "Fei", ""], ["Liu", "Qingwen", ""], ["Ou", "Wenwu", ""]]}, {"id": "2104.00919", "submitter": "Qinyong Wang", "authors": "Qinyong Wang, Hongzhi Yin, Tong Chen, Junliang Yu, Alexander Zhou and\n  Xiangliang Zhang", "title": "Fast-adapting and Privacy-preserving Federated Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the mobile Internet era, recommender systems have become an irreplaceable\ntool to help users discover useful items, thus alleviating the information\noverload problem. Recent research on deep neural network (DNN)-based\nrecommender systems have made significant progress in improving prediction\naccuracy, largely attributed to the widely accessible large-scale user data.\nSuch data is commonly collected from users' personal devices, and then\ncentrally stored in the cloud server to facilitate model training. However,\nwith the rising public concerns on user privacy leakage in online platforms,\nonline users are becoming increasingly anxious over abuses of user privacy.\nTherefore, it is urgent and beneficial to develop a recommender system that can\nachieve both high prediction accuracy and strong privacy protection.\n  To this end, we propose a DNN-based recommendation model called PrivRec\nrunning on the decentralized federated learning (FL) environment, which ensures\nthat a user's data is fully retained on her/his personal device while\ncontributing to training an accurate model. On the other hand, to better\nembrace the data heterogeneity (e.g., users' data vary in scale and quality\nsignificantly) in FL, we innovatively introduce a first-order meta-learning\nmethod that enables fast on-device personalization with only a few data points.\nFurthermore, to defend against potential malicious participants that pose\nserious security threat to other users, we further develop a user-level\ndifferentially private model, namely DP-PrivRec, so attackers are unable to\nidentify any arbitrary user from the trained model. Finally, we conduct\nextensive experiments on two large-scale datasets in a simulated FL\nenvironment, and the results validate the superiority of both PrivRec and\nDP-PrivRec.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 07:42:02 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 09:44:28 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Wang", "Qinyong", ""], ["Yin", "Hongzhi", ""], ["Chen", "Tong", ""], ["Yu", "Junliang", ""], ["Zhou", "Alexander", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "2104.00948", "submitter": "Angelo Salatino", "authors": "Angelo A. Salatino, Francesco Osborne, Thiviyan Thanapalasingam,\n  Enrico Motta", "title": "The CSO Classifier: Ontology-Driven Detection of Research Topics in\n  Scholarly Articles", "comments": "Conference paper at TPDL 2019", "journal-ref": "In Digital Libraries for Open Knowledge. LNCS, vol 11799.\n  Springer, Cham (2019)", "doi": "10.1007/978-3-030-30760-8_26", "report-no": null, "categories": "cs.IR cs.AI cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classifying research papers according to their research topics is an\nimportant task to improve their retrievability, assist the creation of smart\nanalytics, and support a variety of approaches for analysing and making sense\nof the research environment. In this paper, we present the CSO Classifier, a\nnew unsupervised approach for automatically classifying research papers\naccording to the Computer Science Ontology (CSO), a comprehensive ontology of\nre-search areas in the field of Computer Science. The CSO Classifier takes as\ninput the metadata associated with a research paper (title, abstract, keywords)\nand returns a selection of research concepts drawn from the ontology. The\napproach was evaluated on a gold standard of manually annotated articles\nyielding a significant improvement over alternative methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 09:02:32 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Salatino", "Angelo A.", ""], ["Osborne", "Francesco", ""], ["Thanapalasingam", "Thiviyan", ""], ["Motta", "Enrico", ""]]}, {"id": "2104.00975", "submitter": "Gabriella Tognola", "authors": "Emma Chiaramello, Francesco Pinciroli, Alberico Bonalumi, Angelo\n  Caroli, Gabriella Tognola", "title": "Use of 'off-the-shelf' information extraction algorithms in clinical\n  informatics: a feasibility study of MetaMap annotation of Italian medical\n  notes", "comments": "This paper has been published in the Journal of biomedical\n  informatics, Volume 63, October 2016, Pages 22-32", "journal-ref": "Journal of biomedical informatics, Volume 63, October 2016, Pages\n  22-32", "doi": "10.1016/j.jbi.2016.07.017", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Information extraction from narrative clinical notes is useful for patient\ncare, as well as for secondary use of medical data, for research or clinical\npurposes. Many studies focused on information extraction from English clinical\ntexts, but less dealt with clinical notes in languages other than English. This\nstudy tested the feasibility of using 'off the shelf' information extraction\nalgorithms to identify medical concepts from Italian clinical notes. We used\nMetaMap to map medical concepts to the Unified Medical Language System (UMLS).\nThe study addressed two questions: (Q1) to understand if it would be possible\nto properly map medical terms found in clinical notes and related to the\nsemantic group of 'Disorders' to the Italian UMLS resources; (Q2) to\ninvestigate if it would be feasible to use MetaMap as it is to extract these\nmedical concepts from Italian clinical notes. Results in EXP1 showed that the\nItalian UMLS Metathesaurus sources covered 91% of the medical terms of the\n'Disorders' semantic group, as found in the studied dataset. Even if MetaMap\nwas built to analyze texts written in English, it worked properly also with\ntexts written in Italian. MetaMap identified correctly about half of the\nconcepts in the Italian clinical notes. Using MetaMap's annotation on Italian\nclinical notes instead of a simple text search improved our results of about 15\npercentage points. MetaMap showed recall, precision and F-measure of 0.53, 0.98\nand 0.69, respectively. Most of the failures were due to the impossibility for\nMetaMap to generate Italian meaningful variants. MetaMap's performance in\nannotating automatically translated English clinical notes was in line with\nfindings in the literature, with similar recall (0.75), F-measure (0.83) and\neven higher precision (0.95).\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 10:28:50 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Chiaramello", "Emma", ""], ["Pinciroli", "Francesco", ""], ["Bonalumi", "Alberico", ""], ["Caroli", "Angelo", ""], ["Tognola", "Gabriella", ""]]}, {"id": "2104.01105", "submitter": "Saeedeh Shekarpour", "authors": "Sunday C. Ngwobia and Saeedeh Shekarpour and Faisal Alshargi", "title": "Capturing Knowledge of Emerging Entities From Extended Search Snippets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Google and other search engines feature the entity search by representing a\nknowledge card summarizing related facts about the user-supplied entity.\nHowever, the knowledge card is limited to certain entities that have a Wiki\npage or an entry in encyclopedias such as Freebase. The current encyclopedias\nare limited to highly popular entities, which are far fewer compared with the\nemerging entities. Despite the availability of knowledge about the emerging\nentities on the search results, yet there are no approaches to capture,\nabstract, summerize, fuse, and validate fragmented pieces of knowledge about\nthem. Thus, in this paper, we develop approaches to capture two types of\nknowledge about the emerging entities from a corpus extended from top-n search\nsnippets of a given emerging entity. The first kind of knowledge identifies the\nrole(s) of the emerging entity as, e.g., who is s/he? The second kind captures\nthe entities closely associated with the emerging entity. As the testbed, we\nconsidered a collection of 20 emerging entities and 20 popular entities as the\nground truth. Our approach is an unsupervised approach based on text analysis\nand entity embeddings. Our experimental studies show promising results as the\naccuracy of more than $87\\%$ for recognizing entities and $75\\%$ for ranking\nthem. Besides $87\\%$ of the entailed types were recognizable. Our testbed and\nsource code is available on Github\nhttps://github.com/sunnyUD/research_source_code.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:34:54 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Ngwobia", "Sunday C.", ""], ["Shekarpour", "Saeedeh", ""], ["Alshargi", "Faisal", ""]]}, {"id": "2104.01112", "submitter": "Sean Welleck", "authors": "Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin\n  Choi, Kyunghyun Cho", "title": "NaturalProofs: Mathematical Theorem Proving in Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and creating mathematics using natural mathematical language -\nthe mixture of symbolic and natural language used by humans - is a challenging\nand important problem for driving progress in machine learning. As a step in\nthis direction, we develop NaturalProofs, a multi-domain corpus of mathematical\nstatements and their proofs, written in natural mathematical language.\nNaturalProofs unifies broad coverage, deep coverage, and low-resource\nmathematical sources, allowing for evaluating both in-distribution and\nzero-shot generalization. Using NaturalProofs, we benchmark strong neural\nmethods on mathematical reference retrieval and generation tasks which test a\nsystem's ability to determine key results that appear in a proof. Large-scale\nsequence models show promise compared to classical information retrieval\nmethods, yet their performance and out-of-domain generalization leave\nsubstantial room for improvement. NaturalProofs opens many avenues for research\non challenging mathematical tasks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 03:14:48 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 21:58:06 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Welleck", "Sean", ""], ["Liu", "Jiacheng", ""], ["Bras", "Ronan Le", ""], ["Hajishirzi", "Hannaneh", ""], ["Choi", "Yejin", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "2104.01113", "submitter": "Satvik Garg", "authors": "Satvik Garg", "title": "Drug Recommendation System based on Sentiment Analysis of Drug Reviews\n  using Machine Learning", "comments": "7 pages, 8 figures, 2021 11th International Conference on Cloud\n  Computing, Data Science & Engineering (Confluence)", "journal-ref": null, "doi": "10.1109/Confluence51648.2021.9377188", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since coronavirus has shown up, inaccessibility of legitimate clinical\nresources is at its peak, like the shortage of specialists, healthcare workers,\nlack of proper equipment and medicines. The entire medical fraternity is in\ndistress, which results in numerous individuals demise. Due to unavailability,\npeople started taking medication independently without appropriate\nconsultation, making the health condition worse than usual. As of late, machine\nlearning has been valuable in numerous applications, and there is an increase\nin innovative work for automation. This paper intends to present a drug\nrecommender system that can drastically reduce specialists heap. In this\nresearch, we build a medicine recommendation system that uses patient reviews\nto predict the sentiment using various vectorization processes like Bow, TFIDF,\nWord2Vec, and Manual Feature Analysis, which can help recommend the top drug\nfor a given disease by different classification algorithms. The predicted\nsentiments were evaluated by precision, recall, f1score, accuracy, and AUC\nscore. The results show that classifier LinearSVC using TFIDF vectorization\noutperforms all other models with 93% accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 10:11:18 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 03:19:32 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Garg", "Satvik", ""]]}, {"id": "2104.01115", "submitter": "Jason Wang", "authors": "Jason Wang and Robert E. Weiss", "title": "Local and Global Topics in Text Modeling of Web Pages Nested in Web\n  Sites", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Topic models are popular models for analyzing a collection of text documents.\nThe models assert that documents are distributions over latent topics and\nlatent topics are distributions over words. A nested document collection is\nwhere documents are nested inside a higher order structure such as stories in a\nbook, articles in a journal, or web pages in a web site. In a single collection\nof documents, topics are global, or shared across all documents. For web pages\nnested in web sites, topic frequencies likely vary between web sites. Within a\nweb site, topic frequencies almost certainly vary between web pages. A\nhierarchical prior for topic frequencies models this hierarchical structure and\nspecifies a global topic distribution. Web site topic distributions vary around\nthe global topic distribution and web page topic distributions vary around the\nweb site topic distribution. In a nested collection of web pages, some topics\nare likely unique to a single web site. Local topics in a nested collection of\nweb pages are topics unique to one web site. For US local health department web\nsites, brief inspection of the text shows local geographic and news topics\nspecific to each department that are not present in others. Topic models that\nignore the nesting may identify local topics, but do not label topics as local\nnor do they explicitly identify the web site owner of the local topic. For web\npages nested inside web sites, local topic models explicitly label local topics\nand identifies the owning web site. This identification can be used to adjust\ninferences about global topics. In the US public health web site data, topic\ncoverage is defined at the web site level after removing local topic words from\npages. Hierarchical local topic models can be used to identify local topics,\nadjust inferences about if web sites cover particular health topics, and study\nhow well health topics are covered.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 23:16:46 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Wang", "Jason", ""], ["Weiss", "Robert E.", ""]]}, {"id": "2104.01117", "submitter": "Sami Diaf", "authors": "Sami Diaf and Ulrich Fritsche", "title": "Topic Scaling: A Joint Document Scaling -- Topic Model Approach To Learn\n  Time-Specific Topics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new methodology to study sequential corpora by\nimplementing a two-stage algorithm that learns time-based topics with respect\nto a scale of document positions and introduces the concept of Topic Scaling\nwhich ranks learned topics within the same document scale. The first stage\nranks documents using Wordfish, a Poisson-based document scaling method, to\nestimate document positions that serve, in the second stage, as a dependent\nvariable to learn relevant topics via a supervised Latent Dirichlet Allocation.\nThis novelty brings two innovations in text mining as it explains document\npositions, whose scale is a latent variable, and ranks the inferred topics on\nthe document scale to match their occurrences within the corpus and track their\nevolution. Tested on the U.S. State Of The Union two-party addresses, this\ninductive approach reveals that each party dominates one end of the learned\nscale with interchangeable transitions that follow the parties' term of office.\nBesides a demonstrated high accuracy in predicting in-sample documents'\npositions from topic scores, this method reveals further hidden topics that\ndifferentiate similar documents by increasing the number of learned topics to\nunfold potential nested hierarchical topic structures. Compared to other\npopular topic models, Topic Scaling learns topics with respect to document\nsimilarities without specifying a time frequency to learn topic evolution, thus\ncapturing broader topic patterns than dynamic topic models and yielding more\ninterpretable outputs than a plain latent Dirichlet allocation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 12:35:36 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Diaf", "Sami", ""], ["Fritsche", "Ulrich", ""]]}, {"id": "2104.01207", "submitter": "Sarthak Dash", "authors": "Sarthak Dash, Nandana Mihindukulasooriya, Alfio Gliozzo, Mustafa Canim", "title": "Type Prediction Systems", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inferring semantic types for entity mentions within text documents is an\nimportant asset for many downstream NLP tasks, such as Semantic Role Labelling,\nEntity Disambiguation, Knowledge Base Question Answering, etc. Prior works have\nmostly focused on supervised solutions that generally operate on relatively\nsmall-to-medium-sized type systems. In this work, we describe two systems aimed\nat predicting type information for the following two tasks, namely, a\nTypeSuggest module, an unsupervised system designed to predict types for a set\nof user-entered query terms, and an Answer Type prediction module, that\nprovides a solution for the task of determining the correct type of the answer\nexpected to a given query. Our systems generalize to arbitrary type systems of\nany sizes, thereby making it a highly appealing solution to extract type\ninformation at any granularity.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 19:16:42 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dash", "Sarthak", ""], ["Mihindukulasooriya", "Nandana", ""], ["Gliozzo", "Alfio", ""], ["Canim", "Mustafa", ""]]}, {"id": "2104.01488", "submitter": "Chuan Lei", "authors": "Alina Vretinaris, Chuan Lei, Vasilis Efthymiou, Xiao Qin, Fatma\n  \\\"Ozcan", "title": "Medical Entity Disambiguation Using Graph Neural Networks", "comments": "To appear in SIGMOD 2021", "journal-ref": null, "doi": "10.1145/3448016.3457328", "report-no": null, "categories": "cs.IR cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical knowledge bases (KBs), distilled from biomedical literature and\nregulatory actions, are expected to provide high-quality information to\nfacilitate clinical decision making. Entity disambiguation (also referred to as\nentity linking) is considered as an essential task in unlocking the wealth of\nsuch medical KBs. However, existing medical entity disambiguation methods are\nnot adequate due to word discrepancies between the entities in the KB and the\ntext snippets in the source documents. Recently, graph neural networks (GNNs)\nhave proven to be very effective and provide state-of-the-art results for many\nreal-world applications with graph-structured data. In this paper, we introduce\nED-GNN based on three representative GNNs (GraphSAGE, R-GCN, and MAGNN) for\nmedical entity disambiguation. We develop two optimization techniques to\nfine-tune and improve ED-GNN. First, we introduce a novel strategy to represent\nentities that are mentioned in text snippets as a query graph. Second, we\ndesign an effective negative sampling strategy that identifies hard negative\nsamples to improve the model's disambiguation capability. Compared to the best\nperforming state-of-the-art solutions, our ED-GNN offers an average improvement\nof 7.3% in terms of F1 score on five real-world datasets.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 22:04:15 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Vretinaris", "Alina", ""], ["Lei", "Chuan", ""], ["Efthymiou", "Vasilis", ""], ["Qin", "Xiao", ""], ["\u00d6zcan", "Fatma", ""]]}, {"id": "2104.01495", "submitter": "Yang Gao", "authors": "Yang Gao, Yi-Fan Li, Swarup Chandra, Latifur Khan, Bhavani\n  Thuraisingham", "title": "Towards Self-Adaptive Metric Learning On the Fly", "comments": "Accepted by WWW 2019 (Long Paper, Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Good quality similarity metrics can significantly facilitate the performance\nof many large-scale, real-world applications. Existing studies have proposed\nvarious solutions to learn a Mahalanobis or bilinear metric in an online\nfashion by either restricting distances between similar (dissimilar) pairs to\nbe smaller (larger) than a given lower (upper) bound or requiring similar\ninstances to be separated from dissimilar instances with a given margin.\nHowever, these linear metrics learned by leveraging fixed bounds or margins may\nnot perform well in real-world applications, especially when data distributions\nare complex. We aim to address the open challenge of \"Online Adaptive Metric\nLearning\" (OAML) for learning adaptive metric functions on the fly. Unlike\ntraditional online metric learning methods, OAML is significantly more\nchallenging since the learned metric could be non-linear and the model has to\nbe self-adaptive as more instances are observed. In this paper, we present a\nnew online metric learning framework that attempts to tackle the challenge by\nlearning an ANN-based metric with adaptive model complexity from a stream of\nconstraints. In particular, we propose a novel Adaptive-Bound Triplet Loss\n(ABTL) to effectively utilize the input constraints and present a novel\nAdaptive Hedge Update (AHU) method for online updating the model parameters. We\nempirically validate the effectiveness and efficacy of our framework on various\napplications such as real-world image classification, facial verification, and\nimage retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 23:11:52 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Gao", "Yang", ""], ["Li", "Yi-Fan", ""], ["Chandra", "Swarup", ""], ["Khan", "Latifur", ""], ["Thuraisingham", "Bhavani", ""]]}, {"id": "2104.01628", "submitter": "Marouen Kachroudi", "authors": "Marouen Kachroudi", "title": "Revisiting Indirect Ontology Alignment : New Challenging Issues in\n  Cross-Lingual Context", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ontology alignment process is overwhelmingly cited in Knowledge Engineering\nas a key mechanism aimed at bypassing heterogeneity and reconciling various\ndata sources, represented by ontologies, i.e., the the Semantic Web\ncornerstone. In such infrastructures and environments, it is inconceivable to\nassume that all ontologies covering a particular domain of knowledge are\naligned in pairs. Moreover, the high performance of alignment approaches is\nclosely related to two factors, i.e., time consumption and machine resource\nlimitations. Thus, good quality alignments are valuable and it would be\nappropriate to exploit them. Based on this observation, this article introduces\na new method of indirect alignment of ontologies in a cross-lingual context.\nIndeed, the proposed method deals with alignments of multilingual ontologies\nand implements an indirect ontology alignment strategy based on a composition\nand reuse of effective direct alignments. The trigger of the proposed method\nprocess is based on alignment algebra which governs the semantics composition\nof relationships and confidence values. The obtained results, after a thorough\nand detailed experiment are very encouraging and highlight many positive\naspects about the new proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 15:21:09 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kachroudi", "Marouen", ""]]}, {"id": "2104.01716", "submitter": "Rocky Chen", "authors": "Tong Chen, Hongzhi Yin, Xiangliang Zhang, Zi Huang, Yang Wang, Meng\n  Wang", "title": "Quaternion Factorization Machines: A Lightweight Solution to Intricate\n  Feature Interaction Modelling", "comments": "Manuscript is under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a well-established approach, factorization machine (FM) is capable of\nautomatically learning high-order interactions among features to make\npredictions without the need for manual feature engineering. With the prominent\ndevelopment of deep neural networks (DNNs), there is a recent and ongoing trend\nof enhancing the expressiveness of FM-based models with DNNs. However, though\nbetter results are obtained with DNN-based FM variants, such performance gain\nis paid off by an enormous amount (usually millions) of excessive model\nparameters on top of the plain FM. Consequently, the heavy parameterization\nimpedes the real-life practicality of those deep models, especially efficient\ndeployment on resource-constrained IoT and edge devices. In this paper, we move\nbeyond the traditional real space where most deep FM-based models are defined,\nand seek solutions from quaternion representations within the hypercomplex\nspace. Specifically, we propose the quaternion factorization machine (QFM) and\nquaternion neural factorization machine (QNFM), which are two novel lightweight\nand memory-efficient quaternion-valued models for sparse predictive analytics.\nBy introducing a brand new take on FM-based models with the notion of\nquaternion algebra, our models not only enable expressive inter-component\nfeature interactions, but also significantly reduce the parameter size due to\nlower degrees of freedom in the hypercomplex Hamilton product compared with\nreal-valued matrix multiplication. Extensive experimental results on three\nlarge-scale datasets demonstrate that QFM achieves 4.36% performance\nimprovement over the plain FM without introducing any extra parameters, while\nQNFM outperforms all baselines with up to two magnitudes' parameter size\nreduction in comparison to state-of-the-art peer methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 00:02:36 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 07:30:29 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chen", "Tong", ""], ["Yin", "Hongzhi", ""], ["Zhang", "Xiangliang", ""], ["Huang", "Zi", ""], ["Wang", "Yang", ""], ["Wang", "Meng", ""]]}, {"id": "2104.01800", "submitter": "Abdul Syahid", "authors": "Abdul Syahid", "title": "Indonesian Journal of Applied Linguistics: A Bibliometric Portrait of\n  Ten Publication Years", "comments": "23 pages, 4 tables, 9 figures", "journal-ref": "Library Philosophy and Practice (e-journal). 2021. 5178.\n  https://digitalcommons.unl.edu/libphilprac/5178", "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bibliometric portraits of a single journal appear to be rarely taken in the\nfield of applied linguistics. Viewed from the angles of publication, citation,\nand indexation, one of the journals worth a bibliometric portrait is the\nIndonesian Journal of Applied Linguistics. Casting local and regional concerns\non the global applied linguistics, the journal has ranked among the big five\nOpen Access Journals in the Asiatic region since its foundation in 2011.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 07:22:41 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Syahid", "Abdul", ""]]}, {"id": "2104.01894", "submitter": "Ramon Sanabria", "authors": "Ramon Sanabria, Austin Waters, Jason Baldridge", "title": "Talk, Don't Write: A Study of Direct Speech-Based Image Retrieval", "comments": "Accepted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech-based image retrieval has been studied as a proxy for joint\nrepresentation learning, usually without emphasis on retrieval itself. As such,\nit is unclear how well speech-based retrieval can work in practice -- both in\nan absolute sense and versus alternative strategies that combine automatic\nspeech recognition (ASR) with strong text encoders. In this work, we\nextensively study and expand choices of encoder architectures, training\nmethodology (including unimodal and multimodal pretraining), and other factors.\nOur experiments cover different types of speech in three datasets: Flickr\nAudio, Places Audio, and Localized Narratives. Our best model configuration\nachieves large gains over state of the art, e.g., pushing recall-at-one from\n21.8% to 33.2% for Flickr Audio and 27.6% to 53.4% for Places Audio. We also\nshow our best speech-based models can match or exceed cascaded ASR-to-text\nencoding when speech is spontaneous, accented, or otherwise hard to\nautomatically transcribe.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 13:11:40 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 10:16:17 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 17:03:38 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Sanabria", "Ramon", ""], ["Waters", "Austin", ""], ["Baldridge", "Jason", ""]]}, {"id": "2104.01946", "submitter": "Ke Liang", "authors": "Ke Liang and Mitchel Myers", "title": "Machine Learning Applications in the Routing in Computer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development of routing algorithms is of clear importance as the volume of\nInternet traffic continues to increase. In this survey, there is much research\ninto how Machine Learning techniques can be employed to improve the performance\nand scalability of routing algorithms. We surveyed both centralized and\ndecentralized ML routing architectures and using a variety of ML techniques\nbroadly divided into supervised learning and reinforcement learning. Many of\nthe papers showed promise in their ability to optimize some aspect of network\nrouting. We also implemented two routing protocols within 14 surveyed routing\nalgorithms and verified the efficacy of their results. While the results of\nmost of the papers showed promise, many of them are based on simulations of\npotentially unrealistic network configurations. To provide further efficacy to\nthe results, more real-world results are necessary.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 15:08:35 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Liang", "Ke", ""], ["Myers", "Mitchel", ""]]}, {"id": "2104.02061", "submitter": "Bingqing Yu", "authors": "Federico Bianchi, Jacopo Tagliabue and Bingqing Yu", "title": "Query2Prod2Vec Grounded Word Embeddings for eCommerce", "comments": "Published as a conference paper at NAACL2021 - Industry Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Query2Prod2Vec, a model that grounds lexical representations for\nproduct search in product embeddings: in our model, meaning is a mapping\nbetween words and a latent space of products in a digital shop. We leverage\nshopping sessions to learn the underlying space and use merchandising\nannotations to build lexical analogies for evaluation: our experiments show\nthat our model is more accurate than known techniques from the NLP and IR\nliterature. Finally, we stress the importance of data efficiency for product\nsearch outside of retail giants, and highlight how Query2Prod2Vec fits with\npractical constraints faced by most practitioners.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 21:32:43 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Bianchi", "Federico", ""], ["Tagliabue", "Jacopo", ""], ["Yu", "Bingqing", ""]]}, {"id": "2104.02262", "submitter": "Qiang Cui", "authors": "Qiang Cui, Yafeng Zhang, Jinpeng Wang", "title": "CANS-Net: Context-Aware Non-Successive Modeling Network for Next\n  Point-of-Interest Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point-of-Interest (POI) recommendation is an important task in location-based\nsocial networks. It facilitates the sharing between users and locations.\nRecently, researchers tend to recommend POIs by long- and short-term interests.\nHowever, existing models are mostly based on sequential modeling of successive\nPOIs to capture transitional regularities. A few works try to acquire user's\nmobility periodicity or POI's geographical influence, but they omit some other\nspatial-temporal factors. To this end, we propose to jointly model various\nspatial-temporal factors by context-aware non-successive modeling. In the\nlong-term module, we split user's all historical check-ins into seven sequences\nby day of week to obtain daily interest, then we combine them by attention.\nThis will capture temporal effect. In the short-term module, we construct four\nshort-term sequences to acquire sequential, spatial, temporal, and\nspatial-temporal effects, respectively. Attention of interest-level is used to\ncombine all factors and interests. Experiments on two real-world datasets\ndemonstrate the state-of-the-art performance of our method.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 03:08:27 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Cui", "Qiang", ""], ["Zhang", "Yafeng", ""], ["Wang", "Jinpeng", ""]]}, {"id": "2104.02381", "submitter": "Paridhi Maheshwari", "authors": "Paridhi Maheshwari, Ritwick Chaudhry, Vishwa Vinay", "title": "Scene Graph Embeddings Using Relative Similarity Supervision", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene graphs are a powerful structured representation of the underlying\ncontent of images, and embeddings derived from them have been shown to be\nuseful in multiple downstream tasks. In this work, we employ a graph\nconvolutional network to exploit structure in scene graphs and produce image\nembeddings useful for semantic image retrieval. Different from\nclassification-centric supervision traditionally available for learning image\nrepresentations, we address the task of learning from relative similarity\nlabels in a ranking context. Rooted within the contrastive learning paradigm,\nwe propose a novel loss function that operates on pairs of similar and\ndissimilar images and imposes relative ordering between them in embedding\nspace. We demonstrate that this Ranking loss, coupled with an intuitive triple\nsampling strategy, leads to robust representations that outperform well-known\ncontrastive losses on the retrieval task. In addition, we provide qualitative\nevidence of how retrieved results that utilize structured scene information\ncapture the global context of the scene, different from visual similarity\nsearch.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 09:13:05 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Maheshwari", "Paridhi", ""], ["Chaudhry", "Ritwick", ""], ["Vinay", "Vishwa", ""]]}, {"id": "2104.02429", "submitter": "Zhe Ma", "authors": "Jianfeng Dong, Zhe Ma, Xiaofeng Mao, Xun Yang, Yuan He, Richang Hong,\n  Shouling Ji", "title": "Fine-Grained Fashion Similarity Prediction by Attribute-Specific\n  Embedding Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:2002.02814", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper strives to predict fine-grained fashion similarity. In this\nsimilarity paradigm, one should pay more attention to the similarity in terms\nof a specific design/attribute between fashion items. For example, whether the\ncollar designs of the two clothes are similar. It has potential value in many\nfashion related applications, such as fashion copyright protection. To this\nend, we propose an Attribute-Specific Embedding Network (ASEN) to jointly learn\nmultiple attribute-specific embeddings, thus measure the fine-grained\nsimilarity in the corresponding space. The proposed ASEN is comprised of a\nglobal branch and a local branch. The global branch takes the whole image as\ninput to extract features from a global perspective, while the local branch\ntakes as input the zoomed-in region-of-interest (RoI) w.r.t. the specified\nattribute thus able to extract more fine-grained features. As the global branch\nand the local branch extract the features from different perspectives, they are\ncomplementary to each other. Additionally, in each branch, two attention\nmodules, i.e., Attribute-aware Spatial Attention and Attribute-aware Channel\nAttention, are integrated to make ASEN be able to locate the related regions\nand capture the essential patterns under the guidance of the specified\nattribute, thus make the learned attribute-specific embeddings better reflect\nthe fine-grained similarity. Extensive experiments on three fashion-related\ndatasets, i.e., FashionAI, DARN, and DeepFashion, show the effectiveness of\nASEN for fine-grained fashion similarity prediction and its potential for\nfashion reranking. Code and data are available at\nhttps://github.com/maryeon/asenpp .\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 11:26:38 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Dong", "Jianfeng", ""], ["Ma", "Zhe", ""], ["Mao", "Xiaofeng", ""], ["Yang", "Xun", ""], ["He", "Yuan", ""], ["Hong", "Richang", ""], ["Ji", "Shouling", ""]]}, {"id": "2104.02553", "submitter": "Natalia Berloff", "authors": "Kirill P. Kalinin and Natalia G. Berloff", "title": "Large-scale Sustainable Search on Unconventional Computing Hardware", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.ET cs.IR physics.comp-ph physics.optics", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the advent of the Internet, quantifying the relative importance of web\npages is at the core of search engine methods. According to one algorithm,\nPageRank, the worldwide web structure is represented by the Google matrix,\nwhose principal eigenvector components assign a numerical value to web pages\nfor their ranking. Finding such a dominant eigenvector on an ever-growing\nnumber of web pages becomes a computationally intensive task incompatible with\nMoore's Law. We demonstrate that special-purpose optical machines such as\nnetworks of optical parametric oscillators, lasers, and gain-dissipative\ncondensates, may aid in accelerating the reliable reconstruction of principal\neigenvectors of real-life web graphs. We discuss the feasibility of simulating\nthe PageRank algorithm on large Google matrices using such unconventional\nhardware. We offer alternative rankings based on the minimisation of spin\nHamiltonians. Our estimates show that special-purpose optical machines may\nprovide dramatic improvements in power consumption over classical computing\narchitectures.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:48:01 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kalinin", "Kirill P.", ""], ["Berloff", "Natalia G.", ""]]}, {"id": "2104.02962", "submitter": "Zeyu Cui", "authors": "Zeyu Cui, Zekun Li, Shu Wu, Xiaoyu Zhang, Qiang Liu, Liang Wang,\n  Mengmeng Ai", "title": "DyGCN: Dynamic Graph Embedding with Graph Convolutional Network", "comments": "21 pages, 5 figures, submitted to TOIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embedding, aiming to learn low-dimensional representations (aka.\nembeddings) of nodes, has received significant attention recently. Recent years\nhave witnessed a surge of efforts made on static graphs, among which Graph\nConvolutional Network (GCN) has emerged as an effective class of models.\nHowever, these methods mainly focus on the static graph embedding. In this\nwork, we propose an efficient dynamic graph embedding approach, Dynamic Graph\nConvolutional Network (DyGCN), which is an extension of GCN-based methods. We\nnaturally generalizes the embedding propagation scheme of GCN to dynamic\nsetting in an efficient manner, which is to propagate the change along the\ngraph to update node embeddings. The most affected nodes are first updated, and\nthen their changes are propagated to the further nodes and leads to their\nupdate. Extensive experiments conducted on various dynamic graphs demonstrate\nthat our model can update the node embeddings in a time-saving and\nperformance-preserving way.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 07:28:44 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Cui", "Zeyu", ""], ["Li", "Zekun", ""], ["Wu", "Shu", ""], ["Zhang", "Xiaoyu", ""], ["Liu", "Qiang", ""], ["Wang", "Liang", ""], ["Ai", "Mengmeng", ""]]}, {"id": "2104.02981", "submitter": "Kai Wang", "authors": "Kai Wang, Zhene Zou, Qilin Deng, Runze Wu, Jianrong Tao, Changjie Fan,\n  Liang Chen, Peng Cui", "title": "Reinforcement Learning with a Disentangled Universal Value Function for\n  Item Recommendation", "comments": "9 pages, 4 figures, to be published in Proceedings of the AAAI\n  Conference on Artificial Intelligence 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there are great interests as well as challenges in applying\nreinforcement learning (RL) to recommendation systems (RS). In this paper, we\nsummarize three key practical challenges of large-scale RL-based recommender\nsystems: massive state and action spaces, high-variance environment, and the\nunspecific reward setting in recommendation. All these problems remain largely\nunexplored in the existing literature and make the application of RL\nchallenging. We develop a model-based reinforcement learning framework, called\nGoalRec. Inspired by the ideas of world model (model-based), value function\nestimation (model-free), and goal-based RL, a novel disentangled universal\nvalue function designed for item recommendation is proposed. It can generalize\nto various goals that the recommender may have, and disentangle the stochastic\nenvironmental dynamics and high-variance reward signals accordingly. As a part\nof the value function, free from the sparse and high-variance reward signals, a\nhigh-capacity reward-independent world model is trained to simulate complex\nenvironmental dynamics under a certain goal. Based on the predicted\nenvironmental dynamics, the disentangled universal value function is related to\nthe user's future trajectory instead of a monolithic state and a scalar reward.\nWe demonstrate the superiority of GoalRec over previous approaches in terms of\nthe above three practical challenges in a series of simulations and a real\napplication.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 08:13:32 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 13:32:20 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wang", "Kai", ""], ["Zou", "Zhene", ""], ["Deng", "Qilin", ""], ["Wu", "Runze", ""], ["Tao", "Jianrong", ""], ["Fan", "Changjie", ""], ["Chen", "Liang", ""], ["Cui", "Peng", ""]]}, {"id": "2104.03236", "submitter": "Herv\\'e Le Borgne", "authors": "Omar Adjali and Romaric Besan\\c{c}on and Olivier Ferret and Herve Le\n  Borgne and Brigitte Grau", "title": "Multimodal Entity Linking for Tweets", "comments": null, "journal-ref": "In: Jose J. et al. (eds) Advances in Information Retrieval. ECIR\n  2020. Lecture Notes in Computer Science, vol 12035. Springer, Cham", "doi": "10.1007/978-3-030-45439-5_31", "report-no": null, "categories": "cs.IR cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In many information extraction applications, entity linking (EL) has emerged\nas a crucial task that allows leveraging information about named entities from\na knowledge base. In this paper, we address the task of multimodal entity\nlinking (MEL), an emerging research field in which textual and visual\ninformation is used to map an ambiguous mention to an entity in a knowledge\nbase (KB). First, we propose a method for building a fully annotated Twitter\ndataset for MEL, where entities are defined in a Twitter KB. Then, we propose a\nmodel for jointly learning a representation of both mentions and entities from\ntheir textual and visual contexts. We demonstrate the effectiveness of the\nproposed model by evaluating it on the proposed dataset and highlight the\nimportance of leveraging visual information when it is available.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 16:40:23 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Adjali", "Omar", ""], ["Besan\u00e7on", "Romaric", ""], ["Ferret", "Olivier", ""], ["Borgne", "Herve Le", ""], ["Grau", "Brigitte", ""]]}, {"id": "2104.03353", "submitter": "A\\'ecio Solano Rodrigues Santos", "authors": "A\\'ecio Santos, Aline Bessa, Fernando Chirigati, Christopher Musco,\n  Juliana Freire", "title": "Correlation Sketches for Approximate Join-Correlation Queries", "comments": "Proceedings of the 2021 International Conference on Management of\n  Data (SIGMOD '21)", "journal-ref": null, "doi": "10.1145/3448016.3458456", "report-no": null, "categories": "cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of structured datasets, from Web tables and\nopen-data portals to enterprise data, opens up opportunities~to enrich\nanalytics and improve machine learning models through relational data\naugmentation. In this paper, we introduce a new class of data augmentation\nqueries: join-correlation queries. Given a column $Q$ and a join column $K_Q$\nfrom a query table $\\mathcal{T}_Q$, retrieve tables $\\mathcal{T}_X$ in a\ndataset collection such that $\\mathcal{T}_X$ is joinable with $\\mathcal{T}_Q$\non $K_Q$ and there is a column $C \\in \\mathcal{T}_X$ such that $Q$ is\ncorrelated with $C$. A na\\\"ive approach to evaluate these queries, which first\nfinds joinable tables and then explicitly joins and computes correlations\nbetween $Q$ and all columns of the discovered tables, is prohibitively\nexpensive. To efficiently support correlated column discovery, we 1) propose a\nsketching method that enables the construction of an index for a large number\nof tables and that provides accurate estimates for join-correlation queries,\nand 2) explore different scoring strategies that effectively rank the query\nresults based on how well the columns are correlated with the query. We carry\nout a detailed experimental evaluation, using both synthetic and real data,\nwhich shows that our sketches attain high accuracy and the scoring strategies\nlead to high-quality rankings.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 19:08:14 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Santos", "A\u00e9cio", ""], ["Bessa", "Aline", ""], ["Chirigati", "Fernando", ""], ["Musco", "Christopher", ""], ["Freire", "Juliana", ""]]}, {"id": "2104.03354", "submitter": "Shantanu Sharma", "authors": "Yin Li, Dhrubajyoti Ghosh, Peeyush Gupta, Sharad Mehrotra, Nisha\n  Panwar, Shantanu Sharma", "title": "Prism: Private Verifiable Set Computation over Multi-Owner Outsourced\n  Databases", "comments": "This paper has been accepted in ACM SIGMOD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Prism, a secret sharing based approach to compute private\nset operations (i.e., intersection and union), as well as aggregates over\noutsourced databases belonging to multiple owners. Prism enables data owners to\npre-load the data onto non-colluding servers and exploits the additive and\nmultiplicative properties of secret-shares to compute the above-listed\noperations in (at most) two rounds of communication between the servers\n(storing the secret-shares) and the querier, resulting in a very efficient\nimplementation. Also, Prism does not require communication among the servers\nand supports result verification techniques for each operation to detect\nmalicious adversaries. Experimental results show that Prism scales both in\nterms of the number of data owners and database sizes, to which prior\napproaches do not scale.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 19:08:15 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Li", "Yin", ""], ["Ghosh", "Dhrubajyoti", ""], ["Gupta", "Peeyush", ""], ["Mehrotra", "Sharad", ""], ["Panwar", "Nisha", ""], ["Sharma", "Shantanu", ""]]}, {"id": "2104.03940", "submitter": "Abhishek Kaushik Mr.", "authors": "Abhishek Kaushik and Gareth J. F. Jones", "title": "A Conceptual Framework for Implicit Evaluation of Conversational Search\n  Interfaces", "comments": "Accepted in MICROS (Mixed-Initiative ConveRsatiOnal Systems) Workshop\n  at 43rd European Conference on Information Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search (CS) has recently become a significant focus of the\ninformation retrieval (IR) research community. Multiple studies have been\nconducted which explore the concept of conversational search. Understanding and\nadvancing research in CS requires careful and detailed evaluation. Existing CS\nstudies have been limited to evaluation based on simple user feedback on task\ncompletion. We propose a CS evaluation framework which includes multiple\ndimensions: search experience, knowledge gain, software usability, cognitive\nload and user experience, based on studies of conversational systems and IR. We\nintroduce these evaluation criteria and propose their use in a framework for\nthe evaluation of CS systems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:33:18 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Kaushik", "Abhishek", ""], ["Jones", "Gareth J. F.", ""]]}, {"id": "2104.04243", "submitter": "Vivek Gupta", "authors": "J. Neeraja, Vivek Gupta, Vivek Srikumar", "title": "Incorporating External Knowledge to Enhance Tabular Reasoning", "comments": "11 pages, 1 Figure, 14 tables, To appear in NAACL 2021 (Short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reasoning about tabular information presents unique challenges to modern NLP\napproaches which largely rely on pre-trained contextualized embeddings of text.\nIn this paper, we study these challenges through the problem of tabular natural\nlanguage inference. We propose easy and effective modifications to how\ninformation is presented to a model for this task. We show via systematic\nexperiments that these strategies substantially improve tabular inference\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 08:25:01 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Neeraja", "J.", ""], ["Gupta", "Vivek", ""], ["Srikumar", "Vivek", ""]]}, {"id": "2104.04415", "submitter": "Steve Schmidt", "authors": "Steve Schmidt, Denley Lam, Patrick Hayden", "title": "Automatic Knowledge Extraction with Human Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  OrbWeaver, an automatic knowledge extraction system paired with a human\ninterface, streamlines the use of unintuitive natural language processing\nsoftware for modeling systems from their documentation. OrbWeaver enables the\nindirect transfer of knowledge about legacy systems by leveraging open source\ntools in document understanding and processing as well as using web based user\ninterface constructs. By design, OrbWeaver is scalable, extensible, and usable;\nwe demonstrate its utility by evaluating its performance in processing a corpus\nof documents related to advanced persistent threats in the cyber domain. The\nresults indicate better knowledge extraction by revealing hidden relationships,\nlinking co-related entities, and gathering evidence.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:07:11 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Schmidt", "Steve", ""], ["Lam", "Denley", ""], ["Hayden", "Patrick", ""]]}, {"id": "2104.04501", "submitter": "Abhishek Kaushik Mr.", "authors": "Abhishek Kaushik and Gareth J. F. Jones", "title": "Exploring Current User Web Search Behaviours in Analysis Tasks to be\n  Supported in Conversational Search", "comments": "Accepted in SIGIR 2018 Second International Workshop on\n  Conversational Approaches to Information Retrieval (CAIR 18), July 12, 2018,\n  Ann Arbor Michigan, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search presents opportunities to support users in their search\nactivities to improve the effectiveness and efficiency of search while reducing\ntheir cognitive load. Limitations of the potential competency of conversational\nagents restrict the situations for which conversational search agents can\nreplace human intermediaries. It is thus more interesting, initially at least,\nto investigate opportunities for conversational interaction to support less\ncomplex information retrieval tasks, such as typical web search, which do not\nrequire human-level intelligence in the conversational agent. In order to move\ntowards the development of a system to enable conversational search of this\ntype, we need to understand their required capabilities. To progress our\nunderstanding of these, we report a study examining the behaviour of users when\nusing a standard web search engine, designed to enable us to identify\nopportunities to support their search activities using a conversational agent.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 17:38:03 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Kaushik", "Abhishek", ""], ["Jones", "Gareth J. F.", ""]]}, {"id": "2104.04556", "submitter": "Alejandro H\\'ector Toselli", "authors": "E. Vidal, A.H. Toselli, J. Puigcerver", "title": "A Probabilistic Framework for Lexicon-based Keyword Spotting in\n  Handwritten Text Images", "comments": "42 pages, 35 headers, 16 figures/tables", "journal-ref": null, "doi": null, "report-no": "Tech. rep., UPV (2017)", "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Query by String Keyword Spotting (KWS) is here considered as a key technology\nfor indexing large collections of handwritten text images to allow fast textual\naccess to the contents of these collections. Under this perspective, a\nprobabilistic framework for lexicon-based KWS in text images is presented. The\npresentation aims at providing a tutorial view that helps to understand the\nrelations between classical statements of KWS and the relative challenges\nentailed by these statements. More specifically, the development of the\nproposed framework makes it self-evident that word recognition or\nclassification implicitly or explicitly underlies any formulation of KWS.\nMoreover, it clearly suggests that the same statistical models and training\nmethods successfully used for handwriting text recognition can advantageously\nbe used also for KWS, even though KWS does not generally require or rely on any\nkind of previously produced image transcripts. These ideas are developed into a\nspecific, probabilistically sound approach for segmentation-free,\nlexicon-based, query-by-string KWS. Experiments carried out using this approach\nare presented, which support the consistency and general interest of the\nproposed framework. Several datasets, traditionally used for KWS benchmarking\nare considered, with results significantly better than those previously\npublished for these datasets. In addition, results on two new, larger\nhandwritten text image datasets are reported, showing the great potential of\nthe methods proposed in this paper for indexing and textual search in large\ncollections of handwritten documents.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 18:33:32 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Vidal", "E.", ""], ["Toselli", "A. H.", ""], ["Puigcerver", "J.", ""]]}, {"id": "2104.04584", "submitter": "Swakkhar Shatabda", "authors": "Md. Mahinur Rashid, Hasin Kawsar Jahan, Annysha Huzzat, Riyasaat Ahmed\n  Rahul, Tamim Bin Zakir, Farhana Meem, Md. Saddam Hossain Mukta and Swakkhar\n  Shatabda", "title": "Text2Chart: A Multi-Staged Chart Generator from Natural Language Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generation of scientific visualization from analytical natural language text\nis a challenging task. In this paper, we propose Text2Chart, a multi-staged\nchart generator method. Text2Chart takes natural language text as input and\nproduce visualization as two-dimensional charts. Text2Chart approaches the\nproblem in three stages. Firstly, it identifies the axis elements of a chart\nfrom the given text known as x and y entities. Then it finds a mapping of\nx-entities with its corresponding y-entities. Next, it generates a chart type\nsuitable for the given text: bar, line or pie. Combination of these three\nstages is capable of generating visualization from the given analytical text.\nWe have also constructed a dataset for this problem. Experiments show that\nText2Chart achieves best performances with BERT based encodings with LSTM\nmodels in the first stage to label x and y entities, Random Forest classifier\nfor the mapping stage and fastText embedding with LSTM for the chart type\nprediction. In our experiments, all the stages show satisfactory results and\neffectiveness considering formation of charts from analytical text, achieving a\ncommendable overall performance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 19:42:24 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Rashid", "Md. Mahinur", ""], ["Jahan", "Hasin Kawsar", ""], ["Huzzat", "Annysha", ""], ["Rahul", "Riyasaat Ahmed", ""], ["Zakir", "Tamim Bin", ""], ["Meem", "Farhana", ""], ["Mukta", "Md. Saddam Hossain", ""], ["Shatabda", "Swakkhar", ""]]}, {"id": "2104.04697", "submitter": "Cheng-Te Li", "authors": "Chih-Yao Chen, Cheng-Te Li", "title": "ZS-BERT: Towards Zero-Shot Relation Extraction with Attribute\n  Representation Learning", "comments": "Accepted to NAACL 2021. Code is available at\n  https://github.com/dinobby/ZS-BERT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While relation extraction is an essential task in knowledge acquisition and\nrepresentation, and new-generated relations are common in the real world, less\neffort is made to predict unseen relations that cannot be observed at the\ntraining stage. In this paper, we formulate the zero-shot relation extraction\nproblem by incorporating the text description of seen and unseen relations. We\npropose a novel multi-task learning model, zero-shot BERT (ZS-BERT), to\ndirectly predict unseen relations without hand-crafted attribute labeling and\nmultiple pairwise classifications. Given training instances consisting of input\nsentences and the descriptions of their relations, ZS-BERT learns two functions\nthat project sentences and relation descriptions into an embedding space by\njointly minimizing the distances between them and classifying seen relations.\nBy generating the embeddings of unseen relations and new-coming sentences based\non such two functions, we use nearest neighbor search to obtain the prediction\nof unseen relations. Experiments conducted on two well-known datasets exhibit\nthat ZS-BERT can outperform existing methods by at least 13.54\\% improvement on\nF1 score.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 06:53:41 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chen", "Chih-Yao", ""], ["Li", "Cheng-Te", ""]]}, {"id": "2104.04739", "submitter": "Anna Glazkova", "authors": "Mikhail Kotyushev, Anna Glazkova, Dmitry Morozov", "title": "MIPT-NSU-UTMN at SemEval-2021 Task 5: Ensembling Learning with\n  Pre-trained Language Models for Toxic Spans Detection", "comments": "Accepted at SemEval-2021 Workshop, ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes our system for SemEval-2021 Task 5 on Toxic Spans\nDetection. We developed ensemble models using BERT-based neural architectures\nand post-processing to combine tokens into spans. We evaluated several\npre-trained language models using various ensemble techniques for toxic span\nidentification and achieved sizable improvements over our baseline fine-tuned\nBERT models. Finally, our system obtained a F1-score of 67.55% on test data.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 11:27:32 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Kotyushev", "Mikhail", ""], ["Glazkova", "Anna", ""], ["Morozov", "Dmitry", ""]]}, {"id": "2104.05047", "submitter": "Evgeny Frolov", "authors": "Oluwafemi Olaleke, Ivan Oseledets, Evgeny Frolov", "title": "Dynamic Modeling of User Preferences for Stable Recommendations", "comments": "8 pages, 1 figure, accepted at UMAP'21 conference", "journal-ref": null, "doi": "10.1145/3450613.3456830", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In domains where users tend to develop long-term preferences that do not\nchange too frequently, the stability of recommendations is an important factor\nof the perceived quality of a recommender system. In such cases, unstable\nrecommendations may lead to poor personalization experience and distrust,\ndriving users away from a recommendation service. We propose an incremental\nlearning scheme that mitigates such problems through the dynamic modeling\napproach. It incorporates a generalized matrix form of a partial differential\nequation integrator that yields a dynamic low-rank approximation of\ntime-dependent matrices representing user preferences. The scheme allows\nextending the famous PureSVD approach to time-aware settings and significantly\nimproves its stability without sacrificing the accuracy in standard top-$n$\nrecommendations tasks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 16:32:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Olaleke", "Oluwafemi", ""], ["Oseledets", "Ivan", ""], ["Frolov", "Evgeny", ""]]}, {"id": "2104.05216", "submitter": "Yang Deng", "authors": "Yang Deng, Yuexiang Xie, Yaliang Li, Min Yang, Wai Lam, Ying Shen", "title": "Contextualized Knowledge-aware Attentive Neural Network: Enhancing\n  Answer Selection with Knowledge", "comments": "Accepted by TOIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Answer selection, which is involved in many natural language processing\napplications such as dialog systems and question answering (QA), is an\nimportant yet challenging task in practice, since conventional methods\ntypically suffer from the issues of ignoring diverse real-world background\nknowledge. In this paper, we extensively investigate approaches to enhancing\nthe answer selection model with external knowledge from knowledge graph (KG).\nFirst, we present a context-knowledge interaction learning framework,\nKnowledge-aware Neural Network (KNN), which learns the QA sentence\nrepresentations by considering a tight interaction with the external knowledge\nfrom KG and the textual information. Then, we develop two kinds of\nknowledge-aware attention mechanism to summarize both the context-based and\nknowledge-based interactions between questions and answers. To handle the\ndiversity and complexity of KG information, we further propose a Contextualized\nKnowledge-aware Attentive Neural Network (CKANN), which improves the knowledge\nrepresentation learning with structure information via a customized Graph\nConvolutional Network (GCN) and comprehensively learns context-based and\nknowledge-based sentence representation via the multi-view knowledge-aware\nattention mechanism. We evaluate our method on four widely-used benchmark QA\ndatasets, including WikiQA, TREC QA, InsuranceQA and Yahoo QA. Results verify\nthe benefits of incorporating external knowledge from KG, and show the robust\nsuperiority and extensive applicability of our method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 05:52:20 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Deng", "Yang", ""], ["Xie", "Yuexiang", ""], ["Li", "Yaliang", ""], ["Yang", "Min", ""], ["Lam", "Wai", ""], ["Shen", "Ying", ""]]}, {"id": "2104.05307", "submitter": "Qilin Deng", "authors": "Qilin Deng, Kai Wang, Minghao Zhao, Zhene Zou, Runze Wu, Jianrong Tao,\n  Changjie Fan, Liang Chen", "title": "Personalized Bundle Recommendation in Online Games", "comments": "8 pages, 10 figures, accepted paper on CIKM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In business domains, \\textit{bundling} is one of the most important marketing\nstrategies to conduct product promotions, which is commonly used in online\ne-commerce and offline retailers. Existing recommender systems mostly focus on\nrecommending individual items that users may be interested in. In this paper,\nwe target at a practical but less explored recommendation problem named bundle\nrecommendation, which aims to offer a combination of items to users. To tackle\nthis specific recommendation problem in the context of the \\emph{virtual mall}\nin online games, we formalize it as a link prediction problem on a\nuser-item-bundle tripartite graph constructed from the historical interactions,\nand solve it with a neural network model that can learn directly on the\ngraph-structure data. Extensive experiments on three public datasets and one\nindustrial game dataset demonstrate the effectiveness of the proposed method.\nFurther, the bundle recommendation model has been deployed in production for\nmore than one year in a popular online game developed by Netease Games, and the\nlaunch of the model yields more than 60\\% improvement on conversion rate of\nbundles, and a relative improvement of more than 15\\% on gross merchandise\nvolume (GMV).\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 09:28:16 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Deng", "Qilin", ""], ["Wang", "Kai", ""], ["Zhao", "Minghao", ""], ["Zou", "Zhene", ""], ["Wu", "Runze", ""], ["Tao", "Jianrong", ""], ["Fan", "Changjie", ""], ["Chen", "Liang", ""]]}, {"id": "2104.05310", "submitter": "Chen Wu", "authors": "Dawn Drain, Changran Hu, Chen Wu, Mikhail Breslav, Neel Sundaresan", "title": "Generating Code with the Help of Retrieved Template Functions and Stack\n  Overflow Answers", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We approach the important challenge of code autocompletion as an open-domain\ntask, in which a sequence-to-sequence code generator model is enhanced with the\nability to attend to reference code snippets supplied by a semantic code search\nengine. In this work, we present a novel framework to precisely retrieve\ntemplate functions as well as intent-snippet pairs and effectively train such a\nretrieval-guided code generator. To demonstrate the effectiveness of our model\ndesigns, we perform extensive experiments with CodeSearchNet which contains\ntemplate functions and CoNaLa which contains Stack Overflow intent-snippet\npairs. We also investigate different retrieval models, including Elasticsearch,\nDPR, and our fusion representation search model, which currently holds the\nnumber one spot on the CodeSearchNet leaderboard. We observe improvements by\nleveraging multiple database elements and further gain from retrieving diverse\ndata points by using Maximal Marginal Relevance. Overall, we see a 4%\nimprovement to cross-entropy loss, a 15% improvement to edit distance, and a\n44% improvement to BLEU score when retrieving template functions. We see\nsubtler improvements of 2%, 11%, and 6% respectively when retrieving Stack\nOverflow intent-snippet pairs. We also create a novel Stack Overflow-Function\nAlignment dataset, which consists of 150K tuples of functions and Stack\nOverflow intent-snippet pairs that are of help in writing the associated\nfunction, of which 1.7K are manually curated.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 09:37:32 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 02:46:54 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Drain", "Dawn", ""], ["Hu", "Changran", ""], ["Wu", "Chen", ""], ["Breslav", "Mikhail", ""], ["Sundaresan", "Neel", ""]]}, {"id": "2104.05364", "submitter": "Jos\\'e Devezas PhD", "authors": "Jos\\'e Devezas and S\\'ergio Nunes", "title": "Fatigued Random Walks in Hypergraphs: A Neuronal Analogy to Improve\n  Retrieval Performance", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergraphs are data structures capable of capturing supra-dyadic relations.\nWe can use them to model binary relations, but also to model groups of\nentities, as well as the intersections between these groups or the contained\nsubgroups. In previous work, we explored the usage of hypergraphs as an\nindexing data structure, in particular one that was capable of seamlessly\nintegrating text, entities and their relations to support entity-oriented\nsearch tasks. As more information is added to the hypergraph, however, it not\nonly increases in size, but it also becomes denser, making the task of\nefficiently ranking nodes or hyperedges more complex. Random walks can\neffectively capture network structure, without compromising performance, or at\nleast providing a tunable balance between efficiency and effectiveness, within\na nondeterministic universe. For a higher effectiveness, a higher number of\nrandom walks is usually required, which often results in lower efficiency.\nInspired by von Neumann and the neuron in the brain, we propose and study the\nusage of node and hyperedge fatigue as a way to temporarily constrict random\nwalks during keyword-based ad hoc retrieval. We found that we were able to\nimprove search time by a factor of 32, but also worsen MAP by a factor of 8.\nMoreover, by distinguishing between fatigue in nodes and hyperedges, we are\nable to find that, for hyperedge ranking tasks, we consistently obtained lower\nMAP scores when increasing fatigue for nodes. On the other hand, the overall\nimpact of hyperedge fatigue was slightly positive, although it also slightly\nworsened efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 11:32:06 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Devezas", "Jos\u00e9", ""], ["Nunes", "S\u00e9rgio", ""]]}, {"id": "2104.05369", "submitter": "Jos\\'e Devezas PhD", "authors": "Jos\\'e Devezas and S\\'ergio Nunes", "title": "Fatigued PageRank", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connections among entities are everywhere. From social media interactions to\nweb page hyperlinks, networks are frequently used to represent such complex\nsystems. Node ranking is a fundamental task that provides the strategy to\nidentify central entities according to multiple criteria. Popular node ranking\nmetrics include degree, closeness or betweenness centralities, as well as HITS\nauthority or PageRank. In this work, we propose a novel node ranking metric,\nwhere we combine PageRank and the idea of node fatigue, in order to model a\nrandom explorer who wants to optimize coverage - it gets fatigued and avoids\npreviously visited nodes. We formalize and exemplify the computation of\nFatigued PageRank, evaluating it as a node ranking metric, as well as\nquery-independent evidence in ad hoc document retrieval. Based on the Simple\nEnglish Wikipedia link graph with clickstream transitions from the English\nWikipedia, we find that Fatigued PageRank is able to surpass both indegree and\nHITS authority, but only for the top ranking nodes. On the other hand, based on\nthe TREC Washington Post Corpus, we were unable to outperform the BM25\nbaseline, obtaining similar performance for all graph-based metrics, except for\nindegree, which lowered GMAP and MAP, but increased NDCG@10 and P@10.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 11:44:04 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 13:39:55 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Devezas", "Jos\u00e9", ""], ["Nunes", "S\u00e9rgio", ""]]}, {"id": "2104.05740", "submitter": "Xueguang Ma", "authors": "Xueguang Ma, Kai Sun, Ronak Pradeep, and Jimmy Lin", "title": "A Replication Study of Dense Passage Retriever", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text retrieval using learned dense representations has recently emerged as a\npromising alternative to \"traditional\" text retrieval using sparse bag-of-words\nrepresentations. One recent work that has garnered much attention is the dense\npassage retriever (DPR) technique proposed by Karpukhin et al. (2020) for\nend-to-end open-domain question answering. We present a replication study of\nthis work, starting with model checkpoints provided by the authors, but\notherwise from an independent implementation in our group's Pyserini IR toolkit\nand PyGaggle neural text ranking library. Although our experimental results\nlargely verify the claims of the original paper, we arrived at two important\nadditional findings that contribute to a better understanding of DPR: First, it\nappears that the original authors under-report the effectiveness of the BM25\nbaseline and hence also dense--sparse hybrid retrieval results. Second, by\nincorporating evidence from the retriever and an improved answer span scoring\ntechnique, we are able to improve end-to-end question answering effectiveness\nusing exactly the same models as in the original work.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:10:39 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Ma", "Xueguang", ""], ["Sun", "Kai", ""], ["Pradeep", "Ronak", ""], ["Lin", "Jimmy", ""]]}, {"id": "2104.05796", "submitter": "Edoardo D'Amico", "authors": "Giovanni Gabbolini, Edoardo D'Amico, Cesare Bernardis, Paolo Cremonesi", "title": "On the instability of embeddings for recommender systems: the case of\n  Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most state-of-the-art top-N collaborative recommender systems work by\nlearning embeddings to jointly represent users and items. Learned embeddings\nare considered to be effective to solve a variety of tasks. Among others,\nproviding and explaining recommendations. In this paper we question the\nreliability of the embeddings learned by Matrix Factorization (MF). We\nempirically demonstrate that, by simply changing the initial values assigned to\nthe latent factors, the same MF method generates very different embeddings of\nitems and users, and we highlight that this effect is stronger for less popular\nitems. To overcome these drawbacks, we present a generalization of MF, called\nNearest Neighbors Matrix Factorization (NNMF). The new method propagates the\ninformation about items and users to their neighbors, speeding up the training\nprocedure and extending the amount of information that supports recommendations\nand representations. We describe the NNMF variants of three common MF\napproaches, and with extensive experiments on five different datasets we show\nthat they strongly mitigate the instability issues of the original MF versions\nand they improve the accuracy of recommendations on the long-tail.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 20:05:07 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Gabbolini", "Giovanni", ""], ["D'Amico", "Edoardo", ""], ["Bernardis", "Cesare", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "2104.06048", "submitter": "Emanuela Boros", "authors": "Emanuela Boros and Antoine Doucet", "title": "Transformer-based Methods for Recognizing Ultra Fine-grained Entities\n  (RUFES)", "comments": null, "journal-ref": "https://tac.nist.gov/2020/KBP/RUFES/index.html", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper summarizes the participation of the Laboratoire Informatique,\nImage et Interaction (L3i laboratory) of the University of La Rochelle in the\nRecognizing Ultra Fine-grained Entities (RUFES) track within the Text Analysis\nConference (TAC) series of evaluation workshops. Our participation relies on\ntwo neural-based models, one based on a pre-trained and fine-tuned language\nmodel with a stack of Transformer layers for fine-grained entity extraction and\none out-of-the-box model for within-document entity coreference. We observe\nthat our approach has great potential in increasing the performance of\nfine-grained entity recognition. Thus, the future work envisioned is to enhance\nthe ability of the models following additional experiments and a deeper\nanalysis of the results.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 09:23:16 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Boros", "Emanuela", ""], ["Doucet", "Antoine", ""]]}, {"id": "2104.06077", "submitter": "Xinyi Dai", "authors": "Xinyi Dai, Jianghao Lin, Weinan Zhang, Shuai Li, Weiwen Liu, Ruiming\n  Tang, Xiuqiang He, Jianye Hao, Jun Wang, Yong Yu", "title": "An Adversarial Imitation Click Model for Information Retrieval", "comments": "Accepted to WWW 2021", "journal-ref": null, "doi": "10.1145/3442381.3449913", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern information retrieval systems, including web search, ads placement,\nand recommender systems, typically rely on learning from user feedback. Click\nmodels, which study how users interact with a ranked list of items, provide a\nuseful understanding of user feedback for learning ranking models. Constructing\n\"right\" dependencies is the key of any successful click model. However,\nprobabilistic graphical models (PGMs) have to rely on manually assigned\ndependencies, and oversimplify user behaviors. Existing neural network based\nmethods promote PGMs by enhancing the expressive ability and allowing flexible\ndependencies, but still suffer from exposure bias and inferior estimation. In\nthis paper, we propose a novel framework, Adversarial Imitation Click Model\n(AICM), based on imitation learning. Firstly, we explicitly learn the reward\nfunction that recovers users' intrinsic utility and underlying intentions.\nSecondly, we model user interactions with a ranked list as a dynamic system\ninstead of one-step click prediction, alleviating the exposure bias problem.\nFinally, we minimize the JS divergence through adversarial training and learn a\nstable distribution of click sequences, which makes AICM generalize well across\ndifferent distributions of ranked lists. A theoretical analysis has indicated\nthat AICM reduces the exposure bias from $O(T^2)$ to $O(T)$. Our studies on a\npublic web search dataset show that AICM not only outperforms state-of-the-art\nmodels in traditional click metrics but also achieves superior performance in\naddressing the exposure bias and recovering the underlying patterns of click\nsequences.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 10:17:55 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 05:41:07 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dai", "Xinyi", ""], ["Lin", "Jianghao", ""], ["Zhang", "Weinan", ""], ["Li", "Shuai", ""], ["Liu", "Weiwen", ""], ["Tang", "Ruiming", ""], ["He", "Xiuqiang", ""], ["Hao", "Jianye", ""], ["Wang", "Jun", ""], ["Yu", "Yong", ""]]}, {"id": "2104.06312", "submitter": "Keke Zhao", "authors": "Keke Zhao, Xing Zhao, Qi Cao, Linjian Mo", "title": "A Non-sequential Approach to Deep User Interest Model for CTR Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-Through Rate (CTR) prediction plays an important role in many\nindustrial applications, and recently a lot of attention is paid to the deep\ninterest models which use attention mechanism to capture user interests from\nhistorical behaviors. However, most current models are based on sequential\nmodels which truncate the behavior sequences by a fixed length, thus have\ndifficulties in handling very long behavior sequences. Another big problem is\nthat sequences with the same length can be quite different in terms of time,\ncarrying completely different meanings. In this paper, we propose a\nnon-sequential approach to tackle the above problems. Specifically, we first\nrepresent the behavior data in a sparse key-vector format, where the vector\ncontains rich behavior info such as time, count and category. Next, we enhance\nthe Deep Interest Network to take such rich information into account by a novel\nattention network. The sparse representation makes it practical to handle large\nscale long behavior sequences. Finally, we introduce a multidimensional\npartition framework to mine behavior interactions. The framework can partition\ndata into custom designed time buckets to capture the interactions among\ninformation aggregated in different time buckets. Similarly, it can also\npartition the data into different categories and capture the interactions among\nthem. Experiments are conducted on two public datasets: one is an advertising\ndataset and the other is a production recommender dataset. Our models\noutperform other state-of-the-art models on both datasets.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 14:10:49 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 07:29:20 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Zhao", "Keke", ""], ["Zhao", "Xing", ""], ["Cao", "Qi", ""], ["Mo", "Linjian", ""]]}, {"id": "2104.06313", "submitter": "Yang Gao", "authors": "Yang Gao, Yi-Fan Li, Yu Lin, Charu Aggarwal, Latifur Khan", "title": "SetConv: A New Approach for Learning from Imbalanced Data", "comments": "Accepted by EMNLP 2020 (11 pages, 9 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For many real-world classification problems, e.g., sentiment classification,\nmost existing machine learning methods are biased towards the majority class\nwhen the Imbalance Ratio (IR) is high. To address this problem, we propose a\nset convolution (SetConv) operation and an episodic training strategy to\nextract a single representative for each class, so that classifiers can later\nbe trained on a balanced class distribution. We prove that our proposed\nalgorithm is permutation-invariant despite the order of inputs, and experiments\non multiple large-scale benchmark text datasets show the superiority of our\nproposed framework when compared to other SOTA methods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 22:33:30 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Gao", "Yang", ""], ["Li", "Yi-Fan", ""], ["Lin", "Yu", ""], ["Aggarwal", "Charu", ""], ["Khan", "Latifur", ""]]}, {"id": "2104.06529", "submitter": "Rafael Ferreira", "authors": "Rafael Ferreira, David Semedo, Joao Magalhaes", "title": "BERT Embeddings Can Track Context in Conversational Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of conversational assistants to search for information is becoming\nincreasingly more popular among the general public, pushing the research\ntowards more advanced and sophisticated techniques. In the last few years, in\nparticular, the interest in conversational search is increasing, not only\nbecause of the generalization of conversational assistants but also because\nconversational search is a step forward in allowing a more natural interaction\nwith the system.\n  In this work, the focus is on exploring the context present of the\nconversation via the historical utterances and respective embeddings with the\naim of developing a conversational search system that helps people search for\ninformation in a natural way. In particular, this system must be able to\nunderstand the context where the question is posed, tracking the current state\nof the conversation and detecting mentions to previous questions and answers.\nWe achieve this by using a context-tracking component based on neural\nquery-rewriting models. Another crucial aspect of the system is to provide the\nmost relevant answers given the question and the conversational history. To\nachieve this objective, we used a Transformer-based re-ranking method and\nexpanded this architecture to use the conversational context.\n  The results obtained with the system developed showed the advantages of using\nthe context present in the natural language utterances and in the neural\nembeddings generated throughout the conversation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 22:02:24 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Ferreira", "Rafael", ""], ["Semedo", "David", ""], ["Magalhaes", "Joao", ""]]}, {"id": "2104.06892", "submitter": "David Semedo", "authors": "Mariana Leite, Rafael Ferreira, David Semedo, Jo\\~ao Magalh\\~aes", "title": "Knowledge-driven Answer Generation for Conversational Search", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conversational search paradigm introduces a step change over the\ntraditional search paradigm by allowing users to interact with search agents in\na multi-turn and natural fashion. The conversation flows naturally and is\nusually centered around a target field of knowledge. In this work, we propose a\nknowledge-driven answer generation approach for open-domain conversational\nsearch, where a conversation-wide entities' knowledge graph is used to bias\nsearch-answer generation. First, a conversation-specific knowledge graph is\nextracted from the top passages retrieved with a Transformer-based re-ranker.\nThe entities knowledge-graph is then used to bias a search-answer generator\nTransformer towards information rich and concise answers. This conversation\nspecific bias is computed by identifying the most relevant passages according\nto the most salient entities of that particular conversation. Experiments show\nthat the proposed approach successfully exploits entities knowledge along the\nconversation, and outperforms a set of baselines on the search-answer\ngeneration task.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 14:35:54 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Leite", "Mariana", ""], ["Ferreira", "Rafael", ""], ["Semedo", "David", ""], ["Magalh\u00e3es", "Jo\u00e3o", ""]]}, {"id": "2104.06967", "submitter": "Sebastian Hofst\\\"atter", "authors": "Sebastian Hofst\\\"atter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin,\n  Allan Hanbury", "title": "Efficiently Teaching an Effective Dense Retriever with Balanced Topic\n  Aware Sampling", "comments": "Accepted at SIGIR 2021 (Full Paper track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vital step towards the widespread adoption of neural retrieval models is\ntheir resource efficiency throughout the training, indexing and query\nworkflows. The neural IR community made great advancements in training\neffective dual-encoder dense retrieval (DR) models recently. A dense text\nretrieval model uses a single vector representation per query and passage to\nscore a match, which enables low-latency first stage retrieval with a nearest\nneighbor search. Increasingly common, training approaches require enormous\ncompute power, as they either conduct negative passage sampling out of a\ncontinuously updating refreshing index or require very large batch sizes for\nin-batch negative sampling. Instead of relying on more compute capability, we\nintroduce an efficient topic-aware query and balanced margin sampling\ntechnique, called TAS-Balanced. We cluster queries once before training and\nsample queries out of a cluster per batch. We train our lightweight 6-layer DR\nmodel with a novel dual-teacher supervision that combines pairwise and in-batch\nnegative teachers. Our method is trainable on a single consumer-grade GPU in\nunder 48 hours (as opposed to a common configuration of 8x V100s). We show that\nour TAS-Balanced training method achieves state-of-the-art low-latency (64ms\nper query) results on two TREC Deep Learning Track query sets. Evaluated on\nNDCG@10, we outperform BM25 by 44%, a plainly trained DR by 19%, docT5query by\n11%, and the previous best DR model by 5%. Additionally, TAS-Balanced produces\nthe first dense retriever that outperforms every other method on recall at any\ncutoff on TREC-DL and allows more resource intensive re-ranking models to\noperate on fewer passages to improve results further.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:49:18 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 12:04:24 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Hofst\u00e4tter", "Sebastian", ""], ["Lin", "Sheng-Chieh", ""], ["Yang", "Jheng-Hong", ""], ["Lin", "Jimmy", ""], ["Hanbury", "Allan", ""]]}, {"id": "2104.06969", "submitter": "Emanuela Boros", "authors": "Emanuela Boros, Jose G. Moreno, Antoine Doucet", "title": "Event Detection as Question Answering with Entity Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a recent and under-researched paradigm for the task\nof event detection (ED) by casting it as a question-answering (QA) problem with\nthe possibility of multiple answers and the support of entities. The extraction\nof event triggers is, thus, transformed into the task of identifying answer\nspans from a context, while also focusing on the surrounding entities. The\narchitecture is based on a pre-trained and fine-tuned language model, where the\ninput context is augmented with entities marked at different levels, their\npositions, their types, and, finally, the argument roles. Experiments on the\nACE~2005 corpus demonstrate that the proposed paradigm is a viable solution for\nthe ED task and it significantly outperforms the state-of-the-art models.\nMoreover, we prove that our methods are also able to extract unseen event\ntypes.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:53:11 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Boros", "Emanuela", ""], ["Moreno", "Jose G.", ""], ["Doucet", "Antoine", ""]]}, {"id": "2104.07096", "submitter": "Svitlana Vakulenko", "authors": "Svitlana Vakulenko, Evangelos Kanoulas, Maarten de Rijke", "title": "A Large-Scale Analysis of Mixed Initiative in Information-Seeking\n  Dialogues for Conversational Search", "comments": "32 pages; To appear in ACM Transactions on Information Systems\n  (TOIS), Special Issue on Conversational Search and Recommendation. 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search is a relatively young area of research that aims at\nautomating an information-seeking dialogue. In this paper we help to position\nit with respect to other research areas within conversational Artificial\nIntelligence (AI) by analysing the structural properties of an\ninformation-seeking dialogue. To this end, we perform a large-scale dialogue\nanalysis of more than 150K transcripts from 16 publicly available dialogue\ndatasets. These datasets were collected to inform different dialogue-based\ntasks including conversational search. We extract different patterns of mixed\ninitiative from these dialogue transcripts and use them to compare dialogues of\ndifferent types. Moreover, we contrast the patterns found in\ninformation-seeking dialogues that are being used for research purposes with\nthe patterns found in virtual reference interviews that were conducted by\nprofessional librarians. The insights we provide (1) establish close relations\nbetween conversational search and other conversational AI tasks; and (2)\nuncover limitations of existing conversational datasets to inform future data\ncollection tasks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:47:20 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 10:06:01 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["Kanoulas", "Evangelos", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2104.07186", "submitter": "Luyu Gao", "authors": "Luyu Gao, Zhuyun Dai, Jamie Callan", "title": "COIL: Revisit Exact Lexical Match in Information Retrieval with\n  Contextualized Inverted List", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical information retrieval systems such as BM25 rely on exact lexical\nmatch and carry out search efficiently with inverted list index. Recent neural\nIR models shifts towards soft semantic matching all query document terms, but\nthey lose the computation efficiency of exact match systems. This paper\npresents COIL, a contextualized exact match retrieval architecture that brings\nsemantic lexical matching. COIL scoring is based on overlapping query document\ntokens' contextualized representations. The new architecture stores\ncontextualized token representations in inverted lists, bringing together the\nefficiency of exact match and the representation power of deep language models.\nOur experimental results show COIL outperforms classical lexical retrievers and\nstate-of-the-art deep LM retrievers with similar or smaller latency.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 00:53:54 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Gao", "Luyu", ""], ["Dai", "Zhuyun", ""], ["Callan", "Jamie", ""]]}, {"id": "2104.07198", "submitter": "Kyoung-Rok Jang", "authors": "Kyoung-Rok Jang, Junmo Kang, Giwon Hong, Sung-Hyon Myaeng, Joohee\n  Park, Taewon Yoon, Heecheol Seo", "title": "UHD-BERT: Bucketed Ultra-High Dimensional Sparse Representations for\n  Full Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural information retrieval (IR) models are promising mainly because their\nsemantic matching capabilities can ameliorate the well-known synonymy and\npolysemy problems of word-based symbolic approaches. However, the power of\nneural models' dense representations comes at the cost of inefficiency,\nlimiting it to be used as a re-ranker. Sparse representations, on the other\nhand, can help enhance symbolic or latent-term representations and yet take\nadvantage of an inverted index for efficiency, being amenable to symbolic IR\ntechniques that have been around for decades. In order to transcend the\ntrade-off between sparse representations (symbolic or latent-term based) and\ndense representations, we propose an ultra-high dimensional (UHD)\nrepresentation scheme equipped with directly controllable sparsity. With the\nhigh dimensionality, we attempt to make the meaning of each dimension less\nentangled and polysemous than dense embeddings. The sparsity allows for not\nonly efficiency for vector calculations but also the possibility of making\nindividual dimensions attributable to interpretable concepts. Our model,\nUHD-BERT, maximizes the benefits of ultra-high dimensional (UHD) sparse\nrepresentations based on BERT language modeling, by adopting a bucketing\nmethod. With this method, different segments of an embedding (horizontal\nbuckets) or the embeddings from multiple layers of BERT (vertical buckets) can\nbe selected and merged so that diverse linguistic aspects can be represented.\nAn additional and important benefit of our highly disentangled\n(high-dimensional) and efficient (sparse) representations is that this neural\napproach can be harmonized with well-studied symbolic IR techniques (e.g.,\ninverted index, pseudo-relevance feedback, BM25), enabling us to build a\npowerful and efficient neuro-symbolic information retrieval system.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 02:00:01 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Jang", "Kyoung-Rok", ""], ["Kang", "Junmo", ""], ["Hong", "Giwon", ""], ["Myaeng", "Sung-Hyon", ""], ["Park", "Joohee", ""], ["Yoon", "Taewon", ""], ["Seo", "Heecheol", ""]]}, {"id": "2104.07269", "submitter": "Dongsheng Li", "authors": "Dongsheng Li, Haodong Liu, Chao Chen, Yingying Zhao, Stephen M. Chu,\n  Bo Yang", "title": "NeuSE: A Neural Snapshot Ensemble Method for Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In collaborative filtering (CF) algorithms, the optimal models are usually\nlearned by globally minimizing the empirical risks averaged over all the\nobserved data. However, the global models are often obtained via a performance\ntradeoff among users/items, i.e., not all users/items are perfectly fitted by\nthe global models due to the hard non-convex optimization problems in CF\nalgorithms. Ensemble learning can address this issue by learning multiple\ndiverse models but usually suffer from efficiency issue on large datasets or\ncomplex algorithms. In this paper, we keep the intermediate models obtained\nduring global model learning as the snapshot models, and then adaptively\ncombine the snapshot models for individual user-item pairs using a memory\nnetwork-based method. Empirical studies on three real-world datasets show that\nthe proposed method can extensively and significantly improve the accuracy (up\nto 15.9% relatively) when applied to a variety of existing collaborative\nfiltering methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 06:43:40 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Li", "Dongsheng", ""], ["Liu", "Haodong", ""], ["Chen", "Chao", ""], ["Zhao", "Yingying", ""], ["Chu", "Stephen M.", ""], ["Yang", "Bo", ""]]}, {"id": "2104.07345", "submitter": "Jamal Al Qundus", "authors": "Jamal Al Qundus, Ralph Sch\\\"afermeier, Naouel Karam, Silvio Peikert,\n  Adrian Paschke", "title": "ROC: An Ontology for Country Responses towards COVID-19", "comments": "10 pages, 3 figures", "journal-ref": "Qurator2021 - Conference on Digital Curation Technologies", "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ROC ontology for country responses to COVID-19 provides a model for\ncollecting, linking and sharing data on the COVID-19 pandemic. It follows\nsemantic standardization (W3C standards RDF, OWL, SPARQL) for the\nrepresentation of concepts and creation of vocabularies. ROC focuses on country\nmeasures and enables the integration of data from heterogeneous data sources.\nThe proposed ontology is intended to facilitate statistical analysis to study\nand evaluate the effectiveness and side effects of government responses to\nCOVID-19 in different countries. The ontology contains data collected by OxCGRT\nfrom publicly available information. This data has been compiled from\ninformation provided by ECDC for most countries, as well as from various\nrepositories used to collect data on COVID-19.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 10:12:19 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Qundus", "Jamal Al", ""], ["Sch\u00e4fermeier", "Ralph", ""], ["Karam", "Naouel", ""], ["Peikert", "Silvio", ""], ["Paschke", "Adrian", ""]]}, {"id": "2104.07360", "submitter": "Jingwei Yi", "authors": "Jingwei Yi, Fangzhao Wu, Chuhan Wu, Qifei Li, Guangzhong Sun, Xing Xie", "title": "DebiasedRec: Bias-aware User Modeling and Click Prediction for\n  Personalized News Recommendation", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News recommendation is critical for personalized news access. Existing news\nrecommendation methods usually infer users' personal interest based on their\nhistorical clicked news, and train the news recommendation models by predicting\nfuture news clicks. A core assumption behind these methods is that news click\nbehaviors can indicate user interest. However, in practical scenarios, beyond\nthe relevance between user interest and news content, the news click behaviors\nmay also be affected by other factors, such as the bias of news presentation in\nthe online platform. For example, news with higher positions and larger sizes\nare usually more likely to be clicked. The bias of clicked news may bring\nnoises to user interest modeling and model training, which may hurt the\nperformance of the news recommendation model.\n  In this paper, we propose a bias-aware personalized news recommendation\nmethod named DebiasRec, which can handle the bias information for more accurate\nuser interest inference and model training. The core of our method includes a\nbias representation module, a bias-aware user modeling module, and a bias-aware\nclick prediction module. The bias representation module is used to model\ndifferent kinds of news bias and their interactions to capture their joint\neffect on click behaviors. The bias-aware user modeling module aims to infer\nusers' debiased interest from the clicked news articles by using their bias\ninformation to calibrate the interest model. The bias-aware click prediction\nmodule is used to train a debiased news recommendation model from the biased\nclick behaviors, where the click score is decomposed into a preference score\nindicating user's interest in the news content and a news bias score inferred\nfrom its different bias features. Experiments on two real-world datasets show\nthat our method can effectively improve the performance of news recommendation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 10:36:08 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yi", "Jingwei", ""], ["Wu", "Fangzhao", ""], ["Wu", "Chuhan", ""], ["Li", "Qifei", ""], ["Sun", "Guangzhong", ""], ["Xie", "Xing", ""]]}, {"id": "2104.07368", "submitter": "Mengqi Zhang", "authors": "Mengqi Zhang, Shu Wu, Xueli Yu, Qiang Liu and Liang Wang", "title": "Dynamic Graph Neural Networks for Sequential Recommendation", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling user preference from his historical sequences is one of the core\nproblems of sequential recommendation. Existing methods in this field are\nwidely distributed from conventional methods to deep learning methods. However,\nmost of them only model users' interests within their own sequences and ignore\nthe dynamic collaborative signals among different user sequences, making it\ninsufficient to explore users' preferences. We take inspiration from dynamic\ngraph neural networks to cope with this challenge, modeling the user sequence\nand dynamic collaborative signals into one framework. We propose a new method\nnamed Dynamic Graph Neural Network for Sequential Recommendation (DGSR), which\nconnects different user sequences through a dynamic graph structure, exploring\nthe interactive behavior of users and items with time and order information.\nFurthermore, we design a Dynamic Graph Recommendation Network to extract user's\npreferences from the dynamic graph. Consequently, the next-item prediction task\nin sequential recommendation is converted into a link prediction between the\nuser node and the item node in a dynamic graph. Extensive experiments on three\npublic benchmarks show that DGSR outperforms several state-of-the-art methods.\nFurther studies demonstrate the rationality and effectiveness of modeling user\nsequences through a dynamic graph.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 10:53:52 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 02:17:01 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zhang", "Mengqi", ""], ["Wu", "Shu", ""], ["Yu", "Xueli", ""], ["Liu", "Qiang", ""], ["Wang", "Liang", ""]]}, {"id": "2104.07378", "submitter": "Maya Ramanath", "authors": "Saransh Goyal, Pratyush Pandey, Garima Gaur, Subhalingam D, Srikanta\n  Bedathur, Maya Ramanath", "title": "Tracking entities in technical procedures -- a new dataset and baselines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce TechTrack, a new dataset for tracking entities in technical\nprocedures. The dataset, prepared by annotating open domain articles from\nWikiHow, consists of 1351 procedures, e.g., \"How to connect a printer\",\nidentifies more than 1200 unique entities with an average of 4.7 entities per\nprocedure. We evaluate the performance of state-of-the-art models on the\nentity-tracking task and find that they are well below the human annotation\nperformance. We describe how TechTrack can be used to take forward the research\non understanding procedures from temporal texts.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 11:16:41 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Goyal", "Saransh", ""], ["Pandey", "Pratyush", ""], ["Gaur", "Garima", ""], ["D", "Subhalingam", ""], ["Bedathur", "Srikanta", ""], ["Ramanath", "Maya", ""]]}, {"id": "2104.07404", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang", "title": "Two Birds with One Stone: Unified Model Learning for Both Recall and\n  Ranking in News Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recall and ranking are two critical steps in personalized news\nrecommendation. Most existing news recommender systems conduct personalized\nnews recall and ranking separately with different models. However, maintaining\nmultiple models leads to high computational cost and poses great challenge to\nmeeting the online latency requirement of news recommender systems. In order to\nhandle this problem, in this paper we propose UniRec, a unified method for\nrecall and ranking in news recommendation. In our method, we first infer user\nembedding for ranking from the historical news click behaviors of a user using\na user encoder model. Then we derive the user embedding for recall from the\nobtained user embedding for ranking by using it as the attention query to\nselect a set of basis user embeddings which encode different general user\ninterests and synthesize them into a user embedding for recall. The extensive\nexperiments on benchmark dataset demonstrate that our method can improve both\nefficiency and effectiveness for recall and ranking in news recommendation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 12:08:30 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Qi", "Tao", ""], ["Huang", "Yongfeng", ""]]}, {"id": "2104.07407", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang", "title": "MM-Rec: Multimodal News Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate news representation is critical for news recommendation. Most of\nexisting news representation methods learn news representations only from news\ntexts while ignore the visual information in news like images. In fact, users\nmay click news not only because of the interest in news titles but also due to\nthe attraction of news images. Thus, images are useful for representing news\nand predicting user behaviors. In this paper, we propose a multimodal news\nrecommendation method, which can incorporate both textual and visual\ninformation of news to learn multimodal news representations. We first extract\nregion-of-interests (ROIs) from news images via objective detection. Then we\nuse a pre-trained visiolinguistic model to encode both news texts and news\nimage ROIs and model their inherent relatedness using co-attentional\nTransformers. In addition, we propose a crossmodal candidate-aware attention\nnetwork to select relevant historical clicked news for accurate user modeling\nby measuring the crossmodal relatedness between clicked news and candidate\nnews. Experiments validate that incorporating multimodal news information can\neffectively improve news recommendation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 12:11:50 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Qi", "Tao", ""], ["Huang", "Yongfeng", ""]]}, {"id": "2104.07413", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Tao Qi, Yongfeng Huang", "title": "Empowering News Recommendation with Pre-trained Language Models", "comments": "To appear in SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized news recommendation is an essential technique for online news\nservices. News articles usually contain rich textual content, and accurate news\nmodeling is important for personalized news recommendation. Existing news\nrecommendation methods mainly model news texts based on traditional text\nmodeling methods, which is not optimal for mining the deep semantic information\nin news texts. Pre-trained language models (PLMs) are powerful for natural\nlanguage understanding, which has the potential for better news modeling.\nHowever, there is no public report that show PLMs have been applied to news\nrecommendation. In this paper, we report our work on exploiting pre-trained\nlanguage models to empower news recommendation. Offline experimental results on\nboth monolingual and multilingual news recommendation datasets show that\nleveraging PLMs for news modeling can effectively improve the performance of\nnews recommendation. Our PLM-empowered news recommendation models have been\ndeployed to the Microsoft News platform, and achieved significant gains in\nterms of both click and pageview in both English-speaking and global markets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 12:26:23 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Qi", "Tao", ""], ["Huang", "Yongfeng", ""]]}, {"id": "2104.07414", "submitter": "AnChen Li", "authors": "Anchen Li, Bo Yang, Hongxu Chen, Guandong Xu", "title": "Hyperbolic Neural Collaborative Recommender", "comments": "arXiv admin note: substantial text overlap with arXiv:2102.09389", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of hyperbolic geometry and deep learning\ntechniques for recommendation. We present Hyperbolic Neural Collaborative\nRecommender (HNCR), a deep hyperbolic representation learning method that\nexploits mutual semantic relations among users/items for collaborative\nfiltering (CF) tasks. HNCR contains two major phases: neighbor construction and\nrecommendation framework. The first phase introduces a neighbor construction\nstrategy to construct a semantic neighbor set for each user and item according\nto the user-item historical interaction. In the second phase, we develop a deep\nframework based on hyperbolic geometry to integrate constructed neighbor sets\ninto recommendation. Via a series of extensive experiments, we show that HNCR\noutperforms its Euclidean counterpart and state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 12:28:09 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Li", "Anchen", ""], ["Yang", "Bo", ""], ["Chen", "Hongxu", ""], ["Xu", "Guandong", ""]]}, {"id": "2104.07423", "submitter": "Preslav Nakov", "authors": "Shaden Shaar, Firoj Alam, Giovanni Da San Martino, Preslav Nakov", "title": "The Role of Context in Detecting Previously Fact-Checked Claims", "comments": "detecting previously fact-checked claims, fact-checking,\n  disinformation, fake news, social media, political debates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen the proliferation of disinformation and misinformation\nonline, thanks to the freedom of expression on the Internet and to the rise of\nsocial media. Two solutions were proposed to address the problem: (i) manual\nfact-checking, which is accurate and credible, but slow and non-scalable, and\n(ii) automatic fact-checking, which is fast and scalable, but lacks\nexplainability and credibility. With the accumulation of enough manually\nfact-checked claims, a middle-ground approach has emerged: checking whether a\ngiven claim has previously been fact-checked. This can be made automatically,\nand thus fast, while also offering credibility and explainability, thanks to\nthe human fact-checking and explanations in the associated fact-checking\narticle. This is a relatively new and understudied research direction, and here\nwe focus on claims made in a political debate, where context really matters.\nThus, we study the impact of modeling the context of the claim: both on the\nsource side, i.e., in the debate, as well as on the target side, i.e., in the\nfact-checking explanation document. We do this by modeling the local context,\nthe global context, as well as by means of co-reference resolution, and\nreasoning over the target text using Transformer-XH. The experimental results\nshow that each of these represents a valuable information source, but that\nmodeling the source-side context is more important, and can yield 10+ points of\nabsolute improvement.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 12:39:37 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Shaar", "Shaden", ""], ["Alam", "Firoj", ""], ["Martino", "Giovanni Da San", ""], ["Nakov", "Preslav", ""]]}, {"id": "2104.07511", "submitter": "Idan Schwartz", "authors": "Idan Schwartz", "title": "Ensemble of MRR and NDCG models for Visual Dialog", "comments": "Accepted to NAACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assessing an AI agent that can converse in human language and understand\nvisual content is challenging. Generation metrics, such as BLEU scores favor\ncorrect syntax over semantics. Hence a discriminative approach is often used,\nwhere an agent ranks a set of candidate options. The mean reciprocal rank (MRR)\nmetric evaluates the model performance by taking into account the rank of a\nsingle human-derived answer. This approach, however, raises a new challenge:\nthe ambiguity and synonymy of answers, for instance, semantic equivalence\n(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative\ngain (NDCG) metric has been used to capture the relevance of all the correct\nanswers via dense annotations. However, the NDCG metric favors the usually\napplicable uncertain answers such as `I don't know. Crafting a model that\nexcels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should\nanswer a human-like reply and validate the correctness of any answer. To\naddress this issue, we describe a two-step non-parametric ranking approach that\ncan merge strong MRR and NDCG models. Using our approach, we manage to keep\nmost MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG\nstate-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won\nthe recent Visual Dialog 2020 challenge. Source code is available at\nhttps://github.com/idansc/mrr-ndcg.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:09:32 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 16:52:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Schwartz", "Idan", ""]]}, {"id": "2104.07572", "submitter": "Xiquan Cui", "authors": "Mingming Guo, Nian Yan, Xiquan Cui, San He Wu, Unaiza Ahsan, Rebecca\n  West, Khalifeh Al Jadda", "title": "Deep Learning-based Online Alternative Product Recommendations at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Alternative recommender systems are critical for ecommerce companies. They\nguide customers to explore a massive product catalog and assist customers to\nfind the right products among an overwhelming number of options. However, it is\na non-trivial task to recommend alternative products that fit customer needs.\nIn this paper, we use both textual product information (e.g. product titles and\ndescriptions) and customer behavior data to recommend alternative products. Our\nresults show that the coverage of alternative products is significantly\nimproved in offline evaluations as well as recall and precision. The final A/B\ntest shows that our algorithm increases the conversion rate by 12 percent in a\nstatistically significant way. In order to better capture the semantic meaning\nof product information, we build a Siamese Network with Bidirectional LSTM to\nlearn product embeddings. In order to learn a similarity space that better\nmatches the preference of real customers, we use co-compared data from\nhistorical customer behavior as labels to train the network. In addition, we\nuse NMSLIB to accelerate the computationally expensive kNN computation for\nmillions of products so that the alternative recommendation is able to scale\nacross the entire catalog of a major ecommerce site.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:27:45 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Guo", "Mingming", ""], ["Yan", "Nian", ""], ["Cui", "Xiquan", ""], ["Wu", "San He", ""], ["Ahsan", "Unaiza", ""], ["West", "Rebecca", ""], ["Jadda", "Khalifeh Al", ""]]}, {"id": "2104.07748", "submitter": "Venugopal Mani", "authors": "Ramasubramanian Balasubramanian, Venugopal Mani, Abhinav Mathur,\n  Sushant Kumar, Kannan Achan", "title": "Variational Inference for Category Recommendation in E-Commerce\n  platforms", "comments": "8 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Category recommendation for users on an e-Commerce platform is an important\ntask as it dictates the flow of traffic through the website. It is therefore\nimportant to surface precise and diverse category recommendations to aid the\nusers' journey through the platform and to help them discover new groups of\nitems. An often understated part in category recommendation is users'\nproclivity to repeat purchases. The structure of this temporal behavior can be\nharvested for better category recommendations and in this work, we attempt to\nharness this through variational inference. Further, to enhance the variational\ninference based optimization, we initialize the optimizer at better starting\npoints through the well known Metapath2Vec algorithm. We demonstrate our\nresults on two real-world datasets and show that our model outperforms standard\nbaseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 20:09:20 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 02:35:14 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Balasubramanian", "Ramasubramanian", ""], ["Mani", "Venugopal", ""], ["Mathur", "Abhinav", ""], ["Kumar", "Sushant", ""], ["Achan", "Kannan", ""]]}, {"id": "2104.07800", "submitter": "Revanth Reddy", "authors": "Revanth Gangi Reddy, Vikas Yadav, Md Arafat Sultan, Martin Franz,\n  Vittorio Castelli, Heng Ji, Avirup Sil", "title": "Towards Robust Neural Retrieval Models with Synthetic Pre-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has shown that commonly available machine reading comprehension\n(MRC) datasets can be used to train high-performance neural information\nretrieval (IR) systems. However, the evaluation of neural IR has so far been\nlimited to standard supervised learning settings, where they have outperformed\ntraditional term matching baselines. We conduct in-domain and out-of-domain\nevaluations of neural IR, and seek to improve its robustness across different\nscenarios, including zero-shot settings. We show that synthetic training\nexamples generated using a sequence-to-sequence generator can be effective\ntowards this goal: in our experiments, pre-training with synthetic examples\nimproves retrieval performance in both in-domain and out-of-domain evaluation\non five different test sets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 22:12:01 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Reddy", "Revanth Gangi", ""], ["Yadav", "Vikas", ""], ["Sultan", "Md Arafat", ""], ["Franz", "Martin", ""], ["Castelli", "Vittorio", ""], ["Ji", "Heng", ""], ["Sil", "Avirup", ""]]}, {"id": "2104.07814", "submitter": "Zihao He", "authors": "Zihao He, Negar Mokhberian, Antonio Camara, Andres Abeliuk, Kristina\n  Lerman", "title": "Detecting Polarized Topics in COVID-19 News Using Partisanship-aware\n  Contextualized Topic Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Growing polarization of the news media has been blamed for fanning\ndisagreement, controversy and even violence. Early identification of polarized\ntopics is thus an urgent matter that can help mitigate conflict. However,\naccurate measurement of polarization is still an open research challenge. To\naddress this gap, we propose Partisanship-aware Contextualized Topic Embeddings\n(PaCTE), a method to automatically detect polarized topics from partisan news\nsources. Specifically, we represent the ideology of a news source on a topic by\ncorpus-contextualized topic embedding utilizing a language model that has been\nfinetuned on recognizing partisanship of the news articles, and measure the\npolarization between sources using cosine similarity. We apply our method to a\ncorpus of news about COVID-19 pandemic. Extensive experiments on different news\nsources and topics demonstrate the effectiveness of our method to precisely\ncapture the topical polarization and alignment between different news sources.\nTo help clarify and validate results, we explain the polarization using the\nMoral Foundation Theory.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 23:05:52 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["He", "Zihao", ""], ["Mokhberian", "Negar", ""], ["Camara", "Antonio", ""], ["Abeliuk", "Andres", ""], ["Lerman", "Kristina", ""]]}, {"id": "2104.07858", "submitter": "Shitao Xiao", "authors": "Shitao Xiao, Zheng Liu, Yingxia Shao, Defu Lian, Xing Xie", "title": "Search-oriented Differentiable Product Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Product quantization (PQ) is a popular approach for maximum inner product\nsearch (MIPS), which is widely used in ad-hoc retrieval. Recent studies propose\ndifferentiable PQ, where the embedding and quantization modules can be trained\njointly. However, there is a lack of in-depth understanding of appropriate\njoint training objectives; and the improvements over non-differentiable\nbaselines are not consistently positive in reality. In this work, we propose\nSearch-oriented Product Quantization (SoPQ), where a novel training objective\nMCL is formulated. With the minimization of MCL, query and key's matching\nprobability can be maximized for the differentiable PQ. Besides, VCS protocol\nis designed to facilitate the minimization of MCL, and SQL is leveraged to\nrelax the dependency on labeled data. Extensive experiments on 4 real-world\ndatasets validate the effectiveness of our proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 02:25:46 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Xiao", "Shitao", ""], ["Liu", "Zheng", ""], ["Shao", "Yingxia", ""], ["Lian", "Defu", ""], ["Xie", "Xing", ""]]}, {"id": "2104.07869", "submitter": "Yikun Xian", "authors": "Yaxin Zhu, Yikun Xian, Zuohui Fu, Gerard de Melo, Yongfeng Zhang", "title": "Faithfully Explainable Recommendation via Neural Logic Reasoning", "comments": "Accepted in NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs (KG) have become increasingly important to endow modern\nrecommender systems with the ability to generate traceable reasoning paths to\nexplain the recommendation process. However, prior research rarely considers\nthe faithfulness of the derived explanations to justify the decision making\nprocess. To the best of our knowledge, this is the first work that models and\nevaluates faithfully explainable recommendation under the framework of KG\nreasoning. Specifically, we propose neural logic reasoning for explainable\nrecommendation (LOGER) by drawing on interpretable logical rules to guide the\npath reasoning process for explanation generation. We experiment on three\nlarge-scale datasets in the e-commerce domain, demonstrating the effectiveness\nof our method in delivering high-quality recommendations as well as\nascertaining the faithfulness of the derived explanation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 03:07:41 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhu", "Yaxin", ""], ["Xian", "Yikun", ""], ["Fu", "Zuohui", ""], ["de Melo", "Gerard", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2104.07878", "submitter": "Yushan Zheng", "authors": "Yushan Zheng, Zhiguo Jiang, Haopeng Zhang, Fengying Xie, Jun Shi,\n  Chenghai Xue", "title": "Histopathology WSI Encoding based on GCNs for Scalable and Efficient\n  Retrieval of Diagnostically Relevant Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based histopathological image retrieval (CBHIR) has become popular in\nrecent years in the domain of histopathological image analysis. CBHIR systems\nprovide auxiliary diagnosis information for pathologists by searching for and\nreturning regions that are contently similar to the region of interest (ROI)\nfrom a pre-established database. While, it is challenging and yet significant\nin clinical applications to retrieve diagnostically relevant regions from a\ndatabase that consists of histopathological whole slide images (WSIs) for a\nquery ROI. In this paper, we propose a novel framework for regions retrieval\nfrom WSI-database based on hierarchical graph convolutional networks (GCNs) and\nHash technique. Compared to the present CBHIR framework, the structural\ninformation of WSI is preserved through graph embedding of GCNs, which makes\nthe retrieval framework more sensitive to regions that are similar in tissue\ndistribution. Moreover, benefited from the hierarchical GCN structures, the\nproposed framework has good scalability for both the size and shape variation\nof ROIs. It allows the pathologist defining query regions using free curves\naccording to the appearance of tissue. Thirdly, the retrieval is achieved based\non Hash technique, which ensures the framework is efficient and thereby\nadequate for practical large-scale WSI-database. The proposed method was\nvalidated on two public datasets for histopathological WSI analysis and\ncompared to the state-of-the-art methods. The proposed method achieved mean\naverage precision above 0.857 on the ACDC-LungHP dataset and above 0.864 on the\nCamelyon16 dataset in the irregular region retrieval tasks, which are superior\nto the state-of-the-art methods. The average retrieval time from a database\nwithin 120 WSIs is 0.802 ms.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 04:12:33 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zheng", "Yushan", ""], ["Jiang", "Zhiguo", ""], ["Zhang", "Haopeng", ""], ["Xie", "Fengying", ""], ["Shi", "Jun", ""], ["Xue", "Chenghai", ""]]}, {"id": "2104.07969", "submitter": "Jason Wang", "authors": "Jason Wang and Robert E. Weiss", "title": "Hierarchical Topic Presence Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Topic models analyze text from a set of documents. Documents are modeled as a\nmixture of topics, with topics defined as probability distributions on words.\nInferences of interest include the most probable topics and characterization of\na topic by inspecting the topic's highest probability words. Motivated by a\ndata set of web pages (documents) nested in web sites, we extend the Poisson\nfactor analysis topic model to hierarchical topic presence models for analyzing\ntext from documents nested in known groups. We incorporate an unknown binary\ntopic presence parameter for each topic at the web site and/or the web page\nlevel to allow web sites and/or web pages to be sparse mixtures of topics and\nwe propose logistic regression modeling of topic presence conditional on web\nsite covariates. We introduce local topics into the Poisson factor analysis\nframework, where each web site has a local topic not found in other web sites.\nTwo data augmentation methods, the Chinese table distribution and\nP\\'{o}lya-Gamma augmentation, aid in constructing our sampler. We analyze text\nfrom web pages nested in United States local public health department web sites\nto abstract topical information and understand national patterns in topic\npresence.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 08:41:07 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Wang", "Jason", ""], ["Weiss", "Robert E.", ""]]}, {"id": "2104.08051", "submitter": "Jingtao Zhan", "authors": "Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, Shaoping\n  Ma", "title": "Optimizing Dense Retrieval Model Training with Hard Negatives", "comments": "To be published in SIGIR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking has always been one of the top concerns in information retrieval\nresearches. For decades, the lexical matching signal has dominated the ad-hoc\nretrieval process, but solely using this signal in retrieval may cause the\nvocabulary mismatch problem. In recent years, with the development of\nrepresentation learning techniques, many researchers turn to Dense Retrieval\n(DR) models for better ranking performance. Although several existing DR models\nhave already obtained promising results, their performance improvement heavily\nrelies on the sampling of training examples. Many effective sampling strategies\nare not efficient enough for practical usage, and for most of them, there still\nlacks theoretical analysis in how and why performance improvement happens. To\nshed light on these research questions, we theoretically investigate different\ntraining strategies for DR models and try to explain why hard negative sampling\nperforms better than random sampling. Through the analysis, we also find that\nthere are many potential risks in static hard negative sampling, which is\nemployed by many existing training methods. Therefore, we propose two training\nstrategies named a Stable Training Algorithm for dense Retrieval (STAR) and a\nquery-side training Algorithm for Directly Optimizing Ranking pErformance\n(ADORE), respectively. STAR improves the stability of DR training process by\nintroducing random negatives. ADORE replaces the widely-adopted static hard\nnegative sampling method with a dynamic one to directly optimize the ranking\nperformance. Experimental results on two publicly available retrieval benchmark\ndatasets show that either strategy gains significant improvements over existing\ncompetitive baselines and a combination of them leads to the best performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 11:57:35 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhan", "Jingtao", ""], ["Mao", "Jiaxin", ""], ["Liu", "Yiqun", ""], ["Guo", "Jiafeng", ""], ["Zhang", "Min", ""], ["Ma", "Shaoping", ""]]}, {"id": "2104.08253", "submitter": "Luyu Gao", "authors": "Luyu Gao, Jamie Callan", "title": "Is Your Language Model Ready for Dense Representation Fine-tuning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained language models (LM) have become go-to text representation\nencoders. Prior research used deep LMs to encode text sequences such as\nsentences and passages into single dense vector representations. These dense\nrepresentations have been used in efficient text comparison and embedding-based\nretrieval. However, dense encoders suffer in low resource situations. Many\ntechniques have been developed to solve this problem. Despite their success,\nnot much is known about why this happens. This paper shows that one cause lies\nin the readiness of the LM to expose its knowledge through dense representation\nin fine-tuning, which we term Optimization Readiness. To validate the theory,\nwe present Condenser, a general pre-training architecture based on Transformer\nLMs, to improve dense optimization readiness. We show that fine-tuning from\nCondenser significantly improves performance for small and/or noisy training\nsets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:36:44 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Gao", "Luyu", ""], ["Callan", "Jamie", ""]]}, {"id": "2104.08340", "submitter": "Alexandros Ioannidis Mr", "authors": "Alexandros Ioannidis", "title": "An Analysis of a BERT Deep Learning Strategy on a Technology Assisted\n  Review Task", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Document screening is a central task within Evidenced Based Medicine, which\nis a clinical discipline that supplements scientific proof to back medical\ndecisions. Given the recent advances in DL (Deep Learning) methods applied to\nInformation Retrieval tasks, I propose a DL document classification approach\nwith BERT or PubMedBERT embeddings and a DL similarity search path using SBERT\nembeddings to reduce physicians' tasks of screening and classifying immense\namounts of documents to answer clinical queries. I test and evaluate the\nretrieval effectiveness of my DL strategy on the 2017 and 2018 CLEF eHealth\ncollections. I find that the proposed DL strategy works, I compare it to the\nrecently successful BM25 plus RM3 model, and conclude that the suggested method\naccomplishes advanced retrieval performance in the initial ranking of the\narticles with the aforementioned datasets, for the CLEF eHealth Technologically\nAssisted Reviews in Empirical Medicine Task.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 19:45:27 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ioannidis", "Alexandros", ""]]}, {"id": "2104.08405", "submitter": "Te-Lin Wu", "authors": "Te-Lin Wu, Cheng Li, Mingyang Zhang, Tao Chen, Spurthi Amba Hombaiah,\n  Michael Bendersky", "title": "LAMPRET: Layout-Aware Multimodal PreTraining for Document Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Document layout comprises both structural and visual (eg. font-sizes)\ninformation that is vital but often ignored by machine learning models. The few\nexisting models which do use layout information only consider textual contents,\nand overlook the existence of contents in other modalities such as images.\nAdditionally, spatial interactions of presented contents in a layout were never\nreally fully exploited. To bridge this gap, we parse a document into content\nblocks (eg. text, table, image) and propose a novel layout-aware multimodal\nhierarchical framework, LAMPreT, to model the blocks and the whole document.\nOur LAMPreT encodes each block with a multimodal transformer in the lower-level\nand aggregates the block-level representations and connections utilizing a\nspecifically designed transformer at the higher-level. We design hierarchical\npretraining objectives where the lower-level model is trained similarly to\nmultimodal grounding models, and the higher-level model is trained with our\nproposed novel layout-aware objectives. We evaluate the proposed model on two\nlayout-aware tasks -- text block filling and image suggestion and show the\neffectiveness of our proposed hierarchical architecture as well as pretraining\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 23:27:39 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wu", "Te-Lin", ""], ["Li", "Cheng", ""], ["Zhang", "Mingyang", ""], ["Chen", "Tao", ""], ["Hombaiah", "Spurthi Amba", ""], ["Bendersky", "Michael", ""]]}, {"id": "2104.08433", "submitter": "Angana Borah", "authors": "Angana Borah, Manash Pratim Barman, Amit Awekar", "title": "Are Word Embedding Methods Stable and Should We Care About It?", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A representation learning method is considered stable if it consistently\ngenerates similar representation of the given data across multiple runs. Word\nEmbedding Methods (WEMs) are a class of representation learning methods that\ngenerate dense vector representation for each word in the given text data. The\ncentral idea of this paper is to explore the stability measurement of WEMs\nusing intrinsic evaluation based on word similarity. We experiment with three\npopular WEMs: Word2Vec, GloVe, and fastText. For stability measurement, we\ninvestigate the effect of five parameters involved in training these models. We\nperform experiments using four real-world datasets from different domains:\nWikipedia, News, Song lyrics, and European parliament proceedings. We also\nobserve the effect of WEM stability on three downstream tasks: Clustering, POS\ntagging, and Fairness evaluation. Our experiments indicate that amongst the\nthree WEMs, fastText is the most stable, followed by GloVe and Word2Vec.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 03:29:22 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Borah", "Angana", ""], ["Barman", "Manash Pratim", ""], ["Awekar", "Amit", ""]]}, {"id": "2104.08443", "submitter": "Yongqi Li", "authors": "Yongqi Li, Wenjie Li, Liqiang Nie", "title": "A Graph-guided Multi-round Retrieval Method for Conversational\n  Open-domain Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, conversational agents have provided a natural and convenient\naccess to useful information in people's daily life, along with a broad and new\nresearch topic, conversational question answering (QA). Among the popular\nconversational QA tasks, conversational open-domain QA, which requires to\nretrieve relevant passages from the Web to extract exact answers, is more\npractical but less studied. The main challenge is how to well capture and fully\nexplore the historical context in conversation to facilitate effective\nlarge-scale retrieval. The current work mainly utilizes history questions to\nrefine the current question or to enhance its representation, yet the relations\nbetween history answers and the current answer in a conversation, which is also\ncritical to the task, are totally neglected. To address this problem, we\npropose a novel graph-guided retrieval method to model the relations among\nanswers across conversation turns. In particular, it utilizes a passage graph\nderived from the hyperlink-connected passages that contains history answers and\npotential current answers, to retrieve more relevant passages for subsequent\nanswer extraction. Moreover, in order to collect more complementary information\nin the historical context, we also propose to incorporate the multi-round\nrelevance feedback technique to explore the impact of the retrieval context on\ncurrent question understanding. Experimental results on the public dataset\nverify the effectiveness of our proposed method. Notably, the F1 score is\nimproved by 5% and 11% with predicted history answers and true history answers,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 04:39:41 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Li", "Yongqi", ""], ["Li", "Wenjie", ""], ["Nie", "Liqiang", ""]]}, {"id": "2104.08490", "submitter": "Pan Li", "authors": "Pan Li and Alexander Tuzhilin", "title": "Dual Metric Learning for Effective and Efficient Cross-Domain\n  Recommendations", "comments": "Accepted to IEEE TKDE. arXiv admin note: text overlap with\n  arXiv:1910.05189", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross domain recommender systems have been increasingly valuable for helping\nconsumers identify useful items in different applications. However, existing\ncross-domain models typically require large number of overlap users, which can\nbe difficult to obtain in some applications. In addition, they did not consider\nthe duality structure of cross-domain recommendation tasks, thus failing to\ntake into account bidirectional latent relations between users and items and\nachieve optimal recommendation performance. To address these issues, in this\npaper we propose a novel cross-domain recommendation model based on dual\nlearning that transfers information between two related domains in an iterative\nmanner until the learning process stabilizes. We develop a novel latent\northogonal mapping to extract user preferences over multiple domains while\npreserving relations between users across different latent spaces. Furthermore,\nwe combine the dual learning method with the metric learning approach, which\nallows us to significantly reduce the required common user overlap across the\ntwo domains and leads to even better cross-domain recommendation performance.\nWe test the proposed model on two large-scale industrial datasets and six\ndomain pairs, demonstrating that it consistently and significantly outperforms\nall the state-of-the-art baselines. We also show that the proposed model works\nwell with very few overlap users to obtain satisfying recommendation\nperformance comparable to the state-of-the-art baselines that use many overlap\nusers.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 09:18:59 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 01:12:23 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "Pan", ""], ["Tuzhilin", "Alexander", ""]]}, {"id": "2104.08523", "submitter": "Xiaoyang Chen", "authors": "Xiaoyang Chen, Kai Hui, Ben He, Xianpei Han, Le Sun, Zheng Ye", "title": "Co-BERT: A Context-Aware BERT Retrieval Model Incorporating Local and\n  Query-specific Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  BERT-based text ranking models have dramatically advanced the\nstate-of-the-art in ad-hoc retrieval, wherein most models tend to consider\nindividual query-document pairs independently. In the mean time, the importance\nand usefulness to consider the cross-documents interactions and the\nquery-specific characteristics in a ranking model have been repeatedly\nconfirmed, mostly in the context of learning to rank. The BERT-based ranking\nmodel, however, has not been able to fully incorporate these two types of\nranking context, thereby ignoring the inter-document relationships from the\nranking and the differences among queries. To mitigate this gap, in this work,\nan end-to-end transformer-based ranking model, named Co-BERT, has been proposed\nto exploit several BERT architectures to calibrate the query-document\nrepresentations using pseudo relevance feedback before modeling the relevance\nof a group of documents jointly. Extensive experiments on two standard test\ncollections confirm the effectiveness of the proposed model in improving the\nperformance of text re-ranking over strong fine-tuned BERT-Base baselines. We\nplan to make our implementation open source to enable further comparisons.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 12:13:15 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Xiaoyang", ""], ["Hui", "Kai", ""], ["He", "Ben", ""], ["Han", "Xianpei", ""], ["Sun", "Le", ""], ["Ye", "Zheng", ""]]}, {"id": "2104.08542", "submitter": "Huifeng Guo", "authors": "Huifeng Guo, Wei Guo, Yong Gao, Ruiming Tang, Xiuqiang He, Wenzhi Liu", "title": "ScaleFreeCTR: MixCache-based Distributed Training System for CTR Models\n  with Huge Embedding Table", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the superior feature representation ability of deep learning,\nvarious deep Click-Through Rate (CTR) models are deployed in the commercial\nsystems by industrial companies. To achieve better performance, it is necessary\nto train the deep CTR models on huge volume of training data efficiently, which\nmakes speeding up the training process an essential problem. Different from the\nmodels with dense training data, the training data for CTR models is usually\nhigh-dimensional and sparse. To transform the high-dimensional sparse input\ninto low-dimensional dense real-value vectors, almost all deep CTR models adopt\nthe embedding layer, which easily reaches hundreds of GB or even TB. Since a\nsingle GPU cannot afford to accommodate all the embedding parameters, when\nperforming distributed training, it is not reasonable to conduct the\ndata-parallelism only. Therefore, existing distributed training platforms for\nrecommendation adopt model-parallelism. Specifically, they use CPU (Host)\nmemory of servers to maintain and update the embedding parameters and utilize\nGPU worker to conduct forward and backward computations. Unfortunately, these\nplatforms suffer from two bottlenecks: (1) the latency of pull \\& push\noperations between Host and GPU; (2) parameters update and synchronization in\nthe CPU servers. To address such bottlenecks, in this paper, we propose the\nScaleFreeCTR: a MixCache-based distributed training system for CTR models.\nSpecifically, in SFCTR, we also store huge embedding table in CPU but utilize\nGPU instead of CPU to conduct embedding synchronization efficiently. To reduce\nthe latency of data transfer between both GPU-Host and GPU-GPU, the MixCache\nmechanism and Virtual Sparse Id operation are proposed. Comprehensive\nexperiments and ablation studies are conducted to demonstrate the effectiveness\nand efficiency of SFCTR.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 13:36:19 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 14:11:46 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Guo", "Huifeng", ""], ["Guo", "Wei", ""], ["Gao", "Yong", ""], ["Tang", "Ruiming", ""], ["He", "Xiuqiang", ""], ["Liu", "Wenzhi", ""]]}, {"id": "2104.08558", "submitter": "Olabanji Shonibare", "authors": "Olabanji Shonibare", "title": "ASBERT: Siamese and Triplet network embedding for open question\n  answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Answer selection (AS) is an essential subtask in the field of natural\nlanguage processing with an objective to identify the most likely answer to a\ngiven question from a corpus containing candidate answer sentences. A common\napproach to address the AS problem is to generate an embedding for each\ncandidate sentence and query. Then, select the sentence whose vector\nrepresentation is closest to the query's. A key drawback is the low quality of\nthe embeddings, hitherto, based on its performance on AS benchmark datasets. In\nthis work, we present ASBERT, a framework built on the BERT architecture that\nemploys Siamese and Triplet neural networks to learn an encoding function that\nmaps a text to a fixed-size vector in an embedded space. The notion of distance\nbetween two points in this space connotes similarity in meaning between two\ntexts. Experimental results on the WikiQA and TrecQA datasets demonstrate that\nour proposed approach outperforms many state-of-the-art baseline methods.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 14:46:45 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Shonibare", "Olabanji", ""]]}, {"id": "2104.08653", "submitter": "Baban Gain", "authors": "Baban Gain, Dibyanayan Bandyopadhyay, Tanik Saikh, Asif Ekbal", "title": "IITP@COLIEE 2019: Legal Information Retrieval using BM25 and BERT", "comments": "5 pages. IITP@ COLIEE 2019: legal information retrieval using BM25\n  and BERT. Proceedings of the 6th Competition on Legal Information\n  Extraction/Entailment. COLIEE", "journal-ref": null, "doi": "10.13140/RG.2.2.24136.44804", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural Language Processing (NLP) and Information Retrieval (IR) in the\njudicial domain is an essential task. With the advent of availability\ndomain-specific data in electronic form and aid of different Artificial\nintelligence (AI) technologies, automated language processing becomes more\ncomfortable, and hence it becomes feasible for researchers and developers to\nprovide various automated tools to the legal community to reduce human burden.\nThe Competition on Legal Information Extraction/Entailment (COLIEE-2019) run in\nassociation with the International Conference on Artificial Intelligence and\nLaw (ICAIL)-2019 has come up with few challenging tasks. The shared defined\nfour sub-tasks (i.e. Task1, Task2, Task3 and Task4), which will be able to\nprovide few automated systems to the judicial system. The paper presents our\nworking note on the experiments carried out as a part of our participation in\nall the sub-tasks defined in this shared task. We make use of different\nInformation Retrieval(IR) and deep learning based approaches to tackle these\nproblems. We obtain encouraging results in all these four sub-tasks.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 22:28:15 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 19:07:25 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 08:39:42 GMT"}, {"version": "v4", "created": "Thu, 24 Jun 2021 14:40:18 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Gain", "Baban", ""], ["Bandyopadhyay", "Dibyanayan", ""], ["Saikh", "Tanik", ""], ["Ekbal", "Asif", ""]]}, {"id": "2104.08663", "submitter": "Nandan Thakur", "authors": "Nandan Thakur, Nils Reimers, Andreas R\\\"uckl\\'e, Abhishek Srivastava,\n  Iryna Gurevych", "title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information\n  Retrieval Models", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Neural IR models have often been studied in homogeneous and narrow settings,\nwhich has considerably limited insights into their generalization capabilities.\nTo address this, and to allow researchers to more broadly establish the\neffectiveness of their models, we introduce BEIR (Benchmarking IR), a\nheterogeneous benchmark for information retrieval. We leverage a careful\nselection of 17 datasets for evaluation spanning diverse retrieval tasks\nincluding open-domain datasets as well as narrow expert domains. We study the\neffectiveness of nine state-of-the-art retrieval models in a zero-shot\nevaluation setup on BEIR, finding that performing well consistently across all\ndatasets is challenging. Our results show BM25 is a robust baseline and\nReranking-based models overall achieve the best zero-shot performances,\nhowever, at high computational costs. In contrast, Dense-retrieval models are\ncomputationally more efficient but often underperform other approaches,\nhighlighting the considerable room for improvement in their generalization\ncapabilities. In this work, we extensively analyze different retrieval models\nand provide several suggestions that we believe may be useful for future work.\nBEIR datasets and code are available at https://github.com/UKPLab/beir.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 23:29:55 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 13:59:17 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Thakur", "Nandan", ""], ["Reimers", "Nils", ""], ["R\u00fcckl\u00e9", "Andreas", ""], ["Srivastava", "Abhishek", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2104.08707", "submitter": "Sheng-Chieh Lin", "authors": "Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin", "title": "Contextualized Query Embeddings for Conversational Search", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search (CS) plays a vital role in information retrieval. The\ncurrent state of the art approaches the task using a multi-stage pipeline\ncomprising conversational query reformulation and information seeking modules.\nDespite of its effectiveness, such a pipeline often comprises multiple neural\nmodels and thus requires long inference times. In addition, independently\noptimizing the effectiveness of each module does not consider the relation\nbetween modules in the pipeline. Thus, in this paper, we propose a single-stage\ndesign, which supports end-to-end training and low-latency inference. To aid in\nthis goal, we create a synthetic dataset for CS to overcome the lack of\ntraining data and explore different training strategies using this dataset.\nExperiments demonstrate that our model yields competitive retrieval\neffectiveness against state-of-the-art multi-stage approaches but with lower\nlatency. Furthermore, we show that improved retrieval effectiveness benefits\nthe downstream task of conversational question answering.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 04:29:01 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lin", "Sheng-Chieh", ""], ["Yang", "Jheng-Hong", ""], ["Lin", "Jimmy", ""]]}, {"id": "2104.08716", "submitter": "Yu Zhang", "authors": "Huangbin Zhang, Chong Zhao, Yu Zhang, Danlei Wang, Haichao Yang", "title": "Deep Latent Emotion Network for Multi-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feed recommendation models are widely adopted by numerous feed platforms to\nencourage users to explore the contents they are interested in. However, most\nof the current research simply focus on targeting user's preference and lack\nin-depth study of avoiding objectionable contents to be frequently recommended,\nwhich is a common reason that let user detest. To address this issue, we\npropose a Deep Latent Emotion Network (DLEN) model to extract latent\nprobability of a user preferring a feed by modeling multiple targets with\nsemi-supervised learning. With this method, the conflicts of different targets\nare successfully reduced in the training phase, which improves the training\naccuracy of each target effectively. Besides, by adding this latent state of\nuser emotion to multi-target fusion, the model is capable of decreasing the\nprobability to recommend objectionable contents to improve user retention and\nstay time during online testing phase. DLEN is deployed on a real-world\nmulti-task feed recommendation scenario of Tencent QQ-Small-World with a\ndataset containing over a billion samples, and it exhibits a significant\nperformance advantage over the SOTA MTL model in offline evaluation, together\nwith a considerable increase by 3.02% in view-count and 2.63% in user stay-time\nin production. Complementary offline experiments of DLEN model on a public\ndataset also repeat improvements in various scenarios. At present, DLEN model\nhas been successfully deployed in Tencent's feed recommendation system.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 04:55:13 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhang", "Huangbin", ""], ["Zhao", "Chong", ""], ["Zhang", "Yu", ""], ["Wang", "Danlei", ""], ["Yang", "Haichao", ""]]}, {"id": "2104.08737", "submitter": "Akhil Arora", "authors": "Akhil Arora, Alberto Garcia-Duran, Robert West", "title": "Low-rank Subspaces for Unsupervised Entity Linking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity linking is an important problem with many applications. Most previous\nsolutions were designed for settings where annotated training data is\navailable, which is, however, not the case in numerous domains. We propose a\nlight-weight and scalable entity linking method, Eigenthemes, that relies\nsolely on the availability of entity names and a referent knowledge base.\nEigenthemes exploits the fact that the entities that are truly mentioned in a\ndocument (the \"gold entities\") tend to form a semantically dense subset of the\nset of all candidate entities in the document. Geometrically speaking, when\nrepresenting entities as vectors via some given embedding, the gold entities\ntend to lie in a low-rank subspace of the full embedding space. Eigenthemes\nidentifies this subspace using the singular value decomposition and scores\ncandidate entities according to their proximity to the subspace. On the\nempirical front, we introduce multiple strong baselines that compare favorably\nto the existing state of the art. Extensive experiments on benchmark datasets\nfrom a variety of real-world domains showcase the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 06:24:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Arora", "Akhil", ""], ["Garcia-Duran", "Alberto", ""], ["West", "Robert", ""]]}, {"id": "2104.08755", "submitter": "Zhaohao Zeng", "authors": "Zhaohao Zeng and Tetsuya Sakai", "title": "DCH-2: A Parallel Customer-Helpdesk Dialogue Corpus with Distributions\n  of Annotators' Labels", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We introduce a data set called DCH-2, which contains 4,390 real\ncustomer-helpdesk dialogues in Chinese and their English translations. DCH-2\nalso contains dialogue-level annotations and turn-level annotations obtained\nindependently from either 19 or 20 annotators. The data set was built through\nour effort as organisers of the NTCIR-14 Short Text Conversation and NTCIR-15\nDialogue Evaluation tasks, to help researchers understand what constitutes an\neffective customer-helpdesk dialogue, and thereby build efficient and helpful\nhelpdesk systems that are available to customers at all times. In addition,\nDCH-2 may be utilised for other purposes, for example, as a repository for\nretrieval-based dialogue systems, or as a parallel corpus for machine\ntranslation in the helpdesk domain.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 07:35:15 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 15:32:47 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zeng", "Zhaohao", ""], ["Sakai", "Tetsuya", ""]]}, {"id": "2104.08809", "submitter": "Arie Cattan", "authors": "Arie Cattan, Sophie Johnson, Daniel Weld, Ido Dagan, Iz Beltagy, Doug\n  Downey, Tom Hope", "title": "SciCo: Hierarchical Cross-Document Coreference for Scientific Concepts", "comments": "Data and code available at https://github.com/ariecattan/SciCo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Determining coreference of concept mentions across multiple documents is\nfundamental for natural language understanding. Work on cross-document\ncoreference resolution (CDCR) typically considers mentions of events in the\nnews, which do not often involve abstract technical concepts that are prevalent\nin science and technology. These complex concepts take diverse or ambiguous\nforms and have many hierarchical levels of granularity (e.g., tasks and\nsubtasks), posing challenges for CDCR. We present a new task of hierarchical\nCDCR for concepts in scientific papers, with the goal of jointly inferring\ncoreference clusters and hierarchy between them. We create SciCo, an\nexpert-annotated dataset for this task, which is 3X larger than the prominent\nECB+ resource. We find that tackling both coreference and hierarchy at once\noutperforms disjoint models, which we hope will spur development of joint\nmodels for SciCo.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 10:42:20 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cattan", "Arie", ""], ["Johnson", "Sophie", ""], ["Weld", "Daniel", ""], ["Dagan", "Ido", ""], ["Beltagy", "Iz", ""], ["Downey", "Doug", ""], ["Hope", "Tom", ""]]}, {"id": "2104.08912", "submitter": "Amir H. Jadidinejad", "authors": "Amir H. Jadidinejad, Craig Macdonald, Iadh Ounis", "title": "The Simpson's Paradox in the Offline Evaluation of Recommendation\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommendation systems are often evaluated based on user's interactions that\nwere collected from an existing, already deployed recommendation system. In\nthis situation, users only provide feedback on the exposed items and they may\nnot leave feedback on other items since they have not been exposed to them by\nthe deployed system. As a result, the collected feedback dataset that is used\nto evaluate a new model is influenced by the deployed system, as a form of\nclosed loop feedback. In this paper, we show that the typical offline\nevaluation of recommender systems suffers from the so-called Simpson's paradox.\nSimpson's paradox is the name given to a phenomenon observed when a significant\ntrend appears in several different sub-populations of observational data but\ndisappears or is even reversed when these sub-populations are combined\ntogether. Our in-depth experiments based on stratified sampling reveal that a\nvery small minority of items that are frequently exposed by the deployed system\nplays a confounding factor in the offline evaluation of recommendation systems.\nIn addition, we propose a novel evaluation methodology that takes into account\nthe confounder, i.e the deployed system's characteristics. Using the relative\ncomparison of many recommendation models as in the typical offline evaluation\nof recommender systems, and based on the Kendall rank correlation coefficient,\nwe show that our proposed evaluation methodology exhibits statistically\nsignificant improvements of 14% and 40% on the examined open loop datasets\n(Yahoo! and Coat), respectively, in reflecting the true ranking of systems with\nan open loop (randomised) evaluation in comparison to the standard evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 16:58:24 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Jadidinejad", "Amir H.", ""], ["Macdonald", "Craig", ""], ["Ounis", "Iadh", ""]]}, {"id": "2104.08926", "submitter": "Kai Hui", "authors": "Kai Hui and Klaus Berberich", "title": "Transitivity, Time Consumption, and Quality of Preference Judgments in\n  Crowdsourcing", "comments": "Appeared in ECIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Preference judgments have been demonstrated as a better alternative to graded\njudgments to assess the relevance of documents relative to queries. Existing\nwork has verified transitivity among preference judgments when collected from\ntrained judges, which reduced the number of judgments dramatically. Moreover,\nstrict preference judgments and weak preference judgments, where the latter\nadditionally allow judges to state that two documents are equally relevant for\na given query, are both widely used in literature. However, whether\ntransitivity still holds when collected from crowdsourcing, i.e., whether the\ntwo kinds of preference judgments behave similarly remains unclear. In this\nwork, we collect judgments from multiple judges using a crowdsourcing platform\nand aggregate them to compare the two kinds of preference judgments in terms of\ntransitivity, time consumption, and quality. That is, we look into whether\naggregated judgments are transitive, how long it takes judges to make them, and\nwhether judges agree with each other and with judgments from TREC. Our key\nfindings are that only strict preference judgments are transitive. Meanwhile,\nweak preference judgments behave differently in terms of transitivity, time\nconsumption, as well as of quality of judgment.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 18:14:33 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hui", "Kai", ""], ["Berberich", "Klaus", ""]]}, {"id": "2104.08942", "submitter": "Neel Kanwal", "authors": "Neel Kanwal, Giuseppe Rizzo", "title": "Attention-based Clinical Note Summarization", "comments": "9 Pages, 6 Figure, 2 Tables, Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trend of deploying digital systems in numerous industries has induced a\nhike in recording digital information. The health sector has observed a large\nadoption of digital devices and systems generating large volumes of personal\nmedical health records. Electronic health records contain valuable information\nfor retrospective and prospective analysis that is often not entirely exploited\nbecause of the dense information storage. The crude purpose of condensing\nhealth records is to select the information that holds most characteristics of\nthe original documents based on reported disease. These summaries may boost\ndiagnosis and extend a doctor's interaction time with the patient during a high\nworkload situation like the COVID-19 pandemic. In this paper, we propose a\nmulti-head attention-based mechanism to perform extractive summarization of\nmeaningful phrases in clinical notes. This method finds major sentences for a\nsummary by correlating tokens, segments and positional embeddings. The model\noutputs attention scores that are statistically transformed to extract key\nphrases and can be used for a projection on the heat-mapping tool for visual\nand human use.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 19:40:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kanwal", "Neel", ""], ["Rizzo", "Giuseppe", ""]]}, {"id": "2104.08943", "submitter": "Thuy Vu", "authors": "Vivek Krishnamurthy, Thuy Vu, Alessandro Moschitti", "title": "Reference-based Weak Supervision for Answer Sentence Selection using Web\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer sentence selection (AS2) modeling requires annotated data, i.e.,\nhand-labeled question-answer pairs. We present a strategy to collect weakly\nsupervised answers for a question based on its reference to improve AS2\nmodeling. Specifically, we introduce Reference-based Weak Supervision (RWS), a\nfully automatic large-scale data pipeline that harvests high-quality\nweakly-supervised answers from abundant Web data requiring only a\nquestion-reference pair as input. We study the efficacy and robustness of RWS\nin the setting of TANDA, a recent state-of-the-art fine-tuning approach\nspecialized for AS2. Our experiments indicate that the produced data\nconsistently bolsters TANDA. We achieve the state of the art in terms of P@1,\n90.1%, and MAP, 92.9%, on WikiQA.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 19:41:17 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Krishnamurthy", "Vivek", ""], ["Vu", "Thuy", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "2104.08976", "submitter": "Joel Mackenzie", "authors": "Joel Mackenzie and Matthias Petri and Alistair Moffat", "title": "Anytime Ranking on Document-Ordered Indexes", "comments": "Accepted to ACM TOIS, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverted indexes continue to be a mainstay of text search engines, allowing\nefficient querying of large document collections. While there are a number of\npossible organizations, document-ordered indexes are the most common, since\nthey are amenable to various query types, support index updates, and allow for\nefficient dynamic pruning operations. One disadvantage with document-ordered\nindexes is that high-scoring documents can be distributed across the document\nidentifier space, meaning that index traversal algorithms that terminate early\nmight put search effectiveness at risk. The alternative is impact-ordered\nindexes, which primarily support top-k disjunctions, but also allow for anytime\nquery processing, where the search can be terminated at any time, with search\nquality improving as processing latency increases. Anytime query processing can\nbe used to effectively reduce high-percentile tail latency which is essential\nfor operational scenarios in which a service level agreement (SLA) imposes\nresponse time requirements. In this work, we show how document-ordered indexes\ncan be organized such that they can be queried in an anytime fashion, enabling\nstrict latency control with effective early termination. Our experiments show\nthat processing document-ordered topical segments selected by a simple score\nestimator outperforms existing anytime algorithms, and allows query runtimes to\nbe accurately limited in order to comply with SLA requirements.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 23:17:07 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 01:27:52 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Mackenzie", "Joel", ""], ["Petri", "Matthias", ""], ["Moffat", "Alistair", ""]]}, {"id": "2104.09036", "submitter": "Yanqiao Zhu", "authors": "Jinghao Zhang, Yanqiao Zhu, Qiang Liu, Shu Wu, Shuhui Wang, Liang Wang", "title": "Mining Latent Structures for Multimedia Recommendation", "comments": "Accepted to ACM Multimedia 2021. Authors' version", "journal-ref": null, "doi": "10.1145/3474085.3475259", "report-no": null, "categories": "cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia content is of predominance in the modern Web era. Investigating\nhow users interact with multimodal items is a continuing concern within the\nrapid development of recommender systems. The majority of previous work focuses\non modeling user-item interactions with multimodal features included as side\ninformation. However, this scheme is not well-designed for multimedia\nrecommendation. Specifically, only collaborative item-item relationships are\nimplicitly modeled through high-order item-user-item relations. Considering\nthat items are associated with rich contents in multiple modalities, we argue\nthat the latent semantic item-item structures underlying these multimodal\ncontents could be beneficial for learning better item representations and\nfurther boosting recommendation. To this end, we propose a LATent sTructure\nmining method for multImodal reCommEndation, which we term LATTICE for brevity.\nTo be specific, in the proposed LATTICE model, we devise a novel modality-aware\nstructure learning layer, which learns item-item structures for each modality\nand aggregates multiple modalities to obtain latent item graphs. Based on the\nlearned latent graphs, we perform graph convolutions to explicitly inject\nhigh-order item affinities into item representations. These enriched item\nrepresentations can then be plugged into existing collaborative filtering\nmethods to make more accurate recommendations. Extensive experiments on three\nreal-world datasets demonstrate the superiority of our method over\nstate-of-the-art multimedia recommendation methods and validate the efficacy of\nmining latent item-item relationships from multimodal features.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 03:50:24 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 02:27:33 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Zhang", "Jinghao", ""], ["Zhu", "Yanqiao", ""], ["Liu", "Qiang", ""], ["Wu", "Shu", ""], ["Wang", "Shuhui", ""], ["Wang", "Liang", ""]]}, {"id": "2104.09393", "submitter": "Bhaskar Mitra", "authors": "Bhaskar Mitra, Sebastian Hofstatter, Hamed Zamani and Nick Craswell", "title": "Improving Transformer-Kernel Ranking Model Using Conformer and Query\n  Term Independence", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.10434", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Transformer-Kernel (TK) model has demonstrated strong reranking\nperformance on the TREC Deep Learning benchmark -- and can be considered to be\nan efficient (but slightly less effective) alternative to other\nTransformer-based architectures that employ (i) large-scale pretraining (high\ntraining cost), (ii) joint encoding of query and document (high inference\ncost), and (iii) larger number of Transformer layers (both high training and\nhigh inference costs). Since, a variant of the TK model -- called TKL -- has\nbeen developed that incorporates local self-attention to efficiently process\nlonger input sequences in the context of document ranking. In this work, we\npropose a novel Conformer layer as an alternative approach to scale TK to\nlonger input sequences. Furthermore, we incorporate query term independence and\nexplicit term matching to extend the model to the full retrieval setting. We\nbenchmark our models under the strictly blind evaluation setting of the TREC\n2020 Deep Learning track and find that our proposed architecture changes lead\nto improved retrieval quality over TKL. Our best model also outperforms all\nnon-neural runs (\"trad\") and two-thirds of the pretrained Transformer-based\nruns (\"nnlm\") on NDCG@10.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:32:34 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mitra", "Bhaskar", ""], ["Hofstatter", "Sebastian", ""], ["Zamani", "Hamed", ""], ["Craswell", "Nick", ""]]}, {"id": "2104.09399", "submitter": "Bhaskar Mitra", "authors": "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Ellen M.\n  Voorhees and Ian Soboroff", "title": "TREC Deep Learning Track: Reusable Test Collections in the Large Data\n  Regime", "comments": "arXiv admin note: text overlap with arXiv:2003.07820", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TREC Deep Learning (DL) Track studies ad hoc search in the large data\nregime, meaning that a large set of human-labeled training data is available.\nResults so far indicate that the best models with large data may be deep neural\nnetworks. This paper supports the reuse of the TREC DL test collections in\nthree ways. First we describe the data sets in detail, documenting clearly and\nin one place some details that are otherwise scattered in track guidelines,\noverview papers and in our associated MS MARCO leaderboard pages. We intend\nthis description to make it easy for newcomers to use the TREC DL data. Second,\nbecause there is some risk of iteration and selection bias when reusing a data\nset, we describe the best practices for writing a paper using TREC DL data,\nwithout overfitting. We provide some illustrative analysis. Finally we address\na number of issues around the TREC DL data, including an analysis of\nreusability.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:41:28 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Craswell", "Nick", ""], ["Mitra", "Bhaskar", ""], ["Yilmaz", "Emine", ""], ["Campos", "Daniel", ""], ["Voorhees", "Ellen M.", ""], ["Soboroff", "Ian", ""]]}, {"id": "2104.09423", "submitter": "Federico Bianchi", "authors": "Jacopo Tagliabue and Ciro Greco and Jean-Francis Roy and Bingqing Yu\n  and Patrick John Chia and Federico Bianchi and Giovanni Cassani", "title": "SIGIR 2021 E-Commerce Workshop Data Challenge", "comments": "SIGIR eCOM 2021 Data Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2021 SIGIR workshop on eCommerce is hosting the Coveo Data Challenge for\n\"In-session prediction for purchase intent and recommendations\". The challenge\naddresses the growing need for reliable predictions within the boundaries of a\nshopping session, as customer intentions can be different depending on the\noccasion. The need for efficient procedures for personalization is even clearer\nif we consider the e-commerce landscape more broadly: outside of giant digital\nretailers, the constraints of the problem are stricter, due to smaller user\nbases and the realization that most users are not frequently returning\ncustomers. We release a new session-based dataset including more than 30M\nfine-grained browsing events (product detail, add, purchase), enriched by\nlinguistic behavior (queries made by shoppers, with items clicked and items not\nclicked after the query) and catalog meta-data (images, text, pricing\ninformation). On this dataset, we ask participants to showcase innovative\nsolutions for two open problems: a recommendation task (where a model is shown\nsome events at the start of a session, and it is asked to predict future\nproduct interactions); an intent prediction task, where a model is shown a\nsession containing an add-to-cart event, and it is asked to predict whether the\nitem will be bought before the end of the session.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:16:25 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 13:05:53 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 10:02:24 GMT"}, {"version": "v4", "created": "Fri, 16 Jul 2021 14:19:01 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Tagliabue", "Jacopo", ""], ["Greco", "Ciro", ""], ["Roy", "Jean-Francis", ""], ["Yu", "Bingqing", ""], ["Chia", "Patrick John", ""], ["Bianchi", "Federico", ""], ["Cassani", "Giovanni", ""]]}, {"id": "2104.09428", "submitter": "Jamal Al Qundus", "authors": "Jamal Al Qundus, Silvio Peikert, Adrian Paschke", "title": "AI supported Topic Modeling using KNIME-Workflows", "comments": "7 pages, 7 figures. Qurator2020 - Conference on Digital Curation\n  Technologies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Topic modeling algorithms traditionally model topics as list of weighted\nterms. These topic models can be used effectively to classify texts or to\nsupport text mining tasks such as text summarization or fact extraction. The\ngeneral procedure relies on statistical analysis of term frequencies. The focus\nof this work is on the implementation of the knowledge-based topic modelling\nservices in a KNIME workflow. A brief description and evaluation of the\nDBPedia-based enrichment approach and the comparative evaluation of enriched\ntopic models will be outlined based on our previous work. DBpedia-Spotlight is\nused to identify entities in the input text and information from DBpedia is\nused to extend these entities. We provide a workflow developed in KNIME\nimplementing this approach and perform a result comparison of topic modeling\nsupported by knowledge base information to traditional LDA. This topic modeling\napproach allows semantic interpretation both by algorithms and by humans.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 10:19:58 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Qundus", "Jamal Al", ""], ["Peikert", "Silvio", ""], ["Paschke", "Adrian", ""]]}, {"id": "2104.09439", "submitter": "Rajesh N Rao", "authors": "Rajesh N Rao, Manojit Chakraborty", "title": "Vec2GC -- A Graph Based Clustering Method for Text Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  NLP pipelines with limited or no labeled data, rely on unsupervised methods\nfor document processing. Unsupervised approaches typically depend on clustering\nof terms or documents. In this paper, we introduce a novel clustering\nalgorithm, Vec2GC (Vector to Graph Communities), an end-to-end pipeline to\ncluster terms or documents for any given text corpus. Our method uses community\ndetection on a weighted graph of the terms or documents, created using text\nrepresentation learning. Vec2GC clustering algorithm is a density based\napproach, that supports hierarchical clustering as well.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 12:52:30 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Rao", "Rajesh N", ""], ["Chakraborty", "Manojit", ""]]}, {"id": "2104.09632", "submitter": "Kirk Roberts", "authors": "Kirk Roberts, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman, Kyle\n  Lo, Ian Soboroff, Ellen Voorhees, Lucy Lu Wang, William R Hersh", "title": "Searching for Scientific Evidence in a Pandemic: An Overview of\n  TREC-COVID", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an overview of the TREC-COVID Challenge, an information retrieval\n(IR) shared task to evaluate search on scientific literature related to\nCOVID-19. The goals of TREC-COVID include the construction of a pandemic search\ntest collection and the evaluation of IR methods for COVID-19. The challenge\nwas conducted over five rounds from April to July, 2020, with participation\nfrom 92 unique teams and 556 individual submissions. A total of 50 topics (sets\nof related queries) were used in the evaluation, starting at 30 topics for\nRound 1 and adding 5 new topics per round to target emerging topics at that\nstate of the still-emerging pandemic. This paper provides a comprehensive\noverview of the structure and results of TREC-COVID. Specifically, the paper\nprovides details on the background, task structure, topic structure, corpus,\nparticipation, pooling, assessment, judgments, results, top-performing systems,\nlessons learned, and benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 20:49:45 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Roberts", "Kirk", ""], ["Alam", "Tasmeer", ""], ["Bedrick", "Steven", ""], ["Demner-Fushman", "Dina", ""], ["Lo", "Kyle", ""], ["Soboroff", "Ian", ""], ["Voorhees", "Ellen", ""], ["Wang", "Lucy Lu", ""], ["Hersh", "William R", ""]]}, {"id": "2104.09644", "submitter": "Yanshan Wang", "authors": "Bhavani Singh Agnikula Kshatriya, Nicolas A Nunez, Manuel Gardea-\n  Resendez, Euijung Ryu, Brandon J Coombes, Sunyang Fu, Mark A Frye, Joanna M\n  Biernacka, Yanshan Wang", "title": "Neural Language Models with Distant Supervision to Identify Major\n  Depressive Disorder from Clinical Notes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major depressive disorder (MDD) is a prevalent psychiatric disorder that is\nassociated with significant healthcare burden worldwide. Phenotyping of MDD can\nhelp early diagnosis and consequently may have significant advantages in\npatient management. In prior research MDD phenotypes have been extracted from\nstructured Electronic Health Records (EHR) or using Electroencephalographic\n(EEG) data with traditional machine learning models to predict MDD phenotypes.\nHowever, MDD phenotypic information is also documented in free-text EHR data,\nsuch as clinical notes. While clinical notes may provide more accurate\nphenotyping information, natural language processing (NLP) algorithms must be\ndeveloped to abstract such information. Recent advancements in NLP resulted in\nstate-of-the-art neural language models, such as Bidirectional Encoder\nRepresentations for Transformers (BERT) model, which is a transformer-based\nmodel that can be pre-trained from a corpus of unsupervised text data and then\nfine-tuned on specific tasks. However, such neural language models have been\nunderutilized in clinical NLP tasks due to the lack of large training datasets.\nIn the literature, researchers have utilized the distant supervision paradigm\nto train machine learning models on clinical text classification tasks to\nmitigate the issue of lacking annotated training data. It is still unknown\nwhether the paradigm is effective for neural language models. In this paper, we\npropose to leverage the neural language models in a distant supervision\nparadigm to identify MDD phenotypes from clinical notes. The experimental\nresults indicate that our proposed approach is effective in identifying MDD\nphenotypes and that the Bio- Clinical BERT, a specific BERT model for clinical\ndata, achieved the best performance in comparison with conventional machine\nlearning models.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 21:11:41 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kshatriya", "Bhavani Singh Agnikula", ""], ["Nunez", "Nicolas A", ""], ["Resendez", "Manuel Gardea-", ""], ["Ryu", "Euijung", ""], ["Coombes", "Brandon J", ""], ["Fu", "Sunyang", ""], ["Frye", "Mark A", ""], ["Biernacka", "Joanna M", ""], ["Wang", "Yanshan", ""]]}, {"id": "2104.09653", "submitter": "Alexander Spangher", "authors": "Alexander Spangher, Nanyun Peng, Jonathan May and Emilio Ferrara", "title": "Modeling \"Newsworthiness\" for Lead-Generation Across Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Journalists obtain \"leads\", or story ideas, by reading large corpora of\ngovernment records: court cases, proposed bills, etc. However, only a small\npercentage of such records are interesting documents. We propose a model of\n\"newsworthiness\" aimed at surfacing interesting documents. We train models on\nautomatically labeled corpora -- published newspaper articles -- to predict\nwhether each article was a front-page article (i.e., \\textbf{newsworthy}) or\nnot (i.e., \\textbf{less newsworthy}). We transfer these models to unlabeled\ncorpora -- court cases, bills, city-council meeting minutes -- to rank\ndocuments in these corpora on \"newsworthiness\". A fine-tuned RoBERTa model\nachieves .93 AUC performance on heldout labeled documents, and .88 AUC on\nexpert-validated unlabeled corpora. We provide interpretation and visualization\nfor our models.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 21:48:15 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Spangher", "Alexander", ""], ["Peng", "Nanyun", ""], ["May", "Jonathan", ""], ["Ferrara", "Emilio", ""]]}, {"id": "2104.09677", "submitter": "Thilina Ranbaduge", "authors": "Thilina Ranbaduge, Peter Christen, Rainer Schnell", "title": "Large Scale Record Linkage in the Presence of Missing Data", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage is aimed at the accurate and efficient identification of\nrecords that represent the same entity within or across disparate databases. It\nis a fundamental task in data integration and increasingly required for\naccurate decision making in application domains ranging from health analytics\nto national security. Traditional record linkage techniques calculate string\nsimilarities between quasi-identifying (QID) values, such as the names and\naddresses of people. Errors, variations, and missing QID values can however\nlead to low linkage quality because the similarities between records cannot be\ncalculated accurately. To overcome this challenge, we propose a novel technique\nthat can accurately link records even when QID values contain errors or\nvariations, or are missing. We first generate attribute signatures\n(concatenated QID values) using an Apriori based selection of suitable QID\nattributes, and then relational signatures that encapsulate relationship\ninformation between records. Combined, these signatures can uniquely identify\nindividual records and facilitate fast and high quality linking of very large\ndatabases through accurate similarity calculations between records. We evaluate\nthe linkage quality and scalability of our approach using large real-world\ndatabases, showing that it can achieve high linkage quality even when the\ndatabases being linked contain substantial amounts of missing values and\nerrors.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 22:57:19 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ranbaduge", "Thilina", ""], ["Christen", "Peter", ""], ["Schnell", "Rainer", ""]]}, {"id": "2104.09694", "submitter": "Luca Di Liello", "authors": "Luca Di Liello, Matteo Gabburo, Alessandro Moschitti", "title": "Efficient pre-training objectives for Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Transformer architecture deeply changed the natural language processing,\noutperforming all previous state-of-the-art models. However, well-known\nTransformer models like BERT, RoBERTa, and GPT-2 require a huge compute budget\nto create a high quality contextualised representation. In this paper, we study\nseveral efficient pre-training objectives for Transformers-based models. By\ntesting these objectives on different tasks, we determine which of the ELECTRA\nmodel's new features is the most relevant. We confirm that Transformers\npre-training is improved when the input does not contain masked tokens and that\nthe usage of the whole output to compute the loss reduces training time.\nMoreover, inspired by ELECTRA, we study a model composed of two blocks; a\ndiscriminator and a simple generator based on a statistical model with no\nimpact on the computational performances. Besides, we prove that eliminating\nthe MASK token and considering the whole output during the loss computation are\nessential choices to improve performance. Furthermore, we show that it is\npossible to efficiently train BERT-like models using a discriminative approach\nas in ELECTRA but without a complex generator, which is expensive. Finally, we\nshow that ELECTRA benefits heavily from a state-of-the-art hyper-parameters\nsearch.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 00:09:37 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Di Liello", "Luca", ""], ["Gabburo", "Matteo", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "2104.09774", "submitter": "Alexandros Ioannidis Mr", "authors": "Alexandros Ioannidis", "title": "An Analysis of Indexing and Querying Strategies on a Technologically\n  Assisted Review Task", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a preliminary experimentation study using the CLEF 2017\neHealth Task 2 collection for evaluating the effectiveness of different\nindexing methodologies of documents and query parsing techniques. Furthermore,\nit is an attempt to advance and share the efforts of observing the\ncharacteristics and helpfulness of various methodologies for indexing PubMed\ndocuments and for different topic parsing techniques to produce queries. For\nthis purpose, my research includes experimentation with different document\nindexing methodologies, by utilising existing tools, such as the Lucene4IR\n(L4IR) information retrieval system, the Technology Assisted Reviews for\nEmpirical Medicine tool for parsing topics of the CLEF collection and the TREC\nevaluation tool to appraise system's performance. The results showed that\nincluding a greater number of fields to the PubMed indexer of L4IR is a\ndecisive factor for the retrieval effectiveness of L4IR.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 06:09:28 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ioannidis", "Alexandros", ""]]}, {"id": "2104.09791", "submitter": "Xinyu Ma", "authors": "Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Yingyan Li and Xueqi\n  Cheng", "title": "B-PROP: Bootstrapped Pre-training with Representative Words Prediction\n  for Ad-hoc Retrieval", "comments": "Accepted by SIGIR 2021. arXiv admin note: text overlap with\n  arXiv:2010.10137", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-training and fine-tuning have achieved remarkable success in many\ndownstream natural language processing (NLP) tasks. Recently, pre-training\nmethods tailored for information retrieval (IR) have also been explored, and\nthe latest success is the PROP method which has reached new SOTA on a variety\nof ad-hoc retrieval benchmarks. The basic idea of PROP is to construct the\n\\textit{representative words prediction} (ROP) task for pre-training inspired\nby the query likelihood model. Despite its exciting performance, the\neffectiveness of PROP might be bounded by the classical unigram language model\nadopted in the ROP task construction process. To tackle this problem, we\npropose a bootstrapped pre-training method (namely B-PROP) based on BERT for\nad-hoc retrieval. The key idea is to use the powerful contextual language model\nBERT to replace the classical unigram language model for the ROP task\nconstruction, and re-train BERT itself towards the tailored objective for IR.\nSpecifically, we introduce a novel contrastive method, inspired by the\ndivergence-from-randomness idea, to leverage BERT's self-attention mechanism to\nsample representative words from the document. By further fine-tuning on\ndownstream ad-hoc retrieval tasks, our method achieves significant improvements\nover baselines without pre-training or with other pre-training methods, and\nfurther pushes forward the SOTA on a variety of ad-hoc retrieval tasks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:07:56 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 16:00:17 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 01:59:15 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Ma", "Xinyu", ""], ["Guo", "Jiafeng", ""], ["Zhang", "Ruqing", ""], ["Fan", "Yixing", ""], ["Li", "Yingyan", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2104.10083", "submitter": "Tao Qi", "authors": "Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang", "title": "Personalized News Recommendation with Knowledge-aware Interactive\n  Matching", "comments": "SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most important task in personalized news recommendation is accurate\nmatching between candidate news and user interest. Most of existing news\nrecommendation methods model candidate news from its textual content and user\ninterest from their clicked news in an independent way. However, a news article\nmay cover multiple aspects and entities, and a user usually has different kinds\nof interest. Independent modeling of candidate news and user interest may lead\nto inferior matching between news and users. In this paper, we propose a\nknowledge-aware interactive matching method for news recommendation. Our method\ninteractively models candidate news and user interest to facilitate their\naccurate matching. We design a knowledge-aware news co-encoder to interactively\nlearn representations for both clicked news and candidate news by capturing\ntheir relatedness in both semantic and entities with the help of knowledge\ngraphs. We also design a user-news co-encoder to learn candidate news-aware\nuser interest representation and user-aware candidate news representation for\nbetter interest matching. Experiments on two real-world datasets validate that\nour method can effectively improve the performance of news recommendation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:05:16 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 04:12:58 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 17:13:48 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Qi", "Tao", ""], ["Wu", "Fangzhao", ""], ["Wu", "Chuhan", ""], ["Huang", "Yongfeng", ""]]}, {"id": "2104.10129", "submitter": "Sunil Gandhi", "authors": "Hengxin Fun, Sunil Gandhi, Sujith Ravi", "title": "Efficient Retrieval Optimized Multi-task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there have been significant advances in neural methods for tackling\nknowledge-intensive tasks such as open domain question answering (QA). These\nadvances are fueled by combining large pre-trained language models with\nlearnable retrieval of documents. Majority of these models use separate\nencoders for learning query representation, passage representation for the\nretriever and an additional encoder for the downstream task. Using separate\nencoders for each stage/task occupies a lot of memory and makes it difficult to\nscale to a large number of tasks. In this paper, we propose a novel Retrieval\nOptimized Multi-task (ROM) framework for jointly training self-supervised\ntasks, knowledge retrieval, and extractive question answering. Our ROM approach\npresents a unified and generalizable framework that enables scaling efficiently\nto multiple tasks, varying levels of supervision, and optimization choices such\nas different learning schedules without changing the model architecture. It\nalso provides the flexibility of changing the encoders without changing the\narchitecture of the system. Using our framework, we achieve comparable or\nbetter performance than recent methods on QA, while drastically reducing the\nnumber of parameters.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:16:34 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Fun", "Hengxin", ""], ["Gandhi", "Sunil", ""], ["Ravi", "Sujith", ""]]}, {"id": "2104.10557", "submitter": "Tingtian Li", "authors": "Tingtian Li, Zixun Sun, Haoruo Zhang, Jin Li, Ziming Wu, Hui Zhan,\n  Yipeng Yu, Hengcan Shi", "title": "Deep Music Retrieval for Fine-Grained Videos by Exploiting\n  Cross-Modal-Encoded Voice-Overs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the witness of the rapidly growing popularity of short videos on\ndifferent Internet platforms has intensified the need for a background music\n(BGM) retrieval system. However, existing video-music retrieval methods only\nbased on the visual modality cannot show promising performance regarding videos\nwith fine-grained virtual contents. In this paper, we also investigate the\nwidely added voice-overs in short videos and propose a novel framework to\nretrieve BGM for fine-grained short videos. In our framework, we use the\nself-attention (SA) and the cross-modal attention (CMA) modules to explore the\nintra- and the inter-relationships of different modalities respectively. For\nbalancing the modalities, we dynamically assign different weights to the modal\nfeatures via a fusion gate. For paring the query and the BGM embeddings, we\nintroduce a triplet pseudo-label loss to constrain the semantics of the modal\nembeddings. As there are no existing virtual-content video-BGM retrieval\ndatasets, we build and release two virtual-content video datasets HoK400 and\nCFM400. Experimental results show that our method achieves superior performance\nand outperforms other state-of-the-art methods with large margins.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:29:42 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Li", "Tingtian", ""], ["Sun", "Zixun", ""], ["Zhang", "Haoruo", ""], ["Li", "Jin", ""], ["Wu", "Ziming", ""], ["Zhan", "Hui", ""], ["Yu", "Yipeng", ""], ["Shi", "Hengcan", ""]]}, {"id": "2104.10584", "submitter": "Weinan Zhang", "authors": "Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, Xiuqiang He", "title": "Deep Learning for Click-Through Rate Estimation", "comments": "Paper accepted at IJCAI 2021 (Survey Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Click-through rate (CTR) estimation plays as a core function module in\nvarious personalized online services, including online advertising, recommender\nsystems, and web search etc. From 2015, the success of deep learning started to\nbenefit CTR estimation performance and now deep CTR models have been widely\napplied in many industrial platforms. In this survey, we provide a\ncomprehensive review of deep learning models for CTR estimation tasks. First,\nwe take a review of the transfer from shallow to deep CTR models and explain\nwhy going deep is a necessary trend of development. Second, we concentrate on\nexplicit feature interaction learning modules of deep CTR models. Then, as an\nimportant perspective on large platforms with abundant user histories, deep\nbehavior models are discussed. Moreover, the recently emerged automated methods\nfor deep CTR architecture design are presented. Finally, we summarize the\nsurvey and discuss the future prospects of this field.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 15:25:45 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhang", "Weinan", ""], ["Qin", "Jiarui", ""], ["Guo", "Wei", ""], ["Tang", "Ruiming", ""], ["He", "Xiuqiang", ""]]}, {"id": "2104.10671", "submitter": "Yongfeng Zhang", "authors": "Yunqi Li, Hanxiong Chen, Zuohui Fu, Yingqiang Ge, Yongfeng Zhang", "title": "User-oriented Fairness in Recommendation", "comments": "Accepted to the 30th Web Conference (WWW 2021)", "journal-ref": null, "doi": "10.1145/3442381.3449866", "report-no": null, "categories": "cs.IR cs.AI cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a highly data-driven application, recommender systems could be affected by\ndata bias, resulting in unfair results for different data groups, which could\nbe a reason that affects the system performance. Therefore, it is important to\nidentify and solve the unfairness issues in recommendation scenarios. In this\npaper, we address the unfairness problem in recommender systems from the user\nperspective. We group users into advantaged and disadvantaged groups according\nto their level of activity, and conduct experiments to show that current\nrecommender systems will behave unfairly between two groups of users.\nSpecifically, the advantaged users (active) who only account for a small\nproportion in data enjoy much higher recommendation quality than those\ndisadvantaged users (inactive). Such bias can also affect the overall\nperformance since the disadvantaged users are the majority. To solve this\nproblem, we provide a re-ranking approach to mitigate this unfairness problem\nby adding constraints over evaluation metrics. The experiments we conducted on\nseveral real-world datasets with various recommendation algorithms show that\nour approach can not only improve group fairness of users in recommender\nsystems, but also achieve better overall recommendation performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 17:50:31 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Li", "Yunqi", ""], ["Chen", "Hanxiong", ""], ["Fu", "Zuohui", ""], ["Ge", "Yingqiang", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2104.10748", "submitter": "Laura O. Moraes", "authors": "Laura O. Moraes, Carlos Eduardo Pedreira", "title": "Clustering Introductory Computer Science Exercises Using Topic Modeling\n  Methods", "comments": "13 pages, 11 figures, published in IEEE Transactions on Learning\n  Technologies", "journal-ref": "IEEE Transactions on Learning Technologies, vol. 14, no. 1, pp.\n  42-54, Feb. 2021", "doi": "10.1109/TLT.2021.3056907", "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Manually determining concepts present in a group of questions is a\nchallenging and time-consuming process. However, the process is an essential\nstep while modeling a virtual learning environment since a mapping between\nconcepts and questions using mastery level assessment and recommendation\nengines are required. We investigated unsupervised semantic models (known as\ntopic modeling techniques) to assist computer science teachers in this task and\npropose a method to transform Computer Science 1 teacher-provided code\nsolutions into representative text documents, including the code structure\ninformation. By applying non-negative matrix factorization and latent Dirichlet\nallocation techniques, we extract the underlying relationship between questions\nand validate the results using an external dataset. We consider the\ninterpretability of the learned concepts using 14 university professors' data,\nand the results confirm six semantically coherent clusters using the current\ndataset. Moreover, the six topics comprise the main concepts present in the\ntest dataset, achieving 0.75 in the normalized pointwise mutual information\nmetric. The metric correlates with human ratings, making the proposed method\nuseful and providing semantics for large amounts of unannotated code.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 20:23:53 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Moraes", "Laura O.", ""], ["Pedreira", "Carlos Eduardo", ""]]}, {"id": "2104.10810", "submitter": "Munazza Zaib", "authors": "Munazza Zaib and Quan Z. Sheng and Wei Emma Zhang", "title": "A Short Survey of Pre-trained Language Models for Conversational AI-A\n  NewAge in NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a dialogue system that can communicate naturally with humans is a\nchallenging yet interesting problem of agent-based computing. The rapid growth\nin this area is usually hindered by the long-standing problem of data scarcity\nas these systems are expected to learn syntax, grammar, decision making, and\nreasoning from insufficient amounts of task-specific dataset. The recently\nintroduced pre-trained language models have the potential to address the issue\nof data scarcity and bring considerable advantages by generating contextualized\nword embeddings. These models are considered counterpart of ImageNet in NLP and\nhave demonstrated to capture different facets of language such as hierarchical\nrelations, long-term dependency, and sentiment. In this short survey paper, we\ndiscuss the recent progress made in the field of pre-trained language models.\nWe also deliberate that how the strengths of these language models can be\nleveraged in designing more engaging and more eloquent conversational agents.\nThis paper, therefore, intends to establish whether these pre-trained models\ncan overcome the challenges pertinent to dialogue systems, and how their\narchitecture could be exploited in order to overcome these challenges. Open\nchallenges in the field of dialogue systems have also been deliberated.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 01:00:56 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zaib", "Munazza", ""], ["Sheng", "Quan Z.", ""], ["Zhang", "Wei Emma", ""]]}, {"id": "2104.10880", "submitter": "Shimin Di", "authors": "Shimin Di, Quanming Yao, Yongqi Zhang, Lei Chen", "title": "Efficient Relation-aware Scoring Function Search for Knowledge Graph\n  Embedding", "comments": "ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scoring function, which measures the plausibility of triplets in\nknowledge graphs (KGs), is the key to ensure the excellent performance of KG\nembedding, and its design is also an important problem in the literature.\nAutomated machine learning (AutoML) techniques have recently been introduced\ninto KG to design task-aware scoring functions, which achieve state-of-the-art\nperformance in KG embedding. However, the effectiveness of searched scoring\nfunctions is still not as good as desired. In this paper, observing that\nexisting scoring functions can exhibit distinct performance on different\nsemantic patterns, we are motivated to explore such semantics by searching\nrelation-aware scoring functions. But the relation-aware search requires a much\nlarger search space than the previous one. Hence, we propose to encode the\nspace as a supernet and propose an efficient alternative minimization algorithm\nto search through the supernet in a one-shot manner. Finally, experimental\nresults on benchmark datasets demonstrate that the proposed method can\nefficiently search relation-aware scoring functions, and achieve better\nembedding performance than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 06:05:13 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Di", "Shimin", ""], ["Yao", "Quanming", ""], ["Zhang", "Yongqi", ""], ["Chen", "Lei", ""]]}, {"id": "2104.10907", "submitter": "Runlong Yu", "authors": "Runlong Yu, Yuyang Ye, Qi Liu, Zihan Wang, Chunfeng Yang, Yucheng Hu,\n  Enhong Chen", "title": "XCrossNet: Feature Structure-Oriented Learning for Click-Through Rate\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Click-Through Rate (CTR) prediction is a core task in nowadays commercial\nrecommender systems. Feature crossing, as the mainline of research on CTR\nprediction, has shown a promising way to enhance predictive performance.\n  Even though various models are able to learn feature interactions without\nmanual feature engineering, they rarely attempt to individually learn\nrepresentations for different feature structures.\n  In particular, they mainly focus on the modeling of cross sparse features but\nneglect to specifically represent cross dense features.\n  Motivated by this, we propose a novel Extreme Cross Network, abbreviated\nXCrossNet, which aims at learning dense and sparse feature interactions in an\nexplicit manner.\n  XCrossNet as a feature structure-oriented model leads to a more expressive\nrepresentation and a more precise CTR prediction, which is not only explicit\nand interpretable, but also time-efficient and easy to implement.\n  Experimental studies on Criteo Kaggle dataset show significant improvement of\nXCrossNet over state-of-the-art models on both effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 07:37:36 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Yu", "Runlong", ""], ["Ye", "Yuyang", ""], ["Liu", "Qi", ""], ["Wang", "Zihan", ""], ["Yang", "Chunfeng", ""], ["Hu", "Yucheng", ""], ["Chen", "Enhong", ""]]}, {"id": "2104.10925", "submitter": "Junhan Yang", "authors": "Junhan Yang, Zheng Liu, Bowen Jin, Jianxun Lian, Defu Lian, Akshay\n  Soni, Eun Yong Kang, Yajun Wang, Guangzhong Sun, Xing Xie", "title": "Hybrid Encoder: Towards Efficient and Precise Native AdsRecommendation\n  via Hybrid Transformer Encoding Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformer encoding networks have been proved to be a powerful tool of\nunderstanding natural languages. They are playing a critical role in native ads\nservice, which facilitates the recommendation of appropriate ads based on\nuser's web browsing history. For the sake of efficient recommendation,\nconventional methods would generate user and advertisement embeddings\nindependently with a siamese transformer encoder, such that approximate nearest\nneighbour search (ANN) can be leveraged. Given that the underlying semantic\nabout user and ad can be complicated, such independently generated embeddings\nare prone to information loss, which leads to inferior recommendation quality.\nAlthough another encoding strategy, the cross encoder, can be much more\naccurate, it will lead to huge running cost and become infeasible for realtime\nservices, like native ads recommendation. In this work, we propose hybrid\nencoder, which makes efficient and precise native ads recommendation through\ntwo consecutive steps: retrieval and ranking. In the retrieval step, user and\nad are encoded with a siamese component, which enables relevant candidates to\nbe retrieved via ANN search. In the ranking step, it further represents each ad\nwith disentangled embeddings and each user with ad-related embeddings, which\ncontributes to the fine-grained selection of high-quality ads from the\ncandidate set. Both steps are light-weighted, thanks to the pre-computed and\ncached intermedia results. To optimize the hybrid encoder's performance in this\ntwo-stage workflow, a progressive training pipeline is developed, which builds\nup the model's capability in the retrieval and ranking task step-by-step. The\nhybrid encoder's effectiveness is experimentally verified: with very little\nadditional cost, it outperforms the siamese encoder significantly and achieves\ncomparable recommendation quality as the cross encoder.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 08:42:07 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Yang", "Junhan", ""], ["Liu", "Zheng", ""], ["Jin", "Bowen", ""], ["Lian", "Jianxun", ""], ["Lian", "Defu", ""], ["Soni", "Akshay", ""], ["Kang", "Eun Yong", ""], ["Wang", "Yajun", ""], ["Sun", "Guangzhong", ""], ["Xie", "Xing", ""]]}, {"id": "2104.11026", "submitter": "Yang An", "authors": "Yang An and Liang Zhang and Mao You and Xueqing Tian and Bo Jin and\n  Xiaopeng Wei", "title": "MeSIN: Multilevel Selective and Interactive Network for Medication\n  Recommendation", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending medications for patients using electronic health records (EHRs)\nis a crucial data mining task for an intelligent healthcare system. It can\nassist doctors in making clinical decisions more efficiently. However, the\ninherent complexity of the EHR data renders it as a challenging task: (1)\nMultilevel structures: the EHR data typically contains multilevel structures\nwhich are closely related with the decision-making pathways, e.g., laboratory\nresults lead to disease diagnoses, and then contribute to the prescribed\nmedications; (2) Multiple sequences interactions: multiple sequences in EHR\ndata are usually closely correlated with each other; (3) Abundant noise: lots\nof task-unrelated features or noise information within EHR data generally\nresult in suboptimal performance. To tackle the above challenges, we propose a\nmultilevel selective and interactive network (MeSIN) for medication\nrecommendation. Specifically, MeSIN is designed with three components. First,\nan attentional selective module (ASM) is applied to assign flexible attention\nscores to different medical codes embeddings by their relevance to the\nrecommended medications in every admission. Second, we incorporate a novel\ninteractive long-short term memory network (InLSTM) to reinforce the\ninteractions of multilevel medical sequences in EHR data with the help of the\ncalibrated memory-augmented cell and an enhanced input gate. Finally, we employ\na global selective fusion module (GSFM) to infuse the multi-sourced information\nembeddings into final patient representations for medications recommendation.\nTo validate our method, extensive experiments have been conducted on a\nreal-world clinical dataset. The results demonstrate a consistent superiority\nof our framework over several baselines and testify the effectiveness of our\nproposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 12:59:50 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["An", "Yang", ""], ["Zhang", "Liang", ""], ["You", "Mao", ""], ["Tian", "Xueqing", ""], ["Jin", "Bo", ""], ["Wei", "Xiaopeng", ""]]}, {"id": "2104.11287", "submitter": "Morteza Fayazi", "authors": "Zach Colter, Morteza Fayazi, Zineb Benameur-El, Serafina Kamp, Shuyan\n  Yu, Ronald Dreslinski", "title": "Tablext: A Combined Neural Network And Heuristic Based Table Extractor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant portion of the data available today is found within tables.\nTherefore, it is necessary to use automated table extraction to obtain thorough\nresults when data-mining. Today's popular state-of-the-art methods for table\nextraction struggle to adequately extract tables with machine-readable text and\nstructural data. To make matters worse, many tables do not have\nmachine-readable data, such as tables saved as images, making most extraction\nmethods completely ineffective. In order to address these issues, a novel,\ngeneral format table extractor tool, Tablext, is proposed. This tool uses a\ncombination of computer vision techniques and machine learning methods to\nefficiently and effectively identify and extract data from tables. Tablext\nbegins by using a custom Convolutional Neural Network (CNN) to identify and\nseparate all potential tables. The identification process is optimized by\ncombining the custom CNN with the YOLO object detection network. Then, the\nhigh-level structure of each table is identified with computer vision methods.\nThis high-level, structural meta-data is used by another CNN to identify exact\ncell locations. As a final step, Optical Characters Recognition (OCR) is\nperformed on every individual cell to extract their content without needing\nmachine-readable text. This multi-stage algorithm allows for the neural\nnetworks to focus on completing complex tasks, while letting image processing\nmethods efficiently complete the simpler ones. This leads to the proposed\napproach to be general-purpose enough to handle a large batch of tables\nregardless of their internal encodings or their layout complexity.\nAdditionally, it becomes accurate enough to outperform competing\nstate-of-the-art table extractors on the ICDAR 2013 table dataset.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 19:14:20 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Colter", "Zach", ""], ["Fayazi", "Morteza", ""], ["Benameur-El", "Zineb", ""], ["Kamp", "Serafina", ""], ["Yu", "Shuyan", ""], ["Dreslinski", "Ronald", ""]]}, {"id": "2104.11384", "submitter": "Ali Ahmadvand", "authors": "Ali Ahmadvand, Sayyed M. Zahiri, Simon Hughes, Khalifa Al Jadda, Surya\n  Kallumadi, and Eugene Agichtein", "title": "APRF-Net: Attentive Pseudo-Relevance Feedback Network for Query\n  Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query categorization is an essential part of query intent understanding in\ne-commerce search. A common query categorization task is to select the relevant\nfine-grained product categories in a product taxonomy. For frequent queries,\nrich customer behavior (e.g., click-through data) can be used to infer the\nrelevant product categories. However, for more rare queries, which cover a\nlarge volume of search traffic, relying solely on customer behavior may not\nsuffice due to the lack of this signal. To improve categorization of rare\nqueries, we adapt the Pseudo-Relevance Feedback (PRF) approach to utilize the\nlatent knowledge embedded in semantically or lexically similar product\ndocuments to enrich the representation of the more rare queries. To this end,\nwe propose a novel deep neural model named Attentive Pseudo Relevance Feedback\nNetwork (APRF-Net) to enhance the representation of rare queries for query\ncategorization. To demonstrate the effectiveness of our approach, we collect\nsearch queries from a large commercial search engine, and compare APRF-Net to\nstate-of-the-art deep learning models for text classification. Our results show\nthat the APRF-Net significantly improves query categorization by 5.9% on F1@1\nscore over the baselines, which increases to 8.2% improvement for the rare\n(tail) queries. The findings of this paper can be leveraged for further\nimprovements in search query representation and understanding.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 02:34:08 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 18:47:46 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ahmadvand", "Ali", ""], ["Zahiri", "Sayyed M.", ""], ["Hughes", "Simon", ""], ["Jadda", "Khalifa Al", ""], ["Kallumadi", "Surya", ""], ["Agichtein", "Eugene", ""]]}, {"id": "2104.11486", "submitter": "Ashish M Husain", "authors": "Martin Tran, Ashish M Husain", "title": "Structuring and presenting data for testing of automotive electronics to\n  reduce effort during decision making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automotive engineering is recognized as a combination of software and\nmechanical engineering due to the ever-increasing number of software-based\ncomponents in vehicles. Since vehicles have become more sophisticated than\nbefore to ensure robustness, testing of automotive electronics is performed in\nhigh volume, producing immense test-related data.\n  This study investigates how unstructured and decentralized test-related data\nfrom testing of automotive electronics creates issues in decision making during\nthe testing and analysis process of test artifacts by performing an exploratory\ncase-study at one of the leading automotive companies, Volvo Cars. From the\nfindings of the exploratory study, a prototype was designed to improve the data\nand information structure and presentation for test analysis and diagnostics\nfor automotive electronics. The prototype's results showed that providing\nbetter data and information structure significantly increases the efficiency\nand reduces the workload for testers when conducting test analysis and\ndiagnostics. Testers showed a decrease in task load for tasks related to\ntesting due to better information structure, presentation, correctness and\naccessibility. Hence, the improvements aided the testers to arrive at decisions\nregarding root cause analysis of failed tests efficiently. The findings of this\nstudy can assist automotive companies in systematically investigating and\nimproving the testing process of automotive electronics in regards to managing\nand structuring test-related data.\n  Keywords: Testing, Automotive Electronics, Electronic Control Unit, ECU,\nUnstructured Data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 09:07:24 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Tran", "Martin", ""], ["Husain", "Ashish M", ""]]}, {"id": "2104.11530", "submitter": "Junaid Ahmed Ghauri", "authors": "Junaid Ahmed Ghauri, Sherzod Hakimov, Ralph Ewerth", "title": "Supervised Video Summarization via Multiple Feature Sets with Parallel\n  Attention", "comments": "Accepted in IEEE International Conference on Multimedia and Expo\n  (ICME) 2021 (They have copyright to publish camera ready version of this\n  work)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assignment of importance scores to particular frames or (short) segments\nin a video is crucial for summarization, but also a difficult task. Previous\nwork utilizes only one source of visual features. In this paper, we suggest a\nnovel model architecture that combines three feature sets for visual content\nand motion to predict importance scores. The proposed architecture utilizes an\nattention mechanism before fusing motion features and features representing the\n(static) visual content, i.e., derived from an image classification model.\nComprehensive experimental evaluations are reported for two well-known\ndatasets, SumMe and TVSum. In this context, we identify methodological issues\non how previous work used these benchmark datasets, and present a fair\nevaluation scheme with appropriate data splits that can be used in future work.\nWhen using static and motion features with parallel attention mechanism, we\nimprove state-of-the-art results for SumMe, while being on par with the state\nof the art for the other dataset.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 10:46:35 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 16:07:41 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ghauri", "Junaid Ahmed", ""], ["Hakimov", "Sherzod", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2104.11760", "submitter": "Ali Ahmadvand", "authors": "Ali Ahmadvand, Surya Kallumadi, Faizan Javed, and Eugene Agichtein", "title": "DeepCAT: Deep Category Representation for Query Understanding in\n  E-commerce Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping a search query to a set of relevant categories in the product\ntaxonomy is a significant challenge in e-commerce search for two reasons: 1)\nTraining data exhibits severe class imbalance problem due to biased click\nbehavior, and 2) queries with little customer feedback (e.g., tail queries) are\nnot well-represented in the training set, and cause difficulties for query\nunderstanding. To address these problems, we propose a deep learning model,\nDeepCAT, which learns joint word-category representations to enhance the query\nunderstanding process. We believe learning category interactions helps to\nimprove the performance of category mapping on minority classes, tail and torso\nqueries. DeepCAT contains a novel word-category representation model that\ntrains the category representations based on word-category co-occurrences in\nthe training set. The category representation is then leveraged to introduce a\nnew loss function to estimate the category-category co-occurrences for refining\njoint word-category embeddings. To demonstrate our model's effectiveness on\nminority categories and tail queries, we conduct two sets of experiments. The\nresults show that DeepCAT reaches a 10% improvement on minority classes and a\n7.1% improvement on tail queries over a state-of-the-art label embedding model.\nOur findings suggest a promising direction for improving e-commerce search by\nsemantic modeling of taxonomy hierarchies.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 18:04:44 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 06:12:02 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ahmadvand", "Ali", ""], ["Kallumadi", "Surya", ""], ["Javed", "Faizan", ""], ["Agichtein", "Eugene", ""]]}, {"id": "2104.11783", "submitter": "Yanci Zhang", "authors": "Yanci Zhang, Tianming Du, Yujie Sun, Lawrence Donohue, Rui Dai", "title": "Form 10-Q Itemization", "comments": "6 pages, 3 figures, 3 tables, http://review10q.ddns.net/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR econ.GN q-fin.EC q-fin.GN", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The quarterly financial statement, or Form 10-Q, is one of the most\nfrequently required filings for US public companies to disclose financial and\nother important business information. Due to the massive volume of 10-Q filings\nand the enormous variations in the reporting format, it has been a\nlong-standing challenge to retrieve item-specific information from 10-Q filings\nthat lack machine-readable hierarchy. This paper presents a solution for\nitemizing 10-Q files by complementing a rule-based algorithm with a\nConvolutional Neural Network (CNN) image classifier. This solution demonstrates\na pipeline that can be generalized to a rapid data retrieval solution among a\nlarge volume of textual data using only typographic items. The extracted\ntextual data can be used as unlabeled content-specific data to train\ntransformer models (e.g., BERT) or fit into various field-focus natural\nlanguage processing (NLP) applications.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 18:57:49 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 18:56:20 GMT"}, {"version": "v3", "created": "Mon, 26 Jul 2021 02:56:57 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhang", "Yanci", ""], ["Du", "Tianming", ""], ["Sun", "Yujie", ""], ["Donohue", "Lawrence", ""], ["Dai", "Rui", ""]]}, {"id": "2104.11890", "submitter": "Zuoyu Yan", "authors": "Ke Yuan, Zuoyu Yan, Yibo Li, Liangcai Gao, Zhi Tang", "title": "Automatic Description Construction for Math Expression via Topic\n  Relation Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Math expressions are important parts of scientific and educational documents,\nbut some of them may be challenging for junior scholars or students to\nunderstand. Nevertheless, constructing textual descriptions for math\nexpressions is nontrivial. In this paper, we explore the feasibility to\nautomatically construct descriptions for math expressions. But there are two\nchallenges that need to be addressed: 1) finding relevant documents since a\nmath equation understanding usually requires several topics, but these topics\nare often explained in different documents. 2) the sparsity of the collected\nrelevant documents making it difficult to extract reasonable descriptions.\nDifferent documents mainly focus on different topics which makes model hard to\nextract salient information and organize them to form a description of math\nexpressions. To address these issues, we propose a hybrid model (MathDes) which\ncontains two important modules: Selector and Summarizer. In the Selector, a\nTopic Relation Graph (TRG) is proposed to obtain the relevant documents which\ncontain the comprehensive information of math expressions. TRG is a graph built\naccording to the citations between expressions. In the Summarizer, a\nsummarization model under the Integer Linear Programming (ILP) framework is\nproposed. This module constructs the final description with the help of a\ntimeline that is extracted from TRG. The experimental results demonstrate that\nour methods are promising for this task and outperform the baselines in all\naspects.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 06:15:40 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Yuan", "Ke", ""], ["Yan", "Zuoyu", ""], ["Li", "Yibo", ""], ["Gao", "Liangcai", ""], ["Tang", "Zhi", ""]]}, {"id": "2104.12016", "submitter": "Antonio Mallia", "authors": "Antonio Mallia and Omar Khattab and Nicola Tonellotto and Torsten Suel", "title": "Learning Passage Impacts for Inverted Indexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural information retrieval systems typically use a cascading pipeline, in\nwhich a first-stage model retrieves a candidate set of documents and one or\nmore subsequent stages re-rank this set using contextualized language models\nsuch as BERT. In this paper, we propose DeepImpact, a new document\nterm-weighting scheme suitable for efficient retrieval using a standard\ninverted index. Compared to existing methods, DeepImpact improves impact-score\nmodeling and tackles the vocabulary-mismatch problem. In particular, DeepImpact\nleverages DocT5Query to enrich the document collection and, using a\ncontextualized language model, directly estimates the semantic importance of\ntokens in a document, producing a single-value representation for each token in\neach document. Our experiments show that DeepImpact significantly outperforms\nprior first-stage retrieval approaches by up to 17% on effectiveness metrics\nw.r.t. DocT5Query, and, when deployed in a re-ranking scenario, can reach the\nsame effectiveness of state-of-the-art approaches with up to 5.1x speedup in\nefficiency.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 20:18:53 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Mallia", "Antonio", ""], ["Khattab", "Omar", ""], ["Tonellotto", "Nicola", ""], ["Suel", "Torsten", ""]]}, {"id": "2104.12050", "submitter": "Chih-Yi Chiu", "authors": "Munlika Rattaphun, Wen-Chieh Fang, and Chih-Yi Chiu", "title": "Attention on Global-Local Embedding Spaces in Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present a novel clustering-based collaborative filtering\n(CF) method for recommender systems. Clustering-based CF methods can\neffectively deal with data sparsity and scalability problems. However, most of\nthem are applied to a single embedding space, which might not characterize\ncomplex user-item interactions well. We argue that user-item interactions\nshould be observed from multiple views and characterized in an adaptive way. To\naddress this issue, we leveraged the relation between global space and local\nclusters to construct multiple embedding spaces by learning variant training\ndatasets and loss functions. An attention model was then built to provide a\ndynamic blended representation according to the relative importance of the\nembedding spaces for each user-item pair, forming a flexible measure to\ncharacterize variant user-item interactions. Substantial experiments were\nperformed and evaluated on four popular benchmark datasets. The results show\nthat the proposed method is effective and competitive compared to several CF\nmethods where only one embedding space is considered.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 02:21:10 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Rattaphun", "Munlika", ""], ["Fang", "Wen-Chieh", ""], ["Chiu", "Chih-Yi", ""]]}, {"id": "2104.12080", "submitter": "Chaozhuo Li", "authors": "Chaozhuo Li, Bochen Pang, Yuming Liu, Hao Sun, Zheng Liu, Xing Xie,\n  Tianqi Yang, Yanling Cui, Liangjie Zhang, Qi Zhang", "title": "AdsGNN: Behavior-Graph Augmented Relevance Modeling in Sponsored Search", "comments": "Accepted as full paper in SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sponsored search ads appear next to search results when people look for\nproducts and services on search engines. In recent years, they have become one\nof the most lucrative channels for marketing. As the fundamental basis of\nsearch ads, relevance modeling has attracted increasing attention due to the\nsignificant research challenges and tremendous practical value. Most existing\napproaches solely rely on the semantic information in the input query-ad pair,\nwhile the pure semantic information in the short ads data is not sufficient to\nfully identify user's search intents. Our motivation lies in incorporating the\ntremendous amount of unsupervised user behavior data from the historical search\nlogs as the complementary graph to facilitate relevance modeling. In this\npaper, we extensively investigate how to naturally fuse the semantic textual\ninformation with the user behavior graph, and further propose three novel\nAdsGNN models to aggregate topological neighborhood from the perspectives of\nnodes, edges and tokens. Furthermore, two critical but rarely investigated\nproblems, domain-specific pre-training and long-tail ads matching, are studied\nthoroughly. Empirically, we evaluate the AdsGNN models over the large industry\ndataset, and the experimental results of online/offline tests consistently\ndemonstrate the superiority of our proposal.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 06:55:29 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Li", "Chaozhuo", ""], ["Pang", "Bochen", ""], ["Liu", "Yuming", ""], ["Sun", "Hao", ""], ["Liu", "Zheng", ""], ["Xie", "Xing", ""], ["Yang", "Tianqi", ""], ["Cui", "Yanling", ""], ["Zhang", "Liangjie", ""], ["Zhang", "Qi", ""]]}, {"id": "2104.12269", "submitter": "Diwanshu Shekhar", "authors": "Diwanshu Shekhar, Pooran S. Negi, Mohammad Mahoor", "title": "A Bi-Encoder LSTM Model For Learning Unstructured Dialogs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Creating a data-driven model that is trained on a large dataset of\nunstructured dialogs is a crucial step in developing Retrieval-based Chatbot\nsystems. This paper presents a Long Short Term Memory (LSTM) based architecture\nthat learns unstructured multi-turn dialogs and provides results on the task of\nselecting the best response from a collection of given responses. Ubuntu Dialog\nCorpus Version 2 was used as the corpus for training. We show that our model\nachieves 0.8%, 1.0% and 0.3% higher accuracy for Recall@1, Recall@2 and\nRecall@5 respectively than the benchmark model. We also show results on\nexperiments performed by using several similarity functions, model\nhyper-parameters and word embeddings on the proposed architecture\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 21:37:35 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Shekhar", "Diwanshu", ""], ["Negi", "Pooran S.", ""], ["Mahoor", "Mohammad", ""]]}, {"id": "2104.12302", "submitter": "Yunjiang Jiang", "authors": "Yunjiang Jiang, Yue Shang, Rui Li, Wen-Yun Yang, Guoyu Tang, Chaoyi\n  Ma, Yun Xiao and Eric Zhao", "title": "A unified Neural Network Approach to E-CommerceRelevance Learning", "comments": "6 pages", "journal-ref": "DLP-KDD 2019", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Result relevance scoring is critical to e-commerce search user experience.\nTraditional information retrieval methods focus on keyword matching and\nhand-crafted or counting-based numeric features, with limited understanding of\nitem semantic relevance. We describe a highly-scalable feed-forward neural\nmodel to provide relevance score for (query, item) pairs, using only user query\nand item title as features, and both user click feedback as well as limited\nhuman ratings as labels. Several general enhancements were applied to further\noptimize eval/test metrics, including Siamese pairwise architecture, random\nbatch negative co-training, and point-wise fine-tuning. We found significant\nimprovement over GBDT baseline as well as several off-the-shelf deep-learning\nbaselines on an independently constructed ratings dataset. The GBDT model\nrelies on 10 times more features. We also present metrics for select subset\ncombinations of techniques mentioned above.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 01:26:45 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Jiang", "Yunjiang", ""], ["Shang", "Yue", ""], ["Li", "Rui", ""], ["Yang", "Wen-Yun", ""], ["Tang", "Guoyu", ""], ["Ma", "Chaoyi", ""], ["Xiao", "Yun", ""], ["Zhao", "Eric", ""]]}, {"id": "2104.12333", "submitter": "Tao Ni", "authors": "Tao Ni, Qing Wang, Gabriela Ferraro", "title": "Explore BiLSTM-CRF-Based Models for Open Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting multiple relations from text sentences is still a challenge for\ncurrent Open Relation Extraction (Open RE) tasks. In this paper, we develop\nseveral Open RE models based on the bidirectional LSTM-CRF (BiLSTM-CRF) neural\nnetwork and different contextualized word embedding methods. We also propose a\nnew tagging scheme to solve overlapping problems and enhance models'\nperformance. From the evaluation results and comparisons between models, we\nselect the best combination of tagging scheme, word embedder, and BiLSTM-CRF\nnetwork to achieve an Open RE model with a remarkable extracting ability on\nmultiple-relation sentences.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 03:37:22 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ni", "Tao", ""], ["Wang", "Qing", ""], ["Ferraro", "Gabriela", ""]]}, {"id": "2104.12471", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Ting-Wei Wu, Marcel Worring", "title": "Contextualized Keyword Representations for Multi-modal Retinal Image\n  Captioning", "comments": "This paper is accepted by ACM International Conference on Multimedia\n  Retrieval (ICMR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical image captioning automatically generates a medical description to\ndescribe the content of a given medical image. A traditional medical image\ncaptioning model creates a medical description only based on a single medical\nimage input. Hence, an abstract medical description or concept is hard to be\ngenerated based on the traditional approach. Such a method limits the\neffectiveness of medical image captioning. Multi-modal medical image captioning\nis one of the approaches utilized to address this problem. In multi-modal\nmedical image captioning, textual input, e.g., expert-defined keywords, is\nconsidered as one of the main drivers of medical description generation. Thus,\nencoding the textual input and the medical image effectively are both important\nfor the task of multi-modal medical image captioning. In this work, a new\nend-to-end deep multi-modal medical image captioning model is proposed.\nContextualized keyword representations, textual feature reinforcement, and\nmasked self-attention are used to develop the proposed approach. Based on the\nevaluation of the existing multi-modal medical image captioning dataset,\nexperimental results show that the proposed model is effective with the\nincrease of +53.2% in BLEU-avg and +18.6% in CIDEr, compared with the\nstate-of-the-art method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 11:08:13 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Wu", "Ting-Wei", ""], ["Worring", "Marcel", ""]]}, {"id": "2104.12483", "submitter": "Yinjiang Cai", "authors": "Yinjiang Cai, Zeyu Cui, Shu Wu, Zhen Lei, Xibo Ma", "title": "Represent Items by Items: An Enhanced Representation of the Target Item\n  for Recommendation", "comments": "10 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item-based collaborative filtering (ICF) has been widely used in industrial\napplications such as recommender system and online advertising. It models\nusers' preference on target items by the items they have interacted with.\nRecent models use methods such as attention mechanism and deep neural network\nto learn the user representation and scoring function more accurately. However,\ndespite their effectiveness, such models still overlook a problem that\nperformance of ICF methods heavily depends on the quality of item\nrepresentation especially the target item representation. In fact, due to the\nlong-tail distribution in the recommendation, most item embeddings can not\nrepresent the semantics of items accurately and thus degrade the performance of\ncurrent ICF methods. In this paper, we propose an enhanced representation of\nthe target item which distills relevant information from the co-occurrence\nitems. We design sampling strategies to sample fix number of co-occurrence\nitems for the sake of noise reduction and computational cost. Considering the\ndifferent importance of sampled items to the target item, we apply attention\nmechanism to selectively adopt the semantic information of the sampled items.\nOur proposed Co-occurrence based Enhanced Representation model (CER) learns the\nscoring function by a deep neural network with the attentive user\nrepresentation and fusion of raw representation and enhanced representation of\ntarget item as input. With the enhanced representation, CER has stronger\nrepresentation power for the tail items compared to the state-of-the-art ICF\nmethods. Extensive experiments on two public benchmarks demonstrate the\neffectiveness of CER.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 11:28:28 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Cai", "Yinjiang", ""], ["Cui", "Zeyu", ""], ["Wu", "Shu", ""], ["Lei", "Zhen", ""], ["Ma", "Xibo", ""]]}, {"id": "2104.12553", "submitter": "Diego Kozlowski", "authors": "Diego Kozlowski, Dakota S. Murray, Alexis Bell, Will Hulsey, Vincent\n  Larivi\\`ere, Thema Monroe-White and Cassidy R. Sugimoto", "title": "Avoiding bias when inferring race using name-based approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Racial disparity in academia is a widely acknowledged problem. The\nquantitative understanding of racial-based systemic inequalities is an\nimportant step towards a more equitable research system. However, few\nlarge-scale analyses have been performed on this topic, mostly because of the\nlack of robust race-disambiguation algorithms. Identifying author information\ndoes not generally include the author's race. Therefore, an algorithm needs to\nbe employed, using known information about authors, i.e., their names, to infer\ntheir perceived race. Nevertheless, as any other algorithm, the process of\nracial inference can generate biases if it is not carefully considered. When\nthe research is focused on the understanding of racial-based inequalities, such\nbiases undermine the objectives of the investigation and may perpetuate\ninequities. The goal of this article is to assess the biases introduced by the\ndifferent approaches used name-based racial inference. We use information from\nUS census and mortgage applications to infer the race of US author names in the\nWeb of Science. We estimate the effects of using given and family names,\nthresholds or continuous distributions, and imputation. Our results demonstrate\nthat the validity of name-based inference varies by race and ethnicity and that\nthreshold approaches underestimate Black authors and overestimate White\nauthors. We conclude with recommendations to avoid potential biases. This\narticle fills an important research gap that will allow more systematic and\nunbiased studies on racial disparity in science.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 08:36:22 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 12:13:01 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Kozlowski", "Diego", ""], ["Murray", "Dakota S.", ""], ["Bell", "Alexis", ""], ["Hulsey", "Will", ""], ["Larivi\u00e8re", "Vincent", ""], ["Monroe-White", "Thema", ""], ["Sugimoto", "Cassidy R.", ""]]}, {"id": "2104.12558", "submitter": "Aya Salama", "authors": "Nourhan Sakr, Aya Salama, Nadeen Tameesh, Gihan Osman", "title": "EduPal leaves no professor behind: Supporting faculty via a peer-powered\n  recommender system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The swift transitions in higher education after the COVID-19 outbreak\nidentified a gap in the pedagogical support available to faculty. We propose a\nsmart, knowledge-based chatbot that addresses issues of knowledge distillation\nand provides faculty with personalized recommendations. Our collaborative\nsystem crowdsources useful pedagogical practices and continuously filters\nrecommendations based on theory and user feedback, thus enhancing the\nexperiences of subsequent peers. We build a prototype for our local STEM\nfaculty as a proof concept and receive favorable feedback that encourages us to\nextend our development and outreach, especially to underresourced faculty.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 04:16:06 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Sakr", "Nourhan", ""], ["Salama", "Aya", ""], ["Tameesh", "Nadeen", ""], ["Osman", "Gihan", ""]]}, {"id": "2104.12622", "submitter": "Elwin Huaman", "authors": "Elwin Huaman, Amar Tauqeer, Geni Bushati and Anna Fensel", "title": "Towards Knowledge Graphs Validation through Weighted Knowledge Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of applications, such as personal assistants, search engines,\nand question-answering systems, rely on high-quality knowledge bases, a.k.a.\nKnowledge Graphs (KGs). To ensure their quality one important task is Knowledge\nValidation, which measures the degree to which statements or triples of a\nKnowledge Graph (KG) are correct. KGs inevitably contains incorrect and\nincomplete statements, which may hinder the adoption of such KGs in business\napplications as they are not trustworthy. In this paper, we propose and\nimplement a validation approach that computes a confidence score for every\ntriple and instance in a KG. The computed score is based on finding the same\ninstances across different weighted knowledge sources and comparing their\nfeatures. We evaluated the performance of our Validator by comparing a manually\nvalidated result against the output of the Validator. The experimental results\nshowed that compared with the manual validation, our Validator achieved as good\nprecision as the manual validation, although with certain limitations.\nFurthermore, we give insights and directions toward a better architecture to\ntackle KG validation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 14:45:33 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huaman", "Elwin", ""], ["Tauqeer", "Amar", ""], ["Bushati", "Geni", ""], ["Fensel", "Anna", ""]]}, {"id": "2104.12755", "submitter": "Hadi Jahanshahi", "authors": "Hadi Jahanshahi, Syed Kazmi, Mucahit Cevik", "title": "Auto Response Generation in Online Medical Chat Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telehealth helps to facilitate access to medical professionals by enabling\nremote medical services for the patients. These services have become gradually\npopular over the years with the advent of necessary technological\ninfrastructure. The benefits of telehealth have been even more apparent since\nthe beginning of the COVID-19 crisis, as people have become less inclined to\nvisit doctors in person during the pandemic. In this paper, we focus on\nfacilitating the chat sessions between a doctor and a patient. We note that the\nquality and efficiency of the chat experience can be critical as the demand for\ntelehealth services increases. Accordingly, we develop a smart auto-response\ngeneration mechanism for medical conversations that helps doctors respond to\nconsultation requests efficiently, particularly during busy sessions. We\nexplore over 900,000 anonymous, historical online messages between doctors and\npatients collected over nine months. We implement clustering algorithms to\nidentify the most frequent responses by doctors and manually label the data\naccordingly. We then train machine learning algorithms using this preprocessed\ndata to generate the responses. The considered algorithm has two steps: a\nfiltering (i.e., triggering) model to filter out infeasible patient messages\nand a response generator to suggest the top-3 doctor responses for the ones\nthat successfully pass the triggering phase. The method provides an accuracy of\n83.28\\% for precision@3 and shows robustness to its parameters.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:45:10 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Jahanshahi", "Hadi", ""], ["Kazmi", "Syed", ""], ["Cevik", "Mucahit", ""]]}, {"id": "2104.12822", "submitter": "Diego Antognini", "authors": "Martin Milenkoski, Diego Antognini, Claudiu Musat", "title": "Recommending Burgers based on Pizza Preferences: Addressing Data\n  Sparsity with a Product of Experts", "comments": "Under review. 16 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a method to tackle data sparsity and create\nrecommendations in domains with limited knowledge about the user preferences.\nWe expand the variational autoencoder collaborative filtering from a\nsingle-domain to a multi domain setting. The intuition is that user-item\ninteractions in a source domain can augment the recommendation quality in a\ntarget domain. The intuition can be taken to its extreme, where, in a\ncross-domain setup, the user history in a source domain is enough to generate\nhigh quality recommendations in a target one. We thus create a\nProduct-of-Experts (POE) architecture for recommendations that jointly models\nuser-item interactions across multiple domains. The method is resilient to\nmissing data for one or more of the domains, which is a situation often found\nin real life. We present results on two widely-used datasets - Amazon and Yelp,\nwhich support the claim that holistic user preference knowledge leads to better\nrecommendations. Surprisingly, we find that in select cases, a POE recommender\nthat does not access the target domain user representation can surpass a strong\nVAE recommender baseline trained on the target domain. We complete the analysis\nwith a study of the reasons behind this outperformance and an in-depth look at\nthe resulting embedding spaces.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 18:56:04 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Milenkoski", "Martin", ""], ["Antognini", "Diego", ""], ["Musat", "Claudiu", ""]]}, {"id": "2104.13030", "submitter": "Kun Zhang", "authors": "Le Wu, Xiangnan He, Xiang Wang, Kun Zhang, Meng Wang", "title": "A Survey on Neural Recommendation: From Collaborative Filtering to\n  Content and Context Enriched Recommendation", "comments": "In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Influenced by the stunning success of deep learning in computer vision and\nlanguage understanding, research in recommendation has shifted to inventing new\nrecommender models based on neural networks. In recent years, we have witnessed\nsignificant progress in developing neural recommender models, which generalize\nand surpass traditional recommender models owing to the strong representation\npower of neural networks. In this survey paper, we conduct a systematic review\non neural recommender models, aiming to summarize the field to facilitate\nfuture progress. Distinct from existing surveys that categorize existing\nmethods based on the taxonomy of deep learning techniques, we instead summarize\nthe field from the perspective of recommendation modeling, which could be more\ninstructive to researchers and practitioners working on recommender systems.\nSpecifically, we divide the work into three types based on the data they used\nfor recommendation modeling: 1) collaborative filtering models, which leverage\nthe key source of user-item interaction data; 2) content enriched models, which\nadditionally utilize the side information associated with users and items, like\nuser profile and item knowledge graph; and 3) context enriched models, which\naccount for the contextual information associated with an interaction, such as\ntime, location, and the past interactions. After reviewing representative works\nfor each type, we finally discuss some promising directions in this field,\nincluding benchmarking recommender systems, graph reasoning based\nrecommendation models, and explainable and fair recommendations for social\ngood.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 08:03:52 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wu", "Le", ""], ["He", "Xiangnan", ""], ["Wang", "Xiang", ""], ["Zhang", "Kun", ""], ["Wang", "Meng", ""]]}, {"id": "2104.13276", "submitter": "Gabriel Meseguer-Brocal", "authors": "Gabriel Meseguer-Brocal", "title": "MULTIMODAL ANALYSIS: Informed content estimation and audio source\n  separation", "comments": "Ph.D. dissertation. Thesis supervisor: Geoffroy Peeters. Jury:Laurent\n  Girin, Ga\\\"el Richard, Rachel Bittner, Elena Cabrio, Bruno Gas, Perfecto\n  Herrera Boyer, Antoine Liutkus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.DB cs.IR cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This dissertation proposes the study of multimodal learning in the context of\nmusical signals. Throughout, we focus on the interaction between audio signals\nand text information. Among the many text sources related to music that can be\nused (e.g. reviews, metadata, or social network feedback), we concentrate on\nlyrics. The singing voice directly connects the audio signal and the text\ninformation in a unique way, combining melody and lyrics where a linguistic\ndimension complements the abstraction of musical instruments. Our study focuses\non the audio and lyrics interaction for targeting source separation and\ninformed content estimation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 15:45:21 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 14:50:58 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Meseguer-Brocal", "Gabriel", ""]]}, {"id": "2104.13453", "submitter": "Zeyang Liu", "authors": "Zeyang Liu, Ke Zhou and Max L. Wilson", "title": "Meta-evaluation of Conversational Search Evaluation Metrics", "comments": "43 pages", "journal-ref": null, "doi": "10.1145/3445029", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search systems, such as Google Assistant and Microsoft\nCortana, enable users to interact with search systems in multiple rounds\nthrough natural language dialogues. Evaluating such systems is very challenging\ngiven that any natural language responses could be generated, and users\ncommonly interact for multiple semantically coherent rounds to accomplish a\nsearch task. Although prior studies proposed many evaluation metrics, the\nextent of how those measures effectively capture user preference remains to be\ninvestigated. In this paper, we systematically meta-evaluate a variety of\nconversational search metrics. We specifically study three perspectives on\nthose metrics: (1) reliability: the ability to detect \"actual\" performance\ndifferences as opposed to those observed by chance; (2) fidelity: the ability\nto agree with ultimate user preference; and (3) intuitiveness: the ability to\ncapture any property deemed important: adequacy, informativeness, and fluency\nin the context of conversational search. By conducting experiments on two test\ncollections, we find that the performance of different metrics varies\nsignificantly across different scenarios whereas consistent with prior studies,\nexisting metrics only achieve a weak correlation with ultimate user preference\nand satisfaction. METEOR is, comparatively speaking, the best existing\nsingle-turn metric considering all three perspectives. We also demonstrate that\nadapted session-based evaluation metrics can be used to measure multi-turn\nconversational search, achieving moderate concordance with user satisfaction.\nTo our knowledge, our work establishes the most comprehensive meta-evaluation\nfor conversational search to date.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 20:01:03 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Liu", "Zeyang", ""], ["Zhou", "Ke", ""], ["Wilson", "Max L.", ""]]}, {"id": "2104.13640", "submitter": "Navid Rekabsaz", "authors": "Navid Rekabsaz and Simone Kopeinik and Markus Schedl", "title": "Societal Biases in Retrieved Contents: Measurement Framework and\n  Adversarial Mitigation for BERT Rankers", "comments": "Accepted at SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462949", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Societal biases resonate in the retrieved contents of information retrieval\n(IR) systems, resulting in reinforcing existing stereotypes. Approaching this\nissue requires established measures of fairness in respect to the\nrepresentation of various social groups in retrieval results, as well as\nmethods to mitigate such biases, particularly in the light of the advances in\ndeep ranking models. In this work, we first provide a novel framework to\nmeasure the fairness in the retrieved text contents of ranking models.\nIntroducing a ranker-agnostic measurement, the framework also enables the\ndisentanglement of the effect on fairness of collection from that of rankers.\nTo mitigate these biases, we propose AdvBert, a ranking model achieved by\nadapting adversarial bias mitigation for IR, which jointly learns to predict\nrelevance and remove protected attributes. We conduct experiments on two\npassage retrieval collections (MSMARCO Passage Re-ranking and TREC Deep\nLearning 2019 Passage Re-ranking), which we extend by fairness annotations of a\nselected subset of queries regarding gender attributes. Our results on the\nMSMARCO benchmark show that, (1) all ranking models are less fair in comparison\nwith ranker-agnostic baselines, and (2) the fairness of Bert rankers\nsignificantly improves when using the proposed AdvBert models. Lastly, we\ninvestigate the trade-off between fairness and utility, showing that we can\nmaintain the significant improvements in fairness without any significant loss\nin utility.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 08:53:54 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 07:02:56 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Rekabsaz", "Navid", ""], ["Kopeinik", "Simone", ""], ["Schedl", "Markus", ""]]}, {"id": "2104.13748", "submitter": "Matthias Springstein", "authors": "Matthias Springstein and Eric M\\\"uller-Budack and Ralph Ewerth", "title": "QuTI! Quantifying Text-Image Consistency in Multimodal Documents", "comments": "Accepted for publication in: International ACM SIGIR Conference on\n  Research and Development in Information Retrieval 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web and social media platforms have become popular sources for\nnews and information. Typically, multimodal information, e.g., image and text\nis used to convey information more effectively and to attract attention. While\nin most cases image content is decorative or depicts additional information, it\nhas also been leveraged to spread misinformation and rumors in recent years. In\nthis paper, we present a Web-based demo application that automatically\nquantifies the cross-modal relations of entities (persons, locations, and\nevents) in image and text. The applications are manifold. For example, the\nsystem can help users to explore multimodal articles more efficiently, or can\nassist human assessors and fact-checking efforts in the verification of the\ncredibility of news stories, tweets, or other multimodal documents.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 13:28:27 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Springstein", "Matthias", ""], ["M\u00fcller-Budack", "Eric", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2104.13841", "submitter": "Malte Ostendorff", "authors": "Malte Ostendorff, Elliott Ash, Terry Ruas, Bela Gipp, Julian\n  Moreno-Schneider, Georg Rehm", "title": "Evaluating Document Representations for Content-based Legal Literature\n  Recommendations", "comments": "Accepted for publication at ICAIL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems assist legal professionals in finding relevant literature\nfor supporting their case. Despite its importance for the profession, legal\napplications do not reflect the latest advances in recommender systems and\nrepresentation learning research. Simultaneously, legal recommender systems are\ntypically evaluated in small-scale user study without any public available\nbenchmark datasets. Thus, these studies have limited reproducibility. To\naddress the gap between research and practice, we explore a set of\nstate-of-the-art document representation methods for the task of retrieving\nsemantically related US case law. We evaluate text-based (e.g., fastText,\nTransformers), citation-based (e.g., DeepWalk, Poincar\\'e), and hybrid methods.\nWe compare in total 27 methods using two silver standards with annotations for\n2,964 documents. The silver standards are newly created from Open Case Book and\nWikisource and can be reused under an open license facilitating\nreproducibility. Our experiments show that document representations from\naveraged fastText word vectors (trained on legal corpora) yield the best\nresults, closely followed by Poincar\\'e citation embeddings. Combining fastText\nand Poincar\\'e in a hybrid manner further improves the overall result. Besides\nthe overall performance, we analyze the methods depending on document length,\ncitation count, and the coverage of their recommendations. We make our source\ncode, models, and datasets publicly available at\nhttps://github.com/malteos/legal-document-similarity/.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:48:19 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Ostendorff", "Malte", ""], ["Ash", "Elliott", ""], ["Ruas", "Terry", ""], ["Gipp", "Bela", ""], ["Moreno-Schneider", "Julian", ""], ["Rehm", "Georg", ""]]}, {"id": "2104.13971", "submitter": "Ryosuke Motegi", "authors": "Ryosuke Motegi and Yoichi Seki", "title": "SMLSOM: The shrinking maximum likelihood self-organizing map", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the number of clusters in a dataset is a fundamental issue in\ndata clustering. Many methods have been proposed to solve the problem of\nselecting the number of clusters, considering it to be a problem with regard to\nmodel selection. This paper proposes a greedy algorithm that automatically\nselects a suitable number of clusters based on a probability distribution model\nframework. The algorithm includes two components. First, a generalization of\nKohonen's self-organizing map (SOM), which has nodes linked to a probability\ndistribution model, and which enables the algorithm to search for the winner\nbased on the likelihood of each node, is introduced. Second, the proposed\nmethod uses a graph structure and a neighbor defined by the length of the\nshortest path between nodes, in contrast to Kohonen's SOM in which the nodes\nare fixed in the Euclidean space. This implementation makes it possible to\nupdate its graph structure by cutting links to weakly connected nodes to avoid\nunnecessary node deletion. The weakness of a node connection is measured using\nthe Kullback--Leibler divergence and the redundancy of a node is measured by\nthe minimum description length (MDL). This updating step makes it easy to\ndetermine the suitable number of clusters. Compared with existing methods, our\nproposed method is computationally efficient and can accurately select the\nnumber of clusters and perform clustering.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 18:50:36 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Motegi", "Ryosuke", ""], ["Seki", "Yoichi", ""]]}, {"id": "2104.14133", "submitter": "Jia-Huei Ju", "authors": "Jia-Huei Ju, Jheng-Hong Yang, Chuan-Ju Wang", "title": "Text-to-Text Multi-view Learning for Passage Re-ranking", "comments": "Accepted as short paper in SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, much progress in natural language processing has been driven by\ndeep contextualized representations pretrained on large corpora. Typically, the\nfine-tuning on these pretrained models for a specific downstream task is based\non single-view learning, which is however inadequate as a sentence can be\ninterpreted differently from different perspectives. Therefore, in this work,\nwe propose a text-to-text multi-view learning framework by incorporating an\nadditional view -- the text generation view -- into a typical single-view\npassage ranking model. Empirically, the proposed approach is of help to the\nranking performance compared to its single-view counterpart. Ablation studies\nare also reported in the paper.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 06:12:34 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ju", "Jia-Huei", ""], ["Yang", "Jheng-Hong", ""], ["Wang", "Chuan-Ju", ""]]}, {"id": "2104.14200", "submitter": "Junsu Cho", "authors": "Junsu Cho, Dongmin Hyun, SeongKu Kang, Hwanjo Yu", "title": "Learning Heterogeneous Temporal Patterns of User Preference for Timely\n  Recommendation", "comments": "Accepted to The Web Conference (WWW) 2021", "journal-ref": null, "doi": "10.1145/3442381.3449947", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have achieved great success in modeling user's\npreferences on items and predicting the next item the user would consume.\nRecently, there have been many efforts to utilize time information of users'\ninteractions with items to capture inherent temporal patterns of user behaviors\nand offer timely recommendations at a given time. Existing studies regard the\ntime information as a single type of feature and focus on how to associate it\nwith user preferences on items. However, we argue they are insufficient for\nfully learning the time information because the temporal patterns of user\npreference are usually heterogeneous. A user's preference for a particular item\nmay 1) increase periodically or 2) evolve over time under the influence of\nsignificant recent events, and each of these two kinds of temporal pattern\nappears with some unique characteristics. In this paper, we first define the\nunique characteristics of the two kinds of temporal pattern of user preference\nthat should be considered in time-aware recommender systems. Then we propose a\nnovel recommender system for timely recommendations, called TimelyRec, which\njointly learns the heterogeneous temporal patterns of user preference\nconsidering all of the defined characteristics. In TimelyRec, a cascade of two\nencoders captures the temporal patterns of user preference using a proposed\nattention module for each encoder. Moreover, we introduce an evaluation\nscenario that evaluates the performance on predicting an interesting item and\nwhen to recommend the item simultaneously in top-K recommendation (i.e.,\nitem-timing recommendation). Our extensive experiments on a scenario for item\nrecommendation and the proposed scenario for item-timing recommendation on\nreal-world datasets demonstrate the superiority of TimelyRec and the proposed\nattention modules.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 08:37:30 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Cho", "Junsu", ""], ["Hyun", "Dongmin", ""], ["Kang", "SeongKu", ""], ["Yu", "Hwanjo", ""]]}, {"id": "2104.14229", "submitter": "Nasser Ghadiri", "authors": "Hoda Memarzadeh, Nasser Ghadiri, Maryam Lotfi Shahreza and Suresh\n  Pokharel", "title": "Heterogeneous electronic medical record representation for similarity\n  computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the widespread use of tools and the development of text processing\ntechniques, the size and range of clinical data are not limited to structured\ndata. The rapid growth of recorded information has led to big data platforms in\nhealthcare that could be used to improve patients' primary care and serve\nvarious secondary purposes. Patient similarity assessment is one of the\nsecondary tasks in identifying patients who are similar to a given patient, and\nit helps derive insights from similar patients' records to provide better\ntreatment. This type of assessment is based on calculating the distance between\npatients. Since representing and calculating the similarity of patients plays\nan essential role in many secondary uses of electronic records, this article\nexamines a new data representation method for Electronic Medical Records (EMRs)\nwhile taking into account the information in clinical narratives for similarity\ncomputing. Some previous works are based on structured data types, while other\nworks only use unstructured data. However, a comprehensive representation of\nthe information contained in the EMR requires the effective aggregation of both\nstructured and unstructured data. To address the limitations of previous\nmethods, we propose a method that captures the co-occurrence of different\nmedical events, including signs, symptoms, and diseases extracted via\nunstructured data and structured data. It integrates data as discriminative\nfeatures to construct a temporal tree, considering the difference between\nevents that have short-term and long-term impacts. Our results show that\nconsidering signs, symptoms, and diseases in every time interval leads to less\nMSE and more precision compared to baseline representations that do not\nconsider this information or consider them separately from structured data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 09:38:14 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Memarzadeh", "Hoda", ""], ["Ghadiri", "Nasser", ""], ["Shahreza", "Maryam Lotfi", ""], ["Pokharel", "Suresh", ""]]}, {"id": "2104.14289", "submitter": "Sumanth Prabhu", "authors": "Sumanth Prabhu and Moosa Mohamed and Hemant Misra", "title": "Multi-class Text Classification using BERT-based Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Text Classification finds interesting applications in the pickup and delivery\nservices industry where customers require one or more items to be picked up\nfrom a location and delivered to a certain destination. Classifying these\ncustomer transactions into multiple categories helps understand the market\nneeds for different customer segments. Each transaction is accompanied by a\ntext description provided by the customer to describe the products being picked\nup and delivered which can be used to classify the transaction. BERT-based\nmodels have proven to perform well in Natural Language Understanding. However,\nthe product descriptions provided by the customers tend to be short, incoherent\nand code-mixed (Hindi-English) text which demands fine-tuning of such models\nwith manually labelled data to achieve high accuracy. Collecting this labelled\ndata can prove to be expensive. In this paper, we explore Active Learning\nstrategies to label transaction descriptions cost effectively while using BERT\nto train a transaction classification model. On TREC-6, AG's News Corpus and an\ninternal dataset, we benchmark the performance of BERT across different Active\nLearning strategies in Multi-Class Text Classification.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 19:49:39 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Prabhu", "Sumanth", ""], ["Mohamed", "Moosa", ""], ["Misra", "Hemant", ""]]}, {"id": "2104.14336", "submitter": "Rub\\`en P\\'erez Tito", "authors": "Rub\\`en Tito, Dimosthenis Karatzas, Ernest Valveny", "title": "Document Collection Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current tasks and methods in Document Understanding aims to process documents\nas single elements. However, documents are usually organized in collections\n(historical records, purchase invoices), that provide context useful for their\ninterpretation. To address this problem, we introduce Document Collection\nVisual Question Answering (DocCVQA) a new dataset and related task, where\nquestions are posed over a whole collection of document images and the goal is\nnot only to provide the answer to the given question, but also to retrieve the\nset of documents that contain the information needed to infer the answer. Along\nwith the dataset we propose a new evaluation metric and baselines which provide\nfurther insights to the new dataset and task.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 18:05:48 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 15:07:09 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Tito", "Rub\u00e8n", ""], ["Karatzas", "Dimosthenis", ""], ["Valveny", "Ernest", ""]]}, {"id": "2104.14339", "submitter": "Wenhao Wu", "authors": "Wenhao Wu and Sujian Li", "title": "A Comprehensive Attempt to Research Statement Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For a researcher, writing a good research statement is crucial but costs a\nlot of time and effort. To help researchers, in this paper, we propose the\nresearch statement generation (RSG) task which aims to summarize one's research\nachievements and help prepare a formal research statement. For this task, we\nconduct a comprehensive attempt including corpus construction, method design,\nand performance evaluation. First, we construct an RSG dataset with 62 research\nstatements and the corresponding 1,203 publications. Due to the limitation of\nour resources, we propose a practical RSG method which identifies a\nresearcher's research directions by topic modeling and clustering techniques\nand extracts salient sentences by a neural text summarizer. Finally,\nexperiments show that our method outperforms all the baselines with better\ncontent coverage and coherence.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 03:57:00 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Wu", "Wenhao", ""], ["Li", "Sujian", ""]]}, {"id": "2104.14492", "submitter": "Natalia D\\'iaz-Rodr\\'iguez PhD", "authors": "Natalia D\\'iaz-Rodr\\'iguez, R\\=uta Binkyt\\.e-Sadauskien\\.e, Wafae\n  Bakkali, Sannidhi Bookseller, Paola Tubaro, Andrius Bacevicius, Raja Chatila", "title": "Questioning causality on sex, gender and COVID-19, and identifying bias\n  in large-scale data-driven analyses: the Bias Priority Recommendations and\n  Bias Catalog for Pandemics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has spurred a large amount of observational studies\nreporting linkages between the risk of developing severe COVID-19 or dying from\nit, and sex and gender. By reviewing a large body of related literature and\nconducting a fine grained analysis based on sex-disaggregated data of 61\ncountries spanning 5 continents, we discover several confounding factors that\ncould possibly explain the supposed male vulnerability to COVID-19. We thus\nhighlight the challenge of making causal claims based on available data, given\nthe lack of statistical significance and potential existence of biases.\nInformed by our findings on potential variables acting as confounders, we\ncontribute a broad overview on the issues bias, explainability and fairness\nentail in data-driven analyses. Thus, we outline a set of discriminatory policy\nconsequences that could, based on such results, lead to unintended\ndiscrimination. To raise awareness on the dimensionality of such foreseen\nimpacts, we have compiled an encyclopedia-like reference guide, the Bias\nCatalog for Pandemics (BCP), to provide definitions and emphasize realistic\nexamples of bias in general, and within the COVID-19 pandemic context. These\nare categorized within a division of bias families and a 2-level priority\nscale, together with preventive steps. In addition, we facilitate the Bias\nPriority Recommendations on how to best use and apply this catalog, and provide\nguidelines in order to address real world research questions. The objective is\nto anticipate and avoid disparate impact and discrimination, by considering\ncausality, explainability, bias and techniques to mitigate the latter. With\nthese, we hope to 1) contribute to designing and conducting fair and equitable\ndata-driven studies and research; and 2) interpret and draw meaningful and\nactionable conclusions from these.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:07:06 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["D\u00edaz-Rodr\u00edguez", "Natalia", ""], ["Binkyt\u0117-Sadauskien\u0117", "R\u016bta", ""], ["Bakkali", "Wafae", ""], ["Bookseller", "Sannidhi", ""], ["Tubaro", "Paola", ""], ["Bacevicius", "Andrius", ""], ["Chatila", "Raja", ""]]}, {"id": "2104.14595", "submitter": "Hannah Bast", "authors": "Hannah Bast, Johannes Kalmbach, Theresa Klumpp, Florian Kramer, Niklas\n  Schnelle", "title": "Efficient SPARQL Autocompletion via SPARQL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to achieve fast autocompletion for SPARQL queries on very large\nknowledge bases. At any position in the body of a SPARQL query, the\nautocompletion suggests matching subjects, predicates, or objects. The\nsuggestions are context-sensitive in the sense that they lead to a non-empty\nresult and are ranked by their relevance to the part of the query already\ntyped. The suggestions can be narrowed down by prefix search on the names and\naliases of the desired subject, predicate, or object. All suggestions are\nthemselves obtained via SPARQL queries, which we call autocompletion queries.\nFor existing SPARQL engines, these queries are impractically slow on large\nknowledge bases. We present various algorithmic and engineering improvements of\nan existing SPARQL engine such that these autocompletion queries are executed\nefficiently. We provide an extensive evaluation of a variety of suggestion\nmethods on three large knowledge bases, including Wikidata (6.9B triples). We\nexplore the trade-off between the relevance of the suggestions and the\nprocessing time of the autocompletion queries. We compare our results with two\nwidely used SPARQL engines, Virtuoso and Blazegraph. On Wikidata, we achieve\nfully sensitive suggestions with sub-second response times for over 90% of a\nlarge and diverse set of thousands of autocompletion queries. Materials for\nfull reproducibility, an interactive evaluation web app, and a demo are\navailable on: https://ad.informatik.uni-freiburg.de/publications .\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 18:29:39 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Bast", "Hannah", ""], ["Kalmbach", "Johannes", ""], ["Klumpp", "Theresa", ""], ["Kramer", "Florian", ""], ["Schnelle", "Niklas", ""]]}, {"id": "2104.14899", "submitter": "Wen Zhang", "authors": "Chi-Man Wong, Fan Feng, Wen Zhang, Chi-Man Vong, Hui Chen, Yichi\n  Zhang, Peng He, Huan Chen, Kun Zhao, Huajun Chen", "title": "Improving Conversational Recommendation System by Pretraining on\n  Billions Scale of Knowledge Graph", "comments": "Paper is accepted by ICDE2021 industry track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conversational Recommender Systems (CRSs) in E-commerce platforms aim to\nrecommend items to users via multiple conversational interactions.\nClick-through rate (CTR) prediction models are commonly used for ranking\ncandidate items. However, most CRSs are suffer from the problem of data\nscarcity and sparseness. To address this issue, we propose a novel\nknowledge-enhanced deep cross network (K-DCN), a two-step (pretrain and\nfine-tune) CTR prediction model to recommend items. We first construct a\nbillion-scale conversation knowledge graph (CKG) from information about users,\nitems and conversations, and then pretrain CKG by introducing knowledge graph\nembedding method and graph convolution network to encode semantic and\nstructural information respectively.To make the CTR prediction model sensible\nof current state of users and the relationship between dialogues and items, we\nintroduce user-state and dialogue-interaction representations based on\npre-trained CKG and propose K-DCN.In K-DCN, we fuse the user-state\nrepresentation, dialogue-interaction representation and other normal feature\nrepresentations via deep cross network, which will give the rank of candidate\nitems to be recommended.We experimentally prove that our proposal significantly\noutperforms baselines and show it's real application in Alime.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 10:56:41 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Wong", "Chi-Man", ""], ["Feng", "Fan", ""], ["Zhang", "Wen", ""], ["Vong", "Chi-Man", ""], ["Chen", "Hui", ""], ["Zhang", "Yichi", ""], ["He", "Peng", ""], ["Chen", "Huan", ""], ["Zhao", "Kun", ""], ["Chen", "Huajun", ""]]}, {"id": "2104.14994", "submitter": "Golsa Tahmasebzadeh", "authors": "Golsa Tahmasebzadeh, Endri Kacupaj, Eric M\\\"uller-Budack, Sherzod\n  Hakimov, Jens Lehmann, Ralph Ewerth", "title": "GeoWINE: Geolocation based Wiki, Image,News and Event Retrieval", "comments": "Accepted for publication in: International ACM SIGIR Conference on\n  Research and Development in Information Retrieval 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of social media, geolocation inference on news or events has\nbecome a very important task. In this paper, we present the GeoWINE\n(Geolocation-based Wiki-Image-News-Event retrieval) demonstrator, an effective\nmodular system for multimodal retrieval which expects only a single image as\ninput. The GeoWINE system consists of five modules in order to retrieve related\ninformation from various sources. The first module is a state-of-the-art model\nfor geolocation estimation of images. The second module performs a\ngeospatial-based query for entity retrieval using the Wikidata knowledge graph.\nThe third module exploits four different image embedding representations, which\nare used to retrieve most similar entities compared to the input image. The\nembeddings are derived from the tasks of geolocation estimation, place\nrecognition, ImageNet-based image classification, and their combination. The\nlast two modules perform news and event retrieval from EventRegistry and the\nOpen Event Knowledge Graph (OEKG). GeoWINE provides an intuitive interface for\nend-users and is insightful for experts for reconfiguration to individual\nsetups. The GeoWINE achieves promising results in entity label prediction for\nimages on Google Landmarks dataset. The demonstrator is publicly available at\nhttp://cleopatra.ijs.si/geowine/.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 13:27:50 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 12:04:05 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Tahmasebzadeh", "Golsa", ""], ["Kacupaj", "Endri", ""], ["M\u00fcller-Budack", "Eric", ""], ["Hakimov", "Sherzod", ""], ["Lehmann", "Jens", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2104.15104", "submitter": "Tanay Kumar Saha", "authors": "Sanghamitra Dutta and Liang Ma and Tanay Kumar Saha and Di Lu and Joel\n  Tetreault and Alejandro Jaimes", "title": "GTN-ED: Event Detection Using Graph Transformer Networks", "comments": null, "journal-ref": "TextGraphs 2021 : 15th Workshop on Graph-Based Natural Language\n  Processing", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works show that the graph structure of sentences, generated from\ndependency parsers, has potential for improving event detection. However, they\noften only leverage the edges (dependencies) between words, and discard the\ndependency labels (e.g., nominal-subject), treating the underlying graph edges\nas homogeneous. In this work, we propose a novel framework for incorporating\nboth dependencies and their labels using a recently proposed technique called\nGraph Transformer Networks (GTN). We integrate GTNs to leverage dependency\nrelations on two existing homogeneous-graph-based models, and demonstrate an\nimprovement in the F1 score on the ACE dataset.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 16:35:29 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 10:53:10 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Dutta", "Sanghamitra", ""], ["Ma", "Liang", ""], ["Saha", "Tanay Kumar", ""], ["Lu", "Di", ""], ["Tetreault", "Joel", ""], ["Jaimes", "Alejandro", ""]]}]