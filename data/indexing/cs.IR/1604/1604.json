[{"id": "1604.00071", "submitter": "Ruining He", "authors": "Ruining He, Chunbin Lin, Julian McAuley", "title": "Fashionista: A Fashion-aware Graphical System for Exploring Visually\n  Similar Items", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To build a fashion recommendation system, we need to help users retrieve\nfashionable items that are visually similar to a particular query, for reasons\nranging from searching alternatives (i.e., substitutes), to generating stylish\noutfits that are visually consistent, among other applications. In domains like\nclothing and accessories, such considerations are particularly paramount as the\nvisual appearance of items is a critical feature that guides users' decisions.\nHowever, existing systems like Amazon and eBay still rely mainly on keyword\nsearch and recommending loosely consistent items (e.g. based on co-purchasing\nor browsing data), without an interface that makes use of visual information to\nserve the above needs. In this paper, we attempt to fill this gap by designing\nand implementing an image-based query system, called Fashionista, which\nprovides a graphical interface to help users efficiently explore those items\nthat are not only visually similar to a given query, but which are also\nfashionable, as determined by visually-aware recommendation approaches.\nMethodologically, Fashionista learns a low-dimensional visual space as well as\nthe evolution of fashion trends from large corpora of binary feedback data such\nas purchase histories of Women's Clothing & Accessories from Amazon, which we\nuse for this demonstration.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 22:50:40 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["He", "Ruining", ""], ["Lin", "Chunbin", ""], ["McAuley", "Julian", ""]]}, {"id": "1604.00119", "submitter": "Krish Perumal", "authors": "Krish Perumal", "title": "Semi-supervised and Unsupervised Methods for Categorizing Posts in Web\n  Discussion Forums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web discussion forums are used by millions of people worldwide to share\ninformation belonging to a variety of domains such as automotive vehicles,\npets, sports, etc. They typically contain posts that fall into different\ncategories such as problem, solution, feedback, spam, etc. Automatic\nidentification of these categories can aid information retrieval that is\ntailored for specific user requirements. Previously, a number of supervised\nmethods have attempted to solve this problem; however, these depend on the\navailability of abundant training data. A few existing unsupervised and\nsemi-supervised approaches are either focused on identifying a single category\nor do not report category-specific performance. In contrast, this work proposes\nunsupervised and semi-supervised methods that require no or minimal training\ndata to achieve this objective without compromising on performance. A\nfine-grained analysis is also carried out to discuss their limitations. The\nproposed methods are based on sequence models (specifically, Hidden Markov\nModels) that can model language for each category using word and part-of-speech\nprobability distributions, and manually specified features. Empirical\nevaluations across domains demonstrate that the proposed methods are better\nsuited for this task than existing ones.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 03:32:03 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 16:06:13 GMT"}, {"version": "v3", "created": "Sun, 24 Apr 2016 21:27:17 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Perumal", "Krish", ""]]}, {"id": "1604.00125", "submitter": "Ziqiang Cao", "authors": "Ziqiang Cao, Wenjie Li, Sujian Li, Furu Wei and Yanran Li", "title": "AttSum: Joint Learning of Focusing and Summarization with Neural\n  Attention", "comments": "10 pages, 1 figure", "journal-ref": "COLING 2016", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Query relevance ranking and sentence saliency ranking are the two main tasks\nin extractive query-focused summarization. Previous supervised summarization\nsystems often perform the two tasks in isolation. However, since reference\nsummaries are the trade-off between relevance and saliency, using them as\nsupervision, neither of the two rankers could be trained well. This paper\nproposes a novel summarization system called AttSum, which tackles the two\ntasks jointly. It automatically learns distributed representations for\nsentences as well as the document cluster. Meanwhile, it applies the attention\nmechanism to simulate the attentive reading of human behavior when a query is\ngiven. Extensive experiments are conducted on DUC query-focused summarization\nbenchmark datasets. Without using any hand-crafted features, AttSum achieves\ncompetitive performance. It is also observed that the sentences recognized to\nfocus on the query indeed meet the query need.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 04:18:39 GMT"}, {"version": "v2", "created": "Tue, 27 Sep 2016 02:22:33 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Cao", "Ziqiang", ""], ["Li", "Wenjie", ""], ["Li", "Sujian", ""], ["Wei", "Furu", ""], ["Li", "Yanran", ""]]}, {"id": "1604.00126", "submitter": "Ardavan Saeedi", "authors": "Kayhan Batmanghelich, Ardavan Saeedi, Karthik Narasimhan, Sam Gershman", "title": "Nonparametric Spherical Topic Modeling with Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional topic models do not account for semantic regularities in\nlanguage. Recent distributional representations of words exhibit semantic\nconsistency over directional metrics such as cosine similarity. However,\nneither categorical nor Gaussian observational distributions used in existing\ntopic models are appropriate to leverage such correlations. In this paper, we\npropose to use the von Mises-Fisher distribution to model the density of words\nover a unit sphere. Such a representation is well-suited for directional data.\nWe use a Hierarchical Dirichlet Process for our base topic model and propose an\nefficient inference algorithm based on Stochastic Variational Inference. This\nmodel enables us to naturally exploit the semantic structures of word\nembeddings while flexibly discovering the number of topics. Experiments\ndemonstrate that our method outperforms competitive approaches in terms of\ntopic coherence on two different text corpora while offering efficient\ninference.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 04:36:58 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Batmanghelich", "Kayhan", ""], ["Saeedi", "Ardavan", ""], ["Narasimhan", "Karthik", ""], ["Gershman", "Sam", ""]]}, {"id": "1604.00223", "submitter": "Raphael Toledo", "authors": "Raphael R. Toledo, George Danezis and Ian Goldberg", "title": "Lower-Cost epsilon-Private Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Private Information Retrieval (PIR), despite being well studied, is\ncomputationally costly and arduous to scale. We explore lower-cost relaxations\nof information-theoretic PIR, based on dummy queries, sparse vectors, and\ncompositions with an anonymity system. We prove the security of each scheme\nusing a flexible differentially private definition for private queries that can\ncapture notions of imperfect privacy. We show that basic schemes are weak, but\nsome of them can be made arbitrarily safe by composing them with large\nanonymity systems.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 12:55:35 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Toledo", "Raphael R.", ""], ["Danezis", "George", ""], ["Goldberg", "Ian", ""]]}, {"id": "1604.00233", "submitter": "Krzysztof Wo{\\l}k", "authors": "Krzysztof Wo{\\l}k", "title": "Building an Internet Radio System with Interdisciplinary factored system\n  for automatic content recommendation", "comments": "ISBN: 978-3-659-41584-5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic systems for music content recommendation have assumed a new role in\nrecent years. These systems have transformed from being just a convenient,\nstandalone tool into an inseparable element of modern living. In addition, not\nonly do these systems strongly influence human moods and feelings with the\nselection of proper music content, but they also provide significant commercial\nand advertising opportunities. This research aims to examine and implement two\nsuch systems available for the automatic recognition and recommendation of\nmusic and advertisement content for Internet radio. Through analysis of the\npractical issues of application fields and spheres of influence, conclusions\nwill be drawn about the possible perspectives on and future role of such\nsystems. Other content adaptation that is based on music genres will be\ndiscussed, as wellAnother aim of this study is to provide an innovative\nInternet radio implementation as compared to traditional radio and other\nInternet broadcast solutions. This will include automatic content\nrecommendation systems for listeners and marketing companies, as well as the\nusage of a voice synthesizer in in automatic program scheduling.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 13:25:43 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Wo\u0142k", "Krzysztof", ""]]}, {"id": "1604.00837", "submitter": "Dominik Kowald", "authors": "Kowald Dominik and Lex Elisabeth", "title": "The Influence of Frequency, Recency and Semantic Context on the Reuse of\n  Tags in Social Tagging Systems", "comments": "Accepted by Hypertext 2016 conference as short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study factors that influence tag reuse behavior in social\ntagging systems. Our work is guided by the activation equation of the cognitive\nmodel ACT-R, which states that the usefulness of information in human memory\ndepends on the three factors usage frequency, recency and semantic context. It\nis our aim to shed light on the influence of these factors on tag reuse. In our\nexperiments, we utilize six datasets from the social tagging systems Flickr,\nCiteULike, BibSonomy, Delicious, LastFM and MovieLens, covering a range of\nvarious tagging settings. Our results confirm that frequency, recency and\nsemantic context positively influence the reuse probability of tags. However,\nthe extent to which each factor individually influences tag reuse strongly\ndepends on the type of folksonomy present in a social tagging system. Our work\ncan serve as guideline for researchers and developers of tag-based recommender\nsystems when designing algorithms for social tagging environments.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 12:49:02 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Dominik", "Kowald", ""], ["Elisabeth", "Lex", ""]]}, {"id": "1604.00933", "submitter": "Walid Shalaby", "authors": "Walid Shalaby, Khalifeh Al Jadda, Mohammed Korayem and Trey Grainger", "title": "Entity Type Recognition using an Ensemble of Distributional Semantic\n  Models to Enhance Query Understanding", "comments": "A short version of this paper has been accepted in \"COMPSAC 2016: The\n  40th IEEE Computer Society International Conference on Computers, Software &\n  Applications\"", "journal-ref": null, "doi": "10.1109/COMPSAC.2016.109", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an ensemble approach for categorizing search query entities in the\nrecruitment domain. Understanding the types of entities expressed in a search\nquery (Company, Skill, Job Title, etc.) enables more intelligent information\nretrieval based upon those entities compared to a traditional keyword-based\nsearch. Because search queries are typically very short, leveraging a\ntraditional bag-of-words model to identify entity types would be inappropriate\ndue to the lack of contextual information. Our approach instead combines clues\nfrom different sources of varying complexity in order to collect real-world\nknowledge about query entities. We employ distributional semantic\nrepresentations of query entities through two models: 1) contextual vectors\ngenerated from encyclopedic corpora like Wikipedia, and 2) high dimensional\nword embedding vectors generated from millions of job postings using word2vec.\nAdditionally, our approach utilizes both entity linguistic properties obtained\nfrom WordNet and ontological properties extracted from DBpedia. We evaluate our\napproach on a data set created at CareerBuilder; the largest job board in the\nUS. The data set contains entities extracted from millions of job\nseekers/recruiters search queries, job postings, and resume documents. After\nconstructing the distributional vectors of search entities, we use supervised\nmachine learning to infer search entity types. Empirical results show that our\napproach outperforms the state-of-the-art word2vec distributional semantics\nmodel trained on Wikipedia. Moreover, we achieve micro-averaged F 1 score of\n97% using the proposed distributional representations ensemble.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 16:18:44 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Shalaby", "Walid", ""], ["Jadda", "Khalifeh Al", ""], ["Korayem", "Mohammed", ""], ["Grainger", "Trey", ""]]}, {"id": "1604.00942", "submitter": "Emanuel Laci\\'c", "authors": "Emanuel Lacic, Dominik Kowald, Elisabeth Lex", "title": "High Enough? Explaining and Predicting Traveler Satisfaction Using\n  Airline Review", "comments": "5 pages + references, 2 tables, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air travel is one of the most frequently used means of transportation in our\nevery-day life. Thus, it is not surprising that an increasing number of\ntravelers share their experiences with airlines and airports in form of online\nreviews on the Web. In this work, we thrive to explain and uncover the features\nof airline reviews that contribute most to traveler satisfaction. To that end,\nwe examine reviews crawled from the Skytrax air travel review portal. Skytrax\nprovides four review categories to review airports, lounges, airlines and\nseats. Each review category consists of several five-star ratings as well as\nfree-text review content. In this paper, we conducted a comprehensive feature\nstudy and we find that not only five-star rating information such as airport\nqueuing time and lounge comfort highly correlate with traveler satisfaction but\nalso textual features in the form of the inferred review text sentiment. Based\non our findings, we created classifiers to predict traveler satisfaction using\nthe best performing rating features. Our results reveal that given our\nmethodology, traveler satisfaction can be predicted with high accuracy.\nAdditionally, we find that training a model on the sentiment of the review text\nprovides a competitive alternative when no five star rating information is\navailable. We believe that our work is of interest for researchers in the area\nof modeling and predicting user satisfaction based on available review data on\nthe Web.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 16:44:00 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Lacic", "Emanuel", ""], ["Kowald", "Dominik", ""], ["Lex", "Elisabeth", ""]]}, {"id": "1604.01070", "submitter": "Titipat Achakulvisut", "authors": "Titipat Achakulvisut, Daniel E. Acuna, Tulakan Ruangrong, Konrad\n  Kording", "title": "Science Concierge: A fast content-based recommendation system for\n  scientific publications", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0158423", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Finding relevant publications is important for scientists who have to cope\nwith exponentially increasing numbers of scholarly material. Algorithms can\nhelp with this task as they help for music, movie, and product recommendations.\nHowever, we know little about the performance of these algorithms with\nscholarly material. Here, we develop an algorithm, and an accompanying Python\nlibrary, that implements a recommendation system based on the content of\narticles. Design principles are to adapt to new content, provide near-real time\nsuggestions, and be open source. We tested the library on 15K posters from the\nSociety of Neuroscience Conference 2015. Human curated topics are used to cross\nvalidate parameters in the algorithm and produce a similarity metric that\nmaximally correlates with human judgments. We show that our algorithm\nsignificantly outperformed suggestions based on keywords. The work presented\nhere promises to make the exploration of scholarly material faster and more\naccurate.\n", "versions": [{"version": "v1", "created": "Mon, 4 Apr 2016 21:35:16 GMT"}, {"version": "v2", "created": "Thu, 12 May 2016 02:07:59 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Achakulvisut", "Titipat", ""], ["Acuna", "Daniel E.", ""], ["Ruangrong", "Tulakan", ""], ["Kording", "Konrad", ""]]}, {"id": "1604.01131", "submitter": "Khushnood Abbas", "authors": "Khushnood Abbas, Shang Mingsheng and Luo Xin", "title": "Discovering items with potential popularity on social media", "comments": "7 pages in ACM style.7 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future popularity of online content is highly important in\nmany applications. Preferential attachment phenomena is encountered in scale\nfree networks.Under it's influece popular items get more popular thereby\nresulting in long tailed distribution problem. Consequently, new items which\ncan be popular (potential ones), are suppressed by the already popular items.\nThis paper proposes a novel model which is able to identify potential items. It\nidentifies the potentially popular items by considering the number of links or\nratings it has recieved in recent past along with it's popularity decay. For\nobtaining an effecient model we consider only temporal features of the content,\navoiding the cost of extracting other features. We have found that people\nfollow recent behaviours of their peers. In presence of fit or quality items\nalready popular items lose it's popularity. Prediction accuracy is measured on\nthree industrial datasets namely Movielens, Netflix and Facebook wall post.\nExperimental results show that compare to state-of-the-art model our model have\nbetter prediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 04:27:22 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Abbas", "Khushnood", ""], ["Mingsheng", "Shang", ""], ["Xin", "Luo", ""]]}, {"id": "1604.01170", "submitter": "Antonia Godoy-Lorite", "authors": "Antonia Godoy-Lorite, Roger Guimera, Cristopher Moore, Marta\n  Sales-Pardo", "title": "Accurate and scalable social recommendation using mixed-membership\n  stochastic block models", "comments": "9 pages, 4 figures", "journal-ref": "Proc. Natl. Acad. Sci. USA 113 (50) , 14207 -14212 (2016)", "doi": "10.1073/pnas.1606316113", "report-no": null, "categories": "cs.SI cs.IR cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With ever-increasing amounts of online information available, modeling and\npredicting individual preferences-for books or articles, for example-is\nbecoming more and more important. Good predictions enable us to improve advice\nto users, and obtain a better understanding of the socio-psychological\nprocesses that determine those preferences. We have developed a collaborative\nfiltering model, with an associated scalable algorithm, that makes accurate\npredictions of individuals' preferences. Our approach is based on the explicit\nassumption that there are groups of individuals and of items, and that the\npreferences of an individual for an item are determined only by their group\nmemberships. Importantly, we allow each individual and each item to belong\nsimultaneously to mixtures of different groups and, unlike many popular\napproaches, such as matrix factorization, we do not assume implicitly or\nexplicitly that individuals in each group prefer items in a single group of\nitems. The resulting overlapping groups and the predicted preferences can be\ninferred with a expectation-maximization algorithm whose running time scales\nlinearly (per iteration). Our approach enables us to predict individual\npreferences in large datasets, and is considerably more accurate than the\ncurrent algorithms for such large datasets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 08:28:08 GMT"}, {"version": "v2", "created": "Wed, 6 Apr 2016 07:55:35 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Godoy-Lorite", "Antonia", ""], ["Guimera", "Roger", ""], ["Moore", "Cristopher", ""], ["Sales-Pardo", "Marta", ""]]}, {"id": "1604.01272", "submitter": "Despoina Christou", "authors": "Despoina Christou", "title": "Feature extraction using Latent Dirichlet Allocation and Neural\n  Networks: A case study on movie synopses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature extraction has gained increasing attention in the field of machine\nlearning, as in order to detect patterns, extract information, or predict\nfuture observations from big data, the urge of informative features is crucial.\nThe process of extracting features is highly linked to dimensionality reduction\nas it implies the transformation of the data from a sparse high-dimensional\nspace, to higher level meaningful abstractions. This dissertation employs\nNeural Networks for distributed paragraph representations, and Latent Dirichlet\nAllocation to capture higher level features of paragraph vectors. Although\nNeural Networks for distributed paragraph representations are considered the\nstate of the art for extracting paragraph vectors, we show that a quick topic\nanalysis model such as Latent Dirichlet Allocation can provide meaningful\nfeatures too. We evaluate the two methods on the CMU Movie Summary Corpus, a\ncollection of 25,203 movie plot summaries extracted from Wikipedia. Finally,\nfor both approaches, we use K-Nearest Neighbors to discover similar movies, and\nplot the projected representations using T-Distributed Stochastic Neighbor\nEmbedding to depict the context similarities. These similarities, expressed as\nmovie distances, can be used for movies recommendation. The recommended movies\nof this approach are compared with the recommended movies from IMDB, which use\na collaborative filtering recommendation approach, to show that our two models\ncould constitute either an alternative or a supplementary recommendation\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 14:32:48 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Christou", "Despoina", ""]]}, {"id": "1604.01840", "submitter": "Mack Sweeney", "authors": "Mack Sweeney, Huzefa Rangwala, Jaime Lester, Aditya Johri", "title": "Next-Term Student Performance Prediction: A Recommender Systems Approach", "comments": "27 pages, 5 figures, submitted to Journal of Educational Data Mining\n  (JEDM)", "journal-ref": "https://jedm.educationaldatamining.org/index.php/JEDM/article/view/JEDM2016-8-1-3", "doi": "10.5281/zenodo.3554603", "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An enduring issue in higher education is student retention to successful\ngraduation. National statistics indicate that most higher education\ninstitutions have four-year degree completion rates around 50 percent, or just\nhalf of their student populations. While there are prediction models which\nilluminate what factors assist with college student success, interventions that\nsupport course selections on a semester-to-semester basis have yet to be deeply\nunderstood. To further this goal, we develop a system to predict students'\ngrades in the courses they will enroll in during the next enrollment term by\nlearning patterns from historical transcript data coupled with additional\ninformation about students, courses and the instructors teaching them.\n  We explore a variety of classic and state-of-the-art techniques which have\nproven effective for recommendation tasks in the e-commerce domain. In our\nexperiments, Factorization Machines (FM), Random Forests (RF), and the\nPersonalized Multi-Linear Regression model achieve the lowest prediction error.\nApplication of a novel feature selection technique is key to the predictive\nsuccess and interpretability of the FM. By comparing feature importance across\npopulations and across models, we uncover strong connections between instructor\ncharacteristics and student performance. We also discover key differences\nbetween transfer and non-transfer students. Ultimately we find that a hybrid\nFM-RF method can be used to accurately predict grades for both new and\nreturning students taking both new and existing courses. Application of these\ntechniques holds promise for student degree planning, instructor interventions,\nand personalized advising, all of which could improve retention and academic\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 01:05:18 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Sweeney", "Mack", ""], ["Rangwala", "Huzefa", ""], ["Lester", "Jaime", ""], ["Johri", "Aditya", ""]]}, {"id": "1604.02038", "submitter": "Fei Tian", "authors": "Fei Tian, Bin Gao, Di He, Tie-Yan Liu", "title": "Sentence Level Recurrent Topic Model: Letting Topics Speak for\n  Themselves", "comments": "The submitted version was done in Feb.2016. Still in improvement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Sentence Level Recurrent Topic Model (SLRTM), a new topic model\nthat assumes the generation of each word within a sentence to depend on both\nthe topic of the sentence and the whole history of its preceding words in the\nsentence. Different from conventional topic models that largely ignore the\nsequential order of words or their topic coherence, SLRTM gives full\ncharacterization to them by using a Recurrent Neural Networks (RNN) based\nframework. Experimental results have shown that SLRTM outperforms several\nstrong baselines on various tasks. Furthermore, SLRTM can automatically\ngenerate sentences given a topic (i.e., topics to sentences), which is a key\ntechnology for real world applications such as personalized short text\nconversation.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 15:29:45 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 05:45:44 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Tian", "Fei", ""], ["Gao", "Bin", ""], ["He", "Di", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1604.02071", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel, Michail Vlachos, Thomas Parnell, and Celestine\n  D\\\"unner", "title": "Scalable and interpretable product recommendations via overlapping\n  co-clustering", "comments": "In IEEE International Conference on Data Engineering (ICDE) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of generating interpretable recommendations by\nidentifying overlapping co-clusters of clients and products, based only on\npositive or implicit feedback. Our approach is applicable on very large\ndatasets because it exhibits almost linear complexity in the input examples and\nthe number of co-clusters. We show, both on real industrial data and on\npublicly available datasets, that the recommendation accuracy of our algorithm\nis competitive to that of state-of-art matrix factorization techniques. In\naddition, our technique has the advantage of offering recommendations that are\ntextually and visually interpretable. Finally, we examine how to implement our\ntechnique efficiently on Graphical Processing Units (GPUs).\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 16:40:53 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 17:58:51 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Heckel", "Reinhard", ""], ["Vlachos", "Michail", ""], ["Parnell", "Thomas", ""], ["D\u00fcnner", "Celestine", ""]]}, {"id": "1604.02546", "submitter": "Lorenzo Baraldi", "authors": "Lorenzo Baraldi, Costantino Grana and Rita Cucchiara", "title": "Scene-driven Retrieval in Edited Videos using Aesthetic and Semantic\n  Deep Features", "comments": "ICMR 2016", "journal-ref": null, "doi": "10.1145/2911996.2912012", "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel retrieval pipeline for video collections, which\naims to retrieve the most significant parts of an edited video for a given\nquery, and represent them with thumbnails which are at the same time\nsemantically meaningful and aesthetically remarkable. Videos are first\nsegmented into coherent and story-telling scenes, then a retrieval algorithm\nbased on deep learning is proposed to retrieve the most significant scenes for\na textual query. A ranking strategy based on deep features is finally used to\ntackle the problem of visualizing the best thumbnail. Qualitative and\nquantitative experiments are conducted on a collection of edited videos to\ndemonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 09:41:14 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Baraldi", "Lorenzo", ""], ["Grana", "Costantino", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1604.02580", "submitter": "Iana Atanassova", "authors": "Iana Atanassova, Marc Bertin and Vincent Larivi\\`ere", "title": "On the Composition of Scientific Abstracts", "comments": "Preprint: Journal of Documentation, vol. 72, issue 4. Submitted\n  12-Sep-2015, accepted 10-Feb-2016. 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific abstracts contain what is considered by the author(s) as\ninformation that best describe documents' content. They represent a compressed\nview of the informational content of a document and allow readers to evaluate\nthe relevance of the document to a particular information need. However, little\nis known on their composition. This paper contributes to the understanding of\nthe structure of abstracts, by comparing similarity between scientific\nabstracts and the text content of research articles. More specifically, using\nsentence-based similarity metrics, we quantify the phenomenon of text re-use in\nabstracts and examine the positions of the sentences that are similar to\nsentences in abstracts in the IMRaD structure (Introduction, Methods, Results\nand Discussion), using a corpus of over 85,000 research articles published in\nthe seven PLOS journals. We provide evidence that 84% of abstract have at least\none sentence in common with the body of the article. Our results also show that\nthe sections of the paper from which abstract sentence are taken are invariant\nacross the PLOS journals, with sentences mainly coming from the beginning of\nthe introduction and the end of the conclusion.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 17:20:14 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Atanassova", "Iana", ""], ["Bertin", "Marc", ""], ["Larivi\u00e8re", "Vincent", ""]]}, {"id": "1604.03147", "submitter": "Bita Shams", "authors": "Bita Shams, Saman Haratizadeh", "title": "Graph-based Collaborative Ranking", "comments": null, "journal-ref": "Expert Systems with Applications 67(2017), 59-70", "doi": "10.1016/j.eswa.2016.09.013", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sparsity, that is a common problem in neighbor-based collaborative\nfiltering domain, usually complicates the process of item recommendation. This\nproblem is more serious in collaborative ranking domain, in which calculating\nthe users similarities and recommending items are based on ranking data. Some\ngraph-based approaches have been proposed to address the data sparsity problem,\nbut they suffer from two flaws. First, they fail to correctly model the users\npriorities, and second, they cannot be used when the only available data is a\nset of ranking instead of rating values. In this paper, we propose a novel\ngraph-based approach, called GRank, that is designed for collaborative ranking\ndomain. GRank can correctly model users priorities in a new tripartite graph\nstructure, and analyze it to directly infer a recommendation list. The\nexperimental results show a significant improvement in recommendation quality\ncompared to the state of the art graph-based recommendation algorithms and\nother collaborative ranking techniques.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 21:05:16 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 22:14:19 GMT"}, {"version": "v3", "created": "Tue, 31 Jan 2017 09:19:42 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Shams", "Bita", ""], ["Haratizadeh", "Saman", ""]]}, {"id": "1604.03506", "submitter": "Liangjie Hong", "authors": "Liangjie Hong, Adnan Boz", "title": "An Unbiased Data Collection and Content Exploitation/Exploration\n  Strategy for Personalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of missions for personalization systems and recommender systems is to\nshow content items according to users' personal interests. In order to achieve\nsuch goal, these systems are learning user interests over time and trying to\npresent content items tailoring to user profiles. Recommending items according\nto users' preferences has been investigated extensively in the past few years,\nmainly thanks for the popularity of Netflix competition. In a real setting,\nusers may be attracted by a subset of those items and interact with them, only\nleaving partial feedbacks to the system to learn in the next cycle, which leads\nto significant biases into systems and hence results in a situation where user\nengagement metrics cannot be improved over time. The problem is not just for\none component of the system. The data collected from users is usually used in\nmany different tasks, including learning ranking functions, building user\nprofiles and constructing content classifiers. Once the data is biased, all\nthese downstream use cases would be impacted as well. Therefore, it would be\nbeneficial to gather unbiased data through user interactions. Traditionally,\nunbiased data collection is done through showing items uniformly sampling from\nthe content pool. However, this simple scheme is not feasible as it risks user\nengagement metrics and it takes long time to gather user feedbacks. In this\npaper, we introduce a user-friendly unbiased data collection framework, by\nutilizing methods developed in the exploitation and exploration literature. We\ndiscuss how the framework is different from normal multi-armed bandit problems\nand why such method is needed. We layout a novel Thompson sampling for\nBernoulli ranked-list to effectively balance user experiences and data\ncollection. The proposed method is validated from a real bucket test and we\nshow strong results comparing to old algorithms\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 18:32:43 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Hong", "Liangjie", ""], ["Boz", "Adnan", ""]]}, {"id": "1604.03757", "submitter": "Saber Shokat Fadaee", "authors": "Saber Shokat Fadaee, Mohammad Sajjad Ghaemi, Ravi Sundaram, Hossein\n  Azari Soufiani", "title": "Chiron: A Robust Recommendation System with Graph Regularizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems have been widely used by commercial service providers\nfor giving suggestions to users. Collaborative filtering (CF) systems, one of\nthe most popular recommendation systems, utilize the history of behaviors of\nthe aggregate user-base to provide individual recommendations and are effective\nwhen almost all users faithfully express their opinions. However, they are\nvulnerable to malicious users biasing their inputs in order to change the\noverall ratings of a specific group of items. CF systems largely fall into two\ncategories - neighborhood-based and (matrix) factorization-based - and the\npresence of adversarial input can influence recommendations in both categories,\nleading to instabilities in estimation and prediction. Although the robustness\nof different collaborative filtering algorithms has been extensively studied,\ndesigning an efficient system that is immune to manipulation remains a\nsignificant challenge. In this work we propose a novel \"hybrid\" recommendation\nsystem with an adaptive graph-based user/item similarity-regularization -\n\"Chiron\". Chiron ties the performance benefits of dimensionality reduction\n(through factorization) with the advantage of neighborhood clustering (through\nregularization). We demonstrate, using extensive comparative experiments, that\nChiron is resistant to manipulation by large and lethal attacks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 13:16:44 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 20:48:15 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Fadaee", "Saber Shokat", ""], ["Ghaemi", "Mohammad Sajjad", ""], ["Sundaram", "Ravi", ""], ["Soufiani", "Hossein Azari", ""]]}, {"id": "1604.04007", "submitter": "Haibing Wu", "authors": "Haibing Wu, Xiaodong Gu", "title": "Balancing Between Over-Weighting and Under-Weighting in Supervised Term\n  Weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised term weighting could improve the performance of text\ncategorization. A way proven to be effective is to give more weight to terms\nwith more imbalanced distributions across categories. This paper shows that\nsupervised term weighting should not just assign large weights to imbalanced\nterms, but should also control the trade-off between over-weighting and\nunder-weighting. Over-weighting, a new concept proposed in this paper, is\ncaused by the improper handling of singular terms and too large ratios between\nterm weights. To prevent over-weighting, we present three regularization\ntechniques: add-one smoothing, sublinear scaling and bias term. Add-one\nsmoothing is used to handle singular terms. Sublinear scaling and bias term\nshrink the ratios between term weights. However, if sublinear functions scale\ndown term weights too much, or the bias term is too large, under-weighting\nwould occur and harm the performance. It is therefore critical to balance\nbetween over-weighting and under-weighting. Inspired by this insight, we also\npropose a new supervised term weighting scheme, regularized entropy (re). Our\nre employs entropy to measure term distribution, and introduces the bias term\nto control over-weighting and under-weighting. Empirical evaluations on topical\nand sentiment classification datasets indicate that sublinear scaling and bias\nterm greatly influence the performance of supervised term weighting, and our re\nenjoys the best results in comparison with existing schemes.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 01:29:52 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Wu", "Haibing", ""], ["Gu", "Xiaodong", ""]]}, {"id": "1604.04142", "submitter": "Claudio Gennaro", "authors": "Giuseppe Amato, Fabrizio Falchi, Claudio Gennaro", "title": "On Reducing the Number of Visual Words in the Bag-of-Features\n  Representation", "comments": null, "journal-ref": "VISAPP (1). 2013. p. 657-662", "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of applications based on visual search engines are emerging,\nespecially on smart-phones that have evolved into powerful tools for processing\nimages and videos. The state-of-the-art algorithms for large visual content\nrecognition and content based similarity search today use the \"Bag of Features\"\n(BoF) or \"Bag of Words\" (BoW) approach. The idea, borrowed from text retrieval,\nenables the use of inverted files. A very well known issue with this approach\nis that the query images, as well as the stored data, are described with\nthousands of words. This poses obvious efficiency problems when using inverted\nfiles to perform efficient image matching. In this paper, we propose and\ncompare various techniques to reduce the number of words describing an image to\nimprove efficiency and we study the effects of this reduction on effectiveness\nin landmark recognition and retrieval scenarios. We show that very relevant\nimprovement in performance are achievable still preserving the advantages of\nthe BoF base approach.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 13:08:57 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Amato", "Giuseppe", ""], ["Falchi", "Fabrizio", ""], ["Gennaro", "Claudio", ""]]}, {"id": "1604.04358", "submitter": "Lili Mou", "authors": "Xiang Li, Lili Mou, Rui Yan, Ming Zhang", "title": "StalemateBreaker: A Proactive Content-Introducing Approach to Automatic\n  Human-Computer Conversation", "comments": "Accepted by IJCAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing open-domain human-computer conversation systems are typically\npassive: they either synthesize or retrieve a reply provided a human-issued\nutterance. It is generally presumed that humans should take the role to lead\nthe conversation and introduce new content when a stalemate occurs, and that\nthe computer only needs to \"respond.\" In this paper, we propose\nStalemateBreaker, a conversation system that can proactively introduce new\ncontent when appropriate. We design a pipeline to determine when, what, and how\nto introduce new content during human-computer conversation. We further propose\na novel reranking algorithm Bi-PageRank-HITS to enable rich interaction between\nconversation context and candidate replies. Experiments show that both the\ncontent-introducing approach and the reranking algorithm are effective. Our\nfull StalemateBreaker model outperforms a state-of-the-practice conversation\nsystem by +14.4% p@1 when a stalemate occurs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 05:51:29 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Li", "Xiang", ""], ["Mou", "Lili", ""], ["Yan", "Rui", ""], ["Zhang", "Ming", ""]]}, {"id": "1604.04558", "submitter": "Jyothi Korra", "authors": "Jinju Joby and Jyothi Korra", "title": "Accessing accurate documents by mining auxiliary document information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Earlier techniques of text mining included algorithms like k-means, Naive\nBayes, SVM which classify and cluster the text document for mining relevant\ninformation about the documents. The need for improving the mining techniques\nhas us searching for techniques using the available algorithms. This paper\nproposes one technique which uses the auxiliary information that is present\ninside the text documents to improve the mining. This auxiliary information can\nbe a description to the content. This information can be either useful or\ncompletely useless for mining. The user should assess the worth of the\nauxiliary information before considering this technique for text mining. In\nthis paper, a combination of classical clustering algorithms is used to mine\nthe datasets. The algorithm runs in two stages which carry out mining at\ndifferent levels of abstraction. The clustered documents would then be\nclassified based on the necessary groups. The proposed technique is aimed at\nimproved results of document clustering.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 16:27:38 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Joby", "Jinju", ""], ["Korra", "Jyothi", ""]]}, {"id": "1604.05005", "submitter": "Sujatha Das Gollapalli", "authors": "Sujatha Das Gollapalli and Krutarth Patel and Cornelia Caragea", "title": "A Search/Crawl Framework for Automatically Acquiring Scientific\n  Documents", "comments": "8 pages with references, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the advancements in search engine features, ranking methods,\ntechnologies, and the availability of programmable APIs, current-day\nopen-access digital libraries still rely on crawl-based approaches for\nacquiring their underlying document collections. In this paper, we propose a\nnovel search-driven framework for acquiring documents for scientific portals.\nWithin our framework, publicly-available research paper titles and author names\nare used as queries to a Web search engine. Next, research papers and sources\nof research papers are identified from the search results using accurate\nclassification modules. Our experiments highlight not only the performance of\nour individual classifiers but also the effectiveness of our overall\nSearch/Crawl framework. Indeed, we were able to obtain approximately 0.665\nmillion research documents through our fully-automated framework using about\n0.076 million queries. These prolific results position Web search as an\neffective alternative to crawl methods for acquiring both the actual documents\nand seed URLs for future crawls.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 06:09:07 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Gollapalli", "Sujatha Das", ""], ["Patel", "Krutarth", ""], ["Caragea", "Cornelia", ""]]}, {"id": "1604.05462", "submitter": "Shuai Ma", "authors": "Dongsheng Luo, Chen Gong, Renjun Hu, Liang Duan, Shuai Ma", "title": "Ensemble Enabled Weighted PageRank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our solution for WSDM Cup 2016. Ranking the query\nindependent importance of scholarly articles is a critical and challenging\ntask, due to the heterogeneity and dynamism of entities involved. Our approach\nis called Ensemble enabled Weighted PageRank (EWPR). To do this, we first\npropose Time-Weighted PageRank that extends PageRank by introducing a time\ndecaying factor. We then develop an ensemble method to assemble the authorities\nof the heterogeneous entities involved in scholarly articles. We finally\npropose to use external data sources to further improve the ranking accuracy.\nOur experimental study shows that our EWPR is a good choice for ranking\nscholarly articles.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 08:00:08 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Luo", "Dongsheng", ""], ["Gong", "Chen", ""], ["Hu", "Renjun", ""], ["Duan", "Liang", ""], ["Ma", "Shuai", ""]]}, {"id": "1604.05468", "submitter": "Rahul Kamath", "authors": "Rahul Kamath, Masanao Ochi, Yutaka Matsuo", "title": "Understanding Rating Behaviour and Predicting Ratings by Identifying\n  Representative Users", "comments": "The 29th Pacific Asia Conference on Language, Information and\n  Computation (PACLIC-29)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online user reviews describing various products and services are now abundant\non the web. While the information conveyed through review texts and ratings is\neasily comprehensible, there is a wealth of hidden information in them that is\nnot immediately obvious. In this study, we unlock this hidden value behind user\nreviews to understand the various dimensions along which users rate products.\nWe learn a set of users that represent each of these dimensions and use their\nratings to predict product ratings. Specifically, we work with restaurant\nreviews to identify users whose ratings are influenced by dimensions like\n'Service', 'Atmosphere' etc. in order to predict restaurant ratings and\nunderstand the variation in rating behaviour across different cuisines. While\nprevious approaches to obtaining product ratings require either a large number\nof user ratings or a few review texts, we show that it is possible to predict\nratings with few user ratings and no review text. Our experiments show that our\napproach outperforms other conventional methods by 16-27% in terms of RMSE.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 08:31:23 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Kamath", "Rahul", ""], ["Ochi", "Masanao", ""], ["Matsuo", "Yutaka", ""]]}, {"id": "1604.05492", "submitter": "Geoffroy Fouquier", "authors": "Guillaume Pitel, Geoffroy Fouquier, Emmanuel Marchand and Abdul\n  Mouhamadsultane", "title": "Count-Min Tree Sketch: Approximate counting for NLP", "comments": "submitted to the second International Symposium on Web Algorithms\n  (iSwag'2016). arXiv admin note: text overlap with arXiv:1502.04885, In the\n  proceedings of the Second International Symposium on Web Algorithms (iSWAG\n  2016), June 9-10, 2016, Deauville, Normandy, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Count-Min Sketch is a widely adopted structure for approximate event\ncounting in large scale processing. In a previous work we improved the original\nversion of the Count-Min-Sketch (CMS) with conservative update using\napproximate counters instead of linear counters. These structures are\ncomputationaly efficient and improve the average relative error (ARE) of a CMS\nat constant memory footprint. These improvements are well suited for NLP tasks,\nin which one is interested by the low-frequency items. However, if Log counters\nallow to improve ARE, they produce a residual error due to the approximation.\nIn this paper, we propose the Count-Min Tree Sketch (Copyright 2016 eXenSa. All\nrights reserved) variant with pyramidal counters, which are focused toward\ntaking advantage of the Zipfian distribution of text data.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 09:51:34 GMT"}, {"version": "v2", "created": "Thu, 21 Apr 2016 09:44:51 GMT"}, {"version": "v3", "created": "Wed, 15 Jun 2016 06:15:34 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Pitel", "Guillaume", ""], ["Fouquier", "Geoffroy", ""], ["Marchand", "Emmanuel", ""], ["Mouhamadsultane", "Abdul", ""]]}, {"id": "1604.05576", "submitter": "Claudio Gennaro", "authors": "Giuseppe Amato, Paolo Bolettieri, Fabrizio Falchi, Claudio Gennaro,\n  Lucia Vadicamo", "title": "Using Apache Lucene to Search Vector of Locally Aggregated Descriptors", "comments": "In Proceedings of the 11th Joint Conference on Computer Vision,\n  Imaging and Computer Graphics Theory and Applications (VISIGRAPP 2016) -\n  Volume 4: VISAPP, p. 383-392", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surrogate Text Representation (STR) is a profitable solution to efficient\nsimilarity search on metric space using conventional text search engines, such\nas Apache Lucene. This technique is based on comparing the permutations of some\nreference objects in place of the original metric distance. However, the\nAchilles heel of STR approach is the need to reorder the result set of the\nsearch according to the metric distance. This forces to use a support database\nto store the original objects, which requires efficient random I/O on a fast\nsecondary memory (such as flash-based storages). In this paper, we propose to\nextend the Surrogate Text Representation to specifically address a class of\nvisual metric objects known as Vector of Locally Aggregated Descriptors (VLAD).\nThis approach is based on representing the individual sub-vectors forming the\nVLAD vector with the STR, providing a finer representation of the vector and\nenabling us to get rid of the reordering phase. The experiments on a publicly\navailable dataset show that the extended STR outperforms the baseline STR\nachieving satisfactory performance near to the one obtained with the original\nVLAD vectors.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 14:08:34 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Amato", "Giuseppe", ""], ["Bolettieri", "Paolo", ""], ["Falchi", "Fabrizio", ""], ["Gennaro", "Claudio", ""], ["Vadicamo", "Lucia", ""]]}, {"id": "1604.05754", "submitter": "Kalpa Gunaratna", "authors": "Kalpa Gunaratna", "title": "Document Retrieval using Predication Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document retrieval has been an important research problem over many years in\nthe information retrieval community. State-of-the-art techniques utilize\nvarious methods in matching documents to a given document including keywords,\nphrases, and annotations. In this paper, we propose a new approach for document\nretrieval that utilizes predications (subject-predicate-object triples)\nextracted from the documents. We represent documents as sets of predications.\nWe measure the similarity between predications to compute the similarity\nbetween documents. Our approach utilizes the hierarchical information available\nin ontologies in computing concept-concept similarity, making the approach\nflexible. Predication-based document similarity is more precise and forms the\nbasis for a semantically aware document retrieval system. We show that the\napproach is competitive with an existing state-of-the-art related document\nretrieval technique in the biomedical domain.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2016 21:23:23 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Gunaratna", "Kalpa", ""]]}, {"id": "1604.05813", "submitter": "Ruining He", "authors": "Ruining He, Chunbin Lin, Jianguo Wang, Julian McAuley", "title": "Sherlock: Sparse Hierarchical Embeddings for Visually-aware One-class\n  Collaborative Filtering", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building successful recommender systems requires uncovering the underlying\ndimensions that describe the properties of items as well as users' preferences\ntoward them. In domains like clothing recommendation, explaining users'\npreferences requires modeling the visual appearance of the items in question.\nThis makes recommendation especially challenging, due to both the complexity\nand subtlety of people's 'visual preferences,' as well as the scale and\ndimensionality of the data and features involved. Ultimately, a successful\nmodel should be capable of capturing considerable variance across different\ncategories and styles, while still modeling the commonalities explained by\n`global' structures in order to combat the sparsity (e.g. cold-start),\nvariability, and scale of real-world datasets. Here, we address these\nchallenges by building such structures to model the visual dimensions across\ndifferent product categories. With a novel hierarchical embedding architecture,\nour method accounts for both high-level (colorfulness, darkness, etc.) and\nsubtle (e.g. casualness) visual characteristics simultaneously.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 04:36:57 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["He", "Ruining", ""], ["Lin", "Chunbin", ""], ["Wang", "Jianguo", ""], ["McAuley", "Julian", ""]]}, {"id": "1604.05875", "submitter": "Tiep Mai", "authors": "Tiep Mai, Bichen Shi, Patrick K. Nicholson, Deepak Ajwani, Alessandra\n  Sala", "title": "Distributed Entity Disambiguation with Per-Mention Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity disambiguation, or mapping a phrase to its canonical representation in\na knowledge base, is a fundamental step in many natural language processing\napplications. Existing techniques based on global ranking models fail to\ncapture the individual peculiarities of the words and hence, either struggle to\nmeet the accuracy requirements of many real-world applications or they are too\ncomplex to satisfy real-time constraints of applications.\n  In this paper, we propose a new disambiguation system that learns specialized\nfeatures and models for disambiguating each ambiguous phrase in the English\nlanguage. To train and validate the hundreds of thousands of learning models\nfor this purpose, we use a Wikipedia hyperlink dataset with more than 170\nmillion labelled annotations. We provide an extensive experimental evaluation\nto show that the accuracy of our approach compares favourably with respect to\nmany state-of-the-art disambiguation systems. The training required for our\napproach can be easily distributed over a cluster. Furthermore, updating our\nsystem for new entities or calibrating it for special ones is a computationally\nfast process, that does not affect the disambiguation of the other entities.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 09:53:42 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Mai", "Tiep", ""], ["Shi", "Bichen", ""], ["Nicholson", "Patrick K.", ""], ["Ajwani", "Deepak", ""], ["Sala", "Alessandra", ""]]}, {"id": "1604.06194", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin, Kush R. Varshney, and Liu Yang", "title": "Dynamic matrix factorization with social influence", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.SI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization is a key component of collaborative filtering-based\nrecommendation systems because it allows us to complete sparse user-by-item\nratings matrices under a low-rank assumption that encodes the belief that\nsimilar users give similar ratings and that similar items garner similar\nratings. This paradigm has had immeasurable practical success, but it is not\nthe complete story for understanding and inferring the preferences of people.\nFirst, peoples' preferences and their observable manifestations as ratings\nevolve over time along general patterns of trajectories. Second, an individual\nperson's preferences evolve over time through influence of their social\nconnections. In this paper, we develop a unified process model for both types\nof dynamics within a state space approach, together with an efficient\noptimization scheme for estimation within that model. The model combines\nelements from recent developments in dynamic matrix factorization, opinion\ndynamics and social learning, and trust-based recommendation. The estimation\nbuilds upon recent advances in numerical nonlinear optimization. Empirical\nresults on a large-scale data set from the Epinions website demonstrate\nconsistent reduction in root mean squared error by consideration of the two\ntypes of dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 06:51:22 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Varshney", "Kush R.", ""], ["Yang", "Liu", ""]]}, {"id": "1604.06225", "submitter": "Ido Kissos", "authors": "Ido Kissos, Nachum Dershowitz", "title": "OCR Error Correction Using Character Correction and Feature-Based Word\n  Classification", "comments": "Proceedings of the 12th IAPR International Workshop on Document\n  Analysis Systems (DAS2016), Santorini, Greece, April 11-14, 2016", "journal-ref": "Proceedings of the 12th IAPR International Workshop on Document\n  Analysis Systems (DAS 2016), Santorini, Greece, pp. 198-203 (2016)", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of a learned classifier for post-OCR text\ncorrection. Experiments with the Arabic language show that this approach, which\nintegrates a weighted confusion matrix and a shallow language model, improves\nthe vast majority of segmentation and recognition errors, the most frequent\ntypes of error on our dataset.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 09:25:11 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Kissos", "Ido", ""], ["Dershowitz", "Nachum", ""]]}, {"id": "1604.06270", "submitter": "Shuxin Wang", "authors": "Shuxin Wang, Xin Jiang, Hang Li, Jun Xu and Bin Wang", "title": "Incorporating Semantic Knowledge into Latent Matching Model in Search", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relevance between a query and a document in search can be represented as\nmatching degree between the two objects. Latent space models have been proven\nto be effective for the task, which are often trained with click-through data.\nOne technical challenge with the approach is that it is hard to train a model\nfor tail queries and tail documents for which there are not enough clicks. In\nthis paper, we propose to address the challenge by learning a latent matching\nmodel, using not only click-through data but also semantic knowledge. The\nsemantic knowledge can be categories of queries and documents as well as\nsynonyms of words, manually or automatically created. Specifically, we\nincorporate semantic knowledge into the objective function by including\nregularization terms. We develop two methods to solve the learning task on the\nbasis of coordinate descent and gradient descent respectively, which can be\nemployed in different settings. Experimental results on two datasets from an\napp search engine demonstrate that our model can make effective use of semantic\nknowledge, and thus can significantly enhance the accuracies of latent matching\nmodels, particularly for tail queries.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 12:17:42 GMT"}], "update_date": "2016-04-22", "authors_parsed": [["Wang", "Shuxin", ""], ["Jiang", "Xin", ""], ["Li", "Hang", ""], ["Xu", "Jun", ""], ["Wang", "Bin", ""]]}, {"id": "1604.06480", "submitter": "Yannis Kalantidis", "authors": "Yannis Kalantidis, Lyndon Kennedy, Huy Nguyen, Clayton Mellina, David\n  A. Shamma", "title": "LOH and behold: Web-scale visual search, recommendation and clustering\n  using Locally Optimized Hashing", "comments": "Accepted for publication at the 4th Workshop on Web-scale Vision and\n  Social Media (VSM), ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel hashing-based matching scheme, called Locally Optimized\nHashing (LOH), based on a state-of-the-art quantization algorithm that can be\nused for efficient, large-scale search, recommendation, clustering, and\ndeduplication. We show that matching with LOH only requires set intersections\nand summations to compute and so is easily implemented in generic distributed\ncomputing systems. We further show application of LOH to: a) large-scale search\ntasks where performance is on par with other state-of-the-art hashing\napproaches; b) large-scale recommendation where queries consisting of thousands\nof images can be used to generate accurate recommendations from collections of\nhundreds of millions of images; and c) efficient clustering with a graph-based\nalgorithm that can be scaled to massive collections in a distributed\nenvironment or can be used for deduplication for small collections, like search\nresults, performing better than traditional hashing approaches while only\nrequiring a few milliseconds to run. In this paper we experiment on datasets of\nup to 100 million images, but in practice our system can scale to larger\ncollections and can be used for other types of data that have a vector\nrepresentation in a Euclidean space.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 20:23:55 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 02:34:52 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Kalantidis", "Yannis", ""], ["Kennedy", "Lyndon", ""], ["Nguyen", "Huy", ""], ["Mellina", "Clayton", ""], ["Shamma", "David A.", ""]]}, {"id": "1604.07044", "submitter": "Xianming Liu", "authors": "Xianming Liu, Min-Hsuan Tsai, Thomas Huang", "title": "Analyzing User Preference for Social Image Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the incredibly growing amount of multimedia data shared on the social\nmedia platforms, recommender systems have become an important necessity to ease\nusers' burden on the information overload. In such a scenario, extensive amount\nof heterogeneous information such as tags, image content, in addition to the\nuser-to-item preferences, is extremely valuable for making effective\nrecommendations. In this paper, we explore a novel hybrid algorithm termed {\\em\nSTM}, for image recommendation. STM jointly considers the problem of image\ncontent analysis with the users' preferences on the basis of sparse\nrepresentation. STM is able to tackle the challenges of highly sparse user\nfeedbacks and cold-start problmes in the social network scenario. In addition,\nour model is based on the classical probabilistic matrix factorization and can\nbe easily extended to incorporate other useful information such as the social\nrelationships. We evaluate our approach with a newly collected 0.3 million\nsocial image data set from Flickr. The experimental results demonstrate that\nsparse topic modeling of the image content leads to more effective\nrecommendations, , with a significant performance gain over the\nstate-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2016 15:54:02 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Liu", "Xianming", ""], ["Tsai", "Min-Hsuan", ""], ["Huang", "Thomas", ""]]}, {"id": "1604.07209", "submitter": "Tobias Schnabel", "authors": "Tobias Schnabel, Adith Swaminathan, Peter Frazier, Thorsten Joachims", "title": "Unbiased Comparative Evaluation of Ranking Functions", "comments": "Under review; 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eliciting relevance judgments for ranking evaluation is labor-intensive and\ncostly, motivating careful selection of which documents to judge. Unlike\ntraditional approaches that make this selection deterministically,\nprobabilistic sampling has shown intriguing promise since it enables the design\nof estimators that are provably unbiased even when reusing data with missing\njudgments. In this paper, we first unify and extend these sampling approaches\nby viewing the evaluation problem as a Monte Carlo estimation task that applies\nto a large number of common IR metrics. Drawing on the theoretical clarity that\nthis view offers, we tackle three practical evaluation scenarios: comparing two\nsystems, comparing $k$ systems against a baseline, and ranking $k$ systems. For\neach scenario, we derive an estimator and a variance-optimizing sampling\ndistribution while retaining the strengths of sampling-based evaluation,\nincluding unbiasedness, reusability despite missing data, and ease of use in\npractice. In addition to the theoretical contribution, we empirically evaluate\nour methods against previously used sampling heuristics and find that they\ngenerally cut the number of required relevance judgments at least in half.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 11:28:21 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Schnabel", "Tobias", ""], ["Swaminathan", "Adith", ""], ["Frazier", "Peter", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1604.07236", "submitter": "Arkaitz Zubiaga", "authors": "Arkaitz Zubiaga, Alex Voss, Rob Procter, Maria Liakata, Bo Wang, Adam\n  Tsakalidis", "title": "Towards Real-Time, Country-Level Location Classification of Worldwide\n  Tweets", "comments": "Accepted for publication in IEEE Transactions on Knowledge and Data\n  Engineering (IEEE TKDE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to much previous work that has focused on location classification\nof tweets restricted to a specific country, here we undertake the task in a\nbroader context by classifying global tweets at the country level, which is so\nfar unexplored in a real-time scenario. We analyse the extent to which a\ntweet's country of origin can be determined by making use of eight\ntweet-inherent features for classification. Furthermore, we use two datasets,\ncollected a year apart from each other, to analyse the extent to which a model\ntrained from historical tweets can still be leveraged for classification of new\ntweets. With classification experiments on all 217 countries in our datasets,\nas well as on the top 25 countries, we offer some insights into the best use of\ntweet-inherent features for an accurate country-level classification of tweets.\nWe find that the use of a single feature, such as the use of tweet content\nalone -- the most widely used feature in previous work -- leaves much to be\ndesired. Choosing an appropriate combination of both tweet content and metadata\ncan actually lead to substantial improvements of between 20\\% and 50\\%. We\nobserve that tweet content, the user's self-reported location and the user's\nreal name, all of which are inherent in a tweet and available in a real-time\nscenario, are particularly useful to determine the country of origin. We also\nexperiment on the applicability of a model trained on historical tweets to\nclassify new tweets, finding that the choice of a particular combination of\nfeatures whose utility does not fade over time can actually lead to comparable\nperformance, avoiding the need to retrain. However, the difficulty of achieving\naccurate classification increases slightly for countries with multiple\ncommonalities, especially for English and Spanish speaking countries.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 12:50:50 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 18:35:36 GMT"}, {"version": "v3", "created": "Tue, 25 Apr 2017 11:03:05 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Zubiaga", "Arkaitz", ""], ["Voss", "Alex", ""], ["Procter", "Rob", ""], ["Liakata", "Maria", ""], ["Wang", "Bo", ""], ["Tsakalidis", "Adam", ""]]}, {"id": "1604.07521", "submitter": "Hari Krishna Malladi", "authors": "Hari Krishna Malladi and Saikiran Thunuguntla", "title": "Feedback-based Approach to Introduce Freshness in Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems usually face the problem of serving the same\nrecommendations across multiple sessions regardless of whether the user is\ninterested in them or not, thereby reducing their effectiveness. To add\nfreshness to the recommended products, we introduce a feedback loop where the\nset of recommended products in the current session depend on the user's\ninteraction with the previously recommended sets. We also describe ways of\naddressing freshness when there is little or even no direct user interaction.\nWe define a metric to quantify freshness by reducing the problem to measuring\ntemporal diversity.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2016 05:08:13 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Malladi", "Hari Krishna", ""], ["Thunuguntla", "Saikiran", ""]]}, {"id": "1604.07939", "submitter": "Andre Araujo", "authors": "Andre Araujo, Jason Chaves, Haricharan Lakshman, Roland Angst, Bernd\n  Girod", "title": "Large-Scale Query-by-Image Video Retrieval Using Bloom Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of using image queries to retrieve videos from a\ndatabase. Our focus is on large-scale applications, where it is infeasible to\nindex each database video frame independently. Our main contribution is a\nframework based on Bloom filters, which can be used to index long video\nsegments, enabling efficient image-to-video comparisons. Using this framework,\nwe investigate several retrieval architectures, by considering different types\nof aggregation and different functions to encode visual information -- these\nplay a crucial role in achieving high performance. Extensive experiments show\nthat the proposed technique improves mean average precision by 24% on a public\ndataset, while being 4X faster, compared to the previous state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 05:46:52 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 17:58:16 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Araujo", "Andre", ""], ["Chaves", "Jason", ""], ["Lakshman", "Haricharan", ""], ["Angst", "Roland", ""], ["Girod", "Bernd", ""]]}, {"id": "1604.08402", "submitter": "Wenjie Zheng", "authors": "Wenjie Zheng", "title": "Two Differentially Private Rating Collection Mechanisms for Recommender\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design two mechanisms for the recommender system to collect user ratings.\nOne is modified Laplace mechanism, and the other is randomized response\nmechanism. We prove that they are both differentially private and preserve the\ndata utility.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 13:11:54 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Zheng", "Wenjie", ""]]}, {"id": "1604.08420", "submitter": "Wenjie Zheng", "authors": "Wenjie Zheng", "title": "Matrix Factorization Method for Decentralized Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized recommender system does not rely on the central service\nprovider, and the users can keep the ownership of their ratings. This article\nbrings the theoretically well-studied matrix factorization method into the\ndecentralized recommender system, where the formerly prevalent algorithms are\nheuristic and hence lack of theoretical guarantee. Our preliminary simulation\nresults show that this method is promising.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 14:01:54 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Zheng", "Wenjie", ""]]}, {"id": "1604.08608", "submitter": "Eric Makita", "authors": "Eric Makita, Artem Lenskiy", "title": "A movie genre prediction based on Multivariate Bernoulli model and genre\n  correlations", "comments": "5 pages, 8 figues, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Movie ratings play an important role both in determining the likelihood of a\npotential viewer to watch the movie and in reflecting the current viewer\nsatisfaction with the movie. They are available in several sources like the\ntelevision guide, best-selling reference books, newspaper columns, and\ntelevision programs. Furthermore, movie ratings are crucial for recommendation\nengines that track the behavior of all users and utilize the information to\nsuggest items they might like. Movie ratings in most cases, thus, provide\ninformation that might be more important than movie feature-based data. It is\nintuitively appealing that information about the viewing preferences in movie\ngenres is sufficient for predicting a genre of an unlabeled movie. In order to\npredict movie genres, we treat ratings as a feature vector, apply the Bernoulli\nevent model to estimate the likelihood of a movies given genre, and evaluate\nthe posterior probability of the genre of a given movie using the Bayes rule.\nThe goal of the proposed technique is to efficiently use the movie ratings for\nthe task of predicting movie genres. In our approach we attempted to answer the\nquestion: \"Given the set of users who watched a movie, is it possible to\npredict the genre of a movie based on its ratings?\" Our simulation results with\nMovieLens 100k data demonstrated the efficiency and accuracy of our proposed\ntechnique, achieving 59% prediction rate for exact prediction and 69% when\nincluding correlated genres.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 08:49:51 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Makita", "Eric", ""], ["Lenskiy", "Artem", ""]]}, {"id": "1604.08640", "submitter": "Richard Connor", "authors": "Richard Connor, Franco Alberto Cardillo, Lucia Vadicamo and Fausto\n  Rabitti", "title": "Hilbert Exclusion: Improved Metric Search through Finite Isometric\n  Embeddings", "comments": "41 pages, 19 figures", "journal-ref": "ACM Transactions on Information Systems (TOIS), 35, 3, Article 17\n  (2016)", "doi": "10.1145/3001583", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most research into similarity search in metric spaces relies upon the\ntriangle inequality property. This property allows the space to be arranged\naccording to relative distances to avoid searching some subspaces. We show that\nmany common metric spaces, notably including those using Euclidean and\nJensen-Shannon distances, also have a stronger property, sometimes called the\nfour-point property: in essence, these spaces allow an isometric embedding of\nany four points in three-dimensional Euclidean space, as well as any three\npoints in two-dimensional Euclidean space. In fact, we show that any space\nwhich is isometrically embeddable in Hilbert space has the stronger property.\nThis property gives stronger geometric guarantees, and one in particular, which\nwe name the Hilbert Exclusion property, allows any indexing mechanism which\nuses hyperplane partitioning to perform better. One outcome of this observation\nis that a number of state-of-the-art indexing mechanisms over high dimensional\nspaces can be easily extended to give a significant increase in performance;\nfurthermore, the improvement given is greater in higher dimensions. This\ntherefore leads to a significant improvement in the cost of metric search in\nthese spaces.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 22:30:54 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Connor", "Richard", ""], ["Cardillo", "Franco Alberto", ""], ["Vadicamo", "Lucia", ""], ["Rabitti", "Fausto", ""]]}, {"id": "1604.08816", "submitter": "Massimiliano Zanin", "authors": "M. Zanin, D. Papo, P. A. Sousa, E. Menasalvas, A. Nicchi, E. Kubik, S.\n  Boccaletti", "title": "Combining complex networks and data mining: why and how", "comments": "58 pages, 19 figures", "journal-ref": null, "doi": "10.1016/j.physrep.2016.04.005", "report-no": null, "categories": "physics.soc-ph cs.DB cs.IR cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing power of computer technology does not dispense with the need\nto extract meaningful in- formation out of data sets of ever growing size, and\nindeed typically exacerbates the complexity of this task. To tackle this\ngeneral problem, two methods have emerged, at chronologically different times,\nthat are now commonly used in the scientific community: data mining and complex\nnetwork theory. Not only do complex network analysis and data mining share the\nsame general goal, that of extracting information from complex systems to\nultimately create a new compact quantifiable representation, but they also\noften address similar problems too. In the face of that, a surprisingly low\nnumber of researchers turn out to resort to both methodologies. One may then be\ntempted to conclude that these two fields are either largely redundant or\ntotally antithetic. The starting point of this review is that this state of\naffairs should be put down to contingent rather than conceptual differences,\nand that these two fields can in fact advantageously be used in a synergistic\nmanner. An overview of both fields is first provided, some fundamental concepts\nof which are illustrated. A variety of contexts in which complex network theory\nand data mining have been used in a synergistic manner are then presented.\nContexts in which the appropriate integration of complex network metrics can\nlead to improved classification rates with respect to classical data mining\nalgorithms and, conversely, contexts in which data mining can be used to tackle\nimportant issues in complex network theory applications are illustrated.\nFinally, ways to achieve a tighter integration between complex networks and\ndata mining, and open lines of research are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 13:06:32 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 11:12:05 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Zanin", "M.", ""], ["Papo", "D.", ""], ["Sousa", "P. A.", ""], ["Menasalvas", "E.", ""], ["Nicchi", "A.", ""], ["Kubik", "E.", ""], ["Boccaletti", "S.", ""]]}, {"id": "1604.08897", "submitter": "Antonio Fari\\~na", "authors": "Francisco Claude and Antonio Fari\\~na and Miguel A. Mart\\'inez-Prieto\n  and Gonzalo Navarro", "title": "Universal Indexes for Highly Repetitive Document Collections", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941", "journal-ref": "Information Systems, Volume 61, Pages 1-23, 2016", "doi": "10.1016/j.is.2016.04.002", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing highly repetitive collections has become a relevant problem with the\nemergence of large repositories of versioned documents, among other\napplications. These collections may reach huge sizes, but are formed mostly of\ndocuments that are near-copies of others. Traditional techniques for indexing\nthese collections fail to properly exploit their regularities in order to\nreduce space.\n  We introduce new techniques for compressing inverted indexes that exploit\nthis near-copy regularity. They are based on run-length, Lempel-Ziv, or grammar\ncompression of the differential inverted lists, instead of the usual practice\nof gap-encoding them. We show that, in this highly repetitive setting, our\ncompression methods significantly reduce the space obtained with classical\ntechniques, at the price of moderate slowdowns. Moreover, our best methods are\nuniversal, that is, they do not need to know the versioning structure of the\ncollection, nor that a clear versioning structure even exists.\n  We also introduce compressed self-indexes in the comparison. These are\ndesigned for general strings (not only natural language texts) and represent\nthe text collection plus the index structure (not an inverted index) in\nintegrated form. We show that these techniques can compress much further, using\na small fraction of the space required by our new inverted indexes. Yet, they\nare orders of magnitude slower.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 16:05:49 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 00:36:55 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Claude", "Francisco", ""], ["Fari\u00f1a", "Antonio", ""], ["Mart\u00ednez-Prieto", "Miguel A.", ""], ["Navarro", "Gonzalo", ""]]}]