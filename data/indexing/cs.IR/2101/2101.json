[{"id": "2101.00146", "submitter": "Leibo Liu", "authors": "Leibo Liu, Oscar Perez-Concha, Anthony Nguyen, Vicki Bennett, Louisa\n  Jorm", "title": "De-identifying Hospital Discharge Summaries: An End-to-End Framework\n  using Ensemble of De-Identifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective:Electronic Medical Records (EMRs) contain clinical narrative text\nthat is of great potential value to medical researchers. However, this\ninformation is mixed with Protected Health Information (PHI) that presents\nrisks to patient and clinician confidentiality. This paper presents an\nend-to-end de-identification framework to automatically remove PHI from\nhospital discharge summaries. Materials and Methods:Our corpus included 600\nhospital discharge summaries which were extracted from the EMRs of two\nprincipal referral hospitals in Sydney, Australia. Our end-to-end\nde-identification framework consists of three components: 1) Annotation:\nlabelling of PHI in the 600 hospital discharge summaries using five pre-defined\ncategories: person, address, date of birth, individual identification number,\nphone/fax number; 2) Modelling: training and evaluating ensembles of named\nentity recognition (NER) models through the use of three natural language\nprocessing (NLP) toolkits (Stanza, FLAIR and spaCy) and both balanced and\nimbalanced datasets; and 3) De-identification: removing PHI from the hospital\ndischarge summaries. Results:The final model in our framework was an ensemble\nwhich combined six single models using both balanced and imbalanced datasets\nfor training majority voting. It achieved 0.9866 precision, 0.9862 recall and\n0.9864 F1 scores. The majority of false positives and false negatives were\nrelated to the person category. Discussion:Our study showed that the ensemble\nof different models which were trained using three different NLP toolkits upon\nbalanced and imbalanced datasets can achieve good results even with a\nrelatively small corpus. Conclusion:Our end-to-end framework provides a robust\nsolution to de-identifying clinical narrative corpuses safely. It can be easily\napplied to any kind of clinical narrative documents.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 03:09:31 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liu", "Leibo", ""], ["Perez-Concha", "Oscar", ""], ["Nguyen", "Anthony", ""], ["Bennett", "Vicki", ""], ["Jorm", "Louisa", ""]]}, {"id": "2101.00294", "submitter": "Yuning Mao", "authors": "Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao,\n  Jiawei Han, Weizhu Chen", "title": "Reader-Guided Passage Reranking for Open-Domain Question Answering", "comments": "Findings of ACL 2021 Camera-ready. TLDR: Reranking retrieved passages\n  by reader predictions can achieve 10~20 gains in top-1 retrieval accuracy and\n  1~4 gains in Exact Match (EM) without any training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current open-domain question answering systems often follow a\nRetriever-Reader architecture, where the retriever first retrieves relevant\npassages and the reader then reads the retrieved passages to form an answer. In\nthis paper, we propose a simple and effective passage reranking method, named\nReader-guIDEd Reranker (RIDER), which does not involve training and reranks the\nretrieved passages solely based on the top predictions of the reader before\nreranking. We show that RIDER, despite its simplicity, achieves 10 to 20\nabsolute gains in top-1 retrieval accuracy and 1 to 4 Exact Match (EM) gains\nwithout refining the retriever or reader. In addition, RIDER, without any\ntraining, outperforms state-of-the-art transformer-based supervised rerankers.\nRemarkably, RIDER achieves 48.3 EM on the Natural Questions dataset and 66.4 EM\non the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are\nused as the reader input after passage reranking.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 18:54:19 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 22:17:15 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Mao", "Yuning", ""], ["He", "Pengcheng", ""], ["Liu", "Xiaodong", ""], ["Shen", "Yelong", ""], ["Gao", "Jianfeng", ""], ["Han", "Jiawei", ""], ["Chen", "Weizhu", ""]]}, {"id": "2101.00400", "submitter": "Paul Liu", "authors": "Aram Ebtekar and Paul Liu", "title": "An Elo-like System for Massive Multiplayer Competitions", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.GT cs.LG cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rating systems play an important role in competitive sports and games. They\nprovide a measure of player skill, which incentivizes competitive performances\nand enables balanced match-ups. In this paper, we present a novel Bayesian\nrating system for contests with many participants. It is widely applicable to\ncompetition formats with discrete ranked matches, such as online programming\ncompetitions, obstacle courses races, and some video games. The simplicity of\nour system allows us to prove theoretical bounds on robustness and runtime. In\naddition, we show that the system aligns incentives: that is, a player who\nseeks to maximize their rating will never want to underperform. Experimentally,\nthe rating system rivals or surpasses existing systems in prediction accuracy,\nand computes faster than existing systems by up to an order of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 08:14:31 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ebtekar", "Aram", ""], ["Liu", "Paul", ""]]}, {"id": "2101.00430", "submitter": "Gerard De Melo", "authors": "Abu Awal Md Shoeb and Gerard de Melo", "title": "Assessing Emoji Use in Modern Text Processing Tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emojis have become ubiquitous in digital communication, due to their visual\nappeal as well as their ability to vividly convey human emotion, among other\nfactors. The growing prominence of emojis in social media and other instant\nmessaging also leads to an increased need for systems and tools to operate on\ntext containing emojis. In this study, we assess this support by considering\ntest sets of tweets with emojis, based on which we perform a series of\nexperiments investigating the ability of prominent NLP and text processing\ntools to adequately process them. In particular, we consider tokenization,\npart-of-speech tagging, as well as sentiment analysis. Our findings show that\nmany tools still have notable shortcomings when operating on text containing\nemojis.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 11:38:05 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Shoeb", "Abu Awal Md", ""], ["de Melo", "Gerard", ""]]}, {"id": "2101.00436", "submitter": "Omar Khattab", "authors": "Omar Khattab, Christopher Potts, Matei Zaharia", "title": "Baleen: Robust Multi-Hop Reasoning at Scale via Condensed Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-hop reasoning (i.e., reasoning across two or more documents) is a key\ningredient for NLP models that leverage large corpora to exhibit broad\nknowledge. To retrieve evidence passages, multi-hop models must contend with a\nfast-growing search space across the hops, represent complex queries that\ncombine multiple information needs, and resolve ambiguity about the best order\nin which to hop between training passages. We tackle these problems via Baleen,\na system that improves the accuracy and robustness of multi-hop retrieval. To\ntame the search space, we propose condensed retrieval, a pipeline that\nsummarizes the retrieved passages after each hop into a single compact context.\nTo model complex queries, we introduce a focused late interaction retriever\nthat allows different parts of the same query representation to match disparate\nrelevant passages. Lastly, to infer the hopping dependencies among unordered\ntraining passages, we devise latent hop ordering, a weak-supervision strategy\nin which the trained retriever itself selects the sequence of hops. We evaluate\nBaleen on retrieval for two-hop question answering and many-hop claim\nverification, establishing state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 11:52:20 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 09:56:09 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Khattab", "Omar", ""], ["Potts", "Christopher", ""], ["Zaharia", "Matei", ""]]}, {"id": "2101.00480", "submitter": "Somya Mohanty", "authors": "Somya D. Mohanty and Brown Biggers and Saed Sayedahmed and Nastaran\n  Pourebrahim and Evan B. Goldstein and Rick Bunch and Guangqing Chi and\n  Fereidoon Sadri and Tom P. McCoy and Arthur Cosby", "title": "A multi-modal approach towards mining social media data during natural\n  disasters -- a case study of Hurricane Irma", "comments": "46 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Streaming social media provides a real-time glimpse of extreme weather\nimpacts. However, the volume of streaming data makes mining information a\nchallenge for emergency managers, policy makers, and disciplinary scientists.\nHere we explore the effectiveness of data learned approaches to mine and filter\ninformation from streaming social media data from Hurricane Irma's landfall in\nFlorida, USA. We use 54,383 Twitter messages (out of 784K geolocated messages)\nfrom 16,598 users from Sept. 10 - 12, 2017 to develop 4 independent models to\nfilter data for relevance: 1) a geospatial model based on forcing conditions at\nthe place and time of each tweet, 2) an image classification model for tweets\nthat include images, 3) a user model to predict the reliability of the tweeter,\nand 4) a text model to determine if the text is related to Hurricane Irma. All\nfour models are independently tested, and can be combined to quickly filter and\nvisualize tweets based on user-defined thresholds for each submodel. We\nenvision that this type of filtering and visualization routine can be useful as\na base model for data capture from noisy sources such as Twitter. The data can\nthen be subsequently used by policy makers, environmental managers, emergency\nmanagers, and domain scientists interested in finding tweets with specific\nattributes to use during different stages of the disaster (e.g., preparedness,\nresponse, and recovery), or for detailed research.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 17:08:53 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Mohanty", "Somya D.", ""], ["Biggers", "Brown", ""], ["Sayedahmed", "Saed", ""], ["Pourebrahim", "Nastaran", ""], ["Goldstein", "Evan B.", ""], ["Bunch", "Rick", ""], ["Chi", "Guangqing", ""], ["Sadri", "Fereidoon", ""], ["McCoy", "Tom P.", ""], ["Cosby", "Arthur", ""]]}, {"id": "2101.00781", "submitter": "Liang Yile", "authors": "Yile Liang, Tieyun Qian", "title": "Recommending Accurate and Diverse Items Using Bilateral Branch Network", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommender systems have played a vital role in online platforms due to the\nability of incorporating users' personal tastes. Beyond accuracy, diversity has\nbeen recognized as a key factor in recommendation to broaden user's horizons as\nwell as to promote enterprises' sales. However, the trading-off between\naccuracy and diversity remains to be a big challenge, and the data and user\nbiases have not been explored yet.\n  In this paper, we develop an adaptive learning framework for accurate and\ndiversified recommendation. We generalize recent proposed bi-lateral branch\nnetwork in the computer vision community from image classification to item\nrecommendation. Specifically, we encode domain level diversity by adaptively\nbalancing accurate recommendation in the conventional branch and diversified\nrecommendation in the adaptive branch of a bilateral branch network. We also\ncapture user level diversity using a two-way adaptive metric learning backbone\nnetwork in each branch. We conduct extensive experiments on three real-world\ndatasets. Results demonstrate that our proposed approach consistently\noutperforms the state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 05:39:53 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liang", "Yile", ""], ["Qian", "Tieyun", ""]]}, {"id": "2101.00810", "submitter": "Aman Abidi", "authors": "Aman Abidi, Lu Chen, Rui Zhou, Chengfei Liu", "title": "Searching Personalized $k$-wing in Large and Dynamic Bipartite Graphs", "comments": "13 pages, 10 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are extensive studies focusing on the application scenario that all the\nbipartite cohesive subgraphs need to be discovered in a bipartite graph.\nHowever, we observe that, for some applications, one is interested in finding\nbipartite cohesive subgraphs containing a specific vertex. In this paper, we\nstudy a new query dependent bipartite cohesive subgraph search problem based on\n$k$-wing model, named as the personalized $k$-wing search problem. We introduce\na $k$-wing equivalence relationship to summarize the edges of a bipartite graph\n$G$ into groups. Therefore, all the edges of $G$ are segregated into different\ngroups, i.e. $k$-wing equivalence class, forming an efficient and wing number\nconserving index called EquiWing. Further, we propose a more compact version of\nEquiWing, EquiWing-Comp, which is achieved by integrating our proposed\n$k$-butterfly loose approach and discovered hierarchy properties. These indices\nare used to expedite the personalized $k$-wing search with a non-repetitive\naccess to $G$, which leads to linear algorithms for searching the personalized\n$k$-wing. Moreover, we conduct a thorough study on the maintenance of the\nproposed indices for evolving bipartite graphs. We discover novel properties\nthat help us localize the scope of the maintenance at a low cost. By exploiting\nthe discoveries, we propose novel algorithms for maintaining the two indices,\nwhich substantially reduces the cost of maintenance. We perform extensive\nexperimental studies in real, large-scale graphs to validate the efficiency and\neffectiveness of EquiWing and EquiWing-Comp compared to the baseline.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 07:19:52 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 02:13:03 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Abidi", "Aman", ""], ["Chen", "Lu", ""], ["Zhou", "Rui", ""], ["Liu", "Chengfei", ""]]}, {"id": "2101.00870", "submitter": "Olivier Koch", "authors": "Olivier Koch, Amine Benhalloum, Guillaume Genthial, Denis Kuzin,\n  Dmitry Parfenchik", "title": "Scalable representation learning and retrieval for display advertising", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Over the past decades, recommendation has become a critical component of many\nonline services such as media streaming and e-commerce. Recent advances in\nalgorithms, evaluation methods and datasets have led to continuous improvements\nof the state-of-the-art. However, much work remains to be done to make these\nmethods scale to the size of the internet.\n  Online advertising offers a unique testbed for recommendation at scale. Every\nday, billions of users interact with millions of products in real-time. Systems\naddressing this scenario must work reliably at scale. We propose an efficient\nmodel (LED, for Lightweight Encoder-Decoder) reaching a new trade-off between\ncomplexity, scale and performance. Specifically, we show that combining\nlarge-scale matrix factorization with lightweight embedding fine-tuning unlocks\nstate-of-the-art performance at scale. We further provide the detailed\ndescription of a system architecture and demonstrate its operation over two\nmonths at the scale of the internet. Our design allows serving billions of\nusers across hundreds of millions of items in a few milliseconds using standard\nhardware.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 10:29:24 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Koch", "Olivier", ""], ["Benhalloum", "Amine", ""], ["Genthial", "Guillaume", ""], ["Kuzin", "Denis", ""], ["Parfenchik", "Dmitry", ""]]}, {"id": "2101.00884", "submitter": "Arthur Brack", "authors": "Arthur Brack, Daniel Uwe M\\\"uller, Anett Hoppe, Ralph Ewerth", "title": "Coreference Resolution in Research Papers from Multiple Domains", "comments": "Accepted for publication in 43rd European Conference on Information\n  Retrieval (ECIR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coreference resolution is essential for automatic text understanding to\nfacilitate high-level information retrieval tasks such as text summarisation or\nquestion answering. Previous work indicates that the performance of\nstate-of-the-art approaches (e.g. based on BERT) noticeably declines when\napplied to scientific papers. In this paper, we investigate the task of\ncoreference resolution in research papers and subsequent knowledge graph\npopulation. We present the following contributions: (1) We annotate a corpus\nfor coreference resolution that comprises 10 different scientific disciplines\nfrom Science, Technology, and Medicine (STM); (2) We propose transfer learning\nfor automatic coreference resolution in research papers; (3) We analyse the\nimpact of coreference resolution on knowledge graph (KG) population; (4) We\nrelease a research KG that is automatically populated from 55,485 papers in 10\nSTM domains. Comprehensive experiments show the usefulness of the proposed\napproach. Our transfer learning approach considerably outperforms\nstate-of-the-art baselines on our corpus with an F1 score of 61.4 (+11.0),\nwhile the evaluation against a gold standard KG shows that coreference\nresolution improves the quality of the populated KG significantly with an F1\nscore of 63.5 (+21.8).\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 10:52:55 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Brack", "Arthur", ""], ["M\u00fcller", "Daniel Uwe", ""], ["Hoppe", "Anett", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2101.00939", "submitter": "Kun Zhou", "authors": "Kun Zhou, Xiaolei Wang, Yuanhang Zhou, Chenzhan Shang, Yuan Cheng,\n  Wayne Xin Zhao, Yaliang Li, Ji-Rong Wen", "title": "CRSLab: An Open-Source Toolkit for Building Conversational Recommender\n  System", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, conversational recommender system (CRS) has received much\nattention in the research community. However, existing studies on CRS vary in\nscenarios, goals and techniques, lacking unified, standardized implementation\nor comparison. To tackle this challenge, we propose an open-source CRS toolkit\nCRSLab, which provides a unified and extensible framework with highly-decoupled\nmodules to develop CRSs. Based on this framework, we collect 6 commonly-used\nhuman-annotated CRS datasets and implement 18 models that include recent\ntechniques such as graph neural network and pre-training models. Besides, our\ntoolkit provides a series of automatic evaluation protocols and a human-machine\ninteraction interface to test and compare different CRS methods. The project\nand documents are released at https://github.com/RUCAIBox/CRSLab.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 13:10:31 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhou", "Kun", ""], ["Wang", "Xiaolei", ""], ["Zhou", "Yuanhang", ""], ["Shang", "Chenzhan", ""], ["Cheng", "Yuan", ""], ["Zhao", "Wayne Xin", ""], ["Li", "Yaliang", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2101.01039", "submitter": "Suzan Verberne", "authors": "Ken Voskuil and Suzan Verberne", "title": "Improving reference mining in patents with BERT", "comments": "10 pages, 3 figures", "journal-ref": "Published in the 11th International Workshop on\n  Bibliometric-enhanced Information Retrieval (BIR 2021)", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we address the challenge of extracting scientific references\nfrom patents. We approach the problem as a sequence labelling task and\ninvestigate the merits of BERT models to the extraction of these long\nsequences. References in patents to scientific literature are relevant to study\nthe connection between science and industry. Most prior work only uses the\nfront-page citations for this analysis, which are provided in the metadata of\npatent archives. In this paper we build on prior work using Conditional Random\nFields (CRF) and Flair for reference extraction. We improve the quality of the\ntraining data and train three BERT-based models on the labelled data (BERT,\nbioBERT, sciBERT). We find that the improved training data leads to a large\nimprovement in the quality of the trained models. In addition, the BERT models\nbeat CRF and Flair, with recall scores around 97% obtained with cross\nvalidation. With the best model we label a large collection of 33 thousand\npatents, extract the citations, and match them to publications in the Web of\nScience database. We extract 50% more references than with the old training\ndata and methods: 735 thousand references in total. With these\npatent-publication links, follow-up research will further analyze which types\nof scientific work lead to inventions.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 15:56:21 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 10:03:15 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 11:26:01 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Voskuil", "Ken", ""], ["Verberne", "Suzan", ""]]}, {"id": "2101.01141", "submitter": "Hanif Emamgholizadeh", "authors": "Zahra Roozbahani, Jalal Rezaeenour, Roshan Shahrooei, Hanif\n  Emamgholizadeh", "title": "Presenting a Dataset for Collaborator Recommending Systems in Academic\n  Social Network: a Case Study on ReseachGate", "comments": "J. of Data, Inf. and Manag. (2021)", "journal-ref": null, "doi": "10.1007/s42488-021-00041-7", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborator finding systems are a special type of expert finding models.\nThere is a long-lasting challenge for research in the collaborator recommending\nresearch area, which is the lack of the structured dataset to be used by the\nresearchers. We introduce two datasets to fill this gap. The first dataset is\nprepared for designing a consistent, collaborator finding system. The next one,\ncalled a co-author finding model, models an academic social network as a table\nthat contains different relations between the pair of users. Both of them\nprovide an opportunity for introducing potential collaborators to each other.\nThese two models have been extracted from ResearchGate (RG) data set and are\navailable publicly. RG dataset has been collected from Jan. 2019 to April 2019\nand includes raw data of 3980 RG users. The dataset consists of almost complete\ninformation about users. In the preprocessing phase, the well-known Elmo was\nused for analyzing textual data. We call this as ResearchGate dataset for\nRecommending Systems (RGRS). For assessing the validity of data, we analyze\neach layer of data separately, and the results are reported. After preparing\ndata and evaluating the collaborator finding models, we have done some\nassessments on RGRS. Some of these assessments are co-author,\nfollowing-follower, and question answering relations. The outcomes indicate\nthat it is the best relation in propagating knowledge in the network. To the\nbest of our knowledge, there is no processed and analyzed dataset of this size.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 22:23:32 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 14:36:38 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 10:31:56 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Roozbahani", "Zahra", ""], ["Rezaeenour", "Jalal", ""], ["Shahrooei", "Roshan", ""], ["Emamgholizadeh", "Hanif", ""]]}, {"id": "2101.01317", "submitter": "Zhuang Liu", "authors": "Zhuang Liu, Yunpu Ma, Yuanxin Ouyang, Zhang Xiong", "title": "Contrastive Learning for Recommender System", "comments": "arXiv admin note: text overlap with arXiv:1905.08108 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems, which analyze users' preference patterns to suggest\npotential targets, are indispensable in today's society. Collaborative\nFiltering (CF) is the most popular recommendation model. Specifically, Graph\nNeural Network (GNN) has become a new state-of-the-art for CF. In the GNN-based\nrecommender system, message dropout is usually used to alleviate the selection\nbias in the user-item bipartite graph. However, message dropout might\ndeteriorate the recommender system's performance due to the randomness of\ndropping out the outgoing messages based on the user-item bipartite graph. To\nsolve this problem, we propose a graph contrastive learning module for a\ngeneral recommender system that learns the embeddings in a self-supervised\nmanner and reduces the randomness of message dropout. Besides, many recommender\nsystems optimize models with pairwise ranking objectives, such as the Bayesian\nPairwise Ranking (BPR) based on a negative sampling strategy. However, BPR has\nthe following problems: suboptimal sampling and sample bias. We introduce a new\ndebiased contrastive loss to solve these problems, which provides sufficient\nnegative samples and applies a bias correction probability to alleviate the\nsample bias. We integrate the proposed framework, including graph contrastive\nmodule and debiased contrastive module with several Matrix Factorization(MF)\nand GNN-based recommendation models. Experimental results on three public\nbenchmarks demonstrate the effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 02:11:47 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Liu", "Zhuang", ""], ["Ma", "Yunpu", ""], ["Ouyang", "Yuanxin", ""], ["Xiong", "Zhang", ""]]}, {"id": "2101.01431", "submitter": "Jiamou Sun", "authors": "Jiamou Sun, Zhenchang Xing, Hao Guo, Deheng Ye, Xiaohong Li, Xiwei Xu,\n  Liming Zhu", "title": "Generating Informative CVE Description From ExploitDB Posts by\n  Extractive Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ExploitDB is one of the important public websites, which contributes a large\nnumber of vulnerabilities to official CVE database. Over 60\\% of these\nvulnerabilities have high- or critical-security risks. Unfortunately, over 73\\%\nof exploits appear publicly earlier than the corresponding CVEs, and about 40\\%\nof exploits do not even have CVEs. To assist in documenting CVEs for the\nExploitDB posts, we propose an open information method to extract 9 key\nvulnerability aspects (vulnerable product/version/component, vulnerability\ntype, vendor, attacker type, root cause, attack vector and impact) from the\nverbose and noisy ExploitDB posts. The extracted aspects from an ExploitDB post\nare then composed into a CVE description according to the suggested CVE\ndescription templates, which is must-provided information for requesting new\nCVEs. Through the evaluation on 13,017 manually labeled sentences and the\nstatistically sampling of 3,456 extracted aspects, we confirm the high accuracy\nof our extraction method. Compared with 27,230 reference CVE descriptions. Our\ncomposed CVE descriptions achieve high ROUGH-L (0.38), a longest common\nsubsequence based metric for evaluating text summarization methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 09:52:05 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Sun", "Jiamou", ""], ["Xing", "Zhenchang", ""], ["Guo", "Hao", ""], ["Ye", "Deheng", ""], ["Li", "Xiaohong", ""], ["Xu", "Xiwei", ""], ["Zhu", "Liming", ""]]}, {"id": "2101.01896", "submitter": "Xiangchen Song", "authors": "Jieyu Zhang, Xiangchen Song, Ying Zeng, Jiaze Chen, Jiaming Shen,\n  Yuning Mao, Lei Li", "title": "Taxonomy Completion via Triplet Matching Network", "comments": "AAA1 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically constructing taxonomy finds many applications in e-commerce and\nweb search. One critical challenge is as data and business scope grow in real\napplications, new concepts are emerging and needed to be added to the existing\ntaxonomy. Previous approaches focus on the taxonomy expansion, i.e. finding an\nappropriate hypernym concept from the taxonomy for a new query concept. In this\npaper, we formulate a new task, \"taxonomy completion\", by discovering both the\nhypernym and hyponym concepts for a query. We propose Triplet Matching Network\n(TMN), to find the appropriate <hypernym, hyponym> pairs for a given query\nconcept. TMN consists of one primal scorer and multiple auxiliary scorers.\nThese auxiliary scorers capture various fine-grained signals (e.g., query to\nhypernym or query to hyponym semantics), and the primal scorer makes a holistic\nprediction on <query, hypernym, hyponym> triplet based on the internal feature\nrepresentations of all auxiliary scorers. Also, an innovative channel-wise\ngating mechanism that retains task-specific information in concept\nrepresentations is introduced to further boost model performance. Experiments\non four real-world large-scale datasets show that TMN achieves the best\nperformance on both taxonomy completion task and the previous taxonomy\nexpansion task, outperforming existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 07:19:55 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 01:36:22 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 09:51:39 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zhang", "Jieyu", ""], ["Song", "Xiangchen", ""], ["Zeng", "Ying", ""], ["Chen", "Jiaze", ""], ["Shen", "Jiaming", ""], ["Mao", "Yuning", ""], ["Li", "Lei", ""]]}, {"id": "2101.01910", "submitter": "Xiaopeng Lu", "authors": "Xiaopeng Lu, Kyusong Lee, Tiancheng Zhao", "title": "SF-QA: Simple and Fair Evaluation Library for Open-domain Question\n  Answering", "comments": "Accepted to EACL2021 demo track (7 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although open-domain question answering (QA) draws great attention in recent\nyears, it requires large amounts of resources for building the full system and\nis often difficult to reproduce previous results due to complex configurations.\nIn this paper, we introduce SF-QA: simple and fair evaluation framework for\nopen-domain QA. SF-QA framework modularizes the pipeline open-domain QA system,\nwhich makes the task itself easily accessible and reproducible to research\ngroups without enough computing resources. The proposed evaluation framework is\npublicly available and anyone can contribute to the code and evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 08:02:41 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 03:02:29 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Lu", "Xiaopeng", ""], ["Lee", "Kyusong", ""], ["Zhao", "Tiancheng", ""]]}, {"id": "2101.02017", "submitter": "Mihir Parmar", "authors": "Mihir Parmar, Ashwin Karthik Ambalavanan, Hong Guan, Rishab Banerjee,\n  Jitesh Pabla and Murthy Devarakonda", "title": "COVID-19: Comparative Analysis of Methods for Identifying Articles\n  Related to Therapeutics and Vaccines without Using Labeled Data", "comments": "6 pages, 3 Tables, Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Here we proposed an approach to analyze text classification methods based on\nthe presence or absence of task-specific terms (and their synonyms) in the\ntext. We applied this approach to study six different transfer-learning and\nunsupervised methods for screening articles relevant to COVID-19 vaccines and\ntherapeutics. The analysis revealed that while a BERT model trained on\nsearch-engine results generally performed well, it miss-classified relevant\nabstracts that did not contain task-specific terms. We used this insight to\ncreate a more effective unsupervised ensemble.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 08:40:04 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Parmar", "Mihir", ""], ["Ambalavanan", "Ashwin Karthik", ""], ["Guan", "Hong", ""], ["Banerjee", "Rishab", ""], ["Pabla", "Jitesh", ""], ["Devarakonda", "Murthy", ""]]}, {"id": "2101.02028", "submitter": "Ye Tian", "authors": "Ye Tian", "title": "A Multilayer Correlated Topic Model", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We proposed a novel multilayer correlated topic model (MCTM) to analyze how\nthe main ideas inherit and vary between a document and its different segments,\nwhich helps understand an article's structure. The variational\nexpectation-maximization (EM) algorithm was derived to estimate the posterior\nand parameters in MCTM. We introduced two potential applications of MCTM,\nincluding the paragraph-level document analysis and market basket data\nanalysis. The effectiveness of MCTM in understanding the document structure has\nbeen verified by the great predictive performance on held-out documents and\nintuitive visualization. We also showed that MCTM could successfully capture\ncustomers' popular shopping patterns in the market basket analysis.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 21:50:36 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Tian", "Ye", ""]]}, {"id": "2101.02051", "submitter": "Yudhik Agrawal", "authors": "Yudhik Agrawal, Ramaguru Guru Ravi Shanker, Vinoo Alluri", "title": "Transformer-based approach towards music emotion recognition from lyrics", "comments": "Appearing in Proceedings of the 43rd European Conference On\n  Information Retrieval (ECIR) 2021", "journal-ref": "Lecture Notes in Computer Science, 12657 (2021) 167-175", "doi": "10.1007/978-3-030-72240-1_12", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of identifying emotions from a given music track has been an active\npursuit in the Music Information Retrieval (MIR) community for years. Music\nemotion recognition has typically relied on acoustic features, social tags, and\nother metadata to identify and classify music emotions. The role of lyrics in\nmusic emotion recognition remains under-appreciated in spite of several studies\nreporting superior performance of music emotion classifiers based on features\nextracted from lyrics. In this study, we use the transformer-based approach\nmodel using XLNet as the base architecture which, till date, has not been used\nto identify emotional connotations of music based on lyrics. Our proposed\napproach outperforms existing methods for multiple datasets. We used a robust\nmethodology to enhance web-crawlers' accuracy for extracting lyrics. This study\nhas important implications in improving applications involved in playlist\ngeneration of music based on emotions in addition to improving music\nrecommendation systems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 14:07:45 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Agrawal", "Yudhik", ""], ["Shanker", "Ramaguru Guru Ravi", ""], ["Alluri", "Vinoo", ""]]}, {"id": "2101.02098", "submitter": "Furkan Yesiler", "authors": "Furkan Yesiler and Emilio Molina and Joan Serr\\`a and Emilia G\\'omez", "title": "Investigating the efficacy of music version retrieval systems for\n  setlist identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The setlist identification (SLI) task addresses a music recognition use case\nwhere the goal is to retrieve the metadata and timestamps for all the tracks\nplayed in live music events. Due to various musical and non-musical changes in\nlive performances, developing automatic SLI systems is still a challenging task\nthat, despite its industrial relevance, has been under-explored in the academic\nliterature. In this paper, we propose an end-to-end workflow that identifies\nrelevant metadata and timestamps of live music performances using a version\nidentification system. We compare 3 of such systems to investigate their\nsuitability for this particular task. For developing and evaluating SLI\nsystems, we also contribute a new dataset that contains 99.5h of concerts with\nannotated metadata and timestamps, along with the corresponding reference set.\nThe dataset is categorized by audio qualities and genres to analyze the\nperformance of SLI systems in different use cases. Our approach can identify\n68% of the annotated segments, with values ranging from 35% to 77% based on the\ngenre. Finally, we evaluate our approach against a database of 56.8k songs to\nillustrate the effect of expanding the reference set, where we can still\nidentify 56% of the annotated segments.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 15:41:12 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Yesiler", "Furkan", ""], ["Molina", "Emilio", ""], ["Serr\u00e0", "Joan", ""], ["G\u00f3mez", "Emilia", ""]]}, {"id": "2101.02351", "submitter": "Sohom Ghosh", "authors": "Ankush Chopra, Shruti Agrawal and Sohom Ghosh", "title": "Applying Transfer Learning for Improving Domain-Specific Search\n  Experience Using Query to Question Similarity", "comments": "8 pages, accepted in the Proceedings of the 3rd International\n  Conference on Algorithms, Computing and Artificial Intelligence (ACAI), 2020", "journal-ref": null, "doi": "10.1145/3446132.3446403", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search is one of the most common platforms used to seek information. However,\nusers mostly get overloaded with results whenever they use such a platform to\nresolve their queries. Nowadays, direct answers to queries are being provided\nas a part of the search experience. The question-answer (QA) retrieval process\nplays a significant role in enriching the search experience. Most off-the-shelf\nSemantic Textual Similarity models work fine for well-formed search queries,\nbut their performances degrade when applied to a domain-specific setting having\nincomplete or grammatically ill-formed search queries in prevalence. In this\npaper, we discuss a framework for calculating similarities between a given\ninput query and a set of predefined questions to retrieve the question which\nmatches to it the most. We have used it for the financial domain, but the\nframework is generalized for any domain-specific search engine and can be used\nin other domains as well. We use Siamese network [6] over Long Short-Term\nMemory (LSTM) [3] models to train a classifier which generates unnormalized and\nnormalized similarity scores for a given pair of questions. Moreover, for each\nof these question pairs, we calculate three other similarity scores: cosine\nsimilarity between their average word2vec embeddings [15], cosine similarity\nbetween their sentence embeddings [7] generated using RoBERTa [17] and their\ncustomized fuzzy-match score. Finally, we develop a metaclassifier using\nSupport Vector Machines [19] for combining these five scores to detect if a\ngiven pair of questions is similar. We benchmark our model's performance\nagainst existing State Of The Art (SOTA) models on Quora Question Pairs (QQP)\ndataset as well as a dataset specific to the financial domain.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 03:27:32 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Chopra", "Ankush", ""], ["Agrawal", "Shruti", ""], ["Ghosh", "Sohom", ""]]}, {"id": "2101.02488", "submitter": "Juan-Jose Prieto-Gutierrez", "authors": "Francisco Segado-Boj, Juan Martin-Quevedo, Juan Jose Prieto-Gutierrez", "title": "Attitudes toward Open Access, Open Peer Review, and Altmetrics among\n  Contributors to Spanish Scholarly Journals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper aims to gain a better understanding of the perspectives of\ncontributors to Spanish academic journals regarding open access, open peer\nreview, and altmetrics. It also explores how age, gender, professional\nexperience, career history, and perception and use of social media influence\nauthors opinions toward these developments in scholarly publishing. A sample of\ncontributors (n-1254) to Spanish academic journals was invited to participate\nin a survey about the aforementioned topics. The response rate was 24 per cent\n(n-295). Contributors to Spanish scholarly journals hold a favourable opinion\nof open access but were more cautious about open peer review and altmetrics.\nYounger and female scholars were more reluctant to accept open peer review\npractices. A positive attitude toward social networks did not necessarily\ntranslate into enthusiasm for emerging trends in scholarly publishing. Despite\nthis, ResearchGate users were more aware of altmetrics.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 11:09:59 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Segado-Boj", "Francisco", ""], ["Martin-Quevedo", "Juan", ""], ["Prieto-Gutierrez", "Juan Jose", ""]]}, {"id": "2101.02655", "submitter": "Pawe{\\l} Zawistowski", "authors": "Bart{\\l}omiej Twardowski, Pawe{\\l} Zawistowski, Szymon Zaborowski", "title": "Metric Learning for Session-based Recommendations", "comments": "Accepted at European Conference On Information Retrieval (ECIR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommenders, used for making predictions out of users'\nuninterrupted sequences of actions, are attractive for many applications. Here,\nfor this task we propose using metric learning, where a common embedding space\nfor sessions and items is created, and distance measures dissimilarity between\nthe provided sequence of users' events and the next action. We discuss and\ncompare metric learning approaches to commonly used learning-to-rank methods,\nwhere some synergies exist. We propose a simple architecture for problem\nanalysis and demonstrate that neither extensively big nor deep architectures\nare necessary in order to outperform existing methods. The experimental results\nagainst strong baselines on four datasets are provided with an ablation study.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 17:51:04 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Twardowski", "Bart\u0142omiej", ""], ["Zawistowski", "Pawe\u0142", ""], ["Zaborowski", "Szymon", ""]]}, {"id": "2101.02668", "submitter": "Nicola Ferro", "authors": "Marco Ferrante, Nicola Ferro, Norbert Fuhr", "title": "Towards Meaningful Statements in IR Evaluation. Mapping Evaluation\n  Measures to Interval Scales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recently, it was shown that most popular IR measures are not interval-scaled,\nimplying that decades of experimental IR research used potentially improper\nmethods, which may have produced questionable results. However, it was unclear\nif and to what extent these findings apply to actual evaluations and this\nopened a debate in the community with researchers standing on opposite\npositions about whether this should be considered an issue (or not) and to what\nextent.\n  In this paper, we first give an introduction to the representational\nmeasurement theory explaining why certain operations and significance tests are\npermissible only with scales of a certain level. For that, we introduce the\nnotion of meaningfulness specifying the conditions under which the truth (or\nfalsity) of a statement is invariant under permissible transformations of a\nscale. Furthermore, we show how the recall base and the length of the run may\nmake comparison and aggregation across topics problematic.\n  Then we propose a straightforward and powerful approach for turning an\nevaluation measure into an interval scale, and describe an experimental\nevaluation of the differences between using the original measures and the\ninterval-scaled ones.\n  For all the regarded measures - namely Precision, Recall, Average Precision,\n(Normalized) Discounted Cumulative Gain, Rank-Biased Precision and Reciprocal\nRank - we observe substantial effects, both on the order of average values and\non the outcome of significance tests. For the latter, previously significant\ndifferences turn out to be insignificant, while insignificant ones become\nsignificant. The effect varies remarkably between the tests considered but\noverall, on average, we observed a 25% change in the decision about which\nsystems are significantly different and which are not.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:24:01 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Ferrante", "Marco", ""], ["Ferro", "Nicola", ""], ["Fuhr", "Norbert", ""]]}, {"id": "2101.02844", "submitter": "Xiaohan Li", "authors": "Xiaohan Li, Mengqi Zhang, Shu Wu, Zheng Liu, Liang Wang, Philip S. Yu", "title": "Dynamic Graph Collaborative Filtering", "comments": "ICDM 2020 Regular paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamic recommendation is essential for modern recommender systems to provide\nreal-time predictions based on sequential data. In real-world scenarios, the\npopularity of items and interests of users change over time. Based on this\nassumption, many previous works focus on interaction sequences and learn\nevolutionary embeddings of users and items. However, we argue that\nsequence-based models are not able to capture collaborative information among\nusers and items directly. Here we propose Dynamic Graph Collaborative Filtering\n(DGCF), a novel framework leveraging dynamic graphs to capture collaborative\nand sequential relations of both items and users at the same time. We propose\nthree update mechanisms: zero-order 'inheritance', first-order 'propagation',\nand second-order 'aggregation', to represent the impact on a user or item when\na new interaction occurs. Based on them, we update related user and item\nembeddings simultaneously when interactions occur in turn, and then use the\nlatest embeddings to make recommendations. Extensive experiments conducted on\nthree public datasets show that DGCF significantly outperforms the\nstate-of-the-art dynamic recommendation methods up to 30. Our approach achieves\nhigher performance when the dataset contains less action repetition, indicating\nthe effectiveness of integrating dynamic collaborative information.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 04:16:24 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Li", "Xiaohan", ""], ["Zhang", "Mengqi", ""], ["Wu", "Shu", ""], ["Liu", "Zheng", ""], ["Wang", "Liang", ""], ["Yu", "Philip S.", ""]]}, {"id": "2101.02969", "submitter": "Hui Luo", "authors": "Hui Luo, Jingbo Zhou, Zhifeng Bao, Shuangli Li, J. Shane Culpepper,\n  Haochao Ying, Hao Liu, Hui Xiong", "title": "Spatial Object Recommendation with Hints: When Spatial Granularity\n  Matters", "comments": null, "journal-ref": "SIGIR Conference (2020) 781-790", "doi": "10.1145/3397271.3401090", "report-no": null, "categories": "cs.IR cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing spatial object recommendation algorithms generally treat objects\nidentically when ranking them. However, spatial objects often cover different\nlevels of spatial granularity and thereby are heterogeneous. For example, one\nuser may prefer to be recommended a region (say Manhattan), while another user\nmight prefer a venue (say a restaurant). Even for the same user, preferences\ncan change at different stages of data exploration. In this paper, we study how\nto support top-k spatial object recommendations at varying levels of spatial\ngranularity, enabling spatial objects at varying granularity, such as a city,\nsuburb, or building, as a Point of Interest (POI). To solve this problem, we\npropose the use of a POI tree, which captures spatial containment relationships\nbetween POIs. We design a novel multi-task learning model called MPR (short for\nMulti-level POI Recommendation), where each task aims to return the top-k POIs\nat a certain spatial granularity level. Each task consists of two subtasks: (i)\nattribute-based representation learning; (ii) interaction-based representation\nlearning. The first subtask learns the feature representations for both users\nand POIs, capturing attributes directly from their profiles. The second subtask\nincorporates user-POI interactions into the model. Additionally, MPR can\nprovide insights into why certain recommendations are being made to a user\nbased on three types of hints: user-aspect, POI-aspect, and interaction-aspect.\nWe empirically validate our approach using two real-life datasets, and show\npromising performance improvements over several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 11:39:51 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Luo", "Hui", ""], ["Zhou", "Jingbo", ""], ["Bao", "Zhifeng", ""], ["Li", "Shuangli", ""], ["Culpepper", "J. Shane", ""], ["Ying", "Haochao", ""], ["Liu", "Hao", ""], ["Xiong", "Hui", ""]]}, {"id": "2101.03013", "submitter": "Iknoor Singh", "authors": "Iknoor Singh, Carolina Scarton, Kalina Bontcheva", "title": "Multistage BiCross Encoder: Team GATE Entry for MLIA Multilingual\n  Semantic Search Task 2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Coronavirus (COVID-19) pandemic has led to a rapidly growing `infodemic'\nonline. Thus, the accurate retrieval of reliable relevant data from millions of\ndocuments about COVID-19 has become urgently needed for the general public as\nwell as for other stakeholders. The COVID-19 Multilingual Information Access\n(MLIA) initiative is a joint effort to ameliorate exchange of COVID-19 related\ninformation by developing applications and services through research and\ncommunity participation. In this work, we present a search system called\nMultistage BiCross Encoder, developed by team GATE for the MLIA task 2\nMultilingual Semantic Search. Multistage BiCross-Encoder is a sequential three\nstage pipeline which uses the Okapi BM25 algorithm and a transformer based\nbi-encoder and cross-encoder to effectively rank the documents with respect to\nthe query. The results of round 1 show that our models achieve state-of-the-art\nperformance for all ranking metrics for both monolingual and bilingual runs.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 13:59:26 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 20:38:23 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Singh", "Iknoor", ""], ["Scarton", "Carolina", ""], ["Bontcheva", "Kalina", ""]]}, {"id": "2101.03026", "submitter": "Carlos Badenes-Olmedo", "authors": "Carlos Badenes-Olmedo, Jose-Luis Redondo Garc\\'ia, Oscar Corcho", "title": "Scalable Cross-lingual Document Similarity through Language-specific\n  Concept Hierarchies", "comments": "Accepted at the 10th International Conference on Knowledge Capture\n  (K-CAP 2019)", "journal-ref": "AACM Proceedings of the 10th International Conference on Knowledge\n  Capture, pages = 147-153, K-CAP 19 (2020)", "doi": "10.1145/3360901.3364444", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the ongoing growth in number of digital articles in a wider set of\nlanguages and the expanding use of different languages, we need annotation\nmethods that enable browsing multi-lingual corpora. Multilingual probabilistic\ntopic models have recently emerged as a group of semi-supervised machine\nlearning models that can be used to perform thematic explorations on\ncollections of texts in multiple languages. However, these approaches require\ntheme-aligned training data to create a language-independent space. This\nconstraint limits the amount of scenarios that this technique can offer\nsolutions to train and makes it difficult to scale up to situations where a\nhuge collection of multi-lingual documents are required during the training\nphase. This paper presents an unsupervised document similarity algorithm that\ndoes not require parallel or comparable corpora, or any other type of\ntranslation resource. The algorithm annotates topics automatically created from\ndocuments in a single language with cross-lingual labels and describes\ndocuments by hierarchies of multi-lingual concepts from independently-trained\nmodels. Experiments performed on the English, Spanish and French editions of\nJCR-Acquis corpora reveal promising results on classifying and sorting\ndocuments by similar content.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 10:42:40 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Badenes-Olmedo", "Carlos", ""], ["Garc\u00eda", "Jose-Luis Redondo", ""], ["Corcho", "Oscar", ""]]}, {"id": "2101.03054", "submitter": "Serguei Mokhov", "authors": "Yuhao Mao, Serguei A. Mokhov, Sudhir P. Mudur", "title": "Application of Knowledge Graphs to Provide Side Information for Improved\n  Recommendation Accuracy", "comments": "27 pages, 16 figures, 5 tables, 2 algorithms; Submitted to Science of\n  Computer Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized recommendations are popular in these days of Internet driven\nactivities, specifically shopping. Recommendation methods can be grouped into\nthree major categories, content based filtering, collaborative filtering and\nmachine learning enhanced. Information about products and preferences of\ndifferent users are primarily used to infer preferences for a specific user.\nInadequate information can obviously cause these methods to fail or perform\npoorly. The more information we provide to these methods, the more likely it is\nthat the methods perform better. Knowledge graphs represent the current trend\nin recording information in the form of relations between entities, and can\nprovide additional (side) information about products and users. Such\ninformation can be used to improve nearest neighbour search, clustering users\nand products, or train the neural network, when one is used. In this work, we\npresent a new generic recommendation systems framework, that integrates\nknowledge graphs into the recommendation pipeline. We describe its software\ndesign and implementation, and then show through experiments, how such a\nframework can be specialized for a domain, say movie recommendations, and the\nimprovements in recommendation results possible due to side information\nobtained from knowledge graphs representation of such information. Our\nframework supports different knowledge graph representation formats, and\nfacilitates format conversion, merging and information extraction needed for\ntraining recommendation methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 16:52:05 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Mao", "Yuhao", ""], ["Mokhov", "Serguei A.", ""], ["Mudur", "Sudhir P.", ""]]}, {"id": "2101.03207", "submitter": "Sayar Ghosh Roy", "authors": "Sayar Ghosh Roy, Ujwal Narayan, Tathagata Raha, Zubair Abid, Vasudeva\n  Varma", "title": "Leveraging Multilingual Transformers for Hate Speech Detection", "comments": "To be published in: FIRE (Working Notes) 2020, Hate Speech and\n  Offensive Content Identification in Indo-European Languages, HASOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting and classifying instances of hate in social media text has been a\nproblem of interest in Natural Language Processing in the recent years. Our\nwork leverages state of the art Transformer language models to identify hate\nspeech in a multilingual setting. Capturing the intent of a post or a comment\non social media involves careful evaluation of the language style, semantic\ncontent and additional pointers such as hashtags and emojis. In this paper, we\nlook at the problem of identifying whether a Twitter post is hateful and\noffensive or not. We further discriminate the detected toxic content into one\nof the following three classes: (a) Hate Speech (HATE), (b) Offensive (OFFN)\nand (c) Profane (PRFN). With a pre-trained multilingual Transformer-based text\nencoder at the base, we are able to successfully identify and classify hate\nspeech from multiple languages. On the provided testing corpora, we achieve\nMacro F1 scores of 90.29, 81.87 and 75.40 for English, German and Hindi\nrespectively while performing hate speech detection and of 60.70, 53.28 and\n49.74 during fine-grained classification. In our experiments, we show the\nefficacy of Perspective API features for hate speech classification and the\neffects of exploiting a multilingual training scheme. A feature selection study\nis provided to illustrate impacts of specific features upon the architecture's\nclassification head.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 20:23:50 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Roy", "Sayar Ghosh", ""], ["Narayan", "Ujwal", ""], ["Raha", "Tathagata", ""], ["Abid", "Zubair", ""], ["Varma", "Vasudeva", ""]]}, {"id": "2101.03303", "submitter": "Anurag Roy", "authors": "Anurag Roy, Shalmoli Ghosh, Kripabandhu Ghosh, Saptarshi Ghosh", "title": "An Unsupervised Normalization Algorithm for Noisy Text: A Case Study for\n  Information Retrieval and Stance Detection", "comments": "Will be appearing in the ACM Journal of Data and Information Quality.\n  Implementation available at https://github.com/ranarag/UnsupClean", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large fraction of textual data available today contains various types of\n'noise', such as OCR noise in digitized documents, noise due to informal\nwriting style of users on microblogging sites, and so on. To enable tasks such\nas search/retrieval and classification over all the available data, we need\nrobust algorithms for text normalization, i.e., for cleaning different kinds of\nnoise in the text. There have been several efforts towards cleaning or\nnormalizing noisy text; however, many of the existing text normalization\nmethods are supervised and require language-dependent resources or large\namounts of training data that is difficult to obtain. We propose an\nunsupervised algorithm for text normalization that does not need any training\ndata / human intervention. The proposed algorithm is applicable to text over\ndifferent languages, and can handle both machine-generated and human-generated\nnoise. Experiments over several standard datasets show that text normalization\nthrough the proposed algorithm enables better retrieval and stance detection,\nas compared to that using several baseline text normalization methods.\nImplementation of our algorithm can be found at\nhttps://github.com/ranarag/UnsupClean.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 06:57:09 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Roy", "Anurag", ""], ["Ghosh", "Shalmoli", ""], ["Ghosh", "Kripabandhu", ""], ["Ghosh", "Saptarshi", ""]]}, {"id": "2101.03327", "submitter": "Alexander Veretennikov Borisovich", "authors": "Alexander B. Veretennikov", "title": "Selection of Optimal Parameters in the Fast K-Word Proximity Search\n  Based on Multi-component Key Indexes", "comments": "Indexing: Scopus", "journal-ref": "Supplementary Proceedings of the XXII International Conference on\n  Data Analytics and Management in Data Intensive Domains (DAMDID/RCDL 2020),\n  Voronezh, Russia, October 13-16, 2020, P. 336-350, CEUR Workshop Proceedings", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proximity full-text search is commonly implemented in contemporary full-text\nsearch systems. Let us assume that the search query is a list of words. It is\nnatural to consider a document as relevant if the queried words are near each\nother in the document. The proximity factor is even more significant for the\ncase where the query consists of frequently occurring words. Proximity\nfull-text search requires the storage of information for every occurrence in\ndocuments of every word that the user can search. For every occurrence of every\nword in a document, we employ additional indexes to store information about\nnearby words, that is, the words that occur in the document at distances from\nthe given word of less than or equal to the MaxDistance parameter. We showed in\nprevious works that these indexes can be used to improve the average query\nexecution time by up to 130 times for queries that consist of words occurring\nwith high-frequency. In this paper, we consider how both the search performance\nand the search quality depend on the value of MaxDistance and other parameters.\nWell-known GOV2 text collection is used in the experiments for reproducibility\nof the results. We propose a new index schema after the analysis of the results\nof the experiments.\n  This is a pre-print of a contribution published in Supplementary Proceedings\nof the XXII International Conference on Data Analytics and Management in Data\nIntensive Domains (DAMDID/RCDL 2020), Voronezh, Russia, October 13-16, 2020, P.\n336-350, published by CEUR Workshop Proceedings. The final authenticated\nversion is available online at: http://ceur-ws.org/Vol-2790/\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 09:53:44 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Veretennikov", "Alexander B.", ""]]}, {"id": "2101.03382", "submitter": "Sayar Ghosh Roy", "authors": "Tathagata Raha, Sayar Ghosh Roy, Ujwal Narayan, Zubair Abid, Vasudeva\n  Varma", "title": "Task Adaptive Pretraining of Transformers for Hostility Detection", "comments": "To be published in: Proceedings of the First Workshop on Combating\n  Online Hostile Posts in Regional Languages during Emergency Situation\n  (CONSTRAINT) at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying adverse and hostile content on the web and more particularly, on\nsocial media, has become a problem of paramount interest in recent years. With\ntheir ever increasing popularity, fine-tuning of pretrained Transformer-based\nencoder models with a classifier head are gradually becoming the new baseline\nfor natural language classification tasks. In our work, we explore the gains\nattributed to Task Adaptive Pretraining (TAPT) prior to fine-tuning of\nTransformer-based architectures. We specifically study two problems, namely,\n(a) Coarse binary classification of Hindi Tweets into Hostile or Not, and (b)\nFine-grained multi-label classification of Tweets into four categories: hate,\nfake, offensive, and defamation. Building up on an architecture which takes\nemojis and segmented hashtags into consideration for classification, we are\nable to experimentally showcase the performance upgrades due to TAPT. Our\nsystem (with team name 'iREL IIIT') ranked first in the 'Hostile Post Detection\nin Hindi' shared task with an F1 score of 97.16% for coarse-grained detection\nand a weighted F1 score of 62.96% for fine-grained multi-label classification\non the provided blind test corpora.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 15:45:26 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Raha", "Tathagata", ""], ["Roy", "Sayar Ghosh", ""], ["Narayan", "Ujwal", ""], ["Abid", "Zubair", ""], ["Varma", "Vasudeva", ""]]}, {"id": "2101.03392", "submitter": "Yongfeng Zhang", "authors": "Hanxiong Chen, Xu Chen, Shaoyun Shi, Yongfeng Zhang", "title": "Generate Natural Language Explanations for Recommendation", "comments": "Accepted to the SIGIR 2019 Workshop on ExplainAble Recommendation and\n  Search, Paris, France, July 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Providing personalized explanations for recommendations can help users to\nunderstand the underlying insight of the recommendation results, which is\nhelpful to the effectiveness, transparency, persuasiveness and trustworthiness\nof recommender systems. Current explainable recommendation models mostly\ngenerate textual explanations based on pre-defined sentence templates. However,\nthe expressiveness power of template-based explanation sentences are limited to\nthe pre-defined expressions, and manually defining the expressions require\nsignificant human efforts. Motivated by this problem, we propose to generate\nfree-text natural language explanations for personalized recommendation. In\nparticular, we propose a hierarchical sequence-to-sequence model (HSS) for\npersonalized explanation generation. Different from conventional sentence\ngeneration in NLP research, a great challenge of explanation generation in\ne-commerce recommendation is that not all sentences in user reviews are of\nexplanation purpose. To solve the problem, we further propose an auto-denoising\nmechanism based on topical item feature words for sentence generation.\nExperiments on various e-commerce product domains show that our approach can\nnot only improve the recommendation accuracy, but also the explanation quality\nin terms of the offline measures and feature words coverage. This research is\none of the initial steps to grant intelligent agents with the ability to\nexplain itself based on natural language sentences.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 17:00:41 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Chen", "Hanxiong", ""], ["Chen", "Xu", ""], ["Shi", "Shaoyun", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2101.03394", "submitter": "Mohammad Aliannejadi", "authors": "Mohammad Aliannejadi and Hamed Zamani and Fabio Crestani and W. Bruce\n  Croft", "title": "Context-Aware Target Apps Selection and Recommendation for Enhancing\n  Personal Mobile Assistants", "comments": "Accepted to ACM TOIS, 30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users install many apps on their smartphones, raising issues related to\ninformation overload for users and resource management for devices. Moreover,\nthe recent increase in the use of personal assistants has made mobile devices\neven more pervasive in users' lives. This paper addresses two research problems\nthat are vital for developing effective personal mobile assistants: target apps\nselection and recommendation. The former is the key component of a unified\nmobile search system: a system that addresses the users' information needs for\nall the apps installed on their devices with a unified mode of access. The\nlatter, instead, predicts the next apps that the users would want to launch.\nHere we focus on context-aware models to leverage the rich contextual\ninformation available to mobile devices. We design an in situ study to collect\nthousands of mobile queries enriched with mobile sensor data (now publicly\navailable for research purposes). With the aid of this dataset, we study the\nuser behavior in the context of these tasks and propose a family of\ncontext-aware neural models that take into account the sequential, temporal,\nand personal behavior of users. We study several state-of-the-art models and\nshow that the proposed models significantly outperform the baselines.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 17:07:47 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Aliannejadi", "Mohammad", ""], ["Zamani", "Hamed", ""], ["Crestani", "Fabio", ""], ["Croft", "W. Bruce", ""]]}, {"id": "2101.03553", "submitter": "Sayar Ghosh Roy", "authors": "Sayar Ghosh Roy, Nikhil Pinnaparaju, Risubh Jain, Manish Gupta,\n  Vasudeva Varma", "title": "Summaformers @ LaySumm 20, LongSumm 20", "comments": "Proceedings of the First Workshop on Scholarly Document Processing\n  (SDP) at EMNLP 2020", "journal-ref": "In Proceedings of the First Workshop on Scholarly Document\n  Processing, pages 336 - 343, 2020, Online. Association for Computational\n  Linguistics", "doi": "10.18653/v1/2020.sdp-1.39", "report-no": "IIIT/TR/2020/75", "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic text summarization has been widely studied as an important task in\nnatural language processing. Traditionally, various feature engineering and\nmachine learning based systems have been proposed for extractive as well as\nabstractive text summarization. Recently, deep learning based, specifically\nTransformer-based systems have been immensely popular. Summarization is a\ncognitively challenging task - extracting summary worthy sentences is\nlaborious, and expressing semantics in brief when doing abstractive\nsummarization is complicated. In this paper, we specifically look at the\nproblem of summarizing scientific research papers from multiple domains. We\ndifferentiate between two types of summaries, namely, (a) LaySumm: A very short\nsummary that captures the essence of the research paper in layman terms\nrestricting overtly specific technical jargon and (b) LongSumm: A much longer\ndetailed summary aimed at providing specific insights into various ideas\ntouched upon in the paper. While leveraging latest Transformer-based models,\nour systems are simple, intuitive and based on how specific paper sections\ncontribute to human summaries of the two types described above. Evaluations\nagainst gold standard summaries using ROUGE metrics prove the effectiveness of\nour approach. On blind test corpora, our system ranks first and third for the\nLongSumm and LaySumm tasks respectively.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 13:48:12 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Roy", "Sayar Ghosh", ""], ["Pinnaparaju", "Nikhil", ""], ["Jain", "Risubh", ""], ["Gupta", "Manish", ""], ["Varma", "Vasudeva", ""]]}, {"id": "2101.03584", "submitter": "Yingqiang Ge", "authors": "Yingqiang Ge, Shuchang Liu, Ruoyuan Gao, Yikun Xian, Yunqi Li, Xiangyu\n  Zhao, Changhua Pei, Fei Sun, Junfeng Ge, Wenwu Ou, Yongfeng Zhang", "title": "Towards Long-term Fairness in Recommendation", "comments": null, "journal-ref": null, "doi": "10.1145/3437963.3441824", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Recommender Systems (RS) influence more and more people in their daily\nlife, the issue of fairness in recommendation is becoming more and more\nimportant. Most of the prior approaches to fairness-aware recommendation have\nbeen situated in a static or one-shot setting, where the protected groups of\nitems are fixed, and the model provides a one-time fairness solution based on\nfairness-constrained optimization. This fails to consider the dynamic nature of\nthe recommender systems, where attributes such as item popularity may change\nover time due to the recommendation policy and user engagement. For example,\nproducts that were once popular may become no longer popular, and vice versa.\nAs a result, the system that aims to maintain long-term fairness on the item\nexposure in different popularity groups must accommodate this change in a\ntimely fashion.\n  Novel to this work, we explore the problem of long-term fairness in\nrecommendation and accomplish the problem through dynamic fairness learning. We\nfocus on the fairness of exposure of items in different groups, while the\ndivision of the groups is based on item popularity, which dynamically changes\nover time in the recommendation process. We tackle this problem by proposing a\nfairness-constrained reinforcement learning algorithm for recommendation, which\nmodels the recommendation problem as a Constrained Markov Decision Process\n(CMDP), so that the model can dynamically adjust its recommendation policy to\nmake sure the fairness requirement is always satisfied when the environment\nchanges. Experiments on several real-world datasets verify our framework's\nsuperiority in terms of recommendation performance, short-term fairness, and\nlong-term fairness.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 17:36:28 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Ge", "Yingqiang", ""], ["Liu", "Shuchang", ""], ["Gao", "Ruoyuan", ""], ["Xian", "Yikun", ""], ["Li", "Yunqi", ""], ["Zhao", "Xiangyu", ""], ["Pei", "Changhua", ""], ["Sun", "Fei", ""], ["Ge", "Junfeng", ""], ["Ou", "Wenwu", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2101.03617", "submitter": "Harsh Kohli Mr.", "authors": "Harsh Kohli", "title": "Transfer Learning and Augmentation for Word Sense Disambiguation", "comments": "10 pages, 3 figures. Accepted at the 43rd European Conference on\n  Information Retrieval (ECIR) 2021", "journal-ref": null, "doi": "10.1007/978-3-030-72240-1_29", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many downstream NLP tasks have shown significant improvement through\ncontinual pre-training, transfer learning and multi-task learning.\nState-of-the-art approaches in Word Sense Disambiguation today benefit from\nsome of these approaches in conjunction with information sources such as\nsemantic relationships and gloss definitions contained within WordNet. Our work\nbuilds upon these systems and uses data augmentation along with extensive\npre-training on various different NLP tasks and datasets. Our transfer learning\nand augmentation pipeline achieves state-of-the-art single model performance in\nWSD and is at par with the best ensemble results.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 19:56:39 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Kohli", "Harsh", ""]]}, {"id": "2101.03654", "submitter": "Yanqiao Zhu", "authors": "Yichen Xu, Yanqiao Zhu, Feng Yu, Qiang Liu, Shu Wu", "title": "Disentangled Self-Attentive Neural Networks for Click-Through Rate\n  Prediction", "comments": "5 pages, work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) prediction, whose aim is to predict the probability\nof whether a user will click on an item, is an essential task for many online\napplications. Due to the nature of data sparsity and high dimensionality in CTR\nprediction, a key to making effective prediction is to model high-order feature\ninteraction among feature fields. To explicitly model high-order feature\ninteraction, an efficient way is to perform inner product of feature embeddings\nwith self-attentive neural networks. To better model complex feature\ninteraction, in this paper we propose a novel DisentanglEd Self-atTentIve\nNEtwork (DESTINE) framework for CTR prediction that explicitly decouples the\ncomputation of unary importance from pairwise interaction. Specifically, the\nunary term models the general impact of one feature on all other features,\nwhereas the whitened pairwise interaction term contributes to learning the pure\nimportance score for each feature interaction. We conduct extensive experiments\nframework using two real-world benchmark datasets. The results show that\nDESTINE not only maintains computational efficiency but obtains performance\nimprovements over state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 01:14:27 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 12:14:13 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Xu", "Yichen", ""], ["Zhu", "Yanqiao", ""], ["Yu", "Feng", ""], ["Liu", "Qiang", ""], ["Wu", "Shu", ""]]}, {"id": "2101.03771", "submitter": "Savvas A Chatzichristofis", "authors": "Socratis Gkelios, Yiannis Boutalis, Savvas A. Chatzichristofis", "title": "Investigating the Vision Transformer Model for Image Retrieval Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a plug-and-play descriptor that can be effectively\nadopted for image retrieval tasks without prior initialization or preparation.\nThe description method utilizes the recently proposed Vision Transformer\nnetwork while it does not require any training data to adjust parameters. In\nimage retrieval tasks, the use of Handcrafted global and local descriptors has\nbeen very successfully replaced, over the last years, by the Convolutional\nNeural Networks (CNN)-based methods. However, the experimental evaluation\nconducted in this paper on several benchmarking datasets against 36\nstate-of-the-art descriptors from the literature demonstrates that a neural\nnetwork that contains no convolutional layer, such as Vision Transformer, can\nshape a global descriptor and achieve competitive results. As fine-tuning is\nnot required, the presented methodology's low complexity encourages adoption of\nthe architecture as an image retrieval baseline model, replacing the\ntraditional and well adopted CNN-based approaches and inaugurating a new era in\nimage retrieval approaches.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 08:59:54 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Gkelios", "Socratis", ""], ["Boutalis", "Yiannis", ""], ["Chatzichristofis", "Savvas A.", ""]]}, {"id": "2101.04012", "submitter": "Raviraj Joshi", "authors": "Apurva Wani, Isha Joshi, Snehal Khandve, Vedangi Wagh, Raviraj Joshi", "title": "Evaluating Deep Learning Approaches for Covid19 Fake News Detection", "comments": "Accepted at Contraint@AAAI 2021", "journal-ref": null, "doi": "10.1007/978-3-030-73696-5_15", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media platforms like Facebook, Twitter, and Instagram have enabled\nconnection and communication on a large scale. It has revolutionized the rate\nat which information is shared and enhanced its reach. However, another side of\nthe coin dictates an alarming story. These platforms have led to an increase in\nthe creation and spread of fake news. The fake news has not only influenced\npeople in the wrong direction but also claimed human lives. During these\ncritical times of the Covid19 pandemic, it is easy to mislead people and make\nthem believe in fatal information. Therefore it is important to curb fake news\nat source and prevent it from spreading to a larger audience. We look at\nautomated techniques for fake news detection from a data mining perspective. We\nevaluate different supervised text classification algorithms on Contraint@AAAI\n2021 Covid-19 Fake news detection dataset. The classification algorithms are\nbased on Convolutional Neural Networks (CNN), Long Short Term Memory (LSTM),\nand Bidirectional Encoder Representations from Transformers (BERT). We also\nevaluate the importance of unsupervised learning in the form of language model\npre-training and distributed word representations using unlabelled covid tweets\ncorpus. We report the best accuracy of 98.41\\% on the Covid-19 Fake news\ndetection dataset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 16:39:03 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 17:18:02 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wani", "Apurva", ""], ["Joshi", "Isha", ""], ["Khandve", "Snehal", ""], ["Wagh", "Vedangi", ""], ["Joshi", "Raviraj", ""]]}, {"id": "2101.04255", "submitter": "Dominic Widdows", "authors": "Dominic Widdows and Kirsty Kitto and Trevor Cohen", "title": "Quantum Mathematics in Artificial Intelligence", "comments": "Manuscript updated to correct one author's email address, and with\n  some extra references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the decade since 2010, successes in artificial intelligence have been at\nthe forefront of computer science and technology, and vector space models have\nsolidified a position at the forefront of artificial intelligence. At the same\ntime, quantum computers have become much more powerful, and announcements of\nmajor advances are frequently in the news.\n  The mathematical techniques underlying both these areas have more in common\nthan is sometimes realized. Vector spaces took a position at the axiomatic\nheart of quantum mechanics in the 1930s, and this adoption was a key motivation\nfor the derivation of logic and probability from the linear geometry of vector\nspaces. Quantum interactions between particles are modelled using the tensor\nproduct, which is also used to express objects and operations in artificial\nneural networks.\n  This paper describes some of these common mathematical areas, including\nexamples of how they are used in artificial intelligence (AI), particularly in\nautomated reasoning and natural language processing (NLP). Techniques discussed\ninclude vector spaces, scalar products, subspaces and implication, orthogonal\nprojection and negation, dual vectors, density matrices, positive operators,\nand tensor products. Application areas include information retrieval,\ncategorization and implication, modelling word-senses and disambiguation,\ninference in knowledge bases, and semantic composition.\n  Some of these approaches can potentially be implemented on quantum hardware.\nMany of the practical steps in this implementation are in early stages, and\nsome are already realized. Explaining some of the common mathematical tools can\nhelp researchers in both AI and quantum computing further exploit these\noverlaps, recognizing and exploring new directions along the way.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 01:35:56 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 20:58:51 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 17:36:32 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Widdows", "Dominic", ""], ["Kitto", "Kirsty", ""], ["Cohen", "Trevor", ""]]}, {"id": "2101.04328", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Yongfeng Huang, Xing Xie", "title": "Neural News Recommendation with Negative Feedback", "comments": null, "journal-ref": "CCF Transactions on Pervasive Computing and Interaction, 2020,\n  2(3): 178-188", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  News recommendation is important for online news services. Precise user\ninterest modeling is critical for personalized news recommendation. Existing\nnews recommendation methods usually rely on the implicit feedback of users like\nnews clicks to model user interest. However, news click may not necessarily\nreflect user interests because users may click a news due to the attraction of\nits title but feel disappointed at its content. The dwell time of news reading\nis an important clue for user interest modeling, since short reading dwell time\nusually indicates low and even negative interest. Thus, incorporating the\nnegative feedback inferred from the dwell time of news reading can improve the\nquality of user modeling. In this paper, we propose a neural news\nrecommendation approach which can incorporate the implicit negative user\nfeedback. We propose to distinguish positive and negative news clicks according\nto their reading dwell time, and respectively learn user representations from\npositive and negative news clicks via a combination of Transformer and additive\nattention network. In addition, we propose to compute a positive click score\nand a negative click score based on the relevance between candidate news\nrepresentations and the user representations learned from the positive and\nnegative news clicks. The final click score is a combination of positive and\nnegative click scores. Besides, we propose an interactive news modeling method\nto consider the relatedness between title and body in news modeling. Extensive\nexperiments on real-world dataset validate that our approach can achieve more\naccurate user interest modeling for news recommendation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 07:10:47 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Huang", "Yongfeng", ""], ["Xie", "Xing", ""]]}, {"id": "2101.04339", "submitter": "Jay Tenenbaum", "authors": "Haim Kaplan, Jay Tenenbaum", "title": "Locality Sensitive Hashing for Efficient Similar Polygon Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locality Sensitive Hashing (LSH) is an effective method of indexing a set of\nitems to support efficient nearest neighbors queries in high-dimensional\nspaces. The basic idea of LSH is that similar items should produce hash\ncollisions with higher probability than dissimilar items.\n  We study LSH for (not necessarily convex) polygons, and use it to give\nefficient data structures for similar shape retrieval. Arkin et al. represent\npolygons by their \"turning function\" - a function which follows the angle\nbetween the polygon's tangent and the $ x $-axis while traversing the perimeter\nof the polygon. They define the distance between polygons to be variations of\nthe $ L_p $ (for $p=1,2$) distance between their turning functions. This metric\nis invariant under translation, rotation and scaling (and the selection of the\ninitial point on the perimeter) and therefore models well the intuitive notion\nof shape resemblance.\n  We develop and analyze LSH near neighbor data structures for several\nvariations of the $ L_p $ distance for functions (for $p=1,2$). By applying our\nschemes to the turning functions of a collection of polygons we obtain\nefficient near neighbor LSH-based structures for polygons. To tune our\nstructures to turning functions of polygons, we prove some new properties of\nthese turning functions that may be of independent interest.\n  As part of our analysis, we address the following problem which is of\nindependent interest. Find the vertical translation of a function $ f $ that is\nclosest in $ L_1 $ distance to a function $ g $. We prove tight bounds on the\napproximation guarantee obtained by the translation which is equal to the\ndifference between the averages of $ g $ and $ f $.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 08:00:50 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 12:21:08 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 11:09:14 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Kaplan", "Haim", ""], ["Tenenbaum", "Jay", ""]]}, {"id": "2101.04356", "submitter": "Gustavo Penha", "authors": "Gustavo Penha and Claudia Hauff", "title": "On the Calibration and Uncertainty of Neural Learning to Rank Models", "comments": "Accepted for publication in the 16th conference of the European\n  Chapter of the Association for Computational Linguistics (EACL'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  According to the Probability Ranking Principle (PRP), ranking documents in\ndecreasing order of their probability of relevance leads to an optimal document\nranking for ad-hoc retrieval. The PRP holds when two conditions are met: [C1]\nthe models are well calibrated, and, [C2] the probabilities of relevance are\nreported with certainty. We know however that deep neural networks (DNNs) are\noften not well calibrated and have several sources of uncertainty, and thus\n[C1] and [C2] might not be satisfied by neural rankers. Given the success of\nneural Learning to Rank (L2R) approaches-and here, especially BERT-based\napproaches-we first analyze under which circumstances deterministic, i.e.\noutputs point estimates, neural rankers are calibrated. Then, motivated by our\nfindings we use two techniques to model the uncertainty of neural rankers\nleading to the proposed stochastic rankers, which output a predictive\ndistribution of relevance as opposed to point estimates. Our experimental\nresults on the ad-hoc retrieval task of conversation response ranking reveal\nthat (i) BERT-based rankers are not robustly calibrated and that stochastic\nBERT-based rankers yield better calibration; and (ii) uncertainty estimation is\nbeneficial for both risk-aware neural ranking, i.e.taking into account the\nuncertainty when ranking documents, and for predicting unanswerable\nconversational contexts.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 09:05:46 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Penha", "Gustavo", ""], ["Hauff", "Claudia", ""]]}, {"id": "2101.04526", "submitter": "Yoni Halpern", "authors": "Sirui Yao and Yoni Halpern and Nithum Thain and Xuezhi Wang and Kang\n  Lee and Flavien Prost and Ed H. Chi and Jilin Chen and Alex Beutel", "title": "Measuring Recommender System Effects with Simulated Users", "comments": "Presented at Second Workshop on Fairness, Accountability,\n  Transparency, Ethics and Society on the Web (FATES 2020) with the title\n  \"Beyond Next Step Bias: Trajectory Simulation for Understanding Recommender\n  System Behavior\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imagine a food recommender system -- how would we check if it is\n\\emph{causing} and fostering unhealthy eating habits or merely reflecting\nusers' interests? How much of a user's experience over time with a recommender\nis caused by the recommender system's choices and biases, and how much is based\non the user's preferences and biases? Popularity bias and filter bubbles are\ntwo of the most well-studied recommender system biases, but most of the prior\nresearch has focused on understanding the system behavior in a single\nrecommendation step. How do these biases interplay with user behavior, and what\ntypes of user experiences are created from repeated interactions?\n  In this work, we offer a simulation framework for measuring the impact of a\nrecommender system under different types of user behavior. Using this\nsimulation framework, we can (a) isolate the effect of the recommender system\nfrom the user preferences, and (b) examine how the system performs not just on\naverage for an \"average user\" but also the extreme experiences under atypical\nuser behavior. As part of the simulation framework, we propose a set of\nevaluation metrics over the simulations to understand the recommender system's\nbehavior. Finally, we present two empirical case studies -- one on traditional\ncollaborative filtering in MovieLens and one on a large-scale production\nrecommender system -- to understand how popularity bias manifests over time.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 14:51:11 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Yao", "Sirui", ""], ["Halpern", "Yoni", ""], ["Thain", "Nithum", ""], ["Wang", "Xuezhi", ""], ["Lee", "Kang", ""], ["Prost", "Flavien", ""], ["Chi", "Ed H.", ""], ["Chen", "Jilin", ""], ["Beutel", "Alex", ""]]}, {"id": "2101.04615", "submitter": "Chau-Wai Wong", "authors": "Jiele Wu, Chau-Wai Wong, Xinyan Zhao, Xianpeng Liu", "title": "Toward Effective Automated Content Analysis via Crowdsourcing", "comments": "Corrected minor typos. Camera-ready version for the 2021 IEEE\n  International Conference on Multimedia and Expo (ICME)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many computer scientists use the aggregated answers of online workers to\nrepresent ground truth. Prior work has shown that aggregation methods such as\nmajority voting are effective for measuring relatively objective features. For\nsubjective features such as semantic connotation, online workers, known for\noptimizing their hourly earnings, tend to deteriorate in the quality of their\nresponses as they work longer. In this paper, we aim to address this issue by\nproposing a quality-aware semantic data annotation system. We observe that with\ntimely feedback on workers' performance quantified by quality scores, better\ninformed online workers can maintain the quality of their labeling throughout\nan extended period of time. We validate the effectiveness of the proposed\nannotation system through i) evaluating performance based on an expert-labeled\ndataset, and ii) demonstrating machine learning tasks that can lead to\nconsistent learning behavior with 70%-80% accuracy. Our results suggest that\nwith our system, researchers can collect high-quality answers of subjective\nsemantic features at a large scale.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 17:14:18 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 23:59:01 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wu", "Jiele", ""], ["Wong", "Chau-Wai", ""], ["Zhao", "Xinyan", ""], ["Liu", "Xianpeng", ""]]}, {"id": "2101.04617", "submitter": "Zhi Hong", "authors": "Zhi Hong, J. Gregory Pauloski, Logan Ward, Kyle Chard, Ben Blaiszik,\n  and Ian Foster", "title": "AI- and HPC-enabled Lead Generation for SARS-CoV-2: Models and Processes\n  to Extract Druglike Molecules Contained in Natural Language Text", "comments": "17 single-column pages, 6 figures, and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers worldwide are seeking to repurpose existing drugs or discover new\ndrugs to counter the disease caused by severe acute respiratory syndrome\ncoronavirus 2 (SARS-CoV-2). A promising source of candidates for such studies\nis molecules that have been reported in the scientific literature to be\ndrug-like in the context of coronavirus research. We report here on a project\nthat leverages both human and artificial intelligence to detect references to\ndrug-like molecules in free text. We engage non-expert humans to create a\ncorpus of labeled text, use this labeled corpus to train a named entity\nrecognition model, and employ the trained model to extract 10912 drug-like\nmolecules from the COVID-19 Open Research Dataset Challenge (CORD-19) corpus of\n198875 papers. Performance analyses show that our automated extraction model\ncan achieve performance on par with that of non-expert humans.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 17:15:43 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Hong", "Zhi", ""], ["Pauloski", "J. Gregory", ""], ["Ward", "Logan", ""], ["Chard", "Kyle", ""], ["Blaiszik", "Ben", ""], ["Foster", "Ian", ""]]}, {"id": "2101.04817", "submitter": "Yongfeng Zhang", "authors": "Yunqi Li, Shuyuan Xu, Bo Liu, Zuohui Fu, Shuchang Liu, Xu Chen,\n  Yongfeng Zhang", "title": "Discrete Knowledge Graph Embedding based on Discrete Optimization", "comments": "Accepted at the AAAI-20 Workshop on Knowledge Discovery from\n  Unstructured Data in Financial Services", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a discrete knowledge graph (KG) embedding (DKGE) method,\nwhich projects KG entities and relations into the Hamming space based on a\ncomputationally tractable discrete optimization algorithm, to solve the\nformidable storage and computation cost challenges in traditional continuous\ngraph embedding methods. The convergence of DKGE can be guaranteed\ntheoretically. Extensive experiments demonstrate that DKGE achieves superior\naccuracy than classical hashing functions that map the effective continuous\nembeddings into discrete codes. Besides, DKGE reaches comparable accuracy with\nmuch lower computational complexity and storage compared to many continuous\ngraph embedding methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 00:23:07 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Li", "Yunqi", ""], ["Xu", "Shuyuan", ""], ["Liu", "Bo", ""], ["Fu", "Zuohui", ""], ["Liu", "Shuchang", ""], ["Chen", "Xu", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2101.04849", "submitter": "Chen Ma", "authors": "Chen Ma, Liheng Ma, Yingxue Zhang, Ruiming Tang, Xue Liu and Mark\n  Coates", "title": "Probabilistic Metric Learning with Adaptive Margin for Top-K\n  Recommendation", "comments": "Accepted by the 26th ACM SIGKDD Conference on Knowledge Discovery and\n  Data Mining (KDD 2020 Research Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalized recommender systems are playing an increasingly important role\nas more content and services become available and users struggle to identify\nwhat might interest them. Although matrix factorization and deep learning based\nmethods have proved effective in user preference modeling, they violate the\ntriangle inequality and fail to capture fine-grained preference information. To\ntackle this, we develop a distance-based recommendation model with several\nnovel aspects: (i) each user and item are parameterized by Gaussian\ndistributions to capture the learning uncertainties; (ii) an adaptive margin\ngeneration scheme is proposed to generate the margins regarding different\ntraining triplets; (iii) explicit user-user/item-item similarity modeling is\nincorporated in the objective function. The Wasserstein distance is employed to\ndetermine preferences because it obeys the triangle inequality and can measure\nthe distance between probabilistic distributions. Via a comparison using five\nreal-world datasets with state-of-the-art methods, the proposed model\noutperforms the best existing models by 4-22% in terms of recall@K on Top-K\nrecommendation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 03:11:04 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Ma", "Chen", ""], ["Ma", "Liheng", ""], ["Zhang", "Yingxue", ""], ["Tang", "Ruiming", ""], ["Liu", "Xue", ""], ["Coates", "Mark", ""]]}, {"id": "2101.04850", "submitter": "Ziyang Liu", "authors": "Ziyang Liu, Zhaomeng Cheng, Yunjiang Jiang, Yue Shang, Wei Xiong,\n  Sulong Xu, Bo Long, Di Jin", "title": "Heterogeneous Network Embedding for Deep Semantic Relevance Match in\n  E-commerce Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Result relevance prediction is an essential task of e-commerce search engines\nto boost the utility of search engines and ensure smooth user experience. The\nlast few years eyewitnessed a flurry of research on the use of\nTransformer-style models and deep text-match models to improve relevance.\nHowever, these two types of models ignored the inherent bipartite network\nstructures that are ubiquitous in e-commerce search logs, making these models\nineffective. We propose in this paper a novel Second-order Relevance, which is\nfundamentally different from the previous First-order Relevance, to improve\nresult relevance prediction. We design, for the first time, an end-to-end\nFirst-and-Second-order Relevance prediction model for e-commerce item\nrelevance. The model is augmented by the neighborhood structures of bipartite\nnetworks that are built using the information of user behavioral feedback,\nincluding clicks and purchases. To ensure that edges accurately encode\nrelevance information, we introduce external knowledge generated from BERT to\nrefine the network of user behaviors. This allows the new model to integrate\ninformation from neighboring items and queries, which are highly relevant to\nthe focus query-item pair under consideration. Results of offline experiments\nshowed that the new model significantly improved the prediction accuracy in\nterms of human relevance judgment. An ablation study showed that the\nFirst-and-Second-order model gained a 4.3% average gain over the First-order\nmodel. Results of an online A/B test revealed that the new model derived more\ncommercial benefits compared to the base model.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 03:12:53 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Liu", "Ziyang", ""], ["Cheng", "Zhaomeng", ""], ["Jiang", "Yunjiang", ""], ["Shang", "Yue", ""], ["Xiong", "Wei", ""], ["Xu", "Sulong", ""], ["Long", "Bo", ""], ["Jin", "Di", ""]]}, {"id": "2101.04852", "submitter": "Chen Ma", "authors": "Chen Ma, Liheng Ma, Yingxue Zhang, Haolun Wu, Xue Liu and Mark Coates", "title": "Knowledge-Enhanced Top-K Recommendation in Poincar\\'e Ball", "comments": "Accepted by the 35th AAAI Conference on Artificial Intelligence (AAAI\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalized recommender systems are increasingly important as more content\nand services become available and users struggle to identify what might\ninterest them. Thanks to the ability for providing rich information, knowledge\ngraphs (KGs) are being incorporated to enhance the recommendation performance\nand interpretability. To effectively make use of the knowledge graph, we\npropose a recommendation model in the hyperbolic space, which facilitates the\nlearning of the hierarchical structure of knowledge graphs. Furthermore, a\nhyperbolic attention network is employed to determine the relative importances\nof neighboring entities of a certain item. In addition, we propose an adaptive\nand fine-grained regularization mechanism to adaptively regularize items and\ntheir neighboring representations. Via a comparison using three real-world\ndatasets with state-of-the-art methods, we show that the proposed model\noutperforms the best existing models by 2-16% in terms of NDCG@K on Top-K\nrecommendation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 03:16:50 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 20:41:23 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ma", "Chen", ""], ["Ma", "Liheng", ""], ["Zhang", "Yingxue", ""], ["Wu", "Haolun", ""], ["Liu", "Xue", ""], ["Coates", "Mark", ""]]}, {"id": "2101.04965", "submitter": "Mohammad Ahmad", "authors": "Mohammed Azhan, Mohammad Ahmad", "title": "LaDiff ULMFiT: A Layer Differentiated training approach for ULMFiT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In our paper, we present Deep Learning models with a layer differentiated\ntraining method which were used for the SHARED TASK@ CONSTRAINT 2021 sub-tasks\nCOVID19 Fake News Detection in English and Hostile Post Detection in Hindi. We\npropose a Layer Differentiated training procedure for training a pre-trained\nULMFiT arXiv:1801.06146 model. We used special tokens to annotate specific\nparts of the tweets to improve language understanding and gain insights on the\nmodel making the tweets more interpretable. The other two submissions included\na modified RoBERTa model and a simple Random Forest Classifier. The proposed\napproach scored a precision and f1 score of 0.96728972 and 0.967324832\nrespectively for sub-task \"COVID19 Fake News Detection in English\". Also,\nCoarse-Grained Hostility f1 Score and Weighted FineGrained f1 score of 0.908648\nand 0.533907 respectively for sub-task Hostile Post Detection in Hindi. The\nproposed approach ranked 61st out of 164 in the sub-task \"COVID19 Fake News\nDetection in English and 18th out of 45 in the sub-task Hostile Post Detection\nin Hindi\".\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 09:52:04 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Azhan", "Mohammed", ""], ["Ahmad", "Mohammad", ""]]}, {"id": "2101.05223", "submitter": "Michael Luby", "authors": "Michael Luby and Thomas Richardson", "title": "Distributed storage algorithms with optimal tradeoffs", "comments": "22 pages, readability will be improved in revised versions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.IR math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  One of the primary objectives of a distributed storage system is to reliably\nstore large amounts of source data for long durations using a large number $N$\nof unreliable storage nodes, each with $c$ bits of storage capacity. Storage\nnodes fail randomly over time and are replaced with nodes of equal capacity\ninitialized to zeroes, and thus bits are erased at some rate $e$. To maintain\nrecoverability of the source data, a repairer continually reads data over a\nnetwork from nodes at an average rate $r$, and generates and writes data to\nnodes based on the read data.\n  The distributed storage source capacity is the maximum amount of source that\ncan be reliably stored for long periods of time. Previous research shows that\nasymptotically the distributed storage source capacity is at most\n$\\left(1-\\frac{e}{2 \\cdot r}\\right) \\cdot N \\cdot c$ as $N$ and $r$ grow.\n  In this work we introduce and analyze algorithms such that asymptotically the\ndistributed storage source data capacity is at least the above equation. Thus,\nthe above equation expresses a fundamental trade-off between network traffic\nand storage overhead to reliably store source data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 17:36:27 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Luby", "Michael", ""], ["Richardson", "Thomas", ""]]}, {"id": "2101.05415", "submitter": "EPTCS", "authors": "Tommaso Dreossi (Amazon Search), Giorgio Ballardin (Amazon Search),\n  Parth Gupta (Amazon Search), Jan Bakus (Amazon Search), Yu-Hsiang Lin (Amazon\n  Search), Vamsi Salaka (Amazon Search)", "title": "Analysis of E-commerce Ranking Signals via Signal Temporal Logic", "comments": "In Proceedings SNR 2020, arXiv:2101.05256", "journal-ref": "EPTCS 331, 2021, pp. 33-42", "doi": "10.4204/EPTCS.331.3", "report-no": null, "categories": "cs.LO cs.FL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The timed position of documents retrieved by learning to rank models can be\nseen as signals. Signals carry useful information such as drop or rise of\ndocuments over time or user behaviors. In this work, we propose to use the\nlogic formalism called Signal Temporal Logic (STL) to characterize document\nbehaviors in ranking accordingly to the specified formulas. Our analysis shows\nthat interesting document behaviors can be easily formalized and detected\nthanks to STL formulas. We validate our idea on a dataset of 100K product\nsignals. Through the presented framework, we uncover interesting patterns, such\nas cold start, warm start, spikes, and inspect how they affect our learning to\nranks models.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 01:54:31 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Dreossi", "Tommaso", "", "Amazon Search"], ["Ballardin", "Giorgio", "", "Amazon Search"], ["Gupta", "Parth", "", "Amazon Search"], ["Bakus", "Jan", "", "Amazon Search"], ["Lin", "Yu-Hsiang", "", "Amazon\n  Search"], ["Salaka", "Vamsi", "", "Amazon Search"]]}, {"id": "2101.05611", "submitter": "Guangneng Hu", "authors": "Guangneng Hu, Qiang Yang", "title": "TrNews: Heterogeneous User-Interest Transfer Learning for News\n  Recommendation", "comments": "EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate how to solve the cross-corpus news recommendation for unseen\nusers in the future. This is a problem where traditional content-based\nrecommendation techniques often fail. Luckily, in real-world recommendation\nservices, some publisher (e.g., Daily news) may have accumulated a large corpus\nwith lots of consumers which can be used for a newly deployed publisher (e.g.,\nPolitical news). To take advantage of the existing corpus, we propose a\ntransfer learning model (dubbed as TrNews) for news recommendation to transfer\nthe knowledge from a source corpus to a target corpus. To tackle the\nheterogeneity of different user interests and of different word distributions\nacross corpora, we design a translator-based transfer-learning strategy to\nlearn a representation mapping between source and target corpora. The learned\ntranslator can be used to generate representations for unseen users in the\nfuture. We show through experiments on real-world datasets that TrNews is\nbetter than various baselines in terms of four metrics. We also show that our\ntranslator is effective among existing transfer strategies.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 13:52:53 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 06:31:04 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Hu", "Guangneng", ""], ["Yang", "Qiang", ""]]}, {"id": "2101.05625", "submitter": "Shalini Pandey", "authors": "Shalini Pandey, Andrew Lan, George Karypis, Jaideep Srivastava", "title": "Learning Student Interest Trajectory for MOOCThread Recommendation", "comments": "Accepted at IEEE ICDM Workshop on Continual Learning and Adaptation\n  for Time Evolving Data, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Massive Open Online Courses (MOOCs) have witnessed immense\ngrowth in popularity. Now, due to the recent Covid19 pandemic situation, it is\nimportant to push the limits of online education. Discussion forums are primary\nmeans of interaction among learners and instructors. However, with growing\nclass size, students face the challenge of finding useful and informative\ndiscussion forums. This problem can be solved by matching the interest of\nstudents with thread contents. The fundamental challenge is that the student\ninterests drift as they progress through the course, and forum contents evolve\nas students or instructors update them. In our paper, we propose to predict\nfuture interest trajectories of students. Our model consists of two key\noperations: 1) Update operation and 2) Projection operation. Update operation\nmodels the inter-dependency between the evolution of student and thread using\ncoupled Recurrent Neural Networks when the student posts on the thread. The\nprojection operation learns to estimate future embedding of students and\nthreads. For students, the projection operation learns the drift in their\ninterests caused by the change in the course topic they study. The projection\noperation for threads exploits how different posts induce varying interest\nlevels in a student according to the thread structure. Extensive\nexperimentation on three real-world MOOC datasets shows that our model\nsignificantly outperforms other baselines for thread recommendation.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 10:23:11 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 05:01:58 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Pandey", "Shalini", ""], ["Lan", "Andrew", ""], ["Karypis", "George", ""], ["Srivastava", "Jaideep", ""]]}, {"id": "2101.05626", "submitter": "Eisa Alanazi", "authors": "Sarah Alqurashi, Btool Hamoui, Abdulaziz Alashaikh, Ahmad Alhindi,\n  Eisa Alanazi", "title": "Eating Garlic Prevents COVID-19 Infection: Detecting Misinformation on\n  the Arabic Content of Twitter", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid growth of social media content during the current pandemic provides\nuseful tools for disseminating information which has also become a root for\nmisinformation. Therefore, there is an urgent need for fact-checking and\neffective techniques for detecting misinformation in social media. In this\nwork, we study the misinformation in the Arabic content of Twitter. We\nconstruct a large Arabic dataset related to COVID-19 misinformation and\ngold-annotate the tweets into two categories: misinformation or not. Then, we\napply eight different traditional and deep machine learning models, with\ndifferent features including word embeddings and word frequency. The word\nembedding models (\\textsc{FastText} and word2vec) exploit more than two million\nArabic tweets related to COVID-19. Experiments show that optimizing the area\nunder the curve (AUC) improves the models' performance and the Extreme Gradient\nBoosting (XGBoost) presents the highest accuracy in detecting COVID-19\nmisinformation online.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 22:52:21 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Alqurashi", "Sarah", ""], ["Hamoui", "Btool", ""], ["Alashaikh", "Abdulaziz", ""], ["Alhindi", "Ahmad", ""], ["Alanazi", "Eisa", ""]]}, {"id": "2101.05641", "submitter": "Jialiang Han", "authors": "Jialiang Han, Yun Ma", "title": "$C^3DRec$: Cloud-Client Cooperative Deep Learning for Temporal\n  Recommendation in the Post-GDPR Era", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile devices enable users to retrieve information at any time and any\nplace. Considering the occasional requirements and fragmentation usage pattern\nof mobile users, temporal recommendation techniques are proposed to improve the\nefficiency of information retrieval on mobile devices by means of accurately\nrecommending items via learning temporal interests with short-term user\ninteraction behaviors. However, the enforcement of privacy-preserving laws and\nregulations, such as GDPR, may overshadow the successful practice of temporal\nrecommendation. The reason is that state-of-the-art recommendation systems\nrequire to gather and process the user data in centralized servers but the\ninteraction behaviors data used for temporal recommendation are usually\nnon-transactional data that are not allowed to gather without the explicit\npermission of users according to GDPR. As a result, if users do not permit\nservices to gather their interaction behaviors data, the temporal\nrecommendation fails to work. To realize the temporal recommendation in the\npost-GDPR era, this paper proposes $C^3DRec$, a cloud-client cooperative deep\nlearning framework of mining interaction behaviors for recommendation while\npreserving user privacy. $C^3DRec$ constructs a global recommendation model on\ncentralized servers using data collected before GDPR and fine-tunes the model\ndirectly on individual local devices using data collected after GDPR. We design\ntwo modes to accomplish the recommendation, i.e. pull mode where candidate\nitems are pulled down onto the devices and fed into the local model to get\nrecommended items, and push mode where the output of the local model is pushed\nonto the server and combined with candidate items to get recommended ones.\nEvaluation results show that $C^3DRec$ achieves comparable recommendation\naccuracy to the centralized approaches, with minimal privacy concern.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 12:49:34 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Han", "Jialiang", ""], ["Ma", "Yun", ""]]}, {"id": "2101.05667", "submitter": "Jimmy Lin", "authors": "Ronak Pradeep, Rodrigo Nogueira, and Jimmy Lin", "title": "The Expando-Mono-Duo Design Pattern for Text Ranking with Pretrained\n  Sequence-to-Sequence Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a design pattern for tackling text ranking problems, dubbed\n\"Expando-Mono-Duo\", that has been empirically validated for a number of ad hoc\nretrieval tasks in different domains. At the core, our design relies on\npretrained sequence-to-sequence models within a standard multi-stage ranking\narchitecture. \"Expando\" refers to the use of document expansion techniques to\nenrich keyword representations of texts prior to inverted indexing. \"Mono\" and\n\"Duo\" refer to components in a reranking pipeline based on a pointwise model\nand a pairwise model that rerank initial candidates retrieved using keyword\nsearch. We present experimental results from the MS MARCO passage and document\nranking tasks, the TREC 2020 Deep Learning Track, and the TREC-COVID challenge\nthat validate our design. In all these tasks, we achieve effectiveness that is\nat or near the state of the art, in some cases using a zero-shot approach that\ndoes not exploit any training data from the target task. To support\nreplicability, implementations of our design pattern are open-sourced in the\nPyserini IR toolkit and PyGaggle neural reranking library.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 15:29:54 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Pradeep", "Ronak", ""], ["Nogueira", "Rodrigo", ""], ["Lin", "Jimmy", ""]]}, {"id": "2101.05993", "submitter": "Guangtao Wang", "authors": "Guangtao Wang, Qinbao Song and Xiaoyan Zhu", "title": "Ensemble Learning Based Classification Algorithm Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending appropriate algorithms to a classification problem is one of the\nmost challenging issues in the field of data mining. The existing algorithm\nrecommendation models are generally constructed on only one kind of\nmeta-features by single learners. Considering that i) ensemble learners usually\nshow better performance and ii) different kinds of meta-features characterize\nthe classification problems in different viewpoints independently, and further\nthe models constructed with different sets of meta-features will be\ncomplementary with each other and applicable for ensemble. This paper proposes\nan ensemble learning-based algorithm recommendation method. To evaluate the\nproposed recommendation method, extensive experiments with 13 well-known\ncandidate classification algorithms and five different kinds of meta-features\nare conducted on 1090 benchmark classification problems. The results show the\neffectiveness of the proposed ensemble learning based recommendation method.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 07:14:51 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Guangtao", ""], ["Song", "Qinbao", ""], ["Zhu", "Xiaoyan", ""]]}, {"id": "2101.06141", "submitter": "Oana Inel", "authors": "Mats Mulder, Oana Inel, Jasper Oosterman, Nava Tintarev", "title": "Operationalizing Framing to Support Multiperspective Recommendations of\n  Opinion Pieces", "comments": "Accepted to ACM FAccT 2021,\n  https://facctconference.org/2021/acceptedpapers.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diversity in personalized news recommender systems is often defined as\ndissimilarity, and based on topic diversity (e.g., corona versus farmers\nstrike). Diversity in news media, however, is understood as multiperspectivity\n(e.g., different opinions on corona measures), and arguably a key\nresponsibility of the press in a democratic society. While viewpoint diversity\nis often considered synonymous with source diversity in communication science\ndomain, in this paper, we take a computational view. We operationalize the\nnotion of framing, adopted from communication science. We apply this notion to\na re-ranking of topic-relevant recommended lists, to form the basis of a novel\nviewpoint diversification method. Our offline evaluation indicates that the\nproposed method is capable of enhancing the viewpoint diversity of\nrecommendation lists according to a diversity metric from literature. In an\nonline study, on the Blendle platform, a Dutch news aggregator platform, with\nmore than 2000 users, we found that users are willing to consume viewpoint\ndiverse news recommendations. We also found that presentation characteristics\nsignificantly influence the reading behaviour of diverse recommendations. These\nresults suggest that future research on presentation aspects of recommendations\ncan be just as important as novel viewpoint diversification methods to truly\nachieve multiperspectivity in online news environments.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 14:40:34 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 14:39:22 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Mulder", "Mats", ""], ["Inel", "Oana", ""], ["Oosterman", "Jasper", ""], ["Tintarev", "Nava", ""]]}, {"id": "2101.06150", "submitter": "Mathieu Roche", "authors": "Sarah Valentin, Elena Arsevska, Aline Vilain, Val\\'erie De Waele,\n  Renaud Lancelot, Mathieu Roche", "title": "Annotation of epidemiological information in animal disease-related news\n  articles: guidelines", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a method for annotation of epidemiological information\nin animal disease-related news articles. The annotation guidelines are generic\nand aim to embrace all animal or zoonotic infectious diseases, regardless of\nthe pathogen involved or its way of transmission (e.g. vector-borne, airborne,\nby contact). The framework relies on the successive annotation of all the\nsentences from a news article. The annotator evaluates the sentences in a\nspecific epidemiological context, corresponding to the publication of the news\narticle.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 14:48:01 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Valentin", "Sarah", ""], ["Arsevska", "Elena", ""], ["Vilain", "Aline", ""], ["De Waele", "Val\u00e9rie", ""], ["Lancelot", "Renaud", ""], ["Roche", "Mathieu", ""]]}, {"id": "2101.06203", "submitter": "Asia Biega", "authors": "Mich\\`ele Finck and Asia Biega", "title": "Reviving Purpose Limitation and Data Minimisation in Personalisation,\n  Profiling and Decision-Making Systems", "comments": "Max Planck Institute for Innovation & Competition Research Paper No.\n  21-04", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper determines, through an interdisciplinary law and computer science\nlens, whether data minimisation and purpose limitation can be meaningfully\nimplemented in data-driven algorithmic systems, including personalisation,\nprofiling and decision-making systems. Our analysis reveals that the two legal\nprinciples continue to play an important role in mitigating the risks of\npersonal data processing, allowing us to rebut claims that they have become\nobsolete. The paper goes beyond this finding, however. We highlight that even\nthough these principles are important safeguards in the systems under\nconsideration, there are important limits to their practical implementation,\nnamely, (i) the difficulties of measuring law and the resulting open\ncomputational research questions as well as a lack of concrete guidelines for\npractitioners; (ii) the unacknowledged trade-offs between various GDPR\nprinciples, notably between data minimisation on the one hand and accuracy or\nfairness on the other; (iii) the lack of practical means of removing personal\ndata from trained models in order to ensure legal compliance; and (iv) the\ninsufficient enforcement of data protection law.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 16:36:29 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Finck", "Mich\u00e8le", ""], ["Biega", "Asia", ""]]}, {"id": "2101.06286", "submitter": "M. Mehdi Afsar", "authors": "M. Mehdi Afsar, Trafford Crump, Behrouz Far", "title": "Reinforcement learning based recommender systems: A survey", "comments": "Submitted to ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems (RSs) are becoming an inseparable part of our everyday\nlives. They help us find our favorite items to purchase, our friends on social\nnetworks, and our favorite movies to watch. Traditionally, the recommendation\nproblem was considered as a simple classification or prediction problem;\nhowever, the sequential nature of the recommendation problem has been shown.\nAccordingly, it can be formulated as a Markov decision process (MDP) and\nreinforcement learning (RL) methods can be employed to solve it. In fact,\nrecent advances in combining deep learning with traditional RL methods, i.e.\ndeep reinforcement learning (DRL), has made it possible to apply RL to the\nrecommendation problem with massive state and action spaces. In this paper, a\nsurvey on reinforcement learning based recommender systems (RLRSs) is\npresented. We first recognize the fact that algorithms developed for RLRSs can\nbe generally classified into RL- and DRL-based methods. Then, we present these\nRL- and DRL-based methods in a classified manner based on the specific RL\nalgorithm, e.g., Q-learning, SARSA, and REINFORCE, that is used to optimize the\nrecommendation policy. Furthermore, some tables are presented that contain\ndetailed information about the MDP formulation of these methods, as well as\nabout their evaluation schemes. Finally, we discuss important aspects and\nchallenges that can be addressed in the future.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 19:42:10 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Afsar", "M. Mehdi", ""], ["Crump", "Trafford", ""], ["Far", "Behrouz", ""]]}, {"id": "2101.06327", "submitter": "Zhenduo Wang", "authors": "Zhenduo Wang and Qingyao Ai", "title": "Controlling the Risk of Conversational Search via Reinforcement Learning", "comments": "This paper is accepted by The Web Conference (WWW) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users often formulate their search queries with immature language without\nwell-developed keywords and complete structures. Such queries fail to express\ntheir true information needs and raise ambiguity as fragmental language often\nyield various interpretations and aspects. This gives search engines a hard\ntime processing and understanding the query, and eventually leads to\nunsatisfactory retrieval results. An alternative approach to direct answer\nwhile facing an ambiguous query is to proactively ask clarifying questions to\nthe user. Recent years have seen many works and shared tasks from both NLP and\nIR community about identifying the need for asking clarifying question and\nmethodology to generate them. An often neglected fact by these works is that\nalthough sometimes the need for clarifying questions is correctly recognized,\nthe clarifying questions these system generate are still off-topic and\ndissatisfaction provoking to users and may just cause users to leave the\nconversation.\n  In this work, we propose a risk-aware conversational search agent model to\nbalance the risk of answering user's query and asking clarifying questions. The\nagent is fully aware that asking clarifying questions can potentially collect\nmore information from user, but it will compare all the choices it has and\nevaluate the risks. Only after then, it will make decision between answering or\nasking. To demonstrate that our system is able to retrieve better answers, we\nconduct experiments on the MSDialog dataset which contains real-world customer\nservice conversations from Microsoft products community. We also purpose a\nreinforcement learning strategy which allows us to train our model on the\noriginal dataset directly and saves us from any further data annotation\nefforts. Our experiment results show that our risk-aware conversational search\nagent is able to significantly outperform strong non-risk-aware baselines.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 23:28:40 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Zhenduo", ""], ["Ai", "Qingyao", ""]]}, {"id": "2101.06387", "submitter": "Hansi Zeng", "authors": "Hansi Zeng, Zhichao Xu, Qingyao Ai", "title": "A Zero Attentive Relevance Matching Networkfor Review Modeling in\n  Recommendation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  User and item reviews are valuable for the construction of recommender\nsystems. In general, existing review-based methods for recommendation can be\nbroadly categorized into two groups: the siamese models that build static user\nand item representations from their reviews respectively, and the\ninteraction-based models that encode user and item dynamically according to the\nsimilarity or relationships of their reviews. Although the interaction-based\nmodels have more model capacity and fit human purchasing behavior better,\nseveral problematic model designs and assumptions of the existing\ninteraction-based models lead to its suboptimal performance compared to\nexisting siamese models. In this paper, we identify three problems of the\nexisting interaction-based recommendation models and propose a couple of\nsolutions as well as a new interaction-based model to incorporate review data\nfor rating prediction. Our model implements a relevance matching model with\nregularized training losses to discover user relevant information from long\nitem reviews, and it also adapts a zero attention strategy to dynamically\nbalance the item-dependent and item-independent information extracted from user\nreviews. Empirical experiments and case studies on Amazon Product Benchmark\ndatasets show that our model can extract effective and interpretable user/item\nrepresentations from their reviews and outperforms multiple types of\nstate-of-the-art review-based recommendation models.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 07:01:34 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zeng", "Hansi", ""], ["Xu", "Zhichao", ""], ["Ai", "Qingyao", ""]]}, {"id": "2101.06423", "submitter": "Liang Pang", "authors": "Liang Pang, Yanyan Lan, Xueqi Cheng", "title": "Match-Ignition: Plugging PageRank into Transformer for Long-form Text\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic text matching models have been widely used in community question\nanswering, information retrieval, and dialogue. However, these models cannot\nwell address the long-form text matching problem. That is because there are\nusually many noises in the setting of long-form text matching, and it is\ndifficult for existing semantic text matching to capture the key matching\nsignals from this noisy information. Besides, these models are computationally\nexpensive because they simply use all textual data indiscriminately in the\nmatching process. To tackle the effectiveness and efficiency problem, we\npropose a novel hierarchical noise filtering model in this paper, namely\nMatch-Ignition. The basic idea is to plug the well-known PageRank algorithm\ninto the Transformer, to identify and filter both sentence and word level noisy\ninformation in the matching process. Noisy sentences are usually easy to detect\nbecause the sentence is the basic unit of a long-form text, so we directly use\nPageRank to filter such information, based on a sentence similarity graph.\nWhile words need to rely on their contexts to express concrete meanings, so we\npropose to jointly learn the filtering process and the matching process, to\nreflect the contextual dependencies between words. Specifically, a word graph\nis first built based on the attention scores in each self-attention block of\nTransformer, and keywords are then selected by applying PageRank on this graph.\nIn this way, noisy words will be filtered out layer by layer in the matching\nprocess. Experimental results show that Match-Ignition outperforms both\ntraditional text matching models for short text and recent long-form text\nmatching models. We also conduct detailed analysis to show that Match-Ignition\ncan efficiently capture important sentences or words, which are helpful for\nlong-form text matching.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 10:34:03 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Pang", "Liang", ""], ["Lan", "Yanyan", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2101.06426", "submitter": "Jie Yang", "authors": "Jie Yang, Soyeon Caren Han, Josiah Poon", "title": "A Survey on Extraction of Causal Relations from Natural Language Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an essential component of human cognition, cause-effect relations appear\nfrequently in text, and curating cause-effect relations from text helps in\nbuilding causal networks for predictive tasks. Existing causality extraction\ntechniques include knowledge-based, statistical machine learning(ML)-based, and\ndeep learning-based approaches. Each method has its advantages and weaknesses.\nFor example, knowledge-based methods are understandable but require extensive\nmanual domain knowledge and have poor cross-domain applicability. Statistical\nmachine learning methods are more automated because of natural language\nprocessing (NLP) toolkits. However, feature engineering is labor-intensive, and\ntoolkits may lead to error propagation. In the past few years, deep learning\ntechniques attract substantial attention from NLP researchers because of its'\npowerful representation learning ability and the rapid increase in\ncomputational resources. Their limitations include high computational costs and\na lack of adequate annotated training data. In this paper, we conduct a\ncomprehensive survey of causality extraction. We initially introduce primary\nforms existing in the causality extraction: explicit intra-sentential\ncausality, implicit causality, and inter-sentential causality. Next, we list\nbenchmark datasets and modeling assessment methods for causal relation\nextraction. Then, we present a structured overview of the three techniques with\ntheir representative systems. Lastly, we highlight existing open challenges\nwith their potential directions.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 10:49:39 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Yang", "Jie", ""], ["Han", "Soyeon Caren", ""], ["Poon", "Josiah", ""]]}, {"id": "2101.06448", "submitter": "Junliang Yu", "authors": "Junliang Yu, Hongzhi Yin, Jundong Li, Qinyong Wang, Nguyen Quoc Viet\n  Hung, Xiangliang Zhang", "title": "Self-Supervised Multi-Channel Hypergraph Convolutional Network for\n  Social Recommendation", "comments": "12 pages, Accepted to WWW'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Social relations are often used to improve recommendation quality when\nuser-item interaction data is sparse in recommender systems. Most existing\nsocial recommendation models exploit pairwise relations to mine potential user\npreferences. However, real-life interactions among users are very complicated\nand user relations can be high-order. Hypergraph provides a natural way to\nmodel complex high-order relations, while its potentials for improving social\nrecommendation are under-explored. In this paper, we fill this gap and propose\na multi-channel hypergraph convolutional network to enhance social\nrecommendation by leveraging high-order user relations. Technically, each\nchannel in the network encodes a hypergraph that depicts a common high-order\nuser relation pattern via hypergraph convolution. By aggregating the embeddings\nlearned through multiple channels, we obtain comprehensive user representations\nto generate recommendation results. However, the aggregation operation might\nalso obscure the inherent characteristics of different types of high-order\nconnectivity information. To compensate for the aggregating loss, we\ninnovatively integrate self-supervised learning into the training of the\nhypergraph convolutional network to regain the connectivity information with\nhierarchical mutual information maximization. The experimental results on\nmultiple real-world datasets show that the proposed model outperforms the SOTA\nmethods, and the ablation study verifies the effectiveness of the multi-channel\nsetting and the self-supervised task. The implementation of our model is\navailable via https://github.com/Coder-Yu/RecQ.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 14:20:32 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 21:13:42 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2021 18:16:41 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Yu", "Junliang", ""], ["Yin", "Hongzhi", ""], ["Li", "Jundong", ""], ["Wang", "Qinyong", ""], ["Hung", "Nguyen Quoc Viet", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "2101.06484", "submitter": "Hamed Jelodar", "authors": "Hamed Jelodar, Rita Orji, Stan Matwin, Swarna Weerasinghe, Oladapo\n  Oyebode, Yongli Wang", "title": "Artificial Intelligence for Emotion-Semantic Trending and People Emotion\n  Detection During COVID-19 Social Isolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Taking advantage of social media platforms, such as Twitter, this paper\nprovides an effective framework for emotion detection among those who are\nquarantined. Early detection of emotional feelings and their trends help\nimplement timely intervention strategies. Given the limitations of medical\ndiagnosis of early emotional change signs during the quarantine period,\nartificial intelligence models provide effective mechanisms in uncovering early\nsigns, symptoms and escalating trends. Novelty of the approach presented herein\nis a multitask methodological framework of text data processing, implemented as\na pipeline for meaningful emotion detection and analysis, based on the\nPlutchik/Ekman approach to emotion detection and trend detection. We present an\nevaluation of the framework and a pilot system. Results of confirm the\neffectiveness of the proposed framework for topic trends and emotion detection\nof COVID-19 tweets. Our findings revealed Stay-At-Home restrictions result in\npeople expressing on twitter both negative and positive emotional semantics.\nSemantic trends of safety issues related to staying at home rapidly decreased\nwithin the 28 days and also negative feelings related to friends dying and\nquarantined life increased in some days. These findings have potential to\nimpact public health policy decisions through monitoring trends of emotional\nfeelings of those who are quarantined. The framework presented here has\npotential to assist in such monitoring by using as an online emotion detection\ntool kit.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 17:20:33 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Jelodar", "Hamed", ""], ["Orji", "Rita", ""], ["Matwin", "Stan", ""], ["Weerasinghe", "Swarna", ""], ["Oyebode", "Oladapo", ""], ["Wang", "Yongli", ""]]}, {"id": "2101.06637", "submitter": "Rabia Azzi", "authors": "Rabia Azzi and Gayo Diallo", "title": "AMALGAM: A Matching Approach to fairfy tabuLar data with knowledGe grAph\n  Model", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present AMALGAM, a matching approach to fairify tabular data\nwith the use of a knowledge graph. The ultimate goal is to provide fast and\nefficient approach to annotate tabular data with entities from a background\nknowledge. The approach combines lookup and filtering services combined with\ntext pre-processing techniques. Experiments conducted in the context of the\n2020 Semantic Web Challenge on Tabular Data to Knowledge Graph Matching with\nboth Column Type Annotation and Cell Type Annotation tasks showed promising\nresults.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 10:17:06 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Azzi", "Rabia", ""], ["Diallo", "Gayo", ""]]}, {"id": "2101.06821", "submitter": "Hung Du", "authors": "Yong-Bin Kang, Hung Du, Abdur Rahim Mohammad Forkan, Prem Prakash\n  Jayaraman, Amir Aryani, Timos Sellis (Fellow, IEEE)", "title": "ExpFinder: An Ensemble Expert Finding Model Integrating $N$-gram Vector\n  Space Model and $\\mu$CO-HITS", "comments": "15 pages, 18 figures, \"for source code on Github, see\n  https://github.com/Yongbinkang/ExpFinder\", \"Submitted to IEEE Transactions on\n  Knowledge and Data Engineering\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding an expert plays a crucial role in driving successful collaborations\nand speeding up high-quality research development and innovations. However, the\nrapid growth of scientific publications and digital expertise data makes\nidentifying the right experts a challenging problem. Existing approaches for\nfinding experts given a topic can be categorised into information retrieval\ntechniques based on vector space models, document language models, and\ngraph-based models. In this paper, we propose $\\textit{ExpFinder}$, a new\nensemble model for expert finding, that integrates a novel $N$-gram vector\nspace model, denoted as $n$VSM, and a graph-based model, denoted as\n$\\textit{$\\mu$CO-HITS}$, that is a proposed variation of the CO-HITS algorithm.\nThe key of $n$VSM is to exploit recent inverse document frequency weighting\nmethod for $N$-gram words and $\\textit{ExpFinder}$ incorporates $n$VSM into\n$\\textit{$\\mu$CO-HITS}$ to achieve expert finding. We comprehensively evaluate\n$\\textit{ExpFinder}$ on four different datasets from the academic domains in\ncomparison with six different expert finding models. The evaluation results\nshow that $\\textit{ExpFinder}$ is a highly effective model for expert finding,\nsubstantially outperforming all the compared models in 19% to 160.2%.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 00:44:21 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kang", "Yong-Bin", "", "Fellow, IEEE"], ["Du", "Hung", "", "Fellow, IEEE"], ["Forkan", "Abdur Rahim Mohammad", "", "Fellow, IEEE"], ["Jayaraman", "Prem Prakash", "", "Fellow, IEEE"], ["Aryani", "Amir", "", "Fellow, IEEE"], ["Sellis", "Timos", "", "Fellow, IEEE"]]}, {"id": "2101.06927", "submitter": "Peter M\\\"ullner", "authors": "Peter M\\\"ullner, Dominik Kowald, Elisabeth Lex", "title": "Robustness of Meta Matrix Factorization Against Strict Privacy\n  Constraints", "comments": "Accepted at ECIR 2021, Reproducibility Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the reproducibility of MetaMF, a meta matrix\nfactorization framework introduced by Lin et al. MetaMF employs meta learning\nfor federated rating prediction to preserve users' privacy. We reproduce the\nexperiments of Lin et al. on five datasets, i.e., Douban, Hetrec-MovieLens,\nMovieLens 1M, Ciao, and Jester. Also, we study the impact of meta learning on\nthe accuracy of MetaMF's recommendations. Furthermore, in our work, we\nacknowledge that users may have different tolerances for revealing information\nabout themselves. Hence, in a second strand of experiments, we investigate the\nrobustness of MetaMF against strict privacy constraints. Our study illustrates\nthat we can reproduce most of Lin et al.'s results. Plus, we provide strong\nevidence that meta learning is essential for MetaMF's robustness against strict\nprivacy constraints.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 08:30:00 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 06:12:40 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["M\u00fcllner", "Peter", ""], ["Kowald", "Dominik", ""], ["Lex", "Elisabeth", ""]]}, {"id": "2101.06980", "submitter": "Sebastian Hofst\\\"atter", "authors": "Sebastian Hofst\\\"atter, Aldo Lipani, Sophia Althammer, Markus\n  Zlabinger, Allan Hanbury", "title": "Mitigating the Position Bias of Transformer Models in Passage Re-Ranking", "comments": "Accepted at ECIR 2021 (Full paper track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised machine learning models and their evaluation strongly depends on\nthe quality of the underlying dataset. When we search for a relevant piece of\ninformation it may appear anywhere in a given passage. However, we observe a\nbias in the position of the correct answer in the text in two popular Question\nAnswering datasets used for passage re-ranking. The excessive favoring of\nearlier positions inside passages is an unwanted artefact. This leads to three\ncommon Transformer-based re-ranking models to ignore relevant parts in unseen\npassages. More concerningly, as the evaluation set is taken from the same\nbiased distribution, the models overfitting to that bias overestimate their\ntrue effectiveness. In this work we analyze position bias on datasets, the\ncontextualized representations, and their effect on retrieval results. We\npropose a debiasing method for retrieval datasets. Our results show that a\nmodel trained on a position-biased dataset exhibits a significant decrease in\nre-ranking effectiveness when evaluated on a debiased dataset. We demonstrate\nthat by mitigating the position bias, Transformer-based re-ranking models are\nequally effective on a biased and debiased dataset, as well as more effective\nin a transfer-learning setting between two differently biased datasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:38:03 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Hofst\u00e4tter", "Sebastian", ""], ["Lipani", "Aldo", ""], ["Althammer", "Sophia", ""], ["Zlabinger", "Markus", ""], ["Hanbury", "Allan", ""]]}, {"id": "2101.06983", "submitter": "Luyu Gao", "authors": "Luyu Gao, Yunyi Zhang, Jiawei Han, Jamie Callan", "title": "Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup", "comments": "RepL4NLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive learning has been applied successfully to learn vector\nrepresentations of text. Previous research demonstrated that learning\nhigh-quality representations benefits from batch-wise contrastive loss with a\nlarge number of negatives. In practice, the technique of in-batch negative is\nused, where for each example in a batch, other batch examples' positives will\nbe taken as its negatives, avoiding encoding extra negatives. This, however,\nstill conditions each example's loss on all batch examples and requires fitting\nthe entire large batch into GPU memory. This paper introduces a gradient\ncaching technique that decouples backpropagation between contrastive loss and\nthe encoder, removing encoder backward pass data dependency along the batch\ndimension. As a result, gradients can be computed for one subset of the batch\nat a time, leading to almost constant memory usage.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:42:34 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 16:37:28 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Gao", "Luyu", ""], ["Zhang", "Yunyi", ""], ["Han", "Jiawei", ""], ["Callan", "Jamie", ""]]}, {"id": "2101.06984", "submitter": "Jesus Lovon", "authors": "Jesus Lovon-Melgarejo, Laure Soulier, Karen Pinel-Sauvagnat, Lynda\n  Tamine", "title": "Studying Catastrophic Forgetting in Neural Ranking Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Several deep neural ranking models have been proposed in the recent IR\nliterature. While their transferability to one target domain held by a dataset\nhas been widely addressed using traditional domain adaptation strategies, the\nquestion of their cross-domain transferability is still under-studied. We study\nhere in what extent neural ranking models catastrophically forget old knowledge\nacquired from previously observed domains after acquiring new knowledge,\nleading to performance decrease on those domains. Our experiments show that the\neffectiveness of neuralIR ranking models is achieved at the cost of\ncatastrophic forgetting and that a lifelong learning strategy using a\ncross-domain regularizer success-fully mitigates the problem. Using an\nexplanatory approach built on a regression model, we also show the effect of\ndomain characteristics on the rise of catastrophic forgetting. We believe that\nthe obtained results can be useful for both theoretical and practical future\nwork in neural IR.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:42:57 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Lovon-Melgarejo", "Jesus", ""], ["Soulier", "Laure", ""], ["Pinel-Sauvagnat", "Karen", ""], ["Tamine", "Lynda", ""]]}, {"id": "2101.07124", "submitter": "Bhaskar Mitra", "authors": "Jaime Arguello, Adam Ferguson, Emery Fine, Bhaskar Mitra, Hamed Zamani\n  and Fernando Diaz", "title": "Tip of the Tongue Known-Item Retrieval: A Case Study in Movie\n  Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While current information retrieval systems are effective for known-item\nretrieval where the searcher provides a precise name or identifier for the item\nbeing sought, systems tend to be much less effective for cases where the\nsearcher is unable to express a precise name or identifier. We refer to this as\ntip of the tongue (TOT) known-item retrieval, named after the cognitive state\nof not being able to retrieve an item from memory. Using movie search as a case\nstudy, we explore the characteristics of questions posed by searchers in TOT\nstates in a community question answering website. We analyze how searchers\nexpress their information needs during TOT states in the movie domain.\nSpecifically, what information do searchers remember about the item being\nsought and how do they convey this information? Our results suggest that\nsearchers use a combination of information about: (1) the content of the item\nsought, (2) the context in which they previously engaged with the item, and (3)\nprevious attempts to find the item using other resources (e.g., search\nengines). Additionally, searchers convey information by sometimes expressing\nuncertainty (i.e., hedging), opinions, emotions, and by performing relative\n(vs. absolute) comparisons with attributes of the item. As a result of our\nanalysis, we believe that searchers in TOT states may require specialized query\nunderstanding methods or document representations. Finally, our preliminary\nretrieval experiments show the impact of each information type presented in\ninformation requests on retrieval performance.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 15:33:46 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Arguello", "Jaime", ""], ["Ferguson", "Adam", ""], ["Fine", "Emery", ""], ["Mitra", "Bhaskar", ""], ["Zamani", "Hamed", ""], ["Diaz", "Fernando", ""]]}, {"id": "2101.07321", "submitter": "Krenare Pireva Nuci", "authors": "Vedat Apuk, Krenare Pireva Nu\\c{c}i", "title": "Classification of Pedagogical content using conventional machine\n  learning and deep learning model", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advent of the Internet and a large number of digital technologies has\nbrought with it many different challenges. A large amount of data is found on\nthe web, which in most cases is unstructured and unorganized, and this\ncontributes to the fact that the use and manipulation of this data is quite a\ndifficult process. Due to this fact, the usage of different machine and deep\nlearning techniques for Text Classification has gained its importance, which\nimproved this discipline and made it more interesting for scientists and\nresearchers for further study. This paper aims to classify the pedagogical\ncontent using two different models, the K-Nearest Neighbor (KNN) from the\nconventional models and the Long short-term memory (LSTM) recurrent neural\nnetwork from the deep learning models. The result indicates that the accuracy\nof classifying the pedagogical content reaches 92.52 % using KNN model and\n87.71 % using LSTM model.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 20:29:34 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Apuk", "Vedat", ""], ["Nu\u00e7i", "Krenare Pireva", ""]]}, {"id": "2101.07382", "submitter": "Nikos Voskarides", "authors": "Svitlana Vakulenko, Nikos Voskarides, Zhucheng Tu, Shayne Longpre", "title": "A Comparison of Question Rewriting Methods for Conversational Passage\n  Retrieval", "comments": "ECIR 2021 short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational passage retrieval relies on question rewriting to modify the\noriginal question so that it no longer depends on the conversation history.\nSeveral methods for question rewriting have recently been proposed, but they\nwere compared under different retrieval pipelines. We bridge this gap by\nthoroughly evaluating those question rewriting methods on the TREC CAsT 2019\nand 2020 datasets under the same retrieval pipeline. We analyze the effect of\ndifferent types of question rewriting methods on retrieval performance and show\nthat by combining question rewriting methods of different types we can achieve\nstate-of-the-art performance on both datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 00:17:52 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["Voskarides", "Nikos", ""], ["Tu", "Zhucheng", ""], ["Longpre", "Shayne", ""]]}, {"id": "2101.07481", "submitter": "Riku Togashi", "authors": "Riku Togashi, Masahiro Kato, Mayu Otani, Shin'ichi Satoh", "title": "Density-Ratio Based Personalised Ranking from Implicit Feedback", "comments": "Accepted by WWW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from implicit user feedback is challenging as we can only observe\npositive samples but never access negative ones. Most conventional methods cope\nwith this issue by adopting a pairwise ranking approach with negative sampling.\nHowever, the pairwise ranking approach has a severe disadvantage in the\nconvergence time owing to the quadratically increasing computational cost with\nrespect to the sample size; it is problematic, particularly for large-scale\ndatasets and complex models such as neural networks. By contrast, a pointwise\napproach does not directly solve a ranking problem, and is therefore inferior\nto a pairwise counterpart in top-K ranking tasks; however, it is generally\nadvantageous in regards to the convergence time. This study aims to establish\nan approach to learn personalised ranking from implicit feedback, which\nreconciles the training efficiency of the pointwise approach and ranking\neffectiveness of the pairwise counterpart. The key idea is to estimate the\nranking of items in a pointwise manner; we first reformulate the conventional\npointwise approach based on density ratio estimation and then incorporate the\nessence of ranking-oriented approaches (e.g. the pairwise approach) into our\nformulation. Through experiments on three real-world datasets, we demonstrate\nthat our approach not only dramatically reduces the convergence time (one to\ntwo orders of magnitude faster) but also significantly improving the ranking\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 06:38:57 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Togashi", "Riku", ""], ["Kato", "Masahiro", ""], ["Otani", "Mayu", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "2101.07577", "submitter": "Chen Gao", "authors": "Siyi Liu, Chen Gao, Yihong Chen, Depeng Jin, Yong Li", "title": "Learnable Embedding Sizes for Recommender Systems", "comments": "International Conference on Learning Representations (ICLR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The embedding-based representation learning is commonly used in deep learning\nrecommendation models to map the raw sparse features to dense vectors. The\ntraditional embedding manner that assigns a uniform size to all features has\ntwo issues. First, the numerous features inevitably lead to a gigantic\nembedding table that causes a high memory usage cost. Second, it is likely to\ncause the over-fitting problem for those features that do not require too large\nrepresentation capacity. Existing works that try to address the problem always\ncause a significant drop in recommendation performance or suffers from the\nlimitation of unaffordable training time cost. In this paper, we proposed a\nnovel approach, named PEP (short for Plug-in Embedding Pruning), to reduce the\nsize of the embedding table while avoiding the drop of recommendation accuracy.\nPEP prunes embedding parameter where the pruning threshold(s) can be adaptively\nlearned from data. Therefore we can automatically obtain a mixed-dimension\nembedding-scheme by pruning redundant parameters for each feature. PEP is a\ngeneral framework that can plug in various base recommendation models.\nExtensive experiments demonstrate it can efficiently cut down embedding\nparameters and boost the base model's performance. Specifically, it achieves\nstrong recommendation performance while reducing 97-99% parameters. As for the\ncomputation cost, PEP only brings an additional 20-30% time cost compared with\nbase models. Codes are available at\nhttps://github.com/ssui-liu/learnable-embed-sizes-for-RecSys.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:50:33 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 10:38:59 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Liu", "Siyi", ""], ["Gao", "Chen", ""], ["Chen", "Yihong", ""], ["Jin", "Depeng", ""], ["Li", "Yong", ""]]}, {"id": "2101.07609", "submitter": "Chengzhi Zhang", "authors": "Shutian Ma, Heng Zhang, Chengzhi Zhang, Xiaozhong Liu", "title": "Chronological Citation Recommendation with Time Preference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Citation recommendation is an important task to assist scholars in finding\ncandidate literature to cite. Traditional studies focus on static models of\nrecommending citations, which do not explicitly distinguish differences between\npapers that are caused by temporal variations. Although, some researchers have\ninvestigated chronological citation recommendation by adding time related\nfunction or modeling textual topics dynamically. These solutions can hardly\ncope with function generalization or cold-start problems when there is no\ninformation for user profiling or there are isolated papers never being cited.\nWith the rise and fall of science paradigms, scientific topics tend to change\nand evolve over time. People would have the time preference when citing papers,\nsince most of the theoretical basis exist in classical readings that published\nin old time, while new techniques are proposed in more recent papers. To\nexplore chronological citation recommendation, this paper wants to predict the\ntime preference based on user queries, which is a probability distribution of\nciting papers published in different time slices. Then, we use this time\npreference to re-rank the initial citation list obtained by content-based\nfiltering. Experimental results demonstrate that task performance can be\nfurther enhanced by time preference and it's flexible to be added in other\ncitation recommendation frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 13:18:05 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Ma", "Shutian", ""], ["Zhang", "Heng", ""], ["Zhang", "Chengzhi", ""], ["Liu", "Xiaozhong", ""]]}, {"id": "2101.07918", "submitter": "HongChien Yu", "authors": "HongChien Yu, Zhuyun Dai, Jamie Callan", "title": "PGT: Pseudo Relevance Feedback Using a Graph-Based Transformer", "comments": "Accepted at ECIR 2021 (short paper track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most research on pseudo relevance feedback (PRF) has been done in vector\nspace and probabilistic retrieval models. This paper shows that\nTransformer-based rerankers can also benefit from the extra context that PRF\nprovides. It presents PGT, a graph-based Transformer that sparsifies attention\nbetween graph nodes to enable PRF while avoiding the high computational\ncomplexity of most Transformer architectures. Experiments show that PGT\nimproves upon non-PRF Transformer reranker, and it is at least as accurate as\nTransformer PRF models that use full attention, but with lower computational\ncosts.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 01:07:47 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Yu", "HongChien", ""], ["Dai", "Zhuyun", ""], ["Callan", "Jamie", ""]]}, {"id": "2101.08197", "submitter": "Rafael Ferreira", "authors": "Rafael Ferreira, Mariana Leite, David Semedo and Joao Magalhaes", "title": "Open-Domain Conversational Search Assistant with Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain conversational search assistants aim at answering user questions\nabout open topics in a conversational manner. In this paper we show how the\nTransformer architecture achieves state-of-the-art results in key IR tasks,\nleveraging the creation of conversational assistants that engage in open-domain\nconversational search with single, yet informative, answers. In particular, we\npropose an open-domain abstractive conversational search agent pipeline to\naddress two major challenges: first, conversation context-aware search and\nsecond, abstractive search-answers generation. To address the first challenge,\nthe conversation context is modeled with a query rewriting method that unfolds\nthe context of the conversation up to a specific moment to search for the\ncorrect answers. These answers are then passed to a Transformer-based re-ranker\nto further improve retrieval performance. The second challenge, is tackled with\nrecent Abstractive Transformer architectures to generate a digest of the top\nmost relevant passages. Experiments show that Transformers deliver a solid\nperformance across all tasks in conversational search, outperforming the best\nTREC CAsT 2019 baseline.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 16:02:15 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Ferreira", "Rafael", ""], ["Leite", "Mariana", ""], ["Semedo", "David", ""], ["Magalhaes", "Joao", ""]]}, {"id": "2101.08293", "submitter": "Anastasios Nentidis", "authors": "Anastasios Nentidis, Anastasia Krithara, Grigorios Tsoumakas, Georgios\n  Paliouras", "title": "What is all this new MeSH about? Exploring the semantic provenance of\n  new descriptors in the MeSH thesaurus", "comments": "18 pages, 14 figures, 2 tables", "journal-ref": null, "doi": "10.1007/s00799-021-00304-z", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Medical Subject Headings (MeSH) thesaurus is a controlled vocabulary\nwidely used in biomedical knowledge systems, particularly for semantic indexing\nof scientific literature. As the MeSH hierarchy evolves through annual version\nupdates, some new descriptors are introduced that were not previously\navailable. This paper explores the conceptual provenance of these new\ndescriptors. In particular, we investigate whether such new descriptors have\nbeen previously covered by older descriptors and what is their current relation\nto them. To this end, we propose a framework to categorize new descriptors\nbased on their current relation to older descriptors. Based on the proposed\nclassification scheme, we quantify, analyse and present the different types of\nnew descriptors introduced in MeSH during the last fifteen years. The results\nshow that only about 25% of new MeSH descriptors correspond to new emerging\nconcepts, whereas the rest were previously covered by one or more existing\ndescriptors, either implicitly or explicitly. Most of them were covered by a\nsingle existing descriptor and they usually end up as descendants of it in the\ncurrent hierarchy, gradually leading towards a more fine-grained MeSH\nvocabulary. These insights about the dynamics of the thesaurus are useful for\nthe retrospective study of scientific articles annotated with MeSH, but could\nalso be used to inform the policy of updating the thesaurus in the future.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 19:18:52 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 17:21:43 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 10:50:23 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Nentidis", "Anastasios", ""], ["Krithara", "Anastasia", ""], ["Tsoumakas", "Grigorios", ""], ["Paliouras", "Georgios", ""]]}, {"id": "2101.08370", "submitter": "Robert Litschko", "authors": "Robert Litschko and Ivan Vuli\\'c and Simone Paolo Ponzetto and Goran\n  Glava\\v{s}", "title": "Evaluating Multilingual Text Encoders for Unsupervised Cross-Lingual\n  Retrieval", "comments": "accepted at ECIR'21 (preprint)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Pretrained multilingual text encoders based on neural Transformer\narchitectures, such as multilingual BERT (mBERT) and XLM, have achieved strong\nperformance on a myriad of language understanding tasks. Consequently, they\nhave been adopted as a go-to paradigm for multilingual and cross-lingual\nrepresentation learning and transfer, rendering cross-lingual word embeddings\n(CLWEs) effectively obsolete. However, questions remain to which extent this\nfinding generalizes 1) to unsupervised settings and 2) for ad-hoc cross-lingual\nIR (CLIR) tasks. Therefore, in this work we present a systematic empirical\nstudy focused on the suitability of the state-of-the-art multilingual encoders\nfor cross-lingual document and sentence retrieval tasks across a large number\nof language pairs. In contrast to supervised language understanding, our\nresults indicate that for unsupervised document-level CLIR -- a setup with no\nrelevance judgments for IR-specific fine-tuning -- pretrained encoders fail to\nsignificantly outperform models based on CLWEs. For sentence-level CLIR, we\ndemonstrate that state-of-the-art performance can be achieved. However, the\npeak performance is not met using the general-purpose multilingual text\nencoders `off-the-shelf', but rather relying on their variants that have been\nfurther specialized for sentence understanding tasks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 00:15:38 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Litschko", "Robert", ""], ["Vuli\u0107", "Ivan", ""], ["Ponzetto", "Simone Paolo", ""], ["Glava\u0161", "Goran", ""]]}, {"id": "2101.08394", "submitter": "Ehsan Hamzei", "authors": "Ehsan Hamzei, Stephan Winter and Martin Tomko", "title": "Templates of generic geographic information for answering\n  where-questions", "comments": "27 pages, has supplementary material. International Journal of\n  Geographical Information Science (2021)", "journal-ref": null, "doi": "10.1080/13658816.2020.1869977", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In everyday communication, where-questions are answered by place\ndescriptions. To answer where-questions automatically, computers should be able\nto generate relevant place descriptions that satisfy inquirers' information\nneeds. Human-generated answers to where-questions constructed based on a few\nanchor places that characterize the location of inquired places. The challenge\nfor automatically generating such relevant responses stems from selecting\nrelevant anchor places. In this paper, we present templates that allow to\ncharacterize the human-generated answers and to imitate their structure. These\ntemplates are patterns of generic geographic information derived and encoded\nfrom the largest available machine comprehension dataset, MS MARCO v2.1. In our\napproach, the toponyms in the questions and answers of the dataset are encoded\ninto sequences of generic information. Next, sequence prediction methods are\nused to model the relation between the generic information in the questions and\ntheir answers. Finally, we evaluate the performance of predicting templates for\nanswers to where-questions.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 01:47:02 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Hamzei", "Ehsan", ""], ["Winter", "Stephan", ""], ["Tomko", "Martin", ""]]}, {"id": "2101.08595", "submitter": "Md Rashadul Hasan Rakib", "authors": "Md Rashadul Hasan Rakib and Muhammad Asaduzzaman", "title": "Fast Clustering of Short Text Streams Using Efficient Cluster Indexing\n  and Dynamic Similarity Thresholds", "comments": "6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Short text stream clustering is an important but challenging task since\nmassive amount of text is generated from different sources such as\nmicro-blogging, question-answering, and social news aggregation websites. One\nof the major challenges of clustering such massive amount of text is to cluster\nthem within a reasonable amount of time. The existing state-of-the-art short\ntext stream clustering methods can not cluster such massive amount of text\nwithin a reasonable amount of time as they compute similarities between a text\nand all the existing clusters to assign that text to a cluster. To overcome\nthis challenge, we propose a fast short text stream clustering method (called\nFastStream) that efficiently index the clusters using inverted index and\ncompute similarity between a text and a selected number of clusters while\nassigning a text to a cluster. In this way, we not only reduce the running time\nof our proposed method but also reduce the running time of several\nstate-of-the-art short text stream clustering methods. FastStream assigns a\ntext to a cluster (new or existing) using the dynamically computed similarity\nthresholds based on statistical measure. Thus our method efficiently deals with\nthe concept drift problem. Experimental results demonstrate that FastStream\noutperforms the state-of-the-art short text stream clustering methods by a\nsignificant margin on several short text datasets. In addition, the running\ntime of FastStream is several orders of magnitude faster than that of the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 13:22:47 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Rakib", "Md Rashadul Hasan", ""], ["Asaduzzaman", "Muhammad", ""]]}, {"id": "2101.08655", "submitter": "Leonardo Milhomem Franco Christino", "authors": "Leonardo Christino, Martha D. Ferreira, Asal Jalilvand and Fernando V.\n  Paulovich", "title": "Explainable Patterns: Going from Findings to Insights to Support Data\n  Analytics Democratization", "comments": "8 Figures, 10 pages, submitted to VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decades, massive efforts involving companies, non-profit\norganizations, governments, and others have been put into supporting the\nconcept of data democratization, promoting initiatives to educate people to\nconfront information with data. Although this represents one of the most\ncritical advances in our free world, access to data without concrete facts to\ncheck or the lack of an expert to help on understanding the existing patterns\nhampers its intrinsic value and lessens its democratization. So the benefits of\ngiving full access to data will only be impactful if we go a step further and\nsupport the Data Analytics Democratization, assisting users in transforming\nfindings into insights without the need of domain experts to promote\nunconstrained access to data interpretation and verification. In this paper, we\npresent Explainable Patterns (ExPatt), a new framework to support lay users in\nexploring and creating data storytellings, automatically generating plausible\nexplanations for observed or selected findings using an external (textual)\nsource of information, avoiding or reducing the need for domain experts. ExPatt\napplicability is confirmed via different use-cases involving world demographics\nindicators and Wikipedia as an external source of explanations, showing how it\ncan be used in practice towards the data analytics democratization.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:13:44 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Christino", "Leonardo", ""], ["Ferreira", "Martha D.", ""], ["Jalilvand", "Asal", ""], ["Paulovich", "Fernando V.", ""]]}, {"id": "2101.08705", "submitter": "Luis Borges", "authors": "Lu\\'is Borges, Bruno Martins, Jamie Callan", "title": "Assessing the Benefits of Model Ensembles in Neural Re-Ranking for\n  Passage Retrieval", "comments": "ECIR 2021 short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our work aimed at experimentally assessing the benefits of model ensembling\nwithin the context of neural methods for passage reranking. Starting from\nrelatively standard neural models, we use a previous technique named Fast\nGeometric Ensembling to generate multiple model instances from particular\ntraining schedules, then focusing or attention on different types of approaches\nfor combining the results from the multiple model instances (e.g., averaging\nthe ranking scores, using fusion methods from the IR literature, or using\nsupervised learning-to-rank). Tests with the MS-MARCO dataset show that model\nensembling can indeed benefit the ranking quality, particularly with supervised\nlearning-to-rank although also with unsupervised rank aggregation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 16:33:18 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Borges", "Lu\u00eds", ""], ["Martins", "Bruno", ""], ["Callan", "Jamie", ""]]}, {"id": "2101.08729", "submitter": "Rima Hazra", "authors": "Rima Hazra and Hardik Aggarwal and Pawan Goyal and Animesh Mukherjee\n  and Soumen Chakrabarti", "title": "Joint Autoregressive and Graph Models for Software and Developer Social\n  Networks", "comments": "Accepted at ECIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network research has focused on hyperlink graphs, bibliographic\ncitations, friend/follow patterns, influence spread, etc. Large software\nrepositories also form a highly valuable networked artifact, usually in the\nform of a collection of packages, their developers, dependencies among them,\nand bug reports. This \"social network of code\" is rarely studied by social\nnetwork researchers. We introduce two new problems in this setting. These\nproblems are well-motivated in the software engineering community but not\nclosely studied by social network scientists. The first is to identify packages\nthat are most likely to be troubled by bugs in the immediate future, thereby\ndemanding the greatest attention. The second is to recommend developers to\npackages for the next development cycle. Simple autoregression can be applied\nto historical data for both problems, but we propose a novel method to\nintegrate network-derived features and demonstrate that our method brings\nadditional benefits. Apart from formalizing these problems and proposing new\nbaseline approaches, we prepare and contribute a substantial dataset connecting\nmultiple attributes built from the long-term history of 20 releases of Ubuntu,\ngrowing to over 25,000 packages with their dependency links, maintained by over\n3,800 developers, with over 280k bug reports.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 17:11:32 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Hazra", "Rima", ""], ["Aggarwal", "Hardik", ""], ["Goyal", "Pawan", ""], ["Mukherjee", "Animesh", ""], ["Chakrabarti", "Soumen", ""]]}, {"id": "2101.08751", "submitter": "Luyu Gao", "authors": "Luyu Gao, Zhuyun Dai, Jamie Callan", "title": "Rethink Training of BERT Rerankers in Multi-Stage Retrieval Pipeline", "comments": "ECIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained deep language models~(LM) have advanced the state-of-the-art of\ntext retrieval. Rerankers fine-tuned from deep LM estimates candidate relevance\nbased on rich contextualized matching signals. Meanwhile, deep LMs can also be\nleveraged to improve search index, building retrievers with better recall. One\nwould expect a straightforward combination of both in a pipeline to have\nadditive performance gain. In this paper, we discover otherwise and that\npopular reranker cannot fully exploit the improved retrieval result. We,\ntherefore, propose a Localized Contrastive Estimation (LCE) for training\nrerankers and demonstrate it significantly improves deep two-stage models.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 18:04:58 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Gao", "Luyu", ""], ["Dai", "Zhuyun", ""], ["Callan", "Jamie", ""]]}, {"id": "2101.08769", "submitter": "Steffen Rendle", "authors": "Steffen Rendle", "title": "Item Recommendation from Implicit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of item recommendation is to select the best items for a user from a\nlarge catalogue of items. Item recommenders are commonly trained from implicit\nfeedback which consists of past actions that are positive only. Core challenges\nof item recommendation are (1) how to formulate a training objective from\nimplicit feedback and (2) how to efficiently train models over a large item\ncatalogue. This article provides an overview of item recommendation, its unique\ncharacteristics and some common approaches. It starts with an introduction to\nthe problem and discusses different training objectives. The main body deals\nwith learning algorithms and presents sampling based algorithms for general\nrecommenders and more efficient algorithms for dot product models. Finally, the\napplication of item recommenders for retrieval tasks is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 18:50:21 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Rendle", "Steffen", ""]]}, {"id": "2101.08962", "submitter": "Tong Chen", "authors": "Tong Chen, Sirou Zhu, Yiming Wen, Zhaomin Zheng", "title": "Knowledge Graph Completion with Text-aided Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Knowledge Graph Completion is a task of expanding the knowledge graph/base\nthrough estimating possible entities, or proper nouns, that can be connected\nusing a set of predefined relations, or verb/predicates describing\ninterconnections of two things. Generally, we describe this problem as adding\nnew edges to a current network of vertices and edges. Traditional approaches\nmainly focus on using the existing graphical information that is intrinsic of\nthe graph and train the corresponding embeddings to describe the information;\nhowever, we think that the corpus that are related to the entities should also\ncontain information that can positively influence the embeddings to better make\npredictions. In our project, we try numerous ways of using extracted or raw\ntextual information to help existing KG embedding frameworks reach better\nprediction results, in the means of adding a similarity function to the\nregularization part in the loss function. Results have shown that we have made\ndecent improvements over baseline KG embedding methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 06:10:09 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Chen", "Tong", ""], ["Zhu", "Sirou", ""], ["Wen", "Yiming", ""], ["Zheng", "Zhaomin", ""]]}, {"id": "2101.09018", "submitter": "Dehong Gao", "authors": "Dehong Gao, Wenjing Yang, Huiling Zhou, Yi Wei, Yi Hu and Hao Wang", "title": "Network Clustering for Multi-task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Multi-Task Learning (MTL) technique has been widely studied by word-wide\nresearchers. The majority of current MTL studies adopt the hard parameter\nsharing structure, where hard layers tend to learn general representations over\nall tasks and specific layers are prone to learn specific representations for\neach task. Since the specific layers directly follow the hard layers, the MTL\nmodel needs to estimate this direct change (from general to specific) as well.\nTo alleviate this problem, we introduce the novel cluster layer, which groups\ntasks into clusters during training procedures. In a cluster layer, the tasks\nin the same cluster are further required to share the same network. By this\nway, the cluster layer produces the general presentation for the same cluster,\nwhile produces relatively specific presentations for different clusters. As\ntransitions the cluster layers are used between the hard layers and the\nspecific layers. The MTL model thus learns general representations to specific\nrepresentations gradually. We evaluate our model with MTL document\nclassification and the results demonstrate the cluster layer is quite efficient\nin MTL.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 09:31:35 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Gao", "Dehong", ""], ["Yang", "Wenjing", ""], ["Zhou", "Huiling", ""], ["Wei", "Yi", ""], ["Hu", "Yi", ""], ["Wang", "Hao", ""]]}, {"id": "2101.09066", "submitter": "Luis Leiva", "authors": "Lukas Br\\\"uckner and Ioannis Arapakis and Luis A. Leiva", "title": "Query Abandonment Prediction with Recurrent Neural Models of Mouse\n  Cursor Movements", "comments": null, "journal-ref": "Proceedings of the 29th ACM Intl. Conf. on Information And\n  Knowledge Management (CIKM), 2020", "doi": "10.1145/3340531.3412126", "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most successful search queries do not result in a click if the user can\nsatisfy their information needs directly on the SERP. Modeling query\nabandonment in the absence of click-through data is challenging because search\nengines must rely on other behavioral signals to understand the underlying\nsearch intent. We show that mouse cursor movements make a valuable, low-cost\nbehavioral signal that can discriminate good and bad abandonment. We model\nmouse movements on SERPs using recurrent neural nets and explore several data\nrepresentations that do not rely on expensive hand-crafted features and do not\ndepend on a particular SERP structure. We also experiment with data resampling\nand augmentation techniques that we adopt for sequential data. Our results can\nhelp search providers to gauge user satisfaction for queries without clicks and\nultimately contribute to a better understanding of search engine performance.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 11:57:04 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Br\u00fcckner", "Lukas", ""], ["Arapakis", "Ioannis", ""], ["Leiva", "Luis A.", ""]]}, {"id": "2101.09075", "submitter": "Bernhard Etzlinger", "authors": "Bernhard Etzlinger, Barbara Nu{\\ss}baumm\\\"uller, Philipp Peterseil,\n  and Karin Anna Hummel", "title": "Distance Estimation for BLE-based Contact Tracing -- A Measurement Study", "comments": "5 pages short paper, submitted to IEEE 2021 Wireless Days", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mobile contact tracing apps are -- in principle -- a perfect aid to condemn\nthe human-to-human spread of an infectious disease such as COVID-19 due to the\nwide use of smartphones worldwide. Yet, the unknown accuracy of contact\nestimation by wireless technologies hinders the broader use. We address this\nchallenge by conducting a measurement study with a custom testbed to show the\ncapabilities and limitations of Bluetooth Low Energy (BLE) in different\nscenarios. Distance estimation is based on interpreting the signal pathloss\nwith a basic linear and a logarithmic model. Further, we compare our results\nwith accurate ultra-wideband (UWB) distance measurements. While the results\nindicate that distance estimation by BLE is not accurate enough, a contact\ndetector can detect contacts below 2.5 m with a true positive rate of 0.65 for\nthe logarithmic and of 0.54 for the linear model. Further, the measurements\nreveal that multi-path signal propagation reduces the effect of body shielding\nand thus increases detection accuracy in indoor scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 12:21:21 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 07:42:11 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Etzlinger", "Bernhard", ""], ["Nu\u00dfbaumm\u00fcller", "Barbara", ""], ["Peterseil", "Philipp", ""], ["Hummel", "Karin Anna", ""]]}, {"id": "2101.09086", "submitter": "Ioannis Arapakis", "authors": "Ioannis Arapakis, Souneil Park, Martin Pielot", "title": "Impact of Response Latency on User Behaviour in Mobile Web Search", "comments": "In Proceedings of the 2021 ACM SIGIR Conference on Human Information\n  Interaction and Retrieval (CHIIR '21), March 14-19, 2021, Canberra, Australia", "journal-ref": null, "doi": "10.1145/3406522.3446038", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, the efficiency and effectiveness of search systems have both\nbeen of great interest to the information retrieval community. However, an\nin-depth analysis of the interaction between the response latency and users'\nsubjective search experience in the mobile setting has been missing so far. To\naddress this gap, we conduct a controlled study that aims to reveal how\nresponse latency affects mobile web search. Our preliminary results indicate\nthat mobile web search users are four times more tolerant to response latency\nreported for desktop web search users. However, when exceeding a certain\nthreshold of 7-10 sec, the delays have a sizeable impact and users report\nfeeling significantly more tensed, tired, terrible, frustrated and sluggish,\nall which contribute to a worse subjective user experience.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 12:43:46 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Arapakis", "Ioannis", ""], ["Park", "Souneil", ""], ["Pielot", "Martin", ""]]}, {"id": "2101.09087", "submitter": "Ioannis Arapakis", "authors": "Luis A. Leiva, Ioannis Arapakis, Costas Iordanou", "title": "My Mouse, My Rules: Privacy Issues of Behavioral User Profiling via\n  Mouse Tracking", "comments": "In Proceedings of the 2021 ACM SIGIR Conference on Human Information\n  Interaction and Retrieval (CHIIR '21), March 14-19, 2021, Canberra, Australia", "journal-ref": null, "doi": "10.1145/3406522.3446011", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to stir debate about a disconcerting privacy issue on web\nbrowsing that could easily emerge because of unethical practices and\nuncontrolled use of technology. We demonstrate how straightforward is to\ncapture behavioral data about the users at scale, by unobtrusively tracking\ntheir mouse cursor movements, and predict user's demographics information with\nreasonable accuracy using five lines of code. Based on our results, we propose\nan adversarial method to mitigate user profiling techniques that make use of\nmouse cursor tracking, such as the recurrent neural net we analyze in this\npaper. We also release our data and a web browser extension that implements our\nadversarial method, so that others can benefit from this work in practice.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 12:49:03 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Leiva", "Luis A.", ""], ["Arapakis", "Ioannis", ""], ["Iordanou", "Costas", ""]]}, {"id": "2101.09138", "submitter": "Mawulolo Ameko", "authors": "Mawulolo K. Ameko, Sonia Baee, Laura E. Barnes", "title": "LonelyText: A Short Messaging Based Classification of Loneliness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loneliness does not only have emotional implications on a person but also on\nhis/her well-being. The study of loneliness has been challenging and largely\ninconclusive in findings because of the several factors that might correlate to\nthe phenomenon. We present one approach to predicting this event by discovering\npatterns of language associated with loneliness. Our results show insights and\npromising directions for mining text from instant messaging to predict\nloneliness.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 14:59:45 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Ameko", "Mawulolo K.", ""], ["Baee", "Sonia", ""], ["Barnes", "Laura E.", ""]]}, {"id": "2101.09159", "submitter": "Sebastian Schulthei{\\ss}", "authors": "Sebastian Schulthei{\\ss} and Dirk Lewandowski", "title": "Misplaced trust? The relationship between trust, ability to identify\n  commercially influenced results, and search engine preference", "comments": "18 pages, 5 figures, 7 tables, 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  People have a high level of trust in search engines, especially Google, but\nonly limited knowledge of them, as numerous studies have shown. This leads to\nthe question: To what extent is this trust justified considering the lack of\nfamiliarity among users with how search engines work and the business models\nthey are founded on? We assume that trust in Google, search engine preferences,\nand knowledge of result types are interrelated. To examine this assumption, we\nconducted a representative online survey with n = 2,012 German internet users.\nWe show that users with little search engine knowledge are more likely to trust\nand use Google than users with more knowledge. A contradiction revealed itself\n- users strongly trust Google, yet they are unable to adequately evaluate\nsearch results. This may be problematic since it can potentially affect\nknowledge acquisition. Consequently, there is a need to promote user\ninformation literacy to create a more solid foundation for user trust in search\nengines.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 15:34:02 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 14:01:36 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Schulthei\u00df", "Sebastian", ""], ["Lewandowski", "Dirk", ""]]}, {"id": "2101.09244", "submitter": "Zitao Shen", "authors": "Zitao Shen, Yoonkwon Yi, Anusha Bompelli, Fang Yu, Yanshan Wang, Rui\n  Zhang", "title": "Extracting Lifestyle Factors for Alzheimer's Disease from Clinical Notes\n  Using Deep Learning with Weak Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since no effective therapies exist for Alzheimer's disease (AD), prevention\nhas become more critical through lifestyle factor changes and interventions.\nAnalyzing electronic health records (EHR) of patients with AD can help us\nbetter understand lifestyle's effect on AD. However, lifestyle information is\ntypically stored in clinical narratives. Thus, the objective of the study was\nto demonstrate the feasibility of natural language processing (NLP) models to\nclassify lifestyle factors (e.g., physical activity and excessive diet) from\nclinical texts. We automatically generated labels for the training data by\nusing a rule-based NLP algorithm. We conducted weak supervision for pre-trained\nBidirectional Encoder Representations from Transformers (BERT) models on the\nweakly labeled training corpus. These models include the BERT base model,\nPubMedBERT(abstracts + full text), PubMedBERT(only abstracts), Unified Medical\nLanguage System (UMLS) BERT, Bio BERT, and Bio-clinical BERT. We performed two\ncase studies: physical activity and excessive diet, in order to validate the\neffectiveness of BERT models in classifying lifestyle factors for AD. These\nmodels were compared on the developed Gold Standard Corpus (GSC) on the two\ncase studies. The PubmedBERT(Abs) model achieved the best performance for\nphysical activity, with its precision, recall, and F-1 scores of 0.96, 0.96,\nand 0.96, respectively. Regarding classifying excessive diet, the Bio BERT\nmodel showed the highest performance with perfect precision, recall, and F-1\nscores. The proposed approach leveraging weak supervision could significantly\nincrease the sample size, which is required for training the deep learning\nmodels. The study also demonstrates the effectiveness of BERT models for\nextracting lifestyle factors for Alzheimer's disease from clinical notes.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 17:55:03 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 03:42:00 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Shen", "Zitao", ""], ["Yi", "Yoonkwon", ""], ["Bompelli", "Anusha", ""], ["Yu", "Fang", ""], ["Wang", "Yanshan", ""], ["Zhang", "Rui", ""]]}, {"id": "2101.09311", "submitter": "Elena Tutubalina Dr.", "authors": "Zulfat Miftahutdinov, Artur Kadurin, Roman Kudrin, and Elena\n  Tutubalina", "title": "Drug and Disease Interpretation Learning with Biomedical Entity\n  Representation Transformer", "comments": "Accepted to the 43rd European Conference on Information Retrieval\n  (ECIR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept normalization in free-form texts is a crucial step in every\ntext-mining pipeline. Neural architectures based on Bidirectional Encoder\nRepresentations from Transformers (BERT) have achieved state-of-the-art results\nin the biomedical domain. In the context of drug discovery and development,\nclinical trials are necessary to establish the efficacy and safety of drugs. We\ninvestigate the effectiveness of transferring concept normalization from the\ngeneral biomedical domain to the clinical trials domain in a zero-shot setting\nwith an absence of labeled data. We propose a simple and effective two-stage\nneural approach based on fine-tuned BERT architectures. In the first stage, we\ntrain a metric learning model that optimizes relative similarity of mentions\nand concepts via triplet loss. The model is trained on available labeled\ncorpora of scientific abstracts to obtain vector embeddings of concept names\nand entity mentions from texts. In the second stage, we find the closest\nconcept name representation in an embedding space to a given clinical mention.\nWe evaluated several models, including state-of-the-art architectures, on a\ndataset of abstracts and a real-world dataset of trial records with\ninterventions and conditions mapped to drug and disease terminologies.\nExtensive experiments validate the effectiveness of our approach in knowledge\ntransfer from the scientific literature to clinical trials.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 20:01:25 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Miftahutdinov", "Zulfat", ""], ["Kadurin", "Artur", ""], ["Kudrin", "Roman", ""], ["Tutubalina", "Elena", ""]]}, {"id": "2101.09459", "submitter": "Chongming Gao", "authors": "Chongming Gao, Wenqiang Lei, Xiangnan He, Maarten de Rijke, Tat-Seng\n  Chua", "title": "Advances and Challenges in Conversational Recommender Systems: A Survey", "comments": "33 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems exploit interaction history to estimate user preference,\nhaving been heavily used in a wide range of industry applications. However,\nstatic recommendation models are difficult to answer two important questions\nwell due to inherent shortcomings: (a) What exactly does a user like? (b) Why\ndoes a user like an item? The shortcomings are due to the way that static\nmodels learn user preference, i.e., without explicit instructions and active\nfeedback from users. The recent rise of conversational recommender systems\n(CRSs) changes this situation fundamentally. In a CRS, users and the system can\ndynamically communicate through natural language interactions, which provide\nunprecedented opportunities to explicitly obtain the exact preference of users.\n  Considerable efforts, spread across disparate settings and applications, have\nbeen put into developing CRSs. Existing models, technologies, and evaluation\nmethods for CRSs are far from mature. In this paper, we provide a systematic\nreview of the techniques used in current CRSs. We summarize the key challenges\nof developing CRSs in five directions: (1) Question-based user preference\nelicitation. (2) Multi-turn conversational recommendation strategies. (3)\nDialogue understanding and generation. (4) Exploitation-exploration trade-offs.\n(5) Evaluation and user simulation. These research directions involve multiple\nresearch fields like information retrieval (IR), natural language processing\n(NLP), and human-computer interaction (HCI). Based on these research\ndirections, we discuss some future challenges and opportunities. We provide a\nroad map for researchers from multiple communities to get started in this area.\nWe hope this survey can help to identify and address challenges in CRSs and\ninspire future research.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 08:53:15 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 13:26:00 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 09:10:08 GMT"}, {"version": "v4", "created": "Thu, 4 Feb 2021 15:45:37 GMT"}, {"version": "v5", "created": "Sun, 7 Feb 2021 03:58:16 GMT"}, {"version": "v6", "created": "Thu, 27 May 2021 04:10:53 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Gao", "Chongming", ""], ["Lei", "Wenqiang", ""], ["He", "Xiangnan", ""], ["de Rijke", "Maarten", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2101.09656", "submitter": "Aobo Yang", "authors": "Aobo Yang, Nan Wang, Hongbo Deng, Hongning Wang", "title": "Explanation as a Defense of Recommendation", "comments": "WSDM 2021", "journal-ref": null, "doi": "10.1145/3437963.3441726", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Textual explanations have proved to help improve user satisfaction on\nmachine-made recommendations. However, current mainstream solutions loosely\nconnect the learning of explanation with the learning of recommendation: for\nexample, they are often separately modeled as rating prediction and content\ngeneration tasks. In this work, we propose to strengthen their connection by\nenforcing the idea of sentiment alignment between a recommendation and its\ncorresponding explanation. At training time, the two learning tasks are joined\nby a latent sentiment vector, which is encoded by the recommendation module and\nused to make word choices for explanation generation. At both training and\ninference time, the explanation module is required to generate explanation text\nthat matches sentiment predicted by the recommendation module. Extensive\nexperiments demonstrate our solution outperforms a rich set of baselines in\nboth recommendation and explanation tasks, especially on the improved quality\nof its generated explanations. More importantly, our user studies confirm our\ngenerated explanations help users better recognize the differences between\nrecommended items and understand why an item is recommended.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 06:34:36 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Yang", "Aobo", ""], ["Wang", "Nan", ""], ["Deng", "Hongbo", ""], ["Wang", "Hongning", ""]]}, {"id": "2101.09662", "submitter": "Nilanjan Sinhababu", "authors": "Nilanjan Sinhababu, Rahul Saxena, Monalisa Sarma and Debasis Samanta", "title": "Medical Information Retrieval and Interpretation: A Question-Answer\n  based Interaction Model", "comments": "39 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Internet has become a very powerful platform where diverse medical\ninformation are expressed daily. Recently, a huge growth is seen in searches\nlike symptoms, diseases, medicines, and many other health related queries\naround the globe. The search engines typically populate the result by using the\nsingle query provided by the user and hence reaching to the final result may\nrequire a lot of manual filtering from the user's end. Current search engines\nand recommendation systems still lack real time interactions that may provide\nmore precise result generation. This paper proposes an intelligent and\ninteractive system tied up with the vast medical big data repository on the web\nand illustrates its potential in finding medical information.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 07:01:06 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Sinhababu", "Nilanjan", ""], ["Saxena", "Rahul", ""], ["Sarma", "Monalisa", ""], ["Samanta", "Debasis", ""]]}, {"id": "2101.09807", "submitter": "Fabrizio Lillo", "authors": "Fabrizio Lillo and Salvatore Ruggieri", "title": "Estimating the Total Volume of Queries to a Search Engine", "comments": "Accepted on IEEE Transactions on Knowledge and Data Engineering. 13\n  pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of estimating the total number of searches (volume) of\nqueries in a specific domain, which were submitted to a search engine in a\ngiven time period. Our statistical model assumes that the distribution of\nsearches follows a Zipf's law, and that the observed sample volumes are biased\naccordingly to three possible scenarios. These assumptions are consistent with\nempirical data, with keyword research practices, and with approximate\nalgorithms used to take counts of query frequencies. A few estimators of the\nparameters of the distribution are devised and experimented, based on the\nnature of the empirical/simulated data. For continuous data, we recommend using\nnonlinear least square regression (NLS) on the top-volume queries, where the\nbound on the volume is obtained from the well-known Clauset, Shalizi and Newman\n(CSN) estimation of power-law parameters. For binned data, we propose using a\nChi-square minimization approach restricted to the top-volume queries, where\nthe bound is obtained by the binned version of the CSN method. Estimations are\nthen derived for the total number of queries and for the total volume of the\npopulation, including statistical error bounds. We apply the methods on the\ndomain of recipes and cooking queries searched in Italian in 2017. The observed\nvolumes of sample queries are collected from Google Trends (continuous data)\nand SearchVolume (binned data). The estimated total number of queries and total\nvolume are computed for the two cases, and the results are compared and\ndiscussed.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 21:32:49 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Lillo", "Fabrizio", ""], ["Ruggieri", "Salvatore", ""]]}, {"id": "2101.09903", "submitter": "Weixin Jiang", "authors": "Weixin Jiang, Eric Schwenker, Trevor Spreadbury, Nicola Ferrier, Maria\n  K.Y. Chan, Oliver Cossairt", "title": "A Two-stage Framework for Compound Figure Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific literature contains large volumes of complex, unstructured figures\nthat are compound in nature (i.e. composed of multiple images, graphs, and\ndrawings). Separation of these compound figures is critical for information\nretrieval from these figures. In this paper, we propose a new strategy for\ncompound figure separation, which decomposes the compound figures into\nconstituent subfigures while preserving the association between the subfigures\nand their respective caption components. We propose a two-stage framework to\naddress the proposed compound figure separation problem. In particular, the\nsubfigure label detection module detects all subfigure labels in the first\nstage. Then, in the subfigure detection module, the detected subfigure labels\nhelp to detect the subfigures by optimizing the feature selection process and\nproviding the global layout information as extra features. Extensive\nexperiments are conducted to validate the effectiveness and superiority of the\nproposed framework, which improves the detection precision by 9%.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 05:43:36 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Jiang", "Weixin", ""], ["Schwenker", "Eric", ""], ["Spreadbury", "Trevor", ""], ["Ferrier", "Nicola", ""], ["Chan", "Maria K. Y.", ""], ["Cossairt", "Oliver", ""]]}, {"id": "2101.09948", "submitter": "Abdourrahmane Mahamane Atto", "authors": "Abdourrahmane Mahamane Atto (LISTIC), Sylvie Galichet (LISTIC),\n  Dominique Pastor, Nicolas M\\'eger (LISTIC)", "title": "Parametric Rectified Power Sigmoid Units: Learning Nonlinear Neural\n  Transfer Analytical Forms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG eess.SP math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes representation functionals in a dual paradigm where\nlearning jointly concerns both linear convolutional weights and parametric\nforms of nonlinear activation functions. The nonlinear forms proposed for\nperforming the functional representation are associated with a new class of\nparametric neural transfer functions called rectified power sigmoid units. This\nclass is constructed to integrate both advantages of sigmoid and rectified\nlinear unit functions, in addition with rejecting the drawbacks of these\nfunctions. Moreover, the analytic form of this new neural class involves scale,\nshift and shape parameters so as to obtain a wide range of activation shapes,\nincluding the standard rectified linear unit as a limit case. Parameters of\nthis neural transfer class are considered as learnable for the sake of\ndiscovering the complex shapes that can contribute in solving machine learning\nissues. Performance achieved by the joint learning of convolutional and\nrectified power sigmoid learnable parameters are shown outstanding in both\nshallow and deep learning frameworks. This class opens new prospects with\nrespect to machine learning in the sense that learnable parameters are not only\nattached to linear transformations, but also to suitable nonlinear operators.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 08:25:22 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 11:42:08 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Atto", "Abdourrahmane Mahamane", "", "LISTIC"], ["Galichet", "Sylvie", "", "LISTIC"], ["Pastor", "Dominique", "", "LISTIC"], ["M\u00e9ger", "Nicolas", "", "LISTIC"]]}, {"id": "2101.10201", "submitter": "Tim Ziemer", "authors": "Tim Ziemer, Pattararat Kiattipadungkul, Tanyarin Karuchit", "title": "Novel Recording Studio Features for Music Information Retrieval", "comments": "13 pages, 9 figures, Meeting of the Acoustical Society of America,\n  Dec. 2020", "journal-ref": "Proceedings of Meetings on Acoustics 42(1), 2020, paper number\n  035004", "doi": "10.1121/2.0001363", "report-no": null, "categories": "cs.SD cs.IR eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the recording studio, producers of Electronic Dance Music (EDM) spend more\ntime creating, shaping, mixing and mastering sounds, than with compositional\naspects or arrangement. They tune the sound by close listening and by\nleveraging audio metering and audio analysis tools, until they successfully\ncreat the desired sound aesthetics. DJs of EDM tend to play sets of songs that\nmeet their sound ideal. We therefore suggest using audio metering and\nmonitoring tools from the recording studio to analyze EDM, instead of relying\non conventional low-level audio features. We test our novel set of features by\na simple classification task. We attribute songs to DJs who would play the\nspecific song. This new set of features and the focus on DJ sets is targeted at\nEDM as it takes the producer and DJ culture into account. With simple\ndimensionality reduction and machine learning these features enable us to\nattribute a song to a DJ with an accuracy of 63%. The features from the audio\nmetering and monitoring tools in the recording studio could serve for many\napplications in Music Information Retrieval, such as genre, style and era\nclassification and music recommendation for both DJs and consumers of\nelectronic dance music.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:09:25 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Ziemer", "Tim", ""], ["Kiattipadungkul", "Pattararat", ""], ["Karuchit", "Tanyarin", ""]]}, {"id": "2101.10219", "submitter": "Nikos Voskarides", "authors": "Ida Mele, Cristina Ioana Muntean, Mohammad Aliannejadi, Nikos\n  Voskarides", "title": "MICROS: Mixed-Initiative ConveRsatiOnal Systems Workshop", "comments": "ECIR 2021 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 1st edition of the workshop on Mixed-Initiative ConveRsatiOnal Systems\n(MICROS@ECIR2021) aims at investigating and collecting novel ideas and\ncontributions in the field of conversational systems. Oftentimes, the users\nfulfill their information need using smartphones and home assistants. This has\nrevolutionized the way users access online information, thus posing new\nchallenges compared to traditional search and recommendation. The first edition\nof MICROS will have a particular focus on mixed-initiative conversational\nsystems. Indeed, conversational systems need to be proactive, proposing not\nonly answers but also possible interpretations for ambiguous or vague requests.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:31:40 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mele", "Ida", ""], ["Muntean", "Cristina Ioana", ""], ["Aliannejadi", "Mohammad", ""], ["Voskarides", "Nikos", ""]]}, {"id": "2101.10554", "submitter": "Pavel Loskot", "authors": "Pavel Loskot", "title": "pdfPapers: shell-script utilities for frequency-based multi-word phrase\n  extraction from PDF documents", "comments": "23 pages, 4 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical research is intensive in processing information in the previously\npublished papers. This motivated a lot of efforts to provide tools for text\nmining and information extraction from PDF documents over the past decade. The\n*nix (Unix/Linux) operating systems offer many tools for working with text\nfiles, however, very few such tools are available for processing the contents\nof PDF files. This paper reports our effort to develop shell script utilities\nfor *nix systems with the core functionality focused on viewing and searching\nmultiple PDF documents combining logical and regular expressions, and enabling\nmore reliable text extraction from PDF documents with subsequent manipulation\nof the resulting blocks of text. Furthermore, a procedure for extracting the\nmost frequently occurring multi-word phrases was devised and then demonstrated\non several scientific papers in life sciences. Our experiments revealed that\nthe procedure is surprisingly robust to deficiencies in text extraction and the\nactual scoring function used to rank the phrases in terms of their importance\nor relevance. The keyword relevance is strongly context dependent, the word\nstemming did not provide any recognizable advantage, and the stop-words should\nonly be removed from the beginning and the end of phrases. In addition, the\ndeveloped utilities were used to convert the list of acronyms and the index\nfrom a PDF e-book into a large list of biochemical terms which can be exploited\nin other text mining tasks. All shell scripts and data files are available in a\npublic repository named \\pp\\ on the Github. The key lesson learned in this work\nis that semi-automated methods combining the power of algorithms with the\ncapabilities of research experience are the most promising for improving the\nresearch efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 04:35:20 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Loskot", "Pavel", ""]]}, {"id": "2101.10726", "submitter": "Prodromos Malakasiotis", "authors": "Ilias Chalkidis, Manos Fergadiotis, Nikolaos Manginas, Eva Katakalou\n  and Prodromos Malakasiotis", "title": "Regulatory Compliance through Doc2Doc Information Retrieval: A case\n  study in EU/UK legislation where text similarity has limitations", "comments": "Accepted for publication by EACL 2021, 13 pages including references\n  and appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Major scandals in corporate history have urged the need for regulatory\ncompliance, where organizations need to ensure that their controls (processes)\ncomply with relevant laws, regulations, and policies. However, keeping track of\nthe constantly changing legislation is difficult, thus organizations are\nincreasingly adopting Regulatory Technology (RegTech) to facilitate the\nprocess. To this end, we introduce regulatory information retrieval (REG-IR),\nan application of document-to-document information retrieval (DOC2DOC IR),\nwhere the query is an entire document making the task more challenging than\ntraditional IR where the queries are short. Furthermore, we compile and release\ntwo datasets based on the relationships between EU directives and UK\nlegislation. We experiment on these datasets using a typical two-step pipeline\napproach comprising a pre-fetcher and a neural re-ranker. Experimenting with\nvarious pre-fetchers from BM25 to k nearest neighbors over representations from\nseveral BERT models, we show that fine-tuning a BERT model on an in-domain\nclassification task produces the best representations for IR. We also show that\nneural re-rankers under-perform due to contradicting supervision, i.e., similar\nquery-document pairs with opposite labels. Thus, they are biased towards the\npre-fetcher's score. Interestingly, applying a date filter further improves the\nperformance, showcasing the importance of the time dimension.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:38:15 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Chalkidis", "Ilias", ""], ["Fergadiotis", "Manos", ""], ["Manginas", "Nikolaos", ""], ["Katakalou", "Eva", ""], ["Malakasiotis", "Prodromos", ""]]}, {"id": "2101.10737", "submitter": "Anastasiia Kornilova", "authors": "Anastasiia Kornilova and Lucas Bernardi", "title": "Mining the Stars: Learning Quality Ratings with User-facing Explanations\n  for Vacation Rentals", "comments": "8 pages", "journal-ref": null, "doi": "10.1145/3437963.3441812", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Travel Platforms are virtual two-sided marketplaces where guests\nsearch for accommodations and accommodation providers list their properties\nsuch as hotels and vacation rentals. The large majority of hotels are rated by\nofficial institutions with a number of stars indicating the quality of service\nthey provide. It is a simple and effective mechanism that contributes to match\nsupply with demand by helping guests to find options meeting their criteria and\naccommodation suppliers to market their product to the right segment directly\nimpacting the number of transactions on the platform. Unfortunately, no similar\nrating system exists for the large majority of vacation rentals, making it\ndifficult for guests to search and compare options and hard for vacation\nrentals suppliers to market their product effectively. In this work we describe\na machine learned quality rating system for vacation rentals. The problem is\nchallenging, mainly due to explainability requirements and the lack of ground\ntruth. We present techniques to address these challenges and empirical evidence\nof their efficacy. Our system was successfully deployed and validated through\nOnline Controlled Experiments performed in Booking. com, a large Online Travel\nPlatform, and running for more than one year, impacting more than a million\naccommodations and millions of guests.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 12:12:38 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 09:44:03 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Kornilova", "Anastasiia", ""], ["Bernardi", "Lucas", ""]]}, {"id": "2101.10912", "submitter": "Jonas Vogt", "authors": "Andreas Otte, Jens Staub, Jonas Vogt, Horst Wieker", "title": "Cloud-based traffic data fusion for situation evaluation of handover\n  scenarios", "comments": "Preprint. Submitted to the 27th ITS World Congress, Hamburg, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Upcoming vehicles introduce functions at the level of conditional automation\nwhere a driver no longer must supervise the system but must be able to take\nover the driving function when the system request it. This leads to the\nsituation that the driver does not concentrate on the road but is reading mails\nfor example. In this case, the driver is not able to take over the driving\nfunction immediately because she must first orient herself in the current\ntraffic situation. In an urban scenario a situation that an automated vehicle\nis not able to steer further can arise quickly. To find suitable handover\nsituations, data from traffic infrastructure systems, vehicles, and drivers is\nfused in a cloud-based situation to provide the hole traffic environment as\nbase for the decision when the driving function should be transferred best and\npossibly even before a critical situation arises\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 16:28:37 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 16:01:04 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Otte", "Andreas", ""], ["Staub", "Jens", ""], ["Vogt", "Jonas", ""], ["Wieker", "Horst", ""]]}, {"id": "2101.11059", "submitter": "Muthu Kumar Chandrasekaran", "authors": "Kailash Karthik Saravanakumar, Miguel Ballesteros, Muthu Kumar\n  Chandrasekaran, Kathleen McKeown", "title": "Event-Driven News Stream Clustering using Entity-Aware Contextual\n  Embeddings", "comments": "To appear in Proceedings of The 16th Conference of the European\n  Chapter of the Association for Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for online news stream clustering that is a variant of\nthe non-parametric streaming K-means algorithm. Our model uses a combination of\nsparse and dense document representations, aggregates document-cluster\nsimilarity along these multiple representations and makes the clustering\ndecision using a neural classifier. The weighted document-cluster similarity\nmodel is learned using a novel adaptation of the triplet loss into a linear\nclassification objective. We show that the use of a suitable fine-tuning\nobjective and external knowledge in pre-trained transformer models yields\nsignificant improvements in the effectiveness of contextual embeddings for\nclustering. Our model achieves a new state-of-the-art on a standard stream\nclustering dataset of English documents.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:58:30 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Saravanakumar", "Kailash Karthik", ""], ["Ballesteros", "Miguel", ""], ["Chandrasekaran", "Muthu Kumar", ""], ["McKeown", "Kathleen", ""]]}, {"id": "2101.11268", "submitter": "Suyuchen Wang", "authors": "Suyuchen Wang, Ruihui Zhao, Xi Chen, Yefeng Zheng and Bang Liu", "title": "Enquire One's Parent and Child Before Decision: Fully Exploit\n  Hierarchical Structure for Self-Supervised Taxonomy Expansion", "comments": "12 pages, 6 figures. To appear in The Web Conference (WWW) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Taxonomy is a hierarchically structured knowledge graph that plays a crucial\nrole in machine intelligence. The taxonomy expansion task aims to find a\nposition for a new term in an existing taxonomy to capture the emerging\nknowledge in the world and keep the taxonomy dynamically updated. Previous\ntaxonomy expansion solutions neglect valuable information brought by the\nhierarchical structure and evaluate the correctness of merely an added edge,\nwhich downgrade the problem to node-pair scoring or mini-path classification.\nIn this paper, we propose the Hierarchy Expansion Framework (HEF), which fully\nexploits the hierarchical structure's properties to maximize the coherence of\nexpanded taxonomy. HEF makes use of taxonomy's hierarchical structure in\nmultiple aspects: i) HEF utilizes subtrees containing most relevant nodes as\nself-supervision data for a complete comparison of parental and sibling\nrelations; ii) HEF adopts a coherence modeling module to evaluate the coherence\nof a taxonomy's subtree by integrating hypernymy relation detection and several\ntree-exclusive features; iii) HEF introduces the Fitting Score for position\nselection, which explicitly evaluates both path and level selections and takes\nfull advantage of parental relations to interchange information for\ndisambiguation and self-correction. Extensive experiments show that by better\nexploiting the hierarchical structure and optimizing taxonomy's coherence, HEF\nvastly surpasses the prior state-of-the-art on three benchmark datasets by an\naverage improvement of 46.7% in accuracy and 32.3% in mean reciprocal rank.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 08:57:47 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Wang", "Suyuchen", ""], ["Zhao", "Ruihui", ""], ["Chen", "Xi", ""], ["Zheng", "Yefeng", ""], ["Liu", "Bang", ""]]}, {"id": "2101.11333", "submitter": "Kostas Karpouzis", "authors": "George Tsatiris, Kostas Karpouzis", "title": "Developing for personalised learning: the long road from educational\n  objectives to development and feedback", "comments": "3 pages", "journal-ref": "ACM Interaction Design and Children (IDC) conference 2020,\n  Workshop on Technology-mediated personalized learning for younger learners:\n  concepts, methods and practice", "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the development needed to support the functional and\nteaching requirements of iRead, a 4-year EU-funded project which produced an\naward-winning serious game utilising lexical and syntactical game content. The\nmain functional requirement was that the game should retain different profiles\nfor each student, encapsulating both the respective language model (which\nlanguage features should be taught/used in the game first, before moving on to\nmore advanced ones) and the user model (mastery level for each feature, as\nreported by the student's performance in the game). In addition to this,\nresearchers and stakeholders stated additional requirements related to learning\nobjectives and strategies to make the game more interesting and successful;\nthese were implemented as a set of selection rules which take into account not\nonly the mastery level for each feature, but also respect the priorities set by\nteachers, helping avoid repetition of content and features, and maintaining a\nbalance between new content and revision of already mastered features to give\nstudents the sense of progress, while also reinforcing learning.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 11:50:41 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Tsatiris", "George", ""], ["Karpouzis", "Kostas", ""]]}, {"id": "2101.11427", "submitter": "Xiang-Rong Sheng", "authors": "Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai,\n  Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, Xiaoqiang Zhu", "title": "One Model to Serve All: Star Topology Adaptive Recommender for\n  Multi-Domain CTR Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional industrial recommenders are usually trained on a single business\ndomain and then serve for this domain. However, in large commercial platforms,\nit is often the case that the recommenders need to make click-through rate\n(CTR) predictions for multiple business domains. Different domains have\noverlapping user groups and items. Thus, there exist commonalities. Since the\nspecific user groups have disparity and the user behaviors may change in\nvarious business domains, there also have distinctions. The distinctions result\nin domain-specific data distributions, making it hard for a single shared model\nto work well on all domains. To learn an effective and efficient CTR model to\nhandle multiple domains simultaneously, we present Star Topology Adaptive\nRecommender (STAR). Concretely, STAR has the star topology, which consists of\nthe shared centered parameters and domain-specific parameters. The shared\nparameters are applied to learn commonalities of all domains, and the\ndomain-specific parameters capture domain distinction for more refined\nprediction. Given requests from different business domains, STAR can adapt its\nparameters conditioned on the domain characteristics. The experimental result\nfrom production data validates the superiority of the proposed STAR model.\nSince 2020, STAR has been deployed in the display advertising system of\nAlibaba, obtaining averaging 8.0% improvement on CTR and 6.0% on RPM (Revenue\nPer Mille).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:17:55 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 06:34:18 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Sheng", "Xiang-Rong", ""], ["Zhao", "Liqin", ""], ["Zhou", "Guorui", ""], ["Ding", "Xinyao", ""], ["Dai", "Binding", ""], ["Luo", "Qiang", ""], ["Yang", "Siran", ""], ["Lv", "Jingshan", ""], ["Zhang", "Chi", ""], ["Deng", "Hongbo", ""], ["Zhu", "Xiaoqiang", ""]]}, {"id": "2101.11431", "submitter": "Nicola Melluso", "authors": "Silvia Fareri, Nicola Melluso, Filippo Chiarello, Gualtiero Fantoni", "title": "SkillNER: Mining and Mapping Soft Skills from any Text", "comments": null, "journal-ref": "Expert Systems With Applications 184 (2021) 115544", "doi": "10.1016/j.eswa.2021.115544", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In today's digital world, there is an increasing focus on soft skills. On the\none hand, they facilitate innovation at companies, but on the other, they are\nunlikely to be automated soon. Researchers struggle with accurately approaching\nquantitatively the study of soft skills due to the lack of data-driven methods\nto retrieve them. This limits the possibility for psychologists and HR managers\nto understand the relation between humans and digitalisation. This paper\npresents SkillNER, a novel data-driven method for automatically extracting soft\nskills from text. It is a named entity recognition (NER) system trained with a\nsupport vector machine (SVM) on a corpus of more than 5000 scientific papers.\nWe developed this system by measuring the performance of our approach against\ndifferent training models and validating the results together with a team of\npsychologists. Finally, SkillNER was tested in a real-world case study using\nthe job descriptions of ESCO (European Skill/Competence Qualification and\nOccupation) as textual source. The system enabled the detection of communities\nof job profiles based on their shared soft skills and communities of soft\nskills based on their shared job profiles. This case study demonstrates that\nthe tool can automatically retrieve soft skills from a large corpus in an\nefficient way, proving useful for firms, institutions, and workers. The tool is\nopen and available online to foster quantitative methods for the study of soft\nskills.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 11:14:05 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 18:12:46 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Fareri", "Silvia", ""], ["Melluso", "Nicola", ""], ["Chiarello", "Filippo", ""], ["Fantoni", "Gualtiero", ""]]}, {"id": "2101.11446", "submitter": "Zhi Xua Lian", "authors": "Z.X.Lian", "title": "A study on information behavior of scholars for article keywords\n  selection", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This project takes the factors of keyword selection behavior as the research\nobject. Qualitative analysis methods such as interview and grounded theory were\nused to construct causal influence path model. Combined with computer\nsimulation technology such as multi-agent simulation experiment method was used\nto study the factors of keyword selection from two dimensions of individual to\ngroup. The research was carried out according to the path of factor analysis at\nindividual level macro situation simulation optimization of scientific research\ndata management. Based on the aforementioned review of existing researches and\nexplanations of keywords selection, this study adopts a qualitative research\ndesign to expand the explanation, and macro simulation based on the results of\nqualitative research. There are two steps in this study, one is do interview\nwith authors and then design macro simulation according the deductive and\nqualitative content analysis results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:25:36 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lian", "Z. X.", ""]]}, {"id": "2101.11483", "submitter": "Iman Tahamtan", "authors": "Robin Haunschild, Lutz Bornmann, Devendra Potnis, Iman Tahamtan", "title": "Investigating Diffusion of Scientific Knowledge on Twitter: A Study of\n  Topic Networks of Opioid Publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One way to assess the value of scientific research is to measure the\nattention it receives on social media. While previous research has mostly\nfocused on the \"number of mentions\" of scientific research on social media, the\ncurrent study uses \"topic networks\" to measure public attention to scientific\nresearch on Twitter. Topic networks in this research are the \"co-occurring\nauthor keywords\" in scientific publications and the \"co-occurring hashtags\" in\nthe tweets mentioning scientific publications. Since bots (automated social\nmedia accounts) may significantly influence public attention, this study also\ninvestigates whether the topic networks based on the tweets by all accounts\n(bot and non-bot accounts) differ from the topic networks by non-bot accounts.\nOur analysis is based on a set of opioid scientific publications from 2011 to\n2019 and the tweets associated with them. We use co-occurrence network analysis\nto generate topic networks. Results indicated that the public has mostly used\ngeneric terms to discuss opioid publications. Results confirmed that topic\nnetworks provide a legitimate method to visualize public discussions of\n(health-related) scientific publications, and how the public discusses\n(health-related) scientific research differently from the scientific community.\nThere was a significant overlap between the topic networks based on the tweets\nby all accounts and non-bot accounts. This result indicates that in generating\ntopic networks, bot accounts do not need to be excluded as they have negligible\nimpact on the results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 15:22:15 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Haunschild", "Robin", ""], ["Bornmann", "Lutz", ""], ["Potnis", "Devendra", ""], ["Tahamtan", "Iman", ""]]}, {"id": "2101.11556", "submitter": "Manisha Verma", "authors": "Manisha Verma, Kapil Thadani, and Shaunak Mishra", "title": "Powering COVID-19 community Q&A with Curated Side Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Community question answering and discussion platforms such as Reddit, Yahoo!\nanswers or Quora provide users the flexibility of asking open ended questions\nto a large audience, and replies to such questions maybe useful both to the\nuser and the community on certain topics such as health, sports or finance.\nGiven the recent events around COVID-19, some of these platforms have attracted\n2000+ questions from users about several aspects associated with the disease.\nGiven the impact of this disease on general public, in this work we investigate\nways to improve the ranking of user generated answers on COVID-19. We\nspecifically explore the utility of external technical sources of side\ninformation (such as CDC guidelines or WHO FAQs) in improving answer ranking on\nsuch platforms. We found that ranking user answers based on question-answer\nsimilarity is not sufficient, and existing models cannot effectively exploit\nexternal (side) information. In this work, we demonstrate the effectiveness of\ndifferent attention based neural models that can directly exploit side\ninformation available in technical documents or verified forums (e.g., research\npublications on COVID-19 or WHO website). Augmented with a temperature\nmechanism, the attention based neural models can selectively determine the\nrelevance of side information for a given user question, while ranking answers.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 17:24:38 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Verma", "Manisha", ""], ["Thadani", "Kapil", ""], ["Mishra", "Shaunak", ""]]}, {"id": "2101.11873", "submitter": "Yufeng Zhang", "authors": "Yufeng Zhang, Jinghao Zhang, Zeyu Cui, Shu Wu, Liang Wang", "title": "A Graph-based Relevance Matching Model for Ad-hoc Retrieval", "comments": "To appear at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To retrieve more relevant, appropriate and useful documents given a query,\nfinding clues about that query through the text is crucial. Recent deep\nlearning models regard the task as a term-level matching problem, which seeks\nexact or similar query patterns in the document. However, we argue that they\nare inherently based on local interactions and do not generalise to ubiquitous,\nnon-consecutive contextual relationships. In this work, we propose a novel\nrelevance matching model based on graph neural networks to leverage the\ndocument-level word relationships for ad-hoc retrieval. In addition to the\nlocal interactions, we explicitly incorporate all contexts of a term through\nthe graph-of-word text format. Matching patterns can be revealed accordingly to\nprovide a more accurate relevance score. Our approach significantly outperforms\nstrong baselines on two ad-hoc benchmarks. We also experimentally compare our\nmodel with BERT and show our advantages on long documents.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 09:05:09 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 01:49:42 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Zhang", "Yufeng", ""], ["Zhang", "Jinghao", ""], ["Cui", "Zeyu", ""], ["Wu", "Shu", ""], ["Wang", "Liang", ""]]}, {"id": "2101.11916", "submitter": "Lorenzo Porcaro", "authors": "Lorenzo Porcaro, Emilia G\\'omez, Carlos Castillo", "title": "Perceptions of Diversity in Electronic Music: the Impact of Listener,\n  Artist, and Track Characteristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shared practices to assess the diversity of retrieval system results are\nstill debated in the Information Retrieval community, partly because of the\nchallenges of determining what diversity means in specific scenarios, and of\nunderstanding how diversity is perceived by end-users. The field of Music\nInformation Retrieval is not exempt from this issue. Even if fields such as\nMusicology or Sociology of Music have a long tradition in questioning the\nrepresentation and the impact of diversity in cultural environments, such\nknowledge has not been yet embedded into the design and development of music\ntechnologies. In this paper, focusing on electronic music, we investigate the\ncharacteristics of listeners, artists, and tracks that are influential in the\nperception of diversity. Specifically, we center our attention on 1)\nunderstanding the relationship between perceived diversity and computational\nmethods to measure diversity, and 2) analyzing how listeners' domain knowledge\nand familiarity influence such perceived diversity. To accomplish this, we\ndesign a user-study in which listeners are asked to compare pairs of lists of\ntracks and artists, and to select the most diverse list from each pair. We\ncompare participants' ratings with results obtained through computational\nmodels built using audio tracks' features and artist attributes. We find that\nsuch models are generally aligned with participants' choices when most of them\nagree that one list is more diverse than the other, while they present a mixed\nbehaviour in cases where participants have little agreement. Moreover, we\nobserve how differences in domain knowledge, familiarity, and demographics can\ninfluence the level of agreement among listeners, and between listeners and\ndiversity metrics computed automatically.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 10:38:49 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Porcaro", "Lorenzo", ""], ["G\u00f3mez", "Emilia", ""], ["Castillo", "Carlos", ""]]}, {"id": "2101.11954", "submitter": "Tathagata Raha", "authors": "Tathagata Raha, Vijayasaradhi Indurthi, Aayush Upadhyaya, Jeevesh\n  Kataria, Pramud Bommakanti, Vikram Keswani, Vasudeva Varma", "title": "Identifying COVID-19 Fake News in Social Media", "comments": "CONSTRAINT@AAAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The evolution of social media platforms have empowered everyone to access\ninformation easily. Social media users can easily share information with the\nrest of the world. This may sometimes encourage spread of fake news, which can\nresult in undesirable consequences. In this work, we train models which can\nidentify health news related to COVID-19 pandemic as real or fake. Our models\nachieve a high F1-score of 98.64%. Our models achieve second place on the\nleaderboard, tailing the first position with a very narrow margin 0.05% points.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 12:12:50 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 22:27:07 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Raha", "Tathagata", ""], ["Indurthi", "Vijayasaradhi", ""], ["Upadhyaya", "Aayush", ""], ["Kataria", "Jeevesh", ""], ["Bommakanti", "Pramud", ""], ["Keswani", "Vikram", ""], ["Varma", "Vasudeva", ""]]}, {"id": "2101.12001", "submitter": "Thanasis Vergoulis", "authors": "Thanasis Vergoulis, Ilias Kanellos, Claudio Atzori, Andrea Mannocci,\n  Serafeim Chatzopoulos, Sandro La Bruzzo, Natalia Manola, Paolo Manghi", "title": "BIP! DB: A Dataset of Impact Measures for Scientific Publications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The growth rate of the number of scientific publications is constantly\nincreasing, creating important challenges in the identification of valuable\nresearch and in various scholarly data management applications, in general. In\nthis context, measures which can effectively quantify the scientific impact\ncould be invaluable. In this work, we present BIP! DB, an open dataset that\ncontains a variety of impact measures calculated for a large collection of more\nthan 100 million scientific publications from various disciplines.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 13:59:55 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Vergoulis", "Thanasis", ""], ["Kanellos", "Ilias", ""], ["Atzori", "Claudio", ""], ["Mannocci", "Andrea", ""], ["Chatzopoulos", "Serafeim", ""], ["La Bruzzo", "Sandro", ""], ["Manola", "Natalia", ""], ["Manghi", "Paolo", ""]]}, {"id": "2101.12153", "submitter": "Huansheng Ning Prof", "authors": "Sahraoui Dhelim, Nyothiri Aung, Mohammed Amine Bouras, Huansheng Ning\n  and Erik Cambria", "title": "A Survey on Personality-Aware Recommendation Systems", "comments": "Under review in Artificial Intelligence Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the emergence of personality computing as a new research field related\nto artificial intelligence and personality psychology, we have witnessed an\nunprecedented proliferation of personality-aware recommendation systems. Unlike\nconventional recommendation systems, these new systems solve traditional\nproblems such as the cold start and data sparsity problems. This survey aims to\nstudy and systematically classify personality-aware recommendation systems. To\nthe best of our knowledge, this survey is the first that focuses on\npersonality-aware recommendation systems. We explore the different design\nchoices of personality-aware recommendation systems, by comparing their\npersonality modeling methods, as well as their recommendation techniques.\nFurthermore, we present the commonly used datasets and point out some of the\nchallenges of personality-aware recommendation systems.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:03:23 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Dhelim", "Sahraoui", ""], ["Aung", "Nyothiri", ""], ["Bouras", "Mohammed Amine", ""], ["Ning", "Huansheng", ""], ["Cambria", "Erik", ""]]}, {"id": "2101.12335", "submitter": "Babis Magoutas", "authors": "K. Arnaoutaki, E. Bothos, B. Magoutas and G. Mentzas", "title": "Personalization and Recommendation Technologies for MaaS", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Over the last few years, MaaS has been extensively studied and evolved into\noffering a multitude of mobility services that continuously increase, from\nalternative car or bike-sharing modes to autonomous vehicles, that aspire to\nbecome a part of this novel ecosystem. MaaS provides end-users with multimodal,\nintegrated, and digital mobility solutions, including a multitude of different\nchoices able to cover users specific needs in a personalized manner. This\npractically leads to a range of novel MaaS products, that may have complex\nstructures and the challenge of matching them to user preferences and needs so\nthat suitable products can be provided to end-users. Moreover, in the everyday\nuse of MaaS, travelers require support to identify routes to reach their\ndestination that adhere to their personal preferences and are aligned to the\nMaaS product they have purchased. This paper tackles these two user-centric\nchallenges by exploiting state-of-the-art techniques from the field of\nPersonalization and Recommendation systems and integrating them in MaaS\nplatforms and route planning applications.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:11:15 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Arnaoutaki", "K.", ""], ["Bothos", "E.", ""], ["Magoutas", "B.", ""], ["Mentzas", "G.", ""]]}, {"id": "2101.12339", "submitter": "Yutong Wang", "authors": "Ziqi Tang, Yutong Wang, Jiebo Luo", "title": "Are Top School Students More Critical of Their Professors? Mining\n  Comments on RateMyProfessor.com", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Student reviews and comments on RateMyProfessor.com reflect realistic\nlearning experiences of students. Such information provides a large-scale data\nsource to examine the teaching quality of the lecturers. In this paper, we\npropose an in-depth analysis of these comments. First, we partition our data\ninto different comparison groups. Next, we perform exploratory data analysis to\ndelve into the data. Furthermore, we employ Latent Dirichlet Allocation and\nsentiment analysis to extract topics and understand the sentiments associated\nwith the comments. We uncover interesting insights about the characteristics of\nboth college students and professors. Our study proves that student reviews and\ncomments contain crucial information and can serve as essential references for\nenrollment in courses and universities.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 20:01:36 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Tang", "Ziqi", ""], ["Wang", "Yutong", ""], ["Luo", "Jiebo", ""]]}, {"id": "2101.12346", "submitter": "Jiansheng Fang", "authors": "Jiansheng Fang, Huazhu Fu, Jiang Liu", "title": "Deep Triplet Hashing Network for Case-based Medical Image Retrieval", "comments": "12 pages, 6 figures, MedIA Journal", "journal-ref": null, "doi": "10.1016/j.media.2021.101981", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep hashing methods have been shown to be the most efficient approximate\nnearest neighbor search techniques for large-scale image retrieval. However,\nexisting deep hashing methods have a poor small-sample ranking performance for\ncase-based medical image retrieval. The top-ranked images in the returned query\nresults may be as a different class than the query image. This ranking problem\nis caused by classification, regions of interest (ROI), and small-sample\ninformation loss in the hashing space. To address the ranking problem, we\npropose an end-to-end framework, called Attention-based Triplet Hashing (ATH)\nnetwork, to learn low-dimensional hash codes that preserve the classification,\nROI, and small-sample information. We embed a spatial-attention module into the\nnetwork structure of our ATH to focus on ROI information. The spatial-attention\nmodule aggregates the spatial information of feature maps by utilizing\nmax-pooling, element-wise maximum, and element-wise mean operations jointly\nalong the channel axis. The triplet cross-entropy loss can help to map the\nclassification information of images and similarity between images into the\nhash codes. Extensive experiments on two case-based medical datasets\ndemonstrate that our proposed ATH can further improve the retrieval performance\ncompared to the state-of-the-art deep hashing methods and boost the ranking\nperformance for small samples. Compared to the other loss methods, the triplet\ncross-entropy loss can enhance the classification performance and hash\ncode-discriminability\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 01:35:46 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Fang", "Jiansheng", ""], ["Fu", "Huazhu", ""], ["Liu", "Jiang", ""]]}, {"id": "2101.12406", "submitter": "Abhisek Dash", "authors": "Anurag Shandilya, Abhisek Dash, Abhijnan Chakraborty, Kripabandhu\n  Ghosh, Saptarshi Ghosh", "title": "Fairness for Whom? Understanding the Reader's Perception of Fairness in\n  Text Summarization", "comments": "This work has been accepted at International Workshop on Fair and\n  Interpretable Learning Algorithms 2020 (FILA 2020), which was held in\n  conjunction with IEEE BigData 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the surge in user-generated textual information, there has been a recent\nincrease in the use of summarization algorithms for providing an overview of\nthe extensive content. Traditional metrics for evaluation of these algorithms\n(e.g. ROUGE scores) rely on matching algorithmic summaries to human-generated\nones. However, it has been shown that when the textual contents are\nheterogeneous, e.g., when they come from different socially salient groups,\nmost existing summarization algorithms represent the social groups very\ndifferently compared to their distribution in the original data. To mitigate\nsuch adverse impacts, some fairness-preserving summarization algorithms have\nalso been proposed. All of these studies have considered normative notions of\nfairness from the perspective of writers of the contents, neglecting the\nreaders' perceptions of the underlying fairness notions. To bridge this gap, in\nthis work, we study the interplay between the fairness notions and how readers\nperceive them in textual summaries. Through our experiments, we show that\nreader's perception of fairness is often context-sensitive. Moreover, standard\nROUGE evaluation metrics are unable to quantify the perceived (un)fairness of\nthe summaries. To this end, we propose a human-in-the-loop metric and an\nautomated graph-based methodology to quantify the perceived bias in textual\nsummaries. We demonstrate their utility by quantifying the (un)fairness of\nseveral summaries of heterogeneous socio-political microblog datasets.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 05:14:34 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 04:26:52 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Shandilya", "Anurag", ""], ["Dash", "Abhisek", ""], ["Chakraborty", "Abhijnan", ""], ["Ghosh", "Kripabandhu", ""], ["Ghosh", "Saptarshi", ""]]}, {"id": "2101.12430", "submitter": "Vince Lyzinski", "authors": "Al-Fahad M. Al-Qadhi, Carey E. Priebe, Hayden S. Helm, Vince Lyzinski", "title": "Subgraph nomination: Query by Example Subgraph Retrieval in Networks", "comments": "31 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the subgraph nomination inference task, in which\nexample subgraphs of interest are used to query a network for similarly\ninteresting subgraphs. This type of problem appears time and again in real\nworld problems connected to, for example, user recommendation systems and\nstructural retrieval tasks in social and biological/connectomic networks. We\nformally define the subgraph nomination framework with an emphasis on the\nnotion of a user-in-the-loop in the subgraph nomination pipeline. In this\nsetting, a user can provide additional post-nomination light supervision that\ncan be incorporated into the retrieval task. After introducing and formalizing\nthe retrieval task, we examine the nuanced effect that user-supervision can\nhave on performance, both analytically and across real and simulated data\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 06:50:27 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Al-Qadhi", "Al-Fahad M.", ""], ["Priebe", "Carey E.", ""], ["Helm", "Hayden S.", ""], ["Lyzinski", "Vince", ""]]}, {"id": "2101.12457", "submitter": "Cheng-Te Li", "authors": "Cheng Hsu, Cheng-Te Li", "title": "RetaGNN: Relational Temporal Attentive Graph Neural Networks for\n  Holistic Sequential Recommendation", "comments": "Accepted to The Web Conference (WWW) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential recommendation (SR) is to accurately recommend a list of items for\na user based on her current accessed ones. While new-coming users continuously\narrive in the real world, one crucial task is to have inductive SR that can\nproduce embeddings of users and items without re-training. Given user-item\ninteractions can be extremely sparse, another critical task is to have\ntransferable SR that can transfer the knowledge derived from one domain with\nrich data to another domain. In this work, we aim to present the holistic SR\nthat simultaneously accommodates conventional, inductive, and transferable\nsettings. We propose a novel deep learning-based model, Relational Temporal\nAttentive Graph Neural Networks (RetaGNN), for holistic SR. The main idea of\nRetaGNN is three-fold. First, to have inductive and transferable capabilities,\nwe train a relational attentive GNN on the local subgraph extracted from a\nuser-item pair, in which the learnable weight matrices are on various relations\namong users, items, and attributes, rather than nodes or edges. Second,\nlong-term and short-term temporal patterns of user preferences are encoded by a\nproposed sequential self-attention mechanism. Third, a relation-aware\nregularization term is devised for better training of RetaGNN. Experiments\nconducted on MovieLens, Instagram, and Book-Crossing datasets exhibit that\nRetaGNN can outperform state-of-the-art methods under conventional, inductive,\nand transferable settings. The derived attention weights also bring model\nexplainability.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 08:08:34 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Hsu", "Cheng", ""], ["Li", "Cheng-Te", ""]]}, {"id": "2101.12465", "submitter": "Cheng-Te Li", "authors": "Yi-Ju Lu, Cheng-Te Li", "title": "AGSTN: Learning Attention-adjusted Graph Spatio-Temporal Networks for\n  Short-term Urban Sensor Value Forecasting", "comments": "Published in IEEE ICDM 2020. Code is available at\n  https://github.com/l852888/AGSTN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting spatio-temporal correlated time series of sensor values is\ncrucial in urban applications, such as air pollution alert, biking resource\nmanagement, and intelligent transportation systems. While recent advances\nexploit graph neural networks (GNN) to better learn spatial and temporal\ndependencies between sensors, they cannot model time-evolving spatio-temporal\ncorrelation (STC) between sensors, and require pre-defined graphs, which are\nneither always available nor totally reliable, and target at only a specific\ntype of sensor data at one time. Moreover, since the form of time-series\nfluctuation is varied across sensors, a model needs to learn fluctuation\nmodulation. To tackle these issues, in this work, we propose a novel GNN-based\nmodel, Attention-adjusted Graph Spatio-Temporal Network (AGSTN). In AGSTN,\nmulti-graph convolution with sequential learning is developed to learn\ntime-evolving STC. Fluctuation modulation is realized by a proposed attention\nadjustment mechanism. Experiments on three sensor data, air quality, bike\ndemand, and traffic flow, exhibit that AGSTN outperforms the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 08:31:38 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Lu", "Yi-Ju", ""], ["Li", "Cheng-Te", ""]]}, {"id": "2101.12506", "submitter": "Wasim Huleihel", "authors": "Wasim Huleihel and Soumyabrata Pal and Ofer Shayevitz", "title": "Learning User Preferences in Non-Stationary Environments", "comments": "31 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommendation systems often use online collaborative filtering (CF)\nalgorithms to identify items a given user likes over time, based on ratings\nthat this user and a large number of other users have provided in the past.\nThis problem has been studied extensively when users' preferences do not change\nover time (static case); an assumption that is often violated in practical\nsettings. In this paper, we introduce a novel model for online non-stationary\nrecommendation systems which allows for temporal uncertainties in the users'\npreferences. For this model, we propose a user-based CF algorithm, and provide\na theoretical analysis of its achievable reward. Compared to related\nnon-stationary multi-armed bandit literature, the main fundamental difficulty\nin our model lies in the fact that variations in the preferences of a certain\nuser may affect the recommendations for other users severely. We also test our\nalgorithm over real-world datasets, showing its effectiveness in real-world\napplications. One of the main surprising observations in our experiments is the\nfact our algorithm outperforms other static algorithms even when preferences do\nnot change over time. This hints toward the general conclusion that in\npractice, dynamic algorithms, such as the one we propose, might be beneficial\neven in stationary environments.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 10:26:16 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Huleihel", "Wasim", ""], ["Pal", "Soumyabrata", ""], ["Shayevitz", "Ofer", ""]]}, {"id": "2101.12549", "submitter": "Shijie Zhang", "authors": "Shijie Zhang, Hongzhi Yin, Tong Chen, Zi Huang, Lizhen Cui, Xiangliang\n  Zhang", "title": "Graph Embedding for Recommendation against Attribute Inference Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, recommender systems play a pivotal role in helping users\nidentify the most suitable items that satisfy personal preferences. As\nuser-item interactions can be naturally modelled as graph-structured data,\nvariants of graph convolutional networks (GCNs) have become a well-established\nbuilding block in the latest recommenders. Due to the wide utilization of\nsensitive user profile data, existing recommendation paradigms are likely to\nexpose users to the threat of privacy breach, and GCN-based recommenders are no\nexception. Apart from the leakage of raw user data, the fragility of current\nrecommenders under inference attacks offers malicious attackers a backdoor to\nestimate users' private attributes via their behavioral footprints and the\nrecommendation results. However, little attention has been paid to developing\nrecommender systems that can defend such attribute inference attacks, and\nexisting works achieve attack resistance by either sacrificing considerable\nrecommendation accuracy or only covering specific attack models or protected\ninformation. In our paper, we propose GERAI, a novel differentially private\ngraph convolutional network to address such limitations. Specifically, in\nGERAI, we bind the information perturbation mechanism in differential privacy\nwith the recommendation capability of graph convolutional networks.\nFurthermore, based on local differential privacy and functional mechanism, we\ninnovatively devise a dual-stage encryption paradigm to simultaneously enforce\nprivacy guarantee on users' sensitive features and the model optimization\nprocess. Extensive experiments show the superiority of GERAI in terms of its\nresistance to attribute inference attacks and recommendation effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 12:55:22 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Zhang", "Shijie", ""], ["Yin", "Hongzhi", ""], ["Chen", "Tong", ""], ["Huang", "Zi", ""], ["Cui", "Lizhen", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "2101.12555", "submitter": "Xinjiang Lu", "authors": "Haoran Xin, Xinjiang Lu, Tong Xu, Hao Liu, Jingjing Gu, Dejing Dou,\n  Hui Xiong", "title": "Out-of-Town Recommendation with Travel Intention Modeling", "comments": "Accepted by AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Out-of-town recommendation is designed for those users who leave their\nhome-town areas and visit the areas they have never been to before. It is\nchallenging to recommend Point-of-Interests (POIs) for out-of-town users since\nthe out-of-town check-in behavior is determined by not only the user's\nhome-town preference but also the user's travel intention. Besides, the user's\ntravel intentions are complex and dynamic, which leads to big difficulties in\nunderstanding such intentions precisely. In this paper, we propose a\nTRAvel-INtention-aware Out-of-town Recommendation framework, named TRAINOR. The\nproposed TRAINOR framework distinguishes itself from existing out-of-town\nrecommenders in three aspects. First, graph neural networks are explored to\nrepresent users' home-town check-in preference and geographical constraints in\nout-of-town check-in behaviors. Second, a user-specific travel intention is\nformulated as an aggregation combining home-town preference and generic travel\nintention together, where the generic travel intention is regarded as a mixture\nof inherent intentions that can be learned by Neural Topic Model (NTM). Third,\na non-linear mapping function, as well as a matrix factorization method, are\nemployed to transfer users' home-town preference and estimate out-of-town POI's\nrepresentation, respectively. Extensive experiments on real-world data sets\nvalidate the effectiveness of the TRAINOR framework. Moreover, the learned\ntravel intention can deliver meaningful explanations for understanding a user's\ntravel purposes.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 13:14:29 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 05:12:13 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Xin", "Haoran", ""], ["Lu", "Xinjiang", ""], ["Xu", "Tong", ""], ["Liu", "Hao", ""], ["Gu", "Jingjing", ""], ["Dou", "Dejing", ""], ["Xiong", "Hui", ""]]}, {"id": "2101.12631", "submitter": "Mengzhao Wang", "authors": "Mengzhao Wang and Xiaoliang Xu and Qiang Yue and Yuxiang Wang", "title": "A Comprehensive Survey and Experimental Comparison of Graph-Based\n  Approximate Nearest Neighbor Search", "comments": "28 pages, 21 figures, 24 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate nearest neighbor search (ANNS) constitutes an important operation\nin a multitude of applications, including recommendation systems, information\nretrieval, and pattern recognition. In the past decade, graph-based ANNS\nalgorithms have been the leading paradigm in this domain, with dozens of\ngraph-based ANNS algorithms proposed. Such algorithms aim to provide effective,\nefficient solutions for retrieving the nearest neighbors for a given query.\nNevertheless, these efforts focus on developing and optimizing algorithms with\ndifferent approaches, so there is a real need for a comprehensive survey about\nthe approaches' relative performance, strengths, and pitfalls. Thus here we\nprovide a thorough comparative analysis and experimental evaluation of 13\nrepresentative graph-based ANNS algorithms via a new taxonomy and fine-grained\npipeline. We compared each algorithm in a uniform test environment on eight\nreal-world datasets and 12 synthetic datasets with varying sizes and\ncharacteristics. Our study yields novel discoveries, offerings several useful\nprinciples to improve algorithms, thus designing an optimized method that\noutperforms the state-of-the-art algorithms. This effort also helped us\npinpoint algorithms' working portions, along with rule-of-thumb recommendations\nabout promising research directions and suitable algorithms for practitioners\nin different fields.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 15:12:35 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 08:54:16 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wang", "Mengzhao", ""], ["Xu", "Xiaoliang", ""], ["Yue", "Qiang", ""], ["Wang", "Yuxiang", ""]]}, {"id": "2101.12672", "submitter": "Nguyen Thanh Chinh", "authors": "Thanh Chinh Nguyen, Van Nha Nguyen", "title": "NLPBK at VLSP-2020 shared task: Compose transformer pretrained models\n  for Reliable Intelligence Identification on Social network", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our method for tuning a transformer-based pretrained\nmodel, to adaptation with Reliable Intelligence Identification on Vietnamese\nSNSs problem. We also proposed a model that combines bert-base pretrained\nmodels with some metadata features, such as the number of comments, number of\nlikes, images of SNS documents,... to improved results for VLSP shared task:\nReliable Intelligence Identification on Vietnamese SNSs. With appropriate\ntraining techniques, our model is able to achieve 0.9392 ROC-AUC on public test\nset and the final version settles at top 2 ROC-AUC (0.9513) on private test\nset.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 16:19:28 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Nguyen", "Thanh Chinh", ""], ["Nguyen", "Van Nha", ""]]}]