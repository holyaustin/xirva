[{"id": "1208.0153", "submitter": "Mohamed Salah  Gouider Dr", "authors": "Saida Aissi and Mohamed Salah Gouider", "title": "Personalization in Geographic information systems: A survey", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 4, No 3, July 2012 , pp 291-298", "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geographic Information Systems (GIS) are widely used in different domains of\napplications, such as maritime navigation, museums visits and route planning,\nas well as ecological, demographical and economical applications. Nowadays,\norganizations need sophisticated and adapted GIS-based Decision Support System\n(DSS) to get quick access to relevant information and to analyze data with\nrespect to geographic information, represented not only as spatial objects, but\nalso as maps.\n  Several research works on GIS personalization was proposed: Face the great\nchallenge of developing both the theory and practice to provide personalization\nGIS visualization systems. This paper aims to provide a comprehensive review of\nliterature on presented GIS personalization approaches. A benchmarking study of\nGIS personalization methods is proposed. Several evaluation criteria are used\nto identify the existence of trends as well as potential needs for further\ninvestigations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 09:45:07 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Aissi", "Saida", ""], ["Gouider", "Mohamed Salah", ""]]}, {"id": "1208.0359", "submitter": "Pedro Hipola", "authors": "Amed Leiva-Mederos, Jose A. Senso, Sandor Dominguez-Velasco, Pedro\n  Hipola", "title": "An Automat for the Semantic Processing of Structured Information", "comments": "IEEE Intelligent Systems Design and Applications, 2009. ISDA '09.\n  Ninth International Conference on Date of Conference: Nov. 30 2009-Dec. 2\n  2009, Page(s): 85 - 89", "journal-ref": "IEEE Intelligent Systems Design and Applications, 2009. ISDA '09.\n  Ninth International Conference on Date of Conference: Nov. 30 2009-Dec. 2\n  2009, Page(s): 85 - 89", "doi": "10.1109/ISDA.2009.120", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the database of the PuertoTerm project, an indexing system based on the\ncognitive model of Brigitte Enders was built. By analyzing the cognitive\nstrategies of three abstractors, we built an automat that serves to simulate\nhuman indexing processes. The automat allows the texts integrated in the system\nto be assessed, evaluated and grouped by means of the bipartite spectral graph\npartitioning algorithm, which also permits visualization of the terms and the\ndocuments. The system features an ontology and a database to enhance its\noperativity. As a result of the application, we achieved better rates of\nexhaustivity in the indexing of documents, as well as greater precision and\nretrieval of information, with high levels of efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 20:53:23 GMT"}], "update_date": "2012-08-03", "authors_parsed": [["Leiva-Mederos", "Amed", ""], ["Senso", "Jose A.", ""], ["Dominguez-Velasco", "Sandor", ""], ["Hipola", "Pedro", ""]]}, {"id": "1208.0690", "submitter": "Hamed Hassanzadeh", "authors": "Hamed Hassanzadeh and Mohammad Reza Keyvanpour", "title": "Semantic Web Requirements through Web Mining Techniques", "comments": "arXiv admin note: text overlap with arXiv:cs/0011033 by other authors", "journal-ref": "International Journal of Computer Theory and Engineering vol. 4,\n  no. 4, pp. 616-620, 2012", "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In recent years, Semantic web has become a topic of active research in\nseveral fields of computer science and has applied in a wide range of domains\nsuch as bioinformatics, life sciences, and knowledge management. The two\nfast-developing research areas semantic web and web mining can complement each\nother and their different techniques can be used jointly or separately to solve\nthe issues in both areas. In addition, since shifting from current web to\nsemantic web mainly depends on the enhancement of knowledge, web mining can\nplay a key role in facing numerous challenges of this changing. In this paper,\nwe analyze and classify the application of divers web mining techniques in\ndifferent challenges of the semantic web in form of an analytical framework.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 08:42:07 GMT"}], "update_date": "2012-08-06", "authors_parsed": [["Hassanzadeh", "Hamed", ""], ["Keyvanpour", "Mohammad Reza", ""]]}, {"id": "1208.0782", "submitter": "Shang Shang", "authors": "Shang Shang, Pan Hui, Sanjeev R. Kulkarni and Paul W. Cuff", "title": "Wisdom of the Crowd: Incorporating Social Influence in Recommendation\n  Models", "comments": "HotPost 2011, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Recommendation systems have received considerable attention recently.\nHowever, most research has been focused on improving the performance of\ncollaborative filtering (CF) techniques. Social networks, indispensably,\nprovide us extra information on people's preferences, and should be considered\nand deployed to improve the quality of recommendations. In this paper, we\npropose two recommendation models, for individuals and for groups respectively,\nbased on social contagion and social influence network theory. In the\nrecommendation model for individuals, we improve the result of collaborative\nfiltering prediction with social contagion outcome, which simulates the result\nof information cascade in the decision-making process. In the recommendation\nmodel for groups, we apply social influence network theory to take\ninterpersonal influence into account to form a settled pattern of disagreement,\nand then aggregate opinions of group members. By introducing the concept of\nsusceptibility and interpersonal influence, the settled rating results are\nflexible, and inclined to members whose ratings are \"essential\".\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 16:00:35 GMT"}, {"version": "v2", "created": "Fri, 17 May 2013 21:55:30 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Shang", "Shang", ""], ["Hui", "Pan", ""], ["Kulkarni", "Sanjeev R.", ""], ["Cuff", "Paul W.", ""]]}, {"id": "1208.0787", "submitter": "Shang Shang", "authors": "Shang Shang, Sanjeev R. Kulkarni, Paul W. Cuff and Pan Hui", "title": "A Random Walk Based Model Incorporating Social Information for\n  Recommendations", "comments": "2012 IEEE Machine Learning for Signal Processing Workshop (MLSP), 6\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Collaborative filtering (CF) is one of the most popular approaches to build a\nrecommendation system. In this paper, we propose a hybrid collaborative\nfiltering model based on a Makovian random walk to address the data sparsity\nand cold start problems in recommendation systems. More precisely, we construct\na directed graph whose nodes consist of items and users, together with item\ncontent, user profile and social network information. We incorporate user's\nratings into edge settings in the graph model. The model provides personalized\nrecommendations and predictions to individuals and groups. The proposed\nalgorithms are evaluated on MovieLens and Epinions datasets. Experimental\nresults show that the proposed methods perform well compared with other\ngraph-based methods, especially in the cold start case.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 16:15:10 GMT"}, {"version": "v2", "created": "Fri, 17 May 2013 21:57:26 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Shang", "Shang", ""], ["Kulkarni", "Sanjeev R.", ""], ["Cuff", "Paul W.", ""], ["Hui", "Pan", ""]]}, {"id": "1208.1004", "submitter": "Georgios Pitsilis", "authors": "Georgios Pitsilis, Svein J. Knapskog", "title": "Social Trust as a solution to address sparsity-inherent problems of\n  Recommender systems", "comments": "ACM RecSys 2009, Workshop on Recommender Systems & The Social Web,\n  Oct. 2009, ISSN:1613-0073, New York, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trust has been explored by many researchers in the past as a successful\nsolution for assisting recommender systems. Even though the approach of using a\nweb-of-trust scheme for assisting the recommendation production is well\nadopted, issues like the sparsity problem have not been explored adequately so\nfar with regard to this. In this work we are proposing and testing a scheme\nthat uses the existing ratings of users to calculate the hypothetical trust\nthat might exist between them. The purpose is to demonstrate how some basic\nsocial networking when applied to an existing system can help in alleviating\nproblems of traditional recommender system schemes. Interestingly, such schemes\nare also alleviating the cold start problem from which mainly new users are\nsuffering. In order to show how good the system is in that respect, we measure\nthe performance at various times as the system evolves and we also contrast the\nsolution with existing approaches. Finally, we present the results which\njustify that such schemes undoubtedly work better than a system that makes no\nuse of trust at all.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2012 12:32:21 GMT"}], "update_date": "2012-08-07", "authors_parsed": [["Pitsilis", "Georgios", ""], ["Knapskog", "Svein J.", ""]]}, {"id": "1208.1011", "submitter": "Dirk Lewandowski", "authors": "Dirk Lewandowski", "title": "Credibility in Web Search Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web search engines apply a variety of ranking signals to achieve user\nsatisfaction, i.e., results pages that provide the best-possible results to the\nuser. While these ranking signals implicitly consider credibility (e.g., by\nmeasuring popularity), explicit measures of credibility are not applied. In\nthis chapter, credibility in Web search engines is discussed in a broad\ncontext: credibility as a measure for including documents in a search engine's\nindex, credibility as a ranking signal, credibility in the context of universal\nsearch results, and the possibility of using credibility as an explicit measure\nfor ranking purposes. It is found that while search engines-at least to a\ncertain extent-show credible results to their users, there is no fully\nintegrated credibility framework for Web search engines.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2012 14:03:22 GMT"}], "update_date": "2012-08-07", "authors_parsed": [["Lewandowski", "Dirk", ""]]}, {"id": "1208.1187", "submitter": "Stamatina Thomaidou", "authors": "Stamatina Thomaidou, Michalis Vazirgiannis, Kyriakos Liakopoulos", "title": "Toward an Integrated Framework for Automated Development and\n  Optimization of Online Advertising Campaigns", "comments": "Submitted to ACM Transactions on the Web (TWEB) Journal - July 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating and monitoring competitive and cost-effective pay-per-click\nadvertisement campaigns through the web-search channel is a resource demanding\ntask in terms of expertise and effort. Assisting or even automating the work of\nan advertising specialist will have an unrivaled commercial value. In this\npaper we propose a methodology, an architecture, and a fully functional\nframework for semi- and fully- automated creation, monitoring, and optimization\nof cost-efficient pay-per-click campaigns with budget constraints. The campaign\ncreation module generates automatically keywords based on the content of the\nweb page to be advertised extended with corresponding ad-texts. These keywords\nare used to create automatically the campaigns fully equipped with the\nappropriate values set. The campaigns are uploaded to the auctioneer platform\nand start running. The optimization module focuses on the learning process from\nexisting campaign statistics and also from applied strategies of previous\nperiods in order to invest optimally in the next period. The objective is to\nmaximize the performance (i.e. clicks, actions) under the current budget\nconstraint. The fully functional prototype is experimentally evaluated on real\nworld Google AdWords campaigns and presents a promising behavior with regards\nto campaign performance statistics as it outperforms systematically the\ncompeting manually maintained campaigns.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2012 15:18:36 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Thomaidou", "Stamatina", ""], ["Vazirgiannis", "Michalis", ""], ["Liakopoulos", "Kyriakos", ""]]}, {"id": "1208.1259", "submitter": "Ping Li", "authors": "Ping Li and Art Owen and Cun-Hui Zhang", "title": "One Permutation Hashing for Efficient Search and Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the method of b-bit minwise hashing has been applied to large-scale\nlinear learning and sublinear time near-neighbor search. The major drawback of\nminwise hashing is the expensive preprocessing cost, as the method requires\napplying (e.g.,) k=200 to 500 permutations on the data. The testing time can\nalso be expensive if a new data point (e.g., a new document or image) has not\nbeen processed, which might be a significant issue in user-facing applications.\n  We develop a very simple solution based on one permutation hashing.\nConceptually, given a massive binary data matrix, we permute the columns only\nonce and divide the permuted columns evenly into k bins; and we simply store,\nfor each data vector, the smallest nonzero location in each bin. The\ninteresting probability analysis (which is validated by experiments) reveals\nthat our one permutation scheme should perform very similarly to the original\n(k-permutation) minwise hashing. In fact, the one permutation scheme can be\neven slightly more accurate, due to the \"sample-without-replacement\" effect.\n  Our experiments with training linear SVM and logistic regression on the\nwebspam dataset demonstrate that this one permutation hashing scheme can\nachieve the same (or even slightly better) accuracies compared to the original\nk-permutation scheme. To test the robustness of our method, we also experiment\nwith the small news20 dataset which is very sparse and has merely on average\n500 nonzeros in each data vector. Interestingly, our one permutation scheme\nnoticeably outperforms the k-permutation scheme when k is not too small on the\nnews20 dataset. In summary, our method can achieve at least the same accuracy\nas the original k-permutation scheme, at merely 1/k of the original\npreprocessing cost.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2012 12:28:06 GMT"}], "update_date": "2012-08-08", "authors_parsed": [["Li", "Ping", ""], ["Owen", "Art", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1208.1448", "submitter": "Cheng Chen", "authors": "Cheng Chen, Kui Wu, Venkatesh Srinivasan and Kesav Bharadwaj R", "title": "The Best Answers? Think Twice: Online Detection of Commercial Campaigns\n  in the CQA Forums", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an emerging trend, more and more Internet users search for information\nfrom Community Question and Answer (CQA) websites, as interactive communication\nin such websites provides users with a rare feeling of trust. More often than\nnot, end users look for instant help when they browse the CQA websites for the\nbest answers. Hence, it is imperative that they should be warned of any\npotential commercial campaigns hidden behind the answers. However, existing\nresearch focuses more on the quality of answers and does not meet the above\nneed. In this paper, we develop a system that automatically analyzes the hidden\npatterns of commercial spam and raises alarms instantaneously to end users\nwhenever a potential commercial campaign is detected. Our detection method\nintegrates semantic analysis and posters' track records and utilizes the\nspecial features of CQA websites largely different from those in other types of\nforums such as microblogs or news reports. Our system is adaptive and\naccommodates new evidence uncovered by the detection algorithms over time.\nValidated with real-world trace data from a popular Chinese CQA website over a\nperiod of three months, our system shows great potential towards adaptive\nonline detection of CQA spams.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2012 15:41:33 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2013 07:46:59 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Chen", "Cheng", ""], ["Wu", "Kui", ""], ["Srinivasan", "Venkatesh", ""], ["R", "Kesav Bharadwaj", ""]]}, {"id": "1208.1886", "submitter": "Raghu Anantharangachar", "authors": "Raghu Anantharangachar and Ramani Srinivasan", "title": "Semantic Web Techniques for Yellow Page Service Providers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Use of web pages providing unstructured information poses variety of problems\nto the user, such as use of arbitrary formats, unsuitability for machine\nprocessing and likely incompleteness of information. Structured data alleviates\nthese problems but we require more. Very often yellow page systems are\nimplemented using a centralized database. In some cases, human intermediaries\naccessible over the phone network examine a centralized database and use their\nreasoning ability to deal with the user's need for information. Scaling up such\nsystems is difficult. This paper explores an alternative - a highly distributed\nsystem design meeting a variety of needs - considerably reducing efforts\nrequired at a central organization, enabling large numbers of vendors to enter\ninformation about their own products and services, enabling end-users to\ncontribute information such as their own ratings, using an ontology to describe\neach domain of application in a flexible manner for uses foreseen and\nunforeseen, enabling distributed search and mash-ups, use of vendor independent\nstandards, using reasoning to find the best matches to a given query,\ngeo-spatial reasoning and a simple, interactive, mobile application/interface.\nWe give importance to geo-spatial information and mobile applications because\nof the very wide-spread use of mobile phones and their inherent ability to\nprovide some information about the current location of the user. We have\ncreated a prototype using the Jena Toolkit and geo-spatial extensions to\nSPARQL. We have tested this prototype by asking a group of typical users to use\nit and to provide structured feedback. We have summarized this feedback in the\npaper. We believe that the technology can be applied in many contexts in\naddition to yellow page systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2012 12:26:48 GMT"}], "update_date": "2012-08-10", "authors_parsed": [["Anantharangachar", "Raghu", ""], ["Srinivasan", "Ramani", ""]]}, {"id": "1208.1926", "submitter": "Laxmi Choudhary", "authors": "Laxmi Choudhary and Bhawani Shankar Burdak", "title": "Role of Ranking Algorithms for Information Retrieval", "comments": "Keywords: Page Rank, Web Mining, Web Structured Mining, Web Content\n  Mining", "journal-ref": null, "doi": "10.5121/ijaia.2012.3415", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the use of web is increasing more day by day, the web users get easily\nlost in the web's rich hyper structure. The main aim of the owner of the\nwebsite is to give the relevant information according their needs to the users.\nWe explained the Web mining is used to categorize users and pages by analyzing\nuser's behavior, the content of pages and then describe Web Structure mining.\nThis paper includes different Page Ranking algorithms and compares those\nalgorithms used for Information Retrieval. Different Page Rank based algorithms\nlike Page Rank (PR), WPR (Weighted Page Rank), HITS (Hyperlink Induced Topic\nSelection), Distance Rank and EigenRumor algorithms are discussed and compared.\nSimulation Interface has been designed for PageRank algorithm and Weighted\nPageRank algorithm but PageRank is the only ranking algorithm on which Google\nsearch engine works.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2012 14:43:51 GMT"}], "update_date": "2012-08-10", "authors_parsed": [["Choudhary", "Laxmi", ""], ["Burdak", "Bhawani Shankar", ""]]}, {"id": "1208.2175", "submitter": "Michael Bell BSc", "authors": "Michael J. Bell, Colin S. Gillespie, Daniel Swan, Phillip Lord", "title": "An approach to describing and analysing bulk biological annotation\n  quality: a case study using UniProtKB", "comments": "Paper accepted at The European Conference on Computational Biology\n  2012 (ECCB'12). Subsequently will be published in a special issue of the\n  journal Bioinformatics. Paper consists of 8 pages, made up of 5 figures", "journal-ref": null, "doi": "10.1093/bioinformatics/bts372", "report-no": null, "categories": "cs.CE cs.IR q-bio.GN", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Motivation: Annotations are a key feature of many biological databases, used\nto convey our knowledge of a sequence to the reader. Ideally, annotations are\ncurated manually, however manual curation is costly, time consuming and\nrequires expert knowledge and training. Given these issues and the exponential\nincrease of data, many databases implement automated annotation pipelines in an\nattempt to avoid un-annotated entries. Both manual and automated annotations\nvary in quality between databases and annotators, making assessment of\nannotation reliability problematic for users. The community lacks a generic\nmeasure for determining annotation quality and correctness, which we look at\naddressing within this article. Specifically we investigate word reuse within\nbulk textual annotations and relate this to Zipf's Principle of Least Effort.\nWe use UniProt Knowledge Base (UniProtKB) as a case study to demonstrate this\napproach since it allows us to compare annotation change, both over time and\nbetween automated and manually curated annotations.\n  Results: By applying power-law distributions to word reuse in annotation, we\nshow clear trends in UniProtKB over time, which are consistent with existing\nstudies of quality on free text English. Further, we show a clear distinction\nbetween manual and automated analysis and investigate cohorts of protein\nrecords as they mature. These results suggest that this approach holds distinct\npromise as a mechanism for judging annotation quality.\n  Availability: Source code is available at the authors website:\nhttp://homepages.cs.ncl.ac.uk/m.j.bell1/annotation.\n  Contact: phillip.lord@newcastle.ac.uk\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2012 13:48:39 GMT"}], "update_date": "2013-08-22", "authors_parsed": [["Bell", "Michael J.", ""], ["Gillespie", "Colin S.", ""], ["Swan", "Daniel", ""], ["Lord", "Phillip", ""]]}, {"id": "1208.2261", "submitter": "Sudarshan Nandy", "authors": "Sudarshan Nandy, Partha Pratim Sarkar and Achintya Das", "title": "Analysis of Statistical Hypothesis based Learning Mechanism for Faster\n  Crawling", "comments": "14 Pages, 7 Figures This paper has been withdrawn by the author due\n  to a crucial sign error in page no. 3,4,7 and 11. The error is also observed\n  with equation no in page 10", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), Vol.3, No.4, July 2012, 117-130", "doi": "10.5121/ijaia.2012.3409", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of world-wide-web (WWW) spreads its wings from an intangible\nquantities of web-pages to a gigantic hub of web information which gradually\nincreases the complexity of crawling process in a search engine. A search\nengine handles a lot of queries from various parts of this world, and the\nanswers of it solely depend on the knowledge that it gathers by means of\ncrawling. The information sharing becomes a most common habit of the society,\nand it is done by means of publishing structured, semi-structured and\nunstructured resources on the web. This social practice leads to an exponential\ngrowth of web-resource, and hence it became essential to crawl for continuous\nupdating of web-knowledge and modification of several existing resources in any\nsituation. In this paper one statistical hypothesis based learning mechanism is\nincorporated for learning the behavior of crawling speed in different\nenvironment of network, and for intelligently control of the speed of crawler.\nThe scaling technique is used to compare the performance proposed method with\nthe standard crawler. The high speed performance is observed after scaling, and\nthe retrieval of relevant web-resource in such a high speed is analyzed.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2012 19:43:43 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2012 05:40:17 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Nandy", "Sudarshan", ""], ["Sarkar", "Partha Pratim", ""], ["Das", "Achintya", ""]]}, {"id": "1208.2355", "submitter": "Yuri Pritykin", "authors": "Maxim Zhukovskiy and Dmitry Vinogradov and Yuri Pritykin and Liudmila\n  Ostroumova and Evgeniy Grechnikov and Gleb Gusev and Pavel Serdyukov and\n  Andrei Raigorodskii", "title": "Empirical Validation of the Buckley--Osthus Model for the Web Host\n  Graph: Degree and Edge Distributions", "comments": "21 pages, 8 figures; short version to appear in CIKM 2012 (see\n  http://www.cikm2012.org/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a lot of research on random graph models for large real-world\nnetworks such as those formed by hyperlinks between web pages in the world wide\nweb. Though largely successful qualitatively in capturing their key properties,\nsuch models may lack important quantitative characteristics of Internet graphs.\nWhile preferential attachment random graph models were shown to be capable of\nreflecting the degree distribution of the webgraph, their ability to reflect\ncertain aspects of the edge distribution was not yet well studied.\n  In this paper, we consider the Buckley--Osthus implementation of preferential\nattachment and its ability to model the web host graph in two aspects. One is\nthe degree distribution that we observe to follow the power law, as often being\nthe case for real-world graphs. Another one is the two-dimensional edge\ndistribution, the number of edges between vertices of given degrees. We fit a\nsingle \"initial attractiveness\" parameter $a$ of the model, first with respect\nto the degree distribution of the web host graph, and then, absolutely\nindependently, with respect to the edge distribution. Surprisingly, the values\nof $a$ we obtain turn out to be nearly the same. Therefore the same model with\nthe same value of the parameter $a$ fits very well the two independent and\nbasic aspects of the web host graph. In addition, we demonstrate that other\nmodels completely lack the asymptotic behavior of the edge distribution of the\nweb host graph, even when accurately capturing the degree distribution.\n  To the best of our knowledge, this is the first attempt for a real graph of\nInternet to describe the distribution of edges between vertices with respect to\ntheir degrees.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2012 14:51:07 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Zhukovskiy", "Maxim", ""], ["Vinogradov", "Dmitry", ""], ["Pritykin", "Yuri", ""], ["Ostroumova", "Liudmila", ""], ["Grechnikov", "Evgeniy", ""], ["Gusev", "Gleb", ""], ["Serdyukov", "Pavel", ""], ["Raigorodskii", "Andrei", ""]]}, {"id": "1208.2478", "submitter": "Sreenivas Gollapudi", "authors": "Sreenivas Gollapudi, Samuel Ieong, Anitha Kannan", "title": "Structured Query Reformulations in Commerce Search", "comments": "A shorter version appeared in CIKM 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in commerce search has shown that understanding the semantics in\nuser queries enables more effective query analysis and retrieval of relevant\nproducts. However, due to lack of sufficient domain knowledge, user queries\noften include terms that cannot be mapped directly to any product attribute.\nFor example, a user looking for {\\tt designer handbags} might start with such a\nquery because she is not familiar with the manufacturers, the price ranges,\nand/or the material that gives a handbag designer appeal. Current commerce\nsearch engines treat terms such as {\\tt designer} as keywords and attempt to\nmatch them to contents such as product reviews and product descriptions, often\nresulting in poor user experience.\n  In this study, we propose to address this problem by reformulating queries\ninvolving terms such as {\\tt designer}, which we call \\emph{modifiers}, to\nqueries that specify precise product attributes. We learn to rewrite the\nmodifiers to attribute values by analyzing user behavior and leveraging\nstructured data sources such as the product catalog that serves the queries. We\nfirst produce a probabilistic mapping between the modifiers and attribute\nvalues based on user behavioral data. These initial associations are then used\nto retrieve products from the catalog, over which we infer sets of attribute\nvalues that best describe the semantics of the modifiers. We evaluate the\neffectiveness of our approach based on a comprehensive Mechanical Turk study.\nWe find that users agree with the attribute values selected by our approach in\nabout 95% of the cases and they prefer the results surfaced for our\nreformulated queries to ones for the original queries in 87% of the time.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 01:00:24 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Gollapudi", "Sreenivas", ""], ["Ieong", "Samuel", ""], ["Kannan", "Anitha", ""]]}, {"id": "1208.2534", "submitter": "Pedro Pinto", "authors": "Pedro C. Pinto, Patrick Thiran, Martin Vetterli", "title": "Locating the Source of Diffusion in Large-Scale Networks", "comments": "To appear in Physical Review Letters. Includes pre-print of main\n  paper, and supplementary material", "journal-ref": null, "doi": "10.1103/PhysRevLett.109.068702", "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we localize the source of diffusion in a complex network? Due to the\ntremendous size of many real networks--such as the Internet or the human social\ngraph--it is usually infeasible to observe the state of all nodes in a network.\nWe show that it is fundamentally possible to estimate the location of the\nsource from measurements collected by sparsely-placed observers. We present a\nstrategy that is optimal for arbitrary trees, achieving maximum probability of\ncorrect localization. We describe efficient implementations with complexity\nO(N^{\\alpha}), where \\alpha=1 for arbitrary trees, and \\alpha=3 for arbitrary\ngraphs. In the context of several case studies, we determine how localization\naccuracy is affected by various system parameters, including the structure of\nthe network, the density of observers, and the number of observed cascades.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 09:44:57 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Pinto", "Pedro C.", ""], ["Thiran", "Patrick", ""], ["Vetterli", "Martin", ""]]}, {"id": "1208.2547", "submitter": "Lexing Xie", "authors": "Yanxiang Wang, Hari Sundaram, Lexing Xie", "title": "Social Event Detection with Interaction Graph Modeling", "comments": "ACM Multimedia 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.MM physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on detecting social, physical-world events from photos\nposted on social media sites. The problem is important: cheap media capture\ndevices have significantly increased the number of photos shared on these\nsites. The main contribution of this paper is to incorporate online social\ninteraction features in the detection of physical events. We believe that\nonline social interaction reflect important signals among the participants on\nthe \"social affinity\" of two photos, thereby helping event detection. We\ncompute social affinity via a random-walk on a social interaction graph to\ndetermine similarity between two photos on the graph. We train a support vector\nmachine classifier to combine the social affinity between photos and\nphoto-centric metadata including time, location, tags and description.\nIncremental clustering is then used to group photos to event clusters. We have\nvery good results on two large scale real-world datasets: Upcoming and\nMediaEval. We show an improvement between 0.06-0.10 in F1 on these datasets.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 11:20:05 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Wang", "Yanxiang", ""], ["Sundaram", "Hari", ""], ["Xie", "Lexing", ""]]}, {"id": "1208.2782", "submitter": "K.S.Kuppusamy", "authors": "K. S. Kuppusamy and G. Aghila", "title": "Multidimensional Web Page Evaluation Model Using Segmentation And\n  Annotations", "comments": "11 Pages, 4 Figures; International Journal on Cybernetics &\n  Informatics (IJCI), Vol.1, No.4, August 2012. arXiv admin note: substantial\n  text overlap with arXiv:1203.3613", "journal-ref": "International Journal on Cybernetics & Informatics (IJCI), Vol.1,\n  No.4, August 2012", "doi": "10.5121/ijci.2012.1401", "report-no": "2277 - 548X", "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evaluation of web pages against a query is the pivot around which the\nInformation Retrieval domain revolves around. The context sensitive, semantic\nevaluation of web pages is a non-trivial problem which needs to be addressed\nimmediately. This research work proposes a model to evaluate the web pages by\ncumulating the segment scores which are computed by multidimensional evaluation\nmethodology. The model proposed is hybrid since it utilizes both the structural\nsemantics and content semantics in the evaluation process. The score of the web\npage is computed in a bottom-up process by evaluating individual segment's\nscore through a multi-dimensional approach. The model incorporates an approach\nfor segment level annotation. The proposed model is prototyped for evaluation;\nexperiments conducted on the prototype confirm the model's efficiency in\nsemantic evaluation of pages.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2012 04:40:30 GMT"}], "update_date": "2012-11-02", "authors_parsed": [["Kuppusamy", "K. S.", ""], ["Aghila", "G.", ""]]}, {"id": "1208.2808", "submitter": "Sudarshan Nandy", "authors": "Sudarshan Nandy, Partha Pratim Sarkar and Achintya Das", "title": "Analysis of a Statistical Hypothesis Based Learning Mechanism for Faster\n  crawling", "comments": "14 Pages, 7 Figure", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA), Vol.3, No.4, July 2012, 117-130", "doi": "10.5121/ijaia.2012.3409", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of world-wide-web (WWW) spreads its wings from an intangible\nquantities of web-pages to a gigantic hub of web information which gradually\nincreases the complexity of crawling process in a search engine. A search\nengine handles a lot of queries from various parts of this world, and the\nanswers of it solely depend on the knowledge that it gathers by means of\ncrawling. The information sharing becomes a most common habit of the society,\nand it is done by means of publishing structured, semi-structured and\nunstructured resources on the web. This social practice leads to an exponential\ngrowth of web-resource, and hence it became essential to crawl for continuous\nupdating of web-knowledge and modification of several existing resources in any\nsituation. In this paper one statistical hypothesis based learning mechanism is\nincorporated for learning the behavior of crawling speed in different\nenvironment of network, and for intelligently control of the speed of crawler.\nThe scaling technique is used to compare the performance proposed method with\nthe standard crawler. The high speed performance is observed after scaling, and\nthe retrieval of relevant web-resource in such a high speed is analyzed.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2012 08:36:49 GMT"}], "update_date": "2012-08-15", "authors_parsed": [["Nandy", "Sudarshan", ""], ["Sarkar", "Partha Pratim", ""], ["Das", "Achintya", ""]]}, {"id": "1208.2873", "submitter": "Vasileios Lampos", "authors": "Vasileios Lampos", "title": "Detecting Events and Patterns in Large-Scale User Generated Textual\n  Streams with Statistical Learning Methods", "comments": "PhD thesis, 238 pages, 9 chapters, 2 appendices, 58 figures, 49\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.SI stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A vast amount of textual web streams is influenced by events or phenomena\nemerging in the real world. The social web forms an excellent modern paradigm,\nwhere unstructured user generated content is published on a regular basis and\nin most occasions is freely distributed. The present Ph.D. Thesis deals with\nthe problem of inferring information - or patterns in general - about events\nemerging in real life based on the contents of this textual stream. We show\nthat it is possible to extract valuable information about social phenomena,\nsuch as an epidemic or even rainfall rates, by automatic analysis of the\ncontent published in Social Media, and in particular Twitter, using Statistical\nMachine Learning methods. An important intermediate task regards the formation\nand identification of features which characterise a target event; we select and\nuse those textual features in several linear, non-linear and hybrid inference\napproaches achieving a significantly good performance in terms of the applied\nloss function. By examining further this rich data set, we also propose methods\nfor extracting various types of mood signals revealing how affective norms - at\nleast within the social web's population - evolve during the day and how\nsignificant events emerging in the real world are influencing them. Lastly, we\npresent some preliminary findings showing several spatiotemporal\ncharacteristics of this textual information as well as the potential of using\nit to tackle tasks such as the prediction of voting intentions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 18:59:54 GMT"}], "update_date": "2012-08-15", "authors_parsed": [["Lampos", "Vasileios", ""]]}, {"id": "1208.3530", "submitter": "Haimonti Dutta", "authors": "Haimonti Dutta, William Chan, Deepak Shankargouda, Manoj Pooleery,\n  Axinia Radeva, Kyle Rego, Boyi Xie, Rebecca Passonneau, Austin Lee and\n  Barbara Taranto", "title": "Leveraging Subjective Human Annotation for Clustering Historic Newspaper\n  Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The New York Public Library is participating in the Chronicling America\ninitiative to develop an online searchable database of historically significant\nnewspaper articles. Microfilm copies of the newspapers are scanned and high\nresolution Optical Character Recognition (OCR) software is run on them. The\ntext from the OCR provides a wealth of data and opinion for researchers and\nhistorians. However, categorization of articles provided by the OCR engine is\nrudimentary and a large number of the articles are labeled editorial without\nfurther grouping. Manually sorting articles into fine-grained categories is\ntime consuming if not impossible given the size of the corpus. This paper\nstudies techniques for automatic categorization of newspaper articles so as to\nenhance search and retrieval on the archive. We explore unsupervised (e.g.\nKMeans) and semi-supervised (e.g. constrained clustering) learning algorithms\nto develop article categorization schemes geared towards the needs of\nend-users. A pilot study was designed to understand whether there was unanimous\nagreement amongst patrons regarding how articles can be categorized. It was\nfound that the task was very subjective and consequently automated algorithms\nthat could deal with subjective labels were used. While the small scale pilot\nstudy was extremely helpful in designing machine learning algorithms, a much\nlarger system needs to be developed to collect annotations from users of the\narchive. The \"BODHI\" system currently being developed is a step in that\ndirection, allowing users to correct wrongly scanned OCR and providing keywords\nand tags for newspaper articles used frequently. On successful implementation\nof the beta version of this system, we hope that it can be integrated with\nexisting software being developed for the Chronicling America project.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2012 04:48:58 GMT"}], "update_date": "2012-08-20", "authors_parsed": [["Dutta", "Haimonti", ""], ["Chan", "William", ""], ["Shankargouda", "Deepak", ""], ["Pooleery", "Manoj", ""], ["Radeva", "Axinia", ""], ["Rego", "Kyle", ""], ["Xie", "Boyi", ""], ["Passonneau", "Rebecca", ""], ["Lee", "Austin", ""], ["Taranto", "Barbara", ""]]}, {"id": "1208.3623", "submitter": "Rafi Muhammad", "authors": "Muhammad Rafi, Sundus Hassan and Mohammad Shahid Shaikh", "title": "Content-based Text Categorization using Wikitology", "comments": "9 pages; IJCSI August 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major computational burden, while performing document clustering, is the\ncalculation of similarity measure between a pair of documents. Similarity\nmeasure is a function that assign a real number between 0 and 1 to a pair of\ndocuments, depending upon the degree of similarity between them. A value of\nzero means that the documents are completely dissimilar whereas a value of one\nindicates that the documents are practically identical. Traditionally,\nvector-based models have been used for computing the document similarity. The\nvector-based models represent several features present in documents. These\napproaches to similarity measures, in general, cannot account for the semantics\nof the document. Documents written in human languages contain contexts and the\nwords used to describe these contexts are generally semantically related.\nMotivated by this fact, many researchers have proposed semantic-based\nsimilarity measures by utilizing text annotation through external thesauruses\nlike WordNet (a lexical database). In this paper, we define a semantic\nsimilarity measure based on documents represented in topic maps. Topic maps are\nrapidly becoming an industrial standard for knowledge representation with a\nfocus for later search and extraction. The documents are transformed into a\ntopic map based coded knowledge and the similarity between a pair of documents\nis represented as a correlation between the common patterns. The experimental\nstudies on the text mining datasets reveal that this new similarity measure is\nmore effective as compared to commonly used similarity measures in text\nclustering.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2012 15:49:38 GMT"}], "update_date": "2012-08-20", "authors_parsed": [["Rafi", "Muhammad", ""], ["Hassan", "Sundus", ""], ["Shaikh", "Mohammad Shahid", ""]]}, {"id": "1208.3774", "submitter": "Mohammad Saiful Islam Mamun", "authors": "A.F.M. Sultanul Kabir and Mohammad Saiful Islam Mamun", "title": "Graphical Query Builder in Opportunistic Sensor Networks to discover\n  Sensor Information", "comments": "15 pages, International Journal of Computer Applications (IJCA) (0975\n  - 8887) Volume 15- No.1, February 2011", "journal-ref": "International Journal of Computer Applications 15(1):11-25,\n  February 2011. Published by Foundation of Computer Science", "doi": "10.5120/1913-2551 10.5120/1913-2551 10.5120/1913-2551", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of sensor network applications are data-driven. We believe that query\nis the most preferred way to discover sensor services. Normally users are\nunaware of available sensors. Thus users need to pose different types of query\nover the sensor network to get the desired information. Even users may need to\ninput more complicated queries with higher levels of aggregations, and requires\nmore complex interactions with the system. As the users have no prior knowledge\nof the sensor data or services our aim is to develop a visual query interface\nwhere users can feed more user friendly queries and machine can understand\nthose. In this paper work, we have developed an Interactive visual query\ninterface for the users. To accomplish this we have considered several use\ncases and we have derived graphical representation of query from their text\nbased format for those use case scenario. We have facilitated the user by\nextracting class, subclass and properties from Ontology. To do so we have\nparsed OWL file in the user interface and based upon the parsed information\nusers build visual query. Later on we have translated the visual query\nlanguages into SPARQL query, a machine understandable format which helps the\nmachine to communicate with the underlying technology.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2012 18:41:07 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Kabir", "A. F. M. Sultanul", ""], ["Mamun", "Mohammad Saiful Islam", ""]]}, {"id": "1208.3779", "submitter": "Jim Jing-Yan Wang", "authors": "Jim Jing-Yan Wang, Halima Bensmail and Xin Gao", "title": "Multiple graph regularized protein domain ranking", "comments": "21 pages", "journal-ref": "Jim Jing-Yan Wang, Halima Bensmail and Xin Gao: Multiple graph\n  regularized protein domain ranking, BMC Bioinformatics (2012), 13:307", "doi": "10.1186/1471-2105-13-307", "report-no": null, "categories": "cs.LG cs.CE cs.IR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background Protein domain ranking is a fundamental task in structural\nbiology. Most protein domain ranking methods rely on the pairwise comparison of\nprotein domains while neglecting the global manifold structure of the protein\ndomain database. Recently, graph regularized ranking that exploits the global\nstructure of the graph defined by the pairwise similarities has been proposed.\nHowever, the existing graph regularized ranking methods are very sensitive to\nthe choice of the graph model and parameters, and this remains a difficult\nproblem for most of the protein domain ranking methods.\n  Results To tackle this problem, we have developed the Multiple Graph\nregularized Ranking algorithm, MultiG- Rank. Instead of using a single graph to\nregularize the ranking scores, MultiG-Rank approximates the intrinsic manifold\nof protein domain distribution by combining multiple initial graphs for the\nregularization. Graph weights are learned with ranking scores jointly and\nautomatically, by alternately minimizing an ob- jective function in an\niterative algorithm. Experimental results on a subset of the ASTRAL SCOP\nprotein domain database demonstrate that MultiG-Rank achieves a better ranking\nperformance than single graph regularized ranking methods and pairwise\nsimilarity based ranking methods.\n  Conclusion The problem of graph model and parameter selection in graph\nregularized protein domain ranking can be solved effectively by combining\nmultiple graphs. This aspect of generalization introduces a new frontier in\napplying multiple graphs to solving protein domain ranking applications.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2012 19:32:20 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2013 14:19:56 GMT"}, {"version": "v3", "created": "Sun, 21 Apr 2013 06:21:36 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Wang", "Jim Jing-Yan", ""], ["Bensmail", "Halima", ""], ["Gao", "Xin", ""]]}, {"id": "1208.3952", "submitter": "Philipp Schaer", "authors": "Philipp Schaer, Daniel Hienert, Frank Sawitzki, Andias Wira-Alam,\n  Thomas L\\\"uke", "title": "Dealing with Sparse Document and Topic Representations: Lab Report for\n  CHiC 2012", "comments": "12 pages, to appear in CLEF 2012 Labs and Workshop, Notebook Papers.\n  Report for the CHiC Lab: Ad-hoc Retrieval Task and Semantic Enrichment Task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We will report on the participation of GESIS at the first CHiC workshop\n(Cultural Heritage in CLEF). Being held for the first time, no prior experience\nwith the new data set, a document dump of Europeana with ca. 23 million\ndocuments, exists. The most prominent issues that arose from pretests with this\ntest collection were the very unspecific topics and sparse document\nrepresentations. Only half of the topics (26/50) contained a description and\nthe titles were usually short with just around two words. Therefore we focused\non three different term suggestion and query expansion mechanisms to surpass\nthe sparse topical description. We used two methods that build on concept\nextraction from Wikipedia and on a method that applied co-occurrence statistics\non the available Europeana corpus. In the following paper we will present the\napproaches and preliminary results from their assessments.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2012 09:18:37 GMT"}], "update_date": "2012-08-21", "authors_parsed": [["Schaer", "Philipp", ""], ["Hienert", "Daniel", ""], ["Sawitzki", "Frank", ""], ["Wira-Alam", "Andias", ""], ["L\u00fcke", "Thomas", ""]]}, {"id": "1208.4147", "submitter": "Yingzhen Li", "authors": "Yingzhen Li and Ye Zhang", "title": "Generating ordered list of Recommended Items: a Hybrid Recommender\n  System of Microblog", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise recommendation of followers helps in improving the user experience\nand maintaining the prosperity of twitter and microblog platforms. In this\npaper, we design a hybrid recommender system of microblog as a solution of KDD\nCup 2012, track 1 task, which requires predicting users a user might follow in\nTencent Microblog. We describe the background of the problem and present the\nalgorithm consisting of keyword analysis, user taxonomy, (potential)interests\nextraction and item recommendation. Experimental result shows the high\nperformance of our algorithm. Some possible improvements are discussed, which\nleads to further study.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 00:28:32 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2012 06:37:49 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2015 22:55:51 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Li", "Yingzhen", ""], ["Zhang", "Ye", ""]]}, {"id": "1208.4552", "submitter": "Matus Medo", "authors": "Matus Medo", "title": "Network-based information filtering algorithms: ranking and\n  recommendation", "comments": "book chapter; 21 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After the Internet and the World Wide Web have become popular and\nwidely-available, the electronically stored online interactions of individuals\nhave fast emerged as a challenge for researchers and, perhaps even faster, as a\nsource of valuable information for entrepreneurs. We now have detailed records\nof informal friendship relations in social networks, purchases on e-commerce\nsites, various sorts of information being sent from one user to another, online\ncollections of web bookmarks, and many other data sets that allow us to pose\nquestions that are of interest from both academical and commercial point of\nview. For example, which other users of a social network you might want to be\nfriend with? Which other items you might be interested to purchase? Who are the\nmost influential users in a network? Which web page you might want to visit\nnext? All these questions are not only interesting per se but the answers to\nthem may help entrepreneurs provide better service to their customers and,\nultimately, increase their profits.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2012 16:43:17 GMT"}], "update_date": "2012-08-23", "authors_parsed": [["Medo", "Matus", ""]]}, {"id": "1208.5464", "submitter": "Antonis Sidiropoulos", "authors": "Antonis Sidiropoulos", "title": "Finding Communities in Site Web-Graphs and Citation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web is a typical example of a social network. One of the most intriguing\nfeatures of the Web is its self-organization behavior, which is usually faced\nthrough the existence of communities. The discovery of the communities in a\nWeb-graph can be used to improve the effectiveness of search engines, for\npurposes of prefetching, bibliographic citation ranking, spam detection,\ncreation of road-maps and site graphs, etc. Correspondingly, a citation graph\nis also a social network which consists of communities. The identification of\ncommunities in citation graphs can enhance the bibliography search as well as\nthe data-mining. In this paper we will present a fast algorithm which can\nidentify the communities over a given unweighted/undirected graph. This graph\nmay represent a Web-graph or a citation graph.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2012 18:48:32 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Sidiropoulos", "Antonis", ""]]}, {"id": "1208.5556", "submitter": "Alireza Nemaney Pour", "authors": "Alireza Nemaney Pour, Raheleh Kholghi, Soheil Behnam Roudsari", "title": "Minimizing the Time of Spam Mail Detection by Relocating Filtering\n  System to the Sender Mail Server", "comments": "10 pages, 7 figures", "journal-ref": "International Journal of Network Security & Its Applications\n  (IJNSA), Vol.4, No.2, March 2012", "doi": "10.5121/ijnsa.2012.4204", "report-no": null, "categories": "cs.IR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsolicited Bulk Emails (also known as Spam) are undesirable emails sent to\nmassive number of users. Spam emails consume the network resources and cause\nlots of security uncertainties. As we studied, the location where the spam\nfilter operates in is an important parameter to preserve network resources.\nAlthough there are many different methods to block spam emails, most of program\ndevelopers only intend to block spam emails from being delivered to their\nclients. In this paper, we will introduce a new and efficient approach to\nprevent spam emails from being transferred. The result shows that if we focus\non developing a filtering method for spams emails in the sender mail server\nrather than the receiver mail server, we can detect the spam emails in the\nshortest time consequently to avoid wasting network resources.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2012 04:33:29 GMT"}], "update_date": "2012-08-29", "authors_parsed": [["Pour", "Alireza Nemaney", ""], ["Kholghi", "Raheleh", ""], ["Roudsari", "Soheil Behnam", ""]]}, {"id": "1208.5654", "submitter": "Chris De Vries", "authors": "Christopher M. De Vries, Shlomo Geva, Andrew Trotman", "title": "Document Clustering Evaluation: Divergence from a Random Baseline", "comments": "8 pages, 11 figures, WIR2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divergence from a random baseline is a technique for the evaluation of\ndocument clustering. It ensures cluster quality measures are performing work\nthat prevents ineffective clusterings from giving high scores to clusterings\nthat provide no useful result. These concepts are defined and analysed using\nintrinsic and extrinsic approaches to the evaluation of document cluster\nquality. This includes the classical clusters to categories approach and a\nnovel approach that uses ad hoc information retrieval. The divergence from a\nrandom baseline approach is able to differentiate ineffective clusterings\nencountered in the INEX XML Mining track. It also appears to perform a\nnormalisation similar to the Normalised Mutual Information (NMI) measure but it\ncan be applied to any measure of cluster quality. When it is applied to the\nintrinsic measure of distortion as measured by RMSE, subtraction from a random\nbaseline provides a clear optimum that is not apparent otherwise. This approach\ncan be applied to any clustering evaluation. This paper describes its use in\nthe context of document clustering evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2012 13:23:29 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2012 09:04:41 GMT"}], "update_date": "2012-08-30", "authors_parsed": [["De Vries", "Christopher M.", ""], ["Geva", "Shlomo", ""], ["Trotman", "Andrew", ""]]}, {"id": "1208.6268", "submitter": "Tanmoy Chakraborty", "authors": "Tanmoy Chakraborty", "title": "Authorship Identification in Bengali Literature: a Comparative Analysis", "comments": "9 pages, 5 tables, 4 pictures", "journal-ref": "Chakraborty, T., Authorship Identification in Bengali Literature:\n  a Comparative Analysis, Proceedings of COLING 2012: Demonstration Papers,\n  December, 2012, pp. 41-50", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stylometry is the study of the unique linguistic styles and writing behaviors\nof individuals. It belongs to the core task of text categorization like\nauthorship identification, plagiarism detection etc. Though reasonable number\nof studies have been conducted in English language, no major work has been done\nso far in Bengali. In this work, We will present a demonstration of authorship\nidentification of the documents written in Bengali. We adopt a set of\nfine-grained stylistic features for the analysis of the text and use them to\ndevelop two different models: statistical similarity model consisting of three\nmeasures and their combination, and machine learning model with Decision Tree,\nNeural Network and SVM. Experimental results show that SVM outperforms other\nstate-of-the-art methods after 10-fold cross validations. We also validate the\nrelative importance of each stylistic feature to show that some of them remain\nconsistently significant in every model used in this experiment.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2012 19:09:24 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2012 19:34:18 GMT"}, {"version": "v3", "created": "Thu, 6 Sep 2012 17:08:33 GMT"}, {"version": "v4", "created": "Sun, 24 Feb 2013 09:27:56 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Chakraborty", "Tanmoy", ""]]}, {"id": "1208.6335", "submitter": "Aman Chadha Mr.", "authors": "Aman Chadha, Sushmit Mallik and Ravdeep Johar", "title": "Comparative Study and Optimization of Feature-Extraction Techniques for\n  Content based Image Retrieval", "comments": "8 pages, 16 figures, 11 tables", "journal-ref": "International Journal of Computer Applications 52(20):35-42, 2012", "doi": "10.5120/8320-1959", "report-no": "Volume 52, Number 20, 2012", "categories": "cs.CV cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of a Content-Based Image Retrieval (CBIR) system, also known as Query\nby Image Content (QBIC), is to help users to retrieve relevant images based on\ntheir contents. CBIR technologies provide a method to find images in large\ndatabases by using unique descriptors from a trained image. The image\ndescriptors include texture, color, intensity and shape of the object inside an\nimage. Several feature-extraction techniques viz., Average RGB, Color Moments,\nCo-occurrence, Local Color Histogram, Global Color Histogram and Geometric\nMoment have been critically compared in this paper. However, individually these\ntechniques result in poor performance. So, combinations of these techniques\nhave also been evaluated and results for the most efficient combination of\ntechniques have been presented and optimized for each class of image query. We\nalso propose an improvement in image retrieval performance by introducing the\nidea of Query modification through image cropping. It enables the user to\nidentify a region of interest and modify the initial query to refine and\npersonalize the image retrieval results.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2012 23:50:06 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 01:34:05 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Chadha", "Aman", ""], ["Mallik", "Sushmit", ""], ["Johar", "Ravdeep", ""]]}]