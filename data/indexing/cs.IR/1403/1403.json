[{"id": "1403.0068", "submitter": "Nithya C", "authors": "C. Nithya and K. Saravanan", "title": "Semantic Annotation and Search for Educational Resources Supporting\n  Distance Learning", "comments": "Linked Data, Semantic search, Cloud Applications, Web services,\n  Semantic annotation, Ontology", "journal-ref": "IJETT V8(6),277-285 February 2014. ISSN:2231-5381", "doi": "10.14445/22315381/IJETT-V8P252", "report-no": null, "categories": "cs.IR cs.CY cs.DL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Multimedia educational resources play an important role in education,\nparticularly for distance learning environments. With the rapid growth of the\nmultimedia web, large numbers of education articles video resources are\nincreasingly being created by several different organizations. It is crucial to\nexplore, share, reuse, and link these educational resources for better\ne-learning experiences. Most of the video resources are currently annotated in\nan isolated way, which means that they lack semantic connections. Thus,\nproviding the facilities for annotating these video resources is highly\ndemanded. These facilities create the semantic connections among video\nresources and allow their metadata to be understood globally. Adopting Linked\nData technology, this paper introduces a video annotation and browser platform\nwith two online tools: Notitia and Sansu-Wolke. Notitia enables users to\nsemantically annotate video resources using vocabularies defined in the Linked\nData cloud. Sansu-Wolke allows users to browse semantically linked educational\nvideo resources with enhanced web information from different online resources.\nIn the prototype development, the platform uses existing video resources for\neducation articles. The result of the initial development demonstrates the\nbenefits of applying Linked Data technology in the aspects of reusability,\nscalability, and extensibility\n", "versions": [{"version": "v1", "created": "Sat, 1 Mar 2014 09:26:35 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Nithya", "C.", ""], ["Saravanan", "K.", ""]]}, {"id": "1403.0353", "submitter": "Xuzhen Zhu", "authors": "Xuzhen Zhu, Hui Tian, Haifeng Liu, Shimin Cai", "title": "Personalized recommendation against crowd's popular selection", "comments": "This paper has been withdrawn by the author due to a crucial idea\n  repeatation with \"Information filtering via preferential diffusion\" published\n  in Physical Review E", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of personalized recommendation in an ocean of data attracts more\nand more attention recently. Most traditional researches ignore the popularity\nof the recommended object, which resulting in low personality and accuracy. In\nthis Letter, we proposed a personalized recommendation method based on weighted\nobject network, punishing the recommended object that is the crowd's popular\nselection, namely, Anti-popularity index(AP), which can give enhanced\npersonality, accuracy and diversity in contrast to mainstream baselines with a\nlow computational complexity.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 09:45:22 GMT"}, {"version": "v2", "created": "Thu, 6 Mar 2014 03:14:08 GMT"}, {"version": "v3", "created": "Sat, 10 May 2014 06:57:04 GMT"}, {"version": "v4", "created": "Tue, 13 May 2014 06:32:32 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Zhu", "Xuzhen", ""], ["Tian", "Hui", ""], ["Liu", "Haifeng", ""], ["Cai", "Shimin", ""]]}, {"id": "1403.0761", "submitter": "Kieran Greer Dr", "authors": "Kieran Greer", "title": "The Obvious Solution to Semantic Mapping -- Ask an Expert", "comments": "White paper / pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantic mapping problem is probably the main obstacle to\ncomputer-to-computer communication. If computer A knows that its concept X is\nthe same as computer B's concept Y, then the two machines can communicate. They\nwill in effect be talking the same language. This paper describes a relatively\nstraightforward way of enhancing the semantic descriptions of Web Service\ninterfaces by using online sources of keyword definitions. Method interface\ndescriptions can be enhanced using these standard dictionary definitions.\nBecause the generated metadata is now standardised, this means that any other\ncomputer that has access to the same source, or understands standard language\nconcepts, can now understand the description. This helps to remove a lot of the\nheterogeneity that would otherwise build up though humans creating their own\ndescriptions independently of each other. The description comes in the form of\nan XML script that can be retrieved and read through the Web Service interface\nitself. An additional use for these scripts would be for adding descriptions in\ndifferent languages, which would mean that human users that speak a different\nlanguage would also understand what the service was about.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 12:29:40 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Greer", "Kieran", ""]]}, {"id": "1403.1194", "submitter": "Minoru Sasaki", "authors": "Minoru Sasaki", "title": "Latent Semantic Word Sense Disambiguation Using Global Co-occurrence\n  Information", "comments": "6 pages, 2figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I propose a novel word sense disambiguation method based on\nthe global co-occurrence information using NMF. When I calculate the dependency\nrelation matrix, the existing method tends to produce very sparse co-occurrence\nmatrix from a small training set. Therefore, the NMF algorithm sometimes does\nnot converge to desired solutions. To obtain a large number of co-occurrence\nrelations, I propose to use co-occurrence frequencies of dependency relations\nbetween word features in the whole training set. This enables us to solve data\nsparseness problem and induce more effective latent features. To evaluate the\nefficiency of the method of word sense disambiguation, I make some experiments\nto compare with the result of the two baseline methods. The results of the\nexperiments show this method is effective for word sense disambiguation in\ncomparison with the all baseline methods. Moreover, the proposed method is\neffective for obtaining a stable effect by analyzing the global co-occurrence\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 17:20:01 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Sasaki", "Minoru", ""]]}, {"id": "1403.1310", "submitter": "Roshan Ragel", "authors": "M.A.C. Jiffriya, M.A.C. Akmal Jahan, R.G. Ragel and S. Deegalla", "title": "AntiPlag: Plagiarism Detection on Electronic Submissions of Text Based\n  Assignments", "comments": null, "journal-ref": "Industrial and Information Systems (ICIIS), 2013 8th IEEE\n  International Conference on, pp. 376 - 380, 17-20 Dec. 2013", "doi": "10.1109/ICIInfS.2013.6732013", "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plagiarism is one of the growing issues in academia and is always a concern\nin Universities and other academic institutions. The situation is becoming even\nworse with the availability of ample resources on the web. This paper focuses\non creating an effective and fast tool for plagiarism detection for text based\nelectronic assignments. Our plagiarism detection tool named AntiPlag is\ndeveloped using the tri-gram sequence matching technique. Three sets of text\nbased assignments were tested by AntiPlag and the results were compared against\nan existing commercial plagiarism detection tool. AntiPlag showed better\nresults in terms of false positives compared to the commercial tool due to the\npre-processing steps performed in AntiPlag. In addition, to improve the\ndetection latency, AntiPlag applies a data clustering technique making it four\ntimes faster than the commercial tool considered. AntiPlag could be used to\nisolate plagiarized text based assignments from non-plagiarised assignments\neasily. Therefore, we present AntiPlag, a fast and effective tool for\nplagiarism detection on text based electronic assignments.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 01:16:01 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Jiffriya", "M. A. C.", ""], ["Jahan", "M. A. C. Akmal", ""], ["Ragel", "R. G.", ""], ["Deegalla", "S.", ""]]}, {"id": "1403.1314", "submitter": "Roshan Ragel", "authors": "R. G. Ragel, P. Herath and U. Senanayake", "title": "Authorship detection of SMS messages using unigrams", "comments": null, "journal-ref": "Industrial and Information Systems (ICIIS), 2013 8th IEEE\n  International Conference on, pp. 387-392 , 17-20 Dec. 2013", "doi": "10.1109/ICIInfS.2013.6732015", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SMS messaging is a popular media of communication. Because of its popularity\nand privacy, it could be used for many illegal purposes. Additionally, since\nthey are part of the day to day life, SMSes can be used as evidence for many\nlegal disputes. Since a cellular phone might be accessible to people close to\nthe owner, it is important to establish the fact that the sender of the message\nis indeed the owner of the phone. For this purpose, the straight forward\nsolutions seem to be the use of popular stylometric methods. However, in\ncomparison with the data used for stylometry in the literature, SMSes have\nunusual characteristics making it hard or impossible to apply these methods in\na conventional way. Our target is to come up with a method of authorship\ndetection of SMS messages that could still give a usable accuracy. We argue\nthat, considering the methods of author attribution, the best method that could\nbe applied to SMS messages is an n-gram method. To prove our point, we checked\ntwo different methods of distribution comparison with varying number of\ntraining and testing data. We specifically try to compare how well our\nalgorithms work under less amount of testing data and large number of candidate\nauthors (which we believe to be the real world scenario) against controlled\ntests with less number of authors and selected SMSes with large number of\nwords. To counter the lack of information in an SMS message, we propose the\nmethod of stacking together few SMSes.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 01:33:55 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Ragel", "R. G.", ""], ["Herath", "P.", ""], ["Senanayake", "U.", ""]]}, {"id": "1403.1349", "submitter": "Sam Anzaroot", "authors": "Sam Anzaroot, Alexandre Passos, David Belanger, Andrew McCallum", "title": "Learning Soft Linear Constraints with Application to Citation Field\n  Extraction", "comments": "appears in Proc. the 52nd Annual Meeting of the Association for\n  Computational Linguistics (ACL2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately segmenting a citation string into fields for authors, titles, etc.\nis a challenging task because the output typically obeys various global\nconstraints. Previous work has shown that modeling soft constraints, where the\nmodel is encouraged, but not require to obey the constraints, can substantially\nimprove segmentation performance. On the other hand, for imposing hard\nconstraints, dual decomposition is a popular technique for efficient prediction\ngiven existing algorithms for unconstrained inference. We extend the technique\nto perform prediction subject to soft constraints. Moreover, with a technique\nfor performing inference given soft constraints, it is easy to automatically\ngenerate large families of constraints and learn their costs with a simple\nconvex optimization problem during training. This allows us to obtain\nsubstantial gains in accuracy on a new, challenging citation extraction\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 05:24:02 GMT"}, {"version": "v2", "created": "Fri, 17 Oct 2014 13:27:02 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Anzaroot", "Sam", ""], ["Passos", "Alexandre", ""], ["Belanger", "David", ""], ["McCallum", "Andrew", ""]]}, {"id": "1403.1451", "submitter": "Damiano Spina", "authors": "Arkaitz Zubiaga, Damiano Spina, Raquel Mart\\'inez, V\\'ictor Fresno", "title": "Real-Time Classification of Twitter Trends", "comments": "Pre-print of article accepted for publication in Journal of the\n  American Society for Information Science and Technology copyright @ 2013\n  (American Society for Information Science and Technology)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media users give rise to social trends as they share about common\ninterests, which can be triggered by different reasons. In this work, we\nexplore the types of triggers that spark trends on Twitter, introducing a\ntypology with following four types: 'news', 'ongoing events', 'memes', and\n'commemoratives'. While previous research has analyzed trending topics in a\nlong term, we look at the earliest tweets that produce a trend, with the aim of\ncategorizing trends early on. This would allow to provide a filtered subset of\ntrends to end users. We analyze and experiment with a set of straightforward\nlanguage-independent features based on the social spread of trends to\ncategorize them into the introduced typology. Our method provides an efficient\nway to accurately categorize trending topics without need of external data,\nenabling news organizations to discover breaking news in real-time, or to\nquickly identify viral memes that might enrich marketing decisions, among\nothers. The analysis of social features also reveals patterns associated with\neach type of trend, such as tweets about ongoing events being shorter as many\nwere likely sent from mobile devices, or memes having more retweets originating\nfrom a few trend-setters.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 14:23:53 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Zubiaga", "Arkaitz", ""], ["Spina", "Damiano", ""], ["Mart\u00ednez", "Raquel", ""], ["Fresno", "V\u00edctor", ""]]}, {"id": "1403.1486", "submitter": "Giannis Haralabopoulos", "authors": "Giannis Haralabopoulos, Ioannis Anagnostopoulos", "title": "Lifespan and propagation of information in On-line Social Networks a\n  Case Study", "comments": "32 Pages, 13 Figures, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since 1950, information flows have been in the centre of scientific research.\nUp until internet penetration in the late 90s, these studies were based over\ntraditional offline social networks. Several observations in offline\ninformation flows studies, such as two-step flow of communication and the\nimportance of weak ties, were verified in several online studies, showing that\nthe diffused information flows from one Online Social Network (OSN) to several\nothers. Within that flow, information is shared to and reproduced by the users\nof each network. Furthermore, the original content is enhanced or weakened\naccording to its topic, the dynamic and exposure of each OSNs. In such a\nconcept, each OSN is considered a layer of information flows that interacts\nwith each other. In this paper, we examine such flows in several social\nnetworks, as well as their diffusion and lifespan across multiple OSNs, in\nterms of user-generated content. Our results verify the perception of content\nand information connection in various OSNs.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 16:43:16 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Haralabopoulos", "Giannis", ""], ["Anagnostopoulos", "Ioannis", ""]]}, {"id": "1403.1939", "submitter": "Sandeep Sirsat", "authors": "Sandeep Sirsat", "title": "Extraction of Core Contents from Web Pages", "comments": "6 Pages, 3 Figures, 11 references. arXiv admin note: text overlap\n  with arXiv:1207.0246 by other authors without attribution", "journal-ref": "Sandeep Sirsat. \"Extraction of Core Contents from Web Pages\",\n  International Journal of Engineering Trends and Technology(IJETT),\n  V8(9),484-489 February 2014. ISSN:2231-5381. www.ijettjournal.org. published\n  by seventh sense research group", "doi": "10.14445/22315381/IJETT-V8P285", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The information available on web pages mostly contains semi-structured text\ndocuments which are represented either in XML, or HTML, or XHTML format that\nlacks formatted document structure. The document does not discriminate between\nthe text and the schema that represent the text. Also the amount of structure\nused to represent the text depends on the purpose and size of text document. No\nsemantic is applied to semi-structured documents. This requires extracting core\ncontents of text document to analyse words or sentences to generate useful\nknowledge. This paper discusses several techniques and approaches useful for\nextracting core content from semi-structured text documents and their merits\nand demerits\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 06:49:03 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Sirsat", "Sandeep", ""]]}, {"id": "1403.2002", "submitter": "Sadi Seker E", "authors": "Sadi Evren Seker, Cihan Mert, Khaled Al-Naami, Nuri Ozalp, Ugur Ayan", "title": "Time Series Analysis on Stock Market for Text Mining Correlation of\n  Economy News", "comments": "23 pages", "journal-ref": "International Journal of Social Sciences and Humanity Studies Vol\n  6, No 1, 2014 ISSN: 1309-8063 (Online)", "doi": null, "report-no": null, "categories": "cs.CE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an information retrieval method for the economy news. The\neffect of economy news, are researched in the word level and stock market\nvalues are considered as the ground proof. The correlation between stock market\nprices and economy news is an already addressed problem for most of the\ncountries. The most well-known approach is applying the text mining approaches\nto the news and some time series analysis techniques over stock market closing\nvalues in order to apply classification or clustering algorithms over the\nfeatures extracted. This study goes further and tries to ask the question what\nare the available time series analysis techniques for the stock market closing\nvalues and which one is the most suitable? In this study, the news and their\ndates are collected into a database and text mining is applied over the news,\nthe text mining part has been kept simple with only term frequency-inverse\ndocument frequency method. For the time series analysis part, we have studied\n10 different methods such as random walk, moving average, acceleration,\nBollinger band, price rate of change, periodic average, difference, momentum or\nrelative strength index and their variation. In this study we have also\nexplained these techniques in a comparative way and we have applied the methods\nover Turkish Stock Market closing values for more than a 2 year period. On the\nother hand, we have applied the term frequency-inverse document frequency\nmethod on the economy news of one of the high-circulating newspapers in Turkey.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 19:50:15 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Seker", "Sadi Evren", ""], ["Mert", "Cihan", ""], ["Al-Naami", "Khaled", ""], ["Ozalp", "Nuri", ""], ["Ayan", "Ugur", ""]]}, {"id": "1403.2003", "submitter": "Sadi Seker E", "authors": "M. Lutfi Arslan, Sadi Evren Seker", "title": "The Impact of Employment Web Sites' Traffic on Unemployment: A Cross\n  Country Comparison", "comments": "9 pages", "journal-ref": "International Journal of Social Sciences and Humanity Studies Vol\n  5, No 2, 2013 ISSN: 1309-8063 (Online)", "doi": null, "report-no": null, "categories": "stat.AP cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although employment web sites have recently become the main source for re-\ncruitment and selection process, the relation between those sites and unemploy-\nment rates is seldom addressed. Deriving data from 32 countries and 427 web\nsites, this study explores the correlation between unemployment rates of\nEuropean countries and the attractiveness of country specific employment web\nsites. It also compares the changes in unemployment rates and traffic on all\nthe aforementioned web sites. The results showed that there is a strong\ncorrelation between web sites traffic and unemployment rates.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 19:51:19 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Arslan", "M. Lutfi", ""], ["Seker", "Sadi Evren", ""]]}, {"id": "1403.2194", "submitter": "Yannis Haralambous", "authors": "Yannis Haralambous and Pedro Quaresma", "title": "Querying Geometric Figures Using a Controlled Language, Ontological\n  Graphs and Dependency Lattices", "comments": "14 pages, 5 figures, accepted at CICM 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.AI cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic geometry systems (DGS) have become basic tools in many areas of\ngeometry as, for example, in education. Geometry Automated Theorem Provers\n(GATP) are an active area of research and are considered as being basic tools\nin future enhanced educational software as well as in a next generation of\nmechanized mathematics assistants. Recently emerged Web repositories of\ngeometric knowledge, like TGTP and Intergeo, are an attempt to make the already\nvast data set of geometric knowledge widely available. Considering the large\namount of geometric information already available, we face the need of a query\nmechanism for descriptions of geometric constructions.\n  In this paper we discuss two approaches for describing geometric figures\n(declarative and procedural), and present algorithms for querying geometric\nfigures in declaratively and procedurally described corpora, by using a DGS or\na dedicated controlled natural language for queries.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 09:39:12 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 07:06:02 GMT"}, {"version": "v3", "created": "Tue, 13 May 2014 23:44:33 GMT"}], "update_date": "2014-05-15", "authors_parsed": [["Haralambous", "Yannis", ""], ["Quaresma", "Pedro", ""]]}, {"id": "1403.2871", "submitter": "Senosy Arrish", "authors": "Senosy Arrish, Fadhil Noer Afif, Ahmadu Maidorawa and Naomie Salim", "title": "Shape-Based Plagiarism Detection for Flowchart Figures in Texts", "comments": "12 pages", "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 6, No 1, February 2014", "doi": "10.5121/ijcsit.2014.6108", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plagiarism detection is well known phenomenon in the academic arena. Copying\nother people is considered as serious offence that needs to be checked. There\nare many plagiarism detection systems such as turn-it-in that has been\ndeveloped to provide this checks. Most, if not all, discard the figures and\ncharts before checking for plagiarism. Discarding the figures and charts\nresults in look holes that people can take advantage. That means people can\nplagiarized figures and charts easily without the current plagiarism systems\ndetecting it. There are very few papers which talks about flowcharts plagiarism\ndetection. Therefore, there is a need to develop a system that will detect\nplagiarism in figures and charts. This paper presents a method for detecting\nflow chart figure plagiarism based on shape-based image processing and\nmultimedia retrieval. The method managed to retrieve flowcharts with ranked\nsimilarity according to different matching sets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 10:21:25 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Arrish", "Senosy", ""], ["Afif", "Fadhil Noer", ""], ["Maidorawa", "Ahmadu", ""], ["Salim", "Naomie", ""]]}, {"id": "1403.2923", "submitter": "Igor Brigadir", "authors": "Igor Brigadir, Derek Greene, P\\'adraig Cunningham", "title": "Adaptive Representations for Tracking Breaking News on Twitter", "comments": "8 Page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter is often the most up-to-date source for finding and tracking breaking\nnews stories. Therefore, there is considerable interest in developing filters\nfor tweet streams in order to track and summarize stories. This is a\nnon-trivial text analytics task as tweets are short, and standard retrieval\nmethods often fail as stories evolve over time. In this paper we examine the\neffectiveness of adaptive mechanisms for tracking and summarizing breaking news\nstories. We evaluate the effectiveness of these mechanisms on a number of\nrecent news events for which manually curated timelines are available.\nAssessments based on ROUGE metrics indicate that an adaptive approaches are\nbest suited for tracking evolving stories on Twitter.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 13:22:07 GMT"}, {"version": "v2", "created": "Fri, 20 Jun 2014 19:38:31 GMT"}, {"version": "v3", "created": "Fri, 28 Nov 2014 17:21:34 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Brigadir", "Igor", ""], ["Greene", "Derek", ""], ["Cunningham", "P\u00e1draig", ""]]}, {"id": "1403.3185", "submitter": "Md. Ansarul Haque", "authors": "Md. Ansarul Haque", "title": "Sentiment Analysis by Using Fuzzy Logic", "comments": "16 pages.\n  http://airccse.org/journal/ijcseit/papers/4114ijcseit04.pdf, February 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How could a product or service is reasonably evaluated by anyone in the\nshortest time? A million dollar question but it is having a simple answer:\nSentiment analysis. Sentiment analysis is consumers review on products and\nservices which helps both the producers and consumers (stakeholders) to take\neffective and efficient decision within a shortest period of time. Producers\ncan have better knowledge of their products and services through the sentiment\nanalysis (ex. positive and negative comments or consumers likes and dislikes)\nwhich will help them to know their products status (ex. product limitations or\nmarket status). Consumers can have better knowledge of their interested\nproducts and services through the sentiment analysis (ex. positive and negative\ncomments or consumers likes and dislikes) which will help them to know their\ndeserving products status (ex. product limitations or market status). For more\nspecification of the sentiment values, fuzzy logic could be introduced.\nTherefore, sentiment analysis with the help of fuzzy logic (deals with\nreasoning and gives closer views to the exact sentiment values) will help the\nproducers or consumers or any interested person for taking the effective\ndecision according to their product or service interest.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 07:48:15 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Haque", "Md. Ansarul", ""]]}, {"id": "1403.3460", "submitter": "Chi Wang", "authors": "Chi Wang, Xueqing Liu, Yanglei Song, Jiawei Han", "title": "Scalable and Robust Construction of Topical Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated generation of high-quality topical hierarchies for a text\ncollection is a dream problem in knowledge engineering with many valuable\napplications. In this paper a scalable and robust algorithm is proposed for\nconstructing a hierarchy of topics from a text collection. We divide and\nconquer the problem using a top-down recursive framework, based on a tensor\northogonal decomposition technique. We solve a critical challenge to perform\nscalable inference for our newly designed hierarchical topic model. Experiments\nwith various real-world datasets illustrate its ability to generate robust,\nhigh-quality hierarchies efficiently. Our method reduces the time of\nconstruction by several orders of magnitude, and its robust feature renders it\npossible for users to interactively revise the hierarchy.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 23:22:21 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Wang", "Chi", ""], ["Liu", "Xueqing", ""], ["Song", "Yanglei", ""], ["Han", "Jiawei", ""]]}, {"id": "1403.3515", "submitter": "Kieran Greer Dr", "authors": "Kieran Greer", "title": "Concept Trees: Building Dynamic Concepts from Semi-Structured Data using\n  Nature-Inspired Methods", "comments": "Pre-print", "journal-ref": "Q. Zhu, A.T Azar (eds.), Complex system modelling and control\n  through intelligent soft computations, Studies in Fuzziness and Soft\n  Computing, Springer-Verlag, Germany, Vol. 319, pp. 221 - 252, 2014", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for creating structure from heterogeneous\nsources, as part of an information database, or more specifically, a 'concept\nbase'. Structures called 'concept trees' can grow from the semi-structured\nsources when consistent sequences of concepts are presented. They might be\nconsidered to be dynamic databases, possibly a variation on the distributed\nAgent-Based or Cellular Automata models, or even related to Markov models.\nSemantic comparison of text is required, but the trees can be built more, from\nautomatic knowledge and statistical feedback. This reduced model might also be\nattractive for security or privacy reasons, as not all of the potential data\ngets saved. The construction process maintains the key requirement of\ngenerality, allowing it to be used as part of a generic framework. The nature\nof the method also means that some level of optimisation or normalisation of\nthe information will occur. This gives comparisons with databases or\nknowledge-bases, but a database system would firstly model its environment or\ndatasets and then populate the database with instance values. The concept base\ndeals with a more uncertain environment and therefore cannot fully model it\nbeforehand. The model itself therefore evolves over time. Similar to databases,\nit also needs a good indexing system, where the construction process provides\nmemory and indexing structures. These allow for more complex concepts to be\nautomatically created, stored and retrieved, possibly as part of a more\ncognitive model. There are also some arguments, or more abstract ideas, for\nmerging physical-world laws into these automatic processes.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 09:38:01 GMT"}, {"version": "v2", "created": "Mon, 23 Jun 2014 17:07:10 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Greer", "Kieran", ""]]}, {"id": "1403.4289", "submitter": "Thomas Steiner", "authors": "Thomas Steiner", "title": "Telling Breaking News Stories from Wikipedia with Social Multimedia: A\n  Case Study of the 2014 Winter Olympics", "comments": "Proceedings of the 1st International Workshop on Social Multimedia\n  and Storytelling (SoMuS), co-located with the 4th International Conference on\n  Multimedia Retrieval (ICMR '14), Glasgow, Scotland, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ability to watch Wikipedia and Wikidata edits in realtime, the\nonline encyclopedia and the knowledge base have become increasingly used\ntargets of research for the detection of breaking news events. In this paper,\nwe present a case study of the 2014 Winter Olympics, where we tell the story of\nbreaking news events in the context of the Olympics with the help of social\nmultimedia stemming from multiple social network sites. Therefore, we have\nextended the application Wikipedia Live Monitor-a tool for the detection of\nbreaking news events-with the capability of automatically creating media\ngalleries that illustrate events. Athletes winning an Olympic competition, a\nnew country leading the medal table, or simply the Olympics themselves are all\nevents newsworthy enough for people to concurrently edit Wikipedia and\nWikidata-around the world in many languages. The Olympics being an event of\ncommon interest, an even bigger majority of people share the event in a\nmultitude of languages on global social network sites, which makes the event an\nideal subject of study. With this work, we connect the world of Wikipedia and\nWikidata with the world of social network sites, in order to convey the spirit\nof the 2014 Winter Olympics, to tell the story of victory and defeat, and\nalways following the Olympic motto Citius, Altius, Fortius. The proposed\nsystem-generalized for all sort of breaking news stories-has been put in\nproduction in form of the Twitter bot @mediagalleries, available and archived\nat https://twitter.com/mediagalleries.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 22:21:24 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Steiner", "Thomas", ""]]}, {"id": "1403.4362", "submitter": "Abderrahim Mohammed El Amine", "authors": "Mohammed El Amine Abderrahim", "title": "Concept Based vs. Pseudo Relevance Feedback Performance Evaluation for\n  Information Retrieval System", "comments": "arXiv admin note: substantial text overlap with arXiv:1306.3955 by\n  other authors", "journal-ref": "International Journal of Computational Linguistics Research, ISSN:\n  0976-416X, Volume 4, Issue 4, December, 2013, Pages 149-158", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article evaluates the performance of two techniques for query\nreformulation in a system for information retrieval, namely, the concept based\nand the pseudo relevance feedback reformulation. The experiments performed on a\ncorpus of Arabic text have allowed us to compare the contribution of these two\nreformulation techniques in improving the performance of an information\nretrieval system for Arabic texts.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 07:18:39 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Abderrahim", "Mohammed El Amine", ""]]}, {"id": "1403.5006", "submitter": "Ning Yan", "authors": "Ning Yan, Sona Hasani, Abolfazl Asudeh, Chengkai Li", "title": "Generating Preview Tables for Entity Graphs", "comments": "This is the camera-ready version of a SIGMOD16 paper. There might be\n  tiny differences in layout, spacing and linebreaking, compared with the\n  version in the SIGMOD16 proceedings, since we must submit TeX files and use\n  arXiv to compile the files", "journal-ref": null, "doi": "10.1145/2882903.2915221", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Users are tapping into massive, heterogeneous entity graphs for many\napplications. It is challenging to select entity graphs for a particular need,\ngiven abundant datasets from many sources and the oftentimes scarce information\nfor them. We propose methods to produce preview tables for compact presentation\nof important entity types and relationships in entity graphs. The preview\ntables assist users in attaining a quick and rough preview of the data. They\ncan be shown in a limited display space for a user to browse and explore,\nbefore she decides to spend time and resources to fetch and investigate the\ncomplete dataset. We formulate several optimization problems that look for\npreviews with the highest scores according to intuitive goodness measures,\nunder various constraints on preview size and distance between preview tables.\nThe optimization problem under distance constraint is NP-hard. We design a\ndynamic-programming algorithm and an Apriori-style algorithm for finding\noptimal previews. Results from experiments, comparison with related work and\nuser studies demonstrated the scoring measures' accuracy and the discovery\nalgorithms' efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 00:21:37 GMT"}, {"version": "v2", "created": "Wed, 4 May 2016 04:40:31 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Yan", "Ning", ""], ["Hasani", "Sona", ""], ["Asudeh", "Abolfazl", ""], ["Li", "Chengkai", ""]]}, {"id": "1403.5596", "submitter": "Tarek El-Shishtawy Ahmed", "authors": "Tarek El-Shishtawy and Fatma El-Ghannam", "title": "A Lemma Based Evaluator for Semitic Language Text Summarization Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching texts in highly inflected languages such as Arabic by simple\nstemming strategy is unlikely to perform well. In this paper, we present a\nstrategy for automatic text matching technique for for inflectional languages,\nusing Arabic as the test case. The system is an extension of ROUGE test in\nwhich texts are matched on token's lemma level. The experimental results show\nan enhancement of detecting similarities between different sentences having\nsame semantics but written in different lexical forms..\n", "versions": [{"version": "v1", "created": "Sat, 22 Mar 2014 00:13:03 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["El-Shishtawy", "Tarek", ""], ["El-Ghannam", "Fatma", ""]]}, {"id": "1403.5771", "submitter": "Sanjay Singh", "authors": "Rahul Gupta, Gitansh Khirbat and Sanjay Singh", "title": "A Novel Method to Calculate Click Through Rate for Sponsored Search", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": "MU-MIT-ICT-2014-001", "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sponsored search adopts generalized second price (GSP) auction mechanism\nwhich works on the concept of pay per click which is most commonly used for the\nallocation of slots in the searched page. Two main aspects associated with GSP\nare the bidding amount and the click through rate (CTR). The CTR learning\nalgorithms currently being used works on the basic principle of (#clicks_i/\n#impressions_i) under a fixed window of clicks or impressions or time. CTR are\nprone to fraudulent clicks, resulting in sudden increase of CTR. The current\nalgorithms are unable to find the solutions to stop this, although with the use\nof machine learning algorithms it can be detected that fraudulent clicks are\nbeing generated. In our paper, we have used the concept of relative ranking\nwhich works on the basic principle of (#clicks_i /#clicks_t). In this\nalgorithm, both the numerator and the denominator are linked. As #clicks_t is\nhigher than previous algorithms and is linked to the #clicks_i, the small\nchange in the clicks which occurs in the normal scenario have a very small\nchange in the result but in case of fraudulent clicks the number of clicks\nincreases or decreases rapidly which will add up with the normal clicks to\nincrease the denominator, thereby decreasing the CTR.\n", "versions": [{"version": "v1", "created": "Sun, 23 Mar 2014 16:35:29 GMT"}, {"version": "v2", "created": "Tue, 25 Mar 2014 13:34:58 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Gupta", "Rahul", ""], ["Khirbat", "Gitansh", ""], ["Singh", "Sanjay", ""]]}, {"id": "1403.6248", "submitter": "Qifeng Qiao", "authors": "Qifeng Qiao and Peter A. Beling", "title": "Classroom Video Assessment and Retrieval via Multiple Instance Learning", "comments": null, "journal-ref": "The 14th International Conference on Artificial Intelligence in\n  Education 2011", "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multiple instance learning approach to content-based retrieval\nof classroom video for the purpose of supporting human assessing the learning\nenvironment. The key element of our approach is a mapping between the semantic\nconcepts of the assessment system and features of the video that can be\nmeasured using techniques from the fields of computer vision and speech\nanalysis. We report on a formative experiment in content-based video retrieval\ninvolving trained experts in the Classroom Assessment Scoring System, a widely\nused framework for assessment and improvement of learning environments. The\nresults of this experiment suggest that our approach has potential application\nto productivity enhancement in assessment and to broader retrieval tasks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 07:11:03 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Qiao", "Qifeng", ""], ["Beling", "Peter A.", ""]]}, {"id": "1403.6397", "submitter": "Michael R\\\"oder", "authors": "Frank Rosner, Alexander Hinneburg, Michael R\\\"oder, Martin Nettling,\n  Andreas Both", "title": "Evaluating topic coherence measures", "comments": "This work has been presented at the \"Topic Models: Computation,\n  Application and Evaluation\" workshop at the \"Neural Information Processing\n  Systems\" conference 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Topic models extract representative word sets - called topics - from word\ncounts in documents without requiring any semantic annotations. Topics are not\nguaranteed to be well interpretable, therefore, coherence measures have been\nproposed to distinguish between good and bad topics. Studies of topic coherence\nso far are limited to measures that score pairs of individual words. For the\nfirst time, we include coherence measures from scientific philosophy that score\npairs of more complex word subsets and apply them to topic scoring.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 15:44:14 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Rosner", "Frank", ""], ["Hinneburg", "Alexander", ""], ["R\u00f6der", "Michael", ""], ["Nettling", "Martin", ""], ["Both", "Andreas", ""]]}, {"id": "1403.6794", "submitter": "David Olivieri", "authors": "Iv\\'an G\\'omez-Conde and David N. Olivieri", "title": "KPCA Spatio-temporal trajectory point cloud classifier for recognizing\n  human actions in a CBVR system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a content based video retrieval (CBVR) software system for\nidentifying specific locations of a human action within a full length film, and\nretrieving similar video shots from a query. For this, we introduce the concept\nof a trajectory point cloud for classifying unique actions, encoded in a\nspatio-temporal covariant eigenspace, where each point is characterized by its\nspatial location, local Frenet-Serret vector basis, time averaged curvature and\ntorsion and the mean osculating hyperplane. Since each action can be\ndistinguished by their unique trajectories within this space, the trajectory\npoint cloud is used to define an adaptive distance metric for classifying\nqueries against stored actions. Depending upon the distance to other\ntrajectories, the distance metric uses either large scale structure of the\ntrajectory point cloud, such as the mean distance between cloud centroids or\nthe difference in hyperplane orientation, or small structure such as the time\naveraged curvature and torsion, to classify individual points in a fuzzy-KNN.\nOur system can function in real-time and has an accuracy greater than 93% for\nmultiple action recognition within video repositories. We demonstrate the use\nof our CBVR system in two situations: by locating specific frame positions of\ntrained actions in two full featured films, and video shot retrieval from a\ndatabase with a web search application.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 19:10:53 GMT"}], "update_date": "2014-03-27", "authors_parsed": [["G\u00f3mez-Conde", "Iv\u00e1n", ""], ["Olivieri", "David N.", ""]]}, {"id": "1403.7162", "submitter": "Vishal Jain", "authors": "Gagandeep Singh, Vishal Jain", "title": "Information Retrieval (IR) through Semantic Web (SW): An Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large amount of data is present on the web. It contains huge number of web\npages and to find suitable information from them is very cumbersome task. There\nis need to organize data in formal manner so that user can easily access and\nuse them. To retrieve information from documents, we have many Information\nRetrieval (IR) techniques. Current IR techniques are not so advanced that they\ncan be able to exploit semantic knowledge within documents and give precise\nresults. IR technology is major factor responsible for handling annotations in\nSemantic Web (SW) languages and in the present paper knowledgeable\nrepresentation languages used for retrieving information are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 18:36:15 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Singh", "Gagandeep", ""], ["Jain", "Vishal", ""]]}, {"id": "1403.7315", "submitter": "Chuan Shi", "authors": "Yitong Li, Chuan Shi, Philip S. Yu, and Qing Chen", "title": "HRank: A Path based Ranking Framework in Heterogeneous Information\n  Network", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there is a surge of interests on heterogeneous information network\nanalysis. As a newly emerging network model, heterogeneous information networks\nhave many unique features (e.g., complex structure and rich semantics) and a\nnumber of interesting data mining tasks have been exploited in this kind of\nnetworks, such as similarity measure, clustering, and classification. Although\nevaluating the importance of objects has been well studied in homogeneous\nnetworks, it is not yet exploited in heterogeneous networks. In this paper, we\nstudy the ranking problem in heterogeneous networks and propose the HRank\nframework to evaluate the importance of multiple types of objects and meta\npaths. Since the importance of objects depends upon the meta paths in\nheterogeneous networks, HRank develops a path based random walk process.\nMoreover, a constrained meta path is proposed to subtly capture the rich\nsemantics in heterogeneous networks. Furthermore, HRank can simultaneously\ndetermine the importance of objects and meta paths through applying the tensor\nanalysis. Extensive experiments on three real datasets show that HRank can\neffectively evaluate the importance of objects and paths together. Moreover,\nthe constrained meta path shows its potential on mining subtle semantics by\nobtaining more accurate ranking results.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 09:31:43 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Li", "Yitong", ""], ["Shi", "Chuan", ""], ["Yu", "Philip S.", ""], ["Chen", "Qing", ""]]}, {"id": "1403.7335", "submitter": "Duyu Tang", "authors": "Duyu Tang, Bing Qin, Ting Liu, Qiuhui Shi", "title": "Emotion Analysis Platform on Chinese Microblog", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Weibo, as the largest social media service in China, has billions of messages\ngenerated every day. The huge number of messages contain rich sentimental\ninformation. In order to analyze the emotional changes in accordance with time\nand space, this paper presents an Emotion Analysis Platform (EAP), which\nexplores the emotional distribution of each province, so that can monitor the\nglobal pulse of each province in China. The massive data of Weibo and the\nreal-time requirements make the building of EAP challenging. In order to solve\nthe above problems, emoticons, emotion lexicon and emotion-shifting rules are\nadopted in EAP to analyze the emotion of each tweet. In order to verify the\neffectiveness of the platform, case study on the Sichuan earthquake is done,\nand the analysis result of the platform accords with the fact. In order to\nanalyze from quantity, we manually annotate a test set and conduct experiment\non it. The experimental results show that the macro-Precision of EAP reaches\n80% and the EAP works effectively.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 10:45:31 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Tang", "Duyu", ""], ["Qin", "Bing", ""], ["Liu", "Ting", ""], ["Shi", "Qiuhui", ""]]}, {"id": "1403.7591", "submitter": "Yin Cui", "authors": "Yin Cui, Dong Liu, Jiawei Chen, Shih-Fu Chang", "title": "Building A Large Concept Bank for Representing Events in Video", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept-based video representation has proven to be effective in complex\nevent detection. However, existing methods either manually design concepts or\ndirectly adopt concept libraries not specifically designed for events. In this\npaper, we propose to build Concept Bank, the largest concept library consisting\nof 4,876 concepts specifically designed to cover 631 real-world events. To\nconstruct the Concept Bank, we first gather a comprehensive event collection\nfrom WikiHow, a collaborative writing project that aims to build the world's\nlargest manual for any possible How-To event. For each event, we then search\nFlickr and discover relevant concepts from the tags of the returned images. We\ntrain a Multiple Kernel Linear SVM for each discovered concept as a concept\ndetector in Concept Bank. We organize the concepts into a five-layer tree\nstructure, in which the higher-level nodes correspond to the event categories\nwhile the leaf nodes are the event-specific concepts discovered for each event.\nBased on such tree ontology, we develop a semantic matching method to select\nrelevant concepts for each textual event query, and then apply the\ncorresponding concept detectors to generate concept-based video\nrepresentations. We use TRECVID Multimedia Event Detection 2013 and Columbia\nConsumer Video open source event definitions and videos as our test sets and\nshow very promising results on two video event detection tasks: event modeling\nover concept space and zero-shot event retrieval. To the best of our knowledge,\nthis is the largest concept library covering the largest number of real-world\nevents.\n", "versions": [{"version": "v1", "created": "Sat, 29 Mar 2014 05:17:29 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Cui", "Yin", ""], ["Liu", "Dong", ""], ["Chen", "Jiawei", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1403.7766", "submitter": "Tejal Shah", "authors": "Tejal Shah, Fethi Rabhi, Pradeep Ray and Kerry Taylor", "title": "Enhancing Automated Decision Support across Medical and Oral Health\n  Domains with Semantic Web Technologies", "comments": "The paper has been published at the 24th Australasian Conference on\n  Information Systems, 4-6 Dec 2013, Melbourne. The paper can be found at:\n  http://mo.bf.rmit.edu.au/acis2013/382.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research has shown that the general health and oral health of an individual\nare closely related. Accordingly, current practice of isolating the information\nbase of medical and oral health domains can be dangerous and detrimental to the\nhealth of the individual. However, technical issues such as heterogeneous data\ncollection and storage formats, limited sharing of patient information and lack\nof decision support over the shared information are the principal reasons for\nthe current state of affairs. To address these issues, the following research\ninvestigates the development and application of a cross-domain ontology and\nrules to build an evidence-based and reusable knowledge base consisting of the\ninter-dependent conditions from the two domains. Through example implementation\nof the knowledge base in Protege, we demonstrate the effectiveness of our\napproach in reasoning over and providing decision support for cross-domain\npatient information.\n", "versions": [{"version": "v1", "created": "Sun, 30 Mar 2014 14:20:22 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Shah", "Tejal", ""], ["Rabhi", "Fethi", ""], ["Ray", "Pradeep", ""], ["Taylor", "Kerry", ""]]}, {"id": "1403.7899", "submitter": "Wilko van Hoek", "authors": "Wilko van Hoek, Wei Shen and Philipp Mayr", "title": "Identifying User Behavior in domain-specific Repositories", "comments": "Elpub Conference 2014, 10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an analysis of the user behavior of two different\ndomain-specific repositories. The web analytic tool etracker was used to gain a\nfirst overall insight into the user behavior of these repositories. Moreover,\nwe extended our work to describe an apache web log analysis approach which\nfocuses on the identification of the user behavior. Therefore the user traffic\nwithin our systems is visualized using chord diagrams. We could find that\nrecommendations are used frequently and users do rarely combine searching with\nfaceting or filtering.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 07:51:43 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["van Hoek", "Wilko", ""], ["Shen", "Wei", ""], ["Mayr", "Philipp", ""]]}, {"id": "1403.7923", "submitter": "Anders Friberg", "authors": "Anders Friberg, Erwin Schoonderwaldt, Anton Hedblad, Marco Fabiani and\n  Anders Elowsson", "title": "Using perceptually defined music features in music information retrieval", "comments": "submitted to the Journal of the Acoustical Society of America January\n  9, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, the notion of perceptual features is introduced for describing\ngeneral music properties based on human perception. This is an attempt at\nrethinking the concept of features, in order to understand the underlying human\nperception mechanisms. Instead of using concepts from music theory such as\ntones, pitches, and chords, a set of nine features describing overall\nproperties of the music was selected. They were chosen from qualitative\nmeasures used in psychology studies and motivated from an ecological approach.\nThe selected perceptual features were rated in two listening experiments using\ntwo different data sets. They were modeled both from symbolic (MIDI) and audio\ndata using different sets of computational features. Ratings of emotional\nexpression were predicted using the perceptual features. The results indicate\nthat (1) at least some of the perceptual features are reliable estimates; (2)\nemotion ratings could be predicted by a small combination of perceptual\nfeatures with an explained variance up to 90%; (3) the perceptual features\ncould only to a limited extent be modeled using existing audio features. The\nresults also clearly indicated that a small number of dedicated features were\nsuperior to a 'brute force' model using a large number of general audio\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 09:19:54 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Friberg", "Anders", ""], ["Schoonderwaldt", "Erwin", ""], ["Hedblad", "Anton", ""], ["Fabiani", "Marco", ""], ["Elowsson", "Anders", ""]]}]