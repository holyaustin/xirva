[{"id": "1305.0194", "submitter": "Chantal Cherifi", "authors": "Cihan Aksoy, Vincent Labatut, Chantal Cherifi, Jean-Fran\\c{c}ois\n  Santucci", "title": "MATAWS: A Multimodal Approach for Automatic WS Semantic Annotation", "comments": null, "journal-ref": "In International Conference on Networked Digital Technologies,\n  Springer CCIS 136),China (2011)", "doi": "10.1007/978-3-642-22185-9_27", "report-no": null, "categories": "cs.SE cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent works aim at developing methods and tools for the processing of\nsemantic Web services. In order to be properly tested, these tools must be\napplied to an appropriate benchmark, taking the form of a collection of\nsemantic WS descriptions. However, all of the existing publicly available\ncollections are limited by their size or their realism (use of randomly\ngenerated or resampled descriptions). Larger and realistic syntactic (WSDL)\ncollections exist, but their semantic annotation requires a certain level of\nautomation, due to the number of operations to be processed. In this article,\nwe propose a fully automatic method to semantically annotate such large WS\ncollections. Our approach is multimodal, in the sense it takes advantage of the\nlatent semantics present not only in the parameter names, but also in the type\nnames and structures. Concept-to-word association is performed by using Sigma,\na mapping of WordNet to the SUMO ontology. After having described in details\nour annotation method, we apply it to the larger collection of real-world\nsyntactic WS descriptions we could find, and assess its efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 15:07:18 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Aksoy", "Cihan", ""], ["Labatut", "Vincent", ""], ["Cherifi", "Chantal", ""], ["Santucci", "Jean-Fran\u00e7ois", ""]]}, {"id": "1305.0196", "submitter": "Chantal Cherifi", "authors": "Chantal Cherifi, Vincent Labatut, Jean-Fran\\c{c}ois Santucci", "title": "Topological Properties of Web Services Similarity Networks", "comments": null, "journal-ref": "In Strategic Advantage of Computing Information Systems in\n  Enterprise Management, ATINER, pp. 105-117, (2010)", "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of publicly available Web services (WS) is continuously growing.\nTo perform efficient WS discovery, it is desirable to organize the WS space.\nWorks in this direction propose to group WS according to certain shared\nproperties. Such groups commonly called communities are based either on\nsimilarity or on interaction between WS. In this paper we focus on the former,\nand propose a new network-based approach to extract communities from a WS\ncollection. This process is three-stepped: first we define several similarity\nfunctions able to compare WS operations, second we use them to build so-called\nsimilarity networks, and third we identify communities under the form of\nspecific structures in these networks. We apply our method on a collection of\nreal-world WS and comment the resulting communities. Finally, we additionally\nprovide an analysis and an interpretation of our similarity networks with a\ncomplex networks perspective.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 15:16:04 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Cherifi", "Chantal", ""], ["Labatut", "Vincent", ""], ["Santucci", "Jean-Fran\u00e7ois", ""]]}, {"id": "1305.0261", "submitter": "Chantal Cherifi", "authors": "Chantal Cherifi1, Vincent Labatut, Jean-Fran\\c{c}ois Santucci", "title": "Web Services Dependency Networks Analysis", "comments": "arXiv admin note: substantial text overlap with arXiv:1305.0191", "journal-ref": "International Conference of New Media and Interactivity (NMI), pp.\n  115-120 (2010)", "doi": null, "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with a continuously growing number of publicly available Web services\n(WS), we are witnessing a rapid development in semantic-related web\ntechnologies, which lead to the apparition of semantically described WS. In\nthis work, we perform a comparative analysis of the syntactic and semantic\napproaches used to describe WS, from a complex network perspective. First, we\nextract syntactic and semantic WS dependency networks from a collection of\npublicly available WS descriptions. Then, we take advantage of tools from the\ncomplex network field to analyze them and determine their topological\nproperties. We show WS dependency networks exhibit some of the typical\ncharacteristics observed in real-world networks, such as small world and scale\nfree properties, as well as community structure. By comparing syntactic and\nsemantic networks through their topological properties, we show the\nintroduction of semantics in WS description allows modeling more accurately the\ndependencies between parameters, which in turn could lead to improved\ncomposition mining methods.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 15:18:20 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Cherifi1", "Chantal", ""], ["Labatut", "Vincent", ""], ["Santucci", "Jean-Fran\u00e7ois", ""]]}, {"id": "1305.0357", "submitter": "Philipp Mayr", "authors": "Philipp Mayr", "title": "Relevance distributions across Bradford Zones: Can Bradfordizing improve\n  search?", "comments": "11 pages, 2 figures, Preprint of a full paper @ 14th International\n  Society of Scientometrics and Informetrics Conference (ISSI 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to describe the evaluation of the effectiveness\nof the bibliometric technique Bradfordizing in an information retrieval (IR)\nscenario. Bradfordizing is used to re-rank topical document sets from\nconventional abstracting & indexing (A&I) databases into core and more\nperipheral document zones. Bradfordized lists of journal articles and\nmonographs will be tested in a controlled scenario consisting of different A&I\ndatabases from social and political sciences, economics, psychology and medical\nscience, 164 standardized IR topics and intellectual assessments of the listed\ndocuments. Does Bradfordizing improve the ratio of relevant documents in the\nfirst third (core) compared to the second and last third (zone 2 and zone 3,\nrespectively)? The IR tests show that relevance distributions after re-ranking\nimprove at a significant level if documents in the core are compared with\ndocuments in the succeeding zones. After Bradfordizing of document pools, the\ncore has a significant better average precision than zone 2, zone 3 and\nbaseline. This paper should be seen as an argument in favour of alternative\nnon-textual (bibliometric) re-ranking methods which can be simply applied in\ntext-based retrieval systems and in particular in A&I databases.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 07:33:28 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Mayr", "Philipp", ""]]}, {"id": "1305.0540", "submitter": "Shang Shang", "authors": "Shang Shang and Yuk Hui and Pan Hui and Paul Cuff and Sanjeev Kulkarni", "title": "Privacy Preserving Recommendation System Based on Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Recommendation systems have received considerable attention in the recent\ndecades. Yet with the development of information technology and social media,\nthe risk in revealing private data to service providers has been a growing\nconcern to more and more users. Trade-offs between quality and privacy in\nrecommendation systems naturally arise. In this paper, we present a privacy\npreserving recommendation framework based on groups. The main idea is to use\ngroups as a natural middleware to preserve users' privacy. A distributed\npreference exchange algorithm is proposed to ensure the anonymity of data,\nwherein the effective size of the anonymity set asymptotically approaches the\ngroup size with time. We construct a hybrid collaborative filtering model based\non Markov random walks to provide recommendations and predictions to group\nmembers. Experimental results on the MovieLens and Epinions datasets show that\nour proposed methods outperform the baseline methods, L+ and ItemRank, two\nstate-of-the-art personalized recommendation algorithms, for both\nrecommendation precision and hit rate despite the absence of personal\npreference information.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 19:17:08 GMT"}, {"version": "v2", "created": "Mon, 13 May 2013 19:50:41 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Shang", "Shang", ""], ["Hui", "Yuk", ""], ["Hui", "Pan", ""], ["Cuff", "Paul", ""], ["Kulkarni", "Sanjeev", ""]]}, {"id": "1305.0638", "submitter": "Deqing Wang", "authors": "Deqing Wang, Hui Zhang, Rui Liu, Weifeng Lv", "title": "Feature Selection Based on Term Frequency and T-Test for Text\n  Categorization", "comments": "5pages 9 figures CIKM2012 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Much work has been done on feature selection. Existing methods are based on\ndocument frequency, such as Chi-Square Statistic, Information Gain etc.\nHowever, these methods have two shortcomings: one is that they are not reliable\nfor low-frequency terms, and the other is that they only count whether one term\noccurs in a document and ignore the term frequency. Actually, high-frequency\nterms within a specific category are often regards as discriminators.\n  This paper focuses on how to construct the feature selection function based\non term frequency, and proposes a new approach based on $t$-test, which is used\nto measure the diversity of the distributions of a term between the specific\ncategory and the entire corpus. Extensive comparative experiments on two text\ncorpora using three classifiers show that our new approach is comparable to or\nor slightly better than the state-of-the-art feature selection methods (i.e.,\n$\\chi^2$, and IG) in terms of macro-$F_1$ and micro-$F_1$.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 08:26:05 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Wang", "Deqing", ""], ["Zhang", "Hui", ""], ["Liu", "Rui", ""], ["Lv", "Weifeng", ""]]}, {"id": "1305.0688", "submitter": "Chantal Cherifi", "authors": "Chantal Cherifi, Vincent Labatut, Jean-Fran\\c{c}ois Santucci", "title": "On Flexible Web Services Composition Networks", "comments": null, "journal-ref": "In: International Conference on Digital Information and\n  Communication Technology and its Applications (DICTAP), Springer CCIS, Dijon,\n  France (2011)", "doi": "10.1007/978-3-642-21984-9_5", "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantic Web service community develops efforts to bring semantics to Web\nservice descriptions and allow automatic discovery and composition. However,\nthere is no widespread adoption of such descriptions yet, because semantically\ndefining Web services is highly complicated and costly. As a result, production\nWeb services still rely on syntactic descriptions, key-word based discovery and\npredefined compositions. Hence, more advanced research on syntactic Web\nservices is still ongoing. In this work we build syntactic composition Web\nservices networks with three well known similarity metrics, namely Levenshtein,\nJaro and Jaro-Winkler. We perform a comparative study on the metrics\nperformance by studying the topological properties of networks built from a\ntest collection of real-world descriptions. It appears Jaro-Winkler finds more\nappropriate similarities and can be used at higher thresholds. For lower\nthresholds, the Jaro metric would be preferable because it detect less\nirrelevant relationships.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 12:40:17 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Cherifi", "Chantal", ""], ["Labatut", "Vincent", ""], ["Santucci", "Jean-Fran\u00e7ois", ""]]}, {"id": "1305.0699", "submitter": "Jimmy Lin", "authors": "Nima Asadi and Jimmy Lin", "title": "Fast, Incremental Inverted Indexing in Main Memory for Web-Scale\n  Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For text retrieval systems, the assumption that all data structures reside in\nmain memory is increasingly common. In this context, we present a novel\nincremental inverted indexing algorithm for web-scale collections that directly\nconstructs compressed postings lists in memory. Designing efficient in-memory\nalgorithms requires understanding modern processor architectures and memory\nhierarchies: in this paper, we explore the issue of postings lists contiguity.\nNaturally, postings lists that occupy contiguous memory regions are preferred\nfor retrieval, but maintaining contiguity increases complexity and slows\nindexing. On the other hand, allowing discontiguous index segments simplifies\nindex construction but decreases retrieval performance. Understanding this\ntradeoff is our main contribution: We find that co-locating small groups of\ninverted list segments yields query evaluation performance that is\nstatistically indistinguishable from fully-contiguous postings lists. In other\nwords, it is not necessary to lay out in-memory data structures such that all\npostings for a term are contiguous; we can achieve ideal performance with a\nrelatively small amount of effort.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 13:28:02 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Asadi", "Nima", ""], ["Lin", "Jimmy", ""]]}, {"id": "1305.0939", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Debajyoti Mukhopadhyay, Manoj Sharma, Gajanan Joshi, Trupti Pagare,\n  Adarsha Palwe", "title": "Intelligent Agent Based Semantic Web in Cloud Computing Environment", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering today's web scenario, there is a need of effective and meaningful\nsearch over the web which is provided by Semantic Web. Existing search engines\nare keyword based. They are vulnerable in answering intelligent queries from\nthe user due to the dependence of their results on information available in web\npages. While semantic search engines provides efficient and relevant results as\nthe semantic web is an extension of the current web in which information is\ngiven well defined meaning. MetaCrawler is a search tool that uses several\nexisting search engines and provides combined results by using their own page\nranking algorithm. This paper proposes development of a meta-semantic-search\nengine called SemanTelli which works within cloud. SemanTelli fetches results\nfrom different semantic search engines such as Hakia, DuckDuckGo, SenseBot with\nthe help of intelligent agents that eliminate the limitations of existing\nsearch engines.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2013 17:22:12 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Mukhopadhyay", "Debajyoti", ""], ["Sharma", "Manoj", ""], ["Joshi", "Gajanan", ""], ["Pagare", "Trupti", ""], ["Palwe", "Adarsha", ""]]}, {"id": "1305.1114", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Towards User Profile Modelling in Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of profile appeared in the 1970s decade, which was mainly due to\nthe need to create custom applications that could be adapted to the user. In\nthis paper, we treat the different aspects of the user's profile, defining it,\nprofile, its features and its indicators of interest, and then we describe the\ndifferent approaches of modelling and acquiring the user's interests.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 08:42:19 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1305.1343", "submitter": "Arnim Bleier", "authors": "Arnim Bleier and Andreas Strotmann", "title": "Towards an Author-Topic-Term-Model Visualization of 100 Years of German\n  Sociological Society Proceedings", "comments": "Accepted: 14th International Society of Scientometrics and\n  Informetrics Conference, Vienna Austria 15-19th July 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Author co-citation studies employ factor analysis to reduce high-dimensional\nco-citation matrices to low-dimensional and possibly interpretable factors, but\nthese studies do not use any information from the text bodies of publications.\nWe hypothesise that term frequencies may yield useful information for\nscientometric analysis. In our work we ask if word features in combination with\nBayesian analysis allow well-founded science mapping studies. This work goes\nback to the roots of Mosteller and Wallace's (1964) statistical text analysis\nusing word frequency features and a Bayesian inference approach, tough with\ndifferent goals. To answer our research question we (i) introduce a new data\nset on which the experiments are carried out, (ii) describe the Bayesian model\nemployed for inference and (iii) present first results of the analysis.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 22:24:20 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Bleier", "Arnim", ""], ["Strotmann", "Andreas", ""]]}, {"id": "1305.1371", "submitter": "Fan Min", "authors": "Fan Min and William Zhu", "title": "Granular association rules for multi-valued data", "comments": "Proceedings of The 2013 Canadian Conference on Electrical and\n  Computer Engineering (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granular association rule is a new approach to reveal patterns hide in\nmany-to-many relationships of relational databases. Different types of data\nsuch as nominal, numeric and multi-valued ones should be dealt with in the\nprocess of rule mining. In this paper, we study multi-valued data and develop\ntechniques to filter out strong however uninteresting rules. An example of such\nrule might be \"male students rate movies released in 1990s that are NOT\nthriller.\" This kind of rules, called negative granular association rules,\noften overwhelms positive ones which are more useful. To address this issue, we\nfilter out negative granules such as \"NOT thriller\" in the process of granule\ngeneration. In this way, only positive granular association rules are generated\nand strong ones are mined. Experimental results on the movielens data set\nindicate that most rules are negative, and our technique is effective to filter\nthem out.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 01:08:05 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Min", "Fan", ""], ["Zhu", "William", ""]]}, {"id": "1305.1372", "submitter": "Fan Min", "authors": "Fan Min and William Zhu", "title": "Cold-start recommendation through granular association rules", "comments": "Submitted to Joint Rough Sets 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are popular in e-commerce as they suggest items of\ninterest to users. Researchers have addressed the cold-start problem where\neither the user or the item is new. However, the situation with both new user\nand new item has seldom been considered. In this paper, we propose a cold-start\nrecommendation approach to this situation based on granular association rules.\nSpecifically, we provide a means for describing users and items through\ninformation granules, a means for generating association rules between users\nand items, and a means for recommending items to users using these rules.\nExperiments are undertaken on a publicly available dataset MovieLens. Results\nindicate that rule sets perform similarly on the training and the testing sets,\nand the appropriate setting of granule is essential to the application of\ngranular association rules.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 01:08:27 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Min", "Fan", ""], ["Zhu", "William", ""]]}, {"id": "1305.1429", "submitter": "Urmila Shrawankar Ms", "authors": "Urmila Shrawankar, Anjali Mahajan", "title": "Speech User Interface for Information Retrieval", "comments": "Pages: 06 Figures: 01; Conference Proceedings International\n  Conference on Digital Libraires (ICDL 06), 2006, TERI, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the rapid development of information technology, the amount of\ninformation generated at a given time far exceeds human's ability to organize,\nsearch, and manipulate without the help of automatic systems. Now a days so\nmany tools and techniques are available for storage and retrieval of\ninformation. User uses interface to interact with these techniques, mostly text\nuser interface (TUI) or graphical user interface (GUI). Here, I am trying to\nintroduce a new interface i.e. speech for information retrieval. The goal of\nthis project is to develop a speech interface that can search and read the\nrequired information from the database effectively, efficiently and more\nfriendly. This tool will be highly useful to blind people, they will able to\ndemand the information to the computer by giving voice command/s (keyword)\nthrough microphone and listen the required information using speaker or\nheadphones.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 07:28:58 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Shrawankar", "Urmila", ""], ["Mahajan", "Anjali", ""]]}, {"id": "1305.1745", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Mobile Recommender Systems Methods: An Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The information that mobiles can access becomes very wide nowadays, and the\nuser is faced with a dilemma: there is an unlimited pool of information\navailable to him but he is unable to find the exact information he is looking\nfor. This is why the current research aims to design Recommender Systems (RS)\nable to continually send information that matches the user's interests in order\nto reduce his navigation time. In this paper, we treat the different approaches\nto recommend.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 08:38:04 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1305.1787", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Evolution of the user's content: An Overview of the state of the art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of the user's content still remains a problem for an accurate\nrecommendation.This is why the current research aims to design Recommender\nSystems (RS) able to continually adapt information that matches the user's\ninterests. This paper aims to explain this problematic point in outlining the\nproposals that have been made in research with their advantages and\ndisadvantages.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 11:52:39 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1305.1899", "submitter": "Hong Xie", "authors": "Hong Xie, John C.S. Lui", "title": "Mathematical Modeling of Product Rating: Sufficiency, Misbehavior and\n  Aggregation Rules", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many web services like eBay, Tripadvisor, Epinions, etc, provide historical\nproduct ratings so that users can evaluate the quality of products. Product\nratings are important since they affect how well a product will be adopted by\nthe market. The challenge is that we only have {\\em \"partial information\"} on\nthese ratings: Each user provides ratings to only a \"{\\em small subset of\nproducts}\". Under this partial information setting, we explore a number of\nfundamental questions: What is the \"{\\em minimum number of ratings}\" a product\nneeds so one can make a reliable evaluation of its quality? How users' {\\em\nmisbehavior} (such as {\\em cheating}) in product rating may affect the\nevaluation result? To answer these questions, we present a formal mathematical\nmodel of product evaluation based on partial information. We derive theoretical\nbounds on the minimum number of ratings needed to produce a reliable indicator\nof a product's quality. We also extend our model to accommodate users'\nmisbehavior in product rating. We carry out experiments using both synthetic\nand real-world data (from TripAdvisor, Amazon and eBay) to validate our model,\nand also show that using the \"majority rating rule\" to aggregate product\nratings, it produces more reliable and robust product evaluation results than\nthe \"average rating rule\".\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 08:04:23 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Xie", "Hong", ""], ["Lui", "John C. S.", ""]]}, {"id": "1305.1946", "submitter": "Luca Mazzola", "authors": "Elena Camossi, Paola Villa, Luca Mazzola", "title": "Semantic-based Anomalous Pattern Discovery in Moving Object Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate a novel semantic approach for pattern discovery\nin trajectories that, relying on ontologies, enhances object movement\ninformation with event semantics. The approach can be applied to the detection\nof movement patterns and behaviors whenever the semantics of events occurring\nalong the trajectory is, explicitly or implicitly, available. In particular, we\ntested it against an exacting case scenario in maritime surveillance, i.e., the\ndiscovery of suspicious container transportations.\n  The methodology we have developed entails the formalization of the\napplication domain through a domain ontology, extending the Moving Object\nOntology (MOO) described in this paper. Afterwards, movement patterns have to\nbe formalized, either as Description Logic (DL) axioms or queries, enabling the\nretrieval of the trajectories that follow the patterns.\n  In our experimental evaluation, we have considered a real world dataset of 18\nMillion of container events describing the deed undertaken in a port to\naccomplish the shipping (e.g., loading on a vessel, export operation).\nLeveraging events, we have reconstructed almost 300 thousand container\ntrajectories referring to 50 thousand containers travelling along three years.\nWe have formalized the anomalous itinerary patterns as DL axioms, testing\ndifferent ontology APIs and DL reasoners to retrieve the suspicious\ntransportations.\n  Our experiments demonstrate that the approach is feasible and efficient. In\nparticular, the joint use of Pellet and SPARQL-DL enables to detect the\ntrajectories following a given pattern in a reasonable time with big size\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 20:14:03 GMT"}], "update_date": "2013-05-10", "authors_parsed": [["Camossi", "Elena", ""], ["Villa", "Paola", ""], ["Mazzola", "Luca", ""]]}, {"id": "1305.2686", "submitter": "Ali Tourani", "authors": "Ali Tourani and Amir Seyed Danesh", "title": "Using Exclusive Web Crawlers to Store Better Results in Search Engines'\n  Database", "comments": "8pages, 6figures", "journal-ref": "International Journal of Web & Semantic Technology Vol.4, No.2,\n  April 2013", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crawler-based search engines are the mostly used search engines among web and\nInternet users, involve web crawling, storing in database, ranking, indexing\nand displaying to the user. But it is noteworthy that because of increasing\nchanges in web sites search engines suffer high time and transfers costs which\nare consumed to investigate the existence of each page in database while\ncrawling, updating database and even investigating its existence in any\ncrawling operations. \"Exclusive Web Crawler\" proposes guidelines for crawling\nfeatures, links, media and other elements and to store crawling results in a\ncertain table in its database on the web. With doing this, search engines store\neach site's tables in their databases and implement their ranking results on\nthem. Thus, accuracy of data in every table (and its being up-to-date) is\nensured and no 404 result is shown in search results since, in fact, this data\ncrawler crawls data entered by webmaster and the database stores whatever he\nwants to display.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 07:06:30 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Tourani", "Ali", ""], ["Danesh", "Amir Seyed", ""]]}, {"id": "1305.2755", "submitter": "Issam Sahmoudi issam sahmoudi", "authors": "Issam Sahmoudi and Abdelmonaime Lachkar", "title": "Clustering Web Search Results For Effective Arabic Language Browsing", "comments": null, "journal-ref": "International Journal on Natural Language Computing (IJNLC) Vol.\n  2, No.2, April 2013", "doi": "10.5121/ijnlc.2013.2202", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of browsing Search Results is one of the major problems with\ntraditional Web search engines for English, European, and any other languages\ngenerally, and for Arabic Language particularly. This process is absolutely\ntime consuming and the browsing style seems to be unattractive. Organizing Web\nsearch results into clusters facilitates users quick browsing through search\nresults. Traditional clustering techniques (data-centric clustering algorithms)\nare inadequate since they don't generate clusters with highly readable names or\ncluster labels. To solve this problem, Description-centric algorithms such as\nSuffix Tree Clustering (STC) algorithm have been introduced and used\nsuccessfully and extensively with different adapted versions for English,\nEuropean, and Chinese Languages. However, till the day of writing this paper,\nin our knowledge, STC algorithm has been never applied for Arabic Web Snippets\nSearch Results Clustering.In this paper, we propose first, to study how STC can\nbe applied for Arabic Language? We then illustrate by example that is\nimpossible to apply STC after Arabic Snippets pre-processing (stem or root\nextraction) because the Merging process yields many redundant clusters.\nSecondly, to overcome this problem, we propose to integrate STC in a new scheme\ntaking into a count the Arabic language properties in order to get the web more\nand more adapted to Arabic users. The proposed approach automatically clusters\nthe web search results into high quality, and high significant clusters labels.\nThe obtained clusters not only are coherent, but also can convey the contents\nto the users concisely and accurately. Therefore the Arabic users can decide at\na glance whether the contents of a cluster are of interest.....\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 12:28:34 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Sahmoudi", "Issam", ""], ["Lachkar", "Abdelmonaime", ""]]}, {"id": "1305.2831", "submitter": "Urmila Shrawankar Ms", "authors": "Khushboo Thakkar and Urmila Shrawankar", "title": "Test Model for Text Categorization and Text Summarization", "comments": "Pages: 07 Figures : 07", "journal-ref": "International Journal on Computer Science and Engineering (IJCSE),\n  ISSN : 0975-3397, Vol. 3 No. 4 Apr 2011 pp 1539-1545", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text Categorization is the task of automatically sorting a set of documents\ninto categories from a predefined set and Text Summarization is a brief and\naccurate representation of input text such that the output covers the most\nimportant concepts of the source in a condensed manner. Document Summarization\nis an emerging technique for understanding the main purpose of any kind of\ndocuments. This paper presents a model that uses text categorization and text\nsummarization for searching a document based on user query.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 08:06:15 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Thakkar", "Khushboo", ""], ["Shrawankar", "Urmila", ""]]}, {"id": "1305.3384", "submitter": "Lior Rokach", "authors": "Naseem Biadsy, Lior Rokach, Armin Shmilovici", "title": "Transfer Learning for Content-Based Recommender Systems using Tree\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new approach to content-based transfer learning\nfor solving the data sparsity problem in cases when the users' preferences in\nthe target domain are either scarce or unavailable, but the necessary\ninformation on the preferences exists in another domain. We show that training\na system to use such information across domains can produce better performance.\nSpecifically, we represent users' behavior patterns based on topological graph\nstructures. Each behavior pattern represents the behavior of a set of users,\nwhen the users' behavior is defined as the items they rated and the items'\nrating values. In the next step we find a correlation between behavior patterns\nin the source domain and behavior patterns in the target domain. This mapping\nis considered a bridge between the two domains. Based on the correlation and\ncontent-attributes of the items, we train a machine learning model to predict\nusers' ratings in the target domain. When we compare our approach to the\npopularity approach and KNN-cross-domain on a real world dataset, the results\nshow that on an average of 83$%$ of the cases our approach outperforms both\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2013 08:00:54 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Biadsy", "Naseem", ""], ["Rokach", "Lior", ""], ["Shmilovici", "Armin", ""]]}, {"id": "1305.3814", "submitter": "Ali Hadian", "authors": "Ali Hadian, Behrouz Minaei-Bidgoli", "title": "Multi-View Learning for Web Spam Detection", "comments": "I want to upload a major revision in a couple of months", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spam pages are designed to maliciously appear among the top search results by\nexcessive usage of popular terms. Therefore, spam pages should be removed using\nan effective and efficient spam detection system. Previous methods for web spam\nclassification used several features from various information sources (page\ncontents, web graph, access logs, etc.) to detect web spam. In this paper, we\nfollow page-level classification approach to build fast and scalable spam\nfilters. We show that each web page can be classified with satisfiable accuracy\nusing only its own HTML content. In order to design a multi-view classification\nsystem, we used state-of-the-art spam classification methods with distinct\nfeature sets (views) as the base classifiers. Then, a fusion model is learned\nto combine the output of the base classifiers and make final prediction.\nResults show that multi-view learning significantly improves the classification\nperformance, namely AUC by 22%, while providing linear speedup for parallel\nexecution.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2013 14:11:02 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2013 05:21:16 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Hadian", "Ali", ""], ["Minaei-Bidgoli", "Behrouz", ""]]}, {"id": "1305.4054", "submitter": "Ahmad Assaf", "authors": "Ahmad Assaf, Aline Senart", "title": "Data Quality Principles in the Semantic Web", "comments": "ICSC '12 Proceedings of the 2012 IEEE Sixth International Conference\n  on Semantic Computing", "journal-ref": null, "doi": "10.1109/ICSC.2012.39", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing size and availability of web data make data quality a core\nchallenge in many applications. Principles of data quality are recognized as\nessential to ensure that data fit for their intended use in operations,\ndecision-making, and planning. However, with the rise of the Semantic Web, new\ndata quality issues appear and require deeper consideration. In this paper, we\npropose to extend the data quality principles to the context of Semantic Web.\nBased on our extensive industrial experience in data integration, we identify\nfive main classes suited for data quality in Semantic Web. For each class, we\nlist the principles that are involved at all stages of the data management\nprocess. Following these principles will provide a sound basis for better\ndecision-making within organizations and will maximize long-term data\nintegration and interoperability.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 12:00:42 GMT"}], "update_date": "2013-05-20", "authors_parsed": [["Assaf", "Ahmad", ""], ["Senart", "Aline", ""]]}, {"id": "1305.4077", "submitter": "Riadh Bouslimi", "authors": "Abir Messaoudi, Riadh Bouslimi, Jalel Akaichi", "title": "Indexing Medical Images based on Collaborative Experts Reports", "comments": "9 pages, 8 figures. International Journal of Computer Applications,\n  May 2013", "journal-ref": null, "doi": "10.5120/11955-7787", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A patient is often willing to quickly get, from his physician, reliable\nanalysis and concise explanation according to provided linked medical images.\nThe fact of making choices individually by the patient's physician may lead to\nmalpractices and consequently generates unforeseeable damages. The Institute of\nMedicine of the National Sciences Academy(IMNAS) in USA published a study\nestimating that up to 98,000 hospital deathseach year can be attributed to\nmedical malpractice [1]. Moreover, physician, in charge of medical image\nanalysis, might be unavailable at the right time, which may complicate the\npatient's state. The goal of this paper is to provide to physicians and\npatients, a social network that permits to foster cooperation and to overcome\nthe problem of unavailability of doctors on site any time. Therefore, patients\ncan submit their medical images to be diagnosed and commented by several\nexperts instantly. Consequently, the need to process opinions and to extract\ninformation automatically from the proposed social network became a necessity\ndue to the huge number of comments expressing specialist's reviews. For this\nreason, we propose a kind of comments' summary keywords-based method which\nextracts the major current terms and relevant words existing on physicians'\nannotations. The extracted keywords will present a new and robust method for\nimage indexation. In fact, significant extracted terms will be used later to\nindex images in order to facilitate their discovery for any appropriate use. To\novercome this challenge, we propose our Terminology Extraction of Annotation\n(TEA) mixed approach which focuses on algorithms mainly based on statistical\nmethods and on external semantic resources.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 13:43:57 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2013 18:01:35 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Messaoudi", "Abir", ""], ["Bouslimi", "Riadh", ""], ["Akaichi", "Jalel", ""]]}, {"id": "1305.4801", "submitter": "Fan Min", "authors": "Fan Min and William Zhu", "title": "Mining top-k granular association rules for recommendation", "comments": "12 pages, 5 figures, submitted to Advances in Granular Computing and\n  Advances in Rough Sets, 2013. arXiv admin note: substantial text overlap with\n  arXiv:1305.1372", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are important for e-commerce companies as well as\nresearchers. Recently, granular association rules have been proposed for\ncold-start recommendation. However, existing approaches reserve only globally\nstrong rules; therefore some users may receive no recommendation at all. In\nthis paper, we propose to mine the top-k granular association rules for each\nuser. First we define three measures of granular association rules. These are\nthe source coverage which measures the user granule size, the target coverage\nwhich measures the item granule size, and the confidence which measures the\nstrength of the association. With the confidence measure, rules can be ranked\naccording to their strength. Then we propose algorithms for training the\nrecommender and suggesting items to each user. Experimental are undertaken on a\npublicly available data set MovieLens. Results indicate that the appropriate\nsetting of granule can avoid over-fitting and at the same time, help obtaining\nhigh recommending accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 12:44:30 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Min", "Fan", ""], ["Zhu", "William", ""]]}, {"id": "1305.4820", "submitter": "Nader Jelassi", "authors": "Mohamed Nader Jelassi and Sadok Ben Yahia and Engelbert Mephu Nguifo", "title": "Nouvelle approche de recommandation personnalisee dans les folksonomies\n  basee sur le profil des utilisateurs", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In folksonomies, users use to share objects (movies, books, bookmarks, etc.)\nby annotating them with a set of tags of their own choice. With the rise of the\nWeb 2.0 age, users become the core of the system since they are both the\ncontributors and the creators of the information. Yet, each user has its own\nprofile and its own ideas making thereby the strength as well as the weakness\nof folksonomies. Indeed, it would be helpful to take account of users' profile\nwhen suggesting a list of tags and resources or even a list of friends, in\norder to make a personal recommandation, instead of suggesting the more used\ntags and resources in the folksonomy. In this paper, we consider users' profile\nas a new dimension of a folksonomy classically composed of three dimensions\n<users, tags, ressources> and we propose an approach to group users with\nequivalent profiles and equivalent interests as quadratic concepts. Then, we\nuse such structures to propose our personalized recommendation system of users,\ntags and resources according to each user's profile. Carried out experiments on\ntwo real-world datasets, i.e., MovieLens and BookCrossing highlight encouraging\nresults in terms of precision as well as a good social evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 13:59:51 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Jelassi", "Mohamed Nader", ""], ["Yahia", "Sadok Ben", ""], ["Nguifo", "Engelbert Mephu", ""]]}, {"id": "1305.5078", "submitter": "Miron Kursa", "authors": "Alicja A. Wieczorkowska, Miron B. Kursa", "title": "A Comparison of Random Forests and Ferns on Recognition of Instruments\n  in Jazz Recordings", "comments": null, "journal-ref": "Foundations of Intelligent Systems, Lecture Notes in Computer\n  Science Volume 7661, 2012, pp 208-217", "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first apply random ferns for classification of real music\nrecordings of a jazz band. No initial segmentation of audio data is assumed,\ni.e., no onset, offset, nor pitch data are needed. The notion of random ferns\nis described in the paper, to familiarize the reader with this classification\nalgorithm, which was introduced quite recently and applied so far in image\nrecognition tasks. The performance of random ferns is compared with random\nforests for the same data. The results of experiments are presented in the\npaper, and conclusions are drawn.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2013 10:43:25 GMT"}], "update_date": "2013-05-24", "authors_parsed": [["Wieczorkowska", "Alicja A.", ""], ["Kursa", "Miron B.", ""]]}, {"id": "1305.5330", "submitter": "Rom\\`an R. Zapatrin", "authors": "Roman Zapatrin", "title": "A toy model of information retrieval system based on quantum probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent numerical results show that non-Bayesian knowledge revision may be\nhelpful in search engine training and optimization. In order to demonstrate how\nbasic assumption about about the physical nature (and hence the observed\nstatistics) of retrieved documents can affect the performance of search engines\nwe suggest an idealized toy model with minimal number of parameters.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 06:49:20 GMT"}], "update_date": "2013-05-24", "authors_parsed": [["Zapatrin", "Roman", ""]]}, {"id": "1305.5724", "submitter": "Paul Libbrecht", "authors": "Paul Libbrecht", "title": "Escaping the Trap of too Precise Topic Queries", "comments": "12 pages, Conference on Intelligent Computer Mathematics 2013 Bath,\n  UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  At the very center of digital mathematics libraries lie controlled\nvocabularies which qualify the {\\it topic} of the documents. These topics are\nused when submitting a document to a digital mathematics library and to perform\nsearches in a library. The latter are refined by the use of these topics as\nthey allow a precise classification of the mathematics area this document\naddresses. However, there is a major risk that users employ too precise topics\nto specify their queries: they may be employing a topic that is only \"close-by\"\nbut missing to match the right resource. We call this the {\\it topic trap}.\nIndeed, since 2009, this issue has appeared frequently on the i2geo.net\nplatform. Other mathematics portals experience the same phenomenon. An approach\nto solve this issue is to introduce tolerance in the way queries are understood\nby the user. In particular, the approach of including fuzzy matches but this\nintroduces noise which may prevent the user of understanding the function of\nthe search engine.\n  In this paper, we propose a way to escape the topic trap by employing the\nnavigation between related topics and the count of search results for each\ntopic. This supports the user in that search for close-by topics is a click\naway from a previous search. This approach was realized with the i2geo search\nengine and is described in detail where the relation of being {\\it related} is\ncomputed by employing textual analysis of the definitions of the concepts\nfetched from the Wikipedia encyclopedia.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 13:32:02 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Libbrecht", "Paul", ""]]}, {"id": "1305.5827", "submitter": "Monica Shekhar", "authors": "Monica Shekhar and Saravanaguru RA. K", "title": "Semantic Web Search based on Ontology Modeling using Protege Reasoner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Semantic Web works on the existing Web which presents the meaning of\ninformation as well-defined vocabularies understood by the people. Semantic\nSearch, at the same time, works on improving the accuracy if a search by\nunderstanding the intent of the search and providing contextually relevant\nresults. This paper describes a semantic approach toward web search through a\nPHP application. The goal was to parse through a user's browsing history and\nreturn semantically relevant web pages for the search query provided.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 19:02:59 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Shekhar", "Monica", ""], ["K", "Saravanaguru RA.", ""]]}, {"id": "1305.5959", "submitter": "Ahmed AlSum", "authors": "Ahmed AlSum and Michael L. Nelson", "title": "ArcLink: Optimization Techniques to Build and Retrieve the Temporal Web\n  Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archiving the web is socially and culturally critical, but presents problems\nof scale. The Internet Archive's Wayback Machine can replay captured web pages\nas they existed at a certain point in time, but it has limited ability to\nprovide extensive content and structural metadata about the web graph. While\nthe live web has developed a rich ecosystem of APIs to facilitate web\napplications (e.g., APIs from Google and Twitter), the web archiving community\nhas not yet broadly implemented this level of access.\n  We present ArcLink, a proof-of-concept system that complements open source\nWayback Machine installations by optimizing the construction, storage, and\naccess to the temporal web graph. We divide the web graph construction into\nfour stages (filtering, extraction, storage, and access) and explore\noptimization for each stage. ArcLink extends the current Web archive interfaces\nto return content and structural metadata for each URI. We show how this API\ncan be applied to such applications as retrieving inlinks, outlinks,\nanchortext, and PageRank.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2013 19:18:05 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2013 14:18:44 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["AlSum", "Ahmed", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1305.5981", "submitter": "Daqiang Zhang", "authors": "Daqiang Zhang, Rongbo Zhu, Shuqiqiu Men, Vaskar Raychoudhury", "title": "Query Representation with Global Consistency on User Click Graph", "comments": "accepted by Journal of Internet Technology on Sep. 9, 2012. To appear\n  in Vol. 4, September, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensive research has been conducted on query log analysis. A query log is\ngenerally represented as a bipartite graph on a query set and a URL set. Most\nof the traditional methods used the raw click frequency to weigh the link\nbetween a query and a URL on the click graph. In order to address the\ndisadvantages of raw click frequency, researchers proposed the entropy-biased\nmodel, which incorporates raw click frequency with inverse query frequency of\nthe URL as the weighting scheme for query representation. In this paper, we\nobserve that the inverse query frequency can be considered a global property of\nthe URL on the click graph, which is more informative than raw click frequency,\nwhich can be considered a local property of the URL. Based on this insight, we\ndevelop the global consistency model for query representation, which utilizes\nthe click frequency and the inverse query frequency of a URL in a consistent\nmanner. Furthermore, we propose a new scheme called inverse URL frequency as an\neffective way to capture the global property of a URL. Experiments have been\nconducted on the AOL search engine log data. The result shows that our global\nconsistency model achieved better performance than the current models.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2013 01:30:25 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Zhang", "Daqiang", ""], ["Zhu", "Rongbo", ""], ["Men", "Shuqiqiu", ""], ["Raychoudhury", "Vaskar", ""]]}, {"id": "1305.6143", "submitter": "Vivek Narayanan", "authors": "Vivek Narayanan, Ishan Arora, Arjun Bhatia", "title": "Fast and accurate sentiment classification using an enhanced Naive Bayes\n  model", "comments": "8 pages, 2 figures", "journal-ref": "Intelligent Data Engineering and Automated Learning IDEAL 2013\n  Lecture Notes in Computer Science Volume 8206, 2013, pp 194-201", "doi": "10.1007/978-3-642-41278-3_24", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have explored different methods of improving the accuracy of a Naive Bayes\nclassifier for sentiment analysis. We observed that a combination of methods\nlike negation handling, word n-grams and feature selection by mutual\ninformation results in a significant improvement in accuracy. This implies that\na highly accurate and fast sentiment classifier can be built using a simple\nNaive Bayes model that has linear training and testing time complexities. We\nachieved an accuracy of 88.80% on the popular IMDB movie reviews dataset.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 08:37:26 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2013 05:36:29 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Narayanan", "Vivek", ""], ["Arora", "Ishan", ""], ["Bhatia", "Arjun", ""]]}, {"id": "1305.7014", "submitter": "Bohdan Pavlyshenko", "authors": "Bohdan Pavlyshenko", "title": "Tweets Miner for Stock Market Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a software package for the data mining of Twitter\nmicroblogs for the purpose of using them for the stock market analysis. The\npackage is written in R langauge using apropriate R packages. The model of\ntweets has been considered. We have also compared stock market charts with\nfrequent sets of keywords in Twitter microblogs messages.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 06:35:52 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Pavlyshenko", "Bohdan", ""]]}, {"id": "1305.7200", "submitter": "Philippe Martin", "authors": "Philippe A. Martin", "title": "Organizing Linked Data Quality Related Methods", "comments": "7 pages, 10 tables, conference", "journal-ref": "IKE (2012) 376-382", "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the top-level of an ontology categorizing and\ngeneralizing best practices and quality criteria or measures for Linked Data.\nIt permits to compare these techniques and have a synthetic organized view of\nwhat can or should be done for knowledge sharing purposes. This ontology is\npart of a general knowledge base that can be accessed and complemented by any\nWeb user. Thus, it can be seen as a cooperatively built library for the above\ncited elements. Since they permit to evaluate information objects and create\nbetter ones, these elements also permit knowledge-based tools and techniques -\nas well as knowledge providers - to be evaluated and categorized based on their\ninput/output information objects. One top-level distinction permitting to\norganize this ontology is the one between content, medium and containers of\ndescriptions. Various structural, ontological, syntactical and lexical\ndistinctions are then used.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 18:40:30 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Martin", "Philippe A.", ""]]}, {"id": "1305.7265", "submitter": "Ali Seyfi", "authors": "Ali Seyfi", "title": "A Focused Crawler Combinatory Link and Content Model Based on T-Graph\n  Principles", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": "10.1016/j.csi.2015.07.001", "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two significant tasks of a focused Web crawler are finding relevant\ntopic-specific documents on the Web and analytically prioritizing them for\nlater effective and reliable download. For the first task, we propose a\nsophisticated custom algorithm to fetch and analyze the most effective HTML\nstructural elements of the page as well as the topical boundary and anchor text\nof each unvisited link, based on which the topical focus of an unvisited page\ncan be predicted and elicited with a high accuracy. Thus, our novel method\nuniquely combines both link-based and content-based approaches. For the second\ntask, we propose a scoring function of the relevant URLs through the use of\nT-Graph (Treasure Graph) to assist in prioritizing the unvisited links that\nwill later be put into the fetching queue. Our Web search system is called the\nTreasure-Crawler. This research paper embodies the architectural design of the\nTreasure-Crawler system which satisfies the principle requirements of a focused\nWeb crawler, and asserts the correctness of the system structure including all\nits modules through illustrations and by the test results.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 22:38:48 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Seyfi", "Ali", ""]]}, {"id": "1305.7316", "submitter": "Minh-Quoc Nghiem", "authors": "Minh-Quoc Nghiem, Giovanni Yoko Kristianto, Goran Topic, Akiko Aizawa", "title": "A hybrid approach for semantic enrichment of MathML mathematical\n  expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, we present a new approach to the semantic enrichment of\nmathematical expression problem. Our approach is a combination of statistical\nmachine translation and disambiguation which makes use of surrounding text of\nthe mathematical expressions. We first use Support Vector Machine classifier to\ndisambiguate mathematical terms using both their presentation form and\nsurrounding text. We then use the disambiguation result to enhance the semantic\nenrichment of a statistical-machine-translation-based system. Experimental\nresults show that our system archives improvements over prior systems.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 07:34:07 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Nghiem", "Minh-Quoc", ""], ["Kristianto", "Giovanni Yoko", ""], ["Topic", "Goran", ""], ["Aizawa", "Akiko", ""]]}, {"id": "1305.7438", "submitter": "Tian Qiu", "authors": "Tian Qiu, Tian-Tian Wang, Zi-Ke Zhang, Li-Xin Zhong, Guang Chen", "title": "Heterogeneity Involved Network-based Algorithm Leads to Accurate and\n  Personalized Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneity of both the source and target objects is taken into account in\na network-based algorithm for the directional resource transformation between\nobjects. Based on a biased heat conduction recommendation method (BHC) which\nconsiders the heterogeneity of the target object, we propose a heterogeneous\nheat conduction algorithm (HHC), by further taking the source object degree as\nthe weight of diffusion. Tested on three real datasets, the Netflix, RYM and\nMovieLens, the HHC algorithm is found to present a better recommendation in\nboth the accuracy and personalization than two excellent algorithms, i.e., the\noriginal BHC and a hybrid algorithm of heat conduction and mass diffusion\n(HHM), while not requiring any other accessorial information or parameter.\nMoreover, the HHC even elevates the recommendation accuracy on cold objects,\nreferring to the so-called cold start problem, for effectively relieving the\nrecommendation bias on objects with different level of popularity.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 15:01:25 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Qiu", "Tian", ""], ["Wang", "Tian-Tian", ""], ["Zhang", "Zi-Ke", ""], ["Zhong", "Li-Xin", ""], ["Chen", "Guang", ""]]}]