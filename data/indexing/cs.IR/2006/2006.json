[{"id": "2006.00127", "submitter": "Areej Alokaili", "authors": "Areej Alokaili, Nikolaos Aletras, Mark Stevenson", "title": "Automatic Generation of Topic Labels", "comments": "Short paper accepted at SIGIR '20", "journal-ref": null, "doi": "10.1145/3397271.3401185", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modelling is a popular unsupervised method for identifying the\nunderlying themes in document collections that has many applications in\ninformation retrieval. A topic is usually represented by a list of terms ranked\nby their probability but, since these can be difficult to interpret, various\napproaches have been developed to assign descriptive labels to topics. Previous\nwork on the automatic assignment of labels to topics has relied on a two-stage\napproach: (1) candidate labels are retrieved from a large pool (e.g. Wikipedia\narticle titles); and then (2) re-ranked based on their semantic similarity to\nthe topic terms. However, these extractive approaches can only assign candidate\nlabels from a restricted set that may not include any suitable ones. This paper\nproposes using a sequence-to-sequence neural-based approach to generate labels\nthat does not suffer from this limitation. The model is trained over a new\nlarge synthetic dataset created using distant supervision. The method is\nevaluated by comparing the labels it generates to ones rated by humans.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 23:33:13 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Alokaili", "Areej", ""], ["Aletras", "Nikolaos", ""], ["Stevenson", "Mark", ""]]}, {"id": "2006.00166", "submitter": "Hamed Zamani", "authors": "Hamed Zamani, Bhaskar Mitra, Everest Chen, Gord Lueck, Fernando Diaz,\n  Paul N. Bennett, Nick Craswell, Susan T. Dumais", "title": "Analyzing and Learning from User Interactions for Search Clarification", "comments": "To appear in the Proceedings of SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asking clarifying questions in response to search queries has been recognized\nas a useful technique for revealing the underlying intent of the query.\nClarification has applications in retrieval systems with different interfaces,\nfrom the traditional web search interfaces to the limited bandwidth interfaces\nas in speech-only and small screen devices. Generation and evaluation of\nclarifying questions have been recently studied in the literature. However,\nuser interaction with clarifying questions is relatively unexplored. In this\npaper, we conduct a comprehensive study by analyzing large-scale user\ninteractions with clarifying questions in a major web search engine. In more\ndetail, we analyze the user engagements received by clarifying questions based\non different properties of search queries, clarifying questions, and their\ncandidate answers. We further study click bias in the data, and show that even\nthough reading clarifying questions and candidate answers does not take\nsignificant efforts, there still exist some position and presentation biases in\nthe data. We also propose a model for learning representation for clarifying\nquestions based on the user interaction data as implicit feedback. The model is\nused for re-ranking a number of automatically generated clarifying questions\nfor a given query. Evaluation on both click data and human labeled data\ndemonstrates the high quality of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 03:56:34 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Zamani", "Hamed", ""], ["Mitra", "Bhaskar", ""], ["Chen", "Everest", ""], ["Lueck", "Gord", ""], ["Diaz", "Fernando", ""], ["Bennett", "Paul N.", ""], ["Craswell", "Nick", ""], ["Dumais", "Susan T.", ""]]}, {"id": "2006.00385", "submitter": "Chetan Bansal", "authors": "Foyzul Hassan, Chetan Bansal, Nachiappan Nagappan, Thomas Zimmermann,\n  Ahmed Hassan Awadallah", "title": "An Empirical Study of Software Exceptions in the Field using Search Logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software engineers spend a substantial amount of time using Web search to\naccomplish software engineering tasks. Such search tasks include finding code\nsnippets, API documentation, seeking help with debugging, etc. While debugging\na bug or crash, one of the common practices of software engineers is to search\nfor information about the associated error or exception traces on the internet.\n  In this paper, we analyze query logs from a leading commercial\ngeneral-purpose search engine (GPSE) such as Google, Yahoo! or Bing to carry\nout a large scale study of software exceptions. To the best of our knowledge,\nthis is the first large scale study to analyze how Web search is used to find\ninformation about exceptions. We analyzed about 1 million exception related\nsearch queries from a random sample of 5 billion web search queries. To extract\nexceptions from unstructured query text, we built a novel and high-performance\nmachine learning model with a F1-score of 0.82. Using the machine learning\nmodel, we extracted exceptions from raw queries and performed popularity,\neffort, success, query characteristic and web domain analysis. We also\nperformed programming language-specific analysis to give a better view of the\nexception search behavior. These techniques can help improve existing methods,\ndocumentation and tools for exception analysis and prediction. Further, similar\ntechniques can be applied for APIs, frameworks, etc.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 22:59:02 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hassan", "Foyzul", ""], ["Bansal", "Chetan", ""], ["Nagappan", "Nachiappan", ""], ["Zimmermann", "Thomas", ""], ["Awadallah", "Ahmed Hassan", ""]]}, {"id": "2006.00533", "submitter": "Kaustubh Dhole", "authors": "Abhinav Bhatt, Kaustubh D. Dhole", "title": "Benchmarking BioRelEx for Entity Tagging and Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting relationships and interactions between different biological\nentities is still an extremely challenging problem but has not received much\nattention as much as extraction in other generic domains. In addition to the\nlack of annotated data, low benchmarking is still a major reason for slow\nprogress. In order to fill this gap, we compare multiple existing entity and\nrelation extraction models over a recently introduced public dataset, BioRelEx\nof sentences annotated with biological entities and relations. Our\nstraightforward benchmarking shows that span-based multi-task architectures\nlike DYGIE show 4.9% and 6% absolute improvements in entity tagging and\nrelation extraction respectively over the previous state-of-art and that\nincorporating domain-specific information like embeddings pre-trained over\nrelated domains boosts performance.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 14:45:28 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Bhatt", "Abhinav", ""], ["Dhole", "Kaustubh D.", ""]]}, {"id": "2006.00556", "submitter": "Haoji Hu", "authors": "Haoji Hu and Xiangnan He and Jinyang Gao and Zhi-Li Zhang", "title": "Modeling Personalized Item Frequency Information for Next-basket\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next-basket recommendation (NBR) is prevalent in e-commerce and retail\nindustry. In this scenario, a user purchases a set of items (a basket) at a\ntime. NBR performs sequential modeling and recommendation based on a sequence\nof baskets. NBR is in general more complex than the widely studied sequential\n(session-based) recommendation which recommends the next item based on a\nsequence of items. Recurrent neural network (RNN) has proved to be very\neffective for sequential modeling and thus been adapted for NBR. However, we\nargue that existing RNNs cannot directly capture item frequency information in\nthe recommendation scenario.\n  Through careful analysis of real-world datasets, we find that {\\em\npersonalized item frequency} (PIF) information (which records the number of\ntimes that each item is purchased by a user) provides two critical signals for\nNBR. But, this has been largely ignored by existing methods. Even though\nexisting methods such as RNN based methods have strong representation ability,\nour empirical results show that they fail to learn and capture PIF. As a\nresult, existing methods cannot fully exploit the critical signals contained in\nPIF. Given this inherent limitation of RNNs, we propose a simple item frequency\nbased k-nearest neighbors (kNN) method to directly utilize these critical\nsignals. We evaluate our method on four public real-world datasets. Despite its\nrelative simplicity, our method frequently outperforms the state-of-the-art NBR\nmethods -- including deep learning based methods using RNNs -- when patterns\nassociated with PIF play an important role in the data.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 16:42:39 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hu", "Haoji", ""], ["He", "Xiangnan", ""], ["Gao", "Jinyang", ""], ["Zhang", "Zhi-Li", ""]]}, {"id": "2006.00572", "submitter": "Erfaneh Gharavi", "authors": "Erfaneh Gharavi, Hadi Veisi", "title": "Improve Document Embedding for Text Categorization Through Deep Siamese\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing amount of data on the internet, finding a\nhighly-informative, low-dimensional representation for text is one of the main\nchallenges for efficient natural language processing tasks including text\nclassification. This representation should capture the semantic information of\nthe text while retaining their relevance level for document classification.\nThis approach maps the documents with similar topics to a similar space in\nvector space representation. To obtain representation for large text, we\npropose the utilization of deep Siamese neural networks. To embed document\nrelevance in topics in the distributed representation, we use a Siamese neural\nnetwork to jointly learn document representations. Our Siamese network consists\nof two sub-network of multi-layer perceptron. We examine our representation for\nthe text categorization task on BBC news dataset. The results show that the\nproposed representations outperform the conventional and state-of-the-art\nrepresentations in the text classification task on this dataset.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 17:51:08 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Gharavi", "Erfaneh", ""], ["Veisi", "Hadi", ""]]}, {"id": "2006.00617", "submitter": "Casper Hansen", "authors": "Casper Hansen and Christian Hansen and Jakob Grue Simonsen and Stephen\n  Alstrup and Christina Lioma", "title": "Content-aware Neural Hashing for Cold-start Recommendation", "comments": "Accepted to SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401060", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-aware recommendation approaches are essential for providing\nmeaningful recommendations for \\textit{new} (i.e., \\textit{cold-start}) items\nin a recommender system. We present a content-aware neural hashing-based\ncollaborative filtering approach (NeuHash-CF), which generates binary hash\ncodes for users and items, such that the highly efficient Hamming distance can\nbe used for estimating user-item relevance. NeuHash-CF is modelled as an\nautoencoder architecture, consisting of two joint hashing components for\ngenerating user and item hash codes. Inspired from semantic hashing, the item\nhashing component generates a hash code directly from an item's content\ninformation (i.e., it generates cold-start and seen item hash codes in the same\nmanner). This contrasts existing state-of-the-art models, which treat the two\nitem cases separately. The user hash codes are generated directly based on user\nid, through learning a user embedding matrix. We show experimentally that\nNeuHash-CF significantly outperforms state-of-the-art baselines by up to 12\\%\nNDCG and 13\\% MRR in cold-start recommendation settings, and up to 4\\% in both\nNDCG and MRR in standard settings where all items are present while training.\nOur approach uses 2-4x shorter hash codes, while obtaining the same or better\nperformance compared to the state of the art, thus consequently also enabling a\nnotable storage reduction.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 21:29:38 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hansen", "Casper", ""], ["Hansen", "Christian", ""], ["Simonsen", "Jakob Grue", ""], ["Alstrup", "Stephen", ""], ["Lioma", "Christina", ""]]}, {"id": "2006.00674", "submitter": "Milad Haghani", "authors": "Milad Haghani, Michiel C. J. Bliemer", "title": "Covid-19 pandemic and the unprecedented mobilisation of scholarly\n  efforts prompted by a health crisis: Scientometric comparisons across SARS,\n  MERS and 2019-nCov literature", "comments": null, "journal-ref": "Scientometrics (2020)", "doi": "10.1007/s11192-020-03706-z", "report-no": null, "categories": "cs.DL cs.IR physics.soc-ph q-bio.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the current century, each major coronavirus outbreak has triggered a\nquick surge of academic publications on this topic. The spike in research\npublications following the 2019 Novel Coronavirus (Covid-19), however, has been\nlike no other. The global crisis caused by the Covid-19 pandemic has mobilised\nscientific efforts in an unprecedented way. In less than five months, more than\n12,000 research items have been indexed while the number increasing every day.\nWith the crisis affecting all aspects of life, research on Covid-19 seems to\nhave become a focal point of interest across many academic disciplines. Here,\nscientometric aspects of the Covid-19 literature are analysed and contrasted\nwith those of the two previous major Coronavirus diseases, i.e. SARS and MERS.\nThe focus is on the co-occurrence of key-terms, bibliographic coupling and\ncitation relations of journals and collaborations between countries. Certain\nrecurring patterns across all three literatures were discovered. All three\noutbreaks have commonly generated three distinct and major cohort of studies:\n(i) studies linked to the public health response and epidemic control, (ii)\nstudies associated with the chemical constitution of the virus and (iii)\nstudies related to treatment, vaccine and clinical care. While studies\naffiliated with the category (i) seem to have been the first to emerge, they\noverall received least numbers of citations compared to those of the two other\ncategories. Covid-19 studies seem to have been distributed across a broader\nvariety of journals and subject areas. Clear links are observed between the\ngeographical origins of each outbreak or the local geographical severity of\neach outbreak and the magnitude of research originated from regions. Covid-19\nstudies also display the involvement of authors from a broader variety of\ncountries compared to SARS and MRS.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 02:33:23 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Haghani", "Milad", ""], ["Bliemer", "Michiel C. J.", ""]]}, {"id": "2006.00785", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Benet Oriol, Jordi Luque, Ferran Diego and Xavier Giro-i-Nieto", "title": "Transcription-Enriched Joint Embeddings for Spoken Descriptions of\n  Images and Videos", "comments": "Accepted for presentation at EPIC@CVPR2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an effective approach for training unique embedding\nrepresentations by combining three simultaneous modalities: image and spoken\nand textual narratives. The proposed methodology departs from a baseline system\nthat spawns a embedding space trained with only spoken narratives and image\ncues. Our experiments on the EPIC-Kitchen and Places Audio Caption datasets\nshow that introducing the human-generated textual transcriptions of the spoken\nnarratives helps to the training procedure yielding to get better embedding\nrepresentations. The triad speech, image and words allows for a better estimate\nof the point embedding and show an improving of the performance within tasks\nlike image and speech retrieval, even when text third modality, text, is not\npresent in the task.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 08:18:15 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Oriol", "Benet", ""], ["Luque", "Jordi", ""], ["Diego", "Ferran", ""], ["Giro-i-Nieto", "Xavier", ""]]}, {"id": "2006.00804", "submitter": "Mohammed Emtiaz Ahmed", "authors": "Mohammed Emtiaz Ahmed, Md Rafiqul Islam Rabin, Farah Naz Chowdhury", "title": "COVID-19: Social Media Sentiment Analysis on Reopening", "comments": "8 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novel coronavirus (COVID-19) pandemic is the most talked topic in social\nmedia platforms in 2020. People are using social media such as Twitter to\nexpress their opinion and share information on a number of issues related to\nthe COVID-19 in this stay at home order. In this paper, we investigate the\nsentiment and emotion of peoples in the United States on the subject of\nreopening. We choose the social media platform Twitter for our analysis and\nstudy the Tweets to discover the sentimental perspective, emotional\nperspective, and triggering words towards the reopening. During this COVID-19\npandemic, researchers have made some analysis on various social media dataset\nregarding lockdown and stay at home. However, in our analysis, we are\nparticularly interested to analyse public sentiment on reopening. Our major\nfinding is that when all states resorted to lockdown in March, people showed\ndominant emotion of fear, but as reopening starts people have less fear. While\nthis may be true, due to this reopening phase daily positive cases are rising\ncompared to the lockdown situation. Overall, people have a less negative\nsentiment towards the situation of reopening.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 09:15:02 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Ahmed", "Mohammed Emtiaz", ""], ["Rabin", "Md Rafiqul Islam", ""], ["Chowdhury", "Farah Naz", ""]]}, {"id": "2006.00937", "submitter": "Federico Errica", "authors": "Federico Errica, Ludovic Denoyer, Bora Edizel, Fabio Petroni, Vassilis\n  Plachouras, Fabrizio Silvestri, Sebastian Riedel", "title": "Concept Matching for Low-Resource Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a model to tackle classification tasks in the presence of very\nlittle training data. To this aim, we approximate the notion of exact match\nwith a theoretically sound mechanism that computes a probability of matching in\nthe input space. Importantly, the model learns to focus on elements of the\ninput that are relevant for the task at hand; by leveraging highlighted\nportions of the training data, an error boosting technique guides the learning\nprocess. In practice, it increases the error associated with relevant parts of\nthe input by a given factor. Remarkable results on text classification tasks\nconfirm the benefits of the proposed approach in both balanced and unbalanced\ncases, thus being of practical use when labeling new examples is expensive. In\naddition, by inspecting its weights, it is often possible to gather insights on\nwhat the model has learned.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 13:34:01 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Errica", "Federico", ""], ["Denoyer", "Ludovic", ""], ["Edizel", "Bora", ""], ["Petroni", "Fabio", ""], ["Plachouras", "Vassilis", ""], ["Silvestri", "Fabrizio", ""], ["Riedel", "Sebastian", ""]]}, {"id": "2006.00951", "submitter": "Ilias Kanellos", "authors": "Ilias Kanellos, Thanasis Vergoulis, Dimitris Sacharidis, Theodore\n  Dalamagas and Yannis Vassiliou", "title": "Ranking Papers by their Short-Term Scientific Impact", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The constantly increasing rate at which scientific papers are published makes\nit difficult for researchers to identify papers that currently impact the\nresearch field of their interest. Hence, approaches to effectively identify\npapers of high impact have attracted great attention in the past. In this work,\nwe present a method that seeks to rank papers based on their estimated\nshort-term impact, as measured by the number of citations received in the near\nfuture. Similar to previous work, our method models a researcher as she\nexplores the paper citation network. The key aspect is that we incorporate an\nattention-based mechanism, akin to a time-restricted version of preferential\nattachment, to explicitly capture a researcher's preference to read papers\nwhich received a lot of attention recently. A detailed experimental evaluation\non four real citation datasets across disciplines, shows that our approach is\nmore effective than previous work in ranking papers based on their short-term\nimpact.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 13:59:30 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 13:37:08 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kanellos", "Ilias", ""], ["Vergoulis", "Thanasis", ""], ["Sacharidis", "Dimitris", ""], ["Dalamagas", "Theodore", ""], ["Vassiliou", "Yannis", ""]]}, {"id": "2006.01207", "submitter": "Jens Helge Reelfs Reelfs", "authors": "Jens Helge Reelfs and Oliver Hohlfeld and Markus Strohmaier and Niklas\n  Henckell", "title": "Word-Emoji Embeddings from large scale Messaging Data reflect real-world\n  Semantic Associations of Expressive Icons", "comments": "10 pages, to appear in 3rd International Workshop on Emoji\n  Understanding and Applications in Social Media", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train word-emoji embeddings on large scale messaging data obtained from\nthe Jodel online social network. Our data set contains more than 40 million\nsentences, of which 11 million sentences are annotated with a subset of the\nUnicode 13.0 standard Emoji list. We explore semantic emoji associations\ncontained in this embedding by analyzing associations between emojis, between\nemojis and text, and between text and emojis. Our investigations demonstrate\nanecdotally that word-emoji embeddings trained on large scale messaging data\ncan reflect real-world semantic associations. To enable further research we\nrelease the Jodel Emoji Embedding Dataset (JEED1488) containing 1488 emojis and\ntheir embeddings along 300 dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 19:55:56 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Reelfs", "Jens Helge", ""], ["Hohlfeld", "Oliver", ""], ["Strohmaier", "Markus", ""], ["Henckell", "Niklas", ""]]}, {"id": "2006.01208", "submitter": "Nikhita Vedula", "authors": "Nikhita Vedula, Rahul Gupta, Aman Alok, Mukund Sridhar", "title": "Automatic Discovery of Novel Intents & Domains from Text Utterances", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary tasks in Natural Language Understanding (NLU) is to\nrecognize the intents as well as domains of users' spoken and written language\nutterances. Most existing research formulates this as a supervised\nclassification problem with a closed-world assumption, i.e. the domains or\nintents to be identified are pre-defined or known beforehand. Real-world\napplications however increasingly encounter dynamic, rapidly evolving\nenvironments with newly emerging intents and domains, about which no\ninformation is known during model training. We propose a novel framework,\nADVIN, to automatically discover novel domains and intents from large volumes\nof unlabeled data. We first employ an open classification model to identify all\nutterances potentially consisting of a novel intent. Next, we build a knowledge\ntransfer component with a pairwise margin loss function. It learns\ndiscriminative deep features to group together utterances and discover multiple\nlatent intent categories within them in an unsupervised manner. We finally\nhierarchically link mutually related intents into domains, forming an\nintent-domain taxonomy. ADVIN significantly outperforms baselines on three\nbenchmark datasets, and real user utterances from a commercial voice-powered\nagent.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 00:47:10 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Vedula", "Nikhita", ""], ["Gupta", "Rahul", ""], ["Alok", "Aman", ""], ["Sridhar", "Mukund", ""]]}, {"id": "2006.01211", "submitter": "Benjamin Horne", "authors": "Benjamin D. Horne and Maur\\'icio Gruppi and Sibel Adal{\\i}", "title": "Do All Good Actors Look The Same? Exploring News Veracity Detection\n  Across The U.S. and The U.K", "comments": "Published in ICWSM 2020 Data Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major concern with text-based news veracity detection methods is that they\nmay not generalize across countries and cultures. In this short paper, we\nexplicitly test news veracity models across news data from the United States\nand the United Kingdom, demonstrating there is reason for concern of\ngeneralizabilty. Through a series of testing scenarios, we show that text-based\nclassifiers perform poorly when trained on one country's news data and tested\non another. Furthermore, these same models have trouble classifying unseen,\nunreliable news sources. In conclusion, we discuss implications of these\nresults and avenues for future work.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 22:45:28 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Horne", "Benjamin D.", ""], ["Gruppi", "Maur\u00edcio", ""], ["Adal\u0131", "Sibel", ""]]}, {"id": "2006.01279", "submitter": "Fubao Wu", "authors": "Fubao Wu, Lixin Gao", "title": "Scalable Top-k Query on Information Networks with Hierarchical\n  Inheritance Relations", "comments": "18 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph query, pattern mining and knowledge discovery become challenging on\nlarge-scale heterogeneous information networks (HINs). State-of-the-art\ntechniques involving path propagation mainly focus on the inference on nodes\nlabels and neighborhood structures. However, entity links in the real world\nalso contain rich hierarchical inheritance relations. For example, the\nvulnerability of a product version is likely to be inherited from its older\nversions. Taking advantage of the hierarchical inheritances can potentially\nimprove the quality of query results. Motivated by this, we explore\nhierarchical inheritance relations between entities and formulate the problem\nof graph query on HINs with hierarchical inheritance relations. We propose a\ngraph query search algorithm by decomposing the original query graph into\nmultiple star queries and apply a star query algorithm to each star query.\nFurther candidates from each star query result are then constructed for final\ntop-k query answers to the original query. To efficiently obtain the graph\nquery result from a large-scale HIN, we design a bound-based pruning technique\nby using uniform cost search to prune search spaces. We implement our algorithm\nin GraphX to test the effectiveness and efficiency on synthetic and real-world\ndatasets. Compared with two common graph query algorithms, our algorithm can\neffectively obtain more accurate results and competitive performances.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 21:35:36 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Wu", "Fubao", ""], ["Gao", "Lixin", ""]]}, {"id": "2006.01287", "submitter": "Fanghua Ye", "authors": "Fanghua Ye, Zhiwei Lin, Chuan Chen, Zibin Zheng, Hong Huang", "title": "Outlier-Resilient Web Service QoS Prediction", "comments": "12 pages, to appear at the Web Conference (WWW) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of Web services makes it difficult for users to select the\nmost appropriate one among numerous functionally identical or similar service\ncandidates. Quality-of-Service (QoS) describes the non-functional\ncharacteristics of Web services, and it has become the key differentiator for\nservice selection. However, users cannot invoke all Web services to obtain the\ncorresponding QoS values due to high time cost and huge resource overhead.\nThus, it is essential to predict unknown QoS values. Although various QoS\nprediction methods have been proposed, few of them have taken outliers into\nconsideration, which may dramatically degrade the prediction performance. To\novercome this limitation, we propose an outlier-resilient QoS prediction method\nin this paper. Our method utilizes Cauchy loss to measure the discrepancy\nbetween the observed QoS values and the predicted ones. Owing to the robustness\nof Cauchy loss, our method is resilient to outliers. We further extend our\nmethod to provide time-aware QoS prediction results by taking the temporal\ninformation into consideration. Finally, we conduct extensive experiments on\nboth static and dynamic datasets. The results demonstrate that our method is\nable to achieve better performance than state-of-the-art baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 21:53:21 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 15:14:16 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 14:04:30 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Ye", "Fanghua", ""], ["Lin", "Zhiwei", ""], ["Chen", "Chuan", ""], ["Zheng", "Zibin", ""], ["Huang", "Hong", ""]]}, {"id": "2006.01346", "submitter": "Jie Cai", "authors": "Jie Cai, Zhengzhou Zhu, Ping Nie and Qian Liu", "title": "A Pairwise Probe for Understanding BERT Fine-Tuning on Machine Reading\n  Comprehension", "comments": "e.g.: 4 pages, 1 figure", "journal-ref": null, "doi": "10.1145/3397271.3401195", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-trained models have brought significant improvements to many NLP tasks\nand have been extensively analyzed. But little is known about the effect of\nfine-tuning on specific tasks. Intuitively, people may agree that a pre-trained\nmodel already learns semantic representations of words (e.g. synonyms are\ncloser to each other) and fine-tuning further improves its capabilities which\nrequire more complicated reasoning (e.g. coreference resolution, entity\nboundary detection, etc). However, how to verify these arguments analytically\nand quantitatively is a challenging task and there are few works focus on this\ntopic. In this paper, inspired by the observation that most probing tasks\ninvolve identifying matched pairs of phrases (e.g. coreference requires\nmatching an entity and a pronoun), we propose a pairwise probe to understand\nBERT fine-tuning on the machine reading comprehension (MRC) task. Specifically,\nwe identify five phenomena in MRC. According to pairwise probing tasks, we\ncompare the performance of each layer's hidden representation of pre-trained\nand fine-tuned BERT. The proposed pairwise probe alleviates the problem of\ndistraction from inaccurate model training and makes a robust and quantitative\ncomparison. Our experimental analysis leads to highly confident conclusions:\n(1) Fine-tuning has little effect on the fundamental and low-level information\nand general semantic tasks. (2) For specific abilities required for downstream\ntasks, fine-tuned BERT is better than pre-trained BERT and such gaps are\nobvious after the fifth layer.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 02:12:19 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Cai", "Jie", ""], ["Zhu", "Zhengzhou", ""], ["Nie", "Ping", ""], ["Liu", "Qian", ""]]}, {"id": "2006.01439", "submitter": "Maria Janina Sarol", "authors": "M. Janina Sarol, Ly Dinh, Rezvaneh Rezapour, Chieh-Li Chin, Pingjing\n  Yang, Jana Diesner", "title": "An Empirical Methodology for Detecting and Prioritizing Needs during\n  Crisis Events", "comments": null, "journal-ref": null, "doi": "10.18653/v1/2020.findings-emnlp.366", "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In times of crisis, identifying the essential needs is a crucial step to\nproviding appropriate resources and services to affected entities. Social media\nplatforms such as Twitter contain vast amount of information about the general\npublic's needs. However, the sparsity of the information as well as the amount\nof noisy content present a challenge to practitioners to effectively identify\nshared information on these platforms. In this study, we propose two novel\nmethods for two distinct but related needs detection tasks: the identification\nof 1) a list of resources needed ranked by priority, and 2) sentences that\nspecify who-needs-what resources. We evaluated our methods on a set of tweets\nabout the COVID-19 crisis. For task 1 (detecting top needs), we compared our\nresults against two given lists of resources and achieved 64% precision. For\ntask 2 (detecting who-needs-what), we compared our results on a set of 1,000\nannotated tweets and achieved a 68% F1-score.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 08:02:29 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Sarol", "M. Janina", ""], ["Dinh", "Ly", ""], ["Rezapour", "Rezvaneh", ""], ["Chin", "Chieh-Li", ""], ["Yang", "Pingjing", ""], ["Diesner", "Jana", ""]]}, {"id": "2006.01527", "submitter": "Mohamad Yaser Jaradeh", "authors": "Mohamad Yaser Jaradeh, Markus Stocker, S\\\"oren Auer", "title": "Question Answering on Scholarly Knowledge Graphs", "comments": "Pre-print for TPDL2020 accepted full paper, 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering questions on scholarly knowledge comprising text and other\nartifacts is a vital part of any research life cycle. Querying scholarly\nknowledge and retrieving suitable answers is currently hardly possible due to\nthe following primary reason: machine inactionable, ambiguous and unstructured\ncontent in publications. We present JarvisQA, a BERT based system to answer\nquestions on tabular views of scholarly knowledge graphs. Such tables can be\nfound in a variety of shapes in the scholarly literature (e.g., surveys,\ncomparisons or results). Our system can retrieve direct answers to a variety of\ndifferent questions asked on tabular data in articles. Furthermore, we present\na preliminary dataset of related tables and a corresponding set of natural\nlanguage questions. This dataset is used as a benchmark for our system and can\nbe reused by others. Additionally, JarvisQA is evaluated on two datasets\nagainst other baselines and shows an improvement of two to three folds in\nperformance compared to related methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 11:24:02 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Jaradeh", "Mohamad Yaser", ""], ["Stocker", "Markus", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "2006.01626", "submitter": "Bilal Abu-Salih", "authors": "Bilal Abu-Salih, Marwan Al-Tawil, Ibrahim Aljarah, Hossam Faris,\n  Pornpit Wongthongtham", "title": "Relational Learning Analysis of Social Politics using Knowledge Graph\n  Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graphs (KGs) have gained considerable attention recently from both\nacademia and industry. In fact, incorporating graph technology and the copious\nof various graph datasets have led the research community to build\nsophisticated graph analytics tools. Therefore, the application of KGs has\nextended to tackle a plethora of real-life problems in dissimilar domains.\nDespite the abundance of the currently proliferated generic KGs, there is a\nvital need to construct domain-specific KGs. Further, quality and credibility\nshould be assimilated in the process of constructing and augmenting KGs,\nparticularly those propagated from mixed-quality resources such as social media\ndata. This paper presents a novel credibility domain-based KG Embedding\nframework. This framework involves capturing a fusion of data obtained from\nheterogeneous resources into a formal KG representation depicted by a domain\nontology. The proposed approach makes use of various knowledge-based\nrepositories to enrich the semantics of the textual contents, thereby\nfacilitating the interoperability of information. The proposed framework also\nembodies a credibility module to ensure data quality and trustworthiness. The\nconstructed KG is then embedded in a low-dimension semantically-continuous\nspace using several embedding techniques. The utility of the constructed KG and\nits embeddings is demonstrated and substantiated on link prediction,\nclustering, and visualisation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 14:10:28 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Abu-Salih", "Bilal", ""], ["Al-Tawil", "Marwan", ""], ["Aljarah", "Ibrahim", ""], ["Faris", "Hossam", ""], ["Wongthongtham", "Pornpit", ""]]}, {"id": "2006.01644", "submitter": "Luis Leiva", "authors": "Ioannis Arapakis and Luis A. Leiva", "title": "Learning Efficient Representations of Mouse Movements to Predict User\n  Attention", "comments": "arXiv admin note: text overlap with arXiv:2001.07803", "journal-ref": "Proceedings of the 43rd Intl. ACM SIGIR Conf. on Research and\n  Development in Information Retrieval (SIGIR), 2020", "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking mouse cursor movements can be used to predict user attention on\nheterogeneous page layouts like SERPs. So far, previous work has relied heavily\non handcrafted features, which is a time-consuming approach that often requires\ndomain expertise. We investigate different representations of mouse cursor\nmovements, including time series, heatmaps, and trajectory-based images, to\nbuild and contrast both recurrent and convolutional neural networks that can\npredict user attention to direct displays, such as SERP advertisements. Our\nmodels are trained over raw mouse cursor data and achieve competitive\nperformance. We conclude that neural network models should be adopted for\ndownstream tasks involving mouse cursor movements, since they can provide an\ninvaluable implicit feedback signal for re-ranking and evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 09:52:26 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Arapakis", "Ioannis", ""], ["Leiva", "Luis A.", ""]]}, {"id": "2006.01888", "submitter": "Zhuoran Liu", "authors": "Zhuoran Liu and Martha Larson", "title": "Adversarial Item Promotion: Vulnerabilities at the Core of Top-N\n  Recommenders that Use Images to Address Cold Start", "comments": "Our code is available at https://github.com/liuzrcc/AIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  E-commerce platforms provide their customers with ranked lists of recommended\nitems matching the customers' preferences. Merchants on e-commerce platforms\nwould like their items to appear as high as possible in the top-N of these\nranked lists. In this paper, we demonstrate how unscrupulous merchants can\ncreate item images that artificially promote their products, improving their\nrankings. Recommender systems that use images to address the cold start problem\nare vulnerable to this security risk. We describe a new type of attack,\nAdversarial Item Promotion (AIP), that strikes directly at the core of Top-N\nrecommenders: the ranking mechanism itself. Existing work on adversarial images\nin recommender systems investigates the implications of conventional attacks,\nwhich target deep learning classifiers. In contrast, our AIP attacks are\nembedding attacks that seek to push features representations in a way that\nfools the ranker (not a classifier) and directly lead to item promotion. We\nintroduce three AIP attacks insider attack, expert attack, and semantic attack,\nwhich are defined with respect to three successively more realistic attack\nmodels. Our experiments evaluate the danger of these attacks when mounted\nagainst three representative visually-aware recommender algorithms in a\nframework that uses images to address cold start. We also evaluate potential\ndefenses, including adversarial training and find that common,\ncurrently-existing, techniques do not eliminate the danger of AIP attacks. In\nsum, we show that using images to address cold start opens recommender systems\nto potential threats with clear practical implications.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 19:12:13 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 11:46:09 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 13:05:48 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Liu", "Zhuoran", ""], ["Larson", "Martha", ""]]}, {"id": "2006.01894", "submitter": "Barbara Rychalska", "authors": "Jacek D\\k{a}browski, Barbara Rychalska, Micha{\\l} Daniluk, Dominika\n  Basaj, Konrad Go{\\l}uchowski, Piotr Babel, Andrzej Micha{\\l}owski, Adam\n  Jakubowski", "title": "An efficient manifold density estimator for all recommendation systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many unsupervised representation learning methods belong to the class of\nsimilarity learning models. While various modality-specific approaches exist\nfor different types of data, a core property of many methods is that\nrepresentations of similar inputs are close under some similarity function. We\npropose EMDE (Efficient Manifold Density Estimator) - a framework utilizing\narbitrary vector representations with the property of local similarity to\nsuccinctly represent smooth probability densities on Riemannian manifolds. Our\napproximate representation has the desirable properties of being fixed-size and\nhaving simple additive compositionality, thus being especially amenable to\ntreatment with neural networks - both as input and output format, producing\nefficient conditional estimators. We generalize and reformulate the problem of\nmulti-modal recommendations as conditional, weighted density estimation on\nmanifolds. Our approach allows for trivial inclusion of multiple interaction\ntypes, modalities of data as well as interaction strengths for any\nrecommendation setting. Applying EMDE to both top-k and session-based\nrecommendation settings, we establish new state-of-the-art results on multiple\nopen datasets in both uni-modal and multi-modal settings.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 19:20:20 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 10:42:45 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 16:37:00 GMT"}, {"version": "v4", "created": "Fri, 16 Apr 2021 12:50:23 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["D\u0105browski", "Jacek", ""], ["Rychalska", "Barbara", ""], ["Daniluk", "Micha\u0142", ""], ["Basaj", "Dominika", ""], ["Go\u0142uchowski", "Konrad", ""], ["Babel", "Piotr", ""], ["Micha\u0142owski", "Andrzej", ""], ["Jakubowski", "Adam", ""]]}, {"id": "2006.01926", "submitter": "Harshita Sahijwani", "authors": "Harshita Sahijwani, Jason Ingyu Choi, Eugene Agichtein", "title": "Would You Like to Hear the News? Investigating Voice-BasedSuggestions\n  for Conversational News Recommendation", "comments": null, "journal-ref": null, "doi": "10.1145/3343413.3378013", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key benefits of voice-based personal assistants is the potential\nto proactively recommend relevant and interesting information. One of the most\nvaluable sources of such information is the News. However, in order for the\nuser to hear the news that is useful and relevant to them, it must be\nrecommended in an interesting and informative way. However, to the best of our\nknowledge, how to present a news item for a voice-based recommendation remains\nan open question. In this paper, we empirically compare different ways of\nrecommending news, or specific news items, in a voice-based conversational\nsetting. Specifically, we study the user engagement and satisfaction with five\ndifferent variants of presenting news recommendations: (1) a generic news\nbriefing; (2) news about a specific entity relevant to the current\nconversation; (3) news about an entity from a past conversation; (4) news on a\ntrending news topic; and (5) the default - a suggestion to talk about news in\ngeneral. Our results show that entity-based news recommendations exhibit 29%\nhigher acceptance compared to briefing recommendations, and almost 100% higher\nacceptance compared to recommending generic or trending news. Our investigation\ninto the presentation of news recommendations and the resulting insights could\nmake voice assistants more informative and engaging.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 20:24:36 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Sahijwani", "Harshita", ""], ["Choi", "Jason Ingyu", ""], ["Agichtein", "Eugene", ""]]}, {"id": "2006.01969", "submitter": "Faegheh Hasibi", "authors": "Johannes M. van Hulst, Faegheh Hasibi, Koen Dercksen, Krisztian Balog,\n  Arjen P. de Vries", "title": "REL: An Entity Linker Standing on the Shoulders of Giants", "comments": null, "journal-ref": null, "doi": "10.1145/3397271.3401416", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity linking is a standard component in modern retrieval system that is\noften performed by third-party toolkits. Despite the plethora of open source\noptions, it is difficult to find a single system that has a modular\narchitecture where certain components may be replaced, does not depend on\nexternal sources, can easily be updated to newer Wikipedia versions, and, most\nimportant of all, has state-of-the-art performance. The REL system presented in\nthis paper aims to fill that gap. Building on state-of-the-art neural\ncomponents from natural language processing research, it is provided as a\nPython package as well as a web API. We also report on an experimental\ncomparison against both well-established systems and the current\nstate-of-the-art on standard entity linking benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 22:51:17 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["van Hulst", "Johannes M.", ""], ["Hasibi", "Faegheh", ""], ["Dercksen", "Koen", ""], ["Balog", "Krisztian", ""], ["de Vries", "Arjen P.", ""]]}, {"id": "2006.02046", "submitter": "Zuohui Fu", "authors": "Zuohui Fu, Yikun Xian, Ruoyuan Gao, Jieyu Zhao, Qiaoying Huang,\n  Yingqiang Ge, Shuyuan Xu, Shijie Geng, Chirag Shah, Yongfeng Zhang, Gerard de\n  Melo", "title": "Fairness-Aware Explainable Recommendation over Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been growing attention on fairness considerations recently,\nespecially in the context of intelligent decision making systems. Explainable\nrecommendation systems, in particular, may suffer from both explanation bias\nand performance disparity. In this paper, we analyze different groups of users\naccording to their level of activity, and find that bias exists in\nrecommendation performance between different groups. We show that inactive\nusers may be more susceptible to receiving unsatisfactory recommendations, due\nto insufficient training data for the inactive users, and that their\nrecommendations may be biased by the training records of more active users, due\nto the nature of collaborative filtering, which leads to an unfair treatment by\nthe system. We propose a fairness constrained approach via heuristic re-ranking\nto mitigate this unfairness problem in the context of explainable\nrecommendation over knowledge graphs. We experiment on several real-world\ndatasets with state-of-the-art knowledge graph-based explainable recommendation\nalgorithms. The promising results show that our algorithm is not only able to\nprovide high-quality explainable recommendations, but also reduces the\nrecommendation unfairness in several respects.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 05:04:38 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 02:34:35 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Fu", "Zuohui", ""], ["Xian", "Yikun", ""], ["Gao", "Ruoyuan", ""], ["Zhao", "Jieyu", ""], ["Huang", "Qiaoying", ""], ["Ge", "Yingqiang", ""], ["Xu", "Shuyuan", ""], ["Geng", "Shijie", ""], ["Shah", "Chirag", ""], ["Zhang", "Yongfeng", ""], ["de Melo", "Gerard", ""]]}, {"id": "2006.02104", "submitter": "Arkaitz Zubiaga", "authors": "Arkaitz Zubiaga", "title": "Exploiting Class Labels to Boost Performance on Embedding-based Text\n  Classification", "comments": "CIKM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification is one of the most frequent tasks for processing textual\ndata, facilitating among others research from large-scale datasets. Embeddings\nof different kinds have recently become the de facto standard as features used\nfor text classification. These embeddings have the capacity to capture meanings\nof words inferred from occurrences in large external collections. While they\nare built out of external collections, they are unaware of the distributional\ncharacteristics of words in the classification dataset at hand, including most\nimportantly the distribution of words across classes in training data. To make\nthe most of these embeddings as features and to boost the performance of\nclassifiers using them, we introduce a weighting scheme, Term\nFrequency-Category Ratio (TF-CR), which can weight high-frequency,\ncategory-exclusive words higher when computing word embeddings. Our experiments\non eight datasets show the effectiveness of TF-CR, leading to improved\nperformance scores over the well-known weighting schemes TF-IDF and KLD as well\nas over the absence of a weighting scheme in most cases.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 08:53:40 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 19:39:36 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Zubiaga", "Arkaitz", ""]]}, {"id": "2006.02282", "submitter": "Han Zhang", "authors": "Han Zhang, Songlin Wang, Kang Zhang, Zhiling Tang, Yunjiang Jiang, Yun\n  Xiao, Weipeng Yan, Wen-Yun Yang", "title": "Towards Personalized and Semantic Retrieval: An End-to-End Solution for\n  E-commerce Search via Embedding Learning", "comments": "Accepted by SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays e-commerce search has become an integral part of many people's\nshopping routines. Two critical challenges stay in today's e-commerce search:\nhow to retrieve items that are semantically relevant but not exact matching to\nquery terms, and how to retrieve items that are more personalized to different\nusers for the same search query. In this paper, we present a novel approach\ncalled DPSR, which stands for Deep Personalized and Semantic Retrieval, to\ntackle this problem. Explicitly, we share our design decisions on how to\narchitect a retrieval system so as to serve industry-scale traffic efficiently\nand how to train a model so as to learn query and item semantics accurately.\nBased on offline evaluations and online A/B test with live traffics, we show\nthat DPSR model outperforms existing models, and DPSR system can retrieve more\npersonalized and semantically relevant items to significantly improve users'\nsearch experience by +1.29% conversion rate, especially for long tail queries\nby +10.03%. As a result, our DPSR system has been successfully deployed into\nJD.com's search production since 2019.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 14:04:30 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 05:26:31 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 06:39:49 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Zhang", "Han", ""], ["Wang", "Songlin", ""], ["Zhang", "Kang", ""], ["Tang", "Zhiling", ""], ["Jiang", "Yunjiang", ""], ["Xiao", "Yun", ""], ["Yan", "Weipeng", ""], ["Yang", "Wen-Yun", ""]]}, {"id": "2006.02330", "submitter": "Elif Vural", "authors": "Semih Kaya and Elif Vural", "title": "Learning Multi-Modal Nonlinear Embeddings: Performance Bounds and an\n  Algorithm", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3071688", "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many approaches exist in the literature to learn low-dimensional\nrepresentations for data collections in multiple modalities, the\ngeneralizability of multi-modal nonlinear embeddings to previously unseen data\nis a rather overlooked subject. In this work, we first present a theoretical\nanalysis of learning multi-modal nonlinear embeddings in a supervised setting.\nOur performance bounds indicate that for successful generalization in\nmulti-modal classification and retrieval problems, the regularity of the\ninterpolation functions extending the embedding to the whole data space is as\nimportant as the between-class separation and cross-modal alignment criteria.\nWe then propose a multi-modal nonlinear representation learning algorithm that\nis motivated by these theoretical findings, where the embeddings of the\ntraining samples are optimized jointly with the Lipschitz regularity of the\ninterpolators. Experimental comparison to recent multi-modal and single-modal\nlearning algorithms suggests that the proposed method yields promising\nperformance in multi-modal image classification and cross-modal image-text\nretrieval applications.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 15:22:16 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 22:01:04 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Kaya", "Semih", ""], ["Vural", "Elif", ""]]}, {"id": "2006.02366", "submitter": "Olga Scrivner", "authors": "Katy B\\\"orner, Olga Scrivner, Leonard E. Cross, Michael Gallant,\n  Shutian Ma, Adam S. Martin, Elizabeth Record, Haici Yang, Jonathan M. Dilger", "title": "Mapping the co-evolution of artificial intelligence, robotics, and the\n  internet of things over 20 years (1998-2017)", "comments": "10 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0242984", "report-no": null, "categories": "cs.DL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the emergence, co-evolution, and convergence of science and\ntechnology (S&T) areas offers competitive intelligence for researchers,\nmanagers, policy makers, and others. The resulting data-driven decision support\nhelps set proper research and development (R&D) priorities; develop future S&T\ninvestment strategies; monitor key authors, organizations, or countries;\nperform effective research program assessment; and implement cutting-edge\neducation/training efforts. This paper presents new funding, publication, and\nscholarly network metrics and visualizations that were validated via expert\nsurveys. The metrics and visualizations exemplify the emergence and convergence\nof three areas of strategic interest: artificial intelligence (AI), robotics,\nand internet of things (IoT) over the last 20 years (1998-2017). For 32,716\npublications and 4,497 NSF awards, we identify their conceptual space (using\nthe UCSD map of science), geospatial network, and co-evolution landscape. The\nfindings demonstrate how the transition of knowledge (through cross-discipline\npublications and citations) and the emergence of new concepts (through term\nbursting) create a tangible potential for interdisciplinary research and new\ndisciplines.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 16:30:32 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["B\u00f6rner", "Katy", ""], ["Scrivner", "Olga", ""], ["Cross", "Leonard E.", ""], ["Gallant", "Michael", ""], ["Ma", "Shutian", ""], ["Martin", "Adam S.", ""], ["Record", "Elizabeth", ""], ["Yang", "Haici", ""], ["Dilger", "Jonathan M.", ""]]}, {"id": "2006.02526", "submitter": "Yongxin Liu", "authors": "Yongxin Liu", "title": "Study on Key Technologies of Transit Passengers Travel Pattern Mining\n  and Applications based on Multiple Sources of Data", "comments": "My PhD dissertation in Chinese 125 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IR cs.LG cs.SY eess.SY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, we propose a series of methodologies to mine transit riders\ntravel pattern and behavioral preferences, and then we use these knowledges to\nadjust and optimize the transit systems. Contributions are: 1) To increase the\ndata validity: a) we propose a novel approach to rectify the time discrepancy\nof data between the AFC (Automated Fare Collection) systems and AVL (Automated\nVehicle Location) system, our approach transforms data events into signals and\napplies time domain correlation the detect and rectify their relative\ndiscrepancies. b) By combining historical data and passengers ticketing time\nstamps, we induct and compensate missing information in AVL datasets. 2) To\ninfer passengers alighting point, we introduce a maximum probabilistic model\nincorporating passengers home place to recover their complete transit\ntrajectory from semi-complete boarding records.Then we propose an enhance\nactivity identification algorithm which is capable of specifying passengers\nshort-term activity from ordinary transfers. Finally, we analyze the\ntemporal-spatial characteristic of transit ridership. 3) To discover passengers\ntravel demands. We integrate each passengers trajectory data in multiple days\nand construct a Hybrid Trip Graph (HTG). We then use a depth search algorithm\nto derive the spatially closed transit trip chains; Finally, we use closed\ntransit trip chains of passengers to study their travel pattern from various\nperspectives. Finally, we analyze urban transit corridors by aggregating the\npassengers critical transit chains.4) We derive eight influential factors, and\nthen construct passengers choice models under various scenarios. Next, we\nvalidate our model using ridership re-distribute simulations. Finally, we\nconduct a comprehensive analysis on passengers temporal choice preference and\nuse this information to optimize urban transit systems.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 22:35:28 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Liu", "Yongxin", ""]]}, {"id": "2006.02633", "submitter": "Serhad Sarica", "authors": "Serhad Sarica and Jianxi Luo", "title": "Stopwords in Technical Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are increasingly applications of natural language processing techniques\nfor information retrieval, indexing and topic modelling in the engineering\ncontexts. A standard component of such tasks is the removal of stopwords, which\nare uninformative components of the data. While researchers use readily\navailable stopword lists which are derived for general English language, the\ntechnical jargon of engineering fields contains their own highly frequent and\nuninformative words and there exists no standard stopword list for technical\nlanguage processing applications. Here we address this gap by rigorously\nidentifying generic, insignificant, uninformative stopwords in engineering\ntexts beyond the stopwords in general texts, based on the synthesis of\nalternative data-driven approaches, and curating a stopword list ready for\ntechnical language processing applications.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 03:52:59 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Sarica", "Serhad", ""], ["Luo", "Jianxi", ""]]}, {"id": "2006.02770", "submitter": "Zeljko Carevic", "authors": "Zeljko Carevic, Dwaipayan Roy, Philipp Mayr", "title": "Characteristics of Dataset Retrieval Sessions: Experiences from a\n  Real-life Digital Library", "comments": "Accepted for publication at TPDL 2020", "journal-ref": null, "doi": "10.1007/978-3-030-54956-5_14", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secondary analysis or the reuse of existing survey data is a common practice\namong social scientists. Searching for relevant datasets in Digital Libraries\nis a somehow unfamiliar behaviour for this community. Dataset retrieval,\nespecially in the social sciences, incorporates additional material such as\ncodebooks, questionnaires, raw data files and more. Our assumption is that due\nto the diverse nature of datasets, document retrieval models often do not work\nas efficiently for retrieving datasets. One way of enhancing these types of\nsearches is to incorporate the users' interaction context in order to\npersonalise dataset retrieval sessions. As a first step towards this long term\ngoal, we study characteristics of dataset retrieval sessions from a real-life\nDigital Library for the social sciences that incorporates both: research data\nand publications. Previous studies reported a way of discerning queries between\ndocument search and dataset search by query length. In this paper, we argue the\nclaim and report our findings of an indistinguishability of queries, whether\naiming for a dataset or a document. Amongst others, we report our findings of\ndataset retrieval sessions with respect to query characteristics, interaction\nsequences and topical drift within 65,000 unique sessions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 11:01:24 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 14:57:47 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Carevic", "Zeljko", ""], ["Roy", "Dwaipayan", ""], ["Mayr", "Philipp", ""]]}, {"id": "2006.02785", "submitter": "Erik Faessler", "authors": "Erik Faessler, Michel Oleynik, Udo Hahn", "title": "What Makes a Top-Performing Precision Medicine Search Engine? Tracing\n  Main System Features in a Systematic Way", "comments": "Accepted for SIGIR2020, 10 pages", "journal-ref": null, "doi": "10.1145/3397271.3401048", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From 2017 to 2019 the Text REtrieval Conference (TREC) held a challenge task\non precision medicine using documents from medical publications (PubMed) and\nclinical trials. Despite lots of performance measurements carried out in these\nevaluation campaigns, the scientific community is still pretty unsure about the\nimpact individual system features and their weights have on the overall system\nperformance. In order to overcome this explanatory gap, we first determined\noptimal feature configurations using the Sequential Model-based Algorithm\nConfiguration (SMAC) program and applied its output to a BM25-based search\nengine. We then ran an ablation study to systematically assess the individual\ncontributions of relevant system features: BM25 parameters, query type and\nweighting schema, query expansion, stop word filtering, and keyword boosting.\nFor evaluation, we employed the gold standard data from the three TREC-PM\ninstallments to evaluate the effectiveness of different features using the\ncommonly shared infNDCG metric.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 11:25:10 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 05:38:25 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Faessler", "Erik", ""], ["Oleynik", "Michel", ""], ["Hahn", "Udo", ""]]}, {"id": "2006.02972", "submitter": "Yun William Yu", "authors": "Yun William Yu, Jean-Charles Delvenne, Sophia N. Yaliraki, Mauricio\n  Barahona", "title": "Severability of mesoscale components and local time scales in dynamical\n  networks", "comments": "24 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.IR cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major goal of dynamical systems theory is the search for simplified\ndescriptions of the dynamics of a large number of interacting states. For\noverwhelmingly complex dynamical systems, the derivation of a reduced\ndescription on the entire dynamics at once is computationally infeasible. Other\ncomplex systems are so expansive that despite the continual onslaught of new\ndata only partial information is available. To address this challenge, we\ndefine and optimise for a local quality function severability for measuring the\ndynamical coherency of a set of states over time. The theoretical underpinnings\nof severability lie in our local adaptation of the Simon-Ando-Fisher time-scale\nseparation theorem, which formalises the intuition of local wells in the Markov\nlandscape of a dynamical process, or the separation between a microscopic and a\nmacroscopic dynamics. Finally, we demonstrate the practical relevance of\nseverability by applying it to examples drawn from power networks, image\nsegmentation, social networks, metabolic networks, and word association.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 15:50:53 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Yu", "Yun William", ""], ["Delvenne", "Jean-Charles", ""], ["Yaliraki", "Sophia N.", ""], ["Barahona", "Mauricio", ""]]}, {"id": "2006.03048", "submitter": "Mohamed Attia", "authors": "Islam Samy, Mohamed A. Attia, Ravi Tandon, Loukas Lazos", "title": "Asymmetric Leaky Private Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DB cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information-theoretic formulations of the private information retrieval (PIR)\nproblem have been investigated under a variety of scenarios. Symmetric private\ninformation retrieval (SPIR) is a variant where a user is able to privately\nretrieve one out of $K$ messages from $N$ non-colluding replicated databases\nwithout learning anything about the remaining $K-1$ messages. However, the goal\nof perfect privacy can be too taxing for certain applications. In this paper,\nwe investigate if the information-theoretic capacity of SPIR (equivalently, the\ninverse of the minimum download cost) can be increased by relaxing both user\nand DB privacy definitions. Such relaxation is relevant in applications where\nprivacy can be traded for communication efficiency. We introduce and\ninvestigate the Asymmetric Leaky PIR (AL-PIR) model with different privacy\nleakage budgets in each direction. For user privacy leakage, we bound the\nprobability ratios between all possible realizations of DB queries by a\nfunction of a non-negative constant $\\epsilon$. For DB privacy, we bound the\nmutual information between the undesired messages, the queries, and the\nanswers, by a function of a non-negative constant $\\delta$. We propose a\ngeneral AL-PIR scheme that achieves an upper bound on the optimal download cost\nfor arbitrary $\\epsilon$ and $\\delta$. We show that the optimal download cost\nof AL-PIR is upper-bounded as $D^{*}(\\epsilon,\\delta)\\leq\n1+\\frac{1}{N-1}-\\frac{\\delta e^{\\epsilon}}{N^{K-1}-1}$. Second, we obtain an\ninformation-theoretic lower bound on the download cost as\n$D^{*}(\\epsilon,\\delta)\\geq\n1+\\frac{1}{Ne^{\\epsilon}-1}-\\frac{\\delta}{(Ne^{\\epsilon})^{K-1}-1}$. The gap\nanalysis between the two bounds shows that our AL-PIR scheme is optimal when\n$\\epsilon =0$, i.e., under perfect user privacy and it is optimal within a\nmaximum multiplicative gap of $\\frac{N-e^{-\\epsilon}}{N-1}$ for any\n$(\\epsilon,\\delta)$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 17:58:46 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Samy", "Islam", ""], ["Attia", "Mohamed A.", ""], ["Tandon", "Ravi", ""], ["Lazos", "Loukas", ""]]}, {"id": "2006.03185", "submitter": "Zhiwen Tang", "authors": "Limin Chen, Zhiwen Tang, Grace Hui Yang", "title": "Balancing Reinforcement Learning Training Experiences in Interactive\n  Information Retrieval", "comments": "Accepted by SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401200", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive Information Retrieval (IIR) and Reinforcement Learning (RL) share\nmany commonalities, including an agent who learns while interacts, a long-term\nand complex goal, and an algorithm that explores and adapts. To successfully\napply RL methods to IIR, one challenge is to obtain sufficient relevance labels\nto train the RL agents, which are infamously known as sample inefficient.\nHowever, in a text corpus annotated for a given query, it is not the relevant\ndocuments but the irrelevant documents that predominate. This would cause very\nunbalanced training experiences for the agent and prevent it from learning any\npolicy that is effective. Our paper addresses this issue by using domain\nrandomization to synthesize more relevant documents for the training. Our\nexperimental results on the Text REtrieval Conference (TREC) Dynamic Domain\n(DD) 2017 Track show that the proposed method is able to boost an RL agent's\nlearning effectiveness by 22\\% in dealing with unseen situations.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 00:38:39 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 01:41:34 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Chen", "Limin", ""], ["Tang", "Zhiwen", ""], ["Yang", "Grace Hui", ""]]}, {"id": "2006.03210", "submitter": "Minh-Tien Nguyen", "authors": "Minh-Tien Nguyen and Bui Cong Minh and Dung Tien Le and Le Thai Linh", "title": "Sentence Compression as Deletion with Contextual Embeddings", "comments": "12 pages, 3 figures, accepted by ICCCI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentence compression is the task of creating a shorter version of an input\nsentence while keeping important information. In this paper, we extend the task\nof compression by deletion with the use of contextual embeddings. Different\nfrom prior work usually using non-contextual embeddings (Glove or Word2Vec), we\nexploit contextual embeddings that enable our model capturing the context of\ninputs. More precisely, we utilize contextual embeddings stacked by\nbidirectional Long-short Term Memory and Conditional Random Fields for dealing\nwith sequence labeling. Experimental results on a benchmark Google dataset show\nthat by utilizing contextual embeddings, our model achieves a new\nstate-of-the-art F-score compared to strong methods reported on the leader\nboard.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 02:40:46 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Nguyen", "Minh-Tien", ""], ["Minh", "Bui Cong", ""], ["Le", "Dung Tien", ""], ["Linh", "Le Thai", ""]]}, {"id": "2006.03292", "submitter": "Ayush Garg", "authors": "Ayush Garg, Sammed Shantinath Kagi, Mayank Singh", "title": "SEAL: Scientific Keyphrase Extraction and Classification", "comments": "Accepted at JCDL 2020", "journal-ref": null, "doi": "10.1145/3383583.3398625", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic scientific keyphrase extraction is a challenging problem\nfacilitating several downstream scholarly tasks like search, recommendation,\nand ranking. In this paper, we introduce SEAL, a scholarly tool for automatic\nkeyphrase extraction and classification. The keyphrase extraction module\ncomprises two-stage neural architecture composed of Bidirectional Long\nShort-Term Memory cells augmented with Conditional Random Fields. The\nclassification module comprises of a Random Forest classifier. We extensively\nexperiment to showcase the robustness of the system. We evaluate multiple\nstate-of-the-art baselines and show a significant improvement. The current\nsystem is hosted at http://lingo.iitgn.ac.in:5000/.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 08:21:26 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Garg", "Ayush", ""], ["Kagi", "Sammed Shantinath", ""], ["Singh", "Mayank", ""]]}, {"id": "2006.03316", "submitter": "Sayantan Adak", "authors": "Sayantan Adak, Atharva Vyas, Animesh Mukherjee, Heer Ambavi, Pritam\n  Kadasi, Mayank Singh, Shivam Patel", "title": "Gandhipedia: A one-stop AI-enabled portal for browsing Gandhian\n  literature, life-events and his social network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an AI-enabled portal that presents an excellent visualization of\nMahatma Gandhi's life events by constructing temporal and spatial social\nnetworks from the Gandhian literature. Applying an ensemble of methods drawn\nfrom NLTK, Polyglot and Spacy we extract the key persons and places that find\nmentions in Gandhi's written works. We visualize these entities and connections\nbetween them based on co-mentions within the same time frame as networks in an\ninteractive web portal. The nodes in the network, when clicked, fire search\nqueries about the entity and all the information about the entity presented in\nthe corresponding book from which the network is constructed, are retrieved and\npresented back on the portal. Overall, this system can be used as a digital and\nuser-friendly resource to study Gandhian literature.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 09:02:51 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Adak", "Sayantan", ""], ["Vyas", "Atharva", ""], ["Mukherjee", "Animesh", ""], ["Ambavi", "Heer", ""], ["Kadasi", "Pritam", ""], ["Singh", "Mayank", ""], ["Patel", "Shivam", ""]]}, {"id": "2006.03353", "submitter": "Haider Khalid", "authors": "Haider Khalid, Vincent Wade", "title": "Topic Detection from Conversational Dialogue Corpus with Parallel\n  Dirichlet Allocation Model and Elbow Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conversational system needs to know how to switch between topics to\ncontinue the conversation for a more extended period. For this topic detection\nfrom dialogue corpus has become an important task for a conversation and\naccurate prediction of conversation topics is important for creating coherent\nand engaging dialogue systems. In this paper, we proposed a topic detection\napproach with Parallel Latent Dirichlet Allocation (PLDA) Model by clustering a\nvocabulary of known similar words based on TF-IDF scores and Bag of Words (BOW)\ntechnique. In the experiment, we use K-mean clustering with Elbow Method for\ninterpretation and validation of consistency within-cluster analysis to select\nthe optimal number of clusters. We evaluate our approach by comparing it with\ntraditional LDA and clustering technique. The experimental results show that\ncombining PLDA with Elbow method selects the optimal number of clusters and\nrefine the topics for the conversation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 10:24:43 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Khalid", "Haider", ""], ["Wade", "Vincent", ""]]}, {"id": "2006.03481", "submitter": "Fernando Ortega", "authors": "Fernando Ortega, Ra\\'ul Lara-Cabrera, \\'Angel Gonz\\'alez-Prieto,\n  Jes\\'us Bobadilla", "title": "Providing reliability in Recommender Systems through Bernoulli Matrix\n  Factorization", "comments": "28 pages, 8 figures, 8 tables", "journal-ref": "Information Sciences, 2020", "doi": "10.1016/j.ins.2020.12.001", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beyond accuracy, quality measures are gaining importance in modern\nrecommender systems, with reliability being one of the most important\nindicators in the context of collaborative filtering. This paper proposes\nBernoulli Matrix Factorization (BeMF), which is a matrix factorization model,\nto provide both prediction values and reliability values. BeMF is a very\ninnovative approach from several perspectives: a) it acts on model-based\ncollaborative filtering rather than on memory-based filtering, b) it does not\nuse external methods or extended architectures, such as existing solutions, to\nprovide reliability, c) it is based on a classification-based model instead of\ntraditional regression-based models, and d) matrix factorization formalism is\nsupported by the Bernoulli distribution to exploit the binary nature of the\ndesigned classification model. The experimental results show that the more\nreliable a prediction is, the less liable it is to be wrong: recommendation\nquality improves after the most reliable predictions are selected.\nState-of-the-art quality measures for reliability have been tested, which shows\nthat BeMF outperforms previous baseline methods and models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 14:24:27 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 10:10:36 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 11:12:48 GMT"}, {"version": "v4", "created": "Wed, 28 Apr 2021 10:23:15 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Ortega", "Fernando", ""], ["Lara-Cabrera", "Ra\u00fal", ""], ["Gonz\u00e1lez-Prieto", "\u00c1ngel", ""], ["Bobadilla", "Jes\u00fas", ""]]}, {"id": "2006.03541", "submitter": "Mar\\'ia N. Moreno Garc\\'ia", "authors": "Nhan Cach Dang, Mar\\'ia N. Moreno-Garc\\'ia and Fernando De la Prieta", "title": "Sentiment Analysis Based on Deep Learning: A Comparative Study", "comments": null, "journal-ref": "Electronics, 9 (3), 483, 29 pages, 2020", "doi": "10.3390/electronics9030483", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study of public opinion can provide us with valuable information. The\nanalysis of sentiment on social networks, such as Twitter or Facebook, has\nbecome a powerful means of learning about the users' opinions and has a wide\nrange of applications. However, the efficiency and accuracy of sentiment\nanalysis is being hindered by the challenges encountered in natural language\nprocessing (NLP). In recent years, it has been demonstrated that deep learning\nmodels are a promising solution to the challenges of NLP. This paper reviews\nthe latest studies that have employed deep learning to solve sentiment analysis\nproblems, such as sentiment polarity. Models using term frequency-inverse\ndocument frequency (TF-IDF) and word embedding have been applied to a series of\ndatasets. Finally, a comparative study has been conducted on the experimental\nresults obtained for the different models and input features\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 16:28:10 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Dang", "Nhan Cach", ""], ["Moreno-Garc\u00eda", "Mar\u00eda N.", ""], ["De la Prieta", "Fernando", ""]]}, {"id": "2006.03685", "submitter": "Zachariah Zhang", "authors": "Zachariah Zhang, Jingshu Liu, Narges Razavian", "title": "BERT-XML: Large Scale Automated ICD Coding Using BERT Pretraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical interactions are initially recorded and documented in free text\nmedical notes. ICD coding is the task of classifying and coding all diagnoses,\nsymptoms and procedures associated with a patient's visit. The process is often\nmanual and extremely time-consuming and expensive for hospitals. In this paper,\nwe propose a machine learning model, BERT-XML, for large scale automated ICD\ncoding from EHR notes, utilizing recently developed unsupervised pretraining\nthat have achieved state of the art performance on a variety of NLP tasks. We\ntrain a BERT model from scratch on EHR notes, learning with vocabulary better\nsuited for EHR tasks and thus outperform off-the-shelf models. We adapt the\nBERT architecture for ICD coding with multi-label attention. While other works\nfocus on small public medical datasets, we have produced the first large scale\nICD-10 classification model using millions of EHR notes to predict thousands of\nunique ICD codes.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 21:12:43 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Zhang", "Zachariah", ""], ["Liu", "Jingshu", ""], ["Razavian", "Narges", ""]]}, {"id": "2006.03715", "submitter": "Farzad Eskandanian", "authors": "Farzad Eskandanian, Bamshad Mobasher", "title": "Using Stable Matching to Optimize the Balance between Accuracy and\n  Diversity in Recommendation", "comments": null, "journal-ref": null, "doi": "10.1145/3340631.3394858", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Increasing aggregate diversity (or catalog coverage) is an important\nsystem-level objective in many recommendation domains where it may be desirable\nto mitigate the popularity bias and to improve the coverage of long-tail items\nin recommendations given to users. This is especially important in\nmultistakeholder recommendation scenarios where it may be important to optimize\nutilities not just for the end user, but also for other stakeholders such as\nitem sellers or producers who desire a fair representation of their items\nacross recommendation lists produced by the system. Unfortunately, attempts to\nincrease aggregate diversity often result in lower recommendation accuracy for\nend users. Thus, addressing this problem requires an approach that can\neffectively manage the trade-offs between accuracy and aggregate diversity. In\nthis work, we propose a two-sided post-processing approach in which both user\nand item utilities are considered. Our goal is to maximize aggregate diversity\nwhile minimizing loss in recommendation accuracy. Our solution is a\ngeneralization of the Deferred Acceptance algorithm which was proposed as an\nefficient algorithm to solve the well-known stable matching problem. We prove\nthat our algorithm results in a unique user-optimal stable match between items\nand users. Using three recommendation datasets, we empirically demonstrate the\neffectiveness of our approach in comparison to several baselines. In\nparticular, our results show that the proposed solution is quite effective in\nincreasing aggregate diversity and item-side utility while optimizing\nrecommendation accuracy for end users.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 22:12:25 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Eskandanian", "Farzad", ""], ["Mobasher", "Bamshad", ""]]}, {"id": "2006.03736", "submitter": "Aravind Sankar", "authors": "Aravind Sankar, Yanhong Wu, Yuhang Wu, Wei Zhang, Hao Yang, Hari\n  Sundaram", "title": "GroupIM: A Mutual Information Maximization Framework for Neural Group\n  Recommendation", "comments": "SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401116", "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of making item recommendations to ephemeral groups,\nwhich comprise users with limited or no historical activities together.\nExisting studies target persistent groups with substantial activity history,\nwhile ephemeral groups lack historical interactions. To overcome group\ninteraction sparsity, we propose data-driven regularization strategies to\nexploit both the preference covariance amongst users who are in the same group,\nas well as the contextual relevance of users' individual preferences to each\ngroup.\n  We make two contributions. First, we present a recommender\narchitecture-agnostic framework GroupIM that can integrate arbitrary neural\npreference encoders and aggregators for ephemeral group recommendation. Second,\nwe regularize the user-group latent space to overcome group interaction\nsparsity by: maximizing mutual information between representations of groups\nand group members; and dynamically prioritizing the preferences of highly\ninformative members through contextual preference weighting. Our experimental\nresults on several real-world datasets indicate significant performance\nimprovements (31-62% relative NDCG@20) over state-of-the-art group\nrecommendation techniques.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 23:18:19 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 03:13:09 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Sankar", "Aravind", ""], ["Wu", "Yanhong", ""], ["Wu", "Yuhang", ""], ["Zhang", "Wei", ""], ["Yang", "Hao", ""], ["Sundaram", "Hari", ""]]}, {"id": "2006.04084", "submitter": "Kuan Fang", "authors": "RuiXing Wang, Kuan Fang, RiKang Zhou, Zhan Shen, LiWen Fan", "title": "SERank: Optimize Sequencewise Learning to Rank Using\n  Squeeze-and-Excitation Network", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-to-rank (LTR) is a set of supervised machine learning algorithms\nthat aim at generating optimal ranking order over a list of items. A lot of\nranking models have been studied during the past decades. And most of them\ntreat each query document pair independently during training and inference.\nRecently, there are a few methods have been proposed which focused on mining\ninformation across ranking candidates list for further improvements, such as\nlearning multivariant scoring function or learning contextual embedding.\nHowever, these methods usually greatly increase computational cost during\nonline inference, especially when with large candidates size in real-world web\nsearch systems. What's more, there are few studies that focus on novel design\nof model structure for leveraging information across ranking candidates. In\nthis work, we propose an effective and efficient method named as SERank which\nis a Sequencewise Ranking model by using Squeeze-and-Excitation network to take\nadvantage of cross-document information. Moreover, we examine our proposed\nmethods on several public benchmark datasets, as well as click logs collected\nfrom a commercial Question Answering search engine, Zhihu. In addition, we also\nconduct online A/B testing at Zhihu search engine to further verify the\nproposed approach. Results on both offline datasets and online A/B testing\ndemonstrate that our method contributes to a significant improvement.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 08:29:58 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Wang", "RuiXing", ""], ["Fang", "Kuan", ""], ["Zhou", "RiKang", ""], ["Shen", "Zhan", ""], ["Fan", "LiWen", ""]]}, {"id": "2006.04148", "submitter": "Hillel Taub-Tabib", "authors": "Hillel Taub-Tabib, Micah Shlain, Shoval Sadde, Dan Lahav, Matan Eyal,\n  Yaara Cohen, Yoav Goldberg", "title": "Interactive Extractive Search over Biomedical Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that allows life-science researchers to search a\nlinguistically annotated corpus of scientific texts using patterns over\ndependency graphs, as well as using patterns over token sequences and a\npowerful variant of boolean keyword queries. In contrast to previous attempts\nto dependency-based search, we introduce a light-weight query language that\ndoes not require the user to know the details of the underlying linguistic\nrepresentations, and instead to query the corpus by providing an example\nsentence coupled with simple markup. Search is performed at an interactive\nspeed due to efficient linguistic graph-indexing and retrieval engine. This\nallows for rapid exploration, development and refinement of user queries. We\ndemonstrate the system using example workflows over two corpora: the PubMed\ncorpus including 14,446,243 PubMed abstracts and the CORD-19 dataset, a\ncollection of over 45,000 research papers focused on COVID-19 research. The\nsystem is publicly available at https://allenai.github.io/spike\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 13:26:32 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Taub-Tabib", "Hillel", ""], ["Shlain", "Micah", ""], ["Sadde", "Shoval", ""], ["Lahav", "Dan", ""], ["Eyal", "Matan", ""], ["Cohen", "Yaara", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2006.04153", "submitter": "Wenjie Wang", "authors": "Wenjie Wang, Fuli Feng, Xiangnan He, Liqiang Nie, Tat-Seng Chua", "title": "Denoising Implicit Feedback for Recommendation", "comments": null, "journal-ref": "2021. In Proceedings of the Fourteenth ACM International\n  Conference on Web Search and Data Mining (WSDM'21)", "doi": "10.1145/3437963.3441800", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of implicit feedback makes them the default choice to build\nonline recommender systems. While the large volume of implicit feedback\nalleviates the data sparsity issue, the downside is that they are not as clean\nin reflecting the actual satisfaction of users. For example, in E-commerce, a\nlarge portion of clicks do not translate to purchases, and many purchases end\nup with negative reviews. As such, it is of critical importance to account for\nthe inevitable noises in implicit feedback for recommender training. However,\nlittle work on recommendation has taken the noisy nature of implicit feedback\ninto consideration.\n  In this work, we explore the central theme of denoising implicit feedback for\nrecommender training. We find serious negative impacts of noisy implicit\nfeedback,i.e., fitting the noisy data prevents the recommender from learning\nthe actual user preference. Our target is to identify and prune noisy\ninteractions, so as to improve the quality of recommender training. By\nobserving the process of normal recommender training, we find that noisy\nfeedback typically has large loss values in the early stages. Inspired by this\nobservation, we propose a new training strategy namedAdaptive Denoising\nTraining(ADT), which adaptively prunes noisy interactions during training.\nSpecifically, we devise two paradigms for adaptive loss formulation: Truncated\nLoss that discards the large-loss samples with a dynamic threshold in each\niteration; and reweighted Loss that adaptively lowers the weight of large-loss\nsamples. We instantiate the two paradigms on the widely used binary\ncross-entropy loss and test the proposed ADT strategies on three representative\nrecommenders. Extensive experiments on three benchmarks demonstrate that ADT\nsignificantly improves the quality of recommendation over normal training.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 13:55:36 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 09:03:12 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Wenjie", ""], ["Feng", "Fuli", ""], ["He", "Xiangnan", ""], ["Nie", "Liqiang", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "2006.04164", "submitter": "Yue Xu", "authors": "Yue Xu and Hao Chen and Zengde Deng and Junxiong Zhu and Yanghua Li\n  and Peng He and Wenyao Gao and Wenjun Xu", "title": "Single-Layer Graph Convolutional Networks For Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) and their variants have received\nsignificant attention and achieved start-of-the-art performances on various\nrecommendation tasks. However, many existing GCN models tend to perform\nrecursive aggregations among all related nodes, which arises severe\ncomputational burden. Moreover, they favor multi-layer architectures in\nconjunction with complicated modeling techniques. Though effective, the\nexcessive amount of model parameters largely hinder their applications in\nreal-world recommender systems. To this end, in this paper, we propose the\nsingle-layer GCN model which is able to achieve superior performance along with\nremarkably less complexity compared with existing models. Our main contribution\nis three-fold. First, we propose a principled similarity metric named\ndistribution-aware similarity (DA similarity), which can guide the neighbor\nsampling process and evaluate the quality of the input graph explicitly. We\nalso prove that DA similarity has a positive correlation with the final\nperformance, through both theoretical analysis and empirical simulations.\nSecond, we propose a simplified GCN architecture which employs a single GCN\nlayer to aggregate information from the neighbors filtered by DA similarity and\nthen generates the node representations. Moreover, the aggregation step is a\nparameter-free operation, such that it can be done in a pre-processing manner\nto further reduce red the training and inference costs. Third, we conduct\nextensive experiments on four datasets. The results verify that the proposed\nmodel outperforms existing GCN models considerably and yields up to a few\norders of magnitude speedup in training, in terms of the recommendation\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 14:38:47 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Xu", "Yue", ""], ["Chen", "Hao", ""], ["Deng", "Zengde", ""], ["Zhu", "Junxiong", ""], ["Li", "Yanghua", ""], ["He", "Peng", ""], ["Gao", "Wenyao", ""], ["Xu", "Wenjun", ""]]}, {"id": "2006.04239", "submitter": "Chanyoung Park", "authors": "Chanyoung Park, Carl Yang, Qi Zhu, Donghyun Kim, Hwanjo Yu, Jiawei Han", "title": "Unsupervised Differentiable Multi-aspect Network Embedding", "comments": "KDD 2020 (Research Track). 9 Pages + Appendix (2 Pages). Source code\n  can be found https://github.com/pcy1302/asp2vec. Typo fixed in Fig.2", "journal-ref": null, "doi": "10.1145/3394486.3403196", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network embedding is an influential graph mining technique for representing\nnodes in a graph as distributed vectors. However, the majority of network\nembedding methods focus on learning a single vector representation for each\nnode, which has been recently criticized for not being capable of modeling\nmultiple aspects of a node. To capture the multiple aspects of each node,\nexisting studies mainly rely on offline graph clustering performed prior to the\nactual embedding, which results in the cluster membership of each node (i.e.,\nnode aspect distribution) fixed throughout training of the embedding model. We\nargue that this not only makes each node always have the same aspect\ndistribution regardless of its dynamic context, but also hinders the end-to-end\ntraining of the model that eventually leads to the final embedding quality\nlargely dependent on the clustering. In this paper, we propose a novel\nend-to-end framework for multi-aspect network embedding, called asp2vec, in\nwhich the aspects of each node are dynamically assigned based on its local\ncontext. More precisely, among multiple aspects, we dynamically assign a single\naspect to each node based on its current context, and our aspect selection\nmodule is end-to-end differentiable via the Gumbel-Softmax trick. We also\nintroduce the aspect regularization framework to capture the interactions among\nthe multiple aspects in terms of relatedness and diversity. We further\ndemonstrate that our proposed framework can be readily extended to\nheterogeneous networks. Extensive experiments towards various downstream tasks\non various types of homogeneous networks and a heterogeneous network\ndemonstrate the superiority of asp2vec.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 19:26:20 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 00:15:31 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 16:47:06 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Park", "Chanyoung", ""], ["Yang", "Carl", ""], ["Zhu", "Qi", ""], ["Kim", "Donghyun", ""], ["Yu", "Hwanjo", ""], ["Han", "Jiawei", ""]]}, {"id": "2006.04275", "submitter": "Mirko Marras", "authors": "Ludovico Boratto, Gianni Fenu, Mirko Marras", "title": "Connecting User and Item Perspectives in Popularity Debiasing for\n  Collaborative Recommendation", "comments": "Accepted in Information Processing & Management, 58(1), 102387", "journal-ref": "Information Processing & Management, 58(1), 102387 (2021)", "doi": "10.1016/j.ipm.2020.102387", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems learn from historical users' feedback that is often\nnon-uniformly distributed across items. As a consequence, these systems may end\nup suggesting popular items more than niche items progressively, even when the\nlatter would be of interest for users. This can hamper several core qualities\nof the recommended lists (e.g., novelty, coverage, diversity), impacting on the\nfuture success of the underlying platform itself. In this paper, we formalize\ntwo novel metrics that quantify how much a recommender system equally treats\nitems along the popularity tail. The first one encourages equal probability of\nbeing recommended across items, while the second one encourages true positive\nrates for items to be equal. We characterize the recommendations of\nrepresentative algorithms by means of the proposed metrics, and we show that\nthe item probability of being recommended and the item true positive rate are\nbiased against the item popularity. To promote a more equal treatment of items\nalong the popularity tail, we propose an in-processing approach aimed at\nminimizing the biased correlation between user-item relevance and item\npopularity. Extensive experiments show that, with small losses in accuracy, our\npopularity-mitigation approach leads to important gains in beyond-accuracy\nrecommendation quality.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 21:45:55 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 17:32:54 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Boratto", "Ludovico", ""], ["Fenu", "Gianni", ""], ["Marras", "Mirko", ""]]}, {"id": "2006.04279", "submitter": "Mirko Marras", "authors": "Ludovico Boratto, Gianni Fenu, Mirko Marras", "title": "Interplay between Upsampling and Regularization for Provider Fairness in\n  Recommender Systems", "comments": "Accepted in User Model User-Adap Inter", "journal-ref": "User Model User-Adap Inter. (2021)", "doi": "10.1007/s11257-021-09294-8", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the impact of recommendations on item providers is one of the\nduties of multi-sided recommender systems. Item providers are key stakeholders\nin online platforms, and their earnings and plans are influenced by the\nexposure their items receive in recommended lists. Prior work showed that\ncertain minority groups of providers, characterized by a common sensitive\nattribute (e.g., gender or race), are being disproportionately affected by\nindirect and unintentional discrimination. Our study in this paper handles a\nsituation where ($i$) the same provider is associated with multiple items of a\nlist suggested to a user, ($ii$) an item is created by more than one provider\njointly, and ($iii$) predicted user-item relevance scores are biasedly\nestimated for items of provider groups. Under this scenario, we assess\ndisparities in relevance, visibility, and exposure, by simulating diverse\nrepresentations of the minority group in the catalog and the interactions.\nBased on emerged unfair outcomes, we devise a treatment that combines\nobservation upsampling and loss regularization, while learning user-item\nrelevance scores. Experiments on real-world data demonstrate that our treatment\nleads to lower disparate relevance. The resulting recommended lists show fairer\nvisibility and exposure, higher minority item coverage, and negligible loss in\nrecommendation utility.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 22:08:23 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 22:33:47 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 06:38:08 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Boratto", "Ludovico", ""], ["Fenu", "Gianni", ""], ["Marras", "Mirko", ""]]}, {"id": "2006.04282", "submitter": "Mirko Marras", "authors": "Mirko Marras, Ludovico Boratto, Guilherme Ramos, Gianni Fenu", "title": "Equality of Learning Opportunity via Individual Fairness in Personalized\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online educational platforms are playing a primary role in mediating the\nsuccess of individuals' careers. Therefore, while building overlying content\nrecommendation services, it becomes essential to guarantee that learners are\nprovided with equal recommended learning opportunities, according to the\nplatform values, context, and pedagogy. Though the importance of ensuring\nequality of learning opportunities has been well investigated in traditional\ninstitutions, how this equality can be operationalized in online learning\necosystems through recommender systems is still under-explored. In this paper,\nwe formalize educational principles that model recommendations' learning\nproperties, and a novel fairness metric that combines them in order to monitor\nthe equality of recommended learning opportunities among learners. Then, we\nenvision a scenario wherein an educational platform should be arranged in such\na way that the generated recommendations meet each principle to a certain\ndegree for all learners, constrained to their individual preferences. Under\nthis view, we explore the learning opportunities provided by recommender\nsystems in a large-scale course platform, uncovering systematic inequalities.\nTo reduce this effect, we propose a novel post-processing approach that\nbalances personalization and equality of recommended opportunities. Experiments\nshow that our approach leads to higher equality, with a negligible loss in\npersonalization. Our study moves a step forward in operationalizing the ethics\nof human learning in recommendations, a core unit of intelligent educational\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 22:26:51 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 01:45:18 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Marras", "Mirko", ""], ["Boratto", "Ludovico", ""], ["Ramos", "Guilherme", ""], ["Fenu", "Gianni", ""]]}, {"id": "2006.04373", "submitter": "Qiaosheng Zhang", "authors": "Qiaosheng Zhang, Geewon Suh, Changho Suh, Vincent Y. F. Tan", "title": "MC2G: An Efficient Algorithm for Matrix Completion with Social and Item\n  Similarity Graphs", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2021.3052033", "report-no": null, "categories": "cs.LG cs.IR cs.IT eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design and analyze MC2G (Matrix Completion with 2 Graphs),\nan algorithm that performs matrix completion in the presence of social and item\nsimilarity graphs. MC2G runs in quasilinear time and is parameter free. It is\nbased on spectral clustering and local refinement steps. The expected number of\nsampled entries required for MC2G to succeed (i.e., recover the clusters in the\ngraphs and complete the matrix) matches an information-theoretic lower bound up\nto a constant factor for a wide range of parameters. We show via extensive\nexperiments on both synthetic and real datasets that MC2G outperforms other\nstate-of-the-art matrix completion algorithms that leverage graph side\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 06:11:37 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 04:29:32 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhang", "Qiaosheng", ""], ["Suh", "Geewon", ""], ["Suh", "Changho", ""], ["Tan", "Vincent Y. F.", ""]]}, {"id": "2006.04380", "submitter": "Zhi Li", "authors": "Zhi Li, Bo Wu, Qi Liu, Likang Wu, Hongke Zhao, Tao Mei", "title": "Learning the Compositional Visual Coherence for Complementary\n  Recommendations", "comments": "Early version accepted by IJCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complementary recommendations, which aim at providing users product\nsuggestions that are supplementary and compatible with their obtained items,\nhave become a hot topic in both academia and industry in recent years.\n%However, it is challenging due to its complexity and subjectivity. Existing\nwork mainly focused on modeling the co-purchased relations between two items,\nbut the compositional associations of item collections are largely unexplored.\nActually, when a user chooses the complementary items for the purchased\nproducts, it is intuitive that she will consider the visual semantic coherence\n(such as color collocations, texture compatibilities) in addition to global\nimpressions. Towards this end, in this paper, we propose a novel Content\nAttentive Neural Network (CANN) to model the comprehensive compositional\ncoherence on both global contents and semantic contents. Specifically, we first\npropose a \\textit{Global Coherence Learning} (GCL) module based on multi-heads\nattention to model the global compositional coherence. Then, we generate the\nsemantic-focal representations from different semantic regions and design a\n\\textit{Focal Coherence Learning} (FCL) module to learn the focal compositional\ncoherence from different semantic-focal representations. Finally, we optimize\nthe CANN in a novel compositional optimization strategy. Extensive experiments\non the large-scale real-world data clearly demonstrate the effectiveness of\nCANN compared with several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 06:57:18 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Li", "Zhi", ""], ["Wu", "Bo", ""], ["Liu", "Qi", ""], ["Wu", "Likang", ""], ["Zhao", "Hongke", ""], ["Mei", "Tao", ""]]}, {"id": "2006.04466", "submitter": "Weiyu Cheng", "authors": "Weiyu Cheng, Yanyan Shen, Linpeng Huang", "title": "Differentiable Neural Input Search for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent factor models are the driving forces of the state-of-the-art\nrecommender systems, with an important insight of vectorizing raw input\nfeatures into dense embeddings. The dimensions of different feature embeddings\nare often set to a same value empirically, which limits the predictive\nperformance of latent factor models. Existing works have proposed heuristic or\nreinforcement learning-based methods to search for mixed feature embedding\ndimensions. For efficiency concern, these methods typically choose embedding\ndimensions from a restricted set of candidate dimensions. However, this\nrestriction will hurt the flexibility of dimension selection, leading to\nsuboptimal performance of search results. In this paper, we propose\nDifferentiable Neural Input Search (DNIS), a method that searches for mixed\nfeature embedding dimensions in a more flexible space through continuous\nrelaxation and differentiable optimization. The key idea is to introduce a soft\nselection layer that controls the significance of each embedding dimension, and\noptimize this layer according to model's validation performance. DNIS is\nmodel-agnostic and thus can be seamlessly incorporated with existing latent\nfactor models for recommendation. We conduct experiments with various\narchitectures of latent factor models on three public real-world datasets for\nrating prediction, Click-Through-Rate (CTR) prediction, and top-k item\nrecommendation. The results demonstrate that our method achieves the best\npredictive performance compared with existing neural input search approaches\nwith fewer embedding parameters and less time cost.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 10:43:59 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 11:23:13 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Cheng", "Weiyu", ""], ["Shen", "Yanyan", ""], ["Huang", "Linpeng", ""]]}, {"id": "2006.04520", "submitter": "Liang Zhao", "authors": "Yifei Zhao, Yu-Hang Zhou, Mingdong Ou, Huan Xu, Nan Li", "title": "Maximizing Cumulative User Engagement in Sequential Recommendation: An\n  Online Optimization Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To maximize cumulative user engagement (e.g. cumulative clicks) in sequential\nrecommendation, it is often needed to tradeoff two potentially conflicting\nobjectives, that is, pursuing higher immediate user engagement (e.g.,\nclick-through rate) and encouraging user browsing (i.e., more items exposured).\nExisting works often study these two tasks separately, thus tend to result in\nsub-optimal results. In this paper, we study this problem from an online\noptimization perspective, and propose a flexible and practical framework to\nexplicitly tradeoff longer user browsing length and high immediate user\nengagement. Specifically, by considering items as actions, user's requests as\nstates and user leaving as an absorbing state, we formulate each user's\nbehavior as a personalized Markov decision process (MDP), and the problem of\nmaximizing cumulative user engagement is reduced to a stochastic shortest path\n(SSP) problem. Meanwhile, with immediate user engagement and quit probability\nestimation, it is shown that the SSP problem can be efficiently solved via\ndynamic programming. Experiments on real-world datasets demonstrate the\neffectiveness of the proposed approach. Moreover, this approach is deployed at\na large E-commerce platform, achieved over 7% improvement of cumulative clicks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 09:02:51 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Zhao", "Yifei", ""], ["Zhou", "Yu-Hang", ""], ["Ou", "Mingdong", ""], ["Xu", "Huan", ""], ["Li", "Nan", ""]]}, {"id": "2006.04530", "submitter": "Shoujin Wang", "authors": "Shoujin Wang, Longbing Cao, Liang Hu, Shlomo Berkovsky, Xiaoshui\n  Huang, Lin Xiao, Wenpeng Lu", "title": "Jointly Modeling Intra- and Inter-transaction Dependencies with\n  Hierarchical Attentive Transaction Embeddings for Next-item Recommendation", "comments": "Accepted by IEEE Intelligent Systems", "journal-ref": null, "doi": "10.1109/MIS.2020.2997362", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A transaction-based recommender system (TBRS) aims to predict the next item\nby modeling dependencies in transactional data. Generally, two kinds of\ndependencies considered are intra-transaction dependency and inter-transaction\ndependency. Most existing TBRSs recommend next item by only modeling the\nintra-transaction dependency within the current transaction while ignoring\ninter-transaction dependency with recent transactions that may also affect the\nnext item. However, as not all recent transactions are relevant to the current\nand next items, the relevant ones should be identified and prioritized. In this\npaper, we propose a novel hierarchical attentive transaction embedding (HATE)\nmodel to tackle these issues. Specifically, a two-level attention mechanism\nintegrates both item embedding and transaction embedding to build an attentive\ncontext representation that incorporates both intraand inter-transaction\ndependencies. With the learned context representation, HATE then recommends the\nnext item. Experimental evaluations on two real-world transaction datasets show\nthat HATE significantly outperforms the state-ofthe-art methods in terms of\nrecommendation accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 14:04:19 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Wang", "Shoujin", ""], ["Cao", "Longbing", ""], ["Hu", "Liang", ""], ["Berkovsky", "Shlomo", ""], ["Huang", "Xiaoshui", ""], ["Xiao", "Lin", ""], ["Lu", "Wenpeng", ""]]}, {"id": "2006.04532", "submitter": "Edward Gehringer", "authors": "Yunkai Xiao, Gabriel Zingle, Qinjin Jia, Harsh R. Shah, Yi Zhang,\n  Tianyi Li, Mohsin Karovaliya, Weixiang Zhao, Yang Song, Jie Ji, Ashwin\n  Balasubramaniam, Harshit Patel, Priyankha Bhalasubbramanian, Vikram Patel,\n  and Edward F. Gehringer", "title": "Detecting Problem Statements in Peer Assessments", "comments": "8 pages, 9 images. Extended version of a paper published at EDM 2020,\n  13th International Conference on Educational Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective peer assessment requires students to be attentive to the\ndeficiencies in the work they rate. Thus, their reviews should identify\nproblems. But what ways are there to check that they do? We attempt to automate\nthe process of deciding whether a review comment detects a problem. We use over\n18,000 review comments that were labeled by the reviewees as either detecting\nor not detecting a problem with the work. We deploy several traditional\nmachine-learning models, as well as neural-network models using GloVe and BERT\nembeddings. We find that the best performer is the Hierarchical Attention\nNetwork classifier, followed by the Bidirectional Gated Recurrent Units (GRU)\nAttention and Capsule model with scores of 93.1% and 90.5% respectively. The\nbest non-neural network model was the support vector machine with a score of\n89.71%. This is followed by the Stochastic Gradient Descent model and the\nLogistic Regression model with 89.70% and 88.98%.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 03:03:46 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Xiao", "Yunkai", ""], ["Zingle", "Gabriel", ""], ["Jia", "Qinjin", ""], ["Shah", "Harsh R.", ""], ["Zhang", "Yi", ""], ["Li", "Tianyi", ""], ["Karovaliya", "Mohsin", ""], ["Zhao", "Weixiang", ""], ["Song", "Yang", ""], ["Ji", "Jie", ""], ["Balasubramaniam", "Ashwin", ""], ["Patel", "Harshit", ""], ["Bhalasubbramanian", "Priyankha", ""], ["Patel", "Vikram", ""], ["Gehringer", "Edward F.", ""]]}, {"id": "2006.04660", "submitter": "Rajdeep Mukherjee", "authors": "Rajdeep Mukherjee, Hari Chandana Peruri, Uppada Vishnu, Pawan Goyal,\n  Sourangshu Bhattacharya, Niloy Ganguly", "title": "Read what you need: Controllable Aspect-based Opinion Summarization of\n  Tourist Reviews", "comments": "4 pages, accepted in the Proceedings of the 43rd International ACM\n  SIGIR Conference on Research and Development in Information Retrieval\n  (SIGIR), 2020", "journal-ref": null, "doi": "10.1145/3397271.3401269", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manually extracting relevant aspects and opinions from large volumes of\nuser-generated text is a time-consuming process. Summaries, on the other hand,\nhelp readers with limited time budgets to quickly consume the key ideas from\nthe data. State-of-the-art approaches for multi-document summarization,\nhowever, do not consider user preferences while generating summaries. In this\nwork, we argue the need and propose a solution for generating personalized\naspect-based opinion summaries from large collections of online tourist\nreviews. We let our readers decide and control several attributes of the\nsummary such as the length and specific aspects of interest among others.\nSpecifically, we take an unsupervised approach to extract coherent aspects from\ntourist reviews posted on TripAdvisor. We then propose an Integer Linear\nProgramming (ILP) based extractive technique to select an informative subset of\nopinions around the identified aspects while respecting the user-specified\nvalues for various control parameters. Finally, we evaluate and compare our\nsummaries using crowdsourcing and ROUGE-based metrics and obtain competitive\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 15:03:38 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 07:22:48 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Mukherjee", "Rajdeep", ""], ["Peruri", "Hari Chandana", ""], ["Vishnu", "Uppada", ""], ["Goyal", "Pawan", ""], ["Bhattacharya", "Sourangshu", ""], ["Ganguly", "Niloy", ""]]}, {"id": "2006.04680", "submitter": "Aftab Anjum", "authors": "Aftab Anjum, Mazharul Islam, Lin Wang", "title": "Dimensionality Reduction for Sentiment Classification: Evolving for the\n  Most Prominent and Separable Features", "comments": "Pages 1-14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sentiment classification, the enormous amount of textual data, its immense\ndimensionality, and inherent noise make it extremely difficult for machine\nlearning classifiers to extract high-level and complex abstractions. In order\nto make the data less sparse and more statistically significant, the\ndimensionality reduction techniques are needed. But in the existing\ndimensionality reduction techniques, the number of components needs to be set\nmanually which results in loss of the most prominent features, thus reducing\nthe performance of the classifiers. Our prior work, i.e., Term Presence Count\n(TPC) and Term Presence Ratio (TPR) have proven to be effective techniques as\nthey reject the less separable features. However, the most prominent and\nseparable features might still get removed from the initial feature set despite\nhaving higher distributions among positive and negative tagged documents. To\novercome this problem, we have proposed a new framework that consists of\ntwo-dimensionality reduction techniques i.e., Sentiment Term Presence Count\n(SentiTPC) and Sentiment Term Presence Ratio (SentiTPR). These techniques\nreject the features by considering term presence difference for SentiTPC and\nratio of the distribution distinction for SentiTPR. Additionally, these methods\nalso analyze the total distribution information. Extensive experimental results\nexhibit that the proposed framework reduces the feature dimension by a large\nscale, and thus significantly improve the classification performance.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 09:46:52 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Anjum", "Aftab", ""], ["Islam", "Mazharul", ""], ["Wang", "Lin", ""]]}, {"id": "2006.04803", "submitter": "Omar Abdul Wahab", "authors": "Omar Abdel Wahab, Jamal Bentahar, Robin Cohen, Hadi Otrok, Azzam\n  Mourad", "title": "A two-level solution to fight against dishonest opinions in\n  recommendation-based trust systems", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a mechanism to deal with dishonest opinions in\nrecommendation-based trust models, at both the collection and processing\nlevels. We consider a scenario in which an agent requests recommendations from\nmultiple parties to build trust toward another agent. At the collection level,\nwe propose to allow agents to self-assess the accuracy of their recommendations\nand autonomously decide on whether they would participate in the recommendation\nprocess or not. At the processing level, we propose a recommendations\naggregation technique that is resilient to collusion attacks, followed by a\ncredibility update mechanism for the participating agents. The originality of\nour work stems from its consideration of dishonest opinions at both the\ncollection and processing levels, which allows for better and more persistent\nprotection against dishonest recommenders. Experiments conducted on the\nEpinions dataset show that our solution yields better performance in protecting\nthe recommendation process against Sybil attacks, in comparison with a\ncompeting model that derives the optimal network of advisors based on the\nagents' trust values.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 00:34:11 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Wahab", "Omar Abdel", ""], ["Bentahar", "Jamal", ""], ["Cohen", "Robin", ""], ["Otrok", "Hadi", ""], ["Mourad", "Azzam", ""]]}, {"id": "2006.05009", "submitter": "Jiahua Liu", "authors": "Shi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul Bennett,\n  Jianfeng Gao, Zhiyuan Liu", "title": "Few-Shot Generative Conversational Query Rewriting", "comments": "Accepted by SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational query rewriting aims to reformulate a concise conversational\nquery to a fully specified, context-independent query that can be effectively\nhandled by existing information retrieval systems. This paper presents a\nfew-shot generative approach to conversational query rewriting. We develop two\nmethods, based on rules and self-supervised learning, to generate weak\nsupervision data using large amounts of ad hoc search sessions, and to\nfine-tune GPT-2 to rewrite conversational queries. On the TREC Conversational\nAssistance Track, our weakly supervised GPT-2 rewriter improves the\nstate-of-the-art ranking accuracy by 12%, only using very limited amounts of\nmanual query rewrites. In the zero-shot learning setting, the rewriter still\ngives a comparable result to previous state-of-the-art systems. Our analyses\nreveal that GPT-2 effectively picks up the task syntax and learns to capture\ncontext dependencies, even for hard cases that involve group references and\nlong-turn dependencies.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 01:47:58 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Yu", "Shi", ""], ["Liu", "Jiahua", ""], ["Yang", "Jingqin", ""], ["Xiong", "Chenyan", ""], ["Bennett", "Paul", ""], ["Gao", "Jianfeng", ""], ["Liu", "Zhiyuan", ""]]}, {"id": "2006.05312", "submitter": "Dafang Zou", "authors": "Dafang Zou and Leiming Zhang and Jiafa Mao and Weiguo Sheng", "title": "Feature Interaction based Neural Network for Click-Through Rate\n  Prediction", "comments": "10 pages, 5 figure. arXiv admin note: text overlap with\n  arXiv:1905.09433 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-Through Rate (CTR) prediction is one of the most important and\nchallenging in calculating advertisements and recommendation systems. To build\na machine learning system with these data, it is important to properly model\nthe interaction among features. However, many current works calculate the\nfeature interactions in a simple way such as inner product and element-wise\nproduct. This paper aims to fully utilize the information between features and\nimprove the performance of deep neural networks in the CTR prediction task. In\nthis paper, we propose a Feature Interaction based Neural Network (FINN) which\nis able to model feature interaction via a 3-dimention relation tensor. FINN\nprovides representations for the feature interactions on the the bottom layer\nand the non-linearity of neural network in modelling higher-order feature\ninteractions. We evaluate our models on CTR prediction tasks compared with\nclassical baselines and show that our deep FINN model outperforms other\nstate-of-the-art deep models such as PNN and DeepFM. Evaluation results\ndemonstrate that feature interaction contains significant information for\nbetter CTR prediction. It also indicates that our models can effectively learn\nthe feature interactions, and achieve better performances in real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 03:53:24 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zou", "Dafang", ""], ["Zhang", "Leiming", ""], ["Mao", "Jiafa", ""], ["Sheng", "Weiguo", ""]]}, {"id": "2006.05324", "submitter": "Bhaskar Mitra", "authors": "Nick Craswell, Daniel Campos, Bhaskar Mitra, Emine Yilmaz and Bodo\n  Billerbeck", "title": "ORCAS: 18 Million Clicked Query-Document Pairs for Analyzing Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users of Web search engines reveal their information needs through queries\nand clicks, making click logs a useful asset for information retrieval.\nHowever, click logs have not been publicly released for academic use, because\nthey can be too revealing of personally or commercially sensitive information.\nThis paper describes a click data release related to the TREC Deep Learning\nTrack document corpus. After aggregation and filtering, including a k-anonymity\nrequirement, we find 1.4 million of the TREC DL URLs have 18 million\nconnections to 10 million distinct queries. Our dataset of these queries and\nconnections to TREC documents is of similar size to proprietary datasets used\nin previous papers on query mining and ranking. We perform some preliminary\nexperiments using the click data to augment the TREC DL training data, offering\nby comparison: 28x more queries, with 49x more connections to 4.4x more URLs in\nthe corpus. We present a description of the dataset's generation process,\ncharacteristics, use in ranking and suggest other potential uses.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 14:58:21 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 14:45:00 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Craswell", "Nick", ""], ["Campos", "Daniel", ""], ["Mitra", "Bhaskar", ""], ["Yilmaz", "Emine", ""], ["Billerbeck", "Bodo", ""]]}, {"id": "2006.05557", "submitter": "Xinyi Zhou", "authors": "Xinyi Zhou, Apurva Mulay, Emilio Ferrara, Reza Zafarani", "title": "ReCOVery: A Multimodal Repository for COVID-19 News Credibility Research", "comments": "Proceedings of the 29th ACM International Conference on Information\n  and Knowledge Management (CIKM '20)", "journal-ref": null, "doi": "10.1145/3340531.3412880", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First identified in Wuhan, China, in December 2019, the outbreak of COVID-19\nhas been declared as a global emergency in January, and a pandemic in March\n2020 by the World Health Organization (WHO). Along with this pandemic, we are\nalso experiencing an \"infodemic\" of information with low credibility such as\nfake news and conspiracies. In this work, we present ReCOVery, a repository\ndesigned and constructed to facilitate research on combating such information\nregarding COVID-19. We first broadly search and investigate ~2,000 news\npublishers, from which 60 are identified with extreme [high or low] levels of\ncredibility. By inheriting the credibility of the media on which they were\npublished, a total of 2,029 news articles on coronavirus, published from\nJanuary to May 2020, are collected in the repository, along with 140,820 tweets\nthat reveal how these news articles have spread on the Twitter social network.\nThe repository provides multimodal information of news articles on coronavirus,\nincluding textual, visual, temporal, and network information. The way that news\ncredibility is obtained allows a trade-off between dataset scalability and\nlabel accuracy. Extensive experiments are conducted to present data statistics\nand distributions, as well as to provide baseline performances for predicting\nnews credibility so that future methods can be compared. Our repository is\navailable at http://coronavirus-fakenews.com.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 23:40:51 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 20:54:46 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Zhou", "Xinyi", ""], ["Mulay", "Apurva", ""], ["Ferrara", "Emilio", ""], ["Zafarani", "Reza", ""]]}, {"id": "2006.05563", "submitter": "Dung Thai", "authors": "Dung Thai, Zhiyang Xu, Nicholas Monath, Boris Veytsman, Andrew\n  McCallum", "title": "Using BibTeX to Automatically Generate Labeled Data for Citation Field\n  Extraction", "comments": null, "journal-ref": "AKBC 2020", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate parsing of citation reference strings is crucial to automatically\nconstruct scholarly databases such as Google Scholar or Semantic Scholar.\nCitation field extraction (CFE) is precisely this task---given a reference\nlabel which tokens refer to the authors, venue, title, editor, journal, pages,\netc. Most methods for CFE are supervised and rely on training from labeled\ndatasets that are quite small compared to the great variety of reference\nformats. BibTeX, the widely used reference management tool, provides a natural\nmethod to automatically generate and label training data for CFE. In this\npaper, we describe a technique for using BibTeX to generate, automatically, a\nlarge-scale 41M labeled strings), labeled dataset, that is four orders of\nmagnitude larger than the current largest CFE dataset, namely the UMass\nCitation Field Extraction dataset [Anzaroot and McCallum, 2013]. We\nexperimentally demonstrate how our dataset can be used to improve the\nperformance of the UMass CFE using a RoBERTa-based [Liu et al., 2019] model. In\ncomparison to previous SoTA, we achieve a 24.48% relative error reduction,\nachieving span level F1-scores of 96.3%.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 23:54:10 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Thai", "Dung", ""], ["Xu", "Zhiyang", ""], ["Monath", "Nicholas", ""], ["Veytsman", "Boris", ""], ["McCallum", "Andrew", ""]]}, {"id": "2006.05586", "submitter": "Lei Zhu", "authors": "Lei Zhu, Hui Cui, Zhiyong Cheng, Jingjing Li, Zheng Zhang", "title": "Dual-level Semantic Transfer Deep Hashing for Efficient Social Image\n  Retrieval", "comments": "Accepted by IEEE TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network stores and disseminates a tremendous amount of user shared\nimages. Deep hashing is an efficient indexing technique to support large-scale\nsocial image retrieval, due to its deep representation capability, fast\nretrieval speed and low storage cost. Particularly, unsupervised deep hashing\nhas well scalability as it does not require any manually labelled data for\ntraining. However, owing to the lacking of label guidance, existing methods\nsuffer from severe semantic shortage when optimizing a large amount of deep\nneural network parameters. Differently, in this paper, we propose a Dual-level\nSemantic Transfer Deep Hashing (DSTDH) method to alleviate this problem with a\nunified deep hash learning framework. Our model targets at learning the\nsemantically enhanced deep hash codes by specially exploiting the\nuser-generated tags associated with the social images. Specifically, we design\na complementary dual-level semantic transfer mechanism to efficiently discover\nthe potential semantics of tags and seamlessly transfer them into binary hash\ncodes. On the one hand, instance-level semantics are directly preserved into\nhash codes from the associated tags with adverse noise removing. Besides, an\nimage-concept hypergraph is constructed for indirectly transferring the latent\nhigh-order semantic correlations of images and tags into hash codes. Moreover,\nthe hash codes are obtained simultaneously with the deep representation\nlearning by the discrete hash optimization strategy. Extensive experiments on\ntwo public social image retrieval datasets validate the superior performance of\nour method compared with state-of-the-art hashing methods. The source codes of\nour method can be obtained at https://github.com/research2020-1/DSTDH\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 01:03:09 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zhu", "Lei", ""], ["Cui", "Hui", ""], ["Cheng", "Zhiyong", ""], ["Li", "Jingjing", ""], ["Zhang", "Zheng", ""]]}, {"id": "2006.05639", "submitter": "Guorui Zhou", "authors": "Pi Qi, Xiaoqiang Zhu, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren,\n  Ying Fan, and Kun Gai", "title": "Search-based User Interest Modeling with Lifelong Sequential Behavior\n  Data for Click-Through Rate Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rich user behavior data has been proven to be of great value for\nclick-through rate prediction tasks, especially in industrial applications such\nas recommender systems and online advertising. Both industry and academy have\npaid much attention to this topic and propose different approaches to modeling\nwith long sequential user behavior data. Among them, memory network based model\nMIMN proposed by Alibaba, achieves SOTA with the co-design of both learning\nalgorithm and serving system. MIMN is the first industrial solution that can\nmodel sequential user behavior data with length scaling up to 1000. However,\nMIMN fails to precisely capture user interests given a specific candidate item\nwhen the length of user behavior sequence increases further, say, by 10 times\nor more. This challenge exists widely in previously proposed approaches. In\nthis paper, we tackle this problem by designing a new modeling paradigm, which\nwe name as Search-based Interest Model (SIM). SIM extracts user interests with\ntwo cascaded search units: (i) General Search Unit acts as a general search\nfrom the raw and arbitrary long sequential behavior data, with query\ninformation from candidate item, and gets a Sub user Behavior Sequence which is\nrelevant to candidate item; (ii) Exact Search Unit models the precise\nrelationship between candidate item and SBS. This cascaded search paradigm\nenables SIM with a better ability to model lifelong sequential behavior data in\nboth scalability and accuracy. Apart from the learning algorithm, we also\nintroduce our hands-on experience on how to implement SIM in large scale\nindustrial systems. Since 2019, SIM has been deployed in the display\nadvertising system in Alibaba, bringing 7.1\\% CTR and 4.4\\% RPM lift, which is\nsignificant to the business. Serving the main traffic in our real system now,\nSIM models user behavior data with maximum length reaching up to 54000, pushing\nSOTA to 54x.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 03:41:15 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 03:27:18 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Qi", "Pi", ""], ["Zhu", "Xiaoqiang", ""], ["Zhou", "Guorui", ""], ["Zhang", "Yujing", ""], ["Wang", "Zhe", ""], ["Ren", "Lejian", ""], ["Fan", "Ying", ""], ["Gai", "Kun", ""]]}, {"id": "2006.05645", "submitter": "Ilya Amburg", "authors": "Ilya Amburg, Nate Veldt, Austin R. Benson", "title": "Hypergraph Clustering for Finding Diverse and Experienced Groups", "comments": "Added new experiments and refocused around diversity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When forming a team or group of individuals, we often seek a balance of\nexpertise in a particular task while at the same time maintaining diversity of\nskills within each group. Here, we view the problem of finding diverse and\nexperienced groups as clustering in hypergraphs with multiple edge types. The\ninput data is a hypergraph with multiple hyperedge types -- representing\ninformation about past experiences of groups of individuals -- and the output\nis groups of nodes. In contrast to related problems on fair or balanced\nclustering, we model diversity in terms of variety of past experience (instead\nof, e.g., protected attributes), with a goal of forming groups that have both\nexperience and diversity with respect to participation in edge types. In other\nwords, both diversity and experience are measured from the types of the\nhyperedges.\n  Our clustering model is based on a regularized version of an edge-based\nhypergraph clustering objective, and we also show how naive objectives actually\nhave no diversity-experience tradeoff. Although our objective function is\nNP-hard to optimize, we design an efficient 2-approximation algorithm and also\nshow how to compute bounds for the regularization hyperparameter that lead to\nmeaningful diversity-experience tradeoffs. We demonstrate an application of\nthis framework in online review platforms, where the goal is to curate sets of\nuser reviews for a product type. In this context, \"experience\" corresponds to\nusers familiar with the type of product, and \"diversity\" to users that have\nreviewed related products.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 04:12:02 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 14:59:51 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 02:37:35 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Amburg", "Ilya", ""], ["Veldt", "Nate", ""], ["Benson", "Austin R.", ""]]}, {"id": "2006.05700", "submitter": "Sourav Garg", "authors": "Sourav Garg, Ben Harwood, Gaurangi Anand and Michael Milford", "title": "Delta Descriptors: Change-Based Place Representation for Robust Visual\n  Localization", "comments": "8 pages and 7 figures. Published in 2020 IEEE Robotics and Automation\n  Letters (RA-L)", "journal-ref": null, "doi": "10.1109/LRA.2020.3005627", "report-no": null, "categories": "cs.CV cs.IR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual place recognition is challenging because there are so many factors\nthat can cause the appearance of a place to change, from day-night cycles to\nseasonal change to atmospheric conditions. In recent years a large range of\napproaches have been developed to address this challenge including deep-learnt\nimage descriptors, domain translation, and sequential filtering, all with\nshortcomings including generality and velocity-sensitivity. In this paper we\npropose a novel descriptor derived from tracking changes in any learned global\ndescriptor over time, dubbed Delta Descriptors. Delta Descriptors mitigate the\noffsets induced in the original descriptor matching space in an unsupervised\nmanner by considering temporal differences across places observed along a\nroute. Like all other approaches, Delta Descriptors have a shortcoming -\nvolatility on a frame to frame basis - which can be overcome by combining them\nwith sequential filtering methods. Using two benchmark datasets, we first\ndemonstrate the high performance of Delta Descriptors in isolation, before\nshowing new state-of-the-art performance when combined with sequence-based\nmatching. We also present results demonstrating the approach working with four\ndifferent underlying descriptor types, and two other beneficial properties of\nDelta Descriptors in comparison to existing techniques: their increased\ninherent robustness to variations in camera motion and a reduced rate of\nperformance degradation as dimensional reduction is applied. Source code is\nmade available at https://github.com/oravus/DeltaDescriptors.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 07:37:29 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 07:24:52 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Garg", "Sourav", ""], ["Harwood", "Ben", ""], ["Anand", "Gaurangi", ""], ["Milford", "Michael", ""]]}, {"id": "2006.05866", "submitter": "Jia Wu", "authors": "Qi Huang, Junshuai Yu, Jia Wu, Bin Wang", "title": "Heterogeneous Graph Attention Networks for Early Detection of Rumors on\n  Twitter", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of mobile Internet technology and the widespread\nuse of mobile devices, it becomes much easier for people to express their\nopinions on social media. The openness and convenience of social media\nplatforms provide a free expression for people but also cause new social\nproblems. The widespread of false rumors on social media can bring about the\npanic of the public and damage personal reputation, which makes rumor automatic\ndetection technology become particularly necessary. The majority of existing\nmethods for rumor detection focus on mining effective features from text\ncontents, user profiles, and patterns of propagation. Nevertheless, these\nmethods do not take full advantage of global semantic relations of the text\ncontents, which characterize the semantic commonality of rumors as a key factor\nfor detecting rumors. In this paper, we construct a tweet-word-user\nheterogeneous graph based on the text contents and the source tweet\npropagations of rumors. A meta-path based heterogeneous graph attention network\nframework is proposed to capture the global semantic relations of text\ncontents, together with the global structure information of source tweet\npropagations for rumor detection. Experiments on real-world Twitter data\ndemonstrate the superiority of the proposed approach, which also has a\ncomparable ability to detect rumors at a very early stage.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 14:49:08 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Huang", "Qi", ""], ["Yu", "Junshuai", ""], ["Wu", "Jia", ""], ["Wang", "Bin", ""]]}, {"id": "2006.05908", "submitter": "Hansi Hettiarachchi", "authors": "Hansi Hettiarachchi, Mariam Adedoyin-Olowe, Jagdev Bhogal and Mohamed\n  Medhat Gaber", "title": "Embed2Detect: Temporally Clustered Embedded Words for Event Detection in\n  Social Media", "comments": "This is a preprint of an article published in the Journal of Machine\n  Learning, Springer. The final authenticated version is available online at:\n  https://doi.org/10.1007/s10994-021-05988-7", "journal-ref": null, "doi": "10.1007/s10994-021-05988-7", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media is becoming a primary medium to discuss what is happening around\nthe world. Therefore, the data generated by social media platforms contain rich\ninformation which describes the ongoing events. Further, the timeliness\nassociated with these data is capable of facilitating immediate insights.\nHowever, considering the dynamic nature and high volume of data production in\nsocial media data streams, it is impractical to filter the events manually and\ntherefore, automated event detection mechanisms are invaluable to the\ncommunity. Apart from a few notable exceptions, most previous research on\nautomated event detection have focused only on statistical and syntactical\nfeatures in data and lacked the involvement of underlying semantics which are\nimportant for effective information retrieval from text since they represent\nthe connections between words and their meanings. In this paper, we propose a\nnovel method termed Embed2Detect for event detection in social media by\ncombining the characteristics in word embeddings and hierarchical agglomerative\nclustering. The adoption of word embeddings gives Embed2Detect the capability\nto incorporate powerful semantical features into event detection and overcome a\nmajor limitation inherent in previous approaches. We experimented our method on\ntwo recent real social media data sets which represent the sports and political\ndomain and also compared the results to several state-of-the-art methods. The\nobtained results show that Embed2Detect is capable of effective and efficient\nevent detection and it outperforms the recent event detection methods. For the\nsports data set, Embed2Detect achieved 27% higher F-measure than the\nbest-performed baseline and for the political data set, it was an increase of\n29%.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 15:52:52 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 08:47:29 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 13:48:35 GMT"}, {"version": "v4", "created": "Tue, 25 May 2021 21:49:41 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Hettiarachchi", "Hansi", ""], ["Adedoyin-Olowe", "Mariam", ""], ["Bhogal", "Jagdev", ""], ["Gaber", "Mohamed Medhat", ""]]}, {"id": "2006.05933", "submitter": "Yuanxing Zhang", "authors": "Pengyu Zhao, Kecheng Xiao, Yuanxing Zhang, Kaigui Bian, Wei Yan", "title": "AMER: Automatic Behavior Modeling and Interaction Exploration in\n  Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User behavior and feature interactions are crucial in deep learning-based\nrecommender systems. There has been a diverse set of behavior modeling and\ninteraction exploration methods in the literature. Nevertheless, the design of\ntask-aware recommender systems still requires feature engineering and\narchitecture engineering from domain experts. In this work, we introduce AMER,\nnamely Automatic behavior Modeling and interaction Exploration in Recommender\nsystems with Neural Architecture Search (NAS). The core contributions of AMER\ninclude the three-stage search space and the tailored three-step searching\npipeline. In the first step, AMER searches for residual blocks that incorporate\ncommonly used operations in the block-wise search space of stage 1 to model\nsequential patterns in user behavior. In the second step, it progressively\ninvestigates useful low-order and high-order feature interactions in the\nnon-sequential interaction space of stage 2. Finally, an aggregation\nmulti-layer perceptron (MLP) with shortcut connection is selected from flexible\ndimension settings of stage~3 to combine features extracted from the previous\nsteps. For efficient and effective NAS, AMER employs the one-shot random search\nin all three steps. Further analysis reveals that AMER's search space could\ncover most of the representative behavior extraction and interaction\ninvestigation methods, which demonstrates the universality of our design. The\nextensive experimental results over various scenarios reveal that AMER could\noutperform competitive baselines with elaborate feature engineering and\narchitecture engineering, indicating both effectiveness and robustness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 16:41:58 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zhao", "Pengyu", ""], ["Xiao", "Kecheng", ""], ["Zhang", "Yuanxing", ""], ["Bian", "Kaigui", ""], ["Yan", "Wei", ""]]}, {"id": "2006.06251", "submitter": "Rajaa El Hamdani", "authors": "Paul Boniol, George Panagopoulos, Christos Xypolopoulos, Rajaa El\n  Hamdani, David Restrepo Amariles, Michalis Vazirgiannis", "title": "Performance in the Courtroom: Automated Processing and Visualization of\n  Appeal Court Decisions in France", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence techniques are already popular and important in the\nlegal domain. We extract legal indicators from judicial judgment to decrease\nthe asymmetry of information of the legal system and the access-to-justice gap.\nWe use NLP methods to extract interesting entities/data from judgments to\nconstruct networks of lawyers and judgments. We propose metrics to rank lawyers\nbased on their experience, wins/loss ratio and their importance in the network\nof lawyers. We also perform community detection in the network of judgments and\npropose metrics to represent the difficulty of cases capitalising on\ncommunities features.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 08:22:59 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 09:17:36 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 19:47:27 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Boniol", "Paul", ""], ["Panagopoulos", "George", ""], ["Xypolopoulos", "Christos", ""], ["Hamdani", "Rajaa El", ""], ["Amariles", "David Restrepo", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "2006.06867", "submitter": "Onur Varol", "authors": "Mohsen Sayyadiharikandeh, Onur Varol, Kai-Cheng Yang, Alessandro\n  Flammini, Filippo Menczer", "title": "Detection of Novel Social Bots by Ensembles of Specialized Classifiers", "comments": "8 pages, 10 figures, Accepted to CIKM'20", "journal-ref": "Proc. 29th ACM International Conference on Information and\n  Knowledge Management (CIKM), pages 2725-2732, 2020", "doi": "10.1145/3340531.3412698", "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malicious actors create inauthentic social media accounts controlled in part\nby algorithms, known as social bots, to disseminate misinformation and agitate\nonline discussion. While researchers have developed sophisticated methods to\ndetect abuse, novel bots with diverse behaviors evade detection. We show that\ndifferent types of bots are characterized by different behavioral features. As\na result, supervised learning techniques suffer severe performance\ndeterioration when attempting to detect behaviors not observed in the training\ndata. Moreover, tuning these models to recognize novel bots requires retraining\nwith a significant amount of new annotations, which are expensive to obtain. To\naddress these issues, we propose a new supervised learning method that trains\nclassifiers specialized for each class of bots and combines their decisions\nthrough the maximum rule. The ensemble of specialized classifiers (ESC) can\nbetter generalize, leading to an average improvement of 56\\% in F1 score for\nunseen accounts across datasets. Furthermore, novel bot behaviors are learned\nwith fewer labeled examples during retraining. We deployed ESC in the newest\nversion of Botometer, a popular tool to detect social bots in the wild, with a\ncross-validation AUC of 0.99.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 22:59:59 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 20:04:21 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Sayyadiharikandeh", "Mohsen", ""], ["Varol", "Onur", ""], ["Yang", "Kai-Cheng", ""], ["Flammini", "Alessandro", ""], ["Menczer", "Filippo", ""]]}, {"id": "2006.06877", "submitter": "Daniel King", "authors": "Daniel King, Doug Downey, Daniel S. Weld", "title": "High-Precision Extraction of Emerging Concepts from Scientific\n  Literature", "comments": "Accepted to SIGIR 2020", "journal-ref": "Proceedings of the 43rd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (2020) 1549-1552", "doi": "10.1145/3397271.3401235", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of new concepts in scientific literature can help power\nfaceted search, scientific trend analysis, knowledge-base construction, and\nmore, but current methods are lacking. Manual identification cannot keep up\nwith the torrent of new publications, while the precision of existing automatic\ntechniques is too low for many applications. We present an unsupervised concept\nextraction method for scientific literature that achieves much higher precision\nthan previous work. Our approach relies on a simple but novel intuition: each\nscientific concept is likely to be introduced or popularized by a single paper\nthat is disproportionately cited by subsequent papers mentioning the concept.\nFrom a corpus of computer science papers on arXiv, we find that our method\nachieves a Precision@1000 of 99%, compared to 86% for prior work, and a\nsubstantially better precision-yield trade-off across the top 15,000\nextractions. To stimulate research in this area, we release our code and data\n(https://github.com/allenai/ForeCite).\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 23:48:27 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["King", "Daniel", ""], ["Downey", "Doug", ""], ["Weld", "Daniel S.", ""]]}, {"id": "2006.06894", "submitter": "Natasha Noy", "authors": "Omar Benjelloun and Shiyu Chen and Natasha Noy", "title": "Google Dataset Search by the Numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists, governments, and companies increasingly publish datasets on the\nWeb. Google's Dataset Search extracts dataset metadata -- expressed using\nschema.org and similar vocabularies -- from Web pages in order to make datasets\ndiscoverable. Since we started the work on Dataset Search in 2016, the number\nof datasets described in schema.org has grown from about 500K to almost 30M.\nThus, this corpus has become a valuable snapshot of data on the Web. To the\nbest of our knowledge, this corpus is the largest and most diverse of its kind.\nWe analyze this corpus and discuss where the datasets originate from, what\ntopics they cover, which form they take, and what people searching for datasets\nare interested in. Based on this analysis, we identify gaps and possible future\nwork to help make data more discoverable.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 00:54:15 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Benjelloun", "Omar", ""], ["Chen", "Shiyu", ""], ["Noy", "Natasha", ""]]}, {"id": "2006.06922", "submitter": "Deqing Yang", "authors": "Wenjing Meng and Deqing Yang and Yanghua Xiao", "title": "Incorporating User Micro-behaviors and Item Knowledge into Multi-task\n  Learning for Session-based Recommendation", "comments": null, "journal-ref": "SIGIR 2020", "doi": "10.1145/3397271.3401098", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommendation (SR) has become an important and popular\ncomponent of various e-commerce platforms, which aims to predict the next\ninteracted item based on a given session. Most of existing SR models only focus\non exploiting the consecutive items in a session interacted by a certain user,\nto capture the transition pattern among the items. Although some of them have\nbeen proven effective, the following two insights are often neglected. First, a\nuser's micro-behaviors, such as the manner in which the user locates an item,\nthe activities that the user commits on an item (e.g., reading comments, adding\nto cart), offer fine-grained and deep understanding of the user's preference.\nSecond, the item attributes, also known as item knowledge, provide side\ninformation to model the transition pattern among interacted items and\nalleviate the data sparsity problem. These insights motivate us to propose a\nnovel SR model MKM-SR in this paper, which incorporates user Micro-behaviors\nand item Knowledge into Multi-task learning for Session-based Recommendation.\nSpecifically, a given session is modeled on micro-behavior level in MKM-SR,\ni.e., with a sequence of item-operation pairs rather than a sequence of items,\nto capture the transition pattern in the session sufficiently. Furthermore, we\npropose a multi-task learning paradigm to involve learning knowledge embeddings\nwhich plays a role as an auxiliary task to promote the major task of SR. It\nenables our model to obtain better session representations, resulting in more\nprecise SR recommendation results. The extensive evaluations on two benchmark\ndatasets demonstrate MKM-SR's superiority over the state-of-the-art SR models,\njustifying the strategy of incorporating knowledge learning.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 03:06:23 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Meng", "Wenjing", ""], ["Yang", "Deqing", ""], ["Xiao", "Yanghua", ""]]}, {"id": "2006.06963", "submitter": "Neil G. Marchant", "authors": "Neil G. Marchant and Benjamin I. P. Rubinstein", "title": "Needle in a Haystack: Label-Efficient Evaluation under Extreme Class\n  Imbalance", "comments": "30 pages, 8 figures, updated to match version accepted for\n  publication at KDD'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important tasks like record linkage and extreme classification demonstrate\nextreme class imbalance, with 1 minority instance to every 1 million or more\nmajority instances. Obtaining a sufficient sample of all classes, even just to\nachieve statistically-significant evaluation, is so challenging that most\ncurrent approaches yield poor estimates or incur impractical cost. Where\nimportance sampling has been levied against this challenge, restrictive\nconstraints are placed on performance metrics, estimates do not come with\nappropriate guarantees, or evaluations cannot adapt to incoming labels. This\npaper develops a framework for online evaluation based on adaptive importance\nsampling. Given a target performance metric and model for $p(y|x)$, the\nframework adapts a distribution over items to label in order to maximize\nstatistical precision. We establish strong consistency and a central limit\ntheorem for the resulting performance estimates, and instantiate our framework\nwith worked examples that leverage Dirichlet-tree models. Experiments\ndemonstrate an average MSE superior to state-of-the-art on fixed label budgets.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 06:17:26 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 07:33:19 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Marchant", "Neil G.", ""], ["Rubinstein", "Benjamin I. P.", ""]]}, {"id": "2006.07017", "submitter": "Wei Wang", "authors": "Junshu Jiang and Songyun Ye and Wei Wang and Jingran Xu and Xiaosheng\n  Luo", "title": "Learning Effective Representations for Person-Job Fit by Feature Fusion", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person-job fit is to match candidates and job posts on online recruitment\nplatforms using machine learning algorithms. The effectiveness of matching\nalgorithms heavily depends on the learned representations for the candidates\nand job posts. In this paper, we propose to learn comprehensive and effective\nrepresentations of the candidates and job posts via feature fusion. First, in\naddition to applying deep learning models for processing the free text in\nresumes and job posts, which is adopted by existing methods, we extract\nsemantic entities from the whole resume (and job post) and then learn features\nfor them. By fusing the features from the free text and the entities, we get a\ncomprehensive representation for the information explicitly stated in the\nresume and job post. Second, however, some information of a candidate or a job\nmay not be explicitly captured in the resume or job post. Nonetheless, the\nhistorical applications including accepted and rejected cases can reveal some\nimplicit intentions of the candidates or recruiters. Therefore, we propose to\nlearn the representations of implicit intentions by processing the historical\napplications using LSTM. Last, by fusing the representations for the explicit\nand implicit intentions, we get a more comprehensive and effective\nrepresentation for person-job fit. Experiments over 10 months real data show\nthat our solution outperforms existing methods with a large margin. Ablation\nstudies confirm the contribution of each component of the fused representation.\nThe extracted semantic entities help interpret the matching results during the\ncase study.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 09:02:41 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Jiang", "Junshu", ""], ["Ye", "Songyun", ""], ["Wang", "Wei", ""], ["Xu", "Jingran", ""], ["Luo", "Xiaosheng", ""]]}, {"id": "2006.07283", "submitter": "Erik Tjong Kim Sang", "authors": "Shihan Wang, Marijn Schraagen, Erik Tjong Kim Sang and Mehdi Dastani", "title": "Dutch General Public Reaction on Governmental COVID-19 Measures and\n  Announcements in Twitter Data", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public sentiment (the opinions, attitudes or feelings expressed by the\npublic) is a factor of interest for government, as it directly influences the\nimplementation of policies. Given the unprecedented nature of the COVID-19\ncrisis, having an up-to-date representation of public sentiment on governmental\nmeasures and announcements is crucial. While the 'staying-at-home' policy makes\nface-to-face interactions and interviews challenging, analysing real-time\nTwitter data that reflects public opinion toward policy measures is a\ncost-effective way to access public sentiment. In this context, we collect\nstreaming data using the Twitter API starting from the COVID-19 outbreak in the\nNetherlands in February 2020, and track Dutch general public reactions on\ngovernmental measures and announcements. We provide temporal analysis of tweet\nfrequency and public sentiment over the past seven months. We also identify\npublic attitudes towards two Dutch policies in case studies: one regarding\nsocial distancing and one regarding wearing face masks. By presenting those\npreliminary results, we aim to provide visibility into the social media\ndiscussions around COVID-19 to the general public, scientists and policy\nmakers. The data collection and analysis will be updated and expanded over\ntime.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 16:03:58 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 10:10:35 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 19:53:55 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Wang", "Shihan", ""], ["Schraagen", "Marijn", ""], ["Sang", "Erik Tjong Kim", ""], ["Dastani", "Mehdi", ""]]}, {"id": "2006.07515", "submitter": "Bruna Wundervald", "authors": "Bruna Wundervald, Andrew Parnell and Katarina Domijan", "title": "Generalizing Gain Penalization for Feature Selection in Tree-based\n  Models", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new approach for feature selection via gain penalization in\ntree-based models. First, we show that previous methods do not perform\nsufficient regularization and often exhibit sub-optimal out-of-sample\nperformance, especially when correlated features are present. Instead, we\ndevelop a new gain penalization idea that exhibits a general local-global\nregularization for tree-based models. The new method allows for more\nflexibility in the choice of feature-specific importance weights. We validate\nour method on both simulated and real data and implement itas an extension of\nthe popular R package ranger.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 23:55:52 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Wundervald", "Bruna", ""], ["Parnell", "Andrew", ""], ["Domijan", "Katarina", ""]]}, {"id": "2006.07548", "submitter": "Hamed Zamani", "authors": "Helia Hashemi, Hamed Zamani, W. Bruce Croft", "title": "Guided Transformer: Leveraging Multiple External Sources for\n  Representation Learning in Conversational Search", "comments": "To appear in the Proceedings of ACM SIGIR 2020. 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asking clarifying questions in response to ambiguous or faceted queries has\nbeen recognized as a useful technique for various information retrieval\nsystems, especially conversational search systems with limited bandwidth\ninterfaces. Analyzing and generating clarifying questions have been studied\nrecently but the accurate utilization of user responses to clarifying questions\nhas been relatively less explored. In this paper, we enrich the representations\nlearned by Transformer networks using a novel attention mechanism from external\ninformation sources that weights each term in the conversation. We evaluate\nthis Guided Transformer model in a conversational search scenario that includes\nclarifying questions. In our experiments, we use two separate external sources,\nincluding the top retrieved documents and a set of different possible\nclarifying questions for the query. We implement the proposed representation\nlearning model for two downstream tasks in conversational search; document\nretrieval and next clarifying question selection. Our experiments use a public\ndataset for search clarification and demonstrate significant improvements\ncompared to competitive baselines.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 03:24:53 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Hashemi", "Helia", ""], ["Zamani", "Hamed", ""], ["Croft", "W. Bruce", ""]]}, {"id": "2006.07581", "submitter": "Ming Gong", "authors": "Linjun Shou, Shining Bo, Feixiang Cheng, Ming Gong, Jian Pei, Daxin\n  Jiang", "title": "Mining Implicit Relevance Feedback from User Behavior for Web Question\n  Answering", "comments": "Accepted by KDD 2020", "journal-ref": null, "doi": "10.1145/3394486.3403343", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training and refreshing a web-scale Question Answering (QA) system for a\nmulti-lingual commercial search engine often requires a huge amount of training\nexamples. One principled idea is to mine implicit relevance feedback from user\nbehavior recorded in search engine logs. All previous works on mining implicit\nrelevance feedback target at relevance of web documents rather than passages.\nDue to several unique characteristics of QA tasks, the existing user behavior\nmodels for web documents cannot be applied to infer passage relevance. In this\npaper, we make the first study to explore the correlation between user behavior\nand passage relevance, and propose a novel approach for mining training data\nfor Web QA. We conduct extensive experiments on four test datasets and the\nresults show our approach significantly improves the accuracy of passage\nranking without extra human labeled data. In practice, this work has proved\neffective to substantially reduce the human labeling cost for the QA service in\na global commercial search engine, especially for languages with low resources.\nOur techniques have been deployed in multi-language services.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 07:02:08 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 01:10:48 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Shou", "Linjun", ""], ["Bo", "Shining", ""], ["Cheng", "Feixiang", ""], ["Gong", "Ming", ""], ["Pei", "Jian", ""], ["Jiang", "Daxin", ""]]}, {"id": "2006.07934", "submitter": "Yuanjiang Cao", "authors": "Yuanjiang Cao, Xiaocong Chen, Lina Yao, Xianzhi Wang and Wei Emma\n  Zhang", "title": "Adversarial Attacks and Detection on Reinforcement Learning-Based\n  Interactive Recommender Systems", "comments": null, "journal-ref": null, "doi": "10.1145/3397271.3401196", "report-no": null, "categories": "cs.LG cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks pose significant challenges for detecting adversarial\nattacks at an early stage. We propose attack-agnostic detection on\nreinforcement learning-based interactive recommendation systems. We first craft\nadversarial examples to show their diverse distributions and then augment\nrecommendation systems by detecting potential attacks with a deep\nlearning-based classifier based on the crafted data. Finally, we study the\nattack strength and frequency of adversarial examples and evaluate our model on\nstandard datasets with multiple crafting methods. Our extensive experiments\nshow that most adversarial attacks are effective, and both attack strength and\nattack frequency impact the attack performance. The strategically-timed attack\nachieves comparative attack performance with only 1/3 to 1/2 attack frequency.\nBesides, our black-box detector trained with one crafting method has the\ngeneralization ability over several crafting methods.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 15:41:47 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Cao", "Yuanjiang", ""], ["Chen", "Xiaocong", ""], ["Yao", "Lina", ""], ["Wang", "Xianzhi", ""], ["Zhang", "Wei Emma", ""]]}, {"id": "2006.07954", "submitter": "Alexander Veretennikov Borisovich", "authors": "Alexander B. Veretennikov", "title": "An efficient algorithm for three-component key index construction", "comments": "Indexing: Web of Science, Scopus", "journal-ref": "Vestnik Udmurtskogo Universiteta. Matematika. Mekhanika.\n  Komp'yuternye Nauki, 2019, vol. 29, issue 1, pp. 117-132", "doi": "10.20537/vm190111", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, proximity full-text searches in large text arrays are\nconsidered. A search query consists of several words. The search result is a\nlist of documents containing these words. In a modern search system, documents\nthat contain search query words that are near each other are more relevant than\ndocuments that do not share this trait. To solve this task, for each word in\neach indexed document, we need to store a record in the index. In this case,\nthe query search time is proportional to the number of occurrences of the\nqueried words in the indexed documents. Consequently, it is common for search\nsystems to evaluate queries that contain frequently occurring words much more\nslowly than queries that contain less frequently occurring, ordinary words. For\neach word in the text, we use additional indexes to store information about\nnearby words at distances from the given word of less than or equal to\nMaxDistance, which is a parameter. This parameter can take a value of 5, 7, or\neven more. Three-component key indexes can be created for faster query\nexecution. Previously, we presented the results of experiments showing that\nwhen queries contain very frequently occurring words, the average time of the\nquery execution with three-component key indexes is 94.7 times less than that\nrequired when using ordinary inverted indexes. In the current work, we describe\na new three-component key index building algorithm and demonstrate the\ncorrectness of the algorithm. We present the results of experiments creating\nsuch an index that is dependent on the value of MaxDistance.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 16:52:07 GMT"}], "update_date": "2020-06-28", "authors_parsed": [["Veretennikov", "Alexander B.", ""]]}, {"id": "2006.08055", "submitter": "Theja Tulabandhula", "authors": "Theja Tulabandhula, Deeksha Sinha, Prasoon Patidar", "title": "Multi-Purchase Behavior: Modeling and Optimization", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of modeling purchase of multiple items and utilizing it\nto display optimized recommendations, which is a central problem for online\ne-commerce platforms. Rich personalized modeling of users and fast computation\nof optimal products to display given these models can lead to significantly\nhigher revenues and simultaneously enhance the end user experience. We present\na parsimonious multi-purchase family of choice models called the BundleMVL-K\nfamily, and develop a binary search based iterative strategy that efficiently\ncomputes optimized recommendations for this model. This is one of the first\nattempts at operationalizing multi-purchase class of choice models. We\ncharacterize structural properties of the optimal solution, which allow one to\ndecide if a product is part of the optimal assortment in constant time,\nreducing the size of the instance that needs to be solved computationally. We\nalso establish the hardness of computing optimal recommendation sets. We show\none of the first quantitative links between modeling multiple purchase behavior\nand revenue gains. The efficacy of our modeling and optimization techniques\ncompared to competing solutions is shown using several real world datasets on\nmultiple metrics such as model fitness, expected revenue gains and run-time\nreductions. The benefit of taking multiple purchases into account is observed\nto be $6-8\\%$ in relative terms for the Ta Feng and UCI shopping datasets when\ncompared to the MNL model for instances with $\\sim 1500$ products.\nAdditionally, across $8$ real world datasets, the test log-likelihood fits of\nour models are on average $17\\%$ better in relative terms. The simplicity of\nour models and the iterative nature of our optimization technique allows\npractitioners meet stringent computational constraints while increasing their\nrevenues in practical recommendation applications at scale.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 23:47:14 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Tulabandhula", "Theja", ""], ["Sinha", "Deeksha", ""], ["Patidar", "Prasoon", ""]]}, {"id": "2006.08108", "submitter": "Derek Lim", "authors": "Derek Lim, Austin R. Benson", "title": "Expertise and Dynamics within Crowdsourced Musical Knowledge Curation: A\n  Case Study of the Genius Platform", "comments": "12 pages. 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many platforms collect crowdsourced information primarily from volunteers. As\nthis type of knowledge curation has become widespread, contribution formats\nvary substantially and are driven by diverse processes across differing\nplatforms. Thus, models for one platform are not necessarily applicable to\nothers. Here, we study the temporal dynamics of Genius, a platform primarily\ndesigned for user-contributed annotations of song lyrics. A unique aspect of\nGenius is that the annotations are extremely local -- an annotated lyric may\njust be a few lines of a song -- but also highly related, e.g., by song, album,\nartist, or genre. We analyze several dynamical processes associated with lyric\nannotations and their edits, which differ substantially from models for other\nplatforms. For example, expertise on song annotations follows a \"U shape\" where\nexperts are both early and late contributors with non-experts contributing\nintermediately; we develop a user utility model that captures such behavior. We\nalso find several contribution traits appearing early in a user's lifespan of\ncontributions that distinguish (eventual) experts from non-experts. Combining\nour findings, we develop a model for early prediction of user expertise.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 03:19:47 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 02:45:43 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Lim", "Derek", ""], ["Benson", "Austin R.", ""]]}, {"id": "2006.08281", "submitter": "Tomasz Dwojak", "authors": "Tomasz Dwojak and Micha{\\l} Pietruszka and {\\L}ukasz Borchmann and\n  Filip Grali\\'nski and Jakub Ch{\\l}\\k{e}dowski", "title": "On the Multi-Property Extraction and Beyond", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the Dual-source Transformer architecture on the\nWikiReading information extraction and machine reading comprehension dataset.\nThe proposed model outperforms the current state-of-the-art by a large margin.\nNext, we introduce WikiReading Recycled - a newly developed public dataset,\nsupporting the task of multiple property extraction. It keeps the spirit of the\noriginal WikiReading but does not inherit the identified disadvantages of its\npredecessor.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 11:07:52 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Dwojak", "Tomasz", ""], ["Pietruszka", "Micha\u0142", ""], ["Borchmann", "\u0141ukasz", ""], ["Grali\u0144ski", "Filip", ""], ["Ch\u0142\u0119dowski", "Jakub", ""]]}, {"id": "2006.08386", "submitter": "Xavier Favory", "authors": "Xavier Favory, Konstantinos Drossos, Tuomas Virtanen and Xavier Serra", "title": "COALA: Co-Aligned Autoencoders for Learning Semantically Enriched Audio\n  Representations", "comments": "8 pages, 1 figure, workshop on Self-supervision in Audio and Speech\n  at the 37th International Conference on Machine Learning (ICML), 2020,\n  Vienna, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Audio representation learning based on deep neural networks (DNNs) emerged as\nan alternative approach to hand-crafted features. For achieving high\nperformance, DNNs often need a large amount of annotated data which can be\ndifficult and costly to obtain. In this paper, we propose a method for learning\naudio representations, aligning the learned latent representations of audio and\nassociated tags. Aligning is done by maximizing the agreement of the latent\nrepresentations of audio and tags, using a contrastive loss. The result is an\naudio embedding model which reflects acoustic and semantic characteristics of\nsounds. We evaluate the quality of our embedding model, measuring its\nperformance as a feature extractor on three different tasks (namely, sound\nevent recognition, and music genre and musical instrument classification), and\ninvestigate what type of characteristics the model captures. Our results are\npromising, sometimes in par with the state-of-the-art in the considered tasks\nand the embeddings produced with our method are well correlated with some\nacoustic descriptors.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 13:17:18 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 08:33:42 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Favory", "Xavier", ""], ["Drossos", "Konstantinos", ""], ["Virtanen", "Tuomas", ""], ["Serra", "Xavier", ""]]}, {"id": "2006.08672", "submitter": "Ingrid Nunes", "authors": "Ingrid Nunes and Dietmar Jannach", "title": "A systematic review and taxonomy of explanations in decision support and\n  recommender systems", "comments": null, "journal-ref": "User Modeling and User-Adapted Interaction, 27 (3-5), 393-444\n  (2017)", "doi": "10.1007/s11257-017-9195-0", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the recent advances in the field of artificial intelligence, an\nincreasing number of decision-making tasks are delegated to software systems. A\nkey requirement for the success and adoption of such systems is that users must\ntrust system choices or even fully automated decisions. To achieve this,\nexplanation facilities have been widely investigated as a means of establishing\ntrust in these systems since the early years of expert systems. With today's\nincreasingly sophisticated machine learning algorithms, new challenges in the\ncontext of explanations, accountability, and trust towards such systems\nconstantly arise. In this work, we systematically review the literature on\nexplanations in advice-giving systems. This is a family of systems that\nincludes recommender systems, which is one of the most successful classes of\nadvice-giving software in practice. We investigate the purposes of explanations\nas well as how they are generated, presented to users, and evaluated. As a\nresult, we derive a novel comprehensive taxonomy of aspects to be considered\nwhen designing explanation facilities for current and future decision support\nsystems. The taxonomy includes a variety of different facets, such as\nexplanation objective, responsiveness, content and presentation. Moreover, we\nidentified several challenges that remain unaddressed so far, for example\nrelated to fine-grained issues associated with the presentation of explanations\nand how explanation facilities are evaluated.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 18:19:20 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Nunes", "Ingrid", ""], ["Jannach", "Dietmar", ""]]}, {"id": "2006.08732", "submitter": "Shuo Zhang", "authors": "Shuo Zhang and Krisztian Balog", "title": "Evaluating Conversational Recommender Systems via User Simulation", "comments": "Proceedings of the 26th ACM SIGKDD Conference on Knowledge Discovery\n  and Data Mining (KDD '20), 2020", "journal-ref": null, "doi": "10.1145/3394486.3403202", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational information access is an emerging research area. Currently,\nhuman evaluation is used for end-to-end system evaluation, which is both very\ntime and resource intensive at scale, and thus becomes a bottleneck of\nprogress. As an alternative, we propose automated evaluation by means of\nsimulating users. Our user simulator aims to generate responses that a real\nhuman would give by considering both individual preferences and the general\nflow of interaction with the system. We evaluate our simulation approach on an\nitem recommendation task by comparing three existing conversational recommender\nsystems. We show that preference modeling and task-specific interaction models\nboth contribute to more realistic simulations, and can help achieve high\ncorrelation between automatic evaluation measures and manual human assessments.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 20:05:39 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Zhang", "Shuo", ""], ["Balog", "Krisztian", ""]]}, {"id": "2006.08805", "submitter": "Oznur Alkan", "authors": "Oznur Alkan and Elizabeth Daly", "title": "User Profiling from Reviews for Accurate Time-Based Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are a valuable way to engage users in a system, increase\nparticipation and show them resources they may not have found otherwise. One\nsignificant challenge is that user interests may change over time and certain\nitems have an inherently temporal aspect. As a result, a recommender system\nshould try and take into account the time-dependant user-item relationships.\nHowever, temporal aspects of a user profile may not always be explicitly\navailable and so we may need to infer this information from available\nresources. Product reviews on sites, such as Amazon, represent a valuable data\nsource to understand why someone bought an item and potentially who the item is\nfor. This information can then be used to construct a dynamic user profile. In\nthis paper, we demonstrate utilising reviews to extract temporal information to\ninfer the \\textit{age category preference} of users, and leverage this feature\nto generate time-dependent recommendations. Given the predictable and yet\nshifting nature of age and time, we show that, recommendations generated using\nthis dynamic aspect lead to higher accuracy compared with techniques from state\nof art. Mining temporally related content in reviews can enable the recommender\nto go beyond finding similar items or users to potentially predict a future\nneed of a user.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 22:23:17 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Alkan", "Oznur", ""], ["Daly", "Elizabeth", ""]]}, {"id": "2006.08849", "submitter": "Haoxing Lin", "authors": "Haoxing Lin, Rufan Bai, Weijia Jia, Xinyu Yang, Yongjian You", "title": "Preserving Dynamic Attention for Long-Term Spatial-Temporal Prediction", "comments": "11 pages, an ACM SIGKDD 2020 paper", "journal-ref": null, "doi": "10.1145/3394486.3403046", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective long-term predictions have been increasingly demanded in urban-wise\ndata mining systems. Many practical applications, such as accident prevention\nand resource pre-allocation, require an extended period for preparation.\nHowever, challenges come as long-term prediction is highly error-sensitive,\nwhich becomes more critical when predicting urban-wise phenomena with\ncomplicated and dynamic spatial-temporal correlation. Specifically, since the\namount of valuable correlation is limited, enormous irrelevant features\nintroduce noises that trigger increased prediction errors. Besides, after each\ntime step, the errors can traverse through the correlations and reach the\nspatial-temporal positions in every future prediction, leading to significant\nerror propagation. To address these issues, we propose a Dynamic\nSwitch-Attention Network (DSAN) with a novel Multi-Space Attention (MSA)\nmechanism that measures the correlations between inputs and outputs explicitly.\nTo filter out irrelevant noises and alleviate the error propagation, DSAN\ndynamically extracts valuable information by applying self-attention over the\nnoisy input and bridges each output directly to the purified inputs via\nimplementing a switch-attention mechanism. Through extensive experiments on two\nspatial-temporal prediction tasks, we demonstrate the superior advantage of\nDSAN in both short-term and long-term predictions.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 00:56:43 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Lin", "Haoxing", ""], ["Bai", "Rufan", ""], ["Jia", "Weijia", ""], ["Yang", "Xinyu", ""], ["You", "Yongjian", ""]]}, {"id": "2006.08893", "submitter": "Xiaomei Huang", "authors": "Guoqiong Liao, Xiaomei Huang, Neal N. Xiong, and Changxuan Wan", "title": "An Intelligent Group Event Recommendation System in Social networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of contexts has been widely recognized in recommender systems\nfor individuals. However, most existing group recommendation models in\nEvent-Based Social Networks (EBSNs) focus on how to aggregate group members'\npreferences to form group preferences. In these models, the influence of\ncontexts on groups is considered but simply defined in a manual way, which\ncannot model the complex and deep interactions between contexts and groups. In\nthis paper, we propose an Attention-based Context-aware Group Event\nRecommendation model (ACGER) in EBSNs. ACGER models the deep, non-linear\ninfluence of contexts on users, groups, and events through multi-layer neural\nnetworks. Especially, a novel attention mechanism is designed to enable the\ninfluence weights of contexts on users/groups change dynamically with the\nevents concerned. Considering that groups may have completely different\nbehavior patterns from group members, we propose that the preference of a group\nneed to be obtained from indirect and direct perspectives (called indirect\npreference and direct preference respectively). In order to obtain the indirect\npreference, we propose a method of aggregating preferences based on attention\nmechanism. Compared with existing predefined strategies, this method can\nflexibly adapt the strategy according to the events concerned by the group. In\norder to obtain the direct preference, we employ neural networks to directly\nlearn it from group-event interactions. Furthermore, to make full use of rich\nuser-event interactions in EBSNs, we integrate the context-aware individual\nrecommendation task into ACGER, which enhances the accuracy of learning of user\nembeddings and event embeddings. Extensive experiments on two real datasets\nfrom Meetup show that our model ACGER significantly outperforms the\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 03:09:21 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Liao", "Guoqiong", ""], ["Huang", "Xiaomei", ""], ["Xiong", "Neal N.", ""], ["Wan", "Changxuan", ""]]}, {"id": "2006.08904", "submitter": "J. Felipe Montano-Campos", "authors": "Victor Zitian Chen, Felipe Montano-Campos and Wlodek Zadrozny", "title": "Causal Knowledge Extraction from Scholarly Papers in Social Sciences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scale and scope of scholarly articles today are overwhelming human\nresearchers who seek to timely digest and synthesize knowledge. In this paper,\nwe seek to develop natural language processing (NLP) models to accelerate the\nspeed of extraction of relationships from scholarly papers in social sciences,\nidentify hypotheses from these papers, and extract the cause-and-effect\nentities. Specifically, we develop models to 1) classify sentences in scholarly\ndocuments in business and management as hypotheses (hypothesis classification),\n2) classify these hypotheses as causal relationships or not (causality\nclassification), and, if they are causal, 3) extract the cause and effect\nentities from these hypotheses (entity extraction). We have achieved high\nperformance for all the three tasks using different modeling techniques. Our\napproach may be generalizable to scholarly documents in a wide range of social\nsciences, as well as other types of textual materials.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 03:37:40 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Chen", "Victor Zitian", ""], ["Montano-Campos", "Felipe", ""], ["Zadrozny", "Wlodek", ""]]}, {"id": "2006.09119", "submitter": "Samin Mohammadi", "authors": "Samin Mohammadi, Mathieu Chapon, Arthur Fremond", "title": "Query Intent Detection from the SEO Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Google users have different intents from their queries such as acquiring\ninformation, buying products, comparing or simulating services, looking for\nproducts, and so on. Understanding the right intention of users helps to\nprovide i) better content on web pages from the Search Engine Optimization\n(SEO) perspective and ii) more user-satisfying results from the search engine\nperspective. In this study, we aim to identify the user query's intent by\ntaking advantage of Google results and machine learning methods. Our proposed\napproach is a clustering model that exploits some features to detect query's\nintent. A list of keywords extracted from the clustered queries is used to\nidentify the intent of a new given query. Comparing the clustering results with\nthe intents predicted by filtered keywords show the efficiency of the extracted\nkeywords for detecting intents.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 13:08:29 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Mohammadi", "Samin", ""], ["Chapon", "Mathieu", ""], ["Fremond", "Arthur", ""]]}, {"id": "2006.09174", "submitter": "Anastasios Nentidis", "authors": "Anastasios Nentidis, Konstantinos Bougiatiotis, Anastasia Krithara,\n  Georgios Paliouras", "title": "Results of the seventh edition of the BioASQ Challenge", "comments": "17 pages, 2 figures", "journal-ref": "Cellier P., Driessens K. (eds) Machine Learning and Knowledge\n  Discovery in Databases. ECML PKDD 2019. Communications in Computer and\n  Information Science, vol 1168. Springer, Cham", "doi": "10.1007/978-3-030-43887-6_51", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The results of the seventh edition of the BioASQ challenge are presented in\nthis paper. The aim of the BioASQ challenge is the promotion of systems and\nmethodologies through the organization of a challenge on the tasks of\nlarge-scale biomedical semantic indexing and question answering. In total, 30\nteams with more than 100 systems participated in the challenge this year. As in\nprevious years, the best systems were able to outperform the strong baselines.\nThis suggests that state-of-the-art systems are continuously improving, pushing\nthe frontier of research.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 14:23:27 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Nentidis", "Anastasios", ""], ["Bougiatiotis", "Konstantinos", ""], ["Krithara", "Anastasia", ""], ["Paliouras", "Georgios", ""]]}, {"id": "2006.09438", "submitter": "Noveen Sachdeva", "authors": "Noveen Sachdeva, Yi Su, Thorsten Joachims", "title": "Off-policy Bandits with Deficient Support", "comments": "11 pages, 6 figures. Accepted for publication at KDD '20 (Research\n  track)", "journal-ref": null, "doi": "10.1145/3394486.3403139", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning effective contextual-bandit policies from past actions of a deployed\nsystem is highly desirable in many settings (e.g. voice assistants,\nrecommendation, search), since it enables the reuse of large amounts of log\ndata. State-of-the-art methods for such off-policy learning, however, are based\non inverse propensity score (IPS) weighting. A key theoretical requirement of\nIPS weighting is that the policy that logged the data has \"full support\", which\ntypically translates into requiring non-zero probability for any action in any\ncontext. Unfortunately, many real-world systems produce support deficient data,\nespecially when the action space is large, and we show how existing methods can\nfail catastrophically. To overcome this gap between theory and applications, we\nidentify three approaches that provide various guarantees for IPS-based\nlearning despite the inherent limitations of support-deficient data:\nrestricting the action space, reward extrapolation, and restricting the policy\nspace. We systematically analyze the statistical and computational properties\nof these three approaches, and we empirically evaluate their effectiveness. In\naddition to providing the first systematic analysis of support-deficiency in\ncontextual-bandit learning, we conclude with recommendations that provide\npractical guidance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 18:30:02 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Sachdeva", "Noveen", ""], ["Su", "Yi", ""], ["Joachims", "Thorsten", ""]]}, {"id": "2006.09595", "submitter": "Andre Esteva", "authors": "Andre Esteva, Anuprit Kale, Romain Paulus, Kazuma Hashimoto, Wenpeng\n  Yin, Dragomir Radev, Richard Socher", "title": "CO-Search: COVID-19 Information Retrieval with Semantic Search, Question\n  Answering, and Abstractive Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 global pandemic has resulted in international efforts to\nunderstand, track, and mitigate the disease, yielding a significant corpus of\nCOVID-19 and SARS-CoV-2-related publications across scientific disciplines. As\nof May 2020, 128,000 coronavirus-related publications have been collected\nthrough the COVID-19 Open Research Dataset Challenge. Here we present\nCO-Search, a retriever-ranker semantic search engine designed to handle complex\nqueries over the COVID-19 literature, potentially aiding overburdened health\nworkers in finding scientific answers during a time of crisis. The retriever is\nbuilt from a Siamese-BERT encoder that is linearly composed with a TF-IDF\nvectorizer, and reciprocal-rank fused with a BM25 vectorizer. The ranker is\ncomposed of a multi-hop question-answering module, that together with a\nmulti-paragraph abstractive summarizer adjust retriever scores. To account for\nthe domain-specific and relatively limited dataset, we generate a bipartite\ngraph of document paragraphs and citations, creating 1.3 million (citation\ntitle, paragraph) tuples for training the encoder. We evaluate our system on\nthe data of the TREC-COVID information retrieval challenge. CO-Search obtains\ntop performance on the datasets of the first and second rounds, across several\nkey metrics: normalized discounted cumulative gain, precision, mean average\nprecision, and binary preference.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 01:32:48 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Esteva", "Andre", ""], ["Kale", "Anuprit", ""], ["Paulus", "Romain", ""], ["Hashimoto", "Kazuma", ""], ["Yin", "Wenpeng", ""], ["Radev", "Dragomir", ""], ["Socher", "Richard", ""]]}, {"id": "2006.09640", "submitter": "Karn Watcharasupat", "authors": "Karn Watcharasupat, Siddharth Gururani and Alexander Lerch", "title": "Visual Attention for Musical Instrument Recognition", "comments": "6 pages, 7 figures. Karn Watcharasupat is currently with the School\n  of Electrical and Electronic Engineering, Nanyang Technological University.\n  This work was done while she was with the Center for Music Technology,\n  Georgia Institute of Technology on an exchange semester", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.IR cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of music information retrieval, the task of simultaneously\nidentifying the presence or absence of multiple musical instruments in a\npolyphonic recording remains a hard problem. Previous works have seen some\nsuccess in improving instrument classification by applying temporal attention\nin a multi-instance multi-label setting, while another series of work has also\nsuggested the role of pitch and timbre in improving instrument recognition\nperformance. In this project, we further explore the use of attention mechanism\nin a timbral-temporal sense, \\`a la visual attention, to improve the\nperformance of musical instrument recognition using weakly-labeled data. Two\napproaches to this task have been explored. The first approach applies\nattention mechanism to the sliding-window paradigm, where a prediction based on\neach timbral-temporal `instance' is given an attention weight, before\naggregation to produce the final prediction. The second approach is based on a\nrecurrent model of visual attention where the network only attends to parts of\nthe spectrogram and decide where to attend to next, given a limited number of\n`glimpses'.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 03:56:44 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2020 15:53:37 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Watcharasupat", "Karn", ""], ["Gururani", "Siddharth", ""], ["Lerch", "Alexander", ""]]}, {"id": "2006.09736", "submitter": "Casper Hansen", "authors": "Christian Hansen and Casper Hansen and Jakob Grue Simonsen and Birger\n  Larsen and Stephen Alstrup and Christina Lioma", "title": "Factuality Checking in News Headlines with Eye Tracking", "comments": "Accepted to SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401221", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study whether it is possible to infer if a news headline is true or false\nusing only the movement of the human eyes when reading news headlines. Our\nstudy with 55 participants who are eye-tracked when reading 108 news headlines\n(72 true, 36 false) shows that false headlines receive statistically\nsignificantly less visual attention than true headlines. We further build an\nensemble learner that predicts news headline factuality using only eye-tracking\nmeasurements. Our model yields a mean AUC of 0.688 and is better at detecting\nfalse than true headlines. Through a model analysis, we find that eye-tracking\n25 users when reading 3-6 headlines is sufficient for our ensemble learner.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 09:24:21 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Hansen", "Christian", ""], ["Hansen", "Casper", ""], ["Simonsen", "Jakob Grue", ""], ["Larsen", "Birger", ""], ["Alstrup", "Stephen", ""], ["Lioma", "Christina", ""]]}, {"id": "2006.09739", "submitter": "Sakshi Ranjan", "authors": "Sakshi Ranjan, Subhankar Mishra", "title": "Comparative Sentiment Analysis of App Reviews", "comments": "10 pages, 7 figures, Accepted to the 11th ICCCNT, 2020, IIT KGP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Google app market captures the school of thought of users via ratings and\ntext reviews. The critique's viewpoint regarding an app is proportional to\ntheir satisfaction level. Consequently, this helps other users to gain insights\nbefore downloading or purchasing the apps. The potential information from the\nreviews can't be extracted manually, due to its exponential growth. Sentiment\nanalysis, by machine learning algorithms employing NLP, is used to explicitly\nuncover and interpret the emotions. This study aims to perform the sentiment\nclassification of the app reviews and identify the university students'\nbehavior towards the app market. We applied machine learning algorithms using\nthe TF-IDF text representation scheme and the performance was evaluated on the\nensemble learning method. Our model was trained on Google reviews and tested on\nstudents' reviews. SVM recorded the maximum accuracy(93.37\\%), F-score(0.88) on\ntri-gram + TF-IDF scheme. Bagging enhanced the performance of LR and NB with\naccuracy of 87.80\\% and 85.5\\% respectively.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 09:28:07 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Ranjan", "Sakshi", ""], ["Mishra", "Subhankar", ""]]}, {"id": "2006.09904", "submitter": "Paridhi Maheshwari", "authors": "Paridhi Maheshwari, Manoj Ghuhan, Vishwa Vinay", "title": "Learning Colour Representations of Search Queries", "comments": "Accepted as a full paper at SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401095", "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image search engines rely on appropriately designed ranking features that\ncapture various aspects of the content semantics as well as the historic\npopularity. In this work, we consider the role of colour in this relevance\nmatching process. Our work is motivated by the observation that a significant\nfraction of user queries have an inherent colour associated with them. While\nsome queries contain explicit colour mentions (such as 'black car' and 'yellow\ndaisies'), other queries have implicit notions of colour (such as 'sky' and\n'grass'). Furthermore, grounding queries in colour is not a mapping to a single\ncolour, but a distribution in colour space. For instance, a search for 'trees'\ntends to have a bimodal distribution around the colours green and brown. We\nleverage historical clickthrough data to produce a colour representation for\nsearch queries and propose a recurrent neural network architecture to encode\nunseen queries into colour space. We also show how this embedding can be learnt\nalongside a cross-modal relevance ranker from impression logs where a subset of\nthe result images were clicked. We demonstrate that the use of a query-image\ncolour distance feature leads to an improvement in the ranker performance as\nmeasured by users' preferences of clicked versus skipped images.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 14:38:44 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Maheshwari", "Paridhi", ""], ["Ghuhan", "Manoj", ""], ["Vinay", "Vishwa", ""]]}, {"id": "2006.09977", "submitter": "Shan Jiang", "authors": "Cong Wan, Shan Jiang, Cuirong Wang, Cong Wang, Changming Xu, Xianxia\n  Chen, Ying Yuan", "title": "A novel sentence embedding based topic detection method for micro-blog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic detection is a challenging task, especially without knowing the exact\nnumber of topics. In this paper, we present a novel approach based on neural\nnetwork to detect topics in the micro-blogging dataset. We use an unsupervised\nneural sentence embedding model to map the blogs to an embedding space. Our\nmodel is a weighted power mean word embedding model, and the weights are\ncalculated by attention mechanism. Experimental result shows our embedding\nmethod performs better than baselines in sentence clustering. In addition, we\npropose an improved clustering algorithm referred as relationship-aware DBSCAN\n(RADBSCAN). It can discover topics from a micro-blogging dataset, and the topic\nnumber depends on dataset character itself. Moreover, in order to solve the\nproblem of parameters sensitive, we take blog forwarding relationship as a\nbridge of two independent clusters. Finally, we validate our approach on a\ndataset from sina micro-blog. The result shows that we can detect all the\ntopics successfully and extract keywords in each topic.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 09:58:57 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Wan", "Cong", ""], ["Jiang", "Shan", ""], ["Wang", "Cuirong", ""], ["Wang", "Cong", ""], ["Xu", "Changming", ""], ["Chen", "Xianxia", ""], ["Yuan", "Ying", ""]]}, {"id": "2006.09978", "submitter": "Nan Wang", "authors": "Nan Wang, Hongning Wang", "title": "Directional Multivariate Ranking", "comments": "Accepted as a full research paper in KDD'20", "journal-ref": null, "doi": "10.1145/3394486.3403051", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User-provided multi-aspect evaluations manifest users' detailed feedback on\nthe recommended items and enable fine-grained understanding of their\npreferences. Extensive studies have shown that modeling such data greatly\nimproves the effectiveness and explainability of the recommendations. However,\nas ranking is essential in recommendation, there is no principled solution yet\nfor collectively generating multiple item rankings over different aspects. In\nthis work, we propose a directional multi-aspect ranking criterion to enable a\nholistic ranking of items with respect to multiple aspects. Specifically, we\nview multi-aspect evaluation as an integral effort from a user that forms a\nvector of his/her preferences over aspects. Our key insight is that the\ndirection of the difference vector between two multi-aspect preference vectors\nreveals the pairwise order of comparison. Hence, it is necessary for a\nmulti-aspect ranking criterion to preserve the observed directions from such\npairwise comparisons. We further derive a complete solution for the\nmulti-aspect ranking problem based on a probabilistic multivariate tensor\nfactorization model. Comprehensive experimental analysis on a large TripAdvisor\nmulti-aspect rating dataset and a Yelp review text dataset confirms the\neffectiveness of our solution.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 22:43:03 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Wang", "Nan", ""], ["Wang", "Hongning", ""]]}, {"id": "2006.09979", "submitter": "Barbara Rychalska", "authors": "Barbara Rychalska, Dominika Basaj, Jacek D\\k{a}browski, Micha{\\l}\n  Daniluk", "title": "I know why you like this movie: Interpretable Efficient Multimodal\n  Recommender", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Efficient Manifold Density Estimator (EMDE) model has been\nintroduced. The model exploits Local Sensitive Hashing and Count-Min Sketch\nalgorithms, combining them with a neural network to achieve state-of-the-art\nresults on multiple recommender datasets. However, this model ingests a\ncompressed joint representation of all input items for each user/session, so\ncalculating attributions for separate items via gradient-based methods seems\nnot applicable. We prove that interpreting this model in a white-box setting is\npossible thanks to the properties of EMDE item retrieval method. By exploiting\nmultimodal flexibility of this model, we obtain meaningful results showing the\ninfluence of multiple modalities: text, categorical features, and images, on\nmovie recommendation output.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 09:59:28 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Rychalska", "Barbara", ""], ["Basaj", "Dominika", ""], ["D\u0105browski", "Jacek", ""], ["Daniluk", "Micha\u0142", ""]]}, {"id": "2006.10104", "submitter": "Herodotos Herodotou", "authors": "Herodotos Herodotou and Despoina Chatzakou and Nicolas Kourtellis", "title": "A Streaming Machine Learning Framework for Online Aggression Detection\n  on Twitter", "comments": "12 pages, 16 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of online aggression on social media is evolving into a major point\nof concern. Several machine and deep learning approaches have been proposed\nrecently for detecting various types of aggressive behavior. However, social\nmedia are fast paced, generating an increasing amount of content, while\naggressive behavior evolves over time. In this work, we introduce the first,\npractical, real-time framework for detecting aggression on Twitter via\nembracing the streaming machine learning paradigm. Our method adapts its ML\nclassifiers in an incremental fashion as it receives new annotated examples and\nis able to achieve the same (or even higher) performance as batch-based ML\nmodels, with over 90% accuracy, precision, and recall. At the same time, our\nexperimental analysis on real Twitter data reveals how our framework can easily\nscale to accommodate the entire Twitter Firehose (of 778 million tweets per\nday) with only 3 commodity machines. Finally, we show that our framework is\ngeneral enough to detect other related behaviors such as sarcasm, racism, and\nsexism in real time.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 19:00:55 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 11:19:19 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Herodotou", "Herodotos", ""], ["Chatzakou", "Despoina", ""], ["Kourtellis", "Nicolas", ""]]}, {"id": "2006.10124", "submitter": "Arvind Thiagarajan", "authors": "Arvind Thiagarajan", "title": "Improvements in Computation and Usage of Joint CDFs for the\n  N-Dimensional Order Statistic", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Order statistics provide an intuition for combining multiple lists of scores\nover a common index set. This intuition is particularly valuable when the lists\nto be combined cannot be directly compared in a sensible way. We describe here\nthe advantages of a new method for using joint CDFs of such order statistics to\ncombine score lists. We also present, with proof, a new algorithm for computing\nsuch joint CDF values, with runtime linear in the size of the combined list.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 19:51:45 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Thiagarajan", "Arvind", ""]]}, {"id": "2006.10174", "submitter": "Hamed Zamani", "authors": "Hamed Zamani, Gord Lueck, Everest Chen, Rodolfo Quispe, Flint Luu,\n  Nick Craswell", "title": "MIMICS: A Large-Scale Data Collection for Search Clarification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search clarification has recently attracted much attention due to its\napplications in search engines. It has also been recognized as a major\ncomponent in conversational information seeking systems. Despite its\nimportance, the research community still feels the lack of a large-scale data\nfor studying different aspects of search clarification. In this paper, we\nintroduce MIMICS, a collection of search clarification datasets for real web\nsearch queries sampled from the Bing query logs. Each clarification in MIMICS\nis generated by a Bing production algorithm and consists of a clarifying\nquestion and up to five candidate answers. MIMICS contains three datasets: (1)\nMIMICS-Click includes over 400k unique queries, their associated clarification\npanes, and the corresponding aggregated user interaction signals (i.e.,\nclicks). (2) MIMICS-ClickExplore is an exploration data that includes\naggregated user interaction signals for over 60k unique queries, each with\nmultiple clarification panes. (3) MIMICS-Manual includes over 2k unique real\nsearch queries. Each query-clarification pair in this dataset has been manually\nlabeled by at least three trained annotators. It contains graded quality labels\nfor the clarifying question, the candidate answer set, and the landing result\npage for each candidate answer.\n  MIMICS is publicly available for research purposes, thus enables researchers\nto study a number of tasks related to search clarification, including\nclarification generation and selection, user engagement prediction for\nclarification, click models for clarification, and analyzing user interactions\nwith search clarification.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 21:54:41 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Zamani", "Hamed", ""], ["Lueck", "Gord", ""], ["Chen", "Everest", ""], ["Quispe", "Rodolfo", ""], ["Luu", "Flint", ""], ["Craswell", "Nick", ""]]}, {"id": "2006.10207", "submitter": "Lukasz Augustyniak", "authors": "{\\L}ukasz Augustyniak, Krzysztof Rajda, Tomasz Kajdanowicz, Micha{\\l}\n  Bernaczyk", "title": "Political Advertising Dataset: the use case of the Polish 2020\n  Presidential Elections", "comments": "ACL 2020 WiNLP Workshop - accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Political campaigns are full of political ads posted by candidates on social\nmedia. Political advertisements constitute a basic form of campaigning,\nsubjected to various social requirements. We present the first publicly open\ndataset for detecting specific text chunks and categories of political\nadvertising in the Polish language. It contains 1,705 human-annotated tweets\ntagged with nine categories, which constitute campaigning under Polish\nelectoral law. We achieved a 0.65 inter-annotator agreement (Cohen's kappa\nscore). An additional annotator resolved the mismatches between the first two\nannotators improving the consistency and complexity of the annotation process.\nWe used the newly created dataset to train a well established neural tagger\n(achieving a 70% percent points F1 score). We also present a possible direction\nof use cases for such datasets and models with an initial analysis of the\nPolish 2020 Presidential Elections on Twitter.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 23:58:01 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Augustyniak", "\u0141ukasz", ""], ["Rajda", "Krzysztof", ""], ["Kajdanowicz", "Tomasz", ""], ["Bernaczyk", "Micha\u0142", ""]]}, {"id": "2006.10208", "submitter": "Alireza Heidari", "authors": "Alireza Heidari, George Michalopoulos, Shrinu Kushagra, Ihab F. Ilyas,\n  Theodoros Rekatsinas", "title": "Record fusion: A learning approach", "comments": "18 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record fusion is the task of aggregating multiple records that correspond to\nthe same real-world entity in a database. We can view record fusion as a\nmachine learning problem where the goal is to predict the \"correct\" value for\neach attribute for each entity. Given a database, we use a combination of\nattribute-level, recordlevel, and database-level signals to construct a feature\nvector for each cell (or (row, col)) of that database. We use this feature\nvector alongwith the ground-truth information to learn a classifier for each of\nthe attributes of the database.\n  Our learning algorithm uses a novel stagewise additive model. At each stage,\nwe construct a new feature vector by combining a part of the original feature\nvector with features computed by the predictions from the previous stage. We\nthen learn a softmax classifier over the new feature space. This greedy\nstagewise approach can be viewed as a deep model where at each stage, we are\nadding more complicated non-linear transformations of the original feature\nvector. We show that our approach fuses records with an average precision of\n~98% when source information of records is available, and ~94% without source\ninformation across a diverse array of real-world datasets. We compare our\napproach to a comprehensive collection of data fusion and entity consolidation\nmethods considered in the literature. We show that our approach can achieve an\naverage precision improvement of ~20%/~45% with/without source information\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 00:04:37 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Heidari", "Alireza", ""], ["Michalopoulos", "George", ""], ["Kushagra", "Shrinu", ""], ["Ilyas", "Ihab F.", ""], ["Rekatsinas", "Theodoros", ""]]}, {"id": "2006.10213", "submitter": "Yao Zhao", "authors": "Yao Zhao, Mohammad Saleh, Peter J.Liu", "title": "SEAL: Segment-wise Extractive-Abstractive Long-form Text Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most prior work in the sequence-to-sequence paradigm focused on datasets with\ninput sequence lengths in the hundreds of tokens due to the computational\nconstraints of common RNN and Transformer architectures. In this paper, we\nstudy long-form abstractive text summarization, a sequence-to-sequence setting\nwith input sequence lengths up to 100,000 tokens and output sequence lengths up\nto 768 tokens. We propose SEAL, a Transformer-based model, featuring a new\nencoder-decoder attention that dynamically extracts/selects input snippets to\nsparsely attend to for each output segment. Using only the original documents\nand summaries, we derive proxy labels that provide weak supervision for\nextractive layers simultaneously with regular supervision from abstractive\nsummaries. The SEAL model achieves state-of-the-art results on existing\nlong-form summarization tasks, and outperforms strong baseline models on a new\ndataset/task we introduce, Search2Wiki, with much longer input text. Since\ncontent selection is explicit in the SEAL model, a desirable side effect is\nthat the selection can be inspected for enhanced interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 00:13:21 GMT"}], "update_date": "2020-06-20", "authors_parsed": [["Zhao", "Yao", ""], ["Saleh", "Mohammad", ""], ["Liu", "Peter J.", ""]]}, {"id": "2006.10217", "submitter": "Yue Yu", "authors": "Yue Yu, Yinghao Li, Jiaming Shen, Hao Feng, Jimeng Sun and Chao Zhang", "title": "STEAM: Self-Supervised Taxonomy Expansion with Mini-Paths", "comments": "KDD 2020 Research Track Full Paper", "journal-ref": null, "doi": "10.1145/3394486.3403145", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Taxonomies are important knowledge ontologies that underpin numerous\napplications on a daily basis, but many taxonomies used in practice suffer from\nthe low coverage issue. We study the taxonomy expansion problem, which aims to\nexpand existing taxonomies with new concept terms. We propose a self-supervised\ntaxonomy expansion model named STEAM, which leverages natural supervision in\nthe existing taxonomy for expansion. To generate natural self-supervision\nsignals, STEAM samples mini-paths from the existing taxonomy, and formulates a\nnode attachment prediction task between anchor mini-paths and query terms. To\nsolve the node attachment task, it learns feature representations for\nquery-anchor pairs from multiple views and performs multi-view co-training for\nprediction. Extensive experiments show that STEAM outperforms state-of-the-art\nmethods for taxonomy expansion by 11.6\\% in accuracy and 7.0\\% in mean\nreciprocal rank on three public benchmarks. The implementation of STEAM can be\nfound at \\url{https://github.com/yueyu1030/STEAM}.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 00:32:53 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Yu", "Yue", ""], ["Li", "Yinghao", ""], ["Shen", "Jiaming", ""], ["Feng", "Hao", ""], ["Sun", "Jimeng", ""], ["Zhang", "Chao", ""]]}, {"id": "2006.10233", "submitter": "Deqing Yang", "authors": "Deqing Yang and Zengcun Song and Lvxin Xue and Yanghua Xiao", "title": "A Knowledge-Enhanced Recommendation Model with Attribute-Level\n  Co-Attention", "comments": null, "journal-ref": "SIGIR 2020", "doi": "10.1145/3397271.3401313", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been widely employed in recommender systems\nincluding incorporating attention mechanism for performance improvement.\nHowever, most of existing attention-based models only apply item-level\nattention on user side, restricting the further enhancement of recommendation\nperformance. In this paper, we propose a knowledge-enhanced recommendation\nmodel ACAM, which incorporates item attributes distilled from knowledge graphs\n(KGs) as side information, and is built with a co-attention mechanism on\nattribute-level to achieve performance gains. Specifically, each user and item\nin ACAM are represented by a set of attribute embeddings at first. Then, user\nrepresentations and item representations are augmented simultaneously through\ncapturing the correlations between different attributes by a co-attention\nmodule. Our extensive experiments over two realistic datasets show that the\nuser representations and item representations augmented by attribute-level\nco-attention gain ACAM's superiority over the state-of-the-art deep models.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 01:53:39 GMT"}], "update_date": "2020-06-20", "authors_parsed": [["Yang", "Deqing", ""], ["Song", "Zengcun", ""], ["Xue", "Lvxin", ""], ["Xiao", "Yanghua", ""]]}, {"id": "2006.10389", "submitter": "Sijin Zhou", "authors": "Sijin Zhou, Xinyi Dai, Haokun Chen, Weinan Zhang, Kan Ren, Ruiming\n  Tang, Xiuqiang He and Yong Yu", "title": "Interactive Recommender System via Knowledge Graph-enhanced\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive recommender system (IRS) has drawn huge attention because of its\nflexible recommendation strategy and the consideration of optimal long-term\nuser experiences. To deal with the dynamic user preference and optimize\naccumulative utilities, researchers have introduced reinforcement learning (RL)\ninto IRS. However, RL methods share a common issue of sample efficiency, i.e.,\nhuge amount of interaction data is required to train an effective\nrecommendation policy, which is caused by the sparse user responses and the\nlarge action space consisting of a large number of candidate items. Moreover,\nit is infeasible to collect much data with explorative policies in online\nenvironments, which will probably harm user experience. In this work, we\ninvestigate the potential of leveraging knowledge graph (KG) in dealing with\nthese issues of RL methods for IRS, which provides rich side information for\nrecommendation decision making. Instead of learning RL policies from scratch,\nwe make use of the prior knowledge of the item correlation learned from KG to\n(i) guide the candidate selection for better candidate item retrieval, (ii)\nenrich the representation of items and user states, and (iii) propagate user\npreferences among the correlated items over KG to deal with the sparsity of\nuser feedback. Comprehensive experiments have been conducted on two real-world\ndatasets, which demonstrate the superiority of our approach with significant\nimprovements against state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 09:48:48 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Zhou", "Sijin", ""], ["Dai", "Xinyi", ""], ["Chen", "Haokun", ""], ["Zhang", "Weinan", ""], ["Ren", "Kan", ""], ["Tang", "Ruiming", ""], ["He", "Xiuqiang", ""], ["Yu", "Yong", ""]]}, {"id": "2006.10673", "submitter": "Gretchen Stahlman", "authors": "Gretchen R. Stahlman, P. Bryan Heidorn", "title": "Mapping the \"long tail\" of research funding: A topic analysis of NSF\n  grant proposals in the Division of Astronomical Sciences", "comments": "Conference paper accepted to 83rd Annual Meeting of the Association\n  for Information Science & Technology (ASIS&T), October 24-28, 2020", "journal-ref": "Proceedings of the Association for Information Science and\n  Technology. 57(2020) e276", "doi": "10.1002/pra2.276", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Long tail\" data are considered to be smaller, heterogeneous, researcher-held\ndata, which present unique data management and scholarly communication\nchallenges. These data are presumably concentrated within relatively\nlower-funded projects due to insufficient resources for curation. To better\nunderstand the nature and distribution of long tail data, we examine National\nScience Foundation (NSF) funding patterns using Latent Dirichlet Analysis (LDA)\nand bibliographic data. We also introduce the concept of \"Topic Investment\" to\ncapture differences in topics across funding levels and to illuminate the\ndistribution of funding across topics. This study uses the discipline of\nastronomy as a case study, overall exploring possible associations between\ntopic, funding level and research output, with implications for research policy\nand practice. We find that while different topics demonstrate different funding\nlevels and publication patterns, dynamics predicted by the \"long tail\"\ntheoretical framework presented here can be observed within NSF-funded topics\nin astronomy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 16:57:53 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Stahlman", "Gretchen R.", ""], ["Heidorn", "P. Bryan", ""]]}, {"id": "2006.10842", "submitter": "Qiang Yang", "authors": "Qiang Yang, Hind Alamro, Somayah Albaradei, Adil Salhi, Xiaoting Lv,\n  Changsheng Ma, Manal Alshehri, Inji Jaber, Faroug Tifratene, Wei Wang,\n  Takashi Gojobori, Carlos M. Duarte, Xin Gao, Xiangliang Zhang", "title": "SenWave: Monitoring the Global Sentiments under the COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the first alert launched by the World Health Organization (5 January,\n2020), COVID-19 has been spreading out to over 180 countries and territories.\nAs of June 18, 2020, in total, there are now over 8,400,000 cases and over\n450,000 related deaths. This causes massive losses in the economy and jobs\nglobally and confining about 58% of the global population. In this paper, we\nintroduce SenWave, a novel sentimental analysis work using 105+ million\ncollected tweets and Weibo messages to evaluate the global rise and falls of\nsentiments during the COVID-19 pandemic. To make a fine-grained analysis on the\nfeeling when we face this global health crisis, we annotate 10K tweets in\nEnglish and 10K tweets in Arabic in 10 categories, including optimistic,\nthankful, empathetic, pessimistic, anxious, sad, annoyed, denial, official\nreport, and joking. We then utilize an integrated transformer framework, called\nsimpletransformer, to conduct multi-label sentimental classification by\nfine-tuning the pre-trained language model on the labeled data. Meanwhile, in\norder for a more complete analysis, we also translate the annotated English\ntweets into different languages (Spanish, Italian, and French) to generated\ntraining data for building sentiment analysis models for these languages.\nSenWave thus reveals the sentiment of global conversation in six different\nlanguages on COVID-19 (covering English, Spanish, French, Italian, Arabic and\nChinese), followed the spread of the epidemic. The conversation showed a\nremarkably similar pattern of rapid rise and slow decline over time across all\nnations, as well as on special topics like the herd immunity strategies, to\nwhich the global conversation reacts strongly negatively. Overall, SenWave\nshows that optimistic and positive sentiments increased over time, foretelling\na desire to seek, together, a reset for an improved COVID-19 world.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 20:33:41 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Yang", "Qiang", ""], ["Alamro", "Hind", ""], ["Albaradei", "Somayah", ""], ["Salhi", "Adil", ""], ["Lv", "Xiaoting", ""], ["Ma", "Changsheng", ""], ["Alshehri", "Manal", ""], ["Jaber", "Inji", ""], ["Tifratene", "Faroug", ""], ["Wang", "Wei", ""], ["Gojobori", "Takashi", ""], ["Duarte", "Carlos M.", ""], ["Gao", "Xin", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "2006.10909", "submitter": "Pankaj Gupta", "authors": "Pankaj Gupta and Yatin Chaudhary and Thomas Runkler and Hinrich\n  Sch\\\"utze", "title": "Neural Topic Modeling with Continual Lifelong Learning", "comments": "ICML2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelong learning has recently attracted attention in building machine\nlearning systems that continually accumulate and transfer knowledge to help\nfuture learning. Unsupervised topic modeling has been popularly used to\ndiscover topics from document collections. However, the application of topic\nmodeling is challenging due to data sparsity, e.g., in a small collection of\n(short) documents and thus, generate incoherent topics and sub-optimal document\nrepresentations. To address the problem, we propose a lifelong learning\nframework for neural topic modeling that can continuously process streams of\ndocument collections, accumulate topics and guide future topic modeling tasks\nby knowledge transfer from several sources to better deal with the sparse data.\nIn the lifelong process, we particularly investigate jointly: (1) sharing\ngenerative homologies (latent topics) over lifetime to transfer prior\nknowledge, and (2) minimizing catastrophic forgetting to retain the past\nlearning via novel selective data augmentation, co-training and topic\nregularization approaches. Given a stream of document collections, we apply the\nproposed Lifelong Neural Topic Modeling (LNTM) framework in modeling three\nsparse document collections as future tasks and demonstrate improved\nperformance quantified by perplexity, topic coherence and information retrieval\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 00:43:23 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Gupta", "Pankaj", ""], ["Chaudhary", "Yatin", ""], ["Runkler", "Thomas", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2006.10932", "submitter": "Deqing Yang", "authors": "Junyang Jiang and Deqing Yang and Yanghua Xiao and Chenlu Shen", "title": "Convolutional Gaussian Embeddings for Personalized Recommendation with\n  Uncertainty", "comments": null, "journal-ref": "IJCAI 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of existing embedding based recommendation models use embeddings\n(vectors) corresponding to a single fixed point in low-dimensional space, to\nrepresent users and items. Such embeddings fail to precisely represent the\nusers/items with uncertainty often observed in recommender systems. Addressing\nthis problem, we propose a unified deep recommendation framework employing\nGaussian embeddings, which are proven adaptive to uncertain preferences\nexhibited by some users, resulting in better user representations and\nrecommendation performance. Furthermore, our framework adopts Monte-Carlo\nsampling and convolutional neural networks to compute the correlation between\nthe objective user and the candidate item, based on which precise\nrecommendations are achieved. Our extensive experiments on two benchmark\ndatasets not only justify that our proposed Gaussian embeddings capture the\nuncertainty of users very well, but also demonstrate its superior performance\nover the state-of-the-art recommendation models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 02:10:38 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Jiang", "Junyang", ""], ["Yang", "Deqing", ""], ["Xiao", "Yanghua", ""], ["Shen", "Chenlu", ""]]}, {"id": "2006.10964", "submitter": "Yanshan Wang", "authors": "David Oniani, Yanshan Wang", "title": "A Qualitative Evaluation of Language Models on Automatic\n  Question-Answering for COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 has resulted in an ongoing pandemic and as of 12 June 2020, has\ncaused more than 7.4 million cases and over 418,000 deaths. The highly dynamic\nand rapidly evolving situation with COVID-19 has made it difficult to access\naccurate, on-demand information regarding the disease. Online communities,\nforums, and social media provide potential venues to search for relevant\nquestions and answers, or post questions and seek answers from other members.\nHowever, due to the nature of such sites, there are always a limited number of\nrelevant questions and responses to search from, and posted questions are\nrarely answered immediately. With the advancements in the field of natural\nlanguage processing, particularly in the domain of language models, it has\nbecome possible to design chatbots that can automatically answer consumer\nquestions. However, such models are rarely applied and evaluated in the\nhealthcare domain, to meet the information needs with accurate and up-to-date\nhealthcare data. In this paper, we propose to apply a language model for\nautomatically answering questions related to COVID-19 and qualitatively\nevaluate the generated responses. We utilized the GPT-2 language model and\napplied transfer learning to retrain it on the COVID-19 Open Research Dataset\n(CORD-19) corpus. In order to improve the quality of the generated responses,\nwe applied 4 different approaches, namely tf-idf, BERT, BioBERT, and USE to\nfilter and retain relevant sentences in the responses. In the performance\nevaluation step, we asked two medical experts to rate the responses. We found\nthat BERT and BioBERT, on average, outperform both tf-idf and USE in\nrelevance-based sentence filtering tasks. Additionally, based on the chatbot,\nwe created a user-friendly interactive web application to be hosted online.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 05:13:57 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 20:23:04 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Oniani", "David", ""], ["Wang", "Yanshan", ""]]}, {"id": "2006.11011", "submitter": "Yu Zheng", "authors": "Yu Zheng, Chen Gao, Xiang Li, Xiangnan He, Depeng Jin, Yong Li", "title": "Disentangling User Interest and Conformity for Recommendation with\n  Causal Embedding", "comments": "Accepted by WWW'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation models are usually trained on observational interaction data.\nHowever, observational interaction data could result from users' conformity\ntowards popular items, which entangles users' real interest. Existing methods\ntracks this problem as eliminating popularity bias, e.g., by re-weighting\ntraining samples or leveraging a small fraction of unbiased data. However, the\nvariety of user conformity is ignored by these approaches, and different causes\nof an interaction are bundled together as unified representations, hence\nrobustness and interpretability are not guaranteed when underlying causes are\nchanging. In this paper, we present DICE, a general framework that learns\nrepresentations where interest and conformity are structurally disentangled,\nand various backbone recommendation models could be smoothly integrated. We\nassign users and items with separate embeddings for interest and conformity,\nand make each embedding capture only one cause by training with cause-specific\ndata which is obtained according to the colliding effect of causal inference.\nOur proposed methodology outperforms state-of-the-art baselines with remarkable\nimprovements on two real-world datasets on top of various backbone models. We\nfurther demonstrate that the learned embeddings successfully capture the\ndesired causes, and show that DICE guarantees the robustness and\ninterpretability of recommendation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 08:24:14 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 09:49:04 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Zheng", "Yu", ""], ["Gao", "Chen", ""], ["Li", "Xiang", ""], ["He", "Xiangnan", ""], ["Jin", "Depeng", ""], ["Li", "Yong", ""]]}, {"id": "2006.11149", "submitter": "Muhammad Umer Anwaar", "authors": "Muhammad Umer Anwaar, Egor Labintcev, Martin Kleinsteuber", "title": "Compositional Learning of Image-Text Query for Image Retrieval", "comments": "Published at IEEE WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of retrieving images from a\ndatabase based on a multi-modal (image-text) query. Specifically, the query\ntext prompts some modification in the query image and the task is to retrieve\nimages with the desired modifications. For instance, a user of an E-Commerce\nplatform is interested in buying a dress, which should look similar to her\nfriend's dress, but the dress should be of white color with a ribbon sash. In\nthis case, we would like the algorithm to retrieve some dresses with desired\nmodifications in the query dress. We propose an autoencoder based model,\nComposeAE, to learn the composition of image and text query for retrieving\nimages. We adopt a deep metric learning approach and learn a metric that pushes\ncomposition of source image and text query closer to the target images. We also\npropose a rotational symmetry constraint on the optimization problem. Our\napproach is able to outperform the state-of-the-art method TIRG \\cite{TIRG} on\nthree benchmark datasets, namely: MIT-States, Fashion200k and Fashion IQ. In\norder to ensure fair comparison, we introduce strong baselines by enhancing\nTIRG method. To ensure reproducibility of the results, we publish our code\nhere: \\url{https://github.com/ecom-research/ComposeAE}.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 14:21:41 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 06:06:12 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 21:35:55 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Anwaar", "Muhammad Umer", ""], ["Labintcev", "Egor", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "2006.11403", "submitter": "Soroush Vosoughi Dr", "authors": "Lili Wang, Ruibo Liu, and Soroush Vosoughi", "title": "Salienteye: Maximizing Engagement While Maintaining Artistic Style on\n  Instagram Using Deep Neural Networks", "comments": "Proceedings of the 2020 International Conference on Multimedia\n  Retrieval. 2020", "journal-ref": null, "doi": "10.1145/3372278.3390736", "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instagram has become a great venue for amateur and professional photographers\nalike to showcase their work. It has, in other words, democratized photography.\nGenerally, photographers take thousands of photos in a session, from which they\npick a few to showcase their work on Instagram. Photographers trying to build a\nreputation on Instagram have to strike a balance between maximizing their\nfollowers' engagement with their photos, while also maintaining their artistic\nstyle. We used transfer learning to adapt Xception, which is a model for object\nrecognition trained on the ImageNet dataset, to the task of engagement\nprediction and utilized Gram matrices generated from VGG19, another object\nrecognition model trained on ImageNet, for the task of style similarity\nmeasurement on photos posted on Instagram. Our models can be trained on\nindividual Instagram accounts to create personalized engagement prediction and\nstyle similarity models. Once trained on their accounts, users can have new\nphotos sorted based on predicted engagement and style similarity to their\nprevious work, thus enabling them to upload photos that not only have the\npotential to maximize engagement from their followers but also maintain their\nstyle of photography. We trained and validated our models on several Instagram\naccounts, showing it to be adept at both tasks, also outperforming several\nbaseline models and human annotators.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 01:58:02 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Wang", "Lili", ""], ["Liu", "Ruibo", ""], ["Vosoughi", "Soroush", ""]]}, {"id": "2006.11446", "submitter": "Nidhi Rastogi", "authors": "Nidhi Rastogi, Sharmishtha Dutta, Mohammed J. Zaki, Alex Gittens, and\n  Charu Aggarwal", "title": "MALOnt: An Ontology for Malware Threat Intelligence", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.16426.64962", "report-no": null, "categories": "cs.CR cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Malware threat intelligence uncovers deep information about malware, threat\nactors, and their tactics, Indicators of Compromise(IoC), and vulnerabilities\nin different platforms from scattered threat sources. This collective\ninformation can guide decision making in cyber defense applications utilized by\nsecurity operation centers(SoCs). In this paper, we introduce an open-source\nmalware ontology - MALOnt that allows the structured extraction of information\nand knowledge graph generation, especially for threat intelligence. The\nknowledge graph that uses MALOnt is instantiated from a corpus comprising\nhundreds of annotated malware threat reports. The knowledge graph enables the\nanalysis, detection, classification, and attribution of cyber threats caused by\nmalware. We also demonstrate the annotation process using MALOnt on exemplar\nthreat intelligence reports. A work in progress, this research is part of a\nlarger effort towards auto-generation of knowledge graphs (KGs)for gathering\nmalware threat intelligence from heterogeneous online resources.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 00:25:07 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Rastogi", "Nidhi", ""], ["Dutta", "Sharmishtha", ""], ["Zaki", "Mohammed J.", ""], ["Gittens", "Alex", ""], ["Aggarwal", "Charu", ""]]}, {"id": "2006.11511", "submitter": "Abhijit Mahabal", "authors": "Abhijit Mahabal, Yinrui Li, Rajat Raina, Daniel Sun, Revati Mahajan,\n  Jure Leskovec", "title": "Improving Query Safety at Pinterest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query recommendations in search engines is a double edged sword, with\nundeniable benefits but potential of harm. Identifying unsafe queries is\nnecessary to protect users from inappropriate query suggestions. However,\nidentifying these is non-trivial because of the linguistic diversity resulting\nfrom large vocabularies, social-group-specific slang and typos, and because the\ninappropriateness of a term depends on the context. Here we formulate the\nproblem as query-set expansion, where we are given a small and potentially\nbiased seed set and the aim is to identify a diverse set of semantically\nrelated queries. We present PinSets, a system for query-set expansion, which\napplies a simple yet powerful mechanism to search user sessions, expanding a\ntiny seed set into thousands of related queries at nearly perfect precision,\ndeep into the tail, along with explanations that are easy to interpret. PinSets\nowes its high quality expansion to using a hybrid of textual and behavioral\ntechniques (i.e., treating queries both as compositional and as black boxes).\nExperiments show that, for the domain of drugs-related queries, PinSets expands\n20 seed queries into 15,670 positive training examples at over 99\\% precision.\nThe generated expansions have diverse vocabulary and correctly handles words\nwith ambiguous safety. PinSets decreased unsafe query suggestions at Pinterest\nby 90\\%.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 07:35:22 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 04:12:09 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Mahabal", "Abhijit", ""], ["Li", "Yinrui", ""], ["Raina", "Rajat", ""], ["Sun", "Daniel", ""], ["Mahajan", "Revati", ""], ["Leskovec", "Jure", ""]]}, {"id": "2006.11534", "submitter": "Hamid Zafar", "authors": "Hamid Zafar, Mohnish Dubey, Jens Lehmann, Elena Demidova", "title": "IQA: Interactive Query Construction in Semantic Question Answering\n  Systems", "comments": null, "journal-ref": "Journal of Web Semantics Volume 64, October 2020, 100586", "doi": "10.1016/j.websem.2020.100586", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Question Answering (SQA) systems automatically interpret user\nquestions expressed in a natural language in terms of semantic queries. This\nprocess involves uncertainty, such that the resulting queries do not always\naccurately match the user intent, especially for more complex and less common\nquestions. In this article, we aim to empower users in guiding SQA systems\ntowards the intended semantic queries through interaction. We introduce IQA -\nan interaction scheme for SQA pipelines. This scheme facilitates seamless\nintegration of user feedback in the question answering process and relies on\nOption Gain - a novel metric that enables efficient and intuitive user\ninteraction. Our evaluation shows that using the proposed scheme, even a small\nnumber of user interactions can lead to significant improvements in the\nperformance of SQA systems.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 10:02:20 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 07:41:43 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 05:17:03 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Zafar", "Hamid", ""], ["Dubey", "Mohnish", ""], ["Lehmann", "Jens", ""], ["Demidova", "Elena", ""]]}, {"id": "2006.11600", "submitter": "Yangyang Guo", "authors": "Yangyang Guo, Zhiyong Cheng, Jiazheng Jing, Yanpeng Lin, Liqiang Nie,\n  Meng Wang", "title": "Enhancing Factorization Machines with Generalized Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorization Machines (FMs) are effective in incorporating side information\nto overcome the cold-start and data sparsity problems in recommender systems.\nTraditional FMs adopt the inner product to model the second-order interactions\nbetween different attributes, which are represented via feature vectors. The\nproblem is that the inner product violates the triangle inequality property of\nfeature vectors. As a result, it cannot well capture fine-grained attribute\ninteractions, resulting in sub-optimal performance. Recently, the Euclidean\ndistance is exploited in FMs to replace the inner product and has delivered\nbetter performance. However, previous FM methods including the ones equipped\nwith the Euclidean distance all focus on the attribute-level interaction\nmodeling, ignoring the critical intrinsic feature correlations inside\nattributes. Thereby, they fail to model the complex and rich interactions\nexhibited in the real-world data. To tackle this problem, in this paper, we\npropose a FM framework equipped with generalized metric learning techniques to\nbetter capture these feature correlations. In particular, based on this\nframework, we present a Mahalanobis distance and a deep neural network (DNN)\nmethods, which can effectively model the linear and non-linear correlations\nbetween features, respectively. Besides, we design an efficient approach for\nsimplifying the model functions. Experiments on several benchmark datasets\ndemonstrate that our proposed framework outperforms several state-of-the-art\nbaselines by a large margin. Moreover, we collect a new large-scale dataset on\nsecond-hand trading to justify the effectiveness of our method over cold-start\nand data sparsity problems in recommender systems.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 15:46:22 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 03:14:38 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Guo", "Yangyang", ""], ["Cheng", "Zhiyong", ""], ["Jing", "Jiazheng", ""], ["Lin", "Yanpeng", ""], ["Nie", "Liqiang", ""], ["Wang", "Meng", ""]]}, {"id": "2006.11632", "submitter": "Jui-Ting Huang", "authors": "Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang,\n  Philip Pronin, Janani Padmanabhan, Giuseppe Ottaviano, Linjun Yang", "title": "Embedding-based Retrieval in Facebook Search", "comments": "9 pages, 3 figures, 3 tables, to be published in KDD '20", "journal-ref": null, "doi": "10.1145/3394486.3403305", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Search in social networks such as Facebook poses different challenges than in\nclassical web search: besides the query text, it is important to take into\naccount the searcher's context to provide relevant results. Their social graph\nis an integral part of this context and is a unique aspect of Facebook search.\nWhile embedding-based retrieval (EBR) has been applied in eb search engines for\nyears, Facebook search was still mainly based on a Boolean matching model. In\nthis paper, we discuss the techniques for applying EBR to a Facebook Search\nsystem. We introduce the unified embedding framework developed to model\nsemantic embeddings for personalized search, and the system to serve\nembedding-based retrieval in a typical search system based on an inverted\nindex. We discuss various tricks and experiences on end-to-end optimization of\nthe whole system, including ANN parameter tuning and full-stack optimization.\nFinally, we present our progress on two selected advanced topics about\nmodeling. We evaluated EBR on verticals for Facebook Search with significant\nmetrics gains observed in online A/B experiments. We believe this paper will\nprovide useful insights and experiences to help people on developing\nembedding-based retrieval systems in search engines.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 18:34:09 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 20:30:39 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Huang", "Jui-Ting", ""], ["Sharma", "Ashish", ""], ["Sun", "Shuying", ""], ["Xia", "Li", ""], ["Zhang", "David", ""], ["Pronin", "Philip", ""], ["Padmanabhan", "Janani", ""], ["Ottaviano", "Giuseppe", ""], ["Yang", "Linjun", ""]]}, {"id": "2006.11719", "submitter": "Liu Yang", "authors": "Zizhen Wang, Yixing Fan, Jiafeng Guo, Liu Yang, Ruqing Zhang, Yanyan\n  Lan, Xueqi Cheng, Hui Jiang, Xiaozhao Wang", "title": "Match$^2$: A Matching over Matching Model for Similar Question\n  Identification", "comments": "Accepted by SIGIR 2020. 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community Question Answering (CQA) has become a primary means for people to\nacquire knowledge, where people are free to ask questions or submit answers. To\nenhance the efficiency of the service, similar question identification becomes\na core task in CQA which aims to find a similar question from the archived\nrepository whenever a new question is asked. However, it has long been a\nchallenge to properly measure the similarity between two questions due to the\ninherent variation of natural language, i.e., there could be different ways to\nask a same question or different questions sharing similar expressions. To\nalleviate this problem, it is natural to involve the existing answers for the\nenrichment of the archived questions. Traditional methods typically take a\none-side usage, which leverages the answer as some expanded representation of\nthe corresponding question. Unfortunately, this may introduce unexpected noises\ninto the similarity computation since answers are often long and diverse,\nleading to inferior performance. In this work, we propose a two-side usage,\nwhich leverages the answer as a bridge of the two questions. The key idea is\nbased on our observation that similar questions could be addressed by similar\nparts of the answer while different questions may not. In other words, we can\ncompare the matching patterns of the two questions over the same answer to\nmeasure their similarity. In this way, we propose a novel matching over\nmatching model, namely Match$^2$, which compares the matching patterns between\ntwo question-answer pairs for similar question identification. Empirical\nexperiments on two benchmark datasets demonstrate that our model can\nsignificantly outperform previous state-of-the-art methods on the similar\nquestion identification task.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 05:59:34 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Wang", "Zizhen", ""], ["Fan", "Yixing", ""], ["Guo", "Jiafeng", ""], ["Yang", "Liu", ""], ["Zhang", "Ruqing", ""], ["Lan", "Yanyan", ""], ["Cheng", "Xueqi", ""], ["Jiang", "Hui", ""], ["Wang", "Xiaozhao", ""]]}, {"id": "2006.11821", "submitter": "Subhadip Maji", "authors": "Subhadip Maji and Smarajit Bose", "title": "An Improved Relevance Feedback in CBIR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relevance Feedback in Content-Based Image Retrieval is a method where the\nfeedback of the performance is being used to improve itself. Prior works use\nfeature re-weighting and classification techniques as the Relevance Feedback\nmethods. This paper shows a novel addition to the prior methods to further\nimprove the retrieval accuracy. In addition to all of these, the paper also\nshows a novel idea to even improve the 0-th iteration retrieval accuracy from\nthe information of Relevance Feedback.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 15:12:27 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 19:38:22 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Maji", "Subhadip", ""], ["Bose", "Smarajit", ""]]}, {"id": "2006.11887", "submitter": "Emory Hufbauer", "authors": "Emory Hufbauer, Hana Khamfroush", "title": "Automatic Query Optimization for Retrieving Traffic Tweets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter, like many social media and data brokering companies, makes their\ndata available through a search API (application programming interface). In\naddition to filtering results by date and location, researchers can search for\ntweets with specific content with a boolean text query, using {\\it AND}, {\\it\nOR}, and {\\it NOT} operators to select the combinations of phrases which must,\nor must not, appear in matching tweets. This boolean text search system is not\nat all unique to Twitter and is found in many different contexts, including\nacademic, legal, and medical databases, however it is stretched to its limits\nin Twitter's use case because of the relative volume and brevity of tweets. In\naddition, the semi-automated use of such systems was well studied under the\ntopic of Information Retrieval during the 1980s and 1990s, however the study of\nsuch systems has greatly declined since that time. As such, we propose updated\nmethods for automatically selecting and refining complex boolean search queries\nthat can isolate relevant results with greater specificity and completeness.\nFurthermore, we present preliminary results of using an optimized query to\ncollect a sample of traffic-incident-related tweets, along with the results of\nmanually classifying and analyzing them.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 19:37:43 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Hufbauer", "Emory", ""], ["Khamfroush", "Hana", ""]]}, {"id": "2006.11955", "submitter": "Nitish Nag", "authors": "Daniel B. Azzam, Nitish Nag, Julia Tran, Lauren Chen, Kaajal Visnagra,\n  Kailey Marshall, Matthew Wade", "title": "A Novel Epidemiological Approach to Geographically Mapping Population\n  Dry Eye Disease in the United States through Google Trends", "comments": "American Society of Cataract and Refractive Surgery Meeting. Boston,\n  Massachusetts. May 18, 2020. Podium", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dry eye disease (DED) affects approximately half of the United States\npopulation. DED is characterized by dryness on the corena surface due to a\nvariety of causes. This study fills the spatiotemporal gaps in DED epidemiology\nby using Google Trends as a novel epidemiological tool for geographically\nmapping DED in relation to environmental risk factors. We utilized Google\nTrends to extract DED-related queries estimating user intent from 2004-2019 in\nthe United States. We incorporated national climate data to generate heat maps\ncomparing geographic, temporal, and environmental relationships of DED.\nMulti-variable regression models were constructed to generate quadratic\nforecasts predicting DED and control searches. Our results illustrated the\nupward trend, seasonal pattern, environmental influence, and spatial\nrelationship of DED search volume across US geography. Localized patches of DED\ninterest were visualized along the coastline. There was no significant\ndifference in DED queries across US census regions. Regression model 1\npredicted DED searches over time (R^2=0.97) with significant predictors being\ncontrol queries (p=0.0024), time (p=0.001), and seasonality (Winter p=0.0028;\nSpring p<0.001; Summer p=0.018). Regression model 2 predicted DED queries per\nstate (R^2=0.49) with significant predictors being temperature (p=0.0003) and\ncoastal zone (p=0.025). Importantly, temperature, coastal status, and\nseasonality were stronger risk factors of DED searches than humidity, sunshine,\npollution, or region as clinical literature may suggest. Our work paves the way\nfor future exploration of geographic information systems for locating DED and\nother diseases via online search query metrics.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 00:56:05 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Azzam", "Daniel B.", ""], ["Nag", "Nitish", ""], ["Tran", "Julia", ""], ["Chen", "Lauren", ""], ["Visnagra", "Kaajal", ""], ["Marshall", "Kailey", ""], ["Wade", "Matthew", ""]]}, {"id": "2006.12166", "submitter": "Rens van de Schoot", "authors": "Rens van de Schoot, Jonathan de Bruin, Raoul Schram, Parisa Zahedi,\n  Jan de Boer, Felix Weijdema, Bianca Kramer, Martijn Huijts, Maarten\n  Hoogerwerf, Gerbrich Ferdinands, Albert Harkema, Joukje Willemsen, Yongchao\n  Ma, Qixiang Fang, Sybren Hindriks, Lars Tummers, Daniel Oberski", "title": "Open Source Software for Efficient and Transparent Reviews", "comments": "All code for the software ASReview is available under an Apache-2.0\n  license at Github: https://github.com/asreview", "journal-ref": null, "doi": "10.1038/s42256-020-00287-7", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To help researchers conduct a systematic review or meta-analysis as\nefficiently and transparently as possible, we designed a tool (ASReview) to\naccelerate the step of screening titles and abstracts. For many tasks -\nincluding but not limited to systematic reviews and meta-analyses - the\nscientific literature needs to be checked systematically. Currently, scholars\nand practitioners screen thousands of studies by hand to determine which\nstudies to include in their review or meta-analysis. This is error prone and\ninefficient because of extremely imbalanced data: only a fraction of the\nscreened studies is relevant. The future of systematic reviewing will be an\ninteraction with machine learning algorithms to deal with the enormous increase\nof available text. We therefore developed an open source machine learning-aided\npipeline applying active learning: ASReview. We demonstrate by means of\nsimulation studies that ASReview can yield far more efficient reviewing than\nmanual reviewing, while providing high quality. Furthermore, we describe the\noptions of the free and open source research software and present the results\nfrom user experience tests. We invite the community to contribute to open\nsource projects such as our own that provide measurable and reproducible\nimprovements over current practice.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 11:57:10 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 07:00:47 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 08:25:18 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["van de Schoot", "Rens", ""], ["de Bruin", "Jonathan", ""], ["Schram", "Raoul", ""], ["Zahedi", "Parisa", ""], ["de Boer", "Jan", ""], ["Weijdema", "Felix", ""], ["Kramer", "Bianca", ""], ["Huijts", "Martijn", ""], ["Hoogerwerf", "Maarten", ""], ["Ferdinands", "Gerbrich", ""], ["Harkema", "Albert", ""], ["Willemsen", "Joukje", ""], ["Ma", "Yongchao", ""], ["Fang", "Qixiang", ""], ["Hindriks", "Sybren", ""], ["Tummers", "Lars", ""], ["Oberski", "Daniel", ""]]}, {"id": "2006.12191", "submitter": "Duan Zhihua", "authors": "Duan Zhihua, Wang JiaLin", "title": "Potential customer mining application of smart home products based on\n  LightGBM PU learning and Spark ML algorithm practice", "comments": "7 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the case of big data-based intelligent product potential\ncustomer mining internal competition in China Telecom Shanghai Company. Huge\namounts of data based on big data table, the use of machine Learning and data\nanalysis technology, using the algorithm of LightGBM, PySpark machine Learning\nalgorithms, Positive Unlabeled Learning algorithm, and predict whether\ncustomers buy whole house product, precision marketing into artificial\nintelligence for the customer, large data capacity, promote the development of\nintelligent products of the company.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 12:42:53 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zhihua", "Duan", ""], ["JiaLin", "Wang", ""]]}, {"id": "2006.12289", "submitter": "Fabrizio Sebastiani", "authors": "Silvia Corbara, Alejandro Moreo, Fabrizio Sebastiani, Mirko Tavoni", "title": "MedLatin1 and MedLatin2: Two Datasets for the Computational Authorship\n  Analysis of Medieval Latin Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present and make available MedLatin1 and MedLatin2, two datasets of\nmedieval Latin texts to be used in research on computational authorship\nanalysis. MedLatin1 and MedLatin2 consist of 294 and 30 curated texts,\nrespectively, labelled by author, with MedLatin1 texts being of an epistolary\nnature and MedLatin2 texts consisting of literary comments and treatises about\nvarious subjects. As such, these two datasets lend themselves to supporting\nresearch in authorship analysis tasks, such as authorship attribution,\nauthorship verification, or same-author verification.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 14:22:47 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Corbara", "Silvia", ""], ["Moreo", "Alejandro", ""], ["Sebastiani", "Fabrizio", ""], ["Tavoni", "Mirko", ""]]}, {"id": "2006.12379", "submitter": "\\'Angel Gonz\\'alez-Prieto", "authors": "Jes\\'us Bobadilla, \\'Angel Gonz\\'alez-Prieto, Fernando Ortega, Ra\\'ul\n  Lara-Cabrera", "title": "Deep Learning feature selection to unhide demographic recommender\n  systems factors", "comments": "20 pages, 14 figures, 1 table", "journal-ref": "Neural Computing and Applications, 1-18, 2020", "doi": "10.1007/s00521-020-05494-2", "report-no": null, "categories": "cs.IR cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting demographic features from hidden factors is an innovative concept\nthat provides multiple and relevant applications. The matrix factorization\nmodel generates factors which do not incorporate semantic knowledge. This paper\nprovides a deep learning-based method: DeepUnHide, able to extract demographic\ninformation from the users and items factors in collaborative filtering\nrecommender systems. The core of the proposed method is the gradient-based\nlocalization used in the image processing literature to highlight the\nrepresentative areas of each classification class. Validation experiments make\nuse of two public datasets and current baselines. Results show the superiority\nof DeepUnHide to make feature selection and demographic classification,\ncompared to the state of art of feature selection methods. Relevant and direct\napplications include recommendations explanation, fairness in collaborative\nfiltering and recommendation to groups of users.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 17:36:48 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Bobadilla", "Jes\u00fas", ""], ["Gonz\u00e1lez-Prieto", "\u00c1ngel", ""], ["Ortega", "Fernando", ""], ["Lara-Cabrera", "Ra\u00fal", ""]]}, {"id": "2006.12382", "submitter": "Brett Vintch", "authors": "Brett Vintch", "title": "Quick Lists: Enriched Playlist Embeddings for Future Playlist\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending playlists to users in the context of a digital music service is\na difficult task because a playlist is often more than the mere sum of its\nparts. We present a novel method for generating playlist embeddings that are\ninvariant to playlist length and sensitive to local and global track ordering.\nThe embeddings also capture information about playlist sequencing, and are\nenriched with side information about the playlist user. We show that these\nembeddings are useful for generating next-best playlist recommendations, and\nthat side information can be used for the cold start problem.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 17:08:52 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Vintch", "Brett", ""]]}, {"id": "2006.12425", "submitter": "Shan Li", "authors": "Shan Li, Baoxu Shi, Jaewon Yang, Ji Yan, Shuai Wang, Fei Chen, Qi He", "title": "Deep Job Understanding at LinkedIn", "comments": "4 pages, to appear in SIGIR2020", "journal-ref": null, "doi": "10.1145/3397271.3401403", "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the world's largest professional network, LinkedIn wants to create\neconomic opportunity for everyone in the global workforce. One of its most\ncritical missions is matching jobs with processionals. Improving job targeting\naccuracy and hire efficiency align with LinkedIn's Member First Motto. To\nachieve those goals, we need to understand unstructured job postings with noisy\ninformation. We applied deep transfer learning to create domain-specific job\nunderstanding models. After this, jobs are represented by professional\nentities, including titles, skills, companies, and assessment questions. To\ncontinuously improve LinkedIn's job understanding ability, we designed an\nexpert feedback loop where we integrated job understanding models into\nLinkedIn's products to collect job posters' feedback. In this demonstration, we\npresent LinkedIn's job posting flow and demonstrate how the integrated deep job\nunderstanding work improves job posters' satisfaction and provides significant\nmetric lifts in LinkedIn's job recommendation system.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 20:04:59 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Li", "Shan", ""], ["Shi", "Baoxu", ""], ["Yang", "Jaewon", ""], ["Yan", "Ji", ""], ["Wang", "Shuai", ""], ["Chen", "Fei", ""], ["He", "Qi", ""]]}, {"id": "2006.12608", "submitter": "Francesco Silvestri", "authors": "Thomas D. Ahle and Francesco Silvestri", "title": "Similarity Search with Tensor Core Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor Core Units (TCUs) are hardware accelerators developed for deep neural\nnetworks, which efficiently support the multiplication of two dense\n$\\sqrt{m}\\times \\sqrt{m}$ matrices, where $m$ is a given hardware parameter. In\nthis paper, we show that TCUs can speed up similarity search problems as well.\nWe propose algorithms for the Johnson-Lindenstrauss dimensionality reduction\nand for similarity join that, by leveraging TCUs, achieve a $\\sqrt{m}$ speedup\nup with respect to traditional approaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 20:47:38 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Ahle", "Thomas D.", ""], ["Silvestri", "Francesco", ""]]}, {"id": "2006.12870", "submitter": "Jennifer D'Souza", "authors": "Jennifer D'Souza and S\\\"oren Auer", "title": "NLPContributions: An Annotation Scheme for Machine Reading of Scholarly\n  Contributions in Natural Language Processing Literature", "comments": "In Proceedings of the 1st Workshop on Extraction and Evaluation of\n  Knowledge Entities from Scientific Documents (EEKE 2020) co-located with the\n  ACM/IEEE Joint Conference on Digital Libraries in 2020 (JCDL 2020), Virtual\n  Event, China, August 1. http://ceur-ws.org/Vol-2658/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We describe an annotation initiative to capture the scholarly contributions\nin natural language processing (NLP) articles, particularly, for the articles\nthat discuss machine learning (ML) approaches for various information\nextraction tasks. We develop the annotation task based on a pilot annotation\nexercise on 50 NLP-ML scholarly articles presenting contributions to five\ninformation extraction tasks 1. machine translation, 2. named entity\nrecognition, 3. question answering, 4. relation classification, and 5. text\nclassification. In this article, we describe the outcomes of this pilot\nannotation phase. Through the exercise we have obtained an annotation\nmethodology; and found ten core information units that reflect the contribution\nof the NLP-ML scholarly investigations. The resulting annotation scheme we\ndeveloped based on these information units is called NLPContributions.\n  The overarching goal of our endeavor is four-fold: 1) to find a systematic\nset of patterns of subject-predicate-object statements for the semantic\nstructuring of scholarly contributions that are more or less generically\napplicable for NLP-ML research articles; 2) to apply the discovered patterns in\nthe creation of a larger annotated dataset for training machine readers of\nresearch contributions; 3) to ingest the dataset into the Open Research\nKnowledge Graph (ORKG) infrastructure as a showcase for creating user-friendly\nstate-of-the-art overviews; 4) to integrate the machine readers into the ORKG\nto assist users in the manual curation of their respective article\ncontributions. We envision that the NLPContributions methodology engenders a\nwider discussion on the topic toward its further refinement and development.\nOur pilot annotated dataset of 50 NLP-ML scholarly articles according to the\nNLPContributions scheme is openly available to the research community at\nhttps://doi.org/10.25835/0019761.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 10:04:39 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 09:52:38 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 05:23:56 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["D'Souza", "Jennifer", ""], ["Auer", "S\u00f6ren", ""]]}, {"id": "2006.12999", "submitter": "Ziming Li", "authors": "Ziming Li, Julia Kiseleva, Alekh Agarwal, Maarten de Rijke, Ryen W.\n  White", "title": "Optimizing Interactive Systems via Data-Driven Objectives", "comments": "30 pages, 12 figures. arXiv admin note: text overlap with\n  arXiv:1802.06306", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective optimization is essential for real-world interactive systems to\nprovide a satisfactory user experience in response to changing user behavior.\nHowever, it is often challenging to find an objective to optimize for\ninteractive systems (e.g., policy learning in task-oriented dialog systems).\nGenerally, such objectives are manually crafted and rarely capture complex user\nneeds in an accurate manner. We propose an approach that infers the objective\ndirectly from observed user interactions. These inferences can be made\nregardless of prior knowledge and across different types of user behavior. We\nintroduce Interactive System Optimizer (ISO), a novel algorithm that uses these\ninferred objectives for optimization. Our main contribution is a new general\nprincipled approach to optimizing interactive systems using data-driven\nobjectives. We demonstrate the high effectiveness of ISO over several\nsimulations.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 20:49:14 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Li", "Ziming", ""], ["Kiseleva", "Julia", ""], ["Agarwal", "Alekh", ""], ["de Rijke", "Maarten", ""], ["White", "Ryen W.", ""]]}, {"id": "2006.13063", "submitter": "Gabriel De Souza Pereira Moreira", "authors": "Gabriel de Souza P. Moreira, Dietmar Jannach, Adilson Marques da Cunha", "title": "Hybrid Session-based News Recommendation using Recurrent Neural Networks", "comments": "From the Proceeding of the LatinX in AI Research (LXAI) at ICML 2020.\n  arXiv admin note: text overlap with arXiv:1904.10367", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a hybrid meta-architecture -- the CHAMELEON -- for session-based\nnews recommendation that is able to leverage a variety of information types\nusing Recurrent Neural Networks. We evaluated our approach on two public\ndatasets, using a temporal evaluation protocol that simulates the dynamics of a\nnews portal in a realistic way. Our results confirm the benefits of modeling\nthe sequence of session clicks with RNNs and leveraging side information about\nusers and articles, resulting in significantly higher recommendation accuracy\nand catalog coverage than other session-based algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:24:43 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Moreira", "Gabriel de Souza P.", ""], ["Jannach", "Dietmar", ""], ["da Cunha", "Adilson Marques", ""]]}, {"id": "2006.13257", "submitter": "Shen Wang", "authors": "Shen Wang, Jibing Gong, Jinlong Wang, Wenzheng Feng, Hao Peng, Jie\n  Tang, Philip S. Yu", "title": "Attentional Graph Convolutional Networks for Knowledge Concept\n  Recommendation in MOOCs in a Heterogeneous View", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive open online courses are becoming a modish way for education, which\nprovides a large-scale and open-access learning opportunity for students to\ngrasp the knowledge. To attract students' interest, the recommendation system\nis applied by MOOCs providers to recommend courses to students. However, as a\ncourse usually consists of a number of video lectures, with each one covering\nsome specific knowledge concepts, directly recommending courses overlook\nstudents'interest to some specific knowledge concepts. To fill this gap, in\nthis paper, we study the problem of knowledge concept recommendation. We\npropose an end-to-end graph neural network-based approach\ncalledAttentionalHeterogeneous Graph Convolutional Deep Knowledge\nRecommender(ACKRec) for knowledge concept recommendation in MOOCs. Like other\nrecommendation problems, it suffers from sparsity issues. To address this\nissue, we leverage both content information and context information to learn\nthe representation of entities via graph convolution network. In addition to\nstudents and knowledge concepts, we consider other types of entities (e.g.,\ncourses, videos, teachers) and construct a heterogeneous information network to\ncapture the corresponding fruitful semantic relationships among different types\nof entities and incorporate them into the representation learning process.\nSpecifically, we use meta-path on the HIN to guide the propagation of students'\npreferences. With the help of these meta-paths, the students' preference\ndistribution with respect to a candidate knowledge concept can be captured.\nFurthermore, we propose an attention mechanism to adaptively fuse the context\ninformation from different meta-paths, in order to capture the different\ninterests of different students. The promising experiment results show that the\nproposedACKRecis able to effectively recommend knowledge concepts to students\npursuing online learning in MOOCs.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 18:28:08 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Wang", "Shen", ""], ["Gong", "Jibing", ""], ["Wang", "Jinlong", ""], ["Feng", "Wenzheng", ""], ["Peng", "Hao", ""], ["Tang", "Jie", ""], ["Yu", "Philip S.", ""]]}, {"id": "2006.13417", "submitter": "Dezhou Shen", "authors": "Dezhou Shen", "title": "Movie Box office Prediction via Joint Actor Representations and Social\n  Media Sentiment", "comments": "9 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, driven by the Asian film industry, such as China and India,\nthe global box office has maintained a steady growth trend. Previous studies\nhave rarely used long-term, full-sample film data in analysis, lack of research\non actors' social networks. Existing film box office prediction algorithms only\nuse film meta-data, lack of using social network characteristics and the model\nis less interpretable. I propose a FC-GRU-CNN binary classification model in of\nbox office prediction task, combining five characteristics, including the film\nmeta-data, Sina Weibo text sentiment, actors' social network measurement, all\npairs shortest path and actors' art contribution. Exploiting long-term memory\nability of GRU layer in long sequences and the mapping ability of CNN layer in\nretrieving all pairs shortest path matrix features, proposed model is 14%\nhigher in accuracy than the current best C-LSTM model.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 01:35:31 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Shen", "Dezhou", ""]]}, {"id": "2006.13438", "submitter": "Diego Chialva", "authors": "Diego Chialva, Alexis-Michel Mugabushaka", "title": "DINGO: an ontology for projects and grants linked data", "comments": "Accepted for the SKG2020 Workshop co-located with TPDL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DINGO (Data INtegration for Grants Ontology), an ontology that\nprovides a machine readable extensible framework to model data for\nsemantically-enabled applications relative to projects, funding, actors, and,\nnotably, funding policies in the research landscape. DINGO is designed to yield\nhigh modeling power and elasticity to cope with the huge variety in funding,\nresearch and policy practices, which makes it applicable also to other areas\nbesides research where funding is an important aspect. We discuss its main\nfeatures, the principles followed for its development, its community uptake,\nits maintenance and evolution.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 02:47:40 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Chialva", "Diego", ""], ["Mugabushaka", "Alexis-Michel", ""]]}, {"id": "2006.13721", "submitter": "Carsten Eickhoff", "authors": "Cindy Li, Elizabeth Chen, Guergana Savova, Hamish Fraser, Carsten\n  Eickhoff", "title": "Mining Misdiagnosis Patterns from Biomedical Literature", "comments": "AMIA Joint Summits in Translational Science, 2020", "journal-ref": "AMIA Jt Summits Transl Sci Proc. 2020;2020:360-366. Published 2020\n  May 30", "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnostic errors can pose a serious threat to patient safety, leading to\nserious harm and even death. Efforts are being made to develop interventions\nthat allow physicians to reassess for errors and improve diagnostic accuracy.\nOur study presents an exploration of misdiagnosis patterns mined from PubMed\nabstracts. Article titles containing certain phrases indicating misdiagnosis\nwere selected and frequencies of these misdiagnoses calculated. We present the\nresulting patterns in the form of a directed graph with frequency-weighted\nmisdiagnosis edges connecting diagnosis vertices. We find that the most\ncommonly misdiagnosed diseases were often misdiagnosed as many different\ndiseases, with each misdiagnosis having a relatively low frequency, rather than\nas a single disease with greater probability. Additionally, while a\nmisdiagnosis relationship may generally exist, the relationship was often found\nto be one-sided.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 13:34:43 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Li", "Cindy", ""], ["Chen", "Elizabeth", ""], ["Savova", "Guergana", ""], ["Fraser", "Hamish", ""], ["Eickhoff", "Carsten", ""]]}, {"id": "2006.13737", "submitter": "Carsten Eickhoff", "authors": "Gil Alon, Elizabeth Chen, Guergana Savova, Carsten Eickhoff", "title": "Diagnosis Prevalence vs. Efficacy in Machine-learning Based Diagnostic\n  Decision Support", "comments": "AMIA Joint Summits in Translational Science, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent studies use machine learning to predict a small number of\nICD-9-CM codes. In practice, on the other hand, physicians have to consider a\nbroader range of diagnoses. This study aims to put these previously incongruent\nevaluation settings on a more equal footing by predicting ICD-9-CM codes based\non electronic health record properties and demonstrating the relationship\nbetween diagnosis prevalence and system performance. We extracted patient\nfeatures from the MIMIC-III dataset for each admission. We trained and\nevaluated 43 different machine learning classifiers. Among this pool, the most\nsuccessful classifier was a Multi-Layer Perceptron. In accordance with general\nmachine learning expectation, we observed all classifiers' F1 scores to drop as\ndisease prevalence decreased. Scores fell from 0.28 for the 50 most prevalent\nICD-9-CM codes to 0.03 for the 1000 most prevalent ICD-9-CM codes. Statistical\nanalyses showed a moderate positive correlation between disease prevalence and\nefficacy (0.5866).\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 13:52:04 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Alon", "Gil", ""], ["Chen", "Elizabeth", ""], ["Savova", "Guergana", ""], ["Eickhoff", "Carsten", ""]]}, {"id": "2006.13816", "submitter": "Bernal Jimenez Gutierrez", "authors": "Bernal Jim\\'enez Guti\\'errez, Juncheng Zeng, Dongdong Zhang, Ping\n  Zhang, Yu Su", "title": "Document Classification for COVID-19 Literature", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The global pandemic has made it more important than ever to quickly and\naccurately retrieve relevant scientific literature for effective consumption by\nresearchers in a wide range of fields. We provide an analysis of several\nmulti-label document classification models on the LitCovid dataset, a growing\ncollection of 23,000 research papers regarding the novel 2019 coronavirus. We\nfind that pre-trained language models fine-tuned on this dataset outperform all\nother baselines and that BioBERT surpasses the others by a small margin with\nmicro-F1 and accuracy scores of around 86% and 75% respectively on the test\nset. We evaluate the data efficiency and generalizability of these models as\nessential features of any system prepared to deal with an urgent situation like\nthe current health crisis. Finally, we explore 50 errors made by the best\nperforming models on LitCovid documents and find that they often (1) correlate\ncertain labels too closely together and (2) fail to focus on discriminative\nsections of the articles; both of which are important issues to address in\nfuture work. Both data and code are available on GitHub.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 20:03:28 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 21:58:17 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Guti\u00e9rrez", "Bernal Jim\u00e9nez", ""], ["Zeng", "Juncheng", ""], ["Zhang", "Dongdong", ""], ["Zhang", "Ping", ""], ["Su", "Yu", ""]]}, {"id": "2006.13864", "submitter": "Yongzhen Wang", "authors": "Guoqing Zhu, Naga Anjaneyulu Kopalle, Yongzhen Wang, Xiaozhong Liu,\n  Kemi Jona, Katy B\\\"orner", "title": "Community-Based Data Integration of Course and Job Data in Support of\n  Personalized Career-Education Recommendations", "comments": "6 pages, 1 figure, 2 tables", "journal-ref": null, "doi": "10.1002/pra2.324", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does your education impact your professional career? Ideally, the courses\nyou take help you identify, get hired for, and perform the job you always\nwanted. However, not all courses provide skills that transfer to existing and\nfuture jobs; skill terms used in course descriptions might be different from\nthose listed in job advertisements; and there might exist a considerable skill\ngap between what is taught in courses and what is needed for a job. In this\nstudy, we propose a novel method to integrate extensive course description and\njob advertisement data by leveraging heterogeneous data integration and\ncommunity detection. The innovative heterogeneous graph approach along with\nidentified skill communities enables cross-domain information recommendation,\ne.g., given an educational profile, job recommendations can be provided\ntogether with suggestions on education opportunities for re- and upskilling in\nsupport of lifelong learning.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 16:46:07 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Zhu", "Guoqing", ""], ["Kopalle", "Naga Anjaneyulu", ""], ["Wang", "Yongzhen", ""], ["Liu", "Xiaozhong", ""], ["Jona", "Kemi", ""], ["B\u00f6rner", "Katy", ""]]}, {"id": "2006.14279", "submitter": "Erion \\c{C}ano", "authors": "Erion \\c{C}ano, Riccardo Coppola, Eleonora Gargiulo, Marco Marengo,\n  Maurizio Morisio", "title": "Mood-based On-Car Music Recommendations", "comments": "11 pages, 5 figures. Published in proceedings of INISCOM 2016, the\n  2nd International Conference on Industrial Networks and Intelligent Systems,\n  Leicester, UK", "journal-ref": null, "doi": "10.1007/978-3-319-52569-3_14", "report-no": null, "categories": "cs.HC cs.IR cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving and music listening are two inseparable everyday activities for\nmillions of people today in the world. Considering the high correlation between\nmusic, mood and driving comfort and safety, it makes sense to use appropriate\nand intelligent music recommendations based on the mood of drivers and songs in\nthe context of car driving. The objective of this paper is to present the\nproject of a contextual mood-based music recommender system capable of\nregulating the driver's mood and trying to have a positive influence on her\ndriving behaviour. Here we present the proof of concept of the system and\ndescribe the techniques and technologies that are part of it. Further possible\nfuture improvements on each of the building blocks are also presented.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 09:50:26 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["\u00c7ano", "Erion", ""], ["Coppola", "Riccardo", ""], ["Gargiulo", "Eleonora", ""], ["Marengo", "Marco", ""], ["Morisio", "Maurizio", ""]]}, {"id": "2006.14318", "submitter": "Simone Santini", "authors": "Simone Santini", "title": "A random walk on Area Restricted Search", "comments": "67 pages, 30 figures", "journal-ref": null, "doi": null, "report-no": "Cahiers d'informatique N. 26", "categories": "cond-mat.stat-mech cs.IR cs.SI math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  These notes from a graduate class at the Unuversidad Autonoma de Madrid\nanalyze a search behavior known as Area Resticted Search (ARS), widespread in\nthe animal kingdom, and optimal when the resources that one is after are\n\"patchy\". In the first section we study the importance of the behavior in\nanimal and its dependence on the dopamine as a indicator of reward. In the\nsecond section we put together a genetic algorithm to determine the optimality\nof ARS and its characteristics. Finally, we relate ARS to a type of random\nwalks known as \"Levy Walks\", in which the probability of jumping at a distance\nd from the current location follows a power law distribution.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 11:46:44 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Santini", "Simone", ""]]}, {"id": "2006.14492", "submitter": "Dimitar Dimitrov", "authors": "Dimitar Dimitrov, Erdal Baran, Pavlos Fafalios, Ran Yu, Xiaofei Zhu,\n  Matth\\\"aus Zloch, and Stefan Dietze", "title": "TweetsCOV19 -- A Knowledge Base of Semantically Annotated Tweets about\n  the COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": "10.1145/3340531.3412765", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Publicly available social media archives facilitate research in the social\nsciences and provide corpora for training and testing a wide range of machine\nlearning and natural language processing methods. With respect to the recent\noutbreak of the Coronavirus disease 2019 (COVID-19), online discourse on\nTwitter reflects public opinion and perception related to the pandemic itself\nas well as mitigating measures and their societal impact. Understanding such\ndiscourse, its evolution, and interdependencies with real-world events or\n(mis)information can foster valuable insights. On the other hand, such corpora\nare crucial facilitators for computational methods addressing tasks such as\nsentiment analysis, event detection, or entity recognition. However, obtaining,\narchiving, and semantically annotating large amounts of tweets is costly. In\nthis paper, we describe TweetsCOV19, a publicly available knowledge base of\ncurrently more than 8 million tweets, spanning October 2019 - April 2020.\nMetadata about the tweets as well as extracted entities, hashtags, user\nmentions, sentiments, and URLs are exposed using established RDF/S\nvocabularies, providing an unprecedented knowledge base for a range of\nknowledge discovery tasks. Next to a description of the dataset and its\nextraction and annotation process, we present an initial analysis and use cases\nof the corpus.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 15:43:40 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 08:20:49 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 13:25:23 GMT"}, {"version": "v4", "created": "Sat, 15 Aug 2020 06:49:14 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Dimitrov", "Dimitar", ""], ["Baran", "Erdal", ""], ["Fafalios", "Pavlos", ""], ["Yu", "Ran", ""], ["Zhu", "Xiaofei", ""], ["Zloch", "Matth\u00e4us", ""], ["Dietze", "Stefan", ""]]}, {"id": "2006.14765", "submitter": "Tingmin Wu", "authors": "Tingmin Wu, Wanlun Ma, Sheng Wen, Xin Xia, Cecile Paris, Surya Nepal,\n  Yang Xiang", "title": "Analysis of Trending Topics and Text-based Channels of Information\n  Delivery in Cybersecurity", "comments": "13 pages (main content) + 4 pages (references and appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer users are generally faced with difficulties in making correct\nsecurity decisions. While an increasingly fewer number of people are trying or\nwilling to take formal security training, online sources including news,\nsecurity blogs, and websites are continuously making security knowledge more\naccessible. Analysis of cybersecurity texts can provide insights into the\ntrending topics and identify current security issues as well as how cyber\nattacks evolve over time. These in turn can support researchers and\npractitioners in predicting and preparing for these attacks. Comparing\ndifferent sources may facilitate the learning process for normal users by\npersisting the security knowledge gained from different cybersecurity context.\nPrior studies neither systematically analysed the wide-range of digital sources\nnor provided any standardisation in analysing the trending topics from recent\nsecurity texts. Although LDA has been widely adopted in topic generation, its\ngenerated topics cannot cover the cybersecurity concepts completely and\nconsiderably overlap. To address this issue, we propose a semi-automated\nclassification method to generate comprehensive security categories instead of\nLDA-generated topics. We further compare the identified 16 security categories\nacross different sources based on their popularity and impact. We have revealed\nseveral surprising findings. (1) The impact reflected from cyber-security texts\nstrongly correlates with the monetary loss caused by cybercrimes. (2) For most\ncategories, security blogs share the largest popularity and largest\nabsolute/relative impact over time. (3) Websites deliver security information\nwithout caring about timeliness much, where one third of the articles do not\nspecify the date and the rest have a time lag in posting emerging security\nissues.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 03:00:04 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Wu", "Tingmin", ""], ["Ma", "Wanlun", ""], ["Wen", "Sheng", ""], ["Xia", "Xin", ""], ["Paris", "Cecile", ""], ["Nepal", "Surya", ""], ["Xiang", "Yang", ""]]}, {"id": "2006.14774", "submitter": "Kumar Vijay Mishra", "authors": "Jiawei Liu, Kumar Vijay Mishra and Mohammad Saquib", "title": "Co-Designing Statistical MIMO Radar and In-band Full-Duplex Multi-User\n  MIMO Communications", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a spectral co-design of a statistical\nmultiple-input-multiple-output (MIMO) radar and an in-band full-duplex (IBFD)\nmulti-user MIMO (MU-MIMO) communications system both of which concurrently\noperate within the same frequency band. Prior works on\nMIMO-radar-MIMO-communications (MRMC) problem either focus on colocated MIMO\nradars and half-duplex/single-user MIMO communications, seek coexistence\nsolutions, do not jointly design waveforms and receiver processing or omit\npractical system constraints. Here, we jointly design statistical MIMO radar\nwaveform, uplink (UL)/downlink (DL) precoders, and receive filters. To this\nend, we employ a novel performance measure, namely compounded-and-weighted sum\nmutual information (CWSM), that is subjected to multiple practical constraints\nof UL/DL transmit power, UL/DL quality of service, and\npeak-to-average-power-ratio. We solve the resulting non-convex problem by\nincorporating block coordinate descent (BCD) and alternating projection (AP)\nmethods in a single algorithmic framework called BCD-AP MRMC. We achieve this\nby exploiting the relationship between mutual information and weighted minimum\nmean-squared-error (WMMSE), which allows use of the Lagrange dual problem in\nfinding closed-form solutions for precoders and radar waveform. Numerical\nexperiments show that our proposed WMMSE-based method quickly achieves\nmonotonic convergence, improves target detection by $6$-$13$% compared to\nconventional radar coding, and provides $8.3$-$30$% higher rate in IBFD MU-MIMO\nsystem than other precoding strategies.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 03:22:24 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Liu", "Jiawei", ""], ["Mishra", "Kumar Vijay", ""], ["Saquib", "Mohammad", ""]]}, {"id": "2006.14806", "submitter": "Xiang Deng", "authors": "Xiang Deng, Huan Sun, Alyssa Lees, You Wu, Cong Yu", "title": "TURL: Table Understanding through Representation Learning", "comments": "Accepted to VLDB 2021. Extended version with experiments added during\n  revision. Our source code, benchmark, as well as pre-trained models will be\n  available on https://github.com/sunlab-osu/TURL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Relational tables on the Web store a vast amount of knowledge. Owing to the\nwealth of such tables, there has been tremendous progress on a variety of tasks\nin the area of table understanding. However, existing work generally relies on\nheavily-engineered task-specific features and model architectures. In this\npaper, we present TURL, a novel framework that introduces the\npre-training/fine-tuning paradigm to relational Web tables. During\npre-training, our framework learns deep contextualized representations on\nrelational tables in an unsupervised manner. Its universal model design with\npre-trained representations can be applied to a wide range of tasks with\nminimal task-specific fine-tuning. Specifically, we propose a structure-aware\nTransformer encoder to model the row-column structure of relational tables, and\npresent a new Masked Entity Recovery (MER) objective for pre-training to\ncapture the semantics and knowledge in large-scale unlabeled data. We\nsystematically evaluate TURL with a benchmark consisting of 6 different tasks\nfor table understanding (e.g., relation extraction, cell filling). We show that\nTURL generalizes well to all tasks and substantially outperforms existing\nmethods in almost all instances.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 05:44:54 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 02:47:41 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Deng", "Xiang", ""], ["Sun", "Huan", ""], ["Lees", "Alyssa", ""], ["Wu", "You", ""], ["Yu", "Cong", ""]]}, {"id": "2006.14808", "submitter": "Masayoshi Aritsugi", "authors": "Riku Anegawa and Masayoshi Aritsugi", "title": "Text Detection on Roughly Placed Books by Leveraging a Learning-based\n  Model Trained with Another Domain Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text detection enables us to extract rich information from images. In this\npaper, we focus on how to generate bounding boxes that are appropriate to grasp\ntext areas on books to help implement automatic text detection. We attempt not\nto improve a learning-based model by training it with an enough amount of data\nin the target domain but to leverage it, which has been already trained with\nanother domain data. We develop algorithms that construct the bounding boxes by\nimproving and leveraging the results of a learning-based method. Our algorithms\ncan utilize different learning-based approaches to detect scene texts.\nExperimental evaluations demonstrate that our algorithms work well in various\nsituations where books are roughly placed.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 05:53:23 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Anegawa", "Riku", ""], ["Aritsugi", "Masayoshi", ""]]}, {"id": "2006.14827", "submitter": "Xiangyu Zhao", "authors": "Xiangyu Zhao, Haochen Liu, Hui Liu, Jiliang Tang, Weiwei Guo, Jun Shi,\n  Sida Wang, Huiji Gao, Bo Long", "title": "Memory-efficient Embedding for Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical large-scale recommender systems usually contain thousands of\nfeature fields from users, items, contextual information, and their\ninteractions. Most of them empirically allocate a unified dimension to all\nfeature fields, which is memory inefficient. Thus it is highly desired to\nassign different embedding dimensions to different feature fields according to\ntheir importance and predictability. Due to the large amounts of feature fields\nand the nuanced relationship between embedding dimensions with feature\ndistributions and neural network architectures, manually allocating embedding\ndimensions in practical recommender systems can be very difficult. To this end,\nwe propose an AutoML based framework (AutoDim) in this paper, which can\nautomatically select dimensions for different feature fields in a data-driven\nfashion. Specifically, we first proposed an end-to-end differentiable framework\nthat can calculate the weights over various dimensions for feature fields in a\nsoft and continuous manner with an AutoML based optimization algorithm; then we\nderive a hard and discrete embedding component architecture according to the\nmaximal weights and retrain the whole recommender framework. We conduct\nextensive experiments on benchmark datasets to validate the effectiveness of\nthe AutoDim framework.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 07:07:59 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 19:15:37 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zhao", "Xiangyu", ""], ["Liu", "Haochen", ""], ["Liu", "Hui", ""], ["Tang", "Jiliang", ""], ["Guo", "Weiwei", ""], ["Shi", "Jun", ""], ["Wang", "Sida", ""], ["Gao", "Huiji", ""], ["Long", "Bo", ""]]}, {"id": "2006.14939", "submitter": "Jipeng Qiang J", "authors": "Jipeng Qiang and Yun Li and Yi Zhu and Yunhao Yuan and Xindong Wu", "title": "LSBert: A Simple Framework for Lexical Simplification", "comments": "arXiv admin note: text overlap with arXiv:1907.06226", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lexical simplification (LS) aims to replace complex words in a given sentence\nwith their simpler alternatives of equivalent meaning, to simplify the\nsentence. Recently unsupervised lexical simplification approaches only rely on\nthe complex word itself regardless of the given sentence to generate candidate\nsubstitutions, which will inevitably produce a large number of spurious\ncandidates. In this paper, we propose a lexical simplification framework LSBert\nbased on pretrained representation model Bert, that is capable of (1) making\nuse of the wider context when both detecting the words in need of\nsimplification and generating substitue candidates, and (2) taking five\nhigh-quality features into account for ranking candidates, including Bert\nprediction order, Bert-based language model, and the paraphrase database PPDB,\nin addition to the word frequency and word similarity commonly used in other LS\nmethods. We show that our system outputs lexical simplifications that are\ngrammatically correct and semantically appropriate, and obtains obvious\nimprovement compared with these baselines, outperforming the state-of-the-art\nby 29.8 Accuracy points on three well-known benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 09:15:42 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Qiang", "Jipeng", ""], ["Li", "Yun", ""], ["Zhu", "Yi", ""], ["Yuan", "Yunhao", ""], ["Wu", "Xindong", ""]]}, {"id": "2006.14979", "submitter": "Juan C. Correa", "authors": "J.C. Correa, H. Laverde-Rojas, F. Marmolejo-Ramos, J. Tejada, \\v{S}.\n  Bahn\\'ik", "title": "The Sci-hub Effect: Sci-hub downloads lead to more article citations", "comments": "19 pages, 8 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Citations are often used as a metric of the impact of scientific\npublications. Here, we examine how the number of downloads from Sci-hub as well\nas various characteristics of publications and their authors predicts future\ncitations. Using data from 12 leading journals in economics, consumer research,\nneuroscience, and multidisciplinary research, we found that articles downloaded\nfrom Sci-hub were cited 1.72 times more than papers not downloaded from Sci-hub\nand that the number of downloads from Sci-hub was a robust predictor of future\ncitations. Among other characteristics of publications, the number of figures\nin a manuscript consistently predicts its future citations. The results suggest\nthat limited access to publications may limit some scientific research from\nachieving its full impact.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 13:29:07 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 18:27:38 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Correa", "J. C.", ""], ["Laverde-Rojas", "H.", ""], ["Marmolejo-Ramos", "F.", ""], ["Tejada", "J.", ""], ["Bahn\u00edk", "\u0160.", ""]]}, {"id": "2006.15346", "submitter": "Jing Zhu", "authors": "Jing Zhu, Yanan Xu and Yanmin Zhu", "title": "Modeling Long-Term and Short-Term Interests with Parallel Attentions for\n  Session-based Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of session-based recommendation is to predict the users' next clicked\nitem, which is a challenging task due to the inherent uncertainty in user\nbehaviors and anonymous implicit feedback information. A powerful session-based\nrecommender can typically explore the users' evolving interests (i.e., a\ncombination of his/her long-term and short-term interests). Recent advances in\nattention mechanisms have led to state-of-the-art methods for solving this\ntask. However, there are two main drawbacks. First, most of the attention-based\nmethods only simply utilize the last clicked item to represent the user's\nshort-term interest ignoring the temporal information and behavior context,\nwhich may fail to capture the recent preference of users comprehensively.\nSecond, current studies typically think long-term and short-term interests as\nequally important, but the importance of them should be user-specific.\nTherefore, we propose a novel Parallel Attention Network model (PAN) for\nSession-based Recommendation. Specifically, we propose a novel time-aware\nattention mechanism to learn user's short-term interest by taking into account\nthe contextual information and temporal signals simultaneously. Besides, we\nintroduce a gated fusion method that adaptively integrates the user's long-term\nand short-term preferences to generate the hybrid interest representation.\nExperiments on the three real-world datasets show that PAN achieves obvious\nimprovements than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 11:47:51 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 06:43:00 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Zhu", "Jing", ""], ["Xu", "Yanan", ""], ["Zhu", "Yanmin", ""]]}, {"id": "2006.15399", "submitter": "David Sears", "authors": "David R. W. Sears and Gerhard Widmer", "title": "Beneath (or beyond) the surface: Discovering voice-leading patterns with\n  skip-grams", "comments": "This is an original manuscript / preprint of an article published by\n  Taylor & Francis in the Journal of Mathematics and Music, available online:\n  https://doi.org/10.1080/17459737.2020.1785568. 26 pages, 8 figures, 3 tables", "journal-ref": null, "doi": "10.1080/17459737.2020.1785568", "report-no": null, "categories": "cs.SD cs.IR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent voice-leading patterns like the Mi-Re-Do compound cadence (MRDCC)\nrarely appear on the musical surface in complex polyphonic textures, so finding\nthese patterns using computational methods remains a tremendous challenge. The\npresent study extends the canonical n-gram approach by using skip-grams, which\ninclude sub-sequences in an n-gram list if their constituent members occur\nwithin a certain number of skips. We compiled four data sets of Western tonal\nmusic consisting of symbolic encodings of the notated score and a recorded\nperformance, created a model pipeline for defining, counting, filtering, and\nranking skip-grams, and ranked the position of the MRDCC in every possible\nmodel configuration. We found that the MRDCC receives a higher rank in the list\nwhen the pipeline employs 5 skips, filters the list by excluding n-gram types\nthat do not reflect a genuine harmonic change between adjacent members, and\nranks the remaining types using a statistical association measure.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 16:21:18 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Sears", "David R. W.", ""], ["Widmer", "Gerhard", ""]]}, {"id": "2006.15411", "submitter": "David Sears", "authors": "David R. W. Sears", "title": "String-based methods for tonal harmony: A corpus study of Haydn's string\n  quartets", "comments": "This is an original manuscript / preprint of a book chapter: Sears,\n  David R. W (in press). String-based methods for tonal harmony: A corpus study\n  of Haydn's string quartets.\" In D. Shanahan, A. Burgoyne, & I. Quinn (Eds.),\n  Oxford Handbook of Music and Corpus Studies. New York: Oxford University\n  Press. The manuscript contains 2 musical examples, 3 figures, and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter considers how string-based methods might be adapted to address\nmusic-analytic questions related to the discovery of musical organization, with\nparticular attention devoted to the analysis of tonal harmony. I begin by\napplying the taxonomy of mental organization proposed by Mandler (1979) to the\nconcept of musical organization. Using this taxonomy as a guide, I then present\nevidence for three principles of tonal harmony -- recurrence, syntax, and\nrecursion -- using a corpus of Haydn string quartets.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 17:42:15 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Sears", "David R. W.", ""]]}, {"id": "2006.15498", "submitter": "Jingtao Zhan", "authors": "Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Min Zhang, Shaoping Ma", "title": "RepBERT: Contextualized Text Embeddings for First-Stage Retrieval", "comments": "For corresponding code and data, see\n  https://github.com/jingtaozhan/RepBERT-Index", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although exact term match between queries and documents is the dominant\nmethod to perform first-stage retrieval, we propose a different approach,\ncalled RepBERT, to represent documents and queries with fixed-length\ncontextualized embeddings. The inner products of query and document embeddings\nare regarded as relevance scores. On MS MARCO Passage Ranking task, RepBERT\nachieves state-of-the-art results among all initial retrieval techniques. And\nits efficiency is comparable to bag-of-words methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 03:46:32 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 12:51:00 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhan", "Jingtao", ""], ["Mao", "Jiaxin", ""], ["Liu", "Yiqun", ""], ["Zhang", "Min", ""], ["Ma", "Shaoping", ""]]}, {"id": "2006.15516", "submitter": "Wenhui Yu", "authors": "Wenhui Yu and Zheng Qin", "title": "Graph Convolutional Network for Recommendation with Low-pass\n  Collaborative Filters", "comments": "ICML 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\textbf{G}raph \\textbf{C}onvolutional \\textbf{N}etwork (\\textbf{GCN}) is\nwidely used in graph data learning tasks such as recommendation. However, when\nfacing a large graph, the graph convolution is very computationally expensive\nthus is simplified in all existing GCNs, yet is seriously impaired due to the\noversimplification. To address this gap, we leverage the \\textit{original graph\nconvolution} in GCN and propose a \\textbf{L}ow-pass \\textbf{C}ollaborative\n\\textbf{F}ilter (\\textbf{LCF}) to make it applicable to the large graph. LCF is\ndesigned to remove the noise caused by exposure and quantization in the\nobserved data, and it also reduces the complexity of graph convolution in an\nunscathed way. Experiments show that LCF improves the effectiveness and\nefficiency of graph convolution and our GCN outperforms existing GCNs\nsignificantly. Codes are available on \\url{https://github.com/Wenhui-Yu/LCFN}.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 05:22:24 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 13:16:08 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 15:31:25 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Yu", "Wenhui", ""], ["Qin", "Zheng", ""]]}, {"id": "2006.15599", "submitter": "Wenxuan Zhang", "authors": "Wenxuan Zhang, Yang Deng, Wai Lam", "title": "Answer Ranking for Product-Related Questions via Multiple Semantic\n  Relations Modeling", "comments": "Accepted by SIGIR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many E-commerce sites now offer product-specific question answering platforms\nfor users to communicate with each other by posting and answering questions\nduring online shopping. However, the multiple answers provided by ordinary\nusers usually vary diversely in their qualities and thus need to be\nappropriately ranked for each question to improve user satisfaction. It can be\nobserved that product reviews usually provide useful information for a given\nquestion, and thus can assist the ranking process. In this paper, we\ninvestigate the answer ranking problem for product-related questions, with the\nrelevant reviews treated as auxiliary information that can be exploited for\nfacilitating the ranking. We propose an answer ranking model named MUSE which\ncarefully models multiple semantic relations among the question, answers, and\nrelevant reviews. Specifically, MUSE constructs a multi-semantic relation graph\nwith the question, each answer, and each review snippet as nodes. Then a\ncustomized graph convolutional neural network is designed for explicitly\nmodeling the semantic relevance between the question and answers, the content\nconsistency among answers, and the textual entailment between answers and\nreviews. Extensive experiments on real-world E-commerce datasets across three\nproduct categories show that our proposed model achieves superior performance\non the concerned answer ranking task.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 13:26:17 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Zhang", "Wenxuan", ""], ["Deng", "Yang", ""], ["Lam", "Wai", ""]]}, {"id": "2006.15679", "submitter": "Anirban Chakraborty", "authors": "Anirban Chakraborty, Debasis Ganguly, Annalina Caputo, Gareth J. F.\n  Jones", "title": "Kernel Density Estimation based Factored Relevance Model for\n  Multi-Contextual Point-of-Interest Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An automated contextual suggestion algorithm is likely to recommend\ncontextually appropriate and personalized 'points-of-interest' (POIs) to a\nuser, if it can extract information from the user's preference history\n(exploitation) and effectively blend it with the user's current contextual\ninformation (exploration) to predict a POI's 'appropriateness' in the current\ncontext. To balance this trade-off between exploitation and exploration, we\npropose an unsupervised, generic framework involving a factored relevance model\n(FRLM), constituting two distinct components, one pertaining to historical\ncontexts, and the other corresponding to the current context. We further\ngeneralize the proposed FRLM by incorporating the semantic relationships\nbetween terms in POI descriptors using kernel density estimation (KDE) on\nembedded word vectors. Additionally, we show that trip-qualifiers, (e.g.\n'trip-type', 'accompanied-by') are potentially useful information sources that\ncould be used to improve the recommendation effectiveness. Using such\ninformation is not straight forward since users' texts/reviews of visited POIs\ntypically do not explicitly contain such annotations. We undertake a weakly\nsupervised approach to predict the associations between the review-texts in a\nuser profile and the likely trip contexts. Our experiments, conducted on the\nTREC contextual suggestion 2016 dataset, demonstrate that factorization,\nKDE-based generalizations, and trip-qualifier enriched contexts of the\nrelevance model improve POI recommendation.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 18:57:35 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Chakraborty", "Anirban", ""], ["Ganguly", "Debasis", ""], ["Caputo", "Annalina", ""], ["Jones", "Gareth J. F.", ""]]}, {"id": "2006.15772", "submitter": "Himan Abdollahpouri", "authors": "Himan Abdollahpouri and Masoud Mansoury", "title": "Multi-sided Exposure Bias in Recommendation", "comments": "Accepted at the International Workshop on Industrial Recommendation\n  Systems (IRS2020) in Conjunction with ACM KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Academic research in recommender systems has been greatly focusing on the\naccuracy-related measures of recommendations. Even when non-accuracy measures\nsuch as popularity bias, diversity, and novelty are studied, it is often solely\nfrom the users' perspective. However, many real-world recommenders are often\nmulti-stakeholder environments in which the needs and interests of several\nstakeholders should be addressed in the recommendation process. In this paper,\nwe focus on the popularity bias problem which is a well-known property of many\nrecommendation algorithms where few popular items are over-recommended while\nthe majority of other items do not get proportional attention and address its\nimpact on different stakeholders. Using several recommendation algorithms and\ntwo publicly available datasets in music and movie domains, we empirically show\nthe inherent popularity bias of the algorithms and how this bias impacts\ndifferent stakeholders such as users and suppliers of the items. We also\npropose metrics to measure the exposure bias of recommendation algorithms from\nthe perspective of different stakeholders.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 02:00:25 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 18:15:46 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Abdollahpouri", "Himan", ""], ["Mansoury", "Masoud", ""]]}, {"id": "2006.15871", "submitter": "Jannik Fischbach", "authors": "Jannik Fischbach, Benedikt Hauptmann, Lukas Konwitschny, Dominik\n  Spies, Andreas Vogelsang", "title": "Towards Causality Extraction from Requirements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System behavior is often based on causal relations between certain events\n(e.g. If event1, then event2). Consequently, those causal relations are also\ntextually embedded in requirements. We want to extract this causal knowledge\nand utilize it to derive test cases automatically and to reason about\ndependencies between requirements. Existing NLP approaches fail to extract\ncausality from natural language (NL) with reasonable performance. In this\npaper, we describe first steps towards building a new approach for causality\nextraction and contribute: (1) an NLP architecture based on Tree Recursive\nNeural Networks (TRNN) that we will train to identify causal relations in NL\nrequirements and (2) an annotation scheme and a dataset that is suitable for\ntraining TRNNs. Our dataset contains 212,186 sentences from 463 publicly\navailable requirement documents and is a first step towards a gold standard\ncorpus for causality extraction. We encourage fellow researchers to contribute\nto our dataset and help us in finalizing the causality annotation process.\nAdditionally, the dataset can also be annotated further to serve as a benchmark\nfor other RE-relevant NLP tasks such as requirements classification.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 08:35:25 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Fischbach", "Jannik", ""], ["Hauptmann", "Benedikt", ""], ["Konwitschny", "Lukas", ""], ["Spies", "Dominik", ""], ["Vogelsang", "Andreas", ""]]}, {"id": "2006.15939", "submitter": "Xueli Yu", "authors": "Shu Wu, Feng Yu, Xueli Yu, Qiang Liu, Liang Wang, Tieniu Tan, Jie Shao\n  and Fan Huang", "title": "TFNet: Multi-Semantic Feature Interaction for CTR Prediction", "comments": "To appear at SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401304", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CTR (Click-Through Rate) prediction plays a central role in the domain of\ncomputational advertising and recommender systems. There exists several kinds\nof methods proposed in this field, such as Logistic Regression (LR),\nFactorization Machines (FM) and deep learning based methods like Wide&Deep,\nNeural Factorization Machines (NFM) and DeepFM. However, such approaches\ngenerally use the vector-product of each pair of features, which have ignored\nthe different semantic spaces of the feature interactions. In this paper, we\npropose a novel Tensor-based Feature interaction Network (TFNet) model, which\nintroduces an operating tensor to elaborate feature interactions via\nmulti-slice matrices in multiple semantic spaces. Extensive offline and online\nexperiments show that TFNet: 1) outperforms the competitive compared methods on\nthe typical Criteo and Avazu datasets; 2) achieves large improvement of revenue\nand click rate in online A/B tests in the largest Chinese App recommender\nsystem, Tencent MyApp.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 11:17:16 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Wu", "Shu", ""], ["Yu", "Feng", ""], ["Yu", "Xueli", ""], ["Liu", "Qiang", ""], ["Wang", "Liang", ""], ["Tan", "Tieniu", ""], ["Shao", "Jie", ""], ["Huang", "Fan", ""]]}, {"id": "2006.16180", "submitter": "Wenye Li", "authors": "Wenye Li, Shuzhong Zhang", "title": "Binary Random Projections with Controllable Sparsity Patterns", "comments": "19 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random projection is often used to project higher-dimensional vectors onto a\nlower-dimensional space, while approximately preserving their pairwise\ndistances. It has emerged as a powerful tool in various data processing tasks\nand has attracted considerable research interest. Partly motivated by the\nrecent discoveries in neuroscience, in this paper we study the problem of\nrandom projection using binary matrices with controllable sparsity patterns.\nSpecifically, we proposed two sparse binary projection models that work on\ngeneral data vectors. Compared with the conventional random projection models\nwith dense projection matrices, our proposed models enjoy significant\ncomputational advantages due to their sparsity structure, as well as improved\naccuracies in empirical evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 16:45:26 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Li", "Wenye", ""], ["Zhang", "Shuzhong", ""]]}, {"id": "2006.16312", "submitter": "Xiaotian Hao", "authors": "Xiaotian Hao, Zhaoqing Peng, Yi Ma, Guan Wang, Junqi Jin, Jianye Hao,\n  Shan Chen, Rongquan Bai, Mingzhou Xie, Miao Xu, Zhenzhe Zheng, Chuan Yu, Han\n  Li, Jian Xu, Kun Gai", "title": "Dynamic Knapsack Optimization Towards Efficient Multi-Channel Sequential\n  Advertising", "comments": "accepted by ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR cs.SY eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In E-commerce, advertising is essential for merchants to reach their target\nusers. The typical objective is to maximize the advertiser's cumulative revenue\nover a period of time under a budget constraint. In real applications, an\nadvertisement (ad) usually needs to be exposed to the same user multiple times\nuntil the user finally contributes revenue (e.g., places an order). However,\nexisting advertising systems mainly focus on the immediate revenue with single\nad exposures, ignoring the contribution of each exposure to the final\nconversion, thus usually falls into suboptimal solutions. In this paper, we\nformulate the sequential advertising strategy optimization as a dynamic\nknapsack problem. We propose a theoretically guaranteed bilevel optimization\nframework, which significantly reduces the solution space of the original\noptimization space while ensuring the solution quality. To improve the\nexploration efficiency of reinforcement learning, we also devise an effective\naction space reduction approach. Extensive offline and online experiments show\nthe superior performance of our approaches over state-of-the-art baselines in\nterms of cumulative revenue.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 18:50:35 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Hao", "Xiaotian", ""], ["Peng", "Zhaoqing", ""], ["Ma", "Yi", ""], ["Wang", "Guan", ""], ["Jin", "Junqi", ""], ["Hao", "Jianye", ""], ["Chen", "Shan", ""], ["Bai", "Rongquan", ""], ["Xie", "Mingzhou", ""], ["Xu", "Miao", ""], ["Zheng", "Zhenzhe", ""], ["Yu", "Chuan", ""], ["Li", "Han", ""], ["Xu", "Jian", ""], ["Gai", "Kun", ""]]}, {"id": "2006.16642", "submitter": "Erion \\c{C}ano", "authors": "Erion \\c{C}ano and Maurizio Morisio", "title": "A Data-driven Neural Network Architecture for Sentiment Analysis", "comments": "18 pages, 4 tables, 7 figures", "journal-ref": "Data Technologies and Applications, Vol. 53, No. 1, pp. 2-19, 2019", "doi": "10.1108/DTA-03-2018-0017", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fabulous results of convolution neural networks in image-related tasks,\nattracted attention of text mining, sentiment analysis and other text analysis\nresearchers. It is however difficult to find enough data for feeding such\nnetworks, optimize their parameters, and make the right design choices when\nconstructing network architectures. In this paper we present the creation steps\nof two big datasets of song emotions. We also explore usage of convolution and\nmax-pooling neural layers on song lyrics, product and movie review text\ndatasets. Three variants of a simple and flexible neural network architecture\nare also compared. Our intention was to spot any important patterns that can\nserve as guidelines for parameter optimization of similar models. We also\nwanted to identify architecture design choices which lead to high performing\nsentiment analysis models. To this end, we conducted a series of experiments\nwith neural architectures of various configurations. Our results indicate that\nparallel convolutions of filter lengths up to three are usually enough for\ncapturing relevant text features. Also, max-pooling region size should be\nadapted to the length of text documents for producing the best feature maps.\nTop results we got are obtained with feature maps of lengths 6 to 18. An\nimprovement on future neural network models for sentiment analysis, could be\ngenerating sentiment polarity prediction of documents using aggregation of\npredictions on smaller excerpt of the entire text.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 10:08:36 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["\u00c7ano", "Erion", ""], ["Morisio", "Maurizio", ""]]}, {"id": "2006.16742", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Xiting Wang, Yongfeng Huang, Xing Xie", "title": "FairRec: Fairness-aware News Recommendation with Decomposed Adversarial\n  Learning", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News recommendation is important for online news services. Existing news\nrecommendation models are usually learned from users' news click behaviors.\nUsually the behaviors of users with the same sensitive attributes (e.g.,\ngenders) have similar patterns and news recommendation models can easily\ncapture these patterns. It may lead to some biases related to sensitive user\nattributes in the recommendation results, e.g., always recommending sports news\nto male users, which is unfair since users may not receive diverse news\ninformation. In this paper, we propose a fairness-aware news recommendation\napproach with decomposed adversarial learning and orthogonality regularization,\nwhich can alleviate unfairness in news recommendation brought by the biases of\nsensitive user attributes. In our approach, we propose to decompose the user\ninterest model into two components. One component aims to learn a bias-aware\nuser embedding that captures the bias information on sensitive user attributes,\nand the other aims to learn a bias-free user embedding that only encodes\nattribute-independent user interest information for fairness-aware news\nrecommendation. In addition, we propose to apply an attribute prediction task\nto the bias-aware user embedding to enhance its ability on bias modeling, and\nwe apply adversarial learning to the bias-free user embedding to remove the\nbias information from it. Moreover, we propose an orthogonality regularization\nmethod to encourage the bias-free user embeddings to be orthogonal to the\nbias-aware one to better distinguish the bias-free user embedding from the\nbias-aware one. For fairness-aware news ranking, we only use the bias-free user\nembedding. Extensive experiments on benchmark dataset show that our approach\ncan effectively improve fairness in news recommendation with minor performance\nloss.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 12:59:16 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 12:50:39 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Wang", "Xiting", ""], ["Huang", "Yongfeng", ""], ["Xie", "Xing", ""]]}, {"id": "2006.16977", "submitter": "Yongfeng Zhang", "authors": "Shuyuan Xu, Yunqi Li, Shuchang Liu, Zuohui Fu, Xu Chen, Yongfeng Zhang", "title": "Learning Post-Hoc Causal Explanations for Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art recommender systems have the ability to generate\nhigh-quality recommendations, but usually cannot provide intuitive explanations\nto humans due to the usage of black-box prediction models. The lack of\ntransparency has highlighted the critical importance of improving the\nexplainability of recommender systems. In this paper, we propose to extract\ncausal rules from the user interaction history as post-hoc explanations for the\nblack-box sequential recommendation mechanisms, whilst maintain the predictive\naccuracy of the recommendation model. Our approach firstly achieves\ncounterfactual examples with the aid of a perturbation model, and then extracts\npersonalized causal relationships for the recommendation model through a causal\nrule mining algorithm. Experiments are conducted on several state-of-the-art\nsequential recommendation models and real-world datasets to verify the\nperformance of our model on generating causal explanations. Meanwhile, We\nevaluate the discovered causal explanations in terms of quality and fidelity,\nwhich show that compared with conventional association rules, causal rules can\nprovide personalized and more effective explanations for the behavior of\nblack-box recommendation models.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 17:14:12 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 17:32:32 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Xu", "Shuyuan", ""], ["Li", "Yunqi", ""], ["Liu", "Shuchang", ""], ["Fu", "Zuohui", ""], ["Chen", "Xu", ""], ["Zhang", "Yongfeng", ""]]}]