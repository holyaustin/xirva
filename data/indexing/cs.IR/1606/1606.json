[{"id": "1606.00253", "submitter": "Georgios Balikas", "authors": "Georgios Balikas, Massih-Reza Amini, Marianne Clausel", "title": "On a Topic Model for Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic topic models are generative models that describe the content of\ndocuments by discovering the latent topics underlying them. However, the\nstructure of the textual input, and for instance the grouping of words in\ncoherent text spans such as sentences, contains much information which is\ngenerally lost with these models. In this paper, we propose sentenceLDA, an\nextension of LDA whose goal is to overcome this limitation by incorporating the\nstructure of the text in the generative and inference processes. We illustrate\nthe advantages of sentenceLDA by comparing it with LDA using both intrinsic\n(perplexity) and extrinsic (text classification) evaluation tasks on different\ntext collections.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 12:34:50 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Balikas", "Georgios", ""], ["Amini", "Massih-Reza", ""], ["Clausel", "Marianne", ""]]}, {"id": "1606.00411", "submitter": "Saurav Ghosh", "authors": "Saurav Ghosh, Prithwish Chakraborty, Elaine O. Nsoesie, Emily Cohn,\n  Sumiko R. Mekaru, John S. Brownstein and Naren Ramakrishnan", "title": "Temporal Topic Modeling to Assess Associations between News Trends and\n  Infectious Disease Outbreaks", "comments": "This paper has been submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In retrospective assessments, internet news reports have been shown to\ncapture early reports of unknown infectious disease transmission prior to\nofficial laboratory confirmation. In general, media interest and reporting\npeaks and wanes during the course of an outbreak. In this study, we quantify\nthe extent to which media interest during infectious disease outbreaks is\nindicative of trends of reported incidence. We introduce an approach that uses\nsupervised temporal topic models to transform large corpora of news articles\ninto temporal topic trends. The key advantages of this approach include,\napplicability to a wide range of diseases, and ability to capture disease\ndynamics - including seasonality, abrupt peaks and troughs. We evaluated the\nmethod using data from multiple infectious disease outbreaks reported in the\nUnited States of America (U.S.), China and India. We noted that temporal topic\ntrends extracted from disease-related news reports successfully captured the\ndynamics of multiple outbreaks such as whooping cough in U.S. (2012), dengue\noutbreaks in India (2013) and China (2014). Our observations also suggest that\nefficient modeling of temporal topic trends using time-series regression\ntechniques can estimate disease case counts with increased precision before\nofficial reports by health organizations.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2016 19:30:07 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Ghosh", "Saurav", ""], ["Chakraborty", "Prithwish", ""], ["Nsoesie", "Elaine O.", ""], ["Cohn", "Emily", ""], ["Mekaru", "Sumiko R.", ""], ["Brownstein", "John S.", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1606.00577", "submitter": "Justin Wood", "authors": "Justin Wood, Patrick Tan, Wei Wang, Corey Arnold", "title": "Source-LDA: Enhancing probabilistic topic models using prior knowledge\n  sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach to topic modeling involves extracting co-occurring n-grams\nof a corpus into semantic themes. The set of n-grams in a theme represents an\nunderlying topic, but most topic modeling approaches are not able to label\nthese sets of words with a single n-gram. Such labels are useful for topic\nidentification in summarization systems. This paper introduces a novel approach\nto labeling a group of n-grams comprising an individual topic. The approach\ntaken is to complement the existing topic distributions over words with a known\ndistribution based on a predefined set of topics. This is done by integrating\nexisting labeled knowledge sources representing known potential topics into the\nprobabilistic topic model. These knowledge sources are translated into a\ndistribution and used to set the hyperparameters of the Dirichlet generated\ndistribution over words. In the inference these modified distributions guide\nthe convergence of the latent topics to conform with the complementary\ndistributions. This approach ensures that the topic inference process is\nconsistent with existing knowledge. The label assignment from the complementary\nknowledge sources are then transferred to the latent topics of the corpus. The\nresults show both accurate label assignment to topics as well as improved topic\ngeneration than those obtained using various labeling approaches based off\nLatent Dirichlet allocation (LDA).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 08:15:15 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 05:15:36 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 21:03:06 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Wood", "Justin", ""], ["Tan", "Patrick", ""], ["Wang", "Wei", ""], ["Arnold", "Corey", ""]]}, {"id": "1606.00615", "submitter": "Javid Dadashkarimi", "authors": "Javid Dadashkarimi and Masoud Jalili Sabet and Heshaam Faili and\n  Azadeh Shakery", "title": "Low-dimensional Query Projection based on Divergence Minimization\n  Feedback Model for Ad-hoc Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-dimensional word vectors have long been used in a wide range of\napplications in natural language processing. In this paper we shed light on\nestimating query vectors in ad-hoc retrieval where a limited information is\navailable in the original query. Pseudo-relevance feedback (PRF) is a\nwell-known technique for updating query language models and expanding the\nqueries with a number of relevant terms. We formulate the query updating in\nlow-dimensional spaces first with rotating the query vector and then with\nscaling. These consequential steps are embedded in a query-specific projection\nmatrix capturing both angle and scaling. In this paper we propose a new but not\nthe most effective technique necessarily for PRF in language modeling, based on\nthe query projection algorithm. We learn an embedded coefficient matrix for\neach query, whose aim is to improve the vector representation of the query by\ntransforming it to a more reliable space, and then update the query language\nmodel. The proposed embedded coefficient divergence minimization model (ECDMM)\ntakes top-ranked documents retrieved by the query and obtains a couple of\npositive and negative sample sets; these samples are used for learning the\ncoefficient matrix which will be used for projecting the query vector and\nupdating the query language model using a softmax function. Experimental\nresults on several TREC and CLEF data sets in several languages demonstrate\neffectiveness of ECDMM. The experimental results reveal that the new\nformulation for the query works as well as state-of-the-art PRF techniques and\noutperforms state-of-the-art PRF techniques in a TREC collection in terms of\nMAP,P@5, and P@10 significantly.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2016 10:44:52 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 11:36:14 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Dadashkarimi", "Javid", ""], ["Sabet", "Masoud Jalili", ""], ["Faili", "Heshaam", ""], ["Shakery", "Azadeh", ""]]}, {"id": "1606.00979", "submitter": "Kang Liu", "authors": "Yuanzhe Zhang, Kang Liu, Shizhu He, Guoliang Ji, Zhanyi Liu, Hua Wu,\n  Jun Zhao", "title": "Question Answering over Knowledge Base with Neural Attention Combining\n  Global Knowledge Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of knowledge bases (KBs) on the web, how to take full\nadvantage of them becomes increasingly important. Knowledge base-based question\nanswering (KB-QA) is one of the most promising approaches to access the\nsubstantial knowledge. Meantime, as the neural network-based (NN-based) methods\ndevelop, NN-based KB-QA has already achieved impressive results. However,\nprevious work did not put emphasis on question representation, and the question\nis converted into a fixed vector regardless of its candidate answers. This\nsimple representation strategy is unable to express the proper information of\nthe question. Hence, we present a neural attention-based model to represent the\nquestions dynamically according to the different focuses of various candidate\nanswer aspects. In addition, we leverage the global knowledge inside the\nunderlying KB, aiming at integrating the rich KB information into the\nrepresentation of the answers. And it also alleviates the out of vocabulary\n(OOV) problem, which helps the attention model to represent the question more\nprecisely. The experimental results on WEBQUESTIONS demonstrate the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2016 06:40:14 GMT"}], "update_date": "2016-06-06", "authors_parsed": [["Zhang", "Yuanzhe", ""], ["Liu", "Kang", ""], ["He", "Shizhu", ""], ["Ji", "Guoliang", ""], ["Liu", "Zhanyi", ""], ["Wu", "Hua", ""], ["Zhao", "Jun", ""]]}, {"id": "1606.01550", "submitter": "Relja Arandjelovi\\'c", "authors": "Artem Babenko, Relja Arandjelovi\\'c, Victor Lempitsky", "title": "Pairwise Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of lossy compression of high-dimensional vectors through\nquantization. We propose the approach that learns quantization parameters by\nminimizing the distortion of scalar products and squared distances between\npairs of points. This is in contrast to previous works that obtain these\nparameters through the minimization of the reconstruction error of individual\npoints. The proposed approach proceeds by finding a linear transformation of\nthe data that effectively reduces the minimization of the pairwise distortions\nto the minimization of individual reconstruction errors. After such\ntransformation, any of the previously-proposed quantization approaches can be\nused. Despite the simplicity of this transformation, the experiments\ndemonstrate that it achieves considerable reduction of the pairwise distortions\ncompared to applying quantization directly to the untransformed data.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jun 2016 19:57:06 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Babenko", "Artem", ""], ["Arandjelovi\u0107", "Relja", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1606.01621", "submitter": "Shu Kong", "authors": "Shu Kong, Xiaohui Shen, Zhe Lin, Radomir Mech, Charless Fowlkes", "title": "Photo Aesthetics Ranking Network with Attributes and Content Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world applications could benefit from the ability to automatically\ngenerate a fine-grained ranking of photo aesthetics. However, previous methods\nfor image aesthetics analysis have primarily focused on the coarse, binary\ncategorization of images into high- or low-aesthetic categories. In this work,\nwe propose to learn a deep convolutional neural network to rank photo\naesthetics in which the relative ranking of photo aesthetics are directly\nmodeled in the loss function. Our model incorporates joint learning of\nmeaningful photographic attributes and image content information which can help\nregularize the complicated photo aesthetics rating problem.\n  To train and analyze this model, we have assembled a new aesthetics and\nattributes database (AADB) which contains aesthetic scores and meaningful\nattributes assigned to each image by multiple human raters. Anonymized rater\nidentities are recorded across images allowing us to exploit intra-rater\nconsistency using a novel sampling strategy when computing the ranking loss of\ntraining image pairs. We show the proposed sampling strategy is very effective\nand robust in face of subjective judgement of image aesthetics by individuals\nwith different aesthetic tastes. Experiments demonstrate that our unified model\ncan generate aesthetic rankings that are more consistent with human ratings. To\nfurther validate our model, we show that by simply thresholding the estimated\naesthetic scores, we are able to achieve state-or-the-art classification\nperformance on the existing AVA dataset benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2016 06:14:00 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 00:20:07 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Kong", "Shu", ""], ["Shen", "Xiaohui", ""], ["Lin", "Zhe", ""], ["Mech", "Radomir", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1606.02276", "submitter": "Mercan Topkara", "authors": "Nikolaos Pappas, Miriam Redi, Mercan Topkara, Brendan Jou, Hongyi Liu,\n  Tao Chen, Shih-Fu Chang", "title": "Multilingual Visual Sentiment Concept Matching", "comments": null, "journal-ref": "Proceedings ICMR '16 Proceedings of the 2016 ACM on International\n  Conference on Multimedia Retrieval Pages 151-158", "doi": "10.1145/2911996.2912016", "report-no": null, "categories": "cs.CL cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of culture in visual emotion perception has recently captured the\nattention of multimedia research. In this study, we pro- vide powerful\ncomputational linguistics tools to explore, retrieve and browse a dataset of\n16K multilingual affective visual concepts and 7.3M Flickr images. First, we\ndesign an effective crowdsourc- ing experiment to collect human judgements of\nsentiment connected to the visual concepts. We then use word embeddings to\nrepre- sent these concepts in a low dimensional vector space, allowing us to\nexpand the meaning around concepts, and thus enabling insight about\ncommonalities and differences among different languages. We compare a variety\nof concept representations through a novel evaluation task based on the notion\nof visual semantic relatedness. Based on these representations, we design\nclustering schemes to group multilingual visual concepts, and evaluate them\nwith novel metrics based on the crowdsourced sentiment annotations as well as\nvisual semantic relatedness. The proposed clustering framework enables us to\nanalyze the full multilingual dataset in-depth and also show an application on\na facial data subset, exploring cultural in- sights of portrait-related\naffective visual concepts.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 19:40:00 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Pappas", "Nikolaos", ""], ["Redi", "Miriam", ""], ["Topkara", "Mercan", ""], ["Jou", "Brendan", ""], ["Liu", "Hongyi", ""], ["Chen", "Tao", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1606.02976", "submitter": "Gayo Diallo", "authors": "Khadim Dram\\'e (UB), Fleur Mougin (UB), Gayo Diallo (UB)", "title": "Large scale biomedical texts classification: a kNN and an ESA-based\n  approaches", "comments": "Journal of Biomedical Semantics, BioMed Central, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the large and increasing volume of textual data, automated methods for\nidentifying significant topics to classify textual documents have received a\ngrowing interest. While many efforts have been made in this direction, it still\nremains a real challenge. Moreover, the issue is even more complex as full\ntexts are not always freely available. Then, using only partial information to\nannotate these documents is promising but remains a very ambitious issue.\nMethodsWe propose two classification methods: a k-nearest neighbours\n(kNN)-based approach and an explicit semantic analysis (ESA)-based approach.\nAlthough the kNN-based approach is widely used in text classification, it needs\nto be improved to perform well in this specific classification problem which\ndeals with partial information. Compared to existing kNN-based methods, our\nmethod uses classical Machine Learning (ML) algorithms for ranking the labels.\nAdditional features are also investigated in order to improve the classifiers'\nperformance. In addition, the combination of several learning algorithms with\nvarious techniques for fixing the number of relevant topics is performed. On\nthe other hand, ESA seems promising for this classification task as it yielded\ninteresting results in related issues, such as semantic relatedness computation\nbetween texts and text classification. Unlike existing works, which use ESA for\nenriching the bag-of-words approach with additional knowledge-based features,\nour ESA-based method builds a standalone classifier. Furthermore, we\ninvestigate if the results of this method could be useful as a complementary\nfeature of our kNN-based approach.ResultsExperimental evaluations performed on\nlarge standard annotated datasets, provided by the BioASQ organizers, show that\nthe kNN-based method with the Random Forest learning algorithm achieves good\nperformances compared with the current state-of-the-art methods, reaching a\ncompetitive f-measure of 0.55% while the ESA-based approach surprisingly\nyielded reserved results.ConclusionsWe have proposed simple classification\nmethods suitable to annotate textual documents using only partial information.\nThey are therefore adequate for large multi-label classification and\nparticularly in the biomedical domain. Thus, our work contributes to the\nextraction of relevant information from unstructured documents in order to\nfacilitate their automated processing. Consequently, it could be used for\nvarious purposes, including document indexing, information retrieval, etc.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 14:32:50 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Dram\u00e9", "Khadim", "", "UB"], ["Mougin", "Fleur", "", "UB"], ["Diallo", "Gayo", "", "UB"]]}, {"id": "1606.02979", "submitter": "Shaohua Li", "authors": "Shaohua Li, Tat-Seng Chua, Jun Zhu, Chunyan Miao", "title": "Generative Topic Embedding: a Continuous Representation of Documents\n  (Extended Version with Proofs)", "comments": "13 pages. The original version has been accepted in ACL 2016 as a\n  long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding maps words into a low-dimensional continuous embedding space\nby exploiting the local word collocation patterns in a small context window. On\nthe other hand, topic modeling maps documents onto a low-dimensional topic\nspace, by utilizing the global word collocation patterns in the same document.\nThese two types of patterns are complementary. In this paper, we propose a\ngenerative topic embedding model to combine the two types of patterns. In our\nmodel, topics are represented by embedding vectors, and are shared across\ndocuments. The probability of each word is influenced by both its local context\nand its topic. A variational inference method yields the topic embeddings as\nwell as the topic mixing proportions for each document. Jointly they represent\nthe document in a low-dimensional continuous space. In two document\nclassification tasks, our method performs better than eight existing methods,\nwith fewer features. In addition, we illustrate with an example that our method\ncan generate coherent topics even based on only one document.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 14:45:39 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 14:49:07 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Li", "Shaohua", ""], ["Chua", "Tat-Seng", ""], ["Zhu", "Jun", ""], ["Miao", "Chunyan", ""]]}, {"id": "1606.03044", "submitter": "Bob Sturm", "authors": "Bob L. Sturm", "title": "The \"Horse'' Inside: Seeking Causes Behind the Behaviours of Music\n  Content Analysis Systems", "comments": "32 pages, 17 figures, this work was accepted for publication in a\n  journal special issue in Apr. 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building systems that possess the sensitivity and intelligence to identify\nand describe high-level attributes in music audio signals continues to be an\nelusive goal, but one that surely has broad and deep implications for a wide\nvariety of applications. Hundreds of papers have so far been published toward\nthis goal, and great progress appears to have been made. Some systems produce\nremarkable accuracies at recognising high-level semantic concepts, such as\nmusic style, genre and mood. However, it might be that these numbers do not\nmean what they seem. In this paper, we take a state-of-the-art music content\nanalysis system and investigate what causes it to achieve exceptionally high\nperformance in a benchmark music audio dataset. We dissect the system to\nunderstand its operation, determine its sensitivities and limitations, and\npredict the kinds of knowledge it could and could not possess about music. We\nperform a series of experiments to illuminate what the system has actually\nlearned to do, and to what extent it is performing the intended music listening\ntask. Our results demonstrate how the initial manifestation of music\nintelligence in this state-of-the-art can be deceptive. Our work provides\nconstructive directions toward developing music content analysis systems that\ncan address the music information and creation needs of real-world users.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 18:10:31 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Sturm", "Bob L.", ""]]}, {"id": "1606.03048", "submitter": "Canggih Puspo Wibowo", "authors": "Canggih Puspo Wibowo", "title": "A Minimum Spanning Tree Representation of Anime Similarities", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a new way to represent Japanese animation (anime) is presented.\nWe applied a minimum spanning tree to show the relation between anime. The\ndistance between anime is calculated through three similarity measurements,\nnamely crew, score histogram, and topic similarities. Finally, the centralities\nare also computed to reveal the most significance anime. The result shows that\nthe minimum spanning tree can be used to determine the similarity anime.\nFurthermore, by using centralities calculation, we found some anime that are\nsignificant to others.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jun 2016 06:38:32 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 10:23:55 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Wibowo", "Canggih Puspo", ""]]}, {"id": "1606.03066", "submitter": "Jimmy Lin", "authors": "Luchen Tan, Jimmy Lin, Adam Roegiest, Charles L. A. Clarke", "title": "The Effects of Latency Penalties in Evaluating Push Notification Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the effects of different latency penalties in the evaluation of\npush notification systems, as operationalized in the TREC 2015 Microblog track\nevaluation. The purpose of this study is to inform the design of metrics for\nthe TREC 2016 Real-Time Summarization track, which is largely modeled after the\nTREC 2015 evaluation design.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2016 18:58:51 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Tan", "Luchen", ""], ["Lin", "Jimmy", ""], ["Roegiest", "Adam", ""], ["Clarke", "Charles L. A.", ""]]}, {"id": "1606.03333", "submitter": "Mortaza Doulaty", "authors": "Mortaza Doulaty, Oscar Saz, Raymond W. M. Ng, Thomas Hain", "title": "Automatic Genre and Show Identification of Broadcast Media", "comments": "Proc. of 17th Interspeech (2016), San Francisco, California, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huge amounts of digital videos are being produced and broadcast every day,\nleading to giant media archives. Effective techniques are needed to make such\ndata accessible further. Automatic meta-data labelling of broadcast media is an\nessential task for multimedia indexing, where it is standard to use multi-modal\ninput for such purposes. This paper describes a novel method for automatic\ndetection of media genre and show identities using acoustic features, textual\nfeatures or a combination thereof. Furthermore the inclusion of available\nmeta-data, such as time of broadcast, is shown to lead to very high\nperformance. Latent Dirichlet Allocation is used to model both acoustics and\ntext, yielding fixed dimensional representations of media recordings that can\nthen be used in Support Vector Machines based classification. Experiments are\nconducted on more than 1200 hours of TV broadcasts from the British\nBroadcasting Corporation (BBC), where the task is to categorise the broadcasts\ninto 8 genres or 133 show identities. On a 200-hour test set, accuracies of\n98.6% and 85.7% were achieved for genre and show identification respectively,\nusing a combination of acoustic and textual features with meta-data.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2016 14:09:32 GMT"}], "update_date": "2016-06-13", "authors_parsed": [["Doulaty", "Mortaza", ""], ["Saz", "Oscar", ""], ["Ng", "Raymond W. M.", ""], ["Hain", "Thomas", ""]]}, {"id": "1606.03561", "submitter": "P.K. Srijith", "authors": "P. K. Srijith, Mark Hepple, Kalina Bontcheva, Daniel Preotiuc-Pietro", "title": "Sub-Story Detection in Twitter with Hierarchical Dirichlet Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media has now become the de facto information source on real world\nevents. The challenge, however, due to the high volume and velocity nature of\nsocial media streams, is in how to follow all posts pertaining to a given event\nover time, a task referred to as story detection. Moreover, there are often\nseveral different stories pertaining to a given event, which we refer to as\nsub-stories and the corresponding task of their automatic detection as\nsub-story detection. This paper proposes hierarchical Dirichlet processes\n(HDP), a probabilistic topic model, as an effective method for automatic\nsub-story detection. HDP can learn sub-topics associated with sub-stories which\nenables it to handle subtle variations in sub-stories. It is compared with\nstate- of-the-art story detection approaches based on locality sensitive\nhashing and spectral clustering. We demonstrate the superior performance of HDP\nfor sub-story detection on real world Twitter data sets using various\nevaluation measures. The ability of HDP to learn sub-topics helps it to recall\nthe sub- stories with high precision. Another contribution of this paper is in\ndemonstrating that the conversational structures within the Twitter stream can\nbe used to improve sub-story detection performance significantly.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jun 2016 06:52:18 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Srijith", "P. K.", ""], ["Hepple", "Mark", ""], ["Bontcheva", "Kalina", ""], ["Preotiuc-Pietro", "Daniel", ""]]}, {"id": "1606.03662", "submitter": "Haishan Wu", "authors": "Mengwen Xu, Tianyi Wang, Zhengwei Wu, Jingbo Zhou, Jian Li, Haishan Wu", "title": "Store Location Selection via Mining Search Query Logs of Baidu Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing a good location when opening a new store is crucial for the future\nsuccess of a business. Traditional methods include offline manual survey, which\nis very time consuming, and analytic models based on census data, which are un-\nable to adapt to the dynamic market. The rapid increase of the availability of\nbig data from various types of mobile devices, such as online query data and\noffline positioning data, provides us with the possibility to develop automatic\nand accurate data-driven prediction models for business store placement. In\nthis paper, we propose a Demand Distribution Driven Store Placement (D3SP)\nframework for business store placement by mining search query data from Baidu\nMaps. D3SP first detects the spatial-temporal distributions of customer demands\non different business services via query data from Baidu Maps, the largest\nonline map search engine in China, and detects the gaps between demand and sup-\nply. Then we determine candidate locations via clustering such gaps. In the\nfinal stage, we solve the location optimization problem by predicting and\nranking the number of customers. We not only deploy supervised regression\nmodels to predict the number of customers, but also learn to rank models to\ndirectly rank the locations. We evaluate our framework on various types of\nbusinesses in real-world cases, and the experiments results demonstrate the\neffectiveness of our methods. D3SP as the core function for store placement has\nalready been implemented as a core component of our business analytics platform\nand could be potentially used by chain store merchants on Baidu Nuomi.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 03:42:10 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Xu", "Mengwen", ""], ["Wang", "Tianyi", ""], ["Wu", "Zhengwei", ""], ["Zhou", "Jingbo", ""], ["Li", "Jian", ""], ["Wu", "Haishan", ""]]}, {"id": "1606.03783", "submitter": "Thomas Lampert", "authors": "Pedro Chahuara, Thomas Lampert, Pierre Gancarski", "title": "Retrieving and Ranking Similar Questions from Question-Answer Archives\n  Using Topic Modelling and Topic Distribution Regression", "comments": "International Conference on Theory and Practice of Digital Libraries\n  2016 (accepted)", "journal-ref": null, "doi": "10.1007/978-3-319-43997-6_4", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presented herein is a novel model for similar question ranking within\ncollaborative question answer platforms. The presented approach integrates a\nregression stage to relate topics derived from questions to those derived from\nquestion-answer pairs. This helps to avoid problems caused by the differences\nin vocabulary used within questions and answers, and the tendency for questions\nto be shorter than answers. The performance of the model is shown to outperform\ntranslation methods and topic modelling (without regression) on several\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jun 2016 23:50:19 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Chahuara", "Pedro", ""], ["Lampert", "Thomas", ""], ["Gancarski", "Pierre", ""]]}, {"id": "1606.04081", "submitter": "Pedro Mota", "authors": "Pedro Mota, Maxine Eskenazi, Luisa Coheur", "title": "Graph-Community Detection for Cross-Document Topic Segment Relationship\n  Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a graph-community detection approach to identify\ncross-document relationships at the topic segment level. Given a set of related\ndocuments, we automatically find these relationships by clustering segments\nwith similar content (topics). In this context, we study how different\nweighting mechanisms influence the discovery of word communities that relate to\nthe different topics found in the documents. Finally, we test different mapping\nfunctions to assign topic segments to word communities, determining which topic\nsegments are considered equivalent.\n  By performing this task it is possible to enable efficient multi-document\nbrowsing, since when a user finds relevant content in one document we can\nprovide access to similar topics in other documents. We deploy our approach in\ntwo different scenarios. One is an educational scenario where equivalence\nrelationships between learning materials need to be found. The other consists\nof a series of dialogs in a social context where students discuss commonplace\ntopics. Results show that our proposed approach better discovered equivalence\nrelationships in learning material documents and obtained close results in the\nsocial speech domain, where the best performing approach was a clustering\ntechnique.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 19:41:56 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Mota", "Pedro", ""], ["Eskenazi", "Maxine", ""], ["Coheur", "Luisa", ""]]}, {"id": "1606.04130", "submitter": "Zachary Lipton", "authors": "Zachary C. Lipton, David C. Kale, Randall Wetzel", "title": "Modeling Missing Data in Clinical Time Series with RNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a simple strategy to cope with missing data in sequential\ninputs, addressing the task of multilabel classification of diagnoses given\nclinical time series. Collected from the pediatric intensive care unit (PICU)\nat Children's Hospital Los Angeles, our data consists of multivariate time\nseries of observations. The measurements are irregularly spaced, leading to\nmissingness patterns in temporally discretized sequences. While these artifacts\nare typically handled by imputation, we achieve superior predictive performance\nby treating the artifacts as features. Unlike linear models, recurrent neural\nnetworks can realize this improvement using only simple binary indicators of\nmissingness. For linear models, we show an alternative strategy to capture this\nsignal. Training models on missingness patterns only, we show that for some\ndiseases, what tests are run can be as predictive as the results themselves.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2016 20:34:35 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 09:04:23 GMT"}, {"version": "v3", "created": "Thu, 18 Aug 2016 08:51:35 GMT"}, {"version": "v4", "created": "Tue, 20 Sep 2016 01:10:14 GMT"}, {"version": "v5", "created": "Fri, 11 Nov 2016 12:46:53 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Lipton", "Zachary C.", ""], ["Kale", "David C.", ""], ["Wetzel", "Randall", ""]]}, {"id": "1606.04223", "submitter": "Benjamin Piwowarski", "authors": "B. Piwowarski", "title": "Learning Term Weights for Ad-hoc Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most Information Retrieval models compute the relevance score of a document\nfor a given query by summing term weights specific to a document or a query.\nHeuristic approaches, like TF-IDF, or probabilistic models, like BM25, are used\nto specify how a term weight is computed. In this paper, we propose to leverage\nlearning-to-rank principles to learn how to compute a term weight for a given\ndocument based on the term occurrence pattern.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 07:19:08 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Piwowarski", "B.", ""]]}, {"id": "1606.04278", "submitter": "Michiel Stock", "authors": "Michiel Stock and Krzysztof Dembczynski and Bernard De Baets and\n  Willem Waegeman", "title": "Exact and efficient top-K inference for multi-target prediction by\n  querying separable linear relational models", "comments": null, "journal-ref": "Data Min Knowl Disc (2016) 30:1370-1394", "doi": "10.1007/s10618-016-0456-z", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex multi-target prediction problems that concern large target\nspaces are characterised by a need for efficient prediction strategies that\navoid the computation of predictions for all targets explicitly. Examples of\nsuch problems emerge in several subfields of machine learning, such as\ncollaborative filtering, multi-label classification, dyadic prediction and\nbiological network inference. In this article we analyse efficient and exact\nalgorithms for computing the top-$K$ predictions in the above problem settings,\nusing a general class of models that we refer to as separable linear relational\nmodels. We show how to use those inference algorithms, which are modifications\nof well-known information retrieval methods, in a variety of machine learning\nsettings. Furthermore, we study the possibility of scoring items incompletely,\nwhile still retaining an exact top-K retrieval. Experimental results in several\napplication domains reveal that the so-called threshold algorithm is very\nscalable, performing often many orders of magnitude more efficiently than the\nnaive approach.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 09:41:27 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Stock", "Michiel", ""], ["Dembczynski", "Krzysztof", ""], ["De Baets", "Bernard", ""], ["Waegeman", "Willem", ""]]}, {"id": "1606.04335", "submitter": "Maria Kalantzi", "authors": "Maria Kalantzi", "title": "LLFR: A Lanczos-Based Latent Factor Recommender for Big Data Scenarios", "comments": "65 pages, MSc Thesis (in Greek)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose if this master's thesis is to study and develop a new algorithmic\nframework for Collaborative Filtering to produce recommendations in the top-N\nrecommendation problem. Thus, we propose Lanczos Latent Factor Recommender\n(LLFR); a novel \"big data friendly\" collaborative filtering algorithm for top-N\nrecommendation. Using a computationally efficient Lanczos-based procedure, LLFR\nbuilds a low dimensional item similarity model, that can be readily exploited\nto produce personalized ranking vectors over the item space. A number of\nexperiments on real datasets indicate that LLFR outperforms other\nstate-of-the-art top-N recommendation methods from a computational as well as a\nqualitative perspective. Our experimental results also show that its relative\nperformance gains, compared to competing methods, increase as the data get\nsparser, as in the Cold Start Problem. More specifically, this is true both\nwhen the sparsity is generalized - as in the New Community Problem, a very\ncommon problem faced by real recommender systems in their beginning stages,\nwhen there is not sufficient number of ratings for the collaborative filtering\nalgorithms to uncover similarities between items or users - and in the very\ninteresting case where the sparsity is localized in a small fraction of the\ndataset - as in the New Users Problem, where new users are introduced to the\nsystem, they have not rated many items and thus, the CF algorithm can not make\nreliable personalized recommendations yet.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 13:04:57 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Kalantzi", "Maria", ""]]}, {"id": "1606.04351", "submitter": "Georgios Balikas", "authors": "Georgios Balikas, Massih-Reza Amini", "title": "TwiSE at SemEval-2016 Task 4: Twitter Sentiment Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the participation of the team \"TwiSE\" in the SemEval\n2016 challenge. Specifically, we participated in Task 4, namely \"Sentiment\nAnalysis in Twitter\" for which we implemented sentiment classification systems\nfor subtasks A, B, C and D. Our approach consists of two steps. In the first\nstep, we generate and validate diverse feature sets for twitter sentiment\nevaluation, inspired by the work of participants of previous editions of such\nchallenges. In the second step, we focus on the optimization of the evaluation\nmeasures of the different subtasks. To this end, we examine different learning\nstrategies by validating them on the data provided by the task organisers. For\nour final submissions we used an ensemble learning approach (stacked\ngeneralization) for Subtask A and single linear models for the rest of the\nsubtasks. In the official leaderboard we were ranked 9/35, 8/19, 1/11 and 2/14\nfor subtasks A, B, C and D respectively.\\footnote{We make the code available\nfor research purposes at\n\\url{https://github.com/balikasg/SemEval2016-Twitter\\_Sentiment\\_Evaluation}.}\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 13:36:00 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Balikas", "Georgios", ""], ["Amini", "Massih-Reza", ""]]}, {"id": "1606.04429", "submitter": "Arkaitz Zubiaga", "authors": "Alberto P. Garc\\'ia-Plaza and V\\'ictor Fresno and Raquel Mart\\'inez\n  and Arkaitz Zubiaga", "title": "Using Fuzzy Logic to Leverage HTML Markup for Web Page Representation", "comments": "This is the accepted version of an article accepted for publication\n  in IEEE Transactions on Fuzzy Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The selection of a suitable document representation approach plays a crucial\nrole in the performance of a document clustering task. Being able to pick out\nrepresentative words within a document can lead to substantial improvements in\ndocument clustering. In the case of web documents, the HTML markup that defines\nthe layout of the content provides additional structural information that can\nbe further exploited to identify representative words. In this paper we\nintroduce a fuzzy term weighing approach that makes the most of the HTML\nstructure for document clustering. We set forth and build on the hypothesis\nthat a good representation can take advantage of how humans skim through\ndocuments to extract the most representative words. The authors of web pages\nmake use of HTML tags to convey the most important message of a web page\nthrough page elements that attract the readers' attention, such as page titles\nor emphasized elements. We define a set of criteria to exploit the information\nprovided by these page elements, and introduce a fuzzy combination of these\ncriteria that we evaluate within the context of a web page clustering task. Our\nproposed approach, called Abstract Fuzzy Combination of Criteria (AFCC), can\nadapt to datasets whose features are distributed differently, achieving good\nresults compared to other similar fuzzy logic based approaches and TF-IDF\nacross different datasets.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2016 15:44:52 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Garc\u00eda-Plaza", "Alberto P.", ""], ["Fresno", "V\u00edctor", ""], ["Mart\u00ednez", "Raquel", ""], ["Zubiaga", "Arkaitz", ""]]}, {"id": "1606.04648", "submitter": "Liang Pang", "authors": "Liang Pang, Yanyan Lan, Jiafeng Guo, Jun Xu, Xueqi Cheng", "title": "A Study of MatchPyramid Models on Ad-hoc Retrieval", "comments": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been successfully applied to many text matching\ntasks, such as paraphrase identification, question answering, and machine\ntranslation. Although ad-hoc retrieval can also be formalized as a text\nmatching task, few deep models have been tested on it. In this paper, we study\na state-of-the-art deep matching model, namely MatchPyramid, on the ad-hoc\nretrieval task. The MatchPyramid model employs a convolutional neural network\nover the interactions between query and document to produce the matching score.\nWe conducted extensive experiments to study the impact of different pooling\nsizes, interaction functions and kernel sizes on the retrieval performance.\nFinally, we show that the MatchPyramid models can significantly outperform\nseveral recently introduced deep matching models on the retrieval task, but\nstill cannot compete with the traditional retrieval models, such as BM25 and\nlanguage models.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 05:39:30 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Pang", "Liang", ""], ["Lan", "Yanyan", ""], ["Guo", "Jiafeng", ""], ["Xu", "Jun", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1606.04666", "submitter": "Matus Medo", "authors": "Alexandre Vidmer, Matus Medo", "title": "The essential role of time in network-based recommendation", "comments": "7 pages, 3 figures, 2 tables", "journal-ref": "EPL 116, 30007 (2016)", "doi": "10.1209/0295-5075/116/30007", "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random walks on bipartite networks have been used extensively to design\npersonalized recommendation methods. While aging has been identified as a key\ncomponent in the growth of information networks, most research has focused on\nthe networks' structural properties and neglected the often available time\ninformation. Time has been largely ignored both by the investigated\nrecommendation methods as well as by the methodology used to evaluate them. We\nshow that this time-unaware approach overestimates the methods' recommendation\nperformance. Motivated by microscopic rules of network growth, we propose a\ntime-aware modification of an existing recommendation method and show that by\ncombining the temporal and structural aspects, it outperforms the existing\nmethods. The performance improvements are particularly striking in systems with\nfast aging.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2016 07:53:11 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Vidmer", "Alexandre", ""], ["Medo", "Matus", ""]]}, {"id": "1606.05705", "submitter": "Shoou-I Yu", "authors": "Shoou-I Yu, Yi Yang, Zhongwen Xu, Shicheng Xu, Deyu Meng, Zexi Mao,\n  Zhigang Ma, Ming Lin, Xuanchong Li, Huan Li, Zhenzhong Lan, Lu Jiang,\n  Alexander G. Hauptmann, Chuang Gan, Xingzhong Du, Xiaojun Chang", "title": "Strategies for Searching Video Content with Text Queries or Video\n  Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large number of user-generated videos uploaded on to the Internet\neveryday has led to many commercial video search engines, which mainly rely on\ntext metadata for search. However, metadata is often lacking for user-generated\nvideos, thus these videos are unsearchable by current search engines.\nTherefore, content-based video retrieval (CBVR) tackles this metadata-scarcity\nproblem by directly analyzing the visual and audio streams of each video. CBVR\nencompasses multiple research topics, including low-level feature design,\nfeature fusion, semantic detector training and video search/reranking. We\npresent novel strategies in these topics to enhance CBVR in both accuracy and\nspeed under different query inputs, including pure textual queries and query by\nvideo examples. Our proposed strategies have been incorporated into our\nsubmission for the TRECVID 2014 Multimedia Event Detection evaluation, where\nour system outperformed other submissions in both text queries and video\nexample queries, thus demonstrating the effectiveness of our proposed\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2016 23:27:06 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Yu", "Shoou-I", ""], ["Yang", "Yi", ""], ["Xu", "Zhongwen", ""], ["Xu", "Shicheng", ""], ["Meng", "Deyu", ""], ["Mao", "Zexi", ""], ["Ma", "Zhigang", ""], ["Lin", "Ming", ""], ["Li", "Xuanchong", ""], ["Li", "Huan", ""], ["Lan", "Zhenzhong", ""], ["Jiang", "Lu", ""], ["Hauptmann", "Alexander G.", ""], ["Gan", "Chuang", ""], ["Du", "Xingzhong", ""], ["Chang", "Xiaojun", ""]]}, {"id": "1606.05859", "submitter": "Shenglin Zhao", "authors": "Shenglin Zhao, Tong Zhao, Irwin King and Michael R. Lyu", "title": "GT-SEER: Geo-Temporal SEquential Embedding Rank for Point-of-interest\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point-of-interest (POI) recommendation is an important application in\nlocation-based social networks (LBSNs), which learns the user preference and\nmobility pattern from check-in sequences to recommend POIs. However, previous\nPOI recommendation systems model check-in sequences based on either tensor\nfactorization or Markov chain model, which cannot capture contextual check-in\ninformation in sequences. The contextual check-in information implies the\ncomplementary functions among POIs that compose an individual's daily check-in\nsequence. In this paper, we exploit the embedding learning technique to capture\nthe contextual check-in information and further propose the\n\\textit{{\\textbf{SE}}}quential \\textit{{\\textbf{E}}}mbedding\n\\textit{{\\textbf{R}}}ank (\\textit{SEER}) model for POI recommendation. In\nparticular, the \\textit{SEER} model learns user preferences via a pairwise\nranking model under the sequential constraint modeled by the POI embedding\nlearning method. Furthermore, we incorporate two important factors, i.e.,\ntemporal influence and geographical influence, into the \\textit{SEER} model to\nenhance the POI recommendation system. Due to the temporal variance of\nsequences on different days, we propose a temporal POI embedding model and\nincorporate the temporal POI representations into a temporal preference ranking\nmodel to establish the \\textit{T}emporal \\textit{SEER} (\\textit{T-SEER}) model.\nIn addition, We incorporate the geographical influence into the \\textit{T-SEER}\nmodel and develop the \\textit{\\textbf{Geo-Temporal}} \\textit{{\\textbf{SEER}}}\n(\\textit{GT-SEER}) model.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2016 11:45:57 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Zhao", "Shenglin", ""], ["Zhao", "Tong", ""], ["King", "Irwin", ""], ["Lyu", "Michael R.", ""]]}, {"id": "1606.05978", "submitter": "Da-Cheng Juan", "authors": "Da-Cheng Juan, Neil Shah, Mingyu Tang, Zhiliang Qian, Diana\n  Marculescu, Christos Faloutsos", "title": "M3A: Model, MetaModel, and Anomaly Detection in Web Searches", "comments": "10 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  'Alice' is submitting one web search per five minutes, for three hours in a\nrow - is it normal? How to detect abnormal search behaviors, among Alice and\nother users? Is there any distinct pattern in Alice's (or other users') search\nbehavior? We studied what is probably the largest, publicly available, query\nlog that contains more than 30 million queries from 0.6 million users. In this\npaper, we present a novel, user-and group-level framework, M3A: Model,\nMetaModel and Anomaly detection. For each user, we discover and explain a\nsurprising, bi-modal pattern of the inter-arrival time (IAT) of landed queries\n(queries with user click-through). Specifically, the model Camel-Log is\nproposed to describe such an IAT distribution; we then notice the correlations\namong its parameters at the group level. Thus, we further propose the metamodel\nMeta-Click, to capture and explain the two-dimensional, heavy-tail distribution\nof the parameters. Combining Camel-Log and Meta-Click, the proposed M3A has the\nfollowing strong points: (1) the accurate modeling of marginal IAT\ndistribution, (2) quantitative interpretations, and (3) anomaly detection.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 05:46:47 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Juan", "Da-Cheng", ""], ["Shah", "Neil", ""], ["Tang", "Mingyu", ""], ["Qian", "Zhiliang", ""], ["Marculescu", "Diana", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1606.06081", "submitter": "Philipp Schaer", "authors": "Philipp Schaer and Philipp Mayr and Sebastian S\\\"unkler and Dirk\n  Lewandowski", "title": "How Relevant is the Long Tail? A Relevance Assessment Study on Million\n  Short", "comments": "to appear in Experimental IR Meets Multilinguality, Multimodality,\n  and Interaction. 7th International Conference of the CLEF Association, CLEF\n  2016, \\'Evora, Portugal, September 5-8, 2016", "journal-ref": null, "doi": "10.1007/978-3-319-44564-9_20", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users of web search engines are known to mostly focus on the top ranked\nresults of the search engine result page. While many studies support this well\nknown information seeking pattern only few studies concentrate on the question\nwhat users are missing by neglecting lower ranked results. To learn more about\nthe relevance distributions in the so-called long tail we conducted a relevance\nassessment study with the Million Short long-tail web search engine. While we\nsee a clear difference in the content between the head and the tail of the\nsearch engine result list we see no statistical significant differences in the\nbinary relevance judgments and weak significant differences when using graded\nrelevance. The tail contains different but still valuable results. We argue\nthat the long tail can be a rich source for the diversification of web search\nengine result lists but it needs more evaluation to clearly describe the\ndifferences.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 12:18:35 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Schaer", "Philipp", ""], ["Mayr", "Philipp", ""], ["S\u00fcnkler", "Sebastian", ""], ["Lewandowski", "Dirk", ""]]}, {"id": "1606.06083", "submitter": "Vivek Gupta", "authors": "Vivek Gupta, Harish Karnick, Ashendra Bansal, Pradhuman Jhala", "title": "Product Classification in E-Commerce using Distributional Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Product classification is the task of automatically predicting a taxonomy\npath for a product in a predefined taxonomy hierarchy given a textual product\ndescription or title. For efficient product classification we require a\nsuitable representation for a document (the textual description of a product)\nfeature vector and efficient and fast algorithms for prediction. To address the\nabove challenges, we propose a new distributional semantics representation for\ndocument vector formation. We also develop a new two-level ensemble approach\nutilizing (with respect to the taxonomy tree) a path-wise, node-wise and\ndepth-wise classifiers for error reduction in the final product classification.\nOur experiments show the effectiveness of the distributional representation and\nthe ensemble approach on data sets from a leading e-commerce platform and\nachieve better results on various evaluation metrics compared to earlier\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 12:26:21 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 10:38:52 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Gupta", "Vivek", ""], ["Karnick", "Harish", ""], ["Bansal", "Ashendra", ""], ["Jhala", "Pradhuman", ""]]}, {"id": "1606.06086", "submitter": "Navid Rekabsaz", "authors": "Navid Rekabsaz, Mihai Lupu, Allan Hanbury", "title": "Uncertainty in Neural Network Word Embedding: Exploration of Threshold\n  for Similarity", "comments": "Neu-IR Workshop at the ACM Conference on Research and Development in\n  Information Retrieval (NeuIR-SIGIR 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding, specially with its recent developments, promises a\nquantification of the similarity between terms. However, it is not clear to\nwhich extent this similarity value can be genuinely meaningful and useful for\nsubsequent tasks. We explore how the similarity score obtained from the models\nis really indicative of term relatedness. We first observe and quantify the\nuncertainty factor of the word embedding models regarding to the similarity\nvalue. Based on this factor, we introduce a general threshold on various\ndimensions which effectively filters the highly related terms. Our evaluation\non four information retrieval collections supports the effectiveness of our\napproach as the results of the introduced threshold are significantly better\nthan the baseline while being equal to or statistically indistinguishable from\nthe optimal results.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 12:31:13 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 07:33:59 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Rekabsaz", "Navid", ""], ["Lupu", "Mihai", ""], ["Hanbury", "Allan", ""]]}, {"id": "1606.06137", "submitter": "Markus Koskela", "authors": "Petri Luukkonen, Markus Koskela, and Patrik Flor\\'een", "title": "LSTM-Based Predictions for Proactive Information Retrieval", "comments": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21,\n  2016, Pisa, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for proactive information retrieval targeted at\nretrieving relevant information during a writing task. In our method, the\ncurrent task and the needs of the user are estimated, and the potential next\nsteps are unobtrusively predicted based on the user's past actions. We focus on\nthe task of writing, in which the user is coalescing previously collected\ninformation into a text. Our proactive system automatically recommends the user\nrelevant background information. The proposed system incorporates text input\nprediction using a long short-term memory (LSTM) network. We present\nsimulations, which show that the system is able to reach higher precision\nvalues in an exploratory search setting compared to both a baseline and a\ncomparison system.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2016 14:26:33 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Luukkonen", "Petri", ""], ["Koskela", "Markus", ""], ["Flor\u00e9en", "Patrik", ""]]}, {"id": "1606.06424", "submitter": "Tanmay Basu", "authors": "Tanmay Basu, Shraman Kumar, Abhishek Kalyan, Priyanka Jayaswal, Pawan\n  Goyal, Stephen Pettifer and Siddhartha R. Jonnalagadda", "title": "A Novel Framework to Expedite Systematic Reviews by Automatically\n  Building Information Extraction Training Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A systematic review identifies and collates various clinical studies and\ncompares data elements and results in order to provide an evidence based answer\nfor a particular clinical question. The process is manual and involves lot of\ntime. A tool to automate this process is lacking. The aim of this work is to\ndevelop a framework using natural language processing and machine learning to\nbuild information extraction algorithms to identify data elements in a new\nprimary publication, without having to go through the expensive task of manual\nannotation to build gold standards for each data element type. The system is\ndeveloped in two stages. Initially, it uses information contained in existing\nsystematic reviews to identify the sentences from the PDF files of the included\nreferences that contain specific data elements of interest using a modified\nJaccard similarity measure. These sentences have been treated as labeled data.A\nSupport Vector Machine (SVM) classifier is trained on this labeled data to\nextract data elements of interests from a new article. We conducted experiments\non Cochrane Database systematic reviews related to congestive heart failure\nusing inclusion criteria as an example data element. The empirical results show\nthat the proposed system automatically identifies sentences containing the data\nelement of interest with a high recall (93.75%) and reasonable precision\n(27.05% - which means the reviewers have to read only 3.7 sentences on\naverage). The empirical results suggest that the tool is retrieving valuable\ninformation from the reference articles, even when it is time-consuming to\nidentify them manually. Thus we hope that the tool will be useful for automatic\ndata extraction from biomedical research publications. The future scope of this\nwork is to generalize this information framework for all types of systematic\nreviews.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 04:56:33 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Basu", "Tanmay", ""], ["Kumar", "Shraman", ""], ["Kalyan", "Abhishek", ""], ["Jayaswal", "Priyanka", ""], ["Goyal", "Pawan", ""], ["Pettifer", "Stephen", ""], ["Jonnalagadda", "Siddhartha R.", ""]]}, {"id": "1606.06623", "submitter": "Georgios Balikas", "authors": "Georgios Balikas and Massih-Reza Amini", "title": "An empirical study on large scale text classification with skip-gram\n  embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the integration of word embeddings as classification features\nin the setting of large scale text classification. Such representations have\nbeen used in a plethora of tasks, however their application in classification\nscenarios with thousands of classes has not been extensively researched,\npartially due to hardware limitations. In this work, we examine efficient\ncomposition functions to obtain document-level from word-level embeddings and\nwe subsequently investigate their combination with the traditional\none-hot-encoding representations. By presenting empirical evidence on large,\nmulti-class, multi-label classification problems, we demonstrate the efficiency\nand the performance benefits of this combination.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2016 15:39:35 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Balikas", "Georgios", ""], ["Amini", "Massih-Reza", ""]]}, {"id": "1606.06816", "submitter": "Liangjie Hong", "authors": "Liangjie Hong and Yue Shi and Suju Rajan", "title": "Learning Optimal Card Ranking from Query Reformulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile search has recently been shown to be the major contributor to the\ngrowing search market. The key difference between mobile search and desktop\nsearch is that information presentation is limited to the screen space of the\nmobile device. Thus, major search engines have adopted a new type of search\nresult presentation, known as \\textit{information cards}, in which each card\npresents summarized results from one domain/vertical, for a given query, to\naugment the standard blue-links search results. While it has been widely\nacknowledged that information cards are particularly suited to mobile user\nexperience, it is also challenging to optimize such result sets. Typically,\nuser engagement metrics like query reformulation are based on whole ranked list\nof cards for each query and most traditional learning to rank algorithms\nrequire per-item relevance labels. In this paper, we investigate the\npossibility of interpreting query reformulation into effective relevance labels\nfor query-card pairs. We inherit the concept of conventional learning-to-rank,\nand propose pointwise, pairwise and listwise interpretations for query\nreformulation. In addition, we propose a learning-to-label strategy that learns\nthe contribution of each card, with respect to a query, where such\ncontributions can be used as labels for training card ranking models. We\nutilize a state-of-the-art ranking model and demonstrate the effectiveness of\nproposed mechanisms on a large-scale mobile data from a major search engine,\nshowing that models trained from labels derived from user engagement can\nsignificantly outperform ones trained from human judgment labels.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 04:52:18 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Hong", "Liangjie", ""], ["Shi", "Yue", ""], ["Rajan", "Suju", ""]]}, {"id": "1606.06905", "submitter": "Ying Wen", "authors": "Ying Wen, Weinan Zhang, Rui Luo, Jun Wang", "title": "Learning text representation using recurrent convolutional neural\n  network with highway layers", "comments": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the rapid development of word embedding and neural networks has\nbrought new inspiration to various NLP and IR tasks. In this paper, we describe\na staged hybrid model combining Recurrent Convolutional Neural Networks (RCNN)\nwith highway layers. The highway network module is incorporated in the middle\ntakes the output of the bi-directional Recurrent Neural Network (Bi-RNN) module\nin the first stage and provides the Convolutional Neural Network (CNN) module\nin the last stage with the input. The experiment shows that our model\noutperforms common neural network models (CNN, RNN, Bi-RNN) on a sentiment\nanalysis task. Besides, the analysis of how sequence length influences the RCNN\nwith highway layers shows that our model could learn good representation for\nthe long text.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 11:30:47 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 16:17:05 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Wen", "Ying", ""], ["Zhang", "Weinan", ""], ["Luo", "Rui", ""], ["Wang", "Jun", ""]]}, {"id": "1606.06991", "submitter": "Nawal Ould Amer", "authors": "Nawal Ould-Amer and Philippe Mulhem and Mathias Gery", "title": "Toward Word Embedding for Personalized Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents preliminary works on using Word Embedding (word2vec) for\nquery expansion in the context of Personalized Information Retrieval.\nTraditionally, word embeddings are learned on a general corpus, like Wikipedia.\nIn this work we try to personalize the word embeddings learning, by achieving\nthe learning on the user's profile. The word embeddings are then in the same\ncontext than the user interests. Our proposal is evaluated on the CLEF Social\nBook Search 2016 collection. The results obtained show that some efforts should\nbe made in the way to apply Word Embedding in the context of Personalized\nInformation Retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 15:53:29 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Ould-Amer", "Nawal", ""], ["Mulhem", "Philippe", ""], ["Gery", "Mathias", ""]]}, {"id": "1606.07006", "submitter": "Xiao Yang", "authors": "Xiao Yang, Craig Macdonald, Iadh Ounis", "title": "Using Word Embeddings in Twitter Election Classification", "comments": "NeuIR Workshop 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings and convolutional neural networks (CNN) have attracted\nextensive attention in various classification tasks for Twitter, e.g. sentiment\nclassification. However, the effect of the configuration used to train and\ngenerate the word embeddings on the classification performance has not been\nstudied in the existing literature. In this paper, using a Twitter election\nclassification task that aims to detect election-related tweets, we investigate\nthe impact of the background dataset used to train the embedding models, the\ncontext window size and the dimensionality of word embeddings on the\nclassification performance. By comparing the classification results of two word\nembedding models, which are trained using different background corpora (e.g.\nWikipedia articles and Twitter microposts), we show that the background data\ntype should align with the Twitter classification dataset to achieve a better\nperformance. Moreover, by evaluating the results of word embeddings models\ntrained using various context window sizes and dimensionalities, we found that\nlarge context window and dimension sizes are preferable to improve the\nperformance. Our experimental results also show that using word embeddings and\nCNN leads to statistically significant improvements over various baselines such\nas random, SVM with TF-IDF and SVM with word embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 16:37:55 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 10:22:17 GMT"}, {"version": "v3", "created": "Tue, 21 Mar 2017 18:29:49 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Yang", "Xiao", ""], ["Macdonald", "Craig", ""], ["Ounis", "Iadh", ""]]}, {"id": "1606.07056", "submitter": "Abhay Prakash", "authors": "Abhay Prakash, Chris Brockett, Puneet Agrawal", "title": "Emulating Human Conversations using Convolutional Neural Network-based\n  IR", "comments": "5 pages, Neu-IR'16 SIGIR Workshop on Neural Information Retrieval,\n  July 21, 2016, Pisa, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational agents (\"bots\") are beginning to be widely used in\nconversational interfaces. To design a system that is capable of emulating\nhuman-like interactions, a conversational layer that can serve as a fabric for\nchat-like interaction with the agent is needed. In this paper, we introduce a\nmodel that employs Information Retrieval by utilizing convolutional deep\nstructured semantic neural network-based features in the ranker to present\nhuman-like responses in ongoing conversation with a user. In conversations,\naccounting for context is critical to the retrieval model; we show that our\ncontext-sensitive approach using a Convolutional Deep Structured Semantic Model\n(cDSSM) with character trigrams significantly outperforms several conventional\nbaselines in terms of the relevance of responses retrieved.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 19:55:24 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Prakash", "Abhay", ""], ["Brockett", "Chris", ""], ["Agrawal", "Puneet", ""]]}, {"id": "1606.07103", "submitter": "Sai Praneeth Suggu", "authors": "Sai Praneeth Suggu, Kushwanth N. Goutham, Manoj K. Chinnakotla and\n  Manish Shrivastava", "title": "Deep Feature Fusion Network for Answer Quality Prediction in Community\n  Question Answering", "comments": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21,\n  2016, Pisa, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community Question Answering (cQA) forums have become a popular medium for\nsoliciting direct answers to specific questions of users from experts or other\nexperienced users on a given topic. However, for a given question, users\nsometimes have to sift through a large number of low-quality or irrelevant\nanswers to find out the answer which satisfies their information need. To\nalleviate this, the problem of Answer Quality Prediction (AQP) aims to predict\nthe quality of an answer posted in response to a forum question. Current AQP\nsystems either learn models using - a) various hand-crafted features (HCF) or\nb) use deep learning (DL) techniques which automatically learn the required\nfeature representations.\n  In this paper, we propose a novel approach for AQP known as - \"Deep Feature\nFusion Network (DFFN)\" which leverages the advantages of both hand-crafted\nfeatures and deep learning based systems. Given a question-answer pair along\nwith its metadata, DFFN independently - a) learns deep features using a\nConvolutional Neural Network (CNN) and b) computes hand-crafted features using\nvarious external resources and then combines them using a deep neural network\ntrained to predict the final answer quality. DFFN achieves state-of-the-art\nperformance on the standard SemEval-2015 and SemEval-2016 benchmark datasets\nand outperforms baseline approaches which individually employ either HCF or DL\nbased techniques alone.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 20:58:08 GMT"}, {"version": "v2", "created": "Sun, 26 Jun 2016 05:54:51 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Suggu", "Sai Praneeth", ""], ["Goutham", "Kushwanth N.", ""], ["Chinnakotla", "Manoj K.", ""], ["Shrivastava", "Manish", ""]]}, {"id": "1606.07137", "submitter": "Abeed Sarker", "authors": "Abeed Sarker", "title": "Automated Extraction of Number of Subjects in Randomised Controlled\n  Trials", "comments": "unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple approach for automatically extracting the number of\nsubjects involved in randomised controlled trials (RCT). Our approach first\napplies a set of rule-based techniques to extract candidate study sizes from\nthe abstracts of the articles. Supervised classification is then performed over\nthe candidates with support vector machines, using a small set of lexical,\nstructural, and contextual features. With only a small annotated training set\nof 201 RCTs, we obtained an accuracy of 88\\%. We believe that this system will\naid complex medical text processing tasks such as summarisation and question\nanswering.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2016 23:35:59 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Sarker", "Abeed", ""]]}, {"id": "1606.07154", "submitter": "Mihajlo Grbovic", "authors": "Mihajlo Grbovic, Vladan Radosavljevic, Nemanja Djuric, Narayan\n  Bhamidipati, Jaikit Savla, Varun Bhagwan, Doug Sharp", "title": "E-commerce in Your Inbox: Product Recommendations at Scale", "comments": "10 pages, 12 figures, Proceedings of the 21th ACM SIGKDD\n  International Conference on Knowledge Discovery and Data Mining (KDD 2015),\n  Sydney, Australia", "journal-ref": "Proceedings of the 21th ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining (KDD 2015), Sydney, Australia", "doi": "10.1145/2783258.2788627.", "report-no": null, "categories": "cs.AI cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years online advertising has become increasingly ubiquitous and\neffective. Advertisements shown to visitors fund sites and apps that publish\ndigital content, manage social networks, and operate e-mail services. Given\nsuch large variety of internet resources, determining an appropriate type of\nadvertising for a given platform has become critical to financial success.\nNative advertisements, namely ads that are similar in look and feel to content,\nhave had great success in news and social feeds. However, to date there has not\nbeen a winning formula for ads in e-mail clients. In this paper we describe a\nsystem that leverages user purchase history determined from e-mail receipts to\ndeliver highly personalized product ads to Yahoo Mail users. We propose to use\na novel neural language-based algorithm specifically tailored for delivering\neffective product recommendations, which was evaluated against baselines that\nincluded showing popular products and products predicted based on\nco-occurrence. We conducted rigorous offline testing using a large-scale\nproduct purchase data set, covering purchases of more than 29 million users\nfrom 172 e-commerce websites. Ads in the form of product recommendations were\nsuccessfully tested on online traffic, where we observed a steady 9% lift in\nclick-through rates over other ad formats in mail, as well as comparable lift\nin conversion rates. Following successful tests, the system was launched into\nproduction during the holiday season of 2014.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 01:20:59 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Grbovic", "Mihajlo", ""], ["Radosavljevic", "Vladan", ""], ["Djuric", "Nemanja", ""], ["Bhamidipati", "Narayan", ""], ["Savla", "Jaikit", ""], ["Bhagwan", "Varun", ""], ["Sharp", "Doug", ""]]}, {"id": "1606.07188", "submitter": "Ju Yang", "authors": "Ju Yang, Jiancong Tong, Rebecca J. Stones, Zhaohua Zhang, Benjun Ye,\n  Gang Wang, Xiaoguang Liu", "title": "Selective Term Proximity Scoring Via BP-ANN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When two terms occur together in a document, the probability of a close\nrelationship between them and the document itself is greater if they are in\nnearby positions. However, ranking functions including term proximity (TP)\nrequire larger indexes than traditional document-level indexing, which slows\ndown query processing. Previous studies also show that this technique is not\neffective for all types of queries. Here we propose a document ranking model\nwhich decides for which queries it would be beneficial to use a proximity-based\nranking, based on a collection of features of the query. We use a machine\nlearning approach in determining whether utilizing TP will be beneficial.\nExperiments show that the proposed model returns improved rankings while also\nreducing the overhead incurred as a result of using TP statistics.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 05:02:24 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Yang", "Ju", ""], ["Tong", "Jiancong", ""], ["Stones", "Rebecca J.", ""], ["Zhang", "Zhaohua", ""], ["Ye", "Benjun", ""], ["Wang", "Gang", ""], ["Liu", "Xiaoguang", ""]]}, {"id": "1606.07211", "submitter": "Gia-Hung Nguyen", "authors": "Gia-Hung Nguyen, Lynda Tamine, Laure Soulier, Nathalie Bricon-Souf", "title": "Toward a Deep Neural Approach for Knowledge-Based IR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of the semantic gap between a document and a\nquery within an ad-hoc information retrieval task. In this context, knowledge\nbases (KBs) have already been acknowledged as valuable means since they allow\nthe representation of explicit relations between entities. However, they do not\nnecessarily represent implicit relations that could be hidden in a corpora.\nThis latter issue is tackled by recent works dealing with deep representation\nlearn ing of texts. With this in mind, we argue that embedding KBs within deep\nneural architectures supporting documentquery matching would give rise to\nfine-grained latent representations of both words and their semantic relations.\nIn this paper, we review the main approaches of neural-based document ranking\nas well as those approaches for latent representation of entities and relations\nvia KBs. We then propose some avenues to incorporate KBs in deep neural\napproaches for document ranking. More particularly, this paper advocates that\nKBs can be used either to support enhanced latent representations of queries\nand documents based on both distributional and relational semantics or to serve\nas a semantic translator between their latent distributional representations.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 07:21:28 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Nguyen", "Gia-Hung", ""], ["Tamine", "Lynda", ""], ["Soulier", "Laure", ""], ["Bricon-Souf", "Nathalie", ""]]}, {"id": "1606.07219", "submitter": "Nattiya Kanhabua Dr.", "authors": "Nattiya Kanhabua and Huamin Ren and Thomas B. Moeslund", "title": "Learning Dynamic Classes of Events using Stacked Multilayer Perceptron\n  Networks", "comments": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, 6 pages, 4\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People often use a web search engine to find information about events of\ninterest, for example, sport competitions, political elections, festivals and\nentertainment news. In this paper, we study a problem of detecting\nevent-related queries, which is the first step before selecting a suitable\ntime-aware retrieval model. In general, event-related information needs can be\nobserved in query streams through various temporal patterns of user search\nbehavior, e.g., spiky peaks for popular events, and periodicities for\nrepetitive events. However, it is also common that users search for non-popular\nevents, which may not exhibit temporal variations in query streams, e.g., past\nevents recently occurred, historical events triggered by anniversaries or\nsimilar events, and future events anticipated to happen. To address the\nchallenge of detecting dynamic classes of events, we propose a novel deep\nlearning model to classify a given query into a predetermined set of multiple\nevent types. Our proposed model, a Stacked Multilayer Perceptron (S-MLP)\nnetwork, consists of multilayer perceptron used as a basic learning unit. We\nassemble stacked units to further learn complex relationships between neutrons\nin successive layers. To evaluate our proposed model, we conduct experiments\nusing real-world queries and a set of manually created ground truth.\nPreliminary results have shown that our proposed deep learning model\noutperforms the state-of-the-art classification models significantly.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 08:16:38 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2016 21:53:43 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Kanhabua", "Nattiya", ""], ["Ren", "Huamin", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "1606.07287", "submitter": "Andrea Esuli", "authors": "Fabio Carrara, Andrea Esuli, Tiziano Fagni, Fabrizio Falchi, Alejandro\n  Moreo Fern\\'andez", "title": "Picture It In Your Mind: Generating High Level Visual Representations\n  From Textual Descriptions", "comments": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21,\n  2016, Pisa, Italy", "journal-ref": null, "doi": "10.1007/s10791-017-9318-6", "report-no": null, "categories": "cs.IR cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we tackle the problem of image search when the query is a short\ntextual description of the image the user is looking for. We choose to\nimplement the actual search process as a similarity search in a visual feature\nspace, by learning to translate a textual query into a visual representation.\nSearching in the visual feature space has the advantage that any update to the\ntranslation model does not require to reprocess the, typically huge, image\ncollection on which the search is performed. We propose Text2Vis, a neural\nnetwork that generates a visual representation, in the visual feature space of\nthe fc6-fc7 layers of ImageNet, from a short descriptive text. Text2Vis\noptimizes two loss functions, using a stochastic loss-selection method. A\nvisual-focused loss is aimed at learning the actual text-to-visual feature\nmapping, while a text-focused loss is aimed at modeling the higher-level\nsemantic concepts expressed in language and countering the overfit on\nnon-relevant visual components of the visual loss. We report preliminary\nresults on the MS-COCO dataset.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:25:09 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Carrara", "Fabio", ""], ["Esuli", "Andrea", ""], ["Fagni", "Tiziano", ""], ["Falchi", "Fabrizio", ""], ["Fern\u00e1ndez", "Alejandro Moreo", ""]]}, {"id": "1606.07298", "submitter": "Wojciech Samek", "authors": "Leila Arras and Franziska Horn and Gr\\'egoire Montavon and\n  Klaus-Robert M\\\"uller and Wojciech Samek", "title": "Explaining Predictions of Non-Linear Classifiers in NLP", "comments": "7 pages, 3 figures, Paper accepted for 1st Workshop on Representation\n  Learning for NLP at ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layer-wise relevance propagation (LRP) is a recently proposed technique for\nexplaining predictions of complex non-linear classifiers in terms of input\nvariables. In this paper, we apply LRP for the first time to natural language\nprocessing (NLP). More precisely, we use it to explain the predictions of a\nconvolutional neural network (CNN) trained on a topic categorization task. Our\nanalysis highlights which words are relevant for a specific prediction of the\nCNN. We compare our technique to standard sensitivity analysis, both\nqualitatively and quantitatively, using a \"word deleting\" perturbation\nexperiment, a PCA analysis, and various visualizations. All experiments\nvalidate the suitability of LRP for explaining the CNN predictions, which is\nalso in line with results reported in recent image classification studies.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 12:53:31 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Arras", "Leila", ""], ["Horn", "Franziska", ""], ["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1606.07351", "submitter": "Caiing Dong", "authors": "Cailing Dong and Arvind Agarwal", "title": "A Relevant Content Filtering Based Framework For Data Stream\n  Summarization", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media platforms are a rich source of information these days, however,\nof all the available information, only a small fraction is of users' interest.\nTo help users catch up with the latest topics of their interests from the large\namount of information available in social media, we present a relevant content\nfiltering based framework for data stream summarization. More specifically,\ngiven the topic or event of interest, this framework can dynamically discover\nand filter out relevant information from irrelevant information in the stream\nof text provided by social media platforms. It then further captures the most\nrepresentative and up-to-date information to generate a sequential summary or\nevent story line along with the evolution of the topic or event. Our framework\ndoes not depend on any labeled data, it instead uses the weak supervision\nprovided by the user, which matches the real scenarios of users searching for\ninformation about an ongoing event. We experimented on two real events traced\nby a Twitter dataset from TREC 2011. The results verified the effectiveness of\nrelevant content filtering and sequential summary generation of the proposed\nframework. It also shows its robustness of using the most easy-to-obtain weak\nsupervision, i.e., trending topic or hashtag. Thus, this framework can be\neasily integrated into social media platforms such as Twitter to generate\nsequential summaries for the events of interest. We also make the manually\ngenerated gold-standard sequential summaries of the two test events publicly\navailable for future use in the community.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 15:49:08 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Dong", "Cailing", ""], ["Agarwal", "Arvind", ""]]}, {"id": "1606.07496", "submitter": "Roberto Camacho Barranco", "authors": "Roberto Camacho Barranco (1), Laura M. Rodriguez (1), Rebecca Urbina\n  (1), and M. Shahriar Hossain (1) ((1) The University of Texas at El Paso)", "title": "Is a Picture Worth Ten Thousand Words in a Review Dataset?", "comments": "10 pages, 11 figures, \"for associated results, see\n  http://http://auto-captioning.herokuapp.com/\" \"submitted to DLRS 2016\n  workshop\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While textual reviews have become prominent in many recommendation-based\nsystems, automated frameworks to provide relevant visual cues against text\nreviews where pictures are not available is a new form of task confronted by\ndata mining and machine learning researchers. Suggestions of pictures that are\nrelevant to the content of a review could significantly benefit the users by\nincreasing the effectiveness of a review. We propose a deep learning-based\nframework to automatically: (1) tag the images available in a review dataset,\n(2) generate a caption for each image that does not have one, and (3) enhance\neach review by recommending relevant images that might not be uploaded by the\ncorresponding reviewer. We evaluate the proposed framework using the Yelp\nChallenge Dataset. While a subset of the images in this particular dataset are\ncorrectly captioned, the majority of the pictures do not have any associated\ntext. Moreover, there is no mapping between reviews and images. Each image has\na corresponding business-tag where the picture was taken, though. The overall\ndata setting and unavailability of crucial pieces required for a mapping make\nthe problem of recommending images for reviews a major challenge. Qualitative\nand quantitative evaluations indicate that our proposed framework provides high\nquality enhancements through automatic captioning, tagging, and recommendation\nfor mapping reviews and images.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2016 22:04:08 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Barranco", "Roberto Camacho", "", "The University of Texas at El Paso"], ["Rodriguez", "Laura M.", "", "The University of Texas at El Paso"], ["Urbina", "Rebecca", "", "The University of Texas at El Paso"], ["Hossain", "M. Shahriar", "", "The University of Texas at El Paso"]]}, {"id": "1606.07565", "submitter": "Daniel Cohen", "authors": "Daniel Cohen, Qingyao Ai, W. Bruce Croft", "title": "Adaptability of Neural Networks on Varying Granularity IR Tasks", "comments": "4 pages, Neu-IR'16 SIGIR Workshop on Neural Information Retrieval,\n  July 21, 2016, Pisa, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in Information Retrieval (IR) using Deep Learning models has\nyielded state of the art results on a variety of IR tasks. Deep neural networks\n(DNN) are capable of learning ideal representations of data during the training\nprocess, removing the need for independently extracting features. However, the\nstructures of these DNNs are often tailored to perform on specific datasets. In\naddition, IR tasks deal with text at varying levels of granularity from single\nfactoids to documents containing thousands of words. In this paper, we examine\nthe role of the granularity on the performance of common state of the art DNN\nstructures in IR.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 04:40:48 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Cohen", "Daniel", ""], ["Ai", "Qingyao", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1606.07604", "submitter": "Hendy Irawan", "authors": "Hendy Irawan", "title": "Kategorisasi dokumen web secara otomatis berdasarkan folksonomy\n  menggunakan multinomial naive Bayes classifier", "comments": "39 pages, in Indonesian. NIM: 113010043. Program Sarjana, Jurusan\n  Teknik Informatika, Sekolah Tinggi Teknologi TELKOM / Telkom University.\n  Keywords: naive Bayes, text classification, folksonomy, indexing. Kata kunci:\n  naive Bayes, text classification, folksonomy, indexing", "journal-ref": null, "doi": "10.13140/RG.2.1.4053.4641", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Folksonomy is a non-hierarchical document categorizing system, that treats\nevery category in a flat manner, dan every category is entered freely by anyone\nwho submitted a document in these categories. Categorization is done\nautomatically at the time a document is submitted, by entering the list of\ncategories that best fit the document. del.icio.us (http://del.icio.us) site is\none of the most popular social bookmarking sites that uses folksonomy.\n  Usage of folksonomy, although very easy, also has its weaknesses, such as use\nof different tags for the same concept, use of the same tag for different\nconcepts, no quality control, etc. We try to provide a solution for some of\nthese problems by analyzing Web documents' contents and categorizing them\nautomatically using multinomial naive Bayes algorithm.\n  Bayes classifier works by using a set of evidences and a set of classes. By\ntraining the system using sample data, we can determine the probability of an\nevidence given a particular class. Bayes classifier also uses prior probability\nof a class, which can be calculated from sample data. From these analysis, when\ngiven a new document which is formed by a set of evidences (words), the\nprobabilities of each class given that document (posterior probabilities) can\nbe determined.\n  This system is implemented using PHP 5, Apache, and MySQL. The conclusion\nfrom building this system is that the Bayes method can be used to automatically\ncategorize documents and also as an assistive tool for manual categorization.\n  -----\n  Folksonomy merupakan metode kategorisasi dokumen yang tidak hierarkis,\nmenyamaratakan kedudukan setiap kategori, dan judul kategori ditentukan secara\nbebas oleh siapa saja yang memasukkan sebuah dokumen di dalam kategori-kategori\ntersebut.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 08:36:21 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Irawan", "Hendy", ""]]}, {"id": "1606.07608", "submitter": "Dwaipayan Roy", "authors": "Dwaipayan Roy, Debjyoti Paul, Mandar Mitra, Utpal Garain", "title": "Using Word Embeddings for Automatic Query Expansion", "comments": "5 pages, 3 tables, 1 figure. Neu-IR '16 SIGIR Workshop on Neural\n  Information Retrieval July 21, 2016, Pisa, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a framework for Automatic Query Expansion (AQE) is proposed\nusing distributed neural language model word2vec. Using semantic and contextual\nrelation in a distributed and unsupervised framework, word2vec learns a low\ndimensional embedding for each vocabulary entry. Using such a framework, we\ndevise a query expansion technique, where related terms to a query are obtained\nby K-nearest neighbor approach. We explore the performance of the AQE methods,\nwith and without feedback query expansion, and a variant of simple K-nearest\nneighbor in the proposed framework. Experiments on standard TREC ad-hoc data\n(Disk 4, 5 with query sets 301-450, 601-700) and web data (WT10G data with\nquery set 451-550) shows significant improvement over standard term-overlapping\nbased retrieval methods. However the proposed method fails to achieve\ncomparable performance with statistical co-occurrence based feedback method\nsuch as RM3. We have also found that the word2vec based query expansion methods\nperform similarly with and without any feedback information.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 08:41:57 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Roy", "Dwaipayan", ""], ["Paul", "Debjyoti", ""], ["Mitra", "Mandar", ""], ["Garain", "Utpal", ""]]}, {"id": "1606.07659", "submitter": "Florian Strub", "authors": "Florian Strub (CRIStAL, SEQUEL), Romaric Gaudel (CRIStAL, SEQUEL),\n  J\\'er\\'emie Mary (CRIStAL, SEQUEL)", "title": "Hybrid Recommender System based on Autoencoders", "comments": "arXiv admin note: substantial text overlap with arXiv:1603.00806", "journal-ref": "the 1st Workshop on Deep Learning for Recommender Systems, Sep\n  2016, Boston, United States. pp.11 - 16, 2016", "doi": "10.1145/2988450.2988456", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard model for Recommender Systems is the Matrix Completion setting:\ngiven partially known matrix of ratings given by users (rows) to items\n(columns), infer the unknown ratings. In the last decades, few attempts where\ndone to handle that objective with Neural Networks, but recently an\narchitecture based on Autoencoders proved to be a promising approach. In\ncurrent paper, we enhanced that architecture (i) by using a loss function\nadapted to input data with missing values, and (ii) by incorporating side\ninformation. The experiments demonstrate that while side information only\nslightly improve the test error averaged on all users/items, it has more impact\non cold users/items.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 12:37:04 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 15:41:21 GMT"}, {"version": "v3", "created": "Fri, 29 Dec 2017 14:32:51 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Strub", "Florian", "", "CRIStAL, SEQUEL"], ["Gaudel", "Romaric", "", "CRIStAL, SEQUEL"], ["Mary", "J\u00e9r\u00e9mie", "", "CRIStAL, SEQUEL"]]}, {"id": "1606.07660", "submitter": "Christina Lioma Assoc. Prof", "authors": "Christina Lioma and Birger Larsen and Casper Petersen and Jakob Grue\n  Simonsen", "title": "Deep Learning Relevance: Creating Relevant Information (as Opposed to\n  Retrieving it)", "comments": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21,\n  2016, Pisa, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What if Information Retrieval (IR) systems did not just retrieve relevant\ninformation that is stored in their indices, but could also \"understand\" it and\nsynthesise it into a single document? We present a preliminary study that makes\na first step towards answering this question. Given a query, we train a\nRecurrent Neural Network (RNN) on existing relevant information to that query.\nWe then use the RNN to \"deep learn\" a single, synthetic, and we assume,\nrelevant document for that query. We design a crowdsourcing experiment to\nassess how relevant the \"deep learned\" document is, compared to existing\nrelevant documents. Users are shown a query and four wordclouds (of three\nexisting relevant documents and our deep learned synthetic document). The\nsynthetic document is ranked on average most relevant of all.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 12:41:50 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 08:27:29 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Lioma", "Christina", ""], ["Larsen", "Birger", ""], ["Petersen", "Casper", ""], ["Simonsen", "Jakob Grue", ""]]}, {"id": "1606.07674", "submitter": "Yin Zheng", "authors": "Yin Zheng, Cailiang Liu, Bangsheng Tang, Hanning Zhou", "title": "Neural Autoregressive Collaborative Filtering for Implicit Feedback", "comments": "5 pages, 2 figures, accepted by DLRS2016 http://dlrs-workshop.org/", "journal-ref": null, "doi": "10.1145/2988450.2988453", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes implicit CF-NADE, a neural autoregressive model for\ncollaborative filtering tasks using implicit feedback ( e.g. click, watch,\nbrowse behaviors). We first convert a users implicit feedback into a like\nvector and a confidence vector, and then model the probability of the like\nvector, weighted by the confidence vector. The training objective of implicit\nCF-NADE is to maximize a weighted negative log-likelihood. We test the\nperformance of implicit CF-NADE on a dataset collected from a popular digital\nTV streaming service. More specifically, in the experiments, we describe how to\nconvert watch counts into implicit relative rating, and feed into implicit\nCF-NADE. Then we compare the performance of implicit CF-NADE model with the\npopular implicit matrix factorization approach. Experimental results show that\nimplicit CF-NADE significantly outperforms the baseline.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 13:10:50 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 03:11:12 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Zheng", "Yin", ""], ["Liu", "Cailiang", ""], ["Tang", "Bangsheng", ""], ["Zhou", "Hanning", ""]]}, {"id": "1606.07722", "submitter": "Kai-Chun Hsu", "authors": "Kai-Chun Hsu, Szu-Yu Chou, Yi-Hsuan Yang, Tai-Shih Chi", "title": "Neural Network Based Next-Song Recommendation", "comments": "5 pages, 3 figures, the 1st Workshop on Deep Learning for Recommender\n  Systems (DLRS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the next-item/basket recommendation system, which considers the\nsequential relation between bought items, has drawn attention of researchers.\nThe utilization of sequential patterns has boosted performance on several kinds\nof recommendation tasks. Inspired by natural language processing (NLP)\ntechniques, we propose a novel neural network (NN) based next-song recommender,\nCNN-rec, in this paper. Then, we compare the proposed system with several NN\nbased and classic recommendation systems on the next-song recommendation task.\nVerification results indicate the proposed system outperforms classic systems\nand has comparable performance with the state-of-the-art system.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 15:25:55 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Hsu", "Kai-Chun", ""], ["Chou", "Szu-Yu", ""], ["Yang", "Yi-Hsuan", ""], ["Chi", "Tai-Shih", ""]]}, {"id": "1606.07792", "submitter": "Heng-Tze Cheng", "authors": "Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar\n  Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa\n  Ispir, Rohan Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu,\n  Hemal Shah", "title": "Wide & Deep Learning for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear models with nonlinear feature transformations are widely\nused for large-scale regression and classification problems with sparse inputs.\nMemorization of feature interactions through a wide set of cross-product\nfeature transformations are effective and interpretable, while generalization\nrequires more feature engineering effort. With less feature engineering, deep\nneural networks can generalize better to unseen feature combinations through\nlow-dimensional dense embeddings learned for the sparse features. However, deep\nneural networks with embeddings can over-generalize and recommend less relevant\nitems when the user-item interactions are sparse and high-rank. In this paper,\nwe present Wide & Deep learning---jointly trained wide linear models and deep\nneural networks---to combine the benefits of memorization and generalization\nfor recommender systems. We productionized and evaluated the system on Google\nPlay, a commercial mobile app store with over one billion active users and over\none million apps. Online experiment results show that Wide & Deep significantly\nincreased app acquisitions compared with wide-only and deep-only models. We\nhave also open-sourced our implementation in TensorFlow.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 19:07:02 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Cheng", "Heng-Tze", ""], ["Koc", "Levent", ""], ["Harmsen", "Jeremiah", ""], ["Shaked", "Tal", ""], ["Chandra", "Tushar", ""], ["Aradhye", "Hrishi", ""], ["Anderson", "Glen", ""], ["Corrado", "Greg", ""], ["Chai", "Wei", ""], ["Ispir", "Mustafa", ""], ["Anil", "Rohan", ""], ["Haque", "Zakaria", ""], ["Hong", "Lichan", ""], ["Jain", "Vihan", ""], ["Liu", "Xiaobing", ""], ["Shah", "Hemal", ""]]}, {"id": "1606.07828", "submitter": "Jarana Manotumruksa", "authors": "Jarana Manotumruksa, Craig Macdonald and Iadh Ounis", "title": "Modelling User Preferences using Word Embeddings for Context-Aware Venue\n  Recommendation", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Venue recommendation aims to assist users by making personalised suggestions\nof venues to visit, building upon data available from location-based social\nnetworks (LBSNs) such as Foursquare. A particular challenge for this task is\ncontext-aware venue recommendation (CAVR), which additionally takes the\nsurrounding context of the user (e.g. the user's location and the time of day)\ninto account in order to provide more relevant venue suggestions. To address\nthe challenges of CAVR, we describe two approaches that exploit word embedding\ntechniques to infer the vector-space representations of venues, users' existing\npreferences, and users' contextual preferences. Our evaluation upon the test\ncollection of the TREC 2015 Contextual Suggestion track demonstrates that we\ncan significantly enhance the effectiveness of a state-of-the-art venue\nrecommendation approach, as well as produce context-aware recommendations that\nare at least as effective as the top TREC 2015 systems.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2016 20:15:28 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Manotumruksa", "Jarana", ""], ["Macdonald", "Craig", ""], ["Ounis", "Iadh", ""]]}, {"id": "1606.07869", "submitter": "Dwaipayan Roy", "authors": "Dwaipayan Roy, Debasis Ganguly, Mandar Mitra, Gareth J.F. Jones", "title": "Representing Documents and Queries as Sets of Word Embedded Vectors for\n  Information Retrieval", "comments": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval July 21,\n  2016, Pisa, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major difficulty in applying word vector embeddings in IR is in devising an\neffective and efficient strategy for obtaining representations of compound\nunits of text, such as whole documents, (in comparison to the atomic words),\nfor the purpose of indexing and scoring documents. Instead of striving for a\nsuitable method for obtaining a single vector representation of a large\ndocument of text, we rather aim for developing a similarity metric that makes\nuse of the similarities between the individual embedded word vectors in a\ndocument and a query. More specifically, we represent a document and a query as\nsets of word vectors, and use a standard notion of similarity measure between\nthese sets, computed as a function of the similarities between each constituent\nword pair from these sets. We then make use of this similarity measure in\ncombination with standard IR based similarities for document ranking. The\nresults of our initial experimental investigations shows that our proposed\nmethod improves MAP by up to $5.77\\%$, in comparison to standard text-based\nlanguage model similarity, on the TREC ad-hoc dataset.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2016 04:35:47 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Roy", "Dwaipayan", ""], ["Ganguly", "Debasis", ""], ["Mitra", "Mandar", ""], ["Jones", "Gareth J. F.", ""]]}, {"id": "1606.08104", "submitter": "Yifan Chen", "authors": "Yifan Chen, Xiang Zhao, Junjiao Gan, Junkai Ren, and Yang Fang", "title": "Content-Based Top-N Recommendation using Heterogeneous Relations", "comments": "13 pages, 8 figures, ADC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-$N$ recommender systems have been extensively studied. However, the\nsparsity of user-item activities has not been well resolved. While many hybrid\nsystems were proposed to address the cold-start problem, the profile\ninformation has not been sufficiently leveraged. Furthermore, the heterogeneity\nof profiles between users and items intensifies the challenge. In this paper,\nwe propose a content-based top-$N$ recommender system by learning the global\nterm weights in profiles. To achieve this, we bring in PathSim, which could\nwell measures the node similarity with heterogeneous relations (between users\nand items). Starting from the original TF-IDF value, the global term weights\ngradually converge, and eventually reflect both profile and activity\ninformation. To facilitate training, the derivative is reformulated into matrix\nform, which could easily be paralleled. We conduct extensive experiments, which\ndemonstrate the superiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 00:58:16 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Chen", "Yifan", ""], ["Zhao", "Xiang", ""], ["Gan", "Junjiao", ""], ["Ren", "Junkai", ""], ["Fang", "Yang", ""]]}, {"id": "1606.08157", "submitter": "Peter Wittek", "authors": "Peter Wittek, Ying-Hsang Liu, S\\'andor Dar\\'anyi, Tom Gedeon, Ik Soo\n  Lim", "title": "Risk and Ambiguity in Information Seeking: Eye Gaze Patterns Reveal\n  Contextual Behaviour in Dealing with Uncertainty", "comments": "20 pages, 3 figures", "journal-ref": "Frontiers in Psychology, 7, 1790, 2016", "doi": "10.3389/fpsyg.2016.01790", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information foraging connects optimal foraging theory in ecology with how\nhumans search for information. The theory suggests that, following an\ninformation scent, the information seeker must optimize the tradeoff between\nexploration by repeated steps in the search space vs. exploitation, using the\nresources encountered. We conjecture that this tradeoff characterizes how a\nuser deals with uncertainty and its two aspects, risk and ambiguity in economic\ntheory. Risk is related to the perceived quality of the actually visited patch\nof information, and can be reduced by exploiting and understanding the patch to\na better extent. Ambiguity, on the other hand, is the opportunity cost of\nhaving higher quality patches elsewhere in the search space. The aforementioned\ntradeoff depends on many attributes, including traits of the user: at the two\nextreme ends of the spectrum, analytic and wholistic searchers employ entirely\ndifferent strategies. The former type focuses on exploitation first,\ninterspersed with bouts of exploration, whereas the latter type prefers to\nexplore the search space first and consume later. Based on an eye-tracking\nstudy of experts' interactions with novel search interfaces in the biomedical\ndomain, we demonstrate that perceived risk shifts the balance between\nexploration and exploitation in either type of users, tilting it against vs. in\nfavour of ambiguity minimization. Since the pattern of behaviour in information\nforaging is quintessentially sequential, risk and ambiguity minimization cannot\nhappen simultaneously, leading to a fundamental limit on how good such a\ntradeoff can be. This in turn connects information seeking with the emergent\nfield of quantum decision theory.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 08:14:42 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Wittek", "Peter", ""], ["Liu", "Ying-Hsang", ""], ["Dar\u00e1nyi", "S\u00e1ndor", ""], ["Gedeon", "Tom", ""], ["Lim", "Ik Soo", ""]]}, {"id": "1606.08406", "submitter": "Yue Shi", "authors": "Yue Shi, Erheng Zhong, Suju Rajan, Liang Dong, Hao-wei Tseng, Beitao\n  Li", "title": "The Apps You Use Bring The Blogs to Follow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the blog recommendation problem in Tumblr for mobile users in this\npaper. Blog recommendation is challenging since most mobile users would suffer\nfrom the cold start when there are only a limited number of blogs followed by\nthe user. Specifically to address this problem in the mobile domain, we take\ninto account mobile apps, which typically provide rich information from the\nusers. Based on the assumption that the user interests can be reflected from\ntheir app usage patterns, we propose to exploit the app usage data for\nimproving blog recommendation. Building on the state-of-the-art recommendation\nframework, Factorization Machines (FM), we implement app-based FM that\nintegrates app usage data with the user-blog follow relations. In this approach\nthe blog recommendation is generated not only based on the blogs that the user\nfollowed before, but also the apps that the user has often used. We demonstrate\nin a series of experiments that app-based FM can outperform other alternative\napproaches to a significant extent. Our experimental results also show that\nexploiting app usage information is particularly effective for improving blog\nrecommendation quality for cold start users.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2016 18:45:56 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Shi", "Yue", ""], ["Zhong", "Erheng", ""], ["Rajan", "Suju", ""], ["Dong", "Liang", ""], ["Tseng", "Hao-wei", ""], ["Li", "Beitao", ""]]}, {"id": "1606.08521", "submitter": "Fattane Zarrinkalam", "authors": "Fattane Zarrinkalam and Ebrahim Bagheri", "title": "Event Identification in Social Networks", "comments": "It will appear in Encyclopedia with Semantic Computing to be\n  published by World Scientific", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks enable users to freely communicate with each other and share\ntheir recent news, ongoing activities or views about different topics. As a\nresult, they can be seen as a potentially viable source of information to\nunderstand the current emerging topics/events. The ability to model emerging\ntopics is a substantial step to monitor and summarize the information\noriginating from social sources. Applying traditional methods for event\ndetection which are often proposed for processing large, formal and structured\ndocuments, are less effective, due to the short length, noisiness and\ninformality of the social posts. Recent event detection techniques address\nthese challenges by exploiting the opportunities behind abundant information\navailable in social networks. This article provides an overview of the state of\nthe art in event detection from social networks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 00:26:22 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Zarrinkalam", "Fattane", ""], ["Bagheri", "Ebrahim", ""]]}, {"id": "1606.08534", "submitter": "Ian Wesley-Smith", "authors": "Ian Wesley-Smith, Carl T. Bergstrom, Jevin D. West", "title": "Static Ranking of Scholarly Papers using Article-Level Eigenfactor\n  (ALEF)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microsoft Research hosted the 2016 WSDM Cup Challenge based on the Microsoft\nAcademic Graph. The goal was to provide static rankings for the articles that\nmake up the graph, with the rankings to be evaluated against those of human\njudges. While the Microsoft Academic Graph provided metadata about many aspects\nof each scholarly document, we focused more narrowly on citation data and used\nthis contest as an opportunity to test the Article Level Eigenfactor (ALEF), a\nnovel citation-based ranking algorithm, and evaluate its performance against\ncompeting algorithms that drew upon multiple facets of the data from a large,\nreal world dataset (122M papers and 757M citations). Our final submission to\nthis contest was scored at 0.676, earning second place.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 01:55:56 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Wesley-Smith", "Ian", ""], ["Bergstrom", "Carl T.", ""], ["West", "Jevin D.", ""]]}, {"id": "1606.08689", "submitter": "Nemanja Djuric", "authors": "Nemanja Djuric, Hao Wu, Vladan Radosavljevic, Mihajlo Grbovic, Narayan\n  Bhamidipati", "title": "Hierarchical Neural Language Models for Joint Representation of\n  Streaming Documents and their Content", "comments": "24th International World Wide Web Conference", "journal-ref": null, "doi": "10.1145/2736277.2741643", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning distributed representations for documents\nin data streams. The documents are represented as low-dimensional vectors and\nare jointly learned with distributed vector representations of word tokens\nusing a hierarchical framework with two embedded neural language models. In\nparticular, we exploit the context of documents in streams and use one of the\nlanguage models to model the document sequences, and the other to model word\nsequences within them. The models learn continuous vector representations for\nboth word tokens and documents such that semantically similar documents and\nwords are close in a common vector space. We discuss extensions to our model,\nwhich can be applied to personalized recommendation and social relationship\nmining by adding further user layers to the hierarchy, thus learning\nuser-specific vectors to represent individual preferences. We validated the\nlearned representations on a public movie rating data set from MovieLens, as\nwell as on a large-scale Yahoo News data comprising three months of user\nactivity logs collected on Yahoo servers. The results indicate that the\nproposed model can learn useful representations of both documents and word\ntokens, outperforming the current state-of-the-art by a large margin.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 13:32:08 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Djuric", "Nemanja", ""], ["Wu", "Hao", ""], ["Radosavljevic", "Vladan", ""], ["Grbovic", "Mihajlo", ""], ["Bhamidipati", "Narayan", ""]]}, {"id": "1606.08828", "submitter": "Hua Sun", "authors": "Hua Sun and Syed A. Jafar", "title": "The Capacity of Symmetric Private Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Private information retrieval (PIR) is the problem of retrieving as\nefficiently as possible, one out of $K$ messages from $N$ non-communicating\nreplicated databases (each holds all $K$ messages) while keeping the identity\nof the desired message index a secret from each individual database. Symmetric\nPIR (SPIR) is a generalization of PIR to include the requirement that beyond\nthe desired message, the user learns nothing about the other $K-1$ messages.\nThe information theoretic capacity of SPIR (equivalently, the reciprocal of\nminimum download cost) is the maximum number of bits of desired information\nthat can be privately retrieved per bit of downloaded information. We show that\nthe capacity of SPIR is $1-1/N$ regardless of the number of messages $K$, if\nthe databases have access to common randomness (not available to the user) that\nis independent of the messages, in the amount that is at least $1/(N-1)$ bits\nper desired message bit, and zero otherwise. Extensions to the capacity region\nof SPIR and the capacity of finite length SPIR are provided.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2016 19:19:13 GMT"}, {"version": "v2", "created": "Sat, 15 Jul 2017 04:07:25 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Sun", "Hua", ""], ["Jafar", "Syed A.", ""]]}]