[{"id": "1605.00060", "submitter": "Puneet Agarwal", "authors": "Puneet Agarwal and Maya Ramanath and Gautam Shroff", "title": "Relationship Queries on Large graphs using Pregel", "comments": "19 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale graph-structured data arising from social networks, databases,\nknowledge bases, web graphs, etc. is now available for analysis and mining.\nGraph-mining often involves 'relationship queries', which seek a ranked list of\ninteresting interconnections among a given set of entities, corresponding to\nnodes in the graph. While relationship queries have been studied for many\nyears, using various terminologies, e.g., keyword-search, Steiner-tree in a\ngraph etc., the solutions proposed in the literature so far have not focused on\nscaling relationship queries to large graphs having billions of nodes and\nedges, such are now publicly available in the form of 'linked-open-data'. In\nthis paper, we present an algorithm for distributed keyword search (DKS) on\nlarge graphs, based on the graph-parallel computing paradigm Pregel. We also\npresent an analytical proof that our algorithm produces an optimally ranked\nlist of answers if run to completion. Even if terminated early, our algorithm\nproduces approximate answers along with bounds. We describe an optimized\nimplementation of our DKS algorithm along with time-complexity analysis.\nFinally, we report and analyze experiments using an implementation of DKS on\nGiraph the graph-parallel computing framework based on Pregel, and demonstrate\nthat we can efficiently process relationship queries on large-scale subsets of\nlinked-open-data.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 03:15:08 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Agarwal", "Puneet", ""], ["Ramanath", "Maya", ""], ["Shroff", "Gautam", ""]]}, {"id": "1605.00122", "submitter": "Xinyu Fu", "authors": "Xinyu Fu, Eugene Ch'ng, Uwe Aickelin, Lanyun Zhang", "title": "An Improved System for Sentence-level Novelty Detection in Textual\n  Streams", "comments": null, "journal-ref": null, "doi": "10.1049/cp.2015.0250", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty detection in news events has long been a difficult problem. A number\nof models performed well on specific data streams but certain issues are far\nfrom being solved, particularly in large data streams from the WWW where\nunpredictability of new terms requires adaptation in the vector space model. We\npresent a novel event detection system based on the Incremental Term\nFrequency-Inverse Document Frequency (TF-IDF) weighting incorporated with\nLocality Sensitive Hashing (LSH). Our system could efficiently and effectively\nadapt to the changes within the data streams of any new terms with continual\nupdates to the vector space model. Regarding miss probability, our proposed\nnovelty detection framework outperforms a recognised baseline system by\napproximately 16% when evaluating a benchmark dataset from Google News.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 15:04:19 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Fu", "Xinyu", ""], ["Ch'ng", "Eugene", ""], ["Aickelin", "Uwe", ""], ["Zhang", "Lanyun", ""]]}, {"id": "1605.00184", "submitter": "Alexander Nwala", "authors": "Alexander Nwala and Michael Nelson", "title": "A Supervised Learning Algorithm for Binary Domain Classification of Web\n  Queries using SERPs", "comments": "Figure 6. fix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  General purpose Search Engines (SEs) crawl all domains (e.g., Sports, News,\nEntertainment) of the Web, but sometimes the informational need of a query is\nrestricted to a particular domain (e.g., Medical). We leverage the work of SEs\nas part of our effort to route domain specific queries to local Digital\nLibraries (DLs). SEs are often used even if they are not the \"best\" source for\ncertain types of queries. Rather than tell users to \"use this DL for this kind\nof query\", we intend to automatically detect when a query could be better\nserved by a local DL (such as a private, access-controlled DL that is not\ncrawlable via SEs). This is not an easy task because Web queries are short,\nambiguous, and there is lack of quality labeled training data (or it is\nexpensive to create). To detect queries that should be routed to local,\nspecialized DLs, we first send the queries to Google and then examine the\nfeatures in the resulting Search Engine Result Pages (SERPs), and then classify\nthe query as belonging to either the scholar or non-scholar domain. Using\n400,000 AOL queries for the non-scholar domain and 400,000 queries from the\nNASA Technical Report Server (NTRS) for the scholar domain, our classifier\nachieved a precision of 0.809 and F-measure of 0.805.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 23:02:43 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 01:10:45 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Nwala", "Alexander", ""], ["Nelson", "Michael", ""]]}, {"id": "1605.00448", "submitter": "Sihyun Jeong", "authors": "Sihyun Jeong, Giseop Noh, Hayoung Oh, Chong-kwon Kim", "title": "Follow Spam Detection based on Cascaded Social Information", "comments": "34 pages,10 figures, Preprint submitted to Elsevier Information\n  Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last decade we have witnessed the explosive growth of online social\nnetworking services (SNSs) such as Facebook, Twitter, RenRen and LinkedIn.\nWhile SNSs provide diverse benefits for example, forstering interpersonal\nrelationships, community formations and news propagation, they also attracted\nuninvited nuiance. Spammers abuse SNSs as vehicles to spread spams rapidly and\nwidely. Spams, unsolicited or inappropriate messages, significantly impair the\ncredibility and reliability of services. Therefore, detecting spammers has\nbecome an urgent and critical issue in SNSs. This paper deals with Follow spam\nin Twitter. Instead of spreading annoying messages to the public, a spammer\nfollows (subscribes to) legitimate users, and followed a legitimate user. Based\non the assumption that the online relationships of spammers are different from\nthose of legitimate users, we proposed classification schemes that detect\nfollow spammers. Particularly, we focused on cascaded social relations and\ndevised two schemes, TSP-Filtering and SS-Filtering, each of which utilizes\nTriad Significance Profile (TSP) and Social status (SS) in a two-hop subnetwork\ncentered at each other. We also propose an emsemble technique,\nCascaded-Filtering, that combine both TSP and SS properties. Our experiments on\nreal Twitter datasets demonstrated that the proposed three approaches are very\npractical. The proposed schemes are scalable because instead of analyzing the\nwhole network, they inspect user-centered two hop social networks. Our\nperformance study showed that proposed methods yield significantly better\nperformance than prior scheme in terms of true positives and false positives.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 11:58:51 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Jeong", "Sihyun", ""], ["Noh", "Giseop", ""], ["Oh", "Hayoung", ""], ["Kim", "Chong-kwon", ""]]}, {"id": "1605.00596", "submitter": "Shuai Li", "authors": "Shuai Li and Claudio Gentile and Alexandros Karatzoglou", "title": "Graph Clustering Bandits for Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate an efficient context-dependent clustering technique for\nrecommender systems based on exploration-exploitation strategies through\nmulti-armed bandits over multiple users. Our algorithm dynamically groups users\nbased on their observed behavioral similarity during a sequence of logged\nactivities. In doing so, the algorithm reacts to the currently served user by\nshaping clusters around him/her but, at the same time, it explores the\ngeneration of clusters over users which are not currently engaged. We motivate\nthe effectiveness of this clustering policy, and provide an extensive empirical\nanalysis on real-world datasets, showing scalability and improved prediction\nperformance over state-of-the-art methods for sequential clustering of users in\nmulti-armed bandit scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 18:13:04 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Li", "Shuai", ""], ["Gentile", "Claudio", ""], ["Karatzoglou", "Alexandros", ""]]}, {"id": "1605.00635", "submitter": "Hua Sun", "authors": "Hua Sun and Syed A. Jafar", "title": "The Capacity of Robust Private Information Retrieval with Colluding\n  Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Private information retrieval (PIR) is the problem of retrieving as\nefficiently as possible, one out of $K$ messages from $N$ non-communicating\nreplicated databases (each holds all $K$ messages) while keeping the identity\nof the desired message index a secret from each individual database. The\ninformation theoretic capacity of PIR (equivalently, the reciprocal of minimum\ndownload cost) is the maximum number of bits of desired information that can be\nprivately retrieved per bit of downloaded information. $T$-private PIR is a\ngeneralization of PIR to include the requirement that even if any $T$ of the\n$N$ databases collude, the identity of the retrieved message remains completely\nunknown to them. Robust PIR is another generalization that refers to the\nscenario where we have $M \\geq N$ databases, out of which any $M - N$ may fail\nto respond. For $K$ messages and $M\\geq N$ databases out of which at least some\n$N$ must respond, we show that the capacity of $T$-private and Robust PIR is\n$\\left(1+T/N+T^2/N^2+\\cdots+T^{K-1}/N^{K-1}\\right)^{-1}$. The result includes\nas special cases the capacity of PIR without robustness ($M=N$) or $T$-privacy\nconstraints ($T=1$).\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 19:41:15 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Sun", "Hua", ""], ["Jafar", "Syed A.", ""]]}, {"id": "1605.00957", "submitter": "Marco Bertini", "authors": "Andrea Salvi, Simone Ercoli, Marco Bertini and Alberto Del Bimbo", "title": "Bloom Filters and Compact Hash Codes for Efficient and Distributed Image\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for efficient image retrieval, based on a\nsimple and effective hashing of CNN features and the use of an indexing\nstructure based on Bloom filters. These filters are used as gatekeepers for the\ndatabase of image features, allowing to avoid to perform a query if the query\nfeatures are not stored in the database and speeding up the query process,\nwithout affecting retrieval performance. Thanks to the limited memory\nrequirements the system is suitable for mobile applications and distributed\ndatabases, associating each filter to a distributed portion of the database.\nExperimental validation has been performed on three standard image retrieval\ndatasets, outperforming state-of-the-art hashing methods in terms of precision,\nwhile the proposed indexing method obtains a $2\\times$ speedup.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 15:50:54 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Salvi", "Andrea", ""], ["Ercoli", "Simone", ""], ["Bertini", "Marco", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1605.01010", "submitter": "UshaRani Yelipe", "authors": "Yelipe UshaRani, P. Sammulal", "title": "A Novel Approach for Imputation of Missing Attribute Values for\n  Efficient Mining of Medical Datasets - Class Based Cluster Approach", "comments": "Journal Published by University of Zulia, Venezuela and Indexed by\n  Web of Science and Scopus , H.index-5, SJR 0.11 (2014 Elsevier SJR Report),\n  12 Pages", "journal-ref": "Revista Tecnica de la Facultad de Ingeniera, Vol. 39, No 2, 184 -\n  195, 2016", "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing attribute values are quite common in the datasets available in the\nliterature. Missing values are also possible because all attributes values may\nnot be recorded and hence unavailable due to several practical reasons. For all\nthese one must fix missing attribute vales if the analysis has to be done.\nImputation is the first step in analyzing medical datasets. Hence this has\nachieved significant contribution from several medical domain researchers.\nSeveral data mining researchers have proposed various methods and approaches to\nimpute missing values. However very few of them concentrate on dimensionality\nreduction. In this paper, we discuss a novel imputation framework for missing\nvalues imputation. Our approach of filling missing values is rooted on class\nbased clustering approach and essentially aims at medical records\ndimensionality reduction. We use these dimensionality records for carrying\nprediction and classification analysis. A case study is discussed which shows\nhow imputation is performed using proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 18:18:57 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["UshaRani", "Yelipe", ""], ["Sammulal", "P.", ""]]}, {"id": "1605.01478", "submitter": "Yujie Cao", "authors": "Minlie Huang, Yujie Cao, Chao Dong", "title": "Modeling Rich Contexts for Sentiment Classification with LSTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis on social media data such as tweets and weibo has become a\nvery important and challenging task. Due to the intrinsic properties of such\ndata, tweets are short, noisy, and of divergent topics, and sentiment\nclassification on these data requires to modeling various contexts such as the\nretweet/reply history of a tweet, and the social context about authors and\nrelationships. While few prior study has approached the issue of modeling\ncontexts in tweet, this paper proposes to use a hierarchical LSTM to model rich\ncontexts in tweet, particularly long-range context. Experimental results show\nthat contexts can help us to perform sentiment classification remarkably\nbetter.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 03:06:47 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Huang", "Minlie", ""], ["Cao", "Yujie", ""], ["Dong", "Chao", ""]]}, {"id": "1605.02442", "submitter": "Himani Mittal", "authors": "M. Syamala Devi and Himani Mittal", "title": "Machine Learning Techniques with Ontology for Subjective Answer\n  Evaluation", "comments": "11 pages, 5 figures, journal,\n  http://airccse.org/journal/ijnlc/current.html 2016", "journal-ref": null, "doi": "10.5121/ijnlc.2016.5201", "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computerized Evaluation of English Essays is performed using Machine learning\ntechniques like Latent Semantic Analysis (LSA), Generalized LSA, Bilingual\nEvaluation Understudy and Maximum Entropy. Ontology, a concept map of domain\nknowledge, can enhance the performance of these techniques. Use of Ontology\nmakes the evaluation process holistic as presence of keywords, synonyms, the\nright word combination and coverage of concepts can be checked. In this paper,\nthe above mentioned techniques are implemented both with and without Ontology\nand tested on common input data consisting of technical answers of Computer\nScience. Domain Ontology of Computer Graphics is designed and developed. The\nsoftware used for implementation includes Java Programming Language and tools\nsuch as MATLAB, Prot\\'eg\\'e, etc. Ten questions from Computer Graphics with\nsixty answers for each question are used for testing. The results are analyzed\nand it is concluded that the results are more accurate with use of Ontology.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 07:14:52 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Devi", "M. Syamala", ""], ["Mittal", "Himani", ""]]}, {"id": "1605.02916", "submitter": "Dariusz Czerski", "authors": "Pawe{\\l} {\\L}ozi\\'nski, Dariusz Czerski, Mieczys{\\l}aw A. K{\\l}opotek", "title": "Grammatical Case Based IS-A Relation Extraction with Boosting for Polish", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern-based methods of IS-A relation extraction rely heavily on so called\nHearst patterns. These are ways of expressing instance enumerations of a class\nin natural language. While these lexico-syntactic patterns prove quite useful,\nthey may not capture all taxonomical relations expressed in text. Therefore in\nthis paper we describe a novel method of IS-A relation extraction from\npatterns, which uses morpho-syntactical annotations along with grammatical case\nof noun phrases that constitute entities participating in IS-A relation. We\nalso describe a method for increasing the number of extracted relations that we\ncall pseudo-subclass boosting which has potential application in any\npattern-based relation extraction method. Experiments were conducted on a\ncorpus of about 0.5 billion web documents in Polish language.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 10:03:48 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["\u0141ozi\u0144ski", "Pawe\u0142", ""], ["Czerski", "Dariusz", ""], ["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1605.02917", "submitter": "Mohammad Ali Zare Chahooki", "authors": "Seyed Hamid Reza Mohammadi, Mohammad Ali Zare Chahooki", "title": "Web Spam Detection Using Multiple Kernels in Twin Support Vector Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines are the most important tools for web data acquisition. Web\npages are crawled and indexed by search Engines. Users typically locate useful\nweb pages by querying a search engine. One of the challenges in search engines\nadministration is spam pages which waste search engine resources. These pages\nby deception of search engine ranking algorithms try to be showed in the first\npage of results. There are many approaches to web spam pages detection such as\nmeasurement of HTML code style similarity, pages linguistic pattern analysis\nand machine learning algorithm on page content features. One of the famous\nalgorithms has been used in machine learning approach is Support Vector Machine\n(SVM) classifier. Recently basic structure of SVM has been changed by new\nextensions to increase robustness and classification accuracy. In this paper we\nimproved accuracy of web spam detection by using two nonlinear kernels into\nTwin SVM (TSVM) as an improved extension of SVM. The classifier ability to data\nseparation has been increased by using two separated kernels for each class of\ndata. Effectiveness of new proposed method has been experimented with two\npublicly used spam datasets called UK-2007 and UK-2006. Results show the\neffectiveness of proposed kernelized version of TSVM in web spam page\ndetection.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 10:05:40 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Mohammadi", "Seyed Hamid Reza", ""], ["Chahooki", "Mohammad Ali Zare", ""]]}, {"id": "1605.02945", "submitter": "Yuval Pinter", "authors": "Yuval Pinter, Roi Reichart, Idan Szpektor", "title": "The Yahoo Query Treebank, V. 1.0", "comments": "Co-released with the Webscope Dataset (L-28) and with Pinter et al.,\n  Syntactic Parsing of Web Queries with Question Intent, NAACL-HLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A description and annotation guidelines for the Yahoo Webscope release of\nQuery Treebank, Version 1.0, May 2016.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 11:29:28 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 17:20:26 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Pinter", "Yuval", ""], ["Reichart", "Roi", ""], ["Szpektor", "Idan", ""]]}, {"id": "1605.02948", "submitter": "Nasser Ghadiri", "authors": "Milad Moradi, Nasser Ghadiri", "title": "Different approaches for identifying important concepts in probabilistic\n  biomedical text summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic text summarization tools help users in biomedical domain to acquire\ntheir intended information from various textual resources more efficiently.\nSome of the biomedical text summarization systems put the basis of their\nsentence selection approach on the frequency of concepts extracted from the\ninput text. However, it seems that exploring other measures rather than the\nfrequency for identifying the valuable content of the input document, and\nconsidering the correlations existing between concepts may be more useful for\nthis type of summarization. In this paper, we describe a Bayesian summarizer\nfor biomedical text documents. The Bayesian summarizer initially maps the input\ntext to the Unified Medical Language System (UMLS) concepts, then it selects\nthe important ones to be used as classification features. We introduce\ndifferent feature selection approaches to identify the most important concepts\nof the text and to select the most informative content according to the\ndistribution of these concepts. We show that with the use of an appropriate\nfeature selection approach, the Bayesian biomedical summarizer can improve the\nperformance of summarization. We perform extensive evaluations on a corpus of\nscientific papers in biomedical domain. The results show that the Bayesian\nsummarizer outperforms the biomedical summarizers that rely on the frequency of\nconcepts, the domain-independent and baseline methods based on the\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics. Moreover,\nthe results suggest that using the meaningfulness measure and considering the\ncorrelations of concepts in the feature selection step lead to a significant\nincrease in the performance of summarization.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 11:33:33 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 16:02:32 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 14:37:31 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Moradi", "Milad", ""], ["Ghadiri", "Nasser", ""]]}, {"id": "1605.04135", "submitter": "Purushottam Kar", "authors": "Purushottam Kar and Shuai Li and Harikrishna Narasimhan and Sanjay\n  Chawla and Fabrizio Sebastiani", "title": "Online Optimization Methods for the Quantification Problem", "comments": "26 pages, 6 figures. A short version of this manuscript will appear\n  in the proceedings of the 22nd ACM SIGKDD Conference on Knowledge Discovery\n  and Data Mining, KDD 2016", "journal-ref": null, "doi": "10.1145/2939672.2939832", "report-no": null, "categories": "stat.ML cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of class prevalence, i.e., the fraction of a population that\nbelongs to a certain class, is a very useful tool in data analytics and\nlearning, and finds applications in many domains such as sentiment analysis,\nepidemiology, etc. For example, in sentiment analysis, the objective is often\nnot to estimate whether a specific text conveys a positive or a negative\nsentiment, but rather estimate the overall distribution of positive and\nnegative sentiments during an event window. A popular way of performing the\nabove task, often dubbed quantification, is to use supervised learning to train\na prevalence estimator from labeled data.\n  Contemporary literature cites several performance measures used to measure\nthe success of such prevalence estimators. In this paper we propose the first\nonline stochastic algorithms for directly optimizing these\nquantification-specific performance measures. We also provide algorithms that\noptimize hybrid performance measures that seek to balance quantification and\nclassification performance. Our algorithms present a significant advancement in\nthe theory of multivariate optimization and we show, by a rigorous theoretical\nanalysis, that they exhibit optimal convergence. We also report extensive\nexperiments on benchmark and real data sets which demonstrate that our methods\nsignificantly outperform existing optimization techniques used for these\nperformance measures.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 11:14:58 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 04:29:47 GMT"}, {"version": "v3", "created": "Mon, 13 Jun 2016 18:11:54 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Kar", "Purushottam", ""], ["Li", "Shuai", ""], ["Narasimhan", "Harikrishna", ""], ["Chawla", "Sanjay", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1605.04227", "submitter": "Madhav Nimishakavi Mr", "authors": "Madhav Nimishakavi, Uday Singh Saini and Partha Talukdar", "title": "Relation Schema Induction using Tensor Factorization with Side\n  Information", "comments": "Proceedings of the 2016 Conference on Empirical Methods in Natural\n  Language Processing, November 2016. Austin, TX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of documents from a specific domain (e.g., medical research\njournals), how do we automatically build a Knowledge Graph (KG) for that\ndomain? Automatic identification of relations and their schemas, i.e., type\nsignature of arguments of relations (e.g., undergo(Patient, Surgery)), is an\nimportant first step towards this goal. We refer to this problem as Relation\nSchema Induction (RSI). In this paper, we propose Schema Induction using\nCoupled Tensor Factorization (SICTF), a novel tensor factorization method for\nrelation schema induction. SICTF factorizes Open Information Extraction\n(OpenIE) triples extracted from a domain corpus along with additional side\ninformation in a principled way to induce relation schemas. To the best of our\nknowledge, this is the first application of tensor factorization for the RSI\nproblem. Through extensive experiments on multiple real-world datasets, we find\nthat SICTF is not only more accurate than state-of-the-art baselines, but also\nsignificantly faster (about 14x faster).\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 19:44:04 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 04:57:09 GMT"}, {"version": "v3", "created": "Wed, 16 Nov 2016 04:53:42 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Nimishakavi", "Madhav", ""], ["Saini", "Uday Singh", ""], ["Talukdar", "Partha", ""]]}, {"id": "1605.04624", "submitter": "Viet Ha-Thuc", "authors": "Viet Ha-Thuc and Shakti Sinha", "title": "Learning to Rank Personalized Search Results in Professional Networks", "comments": null, "journal-ref": "SIGIR 2016", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LinkedIn search is deeply personalized - for the same queries, different\nsearchers expect completely different results. This paper presents our approach\nto achieving this by mining various data sources available in LinkedIn to infer\nsearchers' intents (such as hiring, job seeking, etc.), as well as extending\nthe concept of homophily to capture the searcher-result similarities on many\naspects. Then, learning-to-rank (LTR) is applied to combine these signals with\nstandard search features.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 00:59:07 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Ha-Thuc", "Viet", ""], ["Sinha", "Shakti", ""]]}, {"id": "1605.04764", "submitter": "Avradeep Bhowmik", "authors": "Avradeep Bhowmik, Nathan Liu, Erheng Zhong, Badri Narayan Bhaskar,\n  Suju Rajan", "title": "Geometry Aware Mappings for High Dimensional Sparse Factors", "comments": "AISTATS 2016, 13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While matrix factorisation models are ubiquitous in large scale\nrecommendation and search, real time application of such models requires inner\nproduct computations over an intractably large set of item factors. In this\nmanuscript we present a novel framework that uses the inverted index\nrepresentation to exploit structural properties of sparse vectors to\nsignificantly reduce the run time computational cost of factorisation models.\nWe develop techniques that use geometry aware permutation maps on a tessellated\nunit sphere to obtain high dimensional sparse embeddings for latent factors\nwith sparsity patterns related to angular closeness of the original latent\nfactors. We also design several efficient and deterministic realisations within\nthis framework and demonstrate with experiments that our techniques lead to\nfaster run time operation with minimal loss of accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 13:21:15 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Bhowmik", "Avradeep", ""], ["Liu", "Nathan", ""], ["Zhong", "Erheng", ""], ["Bhaskar", "Badri Narayan", ""], ["Rajan", "Suju", ""]]}, {"id": "1605.04770", "submitter": "Lamberto Ballan", "authors": "Tiberio Uricchio, Lamberto Ballan, Lorenzo Seidenari, Alberto Del\n  Bimbo", "title": "Automatic Image Annotation via Label Transfer in the Semantic Space", "comments": "To appear in Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2017.05.019", "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image annotation is among the fundamental problems in computer\nvision and pattern recognition, and it is becoming increasingly important in\norder to develop algorithms that are able to search and browse large-scale\nimage collections. In this paper, we propose a label propagation framework\nbased on Kernel Canonical Correlation Analysis (KCCA), which builds a latent\nsemantic space where correlation of visual and textual features are well\npreserved into a semantic embedding. The proposed approach is robust and can\nwork either when the training set is well annotated by experts, as well as when\nit is noisy such as in the case of user-generated tags in social media. We\nreport extensive results on four popular datasets. Our results show that our\nKCCA-based framework can be applied to several state-of-the-art label transfer\nmethods to obtain significant improvements. Our approach works even with the\nnoisy tags of social users, provided that appropriate denoising is performed.\nExperiments on a large scale setting show that our method can provide some\nbenefits even when the semantic space is estimated on a subset of training\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 13:45:15 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 18:24:00 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 13:21:02 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Uricchio", "Tiberio", ""], ["Ballan", "Lamberto", ""], ["Seidenari", "Lorenzo", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1605.04951", "submitter": "Po-Shen Lee", "authors": "Po-shen Lee, Jevin D. West, and Bill Howe", "title": "Viziometrics: Analyzing Visual Information in the Scientific Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientific results are communicated visually in the literature through\ndiagrams, visualizations, and photographs. These information-dense objects have\nbeen largely ignored in bibliometrics and scientometrics studies when compared\nto citations and text. In this paper, we use techniques from computer vision\nand machine learning to classify more than 8 million figures from PubMed into 5\nfigure types and study the resulting patterns of visual information as they\nrelate to impact. We find that the distribution of figures and figure types in\nthe literature has remained relatively constant over time, but can vary widely\nacross field and topic. Remarkably, we find a significant correlation between\nscientific impact and the use of visual information, where higher impact papers\ntend to include more diagrams, and to a lesser extent more plots and\nphotographs. To explore these results and other ways of extracting this visual\ninformation, we have built a visual browser to illustrate the concept and\nexplore design alternatives for supporting viziometric analysis and organizing\nvisual information. We use these results to articulate a new research agenda --\nviziometrics -- to study the organization and presentation of visual\ninformation in the scientific literature.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 21:03:57 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 17:26:22 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Lee", "Po-shen", ""], ["West", "Jevin D.", ""], ["Howe", "Bill", ""]]}, {"id": "1605.05054", "submitter": "Minseok Park", "authors": "Minseok Park, Hanxiang Li, Junmo Kim", "title": "HARRISON: A Benchmark on HAshtag Recommendation for Real-world Images in\n  Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple, short, and compact hashtags cover a wide range of information on\nsocial networks. Although many works in the field of natural language\nprocessing (NLP) have demonstrated the importance of hashtag recommendation,\nhashtag recommendation for images has barely been studied. In this paper, we\nintroduce the HARRISON dataset, a benchmark on hashtag recommendation for real\nworld images in social networks. The HARRISON dataset is a realistic dataset,\ncomposed of 57,383 photos from Instagram and an average of 4.5 associated\nhashtags for each photo. To evaluate our dataset, we design a baseline\nframework consisting of visual feature extractor based on convolutional neural\nnetwork (CNN) and multi-label classifier based on neural network. Based on this\nframework, two single feature-based models, object-based and scene-based model,\nand an integrated model of them are evaluated on the HARRISON dataset. Our\ndataset shows that hashtag recommendation task requires a wide and contextual\nunderstanding of the situation conveyed in the image. As far as we know, this\nwork is the first vision-only attempt at hashtag recommendation for real world\nimages in social networks. We expect this benchmark to accelerate the\nadvancement of hashtag recommendation.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 08:21:07 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Park", "Minseok", ""], ["Li", "Hanxiang", ""], ["Kim", "Junmo", ""]]}, {"id": "1605.05134", "submitter": "Soroush Vosoughi Dr", "authors": "Soroush Vosoughi, Deb Roy", "title": "A Semi-automatic Method for Efficient Detection of Stories on Social\n  Media", "comments": "ICWSM'16, May 17-20, Cologne, Germany. In Proceedings of the 10th\n  International AAAI Conference on Weblogs and Social Media (ICWSM 2016).\n  Cologne, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter has become one of the main sources of news for many people. As\nreal-world events and emergencies unfold, Twitter is abuzz with hundreds of\nthousands of stories about the events. Some of these stories are harmless,\nwhile others could potentially be life-saving or sources of malicious rumors.\nThus, it is critically important to be able to efficiently track stories that\nspread on Twitter during these events. In this paper, we present a novel\nsemi-automatic tool that enables users to efficiently identify and track\nstories about real-world events on Twitter. We ran a user study with 25\nparticipants, demonstrating that compared to more conventional methods, our\ntool can increase the speed and the accuracy with which users can track stories\nabout real-world events.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 12:33:24 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Vosoughi", "Soroush", ""], ["Roy", "Deb", ""]]}, {"id": "1605.05166", "submitter": "Soroush Vosoughi Dr", "authors": "Soroush Vosoughi, Helen Zhou, Deb Roy", "title": "Digital Stylometry: Linking Profiles Across Social Networks", "comments": "SocInfo'15, Beijing, China. In proceedings of the 7th International\n  Conference on Social Informatics (SocInfo 2015). Beijing, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an ever growing number of users with accounts on multiple social\nmedia and networking sites. Consequently, there is increasing interest in\nmatching user accounts and profiles across different social networks in order\nto create aggregate profiles of users. In this paper, we present models for\nDigital Stylometry, which is a method for matching users through stylometry\ninspired techniques. We experimented with linguistic, temporal, and combined\ntemporal-linguistic models for matching user accounts, using standard and novel\ntechniques. Using publicly available data, our best model, a combined\ntemporal-linguistic one, was able to correctly match the accounts of 31% of\n5,612 distinct users across Twitter and Facebook.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 13:47:24 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Vosoughi", "Soroush", ""], ["Zhou", "Helen", ""], ["Roy", "Deb", ""]]}, {"id": "1605.05195", "submitter": "Soroush Vosoughi Dr", "authors": "Soroush Vosoughi, Helen Zhou, Deb Roy", "title": "Enhanced Twitter Sentiment Classification Using Contextual Information", "comments": "In proceedings of the 6th workshop on Computational Approaches to\n  Subjectivity, Sentiment & Social Media Analysis (WASSA) at EMNLP 2015", "journal-ref": null, "doi": "10.18653/v1/W15-2904", "report-no": null, "categories": "cs.SI cs.AI cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The rise in popularity and ubiquity of Twitter has made sentiment analysis of\ntweets an important and well-covered area of research. However, the 140\ncharacter limit imposed on tweets makes it hard to use standard linguistic\nmethods for sentiment classification. On the other hand, what tweets lack in\nstructure they make up with sheer volume and rich metadata. This metadata\nincludes geolocation, temporal and author information. We hypothesize that\nsentiment is dependent on all these contextual factors. Different locations,\ntimes and authors have different emotional valences. In this paper, we explored\nthis hypothesis by utilizing distant supervision to collect millions of\nlabelled tweets from different locations, times and authors. We used this data\nto analyse the variation of tweet sentiments across different authors, times\nand locations. Once we explored and understood the relationship between these\nvariables and sentiment, we used a Bayesian approach to combine these variables\nwith more standard linguistic features such as n-grams to create a Twitter\nsentiment classifier. This combined classifier outperforms the purely\nlinguistic classifier, showing that integrating the rich contextual information\navailable on Twitter into sentiment classification is a promising direction of\nresearch.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 14:51:54 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 03:59:19 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Vosoughi", "Soroush", ""], ["Zhou", "Helen", ""], ["Roy", "Deb", ""]]}, {"id": "1605.05362", "submitter": "Nabiha Asghar", "authors": "Nabiha Asghar", "title": "Yelp Dataset Challenge: Review Rating Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Review websites, such as TripAdvisor and Yelp, allow users to post online\nreviews for various businesses, products and services, and have been recently\nshown to have a significant influence on consumer shopping behaviour. An online\nreview typically consists of free-form text and a star rating out of 5. The\nproblem of predicting a user's star rating for a product, given the user's text\nreview for that product, is called Review Rating Prediction and has lately\nbecome a popular, albeit hard, problem in machine learning. In this paper, we\ntreat Review Rating Prediction as a multi-class classification problem, and\nbuild sixteen different prediction models by combining four feature extraction\nmethods, (i) unigrams, (ii) bigrams, (iii) trigrams and (iv) Latent Semantic\nIndexing, with four machine learning algorithms, (i) logistic regression, (ii)\nNaive Bayes classification, (iii) perceptrons, and (iv) linear Support Vector\nClassification. We analyse the performance of each of these sixteen models to\ncome up with the best model for predicting the ratings from reviews. We use the\ndataset provided by Yelp for training and testing the models.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 20:52:33 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Asghar", "Nabiha", ""]]}, {"id": "1605.05369", "submitter": "Giuseppe Boccignone", "authors": "Alberto Introini, Giorgio Presti, Giuseppe Boccignone", "title": "Audio Features Affected by Music Expressiveness", "comments": "Submitted to ACM SIGIR Conference on Research and Development in\n  Information Retrieval (SIGIR 2016), Pisa, Italy, July 17-21, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within a Music Information Retrieval perspective, the goal of the study\npresented here is to investigate the impact on sound features of the musician's\naffective intention, namely when trying to intentionally convey emotional\ncontents via expressiveness. A preliminary experiment has been performed\ninvolving $10$ tuba players. The recordings have been analysed by extracting a\nvariety of features, which have been subsequently evaluated by combining both\nclassic and machine learning statistical techniques. Results are reported and\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 21:10:07 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Introini", "Alberto", ""], ["Presti", "Giorgio", ""], ["Boccignone", "Giuseppe", ""]]}, {"id": "1605.05402", "submitter": "David Hoyle", "authors": "David C. Hoyle, Andrew Brass", "title": "Statistical mechanics of ontology based annotations", "comments": "27 pages, 5 figures", "journal-ref": "Physica A, 442: 284-299, 2016", "doi": "10.1016/j.physa.2015.09.020", "report-no": null, "categories": "physics.soc-ph cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a statistical mechanical theory of the process of annotating an\nobject with terms selected from an ontology. The term selection process is\nformulated as an ideal lattice gas model, but in a highly structured\ninhomogeneous field. The model enables us to explain patterns recently observed\nin real-world annotation data sets, in terms of the underlying graph structure\nof the ontology. By relating the external field strengths to the information\ncontent of each node in the ontology graph, the statistical mechanical model\nalso allows us to propose a number of practical metrics for assessing the\nquality of both the ontology, and the annotations that arise from its use.\nUsing the statistical mechanical formalism we also study an ensemble of\nontologies of differing size and complexity; an analysis not readily performed\nusing real data alone. Focusing on regular tree ontology graphs we uncover a\nrich set of scaling laws describing the growth in the optimal ontology size as\nthe number of objects being annotated increases. In doing so we provide a\nfurther possible measure for assessment of ontologies.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 00:01:35 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Hoyle", "David C.", ""], ["Brass", "Andrew", ""]]}, {"id": "1605.05721", "submitter": "Ping Li", "authors": "Ping Li", "title": "Linearized GMM Kernels and Normalized Random Fourier Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of \"random Fourier features (RFF)\" has become a popular tool for\napproximating the \"radial basis function (RBF)\" kernel. The variance of RFF is\nactually large. Interestingly, the variance can be substantially reduced by a\nsimple normalization step as we theoretically demonstrate. We name the improved\nscheme as the \"normalized RFF (NRFF)\".\n  We also propose the \"generalized min-max (GMM)\" kernel as a measure of data\nsimilarity. GMM is positive definite as there is an associated hashing method\nnamed \"generalized consistent weighted sampling (GCWS)\" which linearizes this\nnonlinear kernel. We provide an extensive empirical evaluation of the RBF\nkernel and the GMM kernel on more than 50 publicly available datasets. For a\nmajority of the datasets, the (tuning-free) GMM kernel outperforms the\nbest-tuned RBF kernel.\n  We conduct extensive experiments for comparing the linearized RBF kernel\nusing NRFF with the linearized GMM kernel using GCWS. We observe that, to reach\na comparable classification accuracy, GCWS typically requires substantially\nfewer samples than NRFF, even on datasets where the original RBF kernel\noutperforms the original GMM kernel. The empirical success of GCWS (compared to\nNRFF) can also be explained from a theoretical perspective. Firstly, the\nrelative variance (normalized by the squared expectation) of GCWS is\nsubstantially smaller than that of NRFF, except for the very high similarity\nregion (where the variances of both methods are close to zero). Secondly, if we\nmake a model assumption on the data, we can show analytically that GCWS\nexhibits much smaller variance than NRFF for estimating the same object (e.g.,\nthe RBF kernel), except for the very high similarity region.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 19:54:22 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 19:51:39 GMT"}, {"version": "v3", "created": "Thu, 3 Nov 2016 18:42:09 GMT"}, {"version": "v4", "created": "Tue, 21 Feb 2017 17:11:48 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1605.05944", "submitter": "Kimmo Fredriksson", "authors": "Kimmo Fredriksson", "title": "Geometric Near-neighbor Access Tree (GNAT) revisited", "comments": "Minor changes, submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric Near-neighbor Access Tree (GNAT) is a metric space indexing method\nbased on hierarchical hyperplane partitioning of the space. While GNAT is very\nefficient in proximity searching, it has a bad reputation of being a memory\nhog. We show that this is partially based on too coarse analysis, and that the\nmemory requirements can be lowered while at the same time improving the search\nefficiency. We also show how to make GNAT memory adaptive in a smooth way, and\nthat the hyperplane partitioning can be replaced with ball partitioning, which\ncan further improve the search performance. We conclude with experimental\nresults showing the new methods can give significant performance boost.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 13:31:36 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 07:35:48 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Fredriksson", "Kimmo", ""]]}, {"id": "1605.06538", "submitter": "Silvia Puglisi", "authors": "Silvia Puglisi, Javier Parra-Arnau, Jordi Forn\\'e, David\n  Rebollo-Monedero", "title": "On Content-Based Recommendation and User Privacy in Social-Tagging\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems and content filtering approaches based on annotations\nand ratings, essentially rely on users expressing their preferences and\ninterests through their actions, in order to provide personalised content. This\nactivity, in which users engage collectively, has been named social tagging.\nAlthough it has opened a myriad of new possibilities for application\ninteroperability on the semantic web, it is also posing new privacy threats.\nSocial tagging consists in describing online or online resources by using\nfree-text labels (i.e. tags), therefore exposing the user's profile and\nactivity to privacy attacks. Tag forgery is a privacy enhancing technology\nconsisting of generating tags for categories or resources that do not reflect\nthe user's actual preferences. By modifying their profile, tag forgery may have\na negative impact on the quality of the recommendation system, thus protecting\nuser privacy to a certain extent but at the expenses of utility loss. The\nimpact of tag forgery on content-based recommendation is, therefore,\ninvestigated in a real-world application scenario where different forgery\nstrategies are evaluated, and the consequent loss in utility is measured and\ncompared.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 21:00:50 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Puglisi", "Silvia", ""], ["Parra-Arnau", "Javier", ""], ["Forn\u00e9", "Jordi", ""], ["Rebollo-Monedero", "David", ""]]}, {"id": "1605.06650", "submitter": "Peixian Chen", "authors": "Peixian Chen, Nevin L. Zhang, Tengfei Liu, Leonard K.M. Poon, Zhourong\n  Chen and Farhan Khawar", "title": "Latent Tree Models for Hierarchical Topic Detection", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for hierarchical topic detection where topics are\nobtained by clustering documents in multiple ways. Specifically, we model\ndocument collections using a class of graphical models called hierarchical\nlatent tree models (HLTMs). The variables at the bottom level of an HLTM are\nobserved binary variables that represent the presence/absence of words in a\ndocument. The variables at other levels are binary latent variables, with those\nat the lowest latent level representing word co-occurrence patterns and those\nat higher levels representing co-occurrence of patterns at the level below.\nEach latent variable gives a soft partition of the documents, and document\nclusters in the partitions are interpreted as topics. Latent variables at high\nlevels of the hierarchy capture long-range word co-occurrence patterns and\nhence give thematically more general topics, while those at low levels of the\nhierarchy capture short-range word co-occurrence patterns and give thematically\nmore specific topics. Unlike LDA-based topic models, HLTMs do not refer to a\ndocument generation process and use word variables instead of token variables.\nThey use a tree structure to model the relationships between topics and words,\nwhich is conducive to the discovery of meaningful topics and topic hierarchies.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 14:36:33 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 08:59:14 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Chen", "Peixian", ""], ["Zhang", "Nevin L.", ""], ["Liu", "Tengfei", ""], ["Poon", "Leonard K. M.", ""], ["Chen", "Zhourong", ""], ["Khawar", "Farhan", ""]]}, {"id": "1605.06693", "submitter": "Gaurav Singh", "authors": "Gaurav Singh and Benjamin Piwowarski", "title": "Efficient Document Indexing Using Pivot Tree", "comments": "6 Pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for efficiently searching top-k neighbors for\ndocuments represented in high dimensional space of terms based on the cosine\nsimilarity. Mostly, documents are stored as bag-of-words tf-idf representation.\nOne of the most used ways of computing similarity between a pair of documents\nis cosine similarity between the vector representations, but cosine similarity\nis not a metric distance measure as it doesn't follow triangle inequality,\ntherefore most metric searching methods can not be applied directly. We propose\nan efficient method for indexing documents using a pivot tree that leads to\nefficient retrieval. We also study the relation between precision and\nefficiency for the proposed method and compare it with a state of the art in\nthe area of document searching based on inner product.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 19:55:03 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Singh", "Gaurav", ""], ["Piwowarski", "Benjamin", ""]]}, {"id": "1605.06778", "submitter": "Maximilian Schmitt", "authors": "Maximilian Schmitt and Bj\\\"orn W. Schuller", "title": "openXBOW - Introducing the Passau Open-Source Crossmodal Bag-of-Words\n  Toolkit", "comments": "9 pages, 1 figure, pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce openXBOW, an open-source toolkit for the generation of\nbag-of-words (BoW) representations from multimodal input. In the BoW principle,\nword histograms were first used as features in document classification, but the\nidea was and can easily be adapted to, e.g., acoustic or visual low-level\ndescriptors, introducing a prior step of vector quantisation. The openXBOW\ntoolkit supports arbitrary numeric input features and text input and\nconcatenates computed subbags to a final bag. It provides a variety of\nextensions and options. To our knowledge, openXBOW is the first publicly\navailable toolkit for the generation of crossmodal bags-of-words. The\ncapabilities of the tool are exemplified in two sample scenarios:\ntime-continuous speech-based emotion recognition and sentiment analysis in\ntweets where improved results over other feature representation forms were\nobserved.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 12:14:55 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Schmitt", "Maximilian", ""], ["Schuller", "Bj\u00f6rn W.", ""]]}, {"id": "1605.07025", "submitter": "Hyunjik Kim", "authors": "Hyunjik Kim, Xiaoyu Lu, Seth Flaxman, Yee Whye Teh", "title": "Collaborative Filtering with Side Information: a Gaussian Process\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of collaborative filtering (CF) with side information,\nthrough the lens of Gaussian Process (GP) regression. Driven by the idea of\nusing the kernel to explicitly model user-item similarities, we formulate the\nGP in a way that allows the incorporation of low-rank matrix factorisation,\narriving at our model, the Tucker Gaussian Process (TGP). Consequently, TGP\ngeneralises classical Bayesian matrix factorisation models, and goes beyond\nthem to give a natural and elegant method for incorporating side information,\ngiving enhanced predictive performance for CF problems. Moreover we show that\nit is a novel model for regression, especially well-suited to grid-structured\ndata and problems where the dependence on covariates is close to being\nseparable.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 14:19:02 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 16:19:46 GMT"}, {"version": "v3", "created": "Thu, 8 Jun 2017 11:18:56 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Kim", "Hyunjik", ""], ["Lu", "Xiaoyu", ""], ["Flaxman", "Seth", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1605.07422", "submitter": "Rolf Jagerman", "authors": "Rolf Jagerman, Carsten Eickhoff and Maarten de Rijke", "title": "Computing Web-scale Topic Models using an Asynchronous Parameter Server", "comments": "To appear in SIGIR 2017", "journal-ref": null, "doi": "10.1145/3077136.3084135", "report-no": null, "categories": "cs.DC cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models such as Latent Dirichlet Allocation (LDA) have been widely used\nin information retrieval for tasks ranging from smoothing and feedback methods\nto tools for exploratory search and discovery. However, classical methods for\ninferring topic models do not scale up to the massive size of today's publicly\navailable Web-scale data sets. The state-of-the-art approaches rely on custom\nstrategies, implementations and hardware to facilitate their asynchronous,\ncommunication-intensive workloads.\n  We present APS-LDA, which integrates state-of-the-art topic modeling with\ncluster computing frameworks such as Spark using a novel asynchronous parameter\nserver. Advantages of this integration include convenient usage of existing\ndata processing pipelines and eliminating the need for disk writes as data can\nbe kept in memory from start to finish. Our goal is not to outperform highly\ncustomized implementations, but to propose a general high-performance topic\nmodeling framework that can easily be used in today's data processing\npipelines. We compare APS-LDA to the existing Spark LDA implementations and\nshow that our system can, on a 480-core cluster, process up to 135 times more\ndata and 10 times more topics without sacrificing model quality.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 12:40:29 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 08:43:56 GMT"}, {"version": "v3", "created": "Sun, 18 Jun 2017 22:37:23 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Jagerman", "Rolf", ""], ["Eickhoff", "Carsten", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1605.07722", "submitter": "Longqi Yang", "authors": "Longqi Yang, Cheng-Kang Hsieh, Hongjian Yang, Nicola Dell, Serge\n  Belongie, Curtis Cole, Deborah Estrin", "title": "Yum-me: A Personalized Nutrient-based Meal Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nutrient-based meal recommendations have the potential to help individuals\nprevent or manage conditions such as diabetes and obesity. However, learning\npeople's food preferences and making recommendations that simultaneously appeal\nto their palate and satisfy nutritional expectations are challenging. Existing\napproaches either only learn high-level preferences or require a prolonged\nlearning period. We propose Yum-me, a personalized nutrient-based meal\nrecommender system designed to meet individuals' nutritional expectations,\ndietary restrictions, and fine-grained food preferences. Yum-me enables a\nsimple and accurate food preference profiling procedure via a visual quiz-based\nuser interface, and projects the learned profile into the domain of\nnutritionally appropriate food options to find ones that will appeal to the\nuser. We present the design and implementation of Yum-me, and further describe\nand evaluate two innovative contributions. The first contriution is an open\nsource state-of-the-art food image analysis model, named FoodDist. We\ndemonstrate FoodDist's superior performance through careful benchmarking and\ndiscuss its applicability across a wide array of dietary applications. The\nsecond contribution is a novel online learning framework that learns food\npreference from item-wise and pairwise image comparisons. We evaluate the\nframework in a field study of 227 anonymous users and demonstrate that it\noutperforms other baselines by a significant margin. We further conducted an\nend-to-end validation of the feasibility and effectiveness of Yum-me through a\n60-person user study, in which Yum-me improves the recommendation acceptance\nrate by 42.63%.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 04:13:49 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 14:48:18 GMT"}, {"version": "v3", "created": "Sun, 30 Apr 2017 17:43:02 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Yang", "Longqi", ""], ["Hsieh", "Cheng-Kang", ""], ["Yang", "Hongjian", ""], ["Dell", "Nicola", ""], ["Belongie", "Serge", ""], ["Cole", "Curtis", ""], ["Estrin", "Deborah", ""]]}, {"id": "1605.07844", "submitter": "Javid Dadashkarimi", "authors": "Javid Dadashkarimi, Mahsa S. Shahshahani, Amirhossein Tebbifakhr,\n  Heshaam Faili, and Azadeh Shakery", "title": "Dimension Projection among Languages based on Pseudo-relevant Documents\n  for Query Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using top-ranked documents in response to a query has been shown to be an\neffective approach to improve the quality of query translation in\ndictionary-based cross-language information retrieval. In this paper, we\npropose a new method for dictionary-based query translation based on dimension\nprojection of embedded vectors from the pseudo-relevant documents in the source\nlanguage to their equivalents in the target language. To this end, first we\nlearn low-dimensional vectors of the words in the pseudo-relevant collections\nseparately and then aim to find a query-dependent transformation matrix between\nthe vectors of translation pairs appeared in the collections. At the next step,\nrepresentation of each query term is projected to the target language and then,\nafter using a softmax function, a query-dependent translation model is built.\nFinally, the model is used for query translation. Our experiments on four CLEF\ncollections in French, Spanish, German, and Italian demonstrate that the\nproposed method outperforms a word embedding baseline based on bilingual\nshuffling and a further number of competitive baselines. The proposed method\nreaches up to 87% performance of machine translation (MT) in short queries and\nconsiderable improvements in verbose queries.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 12:04:43 GMT"}, {"version": "v2", "created": "Sat, 8 Oct 2016 11:19:10 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Dadashkarimi", "Javid", ""], ["Shahshahani", "Mahsa S.", ""], ["Tebbifakhr", "Amirhossein", ""], ["Faili", "Heshaam", ""], ["Shakery", "Azadeh", ""]]}, {"id": "1605.07852", "submitter": "Javid Dadashkarimi", "authors": "Javid Dadashkarimi, Hossein Nasr Esfahani, Heshaam Faili, and Azadeh\n  Shakery", "title": "SS4MCT: A Statistical Stemmer for Morphologically Complex Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been multiple attempts to resolve various inflection matching\nproblems in information retrieval. Stemming is a common approach to this end.\nAmong many techniques for stemming, statistical stemming has been shown to be\neffective in a number of languages, particularly highly inflected languages. In\nthis paper we propose a method for finding affixes in different positions of a\nword. Common statistical techniques heavily rely on string similarity in terms\nof prefix and suffix matching. Since infixes are common in irregular/informal\ninflections in morphologically complex texts, it is required to find infixes\nfor stemming. In this paper we propose a method whose aim is to find\nstatistical inflectional rules based on minimum edit distance table of word\npairs and the likelihoods of the rules in a language. These rules are used to\nstatistically stem words and can be used in different text mining tasks.\nExperimental results on CLEF 2008 and CLEF 2009 English-Persian CLIR tasks\nindicate that the proposed method significantly outperforms all the baselines\nin terms of MAP.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 12:25:26 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 21:37:19 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Dadashkarimi", "Javid", ""], ["Esfahani", "Hossein Nasr", ""], ["Faili", "Heshaam", ""], ["Shakery", "Azadeh", ""]]}, {"id": "1605.07891", "submitter": "Fernando Diaz", "authors": "Fernando Diaz, Bhaskar Mitra, Nick Craswell", "title": "Query Expansion with Locally-Trained Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous space word embeddings have received a great deal of attention in\nthe natural language processing and machine learning communities for their\nability to model term similarity and other relationships. We study the use of\nterm relatedness in the context of query expansion for ad hoc information\nretrieval. We demonstrate that word embeddings such as word2vec and GloVe, when\ntrained globally, underperform corpus and query specific embeddings for\nretrieval tasks. These results suggest that other tasks benefiting from global\nembeddings may also benefit from local embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 14:09:00 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 00:46:06 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Diaz", "Fernando", ""], ["Mitra", "Bhaskar", ""], ["Craswell", "Nick", ""]]}, {"id": "1605.07895", "submitter": "Nabiha Asghar", "authors": "Nabiha Asghar", "title": "Automatic Extraction of Causal Relations from Natural Language Texts: A\n  Comprehensive Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic extraction of cause-effect relationships from natural language\ntexts is a challenging open problem in Artificial Intelligence. Most of the\nearly attempts at its solution used manually constructed linguistic and\nsyntactic rules on small and domain-specific data sets. However, with the\nadvent of big data, the availability of affordable computing power and the\nrecent popularization of machine learning, the paradigm to tackle this problem\nhas slowly shifted. Machines are now expected to learn generic causal\nextraction rules from labelled data with minimal supervision, in a domain\nindependent-manner. In this paper, we provide a comprehensive survey of causal\nrelation extraction techniques from both paradigms, and analyse their relative\nstrengths and weaknesses, with recommendations for future work.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 14:23:21 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Asghar", "Nabiha", ""]]}, {"id": "1605.07980", "submitter": "Yao Wu", "authors": "Bin Liu, Yao Wu, Neil Zhenqiang Gong, Junjie Wu, Hui Xiong, Martin\n  Ester", "title": "Structural Analysis of User Choices for Mobile App Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in smartphone technology have promoted the rapid development of\nmobile apps. However, the availability of a huge number of mobile apps in\napplication stores has imposed the challenge of finding the right apps to meet\nthe user needs. Indeed, there is a critical demand for personalized app\nrecommendations. Along this line, there are opportunities and challenges posed\nby two unique characteristics of mobile apps. First, app markets have organized\napps in a hierarchical taxonomy. Second, apps with similar functionalities are\ncompeting with each other. While there are a variety of approaches for mobile\napp recommendations, these approaches do not have a focus on dealing with these\nopportunities and challenges. To this end, in this paper, we provide a\nsystematic study for addressing these challenges. Specifically, we develop a\nStructural User Choice Model (SUCM) to learn fine-grained user preferences by\nexploiting the hierarchical taxonomy of apps as well as the competitive\nrelationships among apps. Moreover, we design an efficient learning algorithm\nto estimate the parameters for the SUCM model. Finally, we perform extensive\nexperiments on a large app adoption data set collected from Google Play. The\nresults show that SUCM consistently outperforms state-of-the-art top-N\nrecommendation methods by a significant margin.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 17:47:30 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Liu", "Bin", ""], ["Wu", "Yao", ""], ["Gong", "Neil Zhenqiang", ""], ["Wu", "Junjie", ""], ["Xiong", "Hui", ""], ["Ester", "Martin", ""]]}, {"id": "1605.08494", "submitter": "Luciana Fujii Pontello", "authors": "Pedro H. F. Holanda, Bruno Guilherme, Luciana Fujii Pontello, Olga\n  Goussevskaia, Ana Paula Couto e Silva", "title": "Mixtape Application: Music Map Methodology and Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report discusses dimensionality reduction techniques used to create a\nmusic map - a map where the distances between songs represent their similarity\nand that can be used to recommend songs. We evaluate two techniques: Isomap and\nL-Isomap.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 02:28:28 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 12:38:52 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Holanda", "Pedro H. F.", ""], ["Guilherme", "Bruno", ""], ["Pontello", "Luciana Fujii", ""], ["Goussevskaia", "Olga", ""], ["Silva", "Ana Paula Couto e", ""]]}, {"id": "1605.08872", "submitter": "Chenghao Liu", "authors": "Chenghao Liu, Tao Jin, Steven C.H. Hoi, Peilin Zhao, Jianling Sun", "title": "Online Bayesian Collaborative Topic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Topic Regression (CTR) combines ideas of probabilistic matrix\nfactorization (PMF) and topic modeling (e.g., LDA) for recommender systems,\nwhich has gained increasing successes in many applications. Despite enjoying\nmany advantages, the existing CTR algorithms have some critical limitations.\nFirst of all, they are often designed to work in a batch learning manner,\nmaking them unsuitable to deal with streaming data or big data in real-world\nrecommender systems. Second, the document-specific topic proportions of LDA are\nfed to the downstream PMF, but not reverse, which is sub-optimal as the rating\ninformation is not exploited in discovering the low-dimensional representation\nof documents and thus can result in a sub-optimal representation for\nprediction. In this paper, we propose a novel scheme of Online Bayesian\nCollaborative Topic Regression (OBCTR) which is efficient and scalable for\nlearning from data streams. Particularly, we {\\it jointly} optimize the\ncombined objective function of both PMF and LDA in an online learning fashion,\nin which both PMF and LDA tasks can be reinforced each other during the online\nlearning process. Our encouraging experimental results on real-world data\nvalidate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 10:17:37 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Liu", "Chenghao", ""], ["Jin", "Tao", ""], ["Hoi", "Steven C. H.", ""], ["Zhao", "Peilin", ""], ["Sun", "Jianling", ""]]}, {"id": "1605.09362", "submitter": "Travis Gagie", "authors": "Travis Gagie, Aleksi Hartikainen, Kalle Karhu, Juha K\\\"arkk\\\"ainen,\n  Gonzalo Navarro, Simon J. Puglisi and Jouni Sir\\'en", "title": "Document Retrieval on Repetitive String Collections", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. Accepted to the Information\n  Retrieval Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the fastest-growing string collections today are repetitive, that is,\nmost of the constituent documents are similar to many others. As these\ncollections keep growing, a key approach to handling them is to exploit their\nrepetitiveness, which can reduce their space usage by orders of magnitude. We\nstudy the problem of indexing repetitive string collections in order to perform\nefficient document retrieval operations on them. Document retrieval problems\nare routinely solved by search engines on large natural language collections,\nbut the techniques are less developed on generic string collections. The case\nof repetitive string collections is even less understood, and there are very\nfew existing solutions. We develop two novel ideas, {\\em interleaved LCPs} and\n{\\em precomputed document lists}, that yield highly compressed indexes solving\nthe problem of document listing (find all the documents where a string\nappears), top-$k$ document retrieval (find the $k$ documents where a string\nappears most often), and document counting (count the number of documents where\na string appears). We also show that a classical data structure supporting the\nlatter query becomes highly compressible on repetitive data. Finally, we show\nhow the tools we developed can be combined to solve ranked conjunctive and\ndisjunctive multi-term queries under the simple tf-idf model of relevance. We\nthoroughly evaluate the resulting techniques in various real-life\nrepetitiveness scenarios, and recommend the best choices for each case.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 19:40:18 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 15:00:28 GMT"}, {"version": "v3", "created": "Thu, 18 May 2017 21:06:43 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Gagie", "Travis", ""], ["Hartikainen", "Aleksi", ""], ["Karhu", "Kalle", ""], ["K\u00e4rkk\u00e4inen", "Juha", ""], ["Navarro", "Gonzalo", ""], ["Puglisi", "Simon J.", ""], ["Sir\u00e9n", "Jouni", ""]]}, {"id": "1605.09477", "submitter": "Yin Zheng", "authors": "Yin Zheng, Bangsheng Tang, Wenkui Ding, Hanning Zhou", "title": "A Neural Autoregressive Approach to Collaborative Filtering", "comments": "Accepted by ICML2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes CF-NADE, a neural autoregressive architecture for\ncollaborative filtering (CF) tasks, which is inspired by the Restricted\nBoltzmann Machine (RBM) based CF model and the Neural Autoregressive\nDistribution Estimator (NADE). We first describe the basic CF-NADE model for CF\ntasks. Then we propose to improve the model by sharing parameters between\ndifferent ratings. A factored version of CF-NADE is also proposed for better\nscalability. Furthermore, we take the ordinal nature of the preferences into\nconsideration and propose an ordinal cost to optimize CF-NADE, which shows\nsuperior performance. Finally, CF-NADE can be extended to a deep model, with\nonly moderately increased computational complexity. Experimental results show\nthat CF-NADE with a single hidden layer beats all previous state-of-the-art\nmethods on MovieLens 1M, MovieLens 10M, and Netflix datasets, and adding more\nhidden layers can further improve the performance.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 03:07:06 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Zheng", "Yin", ""], ["Tang", "Bangsheng", ""], ["Ding", "Wenkui", ""], ["Zhou", "Hanning", ""]]}, {"id": "1605.09564", "submitter": "Gregory Grefenstette", "authors": "Gregory Grefenstette (TAO), Lawrence Muchemi (TAO)", "title": "Determining the Characteristic Vocabulary for a Specialized Dictionary\n  using Word2vec and a Directed Crawler", "comments": null, "journal-ref": "GLOBALEX 2016: Lexicographic Resources for Human Language\n  Technology, May 2016, Portoroz, Slovenia. 2016", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specialized dictionaries are used to understand concepts in specific domains,\nespecially where those concepts are not part of the general vocabulary, or\nhaving meanings that differ from ordinary languages. The first step in creating\na specialized dictionary involves detecting the characteristic vocabulary of\nthe domain in question. Classical methods for detecting this vocabulary involve\ngathering a domain corpus, calculating statistics on the terms found there, and\nthen comparing these statistics to a background or general language corpus.\nTerms which are found significantly more often in the specialized corpus than\nin the background corpus are candidates for the characteristic vocabulary of\nthe domain. Here we present two tools, a directed crawler, and a distributional\nsemantics package, that can be used together, circumventing the need of a\nbackground corpus. Both tools are available on the web.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 10:31:16 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Grefenstette", "Gregory", "", "TAO"], ["Muchemi", "Lawrence", "", "TAO"]]}, {"id": "1605.09757", "submitter": "Francois Scharffe", "authors": "Sanchit Arora, Chuck Cho, Paul Fitzpatrick, Francois Scharffe", "title": "Towards ontology driven learning of visual concept detectors", "comments": "unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The maturity of deep learning techniques has led in recent years to a\nbreakthrough in object recognition in visual media. While for some specific\nbenchmarks, neural techniques seem to match if not outperform human judgement,\nchallenges are still open for detecting arbitrary concepts in arbitrary videos.\nIn this paper, we propose a system that combines neural techniques, a large\nscale visual concepts ontology, and an active learning loop, to provide on the\nfly model learning of arbitrary concepts. We give an overview of the system as\na whole, and focus on the central role of the ontology for guiding and\nbootstrapping the learning of new concepts, improving the recall of concept\ndetection, and, on the user end, providing semantic search on a library of\nannotated videos.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 18:35:44 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Arora", "Sanchit", ""], ["Cho", "Chuck", ""], ["Fitzpatrick", "Paul", ""], ["Scharffe", "Francois", ""]]}]