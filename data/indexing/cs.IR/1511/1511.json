[{"id": "1511.00099", "submitter": "Anurag Mittal", "authors": "Sarthak Parui and Anurag Mittal", "title": "Sketch-based Image Retrieval from Millions of Images under Rotation,\n  Translation and Scale Variations", "comments": "submitted to IJCV, April 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proliferation of touch-based devices has made sketch-based image retrieval\npractical. While many methods exist for sketch-based object detection/image\nretrieval on small datasets, relatively less work has been done on large\n(web)-scale image retrieval. In this paper, we present an efficient approach\nfor image retrieval from millions of images based on user-drawn sketches.\nUnlike existing methods for this problem which are sensitive to even\ntranslation or scale variations, our method handles rotation, translation,\nscale (i.e. a similarity transformation) and small deformations. The object\nboundaries are represented as chains of connected segments and the database\nimages are pre-processed to obtain such chains that have a high chance of\ncontaining the object. This is accomplished using two approaches in this work:\na) extracting long chains in contour segment networks and b) extracting\nboundaries of segmented object proposals. These chains are then represented by\nsimilarity-invariant variable length descriptors. Descriptor similarities are\ncomputed by a fast Dynamic Programming-based partial matching algorithm. This\nmatching mechanism is used to generate a hierarchical k-medoids based indexing\nstructure for the extracted chains of all database images in an offline process\nwhich is used to efficiently retrieve a small set of possible matched images\nfor query chains. Finally, a geometric verification step is employed to test\ngeometric consistency of multiple chain matches to improve results. Qualitative\nand quantitative results clearly demonstrate superiority of the approach over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 08:50:43 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Parui", "Sarthak", ""], ["Mittal", "Anurag", ""]]}, {"id": "1511.00271", "submitter": "Tianyi Luo", "authors": "Tianyi Luo, Dong Wang, Rong Liu, Yiqiao Pan", "title": "Stochastic Top-k ListNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ListNet is a well-known listwise learning to rank model and has gained much\nattention in recent years. A particular problem of ListNet, however, is the\nhigh computation complexity in model training, mainly due to the large number\nof object permutations involved in computing the gradients. This paper proposes\na stochastic ListNet approach which computes the gradient within a bounded\npermutation subset. It significantly reduces the computation complexity of\nmodel training and allows extension to Top-k models, which is impossible with\nthe conventional implementation based on full-set permutations. Meanwhile, the\nnew approach utilizes partial ranking information of human labels, which helps\nimprove model quality. Our experiments demonstrated that the stochastic ListNet\nmethod indeed leads to better ranking performance and speeds up the model\ntraining remarkably.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 16:34:52 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Luo", "Tianyi", ""], ["Wang", "Dong", ""], ["Liu", "Rong", ""], ["Pan", "Yiqiao", ""]]}, {"id": "1511.00722", "submitter": "Nemanja Spasojevic", "authors": "Nemanja Spasojevic and Adithya Rao", "title": "Identifying Actionable Messages on Social Media", "comments": "9 pages, 2015 IEEE International Big Data Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text actionability detection is the problem of classifying user authored\nnatural language text, according to whether it can be acted upon by a\nresponding agent. In this paper, we propose a supervised learning framework for\ndomain-aware, large-scale actionability classification of social media\nmessages. We derive lexicons, perform an in-depth analysis for over 25 text\nbased features, and explore strategies to handle domains that have limited\ntraining data. We apply these methods to over 46 million messages spanning 75\ncompanies and 35 languages, from both Facebook and Twitter. The models achieve\nan aggregate population-weighted F measure of 0.78 and accuracy of 0.74, with\nvalues of over 0.9 in some cases.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 21:49:25 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Spasojevic", "Nemanja", ""], ["Rao", "Adithya", ""]]}, {"id": "1511.00725", "submitter": "Wajdi Dhifli", "authors": "Wajdi Dhifli, Abdoulaye Banir\\'e Diallo", "title": "Toward an Efficient Multi-class Classification in an Open Universe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is a fundamental task in machine learning and data mining.\nExisting classification methods are designed to classify unknown instances\nwithin a set of previously known training classes. Such a classification takes\nthe form of a prediction within a closed-set of classes. However, a more\nrealistic scenario that fits real-world applications is to consider the\npossibility of encountering instances that do not belong to any of the training\nclasses, $i.e.$, an open-set classification. In such situation, existing\nclosed-set classifiers will assign a training label to these instances\nresulting in a misclassification. In this paper, we introduce Galaxy-X, a novel\nmulti-class classification approach for open-set recognition problems. For each\nclass of the training set, Galaxy-X creates a minimum bounding hyper-sphere\nthat encompasses the distribution of the class by enclosing all of its\ninstances. In such manner, our method is able to distinguish instances\nresembling previously seen classes from those that are of unknown ones. To\nadequately evaluate open-set classification, we introduce a novel evaluation\nprocedure. Experimental results on benchmark datasets show the efficiency of\nour approach in classifying novel instances from known as well as unknown\nclasses.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 22:04:00 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 02:27:55 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 17:22:56 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Dhifli", "Wajdi", ""], ["Diallo", "Abdoulaye Banir\u00e9", ""]]}, {"id": "1511.00906", "submitter": "Jeremi K. Ochab", "authors": "Jeremi K. Ochab", "title": "Reinventing the Triangles: Rule of Thumb for Assessing Detectability", "comments": "6 pages, 4 figures. Accepted to IEEE Computer Society. Presented at\n  The 4th International Workshop on Complex Networks and their Applications,\n  November 23-27, 2015 Bangkok, Thailand", "journal-ref": null, "doi": "10.1109/SITIS.2015.44", "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical significance of network clustering has been an unresolved problem\nsince it was observed that community detection algorithms produce false\npositives even in random graphs. After a phase transition between undetectable\nand detectable cluster structures was discovered, the connection between\nspectra of adjacency matrices and detectability limits were shown, and both\nwere calculated for a wide range of networks with arbitrary degree\ndistributions and community structure. In practice the full eigenspectrum is\nnot known, and whether a given network has any communities within detectability\nregime cannot be easily established. Based on the global clustering coefficient\nwe construct a criterion telling whether in an undirected, unweighted network\nthere is some/no detectable community structure, or if the network is in a\ntransient regime. The method is simple and faster than methods involving\nbootstrapping.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 13:36:52 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Ochab", "Jeremi K.", ""]]}, {"id": "1511.01259", "submitter": "Gregory Grefenstette", "authors": "Gregory Grefenstette (TAO), Karima Rafes (TAO, LRI)", "title": "Transforming Wikipedia into an Ontology-based Information Retrieval\n  Search Engine for Local Experts using a Third-Party Taxonomy", "comments": "Joint Second Workshop on Language and Ontology \\& Terminology and\n  Knowledge Structures (LangOnto2 + TermiKS) LO2TKS, May 2016, Portoroz,\n  Slovenia. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikipedia is widely used for finding general information about a wide variety\nof topics. Its vocation is not to provide local information. For example, it\nprovides plot, cast, and production information about a given movie, but not\nshowing times in your local movie theatre. Here we describe how we can connect\nlocal information to Wikipedia, without altering its content. The case study we\npresent involves finding local scientific experts. Using a third-party\ntaxonomy, independent from Wikipedia's category hierarchy, we index information\nconnected to our local experts, present in their activity reports, and we\nre-index Wikipedia content using the same taxonomy. The connections between\nWikipedia pages and local expert reports are stored in a relational database,\naccessible through as public SPARQL endpoint. A Wikipedia gadget (or plugin)\nactivated by the interested user, accesses the endpoint as each Wikipedia page\nis accessed. An additional tab on the Wikipedia page allows the user to open up\na list of teams of local experts associated with the subject matter in the\nWikipedia page. The technique, though presented here as a way to identify local\nexperts, is generic, in that any third party taxonomy, can be used in this to\nconnect Wikipedia to any non-Wikipedia data source.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 09:41:31 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 14:45:27 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Grefenstette", "Gregory", "", "TAO"], ["Rafes", "Karima", "", "TAO, LRI"]]}, {"id": "1511.01280", "submitter": "Fabrice Rossi", "authors": "Arnaud De Myttenaere (SAMM, Viadeo), Boris Golden (Viadeo),\n  B\\'en\\'edicte Le Grand (CRI), Fabrice Rossi (SAMM)", "title": "Study of a bias in the offline evaluation of a recommendation algorithm", "comments": "arXiv admin note: substantial text overlap with arXiv:1407.0822", "journal-ref": "Petra Perner. 11th Industrial Conference on Data Mining, ICDM\n  2015, Jul 2015, Hamburg, Germany. Ibai Publishing, pp.57-70, 2015, Advances\n  in Data Mining", "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems have been integrated into the majority of large online\nsystems to filter and rank information according to user profiles. It thus\ninfluences the way users interact with the system and, as a consequence, bias\nthe evaluation of the performance of a recommendation algorithm computed using\nhistorical data (via offline evaluation). This paper describes this bias and\ndiscuss the relevance of a weighted offline evaluation to reduce this bias for\ndifferent classes of recommendation algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 10:46:58 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["De Myttenaere", "Arnaud", "", "SAMM, Viadeo"], ["Golden", "Boris", "", "Viadeo"], ["Grand", "B\u00e9n\u00e9dicte Le", "", "CRI"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1511.01282", "submitter": "Phong Nguyen", "authors": "Phong Nguyen and Jun Wang and Alexandros Kalousis", "title": "Factorizing LambdaMART for cold start recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems often rely on point-wise loss metrics such as the mean\nsquared error. However, in real recommendation settings only few items are\npresented to a user. This observation has recently encouraged the use of\nrank-based metrics. LambdaMART is the state-of-the-art algorithm in learning to\nrank which relies on such a metric. Despite its success it does not have a\nprincipled regularization mechanism relying in empirical approaches to control\nmodel complexity leaving it thus prone to overfitting.\n  Motivated by the fact that very often the users' and items' descriptions as\nwell as the preference behavior can be well summarized by a small number of\nhidden factors, we propose a novel algorithm, LambdaMART Matrix Factorization\n(LambdaMART-MF), that learns a low rank latent representation of users and\nitems using gradient boosted trees. The algorithm factorizes lambdaMART by\ndefining relevance scores as the inner product of the learned representations\nof the users and items. The low rank is essentially a model complexity\ncontroller; on top of it we propose additional regularizers to constraint the\nlearned latent representations that reflect the user and item manifolds as\nthese are defined by their original feature based descriptors and the\npreference behavior. Finally we also propose to use a weighted variant of NDCG\nto reduce the penalty for similar items with large rating discrepancy.\n  We experiment on two very different recommendation datasets, meta-mining and\nmovies-users, and evaluate the performance of LambdaMART-MF, with and without\nregularization, in the cold start setting as well as in the simpler matrix\ncompletion setting. In both cases it outperforms in a significant manner\ncurrent state of the art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 10:49:15 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Nguyen", "Phong", ""], ["Wang", "Jun", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "1511.01556", "submitter": "Chao-Lin Liu", "authors": "Chao-Lin Liu, Chih-Kai Huang, Hongsu Wang, Peter K. Bol", "title": "Mining Local Gazetteers of Literary Chinese with CRF and Pattern based\n  Methods for Biographical Information in Chinese History", "comments": "11 pages, 5 figures, 5 tables, the Third Workshop on Big Humanities\n  Data (2015 IEEE BigData), the 29th Pacific Asia Conference on Language,\n  Information and Computation (PACLIC 29)", "journal-ref": null, "doi": "10.1109/BigData.2015.7363931", "report-no": null, "categories": "cs.CL cs.DL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Person names and location names are essential building blocks for identifying\nevents and social networks in historical documents that were written in\nliterary Chinese. We take the lead to explore the research on algorithmically\nrecognizing named entities in literary Chinese for historical studies with\nlanguage-model based and conditional-random-field based methods, and extend our\nwork to mining the document structures in historical documents. Practical\nevaluations were conducted with texts that were extracted from more than 220\nvolumes of local gazetteers (Difangzhi). Difangzhi is a huge and the single\nmost important collection that contains information about officers who served\nin local government in Chinese history. Our methods performed very well on\nthese realistic tests. Thousands of names and addresses were identified from\nthe texts. A good portion of the extracted names match the biographical\ninformation currently recorded in the China Biographical Database (CBDB) of\nHarvard University, and many others can be verified by historians and will\nbecome as new additions to CBDB.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 23:39:46 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Liu", "Chao-Lin", ""], ["Huang", "Chih-Kai", ""], ["Wang", "Hongsu", ""], ["Bol", "Peter K.", ""]]}, {"id": "1511.01559", "submitter": "Chao-Lin Liu", "authors": "Chao-Lin Liu, Hongsu Wang, Wen-Huei Cheng, Chu-Ting Hsu, Wei-Yun Chiu", "title": "Color Aesthetics and Social Networks in Complete Tang Poems:\n  Explorations and Discoveries", "comments": "10 pages, 1 figure, 8 tables, The 29th Pacific Asia Conference on\n  Language, Information and Computation (PACLIC 29), The 27th Conference on\n  Computational Linguistics and Speech Analysis (ROCLING XXVII, Chinese\n  version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Complete Tang Poems (CTP) is the most important source to study Tang\npoems. We look into CTP with computational tools from specific linguistic\nperspectives, including distributional semantics and collocational analysis.\nFrom such quantitative viewpoints, we compare the usage of \"wind\" and \"moon\" in\nthe poems of Li Bai and Du Fu. Colors in poems function like sounds in movies,\nand play a crucial role in the imageries of poems. Thus, words for colors are\nstudied, and \"white\" is the main focus because it is the most frequent color in\nCTP. We also explore some cases of using colored words in antithesis pairs that\nwere central for fostering the imageries of the poems. CTP also contains useful\nhistorical information, and we extract person names in CTP to study the social\nnetworks of the Tang poets. Such information can then be integrated with the\nChina Biographical Database of Harvard University.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 00:21:40 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Liu", "Chao-Lin", ""], ["Wang", "Hongsu", ""], ["Cheng", "Wen-Huei", ""], ["Hsu", "Chu-Ting", ""], ["Chiu", "Wei-Yun", ""]]}, {"id": "1511.01974", "submitter": "Xu Chen", "authors": "Xu Chen, Han Zhang, Judith Gelernter", "title": "Multi-lingual Geoparsing based on Machine Translation", "comments": "7 pages, 4 figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our method for multi-lingual geoparsing uses monolingual tools and resources\nalong with machine translation and alignment to return location words in many\nlanguages. Not only does our method save the time and cost of developing\ngeoparsers for each language separately, but also it allows the possibility of\na wide range of language capabilities within a single interface. We evaluated\nour method in our LanguageBridge prototype on location named entities using\nnewswire, broadcast news and telephone conversations in English, Arabic and\nChinese data from the Linguistic Data Consortium (LDC). Our results for\ngeoparsing Chinese and Arabic text using our multi-lingual geoparsing method\nare comparable to our results for geoparsing English text with our English\ntools. Furthermore, experiments using our machine translation approach results\nin accuracy comparable to results from the same data that was translated\nmanually.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 03:07:20 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Chen", "Xu", ""], ["Zhang", "Han", ""], ["Gelernter", "Judith", ""]]}, {"id": "1511.02058", "submitter": "Hung-Hsuan Chen", "authors": "Hung-Hsuan Chen, Alexander G. Ororbia II, C. Lee Giles", "title": "ExpertSeer: a Keyphrase Based Expert Recommender for Digital Libraries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe ExpertSeer, a generic framework for expert recommendation based\non the contents of a digital library. Given a query term q, ExpertSeer\nrecommends experts of q by retrieving authors who published relevant papers\ndetermined by related keyphrases and the quality of papers. The system is based\non a simple yet effective keyphrase extractor and the Bayes' rule for expert\nrecommendation. ExpertSeer is domain independent and can be applied to\ndifferent disciplines and applications since the system is automated and not\ntailored to a specific discipline. Digital library providers can employ the\nsystem to enrich their services and organizations can discover experts of\ninterest within an organization. To demonstrate the power of ExpertSeer, we\napply the framework to build two expert recommender systems. The first, CSSeer,\nutilizes the CiteSeerX digital library to recommend experts primarily in\ncomputer science. The second, ChemSeer, uses publicly available documents from\nthe Royal Society of Chemistry (RSC) to recommend experts in chemistry. Using\none thousand computer science terms as benchmark queries, we compared the top-n\nexperts (n=3, 5, 10) returned by CSSeer to two other expert recommenders --\nMicrosoft Academic Search and ArnetMiner -- and a simulator that imitates the\nranking function of Google Scholar. Although CSSeer, Microsoft Academic Search,\nand ArnetMiner mostly return prestigious researchers who published several\npapers related to the query term, it was found that different expert\nrecommenders return moderately different recommendations. To further study\ntheir performance, we obtained a widely used benchmark dataset as the ground\ntruth for comparison. The results show that our system outperforms Microsoft\nAcademic Search and ArnetMiner in terms of Precision-at-k (P@k) for k=3, 5, 10.\nWe also conducted several case studies to validate the usefulness of our\nsystem.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 12:55:17 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Chen", "Hung-Hsuan", ""], ["Ororbia", "Alexander G.", "II"], ["Giles", "C. Lee", ""]]}, {"id": "1511.02290", "submitter": "Marcos Domingues", "authors": "Camila V. Sundermann and Marcos A. Domingues and Ricardo M. Marcacini\n  and Solange O. Rezende", "title": "Combining Privileged Information to Improve Context-Aware Recommender\n  Systems", "comments": "The 12th National Meeting on Artificial and Computational\n  Intelligence (ENIAC'15) collocated with the 4th Brazilian Conference on\n  Intelligent Systems (BRACIS'15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recommender system is an information filtering technology which can be used\nto predict preference ratings of items (products, services, movies, etc) and/or\nto output a ranking of items that are likely to be of interest to the user.\nContext-aware recommender systems (CARS) learn and predict the tastes and\npreferences of users by incorporating available contextual information in the\nrecommendation process. One of the major challenges in context-aware\nrecommender systems research is the lack of automatic methods to obtain\ncontextual information for these systems. Considering this scenario, in this\npaper, we propose to use contextual information from topic hierarchies of the\nitems (web pages) to improve the performance of context-aware recommender\nsystems. The topic hierarchies are constructed by an extension of the\nLUPI-based Incremental Hierarchical Clustering method that considers three\ntypes of information: traditional bag-of-words (technical information), and the\ncombination of named entities (privileged information I) with domain terms\n(privileged information II). We evaluated the contextual information in four\ncontext-aware recommender systems. Different weights were assigned to each type\nof information. The empirical results demonstrated that topic hierarchies with\nthe combination of the two kinds of privileged information can provide better\nrecommendations.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 03:04:02 GMT"}, {"version": "v2", "created": "Sun, 30 Dec 2018 23:55:45 GMT"}, {"version": "v3", "created": "Fri, 4 Jan 2019 20:35:02 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Sundermann", "Camila V.", ""], ["Domingues", "Marcos A.", ""], ["Marcacini", "Ricardo M.", ""], ["Rezende", "Solange O.", ""]]}, {"id": "1511.02433", "submitter": "Andr\\'e Rodrigues Valente", "authors": "Andr\\'e Valente Rodrigues, Al\\'ipio Jorge, In\\^es Dutra", "title": "Accelerating Recommender Systems using GPUs", "comments": null, "journal-ref": "SAC '15 Proceedings of the 30th Annual ACM Symposium on Applied\n  Computing Pages 879-884 ACM New York, NY, USA", "doi": "10.1145/2695664.2695850", "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe GPU implementations of the matrix recommender algorithms CCD++\nand ALS. We compare the processing time and predictive ability of the GPU\nimplementations with existing multi-core versions of the same algorithms.\nResults on the GPU are better than the results of the multi-core versions\n(maximum speedup of 14.8).\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 03:25:28 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Rodrigues", "Andr\u00e9 Valente", ""], ["Jorge", "Al\u00edpio", ""], ["Dutra", "In\u00eas", ""]]}, {"id": "1511.03012", "submitter": "Adrian Groza", "authors": "Adrian Groza and Lidia Corde", "title": "Information retrieval in folktales using natural language processing", "comments": "IEEE 11 International Conference on Intelligent Computer\n  Communication and Processing (ICCP2015), Cluj-Napoca, Romania, 3-5 September\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our aim is to extract information about literary characters in unstructured\ntexts. We employ natural language processing and reasoning on domain\nontologies. The first task is to identify the main characters and the parts of\nthe story where these characters are described or act. We illustrate the system\nin a scenario in the folktale domain. The system relies on a folktale ontology\nthat we have developed based on Propp's model for folktales morphology.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 08:13:49 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Groza", "Adrian", ""], ["Corde", "Lidia", ""]]}, {"id": "1511.03036", "submitter": "Hong Sun", "authors": "Hong Sun, Kristof Depraetere, Jos De Roo, Giovanni Mels, Boris De\n  Vloed, Marc Twagirumukiza, Dirk Colaert", "title": "Semantic processing of EHR data for clinical research", "comments": "Accepted for publication in Journal of Biomedical Informatics, 2015,\n  preprint version", "journal-ref": null, "doi": "10.1016/j.jbi.2015.10.009", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing need to semantically process and integrate clinical data\nfrom different sources for clinical research. This paper presents an approach\nto integrate EHRs from heterogeneous resources and generate integrated data in\ndifferent data formats or semantics to support various clinical research\napplications. The proposed approach builds semantic data virtualization layers\non top of data sources, which generate data in the requested semantics or\nformats on demand. This approach avoids upfront dumping to and synchronizing of\nthe data with various representations. Data from different EHR systems are\nfirst mapped to RDF data with source semantics, and then converted to\nrepresentations with harmonized domain semantics where domain ontologies and\nterminologies are used to improve reusability. It is also possible to further\nconvert data to application semantics and store the converted results in\nclinical research databases, e.g. i2b2, OMOP, to support different clinical\nresearch settings. Semantic conversions between different representations are\nexplicitly expressed using N3 rules and executed by an N3 Reasoner (EYE), which\ncan also generate proofs of the conversion processes. The solution presented in\nthis paper has been applied to real-world applications that process large scale\nEHR data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 09:47:14 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Sun", "Hong", ""], ["Depraetere", "Kristof", ""], ["De Roo", "Jos", ""], ["Mels", "Giovanni", ""], ["De Vloed", "Boris", ""], ["Twagirumukiza", "Marc", ""], ["Colaert", "Dirk", ""]]}, {"id": "1511.03055", "submitter": "Olivier Mor\\`ere", "authors": "Jie Lin, Olivier Mor\\`ere, Julie Petta, Vijay Chandrasekhar, Antoine\n  Veillard", "title": "Tiny Descriptors for Image Retrieval with Unsupervised Triplet Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical image retrieval pipeline starts with the comparison of global\ndescriptors from a large database to find a short list of candidate matches. A\ngood image descriptor is key to the retrieval pipeline and should reconcile two\ncontradictory requirements: providing recall rates as high as possible and\nbeing as compact as possible for fast matching. Following the recent successes\nof Deep Convolutional Neural Networks (DCNN) for large scale image\nclassification, descriptors extracted from DCNNs are increasingly used in place\nof the traditional hand crafted descriptors such as Fisher Vectors (FV) with\nbetter retrieval performances. Nevertheless, the dimensionality of a typical\nDCNN descriptor --extracted either from the visual feature pyramid or the\nfully-connected layers-- remains quite high at several thousands of scalar\nvalues. In this paper, we propose Unsupervised Triplet Hashing (UTH), a fully\nunsupervised method to compute extremely compact binary hashes --in the 32-256\nbits range-- from high-dimensional global descriptors. UTH consists of two\nsuccessive deep learning steps. First, Stacked Restricted Boltzmann Machines\n(SRBM), a type of unsupervised deep neural nets, are used to learn binary\nembedding functions able to bring the descriptor size down to the desired\nbitrate. SRBMs are typically able to ensure a very high compression rate at the\nexpense of loosing some desirable metric properties of the original DCNN\ndescriptor space. Then, triplet networks, a rank learning scheme based on\nweight sharing nets is used to fine-tune the binary embedding functions to\nretain as much as possible of the useful metric properties of the original\nspace. A thorough empirical evaluation conducted on multiple publicly available\ndataset using DCNN descriptors shows that our method is able to significantly\noutperform state-of-the-art unsupervised schemes in the target bit range.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 10:38:37 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Lin", "Jie", ""], ["Mor\u00e8re", "Olivier", ""], ["Petta", "Julie", ""], ["Chandrasekhar", "Vijay", ""], ["Veillard", "Antoine", ""]]}, {"id": "1511.03518", "submitter": "Ya-Hui An", "authors": "Ya-Hui An, Qiang Dong, Chong-Jing Sun, Da-Cheng Nie and Yan Fu", "title": "Diffusion-like recommendation with enhanced similarity of objects", "comments": null, "journal-ref": "Physica A: Statistical Mechanics and its Applications 461 (2016)\n  708-715", "doi": "10.1016/j.physa.2016.06.027", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In last decades, diversity and accuracy have been regarded as two important\nmeasures in evaluating a recommendation model. However, a clear concern is that\na model focusing excessively on one measure will put the other one at risk,\nthus it is not easy to greatly improve diversity and accuracy simultaneously.\nIn this paper, we propose to enhance the Resource-Allocation (RA) similarity in\nresource transfer equations of diffusion-like models, by giving a tunable\nexponent to the RA similarity, and traversing the value of the exponent to\nachieve the optimal recommendation results. In this way, we can increase the\nrecommendation scores (allocated resource) of many unpopular objects.\nExperiments on three benchmark data sets, MovieLens, Netflix, and RateYourMusic\nshow that the modified models can yield remarkable performance improvement\ncompared with the original ones.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 14:43:32 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 06:13:15 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["An", "Ya-Hui", ""], ["Dong", "Qiang", ""], ["Sun", "Chong-Jing", ""], ["Nie", "Da-Cheng", ""], ["Fu", "Yan", ""]]}, {"id": "1511.03546", "submitter": "Guorui Zhou", "authors": "Guorui Zhou, Guang Chen", "title": "Hierarchical Latent Semantic Mapping for Automated Topic Generation", "comments": "9 pages, 3 figures, Under Review as a conference at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of information sits in an unprecedented amount of text data. Managing\nallocation of these large scale text data is an important problem for many\nareas. Topic modeling performs well in this problem. The traditional generative\nmodels (PLSA,LDA) are the state-of-the-art approaches in topic modeling and\nmost recent research on topic generation has been focusing on improving or\nextending these models. However, results of traditional generative models are\nsensitive to the number of topics K, which must be specified manually. The\nproblem of generating topics from corpus resembles community detection in\nnetworks. Many effective algorithms can automatically detect communities from\nnetworks without a manually specified number of the communities. Inspired by\nthese algorithms, in this paper, we propose a novel method named Hierarchical\nLatent Semantic Mapping (HLSM), which automatically generates topics from\ncorpus. HLSM calculates the association between each pair of words in the\nlatent topic space, then constructs a unipartite network of words with this\nassociation and hierarchically generates topics from this network. We apply\nHLSM to several document collections and the experimental comparisons against\nseveral state-of-the-art approaches demonstrate the promising performance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 15:58:30 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 13:47:53 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 05:23:58 GMT"}, {"version": "v4", "created": "Thu, 26 Nov 2015 01:35:58 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Zhou", "Guorui", ""], ["Chen", "Guang", ""]]}, {"id": "1511.03759", "submitter": "Chuan Shi", "authors": "Chuan Shi, Jian Liu, Fuzhen Zhuang, Philip S. Yu, Bin Wu", "title": "Integrating Heterogeneous Information via Flexible Regularization\n  Framework for Recommendation", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there is a surge of social recommendation, which leverages social\nrelations among users to improve recommendation performance. However, in many\napplications, social relations are absent or very sparse. Meanwhile, the\nattribute information of users or items may be rich. It is a big challenge to\nexploit these attribute information for the improvement of recommendation\nperformance. In this paper, we organize objects and relations in recommendation\nsystem as a heterogeneous information network, and introduce meta path based\nsimilarity measure to evaluate the similarity of users or items. Furthermore, a\nmatrix factorization based dual regularization framework SimMF is proposed to\nflexibly integrate different types of information through adopting the\nsimilarity of users and items as regularization on latent factors of users and\nitems. Extensive experiments not only validate the effectiveness of SimMF but\nalso reveal some interesting findings. We find that attribute information of\nusers and items can significantly improve recommendation accuracy, and their\ncontribution seems more important than that of social relations. The\nexperiments also reveal that different regularization models have obviously\ndifferent impact on users and items.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 02:20:53 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Shi", "Chuan", ""], ["Liu", "Jian", ""], ["Zhuang", "Fuzhen", ""], ["Yu", "Philip S.", ""], ["Wu", "Bin", ""]]}, {"id": "1511.03780", "submitter": "Yong Zheng", "authors": "Yong Zheng", "title": "A User's Guide to CARSKit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-aware recommender systems extend traditional recommenders by adapting\ntheir suggestions to users' contextual situations. CARSKit is a Java-based\nopen-source library specifically designed for the context-aware recommendation,\nwhere the state-of-the-art context-aware recommendation algorithms have been\nimplemented. This report provides the basic user's guide to CARSKit, including\nhow to prepare the data set, how to configure the experimental settings, and\nhow to evaluate the algorithms, as well as interpreting the outputs. The\ninstructions in this guide are applicable for CARSKit v0.3.5 and above.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 05:24:35 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 23:46:26 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Zheng", "Yong", ""]]}, {"id": "1511.04674", "submitter": "Sherief Abdallah", "authors": "Sherief Abdallah", "title": "Using Text Mining To Analyze Real Estate Classifieds", "comments": "18 pages, 3 figures, and 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many brokers have adapted their operation to exploit the potential of the\nweb. Despite the importance of the real estate classifieds, there has been\nlittle work in analyzing such data. In this paper we propose a two-stage\nregression model that exploits the textual data in real estate classifieds. We\nshow how our model can be used to predict the price of a real estate\nclassified. We also show how our model can be used to highlight keywords that\naffect the price positively or negatively. To assess our contributions, we\nanalyze four real world data sets, which we gathered from three different\nproperty websites. The analysis shows that our model (which exploits textual\nfeatures) achieves significantly lower root mean squared error across the\ndifferent data sets and against variety of regression models.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 09:00:56 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Abdallah", "Sherief", ""]]}, {"id": "1511.04717", "submitter": "Fayrouz Soualah-Alila", "authors": "Fayrouz Soualah-Alila (L3I), Cyril Faucher (L3I), Fr\\'ed\\'eric\n  Bertrand (L3I), Micka\\\"el Coustaty (L3I), Antoine Doucet (L3I)", "title": "Applying Semantic Web Technologies for Improving the Visibility of\n  Tourism Data", "comments": "ESAIR: Exploiting Semantic Annotations in Information Retrieval, Oct\n  2015, Melbourne, Austria. 2015", "journal-ref": null, "doi": "10.1145/2810133.2810137", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tourism industry is an extremely information-intensive, complex and dynamic\nactivity. It can benefit from semantic Web technologies, due to the significant\nheterogeneity of information sources and the high volume of on-line data. The\nmanagement of semantically diverse annotated tourism data is facilitated by\nontologies that provide methods and standards, which allow flexibility and more\nintelligent access to on-line data. This paper provides a description of some\nof the early results of the Tourinflux project which aims to apply semantic Web\ntechnologies to support tourist actors in effectively finding and publishing\ninformation on the Web.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 15:55:21 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Soualah-Alila", "Fayrouz", "", "L3I"], ["Faucher", "Cyril", "", "L3I"], ["Bertrand", "Fr\u00e9d\u00e9ric", "", "L3I"], ["Coustaty", "Micka\u00ebl", "", "L3I"], ["Doucet", "Antoine", "", "L3I"]]}, {"id": "1511.04946", "submitter": "Haluk O. Bingol", "authors": "Metin Doslu and Haluk O. Bingol", "title": "Context Sensitive Article Ranking with Citation Context Analysis", "comments": null, "journal-ref": "Scientometrics (2016) 108:653-671", "doi": "10.1007/s11192-016-1982-6", "report-no": null, "categories": "cs.DL cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is hard to detect important articles in a specific context. Information\nretrieval techniques based on full text search can be inaccurate to identify\nmain topics and they are not able to provide an indication about the importance\nof the article. Generating a citation network is a good way to find most\npopular articles but this approach is not context aware.\n  The text around a citation mark is generally a good summary of the referred\narticle. So citation context analysis presents an opportunity to use the wisdom\nof crowd for detecting important articles in a context sensitive way. In this\nwork, we analyze citation contexts to rank articles properly for a given topic.\nThe model proposed uses citation contexts in order to create a directed and\nweighted citation network based on the target topic. We create a directed and\nweighted edge between two articles if citation context contains terms related\nwith the target topic. Then we apply common ranking algorithms in order to find\nimportant articles in this newly created network. We showed that this method\nsuccessfully detects a good subset of most prominent articles in a given topic.\nThe biggest contribution of this approach is that we are able to identify\nimportant articles for a given search term even though these articles do not\ncontain this search term. This technique can be used in other linked documents\nincluding web pages, legal documents, and patents.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 13:06:02 GMT"}, {"version": "v2", "created": "Tue, 26 Apr 2016 11:02:03 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Doslu", "Metin", ""], ["Bingol", "Haluk O.", ""]]}, {"id": "1511.05202", "submitter": "Sean Welleck", "authors": "Sean J. Welleck", "title": "Efficient AUC Optimization for Information Ranking Applications", "comments": "12 pages", "journal-ref": "ECIR 2016, LNCS 9626, pp.159-170, 2016", "doi": "10.1007/978-3-319-30671-1_12", "report-no": null, "categories": "cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adequate evaluation of an information retrieval system to estimate future\nperformance is a crucial task. Area under the ROC curve (AUC) is widely used to\nevaluate the generalization of a retrieval system. However, the objective\nfunction optimized in many retrieval systems is the error rate and not the AUC\nvalue. This paper provides an efficient and effective non-linear approach to\noptimize AUC using additive regression trees, with a special emphasis on the\nuse of multi-class AUC (MAUC) because multiple relevance levels are widely used\nin many ranking applications. Compared to a conventional linear approach, the\nperformance of the non-linear approach is comparable on binary-relevance\nbenchmark datasets and is better on multi-relevance benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 22:12:00 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 21:28:00 GMT"}, {"version": "v3", "created": "Sat, 23 Apr 2016 23:42:09 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Welleck", "Sean J.", ""]]}, {"id": "1511.05262", "submitter": "Ivens Portugal", "authors": "Ivens Portugal, Paulo Alencar, Donald Cowan", "title": "Requirements Engineering for General Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In requirements engineering for recommender systems, software engineers must\nidentify the data that drives the recommendations. This is a labor-intensive\ntask, which is error-prone and expensive. One possible solution to this problem\nis the adoption of automatic recommender system development approach based on a\ngeneral recommender framework. One step towards the creation of such a\nframework is to determine the type of data used in recommender systems. In this\npaper, a systematic review has been conducted to identify the type of user and\nrecommendation data items needed by a general recommender system. A user and\nitem model is proposed, and some considerations about algorithm specific\nparameters are explained. A further goal is to study the impact of the fields\nof big data and Internet of things on the development of recommender systems.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 03:09:59 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 14:38:15 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2016 15:36:29 GMT"}, {"version": "v4", "created": "Wed, 24 Feb 2016 18:58:31 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Portugal", "Ivens", ""], ["Alencar", "Paulo", ""], ["Cowan", "Donald", ""]]}, {"id": "1511.05263", "submitter": "Ivens Portugal", "authors": "Ivens Portugal, Paulo Alencar, Donald Cowan", "title": "The Use of Machine Learning Algorithms in Recommender Systems: A\n  Systematic Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems use algorithms to provide users with product or service\nrecommendations. Recently, these systems have been using machine learning\nalgorithms from the field of artificial intelligence. However, choosing a\nsuitable machine learning algorithm for a recommender system is difficult\nbecause of the number of algorithms described in the literature. Researchers\nand practitioners developing recommender systems are left with little\ninformation about the current approaches in algorithm usage. Moreover, the\ndevelopment of a recommender system using a machine learning algorithm often\nhas problems and open questions that must be evaluated, so software engineers\nknow where to focus research efforts. This paper presents a systematic review\nof the literature that analyzes the use of machine learning algorithms in\nrecommender systems and identifies research opportunities for software\nengineering research. The study concludes that Bayesian and decision tree\nalgorithms are widely used in recommender systems because of their relative\nsimplicity, and that requirement and design phases of recommender system\ndevelopment appear to offer opportunities for further research.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 03:14:46 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2015 14:38:22 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2016 15:36:32 GMT"}, {"version": "v4", "created": "Wed, 24 Feb 2016 18:58:32 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Portugal", "Ivens", ""], ["Alencar", "Paulo", ""], ["Cowan", "Donald", ""]]}, {"id": "1511.05266", "submitter": "Rana Forsati Dr.", "authors": "Iman Barjasteh, Rana Forsati, Abdol-Hossein Esfahanian, Hayder Radha", "title": "Semi-supervised Collaborative Ranking with Push at Top", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing collaborative ranking based recommender systems tend to perform best\nwhen there is enough observed ratings for each user and the observation is made\ncompletely at random. Under this setting recommender systems can properly\nsuggest a list of recommendations according to the user interests. However,\nwhen the observed ratings are extremely sparse (e.g. in the case of cold-start\nusers where no rating data is available), and are not sampled uniformly at\nrandom, existing ranking methods fail to effectively leverage side information\nto transduct the knowledge from existing ratings to unobserved ones. We propose\na semi-supervised collaborative ranking model, dubbed \\texttt{S$^2$COR}, to\nimprove the quality of cold-start recommendation. \\texttt{S$^2$COR} mitigates\nthe sparsity issue by leveraging side information about both observed and\nmissing ratings by collaboratively learning the ranking model. This enables it\nto deal with the case of missing data not at random, but to also effectively\nincorporate the available side information in transduction. We experimentally\nevaluated our proposed algorithm on a number of challenging real-world datasets\nand compared against state-of-the-art models for cold-start recommendation. We\nreport significantly higher quality recommendations with our algorithm compared\nto the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 04:02:26 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Barjasteh", "Iman", ""], ["Forsati", "Rana", ""], ["Esfahanian", "Abdol-Hossein", ""], ["Radha", "Hayder", ""]]}, {"id": "1511.05520", "submitter": "Tian Wang", "authors": "Peter Li and Jiyuan Qian and Tian Wang", "title": "Automatic Instrument Recognition in Polyphonic Music Using Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional methods to tackle many music information retrieval tasks\ntypically follow a two-step architecture: feature engineering followed by a\nsimple learning algorithm. In these \"shallow\" architectures, feature\nengineering and learning are typically disjoint and unrelated. Additionally,\nfeature engineering is difficult, and typically depends on extensive domain\nexpertise.\n  In this paper, we present an application of convolutional neural networks for\nthe task of automatic musical instrument identification. In this model, feature\nextraction and learning algorithms are trained together in an end-to-end\nfashion. We show that a convolutional neural network trained on raw audio can\nachieve performance surpassing traditional methods that rely on hand-crafted\nfeatures.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 19:43:53 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Li", "Peter", ""], ["Qian", "Jiyuan", ""], ["Wang", "Tian", ""]]}, {"id": "1511.05643", "submitter": "Md Kamrul Hasan", "authors": "Md Kamrul Hasan, Christopher J. Pal", "title": "A New Smooth Approximation to the Zero One Loss with a Probabilistic\n  Interpretation", "comments": "32 pages, 7 figures, 15 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a new form of smooth approximation to the zero one loss in which\nlearning is performed using a reformulation of the widely used logistic\nfunction. Our approach is based on using the posterior mean of a novel\ngeneralized Beta-Bernoulli formulation. This leads to a generalized logistic\nfunction that approximates the zero one loss, but retains a probabilistic\nformulation conferring a number of useful properties. The approach is easily\ngeneralized to kernel logistic regression and easily integrated into methods\nfor structured prediction. We present experiments in which we learn such models\nusing an optimization method consisting of a combination of gradient descent\nand coordinate descent using localized grid search so as to escape from local\nminima. Our experiments indicate that optimization quality is improved when\nlearning meta-parameters are themselves optimized using a validation set. Our\nexperiments show improved performance relative to widely used logistic and\nhinge loss methods on a wide variety of problems ranging from standard UC\nIrvine and libSVM evaluation datasets to product review predictions and a\nvisual information extraction task. We observe that the approach: 1) is more\nrobust to outliers compared to the logistic and hinge losses; 2) outperforms\ncomparable logistic and max margin models on larger scale benchmark problems;\n3) when combined with Gaussian- Laplacian mixture prior on parameters the\nkernelized version of our formulation yields sparser solutions than Support\nVector Machine classifiers; and 4) when integrated into a probabilistic\nstructured prediction technique our approach provides more accurate\nprobabilities yielding improved inference and increasing information extraction\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 02:31:16 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Hasan", "Md Kamrul", ""], ["Pal", "Christopher J.", ""]]}, {"id": "1511.05659", "submitter": "Aiwen Jiang", "authors": "Aiwen Jiang and Hanxi Li and Yi Li and Mingwen Wang", "title": "Learning Discriminative Representations for Semantic Cross Media\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous gap among different modalities emerges as one of the critical\nissues in modern AI problems. Unlike traditional uni-modal cases, where raw\nfeatures are extracted and directly measured, the heterogeneous nature of cross\nmodal tasks requires the intrinsic semantic representation to be compared in a\nunified framework. This paper studies the learning of different representations\nthat can be retrieved across different modality contents. A novel approach for\nmining cross-modal representations is proposed by incorporating explicit linear\nsemantic projecting in Hilbert space. The insight is that the discriminative\nstructures of different modality data can be linearly represented in\nappropriate high dimension Hilbert spaces, where linear operations can be used\nto approximate nonlinear decisions in the original spaces. As a result, an\nefficient linear semantic down mapping is jointly learned for multimodal data,\nleading to a common space where they can be compared. The mechanism of \"feature\nup-lifting and down-projecting\" works seamlessly as a whole, which accomplishes\ncrossmodal retrieval tasks very well. The proposed method, named as shared\ndiscriminative semantic representation learning (\\textbf{SDSRL}), is tested on\ntwo public multimodal dataset for both within- and inter- modal retrieval. The\nexperiments demonstrate that it outperforms several state-of-the-art methods in\nmost scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 05:20:32 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Jiang", "Aiwen", ""], ["Li", "Hanxi", ""], ["Li", "Yi", ""], ["Wang", "Mingwen", ""]]}, {"id": "1511.05798", "submitter": "Dirk Lewandowski", "authors": "Dirk Lewandowski", "title": "Problems with the use of Web search engines to find results in foreign\n  languages", "comments": "Research paper, World Wide Web, search engines, advanced search\n  options, language restriction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose - To test the ability of major search engines, Google, Yahoo, MSN,\nand Ask, to distinguish between German and English-language documents\n  Design/methodology/approach - 50 queries, using words common in German and in\nEnglish, were posed to the engines. The advanced search option of language\nrestriction was used, once in German and once in English. The first 20 results\nper engine in each language were investigated.\n  Findings - While none of the search engines faces problems in providing\nresults in the language of the interface that is used, both Google and MSN face\nproblems when the results are restricted to a foreign language.\n  Research limitations/implications - Search engines were only tested in German\nand in English. We have only anecdotal evidence that the problems are the same\nwith other languages.\n  Practical implications - Searchers should not use the language restriction in\nGoogle and MSN when searching for foreign-language documents. Instead,\nsearchers should use Yahoo or Ask. If searching for foreign language documents\nin Google or MSN, the interface in the target language/country should be used.\n  Value of paper - Demonstrates a problem with search engines that has not been\npreviously investigated.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 14:26:58 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Lewandowski", "Dirk", ""]]}, {"id": "1511.05800", "submitter": "Dirk Lewandowski", "authors": "Dirk Lewandowski", "title": "The Retrieval Effectiveness of Web Search Engines: Considering Results\n  Descriptions", "comments": "Research paper, Word Wide Web, search engines, retrieval\n  effectiveness, results descriptions, retrieval measures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To compare five major Web search engines (Google, Yahoo, MSN,\nAsk.com, and Seekport) for their retrieval effectiveness, taking into account\nnot only the results but also the results descriptions.\n  Design/Methodology/Approach: The study uses real-life queries. Results are\nmade anonymous and are randomised. Results are judged by the persons posing the\noriginal queries.\n  Findings: The two major search engines, Google and Yahoo, perform best, and\nthere are no significant differences between them. Google delivers\nsignificantly more relevant result descriptions than any other search engine.\nThis could be one reason for users perceiving this engine as superior.\n  Research Limitations: The study is based on a user model where the user takes\ninto account a certain amount of results rather systematically. This may not be\nthe case in real life.\n  Practical Implications: Implies that search engines should focus on relevant\ndescriptions. Searchers are advised to use other search engines in addition to\nGoogle.\n  Originality/Value: This is the first major study comparing results and\ndescriptions systematically and proposes new retrieval measures to take into\naccount results descriptions\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 14:29:50 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Lewandowski", "Dirk", ""]]}, {"id": "1511.05802", "submitter": "Dirk Lewandowski", "authors": "Nadine Hoechstoetter, Dirk Lewandowski", "title": "What Users See - Structures in Search Engine Results Pages", "comments": "Search engines, evaluation, search engine results pages, search\n  shortcuts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the composition of search engine results pages. We\ndefine what elements the most popular web search engines use on their results\npages (e.g., organic results, advertisements, shortcuts) and to which degree\nthey are used for popular vs. rare queries. Therefore, we send 500 queries of\nboth types to the major search engines Google, Yahoo, Live.com and Ask. We\ncount how often the different elements are used by the individual engines. In\ntotal, our study is based on 42,758 elements. Findings include that search\nengines use quite different approaches to results pages composition and\ntherefore, the user gets to see quite different results sets depending on the\nsearch engine and search query used. Organic results still play the major role\nin the results pages, but different shortcuts are of some importance, too.\nRegarding the frequency of certain host within the results sets, we find that\nall search engines show Wikipedia results quite often, while other hosts shown\ndepend on the search engine used. Both Google and Yahoo prefer results from\ntheir own offerings (such as YouTube or Yahoo Answers). Since we used the .com\ninterfaces of the search engines, results may not be valid for other\ncountry-specific interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 14:33:27 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Hoechstoetter", "Nadine", ""], ["Lewandowski", "Dirk", ""]]}, {"id": "1511.05806", "submitter": "Dirk Lewandowski", "authors": "Dirk Lewandowski", "title": "Ranking library materials", "comments": "Conceptual paper, OPAC, search engines, ranking, results presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: This paper discusses ranking factors suitable for library materials\nand shows that ranking in general is a complex process and that ranking for\nlibrary materials requires a variety of techniques.\nDesign/methodology/approach: The relevant literature is reviewed to provide a\nsystematic overview of suitable ranking factors. The discussion is based on an\noverview of ranking factors used in Web search engines. Findings: While there\nare a wide variety of ranking factors applicable to library materials, todays\nlibrary systems use only some of them. When designing a ranking component for\nthe library catalogue, an individual weighting of applicable factors is\nnecessary. Research limitations/applications: While this article discusses\ndifferent factors, no particular ranking formula is given. However, this\narticle presents the argument that such a formula must always be individual to\na certain use case. Practical implications: The factors presented can be\nconsidered when designing a ranking component for a librarys search system or\nwhen discussing such a project with an ILS vendor. Originality/value: This\npaper is original in that it is the first to systematically discuss ranking of\nlibrary materials based on the main factors used by Web search engines.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 14:36:20 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Lewandowski", "Dirk", ""]]}, {"id": "1511.05810", "submitter": "Dirk Lewandowski", "authors": "Dirk Lewandowski", "title": "The Influence of Commercial Intent of Search Results on Their Perceived\n  Relevance", "comments": "Measurement, Performance, Experimentation, Worldwide Web, search\n  engines, commerciality, evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We carried out a retrieval effectiveness test on the three major web search\nengines (i.e., Google, Microsoft and Yahoo). In addition to relevance\njudgments, we classified the results according to their commercial intent and\nwhether or not they carried any advertising. We found that all search engines\nprovide a large number of results with a commercial intent. Google provides\nsignificantly more commercial results than the other search engines do.\nHowever, the commercial intent of a result did not influence jurors in their\nrelevance judgments.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 14:43:22 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Lewandowski", "Dirk", ""]]}, {"id": "1511.05812", "submitter": "Dirk Lewandowski", "authors": "Dirk Lewandowski", "title": "The retrieval effectiveness of search engines on navigational queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose - To test major Web search engines on their performance on\nnavigational queries, i.e. searches for homepages. Design/methodology/approach\n- 100 real user queries are posed to six search engines (Google, Yahoo, MSN,\nAsk, Seekport, and Exalead). Users described the desired pages, and the results\nposition of these is recorded. Measured success N and mean reciprocal rank are\ncalculated. Findings - Performance of the major search engines Google, Yahoo,\nand MSN is best, with around 90 percent of queries answered correctly. Ask and\nExalead perform worse but receive good scores as well. Research\nlimitations/implications - All queries were in German, and the German-language\ninterfaces of the search engines were used. Therefore, the results are only\nvalid for German queries. Practical implications - When designing a search\nengine to compete with the major search engines, care should be taken on the\nperformance on navigational queries. Users can be influenced easily in their\nquality ratings of search engines based on this performance. Originality/value\n- This study systematically compares the major search engines on navigational\nqueries and compares the findings with studies on the retrieval effectiveness\nof the engines on informational queries. Paper type - research paper\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 14:45:37 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Lewandowski", "Dirk", ""]]}, {"id": "1511.05817", "submitter": "Dirk Lewandowski", "authors": "Dirk Lewandowski", "title": "A Framework for Evaluating the Retrieval Effectiveness of Search Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter presents a theoretical framework for evaluating next generation\nsearch engines. We focus on search engines whose results presentation is\nenriched with additional information and does not merely present the usual list\nof 10 blue links, that is, of ten links to results, accompanied by a short\ndescription. While Web search is used as an example here, the framework can\neasily be applied to search engines in any other area. The framework not only\naddresses the results presentation, but also takes into account an extension of\nthe general design of retrieval effectiveness tests. The chapter examines the\nways in which this design might influence the results of such studies and how a\nreliable test is best designed.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 14:54:10 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Lewandowski", "Dirk", ""]]}, {"id": "1511.05819", "submitter": "Dirk Lewandowski", "authors": "Georg Singer, Pille Pruulmann-Vengerfeldt, Ulrich Norbisrath, Dirk\n  Lewandowski", "title": "The relationship between internet user type and user performance when\n  carrying out simple vs. complex search tasks", "comments": "http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/3960/3245", "journal-ref": null, "doi": "10.5210/fm.v17i6.3960", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely known that people become better at an activity if they perform\nthis activity long and often. Yet, the question is whether being active in\nrelated areas like communicating online, writing blog articles or commenting on\ncommunity forums have an impact on a persons ability to perform Web searches,\nis still unanswered. Web searching has become a key task conducted online; in\nthis paper we present our findings on whether the user type, which categorises\na persons online activities, has an impact on her or his search capabilities.\nWe show (1) the characteristics of different user types when carrying out\nsimple search tasks; (2) their characteristics when carrying out complex search\ntasks; and, (3) the significantly different user type characteristics between\nsimple and complex search tasks. The results are based on an experiment with 56\nordinary Web users in a laboratory environment. The Search-Logger study\nframework was used to analyze and measure user behavior when carrying out a set\nof 12 predefined search tasks. Our findings include the fact that depending on\ntask type (simple or complex) significant differences can be observed between\nusers of different types.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 14:57:04 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Singer", "Georg", ""], ["Pruulmann-Vengerfeldt", "Pille", ""], ["Norbisrath", "Ulrich", ""], ["Lewandowski", "Dirk", ""]]}, {"id": "1511.06033", "submitter": "Athanasios N. Nikolakopoulos", "authors": "Athanasios N. Nikolakopoulos, Vassilis Kalantzis, Efstratios\n  Gallopoulos and John D. Garofalakis", "title": "EigenRec: Generalizing PureSVD for Effective and Efficient Top-N\n  Recommendations", "comments": "23 pages. Journal version of the conference paper \"Factored Proximity\n  Models for Top-N Recommendation\"", "journal-ref": null, "doi": "10.1007/s10115-018-1197-7", "report-no": null, "categories": "cs.IR cs.DB cs.DC cs.NA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce EigenRec; a versatile and efficient Latent-Factor framework for\nTop-N Recommendations that includes the well-known PureSVD algorithm as a\nspecial case. EigenRec builds a low dimensional model of an inter-item\nproximity matrix that combines a similarity component, with a scaling operator,\ndesigned to control the influence of the prior item popularity on the final\nmodel. Seeing PureSVD within our framework provides intuition about its inner\nworkings, exposes its inherent limitations, and also, paves the path towards\npainlessly improving its recommendation performance. A comprehensive set of\nexperiments on the MovieLens and the Yahoo datasets based on widely applied\nperformance metrics, indicate that EigenRec outperforms several\nstate-of-the-art algorithms, in terms of Standard and Long-Tail recommendation\naccuracy, exhibiting low susceptibility to sparsity, even in its most extreme\nmanifestations -- the Cold-Start problems. At the same time EigenRec has an\nattractive computational profile and it can apply readily in large-scale\nrecommendation settings.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 00:34:51 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 16:00:46 GMT"}, {"version": "v3", "created": "Tue, 5 Dec 2017 12:56:14 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Nikolakopoulos", "Athanasios N.", ""], ["Kalantzis", "Vassilis", ""], ["Gallopoulos", "Efstratios", ""], ["Garofalakis", "John D.", ""]]}, {"id": "1511.06252", "submitter": "Matus Medo", "authors": "Fei Yu, An Zeng, Sebastien Gillard, Matus Medo", "title": "Network-based recommendation algorithms: A review", "comments": "review article; 16 pages, 4 figures, 4 tables", "journal-ref": "Physica A 452, 192 (2016)", "doi": "10.1016/j.physa.2016.02.021", "report-no": null, "categories": "cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are a vital tool that helps us to overcome the\ninformation overload problem. They are being used by most e-commerce web sites\nand attract the interest of a broad scientific community. A recommender system\nuses data on users' past preferences to choose new items that might be\nappreciated by a given individual user. While many approaches to recommendation\nexist, the approach based on a network representation of the input data has\ngained considerable attention in the past. We review here a broad range of\nnetwork-based recommendation algorithms and for the first time compare their\nperformance on three distinct real datasets. We present recommendation topics\nthat go beyond the mere question of which algorithm to use - such as the\npossible influence of recommendation on the evolution of systems that use it -\nand finally discuss open research directions and challenges.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 16:51:44 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Yu", "Fei", ""], ["Zeng", "An", ""], ["Gillard", "Sebastien", ""], ["Medo", "Matus", ""]]}, {"id": "1511.06798", "submitter": "Luke Miratrix", "authors": "Luke Miratrix and Robin Ackerman", "title": "Conducting sparse feature selection on arbitrarily long phrases in text\n  corpora with a focus on interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for topic-specific summarization of large text\ncorpora, and illustrate how it can be used for analysis in two quite different\ncontexts: an OSHA database of fatality and catastrophe reports (to facilitate\nsurveillance for patterns in circumstances leading to injury or death) and\nlegal decisions on workers' compensation claims (to explore relevant case law).\nOur summarization framework, built on sparse classification methods, is a\ncompromise between simple word frequency based methods currently in wide use,\nand more heavyweight, model-intensive methods such as Latent Dirichlet\nAllocation (LDA). For a particular topic of interest (e.g., mental health\ndisability, or chemical reactions), we regress a labeling of documents onto the\nhigh-dimensional counts of all the other words and phrases in the documents.\nThe resulting small set of phrases found as predictive are then harvested as\nthe summary. Using a branch-and-bound approach, this method can be extended to\nallow for phrases of arbitrary length, which allows for potentially rich\nsummarization. We discuss how focus on the purpose of the summaries can inform\nchoices of regularization parameters and model constraints. We evaluate this\ntool by comparing computational time and summary statistics of the resulting\nword lists to three other methods in the literature. We also present a new R\npackage, textreg. Overall, we argue that sparse methods have much to offer text\nanalysis, and is a branch of research that should be considered further in this\ncontext.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 23:39:48 GMT"}, {"version": "v2", "created": "Sat, 23 Jul 2016 00:18:19 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Miratrix", "Luke", ""], ["Ackerman", "Robin", ""]]}, {"id": "1511.06820", "submitter": "Yike Liu", "authors": "Yike Liu, Neil Shah, Danai Koutra", "title": "An Empirical Comparison of the Summarization Power of Graph Clustering\n  Methods", "comments": "NIPS workshop: Networks in the Social and Information Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do graph clustering techniques compare with respect to their\nsummarization power? How well can they summarize a million-node graph with a\nfew representative structures? Graph clustering or community detection\nalgorithms can summarize a graph in terms of coherent and tightly connected\nclusters. In this paper, we compare and contrast different techniques: METIS,\nLouvain, spectral clustering, SlashBurn and KCBC, our proposed k-core-based\nclustering method. Unlike prior work that focuses on various measures of\ncluster quality, we use vocabulary structures that often appear in real graphs\nand the Minimum Description Length (MDL) principle to obtain a graph summary\nper clustering method. Our main contributions are: (i) Formulation: We propose\na summarization-based evaluation of clustering methods. Our method,\nVOG-OVERLAP, concisely summarizes graphs in terms of their important structures\nwhich lead to small edge overlap, and large node/edge coverage; (ii) Algorithm:\nwe introduce KCBC, a graph decomposition technique, in the heart of which lies\nthe k-core algorithm (iii) Evaluation: We compare the summarization power of\nfive clustering techniques on large real graphs, and analyze their compression\nperformance, summary statistics and runtimes.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 02:57:41 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Liu", "Yike", ""], ["Shah", "Neil", ""], ["Koutra", "Danai", ""]]}, {"id": "1511.06833", "submitter": "Thenmalar S", "authors": "S. Thenmalar, J. Balaji, and T.V. Geetha", "title": "Semi-supervised Bootstrapping approach for Named Entity Recognition", "comments": "13 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of Named Entity Recognition (NER) is to identify references of named\nentities in unstructured documents, and to classify them into pre-defined\nsemantic categories. NER often aids from added background knowledge in the form\nof gazetteers. However using such a collection does not deal with name variants\nand cannot resolve ambiguities associated in identifying the entities in\ncontext and associating them with predefined categories. We present a\nsemi-supervised NER approach that starts with identifying named entities with a\nsmall set of training data. Using the identified named entities, the word and\nthe context features are used to define the pattern. This pattern of each named\nentity category is used as a seed pattern to identify the named entities in the\ntest set. Pattern scoring and tuple value score enables the generation of the\nnew patterns to identify the named entity categories. We have evaluated the\nproposed system for English language with the dataset of tagged (IEER) and\nuntagged (CoNLL 2003) named entity corpus and for Tamil language with the\ndocuments from the FIRE corpus and yield an average f-measure of 75% for both\nthe languages.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 04:11:44 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Thenmalar", "S.", ""], ["Balaji", "J.", ""], ["Geetha", "T. V.", ""]]}, {"id": "1511.06939", "submitter": "Bal\\'azs Hidasi", "authors": "Bal\\'azs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, Domonkos\n  Tikk", "title": "Session-based Recommendations with Recurrent Neural Networks", "comments": "Camera ready version (17th February, 2016) Affiliation update (29th\n  March, 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply recurrent neural networks (RNN) on a new domain, namely recommender\nsystems. Real-life recommender systems often face the problem of having to base\nrecommendations only on short session-based data (e.g. a small sportsware\nwebsite) instead of long user histories (as in the case of Netflix). In this\nsituation the frequently praised matrix factorization approaches are not\naccurate. This problem is usually overcome in practice by resorting to\nitem-to-item recommendations, i.e. recommending similar items. We argue that by\nmodeling the whole session, more accurate recommendations can be provided. We\ntherefore propose an RNN-based approach for session-based recommendations. Our\napproach also considers practical aspects of the task and introduces several\nmodifications to classic RNNs such as a ranking loss function that make it more\nviable for this specific problem. Experimental results on two data-sets show\nmarked improvements over widely used approaches.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 23:42:59 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 21:13:50 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 16:41:37 GMT"}, {"version": "v4", "created": "Tue, 29 Mar 2016 14:52:58 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Hidasi", "Bal\u00e1zs", ""], ["Karatzoglou", "Alexandros", ""], ["Baltrunas", "Linas", ""], ["Tikk", "Domonkos", ""]]}, {"id": "1511.06975", "submitter": "Shini Renjith", "authors": "Shini Renjith", "title": "An Integrated Framework to Recommend Personalized Retention Actions to\n  Control B2C E-Commerce Customer Churn", "comments": "6 pages, 7 figures, Published with International Journal of\n  Engineering Trends and Technology (IJETT)", "journal-ref": "IJETT, V27(3),152-157 September 2015. ISSN:2231-5381.\n  www.ijettjournal.org", "doi": "10.14445/22315381/IJETT-V27P227", "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the level of competition prevailing in Business-to-Consumer (B2C)\nE-Commerce domain and the huge investments required to attract new customers,\nfirms are now giving more focus to reduce their customer churn rate. Churn rate\nis the ratio of customers who part away with the firm in a specific time\nperiod. One of the best mechanism to retain current customers is to identify\nany potential churn and respond fast to prevent it. Detecting early signs of a\npotential churn, recognizing what the customer is looking for by the movement\nand automating personalized win back campaigns are essential to sustain\nbusiness in this era of competition. E-Commerce firms normally possess large\nvolume of data pertaining to their existing customers like transaction history,\nsearch history, periodicity of purchases, etc. Data mining techniques can be\napplied to analyse customer behaviour and to predict the potential customer\nattrition so that special marketing strategies can be adopted to retain them.\nThis paper proposes an integrated model that can predict customer churn and\nalso recommend personalized win back actions.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 07:37:53 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Renjith", "Shini", ""]]}, {"id": "1511.07004", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, George Fazekas, Mark Sandler", "title": "Understanding Music Playlists", "comments": "International Conference on Machine Learning (ICML) 2015, Machine\n  Learning for Music Discovery Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As music streaming services dominate the music industry, the playlist is\nbecoming an increasingly crucial element of music consumption. Con- sequently,\nthe music recommendation problem is often casted as a playlist generation prob-\nlem. Better understanding of the playlist is there- fore necessary for\ndeveloping better playlist gen- eration algorithms. In this work, we analyse\ntwo playlist datasets to investigate some com- monly assumed hypotheses about\nplaylists. Our findings indicate that deeper understanding of playlists is\nneeded to provide better prior infor- mation and improve machine learning\nalgorithms in the design of recommendation systems.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 12:33:08 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "George", ""], ["Sandler", "Mark", ""]]}, {"id": "1511.07148", "submitter": "Naama Kraus", "authors": "Naama Kraus, David Carmel, Idit Keidar, Meni Orenbach", "title": "NearBucket-LSH: Efficient Similarity Search in P2P Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present NearBucket-LSH, an effective algorithm for similarity search in\nlarge-scale distributed online social networks organized as peer-to-peer\noverlays. As communication is a dominant consideration in distributed systems,\nwe focus on minimizing the network cost while guaranteeing good search quality.\nOur algorithm is based on Locality Sensitive Hashing (LSH), which limits the\nsearch to collections of objects, called buckets, that have a high probability\nto be similar to the query. More specifically, NearBucket-LSH employs an LSH\nextension that searches in near buckets, and improves search quality but also\nsignificantly increases the network cost. We decrease the network cost by\nconsidering the internals of both LSH and the P2P overlay, and harnessing their\nproperties to our needs. We show that our NearBucket-LSH increases search\nquality for a given network cost compared to previous art. In many cases, the\nsearch quality increases by more than 50%.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 09:33:24 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Kraus", "Naama", ""], ["Carmel", "David", ""], ["Keidar", "Idit", ""], ["Orenbach", "Meni", ""]]}, {"id": "1511.07237", "submitter": "Thomas Demeester", "authors": "Thomas Demeester, Robin Aly, Djoerd Hiemstra, Dong Nguyen, Chris\n  Develder", "title": "Predicting Relevance based on Assessor Disagreement: Analysis and\n  Practical Applications for Search Evaluation", "comments": "Accepted for publication in Springer Information Retrieval Journal,\n  special issue on Information Retrieval Evaluation using Test Collections", "journal-ref": null, "doi": "10.1007/s10791-015-9275-x", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of search engines relies on assessments of search results for\nselected test queries, from which we would ideally like to draw conclusions in\nterms of relevance of the results for general (e.g., future, unknown) users. In\npractice however, most evaluation scenarios only allow us to conclusively\ndetermine the relevance towards the particular assessor that provided the\njudgments. A factor that cannot be ignored when extending conclusions made from\nassessors towards users, is the possible disagreement on relevance, assuming\nthat a single gold truth label does not exist. This paper presents and analyzes\nthe Predicted Relevance Model (PRM), which allows predicting a particular\nresult's relevance for a random user, based on an observed assessment and\nknowledge on the average disagreement between assessors. With the PRM, existing\nevaluation metrics designed to measure binary assessor relevance, can be\ntransformed into more robust and effectively graded measures that evaluate\nrelevance towards a random user. It also leads to a principled way of\nquantifying multiple graded or categorical relevance levels for use as gains in\nestablished graded relevance measures, such as normalized discounted cumulative\ngain (nDCG), which nowadays often use heuristic and data-independent gain\nvalues. Given a set of test topics with graded relevance judgments, the PRM\nallows evaluating systems on different scenarios, such as their capability of\nretrieving top results, or how well they are able to filter out non-relevant\nones. Its use in actual evaluation scenarios is illustrated on several\ninformation retrieval test collections.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 14:31:43 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Demeester", "Thomas", ""], ["Aly", "Robin", ""], ["Hiemstra", "Djoerd", ""], ["Nguyen", "Dong", ""], ["Develder", "Chris", ""]]}, {"id": "1511.07527", "submitter": "Thijs Laarhoven", "authors": "Thijs Laarhoven", "title": "Tradeoffs for nearest neighbors on the sphere", "comments": "16 pages, 1 table, 2 figures. Mostly subsumed by arXiv:1608.03580\n  [cs.DS] (along with arXiv:1605.02701 [cs.DS])", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider tradeoffs between the query and update complexities for the\n(approximate) nearest neighbor problem on the sphere, extending the recent\nspherical filters to sparse regimes and generalizing the scheme and analysis to\naccount for different tradeoffs. In a nutshell, for the sparse regime the\ntradeoff between the query complexity $n^{\\rho_q}$ and update complexity\n$n^{\\rho_u}$ for data sets of size $n$ is given by the following equation in\nterms of the approximation factor $c$ and the exponents $\\rho_q$ and $\\rho_u$:\n$$c^2\\sqrt{\\rho_q}+(c^2-1)\\sqrt{\\rho_u}=\\sqrt{2c^2-1}.$$\n  For small $c=1+\\epsilon$, minimizing the time for updates leads to a linear\nspace complexity at the cost of a query time complexity $n^{1-4\\epsilon^2}$.\nBalancing the query and update costs leads to optimal complexities\n$n^{1/(2c^2-1)}$, matching bounds from [Andoni-Razenshteyn, 2015] and [Dubiner,\nIEEE-TIT'10] and matching the asymptotic complexities of [Andoni-Razenshteyn,\nSTOC'15] and [Andoni-Indyk-Laarhoven-Razenshteyn-Schmidt, NIPS'15]. A\nsubpolynomial query time complexity $n^{o(1)}$ can be achieved at the cost of a\nspace complexity of the order $n^{1/(4\\epsilon^2)}$, matching the bound\n$n^{\\Omega(1/\\epsilon^2)}$ of [Andoni-Indyk-Patrascu, FOCS'06] and\n[Panigrahy-Talwar-Wieder, FOCS'10] and improving upon results of\n[Indyk-Motwani, STOC'98] and [Kushilevitz-Ostrovsky-Rabani, STOC'98].\n  For large $c$, minimizing the update complexity results in a query complexity\nof $n^{2/c^2+O(1/c^4)}$, improving upon the related exponent for large $c$ of\n[Kapralov, PODS'15] by a factor $2$, and matching the bound $n^{\\Omega(1/c^2)}$\nof [Panigrahy-Talwar-Wieder, FOCS'08]. Balancing the costs leads to optimal\ncomplexities $n^{1/(2c^2-1)}$, while a minimum query time complexity can be\nachieved with update complexity $n^{2/c^2+O(1/c^4)}$, improving upon the\nprevious best exponents of Kapralov by a factor $2$.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 01:07:08 GMT"}, {"version": "v2", "created": "Sat, 10 Sep 2016 00:21:29 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Laarhoven", "Thijs", ""]]}, {"id": "1511.07643", "submitter": "Vincenzo Nicosia", "authors": "Valerio Ciotti, Moreno Bonaventura, Vincenzo Nicosia, Pietro\n  Panzarasa, Vito Latora", "title": "Homophily and missing links in citation networks", "comments": "11 pages, 4 figures, 1 table", "journal-ref": "EPJ Data Science 5:7 doi:10.1140/epjds/s13688-016-0068-2 (2016)", "doi": "10.1140/epjds/s13688-016-0068-2", "report-no": null, "categories": "physics.soc-ph cs.DL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citation networks have been widely used to study the evolution of science\nthrough the lenses of the underlying patterns of knowledge flows among academic\npapers, authors, research sub-fields, and scientific journals. Here we focus on\ncitation networks to cast light on the salience of homophily, namely the\nprinciple that similarity breeds connection, for knowledge transfer between\npapers. To this end, we assess the degree to which citations tend to occur\nbetween papers that are concerned with seemingly related topics or research\nproblems. Drawing on a large data set of articles published in the journals of\nthe American Physical Society between 1893 and 2009, we propose a novel method\nfor measuring the similarity between articles through the statistical\nvalidation of the overlap between their bibliographies. Results suggest that\nthe probability of a citation made by one article to another is indeed an\nincreasing function of the similarity between the two articles. Our study also\nenables us to uncover missing citations between pairs of highly related\narticles, and may thus help identify barriers to effective knowledge flows. By\nquantifying the proportion of missing citations, we conduct a comparative\nassessment of distinct journals and research sub-fields in terms of their\nability to facilitate or impede the dissemination of knowledge. Findings\nindicate that knowledge transfer seems to be more effectively facilitated by\njournals of wide visibility, such as Physical Review Letters, than by\nlower-impact ones. Our study has important implications for authors, editors\nand reviewers of scientific journals, as well as public preprint repositories,\nas it provides a procedure for recommending relevant yet missing references and\nproperly integrating bibliographies of papers.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 10:57:07 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Ciotti", "Valerio", ""], ["Bonaventura", "Moreno", ""], ["Nicosia", "Vincenzo", ""], ["Panzarasa", "Pietro", ""], ["Latora", "Vito", ""]]}, {"id": "1511.08299", "submitter": "Aditya Jami", "authors": "Matthew Long, Aditya Jami, Ashutosh Saxena", "title": "Hierarchical classification of e-commerce related social media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we attempt to classify tweets into root categories of the\nAmazon browse node hierarchy using a set of tweets with browse node ID labels,\na much larger set of tweets without labels, and a set of Amazon reviews.\nExamining twitter data presents unique challenges in that the samples are short\n(under 140 characters) and often contain misspellings or abbreviations that are\ntrivial for a human to decipher but difficult for a computer to parse. A\nvariety of query and document expansion techniques are implemented in an effort\nto improve information retrieval to modest success.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 06:57:06 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Long", "Matthew", ""], ["Jami", "Aditya", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1511.08417", "submitter": "Ziqiang Cao", "authors": "Ziqiang Cao, Chengyao Chen, Wenjie Li, Sujian Li, Furu Wei, Ming Zhou", "title": "TGSum: Build Tweet Guided Multi-Document Summarization Dataset", "comments": "7 pages, 1 figure in AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The development of summarization research has been significantly hampered by\nthe costly acquisition of reference summaries. This paper proposes an effective\nway to automatically collect large scales of news-related multi-document\nsummaries with reference to social media's reactions. We utilize two types of\nsocial labels in tweets, i.e., hashtags and hyper-links. Hashtags are used to\ncluster documents into different topic sets. Also, a tweet with a hyper-link\noften highlights certain key points of the corresponding document. We\nsynthesize a linked document cluster to form a reference summary which can\ncover most key points. To this aim, we adopt the ROUGE metrics to measure the\ncoverage ratio, and develop an Integer Linear Programming solution to discover\nthe sentence set reaching the upper bound of ROUGE. Since we allow summary\nsentences to be selected from both documents and high-quality tweets, the\ngenerated reference summaries could be abstractive. Both informativeness and\nreadability of the collected summaries are verified by manual judgment. In\naddition, we train a Support Vector Regression summarizer on DUC generic\nmulti-document summarization benchmarks. With the collected data as extra\ntraining resource, the performance of the summarizer improves a lot on all the\ntest sets. We release this dataset for further research.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 15:22:54 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Cao", "Ziqiang", ""], ["Chen", "Chengyao", ""], ["Li", "Wenjie", ""], ["Li", "Sujian", ""], ["Wei", "Furu", ""], ["Zhou", "Ming", ""]]}, {"id": "1511.08762", "submitter": "Tijl De Bie", "authors": "Tijl De Bie, Jefrey Lijffijt, Raul Santos-Rodriguez, Bo Kang", "title": "Informative Data Projections: A Framework and Two Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for Projection Pursuit aim to facilitate the visual exploration of\nhigh-dimensional data by identifying interesting low-dimensional projections. A\nmajor challenge is the design of a suitable quality metric of projections,\ncommonly referred to as the projection index, to be maximized by the Projection\nPursuit algorithm. In this paper, we introduce a new information-theoretic\nstrategy for tackling this problem, based on quantifying the amount of\ninformation the projection conveys to a user given their prior beliefs about\nthe data. The resulting projection index is a subjective quantity, explicitly\ndependent on the intended user. As a useful illustration, we developed this\nidea for two particular kinds of prior beliefs. The first kind leads to PCA\n(Principal Component Analysis), shining new light on when PCA is (not)\nappropriate. The second kind leads to a novel projection index, the\nmaximization of which can be regarded as a robust variant of PCA. We show how\nthis projection index, though non-convex, can be effectively maximized using a\nmodified power method as well as using a semidefinite programming relaxation.\nThe usefulness of this new projection index is demonstrated in comparative\nempirical experiments against PCA and a popular Projection Pursuit method.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 17:53:46 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["De Bie", "Tijl", ""], ["Lijffijt", "Jefrey", ""], ["Santos-Rodriguez", "Raul", ""], ["Kang", "Bo", ""]]}, {"id": "1511.08996", "submitter": "Yi Zhang", "authors": "Yi Zhang, Yanghua Xiao, Seung-won Hwang, Haixun Wang, X. Sean Wang,\n  Wei Wang", "title": "Entity Suggestion by Example using a Conceptual Taxonomy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity suggestion by example (ESbE) refers to a type of entity acquisition\nquery in which a user provides a set of example entities as the query and\nobtains in return some entities that best complete the concept underlying the\ngiven query. Such entity acquisition queries can be useful in many applications\nsuch as related-entity recommendation and query expansion. A number of ESbE\nquery processing solutions exist in the literature. However, they mostly build\nonly on the idea of entity co-occurrences either in text or web lists, without\ntaking advantage of the existence of many web-scale conceptual taxonomies that\nconsist of hierarchical isA relationships between entity-concept pairs. This\npaper provides a query processing method based on the relevance models between\nentity sets and concepts. These relevance models can be used to obtain the\nfine-grained concepts implied by the query entity set, and the entities that\nbelong to a given concept, thereby providing the entity suggestions. Extensive\nevaluations with real data sets show that the accuracy of the queries processed\nwith this new method is significantly higher than that of existing solutions.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 11:18:10 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Zhang", "Yi", ""], ["Xiao", "Yanghua", ""], ["Hwang", "Seung-won", ""], ["Wang", "Haixun", ""], ["Wang", "X. Sean", ""], ["Wang", "Wei", ""]]}, {"id": "1511.09009", "submitter": "Yi Zhang", "authors": "Yi Zhang, Yanghua Xiao, Seung-won Hwang, Wei Wang", "title": "Long Concept Query on Conceptual Taxonomies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of finding typical entities when the concept\nis given as a query. For a short concept such as university, this is a\nwell-studied problem of retrieving knowledge base such as Microsoft's Probase\nand Google's isA database pre-materializing entities found for the concept in\nHearst patterns of the web corpus. However, we find most real-life queries are\nlong concept queries (LCQs), such as top American private university, which\ncannot and should not be pre-materialized. Our goal is an online construction\nof entity retrieval for LCQs. We argue a naive baseline of rewriting LCQs into\nan intersection of an expanded set of composing short concepts leads to highly\nprecise results with extremely low recall. Instead, we propose to augment the\nconcept list, by identifying related concepts of the query concept. However, as\nsuch increase of recall often invites false positives and decreases precision\nin return, we propose the following two techniques: First, we identify concepts\nwith different relatedness to generate linear orderings and pairwise ordering\nconstraints. Second, we rank entities trying to avoid conflicts with these\nconstraints, to prune out lowly ranked one (likely false positives). With these\nnovel techniques, our approach significantly outperforms state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 13:20:01 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Zhang", "Yi", ""], ["Xiao", "Yanghua", ""], ["Hwang", "Seung-won", ""], ["Wang", "Wei", ""]]}, {"id": "1511.09128", "submitter": "Haibing Wu", "authors": "Haibing Wu, Yiwei Gu, Shangdi Sun and Xiaodong Gu", "title": "Aspect-based Opinion Summarization with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers Aspect-based Opinion Summarization (AOS) of reviews on\nparticular products. To enable real applications, an AOS system needs to\naddress two core subtasks, aspect extraction and sentiment classification. Most\nexisting approaches to aspect extraction, which use linguistic analysis or\ntopic modeling, are general across different products but not precise enough or\nsuitable for particular products. Instead we take a less general but more\nprecise scheme, directly mapping each review sentence into pre-defined aspects.\nTo tackle aspect mapping and sentiment classification, we propose two\nConvolutional Neural Network (CNN) based methods, cascaded CNN and multitask\nCNN. Cascaded CNN contains two levels of convolutional networks. Multiple CNNs\nat level 1 deal with aspect mapping task, and a single CNN at level 2 deals\nwith sentiment classification. Multitask CNN also contains multiple aspect CNNs\nand a sentiment CNN, but different networks share the same word embeddings.\nExperimental results indicate that both cascaded and multitask CNNs outperform\nSVM-based methods by large margins. Multitask CNN generally performs better\nthan cascaded CNN.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 01:46:15 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Wu", "Haibing", ""], ["Gu", "Yiwei", ""], ["Sun", "Shangdi", ""], ["Gu", "Xiaodong", ""]]}, {"id": "1511.09142", "submitter": "Dr. Zubair Asghar", "authors": "Muhammad Zubair Asghar, Shakeel Ahmad, Afsana Marwat, Fazal Masud\n  Kundi", "title": "Sentiment Analysis on YouTube: A Brief Survey", "comments": null, "journal-ref": "MAGNT Research Report (ISSN. 1444-8939), Vol.3 (1). PP: 1250-1257,\n  2015", "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment analysis or opinion mining is the field of study related to analyze\nopinions, sentiments, evaluations, attitudes, and emotions of users which they\nexpress on social media and other online resources. The revolution of social\nmedia sites has also attracted the users towards video sharing sites, such as\nYouTube. The online users express their opinions or sentiments on the videos\nthat they watch on such sites. This paper presents a brief survey of techniques\nto analyze opinions posted by users about a particular video.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 03:12:12 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Asghar", "Muhammad Zubair", ""], ["Ahmad", "Shakeel", ""], ["Marwat", "Afsana", ""], ["Kundi", "Fazal Masud", ""]]}, {"id": "1511.09290", "submitter": "Pedro Saleiro", "authors": "Pedro Saleiro, Lu\\'is Sarmento", "title": "\"Piaf\" vs \"Adele\": classifying encyclopedic queries using automatically\n  labeled training data", "comments": "in Proceedings of the 10th Conference on Open Research Areas in\n  Information Retrieval, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encyclopedic queries express the intent of obtaining information typically\navailable in encyclopedias, such as biographical, geographical or historical\nfacts. In this paper, we train a classifier for detecting the encyclopedic\nintent of web queries. For training such a classifier, we automatically label\ntraining data from raw query logs. We use click-through data to select positive\nexamples of encyclopedic queries as those queries that mostly lead to Wikipedia\narticles. We investigated a large set of features that can be generated to\ndescribe the input query. These features include both term-specific patterns as\nwell as query projections on knowledge bases items (e.g. Freebase). Results\nshow that using these feature sets it is possible to achieve an F1 score above\n87%, competing with a Google-based baseline, which uses a much wider set of\nsignals to boost the ranking of Wikipedia for potential encyclopedic queries.\nThe results also show that both query projections on Wikipedia article titles\nand Freebase entity match represent the most relevant groups of features. When\nthe training set contains frequent positive examples (i.e rare queries are\nexcluded) results tend to improve.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 13:08:31 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Saleiro", "Pedro", ""], ["Sarmento", "Lu\u00eds", ""]]}]