[{"id": "1401.0131", "submitter": "Avinash Bhute", "authors": "Avinash N Bhute and B B Meshram", "title": "System Analysis And Design For Multimedia Retrieval Systems", "comments": "20 pages, 12 Figures. arXiv admin note: substantial text overlap with\n  arXiv:1211.4683", "journal-ref": "The International Journal of Multimedia & Its Applications (IJMA)\n  Vol.5, No.6, December 2013", "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the extensive use of information technology and the recent\ndevelopments in multimedia systems, the amount of multimedia data available to\nusers has increased exponentially. Video is an example of multimedia data as it\ncontains several kinds of data such as text, image, meta-data, visual and\naudio. Content based video retrieval is an approach for facilitating the\nsearching and browsing of large multimedia collections over WWW. In order to\ncreate an effective video retrieval system, visual perception must be taken\ninto account. We conjectured that a technique which employs multiple features\nfor indexing and retrieval would be more effective in the discrimination and\nsearch tasks of videos. In order to validate this, content based indexing and\nretrieval systems were implemented using color histogram, Texture feature\n(GLCM), edge density and motion..\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2013 11:17:42 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Bhute", "Avinash N", ""], ["Meshram", "B B", ""]]}, {"id": "1401.0255", "submitter": "Dinesh Govindaraj", "authors": "Dinesh Govindaraj, Tao Wang, S.V.N. Vishwanathan", "title": "Modeling Attractiveness and Multiple Clicks in Sponsored Search Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click models are an important tool for leveraging user feedback, and are used\nby commercial search engines for surfacing relevant search results. However,\nexisting click models are lacking in two aspects. First, they do not share\ninformation across search results when computing attractiveness. Second, they\nassume that users interact with the search results sequentially. Based on our\nanalysis of the click logs of a commercial search engine, we observe that the\nsequential scan assumption does not always hold, especially for sponsored\nsearch results. To overcome the above two limitations, we propose a new click\nmodel. Our key insight is that sharing information across search results helps\nin identifying important words or key-phrases which can then be used to\naccurately compute attractiveness of a search result. Furthermore, we argue\nthat the click probability of a position as well as its attractiveness changes\nduring a user session and depends on the user's past click experience. Our\nmodel seamlessly incorporates the effect of externalities (quality of other\nsearch results displayed in response to a user query), user fatigue, as well as\npre and post-click relevance of a sponsored search result. We propose an\nefficient one-pass inference scheme and empirically evaluate the performance of\nour model via extensive experiments using the click logs of a large commercial\nsearch engine.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2014 06:45:58 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Govindaraj", "Dinesh", ""], ["Wang", "Tao", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1401.0480", "submitter": "Denzil Correa", "authors": "Denzil Correa and Ashish Sureka", "title": "Chaff from the Wheat : Characterization and Modeling of Deleted\n  Questions on Stack Overflow", "comments": "11 pages, Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Stack Overflow is the most popular CQA for programmers on the web with 2.05M\nusers, 5.1M questions and 9.4M answers. Stack Overflow has explicit, detailed\nguidelines on how to post questions and an ebullient moderation community.\nDespite these precise communications and safeguards, questions posted on Stack\nOverflow can be extremely off topic or very poor in quality. Such questions can\nbe deleted from Stack Overflow at the discretion of experienced community\nmembers and moderators. We present the first study of deleted questions on\nStack Overflow. We divide our study into two parts (i) Characterization of\ndeleted questions over approx. 5 years (2008-2013) of data, (ii) Prediction of\ndeletion at the time of question creation. Our characterization study reveals\nmultiple insights on question deletion phenomena. We observe a significant\nincrease in the number of deleted questions over time. We find that it takes\nsubstantial time to vote a question to be deleted but once voted, the community\ntakes swift action. We also see that question authors delete their questions to\nsalvage reputation points. We notice some instances of accidental deletion of\ngood quality questions but such questions are voted back to be undeleted\nquickly. We discover a pyramidal structure of question quality on Stack\nOverflow and find that deleted questions lie at the bottom (lowest quality) of\nthe pyramid. We also build a predictive model to detect the deletion of\nquestion at the creation time. We experiment with 47 features based on User\nProfile, Community Generated, Question Content and Syntactic style and report\nan accuracy of 66%. Our feature analysis reveals that all four categories of\nfeatures are important for the prediction task. Our findings reveal important\nsuggestions for content quality maintenance on community based question\nanswering websites.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2014 17:36:32 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Correa", "Denzil", ""], ["Sureka", "Ashish", ""]]}, {"id": "1401.0629", "submitter": "Thomas Niebler", "authors": "Stephan Doerfel and Daniel Zoller and Philipp Singer and Thomas\n  Niebler and Andreas Hotho and Markus Strohmaier", "title": "Of course we share! Testing Assumptions about Social Tagging Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social tagging systems have established themselves as an important part in\ntoday's web and have attracted the interest from our research community in a\nvariety of investigations. The overall vision of our community is that simply\nthrough interactions with the system, i.e., through tagging and sharing of\nresources, users would contribute to building useful semantic structures as\nwell as resource indexes using uncontrolled vocabulary not only due to the\neasy-to-use mechanics. Henceforth, a variety of assumptions about social\ntagging systems have emerged, yet testing them has been difficult due to the\nabsence of suitable data. In this work we thoroughly investigate three\navailable assumptions - e.g., is a tagging system really social? - by examining\nlive log data gathered from the real-world public social tagging system\nBibSonomy. Our empirical results indicate that while some of these assumptions\nhold to a certain extent, other assumptions need to be reflected and viewed in\na very critical light. Our observations have implications for the design of\nfuture search and other algorithms to better reflect the actual user behavior.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2014 11:59:34 GMT"}, {"version": "v2", "created": "Thu, 27 Mar 2014 13:03:32 GMT"}, {"version": "v3", "created": "Fri, 28 Mar 2014 09:41:21 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Doerfel", "Stephan", ""], ["Zoller", "Daniel", ""], ["Singer", "Philipp", ""], ["Niebler", "Thomas", ""], ["Hotho", "Andreas", ""], ["Strohmaier", "Markus", ""]]}, {"id": "1401.0864", "submitter": "Maryam Khademi", "authors": "Mingming Fan, Maryam Khademi", "title": "Predicting a Business Star in Yelp from Its Reviews Text Alone", "comments": "5 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yelp online reviews are invaluable source of information for users to choose\nwhere to visit or what to eat among numerous available options. But due to\noverwhelming number of reviews, it is almost impossible for users to go through\nall reviews and find the information they are looking for. To provide a\nbusiness overview, one solution is to give the business a 1-5 star(s). This\nrating can be subjective and biased toward users personality. In this paper, we\npredict a business rating based on user-generated reviews texts alone. This not\nonly provides an overview of plentiful long review texts but also cancels out\nsubjectivity. Selecting the restaurant category from Yelp Dataset Challenge, we\nuse a combination of three feature generation methods as well as four machine\nlearning models to find the best prediction result. Our approach is to create\nbag of words from the top frequent words in all raw text reviews, or top\nfrequent words/adjectives from results of Part-of-Speech analysis. Our results\nshow Root Mean Square Error (RMSE) of 0.6 for the combination of Linear\nRegression with either of the top frequent words from raw data or top frequent\nadjectives after Part-of-Speech (POS).\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2014 03:29:05 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Fan", "Mingming", ""], ["Khademi", "Maryam", ""]]}, {"id": "1401.1236", "submitter": "Sergio G\\'omez", "authors": "Sergio Gomez, Alberto Fernandez, Clara Granell, Alex Arenas", "title": "Structural patterns in complex systems using multidendrograms", "comments": null, "journal-ref": "Entropy 15 (2013) 5464-5474", "doi": "10.3390/e15125464", "report-no": null, "categories": "physics.data-an cs.IR cs.SI physics.comp-ph physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems are usually represented as an intricate set of relations\nbetween their components forming a complex graph or network. The understanding\nof their functioning and emergent properties are strongly related to their\nstructural properties. The finding of structural patterns is of utmost\nimportance to reduce the problem of understanding the structure-function\nrelationships. Here we propose the analysis of similarity measures between\nnodes using hierarchical clustering methods. The discrete nature of the\nnetworks usually leads to a small set of different similarity values, making\nstandard hierarchical clustering algorithms ambiguous. We propose the use of\n\"multidendrograms\", an algorithm that computes agglomerative hierarchical\nclusterings implementing a variable-group technique that solves the\nnon-uniqueness problem found in the standard pair-group algorithm. This problem\narises when there are more than two clusters separated by the same maximum\nsimilarity (or minimum distance) during the agglomerative process. Forcing\nbinary trees in this case means breaking ties in some way, thus giving rise to\ndifferent output clusterings depending on the criterion used. Multidendrograms\nsolves this problem grouping more than two clusters at the same time when ties\noccur.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 23:08:51 GMT"}], "update_date": "2014-01-08", "authors_parsed": [["Gomez", "Sergio", ""], ["Fernandez", "Alberto", ""], ["Granell", "Clara", ""], ["Arenas", "Alex", ""]]}, {"id": "1401.1456", "submitter": "Margarita Karkali", "authors": "Margarita Karkali, Francois Rousseau, Alexandros Ntoulas, Michalis\n  Vazirgiannis", "title": "Using temporal IDF for efficient novelty detection in text streams", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty detection in text streams is a challenging task that emerges in quite\na few different scenarios, ranging from email thread filtering to RSS news feed\nrecommendation on a smartphone. An efficient novelty detection algorithm can\nsave the user a great deal of time and resources when browsing through relevant\nyet usually previously-seen content. Most of the recent research on detection\nof novel documents in text streams has been building upon either geometric\ndistances or distributional similarities, with the former typically performing\nbetter but being much slower due to the need of comparing an incoming document\nwith all the previously-seen ones. In this paper, we propose a new approach to\nnovelty detection in text streams. We describe a resource-aware mechanism that\nis able to handle massive text streams such as the ones present today thanks to\nthe burst of social media and the emergence of the Web as the main source of\ninformation. We capitalize on the historical Inverse Document Frequency (IDF)\nthat was known for capturing well term specificity and we show that it can be\nused successfully at the document level as a measure of document novelty. This\nenables us to avoid similarity comparisons with previous documents in the text\nstream, thus scaling better and leading to faster execution times. Moreover, as\nthe collection of documents evolves over time, we use a temporal variant of IDF\nnot only to maintain an efficient representation of what has already been seen\nbut also to decay the document frequencies as the time goes by. We evaluate the\nperformance of the proposed approach on a real-world news articles dataset\ncreated for this task. The results show that the proposed method outperforms\nall of the baselines while managing to operate efficiently in terms of time\ncomplexity and memory usage, which are of great importance in a mobile setting\nscenario.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 17:43:37 GMT"}, {"version": "v2", "created": "Sun, 9 Nov 2014 15:58:35 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Karkali", "Margarita", ""], ["Rousseau", "Francois", ""], ["Ntoulas", "Alexandros", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1401.1732", "submitter": "Alessandro Sordoni", "authors": "Alessandro Sordoni and Jian-Yun Nie", "title": "Looking at Vector Space and Language Models for IR using Density\n  Matrices", "comments": "In Proceedings of Quantum Interaction 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we conduct a joint analysis of both Vector Space and Language\nModels for IR using the mathematical framework of Quantum Theory. We shed light\non how both models allocate the space of density matrices. A density matrix is\nshown to be a general representational tool capable of leveraging capabilities\nof both VSM and LM representations thus paving the way for a new generation of\nretrieval models. We analyze the possible implications suggested by our\nfindings.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 15:46:35 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Sordoni", "Alessandro", ""], ["Nie", "Jian-Yun", ""]]}, {"id": "1401.1742", "submitter": "Avinash Bhute", "authors": "Avinash N Bhute, B. B. Meshram", "title": "Content Based Image Indexing and Retrieval", "comments": "12 pages", "journal-ref": "IJGIP 2013 Vol 3 issue 4", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the efficient content based image retrieval systems\nwhich employ the color, texture and shape information of images to facilitate\nthe retrieval process. For efficient feature extraction, we extract the color,\ntexture and shape feature of images automatically using edge detection which is\nwidely used in signal processing and image compression. For facilitated the\nspeedy retrieval we are implements the antipole-tree algorithm for indexing the\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 16:22:09 GMT"}], "update_date": "2014-01-09", "authors_parsed": [["Bhute", "Avinash N", ""], ["Meshram", "B. B.", ""]]}, {"id": "1401.1766", "submitter": "Yuanyuan Zhang", "authors": "James Z. Wang, Yuanyuan Zhang, Liang Dong, Lin Li, Pradip K Srimani,\n  Philip S. Yu", "title": "G-Bean: an ontology-graph based web tool for biomedical literature\n  retrieval", "comments": "This paper has been withdrawn by the author due to errors in figure 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Currently, most people use PubMed to search the MEDLINE database, an\nimportant bibliographical information source for life science and biomedical\ninformation. However, PubMed has some drawbacks that make it difficult to find\nrelevant publications pertaining to users' individual intentions, especially\nfor non-expert users. To ameliorate the disadvantages of PubMed, we developed\nG-Bean, a graph based biomedical search engine, to search biomedical articles\nin MEDLINE database more efficiently.G-Bean addresses PubMed's limitations with\nthree innovations: parallel document index creation,ontology-graph based query\nexpansion, and retrieval and re-ranking of documents based on user's search\nintention.Performance evaluation with 106 OHSUMED benchmark queries shows that\nG-Bean returns more relevant results than PubMed does when using these queries\nto search the MEDLINE database. PubMed could not even return any search result\nfor some OHSUMED queries because it failed to form the appropriate Boolean\nquery statement automatically from the natural language query strings. G-Bean\nis available at http://bioinformatics.clemson.edu/G-Bean/index.php.G-Bean\naddresses PubMed's limitations with ontology-graph based query expansion,\nautomatic document indexing, and user search intention discovery. It shows\nsignificant advantages in finding relevant articles from the MEDLINE database\nto meet the information need of the user.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2014 18:15:31 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2014 16:00:11 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2014 15:49:22 GMT"}, {"version": "v4", "created": "Fri, 28 Feb 2014 14:55:12 GMT"}, {"version": "v5", "created": "Mon, 31 Aug 2015 19:27:00 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Wang", "James Z.", ""], ["Zhang", "Yuanyuan", ""], ["Dong", "Liang", ""], ["Li", "Lin", ""], ["Srimani", "Pradip K", ""], ["Yu", "Philip S.", ""]]}, {"id": "1401.2229", "submitter": "Jensi", "authors": "R.Jensi and Dr.G.Wiselin Jiji", "title": "A Survey on optimization approaches to text document clustering", "comments": "14 pages", "journal-ref": "International Journal on Computational Sciences & Applications\n  (IJCSA) Vol.3, No.6, December 2013", "doi": "10.5121/ijcsa.2013.3604", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text Document Clustering is one of the fastest growing research areas because\nof availability of huge amount of information in an electronic form. There are\nseveral number of techniques launched for clustering documents in such a way\nthat documents within a cluster have high intra-similarity and low\ninter-similarity to other clusters. Many document clustering algorithms provide\nlocalized search in effectively navigating, summarizing, and organizing\ninformation. A global optimal solution can be obtained by applying high-speed\nand high-quality optimization algorithms. The optimization technique performs a\nglobalized search in the entire solution space. In this paper, a brief survey\non optimization approaches to text document clustering is turned out.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 04:47:05 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Jensi", "R.", ""], ["Jiji", "Dr. G. Wiselin", ""]]}, {"id": "1401.2258", "submitter": "Benjamin Roth", "authors": "Benjamin Roth", "title": "Assessing Wikipedia-Based Cross-Language Retrieval Models", "comments": "74 pages; MSc thesis at Saarland University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work compares concept models for cross-language retrieval: First, we\nadapt probabilistic Latent Semantic Analysis (pLSA) for multilingual documents.\nExperiments with different weighting schemes show that a weighting method\nfavoring documents of similar length in both language sides gives best results.\nConsidering that both monolingual and multilingual Latent Dirichlet Allocation\n(LDA) behave alike when applied for such documents, we use a training corpus\nbuilt on Wikipedia where all documents are length-normalized and obtain\nimprovements over previously reported scores for LDA. Another focus of our work\nis on model combination. For this end we include Explicit Semantic Analysis\n(ESA) in the experiments. We observe that ESA is not competitive with LDA in a\nquery based retrieval task on CLEF 2000 data. The combination of machine\ntranslation with concept models increased performance by 21.1% map in\ncomparison to machine translation alone. Machine translation relies on parallel\ncorpora, which may not be available for many language pairs. We further explore\nhow much cross-lingual information can be carried over by a specific\ninformation source in Wikipedia, namely linked text. The best results are\nobtained using a language modeling approach, entirely without information from\nparallel corpora. The need for smoothing raises interesting questions on\nsoundness and efficiency. Link models capture only a certain kind of\ninformation and suggest weighting schemes to emphasize particular words. For a\ncombined model, another interesting question is therefore how to integrate\ndifferent weighting schemes. Using a very simple combination scheme, we obtain\nresults that compare favorably to previously reported results on the CLEF 2000\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2014 08:50:54 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Roth", "Benjamin", ""]]}, {"id": "1401.2516", "submitter": "Trisiladevi C Nagavi", "authors": "Trisiladevi C. Nagavi and Nagappa U. Bhajantri", "title": "Progressive Filtering Using Multiresolution Histograms for Query by\n  Humming System", "comments": "12 Pages, 6 Figures, Full version of the paper published at\n  ICMCCA-2012 with the same title,\n  Link:http://link.springer.com/chapter/10.1007/978-81-322-1143-3_21", "journal-ref": "Proc. of the First International Conference on Multimedia\n  Processing,Communication and Computing Applications(ICMCCA) 13-15 December\n  2012,Bangalore India,Lecture Notes in Electrical Engineering,Volume 213,Pages\n  253-265,Springer India,2013", "doi": "10.1007/978-81-322-1143-3_21", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rising availability of digital music stipulates effective categorization\nand retrieval methods. Real world scenarios are characterized by mammoth music\ncollections through pertinent and non-pertinent songs with reference to the\nuser input. The primary goal of the research work is to counter balance the\nperilous impact of non-relevant songs through Progressive Filtering (PF) for\nQuery by Humming (QBH) system. PF is a technique of problem solving through\nreduced space. This paper presents the concept of PF and its efficient design\nbased on Multi-Resolution Histograms (MRH) to accomplish searching in\nmanifolds. Initially the entire music database is searched to obtain high\nrecall rate and narrowed search space. Later steps accomplish slow search in\nthe reduced periphery and achieve additional accuracy.\n  Experimentation on large music database using recursive programming\nsubstantiates the potential of the method. The outcome of proposed strategy\nglimpses that MRH effectively locate the patterns. Distances of MRH at lower\nlevel are the lower bounds of the distances at higher level, which guarantees\nevasion of false dismissals during PF. In due course, proposed method helps to\nstrike a balance between efficiency and effectiveness. The system is scalable\nfor large music retrieval systems and also data driven for performance\noptimization as an added advantage.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2014 10:29:34 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Nagavi", "Trisiladevi C.", ""], ["Bhajantri", "Nagappa U.", ""]]}, {"id": "1401.2545", "submitter": "Sanjay Singh", "authors": "Vikram Santhalia and Sanjay Singh", "title": "Design and Development of a User Specific Dynamic E-Magazine", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": "MUITTR-2014-0001", "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet and electronic media gaining more popularity due to ease and speed,\nthe count of Internet users has increased tremendously. The world is moving\nfaster each day with several events taking place at once and the Internet is\nflooded with information in every field. There are categories of information\nranging from most relevant to user, to the information totally irrelevant or\nless relevant to specific users. In such a scenario getting the information\nwhich is most relevant to the user is indispensable to save time. The\nmotivation of our solution is based on the idea of optimizing the search for\ninformation automatically. This information is delivered to user in the form of\nan interactive GUI. The optimization of the contents or information served to\nhim is based on his social networking profiles and on his reading habits on the\nproposed solution. The aim is to get the user's profile information based on\nhis social networking profile considering that almost every Internet user has\none. This helps us personalize the contents delivered to the user in order to\nproduce what is most relevant to him, in the form of a personalized e-magazine.\nFurther the proposed solution learns user's reading habits for example the news\nhe saves or clicks the most and makes a decision to provide him with the best\ncontents.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2014 16:28:23 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Santhalia", "Vikram", ""], ["Singh", "Sanjay", ""]]}, {"id": "1401.2610", "submitter": "Andrea Ballatore", "authors": "Andrea Ballatore, David C. Wilson, Michela Bertolotto", "title": "A Survey of Volunteered Open Geo-Knowledge Bases in the Semantic Web", "comments": null, "journal-ref": "in Quality Issues in the Management of Web Information, ISRL 50,\n  pp. 93-120, Springer, 2013", "doi": "10.1007/978-3-642-37688-7_5", "report-no": null, "categories": "cs.DL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, rapid advances in web technologies, coupled with\ninnovative models of spatial data collection and consumption, have generated a\nrobust growth in geo-referenced information, resulting in spatial information\noverload. Increasing 'geographic intelligence' in traditional text-based\ninformation retrieval has become a prominent approach to respond to this issue\nand to fulfill users' spatial information needs. Numerous efforts in the\nSemantic Geospatial Web, Volunteered Geographic Information (VGI), and the\nLinking Open Data initiative have converged in a constellation of open\nknowledge bases, freely available online. In this article, we survey these open\nknowledge bases, focusing on their geospatial dimension. Particular attention\nis devoted to the crucial issue of the quality of geo-knowledge bases, as well\nas of crowdsourced data. A new knowledge base, the OpenStreetMap Semantic\nNetwork, is outlined as our contribution to this area. Research directions in\ninformation integration and Geographic Information Retrieval (GIR) are then\nreviewed, with a critical discussion of their current limitations and future\nprospects.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2014 12:04:48 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Ballatore", "Andrea", ""], ["Wilson", "David C.", ""], ["Bertolotto", "Michela", ""]]}, {"id": "1401.2618", "submitter": "Deepali Virmani", "authors": "Deepali Virmani, Vikrant Malhotra, Ridhi Tyagi", "title": "Sentiment Analysis Using Collaborated Opinion Mining", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opinion mining and Sentiment analysis have emerged as a field of study since\nthe widespread of World Wide Web and internet. Opinion refers to extraction of\nthose lines or phrase in the raw and huge data which express an opinion.\nSentiment analysis on the other hand identifies the polarity of the opinion\nbeing extracted. In this paper we propose the sentiment analysis in\ncollaboration with opinion extraction, summarization, and tracking the records\nof the students. The paper modifies the existing algorithm in order to obtain\nthe collaborated opinion about the students. The resultant opinion is\nrepresented as very high, high, moderate, low and very low. The paper is based\non a case study where teachers give their remarks about the students and by\napplying the proposed sentiment analysis algorithm the opinion is extracted and\nrepresented.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2014 12:35:57 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Virmani", "Deepali", ""], ["Malhotra", "Vikrant", ""], ["Tyagi", "Ridhi", ""]]}, {"id": "1401.2684", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "Pokkuluri Kiran Sree, Inampudi Ramesh Babu", "title": "Improving Quality of Clustering using Cellular Automata for Information\n  retrieval", "comments": "Journal of Computer Science 4 (2): 167-171, 2008,ISSN 1549-3636, 2008\n  Science Publications", "journal-ref": "Journal of Computer Science 4 (2): 167-171, 2008,ISSN 1549-3636,\n  2008 Science Publications", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering has been widely applied to Information Retrieval (IR) on the\ngrounds of its potential improved effectiveness over inverted file search.\nClustering is a mostly unsupervised procedure and the majority of the\nclustering algorithms depend on certain assumptions in order to define the\nsubgroups present in a data set .A clustering quality measure is a function\nthat, given a data set and its partition into clusters, returns a non-negative\nreal number representing the quality of that clustering. Moreover, they may\nbehave in a different way depending on the features of the data set and their\ninput parameters values. Therefore, in most applications the resulting\nclustering scheme requires some sort of evaluation as regards its validity. The\nquality of clustering can be enhanced by using a Cellular Automata Classifier\nfor information retrieval. In this study we take the view that if cellular\nautomata with clustering is applied to search results (query-specific\nclustering), then it has the potential to increase the retrieval effectiveness\ncompared both to that of static clustering and of conventional inverted file\nsearch. We conducted a number of experiments using ten document collections and\neight hierarchic clustering methods. Our results show that the effectiveness of\nquery-specific clustering with cellular automata is indeed higher and suggest\nthat there is scope for its application to IR.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2014 00:05:34 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Sree", "Pokkuluri Kiran", ""], ["Babu", "Inampudi Ramesh", ""]]}, {"id": "1401.2851", "submitter": "Md. Naseef-Ur-Rahman Chowdhury", "authors": "Md. Naseef-Ur-Rahman Chowdhury, Suvankar Paul, and Kazi Zakia Sultana", "title": "Statistical Analysis based Hypothesis Testing Method in Biological\n  Knowledge Discovery", "comments": "9 pages, published on International Journal on Computational Sciences\n  & Applications (IJCSA) Vol.3, No.6, December 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The correlation and interactions among different biological entities comprise\nthe biological system. Although already revealed interactions contribute to the\nunderstanding of different existing systems, researchers face many questions\neveryday regarding inter-relationships among entities. Their queries have\npotential role in exploring new relations which may open up a new area of\ninvestigation. In this paper, we introduce a text mining based method for\nanswering the biological queries in terms of statistical computation such that\nresearchers can come up with new knowledge discovery. It facilitates user to\nsubmit their query in natural linguistic form which can be treated as\nhypothesis. Our proposed approach analyzes the hypothesis and measures the\np-value of the hypothesis with respect to the existing literature. Based on the\nmeasured value, the system either accepts or rejects the hypothesis from\nstatistical point of view. Moreover, even it does not find any direct\nrelationship among the entities of the hypothesis, it presents a network to\ngive an integral overview of all the entities through which the entities might\nbe related. This is also congenial for the researchers to widen their view and\nthus think of new hypothesis for further investigation. It assists researcher\nto get a quantitative evaluation of their assumptions such that they can reach\na logical conclusion and thus aids in relevant re-searches of biological\nknowledge discovery. The system also provides the researchers a graphical\ninteractive interface to submit their hypothesis for assessment in a more\nconvenient way.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 18:05:15 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Chowdhury", "Md. Naseef-Ur-Rahman", ""], ["Paul", "Suvankar", ""], ["Sultana", "Kazi Zakia", ""]]}, {"id": "1401.2902", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Sukanta Sinha, Rana Dattagupta, Debajyoti Mukhopadhyay", "title": "An Alternate Approach for Designing a Domain Specific Image Search\n  Prototype Using Histogram", "comments": "10 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Everyone knows that thousand of words are represented by a single image. As a\nresult image search has become a very popular mechanism for the Web searchers.\nImage search means, the search results are produced by the search engine should\nbe a set of images along with their Web page Unified Resource Locator. Now Web\nsearcher can perform two types of image search, they are Text to Image and\nImage to Image search. In Text to Image search, search query should be a text.\nBased on the input text data system will generate a set of images along with\ntheir Web page URL as an output. On the other hand, in Image to Image search,\nsearch query should be an image and based on this image system will generate a\nset of images along with their Web page URL as an output. According to the\ncurrent scenarios, Text to Image search mechanism always not returns perfect\nresult. It matches the text data and then displays the corresponding images as\nan output, which is not always perfect. To resolve this problem, Web\nresearchers have introduced the Image to Image search mechanism. In this paper,\nwe have also proposed an alternate approach of Image to Image search mechanism\nusing Histogram.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 05:10:24 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Sinha", "Sukanta", ""], ["Dattagupta", "Rana", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "1401.3230", "submitter": "K Paramesha", "authors": "K Paramesha and K C Ravishankar", "title": "Optimization Of Cross Domain Sentiment Analysis Using Sentiwordnet", "comments": null, "journal-ref": "International Journal in Foundations of Computer Science &\n  Technology (IJFCST), Vol. 3, No.5, September 2013", "doi": "10.5121/ijfcst.2013.3504", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The task of sentiment analysis of reviews is carried out using manually built\n/ automatically generated lexicon resources of their own with which terms are\nmatched with lexicon to compute the term count for positive and negative\npolarity. On the other hand the Sentiwordnet, which is quite different from\nother lexicon resources that gives scores (weights) of the positive and\nnegative polarity for each word. The polarity of a word namely positive,\nnegative and neutral have the score ranging between 0 to 1 indicates the\nstrength/weight of the word with that sentiment orientation. In this paper, we\nshow that using the Sentiwordnet, how we could enhance the performance of the\nclassification at both sentence and document level.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2013 13:48:02 GMT"}], "update_date": "2014-01-15", "authors_parsed": [["Paramesha", "K", ""], ["Ravishankar", "K C", ""]]}, {"id": "1401.3413", "submitter": "Avneesh Saluja", "authors": "Avneesh Saluja, Mahdi Pakdaman, Dongzhen Piao, Ankur P. Parikh", "title": "Infinite Mixed Membership Matrix Factorization", "comments": "For ICDM 2013 Workshop Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rating and recommendation systems have become a popular application area for\napplying a suite of machine learning techniques. Current approaches rely\nprimarily on probabilistic interpretations and extensions of matrix\nfactorization, which factorizes a user-item ratings matrix into latent user and\nitem vectors. Most of these methods fail to model significant variations in\nitem ratings from otherwise similar users, a phenomenon known as the \"Napoleon\nDynamite\" effect. Recent efforts have addressed this problem by adding a\ncontextual bias term to the rating, which captures the mood under which a user\nrates an item or the context in which an item is rated by a user. In this work,\nwe extend this model in a nonparametric sense by learning the optimal number of\nmoods or contexts from the data, and derive Gibbs sampling inference procedures\nfor our model. We evaluate our approach on the MovieLens 1M dataset, and show\nsignificant improvements over the optimal parametric baseline, more than twice\nthe improvements previously encountered for this task. We also extract and\nevaluate a DBLP dataset, wherein we predict the number of papers co-authored by\ntwo authors, and present improvements over the parametric baseline on this\nalternative domain as well.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 02:39:15 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Saluja", "Avneesh", ""], ["Pakdaman", "Mahdi", ""], ["Piao", "Dongzhen", ""], ["Parikh", "Ankur P.", ""]]}, {"id": "1401.3457", "submitter": "S.R.K. Branavan", "authors": "S.R.K. Branavan, Harr Chen, Jacob Eisenstein, Regina Barzilay", "title": "Learning Document-Level Semantic Properties from Free-Text Annotations", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 34, pages\n  569-603, 2009", "doi": "10.1613/jair.2633", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method for inferring the semantic properties of\ndocuments by leveraging free-text keyphrase annotations. Such annotations are\nbecoming increasingly abundant due to the recent dramatic growth in\nsemi-structured, user-generated online content. One especially relevant domain\nis product reviews, which are often annotated by their authors with pros/cons\nkeyphrases such as a real bargain or good value. These annotations are\nrepresentative of the underlying semantic properties; however, unlike expert\nannotations, they are noisy: lay authors may use different labels to denote the\nsame property, and some labels may be missing. To learn using such noisy\nannotations, we find a hidden paraphrase structure which clusters the\nkeyphrases. The paraphrase structure is linked with a latent topic model of the\nreview texts, enabling the system to predict the properties of unannotated\ndocuments and to effectively aggregate the semantic properties of multiple\nreviews. Our approach is implemented as a hierarchical Bayesian model with\njoint inference. We find that joint inference increases the robustness of the\nkeyphrase clustering and encourages the latent topics to correlate with\nsemantically meaningful properties. Multiple evaluations demonstrate that our\nmodel substantially outperforms alternative approaches for summarizing single\nand multiple documents into a set of semantically salient keyphrases.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:14:31 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Branavan", "S. R. K.", ""], ["Chen", "Harr", ""], ["Eisenstein", "Jacob", ""], ["Barzilay", "Regina", ""]]}, {"id": "1401.3479", "submitter": "Yllias  Chali", "authors": "Yllias Chali, Shafiq Rayhan Joty, Sadid A. Hasan", "title": "Complex Question Answering: Unsupervised Learning Approaches and\n  Experiments", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  1-47, 2009", "doi": "10.1613/jair.2784", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex questions that require inferencing and synthesizing information from\nmultiple documents can be seen as a kind of topic-oriented, informative\nmulti-document summarization where the goal is to produce a single text as a\ncompressed version of a set of documents with a minimum loss of relevant\ninformation. In this paper, we experiment with one empirical method and two\nunsupervised statistical machine learning techniques: K-means and Expectation\nMaximization (EM), for computing relative importance of the sentences. We\ncompare the results of these approaches. Our experiments show that the\nempirical approach outperforms the other two techniques and EM performs better\nthan K-means. However, the performance of these approaches depends entirely on\nthe feature set used and the weighting of these features. In order to measure\nthe importance and relevance to the user query we extract different kinds of\nfeatures (i.e. lexical, lexical semantic, cosine similarity, basic element,\ntree kernel based syntactic and shallow-semantic) for each of the document\nsentences. We use a local search technique to learn the weights of the\nfeatures. To the best of our knowledge, no study has used tree kernel functions\nto encode syntactic/semantic information for more complex tasks such as\ncomputing the relatedness between the query sentences and the document\nsentences in order to generate query-focused summaries (or answers to complex\nquestions). For each of our methods of generating summaries (i.e. empirical,\nK-means and EM) we show the effects of syntactic and shallow-semantic features\nover the bag-of-words (BOW) features.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:33:57 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Chali", "Yllias", ""], ["Joty", "Shafiq Rayhan", ""], ["Hasan", "Sadid A.", ""]]}, {"id": "1401.3482", "submitter": "Estela Saquete", "authors": "Estela Saquete, Jose Luis Vicedo, Patricio Mart\\'inez-Barco, Rafael\n  Mu\\~noz, Hector Llorens", "title": "Enhancing QA Systems with Complex Temporal Question Processing\n  Capabilities", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 35, pages\n  775-811, 2009", "doi": "10.1613/jair.2805", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a multilayered architecture that enhances the\ncapabilities of current QA systems and allows different types of complex\nquestions or queries to be processed. The answers to these questions need to be\ngathered from factual information scattered throughout different documents.\nSpecifically, we designed a specialized layer to process the different types of\ntemporal questions. Complex temporal questions are first decomposed into simple\nquestions, according to the temporal relations expressed in the original\nquestion. In the same way, the answers to the resulting simple questions are\nrecomposed, fulfilling the temporal restrictions of the original complex\nquestion. A novel aspect of this approach resides in the decomposition which\nuses a minimal quantity of resources, with the final aim of obtaining a\nportable platform that is easily extensible to other languages. In this paper\nwe also present a methodology for evaluation of the decomposition of the\nquestions as well as the ability of the implemented temporal layer to perform\nat a multilingual level. The temporal layer was first performed for English,\nthen evaluated and compared with: a) a general purpose QA system (F-measure\n65.47% for QA plus English temporal layer vs. 38.01% for the general QA\nsystem), and b) a well-known QA system. Much better results were obtained for\ntemporal questions with the multilayered system. This system was therefore\nextended to Spanish and very good results were again obtained in the evaluation\n(F-measure 40.36% for QA plus Spanish temporal layer vs. 22.94% for the general\nQA system).\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:35:49 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Saquete", "Estela", ""], ["Vicedo", "Jose Luis", ""], ["Mart\u00ednez-Barco", "Patricio", ""], ["Mu\u00f1oz", "Rafael", ""], ["Llorens", "Hector", ""]]}, {"id": "1401.3488", "submitter": "Harr Chen", "authors": "Harr Chen, S.R.K. Branavan, Regina Barzilay, David R. Karger", "title": "Content Modeling Using Latent Permutations", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 36, pages\n  129-163, 2009", "doi": "10.1613/jair.2830", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Bayesian topic model for learning discourse-level document\nstructure. Our model leverages insights from discourse theory to constrain\nlatent topic assignments in a way that reflects the underlying organization of\ndocument topics. We propose a global model in which both topic selection and\nordering are biased to be similar across a collection of related documents. We\nshow that this space of orderings can be effectively represented using a\ndistribution over permutations called the Generalized Mallows Model. We apply\nour method to three complementary discourse-level tasks: cross-document\nalignment, document segmentation, and information ordering. Our experiments\nshow that incorporating our permutation-based model in these applications\nyields substantial improvements in performance over previously proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 05:38:17 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Chen", "Harr", ""], ["Branavan", "S. R. K.", ""], ["Barzilay", "Regina", ""], ["Karger", "David R.", ""]]}, {"id": "1401.3510", "submitter": "Saurabh  Varshney Mr.", "authors": "Saurabh Varshney and Jyoti Bajpai", "title": "Improving Performance Of English-Hindi Cross Language Information\n  Retrieval Using Transliteration Of Query Terms", "comments": "International Journal on Natural Language Computing (IJNLC) Vol. 2,\n  No.6, December 2013 http://airccse.org/journal/ijnlc/index.html", "journal-ref": null, "doi": "10.5121/ijnlc.2013.2604", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main issue in Cross Language Information Retrieval (CLIR) is the poor\nperformance of retrieval in terms of average precision when compared to\nmonolingual retrieval performance. The main reasons behind poor performance of\nCLIR are mismatching of query terms, lexical ambiguity and un-translated query\nterms. The existing problems of CLIR are needed to be addressed in order to\nincrease the performance of the CLIR system. In this paper, we are putting our\neffort to solve the given problem by proposed an algorithm for improving the\nperformance of English-Hindi CLIR system. We used all possible combination of\nHindi translated query using transliteration of English query terms and\nchoosing the best query among them for retrieval of documents. The experiment\nis performed on FIRE 2010 (Forum of Information Retrieval Evaluation) datasets.\nThe experimental result show that the proposed approach gives better\nperformance of English-Hindi CLIR system and also helps in overcoming existing\nproblems and outperforms the existing English-Hindi CLIR system in terms of\naverage precision.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2014 08:07:08 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Varshney", "Saurabh", ""], ["Bajpai", "Jyoti", ""]]}, {"id": "1401.3590", "submitter": "Karim Mahmoud", "authors": "Karim M. Mahmoud", "title": "An Enhanced Method For Evaluating Automatic Video Summaries", "comments": "This paper has been withdrawn by the author due to some errors and\n  incomplete study", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of automatic video summaries is a challenging problem. In the past\nyears, some evaluation methods are presented that utilize only a single feature\nlike color feature to detect similarity between automatic video summaries and\nground-truth user summaries. One of the drawbacks of using a single feature is\nthat sometimes it gives a false similarity detection which makes the assessment\nof the quality of the generated video summary less perceptual and not accurate.\nIn this paper, a novel method for evaluating automatic video summaries is\npresented. This method is based on comparing automatic video summaries\ngenerated by video summarization techniques with ground-truth user summaries.\nThe objective of this evaluation method is to quantify the quality of video\nsummaries, and allow comparing different video summarization techniques\nutilizing both color and texture features of the video frames and using the\nBhattacharya distance as a dissimilarity measure due to its advantages. Our\nExperiments show that the proposed evaluation method overcomes the drawbacks of\nother methods and gives a more perceptual evaluation of the quality of the\nautomatic video summaries.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 09:30:44 GMT"}, {"version": "v2", "created": "Wed, 30 Apr 2014 23:04:43 GMT"}, {"version": "v3", "created": "Tue, 19 Apr 2016 16:05:08 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Mahmoud", "Karim M.", ""]]}, {"id": "1401.3832", "submitter": "Matthew Michelson", "authors": "Matthew Michelson, Craig A. Knoblock", "title": "Constructing Reference Sets from Unstructured, Ungrammatical Text", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 38, pages\n  189-221, 2010", "doi": "10.1613/jair.2937", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vast amounts of text on the Web are unstructured and ungrammatical, such as\nclassified ads, auction listings, forum postings, etc. We call such text\n\"posts.\" Despite their inconsistent structure and lack of grammar, posts are\nfull of useful information. This paper presents work on semi-automatically\nbuilding tables of relational information, called \"reference sets,\" by\nanalyzing such posts directly. Reference sets can be applied to a number of\ntasks such as ontology maintenance and information extraction. Our\nreference-set construction method starts with just a small amount of background\nknowledge, and constructs tuples representing the entities in the posts to form\na reference set. We also describe an extension to this approach for the special\ncase where even this small amount of background knowledge is impossible to\ndiscover and use. To evaluate the utility of the machine-constructed reference\nsets, we compare them to manually constructed reference sets in the context of\nreference-set-based information extraction. Our results show the reference sets\nconstructed by our method outperform manually constructed reference sets. We\nalso compare the reference-set-based extraction approach using the\nmachine-constructed reference set to supervised extraction approaches using\ngeneric features. These results demonstrate that using machine-constructed\nreference sets outperforms the supervised methods, even though the supervised\nmethods require training data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:49:45 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Michelson", "Matthew", ""], ["Knoblock", "Craig A.", ""]]}, {"id": "1401.3865", "submitter": "Xavier Tannier", "authors": "Xavier Tannier, Philippe Muller", "title": "Evaluating Temporal Graphs Built from Texts via Transitive Reduction", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  375-413, 2011", "doi": "10.1613/jair.3118", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal information has been the focus of recent attention in information\nextraction, leading to some standardization effort, in particular for the task\nof relating events in a text. This task raises the problem of comparing two\nannotations of a given text, because relations between events in a story are\nintrinsically interdependent and cannot be evaluated separately. A proper\nevaluation measure is also crucial in the context of a machine learning\napproach to the problem. Finding a common comparison referent at the text level\nis not obvious, and we argue here in favor of a shift from event-based measures\nto measures on a unique textual object, a minimal underlying temporal graph, or\nmore formally the transitive reduction of the graph of relations between event\nboundaries. We support it by an investigation of its properties on synthetic\ndata and on a well-know temporal corpus.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:05:45 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Tannier", "Xavier", ""], ["Muller", "Philippe", ""]]}, {"id": "1401.3874", "submitter": "Fei Wu", "authors": "Fei Wu, Jayant Madhavan, Alon Halevy", "title": "Identifying Aspects for Web-Search Queries", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 40, pages\n  677-700, 2011", "doi": "10.1613/jair.3182", "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many web-search queries serve as the beginning of an exploration of an\nunknown space of information, rather than looking for a specific web page. To\nanswer such queries effec- tively, the search engine should attempt to organize\nthe space of relevant information in a way that facilitates exploration. We\ndescribe the Aspector system that computes aspects for a given query. Each\naspect is a set of search queries that together represent a distinct\ninformation need relevant to the original search query. To serve as an\neffective means to explore the space, Aspector computes aspects that are\northogonal to each other and to have high combined coverage. Aspector combines\ntwo sources of information to compute aspects. We discover candidate aspects by\nanalyzing query logs, and cluster them to eliminate redundancies. We then use a\nmass-collaboration knowledge base (e.g., Wikipedia) to compute candidate\naspects for queries that occur less frequently and to group together aspects\nthat are likely to be \"semantically\" related. We present a user study that\nindicates that the aspects we compute are rated favorably against three\ncompeting alternatives -related searches proposed by Google, cluster labels\nassigned by the Clusty search engine, and navigational searches proposed by\nBing.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:09:56 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Wu", "Fei", ""], ["Madhavan", "Jayant", ""], ["Halevy", "Alon", ""]]}, {"id": "1401.3883", "submitter": "Anna Khudyak Kozorovitsky", "authors": "Anna Khudyak Kozorovitsky, Oren Kurland", "title": "From \"Identical\" to \"Similar\": Fusing Retrieved Lists Based on\n  Inter-Document Similarities", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  267-296, 2011", "doi": "10.1613/jair.3214", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for fusing document lists that were retrieved in response to a query\noften utilize the retrieval scores and/or ranks of documents in the lists. We\npresent a novel fusion approach that is based on using, in addition,\ninformation induced from inter-document similarities. Specifically, our methods\nlet similar documents from different lists provide relevance-status support to\neach other. We use a graph-based method to model relevance-status propagation\nbetween documents. The propagation is governed by inter-document-similarities\nand by retrieval scores of documents in the lists. Empirical evaluation\ndemonstrates the effectiveness of our methods in fusing TREC runs. The\nperformance of our most effective methods transcends that of effective fusion\nmethods that utilize only retrieval scores or ranks.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:13:26 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Kozorovitsky", "Anna Khudyak", ""], ["Kurland", "Oren", ""]]}, {"id": "1401.3896", "submitter": "Oren Kurland", "authors": "Oren Kurland, Eyal Krikon", "title": "The Opposite of Smoothing: A Language Model Approach to Ranking\n  Query-Specific Document Clusters", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 41, pages\n  367-395, 2011", "doi": "10.1613/jair.3327", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting information induced from (query-specific) clustering of\ntop-retrieved documents has long been proposed as a means for improving\nprecision at the very top ranks of the returned results. We present a novel\nlanguage model approach to ranking query-specific clusters by the presumed\npercentage of relevant documents that they contain. While most previous cluster\nranking approaches focus on the cluster as a whole, our model utilizes also\ninformation induced from documents associated with the cluster. Our model\nsubstantially outperforms previous approaches for identifying clusters\ncontaining a high relevant-document percentage. Furthermore, using the model to\nproduce document ranking yields precision-at-top-ranks performance that is\nconsistently better than that of the initial ranking upon which clustering is\nperformed. The performance also favorably compares with that of a\nstate-of-the-art pseudo-feedback-based retrieval method.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:18:05 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Kurland", "Oren", ""], ["Krikon", "Eyal", ""]]}, {"id": "1401.3908", "submitter": "Ricardo Ribeiro", "authors": "Ricardo Ribeiro, David Martins de Matos", "title": "Centrality-as-Relevance: Support Sets and Similarity as Geometric\n  Proximity", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 42, pages\n  275-308, 2011", "doi": "10.1613/jair.3387", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In automatic summarization, centrality-as-relevance means that the most\nimportant content of an information source, or a collection of information\nsources, corresponds to the most central passages, considering a representation\nwhere such notion makes sense (graph, spatial, etc.). We assess the main\nparadigms, and introduce a new centrality-based relevance model for automatic\nsummarization that relies on the use of support sets to better estimate the\nrelevant content. Geometric proximity is used to compute semantic relatedness.\nCentrality (relevance) is determined by considering the whole input source (and\nnot only local information), and by taking into account the existence of minor\ntopics or lateral subjects in the information sources to be summarized. The\nmethod consists in creating, for each passage of the input source, a support\nset consisting only of the most semantically related passages. Then, the\ndetermination of the most relevant content is achieved by selecting the\npassages that occur in the largest number of support sets. This model produces\nextractive summaries that are generic, and language- and domain-independent.\nThorough automatic evaluation shows that the method achieves state-of-the-art\nperformance, both in written text, and automatically transcribed speech\nsummarization, including when compared to considerably more complex approaches.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 05:23:22 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["Ribeiro", "Ricardo", ""], ["de Matos", "David Martins", ""]]}, {"id": "1401.4529", "submitter": "Bal\\'azs Hidasi", "authors": "Bal\\'azs Hidasi, Domonkos Tikk", "title": "General factorization framework for context-aware recommendations", "comments": "The final publication is available at Springer via\n  http://dx.doi.org/10.1007/s10618-015-0417-y. Data Mining and Knowledge\n  Discovery, 2015", "journal-ref": null, "doi": "10.1007/s10618-015-0417-y", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-aware recommendation algorithms focus on refining recommendations by\nconsidering additional information, available to the system. This topic has\ngained a lot of attention recently. Among others, several factorization methods\nwere proposed to solve the problem, although most of them assume explicit\nfeedback which strongly limits their real-world applicability. While these\nalgorithms apply various loss functions and optimization strategies, the\npreference modeling under context is less explored due to the lack of tools\nallowing for easy experimentation with various models. As context dimensions\nare introduced beyond users and items, the space of possible preference models\nand the importance of proper modeling largely increases.\n  In this paper we propose a General Factorization Framework (GFF), a single\nflexible algorithm that takes the preference model as an input and computes\nlatent feature matrices for the input dimensions. GFF allows us to easily\nexperiment with various linear models on any context-aware recommendation task,\nbe it explicit or implicit feedback based. The scaling properties makes it\nusable under real life circumstances as well.\n  We demonstrate the framework's potential by exploring various preference\nmodels on a 4-dimensional context-aware problem with contexts that are\navailable for almost any real life datasets. We show in our experiments --\nperformed on five real life, implicit feedback datasets -- that proper\npreference modelling significantly increases recommendation accuracy, and\npreviously unused models outperform the traditional ones. Novel models in GFF\nalso outperform state-of-the-art factorization algorithms.\n  We also extend the method to be fully compliant to the Multidimensional\nDataspace Model, one of the most extensive data models of context-enriched\ndata. Extended GFF allows the seamless incorporation of information into the\nfac[truncated]\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2014 11:13:26 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 11:50:22 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Hidasi", "Bal\u00e1zs", ""], ["Tikk", "Domonkos", ""]]}, {"id": "1401.4740", "submitter": "Noah Friedkin", "authors": "Noah E. Friedkin", "title": "Generalization of the PageRank Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a generalization of the PageRank model of page\ncentralities in the global webgraph of hyperlinks. The webgraph of adjacencies\nis generalized to a valued directed graph, and the scalar dampening coefficient\nfor walks through the graph is relaxed to allow for heterogeneous values. A\nvisitation count approach may be employed to apply the more general model,\nbased on the number of visits to a page and the page's proportionate\nallocations of these visits to other nodes of the webgraph.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2014 21:31:59 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Friedkin", "Noah E.", ""]]}, {"id": "1401.5226", "submitter": "Nicolas Gillis", "authors": "Nicolas Gillis", "title": "The Why and How of Nonnegative Matrix Factorization", "comments": "25 pages, 5 figures. Some typos and errors corrected, Section 3.2\n  reorganized", "journal-ref": null, "doi": null, "report-no": "In: \"Regularization, Optimization, Kernels, and Support Vector\n  Machines\", J.A.K. Suykens, M. Signoretto and A. Argyriou (eds), Chapman &\n  Hall/CRC, Machine Learning and Pattern Recognition Series, pp. 257-291, 2014", "categories": "stat.ML cs.IR cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF) has become a widely used tool for the\nanalysis of high-dimensional data as it automatically extracts sparse and\nmeaningful features from a set of nonnegative data vectors. We first illustrate\nthis property of NMF on three applications, in image processing, text mining\nand hyperspectral imaging --this is the why. Then we address the problem of\nsolving NMF, which is NP-hard in general. We review some standard NMF\nalgorithms, and also present a recent subclass of NMF problems, referred to as\nnear-separable NMF, that can be solved efficiently (that is, in polynomial\ntime), even in the presence of noise --this is the how. Finally, we briefly\ndescribe some problems in mathematics and computer science closely related to\nNMF via the nonnegative rank.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 09:03:12 GMT"}, {"version": "v2", "created": "Fri, 7 Mar 2014 10:32:43 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Gillis", "Nicolas", ""]]}, {"id": "1401.5389", "submitter": "Sajib Dasgupta", "authors": "Sajib Dasgupta, Vincent Ng", "title": "Which Clustering Do You Want? Inducing Your Ideal Clustering with\n  Minimal Feedback", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 39, pages\n  581-632, 2010", "doi": "10.1613/jair.3003", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While traditional research on text clustering has largely focused on grouping\ndocuments by topic, it is conceivable that a user may want to cluster documents\nalong other dimensions, such as the authors mood, gender, age, or sentiment.\nWithout knowing the users intention, a clustering algorithm will only group\ndocuments along the most prominent dimension, which may not be the one the user\ndesires. To address the problem of clustering documents along the user-desired\ndimension, previous work has focused on learning a similarity metric from data\nmanually annotated with the users intention or having a human construct a\nfeature space in an interactive manner during the clustering process. With the\ngoal of reducing reliance on human knowledge for fine-tuning the similarity\nfunction or selecting the relevant features required by these approaches, we\npropose a novel active clustering algorithm, which allows a user to easily\nselect the dimension along which she wants to cluster the documents by\ninspecting only a small number of words. We demonstrate the viability of our\nalgorithm on a variety of commonly-used sentiment datasets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2014 04:56:03 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Dasgupta", "Sajib", ""], ["Ng", "Vincent", ""]]}, {"id": "1401.5644", "submitter": "Issam Sahmoudi issam sahmoudi", "authors": "Issam Sahmoudi and Hanane Froud and Abdelmonaime Lachkar", "title": "A new keyphrases extraction method based on suffix tree data structure\n  for arabic documents clustering", "comments": "17 pages, 3 figures", "journal-ref": "International Journal of Database Management Systems ( IJDMS )\n  Vol.5, No.6, December 2013", "doi": "10.5121/ijdms.2013.5602", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document Clustering is a branch of a larger area of scientific study known as\ndata mining .which is an unsupervised classification using to find a structure\nin a collection of unlabeled data. The useful information in the documents can\nbe accompanied by a large amount of noise words when using Full Text\nRepresentation, and therefore will affect negatively the result of the\nclustering process. So it is with great need to eliminate the noise words and\nkeeping just the useful information in order to enhance the quality of the\nclustering results. This problem occurs with different degree for any language\nsuch as English, European, Hindi, Chinese, and Arabic Language. To overcome\nthis problem, in this paper, we propose a new and efficient Keyphrases\nextraction method based on the Suffix Tree data structure (KpST), the extracted\nKeyphrases are then used in the clustering process instead of Full Text\nRepresentation. The proposed method for Keyphrases extraction is language\nindependent and therefore it may be applied to any language. In this\ninvestigation, we are interested to deal with the Arabic language which is one\nof the most complex languages. To evaluate our method, we conduct an\nexperimental study on Arabic Documents using the most popular Clustering\napproach of Hierarchical algorithms: Agglomerative Hierarchical algorithm with\nseven linkage techniques and a variety of distance functions and similarity\nmeasures to perform Arabic Document Clustering task. The obtained results show\nthat our method for extracting Keyphrases increases the quality of the\nclustering results. We propose also to study the effect of using the stemming\nfor the testing dataset to cluster it with the same documents clustering\ntechniques and similarity/distance measures.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 12:36:38 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Sahmoudi", "Issam", ""], ["Froud", "Hanane", ""], ["Lachkar", "Abdelmonaime", ""]]}, {"id": "1401.5741", "submitter": "Peter Pollner", "authors": "Gergely Tib\\'ely, P\\'eter Pollner, Tam\\'as Vicsek, Gergely Palla", "title": "Extracting tag hierarchies", "comments": "25 pages with 21 pages of supporting information, 25 figures", "journal-ref": "PLoS ONE 8, e84133 (2013)", "doi": "10.1371/journal.pone.0084133", "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tagging items with descriptive annotations or keywords is a very natural way\nto compress and highlight information about the properties of the given entity.\nOver the years several methods have been proposed for extracting a hierarchy\nbetween the tags for systems with a \"flat\", egalitarian organization of the\ntags, which is very common when the tags correspond to free words given by\nnumerous independent people. Here we present a complete framework for automated\ntag hierarchy extraction based on tag occurrence statistics. Along with\nproposing new algorithms, we are also introducing different quality measures\nenabling the detailed comparison of competing approaches from different\naspects. Furthermore, we set up a synthetic, computer generated benchmark\nproviding a versatile tool for testing, with a couple of tunable parameters\ncapable of generating a wide range of test beds. Beside the computer generated\ninput we also use real data in our studies, including a biological example with\na pre-defined hierarchy between the tags. The encouraging similarity between\nthe pre-defined and reconstructed hierarchy, as well as the seemingly\nmeaningful hierarchies obtained for other real systems indicate that tag\nhierarchy extraction is a very promising direction for further research with a\ngreat potential for practical applications.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 17:52:11 GMT"}], "update_date": "2014-01-23", "authors_parsed": [["Tib\u00e9ly", "Gergely", ""], ["Pollner", "P\u00e9ter", ""], ["Vicsek", "Tam\u00e1s", ""], ["Palla", "Gergely", ""]]}, {"id": "1401.5814", "submitter": "Johannes Schneider", "authors": "Johannes Schneider and Michail Vlachos", "title": "On Randomly Projected Hierarchical Clustering with Guarantees", "comments": "This version contains the conference paper \"On Randomly Projected\n  Hierarchical Clustering with Guarantees'', SIAM International Conference on\n  Data Mining (SDM), 2014 and, additionally, proofs omitted in the conference\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical clustering (HC) algorithms are generally limited to small data\ninstances due to their runtime costs. Here we mitigate this shortcoming and\nexplore fast HC algorithms based on random projections for single (SLC) and\naverage (ALC) linkage clustering as well as for the minimum spanning tree\nproblem (MST). We present a thorough adaptive analysis of our algorithms that\nimprove prior work from $O(N^2)$ by up to a factor of $N/(\\log N)^2$ for a\ndataset of $N$ points in Euclidean space. The algorithms maintain, with\narbitrary high probability, the outcome of hierarchical clustering as well as\nthe worst-case running-time guarantees. We also present parameter-free\ninstances of our algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 22:01:05 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Schneider", "Johannes", ""], ["Vlachos", "Michail", ""]]}, {"id": "1401.6092", "submitter": "Christopher Engstr\\\"om", "authors": "Christopher Engstr\\\"om, Sergei Silvestrov", "title": "PageRank for evolving link structures", "comments": "56 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we will look at the PageRank algorithm used as part of the\nranking process of different Internet pages in search engines by for example\nGoogle. This article has its main focus in the understanding of the behavior of\nPageRank as the system dynamically changes either by contracting or expanding\nsuch as when adding or subtracting nodes or links or groups of nodes or links.\nIn particular we will take a look at link structures consisting of a line of\nnodes or a complete graph where every node links to all others.\n  We will look at PageRank as the solution of a linear system of equations and\ndo our examination in both the ordinary normalized version of PageRank as well\nas the non-normalized version found by solving the linear system. We will see\nthat it is possible to find explicit formulas for the PageRank in some simple\nlink structures and using these formulas take a more in-depth look at the\nbehavior of the ranking as the system changes.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 12:07:29 GMT"}], "update_date": "2014-01-24", "authors_parsed": [["Engstr\u00f6m", "Christopher", ""], ["Silvestrov", "Sergei", ""]]}, {"id": "1401.6118", "submitter": "Smita Nirkhi", "authors": "Smita Nirkhi, R.V. Dharaskar", "title": "Comparative study of Authorship Identification Techniques for Cyber\n  Forensics Analysis", "comments": null, "journal-ref": "published 2013", "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authorship Identification techniques are used to identify the most\nappropriate author from group of potential suspects of online messages and find\nevidences to support the conclusion. Cybercriminals make misuse of online\ncommunication for sending blackmail or a spam email and then attempt to hide\ntheir true identities to void detection.Authorship Identification of online\nmessages is the contemporary research issue for identity tracing in cyber\nforensics. This is highly interdisciplinary area as it takes advantage of\nmachine learning, information retrieval, and natural language processing. In\nthis paper, a study of recent techniques and automated approaches to\nattributing authorship of online messages is presented. The focus of this\nreview study is to summarize all existing authorship identification techniques\nused in literature to identify authors of online messages. Also it discusses\nevaluation criteria and parameters for authorship attribution studies and list\nopen questions that will attract future work in this area.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 10:40:00 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Nirkhi", "Smita", ""], ["Dharaskar", "R. V.", ""]]}, {"id": "1401.6124", "submitter": "Fabricio de Franca Olivetti", "authors": "Fabricio Olivetti de Franca", "title": "Iterative Universal Hash Function Generator for Minhashing", "comments": "6 pages, 4 tables, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minhashing is a technique used to estimate the Jaccard Index between two sets\nby exploiting the probability of collision in a random permutation. In order to\nspeed up the computation, a random permutation can be approximated by using an\nuniversal hash function such as the $h_{a,b}$ function proposed by Carter and\nWegman. A better estimate of the Jaccard Index can be achieved by using many of\nthese hash functions, created at random. In this paper a new iterative\nprocedure to generate a set of $h_{a,b}$ functions is devised that eliminates\nthe need for a list of random values and avoid the multiplication operation\nduring the calculation. The properties of the generated hash functions remains\nthat of an universal hash function family. This is possible due to the random\nnature of features occurrence on sparse datasets. Results show that the\nuniformity of hashing the features is maintaned while obtaining a speed up of\nup to $1.38$ compared to the traditional approach.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 19:03:38 GMT"}], "update_date": "2014-01-25", "authors_parsed": [["de Franca", "Fabricio Olivetti", ""]]}, {"id": "1401.6169", "submitter": "Hossein Soleimani", "authors": "Hossein Soleimani, David J. Miller", "title": "Parsimonious Topic Models with Salient Word Discovery", "comments": null, "journal-ref": "IEEE Transaction on Knowledge and Data Engineering, 27 (2015)\n  824-837", "doi": "10.1109/TKDE.2014.2345378", "report-no": null, "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a parsimonious topic model for text corpora. In related models\nsuch as Latent Dirichlet Allocation (LDA), all words are modeled\ntopic-specifically, even though many words occur with similar frequencies\nacross different topics. Our modeling determines salient words for each topic,\nwhich have topic-specific probabilities, with the rest explained by a universal\nshared model. Further, in LDA all topics are in principle present in every\ndocument. By contrast our model gives sparse topic representation, determining\nthe (small) subset of relevant topics for each document. We derive a Bayesian\nInformation Criterion (BIC), balancing model complexity and goodness of fit.\nHere, interestingly, we identify an effective sample size and corresponding\npenalty specific to each parameter type in our model. We minimize BIC to\njointly determine our entire model -- the topic-specific words,\ndocument-specific topics, all model parameter values, {\\it and} the total\nnumber of topics -- in a wholly unsupervised fashion. Results on three text\ncorpora and an image dataset show that our model achieves higher test set\nlikelihood and better agreement with ground-truth class labels, compared to LDA\nand to a model designed to incorporate sparsity.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2014 21:47:48 GMT"}, {"version": "v2", "created": "Thu, 11 Sep 2014 20:24:41 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Soleimani", "Hossein", ""], ["Miller", "David J.", ""]]}, {"id": "1401.6399", "submitter": "Daniel Lemire", "authors": "Daniel Lemire, Leonid Boytsov, Nathan Kurz", "title": "SIMD Compression and the Intersection of Sorted Integers", "comments": null, "journal-ref": "Software: Practice and Experience Volume 46, Issue 6, pages\n  723-749, June 2016", "doi": "10.1002/spe.2326", "report-no": null, "categories": "cs.IR cs.DB cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sorted lists of integers are commonly used in inverted indexes and database\nsystems. They are often compressed in memory. We can use the SIMD instructions\navailable in common processors to boost the speed of integer compression\nschemes. Our S4-BP128-D4 scheme uses as little as 0.7 CPU cycles per decoded\ninteger while still providing state-of-the-art compression.\n  However, if the subsequent processing of the integers is slow, the effort\nspent on optimizing decoding speed can be wasted. To show that it does not have\nto be so, we (1) vectorize and optimize the intersection of posting lists; (2)\nintroduce the SIMD Galloping algorithm. We exploit the fact that one SIMD\ninstruction can compare 4 pairs of integers at once.\n  We experiment with two TREC text collections, GOV2 and ClueWeb09 (Category\nB), using logs from the TREC million-query track. We show that using only the\nSIMD instructions ubiquitous in all modern CPUs, our techniques for conjunctive\nqueries can double the speed of a state-of-the-art approach.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 16:53:37 GMT"}, {"version": "v10", "created": "Fri, 6 Mar 2015 19:50:27 GMT"}, {"version": "v11", "created": "Thu, 12 Mar 2015 01:32:13 GMT"}, {"version": "v12", "created": "Thu, 7 May 2015 00:52:07 GMT"}, {"version": "v13", "created": "Mon, 20 Apr 2020 19:42:24 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2014 16:37:12 GMT"}, {"version": "v3", "created": "Tue, 4 Feb 2014 01:02:27 GMT"}, {"version": "v4", "created": "Thu, 27 Feb 2014 23:38:50 GMT"}, {"version": "v5", "created": "Mon, 28 Apr 2014 19:15:10 GMT"}, {"version": "v6", "created": "Thu, 15 May 2014 14:57:33 GMT"}, {"version": "v7", "created": "Thu, 17 Jul 2014 19:42:12 GMT"}, {"version": "v8", "created": "Wed, 23 Jul 2014 19:16:39 GMT"}, {"version": "v9", "created": "Wed, 24 Dec 2014 20:30:29 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Lemire", "Daniel", ""], ["Boytsov", "Leonid", ""], ["Kurz", "Nathan", ""]]}, {"id": "1401.6571", "submitter": "Shibamouli Lahiri", "authors": "Shibamouli Lahiri, Sagnik Ray Choudhury, Cornelia Caragea", "title": "Keyword and Keyphrase Extraction Using Centrality Measures on\n  Collocation Networks", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword and keyphrase extraction is an important problem in natural language\nprocessing, with applications ranging from summarization to semantic search to\ndocument clustering. Graph-based approaches to keyword and keyphrase extraction\navoid the problem of acquiring a large in-domain training corpus by applying\nvariants of PageRank algorithm on a network of words. Although graph-based\napproaches are knowledge-lean and easily adoptable in online systems, it\nremains largely open whether they can benefit from centrality measures other\nthan PageRank. In this paper, we experiment with an array of centrality\nmeasures on word and noun phrase collocation networks, and analyze their\nperformance on four benchmark datasets. Not only are there centrality measures\nthat perform as well as or better than PageRank, but they are much simpler\n(e.g., degree, strength, and neighborhood size). Furthermore, centrality-based\nmethods give results that are competitive with and, in some cases, better than\ntwo strong unsupervised baselines.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2014 19:05:45 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Lahiri", "Shibamouli", ""], ["Choudhury", "Sagnik Ray", ""], ["Caragea", "Cornelia", ""]]}, {"id": "1401.6596", "submitter": "Sadi Seker E", "authors": "Sadi Evren Seker, Oguz Altun, U\\u{g}ur Ayan and Cihan Mert", "title": "A Novel String Distance Function based on Most Frequent K Characters", "comments": null, "journal-ref": "International Journal of Machine Learning and Computation (IJMLC),\n  Issn : 2010-3700, vol.4, is.2, pp.177-183, 2014", "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aims to publish a novel similarity metric to increase the speed of\ncomparison operations. Also the new metric is suitable for distance-based\noperations among strings. Most of the simple calculation methods, such as\nstring length are fast to calculate but does not represent the string\ncorrectly. On the other hand the methods like keeping the histogram over all\ncharacters in the string are slower but good to represent the string\ncharacteristics in some areas, like natural language. We propose a new metric,\neasy to calculate and satisfactory for string comparison. Method is built on a\nhash function, which gets a string at any size and outputs the most frequent K\ncharacters with their frequencies. The outputs are open for comparison and our\nstudies showed that the success rate is quite satisfactory for the text mining\noperations.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2014 23:40:46 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Seker", "Sadi Evren", ""], ["Altun", "Oguz", ""], ["Ayan", "U\u011fur", ""], ["Mert", "Cihan", ""]]}, {"id": "1401.6891", "submitter": "Gabriela Csurka", "authors": "Gabriela Csurka and Julien Ah-Pine and St\\'ephane Clinchant", "title": "Unsupervised Visual and Textual Information Fusion in Multimedia\n  Retrieval - A Graph-based Point of View", "comments": "An extended version of the paper: Visual and Textual Information\n  Fusion in Multimedia Retrieval using Semantic Filtering and Graph based\n  Methods, by J. Ah-Pine, G. Csurka and S. Clinchant, submitted to ACM\n  Transactions on Information Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia collections are more than ever growing in size and diversity.\nEffective multimedia retrieval systems are thus critical to access these\ndatasets from the end-user perspective and in a scalable way. We are interested\nin repositories of image/text multimedia objects and we study multimodal\ninformation fusion techniques in the context of content based multimedia\ninformation retrieval. We focus on graph based methods which have proven to\nprovide state-of-the-art performances. We particularly examine two of such\nmethods : cross-media similarities and random walk based scores. From a\ntheoretical viewpoint, we propose a unifying graph based framework which\nencompasses the two aforementioned approaches. Our proposal allows us to\nhighlight the core features one should consider when using a graph based\ntechnique for the combination of visual and textual information. We compare\ncross-media and random walk based results using three different real-world\ndatasets. From a practical standpoint, our extended empirical analysis allow us\nto provide insights and guidelines about the use of graph based methods for\nmultimodal information fusion in content based multimedia information\nretrieval.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2014 15:29:14 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Csurka", "Gabriela", ""], ["Ah-Pine", "Julien", ""], ["Clinchant", "St\u00e9phane", ""]]}, {"id": "1401.6931", "submitter": "David Shepherd", "authors": "Xi Ge, David Shepherd, Kostadin Damevski, Emerson Murphy-Hill", "title": "How the Sando Search Tool Recommends Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developers spend a significant amount of time searching their local codebase.\nTo help them search efficiently, researchers have proposed novel tools that\napply state-of-the-art information retrieval algorithms to retrieve relevant\ncode snippets from the local codebase. However, these tools still rely on the\ndeveloper to craft an effective query, which requires that the developer is\nfamiliar with the terms contained in the related code snippets. Our empirical\ndata from a state-of-the-art local code search tool, called Sando, suggests\nthat developers are sometimes unacquainted with their local codebase. In order\nto bridge the gap between developers and their ever-increasing local codebase,\nin this paper we demonstrate the recommendation techniques integrated in Sando.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2014 17:05:37 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Ge", "Xi", ""], ["Shepherd", "David", ""], ["Damevski", "Kostadin", ""], ["Murphy-Hill", "Emerson", ""]]}, {"id": "1401.8042", "submitter": "Kun Tu", "authors": "Kun Tu, Bruno Ribeiro, Hua Jiang, Xiaodong Wang, David Jensen, Benyuan\n  Liu and Don Towsley", "title": "Online Dating Recommendations: Matching Markets and Learning Preferences", "comments": "6 pages, 4 figures, submission on 5th International Workshop on\n  Social Recommender Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems for online dating have recently attracted much\nattention from the research community. In this paper we proposed a two-side\nmatching framework for online dating recommendations and design an LDA model to\nlearn the user preferences from the observed user messaging behavior and user\nprofile features. Experimental results using data from a large online dating\nwebsite shows that two-sided matching improves significantly the rate of\nsuccessful matches by as much as 45%. Finally, using simulated matchings we\nshow that the the LDA model can correctly capture user preferences.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2014 02:30:31 GMT"}], "update_date": "2014-02-03", "authors_parsed": [["Tu", "Kun", ""], ["Ribeiro", "Bruno", ""], ["Jiang", "Hua", ""], ["Wang", "Xiaodong", ""], ["Jensen", "David", ""], ["Liu", "Benyuan", ""], ["Towsley", "Don", ""]]}]